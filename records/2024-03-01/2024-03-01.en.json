[
  {
    "id": 39548088,
    "title": "KDE Plasma 6: A Revamped Desktop Experience",
    "originLink": "https://lwn.net/SubscriberLink/963851/0c64b8038c62432c/",
    "originBody": "LWN .net News from the source Content Weekly Edition Archives Search Kernel Security Events calendar Unread comments LWN FAQ Write for us Edition Return to the Front page User: Password:| Subscribe / Log in / New account The KDE desktop gets an overhaul with Plasma 6 [LWN subscriber-only content] Welcome to LWN.net The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider accepting the trial offer on the right. Thank you for visiting LWN.net! Free trial subscription Try LWN for free for 1 month: no payment or credit card required. Activate your trial subscription now and see why thousands of readers subscribe to LWN.net. By Joe Brockmeier February 28, 2024 It's been nearly 10 years since KDE Plasma 5, which is the last major release of the desktop. On February 28 the project announced its \"mega release\" of KDE Plasma 6, KDE Frameworks 6, and KDE Gear 24.02 — all based on the Qt 6 development framework. This release focuses heavily on migrating to Wayland, and aspires to be a seamless upgrade for the user while improving performance, security, and support for newer hardware. For developers, a lot of work has gone into removing deprecated frameworks and decreasing dependencies to make it easier to write applications targeting KDE. What's in Plasma 6 For the purpose of this article, we'll mostly look at Plasma and Frameworks, as Gear 24.02 contains too many applications like KMail, Kate, and the Kdenlive video editor that deserve more attention in their own right. I ran Fedora Kinoite's nightly releases with pre-release Plasma 6 packages, which has proved pleasantly stable and performant on an aging ThinkPad X280 with 16GB of RAM and a Core i7-8650U CPU. The difference between Plasma 5 and 6 is apparent, but not pronounced. Users who are comfortable with Plasma 5 are unlikely to feel discomfited with Plasma 6, or have a hard time adapting to the changes sprinkled throughout the desktop. Plasma 6 has a number of changes to default settings. The big change, of course, is Wayland as the default graphical session. Plasma 6 also has a smattering of smaller, less controversial changes. For example, prior to Plasma 6, the desktop defaulted to single-click to open a folder, launch a program, or open a file. Users coming from other operating systems or Linux desktop environments are often used to double-clicking to do these things. Now, KDE upstream has relented on using a single-click to open files and defaults to double-click instead. Distributions like Fedora, Kubuntu, and Manjaro had been changing the upstream default anyway, so KDE developer Nate Graham suggested disabling the feature. \"Distros are closer to users and clearly the feedback they've been getting is that double-click is a better default...Let's admit it and switch to double-click by default ourselves\". Plasma 6 is also supposed to do away with the default of using the scroll wheel on the desktop to switch virtual desktops. However, this setting is still active in Fedora Kinoite as of this writing. Scrolling to switch virtual desktops has been the default for some time, but Graham argued in another proposal to disable the feature because it can easily surprise users with unexpected and unwanted behavior. Users who prefer the old behavior can toggle it back on in the \"Mouse Actions\" settings under \"Desktop Folder Settings\", so it's not going away entirely. Another change to scrolling behavior in this release is that clicking on a scrollbar moves the window to the location clicked, rather than one \"page\" at a time. This is meant to reduce the amount of fiddling with the scroll wheel to move up or down a long \"distance\", in order to be a better option for users with repetitive strain injuries (RSIs) — or for users who'd like to avoid RSIs in the first place. Breeze is Plasma's default theme and it has been updated for Plasma 6, but it's a subtle change — sort of like repainting a room and changing the color from \"flat white\" to \"eggshell white\". It has some changes to spacing that make it feel a little less crowded, and it has fewer lines separating UI elements. The System Settings application has also been revamped. This may be more noticeable, as some of the settings have migrated to new locations. The nice thing about KDE is that so much is configurable, but finding configuration settings is still a challenge in Plasma 6. For example, the aforementioned setting to scroll virtual desktops is found in the Desktop Folder Settings application, but not in the System Settings application under the Virtual Desktop settings. Dolphin, KDE's file manager, had its configuration settings redesigned to make them easier to navigate. The prior version of Dolphin included six tabs of settings for navigation, its context menu, startup behavior, view modes, behavior of the trash, and general settings. The redesign condenses this into four tabs, scooting the navigation options and startup options into the new interface tab. It also adds a fifth tab for user feedback, with options to contribute statistics and participate in surveys. These are, as one would expect from an open-source project that respects its users, set to share no data by default. Users who wish to participate, though, can choose just how much participation they're willing to engage in. This ranges from sharing just a few details like version of the application and operating system, to more telemetry like screen resolution, time Dolphin is used, how many network shares are available, and more. The Dolphin interface changes are minor. Instead of showing recent files from today and yesterday, Dolphin now shows recent files and recent locations. Settings for file history are system-wide, and found in System Settings rather than Dolphin's settings — users can opt for keeping history \"forever\" or a period of months, or turn off history entirely. (Days or hours do not appear to be an option.) Users can also specify the applications allowed to access file history, rather than granting access to any application. Plasma 6 on Wayland has some support for high dynamic range (HDR) and color management, depending on the application and if one has a supported monitor. Sadly, the monitors I have on hand are not supported. One thing that did work nicely, however, was setting the scaling for a laptop monitor and external monitor, independently. It was easy to set the external monitor to 100% scaling while the ThinkPad screen was set to 125% so that windows appeared to be the same size when moved from one monitor to the other. The Plasma Search feature, which is part of KRunner and the Kickoff application launcher, has been refactored and is claimed to be much faster in this release. The release announcement claimed major speedups for searching local documents and for applications, while reducing CPU usage. It's hard to verify this, but KRunner did feel snappy when performing web and document searches. Spectacle, KDE's screenshot utility, now takes screenshots and recordings of the entire desktop, an application window, or just a selection of the screen. This promises to be a handy tool to create tutorials, or share a recording of application behavior when filing a bug. KDE giveth, and KDE taketh away. As is common with major updates, some features and settings have been removed due to design changes or difficulty with underlying drivers or software. For example, GUI configuration for Synaptics touchpads and evdev input devices has been removed because the drivers have been superseded with libinput in Wayland. Unmaintained features like the Air theme, icon view for System Settings, and KHotkeys were all scuttled in this release. The ability to grab wallpapers from the Unsplash free stock image site was removed due to API changes and the QuickShare applet for file transfer was dropped because it never worked as intended. Under the hood Even though Plasma 6 may not feel like a major update, a lot of work has gone into KDE Frameworks 6 to make it possible. I asked KDE developer Carl Schwan by email about the developer-facing changes and plans for KDE 5 now that Plasma 6 has been released. Schwan said most of the work in Frameworks 6 was about reducing rather than adding features. Schwan pointed to removal of deprecated frameworks, like KHtml, the KJS javascript engine, and KHotkeys. The project has also worked to get rid of deprecated Qt APIs, such as QtCodecs, and to decrease dependencies between frameworks so external Qt applications can just use one or two KDE frameworks. Schwan also said that KDE has removed a lot of APIs \"which were barely used or [...] have better alternatives either in another framework or in Qt itself\". In particular, he noted that KDE's plugin system has moved from two APIs to a single API. Schwan said that Qt 6 itself didn't have many API changes, but it did add an abstraction layer for graphics APIs like Metal, Vulkan, OpenGL, and DirectX \"instead of only supporting OpenGL+Angle\". In addition, Qt has switched to CMake, away from the qmake build system, which Schwan said helped a lot to improve developer tooling. Finally, Qt 6 brought a number of improvements to Qt Wayland, which Schwan said had been driven forward in part by KDE developers. Support for KDE 5 and X11 Plasma 6 is likely to be a little bit rough around the edges for a while, and users might want to review known issues before deciding to upgrade. Obviously Plasma 6 won't be immediately available in most distributions, but users can refer to KDE's community wiki for instructions on how to test Plasma 6 right away. Users can choose to build from source, try the KDE neon testing edition, or try one of the other distribution-specific methods for Fedora, Gentoo, KaOS, NixOS, or openSUSE. There is no rush to switch — KDE 5 is not quite out of the picture just yet. On February 12, on the Plasma development list, David Edmundson said he'd seen enough patches that should go into 5.27 to warrant another release. Justin Zobel agreed and noted that \"many distros won't [adopt] it for some time. Major bugfixes and security fixes should definitely continue being applied until such time that most major distros have updated to 6\". Valorie Zimmerman, from the Kubuntu project, said this is good news since the next long-term support (LTS) release for Kubuntu is coming in March and won't be based on Qt 6. On February 19, Jonathan Riddell reported the Plasma team planned to do a Plasma 5.27.11 release on March 6. Even though many in the Fedora project are eager to drop X11 support, KDE upstream plans to continue including X11 support for users who depend on it in the short term. Users can expect to see support in Plasma 6 as well, but Schwan says there's \"no fixed timeline\" with various estimates ranging from two to five years before support is fully removed. He stressed that there will be \"plenty of communication beforehand\" and the project \"certainly won't drop the support from one day to the other\". Overall, Plasma 6 looks to be a smooth upgrade for users, and KDE Frameworks 6 seems to be a solid foundation for the next few years of KDE development. It should be interesting to watch how Plasma evolves over the next few years. Did you like this article? Please accept our trial subscription offer to be able to see more content like it and to participate in the discussion. (Log in to post comments) The KDE desktop gets an overhaul with Plasma 6 Posted Feb 28, 2024 19:51 UTC (Wed) by jfebrer (subscriber, #82539) [Link] Congratulation to the team! It's a really awesome release! The KDE desktop gets an overhaul with Plasma 6 Posted Feb 28, 2024 22:20 UTC (Wed) by hrw (subscriber, #44826) [Link] \"aspires to be a seamless upgrade for the user\" is a key. KDE/Plasma 6 upgrade also means saying goodbye to all 3rdparty Plasma widgets because they need to be updated. KWin scripts needs to be updated as well. Nice to have a new version still. The KDE desktop gets an overhaul with Plasma 6 Posted Feb 29, 2024 2:19 UTC (Thu) by flussence (subscriber, #85566) [Link] Looks like this release went much better than 4.0 and 5.0 did. It might even be bearable in Gentoo now that they have official binary packages! qtwebengine's rapid upgrade schedule combined with being perpetually 40-50 versions behind chromium and being mostly used for trivial yet non-optional features was a source of many pitchforks. The KDE desktop gets an overhaul with Plasma 6 Posted Feb 29, 2024 10:59 UTC (Thu) by jnareb (subscriber, #46500) [Link] I do wonder if sharing the application window, and/or sharing the whole desktop area, works from in-browser video conference applications, such as Big Blue Button or Discord, or you are limited to sharing browser tabs only. The KDE desktop gets an overhaul with Plasma 6 Posted Feb 29, 2024 11:23 UTC (Thu) by meven-collabora (subscriber, #168883) [Link] It does, using either XWaylandVideoBridge https://www.phoronix.com/news/KDE-XWaylandVideoBridge or Xdg-desktop-portal and pipewire for Wayland native. Ref: http://blog.davidedmundson.co.uk/blog/xwaylandvideobridge/ Copyright © 2024, Eklektix, Inc. Comments and public postings are copyrighted by their creators. Linux is a registered trademark of Linus Torvalds",
    "commentLink": "https://news.ycombinator.com/item?id=39548088",
    "commentBody": "The KDE desktop gets an overhaul with Plasma 6 (lwn.net)765 points by jrepinc 22 hours agohidepastfavorite555 comments seancolsen 19 hours agoI'm loving Plasma 6 so far. Wayland support is much better! I had been using a keyboard shortcut to switch to the previously-used desktop. When KDE removed it [1], I filed a bug [2]. Hours later, a KDE dev created a new KWin script [3] to replace this functionality, fixing my workflow. THANKS! KDE is awesome! [1]: https://invent.kde.org/plasma/kwin/-/merge_requests/3871 [2]: https://bugs.kde.org/show_bug.cgi?id=481985 [3]: https://invent.kde.org/vladz/switch-to-previous-desktop reply rstuart4133 13 hours agoparentI'm using KDE on Debian / Wayland because I was forced to [0]. I moved to it from from Gnome, which I was forced to use for similar reasons. I can't believe it, but I badly miss the \"Super\" (Windows logo) button on KDE not behaving the same was as Gnome. On KDE Ctrl-F9 does the same thing, but after using Gnome that function became \"the\" way I flipped between hidden Windows. The \"Super\" button is right place for it, Ctrl-F9 is far too fiddly. The task bar I was brought up in in my Windows / Mac days is just hopeless for task switching in comparison. The rest of KDE (particularly it's configurability) is better than Gnome, of course. Except for bugs. KDE has so many UI glitches and bugs compared to Gnome. It drives me nuts. I might give Plasma 6 a go, but if the bug situation hasn't improved I will be moving onto something else. These bugs have nothing to do with Wayland per se. [0] I have a Thinkpad X1 extreme gen 2. A beautiful laptop on paper also in person because it's 4K OLED screen, but I'd never have another one. Charging from the USB-C connector is a lottery - but can be made to work with enough reinsertions. The 4K screen is scratched by the keyboard because the keys touch when closed. On the gen 2 they pushed the external video path through the Nvidia card. You can get an external monitor to work if you hold your head just the right way. With Debian 11 the right was to run Wayland, and only Gnome supported it well. With Debian 12, the right way is to boot using Gnomes display manager (gdm3) with Wayland, wait until the monitor sync's, then login using your KDE Wayland desktop. If for example you use Gnome as your desktop all you get is blank screens. Other combinations all fail in their own unique ways. reply EasyMark 2 hours agorootparentIt's pretty easy to reassign/unassign keys however you like. reply woodrowbarlow 12 hours agorootparentprevi'm in a similar boat -- i miss being able to tap the super key. i don't mind that the defaults are different, but i'm sad that (since it's considered a modifier key) KDE doesn't allow it to be bound on tap. this prevents me from replicating Gnome's behavior. reply emilsedgh 11 hours agorootparentYou can do that. I'm not on KDE right now but basically there's 2 steps: 1. You can go to System Settings -> Keyboard and there, enable Super key to act as another button. 2. Set the shortcut to that another button. reply Per_Bothner 9 hours agorootparentOn Gnome, the Super (Windows) key does an Expose (show windows reduced-size and non-overlapping) and lets you launch applications (and more). On KDE 5, the Super key brings up the Application Launcher, which is nice. And Super+W (which isn't too painful to type) does an Expose. But it would be nice if there was an option for Super to do an Expose and bring up Application Launcher. reply troyvit 16 hours agoparentprev> I'm loving Plasma 6 so far. Wayland support is much better! I'm jealous. I lasted about half an hour on Wayland, but several apps I use still don't work. xtrlock (anti-cat measures) and freetube both wouldn't work, but worse was that games like Dying Light crash almost immediately. On KDE 6 / X11 it's a little better but the game still craters after an hour. Still figuring out why. Maybe it's because the laptop is an AMD ecosystem. reply gtirloni 16 hours agorootparentI'd imagine XWayland Xorg emulation is far from perfect so I wouldn't be surprised if games that depend on that would crash. That being said, I recently switched to Wayland again after a hiatus and it seems support keeps improving. I'm not using proprietary NVIDIA drivers currently so that might be it. reply bitwize 12 hours agorootparentThe thing that you have to remember about Xwayland is that it is Xorg. It just has a Wayland DDX[0] on the back end rather than a device-specific DDX or one that talks e.g., directly to the modesetting driver. [0] Device-Dependent X, i.e., the bits of X that talk directly to the display. Contrasted with Device-Independent X (DIX), i.e., the bits that do state tracking and protocol communication with clients. reply angiosperm 2 hours agorootparentSo, it is the Wayland DDX part that is buggy? How is that different from \"Wayland Xorg emulation is buggy\"? reply bogwog 16 hours agorootparentprev> freetube I don't know about the other apps/games, but I use freetube all the time on my KDE5/Nvidia/Wayland system and have never had an issue with it. Which distro/gpu/driver version are you on? reply rossy 8 hours agorootparentprevIf a game doesn't work in Wayland, you could always launch it in gamescope[1], which AFAIR doesn't expose WAYLAND_DISPLAY by default, so games should treat it the same as an X11 desktop. [1]: https://github.com/ValveSoftware/gamescope reply bigstrat2003 6 hours agorootparentI haven't had anything flat out refuse to work on Wayland, but unfortunately Discord won't work right. It runs, but it can't detect that you're AFK any more, so you stop getting messages on the mobile app. It's a bummer because otherwise I would love to use Wayland. reply tamimio 16 hours agorootparentprev> freetube I have it and works fine on plasma6/wayland. reply troyvit 13 hours agorootparentThis must just be me then. I'll give it another shot! reply MBCook 17 hours agoparentprevCan you explain what’s so much better about Wayland support? reply sho_hn 17 hours agorootparentThere's a large amount of robustness improvements, particularly around multi-monitor and docking scenarios with dynamic and fractional DPI. We've also introduced technology to allow client apps to stay running should the compositor crash and restart. We've replaced some originally homebrew Wayland protocol extensions with newer extensions maintained by the wider Wayland community. For example, our own panels now use the layer-shell protocol. This improves interoperability, e.g. enabling third-party panels. We've added initial support for HDR and color management, in particular for games with HDR rendering (we've been learning a lot about the gaming community and their needs from the Steam Deck). More complete porting of many little quality-of-life workspace and toolkit features and refinements when running in Wayland. Performance work. Screen sharing got a revamp, now supporting RDP and the latest portal dialogs when invoked by apps and so on. Various other compositor-y bits, e.g. support for the Presentation Time frame scheduling extension, which helps video players and game engines. Some of these got done in Plasma and KDE software itself, some in Qt 6, where we've been a major contributor to the QtWayland module. Some required contributions to the Wayland protocol stack itself, e.g. the modern focus handover protocol. reply hellcow 9 hours agorootparentI installed plasma6 on my Framework 13 running NixOS this morning, and it was the first time in my life that not only did fractional scaling work out-of-the-box, but it automatically picked the appropriate scale! Love when the little things work so well. Thank you! reply FirmwareBurner 17 hours agorootparentprevThank you(plural) for all your efforts. Donated as well. I feel like this iteration of KDE will finally convince me to move to linux permanently. reply sho_hn 15 hours agorootparentThanks for the donation, it really helps :-) reply mostlysimilar 16 hours agorootparentprevHDR support is huge and is the last thing preventing me from ditching Windows on my gaming PC. reply Zambyte 16 hours agorootparentprevSuper excited to play around with this on my Steam Deck! Wayland support was actually one of the main reasons I left KDE on my primary machine, eventually in favor of Sway. Really glad to see so much progress has been made on that front :) reply MrDrMcCoy 13 hours agorootparentprevThat RDP screen sharing is very interesting. Does it require an open session attached to a real screen, or is starting headless remote sessions possible? reply veidr 5 hours agorootparentInteresting, I just posted a question to Reddit about this[1] but I have been testing and using KRdp myself for a while. I'm one of those users who (due to blah blah blah) must use Wayland, but also must have some remote desktop capability (to connext to my Linux GUI environment). For this reason, I was stuck on Fedora GNOME for a long while, because only they had it. KRdp[2][3] looks promising, even though (like GNOME) it requires a GUI session to exist first, before you can connect via RDP. But there are ways to deal with that if you really gotta have it. But this support is not actually implemented in any app in KDE Plasma 6, is it? My understanding is that it is possible, but there is no built-in functionality to take advantage of it. More \"you can build an RDP server now that will work on Wayland\" than \"there is an RDP server that has been released\". (I think?) [1]: My reddit question, which also includes how to download and try the KRdp alpha/example thing: https://www.reddit.com/r/kde/comments/1b3kc0p/what_ever_happ... [2]: the project itself doesn't have a lot of info: https://invent.kde.org/plasma/krdp [3]: the announcement blog has more: https://debugpointnews.com/krdp-wayland/ reply maybenotmart 28 minutes agorootparentprevyes, for now it shares your open session, but we plan to expand on it in the future so you could have completely headless sessions reply stock_toaster 10 hours agorootparentprevI believe[1] this is just attaching to an open session. From what I have read, login sessions require integration with a login manager, and from what I understand that work isn't done yet (this work being a prerequisite). It seems[2] that gnome is going to support this in a future release (march?)? I'm sure KDE will have something similar in the works eventually. I currently use xrdp/xorg-xrdp for headless remote login sessions, and while it works ok, I would love to switch to using something more integrated into kde itself. [1]: https://discuss.kde.org/t/remote-desktop-using-the-rdp-proto... [2]: https://www.phoronix.com/news/GNOME-RDP-Remote-Login reply maybenotmart 26 minutes agorootparentprevalso in next point Plasma releases we'll focus to make its experience more out of the box, to be able to configure it from System Settings and what not reply nijave 14 hours agorootparentprev>We've also introduced technology to allow client apps to stay running should the compositor crash and restart Sweet this is one of the reasons I gave up on KDE 5 I tried a couple months ago. Some combination of KVM and amdgpu was causing crashes (I think something to do with hotplugging displays and Wayland) and it seemed like everything downstream got nuked as well reply __loam 16 hours agorootparentprevI'm not sure if it has anything to do with KDE itself or if it's Kubuntu's fault but one small annoyance I've dealt with is how my wacom drawing tablet is mapped to the screen space. I've had to manually map it so that the tablet touch space isn't spread across three monitors and I've also had this setting get reset. I'm really excited to see more support for multiple monitors coming down the pipe. Do you know anything about the tablet issue or is this something I just need to do some more research on? reply pjerem 4 hours agorootparentActually you can configure all of this with xsetwacom. I don’t remember the exact commands and it’s mildly tricky but after that you get a configuration file that shouldn’t move (or that you can backup). reply MBCook 16 hours agorootparentprevWow. That’s quite a lot. Thanks. reply indymike 6 hours agorootparentprevFaster, better support for multiple screens with different geometry, safer, and ultimately maintained. Also, at this point, it works really well, and honestly is not a drag on the user. I moved to Wayland last December and have really enjoyed it. I'm using the KDE Neon distribution, and it's really really really nice. reply graphe 3 hours agorootparentprevIt's safer as every program is isolated but it crashes more often because programs are isolated and expect not to be isolated. I often switch back to x11 whenever I try it I think nice it's fast and has bells and whistles but there basics aren't there. It took x11 a LONG time to be usable so I expect the same with Wayland. reply smallerfish 21 hours agoprevGood job KDE team - nice to see steady progress. I encourage anybody using KDE to occasionally file tickets at bugs.kde.org; Nate is a powerhouse and seems to review all inbound tickets, and anything critical will reliably get worked on within a reasonable period of time. They're also very open to ideas and feedback (that fit into their general UX guidelines.) I would love to see more distros switch to an opinionated KDE (and also to KDE by default). It's so malleable, and yet most distros just dump the basic default setup on users. reply contrarian1234 20 hours agoparent\"I encourage anybody using KDE\" Don't you effectively need to be running Neon, b/c you can only file tickets against the latest version? I have lots of small bugs with Ubuntu LTS (particularly with KDE Connect transfers) - but I assume nobody is interested in those. They're also basically impossible to replicate (ex: \"Transfer failed for unknown reason\" or \"File arrived corrupted for unknown reason\") reply bayindirh 19 hours agorootparentI use Debian Testing which trails release by a couple of versions. I search the bug in the Bugzilla, and if it's not filed, I file the bug. Sometimes it's marked as a duplicate (but additional feedback is useful), sometimes new, but very rarely a duplicate of a closed bug. So, you don't have to use Neon. KDE is a massive project. reply topaz0 20 hours agorootparentprevI don't know how much it affects priority, but they certainly accept bugs filed against older versions. Specifying versions is a big part of the form for a new bug, and they let you select versions going way back. reply Filligree 19 hours agorootparentI have a feeling they wouldn't accept bugs filed against NixOS, however, considering the huge number of patches applied. Which is a problem. NixOS remains by far the least aggravating OS I know, and... yeah. I wish there existed a desktop environment that played well with it. reply topaz0 18 hours agorootparentI second my sibling comment -- just submit a bug. I think you are wrong that they would ignore it just because their context is somewhat different. Linux is full of differences like this. But also: Isn't one of the main benefits of NixOS that you can fairly painlessly get and try different versions of things? Just see if you can reproduce the bug in the same version of upstream. If so, file the bug with kde, if not, file the bug with NixOS because it's presumably caused by one of the patches. reply NoahKAndrews 18 hours agorootparentI think the point is that it won't run on NixOS without patches reply viraptor 3 hours agorootparentThis concern is a bit overblown here. There are 23 KDE patches at the moment, across multiple packages. kwin itself has 5, plasma-desktop has 4. Each one has an extremely limited scope - mostly one line changes related to paths rather than visible functionality. If you have a specific issue under nixos, it's going to be fairly easy to see if it's very unlikely to be caused by custom patches. reply bombcar 13 hours agorootparentprevFrom my experience, reproducibility is king. If you can give a formula to reproduce a bug, even if it is obscure, it makes it so much easier to track down. That can be up to and including a downloadable VM image that shows the issue. reply Arnavion 6 hours agorootparentprevFWIW your attitude is correct in general. When you have a bug with a distro package and you haven't root-caused it yourself to a bug in upstream source, the best thing is to report it on the distro tracker. The distro package maintainer can then do that root-causing, and if they determine it's an upstream bug they can forward it to upstream. This is how it used to work (and still does with enterprise distros, because you might as well use the support contract you paid for). But users these days have gotten savvy enough to start engaging with upstream directly, especially since many upstreams have made it easier to be engaged with by using GitHub etc instead of mailing lists etc. Sometimes engaging directly with upstream works, as it apparently does with KDE according to other comments in this thread. Sometimes upstream gets annoyed because they only care about their tree, eg systemd devs get annoyed when users report bugs that have long been fixed in master, but still exist in an old stable release that happens to be the latest on some LTS distro. It depends on the project, so when in doubt start with the distro tracker. reply ParetoOptimal 19 hours agorootparentprevJust submit a bug. If they dont accept it submit a bug to NixOS about KDE not accepting NixOS KDE users bugs. reply aseipp 14 hours agorootparentprevThey have accepted bugs from NixOS before (there are nearly a 100 marked as \"NixOS Linux\" at bugs.kde.org), and I don't see any major reason they would stop. It's true there are many patches for the KDE libraries and Plasma, but realistically most of them are fairly \"procedural\" changes to adapt to non-FHS layouts, etc. For reference I count about ~66 patch files among the KDE expression in nixpkgs as of today, including Plasma, all libraries, and related apps. Most of them are in the range of 20 lines long, and they are .patch files, i.e. the actual applied diff is smaller than that. The largest patch is barely 190 lines long and it's for Akonadi, mostly rewriting hardcoded FHS paths throughout the codebase. I agree there are some quirks with most desktop environments on NixOS in my experience but realistically there's a huge amount of stuff in the ecosystem that plays anywhere from well-to-poorly in such environments, and the Linux desktop stack definitely was not designed at a time where this stuff was common. It is what it is, I guess. reply kekebo 18 hours agorootparentprevOut of interest, what are the issues you're facing? I'm having a great time with NixOS / Plasma 5 reply Aerbil313 13 hours agorootparentprevI am using KDE on NixOS and IME there are no issues except the occasional screen freeze on X11 with Nvidia cards (and none with Wayland, you read that right!). Filed bugs and merged changes too. reply skykooler 17 hours agorootparentprevI don't know how long it will take KDE 6 to arrive on Ubuntu LTS, but there have been several networking improvements to KDE Connect in this release (including supporting mDNS and bluetooth for more reliable operation), so possibly it may be better for your use case now? reply MrDrMcCoy 13 hours agorootparentYou can always add the NEON repos to get KDE 6 on Ubuntu LTS. I'm running with that right now on 22.04. Be sure to also add a repo to get newer Pipewire, as that really helps to avoid many papercuts. reply grepfru_it 15 hours agorootparentprev>how long it will take KDE 6 to arrive on Ubuntu LTS Probably Ubuntu 26. The release notes for 24.04 do not mention kde6. It is also likely the 25.04 or 25.10 release will incorporate kde6. reply jamesgeck0 13 hours agorootparentprevAny rolling release distro is probably fine? openSUSE Tumbleweed generally gets new packages within a week or two of release. reply wkat4242 20 hours agorootparentprevIt works great on FreeBSD and I get the latest versions within days of release. reply ceeam 19 hours agorootparentI will be _very_ surprised if there's a KDE6 in official ports within three months. reply vedranm 18 hours agorootparentIt's already there for several months. From the last status report: > KDE Frameworks 6 (alpha) 5.247 was updated in the ports tree. > KDE Plasma Desktop 6 (beta 2) 5.91.0 was updated in the ports tree. https://www.freebsd.org/status/report-2023-10-2023-12/#_kde_... reply Aerbil313 13 hours agoparentprevI can't believe! I recently filed a ticket and the guy who reviewed was the KDE leader? If you are reading this, thank you Nate. reply moffkalast 18 hours agoparentprevIt's great to see, though my main gripe with KDE right now is Dolphin, the file manager. It tries to do everything but is just ever so slightly buggy in every way, it can't run as root, and asks to confirm saving twice every single time when editing a networked file. As much as it is less featured and ugly, Nautilus was less annoying to use. reply COGlory 18 hours agorootparentDolphin is legitimately my favorite KDE program. My experience is that it's a phenomenal productivity tool. Being able to quickly open or close a terminal. How customizable the ordering and appearance and columns are. Easy to manipulate tabs. I think I can count on one hand having a root file manager would be beneficial. Are you logging into a desktop as root? reply binkHN 17 hours agorootparent> Dolphin is legitimately my favorite KDE program. Happiness is clicking on a folder with lots of files, hitting forward slash, typing a search term, and instantly finding your file. reply dizhn 14 hours agorootparentI thought I didn't even have to hit / ? Perhaps that's in file picker mode. reply moffkalast 17 hours agorootparentprev> Are you logging into a desktop as root? Nah, but I would occasionally like to move things around outside of /home without doing it manually in the terminal. I'm not sure why that's such a problem. reply kuschku 17 hours agorootparentDolphin can do that! You'll need to install kio-admin and you'll get an option to \"open folder as root\". After you authenticate, you'll be able to e.g. move files in /etc/ See https://invent.kde.org/system/kio-admin for details. This will be included in dolphin natively in the near future. reply codewiz 15 hours agorootparentThank you for the tip, didn't know that! reply dotancohen 8 hours agorootparentprevI promise not to tell you \"you're doing it wrong\" but I would love to know what your use case is. What files are you moving so often, and why? Thank you. reply mikae1 17 hours agorootparentprevI've historically had a lot of problems with Dolphin too. It has gotten a lot better though. Try https://github.com/lxqt/pcmanfm-qt as an alternative. It feels native in Plasma/Breeze and is more traditional. I like it. reply ykonstant 12 hours agorootparentprevI wonder, why are more people not using Krusader? I understand it is a nuclear bomb for killing mosquitos, but with a bit of tweaking it can be fast and easy to use; plus, when you really need the big guns, you have them right there. reply JohnFen 11 hours agorootparentI've been using KDE since the beginning, and have never heard of Krusader before reading your comment. Maybe I'm not the only one, and that's why? Looking at it quickly, it doesn't seem to be my cup of tea, but until now I didn't even know it was there to consider. reply thriftwy 21 hours agoparentprevKDE should probably invest in better defaults if these need tweaking. People don't usually dig in the settings menu unless something is bothering them. If there are great opt-in features they're going to stay off. reply sho_hn 20 hours agorootparent> KDE should probably invest in better defaults if these need tweaking. We've done that a lot the last couple of years! We've changed many defaults to values that reflect better what the users actually use, based on reviewing what distros do, studies, and opt-in telemetry. A lot of this already happened in the back half of the 5.x era, but 6.0 includes additional changes in this regard. And you're not wrong, it does help a lot. reply moffkalast 18 hours agorootparentGreat, but konsole tabs on the bottom by default? Why? reply alxlaz 17 hours agorootparentThis is an excellent illustration of why \"better defaults\" is a gateway to endless bikeshedding. One person's \"better defaults\" are another person's \"why?\" The only \"better\" defaults are those that match what people already know, not necessarily because they're objectively better but because most people will already know how to use them. You literally can't get a learning curve better than \"you already know it\". Konsole has had tabs at the bottom for about 25 years now (I don't recall KDE 1.x, but they were definitely there in 2.x). Who do you prioritize in a design? Everyone who already uses KDE, and expects them at the bottom, or a subset of users who might switch to KDE and expect them at the top? More importantly, is the position of tabs -- especially one that you can change! -- like, a real, actual problem? reply bee_rider 15 hours agorootparentI wonder if it would be worthwhile to have a “here are some of the options we’ve got, pick one, there’s no default!” splash screen on first run. reply alxlaz 14 hours agorootparentKDE had exactly that back in its 3.x era, actually. It had a first run wizard that allowed you to choose things like whether you open things with a single or a double click, and options were largely organised based on platforms with similar conventions (as in, it had options like \"single click selects, double click opens (Windows style)\"). It was remarkably friction-free actually, people could just pick the mode that they were already familiar with and that was that. It had all the good parts of \"the right default\" (i.e. the \"right\" default was always the one you liked best) and required exactly one click to configure. reply jorvi 14 hours agorootparentprevZorin OS takes this approach (and pretty far). At first boot you get to select Windows, Mac, Unity or Zorin style and it shifts a bunch of things around based on that. It would be nice for KDE to have three presets: Windows, Mac, and Classic (= KDE). reply Dalewyn 14 hours agorootparentprev>Who do you prioritize in a design? Everyone who already uses KDE, and expects them at the bottom, or a subset of users who might switch to KDE and expect them at the top? If you want to grow your user base: The latter. reply alxlaz 41 minutes agorootparent> If you want to grow your user base: The latter. Prioritizing the needs of potential new users may bring you actual new users, but not prioritizing those of existing users may send some of them away. How do you know which group is bigger? Sure, the group of potential users is massive but not all of them will switch. Meanwhile you're making software worse for people who actually use it and, at least for FOSS software, are usually your biggest advocates, part of the developer base, and one of the most important means through which new users are brought in. reply eitland 13 hours agorootparentprevNooooo! That is the most frustrating thing about some projects: They take existing users for granted and make a lot of changes to accommodate the new users they envision coming in torrents. These users of course never arrive and in the meantime they have alienated the old user base. With KDE you can put the tabs where you want them. Or, if you want everything to be like in Gnome or Windows or Mac you can just use these. reply aseipp 13 hours agorootparentIn KDE's case the \"you can just customize it\" works both ways though. They could change the defaults and instead and let old users customize it back to the old way it was, rather than every new user customizing it. It comes across as a pretty weak argument. The reality is that most users are simply bitterly opposed to change, especially in \"subjective\" parts of the system like UI design, and it has nothing to do with whether or not the change is actually an improvement that helps people, or can be undone with in 1 minute through a KWin tweak, or whatever. The very example you're theorizing about (accomodating new users who don't yet exist through UI improvements) actually has happened before with positive and negative examples e.g. Blender's complete UI overhaul in 2.8 which was widely praised, versus Gimp which continues to receive flack for its UI choices, versus Gnome which people just endlessly argue over both ways. It is not as simple as \"New UI bad, old UI good\" no matter how common of a mindset (and how over-represented) that is here. Developers of the project have to balance these concerns as they see fit, and that is their right. Being an older user of the project (or any user, actually) does not mean every decision and plan in the project is going to revolve around you exclusively, at the end of the day. reply eviks 4 hours agorootparentThey shouldn't require wasting time to get the old behavior, though, that's indeed alienating, it should instead be saved to user config and preserved on updates reply eviks 4 hours agorootparentprevThey \"of course never arrive\" because of course you never implemented the proper match in behavior But the old users can have it too, that's what configurability is for, just save the state to user config and don't change any such behavior on upgrades, only for fresh installs reply ako 13 hours agorootparentprevNow you’re assuming that the group that would switch is bigger than the group that appreciates the tabs at the bottom. I doubt that the tabs at the bottom are the main reason people wouldn’t switch to KDE. reply shiroiushi 8 hours agorootparentNot only this, but who exactly uses Konsole? It's certainly not grandma, who you set up with a KDE laptop. She's not going to know what to do with the command line, and even if she did need to use it for some weird reason one day under your direction, she sure as hell isn't going to open multiple tabs in Konsole. The people using Konsole are already the highly technical people, and therefore probably not newbie users. reply jraph 18 hours agorootparentprevI don't care much either way, why would this matter so much? Also, if this is the only thing that's annoying, KDE won I guess. reply agildehaus 18 hours agorootparentprevTabs are on top by for me on 6.0. Seems to be the default, unless my distro (Arch) changed the default. reply FirmwareBurner 17 hours agorootparentI can't say which position is the \"right\" one, but I also noticed different distros have different defaults on where the tabs are positioned. It's cool that KDE lets you do that, but it's a bit annoying actually as it messes with the consistency of KDE. Sure, users can always change their preference to what suits them best, but it would be nice if out of the box all KDEs behaved and looked the same and leave the personalization to the user after installation. reply agildehaus 15 hours agorootparenthttps://github.com/KDE/konsole/blob/61264c1917770102a85123d3... It appears the default is Top in Konsole's source. reply JohnFen 11 hours agorootparentprevI only very rarely use konsole tabs at all (I prefer multiple windows), but when I do, I appreciate that they're at the bottom of the screen. That tends to be where my attention is when I'm using konsole. reply dietr1ch 11 hours agorootparentprevI'm guessing because tabs on top push the terminal down, moving all the text and maybe being distracting while reading since we are mostly reading from the top of the terminal buffer. reply troyvit 16 hours agorootparentprevI think it took you longer to ask that question than it does to move the tabs to where you want them. Personally I have them on the bottom and like it. reply thriftwy 18 hours agorootparentprevI have them on the bottom for 20 years. The first thing I'll change on fresh system. Glad I won't need to do that anymore. I wonder if \"new tab\" button is always visible now too. reply moffkalast 18 hours agorootparentTo each his own I suppose, as long as it's adjustable :) Is it a browser tabs vs. taskbar tabs ideology? reply bombcar 12 hours agorootparentFor me it's a Windows/Mac thing - if you're a Mac user, you're used to having a menu bar at the top, and you're always up around the top, so top tabs feel right. When I was on Windows, the Start Menu/taskbar was on the bottom, and bottom tabs felt right (as they became available). Let's all agree that side tabs are of the devil. reply ambichook 9 hours agorootparentmy discord addled gen z brain keeps tabs and taskbar to the left, there's always gonna be someone reply kuschku 17 hours agorootparentprevPeople like tabs right next to the area they look at most of the time. In browsers, that's always at the top. In terminals, depending on if you're a heavy user (and as result, the prompt is at the bottom) or a light user (and the prompt is at the top), you'll likely prefer tabs to be in the same area, too. I've actually got different settings for taskbar position and terminal tab position between my work ubuntu, personal ubuntu, and personal windows systems. reply wkat4242 21 hours agorootparentprevMost people I know that use KDE use it because it's so customisable. It's not the same crowd as Gnome. I don't think this is an impediment at all. reply DrewADesign 20 hours agorootparentMost useful software that badly needs usability improvements has a group of people that just got used to it, and they will complain bitterly about any attempt to correct UI mistakes. If it’s customizable, they can configure it back after making improvements so it’s useful to everybody else, too. I hate the way gnome is set up, and welcome any updates to KDE with open arms. reply refset 20 hours agorootparentprevAgreed - the main reason I switched to KDE from Gnome was so I could have a vertical taskbar. reply tsimionescu 19 hours agorootparentHeh, in the meantime modern Gnome doesn't have a Taskbar at all, because it \"is not ok to distract users with a list of other things they could be doing when they have already selected one task to look at\"... reply Zambyte 16 hours agorootparentThere are merits to the GNOME design philosophy. My Sway workflow and customizations are actually inspired quite a bit by GNOME. I don't use any taskbar or system tray. I don't even have a clock; I open a terminal and check the date command if I want to know the time. I make heavy use of workspaces rather than \"minimizing\" (Sway calls this the scratchpad or something like that, which I only use if I want to \"background\" graphical applications). I have absolutely no flashy styling or animations; I simply use nord where I can. It's not for everyone. It would be next to impossible for someone to sit at my computer and be productive, because I have accumulated my configuration over years, with no real thought into making things that I configured discoverable (I know it's there because I put it there). But it works really well for me. I find the ability setup a workspace how I want for one task, and then switching workspaces to context switch to be very nice. reply wkat4242 16 hours agorootparentI do something very similar in KDE with a whole bunch of virtual desktops. Though I do use a taskbar because I like the overview of it but I don't actually use it to switch tasks :) I always thought Gnome was not very useful for this because workspaces are created on the fly whereas I want to have them spatially oriented in a fixed grid that persists on every boot with the right application tiles in them. And I have hotkeys mapped to each one directly on the numpad (without key combos, I hate those). So my numpad is not a numpad at all but a workspace switcher :P The problem with the gnome design philosophy is that it only works for you if you agree with them on everything. If you're pretty opinionated yourself (as I am and it sounds like you are too), opinionated software only works well if you have the exact same opinions as its creators. With something as complex as a DE this will run into many mismatches quickly. This is why configurable software is so great if you're not willing to compromise on how you want things. reply the_why_of_y 11 hours agorootparentCurrent versions of Gnome allow setting a fixed number of workspaces in Settings, Multitasking. To configure hotkeys, you have to set some dconf entries manually, with dconf-editor or gsettings from a terminal: > gsettings set org.gnome.desktop.wm.keybindings switch-to-workspace-1 \"[\\\"F1\\\"]\" reply tsimionescu 15 hours agorootparentprevI absolutely agree that it's nice for this to exist as an option for users who are used to it. I'm a somewhat heavy emacs user, so I'm not at all opposed to esoteric workflows. But I think it's clearly proven to be a bad design as a default. Discoverability is very important, especially to people who work a lot with a mouse. And using multiple apps at the same time is a very common work flow, one that an always-on-screen task switcher makes much simpler than an alternate view you have to bring up, especially if that alternate view also obscures all of your windows. I will also say I find workspaces a hard to use UI, as I always lose context when I have to switch workspace, but maybe this is just how my mind works. And the idea of one-task-per-workspace has never worked well for me, as there are several apps that I use in every task, such as chat or email while I'm coding and while doing a presentation and while writing some design. It also seems to require a lot of setup and discipline. Finally, as a nitpick, moving apps to a different workspace instead of minimizing to taskbar/systray seems like much more work to me. Personally I'm a huge fan of Win7's grouped taskbar with window previews, along with its window snap support (extended in Win 10). reply shiroiushi 8 hours agorootparentI'd say workspaces aren't for everyone, and you're right: they require setup and discipline or else the context switching is too disruptive. I think they're good if you can organize them effectively with separate tasks on the different workspaces, such that you don't need to switch between them frequently: this way, you can minimize distractions from other tasks that you're not working on currently, and might not work on for hours or days even. For the chat and email stuff, as long as you have screen space (or if you don't mind them going to the background sometimes), you can set those windows to be on every workspace. KDE has a thumbtack icon that you can click in the window bar to make that window show on every desktop. reply omegabravo 17 hours agorootparentprevit suits my workflow since all windows are full screen on all the monitors. I either use alt+tab or super key to get the exploded view. it won't suit all workflows of course reply ako 13 hours agorootparentThat not only depends on preference, but also on hardware. When only using laptop display, most of my windows are full screen. However, when using a 32 inch 4K display, I prefer my windows smaller, often having multiple windows side by side, sometimes tiled. reply Vt71fcAqt7 19 hours agorootparentprevIs that a quote? I couldn't find the source if so. Hard to tell if it's a joke based on other GNOME statements. reply NekkoDroid 17 hours agorootparentI don't know either if it's a joke. The way I always thought of it is that the taskbar generally just can at best have a limited set of applications listed and takes up precious vertical monitor space, so its mostly limited to the overview/activities since that is the \"I want to change apps\" mode of the desktop and is just 1 click away (either super or top-left on the desktop). Then again I am one of the (probably) few people that would probably even do away with the top bar currently still in GNOME and not have anything other than the app visible by default. I use to be annoyed at the behavior as well when I started using GNOME, but at some point I actually started preferring it and now barely use the taskbar on Windows. reply tsimionescu 15 hours agorootparentprevI remembered it as a version of what some Gnome designer claimed, but I tried searching for some statement on the topic and I couldn't find any explicitly mentioning it. reply jraph 18 hours agorootparentprevI don't think I like KDE more because of its customizability but that's certainly welcome (as long as the default are good, which they also are) reply thriftwy 21 hours agorootparentprevI'm using KDE for 20 years already because it has great miscellaneous apps (though it's less important in 2024), slick integration between components and nice all-encompassing settings app. I do tweak a few things when I boot up a fresh install, but generally, I don't feel the need to do a deep customization, and am not aware of any missed opportunities. reply binkHN 19 hours agorootparent> I'm using KDE ... because it has great miscellaneous apps... KWrite and Dolphin are insane! reply izacus 19 hours agorootparentprevThat sounds like survivorship bias though? Everyone else already left it and jumped ship to GNOME (including pretty much all of the distros). Are the current users the full target market or just leftovers? reply wkat4242 14 hours agorootparentNot really, Most distros allow you install what you want. For example you don't need Kubuntu to install KDE. Just install kde-desktop. reply pcdoodle 19 hours agorootparentprevIsn't GNOME the one that doesn't have a \"desktop\"? reply abenga 2 hours agorootparentYes? What's this question supposed to imply? No one should use Gnome? reply Phlogi 21 hours agorootparentprevjust what they did with V6. reply graemep 20 hours agorootparentprevMost distros will change the defaults anyway., and, as others have said, a lot of KDE users use it for its customisability. reply haunter 21 hours agoprevPersonally I feel that KDE is what GNOME wanted to be but can’t. Not just the DE itself but the KDE applications too, just look at Krita for example compared to GIMP. Somehow KDE could accomplish much more and feels more mature and robust too. I loved GNOME2 back then but feels like something went wrong with GNOME3 regarding the whole project and how users reacted to the different UI. I’d say the classic Windows NT era UI (95, 98, 2000, Xp) was peak design so I’m glad KDE stick to that more or less and made it even better and modern. reply badsectoracula 18 hours agoparent> just look at Krita for example compared to GIMP FWIW technically the programs have different purposes, even if they also have a lot of overlapping functionality: Krita is primarily a digital painting application, which you can also use to do some general image editing while GIMP is primarily an image editing application which you can also use to do some digital painting. However if you compare the focus of each application to the equivalent of the other you'll see that Krita's image editing functionality - especially on things outside digital painting - is lacking while GIMP is stronger there and at the same time GIMP's digital painting functionality much more limited when compared to Krita's. reply lukan 18 hours agorootparent\"Krita's image editing functionality ..is lacking\" What is missing, compared to gimp? reply kuschku 17 hours agorootparentMoving selections with handles after the fact. Precise selection positioning in general. And where gimp has an always visible panel for filters, krita has always visible panels for brushes. It'd be awesome if krita gained more such functionality, but considering krita's recent expansion into vector images, these features are likely on the horizon anyway. reply Blackthorn 16 hours agorootparentFwiw I don't think selection positioning is that precise in gimp either. It's nothing compared to, like, a cad kernel. reply badsectoracula 6 hours agorootparentprevSome things off the top of my head... I've been doing some texture painting recently in Krita and i find it to lack things like various filters. It does have the GMIC plugin which has its own filters but those are way slower than \"native\" Krita plugins and not as integrated (which makes sense). GIMP has more ways to adjust the colors of an image - an entire menu of the stuff actually. Something i often need is adjusting the brightness and contrast of an image, but there isn't such a thing in Krita (well, there is in GMIC, but it has its own issues). GIMP can work with indexed images directly and has decent functionality for them - Krita can only export indexed images and even that has very little control. You can apply a palette as a filter and when exporting the image you can have it save it as an indexed image \"if it can be done\" (meaning it wont be forced) and that if the exporter supports it. This also means that it wont preserve the palette indices since it doesn't know about them. And the selection stuff mentioned by kuschku. In fact IMO selections and copy/paste work in weird ways in Krita. There are more things that i notice when using Krita (and is what i tend to use these days), but i can't think of them right now. At the end of the day there are ways around things so the only thing that remains is my impression that is weaker on stuff outside \"2D digital painting\" (where it is very strong). reply int_19h 14 hours agorootparentprevIt started during GNOME 2. Remember the whole \"spatial Nautilus\" debacle? But at least in GNOME 2 all such weird choices were configurable, although in some cases you had to go to GConf to do so. reply dheera 7 hours agorootparentprevNever heard of Krita, just downloaded it. It starts up way slower than Gimp and the UI looks non-native with fonts that aren't the same as the rest of my system. Bleh. It's probably busy spinning up some mcop's, dcop's, more cops, and other bloat. Qt/KDE ecosystem apps are slow and memory-hogging compared to their GTK counterparts. Oh well, back to GIMP. reply badsectoracula 7 hours agorootparent> It starts up way slower than Gimp On my computer (on which i'm not even using KDE as my DE, i use plain Xorg with Window Maker - and the system isn't even a high end one, it is a cheap PC i bought 5-6 years ago) Krita and GIMP start up pretty much the same time, which is around a second. > the UI looks non-native with fonts that aren't the same as the rest of my system Non-native compared to what? GIMP is only \"native\" on GNOME (though with it still being on Gtk2 it may feel off even on GNOME) and like any non-GNOME app, Krita would obviously not look \"native\" to it. But this is the case on pretty much most desktop applications these days on pretty much any desktop OS, the only OS where you may get some semblance of uniformity is macOS (but that comes with its own can of worms). > spinning up some mcop's, dcop's, more cops The last time KDE had such things were in KDE3, the last version of which was released 15 years ago. reply alwayslikethis 7 hours agorootparentWith a reasonable amount of effort you can get Qt apps to look good on GNOME. The same often can't be said about GNOME apps on KDE due to recent pushes of libadwaita, among other things, which greatly hinder the ability for a user to apply system themes. reply eviks 4 hours agorootparentprevYou can't learn things like that if you have a budget of one minute to test an app before removing it reply rtpg 20 hours agoparentprevThe simple 2-bit explanation is KDE is following Windows trends, Gnome is following Mac trends. Even the screenshot widgets are both following the closed-source versions (recent Gnome screenshot widget is exactly the new MacOS screenshot widget) I think it's a bit of a shame that Ubuntu is the \"no headaches\" distro, but ships with a DE that will annoy nerds much more than KDE does. My Linux experience got so much better under KDE. I respect what Gnome does a lot but I feel at home in KDE land. reply flohofwoe 20 hours agorootparentIMHO the difference is that KDE took the classic Windows desktop as starting point and has developed it into something that's now actually better than the Win10/11 desktop. GNOME OTH might be trying to imitate macOS but if that's actually the case they are doing a very poor job (I spend most of my time on a Mac, but have recently switched from GNOME to KDE on my Linux laptop because after updating to Ubuntu 24 I was finally fed up with GNOME's UX only ever getting worse, never improving). PS: switching from GNOME to a KDE desktop session was absolutely trivial and quick on Ubuntu btw. reply pxc 14 hours agorootparent> IMHO the difference is that KDE took the classic Windows desktop as starting point and has developed it into something that's now actually better than the Win10/11 desktop. In some areas KDE has also taken inspiration from macOS, and imo significantly improved over the original. The best example in my view is the Present Windows desktop effect, which is fundamentally a take on Exposé/Mission Control but massively outdoes those equivalents in usability by adding fuzzy filtering as you type to select windows. A less appealing version of that (Contexts) is something I have to pay money to a proprietary app developer for on macOS. reply keeglin 13 hours agorootparentprevI recently started using a Mac at work and Gnome aping MacOS is the only thing that makes sense. The applications selector, the settings drop-downs... spatial Nautilus... it didn't just start with Gnome 3. These are all poorly-implemented, half-baked versions of MacOS features. It has been going on for years. I mean, the thin scroll bars for $deity's sake! On MacOS this makes sense because the trackpad and trackpad/mouse work, and work very well. On Gnome, it makes no sense at all since you can't hit them with the mouse pointer. The pain is very real with Gnome. It's a very, very poor ape of MacOS. reply everybodyknows 15 hours agorootparentprev> Ubuntu 24 You're running a pre-release? https://ubuntu.com/download/desktop reply flohofwoe 2 hours agorootparentOoops, it's actually 23.10. Not sure why I got confused (for some reason the version number 24.04 was fixed in my mind). reply AlienRobot 18 hours agorootparentprevIgnoring all the other bad stuff with Windows 11, one thing that made me switch to Linux was the ugly \"modern\" design. iirc, someone on HN said that Windows designers don't even use Windows, they use Mac. But then I switched to Linux and a lot of apps, specially gnome and gnome-inspired apps, have such terrible design as well. I'm going to spare you the details because I could rant about it for hours. reply shrimp_emoji 18 hours agorootparentnext [12 more] [flagged] flohofwoe 17 hours agorootparentIMHO The pinacle of the Windows desktop was Windows 2000. Windows XP was ok except for the default bubble gum theme. We don't talk about Windows Vista and Windows 8. Windows 7 was sort-of ok. Windows 10 is was trying to salvage some of the Windows 8 mess with little success. Can't comment on Windows 11 because I'll stick with Windows 10 as long as possible ;) reply mjevans 15 hours agorootparentI agree with this more when you include how the then current versions of MS Office felt to use. Ribbons might have a place at the absolute entry level of usability, but they'll never replace a well designed menu system that includes keyboard shortcut documentation in the UI within a super information dense presentation. reply int_19h 14 hours agorootparentI hate the way menu bar is detached from the window/app that it applies to in macOS, but over time I've learned to grudgingly appreciate one thing about this design: it forces the apps to have a menu bar, because if you don't, it is such a visual sore point. So even when designers go nuts with UX layout, which seems to be so common these days, they still have to provide access to various things in the menu bar in a way that is mostly consistent across apps, and in any case is easier to find things in (esp. thanks to menu bar search as standard feature). reply grepfru_it 15 hours agorootparentprev>keyboard shortcut documentation Who are you targeting as the main user of this software? Most users do not depend on keyboard shortcuts but rather repeatable actions they can use the mouse for. The ribbon only annoys power users which is a number much much smaller than 50% of all users. Plus keyboard shortcuts still exist with the ribbon system. As someone who has spent a lot of time with “regular users”, no body is complaining about the ribbon… reply dpassens 14 hours agorootparentIt also annoys infrequent users, because you need to remember into which ribbon they stuffed what you want to do and what icon it uses. With a classical menu bar, the organization tends to be more intuitive (in my experience, at least) and you can skim the different menu items to find what you need. reply pessimizer 14 hours agorootparentprev> includes keyboard shortcut documentation You seem to have omitted a word. > Plus keyboard shortcuts still exist with the ribbon system. So this objection is completely manufactured? > The ribbon only annoys power users which is a number much much smaller than 50% of all users. It's 0% of the users who are new to the software, and 90% of the users who have used the software for some time. The UI shouldn't be optimized for people who are only going to use the software a few times unless the software is only meant to be used a few times. But in the case of office software, what you want is affordances that can be eliminated at the user's own pace while they get to know the software over years or decades. Things like indicating the keyboard shortcut next to the menu entry, which is standard for most UI toolkits. Or things like allowing the \"ribbon\" to be disabled, which I would be really surprised if you could come up with a reasonable opposition to. reply AlienRobot 13 hours agorootparentprevWere they complaining about the menu bars? reply khimaros 18 hours agorootparentprevi think you could have communicated this more effectively without the ad hominems. reply AlienRobot 18 hours agorootparentprevThis almost makes me want to try a Mac. Everyone is copying them, they must be pretty good, right? I just miss it when my apps had main menus, and dialog windows instead of transitions, and it didn't feel like every window was a browser even when they weren't electron apps... and I miss the window borders, and the colored icons, and when themes weren't just light or dark and... reply toyg 0 minutes agorootparentAs sibling comment says, you'll be disappointed - and worse, unlike on Windows or Linux there will be no way to change things you don't like. With Apple, it's their way or the highway. For example, they just don't do themes at all, and have slowly deprecated or removed even the basic customization features they used to do well (like changing the icon for a folder). flohofwoe 17 hours agorootparentprevYou'll be disappointed. Even Apple isn't adhering to its own Human Interface Guidelines anymore. It might be the least bad option of the current desktop environments, but that doesn't mean much. reply reddalo 17 hours agorootparentprevI wouldn't say GNOME 3+ is following Mac trends. GNOME 3 has been a horrible mess in my opinion, it's unusable for both Windows and macOS users. reply dgellow 2 hours agorootparentI personally love gnome 3 and also use windows and macOS. It’s perfectly usable. reply eitland 13 hours agorootparentprevDoesn't Gnome has the same application switching as Mac OS anymore for example? reply pjmlp 2 hours agorootparentprevIf GNOME would be following Mac trends, we would get more Vala and less JavaScript, and proper developer tools instead of writing XML based layouts by hand. reply eitland 13 hours agorootparentprevHeh, I think the last few years Windows has copied KDE, not the other way around. I say this as someone who has used the latest versions of KDE and Windows until around the release of Windows 11 (but I have seen that too). reply indymike 6 hours agorootparentprevI think you have this backwards. KDE has been ahead of windows since the Windows Vista era. KDE4 and Plasma (KDE5) are extremely good and have been borrowed from liberally by the commercial desktops for quite some time. reply kergonath 17 hours agorootparentprev> The simple 2-bit explanation is KDE is following Windows trends, Gnome is following Mac trends. I am a heavy Mac user at home (for about 20 years), and a heavy Linux (and to a lesser extent Windows) user at work, and I don’t see that at all. Gnome is infuriating even for a Mac user. I don’t like KDE either, so I use XFCE, but I am absolutely not at home in Gnome. I feel that this perception that Gnome is Mac-like is because the Gnome devs have strong opinions and don’t tend to compromise. But as a piece of software and desktop environment, Gnome is not more “Mac-inspired” than KDE. reply thesuitonym 18 hours agorootparentprev> The simple 2-bit explanation is KDE is following Windows trends, Gnome is following Mac trends. I find it more that Gnome is following Android/iOS trends. They're trying to be the mobile DE, but Linux (aside from Android) on the mobile phone was DOA. reply WhereIsTheTruth 19 hours agorootparentprev> Gnome is following Mac trends I disagree, macOS has both a system tray and a global menu, a totally foreign concept for Gnome Gnome wants to be a touch-screen/tablet OS, and it shows with their design choices Unity 7.0 from canonical was closer to macOS Apple has 4 distinct OS and UX for their different form factors (watch, phone, tablet, desktop) Gnome's future looks even more Phone/Tablet oriented: https://linuxiac.com/gnome-background-apps/ I quit the gnome ecosystem when Canonical announced killing Unity, that was my perfect Desktop Environment, it was perfect, it's sad.. reply jwells89 19 hours agorootparentYep, GNOME’s closest proprietary analogue is iPadOS, not macOS. GNOME omits all sorts of little power user features in comparison and takes the whole minimalism thing much further than macOS ever did (often too far IMHO). This applies to Pantheon too, even if it’s prettier. There unfortunately isn’t a Mac-like DE. reply Sunspark 17 hours agorootparentprevUnity is back. An enthusiast resurrected it and now it's an official Ubuntu flavour again: https://ubuntuunity.org/ reply zilti 20 hours agorootparentprev> I think it's a bit of a shame that Ubuntu is the \"no headaches\" distro Is it though? I mean, it is advertised by magazines and shills as such, but it really is not in practice, never has been. Back in the days, Mandriva was the \"no headaches\" distro, since then many distros have caught up - my go-to for many years that I also successfully got non-nerds to use has been OpenSUSE. reply toyg 19 hours agorootparent> Mandriva was the \"no headaches\" distro The original name was Mandrake, precisely because it would magically autoconfigure all your hardware and software - well before Ubuntu existed. The issue Mandrake/Mandriva always had, was that they would go a bit overboard with the approach, ending up with a system that could feel a bit sluggish - because it had all sorts of stuff preinstalled \"just in case\". It was also a bit of a separate kingdom - used RPM but wasn't really compatible with the wider array of RedHat packages. The Ubuntu innovation was that they hit a better middle ground: they were fundamentally Debian-compatible, and their autoconfiguration worked well (particularly with 3d cards, at the start) but also gave you a fairly fast desktop. These days it's all much of a muchness really. reply mark_undoio 19 hours agorootparentIn the early days of my Linux use I was on Mandrake 7.2 and loved it. All the \"just in case\" random packages were very entertaining and educational to me, although they were probably a distraction from whatever I was meant to be doing! Still, the experience seems to have served me well in the end. I do miss that feeling of discovering all the weird themes and window managers they packaged by default, I don't get the same vibes of \"any UI is possible\" these days (even though the UX is probably much better by conventional criteria). reply qup 18 hours agorootparentMandrake! I'm the other guy who used it! In 1999 I paid about $30 for a copy so I didn't have to spend weeks downloading it over 56k. reply toyg 16 hours agorootparentIt was actually pretty popular here in Europe (I have a feeling the core devs were French, but I could be wrong). reply kergonath 17 hours agorootparentprevSame! So I guess there are at least 3 of us :) Memories… reply jtorrents 15 hours agorootparentAs the sibling comment says, was relatively popular here in Europe. It was my first GNU/Linux disto. I had problems installing Debian in a laptop with a nasty Wifi PCMCIA card, Mandrake was able to make it work. reply qup 14 hours agorootparentSame, broadcom wifi issues and 3dfx driver issues for my voodoo card but mandrake mostly just worked. I eventually learned enough to install debian-netinst and get everything working, probably within about a year. reply jtorrents 14 hours agorootparentExactly the same here ;) I also ended using Debian when I learned enough. reply kergonath 10 hours agorootparentSuSE for me (well, actually, FreeBSD in between, and MacOS the whole time). But same difference. reply kergonath 17 hours agorootparentprevOpenSuse is fantastic. It’s very easy to set up and nice to use out of the box. It’s also fairly close to the bleeding edge and at the same time very stable. I am quite happy with it. reply SomeoneFromCA 17 hours agorootparentIMHO the best update strategy I've seen is the FreeBSD/NetBSD quarterly update, with \"base\" part of the system not updating. OpenSUSE is too frequent to my taste. reply jlpcsl 20 hours agorootparentprevThe one I usually install to normal users who do not know computers well is KDE Neon. But yeah with recent very positive experiences with openSUSE Tumbleweed, I am also thinking about using oST instead. reply indigodaddy 19 hours agorootparentSo if I install Tumbleweed I should get this latest KDE version very soon? reply MrDrMcCoy 13 hours agorootparentYes, if it's not out already. I'm not currently on Tumbleweed for reasons, but I do love that distro. reply zilti 7 hours agorootparentprevYes, there is even also openSUSE Krypton and Argon, basically KDE Neon but from openSUSE. reply jacooper 16 hours agorootparentprevSaying gnome is following MacOS just says you haven't used gnome since ages, give gnome 45 a spin and tell me how it's following macOS, it's better than macOS will ever be. reply rtpg 2 hours agorootparentI was on Gnome on my laptop until January 2024 (was running KDE on the desktop). I have gotten a Mac laptop for reasons and... I stand by my judgement. I think you could say Gnome is better than MacOS's DE, or that it's worse (to be clear, I don't think it's worse really), but my point was more that the design philosophies are close along so many axes reply pavlov 21 hours agoparentprevKDE’s underlying GUI framework is Qt which is backed by a successful corporation and is used by lots of high-end professional desktop apps. That goes a long way to explain why Krita feels more right than GIMP. reply orbital-decay 20 hours agorootparentSimplifying Krita vs GIMP as a difference between application frameworks is reductionist. Krita has much better connection with actual users and their needs, in the first place. Same with Kate and many other KDE apps which became fairly competent in their niches in recent years. KDE ecosystem in general has a working user feedback loop, something that is historically hard to come by in FOSS world. reply pavlov 20 hours agorootparentYes, that’s absolutely what makes the difference in the end. But if you’re going to build an app for professional content creators, it definitely helps to be using the framework that powers Autodesk Maya and many other tools that they’re already familiar with. A lot of non-obvious product needs on the framework level for this niche have already been solved. GNOME just never had that kind of solution pull. It’s always been more of a research project. reply okanat 18 hours agorootparentYes availability of technical solutions will dictate what the clients of the software can do here. You can have great connections with the users but if the core libraries you use doesn't help you to deliver the features you promised, they will leave for other solutions that actually deliver in shorter time while you struggle with GTK. This is exactly what is going on with GIMP. GTK basically either doesn't support or make it really hard to create certain workflows outside very simple applications with limited things yo click. Also it is a C library with very leaky abstractions including gtkmm. So developing complex applications suck and waste a lot of developer time Qt is C++ on steroids. It adds a bunch of features for GUI development, comes with a great library and many tools for testing, design and internationalization. It is overall nicer and IMO simpler to develop with. So you can go from a simple image viewer to a one with okay editing features and the difficulty doesn't skyrocket. Another aspect is Windows support. GTK 3+ doesn't support Windows. It looks like it does but due to GNOME locking down their overall system design, the integration suffers. The UI looks off due to GNOME's insistence in client side decorated windows. Projects like Krita have lots of Windows and Mac users and Qt is the only low level cross platform UI library that actually delivers. reply bogwog 16 hours agorootparentprev> But if you’re going to build an app for professional content creators, it definitely helps to be using the framework that powers Autodesk Maya and many other tools that they’re already familiar with. A lot of non-obvious product needs on the framework level for this niche have already been solved. There are tons of professional and highly successful apps for content creators that use custom made (and often shitty/mediocre) GUI frameworks. Whatever difference using Qt makes, it's negligible. Actual features are what sell the product. reply badsectoracula 19 hours agorootparentprev> But if you’re going to build an app for professional content creators, it definitely helps to be using the framework that powers Autodesk Maya and many other tools that they’re already familiar with. Qt isn't that sort of framework though, it is just a GUI toolkit[0] and there is nothing special about it that makes it better than Gtk for an application like Krita. The reason Krita is so successful is because of what orbital-decay wrote, they connect and listen to the users, not because of Qt. Obviously Krita is built on the KDE frameworks and the KDE frameworks are built on Qt, so Krita relies a ton on Qt to the point where if you consider on replacing it you might as well just rewrite the program from scratch. But Krita could have been written on, say, Java Swing, wxWidgets, Gtk or whatever other mature GUI framework and it'd still be as successful. After all keep in mind that many other popular digital content creation tools use custom toolkits instead of Qt (e.g. Blender which is way more popular than Krita). [0] ok, it has more functionality than GUI, but that's the main functionality and everything else can be found in many other libraries reply pavlov 18 hours agorootparentIn my experience it's not that simple. I certainly don't believe Krita written in Java Swing would be as successful. There's a lot of complexity in GUI frameworks, and they are not interchangeable because they end up making different design choices. An application like Maya with very complex user-manipulated data structures will expose weaknesses in the framework, and the fixes and design improvements end up in the framework. A competing framework whose primary users are lightweight consumer-oriented apps doesn't get those benefits. reply badsectoracula 6 hours agorootparent> In my experience it's not that simple. I certainly don't believe Krita written in Java Swing would be as successful. I disagree here, i'm certain it would be as successful because the GUI framework is not the reason for Krita's success, it is the functionality it provides and how the developers interact with the community. The GUI framework does not have any image manipulation specific functionality (all of that is implemented by the Krita developers) and the community interaction isn't even a technical thing in the first place. > There's a lot of complexity in GUI frameworks, and they are not interchangeable because they end up making different design choices. I did not claim that they are interchangeable (though they can be, depending on the program's design), i even explicitly wrote that taking Qt out of Krita would mean almost rewriting the entire program as it relies heavily on it. What i claimed was that Qt is not the reason for Krita's success and it could have the same success with other mature toolkits. There is nothing special about Qt aside from being around for long enough time to have its functionality \"battle tested\". This is not unique to Qt though. > An application like Maya with very complex user-manipulated data structures will expose weaknesses in the framework, and the fixes and design improvements end up in the framework. This is the case with any toolkit or really any library that has a lot of applications written against it, assuming the developers do not ignore all bug reports and issues the users of their libraries report. Also since you brought up Maya specifically, Maya used to be based on the Motif toolkit until Maya 2010 (it was changed to Qt in Maya 2011), which by the same logic would mean that up until 2011, using Motif would be great for professional content creation applications since Maya used it too. reply jrepinc 18 hours agorootparentprevYou forget about the desktop integration. At the company I work for we also selected Qt, why, because it has very good integration with many desktops. GTK is terrible in this regard (even support for other desktop on GNU/Linux apart from GNOME is not the best, let alone other OSes). And yes also Qt offers a lot more and is also more intuitive to work with and man the documentation it has, just superb. So yes, listening to user feedback is the most important but the role of a great toolkit to build on is also very important. reply badsectoracula 6 hours agorootparent> You forget about the desktop integration [...] the documentation it has, just superb. So yes, listening to user feedback is the most important but the role of a great toolkit to build on is also very important. I did not forget it, Qt has great integration and documentation but this was not a comparison of the specific features Qt and some other toolkit like Gtk may have. My claim was that Qt isn't something special that would make Krita successful while using anything else would make it less successful. I didn't bring those things up because they weren't really relevant for my claim. Also FWIW desktop integration for Krita isn't as important as it'd be for some other types of applications - consider that Krita even comes out of the box with its own themes that it uses instead of trying to \"blend in\" the underlying desktop looks. In terms of what Krita does, there isn't any functionality that it uses from Qt that couldn't be found in other toolkits like Gtk - or other libraries. It wouldn't be the same way and certainly not with the same code, but Krita could have been written using a different GUI library and framework and even in a different language and still had the same success because the GUI framework it uses is not why it is successful: it is the functionality the program provides (which was written by the Krita developers themselves) and the communication the developers have with the users (which isn't even something technical). reply SomeoneFromCA 17 hours agorootparentprevQt is very special because it has excellent, \"vector\" fractional scaling (in a way, similar to Windows), compared to Gtk which has awful \"bitmap\" fractional scaling (akin to MacOS). reply badsectoracula 6 hours agorootparent> Qt is very special because it has excellent, \"vector\" fractional scaling (in a way, similar to Windows), compared to Gtk which has awful \"bitmap\" fractional scaling (akin to MacOS). This isn't unique to Qt though, other widget toolkits can provide that functionality. In fact LCL/Lazarus provides such fractional scaling even for Gtk itself by doing the scaling \"manually\" when using the Gtk backend. reply frameset 18 hours agorootparentprevI came back to KDE after more than 15 years away and the improvement in Kate is astounding. It has features I would never have expected from the basic text editor. reply raffraffraff 19 hours agorootparentprevIt was such a pity about Amarok :( That whole \"2.0\" debacle put me off the entire KDE ecosystem for years. It's great to see them back on track. But there are still no decent music libraries / players on Linux. reply jrepinc 18 hours agorootparentStrawberry is plenty decent for me https://www.strawberrymusicplayer.org/ reply worble 17 hours agorootparent+1 for strawberry, coming from Windows and foobar2000, this is the only music player on Linux really up to the task of playing huge music libraries and doing it well. reply brnt 13 hours agorootparentCheckout Quod Libet. Better than foobar, which I used through wine for ages. It's just about the only GTK app on my KDE boxen and I gladly make the exception. reply tmtvl 12 hours agorootparentprevI quite like Cantata. It does everything it needs to do and it's stable as a mountain. reply justinclift 20 hours agorootparentprev> backed by a successful corporation Are they profitable these days? That used to be their main problem, business wise. Always losing money, so making weird choices trying to stop that. reply pavlov 20 hours agorootparentYes, Qt Group is profitable. It’s publicly listed and has a market cap of around $2 billion. So not very big compared to a lot of enterprise software vendors, but could be an interesting acquisition target at this price. For a couple of years Qt was owned by Nokia, then spun off after their Microsoft OS pivot. Today I’m guessing an acquirer might be in the embedded/automotive space instead where Qt is apparently doing quite well. reply toyg 19 hours agoparentprev> KDE is what GNOME wanted to be Lol, from a historical perspective this is quite literally true: GNOME was born to be a GPL clone of KDE, back when QT had a gnarly license. reply moffkalast 18 hours agorootparentGNOME doesn't seem ideologically similar to KDE at all though, it's very hardcoded with hardly anything is adjustable. KDE is like the opposite of that, it can mimic most Windows features as well, e.g. quicklaunch, non-grouped taskbar windows with titles. reply toyg 16 hours agorootparentThis philosophy emerged later, when GNOME tried to differentiate. In the first few versions it was as flexible as KDE, it had fewer trinkets only because they came later and had to catch-up. It was only with version 3 that they went \"full Apple\", when they adopted a somewhat-dictatorial style of development. reply MrDrMcCoy 13 hours agorootparentI wonder how much of that dictatorial nature comes from more and more of the developers getting hired by Red Hat, who basically decides everything related to systemd/gnome/freedesktop these days... reply rodgerd 11 hours agorootparentprevhttps://en.wikipedia.org/wiki/GNOME_1#/media/File:GNOME_1.0_... is what GNOME 1 looked like. reply EverForever 21 hours agoparentprevGnome has a completely different workflow than KDE. Gnome is the reason why I use Linux. If I had to use KDE I would stay with Windows, the workflow has the same logic, is almost the same, except that with Windows I have no restrictions with applications. reply graemep 20 hours agorootparentCan you explain that? How is the workflow like Windows? All I can see is some superficially Windows like defaults (good for newbies) in the initial look. KDE has a lot of stuff very different from Windows - or at least Windows at the time I switched. Transparent sftp in all applications, highly customisable (I currently use window tiling, have a small icon only task switcher I hard use, window titles in the panel, I use multiple desktops, KRunner to launch/switch apps.....), very different file managers from windows, a excellent text editor that integrates nicely with everything else. reply xcdzvyn 21 hours agorootparentprevI'm almost the opposite. If I had to use Linux with GNOME, I'd just use macOS instead. The Linux desktop needs a shtick. Maybe when desktop cubes make a comeback we can make peace :) reply rpgbr 20 hours agorootparentFunnily enough, Plasma 6 brings back the cube effect[1]. [1] https://pointieststick.com/2023/10/27/these-past-2-weeks-in-... reply veidr 20 hours agorootparentprevThe desktop cube is back[1] in KDE Plasma 6! :-D Oh, did you mean the other kind of desktop cube... [1] https://kde.org/announcements/megarelease/6/cube.webm reply realusername 20 hours agorootparentprevGNOME might look a bit macOS-like from far away but it's really not when using it. I personally hate macOS but do love GNOME. reply l72 18 hours agorootparentI agree, especially when it comes to window management and virtual desktops. I have been running Linux desktop since the late 90s and used A LOT of different desktops and window managers. I remember when gnome 2 came out and everyone hated it! (sound familiar?) For work, I have my desktop running gnome and I have a macbook that I also use when traveling or at the office. I find my productivity on mac os drops with its absolutely terrible window management and terrible virtual desktop implementation. I instead run fedora in a UTM VM fullscreen and only use mac as a \"host\" for the VM. Gnome (with version 3) required a change in how you use it as a desktop. In gnome 2 days, I used to have a grid of virtual desktops and maybe always assigned email to 1, chat to 2, etc. The task bar was heavily used and important. But with Gnome > 3, I really love the dynamic virtual desktops. Every task I am working gets is own virtual desktop. As I finish a task and close windows with that task, that virtual desktop goes away. If I have a long running multi-day task, that virtual desktop with windows associated with it stay open for that whole duration. Only things related to that task are on the virtual desktop. I might have 25 browser tabs open in total, but 3 of them are tied to a specific task on the firefox window on desktop 2, 5 are tied to another firefox window on desktop 5 and so on. Everything is _very_ keyboard driven, and I don't ever touch a mouse to interact with gnome itself. This makes task switching really nice. There is no need for a tab bar with 50 items on it, or a browser window with 50+ tabs open. One thing I do miss from some of the older window managers, is the ability for the window manager to do grouping/tabbing. I'd prefer if now application implemented tabs, and instead the window manager did it. reply int_19h 14 hours agorootparentIt's great that it works for your workflow. The problem is that GNOME is very opinionated in that the workflows they enable are the right workflows for everyone, and resist any configurability that would actually make it usable for the rest of us. Of course, one can always use a different DE, but there's always friction in not going with what the distro you're using picked as their default (and tends to support better in practice). I think a lot of GNOME hate is coming from the users who feel that a DE that does not adequately reflect their workflow is being pushed on them so aggressively by their distros. reply hannofcart 20 hours agorootparentprevI moved from Gnome to KDE recently. There is likely no desktop environment that's more customisable while at the same time being full batteries included as KDE is. And I've probably tried them sll: Gnome, XFCE, Enlightenment, Cinnamon, Mate, i3wm... If there's a flow you've grown accustomed to, you can most probably replicate that in KDE. reply anneessens 21 hours agorootparentprevInteresting. For me, Linux would be unusuable if I had to use GNOME. What do you like specifically about GNOME compared to Plasma or Windows? reply brink 20 hours agorootparentI use Gnome (and Sway, depending on which computer I'm on). I use Gnome because it works great with wayland, and I just need to get work done, and Gnome does a pretty alright job of staying out of the way. KDE's integration with Wayland feels too clunky for me at this point. Plus I get rendering artifacts on the edge of the screen when I use plasma with screen scaling. reply anneessens 20 hours agorootparentI believe improving Wayland support was one of the major goals of Plasma 6. So if it was just the Wayland integration putting you off, then maybe consider trying Plasma again soon. reply cogman10 19 hours agorootparentPlasma 5s Wayland support has been pretty good since I started using it. I started using it back in December. Gnome just does way too many things I don't want it to do and that can't be disabled. reply anneessens 17 hours agorootparentI experience some random visual bugs occasionally with Wayland, but yes generally it's decent. But I could understand if someone would want a more stable experience. Yes, I don't like that about GNOME either. reply l72 18 hours agorootparentprevIsn't it great, that unlike Windows or Mac, we have a choice! We don't have to try to create something for the lowest common denominator of user, and we can find something that works really well for us, individually. reply anneessens 18 hours agorootparentI absolutely agree with that. I was just curious to know what he doesn't like. reply Octabrain 19 hours agorootparentprevI like its simplicity and the straight forward workflow it provides. Years ago, I used to use KDE and enjoyed it but these days, I want something that is functional while being vanilla and standard as possible and personally, that's what GNOME gives me. reply tsimionescu 19 hours agorootparentIt's so straightforward you can't even switch to another window without pressing a separate button first! The Gnome designers have apparently discovered that taskbars are attention vampires and a detriment to users. reply Octabrain 18 hours agorootparentI don't get your point. I just use two ways: 1. With mouse -> Up left corner (a.k.a hot corner) -> Click on the window I want. 2. With keyboard -> Alt + Tab -> Select the window I want. I find that quite straight forward. Again, it's a personal thing. reply anneessens 18 hours agorootparentprevFair enough. I guess I have a hard time understanding why you wouldn't be interested to make the workflow fit better for yourself on a device you spend hours per day using. reply Octabrain 18 hours agorootparentIt's just a personal thing. I try to stick to using tools that provide me the best defaults + being open source. I don't want to spend time customizing my desktop or getting overwhelmed by the amount of different choices I have available. Don't get me wrong, KDE is a beautiful and great project, it's just that, a very personal thing. reply binkHN 18 hours agorootparentprevI can't agree with this more and that's the beauty of KDE. If I'm sitting down using this thing 8 hours a day, 5 days a week, little niceties and optimizations go a long way to making me happy and productive. And it doesn't take very long to make these little tweaks. reply anneessens 17 hours agorootparentYes, this exactly. It's a small time investment that improves my experience significantly. reply jklinger410 19 hours agorootparentprev> Linux would be unusuable if I had to use GNOME. This type of hyperbole is what feeds the DE wars. GNOME is very usable, and if it's not, you don't know how to use a computer at all. reply anneessens 19 hours agorootparentWell it's not a hyperbole, my productivity would suffer immensely if I had to use GNOME. And since GNOME doesn't offer much customisation, I couldn't make it work better for me, which is why I use Plasma. That doesn't mean I hate GNOME or something and I'm glad it exists for the people who do like its approach. reply criddell 18 hours agorootparentIn what ways does Gnome hamper your productivity? Are you really using the DE a lot? Most of my day is spent in applications. I launch an application and that's where I'm spending my time. I'm not using the desktop environment all that much. I really don't find much difference working in Windows, macOS, KDE or Gnome or even iPadOS as far as interacting with the graphical environment goes. reply anneessens 17 hours agorootparentYes, absolutely. Perhaps not directly with the DE itself, but the DE affects how I work. On Plasma, I have it set up so I have all title bars hidden and I use custom keybinds to close, minimize and maximize windows, which saves screen space and reduces clutter. On GNOME you cannot minimize windows at all if I remember correctly. I have virtual desktops disabled and only use one desktop to manage all of my windows, while GNOME fundamentally works around using multiple virtual desktops as far as I know. GNOME doesn't have a system tray, which I find essential. For example, I can see just by looking if Discord has an unread notification. Or I can close OBS to the system tray without quiting the application, which reduces visual clutter. I know you can add this with an extension, but I'm just referring to vanilla GNOME. I often use KRunner to temporarily write something while still seeing the contents of my screen, while GNOME's equivalent is full screen I believe. I'm sure there are many other ways, but these are the ones I can quickly think of. reply ColonelPhantom 16 hours agorootparent> On GNOME you cannot minimize windows at all if I remember correctly. This is incorrect. You can minimize windows on Gnome, but the button to do it is hidden by default. It can be re-enabled in Gnome Tweaks, and there is also a keyboard shortcut (Super+H) for minimizing. Gnome is however indeed fairly workspace-centric. As for customization, out of the box Gnome is quite rigid, but its extension ecosystem far surpasses that of KDE. You can use extensions on Gnome to for example get a dock or system tray back. reply int_19h 13 hours agorootparent> You can use extensions on Gnome to for example get a dock or system tray back. As I recall, those are exactly the kinds of extensions that get broken by Gnome updates on a regular basis. reply anneessens 15 hours agorootparentprevOh, I didn't know that shortcut for minimizing. Is there a reason the button is hidden by default? I never really understood how to efficiently use virtual desktops or what their benefits are compared to one desktop. Would you mind to explain? Well, I would imagine that is because you generally only need extensions on KDE for niche things, while GNOME needs extensions for more 'basic' things. Obviously you don't need an extension for a system tray if one already exists by default. reply criddell 16 hours agorootparentprevI think I see one difference - I'm not trying to use each environment the same. My iPad wants everything to be full screen, so that's how I use it (although I have been playing with Stage Manager). Windows has good support for tiling now, so I use that. On Gnome I lean into the workspace stuff. KDE I don't know as well, so I use the mouse for just about everything. I enjoy learning the ins and outs of the different environments and frankly I wish the differences ran even deeper. I often think about how fun it would be if Commodore Amiga, Atari ST, BeOS, SGI IRIX, OS/2, Sun CDE, and all the other systems were still being developed. But then the Electron / web app people would probably still try to pave over everything cool and unique on each system to run one mediocre app everywhere. reply anneessens 15 hours agorootparentI understand that GNOME has a clear way how it wants you to use the desktop, but I don't like that way for the reasons I described. And it's not just a 'different' way, I feel like I lose functionality and flexibility in a lot of regards. Although, I guess it's hard to say for sure since I never used GNOME for an extended period of time. reply criddell 14 hours agorootparentThat's the beauty of different systems. You always lose functionality no matter which way you switch. A Windows user might miss PowerShell + COM on Linux. A Linux user would miss having access to the filesystem on iOS. An iOS user misses the ubiquitous URL scheme for sharing code and data when they switch to Windows or Linux. I still miss Rexx and the object-oriented workplace shell of OS/2. I'm sure if you gave GNOME an extended trial, you would adapt and find some things you actually prefer. reply jklinger410 14 hours agorootparentprevIt is hyperbole, because you could use it. You would have to be incompetent to not be able to use it. Having lower productivity does not mean something is \"unusable.\" It is, in fact, still usable. You just don't like it. Maybe learn what unusable and hyperbole mean. reply anneessens 14 hours agorootparentIt's unusable enough for me that I would rather switch back to Windows than keep using GNOME. And I really don't like Windows. What does this discussion gain from you being pedantic? Everyone with common sense knew what I meant. reply jklinger410 14 hours agorootparentBecause > This type of hyperbole is what feeds the DE wars. You not liking something is not the same as it not being \"usable.\" You simply don't like it as much. Your comment would be a lot less interesting if it were written without hyperbole. It would simply be \"I don't like GNOME as much as KDE.\" And no one would really care about that, it wouldn't be a notable comment. reply anneessens 13 hours agorootparentYou're the only one who takes this 'war' seriously. The rest of us here are adults who can appreciate all desktop environments, even if we don't personally like to use them. Go annoy someone else. reply jklinger410 13 hours agorootparentMy entire point is that both desktops can be appreciated for what they are. I can use KDE or GNOME, I just prefer GNOME. I would never call KDE unusable, because it works just fine for those who like it. People who go around saying they \"can't use GNOME\" because it's \"not customizable\" without ever even trying would be the ones that are not appreciating all desktops, like an adult. reply anneessens 13 hours agorootparentNo one here said that GNOME shouldn't be appreciated. Just because I said GNOME is unusable for me personally doesn't mean I can't appreciate it. I have tried GNOME before, thanks for your assumption, so I know for a fact it's less customisable than Plasma. But less customisation doesn't equal less value anyways, so I don't even know what your point is. reply leeoniya 20 hours agorootparentprev> except that with Windows I have no restrictions with applications. what you do get with windows is a UI that changes, resets, and ignores your previous customizations with every os update, which you cannot stop/prevent. even group policy hacks and regedits wont always save you. LTSC is apparently a thing but you cannot pay anyone money to actually get that license as an individual user. dark patterns to prevent users from creating offline, local-only accounts. you have to yank the ethernet cable now during initial setup to get the option not to log in to your ms cloud account? (or some insane nonsense like that) plus more cloud services that i didnt ask for with each update, more things bloating ram and disk/cpu on startup, more telemetry. and ads. always. more. ads. ads in the browser, ads in the start menu, ads in the widgets. windows decided one day to auto-update and fuck up my linux dual boot setup. after more than two decades of windows following DOS, i couldnt do it any more with this omnipresent Windows SaaS shit. tried Mint and Manjaro for a while, then switched to EndeavourOS + KDE/Plasma and never looked back. everything is just faster on linux and nothing changes out from under me in the past 3 years of daily rolling updates. reply juujian 20 hours agorootparentprevThat maybe be true if Windows (and maybe KDE) 10-15 years ago, I don't think that's true anymore today. KDE has really grown into itself. reply KronisLV 20 hours agorootparentprevI honestly love the variety of options, everyone can find something suitable for themselves! Personally, XFCE is a good fit for me often (especially on older devices), or maybe something like Cinnamon since it mostly gets out of the way and lets me work. Then again, I also enjoyed Unity when it was the default in Ubuntu, unlike a lot of folks hah. reply amykhar 21 hours agoparentprevKDE has spoiled me. I installed a Gnome distribution a short while back, but used it for a couple of hours and missed KDE so much that I wiped the hard drive and went back to Manjaro and KDE. reply tcbawo 20 hours agorootparent> wiped the hard drive and went back to Manjaro I think this is the reason Linux hasn't penetrated the desktop more than it has. “Just reinstall” is too often the solution to issues. Starting over will often throw away hours of someone’s time. This can be catastrophic for a non-technical user. I wish the Linux desktop was implemented more like a user extension on top of a rock solid base server layer (eg hypervisor). Maybe such a setup exists, but I’m unaware of it. reply georgyo 20 hours agorootparentIt's not \"the solution\", it is a solution. It is the easiest solution, requires no research or technical ability, and will not have any left over cruft from the hours of customizing. The same goes for windows, I know people who reinstall every 6 months just to keep their system clean and working optimally. > I wish the Linux desktop was implemented more like a user extension on top of a rock solid base server layer. I would argue that the Linux kernel is that server layer, but let's not open that can of worms. Maybe Fedora Silverblue is up your ally. All the apps, including the desktop environment are containers. Or if you really want an actual hypervisor you could try Qubes, but that is not for the faint of heart. reply tcbawo 16 hours agorootparentI was not familiar with Silverblue. It looks very promising. The idea of creating a fundamental, shared base system should make troubleshooting significantly easier — possibly an exponential reduction in the possible installed permutations. Thanks for the suggestion! reply qwertox 20 hours agorootparentprev> I would argue that the Linux kernel is that server layer, but let's not open that can of worms. Yes, the package manager is definitely not a part of the desktop. reply flohofwoe 20 hours agorootparentprevSwitching desktop environments on Linux is absolutely trivial and doesn't require a reinstall though (at least in my experience of switching from GNOME to KDE on Ubuntu, which took a couple of minutes to pull down the KDE packages and then logging out and picking Plasma from a dropdown in the login screen - and if I feel like it I can switch back to GNOME anytime). reply jraph 18 hours agorootparentIt's not trivial. Just installing KDE packages on a GNOME install will work and is quite easy, but will lead to some mix / subtle setting issues, it's less clean than just a brand new install. Installing and running KDE will mess up GTK settings in GNOME for instance. You might end up with the Breeze GTK theme in the GNOME session. Which works, but this is most likely not wanted (even though GNOME looks great with the Breeze theme). I'd not advise regular users to do this without a warning. reply flohofwoe 17 hours agorootparentI haven't seen this on my Linux laptop, but TBH some UI elements in GNOME look so weird in Ubuntu 24 that I'm not sure if it's broken or intended (but already did before installing KDE). reply askonomm 18 hours agorootparentprevTrivial to who? A seasoned Linux nerd? Maybe. A regular, non-tech person? Nope. And that is why there is no year of the linux desktop. And if you expect a regular, non-tech person to be able to master the terminal and type in commands you're delusional. reply flohofwoe 17 hours agorootparentTrivial in the sense of googling \"how to install KDE on Ubuntu\", picking a result that looks somewhat recent, and following those steps. It ends up being a handful terminal commands which shouldn't be too hard for anybody who has used a keyboard before. That's how I did it at least. There might be more UI centric options. Also, trying to chase the elusive 'casual user' is what caused all the GNOME UX mess in the first place I guess. I'm not an 'archetypical' Linux nerd, I hate wasting time with fixing stuff that should \"just work\", but I'm also expecting a computer to be a professional tool which I can customize to my needs (within reason at least). reply crq-yml 14 hours agorootparentprevWindows is not better at this. Plenty of troubleshooting advice says \"Now open the registry editor and...\" or \"Now open this .ini file and...\" or \"Now open cmd in admin mode and...\" The ease of the GUI ends when a serious system-level issue arises. This has never not been the case, it's just a matter of how much the documentation expects you to know what's going on, and how much that impacts the first-run experience. If the first-run is good enough, \"reinstall\" becomes the go-to fix. reply wasmitnetzen 18 hours agorootparentprevI wouldn't expect a non-tech person to even understand the difference between an operating system and a desktop environment and why you can switch the latter while keeping the former. Nor would I expect them to care. reply markles 18 hours agorootparentprevYou can install it through the Software Manager. At least on Mint that's how it is. Click, install, and I believe it tells you to logout and back in. reply thesuitonym 18 hours agorootparentprevThat's just not true at all. The reason Linux hasn't penetrated the desktop is because it's not installed by default. Even if that isn't the reason, the GPs preference for reinstalling is certainly not. Switching DEs doesn't require reinstalling the OS, it requires searching your distros app store for KDE, and then logging out and selecting \"KDE\" when you log in again. You could even switch between them each time you log in, depending on your mood that day. reply SomeoneFromCA 17 hours agorootparentNo, Linux has poor isolation between the base system and application and third-pardty software and poor backwards compatibility (FreeBSD is slightly better in that respect). The only OSS Posix system that getting it right seems to be Haiku. reply tapoxi 9 hours agorootparentFedora Atomic Desktops, Nix reply betaby 19 hours agorootparentprev“Just reinstall” is a solution in Windows world even more often. reply CoolCold 20 hours agorootparentprevSome say Windows + WSL2 is the most stable ABI/API for the year of Linux on desktop. While its a joke, every joke contains some portion of a joke. reply cpburns2009 20 hours agorootparentI thought the joke was the reverse? The most stable ABI for Linux is Win32 (via Wine of course). reply cfiggers 20 hours agorootparentprevThat's what I use. I love it! reply kelnos 14 hours agorootparentprevIt's funny that you say that, since that was the solution to Windows issues for... decades? Not sure if that's still the case, as I haven't touched it in forever. Regardless, not sure where you've gotten that impression of Linux. The only times I've reinstalled is when I've gotten a new laptop, and in those cases I just copy my home directory over to the new laptop and everything just works. The GP's example of needing to reinstall because they wanted to change desktop environments is nonsensical; I don't think anyone even remotely knowledgeable would recommend a reinstall in that case. Just a trip to the package manager app and a restart. I think there are quite a few reasons why the Linux desktop isn't more common, but \"need to rein",
    "originSummary": [
      "KDE Plasma 6 is the latest major release of the desktop environment, emphasizing the transition to Wayland, enhancing performance, security, and compatibility with new hardware.",
      "Changes in KDE Plasma 6 include adopting double-click to open files by default, updating the Breeze theme, and enhancing applications like Dolphin and Spectacle.",
      "KDE Frameworks 6 have been launched, laying a robust groundwork for future development, as users eagerly anticipate the evolution of Plasma in the upcoming years."
    ],
    "commentSummary": [
      "The release of KDE Plasma 6 is generating discussions as some users switch from Gnome to KDE for improved Wayland support and functionality differences.",
      "Despite reports of UI glitches and bugs in KDE, users are hopeful for enhancements in Plasma 6, discussing topics like mapping Wacom tablets, bug reporting, and comparing KDE with Gnome desktop environments.",
      "User preferences for KDE's customizability versus GNOME's simplicity highlight the focus on different desktop environments, workflows, and design philosophies in the Linux community, underlining the importance of individual choice and customization options."
    ],
    "points": 765,
    "commentCount": 555,
    "retryCount": 0,
    "time": 1709206404
  },
  {
    "id": 39554539,
    "title": "Billing Team Resolves Leap Year Bug Overcharging Subscribers",
    "originLink": "https://news.ycombinator.com/item?id=39554539",
    "originBody": "After a frantic scramble this morning, our billing team has finished patching a bug which erroneously was charging our monthly subscribers for an extra day.All test suites are passing now, and SRE has scheduled a postmortem after the QA confirms the fix in 2028.",
    "commentLink": "https://news.ycombinator.com/item?id=39554539",
    "commentBody": "Did you encounter any leap year bugs today?422 points by sjr1 13 hours agohidepastfavorite401 comments After a frantic scramble this morning, our billing team has finished patching a bug which erroneously was charging our monthly subscribers for an extra day. All test suites are passing now, and SRE has scheduled a postmortem after the QA confirms the fix in 2028. kccqzy 13 hours agoHeard from a friend in China: the age calculation portion of the app to schedule a marriage certificate had a bug where they subtracted 22 (legal minimum age) from the year, which resulted in 2002-02-29 which doesn't exist. The app intends to compare this against the user's birth date. The error handling code assumes all errors are from the comparison. The app then rejected all marriage certificate appointments by complaining that the users are too young to marry legally. reply chaorace 12 hours agoparentHaha, that would be quite the appropriate place to put one of those \"Please wait and try again\" error messages. reply tedajax 10 hours agorootparent\"look, today the math just doesn't work out, try tomorrow\" reply Saigonautica 6 hours agorootparentThis is often the result of consulting an astrologer in Asia for a marriage date, to be fair. reply xattt 10 hours agoparentprevHow do leap day birthdays get handled in general? How is the right age iterated every year? reply usr1106 2 hours agorootparentRelevant question for driving, voting, marrying, drinking. I'd assume just the date is compared. If you are born on 29th, on future 28th you are considered \"too young\", regardless whether a 29th exists or not. On future March 1st you are \"old enough\" again regardless. If a 29th exists you are old enough already on that date. Drinking beer in Germany at 16, I guess in some countries at 20 could be relevant cases. For the more common minimum age of 18 for many things, the limit is reached always on March, 1st because a 29th cannot exist. reply paulddraper 7 hours agorootparentprevBoth Feb 28 and Mar 1 are commonly used to celebrate birthdays. AFAIK there is no firm convention. Feb is more natural (\"My birthday is in February\"), Mar is more logical (the 60th days of the year). reply js2 7 hours agorootparentIs March more logical? Feb 29 + 1 year = Feb 28. reply paulddraper 3 hours agorootparentYou're begging the question. [1] \"Feb 29 + 1 year\" is verbatim restatement. --- You can say Feb 29 + 365 days = Feb 28. (And Feb 29 + 730 days = Feb 27.) --- EDIT: Note that in the context of birthdays, people use \"calendar years,\" not \"unit of time which is ~1 revolution around the sun.\" Birthdays aren't celebrated every X million seconds after the moment of birth. They are celebrated the same day each calendar year -- notwithstanding the fuzzy concept of \"same day\" for incongruent calendar years. There's no singuar right answer, but that is the core question: \"what is the same day next (calendar) year?\" [1] https://en.wikipedia.org/wiki/Begging_the_question reply quietbritishjim 1 hour agorootparent> Feb 29 + 365 days = Feb 28. Correct. > And Feb 29 + 730 days = Feb 27 Incorrect. It's Feb 28 again. In a normal, non-leap year, if you add 365 days then you get back to the same date. reply Scarblac 2 hours agorootparentprevFeb 28 is definitely too early, you haven't lived a whole year yet since your birthday. So March 1 it is. It's not your birthday but you can celebrate having made it another year. reply edflsafoiewq 4 hours agorootparentprevBy this logic, all birthdays would slowly drift as leap years pass. reply drdeca 3 hours agorootparentTake DOB + n * 365.25 and then pick whatever day that falls in? That way it shouldn’t drift overall. Though, I guess it would imply that what day of the year people celebrate on, would be off by one day on leap years compared to what it is on other years? reply iforgotpassword 3 hours agorootparentThis formula doesn't consider how we sometimes skip leap years, and sometimes have leap seconds, so it's vastly imprecise. reply munch117 2 hours agorootparentSo make it more precise. DOB + n * 365.2421. reply histories 1 hour agorootparent365.2425 reply shzhdbi09gv8ioi 2 minutes agorootparent365.2421 is more correct. Nasa explains: 365 +0.25 - 0.01 + 0.0025 - 0.00025 = 365.24225 https://pumas.nasa.gov/examples/how-many-days-are-year silisili 5 hours agorootparentprevThink of it this way: if Feb 29 didn't exist that year, it would have been March 1. If we're being scientific I guess all of us should move our birthday up by 1 day on leap years, but that seems annoying and not sure anyone really cares enough to. reply pests 4 hours agorootparentprevSimilar to the recent standup maths video where the town did it's art installation incorrectly - the fence post problem / off by one. https://youtu.be/FAdmpAZTH_M?si=r_INH_C5j9mbpJSh reply jey 6 hours agorootparentprevWhat’s the significance of 365? Each Earth year has just a smidge under 365.25 days. reply js2 6 hours agorootparent365 days is what people commonly think of as a year. So if you're born Feb 29th and celebrate your birthday one year later (whether you call that 365 or 365.25), you land on Feb 28th. Then again, folks born between Jan 1 and Feb 28th of a leap year celebrate their birthdays 366 days later by calendar days. Anyway, I'm not sure there's any more or less logical date to use and Feb 29th babies seem to choose both about equally: > “I love when people ask me, ‘Do you celebrate on Feb. 28 or March 1?’” said Raenell Dawn, a co-founder of Honor Society of Leap Year Day Babies. “I get to tell them, ‘Both, because I can.’ But I’m a February baby; I was not born in March.” An informal poll of the society’s members showed about a 50-50 split between the two dates, said Ms. Dawn, who is celebrating her “Sweeter 16” by turning 64 this year. https://www.nytimes.com/2024/02/28/style/leap-year-explained... reply dexterdog 5 hours agorootparentCelebrating on the 28th makes no sense. That is when you celebrate your Birthday Eve every year so your birthday is the next day, whatever that is. reply smrq 4 hours agorootparentYou can make the same argument that March 1st makes no sense, as that is the day of your birthday hangover, the day after whenever your birthday is. reply carleverett 4 hours agorootparentprevBut 1 day is still 24 hours. If you want to celebrate your birthday at the moment earth reaches the same spot around the sun as when you were born, then we’d all have the same issue - we’d have to celebrate it 6 hours later every year, reseting every 4 years. February 29 is very much a new day… which simply doesn’t exist on non-leap years. reply runarberg 3 hours agorootparentprevI asked JavaScript: const { format } = Intl.DateTimeFormat(\"en-UK\", { month: \"short\", day: \"numeric\" }); const date = new Date(2024, 1, 29); console.log(format(date)); // 29 Feb date.setYear(2025); console.log(format(date)); // 1 Mar const year = 1000 * 60 * 60 * 24 * 365.2425; console.log(format(new Date(2024, 1, 29).getTime() + year)); // 28 Feb I guess both are logical by some definition of logic. reply DinaCoder99 8 hours agorootparentprevSadly, I can say from experience the American draft does not count the number of birthdays celebrated to figure out if you're eligible to serve. reply qup 7 hours agorootparentYou got drafted when you were five? reply guappa 1 hour agorootparentMore like 4… since you can go to vietnam at 17. You're thinking of the age to drink a beer. reply qup 45 minutes agorootparentCan't get drafted at 17. I don't know if you can go or not. reply ao98 10 hours agorootparentprevThe most common approach I’ve seen is to alias them to Mar 1st. reply swasheck 9 hours agoparentprevso … many bullets were dodged today reply DANmode 9 hours agorootparentOpinionated devs are the best devs. Tell us about your last marriage?, lol reply peteradio 9 hours agorootparentprevIt's a leap year miracle! reply alphanumeric0 7 hours agoparentprevThis is why you use a datetime library. reply tgtweak 4 hours agoprevI have a friend who was born on Feb 29, and in Quebec your driver's license fees must be paid on or before your birthday or your license is effectively revoked (It's a convenient reminder). He was on his way to pay them on the 29th and got pulled over for having an expired license... after some awkward common confusion with the police they came to the conclusion that the license bureau moves your \"reminder\" birthday up by 1 day when the actual day is on Feb 29, instead of back to March 1st, so they don't miss out on 3 years of license payments for leap year birthday citizens. The cop had never encountered this before (1/1460 chance of occurring * the odds of being pulled over on that day) I don't think they ever patched this, so watch out if you're a leap day license fee procrastinator in Quebec! reply redox99 2 hours agoparentWouldn't the drivers license have the expiration date printed on it? reply xandrius 1 hour agoparentprevI mean, they could have paid it any day before then, right? I would have just gone on the 28th and be done with it. reply badcppdev 1 hour agorootparentLet me tell you about this human trait called procrastination.... actually I just have to do something first reply tudorw 7 minutes agorootparentPay By = Wait Until reply stevage 1 hour agorootparentprevSure...if you knew about this bug in the system. reply jewel 11 hours agoprevWe have a product that uses ChatGPT via the API, using the 3.5 turbo version. Our query involves some dates. Instead of giving back text like it usually does, today it has been giving errors because it does not think 2024-02-29 is a valid date. This is easy to reproduce with the web interface, at least sometimes [0]. It start out by saying it's not a valid date and then as it's explaining why it isn't it realizes its mistake and sometimes corrects itself. [0] https://chat.openai.com/share/37490c9f-81d6-499f-b491-116536... reply esalman 3 hours agoparentBlows my mind that people consider using ChatGPT for serious applications. I mean it's fine as a code autocorrect/autocomplete tool as in GitHub copilot. But it should not replace the code itself. You encounter a bug in the code, you fix it, you never encounter it again. But ChatGPT will repeat the same mistake sooner or later. That's not how we should engineer solution for critical problems. reply makoto12 1 hour agorootparentIf you sandbox your connection to openAI correctly, then you can get the benefit of a llm without making your application look silly at the same time. Identifying the correct places in your business to use it is tricky, but imo it certainly makes sense in a lot of specific areas. Just not a catch all that can run your business for you reply lucumo 3 hours agorootparentprevIf the cost-savings is worth it compared to the problems... I mean, that's how we do it with humans. It's quite a common occurrence to keep a part of a business process human because automating it would we too expense due to edge cases. Humans make mistakes and are expensive, but are also flexible and usually smartish. ChatGPT makes mistakes and is usually dumbish, but is also flexible and cheap. Engineering is about picking the right trade-offs in your solution. reply fuzztester 2 hours agorootparent>I mean, that's how we do it with humans. It's quite a common occurrence to keep a part of a business process human because automating it would we too expense due to edge cases. Which part of the human do people keep? The head? Arms? ;) #ParsingAmbiguityError reply fractalb 2 hours agorootparentprev> ChatGPT makes mistakes and is usually dumbish, but is also flexible and cheap. People wouldn't mind it if the keyword `dumbish` has been all along there. reply littlestymaar 10 hours agoparentprevWired: LLM are practically AGI Tired: ChatGPT thinks February 29th isn't a valid date. reply pquki4 9 hours agorootparentMy favorite prompt: asking \"How many e's are there in the word egregious\". Always says three (ChatGPT 3.5, 4, 4 turbo). When you ask it which three, it realizes its mistake and apologizes (or sometimes tells you where they are that is completely wrong). Looks like it just outputs gibberish for these things. reply moozilla 7 hours agorootparentChatGPT is specifically bad at these kinds of tasks because of tokenization. If you plug your query into https://platform.openai.com/tokenizer, you can see that\"egregious\" is a single token, so the LLM doesn't actually see any \"e\" characters -- to answer your question it would have had to learn a fact about how the word was spelled from it's training data, and I imagine texts explicitly talking about how words are spelled are not very common. Good explanation here if this still doesn't make sense: https://twitter.com/npew/status/1525900849888866307, or check out Andrej Karpathy's latest video if you have 2 hours for a deep dive: https://www.youtube.com/watch?v=zduSFxRajkE IMO questions about spelling or number sense are pretty tired as gotchas, because they are all basically just artifacts of this implementation detail. There are other language models available that don't have this issue. BTW this is also the reason DALL-E etc suck at generating text in images. reply chgs 30 minutes agorootparentChatgpt could say “I don’t know” reply Izkata 7 hours agorootparentprev> If you plug your query into https://platform.openai.com/tokenizer, you can see that\"egregious\" is a single token That says it's 3 tokens. reply wruza 6 hours agorootparentIt doesn’t even matter how many tokens there is, because LLMs are completely ignorant about how their input is structured. They don’t see letters or syllables cause they have no “eyes”. The closest analogy with a human is that vocal-ish concepts just emerge in their mind without any visual representation. They can only “recall” how many “e”s are there, but cannot look and count. reply brewtide 5 hours agorootparentPre-cogs, I knew it. reply redox99 2 hours agorootparentprev\" egregious\" (with a leading space) is the single token. Most lower case word tokens start with a space. reply ToValueFunfetti 4 hours agorootparentprevThe number of tokens depends on context; if you just entered 'egregious' it will have broken it into three tokens, but with the whole query it's one. reply fuzztester 2 hours agorootparentWhy three tokens, not one? reply 317070 1 hour agorootparentwithout the leading space, it is not common enough as a word to have become a token in its own right. Like the vast majority of lowercase words, in OpenAIs tokenizer you need to start \" egregious\" with a space character for the single token. reply devjab 8 hours agorootparentprevIt’s always gibberish, it’s just really good at guessing. I forgot what exactly it was I was doing. We were trying to get it to generate word lists of words ending with x or maybe it was starting with. For a marketing PoC and it made up oceans of words that not only didn’t start/end with x but mostly didn’t include x at all. Isn’t this also why it can pass CS exams and job interviews better than like 95% of us, but then can’t help you solve the most simple business process in the world. Because nobody has asked that question two billion times on its training data. reply mewpmewp2 8 hours agorootparentBut also it doesn't see characters. It sees tokens. The only way it would be reliably able to solve it is if it had a lookup table of token to characters. Which it likely doesn't. You couldn't do it either unless you learned the exact matchings of all tokens to all characters in that token and their positions if you were given tokens as an input. You would have learned the meaning of the token, but not what the exact characters it represents. reply yousif_123123 7 hours agorootparentEven if it sees tokens, I don't think it's an impossible task. Certainly an advanced enough LLM should be able to decipher token meanings, to know that a word is made up of the individual character tokens regardless of how the full word is tokenized. Maybe something gpt5 can do (or there's some real technical limitation which I don't understand) reply Izkata 6 hours agorootparent> to know that a word is made up of the individual character tokens A token is the smallest unit, it's not made of further tokens. It maps to a number. reply nvader 4 hours agorootparentI think what of is getting at is that given {the:1, t: 2, h:3, e:4} There should be somewhere in the corpus, \"the is spelled t h e\" that this system can use to pull this out. We can ask gpt to spell out individual words in NATO phonetic and see how it does. reply littlestymaar 1 hour agorootparent> There should be somewhere in the corpus, \"the is spelled t h e\" that this system can use to pull this out. Such an approach would require an enormous table, containing all written words, including first and last names, and would still fail for made up words. A more tractable approach would be to give it the map between the individual tokens and their letter component, but then you have the problem that this matching depends on the specific encoding used by the model (it varies between models). You could give it to the model during fine-tuning though. reply pests 5 hours agorootparentprevHe's saying the LLM will figure out how many letters are in each token. reply wruza 3 hours agorootparentIt's as feasible as telling how many chars in html lead to this comment by looking at a screenshot. LLM doesn't see characters, tokens, numbers or its own activations. LLM is a \"set of rules\" component in a chinese room scenario. Anything an operator of that room does is lower-level. GGP's idea suggests that an LLM, allegedly as a whole-room, receives something like: \"hey, look at these tokens: , please infer the continuation\". This puts it into a nested-room's-operator position, which (1) it is not, (2) there's no nested room. reply littlestymaar 3 hours agorootparentprevThey cannot “figure” it, they could learn it but for that it would need to be in it's training data (which isn't because nobody is writing down the actual pairing in every byte pair encoding in plain text. Also the LLM has no clue about what encoding it uses unless you tell it somehow in the fine-tuning process or the prompt.) reply botanical 1 hour agorootparentprevI just asked it with Gemini; at first, it got it right. Then I asked if it was sure and it apologised and said 3 is the correct answer. When asked what are the 3 \"e\"s, it says: > The three \"e\"s in \"egregious\" are: > > 1. The first \"e\" is located in the first syllable, between the \"g\" and the \"g\". > 2. The second \"e\" is located in the second syllable, following the \"r\". > 3. The third \"e\" is located in the third syllable, following the \"i\" and before the last \"o\". reply pants2 9 hours agorootparentprevFor the record, both ChatGPT-4 and Gemini Ultra affirmed that it's a valid date. Gemini reasoned through it and GPT-4 ran python code to make sure 2024 was divisible by 4. reply roland35 9 hours agorootparentInteresting... But that isn't exactly true! Centuries that are not divisible by 4 don't count! reply swores 8 hours agorootparentI've yet to come across a form of 100 that isn't divisible by 4... since 25 usually still exists! But I do remember there being some weird niche rules about which years are or aren't leap years, so I'm guessing your comment is basically right just wrongly worded? reply latexr 8 hours agorootparentThe rule is that leap years are the ones divisible by 4. Unless it’s also divisible by 100. Unless unless it’s divisible by 400. So 2000 was leap, but 2100, 2200, and 2300 won’t be, but 2400 will be. reply swores 8 hours agorootparentAhh, so it's centuries that aren't divisible by 400 rather than that aren't divisible by 4, that makes more sense! Thanks for answering reply thaumasiotes 6 hours agorootparentIt's centuries that aren't divisible by 4. It isn't years that aren't divisible by 4. reply inglor_cz 50 minutes agorootparentprevThe GP formulated it in a somewhat unclear way. \"Centuries\" divisible by 4 probably meant \"years\" divisible by 400. So, 19th century (1900 is the last year) isn't divisible by 4 (19/4 is not integer), which is the same as saying that 1900 isn't divisible by 400. This is the main reform of the Gregorian calendar - leap days aren't introduced on xy00 years which aren't divisible by 400. This corrects the length of a year to 365.2425 days, which is fairly close to the real value of 364.2422 days. The original Julian calendar had year of 365.25 days, which aggregated an error of more than ten days over the centuries. reply daseiner1 9 hours agorootparentprevDid it also check if 2024 is divisible by 100 but not 400? reply AlienRobot 10 hours agorootparentprevThat's because it's trained on 2021 data. No February 29 back then! reply peteradio 9 hours agorootparentprevBut akshually neither does my uncle Ned reply lynx23 3 hours agoparentprevSaw the same via the API with gpt-4-0125-preview. IOW, even gpt-4 thinks 2024 is not a leap year. reply baron816 8 hours agoparentprevChatGPT thinks today is March 1. Go ahead and ask it what today’s date is. reply huytersd 8 hours agorootparent> No, 2024 is not a leap year. Leap years are years divisible by 4, except for years that are divisible by 100 unless they are also divisible by 400. Therefore, the next leap year will be 2024. reply rodface 8 hours agorootparentAs the proud new owner of a leap day baby, I find this to be extra hilarious reply hilux 1 hour agorootparentCongratulations! Big savings on parties and cake! reply pizzafeelsright 6 hours agorootparentprevI was at the hospital and heard two babies being born. I love that music. reply heywoods 7 hours agorootparentprevCongratulations! reply s9df898r32h 7 hours agorootparentprevthats better then Mistral... it \"thinks\" it is March 15th 2023 \"The current date is March 15, 2023. However, please note that as a large language model, my knowledge is based on the data I was trained on, which is up to 2021. Therefore, I cannot provide real-time information or updates on current events or dates. I recommend checking a reliable source such as a calendar or a trusted news website for the most accurate and up-to-date information.\" reply vdfs 8 hours agorootparentprevIt is March 1 in many places reply cheapgeek 6 hours agorootparentprevIt's UTC. Ask it what day was yesterday. reply js2 7 hours agorootparentprevAs of 10 PM US/Eastern, ChatGPT 3.5 answers as follows for me: > You: what is today's date? > ChatGPT: Today's date is February 29, 2024. Please note that February 29 occurs only in leap years. If you have any more questions or if there's anything else I can help you with, feel free to ask! reply floydianspiral 7 hours agorootparentprevopenai actually in their platform for billing, etc shows that its actually march 1 as well. reply pavon 13 hours agoprevNo, but some of our software writes data to rotating directories named after the date, and while doing some manual debugging on a test system, it started failing to create these directories the first time it rotated on Feb 29 UTC. Turns out it just happened to run out of disk space at that time, but I had myself convinced that it was a leap year bug for over an hour. :) reply Imagenuity 5 hours agoparentLOL, a good example of correlation does not equal causation. reply KomoD 13 hours agoprevYes. > During the morning on Thursday, no ICA store in Sweden could accept card payments. Instead, you had to use cash, Swish or pay via their app. > The reason behind the problem was an internal problem in the payment systems at ICA as a result of an extra day in February, leap day. ICA being the biggest grocery store chain in Sweden reply evanb 1 hour agoparentParticularly odd since the Swedish calendar has had a lot of leap-day shenanigans, including a one-time Feb 30th. https://en.wikipedia.org/wiki/List_of_non-standard_dates#Feb... reply guappa 1 hour agoparentprevAnd people in sweden look at me weird when I say I keep some cash around. At least it wasn't 3 days like when Coop's provider got hacked. They also handle our pay checks at work… makes me feel so safe :D reply hannob 1 hour agoprevI know I'm late to the party, but I have a Python script that creates a security.txt for one of my own project, and it sets the \"Expires\" date to one year in the future. Old code: expires = datetime.datetime.now(tz) expires = expires.replace(year=expires.year + 1) It broke yesterday, throws an exception (\"ValueError: day is out of range for month\"). It's kinda obvious that it does. Fixed version code: expires = datetime.datetime.now(tz) + datetime.timedelta(days=365) expires = expires.isoformat(timespec=\"seconds\") Now we're just going 365 days into the future. Of course, this has a slightly different meaning and outcome, we are not always ending up on the \"same date next year\". But in this use case it doesn't really matter. reply yanateras 12 hours agoprevThe other way around! Today a few services that don't congratulate me on my birthday (on non-leap years) did. I was born on February 29th. reply chatmasta 10 hours agoparentHow often do you encounter challenges related to your birthday? Meaning, not just on the day of Feb 29, but any day of the year when you're trying to select your birth date or something like that. Do you ever find forms where the date is missing or the backend wont accept it? reply yanateras 10 hours agorootparentOccasionally, but less often than one could imagine! About a dozen times in my life. Whenever February 29 wasn’t present as an option, the frontend was at fault, so I could set the rightvalue in the inspector as a workaround. Other times February 29 was present as an option, only to be saved as February 28. Never March 1, which I’d say would be more coherent. reply nyczomg 7 hours agorootparentIf it makes you feel better.... On February 29, 2008 I was working on a system as a government contractor. Part of it was outsourced to a sub-contractor who decided to write their own date/time library because how hard could that be (which I only learned about later)? For some reason NOTHING would work right that day. Took me longer than I'd care to admit of staring at the logs to realize that some started with March 1 2008.... edit: also happy birthday! reply yanateras 6 hours agorootparentDefinitely not any more coherent on a leap year! That remark only applied to dates stored without a year, as in “February 29” rather than “February 29, 2008”. Thank you..! reply whycome 7 hours agorootparentprev> more coherent depends on what you're trying to organize by. maybe 'birth month' is an important signifier. reply yanateras 7 hours agorootparentHere’s why: * I was N years old on February 28th this year * I will be N+1 years old on March 1st this year Therefore: * I was N-1 years old on February 28th last year * I was N years old on March 1st last year Perhaps the month is February and the non-leap year date is March 1st... reply codetiger 7 hours agoparentprevHappy birthday to you! It’s a special day and We don’t get this chance to say this often. Enjoy your day reply yanateras 7 hours agorootparentThank you :3 reply gorbachev 2 hours agoparentprevhttps://www.askamanager.org/2018/01/i-paid-for-fake-referenc... You're not alone! reply bombcar 11 hours agoparentprevWere you born on the day after Feb 28 or the day before Mar 1? ;) reply jagged-chisel 7 hours agorootparentYes reply swader999 10 hours agoparentprevHappy birthday! reply yanateras 10 hours agorootparentThank you ^_~ reply seabass-labrax 9 hours agoparentprevStay, yanateres, stay! They have no legal claim! No shadow of a shame will fall upon thy name: stay, yanateres, stay! reply Wowfunhappy 10 hours agoparentprev...it occurs to me, when should those services congratulate you? Should it be on February 28 or March 1? reply jedberg 10 hours agorootparentAccording to the government, Mar 1. Anything that is based on your birthday (driver's license, drinking, signing contracts, etc) all happen on March 1 if there is no Feb 29 that year. Which makes sense since then it is \"after\" your birthday. reply yanateras 9 hours agorootparentprevI’d say March 1! February 28 might be too early. reply wolfendin 10 hours agorootparentprev365 days ago was March 1, 2023 reply jerbear4328 8 hours agorootparentIn 365 days, it will be February 28, 2025 Which is more important, the past or the future? reply mr_toad 7 hours agorootparentprevThis year has 366 days, or at least it does after Feb 29. reply FumblingBear 12 hours agoprevThis one is rather specific, but a game rhythm based Final Fantasy game called Theatrhythm Final Bar Line is simply not allowing people to play today because it has an internal system that awards prizes for specific days and they didn't handle the case of what to do when it's on a leap day. You can boot it up but can't actually play the game as a result. Not working on the game or anything but found it moderately amusing as someone who owns the game! reply brian-armstrong 4 hours agoparentYeah, this one was odd. I also ran into it. https://gamerant.com/theatrhythm-final-fantasy-bar-line-not-... reply addandsubtract 9 hours agoparentprevThat's why they had to release FF7R2 today. It was planned all along /s reply pastureofplenty 12 hours agoprevYes, I have a bot that posts daily San Francisco weather records to Mastodon. It did not post as scheduled today. This is because I am looking at all the high temperatures, low temperatures, and precipitation on today's date from 1875 (about as far back as there are digitized weather records I can work with) to the present. Since there was no such date as February 29, 1875 it is throwing an error. reply netcraft 10 hours agoparentI just dont get the “daily” weather records thing. Meteorologists talk about them constantly but theyre completely meaningless right? who cares what this particular day of the year highs were? its just noise! at least do it on a weekly resolution instead but even that seems so noisy itd be meaningless. sorry, this has nothing to do with you, just a pet peeve. reply jrockway 9 hours agorootparentPeople like them because they happen more often. \"This is the hottest February 29th on record!\" but it's not the hottest February 28th or March 1st. Compare that to something more mathematically rigorous: \"Today is warmer than 98.72% of days between February 25th and March 5th\". Nobody's going to click that link. reply DANmode 9 hours agorootparentYour words read like you're disagreeing, but the idea seems like it's agreeing. reply jasomill 9 hours agorootparentprevAs weather conditions are roughly cyclical with a period of a year, this doesn't strike me as a particularly unusual thing for a weatherperson to mention in passing. Though you're correct in that year-over-year weather changes and daily maxima lack the significance of long-term climate trends. reply igammarays 2 hours agoprevSurprised no one else mentioned the Cloudflare billing issue today. I got incorrectly invoiced yesterday and the file was named cloudflare-invoice-1970-01-01. https://www.cloudflarestatus.com/ reply fdgjgbdfhgb 12 hours agoprevAll street lamps in Paris turned off at midnight :) https://www.leparisien.fr/paris-75/paris-pourquoi-les-rues-d... reply aussieguy1234 8 hours agoparentFinally, night time photography without the street light pollution. reply blahedo 3 hours agoparentprevOh! The street lamps on my campus didn't come on tonight when it got dark—it didn't occur to me then but this could totally be why. reply Cyberdog 3 hours agoparentprevThe street lights use some sort of timer that's tracking dates? Don't they typically just use light sensors to know when to turn on? It seems to me that'd be a simpler solution and also provide light during solar eclipses, thick storms, etc. reply iforgotpassword 2 hours agorootparentWhy should we pick a simple solution like that? Is it the 90s or what? Only a complete clown would pick anything other than a network of zigbee relays controlled by an unmaintained node.js app written by an out-of-business contractor, pulling in sunrise/sunset data from an external API and syncing its local time via a homegrown NTP alternative. I mean look at electric cars. reply WarOnPrivacy 10 hours agoparentprevImproved nighting is an awesome gift. I'm jealous. reply dr_kiszonka 9 hours agoprevOne thing I have learned from HN is that datetime issues are hard, prolific, programming language agnostic, and not to trust myself to get the logic right. (The same applies to floats.) reply wruza 5 hours agoparentAnother thing programmers should know and fix is that most of it is self-inflicted. When there’s no easy way to add a day, people add 86400000 and stumble upon a leap second. When time is not needed, they use fixed hours and fail at timezones. And so on. Most date libraries provide mostly trivial and at the same time low-level use-cases, so people do all the stupid math with what essentially is an irregularly-based number. reply submain 9 hours agoparentprevjust like you don't roll your own crypto, you don't roll your own date libraries. reply epolanski 9 hours agorootparentTakes me less to launch some coin fork than getting even a single date time api correctly. reply zormino 9 hours agorootparentThe API is correct, you just called it at the wrong time reply wut-wut 8 hours agorootparentprevFacts! reply anoopelias 9 hours agoparentprevFrom \"Falsehoods Programmers believes in\" series: https://news.ycombinator.com/item?id=4128208 reply axegon_ 48 minutes agoprevNot today but several years ago while lending a hand on a project that was wildly overdue and the guys needed all the help they could get. Disclosure: it was not in software that was in production yet(otherwise you would have likely heard and been affected by it by now I suspect). The software's purpose was dealing with commercial airlines - routes, connections and all that. The travelling salesman problem basically. The software was meant to replace an antique system which no one knew how to maintain since in 2020 it was still running on mainframes and written in fortran. There were specifications on how the system worked but there was no one who could even read the code. Anyhow, the way dates were stored, read and calculated was absolutely insane. I can't recall what the exact deal was, but each date was represented with either 4 or 5 bytes with some really awkward algorithm to make it into something meaningful. With a bunch of additional patches surrounding Y2K. Unfortunately it was a lot more subtle than getting March 1-st instead of Feb 29-th. Each date was calculated with a dynamic offset depending on the month and year. So during leap years, everything was fine until the 14-th of May at 01:00 UTC. The second the clock hit, 14-th of May at 01:01, all hell broke loose incrementally as time want on until the end of each year. The weird string representation for Nov 1-st was being calculated as January 3-rd for instance. But as soon as the clock hit January 1-st, everything went back to normal. It took 6 people 2 days to figure it out. As you might expect, this was not mentioned anywhere in the documentation. I'm not sure what has happened ever since, but if the system made it into production and you didn't get stuck in an airport... You're welcome lol reply animal_spirits 6 hours agoprevDidn't see it mentioned here but cloudflare sent me an invoice today and the attached PDF was titled cloudflare-invoice-1970-01-01.pdf :) reply guappa 1 hour agoparentTheir 32 bit time_t ran out sooner than expected! reply qatanah 6 hours agoparentprevoh no wonder they had a billing outage. https://www.cloudflarestatus.com/ reply mj1586 4 hours agoparentprevWas the content of the PDF correct? Or did it also have some strange dates in it? reply userbinator 7 hours agoprevIt's scary how much code out there is less than 4 years old, and even scarier how much of it is because someone decided to somehow rewrite it unnecessarily and thus introduce this bug. I don't even want to know how many people think the code they wrote will be in use for more than 4 years. reply ghewgill 6 hours agoparentOn the other end of the scale, the code base I worked on today has seen at least 7 separate leap days in production. Last week a manager asked if we expected any issues with the leap day, and we all just kind of laughed (and politely said no). reply barbequeer 24 minutes agoprevI don't know if it was a Leap year bug but after midnight my wife's phone still said Feb 29th even though it should have switched over to March 30th at 00:01 reply EnigmaticLion 8 hours agoprevWe have a rails 6 app, and there's a test that essentially expects time_ago_in_words(1.year.from_now) to return \"about 1 year\" (as part of a user facing message). The test failed. I thought it's a flaky test but i was able to reproduce it locally. Turns out executing that code on a leap day returns \"almost 1 year\" instead. Can test it in rails console if you are interested: irb(main):001:0> include ActionView::Helpers::DateHelper => Object irb(main):002:0> distance_of_time_in_words(Date.new(2025, 2, 28), Date.new(2024, 2, 29)) => \"almost 1 year\" irb(main):003:0> distance_of_time_in_words(Date.new(2025, 3, 1), Date.new(2024, 2, 29)) => \"about 1 year\" irb(main):004:0> distance_of_time_in_words(Date.new(2025, 2, 28), Date.new(2024, 2, 28)) => \"about 1 year\" reply DinaCoder99 7 hours agoparent> distance_of_time_in_words(Date.new(2025, 2, 28), Date.new(2024, 2, 28)) Curiously, this is also not quite a year, even though the days and months are the same. reply mr_toad 7 hours agorootparentIt’s probably comparing the number of days between the dates with the number of days in a year, which would usually work. reply EnigmaticLion 3 hours agorootparentSource is a lot more complicated than i thought it will be: https://github.com/rails/rails/blob/v6.1.7.7/actionview/lib/... reply whatever1 7 hours agoprevWhy this is not a global holiday? Let's all come together and agree to do nothing on this bonus day. Calendar Problems solved. reply IvyMike 3 hours agoparentEven those \"24/7/365\" people technically agree, today is a day off. reply pizzafeelsright 6 hours agoparentprevWe should get it off work. We aren't getting paid for the extra day of work right? reply chgs 24 minutes agorootparentNo power, no hospitals, no internet, no phones, no care homes etc. reply whatever1 3 hours agorootparentprevExactly. Off the books for everyone. reply hiAndrewQuinn 20 minutes agoprevYeah! Our friends got married yesterday, and now they have until 2028.02.29 to legally change their names. reply bxparks 12 hours agoprevI can understand getting the years 2000 (leap), 2100 (not leap), 2200 (not leap), and 2300 (not leap) wrong. But getting the year 2024 wrong is, disappointing, to put it diplomatically. reply samatman 10 hours agoparentThose are harder to get wrong, since leap year bugs are almost always from consumers of dates. To screw up a not-a-leap-year-year you'd need to produce a bogus February 29 on a real March 1. Date producers tend to understand how the calendar works better than that. reply JohnFen 12 hours agoparentprevThat's putting it very diplomatically indeed. reply nortlov 13 hours agoprevThe Casio F-91W doesn’t account for the year, it showed today’s date as Thursday, March 1st. reply LeoPanthera 12 hours agoparentThat's more of a design decision than a bug. It's intentional to make the product cheaper. The manual does mention it. reply jdmdmdmdmd 10 hours agorootparentWow, you're right. >Calendar system: Auto-calendar set at 28 days for February [0] [0] https://support.casio.com/en/manual/009/qw593.pdf reply tjvc 12 hours agoparentprevI noticed this too and clicked this thread wondering if it would show up! Still an awesome watch. reply computerfriend 5 hours agoparentprevI just looked at my watch and indeed the date is wrong. Thanks! reply bakoo 13 hours agoparentprevSame with my colleagues most recent top model Samsung Galaxy Watch :] reply dotnet00 12 hours agorootparentOdd, mine is showing the right date. reply davchana 11 hours agorootparentAre you sure? My F91W doesn't even ask for year in settings. How would have it known it is a leap year or not. Some other models (not F91W) does track year. reply dotnet00 11 hours agorootparentI was responding to the claim about the Samsung Watch not showing the right date, I unfortunately have never owned the older kinds of \"smartwatches\" :) reply frankgrimesjr 11 hours agorootparentprevAre they using a third party watch face? My Galaxy Watch 4 has no issues. reply kennethrc 8 hours agorootparentI have a custom watch face on my GW4 and it's showing the 29th reply linker3000 12 hours agorootparentprevOK on my Watch4 Classic FWIW. reply spicybright 12 hours agoparentprevThank you so much for the reminder, that would have screwed me up today :) reply hsuduebc2 6 hours agoprevSpotify Artist which is app for managing as name states your artist profile should allow you to run promo campaigns but they need to be set till the end of the previous month. Well. 29 February according to spotify is not a valid February date. I know it's somehow an edge case but it's one which appears every four years so I dont understand how it could be missed by such a big company. reply hsuduebc2 6 hours agoparentAnd if someone wanted to point out that I shouldn't let things to last moment. He would be absolutely right. reply Too 3 hours agoprevStupid question. How do these bugs happen? If you just take the naive approach and calculate everything as unix timestamp deltas, “yesterday” in human terms will still be 86400 seconds ago. No difference leap day or not. If you use a date library with fancy “subtract one day” functions, it should also be handled automatically. Just like you don’t have to care that March has 31 days and April 30? No difference if leap day or not. Someone must have spent a lot of effort to manually create date logic, hard coding the number of days in a month or something? Only other edge case is if someone takes a 2024-02-29 timestamp and modify the year part, without using date library to do so. That should be rare, only use case is a yearly report. That’s not something that should take down the whole billing system. reply rfoo 2 hours agoparent> Only other edge case is if someone takes a 2024-02-29 timestamp and modify the year part, without using date library to do so. Date library does not help here, for example, both Python and Java's date library [0][1] have only \"days\" in their timedelta/Duration constructor. Month and year are ambiguous. What would you do if you want someone's birthday this year? The most obvious way: birthday: datetime.date birthday_this_year = birthday.replace(year=datetime.date.today().year) is just broken. The second line throws. [0] https://docs.python.org/3/library/datetime.html#datetime.tim... [1] https://docs.oracle.com/javase/8/docs/api/java/time/Duration... reply perlgeek 2 hours agoparentprev> Only other edge case is if someone takes a 2024-02-29 timestamp and modify the year part, without using date library to do so. That should be rare, only use case is a yearly report. Another use case: you create a pair of cryptographic certificates with a validity period of 1 year. The other side rejects the certificate because of an invalid date. This happened 12 years ago to Azure: https://azure.microsoft.com/en-us/blog/summary-of-windows-az... reply Karliss 1 hour agoparentprevOne case I can imagine is if the authors of law/rules that the code tries to implement were trying to be clever and defined the specific way this edge case needs to be handled. In such situation developer is likely forced to do some manual logic instead of the straight forward whatever happens when offsetting timestamp by 365 days using time library. reply lancebeet 2 hours agoparentprevI've seen many date-related bugs similar to these and in very few cases it's as simple as \"one developer was incompetent and did something stupid\". Incompetence may be part of the problem, but in the background there are often data sources or services that provide the data in non-standard or unpredictable ways as well as specific business requirements that complicate the process. reply jung_j 13 hours agoprevThere was one in the Phoenix Framework (Elixir) about issuing certificates with an invalid end date: https://github.com/phoenixframework/phoenix/issues/5737 Interestingly, Azure had this bug some years ago too leading to an outage. https://azure.microsoft.com/en-us/blog/summary-of-windows-az... reply lukan 41 minutes agoprevI unsubscribed from Netflix and was supposed to have it till the end of februar. But yesterday the 29. there was already no more access .. Quite trivial, but still a bit ridiculous. reply jonsagara 13 hours agoprevHere's a blog post that is tracking issues: https://codeofmatt.com/list-of-2024-leap-day-bugs/ reply webel0 13 hours agoprevYes, Hesai LiDAR [1] bug is grounding cars. [1] https://pandaily.com/hesai-technology-addresses-lidar-produc... reply trebligdivad 10 hours agoparentFascinating - why does a LiDAR involve the date? reply bnqscrtm 7 hours agorootparentLiDARs and other sensors typically need to be time-synchronized with the rest of the robotic system. I wouldn't be surprised if the bug is related to that. reply worik 10 hours agorootparentprevFor the survalence records Telling base where, and when, you were reply nemomarx 10 hours agorootparentShouldn't that just be a unix timestamp or something? You'd think resolving that to a date time could be done in the UI instead of in the business logic reply defrost 7 hours agorootparentUnix time has discrepancies whenever leap seconds occur (several times in my career aquiring geophysical data). If you're measuringcontrolling objects in the physical world (cars, rockets, etc) then you should not use unix time - those glitches will happen and instantaneous computations will go kooky. https://en.wikipedia.org/wiki/Unix_time#Leap_seconds reply redox99 2 hours agorootparentBut so does resolving to a date. I don't see how resolving to a date which cares about leap days fixes any of that. You should use a monotonic clock with an arbitrary starting point anyway, unless you need some kind of synchronization between devices, but you probably wouldn't use unixtime there anyway. reply defrost 2 hours agorootparent> But so does resolving to a date. I don't see how resolving to a date which cares about leap days fixes any of that. So why bring it up then? > You should use a monotonic clock with an arbitrary starting point anyway Sure. We started doing that more than 50 years ago now when broad area geophysical surveying started off. > unless you need some kind of synchronization between devices, Can't see the problem - there are ways of syncing base station records against aircraftboatvehicle records in post processing .. all the stations, fixed or mobile, use a monotonic epoch based record structure that hold channel data and any sync marks that are broadcast by whatever means - raw GPS time serves well enough for a grain of 1.5 seconds, other marks can be used as required. reply topaz0 9 hours agorootparentprev\"should\" reply zeroCalories 10 hours agoprevRemember, always use a library for cryptography, payments, and date calculations. reply interbased 6 hours agoparentYup. I use the datetime module in Python and didn’t need to do anything extra. reply tdhopper 13 hours agoprevMy sister is staying in a hotel and all the keycards stopped working. reply mj1586 9 hours agoparentThis also happened on Leap Day 2020 with Onity locks used in Crown Plaza Hotels. See https://i.imgur.com/GI5A3jW.png. I wonder if it's the same issue never fixed? Can you share with us the hotel chain and/or the lock manufacturer? Thanks. reply ant6n 11 hours agoparentprevWhat happens in a situation like this? Does the staff issue physical keys? Do the doors even have manual locks? Do the staff walk every body to their room? reply atomicstack 7 hours agorootparentAbout a year ago I stayed in a hotel and the door lock started misbehaving one morning while I was at breakfast. Went to the front desk, got a new card, went back to the room and discovered it still didn't work. After doing that two more times, someone was sent up to the room with a mysterious, palm-sized device with a USB cable hanging off it, which they plugged into a well hidden USB port on the bottom of the lock. The device performed some black magic, and after about 30 seconds a light on the lock changed colour from orange to red and it started functioning correctly again. reply amatix 13 hours agoprev> A number of New Zealand petrol pumps stopped working on Thursday due to a \"leap year glitch\" in payment software, fuel stations and the payment service provider said. https://www.reuters.com/world/asia-pacific/leap-year-glitch-... reply dang 2 hours agoparentRelated ongoing thread: Self-pay gas station pumps break across NZ as software can't handle Leap Day - https://news.ycombinator.com/item?id=39553755 - Feb 2024 (31 comments) reply PlunderBunny 11 hours agoparentprevI'm sure it's more complicated than \"we didn't think about leap years\", but it certainly sounds pretty amateurish. Old programmer rant: In my day, we fixed the Y2K bug - we went to the future and back several times a day! reply kevin_thibedeau 10 hours agorootparentIn your day people weren't doing everything with string ops in JavaScript. reply PlunderBunny 9 hours agorootparentOh, the progress we've made! reply mustntmumble 8 hours agorootparentprevIt does sound amateurish eh? I do most of my business logic inside Microsoft SQL Server Stored Procedure, and MS SQL just takes care of date pretty well. But then, come to think of it, you don't need to use SQL to have good date logic. C# and Java both have excellent date handing libraries. I don't know about C++, but I'd be surprised if there was not a modern date library for C++, so I am of the opinion there should be no excuses for software not to work on the 29th of Feb. reply throwaway2037 5 hours agorootparent> but I'd be surprised if there was not a modern date library for C++ The standard library now includes . AFAIK: It was mostly written by Howard Hinnant. He now has more date/time libs that expand upon : https://github.com/HowardHinnant/date reply SoftTalker 6 hours agorootparentprevYes if you're writing your own date/time routines, dealing with dates as seconds since epoch, or anything like that you're going to get burned. Use date handling functions and libraries that have been developed by people who know how to do it and have been battle tested. reply mttpgn 6 hours agoprevPython script using datetime crashed with a \"ValueError: day is out of range for month\" exception. reply Smoosh 12 hours agoprevI certainly did. There is a batch process to cull old records. It checks for customers who do not have a date of death recorded but are > 130 years old, as it assumes that we weren't informed of their death. It takes 130 years from the current date and uses that in an SQL statement to compares it to the date of birth. DB2 doesn't like 1894-02-29. Apparently it happens every 4 years, but no-one can be bothered to fix it. reply verbify 10 hours agoparentA simple fix would be to change it to a multiple of 4 (e.g. 124 years). Then it would fail every 100 years. reply samatman 10 hours agorootparent128! It was right there! reply gield 9 hours agorootparentprev2000 was a leap year so it would not fail in 2100. But it would indeed fail in 2200/2300/2400 as 2100/2200/2300 are not leap years. reply addandsubtract 9 hours agorootparentBut then it's someone else's problem. reply topaz0 9 hours agorootparentMost likely nobody's problem, really. reply ajdude 5 hours agoprevEA Games were crashing today and they announced that the solution is to set the date to March 1st: https://twitter.com/eahelp/status/1763192739322085598 reply jonathantf2 10 hours agoprevOur Sophos anti-virus at work blocked access to every website this morning because of the leap year, had to disable the web filtering to get us back up and running. reply fostware 9 hours agoparentYeah, so many \"untrusted website\" screenshots in the office chat. Half a productive day for staff in Australia reply nameoda 12 hours agoprevMy monthly bus passes for both February and March did not work in Dallas today. The driver was aware of the issue and just waved me in. reply mrwizrd 10 hours agoprevUnless you're in your 70's, YouTube doesn't allow you to buy premium if you were born on a leap year, because their age calculation is broken, and it thinks you're under 18. Google One works fine. Support will suggest that you change the birthday on your Google account, which breaks account recovery processes that depend on you furnishing matching identity documentation. Mind-boggling. Because nobody at YouTube has apparently ever encountered this problem, I ended up having my partner buy a family plan. As much as I hate paying so much a month for YouTube of all things, screen-off background play is a paid feature on iPhone, having one of the two accounts on your TV showing ads is absolutely infuriating and I'm not sure I fancy risking an account ban for anything they can associate with someone costing them ad revenue. reply akincisor 9 hours agoparentSo you're paying them to not fix a bug in their system, am I reading that right? reply kunley 1 hour agoparentprevThat should be a top comment here as it is most annoying and prevalent bug of all presented in this thread. reply mateo1 8 hours agoparentprevI wouldn't expect google to mess this up but then again someone probably calculated value produced vs project cost and tossed this one at the bottom of the pile 10 times already. 66/100k people (0.066%, close to the definition for a rare disease) would be born on 29th of feb on a uniform distribution, probably minus another 5% or so because february is in many places the least likely month to be born into. And out of those people how many actually put in their real birthday? Still odd to mess up a subtraction though. reply rvba 18 minutes agorootparentGoogle is now outsourcing development to low cost countries. They dont care about quality much reply mj1586 10 hours agoparentprevFascinating. I presume you were born on Feb 29 of 2004 or earlier. What did the system do when you tried to purchase? I'm curious how the error manifested. Thanks. reply thayne 2 hours agoprevRan into a bug where some code was taking an hour long interval and subtracting 1 year from the start and end times. But the time library handled subtracting a year from Feb 29 as converting it to Feb 28th of the previous year, at the same time. So on-call got alerted because a job failed when an exception was thrown because the start time was after the end time. Fortunately, other than the alert, there wasn't any negative impact. Although, I think if this isn't addressed it may cause issues next year when it includes a bigger interval of time than expected on March 1st. reply pizza234 10 hours agoprevYes, an amusing one: the Android app for the Berlin public transport, on the 29th, listed the results with the date of the previous day (28th). The funny thing is that the list is prefaced by a banner saying that this is a known bug, and the results actually refer to the 29th. Interesting way to workaround a bug :) reply rahimnathwani 9 hours agoparentThe app was probably designed to be able to show an emergency banner if ever needed. Easier to change that message than fix a complicated API. reply mj1586 9 hours agoparentprevI think this is it here, yes? https://twitter.com/abhyudaypratap_/status/17632771389442543... reply pizza234 3 hours agorootparentYes :) reply kdeldycke 1 hour agoprevBilling. It always has to be the billing. For a list of all other edge cases, you have: https://github.com/kdeldycke/awesome-falsehood#readme reply masklinn 3 hours agoprevHad a pair of tests break CI. At least one of them was the test, code it was testing did its job fine but test could not cope with February 29th. Obviously the test is broken in two different ways: it fails on Feb 29 and more importantly tests the behaviour on $current-date so only tests edge cases on the day of. Other I didn’t follow. reply pgeorgi 11 hours agoprevhttps://review.coreboot.org/c/coreboot/+/80790 reply williamstein 13 hours agoprevI have some unit tests for billing and subscription code for my company that started breaking in CI today due to the leap day: https://github.com/sagemathinc/cocalc/commit/8575029c2b76787... reply bregma 13 hours agoprevPython. cls = , data_string = 'Feb 29 04:55:03.687' format = '%b %d %H:%M:%S.%f' E ValueError: day is out of range for month reply ptx 11 hours agoparentAs mentioned by sibling comments, it's because you're not specifying a year. If you change the day to the 28th you'll see that it defaults to the year 1900: >>> datetime.strptime('Feb 28 04:55:03.687', '%b %d %H:%M:%S.%f') datetime.datetime(1900, 2, 28, 4, 55, 3, 687000) >>> datetime.strptime('Feb 28 13:37:06.942', '%b %d %H:%M:%S.%f') datetime.datetime(1900, 2, 28, 13, 37, 6, 942000) reply OJFord 11 hours agorootparentThat makes it weird though, because 1900 was a leap year? I sort of get it, but it's a slightly odd and inconsistent decision. Edit: no it's not, it's absolutely correct, leap years just aren't as simple as I thought! reply jesprenj 11 hours agorootparentIt wasn't. https://learn.microsoft.com/sl-SI/office/troubleshoot/excel/... reply OJFord 10 hours agorootparentCrikey - more helpful to me is the page linked from there - https://learn.microsoft.com/sl-SI/office/troubleshoot/excel/... > However, there is still a small error that must be accounted for. To eliminate this error, the Gregorian calendar stipulates that a year that is evenly divisible by 100 (for example, 1900) is a leap year only if it is also evenly divisible by 400. > For this reason, the following years are not leap years: > 1700, 1800, 1900, 2100, 2200, 2300, 2500, 2600 I had no idea! reply ahazred8ta 6 hours agorootparent\"We establish that a bissextile [366th day] shall be inserted every four years (as with the present custom), except in centennial years. So the years 1700, 1800, and 1900 will not be leap years. Assuredly, the year 2000 will have an extra day in it.\" -- Greg XIII, 1582 reply pxx 11 hours agorootparentprev1900 was not a leap year... It's 0 mod 4, yes, but it's 0 mod 100 and not 0 mod 400 reply OJFord 10 hours agorootparentYep, thanks, I just wasn't aware of the latter rule at all. Time to re-read Falsehoods perhaps! reply derefr 12 hours agoparentprevThat doesn’t seem incorrect; given that no year is specified, it seems like it’s evaluating the constraint in the context of an implicit default year. (1970? 0CE?) The confusing part, to me, is that Python would consider the above string to be parsed into a date in the first place, given that it has no year. reply herpderperator 12 hours agoparentprevInteresting... I suppose that is because there is no year? What year does it default to? Can you show your exact line of code? reply nicholasjarnold 12 hours agoparentprevconfirmed. and interesting/unexpected! this breaks: datetime.strptime('Feb 29 13:37:06.942', '%b %d %H:%M:%S.%f') edit: added code example. import datetime from datetime first obvi reply LeoPanthera 12 hours agoparentprevThat's because you didn't specify a year. reply mirthturtle 9 hours agoprevFound one on iMessage! Wished former coworker a happy birthday, and it said, \"Siri found a birthday: March 1\". Though when I pressed update, it correctly marked it as the 29th. reply fuzztester 2 hours agoprevWonder if orgs are getting ready for 2038 like they did for Y2K. Only 14 years or so away. https://en.m.wikipedia.org/wiki/Year_2038_problem https://en.m.wikipedia.org/wiki/Year_2000_problem reply o11c 12 hours agoprevWe got a bunch of close-dated yogurt the other day. Several were best by 2-27, 2-28, 3-01, and 3-02, but none by 2-29. reply jareklupinski 12 hours agoparent> none by 2-29 traditionally known as the \"Angel's Share\" :P reply mindcrime 9 hours agoprevLast night around 8:00 PM I noticed the time on my Linux laptop was wrong. I tried to reset it manually and got a \"Cannot set current time\" error. Closed the dialog and a minute or two later the time reset to the correct value. I shrugged it off as something random. Then today I get home from work, un-suspend that same laptop and the time and date are both wrong now. It has it as around 10pm last night. Turn off auto updates and turn that back on, and nothing happens. Wait 4 or 5 minutes, nothing happens. So I finally decided to reboot, which does indeed clear things up. Only then did I remember that today is Feb 29th and make the connection that this might be related to the leap year. Can't say for 100% certain that it was, but it would be one hell of a coincidence otherwise. reply swores 8 hours agoparentIf you have date/time problems that are unrelated to leap years, there's a 1/~1,461 chance of it happening on a Feb 29th. Given how many devs/techies/linux users/(and even non-techy users of tech) will encounter random time or date related bugs and problems every year, it would be surprising if there weren't a few people getting that coincidence every time a Feb 29th rolls round (or put another way, it seems likely that there's more than one person in the world having a similar \"no idea what caused that\" date/time bug every day of the year, including this one). So maybe related, but I wouldn't call it one hell of a coincidence if it's not related. reply inciampati 12 hours agoprevYes, in T-Mobile billing, I tried to set up automatic payments on the 26th, but the system both told me that this was impossible (because it was \"less than 2 days from the end of the month\") and then accepted it, because why wouldn't it. reply 151 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The billing team resolved a bug that was inaccurately billing monthly subscribers for an additional day, ensuring fair charges for all users.",
      "All test suites have successfully passed, indicating the bug fix is effective, and a postmortem is planned for 2028 pending QA validation."
    ],
    "commentSummary": [
      "The dialogue focused on leap year bugs affecting different applications, such as billing mistakes and appointment rejections.",
      "Debates emerged on managing leap year birthdays and the complexities AI models like ChatGPT face.",
      "Challenges with date processing, incidents on February 29th, and the significance of accurate date calculations, especially the risks posed by leap days in operational systems, were underscored."
    ],
    "points": 422,
    "commentCount": 401,
    "retryCount": 0,
    "time": 1709238163
  },
  {
    "id": 39547940,
    "title": "Hetzner Introduces Hourly Billing for Most Products",
    "originLink": "https://docs.hetzner.com/general/others/new-billing-model/",
    "originBody": "Questions about billing changes between March and May 2024 What will change about the way that Hetzner calculates the invoice for March 2024 and why? We are in the process of changing our billing structure to make it more unified; by doing this, we hope to make our products more user friendly. As part of this process, we will change the billing for many of our products from monthly to hourly. Some products will be based on consumption. This structure will be similar to our Cloud billing. It will take effect in March 2024. If you use a product for the whole month, we will continue to invoice you for the same monthly amount as always. During the transition period, which will start with the March 2024 invoice, you may see some unusual things on your invoice. Below, you can see an example of what these changes may look like during the transition period. Just to make it clear, the montly price you will pay will NOT change, even if you see some short-term changes in monthly totals on your invoices during the transition period. It is possible that you will see a smaller-than-normal total on your invoice for March. In April, we will retro-actively invoice you for March for any hourly-based products; the invoice will be pro rata (proportional to the amount of hours in March that you used those products.) Why will my March invoice be different from my earlier invoices? Starting in April, we will only invoice customers for the services that they have used AFTER the month that they use those services. For that reason, the March invoice will NOT contain any products whose prices we already calculated in March under the old billing structure. That's why your invoice for March may have a smaller total that normal. The affected products (the ones with hourly-based billing) for March will then appear retro-actively on the APRIL invoice, but with the new hourly based billing. Please take a look at the example invoices below. Why will my April invoice be different from my earlier invoices? Your April 2024 invoice will be the first one that is based on hourly billing. It will cover your hourly usage for the month of March. The hourly billing will apply to most products, but not all. For example, it will not apply to one-time products (like setup fees) or to products like domains and SSL certificates, which will continue to have yearly payment periods. For this one time, the invoice will retro-actively cover products that were NOT covered on March's invoice as part of the transition period. After that (starting in April) all future invoices will have a service period for hourly-based products. The service period will start on the first day of the month and end on the last day of the month. If you use these products for a full month, you will see the normal monthly price. For more details, please look at the section below (\"Example invoices\"). What will be new in the May invoice? By the time you get your May invoice, the transition period will be finished. The May invoice will show you the products that you used in April. For hourly-based products, you will see how many hours you used each product for April. If you have used the product for the entire month of April, you will see the normal monthly price. If you use the product for less than a month, and the hourly calculation is less than the monthly total, we will charge you the smaller hourly-based amount. For more details, please look at the section below (\"Example invoices\"). Why didn't I get an invoice yet for April? There may be a one-time delay in when we send you your April invoice. We apologize for that in advance and ask you for your understanding. Naturally, we will extend the time that you will have to pay your April invoice. In the months following April, we will send you invoices regularly and on time. Why am I now receiving my invoice on a different day of the month? We want to provide you with better and quicker customer service in the future when you have billing questions. That's why we have decided to spread out the days of the month when we send out invoices to customers. You will always receive your invoice on the same day of the month. But this day will be different from customer to customer. This will allow our support team to answer billing questions more quickly. We sent you an email informing you about this change; it had the subject line \"Changes to the billing model\". We sent you this email between 29 February and 15 March. You can find a copy of this email below. You can find your new billing day by going to your account. Example invoices Example: You have two servers, Server A and Server B under the old billing model. Server A had a service period of 21 February to 20 March. Server B had a service period of 27 January to 26 February. In February you received, as usual, an invoice for both servers with their two different service periods: In March, you will NOT see any products on the invoice that will have a service period that extends into April 2024. To be specific in this example, only Server B would appear on March's invoice because its service period does not fall into April. It only goes from 27 February to 26 March. Server A, on the other hand, has a service period that would extend into April. So it would NOT be calculated on the March 2024 invoice. That's why the invoice for this example for March would be smaller. Now let's imagine that you order another server, Server C, in March, and Hetzner gives you access to the server on 15 March. The April invoice will be the first retro-active invoice. It will include the service period of March for any products that have not already been calculated in March's invoice: Server A for 21-31 March (previous invoice calculated in February) Server B for 27-31 March (previous invoice calculated in March) Server C for 15-31 March (new product; first invoice) The calculation for these invoices is based on hourly usage. Now let's imagine that you cancelled Server A and that cancellation takes effect on 31 March. To replace this server, you order a new one, Server D, which Hetzner gives you access to on 20 April. The invoice for May will include the service period from the previous month, April. So you would see the full service period for April listed for each server. Since you used Server B and Server C for the whole month, you would see the full monthly price there. And since you only used Server D for part of the month of April, you would see the calculation there based on hourly usage. Questions about your invoices and payments Can I change the date that Hetzner issues my invoice? Unfortunately, no. For the foreseeable future, it will not be possible to change the date that we issue your invoice to you. We're sorry about that. How precisely does Hetzner calculate the hourly billing? The beginning of the hourly billing starts as soon as the product becomes available to you. We always round up when calculating the hourly use. If you only use the product for a few minutes, we will charge you a full hour. If you use the product for close to a month, and the montly price is cheaper, we will always charge you the cheaper monthly price rather than a higher hourly price. When will Hetzner charge me the hourly price? We will always use the hourly price when it saves you money, meaning when you have used a product for less than a month, and the total hourly price is less than the monthly price. When does the time for the hourly price start? It starts when Hetzner makes the product you ordered available to you. This is true even for products that you use for just a short period of time. Can I pay in yearly payment periods? Unfortunately not. In the new system, we only allow monthly payments. However, you can make automated payments with two different payment options to help make it easier to make your payments. You can do automated payments using SEPA bank transfers or credit cards. Important note: It is not always possible to do automated payments. There may be some exceptions. Email from Hetzner I don't have the email from Hetzner about the new billing structure at hand. What is changing? Starting in March 2024, almost all of our products will have consumption-based and hour-based billing, similar to our Cloud products. What does this mean for you? Consumption-based billing: The invoices for these products is based on the service we provide. This means you always receive an invoice for the previous month. Hour-based billing: Billing is based on the actual usage time in hours. There are some exceptions: products with an annual fee (domains and SSL certificates) and licences. These products' invoices are based on the entire calendar month regardless of when during the month you order them. If you use the products for a whole month, you will be charged the monthly price as usual. What is important during the changeover phase in March and April? During the changeover phase, your products will be charged pro rata (based on the proportion of the month). This may result in different invoice amounts for March and April. The changeover will happen automatically. You do not need to take further action. Your monthly price will not change. What is important after the changeover? We want to be able to offer you even better customer service in the future. For that reason, we want to change the date of your invoice. This will make it easier for us to quickly answer questions about your invoices.",
    "commentLink": "https://news.ycombinator.com/item?id=39547940",
    "commentBody": "Hetzner switches to new billing model (hetzner.com)305 points by throwaway220033 23 hours agohidepastfavorite181 comments MathiasPius 20 hours agoThis is actually great. I maintain a library[1] for interacting with the Robot interface using Rust, but testing is heavily gated because of the potential costs it might incur, which is why a lot of the purchasing/cancellation APIs haven't been thoroughly tested. With this billing, I'll be able to do thorough integration testing without breaking the bank. And of course with hourly billing, horizontal scaling becomes much more feasible. [1] https://github.com/MathiasPius/hrobot-rs reply jupp0r 18 hours agoparentHave you reached out to Hetzner? Feels like they should waive those bills in exchange for the free dev work you do for them. reply MathiasPius 16 hours agorootparentI haven't actually. I believe they do offer modest amounts of credit for creating useful libraries, documentation and such targeting their platform, but I never really looked into it. reply sonicanatidae 14 hours agorootparentprevA company, acting reasonably? Are you new to Earth? reply joeig 19 hours agoparentprevAt the moment there is still a one-time order fee for dedicated servers and the docs don’t clarify if it will remain. reply diggan 19 hours agorootparent> At the moment there is still a one-time order fee for dedicated servers For (some) dedicated servers, I think. Last time I checked the lowest tiered dedicated servers didn't have any setup fees. If it's not mentioned, assumed it'll remain. reply MathiasPius 16 hours agorootparentprevAs diggan said, there's usually at least one of their dedicated server offerings for which setup fees are waived, so I figure my tests would use the API to find out which one and use it :) reply mikkom 21 hours agoprevI have a dedicated server in Hetzner and I don't understand what this means. How do they decide when the server is \"used\"? Based on CPU allocation? SSH sessions? HTTP traffic? Power? It's easy to shut down virtual servers and continue from the same position. It's not that easy to do the same for servers. The linked page is very unclear about this. reply boredpudding 21 hours agoparentIf you have a dedicated server, you'll just be paying the monthly price. It's always in use. If you decide to stop completely with the server halfway through the month, you'll pay for the hours. Of course, a shut down server that's yours is still yours and you'll be billed for it. reply dmw_ng 20 hours agorootparentI'm not so sure about this, their description specifically calls out one-time-only costs like domain registrations as being the only things excluded. Cautiously optimistic in addition to the recent reappearance of new GPU models being offered to existing customers, Hetzner may be prepping an hourly-billed bare metal GPU product at a great price Hetzner bare metal already deploys in a couple of minutes when renting existing hardware, and there is plenty of precedent for hourly-billed bare metal services around. I think they're intending to move all their bare metal over to hourly billing. reply sdwr 19 hours agorootparentOn the cloud product listing they make it pretty clear that \"in use\" means \"allocated\" and not \"consuming electricity\". Pretty sure their fees are staying the same, and we are still paying for every hour a server exists. reply pretext-1 21 hours agoparentprev„Used“ means as long as you rent them / as long as they’re assigned to your account / as long as you have access. Dedicated servers are physical objects in the real world, contrary to virtual servers they don’t stop „to exist“ just because they’re no longer yours. Previously you could only rent physical servers for an entire month. If you’re doing this the price is still the same. But going forward it will be possible to rent for a portion of the month only (x amount of hours). reply devrand 15 hours agoparentprevIt seems pretty clear to me: > How precisely does Hetzner calculate the hourly billing? > The beginning of the hourly billing starts as soon as the product becomes available to you. So for a dedicated server you start paying once the server has been commissioned to you. You stop paying once you return it. reply c0balt 21 hours agoparentprevIf I understand correctly your cost will stay the same. The bill for march might be a bit different but the total cost should be identical. Their usage will also stay the same-ish with usage = allocation for CPUS, Ram and storage. reply Vvector 19 hours agoparentprev\"If you use a product for the whole month, we will continue to invoice you for the same monthly amount as always.\" reply tiffanyh 19 hours agoparentprev> How do they decide when the server is \"used\"? If you turned on the server, it's being used. reply remram 18 hours agorootparenthttps://docs.hetzner.com/cloud/billing/faq/ > Do you bill servers that are off? > Yes. Until you, the customer, delete your servers, we will bill you for them, regardless of their state. reply lopkeny12ko 19 hours agorootparentprevA server that is simply turned on does not necessarily mean it is \"in use.\" reply corobo 19 hours agorootparentA server that can't be sold to someone else is in use reply michaelmior 19 hours agorootparentprevThis is true, but it still may be how Hetzner decides usage. I'm not sure if this is the case, but it seems pretty reasonable when you're talking about bare metal. reply foofie 16 hours agorootparentprevI think you're getting lost in semantic games. Cloud providers sell you access to their computational resources. When you pay for a server, you're paying for the right to access that server and do what you wish to do with it. Much like when you rent a car, you still pay for it if you keep it parked. reply EVa5I7bHFq9mnYK 18 hours agoparentprevWhatever that means, you will pay more. reply RobAley 18 hours agorootparentNo, you'll max pay the same. May pay less. reply EVa5I7bHFq9mnYK 18 hours agorootparentWonna bet? reply tpetry 17 hours agorootparentThey‘ve stated clearly that you pay the same. Its just calculating stuff differently that you can now remove a dedicated machine mid-month and get the remaining time‘s money back. Which wasnt the case until now. reply EVa5I7bHFq9mnYK 12 hours agorootparentWhy would they do that and lose money? Apparently, they plan to charge more and push you to cloud services, which are more lucrative. reply asyx 1 hour agorootparentThey have been using this business model for years for their VPS products and Hetzner has slowly been moving to unify their products because it's such a disjointed mess right now. Like, it's completely unreasonable to expect to pay more. reply StressedDev 12 hours agorootparentprevThey are doing it to stay competitive. Remember, cloud computing is a highly competitive market and customers can and do switch providers. reply sodality2 12 hours agorootparentprevThis is actually revenue-loss for them as canceling a dedicated server in the middle of the month will no longer result in a full month charge... reply EVa5I7bHFq9mnYK 13 minutes agorootparentExactly. reply jeltz 13 hours agorootparentprevSure, I could bet $1000. reply EVa5I7bHFq9mnYK 13 hours agorootparentBet accepted, term: 1 year. reply BilalBudhani 18 hours agoprevHetzner is one of those \"just take my money\" services, so more power to them. Besides, for some reason I always thought they charge hourly but show pricing in monthly format for easier pricing. reply lannisterstark 11 hours agoparent>just take my money and run MITM services*, yes. https://news.ycombinator.com/item?id=37961166 reply bcye 18 hours agoparentprevThat was only for cloud products until now I think reply reddalo 14 hours agoparentprevWay better than DigitalOcean, I hope they keep being that way reply throwaway2037 8 hours agorootparentCan you share some specific reasons? reply waldrews 11 hours agoparentprevWhat other services fall into the \"just take my money\" category? I fully agree, Hetzner is a good deal. reply lionkor 22 hours agoprevFor anyone else who may be confused - this is already the case in Cloud (obviously), but now will apply for Robot as well. > This structure will be similar to our Cloud billing. It says it here, but I glanced over it :) reply the_common_man 22 hours agoparentWhat is the difference between cloud and robot? reply aaronmdjones 22 hours agorootparentHetzner's cloud console (https://console.hetzner.cloud/) was introduced for their vServer migration (competing with the likes of Linode). Robot (https://robot.your-server.de/) was used for their vServers previously and is still used for their dedicated servers and domain names and such. reply sunbum 22 hours agorootparentprevCloud is VM's Robot is bare metal reply koolba 21 hours agorootparents/‘s/s,/ Otherwise the VM possessive reads like: Cloud = VM’s Robot = bare metal reply MonkeyClub 20 hours agorootparentConsequently cloud = bare metal; which agrees with the axiom that \"it's not a cloud, it's just someone else's computer\". reply randomdata 16 hours agorootparentTraditionally, cloud referred to an abstraction above \"someone else's computer\", keeping the specific details of those computers hidden from the user, allowing the operator to do things like swap out the hardware, move the data to another datacenter, etc. without the end user ever noticing. But tech people love to come up with new definitions for the same terms all the time, so anything goes. reply jeanlucas 21 hours agorootparentprevThank you reply lovegrenoble 21 hours agoprevI've heard that Hetzner is a good provider, so I want to buy a cheap unmanaged VPS server in the USA. But can not find any mention about VPS. Any help, please? Just need to replace this one (the same bad story $$$ as Netlify recently): https://www.leaseweb.com/fr/cloud/virtual-server reply treffer 20 hours agoparentVirtual private server == anything that is not a whole machine, usually referring to small VMs. In hetzner speak you want a \"shared vCPU server\". That's roughly what others offer as VPS, just more clearly spelled out (IMHO). You get fractional CPU usage that should be burstable and roundabout one core. reply lovegrenoble 20 hours agorootparentThank you, finally I understood that. reply Sebb767 21 hours agoparentprevYou're looking for the cloud servers: https://www.hetzner.com/cloud/ reply lovegrenoble 20 hours agorootparentSorry for my silliness, so 'cloud' is a new name for 'VPS'? reply CodesInChaos 20 hours agorootparentVPS are a part of the cloud offering now. Just like EC2 is part of AWS. Once change is that traditionally VPSs used local disk storage, while cloud servers use network block storage. Usually clouds offer additional higher level features, in addition to basic VMs, such as load balancers, object storage and managed containers. Though Hetzner currently offers very little in that regard. reply skrause 17 hours agorootparent> Once change is that traditionally VPSs used local disk storage, while cloud servers use network block storage. Hetzner Cloud uses local NVMe SSD storage. They used to have Ceph based network block storage, but deprecated that around 2 years ago. reply FooBarWidget 18 hours agorootparentprevThis rename has been a thing for 10 years now. All the VPS providers jumped on the cloud bandwagon and call themselves cloud. reply mglz 21 hours agoparentprevManaged virtual servers are available here: https://www.hetzner.com/managed-server/ reply dakiol 20 hours agoparentprevBe ready to handover a copy of your national ID/passport reply fortunateregard 15 hours agorootparentI've had a similar experience where my account was banned despite providing all the info they requested (including my passport). Hetzner was the only provider where I was banned despite providing everything I was asked for. I've deployed services on plenty of providers (AWS, GCP, Digital Ocean, Vultr, etc) without issue. Apparently, this has been an on-going struggle for them. A few weeks ago they were even called out on an episode of the syntaxFM podcast for this behavior after one of the hosts had his account banned [0]. It's still a bit of a mystery to me honestly. Is their fraud detection so poor where they're forced to ban new sign-ups in this manner? How can Hetzner's competition tell that these accounts are legitimate (without ridiculous requirements such as passport) and Hetzner can't (even when provided with full home address, passport, etc)? I'm assuming Hetzner can't because they've seemingly developed a reputation for banning legitimate developers and the only reason I can think of where they'd be fine with that is if they actually have a very difficult time telling whether an account is legitimate or not. [0]: https://twitter.com/stolinski/status/1750226126499139665 reply jamesblonde 13 hours agorootparentThey are like Ryanair and Lidl brands - you get a reasonable product super-cheap. Don't argue about the terms and conditions, just be happy to do business with us. If you don't buy, somebody else after you will. This is not classic VC-funded IT. It's mass-market, low margin, low risk consumer products. It's not Enterprise IT. reply crotchfire 34 minutes agorootparentIt's mass-market, low margin, low risk consumer products None of those have anything to do with demanding that customers expose themselves to identity theft risk. reply e145bc455f1 16 hours agorootparentprevThey didn't even ask me for that, straight up banned my account. reply alwayslikethis 20 hours agorootparentprevCan you elaborate? I have a VPS and a storage box on Hetzner and I'm so far satisfied with it, minus the sometimes confusing interface between their different products. reply MikusR 19 hours agorootparentBecause they are so cheap, they are often used by spammers or other shady entities. So occasionally they may ask for ID. For example, if you connect from one country, use an address from another country and use prepaid card from yet another country. reply crotchfire 34 minutes agorootparentused by spammers Then why don't they just ask for ID if you want port 25 unblocked? Doesn't add up. reply formerly_proven 19 hours agorootparentprevnext [5 more] [flagged] hypefi 16 hours agorootparentThis is what happened to me basically, I was so happy I found a cheap provider, I got locked out I didn't even know what to do to unban my account. I will keep using my old provider ... reply thijsvandien 17 hours agorootparentprevI'm impressed other people are actually given options. reply cbg0 18 hours agorootparentprevWhere exactly can someone else buy a car with a copy of your ID? reply toast0 18 hours agorootparentA car dealership? My credit is great (and currently locked), and is probably enough to get a car with a \"$0 down\" offer, although those often do require some payments at time of purchase. If you can answer the credit application questions and present ID with my name that looks good, you've created a big mess for me. reply jeltz 13 hours agorootparentprevThey have never asked me for any of my three accounts. And one I paid with credit card the two other with bank wire. reply edm0nd 21 hours agoparentprevJust use DigitalOcean. Easy setup and very cheap VPS options. reply MikusR 20 hours agorootparentDigitalOcean is at least 2-3 times more expensive with 10-3 times less bandwidth reply AnonHP 14 hours agorootparentprevDigitalOcean, even before it had huge layoffs, blocked my new account and wouldn’t allow me to pay. There was absolutely no support whatsoever for that. Just a template email and no other way to escalate or write to a human. I can only imagine how much worse it would be now with reduced staff. reply lovegrenoble 20 hours agorootparentprevThank you, Sir. reply stefs 20 hours agorootparentthat said, while digital ocean is great, as far as i know hetzner is somewhat cheaper. compare! at digital ocean, 12$: 2GB RAM, 1vCPU, 20GB SSD, 2TB transfer at hetzner, 10,50$: 8GB RAM, 2vCPU, 80GB SSD, 20TB transfer reply Citizen_Lame 20 hours agorootparentYes, but Hetzner costumer service is from another planet. So in case of issues they won't be able to help, and you will have issues. reply remram 18 hours agorootparentDigitalOcean has a long history of kicking out customers with no warning and no explanation. https://news.ycombinator.com/item?id=11036554 https://news.ycombinator.com/item?id=20064169 https://news.ycombinator.com/item?id=29596138 https://news.ycombinator.com/item?id=29600320 https://news.ycombinator.com/item?id=38880713 https://news.ycombinator.com/item?id=39127683 reply radiator 20 hours agorootparentprevAnd in case of no issues, you save quite a lot of money. reply icetngugzer 15 hours agorootparentprevSimilar to the other commenters I much disagree - \"but this ofc. rly depends on what you expect from them...\" If the thing has power, network, working hardware and is able to \"boot into there rescue-system\" that is all they care about. And for that support has been stellar including preventive disk swaps, moving disk into new chassis and such with feedback and action within minutes basically 24/7. But the software end is ofc. entirely on you, down to the BIOS where they roll you a network-KVM-Cart to the box for some hours after requesting it with a click. They wont fix your HTML errors, nor your php syntax errors, or explain how to setup SSL on your apache, nor will they help when you dont know how to operate ssh/putty ;) and if \"with such an issue\" one bothers the 24/7-datacenter guys (and not just \"8-16/5 normal customer support\") I may even understand that it seems a little rude ;-) reply kosolam 19 hours agorootparentprevHetzner has good support. reply huggingmouth 19 hours agorootparentI sttongly disagree based in my interaction with them. They were very rude. That said, their services are reasonably priced and they have been competent throughout my time with them. Hetzner would be far more appealing if they ever decide to muzzle their rebid support team. reply rglullis 18 hours agorootparent> They were very rude. No, they were German. Knowledgeable and efficient, but they won't coddle you or pretend that their life depend on your miser account. If you have a problem that can not be solved by their standard process, they will rather drop you than try to accommodate you. reply huggingmouth 4 hours agorootparentNo, I'm afraid you're way off. Their response was far from efficient. I reported a fault in their interface and they wrote a whole peragraph demeaning me in a passive aggressive manner. The workaround (to their subpar system) was a single line at the end. Very opposite from German efficiency if you ask me. That said, I did switch newer deployments to another cloud provider who is more professional, so it really does seem that they do not care about my measly account :-) Cheers! reply rnewme 17 hours agorootparentprevThis was my experience too, both with my tiny personal account and my giant clients accounts reply asmor 17 hours agorootparentprevTheir abuse team is rude. Their technical support is usually professional, and the one time I talked to their legal team they were superb. reply reddalo 14 hours agorootparentThey're Germans, after all reply Radim 20 hours agorootparentprevCare to share why? I've been a Hetzner customer for nearly 10 years now. I just checked – first invoice in June 2014. And during that period, Hetzner's support – which I needed maybe 3 times since things generally just work – has always been stellar. I've used Linode too, and their support was comparable (=also great). But Hetzner wins on pricing, hands down. I actually appreciate Hetzner hasn't join the BS enshittification gimmick train (yet). If that's your complaint. reply pointlessone 22 hours agoprev> We will always use the hourly price when it saves you money, meaning when you have used a product for less than a month, and the total hourly price is less than the monthly price. That’s nice, I guess. > we have decided to no longer send invoices to all of our customers on the same day. Instead, we will spread them out throughout the month. You will always receive your invoice on the same day of the month. But this day will be different from customer to customer. It’s not clear to me when the billing moth starts. Is it the day the invoice is sent or is it a regular calendar month? Either way it might be confusing. reply flexd 21 hours agoparentThe email I got said this: >What is important after the changeover? • We want to be able to offer you even better customer service in the future. For that reason, we want to change the date of your invoice. This will make it easier for us to quickly answer questions about your invoices. Starting in April, your new permanent billing day will be the 12. of the month. I think they have just spread the billing day out across the month for all customers reply nilsb 22 hours agoparentprevI'd hope that it's based on calendar months. Wouldn't invoices that span multiple years make things a lot more complicated in terms of accounting in general? reply everfrustrated 22 hours agorootparentAccording to the doc everyone will be getting a new random billing date to help rebalance customer support load over the month. reply throwbadubadu 22 hours agoparentprevYou certainly will read that in the invoice? reply voytec 22 hours agoprevIf monthly price won't increase than this appears to be a good change. But their dedicated servers have an \"installation fee\", so it won't be cost-effective to rent them for short periods, and VMs are billed on hourly basis already, to my knowledge. Not sure which products this change covers. reply aaronmdjones 22 hours agoparentNot all of their dedicated servers have a setup fee; for example, the lowest spec AX model does not. https://www.hetzner.com/dedicated-rootserver/matrix-ax/ reply trollied 20 hours agorootparentServer auctions don't have a setup fee either. https://www.hetzner.com/sb/ reply boplicity 19 hours agoprevI don't seem capable of actually setting up an account with Heztner and logging in. Their security is so tight, it seems to constantly lock me out. What if I were an actual customer? Yikes. Does anyone else have this issue? reply david_allison 19 hours agoparentYep: Dear Mr David Allison After reviewing your updated customer information, we have decided to deactivate your account because of some concerns we have regarding this information. Therefore, we have cancelled all your existing products and orders with us. Best regards Your Hetzner Online Team reply thijsvandien 17 hours agorootparentLiterally this. Their staff won't provide any details, nor even a route towards a solution. When I asked what they suggest doing in this situation, they simply stopped responding. reply sofixa 7 hours agorootparentOf course they don't. If they tell you why they have banned you, if you were a scammer, you'd know how they caught you and what to do better next time. That's why none of the fraud prevention systems, be they at a tech company or a bank, tell you exactly why you've been banned/denied. reply slig 18 hours agorootparentprevThat's terrifying. Did they at least give you time to backup your data? reply amar0c 17 hours agorootparentPlease note that Hetzner does this only in couple cases: a) fake account data ; b) previous strikes (unpaid invoices, abuse etc..) with them ; c) in some cases customer is from country they do not do business with. I bet gazzilion OP above is within a or b reply Joe_Cool 15 hours agorootparentTheir ToS also classifies Crypto-Mining, farming or plotting (whatever that is) as grounds for cancellation. Also everything forbidden by German law results in a cancellation. So if you would have for example posted something before 2017 about a state leader that might seem like an insult your server might be gone. ( https://www.loc.gov/item/global-legal-monitor/2017-07-26/ger... ) https://cdn.hetzner.com/assets/Uploads/downloads/AGB-en.pdf Section 8.3 reply creatonez 4 hours agorootparentMake sure you avoid p2p stuff that scans for other servers on the hetzner network, too, even if it's legitimate and not infringing on copyright. They won't ban you right away, but their systems will detect it and give you 24h to confirm you're not infected with malware before shutting down the server. For IPFS or Bittorrent, you should be able to use it for legitimate purposes after blocking the local IPs. reply e145bc455f1 16 hours agorootparentprevI got the same thing. My account was neither fake or had any previous strikes, it was a new account. reply thijsvandien 16 hours agorootparentprevA) No B) No C) No reply Etherlord87 15 hours agorootparentprevd) the customer is black, romanian or a jew I'm awaiting donwvotes, but honestly, if they don't give you a reason, everything can be one, and refusing a service on arbitrary grounds seems illegal to me :D reply reddalo 14 hours agorootparentIt's a private service; private companies can choose whoever they want to do business with. They're not a public service. reply Etherlord87 12 hours agorootparentAre you sure about that? I know of a precedent in Poland, where a company refused to service (print some kind of invitation cards) some LGBT people, and was punished for it. I imagine the laws in Germany are similar to the laws in Poland in that regard, both countries being in the European Union. In USA, I really doubt any company could refuse to serve black people. Or women. I wonder if the downvoters of my previous message share your (I'd say quite wrong) opinion or there's another reason for the downvotes - I'm up for a discussion! reply pinkgolem 4 hours agorootparentIn Germany you are allowed to discriminate on all not protected factors. protected factors are: race or ethnic background, gender, religion or belief, disability, age, or sexual orientation. belief has a very high bar to meet, you are for example still allowed to discriminate against members of a political party. discriminating based on location matching ip adress or not doing business with certain countrys is fully legal reply reddalo 11 hours agorootparentprevI don't know about the downvotes, and I'm not a lawyer so take this with a grain of salt, but there's a difference between a service to the public (e.g. a café or a restaurant) and a service such as Hetzner. reply pinkgolem 4 hours agorootparentNo there is not, there is a difference between government services and private businesses but all private businesses play by the same rules reply david_allison 15 hours agorootparentprevI was evaluating them at the time. No data loss reply dotancohen 6 hours agorootparentMaybe you personally had no data loss, but I believe that the question was the more general \"could other users in this situation expect to be able to recover their data?\". reply MyFirstSass 17 hours agorootparentprevWhat? That sounds extremely unacceptable. Can we get some context please, there must be a reason. Were you able to backup first? reply david_allison 15 hours agorootparentI created an account to evaluate them on Sun, 18 Jun 2023, 13:25 UTC. I verified my account on Sun, 18 Jun 2023, 13:29. I received the above email entitled 'Rejection of Your account XXXXX' on Mon, 19 Jun 2023, 09:15. I didn't follow up further. No backups required as I hadn't purchased anything reply thenickdude 13 hours agoparentprevYup, I bought a dedicated server because I needed a CPU with AVX2 and VT-x, and it went poof some days later. They didn't even do me the courtesy of emailing me to tell me about it, I had to ask support: > > When I try to log in to my account it tells me that my credentials are invalid, and when I use forgot password it tells me that my account is disabled? >We recently did some routine reviews of our customer accounts. We noticed some suspicious information in your account. >We have some concerns regarding this information and we have decided to close your account. >We do not share details about why certain accounts appear suspicious. Publishing this information would make it easier for people to create fake accounts and abuse our services. I even went through their live-video face verification and passport scan and they didn't budge on this. reply 15457345234 2 hours agorootparentWere you an 'uncomplicated' customer i.e. from a country not associated with tons of online fraud, connecting plainly from a 'normal' ISP account (not a VPN, not using a browser cranked to the gills with privacy plugins) and using a conventional payment method registered to you at your address in the same country you were connecting from etc? reply kobalsky 19 hours agoparentprevI had the inverse experience. I setup 5 x $220/month servers and I wanted to make sure my payment method worked, contacted support a couple of times, wanted to make a prepayment or something. No dice, they didn't want to take my money until the billing period closed, luckily everything worked fine. It wasn't their cloud offering though, it was for the dedicated servers (Hetzner Robot), I don't know if that makes a difference. reply EVa5I7bHFq9mnYK 18 hours agorootparentI didn't ask to \"take my money\", just gave it to them forcibly by wire transfer, it showed up in my account all right. reply severino 20 hours agoprevIt would be nice if cloud offerings like Hetzner would allow one to set the contract duration and pay in advance, for example, one year. And once that year passes, you could tell them to extend it for another year, or do nothing and get your server deleted. This way you have a control over your spending and avoid surprises. I guess some providers (OVH?) allow you to do that, but of course it requires that all costs be fixed and have no \"extras\" for bandwidth consumption or things like that. reply groestl 20 hours agoparent> and avoid surprises I think for most people and organizations, auto-deletion of a server _is_ the surprise ;) reply V__ 20 hours agoparentprevYou can buy credits using a bank transfer [1]. You can also set E-Mail alerts in case your monthly invoice is higher then expected (more traffic etc.) [2]. However, there is no auto-delete as far as I can tell. [1] https://docs.hetzner.com/accounts-panel/accounts/payment-faq... [2] https://docs.hetzner.com/cloud/billing/faq/#how-do-i-keep-my... reply AnonHP 14 hours agorootparentI’ve been waiting for Hetzner to allow adding credits into one’s account using credit cards or debit cards (or other modes like PayPal). Being outside Germany (and the EU), a bank transfer is not possible or is difficult and prohibitively expensive. Hetzner also didn’t seem to support credits across all its services when I checked a few years ago. reply wyan 12 hours agorootparentYou can use a bank like Wise which lets you have accounts in many currencies for such bank transfers. Fees between their accounts are quite competitive (I used them many times in the past). reply severino 17 hours agorootparentprevDidn't know you could buy credits, but well, for me the key is that no debt is generated when your credit goes to zero. In this case, if you exhaust your credit but your server keeps running, Hetzner will not stop it and will send you the bill next month. Yes, you may have received an email warning but it does not give the same peace of mind. reply Saris 14 hours agoparentprevAs far as I known there are no surprises on Hetzner, they're a solid company, not one of the shady ones that bill on usage without limits. reply severino 12 hours agorootparentI agree it's not as bad as other companies (like the other day Netlify story) but Hetzner doesn't allow you to set limits either, so in theory you can still be charged without knowing how much it'll be in advance. You can also leave something on while thinking it's off and, yes, it's your fault too, but it wouldn't happen if it just consumed your credits and no more. reply iforgotpassword 19 hours agoparentprevAt least with the dedicated servers there are no surprises. Monthly cost is fixed, if you exceed the bandwidth limit you'll be throttled. Maybe their cloud offerings offer the same? reply severino 12 hours agorootparentFrom what I can see in their webpage, they charge something like 1 eur for every extra TB but apparently there's no way to set a limit (so you can be protected in the event of a DDoS), only an alert. reply janus24 20 hours agoparentprevScalingo (a European PaaS) allows you to do this by buying credit, a simple solution. I wonder why other providers don't offer this. reply tiffanyh 19 hours agoprevWill this cost more for existing customers? What they should have made more clear is, for existing customers - will this be cost neutral or cost more. Ideally, this is cost neutral for existing customer and gives the flexibility of paying partial month for partial usage. reply kobalsky 19 hours agoparentthe post says: \"Just to make it clear, the montly price you will pay will NOT change, even if you see some short-term changes in monthly totals on your invoices during the transition period.\" reply naiv 18 hours agorootparentThis is for existing customers. If they offer servers for an hourly rate I would expect prices to up with a priced in setup fee reply antman 6 hours agoprevNice but illegal in some European countries. If you have a cost with a fixed deal (aka bare metal server per month) law has it that its upfront. If its a monthly cost it should be invoiced be the first 5 days of the following month. I guess I can tell the IRS that Hetzner has a problem with their cron jobs or with the uniform distribution of their accounting people workload. reply CamelCaseName 17 hours agoprevI've only ever heard fantastic things about Hetzner. I'm shocked PE hasn't bought them yet and raised prices. reply thijsvandien 17 hours agoparentSo had I, until I actually tried to sign up. Have a look at the way they treat you when they have (unfounded) suspicions. It's beyond ridiculous. reply rsync 16 hours agorootparentSo many problems just disappear when your prices are (relatively) expensive… reply AnonHP 14 hours agorootparentMaybe. But price is still one of the larger concerns for many people. When I checked this a couple of years ago, it seemed like Hetzner’s Storage Box (the one sorta competes with your rsync.net) had a much lower price, a much larger capacity per dollar/euro and more storage tiers. reply AnonHP 14 hours agorootparentprevThey ask for a government ID in some cases. I was able to provide one with some details masked, and they wanted the name on my account to match the name on the government ID. This is a commercial transaction, not Facebook. So I was kinda ok with that. But it is weird and ridiculous. reply thijsvandien 14 hours agorootparentWhether to provide proof is at least a choice you get to make, but in my case even that wasn't on the (empty) table. reply cyberpunk 17 hours agoparentprevMeh, ymmv. I migrated a bunch of stuff from OVH->Hetzner a few years ago and the support experience was absolutely, insanely, beyond-belief bad. P1 call having to explain \"what's an ip address\" to their 'engineers'. About 6 hours after we moved prod over, they brought down my EIPs for like 4 hours because of a route they didn't understand and flagged as spam, and in the end we rolled back the migration and cancelled the contracts. Bummer for them; we ended up scaling out to several hundred thousand euro a year in OVH, which has been absolutely fucking exemplar. So while some people really do rave about them, I was so annoyed by that experience that I will almost certainly never use them again. Being a German myself, I'm 90% sure it's because the support is based in Germany, where technology goes to die. reply pinkgolem 16 hours agorootparentWhat are EIPs? If you are referring to the eth stuff, everything to do with crypto is banned on hetzner reply steezeburger 15 hours agorootparentI assume they're talking about elastic IPs and not Ethereum Improvement Proposals reply gowthamgts12 15 hours agorootparentprevElastic IPs i believe - you can attach/detach an IP to a server instead of having a new IP everytime reply ayi 15 hours agoprevIf we can get over the fact that they ban accounts out of nowhere, that can be good news. But not for me. Because I couldn't even sign up. reply pquki4 15 hours agoparentSame here. I provided an authentic US drivers license for their verification (which I normally wouldn't do) yet that didn't pass the verification, no reason given. Well, if they don't my money, fine, I'll do it elsewhere. reply DonnyV 18 hours agoprevDoes anyone know if Hetzner is looking to open up a datacenter in Australia or in Southeast Asia? reply rmbyrro 17 hours agoparentSouth America would be great as well reply diggan 17 hours agorootparentSouth Africa sounds like a good compromise between Australia and South America :) Wonder what's the shortest possible latency between those points? Alternative, more interesting question: Assuming Hetzner have a data center in Germany and one in Finland, where should their next data center be in order to reduce latency as much as possible, in the most countries? reply Tomte 15 hours agorootparentThey also have one in the US. So probably East Asia? reply diggan 13 minutes agorootparentThat's just for cloud I think, and also not their own data centers. reply sixhobbits 19 hours agoprevThis overall sounds like it makes sense and brings their billing into line with other providers. But it's super confusingly written and presented. Why is this an essay with CAPITAL LETTERS to try to simplify things. It looks like they put in a ton of effort to make sure people aren't confused, but this doesn't seem like the thing to give people to ensure that they aren't confused. reply iforgotpassword 19 hours agoparentHuh, to me it feels they just tried to emphasize the most important things, like your bill never being higher than the monthly price would be. Like, the opposite of what tech companies usually do. reply willcodeforfoo 22 hours agoprevLast time I checked, Hetzner doesn’t have any dedicated offerings in the US, only cloud, is this still the case? reply isodev 20 hours agoparentWhat does it mean to have a \"dedicated\" offering for the US? Hetzner lets you create servers in their US datacenter, isn't this enough? reply bravetraveler 20 hours agorootparentDedicated servers, the things we used before cloud or vps reply CoolCold 21 hours agoparentprevStill the case, right reply shortlived 21 hours agorootparentAny recommendation for a provider with dedicated servers in the US? I’m with a provider now who is phasing them out. reply sethhochberg 14 hours agorootparentAt a prior company I'd occasionally lease dedicated servers from INAP (now calling that part of their business HorizonIQ after bankruptcy and reorg) - https://www.horizoniq.com/services/compute/bare-metal/ Their business was always a bit chaotic but the technical side of the organization was competent. We were colo'd in one of their datacenters so it was nice to be able to rent additional capacity in the same facility. Servers were manually provisioned, but manageable as you'd expect via online portal after provisioning was complete. So... not a glowing recommendation I guess, given their corporate instability? But a recommendation nonetheless, the corporate instability never impacted our technical operations and the product was good. reply TheNorthman 20 hours agorootparentprevOVH have some in the US. I've only had good experiences with them. I like Server Hunter to get a general overview: https://www.serverhunter.com/#query=product_type%3Adedicated... reply CoolCold 20 hours agorootparentprevNo suggestions from my own experience. Probably Leaseweb or OVH worth checking. reply manishsharan 20 hours agorootparentprevThere are several dedicated server providers in US but not much is known about their track review. OVH has a datacenter in Toronto which may close enough to the US for many people. They provide dedicated servers. reply heffer 19 hours agorootparentOVH's Toronto datacentre is not operational yet. But you can pre-order. It's going to be interesting to see what this does to pricing for the Toronto hosting and compute market, as that location has always been more expensive compared to other North American locations. Right now it looks like OVH will not be offering their lower tier offers out of the Toronto DC. reply betaby 18 hours agorootparentprevOVH operates data-center close to Montreal (BHS), 8ms latency to NYC. Also it's 100% green hydro electricity powered. reply StressedDev 11 hours agorootparentHydro power is not green. The benefit are it's cheap (in terms of cost to produce) and produces no CO2 emissions. The downside is it makes it harder to fish to travel from the ocean to their spawning sites. Like all power sources, it has benefits and drawbacks and it absolutely impacts the environment. reply DonnyV 17 hours agoprevWhy isn't their US Data Centers not listed in the table of Data Centers they run? https://docs.hetzner.com/general/others/data-centers-and-con... reply diggan 17 hours agoparentMaybe they don't operate those data centers themselves? Outsourced perhaps. > Which data centers does Hetzner operate? reply ffsm8 17 hours agorootparentYes, they state as much when you order anything and talked about it at length when they introduced the US offering. They're shipping their specialized server systems to select providers for housing, essentially. Most maintenance is done remotely... aside from replacing faulty hardware etc reply JonathonW 17 hours agorootparentprevHetzner's US cloud zones are colocated in someone else's facility (AFAICT they don't specify where): https://docs.hetzner.com/cloud/general/locations/#are-the-se... reply nottorp 21 hours agoprevSo basically some large customers wanted to rent physical servers by the hour so now everyone gets to rent physical servers by the hour? reply yaseer 17 hours agoprevIf anybody has migrated from AWS to Hetzner what was your experience? What kind of cost savings did you see? I know it's not a like-for-like comparison, I am particularly curious about the price differentials though, AWS is often a premium. reply diggan 17 hours agoparentI helped some smaller companies move from AWS EC2 to Hetzner dedicated servers. One example: Biggest cost saving was in the bandwidth bills. They also realized they don't really need to be able to scale up/down in minutes or having 6 instances online, but having two beefy machines lead to better performance, less latency and cheaper monthly bills. Originally they just went with AWS because the developer who did infrastructure stuff was most (only?) used to it and had some certification or similar, without really thinking about why AWS. Reached out to me when they started to wonder why things were so expensive for what they were doing. reply roschdal 21 hours agoprevHetzner ist best, kein protest. reply Citizen_Lame 20 hours agoparentLol. For kebab production, maybe. reply unixhero 18 hours agoprevI just started a hosting gig myself. I got tired of the costs. reply degun 13 hours agoprevWhat a beautiful company. reply ta8903 20 hours agoprev [–] When are they allowing sign ups for third world countries? reply gowthamgts12 15 hours agoparentI signed up after lot of failed attempts. I think now they have fixed their verification process. Please give it a try reply tomas789 18 hours agoparentprevThere is no such thing as third world country anymore. That is a relic or yesteryear. [1] [1] https://youtu.be/hVimVzgtD6w?si=g0aCYsjo15RAyx-e reply slig 18 hours agoparentprevYes, signed up from a 3rd World Country late last year. reply thijsvandien 17 hours agoparentprev [–] Or any country for that matter? I was rejected from right next door (NL). reply reddalo 14 hours agorootparent [–] It's very strange that they rejected your account from another EU country. Are you sure you created your account with real data? reply thijsvandien 13 hours agorootparent [–] One doesn't accidentally make things up, and for invoices to be booked, my accountant wouldn't have it any other way. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Hetzner will switch from monthly to hourly billing for most products starting in March 2024, aiming to enhance user-friendliness.",
      "Invoices during the March and April transition period may fluctuate due to the hourly billing system, reflecting the previous month's product usage.",
      "Automated payments are an option, and the billing date might be adjusted to improve customer service; however, domains, SSL certificates, and licenses will maintain their annual fee structure."
    ],
    "commentSummary": [
      "Hetzner is shifting to hourly billing for testing and scaling, with possible waivers for developers and questions about setup fees for dedicated servers.",
      "Users could incur charges upon server setup until return, while the company mulls over introducing bare metal GPU products.",
      "Discussions involve mixed user feedback on Hetzner's services, encompassing account bans, support quality, and setup challenges, alongside considerations of data center expansions and comparisons with providers like AWS and OVH in terms of cost and performance."
    ],
    "points": 305,
    "commentCount": 181,
    "retryCount": 0,
    "time": 1709201897
  },
  {
    "id": 39548410,
    "title": "Serving Blog Posts as Linux Manual Pages",
    "originLink": "https://jamesg.blog/2024/02/29/linux-manual-pages/",
    "originBody": "Serving my blog posts as Linux manual pages Published on February 29, 2024 under the Coding category. Toggle Memex mode Intended audience: You are likely to enjoy this post the most if you are interested in Linux and/or Linux manual pages, or if you enjoy reading about esoteric programming projects. Linux computers come with pre-installed manual pages that describe how to use specific commands. These pages are readable by typing maninto your terminal. For example, you can get the manual for the tac command, which prints out a file in from bottom-to-top by using the command man tac. Some command line software you install adds manual pages, too. Linux manual pages are formatted using the roff syntax, which you can use to mark up documents. roff was the first typesetting command line software for Unix, developed at Bell Labs. Earlier this week, with a spark for building but no particular idea in mind, I started to think about the Linux manual page. Could I serve my blog posts as Linux manual pages? Herein lay an adventure. TL;DR: You can request a Linux manual page version of a blog post with the following HTTP request: curl -sL -H \"Accept: text/roff\" https://jamesg.blog/2024/02/28/programming-projects/ > post.page && man ./post.page Bash Copy Devising the system: Content negotiation I had an idea for how I wanted this to work in mind. I wanted a user to be able to request a roff version of a blog post using content negotiation, part of HTTP that lets you specify ixn what format you want a file. For example, you could request an image with an Accept: image/png. This tells a server that, if possible, it should send a PNG file. There are lots of intricacies to content negotiation. You can provide a list of types of content you can accept, and the order in which a server should try to return them But! That's a rabbit hole for another day. What's important here is that an application can ask a server for content in a specific format using a HTTP header. With content negotiation, I can route requests if a user sends an Accept header. If a user asks for an text/roff document, I could return a manual page that can be opened with the man command. Writing the manual pages Manual pages use the roffsyntax, so I would need to have versions of my blog posts in that format. To do this, I updated my site to generate man pages for each blog post. The template I used to generate the manual page was as follows: .TH jamesg.blog 1 \"\" \"jamesg.blog\" .SH TITLE ... .SH AUTHOR James' Coffee Blog (https://jamesg.blog) .SH PUBLISHED ... .SH POST ... .SH URL ... Plain text Copy Here, I set a header with my domain name and create five sections: title, author, published date, the post content, and the URL of the post. The raw content is markdown. This doesn't always turn out well in a manual as spacing can sometimes be off. But, markdown was more readable than HTML and resulted in less information loss (i.e. titles having no distinction to paragraphs other than being on their own line) than using plain text. I now had: Properly formatted manual pages, and; The knowledge that content negotiation could allow someone to request a manual page. Now came the final piece of the puzzle: using content negotiation to facilitate the request for manual pages. Requesting a manual page You can use the following command to request the roff format of blog posts on this website: curl -sL -H \"Accept: text/roff\" https://jamesg.blog/2024/02/28/programming-projects/ > post.page Bash Copy You can then open the result as a Linux manual page: man ./post.page Bash Copy Let's talk about how this works! When a browser makes a request to https://jamesg.blog/2024/02/19/personal-website-ideas/, it asks for the HTML version of the page. In the curl command above, the command asks for the text/roff version. I added a few lines of text in my NGINX configuration to change how the server responds when text/roff is requested for a blog post. First, I declared a few variables in my /etc/nginx/nginx.conf file that let me raise a flag when a specific content type was identified: map $uri $redirect_suffix { ~^/(.*)/$ $1; default \"\"; } map $http_accept $redirect_location { default \"\"; \"~^text/roff\" 1; } nginx Copy You can add multiple different redirect locations, but I only need two: the default, and my custom text/roff rule. In my site NGINX configuration (the file in the /etc/nginx/sites-enabled folder), I used the following code to handle requests differently if a roff page is requested: server { ... location / { if ($redirect_location = 1) { rewrite ^/(.*)/$ /$1.man last; } ... } } nginx Copy Here, I say: take a URL, and add .man to the end, removing the trailing slash, as long as the Accept: text/roff header is set. This tells NGINX to read from the .man file instead of the index.html file associated with each post on my site. That is to say you can now read blog posts on this website as a Linux manual page. This was a fun investigation into using content negotiation in NGINX and a reminder of how far we have come with typesetting technology from the command line interfaces to modern-day typesetting software and HTML. Thank you to Todd for providing guidance on setting up my NGINX configuration. Todd's help was sincerely appreciated! Responses Comment on this post Respond to this post by sending a Webmention. Have a comment? Email me at readers@jamesg.blog.",
    "commentLink": "https://news.ycombinator.com/item?id=39548410",
    "commentBody": "Serving my blog posts as Linux manual pages (jamesg.blog)296 points by zerojames 17 hours agohidepastfavorite126 comments yegle 15 hours agoIt would be cool to provide a deb repo as a way of subscribing to your blog. So `apt update` will pull in all blog posts, `man your-blog` will show the latest post with links to the index of all your other posts. reply dcminter 13 hours agoparentOn the one hand this idea is brilliant - on the other hand if it caught on, there are some obvious opportunities for malware intrinsic to the approach :'( I think I'd be too chicken to subscribe. reply codetrotter 10 hours agorootparentMake the CI pipeline create a Docker image that contains Debian slim and the man pages installed. reply rezonant 10 hours agorootparentOn the other hand, it's Docker, so you know every blog will use their own base image. reply astrea 5 hours agorootparentGuys: Serve your blog via Docker itself. Docker image names are _already_ URLs. You could literally just have people do `docker pull myblog.com/recipes:latest` or `myblog.com/recipes:lasagna`. Edit: and navigate like so: `docker image myblog.com/recipes` to list all recipes. God I love APIs. reply mrmattyboy 4 hours agorootparentAnd add search logic to the tag, so you don't need to know the name of the post, just the right word(s) to find it and let the \"I'm feeling lucky\" algorithm pick the right one :) reply usr1106 2 hours agorootparentprevDocker is the example implementation of \"containers do not contain\". At least I would use podman. reply westurner 9 hours agorootparentprevHow does HTTP(S) Same Origin policy work with local file:/// URLs? TIL there's no `ls -al /usr/share/man/**man --html`; though it would be easy to build one with Python's http.server, or bottlepy, or bash,. Prompt: An http server ( with bash and /dev/tcp/ ) that serves HTML versions of local manpages with linkification of URLs and references to other manpages reply awithrow 5 hours agorootparent> How does HTTP(S) Same Origin policy work with local file:/// URLs? It doesn't since file:/// is just a uri and not part of the http protocol reply jwilk 2 hours agorootparenthttps://developer.mozilla.org/en-US/docs/Web/Security/Same-o... reply westurner 3 hours agorootparentprevIf I open an HTML copy of a manpage that contains JS from a file:/// URL, can it read other local files and send them to another server with img URLs? If so, Isn't it thus probably better to run an HTTP server over a permissioned socket than to serve static HTML [manpages] from file URLs [in a [DEB] package]? reply dredmorbius 6 hours agoparentprevThere's some precedent for this. Debian used to provide access to the now-defunct Linux Gazette, and still offers numerous informational packages (package docs, manpages, info pages, RFCs, Linux HOWTOs, and more), all of which can be served locally through the dww package, \"Read all on-line documentation with a WWW browser\":(Joerg Jaspert was the former maintainer of Linux Gazette packages:(2002).) This remains to me one of the best examples of integrating information delivery and documentation on an operating system I've ever encountered. Notably, it makes both manual and info documents more useful and usable than their traditional terminal-based interfaces. There are Debian-associated blogs (Debian Planet), though I don't believe that was ever packaged for Debian itself. Frankly, RSS is probably a better option for blog subscription. reply anthk 20 minutes agorootparentYou could read LG without dwww just fine by just pointing the file:/// path. I remember getting all of those with the Debian Sarge DVD's, good stuff, among the Anarchist Faq, the 'Derivations' math book, and tons offline documentation when I didn't have internet at home. reply zerojames 15 hours agoparentprev_adds to their TODO list :D_ reply tutfbhuf 14 hours agorootparentPlease also create an AUR package for Arch Linux. reply basilgohar 14 hours agorootparentRelease as a Flatpak and make it distro agnostic! reply gkbrk 14 hours agorootparentMake it even better and release an AppImage! reply edvinbesic 14 hours agorootparentEven even better, put it on the web so it’s accessible from anywhere reply abulman 13 hours agorootparentwow, there. Lets not get crazy there. It's plenty enough to support multiple package managers, but what you are considering is just too much! reply hk1337 11 hours agorootparentOkay, just create an iOS application. reply zerojames 12 hours agoparentprevThis is now in progress. https://github.com/capjamesg/jamesg.blog.deb has all you need to build a man-page-only deb file using: git clone https://github.com/capjamesg/jamesg.blog.deb cd jamesg.blog.deb dpkg-deb --build --root-owner-group jamesg.blog sudo dpkg -i jamesg.blog.deb You should see: ... Processing triggers for man-db (2.9.1-1) ... Which indicates the man page for `man jamesg.blog` is available. There is just a placeholder in there for now. I will perhaps finish this tomorrow! NB: This may become a blog post soon :D reply out_of_memory 14 hours agoparentprevfirst thing came to my mind when i read the blog. it would be a very cool thing. i hope this becomes a thing. reply potta_coffee 11 hours agoparentprevRelease your blog as an Electron application please. reply usr1106 2 hours agorootparentHow would that fit on a RK07 disk? reply Gormo 14 hours agoprevThere's no need to fork or use an intermediate file; you can just pipe straight into `man`: `curl -sL -H \"Accept: text/roff\" https://jamesg.blog/2024/02/28/programming-projects/man -l -` reply godelski 11 hours agoparentPlease don't... Actually yrro posted something similar 2hrs before you and now we have the whole {curl,wget} pipe into command discussion again... Friends don't let friends pipe streams into commands https://news.ycombinator.com/item?id=39554044 reply rezonant 10 hours agorootparentThe top reply to that comment rightfully points out that this applies to your shell specifically. This isn't a concern for formats that don't have executable code (and I assume roff doesn't, right?) You might say, yes, but it's still a good idea to review the file before you open it. OK sure, but that isn't going to work for binary files anyway, so \"Friends don't let friends pipe streams into commands\" should not be the general rule. \"Friends don't let friends pipe streams into shells\" is certainly a good enough general rule. reply scorpicam 8 hours agorootparentYou execute arbitrary commands with troff. I used this feature a lot for getting output from code samples, but it's definitely a security issue if you don't trust the source. Or, if you think shelling out is too boring, the troff macro language is actually turing complete and is a disturbingly usable for scripting. reply jwilk 1 hour agorootparent> You execute arbitrary commands with troff. This should be disabled by default since groff v1.17 (released in 2001): https://git.savannah.gnu.org/cgit/groff.git/commit/?id=7b3f5... reply rezonant 8 hours agorootparentprevOof, I kind of suspected this might be the case. reply godelski 7 hours agorootparentprev1. Regardless, it is bad practice. It isn't a good idea to teach people to pipe downloaded content into any command. Regardless of how naive and safe the command seems. You may not know. Especially considering how you aren't guaranteed to run the full contents and may even run partial lines. Just don't get in the habit of streaming anything into a command. Because you'll be more tempted to stream into a different one which might have issues. Running two commands (or &&) just isn't a big deal. Low cost for high reward. 2. The person replying to me mentioned ffmpeg and well... see the reply to them. Parsers may not seem like a big deal, but see this about `cat`[0] or this about `less`[1] (a quick search shows a lot of pipe and pager type of vulnerabilities, including privilege escalation). Programs that look simple and a non-risk are probably actually prime targets for hackers because it can lull someone into a false sense of security. 3. You can detect bash piping server side. There's been a bunch of HN and reddit posts, several have been shared already so I won't repeat. I'm sticking to: Friends don't let friends pipe streams into commands It's just safer. Risk is pretty low, but does using `&&` or a `;` instead really create a lot more work? How about curl -sL -H \"Accept: text/roff\" https://jamesg.blog/2024/02/28/programming-projects/ > post.page && man ./post.page && rm post.page This at least guarantees you don't get a truncated execution.You'll delete the file after you close man. This at least guarantees you don't get a truncated execution. Better if we send to /tmp/post.page and not delete, in case something is fuzzy. Both of these also prevent server side detection and possible tomfoolery. /tmp will be cleared on reboot anyways and it's better to have the file in case something DOES happen. curl -sL -H \"Accept: text/roff\" https://jamesg.blog/2024/02/28/programming-projects/ > /tmp/post.page && man /tmp/post.page [0] https://security.stackexchange.com/questions/56307/can-cat-i... [1] https://ubuntu.com/security/notices/USN-6664-1 Edit: groff vulns: https://www.cvedetails.com/vulnerability-list/vendor_id-72/p... reply Crestwave 37 minutes agorootparent> curl -sL -H \"Accept: text/roff\" https://jamesg.blog/2024/02/28/programming-projects/ > post.page && man ./post.page && rm post.page What if I have a post.page in my current directory? > curl -sL -H \"Accept: text/roff\" https://jamesg.blog/2024/02/28/programming-projects/ > /tmp/post.page && man /tmp/post.page What if another user runs the command at the same time, then? Or what if a malicious user creates a 666 mode /tmp/post.page file beforehand, detects when you finish writing to it, then attaches a payload right before `man` reads it? Unfortunately, there is no perfect solution for this problem; I run arbitrary html, css, and javascript every day by browsing the web. It's debatable whether switching to command chains instead of piping results in overall benefits. Of course, the same goes for vice versa as well. reply hereonout2 14 hours agoparentprevShould have read the manual. reply 0xEF 13 hours agorootparentrtfm.lol I guess reply zerojames 12 hours agorootparentSee https://news.ycombinator.com/item?id=39552852 for more discussion. reply hk1337 11 hours agorootparentprevhttps://rtfm.lol already taken. reply yrro 15 hours agoprevFYI, 'curl -sL -H \"Accept: text/roff\" https://jamesg.blog/2024/02/28/programming-projects/man -l /dev/stdin' works for me - no need to safe the roff file locally. reply godelski 14 hours agoparentI'm pretty sure the OP is intentionally not doing this because piping commands/things-from-the-internet into \"bash\" (or anything else) is generally considered bad practice. For me, I'm all for this. Anyone that is aware of the security implications is extremely likely aware of how to make the conversion like you've provided. In that case, no reason to tell people, because they already know. But it's not a great thing to tell noobies to do because they will get burned. So don't tell them. As they advance they'll naturally learn about this feature. And hopefully they have learned the implications by the time they learn how to do this. (not me): https://www.seancassidy.me/dont-pipe-to-your-shell.html reply hk__2 13 hours agorootparent> I'm pretty sure the OP is intentionally not doing this because piping commands/things-from-the-internet into \"bash\" (or anything else) is generally considered bad practice. It’s generally considered bad practice only for bash and similar commands that execute their input. It’s not a bad practice at all for commands that just display or transform their input, like `man`, `less`, `ffmpeg`, etc. reply mananaysiempre 13 hours agorootparentMan on Linux runs groff, which (like the original nroff/troff) is a fully general macro processor in addition to being a typesetting system. I wouldn’t bet on it not being able to launch subprocesses, or especially on it having no overflows and such on untrusted input. I’m not even sure about OpenBSD’s much more limited mandoc. (Also, I don’t know about ffmpeg as it is somewhat more rare to have it be public-facing, but there have definitely been exploits against ImageMagick, usually targeted at websites using it to process user input.) reply codelobe 11 hours agorootparentprev[insert confused trollface] > ffmpeg There is certainly a few hundered exploitable vectors in that program alone... to say nothing of the rest. When in doubt, spin up a VM to run the random untrusted thing -- And then go read its mailing list/issue tracker for known VM escaping exploits. I have a machine setup to test malware, so I just hit my \"airgap\" switch to isolate the system from my network once the questionable code is in place and ready to run (potentially amok). Study-up about ARP-poison attacks, and remember ARP does not transit to upstream routers/switches (Y \"combinate\" your network for fun and profit). Before you assume non malicious simple text output, consider \"ANSI\" escape code complexity as an intrusion vector for whatever terminal you run. I've got \"0-days\" for this going back to MSDOS: ANSI Bomb => arbitrary CMD entry. You don't have to take my word for it, your terminal of choice is most certainly vulnerable to some ANSI/escape code related exploit, look it up. reply hk__2 0 minutes agorootparentFine but we’re not talking about piping random stuff from the Internet here; we’re just using curl as a convenience not to use an intermediary file. rezonant 10 hours agorootparentprevThis is why I spin up a VM whenever I want to look at an image. The risk is too great. Even text files, after all we never know if there's a zero day in the UTF-8 decoder. Better safe than sorry. Wait a minute I just realized there could be a zero day in the VM hypervisor too. I guess I'll just have to buy a fresh Raspberry Pi for each file I want to open. /s reply _joel 13 hours agorootparentprevThere's not much difference doing a pipe or using and intermediary file and \"excecuting\" that straight away. There's no manual step in the documentation there to read the output before it's run. reply jmb99 12 hours agorootparentThere definitely is a difference. See [1] (or [2] since the original site has misconfigured TLS). Long story short, using some fairly trivial heuristics, a malicious server could change its response when it detects its output being piped to a shell rather than saved to a file. Thus, a security-minded person downloading the file (who could inspect it) would be given a clean copy, but someone less security-minded piping it straight to bash would be given the malicious copy. The security-minded person wouldn't be able to warn people not to pipe the script to a shell, since it appears safe. [1] https://www.idontplaydarts.com/2016/04/detecting-curl-pipe-b... [2] https://web.archive.org/web/20240228190305/https://www.idont... reply rezonant 10 hours agorootparentThis is extremely specifically is about detecting that Bash is pulling on the stream, by using it's line by line interpretation to do a timing attack. It's not clear there's a timing difference between pushing the stream to disk versus pushing the stream to man, since it seems fairly likely that man would read the entire file into memory before operating on it (at least modern man) reply _joel 10 hours agorootparentprevThis is piping to man, not bash. Interesting vector if you're worried about people piping into man from curl, but there you go. reply godelski 12 hours agorootparentprevThis is not accurate. If the download gets interrupted bash will execute the partial line. That means `rm -r /tmp/foo.ext` or `rm -r ${HOME}/.tmp_config` can execute as `rm -r /`. This can be mitigated by wrapping the script, but clearly no one is looking at the code so this isn't really verified anyways. And it's not like we see that all the time. Edit: But my main point is about habits. There's the concern mananaysiempre brings up[0], but either way, it is best to be in good habits. [0] https://news.ycombinator.com/item?id=39554895 reply _joel 10 hours agorootparentFrom the post curl -sL -H \"Accept: text/roff\" https://jamesg.blog/2024/02/28/programming-projects/ > post.page && man ./post.page If curl's process is interupted, it'll generate a non-0 exit code and the man command won't be exectued. That's how double ampersands work in shell. reply godelski 7 hours agorootparentI think you've missed the context of my comment. We're talking about `curlman` not `curl && man`. You're right, `&&` doesn't have that risk. But `|` sure does. reply fragmede 5 hours agorootparentprevthankfully rm -r / doesn't work on modern systems, though that doesn't invalidate your point. reply godelski 3 hours agorootparentOn what system are you talking about? I mean it won't kill everything because sudo but everything you have permissions to are still in danger reply godelski 12 hours agorootparentprevCan't edit my comment so I'll write as a reply This is to all the people saying no difference between downloading and running right away If the download gets interrupted bash will execute the partial line. That means `rm -r /tmp/foo.ext` or `rm -r ${HOME}/.tmp_config` can execute as `rm -r /`. This can be mitigated by wrapping the script. Best way to do this is wrap the whole script into a function and then execute at the last line[0]. Oh, and you can detect `curl|bash` server side[1] [0] https://archive.is/20160603044800/https://sandstorm.io/news/... [1] https://archive.is/20230325190353/https://www.idontplaydarts... Edit: as an example, rust does the warpping. But they still place this stupid shit on in their install instructions. Bad Rust! Bad! https://www.rust-lang.org/tools/install reply Arnavion 6 hours agorootparent>Best way to do this is wrap the whole script into a function and then execute at the last line[0]. It's not the best way. If your function is named `lsp_init` and your last line is `lsp_init`, a partial line can result in the execution of `ls`. AFAIK the best way is to just wrap your script in `()`. reply godelski 6 hours agorootparentYou're right! That's a good thing to point out. Elsewhere I suggested Rust change their install to this though. Solves pretty much all the probems curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs > /tmp/rust_install.sh && sh /tmp/rust_install.sh (obviously doesn't solve the problem of inspection, but I think we all know that's not going to happen anyways) reply rezonant 10 hours agorootparentprevI agree that it's a risk to pipe to sh but that's not what we're doing here, and I'd hope that most of the people aren't saying there's no difference between piping to man and piping to sh. reply jimmaswell 13 hours agorootparentprevHas there been a single recorded case of malware getting around this way? reply godelski 11 hours agorootparentI'm sorry, your argument is... what exactly? That we shouldn't take simple or even trivial preventative measures that also reduce other potential problems because we... haven't seen anybody abuse such a thing before? Really? What a terrible argument. Not it's trivial to resolve. Why not just fix things that we know are problems or can lead to serious problems instead of waiting for it to become a problem where it'll then be FAR more work to clean it up? Seriously, you're a human, not a bug. You have the ability to solve things before they become problems. Use it. While I'm not sure of a specific example, I feel quite confident in saying that this has been done before. reply jimmaswell 4 hours agorootparentIf you're willing to run an executable installer from a website then piping into sh is no better. reply rezonant 10 hours agorootparentprevThere was no argument here, only a question. And it's a fair question to ask for the purposes of discussion. You're right, piping to sh is definitely a risk, and we should do better as a community, and not make users normalized to piping to sh. reply godelski 7 hours agorootparentIt appeared as a rhetorical question used to rebut the claim. Maybe I misinterpreted it. Fair. But that is a possible interpretation. But as you can see elsewhere, we shouldn't pipe streams into anything. It's just not hard to avoid this. A few extra characters and you're good to go. Let's take rust for example. Users are copy pasting anyways They give curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rssh But why not curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs > /tmp/rust_install.sh && sh /tmp/rust_install.sh For user experience, it is till a one liner, but it is infinitely better. Still has some risks, but a lot less. (at least rust does wrap the install so you don't run the risk of partial line execution) And it gives the user a way to verify that the file was correct if they provide a checksum. There's just no good reason to not do this. We especially shouldn't be teaching noobies to pipe streams into any command. Let's be real, most people don't know linux well, even if they use linux. reply globular-toast 14 hours agorootparentprevPiping to your shell and piping to `man` are not the same thing. The really dangerous thing is copy/pasting from a browser to your terminal. Always do Ctrl-X Ctrl-E to open an editor, paste it in there, then inspect it before saving/closing the editor to run the command. reply vladxyz 13 hours agorootparentAren't they? Are you sure piping to 'man' can't result in arbitrary code execution? The two things you need to be able to say you trust are your CA store, and the source of your curl -> shell. reply johannes1234321 11 hours agorootparentEven if it were: There is no practical difference. \"Nobody\" will inspect the man page using a different viewer first. So if I download to disk and then view via man or directly via man is no difference. A shell script one might inspect first using some viewer. While only few probably do. reply globular-toast 12 hours agorootparentprevNo, they're not the same thing! It's not piping to your shell! The shell's single purpose is to execute code. Man is not supposed to do that and it would be considered a huge security issue if it could. In any case, how would you check the downloaded file? With a text editor? Are you sure that can't result in arbitrary code execution? reply AlecSchueler 12 hours agorootparentMan can run groff which can in turn run arbitrary subprocesses. reply crtasm 13 hours agorootparentprevNever knew that shortcut, useful! reply godelski 12 hours agorootparentI only recently learned of it myself despite being on nix for over a decade and reading many advanced bash shortcut blogs. IDK why it isn't well known. For zsh users see here: https://nuclearsquid.com/writings/edit-long-commands/ Note: this can be tricky depending on where it goes in your zshrc. If you use a plugin manager (like Sheldon) then this should be above that. I ended up with this and it works well on OSX and linux autoload -U edit-command-line # Emacs style () zle -N edit-command-line # make sure `set -o vi` is above this line bindkey '^xe' edit-command-line bindkey '^x^e' edit-command-line # (VIM) Use visual mode bindkey -M vicmd v edit-command-line reply zerojames 15 hours agoparentprevUnfortunately, that command doesn't work on macOS (`/usr/bin/man: illegal option -- l`). I tried to get a one-liner with piping working on my Mac but I always ran into errors. The -l flag doesn't exist in the macOS man implementation (I consulted the man page; meta!). reply jolmg 15 hours agorootparentThe `-l` can be skipped (probably because there are `/`s in man's argument): curl -sL -H \"Accept: text/roff\" https://jamesg.blog/2024/02/28/programming-projects/man /dev/stdin reply divbzero 14 hours agorootparentThis still doesn’t work for me on macOS, perhaps a difference in how /dev/stdin is implemented? Fortunately, macOS has ZSH as the default shell so the following does work: man =(curl -sL -H \"Accept: text/roff\" https://jamesg.blog/2024/02/28/programming-projects/) reply BossingAround 15 hours agorootparentprev(which still doesn't work on mac) reply penguinjanitor 15 hours agorootparentprevMaybe 'man/tmp/file; outer /tmp/file`. This works with programs that don't read from stdin, or need two or more inputs, for example. reply BossingAround 15 hours agorootparentprevNope reply salviati 14 hours agoparentprevYou can save a few chars with a process sostitution instead of a pipe if you're using bash: man -l* I was disappointed that there was no markdown-to-roff conversion I found this rather surprising too. Pandoc can trivially convert markdown to man-page roff. Insert that into the given template and it looks like more like an actual man page. reply zerojames 15 hours agorootparentGood suggestion! Context: All man pages are generated on the fly on GitHub Pages. My site generates ~2500 pages, for which 826 are eligible for a man page. I didn't want to introduce another parser since I just got my site build times down :D I can counter increased build times with caching, but it gets a bit icky since some blog pages are evergreen (i.e. my blogroll). [insert cache invalidation complaint here] But there's certainly a way! reply jorams 12 hours agorootparentI feel that, build times can be a pain. I'm calling out to pandoc to build a static site and I've had to parallelize it to get build times down, and that's with far fewer pages. reply sureglymop 5 hours agoprevI did a somewhat similar thing on my portfolio page. The server recognizes curl and wget and then serves the page as plaintext. If a PDF is requested, it sends my CV. My idea is that if a recruiter overcomes that first hurdle of getting to the CV, then I may as well hear them out. reply urbandw311er 10 hours agoprevLittle real world application, super fun challenge. This is why I come here. reply nulbyte 16 hours agoprevI never thought of making a blog post a man page. That's pretty awesome, actually. I'd be interested to see the code for the underlying conversion, but maybe I'll just try my hand at it myself this weekend. Btw, in Bash, you can use process substitution to avoid littering your folder with files, if you don't want to save them: $ manI'd be interested to see the code for the underlying conversion The post includes the template he used. You can adapt it to any templating engine or put some placeholders in and use sed or whatever. reply worble 15 hours agoprevCool idea. I'm starting on the timer until we get \"serving my blog posts as playable DOOM wads\". reply vonjuice 15 hours agoparentAdd it to the list of the handful of actually cool things that AI could facilitate. reply dpassens 14 hours agoprevThe correct media type would be text/troff, as per RFC 4263 (https://www.rfc-editor.org/rfc/rfc4263.html). reply notorandit 3 hours agoprevUnix man pages. Man pages were here since long before Linux existed! reply harryvederci 12 hours agoprevI feel like this is too accessible. Anyone doing this but with Vim help files? reply akritid 6 hours agoprevWith some plumbing, it should be possible to follow links: match the URL with regex using the terminal and launch the link target in a new window. reply jedberg 14 hours agoprevIronically it breaks when you request this page as a man page. :) reply rahimnathwani 15 hours agoprevIf you like this you might like mdless, which does exactly what the name suggests. https://github.com/ttscoff/mdless reply adriangrigore 13 hours agoprevI prefer my own UNIX web dev solution https://mkws.sh! Man pages ar too much imo. reply hk__2 13 hours agoparent> I prefer my own UNIX web dev solution https://mkws.sh! Man pages ar too much imo. Those are orthogonal subjects; you could generate your own static page with your generator AND also serve them as manpages. The linked article does not suggest to server manpages to everyone, just to user agents that request them. reply gglitch 15 hours agoprevFun project :) Next step: Texinfo, which will output to info, html, and pdf, among others, and which includes links and indexes. reply heartag 16 hours agoprevThat's a fun idea, and well executed. reply pjmlp 15 hours agoprevCool experiment, as means to serve UNIX man pages. reply _xerces_ 11 hours agoprevFor the lazy or short of time, does with worth with tldr pages :) https://tldr.sh/ reply anthk 12 hours agoprevWIth mandoc and maybe groff you can typeset pages to man pages, HTML, PS and PDF files. reply Annatar 16 hours agoprevOne could pipe that output directly to nroff or groff:soelimtbleqnnroff -man -$PAGERsoelimtbleqnnroff -man -Tpost -/usr/lib/postscript/bin/dpostps2pdf - > ~/Desktop/blog.PDFsoelimtbleqngroff -Tps -man -ps2pdf - > ~/Desktop/blog.PDF reply anthk 12 hours agoparentwith groff you don't need soelimtbl... just rungroff -Tpdf -step -k > ~/Desktop/blog.pdf reply chanamasala 15 hours agoprevnext [2 more] [flagged] bravetraveler 15 hours agoparentSomewhere lower than both of us me: for engaging with this you: for making an account to post this in the first place reply throwaway81523 15 hours agoprev [–] That doesn't look like man pages. Was it supposed to be an example? (EDIT: oh nm, I see, I have to view the image. Nice. But the web view should also look like that.) I noticed the \"written by human, not AI\" logo, a little too cute but the sentiment is good. I had been thinking of putting something like \"this page created by natural stupidity\" in mine. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author transformed blog posts into Linux manual pages using content negotiation and roff syntax, updating their site to create manual pages for each post and configuring NGINX to process requests for text/roff versions.",
      "Users can request manual pages using the curl command and view them using the man command, making it an engaging exploration of typesetting tech and command line interfaces."
    ],
    "commentSummary": [
      "Discussions focus on serving blog posts as Linux manual pages, with tips on creating AUR packages for Arch Linux and avoiding piping streams into commands for security.",
      "Security concerns, best practices for handling downloads, and executing scripts from the internet are highlighted, emphasizing caution and risk mitigation.",
      "Users share insights on workflows, subprocesses, and content serving in multiple formats, showing interest in experimenting with serving blog posts as Unix man pages using tools like tldr pages, mandoc, and groff."
    ],
    "points": 296,
    "commentCount": 126,
    "retryCount": 0,
    "time": 1709209100
  },
  {
    "id": 39549486,
    "title": "Modernizing a C++ Legacy Codebase: Best Practices for Security and Efficiency",
    "originLink": "https://gaultier.github.io/blog/you_inherited_a_legacy_cpp_codebase_now_what.html",
    "originBody": "Philippe Gaultier Body of work Resume LinkedIn Github Feed Published on 2024-02-29. You’ve just inherited a legacy C++ codebase, now what? You were minding your own business, and out of nowhere something fell on your lap. Maybe you started a new job, or perhaps changed teams, or someone experienced just left. And now you are responsible for a C++ codebase. It’s big, complex, idiosyncratic; you stare too long at it and it breaks in various interesting ways. In a word, legacy. But somehow bugs still need to be fixed, the odd feature to be added. In short, you can’t just ignore it or better yet nuke it out of existence. It matters. At least to someone who’s paying your salary. So, it matters to you. What do you do now? Well, fear not, because I have experience this many times in numerous places (the snarky folks in the back will mutter: what C++ codebase isn’t exactly like I described above), and there is a way out, that’s not overly painful and will make you able to actually fix the bugs, add features, and, one can dream, even rewrite it some day. So join me on a recollection of what worked for me and what one should absolutely avoid. And to be fair to C++, I do not hate it (per se), it just happens to be one of these languages that people abuse and invariably leads to a horrifying mess and poor C++ is just the victim here and the C++ committee will fix it in C++45, worry not, by adding std::cmake to the standard library and you’ll see how it’s absolutely a game changer, and - Ahem, ok let’s go back to the topic at hand. So here’s an overview of the steps to take: Get it to work locally, by only doing the minimal changes required in the code and build system, ideally none. No big refactorings yet, even if itches really bad! Get out the chainsaw and rip out everything that’s not absolutely required to provide the features your company/open source project is advertising and selling Make the project enter the 21st century by adding CI, linters, fuzzing, auto-formatting, etc Finally we get to make small, incremental changes to the code, Rinse and repeat until you’re not awaken every night by nightmares of Russian hackers p@wning your application after a few seconds of poking at it If you can, contemplate rewrite some parts in a memory safe language The overarching goal is exerting the least amount of effort to get the project in an acceptable state in terms of security, developer experience, correctness, and performance. It’s crucial to always keep that in mind. It’s not about ‘clean code’, using the new hotness language features, etc. Ok, let’s dive in! By the way, everything here applies to a pure C codebase or a mixed C and C++ codebase, so if that’s you, keep reading! Table of contents Get buy-in Write down the platforms you support Get the build working on your machine Get the tests passing on your machine Write down in the README how to build and test the application Find low hanging fruits to speed up the build and tests Remove all unnecessary code Linters Code formatting Sanitizers Add a CI pipeline Incremental code improvements Rewrite in a memory safe language? Conclusion Addendum: Dependency management Get buy-in You thought I was going to compare the different sanitizers, compile flags, or build systems? No sir, before we do any work, we talk to people. Crazy, right? Software engineering needs to be a sustainable practice, not something you burn out of after a few months or years. We cannot do this after hours, on a death march, or even, alone! We need to convince people to support this effort, have them understand what we are doing, and why. And that encompasses everyone: your boss, your coworkers, even non-technical folks. And who knows, maybe you’ll go on vacation and return to see that people are continuing this effort when you’re out of office. All of this only means: explain in layman terms the problem with a few simple facts, the proposed solution, and a timebox. Simple right? For example (to quote South Park: All characters and events in this show—even those based on real people—are entirely fictional): Hey boss, the last hire took 3 weeks to get the code building on his machine and make his first contribution. Wouldn’t it be nice if, with minimal effort, we could make that a few minutes? Hey boss, I put quickly together a simple fuzzing setup (‘inputting random data in the app like a monkey and seeing what happens’), and it manages to crash the app 253 times within a few seconds. I wonder what would happen if people try to do that in production with our app? Hey boss, the last few urgent bug fixes took several people and 2 weeks to be deployed in production because the app can only be built by this one build server with this ancient operating system that has not been supported for 8 years (FreeBSD 9, for the curious) and it kept failing. Oh by the way whenever this server dies we have no way to deploy anymore, like at all. Wouldn’t it be nice to be able to build our app on any cheap cloud instance? Hey boss, we had a cryptic bug in production affecting users, it took weeks to figure out and fix, and it turns out if was due to undefined behavior (‘a problem in the code that’s very hard to notice’) corrupting data, and when I run this industry standard linter (‘a program that finds issues in the code’) on our code, it detects the issue instantly. We should run that tool every time we make a change! Hey boss, the yearly audit is coming up and the last one took 7 months to pass because the auditor was not happy with what they saw. I have ideas to make that smoother. Hey boss, there is a security vulnerability in the news right now about being able to decrypt encrypted data and stealing secrets, I think we might be affected, but I don’t know for sure because the cryptography library we use has been vendored (‘copy-pasted’) by hand with some changes on top that were never reviewed by anyone. We should clean that up and setup something so that we get alerted automatically if there is a vulnerability that affects us. And here’s what to avoid, again totally, super duper fictional, never-really-happened-to-me examples: We are not using the latest C++ standard, we should halt all work for 2 weeks to upgrade, also I have no idea if something will break because we have no tests I am going to change a lot of things in the project on a separate branch and work on it for months. It’s definitely getting merged at some point! (narrator’s voice: it wasn’t) We are going to rewrite the project from scratch, it should take a few weeks tops We are going to improve the codebase, but no idea when it will be done or even what we are going to do exactly Ok, let’s say that now you have buy-in from everyone that matters, let’s go over the process: Every change is small and incremental. The app works before and works after. Tests pass, linters are happy, nothing was bypassed to apply the change (exceptions do happen but that’s what they are, exceptional) If an urgent bug fix has to be made, it can be done as usual, nothing is blocked Every change is a measurable improvement and can be explained and demoed to non experts If the whole effort has to be suspended or stopped altogether (because of priorities shifting, budget reasons, etc), it’s still a net gain overall compared to before starting it (and that gain is in some form measurable) In my experience, with this approach, you keep everyone happy and can do the improvements that you really need to do. Alright, let’s get down to business now! Write down the platforms you support This is so important and not many projects do it. Write in the README (you do have a README, right?). It’s just a list of - pair, e.g. x86_64-linux or aarch64-darwin, that your codebase officially supports. This is crucial for getting the build working on every one of them but also and we’ll see later, removing cruft for platforms you do not support. If you want to get fancy, you can even write down which version of the architecture such as ARMV6 vs ARMv7, etc. That helps answer important questions such as: Can we rely on having hardware support for floats, or SIMD, or SHA256? Do we even care about supporting 32 bits? Are we ever running on a big-endian platform? (The answer is very likely: no, never did, never will - if you do, please email me with the details because that sounds interesting). Can a char be 7 bits? And an important point: This list should absolutely include the developers workstations. Which leads me to my next point: Get the build working on your machine You’d be amazed at how many C++ codebase in the wild that are a core part of a successful product earning millions and they basically do not compile. Well, if all the stars are aligned they do. But that’s not what I’m talking about. I’m talking about reliably, consistently building on all platforms you support. No fuss, no ‘I finally got it building after 3 weeks of hair-pulling’ (this brings back some memories). It just works(tm). A small aparte here. I used to be really into Karate. We are talking 3, 4 training sessions a week, etc. And I distinctly remember one of my teachers telling me (picture a wise Asian sifu - hmm actually my teacher was a bald white guy… picture Steve Ballmer then): You do not yet master this move. Sometimes you do and sometimes you don’t, so you don’t. When eating with a spoon, do you miss your mouth one out of five times? And I carried that with me as a Software Engineer. ‘The new feature works’ means it works every time. Not four out of five times. And so the build is the same. Experience has shown me that the best way to produce software in a fast and efficient way is to be able to build on your machine, and ideally even run it on your machine. Now if your project is humongous that may be a problem, your system might not even have enough RAM to complete the build. A fallback is to rent a big server somewhere and run your builds here. It’s not ideal but better than nothing. Another hurdle is the code requiring some platform specific API, for example io_uring on Linux. What can help here is to implement a shim, or build inside a virtual machine on your workstation. Again, not ideal but better than nothing. I have done all of the above in the past and that works but building directly on your machine is still the best option. Get the tests passing on your machine First, if there are no tests, I am sorry. This is going to be really difficult to do any change at all. So go write some tests before doing any change to the code, make them pass, and come back. The easiest way is to capture inputs and outputs of the program running in the real world and write end-to-end tests based on that, the more varied the better. It will ensure there are no regressions when making changes, not that the behavior was correct in the first place, but again, better than nothing. So, now you have a test suite. If some tests fail, disable them for now. Make them pass, even if the whole test suite takes hours to run. We’ll worry about that later. Write down in the README how to build and test the application Ideally it’s one command to build and one for testing. At first it’s fine if it’s more involved, in that case the respective commands can be put in a build.sh and test.sh that encapsulate the madness. The goal is to have a non C++ expert be able to build the code and run the tests without having to ask you anything. Here some folks would recommend documenting the project layout, the architecture, etc. Since the next step is going to rip out most of it, I’d say don’t waste your time now, do that at the end. Find low hanging fruits to speed up the build and tests Emphasis on ‘low hanging’. No change of the build system, no heroic efforts (I keep repeating that in this article but this is so important). Again, in a typical C++ project, you’d be amazed at how much work the build system is doing without having to do it at all. Try these ideas below and measure if that helps or not: Building and running tests of your dependencies. In a project which was using unittest++ as a test framework, built as a CMake subproject, I discovered that the default behavior was to build the tests of the test framework, and run them, every time! That’s crazy. Usually there is a CMake variable or such to opt-out of this. Building and running example programs of your dependencies. Same thing as above, the culprit that time was mbedtls. Again, setting a CMake variable to opt-out of that solved it. Building and running the tests of your project by default when it’s being included as a subproject of another parent project. Yeah the default behavior we just laughed at in our dependencies? It turns out we’re doing the same to other projects! I am no CMake expert but it seems that there is no standard way to exclude tests in a build. So I recommend adding a build variable called MYPROJECT_TEST unset by default and only build and run tests when it is set. Typically only developers working on the project directly will set it. Same with examples, generating documentation, etc. Building all of a third-party dependency when you only need a small part of it: mbedtls comes to mind as a good citizen here since it exposes many compile-time flags to toggle lots of parts you might not need. Beware of the defaults, and only build what you need! Wrong dependencies listed for a target leading to rebuilding the world when it does not have to: most build systems have a way to output the dependency graph from their point of view and that can really help diagnose these issues. Nothing feels worse than waiting for minutes or hours for a rebuild, when deep inside, you know it should have only rebuilt a few files. Experiment with a faster linker: mold is one that can be dropped in and really help at no cost. However that really depends on how many libraries are being linked, whether that’s a bottleneck overall, etc. Experiment with a different compiler, if you can: I have seen projects where clang is twice as fast as gcc, and others where there is no difference. Once that’s done, here are a few things to additionally try, although the gains are typically much smaller or sometimes negative: LTO: off/on/thin Split debug information Make vs Ninja The type of file system in use, and tweaking its settings Once the iteration cycle feels ok, the code gets to go under the microscope. If the build takes ages, it’s not realistic to want to modify the code. Remove all unnecessary code Dad, I see dead lines of code. (Get the reference? Well, ok then.) I have seen 30%, sometimes more, of a codebase, being completely dead code. That’s lines of code you pay for every time you compile, you want to make a refactoring, etc. So let’s rip them out. Here are some ways to go about it: The compiler has a bunch of -Wunused-xxx warnings, e.g. -Wunused-function. They catch some stuff, but not everything. Every single instance of these warnings should be addressed. Usually it’s as easy as deleting the code, rebuilding and re-running the tests, done. In rare cases it’s a symptom of a bug where the wrong function was called. So I’d be somewhat reluctant to fully automate this step. But if you’re confident in your test suite, go for it. Linters can find unused functions or class fields, e.g. cppcheck. In my experience there are quite a lot of false positives especially regarding virtual functions in the case of inheritance, but the upside is that these tools absolutely find unused things that the compilers did not notice. So, a good excuse for adding a linter to your arsenal, if not to the CI (more on that later). I have seen more exotic techniques were the linker is instructed to put each function in its own section and print every time a section is removed because it’s detected to be unused at link time, but that results in so much noise e.g. about standard library functions being unused, that I have not found that really practical. Others inspect the generated assembly and compare which functions are present there with the source code, but that does not work for virtual functions. So, maybe worth a shot, depending on your case? Remember the list of supported platforms? Yeah, time to put it to use to kill all the code for unsupported platforms. Code trying to support ancient versions of Solaris on a project that exclusively ran on FreeBSD? Out of the window it goes. Code trying to provide its own random number generator because maybe the platform we run on does not have one (of course it turned out that was never the case)? To the bin. Hundred of lines of code in case POSIX 2001 is not supported, when we only run on modern Linux and macOS? Nuke it. Checking if the host CPU is big-endian and swapping bytes if it is? Ciao (when was the last time you shipped code for a big-endian CPU? And if yes, how are you finding IBM?). That code introduced years ago for a hypothetical feature that never came? Hasta la vista. And the bonus for doing all of this, is not only that you sped up the build time by a factor of 5 with zero downside, is that, if your boss is a tiny bit technical, they’ll love seeing PRs deleting thousands of lines of code. And your coworkers as well. Linters Don’t go overboard with linter rules, add a few basic ones, incorporate them in the development life cycle, incrementally tweak the rules and fix the issues that pop up, and move on. Don’t try to enable all the rules, it’s just a rabbit hole of diminishing returns. I have used clang-tidy and cppcheck in the past, they can be helpful, but also incredibly slow and noisy, so be warned. Having no linter is not an option though. The first time you run the linter, it’ll catch so many real issues that you’ll wonder why the compiler is not detecting anything even with all the warnings on. Code formatting Wait for the appropriate moment where no branches are active (otherwise people will have horrendous merge conflicts), pick a code style at random, do a one time formatting of the entire codebase (no exceptions), typically with clang-format, commit the configuration, done. Don’t waste any bit of saliva arguing about the actual code formatting. It only exists to make diffs smaller and avoid arguments, so do not argue about it! Sanitizers Same as linters, it can be a rabbit hole, unfortunately it’s absolutely required to spot real, production affecting, hard to detect, bugs and to be able to fix them. -fsanitize=address,undefined is a good baseline. They usually do not have false positives so if something gets detected, go fix it. Run the tests with it so that issues get detected there as well. I even heard of people running the production code with some sanitizers enabled, so if your performance budget can allow it, it could be a good idea. If the compiler you (have to) use to ship the production code does not support sanitizers, you can at least use clang or such when developing and running tests. That’s when the work you did on the build system comes in handy, it should be relatively easy to use different compilers. One thing is for sure: even in the best codebase in the world, with the best coding practices and developers, the second you enable the sanitizers, you absolutely will uncover horrifying bugs and memory leaks that went undetected for years. So do it. Be warned that fixing these can require a lot of work and refactorings. Each sanitizer also has options so it could be useful to inspect them if your project is a special snowflake. One last thing: ideally, all third-party dependencies should also be compiled with the sanitizers enabled when running tests, to spot issues in them as well. Add a CI pipeline As Bryan Cantrill once said (quoting from memory), ‘I am convinced most firmware just comes out of the home directory of a developer’s laptop’. Setting up a CI is quick, free, and automates all the good things we have set up so far (linters, code formatting, tests, etc). And that way we can produce in a pristine environment the production binaries, on every change. If you’re not doing this already as a developer, I don’t think you really have entered the 21st century yet. Cherry on the cake: most CI systems allow for running the steps on a matrix of different platforms! So you can demonstrably check that the list of supported platforms is not just theory, it is real. Typically the pipeline just looks like make all test lint fmt so it’s not rocket science. Just make sure that issues that get reported by the tools (linters, sanitizers, etc) actually fail the pipeline, otherwise no one will notice and fix them. Incremental code improvements Well that’s known territory so I won’t say much here. Just that lots of code can often be dramatically simplified. I remember iteratively simplifying a complicated class that manually allocated and (sometimes) deallocated memory, was meant to handle generic things, and so on. All the class did, as it turned out, was allocate a pointer, later check whether the pointer was null or not, and…that’s it. Yeah that’s a boolean in my book. True/false, nothing more to it. I feel that’s the step that’s the hardest to timebox because each round of simplification opens new avenues to simplify further. Use your best judgment here and stay on the conservative side. Focus on tangible goals such as security, correctness and performance, and stray away from subjective criteria such as ‘clean code’. In my experience, upgrading the C++ standard in use in the project can at times help with code simplifications, for example to replace code that manually increments iterators by a for (auto x : items) loop, but remember it’s just a means to an end, not an end in itself. If all you need is std::clamp, just write it yourself. Rewrite in a memory safe language? I am doing this right now at work, and that deserves an article of its own. Lots of gotchas there as well. Only do this with a compelling reason. Conclusion Well, there you have it. A tangible, step-by-step plan to get out of the finicky situation that’s a complex legacy C++ codebase. I have just finished going through that at work on a project, and it’s become much more bearable to work on it now. I have seen coworkers, who previously would not have come within a 10 mile radius of the codebase, now make meaningful contributions. So it feels great. There are important topics that I wanted to mention but in the end did not, such as the absolute necessity of being able to run the code in a debugger locally, fuzzing, dependency scanning for vulnerabilities, etc. Maybe for the next article! If you go through this on a project, and you found this article helpful, shoot me an email! It’s nice to know that it helped someone. Addendum: Dependency management This section is very subjective, it’s just my strong, biased opinion. There’s a hotly debated topic that I have so far carefully avoided and that’s dependency management. So in short, in C++ there’s none. Most people resort to using the system package manager, it’s easy to notice because their README looks like this: On Ubuntu 20.04: `sudo apt install [100 lines of packages]` On macOS: `brew install [100 lines of packages named slightly differently]` Any other: well you're out of luck buddy. I guess you'll have to pick a mainstream OS and reinstall ¯\\_(ツ)_/¯ Etc. I have done it myself. And I think this is a terrible idea. Here’s why: The installation instructions, as we’ve seen above, are OS and distribution dependent. Worse, they’re dependent on the version of the distribution. I remember a project that took months to move from Ubuntu 20.04 to Ubuntu 22.04, because they ship different versions of the packages (if they ship the same packages at all), and so upgrading the distribution also means upgrading the 100 dependencies of your project at the same time. Obviously that’s a very bad idea. You want to upgrade one dependency at a time, ideally. There’s always a third-party dependency that has no package and you have to build it from source anyway. The packages are never built with the flags you want. Fedora and Ubuntu have debated for years whether to build packaged with the frame pointer enabled (they finally do since very recently). Remember the section about sanitizers? How are you going to get dependencies with sanitizer enabled? It’s not going to happen. But there are way more examples: LTO, -march, debug information, etc. Or they were built with a different C++ compiler version from the one you are using and they broke the C++ ABI between the two. You want to easily see the source of the dependency when auditing, developing, debugging, etc, for the version you are currently using. You want to be able to patch a dependency easily if you encounter a bug, and rebuild easily without having to change the build system extensively You never get the exact same version of a package across systems, e.g. when developer Alice is on macOS, Bob on Ubuntu and the production system on FreeBSD. So you have weird discrepancies you cannot reproduce and that’s annoying. Corollary of the point above: You don’t know exactly which version(s) you are using across systems and it’s hard to produce a Bill of Material (BOM) in an automated fashion, which is required (or going to be required very soon? Anyway it’s a good idea to have it) in some fields. The packages sometimes do not have the version of the library you need (static or dynamic) So you’re thinking, I know, I will use those fancy new package managers for C++, Conan, vcpkg and the like! Well, not so fast: They require external dependencies so your CI becomes more complex and slower (e.g. figuring out which exact version of Python they require, which surely will be different from the version of Python your project requires) They do not have all versions of a package. Example: Conan and mbedtls, it jumps from version 2.16.12 to 2.23.0. What happened to the versions in between? Are they flawed and should not be used? Who knows! Security vulnerabilities are not listed anyways for the versions available! Of course I had a project in the past where I had to use version 2.17… They might not support some operating systems or architectures you care about (FreeBSD, ARM, etc) I mean, if you have a situation where they work for you, that’s great, it’s definitely an improvement over using system packages in my mind. It’s just that I never encountered (so far) a project where I could make use of them - there was always some blocker. So what do I recommend? Well, the good old git submodules and compiling from source approach. It’s cumbersome, yes, but also: It’s dead simple It’s better than manually vendoring because git has the history and the diff functionalities You know exactly, down to the commit, which version of the dependency is in use Upgrading the version of a single dependency is trivial, just run git checkout It works on every platform You get to choose exactly the compilation flags, compiler, etc to build all the dependencies. And you can even tailor it per dependency! Developers know it already even if they have no C++ experience Fetching the dependencies is secure and the remote source is in git. No one is changing that sneakily. It works recursively (i.e.: transitively, for the dependencies of your dependencies) Compiling each dependency in each submodule can be as simple as add_subdirectory with CMake, or git submodule foreach make by hand. If submodules are really not an option, an alternative is to still compile from source but do it by hand, with one script, that fetches each dependency and builds it. Example in the wild: Neovim. Of course, if your dependency graph visualized in Graphviz looks like a Rorschach test and has to build thousands of dependencies, it is not easily doable, but it might be still possible, using a build system like Buck2, which does hybrid local-remote builds, and reuses build artifacts between builds from different users. If you look at the landscape of package managers for compiled languages (Go, Rust, etc), all of them that I know of compile from source. It’s the same approach, minus git, plus the automation. If you liked this article and you want to support me, and can afford it: Donate",
    "commentLink": "https://news.ycombinator.com/item?id=39549486",
    "commentBody": "You've just inherited a legacy C++ codebase, now what? (gaultier.github.io)253 points by broken_broken_ 19 hours agohidepastfavorite252 comments eschneider 13 hours agoSome good advice here, and some more...controversial advice here. After inheriting quite a few giant C++ projects over the years, there are a few obvious big wins to start with: * Reproducible builds. The sanity you save will be your own. Pro-tip: wrap your build environment with docker (or your favorite packager) so that your tooling and dependencies become both explicit and reproducable. The sanity you save will be your own. * Get the code to build clean with -Wall. This is for a couple of reasons. a) You'll turn up some amount of bad code/undefined behavior/bugs this way. Fix them and make the warning go away. It's ok to #pragma away some warnings once you've determined you understand what's happening and it's \"ok\" in your situation. But that should be rare. b) Once the build is clean, you'll get obvious warnings when YOU do something sketchy and you can fix that shit immediately. Again, the sanity you save will be your own. * Do some early testing with something like valgrind and investigate any read/write errors it turns up. This is an easy win from a bugfix/stability point of view. * At least initially, keep refactorings localized. If you work on a section and learn what it's doing, it's fine to clean it up and make it better, but rearchitecting the world before you have a good grasp on what's going on globally is just asking for pain and agony. reply hobofan 18 minutes agoparent> wrap your build environment with docker (or your favorite packager) so that your tooling and dependencies become both explicit and reproducable If you want explicitness and reproducibility please don't reach for Docker. Unless you take a lot of care, you will only get the most watered down version of reproducibility with Docker probably luring you into a false sense of security. E.g. pointing to mutable image tags without integrity hashes and invoking apt-get are things you'll find in most Dockerfiles out there and both leave open a huge surface area for things to go wrong and end up in slightly different states. And while they are not that easy to pick up, solutions like Bazel and Nix will give you a lot better foundation to stand on. reply dataflow 12 hours agoparentprevStep 0: reproducible builds (like you said) Step 1: run all tests, mark all the flaky ones. Step 2: run all tests under sanitizers, mark all the ones that fail. Step 3: fix all the sanitizer failures. Step 4: (the other stuff you wrote) reply hyperman1 11 hours agorootparentIf we're going to visit the circles of hell, let's do it properly: Step -1: Get it under source control and backed up. Step -2: Find out if the source code corresponds to the executable. Which of the 7 variants of the source code (if any). Step -3: Do dark rituals over a weekend with cdparanioa to scrape the source code from the bunch of scratched cd's found in someone's bottom drawer. Bonus point if said person died last week, and other eldritch horrors lurk in that bottom drawer. Build a VM clone of the one machine still capable of compiling it. Yes, I have scars, why do you ask? reply skissane 6 hours agorootparentQuestion: Why does some of the product source code look like it is the output of a decompiler? Answer: Our office was in the WTC and was destroyed on 9/11. Luckily everyone got out alive, but then we discovered we had no off-site backups of the source code. In order to continue development, we had to retrieve the released binaries from our customers and decompile them to get back source code. reply guappa 41 minutes agorootparentI see this as a case of: \"see, allowing wfh would have saved you there.\" reply fransje26 41 minutes agorootparentprevOh.. Ouch, ouch, ouch. I feel for you. That must have been hell. reply eschneider 11 hours agorootparentprevThere was that time when I had to dump the roms off a 'test' MRI machine because that's the only version of the code they had, then decompiled it, and rewrite it from that. I think about that a lot now that I'm older and spend a fair bit of time in MRI machines... reply reactordev 8 hours agorootparentDang man that’s tough, at least you know they work ;) reply skipkey 7 hours agorootparentprevEveryone who has been around this game for long enough has scars. At one point in the early 90s I was asked to take over maintenance of a vertical market accounting app that had a few hundred users. It had been written using Lattice C, but at the time was being built with the ultra modern MS C 5.1. The first time I looked at it, I saw that the make file set the warning level to 0 and redirected the output to NUL. Removing that, I ran nmake and prayed. About 45 minutes later it finished building, evidently successfully. It turned out that having the warnings actually print added 10 minutes to the build. It was averaging more than one error per line of code at /w3. And it was around 40k lines of code. Not large by the standards of today but huge back then for MSDOS. Peeking, it used K&R c and included no header files. So step 1 for me was to hook up some scaffolding using various tools from mks to make sure as I edited things that the error count didn’t increase. The biggest thing I learned from this project was to not combine other coding with the warning removal or other cleanup. Makes it much easier to spot when you introduce bugs. reply galangalalgol 11 hours agorootparentprevI was assuming it already had unit and system tests with decent coverage. I forgot how bad stuff gets. Maybe VM clones of various users too, and recordings of their work flows? reply Cerium 10 hours agorootparentI'm always careful to dump the bash history as soon as I get access to a machine involved in a legacy project. reply cqqxo4zV46cp 1 hour agorootparentYep! Learned this one the easy way! I don’t get to say that often, so I’m taking full advantage. reply eschneider 9 hours agorootparentprevOooh, smart! reply thrwwycbr 5 hours agorootparentprevShouldn't it be step -3 to -1? reply hyperman1 3 hours agorootparentOf course not. You try to do 0 but that's impossible because you need to do -1 first. So you drop everything, try to do -1 but that's ... It's yak shaving's evil twin! reply denkmoon 10 hours agorootparentprevThis gave me a good laugh. I too have been here. reply ta2234234242 6 hours agorootparentprevSome people believe that if you read the C++ standard recreationally, it should be interpreted as a call for help, and intervention is required, putting the subject under 24/7 monitoring and physical restraints. /s Step -4: Get the version of windows and the compiler it was last known to compile with. reply fransje26 34 minutes agorootparentStep -3.5: Do the service pack and .net framework update dance, wave a dead chicken, and hopefully, by shear luck, install them in the correct order. If not, uninstall and goto -4; Been there. Done that. reply groby_b 8 hours agorootparentprevStep 0 sounds so easy. Until you realize __time__ exists. Then you take that away, and you find out that some compiler heuristics might not be deterministic. Then you discover -frandom-seed - and go ballistic when you read \"but it should be a different seed for each sourcefile\" Then you figure out your linker likes emitting a timestamp in object files. Then you discover /Brepro (if you're lucky enough to use lld-link. Then you used to discover that Win7's app compat db expected a \"real\" timestamp, and a hash just won't do. (Thank God, that's dead now). This is usually the part where you start questioning your life choices. Then somebody comes to your desk and asks if you can also make partial rebuilds deterministic. On the upside, step 1 is usually quick, there will be no tests. reply dataflow 7 hours agorootparentI think (well, assumed) what they meant by deterministic builds was merely hermetic builds, which are easier. True determinism is overkill for step 0. reply rfoo 4 hours agorootparentAgreed. Though in the last 6 years I've seen at least one case where truly deterministic builds mattered: A performance bug only happens when a malloc() during init was not aligned to 32 bytes, glibc on x86_64 only guaranteed 16 bytes, but depending on what alloc / dealloc happened before it may just land on 32 bytes boundary. The alloc / dealloc sequence before that point was pretty deterministic, however there were a few strings containing __FILE__. And gitlab runner checked-out codes to a path with random number (or an index? I don't remember) without -ffile-prefix-map or $PWD trick so its length varies. reply groby_b 7 hours agorootparentprevOften yes. Sometimes, no. You haven't enjoyed C++ until you get reports of the app intermittently crashing, and your build at the same version just won't. But yes, if the goal is \"slap it all in a container\", that's probably good and at least somewhat reproducible. We aren't Python here! ;) reply boolemancer 7 hours agorootparent> Often yes. Sometimes, no. You haven't enjoyed C++ until you get reports of the app intermittently crashing, and your build at the same version just won't. That's okay, it's probably just some bank in a random country that requires some software package to be installed, presumably in the interest of security, which injects a dll into every process on the machine and unsurprisingly has a bug which causes your process to crash at random in only that part of the world. reply nightpool 7 hours agorootparentprevyeah I don't think OP is talking about byte perfect determinism, they just want CI not to explode. that's the triage goal, byte perfect determinism is not your first priority when stopping the bleeding on a legacy c++ project reply eschneider 12 hours agorootparentprevJust a note on legacy tests: Step 0.5: understand the tests. They need to be examined to see if they've rotted or not. Tests passing/failing doesn't really mean code under test works or not. The tests might have been abandoned under previous management and don't accurately reflect how the code is _supposed_ to be working. reply fransje26 33 minutes agorootparentLook at mister fancy here, having tests in his legacy code base. reply dataflow 10 hours agorootparentprevI'd put that under 2.5 or 3.5, if not later. You only really need to do it before you start modifying code, and it's a massive effort to understand a new codebase. Better pick the lower-hanging fruit (like corruption bugs) so you can at least stay sane when you run the tests and try to understand them. reply daemin 11 hours agorootparentprevProbably insert another Step 1: implement tests Be they simple acceptance tests, integration tests, or even unit tests for some things. reply microtherion 12 hours agoparentprev> Get the code to build clean with -Wall. This is fine, but I would strongly recommend against putting something like -Wall -Werror into production builds. Some of the warnings produced by compilers are opinion based, and new compiler versions may add new warnings, and suddenly the previous \"clean\" code is no longer accepted. If you must use -Werror, use it in debug builds. reply nanolith 44 minutes agorootparentThe compiler and toolchain is a dependency like any other. Upgrading to a new compiler version is an engineering task just like upgrading a library version. It must be managed as such. If this leads to new errors, then this becomes part of the upgrade management. Likewise, since the code generator and optimizers have changed, this upgrade must be tested like any other new feature or fix. Create an appropriate topic/feature branch, and dig in. reply ladberg 12 hours agorootparentprevThis is fixed by the suggestion right before it: > * Reproducible builds. The sanity you save will be your own. Pro-tip: wrap your build environment with docker (or your favorite packager) so that your tooling and dependencies become both explicit and reproducable. The sanity you save will be your own. Upgrading compiler versions shouldn't be done out-of-band with your normal PR process. reply jchw 12 hours agorootparentI agree that this helps, although I still think that in general, the default build should never do -Werror, since people may use other toolchains and it shouldn't surprise-break downstream (I'm pretty sure this is a problem Linux distros struggle with all the time..) If it does it only in your fully reproducible CI, then it should be totally fine, of course. reply eschneider 12 hours agorootparentThe scripted, packaged docker with toolchain dependencies and _is_ the build. If someone decides to use a different toolchain, the problems are on them. reply jchw 11 hours agorootparentYeah that works if you are not dealing with open source. If you are dealing with open source, though, it really won't save you that much trouble, if anything it will just lead to unnecessarily hostile interactions. You're not really obligated to fix any specific issues that people report, but shrugging and saying \"Your problem.\" is just non-productive and harms valuable downstreams like Linux distributions. Especially when a lot of new failures actually do indicate bugs and portability issues. reply saagarjha 52 minutes agorootparentIt doesn't even work outside of open source. I am running a prerelease toolchain almost all the time on my computer. If the project at work turns on -Werror, I immediately turn it off and store away the change. Of course this means that I send in code fixes for things that don't reproduce on other people's machines yet, but I literally never receive pushback for this. reply aseipp 8 hours agorootparentprevSupporting every Linux distribution and their small differences isn't free, and Linux distributions shipping things you haven't tested directly is also a way for users to get bitten by bugs or bad interactions, which they will then report to you directly anyway so you're responsible for it. It's complicated. It's happened plenty of times where e.g. I've run into an obscure and bad bug caused by a packaging issue, or a downstream library that wasn't tested -- or there's a developer who has to get involved with a specific distro team to solve bugs their users are reporting directly to them but that they can't reproduce or pinpoint, because the distro is different from their own environment. Sometimes these point out serious issues, but other times it can be a huge squeeze to only get a little juice. For some things the tradeoffs are much less clear, open-source or not e.g. a complex multi-platform GUI application. If you're going to ship a Flatpak to Linux users for example, then the utility of allowing any random build environments is not so clear; users will be running your produced binaries anyway. These are the minority of cases, though. (No, maybe not every user wants a Flatpak, but the developers also need to make decisions that balance many needs, and not everything will be perfect.) Half of the problem, of course, is C and C++'s lack of consistent build environments/build systems/build tooling, but that's a conversation for another day. That said, I generally agree with you that if you want to be a Good Citizen in the general realm of open-source C and C++ code, you should not use -Werror by default, and you should try (to whatever reasonable extent) to allow and support dependencies your users have. And try to support sanitizers, custom CFLAGS/CXXFLAGS, allow PREFIX and DESTDIR installation options, obey the FHS, etc etc. A lot of things have consolidated in the Linux world over the years, so this isn't as bad as it used to be -- and sometimes really does find legitimate issues in your code, or even issues in other projects. reply jchw 6 hours agorootparentAgain, you don't have to fix bugs that are reported, but treating it as invalid to use any compiler versions except for the exact ones that you use is just counterproductive. The \"utility\" of allowing \"any random build environment\" is that those random build environments are the ones that exist on your user's computers, and absent a particularly good reason why it shouldn't work (like, your compiler is too old, or literally broken,) for the most part it should, and usually, it's not even that hard to make it work. Adopting practices like defaulting -Werror -Wall on and closing bugs as WONTFIX INVALID because it's not any of the blessed toolchains gains you... not sure. I guess piece of mind from having less open issues and one less flag in your CI? But it is sure to be very annoying to users who have fairly standard setups and are trying to build your software; it's pretty standard behavior to report your build failures upstream, because again, usually it does actually signal something wrong somewhere. Developers are free to do whatever they want when releasing open source code. That doesn't mean that what they are doing is good or makes any sense. There are plenty of perfectly legal things that are utterly stupid to do, like that utterly bizarre spat between Home Assistant and NixOS. reply nicoburns 9 hours agorootparentprevC++ is super annoying in this way. Many other languages (e.g Rust) only have one compiler and good portability out of the box which completely avoids this problem. And other ecosystems that do have multiple implementation (e.g. JavaScript) seem to have much better compatibility/interop such that it's not typically a problem you have to spend much if any time on in practice. reply jxramos 1 hour agorootparentI'm curious what sort of CPUs and OSes do those languages run on. C++ runs on all sorts of obscure real time OSes, all the standard mainstream ones as well as on embedded equipment and various CPUs, but a lot of that is possible because of the variety of compilers. reply adastra22 1 hour agorootparentprevI’ve had rust projects with strict clippy rules break when rustc is upgraded. reply galangalalgol 11 hours agorootparentprevI would do wall wextra and werror. Again mostly for my own sanity. But I'd wait to add werror until they were all fixed so regression testing would continue as the warnings got fixed. Cpp_check and clang tidy would also eventually halt the pipeline. And *san on the tests as compiled in both debug and O3 with a couple compilers. reply charcircuit 8 hours agorootparentprevChanging other dependencies can also cause the build to break. The best thing to do is to use the dependencies the project specifies. reply jchw 6 hours agorootparentTechnically changing literally anything, including the processor microarchitecture that the developer originally tested the code on, could easily cause a real-world breakage. That doesn't mean it should, though. Most libraries not written by Google have some kind of backwards compatibility policy. This is for good reasons. For example, if Debian updates libpng because there's a new RCE, it's ideal if they can update every package to the same new version of libpng all at once. If we go to the extreme of \"exact dependencies for every package\", then this would actually mean that you have to update every dependent package to a new release that has the new version of libpng, all at the same time, across all supported versions of the distribution. Not to mention, imagine the number of duplicate libraries. Many Linux distros, including Debian, have adopted a policy of only having one version of any given library across the whole repo. As far as I understand, that even includes banning statically linked copies, requiring potentially invasive patching to make sure that downstream packages use the dynamically linked system version. And trust me, if they want to do this, they *will* do this. If they can do it for Chromium, they sure as hell can do it for literally any package. There's a balance, of course. If a distro does invasive patching and it is problematic, I think most people will be reasonable about it and accept that they need to report the issue to their distribution instead. Distros generally do accept bugs for the packages that they manage, and honestly for most packages, by the time a bug gets to you, there is a pretty reasonable chance that it's actually a valid issue, so throwing away the issue simply because it came from someone running an \"unofficial\" build seems really counterproductive and definitely not in the spirit of open source. Reproducibility is good for many reasons. I do not feel it is a good excuse to just throw away potentially valid bug reports though. It's not that maintainers are under any obligation to actually act on bug reports, or for that matter, even accept them at all in the first place, but if you do accept bugs, I think that \"this is broken in new version of Clang\" is a very good and useful bug report that likely signals a problem. reply charcircuit 6 hours agorootparent>For example, if Debian updates libpng because there's a new RCE, it's ideal if they can update every package to the same new version of libpng all at once. It Debian is upgrading a dependency instead of a developer, then Debian should be ready to fix any bugs they introduce. >then this would actually mean that you have to update every dependent package to a new release that has the new version of libpng, all at the same time, across all supported versions of the distribution This is already how it works. All vulnerable programs make an update and try to hold off in releasing it until near an embargo date. You don't have to literally update them all at the same time. It's okay of some are updated at different times than others. >Not to mention, imagine the number of duplicate libraries. Duplicate libraries are not an issue. >Many Linux distros, including Debian, have adopted a policy of only having one version of any given library across the whole repo. This is a ridiculous policy to me as you are forcing programs to use dependencies they were not designed for. This is something that should be avoided as much as possible. >by the time a bug gets to you, there is a pretty reasonable chance that it's actually a valid issue That doesn't mean there isn't damage done. There are many people who consider kdenlive an unstable program that constantly crashes because of distros shipping it with the incorrect dependencies. This creates reputational damage. reply jchw 5 hours agorootparent> It Debian is upgrading a dependency instead of a developer, then Debian should be ready to fix any bugs they introduce. That's what the Debian Bug Tracking System is for. However, if the package is actually broken, and it's because e.g. it uses the dependency improperly and broke because the update broke a bad assumption, then it would ideally be reported upstream. > This is already how it works. All vulnerable programs make an update and try to hold off in releasing it until near an embargo date. You don't have to literally update them all at the same time. It's okay of some are updated at different times than others. That's not how it works in the vast majority of Linux distributions, for many reasons, such as the common rule of having only one version, or the fact that Debian probably does not want to update Blender to a new major version because libpng bumped. That would just turn all supported branches of Debian effectively into a rolling release distro. > Duplicate libraries are not an issue. In your opinion, anyway. I don't really think that there's one way of thinking about this, but duplicate libraries certainly are an issue, whether you choose to address them or not. > This is a ridiculous policy to me as you are forcing programs to use dependencies they were not designed for. This is something that should be avoided as much as possible. Honestly, this whole tangent is pointless. Distributions like Debian have been operating like this for like 20+ years. It's dramatically too late to argue about it now, but if you're going to, this is not exactly the strongest argument. Based on this logic, effectively programs are apparently usually designed for exactly one specific code snapshot in time of each of its dependencies. So let's say I want to depend on two libraries, and both of them eventually depend on two different but compatible versions of a library, and only one of them can be loaded into the process space. Is this a made-up problem? No, this exact thing happens constantly, for example with libwayland. Of course you can just pick any newer version of libwayland and it works absolutely perfectly fine, because that's why we have shared libraries and semver to begin with. We solved this problem absolutely eons ago. The solution isn't perfect, but it's not a shocking new thing, it's been the status quo for as long as I've been using Linux! > That doesn't mean there isn't damage done. There are many people who consider kdenlive an unstable program that constantly crashes because of distros shipping it with the incorrect dependencies. This creates reputational damage. If you want your software to work better on Linux distributions, you could always decide to take supporting them more seriously. If your program is segfaulting because of slightly different library versions, this is a serious problem. Note that Chromium is a vastly larger piece of software than Kdenlive, packaged downstream by many Linux distributions using this very same policy, and yet it is quite stable. For particularly complex and large programs, at some point it becomes a matter of, OK, it's literally just going to crash sometimes, even if distributions don't package unintended versions of packages, how do we make it better? There are tons of avenues for this, like improving crash recovery, introducing fault isolation, and simply, being more defensive when calling into third party libraries in the first place (e.g. against unexpected output.) Maintainers, of course, are free to complain about this situation, mark bugs as WONTFIX INVALID, whatever they want really, but it won't fix their problem. If you don't want downstreams, then fine: don't release open source code. If you don't want people to build your software outside of your exact specification because it might damage its reputation, then simply do not release code whose license is literally for the primary purpose of making what Linux distributions do possible. You of course give up access to copyleft code, and that's intended. That's the system working as intended. I believe that ultimately releasing open source code does indeed not obligate you as a maintainer to do anything at all. You can do all manner of things, foul or otherwise, as you please. However, note that this relationship is mutual. When you release open source code, you relinquish yourself of liability and warranty, but you grant everyone else the right to modify, use and share that code under the terms of the license. Nowhere in the license does it say you can't modify it in specific ways that might damage your program's reputation, or even yours. reply charcircuit 3 hours agorootparent>That's what the Debian Bug Tracking System is for. Software should be extensively tested and code review should be done before it gets shipped to users. Most users don't know about the Debian Bug Tracking system, but they do know about upstream. >Honestly, this whole tangent is pointless. Distributions like Debian have been operating like this for like 20+ years. It's dramatically too late to argue about it now, but if you're going to, this is not exactly the strongest argument. It's not too late as evidence by the growth of solutions like appimage and flatpak which allows developers to avoid this. >So let's say I want to depend on two libraries, and both of them eventually depend on two different but compatible versions of a library, and only one of them can be loaded into the process space. Is this a made-up problem? No, this exact thing happens constantly, for example with libwayland. Multiple versions of a library can be loaded into the same address space. Developers can choose to have their libraries support a range of versions. >that's why we have shared libraries and semver to begin with Hyrum's Law. Semver doesn't prevent breakages on minor bumps. reply pizlonator 9 hours agorootparentprevI think this depends on a bunch of stuff. - Who are the consumers of the source code, i.e. who will ever check it out and build it? Sometimes, it's just one person. Sometimes, it's a team of engineers. In that case, -W -Werror is fine. - How does a warning being reported make the engineers on the team feel? If the answer is, \"Hold my beer for five minutes while I commit a fix\", then -W -Werror might be the right call. I've been on projects like that and some of them had nontrivial source code consumers. - How easy is it to hack the build system? Some projects have wonderfully laid out build systems. If that's the case and -W -Werror is the default, then it's not hard to go in there and change the default, if the -Werror creates problems. - Does the project have a facility (in the build system) and policy (as a matter of process) to just simply add -Wno-blah-blah as the immediate fix for any new warning that arises? I've seen that, too. (I'm using -Werror in some parts of a personal project. If you're a solo maintainer of a codebase that can be built that way, then it's worth it - IMO much lower cognitive load to never have non-error warnings. The choice of what to do when the compiler complains is a more straightforward choice.) reply __d 2 hours agorootparentprevThat's a feature! New warnings added to new compiler versions can identify problems that weren't previously detected. You _want_ those to -Werror when they happen, so you can fix them if they need it. Changing a compiler version is a task that has to be resourced appropriately. Part of that is dealing with any fallout like this. Randomly updating your compiler is just asking for trouble. reply adastra22 1 hour agorootparentIt is certainly not a feature because it make all infrastructure including just regular old checkout-and-build workflows break for historical versions of the code. It’s so annoying to have to checkout an older version and then have to go disable -Wall -Werror everywhere just to get the damn thing to build. Keep master clean of any warnings, for sure. But don’t put it straight into the build system defaults. reply dheera 6 hours agoparentprev> wrap your build environment with docker For microservices it is fine, but you can't always deploy everything else with docker, especially for people who want to use your app inside a docker. Docker-in-docker is a situation that should never happen. Containers are nice but they're a horrible way to pretend the problem doesn't exist. Bundle all the dependencies and make sure it doesn't depend on 5 billion things being in /usr/lib and having the correct versions. reply cshokie 5 hours agorootparentNot the OP, but I don’t think they meant that the build output is in a container. They meant that the thing you use to compile the code is in docker (and you just copy out the result). That would help ensure consistency of builds without having any effect on downstream users. reply golergka 11 hours agoparentprevGreat advice. Almost all of it applies to any programming language. reply SleepyMyroslav 12 hours agoparentprevI would not call that 'controversial'. In the internet days people call this behavior trolling for a reason. The punchline about rewriting code in different language gives an easy hint at where this all going. PS. I have been in the shoes of inheriting old projects before. And I hope i left them in better state than they were before. reply legacybob 2 minutes agoprevEvery morning I wake up in my legacy bed, before taking breakfast using a legacy coffee cup. I then take a shower using - you guessed it - legacy shower taps (after all, I do live in a legacy building). I then sit on my legacy chair to browse the Internet and read about brand new programming things (through a legacy monitor). reply Jtsummers 17 hours agoprevI'd swap 2 and 3. Getting CI, linting, auto-formatting, etc. going is a higher priority than tearing things out. Why? Because you don't know what to tear out yet or even the consequence of tearing them out. Linting (and other static analysis tools) also give you a lot of insight into where the program needs work. Things that get flagged by a static analysis tool (today) will often be areas where you can tear out entire functions and maybe even classes and files because they'll be a re-creation of STL concepts. Like homegrown iterator libraries (with subtle problems) that can be replaced with the STL algorithms library, or homegrown smart pointers that can just be replaced with actual smart pointers, or replacing the C string functions with C++'s on string class (and related classes) and functions/methods. But you won't see that as easily until you start scanning the code. And you won't be able to evaluate the consequences until you push towards more rapid test builds (at least) if not deployment. reply jasonwatkinspdx 12 hours agoparentYeah, I've done a fair bit of agency work dropping in to rescue code bases, and the first thing I do is run unit tests and check coverage. I add basic smoke tests anywhere they're missing. This actually speeds me up, rather than slowing me down, because once I have reasonably good coverage I can move dramatically faster when refactoring. It's a small investment that pays off. reply dralley 17 hours agoparentprevOn the flip side, auto-formatting will trash your version history and impede analysis of \"when and why was this line added\". reply Jtsummers 17 hours agorootparentI'm not hardcore on auto-formatters, but I think their impact on code history is negligible in the case of every legacy system I've worked on. The code history just isn't there. These aren't projects that used git until recently (if at all). Before that they used something else, but when they transitioned they didn't preserve the history. And that's if they used any version control system. I've tried to help teams whose idea of version control was emailing someone (they termed them \"QA/CM\") to make a read-only backup of the source directory every few months (usually at a critical review period in the project, so a lot of code was changed between these snapshots). That said, sure, skip them if you're worried about the history getting messed up or use them more selectively. reply bear8642 11 hours agorootparent>I think their impact on code history is negligible in the case of every legacy system I've worked on. The code history just isn't there. Not sure if I agree here or not - whilst yes, the history isn't there, if it's a small enough team you'll have a good guess at who wrote it. Definitely found I've learnt the style of colleages so know who to ask just from the code outline. reply Jtsummers 11 hours agorootparentLegacy systems that you inherit don't have people coming with them very often. That's part of the context of this. You often don't have people to trace it back to or at least not the people who actually wrote it (maybe someone who worked with them before they got laid off a decade ago), and reformatting the code is not going to make it any harder to get answers from people who aren't there. reply KerrAvon 14 hours agorootparentprevSVN was a thing by the mid-2000's, and history from that is easy to preserve in git. Just how old are the sourcebases in question? (Not to shoot the messenger; just like, wow.) edit:typo reply olvy0 2 hours agorootparentI maintain a C++ codebase that was originally written in 1996, and is mission critical for my organization. Originally maintained in Visual Sourcesafe, then in TFS source control, and now git. Some parts of it were rewritten (several times) in C#, but the core is still C++. I was very worried when we transitioned to git that history will not be preserved and tried to preserve it, but it proved too much hassle so I dropped it. In fact that proved not to be a problem. Well, not a problem for me, since I remember all the history of the code and all the half forgotten half baked features and why they are there. But if I'm gone then yes, it's going to be a problem. It's in a dire need for a rewrite, but this has been postponed again and again. reply pyuser583 3 hours agorootparentprevI've had issues doing decent copies from SVN to GIT. They both have different ideas about user identity, and how fragmented it can be. reply varjag 13 hours agorootparentprevThe first large C++ project I worked on in mid-1990s was basically preserving a bunch of archived copies of the source tree. CVS was a thing but not on Windows, and SourceSafe was creating more problems than it been solving. reply Jtsummers 13 hours agorootparentprevSome of these systems dated back to the 1970s. The worst offenders were from the 1980s and 1990s though. It's all about the team or organization and their laziness or non-laziness. reply jamesfinlayson 8 hours agorootparentprevI looked at a C++ codebase from 1997 at a previous job - I don't know much about the history but comments in one of the old files tracked dates and changes to 2001. Not sure what happened after that but in 2017 someone copy-pasted the project from TFS to git and obliterated the prior history. reply Pfiffer 13 hours agorootparentprevI've heard a lot of stories about mid-90s codebases for sure reply skrebbel 17 hours agorootparentprevYou can ignore commits from git blame by adding them to a .gitattributes file. This is assuming Git of course, which is not a given at all for the average legacy c++ codebase. reply fransje26 30 minutes agorootparentGood to know. Thanks for the tip! reply lpapez 12 hours agorootparentprevYou can instruct git to ignore specific commits for blame and diff commands. See \"git blame ignore revs file\". Intended use is exactly to ignore bulk changes like auto formatting. reply westurner 11 hours agorootparent+1 man git-blame git help blame https://git-scm.com/docs/git-blame reply duped 11 hours agorootparentprevThis is another reason why you should track important information in comments alongside the code instead of trusting VCS to preserve it in logs/commit messages, and to reject weird code missing comments from being merged. Not saying that fixes decades of cruft because you shouldn't change files without good reason and non-white space formatting is not a good reason, but I'm mentioning it because I've seen people naively belief bullshit like \"code is self explanatory\" and \"the reason is in the commit message\" Just comment your code folks, this becomes less of a problem reply mb7733 9 hours agorootparentprevHow does reformatting trash the history? It's one extra commit.. I guess if it splits or combines lines that could cause some noise if you really want the history of a single line... But that happens all the time, and I don't see how it would really prevent understanding the history. You can always do a blame on a range of lines. Maybe I'm missing something though, genuinely curious for a concrete example where reformatting makes it hard to understand history! reply PreachSoup 15 hours agorootparentprevOn per file level it's just 1 commit. It's not really a big deal reply exDM69 13 hours agorootparentprevclang-format can be applied to new changes only, for this very reason. Adding it will remove white space nitpicking from code review, even if it isn't perfect. reply IshKebab 12 hours agorootparentprevI believe you can configure `git blame` to skip a specific commit. But in my experience it doesn't matter anyway for two reasons: 1. You're going to reformat it eventually anyway. You're just delaying things. The best time to plant a tree, etc. 2. If it's an old codebase and you're trying to understand some bit of code you're almost always going to have to walk through about 5 commits to get to the original one anyway. One extra formatting commit doesn't really make any difference. reply thrwyexecbrain 12 hours agoparentprevI would absolutely not recommend auto-formatting a legacy codebase. In my experience large C++ projects tend to have not only code generation scripts (python/perl/whatever) but also scripts that parse the code (usually to gather data for code generation). Auto formatting might break that. I have even seen some really cursed projects where the _users_ parsed public header files with rather fragile scripts. reply Jtsummers 12 hours agorootparentI was listing the items in the original article's #3 and saying I'd move them up to #2 before I'd go about excising portions of the project, the original #2. I still stand by that. But you can read my other comment where I don't really defend auto-formatting to see that I don't care either way. I made it about four hours ago so maybe you missed it if you didn't refresh the page in the last few hours. reply btown 17 hours agoparentprevCI is different from the others, here! At minimum, building a \"happy path(s)\" test harness that can run with replicable results, and will run on every one of your commits, is a first step, and also helps to understand the codebase. And you're jumping around - and you'll have to! - odds are you'll have a bunch of things changed locally, and might accidentally create a commit that doesn't separate out one concern from another. CI will be a godsend at that point. reply broken_broken_ 17 hours agoparentprevFair point! reply politician 17 hours agoparentprevNit: The post scopes \"tearing things out\" to dead code as guided by compiler warnings and unsupported architectures. If going the route, I'd recommend commenting out the lines rather than removing them outright to simplify the diffs at least until you're ready to squash and merge the branch. reply SAI_Peregrinus 14 hours agorootparentBetter to use `#if` or `#ifdef` to prevent compilation. C & C++ don't support nested comments, so you can end up with existing comments in the code ending the comment block. reply kccqzy 13 hours agorootparentI think `#if` and `#ifdef` are not good ideas because they prevent the compiler from seeing them in the first place. A better solution is just `if (false)` which is nestable, and the code is still checked by the compiler so it won't bit rot. reply vijucat 11 hours agoprevNot mentioned were code comprehension tools / techniques: I used to use a tool called Source Navigator (written in Tcl/tk!) that was great at indexing code bases. You could then check the Call Hierarchy of the current method, for example, then use that to make UML Sequence Diagrams. A similar one called Source Insight shown below [1]. And oh, notes. Writing as if you're teaching someone is key. Over the years, I got quite good at comprehending code, even code written by an entire team over years. For a brief period, I was the only person actively supporting and developing an algorithmic trading code base in Java that traded ~$200m per day on 4 or 5 exchanges. I had 35 MB of documentation on that, lol. Loved the responsibility (ignoring the key man risk :|). Honestly, there's a lot of overengineering and redundancy in most large code bases. [1] References in \"Source Insight\" https://d4.alternativeto.net/6S4rr6_0rutCUWnpHNhVq7HMs8GTBs6... reply t43562 20 minutes agoprevI have had this problem before. Some of the problems were the constraints - like we were supporting very old platforms and therefore had to use very old compilers and old language features. IMO the best thing we could have done overall would have been to know what our customers were using/not using actively and to just restrict support at some version level for those older customers who were happily using the old version and say to them: well no new features for you. They would very likely have been fine with that and we could have struck windows 2k or redhat 6 off our list of supported platforms for future releases. Given the ability to use new tools we could have done amazing things - getting clang to be able to compile the code would have let us use all those wonderful tools like clang-format. I even think we could have done some automated refactoring with a tool and I'd have liked to explore that. Regexp search and replace are too primitive but something that could find patterns in the AST and then replace those would have been amazing. I actually recall clang having something like this. Most of the problems were just with understanding the minds of the developers - they were doing something difficult and at a level of complexity that somewhat overmatched the problem most of the time but the complexity was there to handle the edge cases. So for me I wanted to go around adding comments to the files and classes as I understood bits of it. I was working with one of the original developers who was of course not at all interested in anyone understanding it or making it clearer and this kind of effort tended to get shot down. If you don't have good tests you're dead in the water. I have twice inherited python projects without tests at all and those were a complete nightmare until I added some. One was a long running build process in which unit tests were only partially helpful. Until I came up with a fake android source tree that could build in under a minute I was extremely handicapped. Once I had that everything started to get much better. My favorite game ... is an open source C++ thing called warzone2100 - no tests. It's not easy to make changes with any confidence. I imagine to myself that one day my contribution will be to add some. The problem is that I cannot imagine the current developers taking all that kindly to it. Some people get to competence in a codebase and leave it at that. reply mk_chan 13 hours agoprevI’m not sure why there’s so much focus on refactoring or improving it. When a feature needs to be added that can just be tacked onto the code, do it without touching anything else. If it’s a big enough change, export whatever you need out of the legacy code (by calling an external function/introducing a network layer/pulling the exact same code out into a library/other assorted ways of separating code) and do the rest in a fresh environment. I wouldn’t try to do any major refactor unless several people are going to work on the code in the future and the code needs to have certain assumptions and standards so it is easy for the group to work together on it. reply dj_mc_merlin 13 hours agoparentThe post argues against major refactors. The incremental suggestions it gives progressively make the code easier to work with. What you suggest works until it doesn't -- something suddenly breaks when you make a change and there's so much disorganized stuff that you can't pinpoint the cause for much longer than necessary. The OP is basically arguing for decluttering in order to be able to do changes easier, while still maintaining cohesion and avoiding a major rewrite. reply bluGill 12 hours agoparentprevThe right answer depends on the future. I've worked on C++ code where the replacement was already in the market but we had to do a couple more releases of the old code. Sometimes it is adding the same feature to both versions. There is a big difference in how you treat code that you know the last release is coming soon and code where you expect to maintain and add features for a few more decades. reply mk_chan 3 hours agorootparentYes, you have to expect the future (or even better if your manager/boss already has expectations you can adopt to begin with) and then choose the right way to tackle the changes required. That's why I laid out 3 possible cases the last of which points out that I prefer to refactor primarily when there's a lot of work incoming on the codebase. Personally, I don't see much value in refactoring code significantly if you alone are going to be editing it because refactoring for ease of editing + the cost of editing in the refactored codebase is often less than just eating the higher cost of editing in the pre-refactored codebase and you don't reap the scaling benefits of refactoring as much. However, like I mentioned in the above paragraph, it depends. In the end it's all about managing the debt to get the most out of it in a _relatively_ fixed time period. reply FpUser 11 hours agoparentprev>\"When a feature needs to be added that can just be tacked onto the code, do it without touching anything else.\" In few lucky cases. In real life new feature is most likely change in behavior of already existing one and suddenly you have to do some heavy refactoring in numerous places. reply convolvatron 9 hours agorootparentif you're going to own it for the foreseeable future. then own it. learn it, refactor it, test the hell of out of it. otherwise you're never going to be able to debug or extend it. one thing I always do is throwaway major refactors. its the fastest way for me to learn what the structure is, what depends on what, and what's really kinky. and I might just learn enough to do it for real should it become necessary. reply Night_Thastus 17 hours agoprev>worry not, by adding std::cmake to the standard library and you’ll see how it’s absolutely a game changer I'm pretty sure my stomach did somersaults on that. But as for the advice: >Get out the chainsaw and rip out everything that’s not absolutely required to provide the features your company/open source project is advertising and selling I hear you, but this is incredibly dangerous. Might as well take that chainsaw to yourself if you want to try this. It's dangerous for multiple reasons. Mainly it's a case of Chesterton's fence. Unless you fully understand why X was in the software and fully understand how the current version of the software is used, you cannot remove it. A worst case scenario would be that maybe a month or so later you make a release and the users find out an important feature is subtly broken. You'll spend days trying to track down exactly how it broke. >Make the project enter the 21st century by adding CI, linters, fuzzing, auto-formatting, etc It's a nice idea, but it's hard to do. One person is using VIM, another is using emacs, another is using QTCreator, another primarily edits in VSCode.. Trying to get everyone on the same page about all this is very, very hard. If it's an optional step that requires that they install something new (like commit hook) it's just not going to happen. Linters also won't do you any good when you open the project and 2000+ warnings appear. reply electroly 17 hours agoparent> It's a nice idea, but it's hard to do. One person is using VIM... The things the author listed there are commonly not IDE integrated. I've never seen a C++ development environment where cpplint/clang-tidy and fuzzers are IDE integrated, they're too slow to run automatically on keystrokes. Auto-formatting is the only one that is sometimes integrated. All of this stuff you can do from the command line without caring about each user's chosen development environment. You should definitely at least try rather than giving up before you start just because you have two different text editors in use. This is C++; if your team won't install any tools, you're gonna have a bad time. Consider containerizing the tools so it's easier. reply throwaway2037 8 hours agorootparent> I've never seen a C++ development environment where cpplint/clang-tidy and fuzzers are IDE integrated CLion from JetBrains has clang-tidy integrated (real-time). reply gpderetta 10 hours agorootparentprevClangd will happily run clang-tidy as part of completion/compile-as-you-type/refactor/auto independent. On the editor/IDE of your choose. I wouldn't call it fast, but it is quite usable. reply j-krieger 12 hours agoparentprev> It's a nice idea, but it's hard to do. One person is using VIM, another is using emacs, another is using QTCreator, another primarily edits in VSCode.. Trying to get everyone on the same page about all this is very, very hard. I must have missed the memo where I could just say no to basic things my boss requires of me. You know, the guy that pays my salary. reply saagarjha 47 minutes agorootparentAs others have mentioned, none of these things actually change your development workflow. But if they did, you do have the ability to say no. If your boss fails to understand that you have an environment that you're productive in, that sounds like a bad place to work. reply zer00eyz 17 hours agoparentprev>> It's a nice idea, but it's hard to do. One person is using VIM, another is using emacs, another is using QTCreator, another primarily edits in VSCode.. Trying to get everyone on the same page about all this is very, very hard. This is what's wrong with our industry, and it's no longer an acceptable answer. We're supposed to be fucking professional, and if a job needs to build a tool chain from the IDE up we need to learn to use it and live with it. Built on my machine, with my IDE, the way I like it and it works is not software. It's arts and fucking crafts. reply otabdeveloper4 1 hour agorootparent> every single carpenter in the world should use the exact same make and model of saw, for, uh, professionalism reasons reply saagarjha 46 minutes agorootparentprevSoftware is arts and crafts :) reply cratermoon 14 hours agorootparentprevIf you're saying everyone should agree on the same IDE and personal development toolset, I disagree, sort of. The GP was suggesting the effort to add CI, linters, fuzzing, auto-formatting, etc was too hard. If that can be abandoned entirely, perhaps the legacy codebase isn't providing enough value, and the effort to maintain it would be better spent replacing it. But the implication is that the value outweighs the costs. Put all the linters, fuzzing, and format checking in an automated build toolchain. Allow individuals to work how they want, except they can't break the build. Usually this will reign in the edge cases using inadequate tools. The \"built on my machine, with my IDE, the way I like it and it works\" is no longer the arbiter of correct, but neither does the organization have to deal with the endless yak shaving over brace style and tool choice. reply eropple 11 hours agorootparent> neither does the organization have to deal with the endless yak shaving over brace style and tool choice I hear you, but an organization that fears this, instead of Just Pick Something And Deal With It, is an organization that probably doesn't have the right people in it to succeed at any task more arduous than that. reply cratermoon 4 hours agorootparentConversely, and organization that imposes arbitrary choices and isn't capable of allowing people do use the tools they know best probably doesn't attract the best people. There are many different kinds of hammers, and making everyone who uses hammers use the same kind is, to say the least, counter productive. reply aaronbrethorst 17 hours agoparentprevAn optional step locally like pre-commit hooks should be backed up by a required step in the CI. In other words: the ability to run tests locally, lint, fuzz, format, verify Yaml format, check for missing EOF new lines, etc, should exist to help a developer prevent a CI failure before they push. As far as linters causing thousands of warnings to appear on opening the project, the developer adding the linter should make sure that the linter returns no warnings before they merge that change. This can be accomplished by disabling the linter for some warnings, some files, making some fixes, or some combination of the above. reply theamk 5 hours agoparentprev> It's dangerous for multiple reasons. Mainly it's a case of Chesterton's fence. Unless you fully understand why X was in the software... If this is a function that no one links to, and your project does not mess with manual dynamic linking (or the function is not exposed), then it's pretty safe to remove it. If it's internal utility which does not get packaged into final release package, it is likely be safe to remove too. If it's a program which does not compile because it requires Solaris STREAMS and your targets are Linux + MacOS - kill it with fire. (Of course removing function calls, or removing functionality that in-use code depends on, is dangerous. But there is plenty of stuff which has no connection to main code) reply Kamq 10 hours agoparentprev> It's a nice idea, but it's hard to do. One person is using VIM, another is using emacs, another is using QTCreator, another primarily edits in VSCode.. Trying to get everyone on the same page about all this is very, very hard. Bullshit, all of these (and additionally C lion) are fairly easy to configure these for, with the possible exception of QTCreator (not a ton of experience on my end). Just make it a CI requirement, and let everyone figure it out for their own tools. If you can't figure that out, you get to run it as a shell script before you do your PRs. If you can't figure that out, you probably shouldn't be on a legacy C++ project. reply IshKebab 12 hours agoparentprev> One person is using VIM, ... I don't get your point. You know you can autoformat outside editors right? Just configure pre-commit and run it in CI. It's trivial. > If it's an optional step that requires that they install something new (like commit hook) it's just not going to happen. It will because if they don't then their PRs will fail CI. This is really basic stuff, but knowledge of how to do CI and infra right does seem to vary massively. reply keepamovin 17 hours agoprevIt's funny. My first step would be 0. You reach out to the previous maintainers, visit them, buy them tea/beer and chat (eventually) about the codebase. Learned Wizards will teach you much. But I didn't see that anywhere. I think the rest of the suggestions (like get it running across platform, get tests passing) are useful stress tests likely to lead you to robustness and understanding however. But I'd def be going for that sweeet, sweet low-hangin' fruit of just talking to the ol' folks who came that way before. Haha :) reply fransje26 9 minutes agoparent> You reach out to the previous maintainers, visit them I could have brought them flowers, and shared a moment of silence contemplating eternity. I don't know if it would significantly have helped understanding the code base though.. reply GuB-42 17 hours agoparentprevI wouldn't make it the first step. If you do, you will probably waste their time more than anything. Try to work on it a little bit first, and once you get stuck in various places, now you can talk to the previous maintainers, it will be much more productive. They will also appreciate the effort. reply keepamovin 4 hours agorootparentYeah, that's fair enough. I guess since we're already zero-indexed maybe my -1 step is Prep. Hahaha! :) reply guhidalg 11 hours agorootparentprevThere’s a fine balance with no right or wrong answer. Previous maintainers will appreciate if you spent literally more than a second trying to understand before you reach out to them, but for your own sanity you should know when it’s time to stop and call for help. reply lelanthran 2 hours agoparentprev> It's funny. My first step would be > 0. You reach out to the previous maintainers, visit them, buy them tea/beer and chat (eventually) about the codebase. Learned Wizards will teach you much. Have you ever tried that? This is legacy code. Even if the handover was yesterday, they cannot tell you about anything useful they did more than 6m in the past. And that's the best-case scenario. The common-case is \"That's the way I got it when it was handed over to me\" answer to every question you ask. reply theamk 16 hours agoparentprevMaybe do a quick look at codebase first so you can identify biggest WTF's and ask about them. After all, if you have inherited a codebase with no tests, with build process which fails every other time, with unknown dependency info, and which can only be built on a single server with severely outdated OS... are you sure the previous maintainer is a real wizard and all the problems are result of not enough time? Or are they a \"wizard\" who keep things broken for job security and/or because they don't want to learn new things? reply vaylian 1 hour agorootparentEven if that person is not a great archwizard, they still have more experience in that project than you and you will probably learn some things that will make your life less miserable, because you will better understand what to expect and what kind of failed solutions have been tried before. reply keepamovin 4 hours agorootparentprevYes! Good idea. Locking in at -1. Hahah! :) reply Night_Thastus 17 hours agoparentprevIME, this only works if you can get regular help from them. A one-off won't help much at all. reply mellutussa 17 hours agorootparent> A one-off won't help much at all. Monumentally disagree. One-off session with a guy who knows the codebase inside out can save you days of research work. Plus telling you all about the problematic/historical areas. reply Night_Thastus 17 hours agorootparentI'm just stating my experience. A single day, if they still have access to the codebase might be able to clear up some top-level concepts. But the devil is in all the tiny details. What is this tiny correction factor that was added 20 years ago? Why was this value cut off to X decimals? Why didn't they just do Y here? Why do we override this default behavior? It's tens of thousands of tiny questions like that which you can't ask until you're there. reply slingnow 11 hours agorootparentI don't understand what you're saying. Clearly both types of meetings (one-off vs recurring) would be helpful. The one-off may save you days/weeks of research, but it seems like you're not satisfied with that unless you can answer every single minor question you might have across the entire codebase. reply Night_Thastus 5 hours agorootparentI'm saying that if you're maintaining a code base for years, a single day's explanations won't do much of anything. It's a drop in the bucket. It's not a bad thing, and it's certainly good to do, but it's not a solution to the problem. reply mellutussa 2 hours agorootparentIf your granularity for a task is measured in years then you have a much different and harder problem. Effectively everything becomes a \"drop in the bucket\". reply keepamovin 4 hours agorootparentprevExactly. Someone to guide you in the right path. Gonna help a lot! Hahaha :) reply keepamovin 17 hours agorootparentprevYeah you need to cultivate those relationships. But with a willing partner that first session will take you from 0 to 1 :) reply Joel_Mckay 17 hours agorootparentprevI've always found discussing why former employees left a project incredibly enlightening. They will usually explain the reality behind the PR given they are no longer involved in the politics. Most importantly they will often tell you your best case future with a firm. Normally, employment agreements specifically restrict contact with former staff, or discussions of sensitive matters like compensation packages. C++ is like any other language, in that it will often take 3 times longer to understand something than re-implement the same. If you are lucky, than everything is a minimal API lib, and you get repetitive examples of the use cases... but the cooperative OSS breadcrumb model almost never happens in commercial shops... Legacy code bases can be hell to work with, as you end up with the responsibility for 14 years of IT kludges. Also, the opinions from entrenched lamers on what productivity means will be painful at first. Usually, with C++ it can become its own project specific language variant (STL or Boost may help wrangle the chaos). You have my sympathy, but no checklist can help with naive design inertia. Have a wonderful day. =) reply saagarjha 34 minutes agorootparentNote that depending on your jurisdiction discussion of compensation may be a right protected by law. reply keepamovin 4 hours agorootparentprevUh, yeah, that's a great idea, too! That's the big question. What happened to the previous team? Can give you org insights as well as code ones. Info on company strategy, priorities, work cadence. Actually a pretty good open ended conversation opener! Hahaha! :) reply raverbashing 17 hours agoparentprevEasier said than done My step 0 would be: run it through an UML tool to get a class diagram and other diagrams. This will help you a lot. > Get the tests passing on your machine Tests? On a C++ codebase? I like your optimism, rs reply Joel_Mckay 17 hours agorootparentNo one has time to maintain the UML in production. You may luck out with auto-generated documentation, but few use these tools properly (javadoc or doxygen). =) reply cratermoon 16 hours agorootparentThe GP said nothing about keeping and maintaining it, only generating it. Use it to understand the codebase, then archive it or throw it out. reply Jtsummers 16 hours agorootparentExactly. You inherited 500k SLOC of C++ that grew together since 1985. You don't know the interconnections between the classes that have accumulated in that time. It was also developed by multiple teams, and likely had very different approaches to OO during these past nearly 40 years. The UML diagrams won't tell you everything, but they will tell you things like the inheritance hierarchy (if this was a 1990s C++ project it's probably pretty nasty), what classes are referenced by others via member variables, etc. This can be hugely informative when you want to transform a program into something saner. reply Joel_Mckay 16 hours agorootparentI always interpreted most polymorphism as sloppy context-specific state-machine embedding, and fundamentally an unmaintainable abomination from OOP paradigms. OO requires a lot of planning to get right (again, no shop will give your team time to do this properly), and in practice it usually degenerates into spiral development rather quickly ( 3. Make the project enter the 21st century by adding CI, linters, fuzzing, auto-formatting, etc I would break this down: a) CI - Ensure not just you can build this, but it can be built elsewhere too. This should prevent compile-based regressions. b) Compiler warnings and static analysers - They are likely both smarter than you. When it says \"warning, you're doing weird things with a pointer and it scares me\", it's a good indication you should go check it out. c) Unit testing - Set up a series of tests for important parts of the code to ensure it performs precisely the task you expect it to, all the way down to the low level. There's a really good chance it doesn't, and you need to understand why. Fixing something could cause something else to blow up as it was written around this bugged code. You also end up with a series of regression tests for the most important code. n) Auto-formatting - Not a priority. You should adopt the same style as the original maintainer. > 5. If you can, contemplate rewrite some parts in a memory safe language The last step of an inherited C++ codebase is to rewrite it in a memory safe language? A few reasons why this probably won't work: 1. Getting resources to do additional work on something that isn't broken can be difficult. 2. Rather than just needing knowledge in C++, you now also need knowledge in an additional language too. 3. Your testing potentially becomes more complex. 4. Your project likely won't lend itself to being written in multiple languages, due to memory/performance constraints. It must be a significantly hard problem that you didn't just write it yourself. 5. You have chosen to inherit a legacy codebase rather than write something from scratch. It's an admittance that you don't have some resource (time/money/knowledge/etc) to do so. reply jpc0 1 hour agoparent> The last step of an inherited C++ codebase is to rewrite it in a memory safe language Simply getting rid of any actually memory unsafe C++ and enforcing guidelines will do this for you in the C++ codebase. \"Rewrite it in X\" only adds complexity because it's the flavour of the month as you said in your comment. Author is already doing the work of rewriting large chunks of the codebase in C++, they may as well follow and implement a more restrictive subset of the language, I find High integrity C++ to be good. If I can get my hands on the latest MISRA standard that is likely good as well. These may not be \"required\" but they specify what is enforced in . So instead of having to reskill your entire devteam on a new language which has many many sharp edges, how about just having your dev team use the language they already know and enforce guidelines to avoid known footguns. reply rurban 3 hours agoprevI just went this very same dance with an old project, smart, which evaluates string matching algorithms. Faster strstr(). From 2013. It was in a better shape than zlib, but still. Their shell build script was called makefile, kid me not. So first create a proper dependency management: GNUmakefile. A BSD makefile would have been prettier, but not many are used to this. dos2unix, chmod -x `find . -name *.c -o name *\\.h`, clang-format -i All in seperate commits. Turns out there was a .h file not a header, but some custom list of algorithm states, broken by fmt. Dontg do that. Either keep it a header file, or rename it to .lst or such. Fix all the warnings, hundreds. Check with sanitizers. Check the tests, disable broken algorithms, and mark them as such. Improve the codebase. There are lots of hints of thought about features. write them. Simplify the state handling. Improve the tests. Add make check lint. Check all the linter warnings. Add a CI. Starting with Linux, Windows mingw, macos and aarch64. Turns out the code is Linux x64 only, ha. Make it compat with sse checks, windows quirks. Waiting for GH actions suck, write Dockerfiles and qemu drivers into your makefile. Maybe automake would have been a better idea after all. Or even proper autoconf. Find the missing algorithms described elsewhere. Add them. Check their limitations. Reproducible builds? Not for this one, sorry. This is luxury. Rather check clang-tidy, and add fuzzing. https://github.com/rurban/smart reply Kon-Peki 11 hours agoprevThe article doesn't mention anything about global variables, but reducing/eliminating them would be a high priority for me. The approach I've taken is, when you do work on a function and find that it uses a global variable, try to add the GV as a function parameter (and update the calling sites). Even if it's just a pointer to the global variable, you now have another function that is more easily testable. Eventually you can get to the point where the GV can be trivially changed to a local variable somewhere appropriate. reply tehnub 17 hours agoprevI enjoyed the article and learned something. But I've been wondering: When people say \"rewrite in a memory-safe language\", what languages are they suggesting? Is this author rewriting parts in Go, Java, C#? Or is it just a smirky, plausibly deniable way of saying to rewrite it in Rust? reply broken_broken_ 15 hours agoparentAuthor here, thanks! A second article will cover this, but the bottom line is that it entirely depends on the team and the constraints e.g. is a GC an option (then Go is a good option), is security the highest priority, etc. I’d say that most C++ developers will generally have an easy time using Rust and will get equivalent performance. But sometimes the project did not have a good reason to be in C++ in the first place and I’ve seen successful rewrites in Java for example. Apple is rewriting some C++ code in Swift, etc. So, the language the team/company is comfortable with is a good rule of thumb. reply tehnub 15 hours agorootparentMakes sense, thanks! reply avgcorrection 13 hours agoparentprevSo you saw a post about C++, it didn’t mention “Rust” once, mentioned “memory safe” languages which there are dozens of, and yet found a way to shoehorn in a dismissive comment about a meme. Nice. We’ve reached the rewrite-in-rust meme stage of questioning whether the author is a nefarious crypto-Rust programmer in lieu of not being able to complain about it (since it wasn’t brought up!). reply bsdpufferfish 5 hours agorootparent(author actually shows up to advocate for rust) reply bluetomcat 17 hours agoprevBeen there, done that. Don't be a code beauty queen. Make it compile and make it run on your machine. Study the basic control-flow graph starting from the entry point and see the relations between source files. Debug it with step-into and see how deep you go. Only then can you gradually start seeing the big picture and any potential improvements. reply bluGill 12 hours agoparentIn my experience it takes at least a year straight working with code before you can form an opinion on if it is beautiful or not. People who have not worked in a code base for that long do not understand what is a beautiful design corrupted by the real world vs what is ugly code. Most code started out with a beautiful design but the real world forced ugly on it - you might be able to improve this a little with full rewrite but the real world will still force a lot of ugly on you. However some code really is bad. reply johngossman 17 hours agoparentprevAbsolutely. Read the code. Step through with a debugger. Fix obvious bugs. If it’s legacy and somebody is still paying to have it worked on, it must mostly work. Changing things for “cleanliness and modernization” is likely to break it. reply cratermoon 16 hours agorootparent> Fix obvious bugs. Be careful about that. Hyrum's Law and all. reply johngossman 9 hours agorootparentShould have been clearer. You’ve probably been put on the project because something isn’t working. Fix the simplest, most obvious of these. Fixing a bug is a good way to learn. reply cratermoon 4 hours agorootparent> You’ve probably been put on the project because something isn’t working. Perhaps if it's a change requested by the organization or the users. Just don't go \"fixing\" things that look like bugs without knowing if it's really a bug or expected behavior. reply hgs3 10 hours agoprevRewriting is questionable. Joel Spolsky has a famous blog post about this from two decades ago that's still relevant today [1]. [1] https://www.joelonsoftware.com/2000/04/06/things-you-should-... reply pvarangot 10 hours agoprevBesides what everyone else told you make sure you are making at least 250k/y reply VyseofArcadia 13 hours agoprev> Get out the chainsaw and rip out everything that’s not absolutely required to provide the features your company/open source project is advertising and selling Except every legacy C++ codebase I've worked on is decades old. Just enumerating the different \"features\" is a fool's errand. Because of reshuffling and process changes, even marketing doesn't have a complete list of our \"features\". And even it there was a complete list of features, we have too many customers that rely on spacebar heating[0] to just remove code that we think doesn't map to a feature. That's if we can even tease apart which bits of code map to a feature. It's not like we only added brand new code for each feature. We also relied on and modified existing code. The only code that's \"safe\" to remove is dead code, and sometimes that's not as dead as you might think. Even if we had a list of features and even if code mapped cleanly to features, the idea of removing all code not related to \"features your company is advertising or selling\" is absurd. Sometimes a feature is so widely used that you don't advertise it anymore. It's just there. Should Microsoft remove boldface text from Word because they're not actively advertising it? The only way this makes sense is if the author and I have wildly different ideas about what \"legacy\" means. [0] https://xkcd.com/1172/ reply lelanthran 43 minutes agoparent> > Get out the chainsaw and rip out everything that’s not absolutely required to provide the features your company/open source project is advertising and selling > Except every legacy C++ codebase I've worked on is decades old. Just enumerating the different \"features\" is a fool's errand. Because of reshuffling and process changes, even marketing doesn't have a complete list of our \"features\". Yeah, this struck me also, and your post should be modded up more. Anyone with significant experience in development knows what \"Legacy\" means. Regardless of the language, after a specific point in a product's lifetime you cannot \"know\" all the features. Just not possible, no matter how well you think you documented it. In an old product, every single line of code is there because of a reason that is not in the docs. Some examples that I've seen: 1. Using `int8_t` because at some point we integrated a precompiled library that was compiled with signed char, and we want warnings to pop up when we mix signs. 2. Wrote our own stripped-down SSL library because OpenSSL was not EMV certified at the time and did not come with the devkit. Now callers depend on a feature in our own library. 3. Client calls our DLL with Windows-specific UTF-16 strings. That's why that function has three variants that take 3 different types of strings. 4. This library can't be compiled with any gcc/glibc newer than X.Y, because the compiled library is loaded with `dlopen` in some environments. 5. We have our own 'safe' versions of string functions, because MSVC which takes the same parameter types for those functions assigns different meanings to the `size` parameter. 6. Converting fixed-precision floats to an int, performing the additions, and then converting the last division is faster and more accurate, but the test suite at the client expects the cumulative floating point errors and the test will fail. Not to mention uncountable \"marketing said this is not offered as a feature, but client depends on it\" things. reply Palomides 8 hours agoparentprevhard agree removing a feature is possibly the most politically intractable thing you can try to do with a legacy codebase, almost never worth trying reply sjc02060 17 hours agoprevA good read. We recently did \"Rewrite in a memory safe language?\" successfully. It was something that shouldn't have been written in C++ in the first place (it was never performance sensitive). reply jstimpfle 13 hours agoparentProbably not a project spanning more than 3 decades of development and millions of lines of code? reply throwaway2037 8 hours agoparentprevDo you have a public write-up (blog post)? If yes, you should post it on HN. It would probably generate lots of interesting conversation. reply tehnub 17 hours agoparentprevWould you mind sharing what language you used? reply delta_p_delta_x 11 hours agoprev> So what do I recommend? Well, the good old git submodules and compiling from source approach.= It is strange that the author complains so much about automating BOMs, package versioning, dependency sources, etc, and then proceeds to suggest git submodules as superior to package managers. The author needs to try vcpkg before making these criticisms; almost all of these are straightforwardly satisfied with vcpkg, barring a few sharp edges (updating dependencies is a little harder than with git submodules, but that's IMO a feature and not a bug—dependencies are built in individual sandboxes which are then installed to a specified directory. vcpkg can set internal repositories as the registry instead of the official one, thus maintaining the 'vendored in' aspect. vcpkg can chainload toolchains to compile everything with a fixed set of flags, and allows users to specify per-port customisations. These are useful abstractions and it's why package managers are so popular, rather than having everyone deal with veritable bedsheets' worth of strings containing compile flags, macros, warnings, etc. reply Jean-Papoulos 2 hours agoprevA lot of this assumes the codebase is testable, but most of those legacy applications rely on global state a lot... reply sk11001 17 hours agoprevIs it worth getting more into C++ in 2024? Lots of interesting jobs in finance require it but it seems almost impossible to get hired without prior experience (with C++ and in finance). reply optimalsolver 17 hours agoparentYes. I switched from Python to C++ because Cython, Numba, etc. just weren't cutting it for my CPU-intensive research needs (program synthesis), and I've never looked back. reply sk11001 17 hours agorootparentMy question isn't whether it's a good fit for a specific project, I'm more interested in whether it's a good career choice e.g. can you get a job using C++ without C++ experience; how realistic is it to ramp up on it quickly; whether you're likely to end up with some gnarly legacy codebase as described in the OP; is it worth pursuing this direction at all. reply jandrewrogers 12 hours agorootparentModern C++ is the language of choice for high-performance, high-scale data-intensive applications and will remain so for the foreseeable future. This is a class of application for which it is uniquely suited (C and Rust both have significant limitations in this domain). There are other domains like gaming that are also heavily into C++. Avoiding legacy C++ codebases is more about choosing where you work carefully. It goes without saying that if you don't like the kinds of applications where C++ excels then it may not be a good career choice because it is not a general purpose language in practice. reply throwaway2037 8 hours agorootparent> Modern C++ is the language of choice for high-performance, high-scale data-intensive applications > C and Rust both have significant limitations in this domain Rust? Please provide concrete examples. I don't believe it. reply TillE 16 hours agorootparentprevC++ is really a language that you want to specialize in and cultivate years of deep expertise with, rather than having it as one tool in your belt like you can with other languages. That's certainly a choice you can make, and modern C++ is generally a pretty good experience to work with. I would hope that there's not a ton of active C++ projects which are still mostly using the pre-2011 standard, but who knows. reply sgerenser 13 hours agorootparentThis exactly. It’s a blessing and a curse, because I’d love to move to a “better” language like Rust or even Zig. But with 20+ years of C++ experience I feel like I’d be throwing away too much to avoid C++ completely. Also agreed that modern C++ is pretty decent. Lamenting that I’m back in a codebase that started before C++11 vs my previous job that was greenfield C++14/17. reply stagger87 12 hours agorootparentprevI would be very surprised if most people actually choose to develop in C++. It's a very good language choice for many domains, and I suspect interest and expertise in those domains drives people to C++ more than a desire to program in C++. reply patrick451 10 hours agorootparentprevIME, c++ was easier to ramp up on than typescript. C++ still a lingua franca in many domains, e.g., robotics, games, finance. reply throwaway2037 8 hours agorootparentFinance? No, most of it was rewritten in the 2000s to Java or DotNet. Sure, a bunch of HNers will reply here that they work on high frequency market making systems that use C++, but they are an extreme minority in the industry at this point. reply hilux 16 hours agorootparentprevDid you see yesterday's article about the White House Office of the National Cyber Director (ONCD) advising developers to dump C, C++, and other languages with memory-safety issues? reply mkipper 14 hours agorootparentI still think knowing C++ is pretty valuable to someone's career (at least over the next 10 - 15 years) if they're looking to work in fields that traditionally use C++ but might be transitioning away from it. The obvious comparison is Rust. There are way more C++ jobs out there than Rust jobs. And even if I'm hiring for a team developing something in Rust, I'd generally prefer candidates with similar C++ experience and a basic understanding of Rust over candidates with a strong knowledge of Rust and no domain experience. Modern C++ and Rust aren't _that_ dissimilar, and a lot of ideas and techniques carry over from C++ to Rust. Even if the DoD recommends that contractors stop using C++ and tech / finance are moving away from it, I'd say we're still years away from the point where Rust catches up to C++ in terms of job opportunities. If your main goal is employment in a certain industry, you'll probably have an easier time getting your foot in the door with C++ than Rust. Both paths are viable but the Rust path would be much harder IMO. reply sk11001 16 hours agorootparentprevYes, and at the same time I’m seeing ads for jobs that pay more than double what I make that require C++. reply avgcorrection 13 hours agorootparentprevWe’re still in for another 20 years of hardcore veteran Cxx programmers insisting that either the memory safety issue is overblown or just a theoretical issue if you are experienced enough/use a new enough edition of the language. reply bluGill 12 hours agorootparentThe C++ committee is looking hard at how to make C++ memory safe. If you use modern C++ you are already reasonably memory safe - the trick is how do we force developers to not access raw memory (no new/malloc, use vector not arrays...). There are some things that seem like they will come soon. Of course if you really need that non-memory safe stuff - which all your existing code does - then you can't take advantage of it. However you can migrate your C++ to modern C++ and add those features to your code. This is probably easier than migrating to something like Rust (Rust cannot work with C++ unless you stick with the C subset from what I can tell) since you can work in small chunks at a time in at least some situations. reply d_sem 11 hours agoparentprevDepends on the industry you are interested in entering. My myopic view of the world has seen the general trend from C to C++ for realtime embedded applications. For example: in the Automotive Industry all the interesting automotive features are written in C++. reply ecshafer 17 hours agoprevThis is pretty great advice for any legacy code project. Even outside of C++ there is a huge amount of code bases out there that do not compile/run on a dev machine without tons of work. I once worked on a Java project that due to some weird dependencies, the dev mode was to run a junit test which started spring and went into an infinite loop. Getting a standard run to work helped a ton. reply bluGill 11 hours agoparentThe difference between greenfield and legacy code is just a few years. So learn to work with legacy code and how to make it better over time. reply grandinj 16 hours agoprevThis is generally the same path that LibreOffice followed. Works reasonably well. We built our own find-dead-code tool, because the extant ones were imprecise, and boy oh boy did they find lots of dead stuff. And more dead stuff. And more dead stuff. Like peeling an onion, it went on for quite a while. But totally worth it in the end, made various improvements much easier. reply bobnamob 11 hours agoprevThis article (and admittedly most comments here) doesn't emphasize the value of a comprehensive e2e test suite enough. So much talk about change and large LoC deltas without capturing the expected behavior of the system first reply w10-1 8 hours agoprevOnce you have the SCM in order, and before you make any changes: Structure101 is the best way to grok the architecture er tangles of a large code base. They have a trial period that would give you the overview, but their refactoring support is fantastic (in Java at least). https://structure101.com/products/workspace/ reply sega_sai 13 hours agoprevTo be honest a lot of recommendations apply to other languages as well. I.e. start with tests only then change, add autoformatting etc. At least I had experience of applying a similar sequence of steps to a python package. reply joshmarinacci 11 hours agoprevYou need to install linters and formatters and security checkers. But you need to start using them incrementally. Trying to fix all the issues found at once is a quick recipe for madness. I suggest using clang-tidy with a meta-linter like Trunk Check docs: https://docs.trunk.io/check/configuration/configuring-existi...) reply myrmidon 16 hours agoprevReally liked it! Especially the \"get buy in\" is really good advice-- always stressing how the effort spent on refactoring actually improves things, and WHY its necessary. Something that's kinda implied that I would really stress: Establish a \"single source of truth\" for any release/binary that reaches production/customers, before even touching ANY code (Ideally CI. And ideally builds are reproducible). If you build from different machines/environments/toolchains, its only a matter of time before that in itself breaks something, and those kinds of problems can be really \"interesting\" to find (an obscure race condition that only occurs when using a newer compiler, etc.) reply dureuill 11 hours agoprev> What do you do now? Look for another job > You’d be amazed at how many C++ codebase in the wild that are a core part of a successful product earning millions and they basically do not compile. Wow I really hope this is hyperbole. I feel like I was lucky to work on a codebase that had CI to test on multiple computers with WError reply throwaway71271 11 hours agoparent> Wow I really hope this is hyperbole. I am sure its not, I dont have much experience as I have worked in only 3 companies in the last 25 years, but so far I have found no relation between code quality and company earnings. reply throwaway2037 8 hours agorootparent> so far I have found no relation between code quality and company earnings. This! What matters is the market fit and customer experience. You can deliver a lot of value with average programmers working on a shitty code base. reply gmueckl 6 hours agorootparentI started to joke that in order to have a successful software startup, you need to essentially write the most godawful program code you can get away with. The money is much better spent on a good/aggressive sales strategy. Elegant technology never wins on its own merits. reply geoelectric 10 hours agoprevIf the blog author is lurking on here, I tried to bookmark the article since it's very relevant to my current situation, but it didn't have a title set for the page. NBD to copy/paste one in, but it took me by surprise. reply girafffe_i 7 hours agoprevRewrite it in Rust. reply rwmj 17 hours agoprevSurprisingly good advice. In a similar vein, Joel's 12 steps to better software: https://www.joelonsoftware.com/2000/08/09/the-joel-test-12-s... reply codelobe 12 hours agoprevMy first thing is usually: #0: Replace the custom/proprietary Hashmap implementation with the STL version. Once upon a time, C++ academics brow beat the lot of us into accepting Red-Black-Tree as the only Map implementation, arguing (in good faith yet from ignorance) that the \"Big O\" (an orgasm joke, besides others) worst case scenario (Oops, pregnancy) categorized Hash Map as O(n) on insert, etc. due to naieve implementations frequently placing hash colliding keys in a bucket via linked list or elsewise iterating to other \"adjacent\" buckets. Point being: The One True Objective Standard of \"benchmark or die\" was not considered, i.e., the average case is obviously the best deciding factor -- or, as Spock simply logic'd it, \"The needs of the many outweigh the needs of the few\". Thus, it came to pass that STL was missing its Hashmap implementation; And since it is typically trivial (or a non issue) to avoid \"worst case scenario\" (of Waat? A Preggers Table Bucket?), e.g., use of iterative re-mapping of the hashmap. So it was that many \"legacy\" codebases built their own Hashmap implementations to get at that (academically forbidden) effective/average case insert/access/etc. sweet spot of constant time \"O(1)\" [emphasis on the scare quotes: benchmark it and see -- there is no real measure of the algo otherwise, riiight?]. Therefore, the affore-prophesied fracturing of the collections APIs via the STL's failure to fill the niche that a Hashmap would inevitably have to occupy came to pass -- Who could have forseen this?! What is done is done. The upshot is: One can typically familiarize oneself with a legacy codebase whilst paying lip service to \"future maintainability\" by (albeit usually needless) replacing of custom Hashmap implementations with the one that the C++ standards body eventually accepted into the codebase despite the initial \"academic\" protesting too much via \"Big O\" notation (which is demonstrably a sex-humor-based system meant to be of little use in practical/average case world that we live in). Yes, once again the apprentice has been made the butt of the joke. reply sgerenser 11 hours agoparentIt’s unfortunate that the hashmap picked by the standards comittee (std::unordered_map) is both awkwardly named and not very performant. Still probably better than whatever was hacked up in 1998, but nowadays you can do much better for any case where performance actually mattered. Note, still don’t roll your own, but there’s plenty of options from e.g. Abseil or Facebook’s Folly. I worked on a project a few years ago where all data was stored in hashmaps. Just swapping out std::unordered map for an optimized implementation of a robin hood hash map increased performance by something like 2x and cut memory usage in half on many larger test cases. reply bluGill 11 hours agoparentprevIn the mid 1990s when C++ was getting std::map and the other containers CPU caches were not a big deal. Average case was the correct thing to optimize for. These days CPU caches are a big deal and so your average case is typically dominated by CPU cache miss pipeline stalls. This means for most work you need different data structures. The world is still catching up to what this means. reply codelobe 11 hours agorootparentWell, Red-Black algos are supposed to be better at cache-locality, but I have an AVL-tree impl (ugg jokes, again: AVUL (ALV) is the \"evil\" tree of \"forbidden\" {carnal?} wisdom from The Garden of Eden, associated with Yggdrasil/Odin [a \"pagan\" God of Balance & Pleasure]) that has improved cache locality since its data nodes can be made to contain AvlTreeNode structure(s), and avoid copying any data, as users are made to provide node alloc/free function pointers to this C lib's Tree \"constructor\". This means, for real example, I have a command line option interpreter with const structures for each option, each node added to two AVL trees (to find by unicode codepoint and find by length prefixed unicode string name). C++ STL Map implementations can not conditionally generate code for const types and thus do needless coppies, whereas my C collections API causes 0 calls to malloc (vs STL's 2 mallocs per node insert). NodeAlloc is just pointer math to get at the apt AvlNode, NodeFree is NoOp. Benchmarking the STL vs my AVL approach results in millions of times quicker cmd line opt interpretaion (for my gnu getopt replacement lib) due to reduction of pointer chasing... And if I want to do something similar in C++ (overloading operator new), I have to instantiate multiple copies of the Tree code, one per each \"class\". What if I want to use my Sortable class with various allocators: OBJ cache, dynamic GC'd, static (no alloc, its in the .data section already)...? Well then I get N copies of EXACT SAME template code for no real reason, only differing in delete and new [con|de]structors. The cache-misses galore this causes isn't even fair to bench against the C w/ fn() ptr approach. reply bluGill 9 hours agorootparentTry benchmarking on something from 1995, like a 80486. Cache misses won't matter much. reply samatman 7 hours agoparentprev> that the \"Big O\" (an orgasm joke, besides others) > worst case scenario (Oops, pregnancy) > (of Waat? A Preggers Table Bucket?) > \"Big O\" notation (which is demonstrably a sex-humor-based system This post reeks of obesity, desperation, poor life choices, and old-fashioned body odor. reply Kapura 17 hours agoprev> Get out the chainsaw and rip out everything that’s not absolutely required to provide the features your company/open source project is advertising and selling Great advice! People do not often think about the value of de-cluttering the codebase, especially _before_ a refactor. reply cloudhan 7 hours agoprevYou run or your code runs, choose one and choose it wisely ;) reply davidw 17 hours agoprevWell, tomorrow is the \"who's hiring?\" thread... reply dirkc 1 hour agoprevMy take: 1. Source control 2. Reproducible builds 3. Learn the functionality 4. ... If you don't understand what the code does, you're probably going to regret any changes you make before step 3! reply Scubabear68 16 hours agoprevThe “rip everything out” step is not recommended. You will break things you don’t understand, invoke Chesterson’s Fence, and create enormous amounts of unnecessary work for yourself. Make it compile, automate what you can, try not to poke the bear as much as you can, pray you can start strangling it by porting pieces to something else over time. reply elzbardico 12 hours agoprev1990 Windows C++ code? Consider euthanasia as a painless solution. reply jcarrano 12 hours agoprevGood points, but this is not something you can solve with a recipe. Investigate, talk to people and make sure you are solving actual problems and prioritizing the right tasks. This is an extremely crucial step that you must do first: familiarize yourself with the system, its uses and the reasons it works like it does. Most things will be there for a reason, even if not written to the highest standard. Other parts might at first sight seem very problematic yet be only minor issues. Be careful with number 4 and 5. Do not rush to fix or rewrite things just because they look like they can be improved. If it is not causing issues and it is not central to the system, better spend your resources somewhere else. Get the team to adopt good practices, both in the actual code and in the process. Observe the team and how they work and address the worst issues first, but do not overwhelm them. They may not even be aware of their inefficiencies (e.g. they might consider complete rebuilds as something normal to do). reply bombcar 12 hours agoparentI did not find Chesterton's fence, and was sad. The very first thing to do with a new codebase is don't touch anything until you understand it, and then don't touch anything until you realize how mistaken your understanding was. reply Merik 12 hours agoprevif you have access to Gemini Pro 1.5 you could put the whole code base into the context and start asking questions about architecture, style, potential pain paints etc. reply huqedato 17 hours agoprevWhenever I inherited a project containing legacy code, regardless of the frameworks, tools, or languages used, we always found it necessary to drop it and begin anew. Despite my efforts to reuse, update, or refactor it, we inevitably reached a point where it was unusable for further development. reply professorTuring 13 hours agoprevHow good is AI refactoring the code? Haven’t tried it yet, but… as someone who has need to work on tons of legacy in the past… looks interesting! reply bluGill 11 hours agoparentVery mixed. Sometimes great, but you have too watch it close as once in a while it will do garbage. There is a lot of non-AI refactoring for C++ these days that is very good. And many more tools that will point to areas that there is a problem and often a manual fix of those areas is \"easy\". reply throw_m239339 5 hours agoprevI quit. Life is short. reply bun_terminator 17 hours agoprev> Rewrite in a memory safe language? like c++11 and later? reply z_open 17 hours agoparentHow is that memory safe? Even vector out of bounds index is not memory safe. reply bluGill 11 hours agorootparentvector.at() is memory safe. You get a choice. Easy to ban [] if you cannot statically prove it is safe. C++11 isn't the most memor",
    "originSummary": [
      "The article offers a detailed guide on enhancing a legacy C++ codebase by gradually enhancing security, developer experience, correctness, and performance through steps like updating with CI tools, linters, and auto-formatting.",
      "It emphasizes the significance of effective communication, collaboration, and code quality maintenance for efficient and secure software development while highlighting the complexities of dependency management in C++.",
      "Recommended practices include utilizing git submodules and compiling from source for improved reliability and control in managing dependencies."
    ],
    "commentSummary": [
      "The article covers strategies for handling legacy C++ codebases, with tips on reproducible builds, fixing compiler warnings, and using tools like valgrind for testing.",
      "It discusses challenges like lost source code, maintaining open source projects, and managing dependencies in Linux distributions.",
      "Emphasizes the significance of code comprehension tools, enhancing code quality, transitioning to modern code, and debates the use of C++ versus Rust in various industries for job prospects."
    ],
    "points": 253,
    "commentCount": 252,
    "retryCount": 0,
    "time": 1709215919
  },
  {
    "id": 39551064,
    "title": "Introducing Row Zero: The Ultimate Spreadsheet Solution",
    "originLink": "https://rowzero.io",
    "originBody": "Row Zero Use cases FeaturesIntegrationsPricingBlogDocsSupport Open main menu Sign upLog in Your business wants a spreadsheet. Give it to them. Enable teams to access the data they need with a tool they already know how to use. Try it now No account needed Play video Row Zero is an impressive feat of engineering, making big data feel small in a familiar spreadsheet interface. Wes McKinney Creator of Pandas and Apache Arrow Use cases Give your business access to cloud data sources in a tool they already know how to use. Explore hundreds of millions of rows, perform ad-hoc analyses, and monitor trends from the comfort of a spreadsheet. Business Intelligence Finance Operations Marketing Power and speed Write Excel-compatible formulas to process hundreds of millions of rows instantly No more slow dashboards - filter, sort, pivot, and plot in milliseconds Upload multi-GB CSV and JSONL files - no need for databases or expensive BI tools Familiar UI Excel compatible - execute VLOOKUPS, XLOOKUPS, COUNTIFS, INDEX MATCHs, and more Filter, sort, pivot, and plot the way you already know how - no BI tool training required Enable your business teams with an analysis tool they already know how to use Connect to any data source Row Zero runs in the cloud and connects directly to any data source Connect data warehouses, data lakes, APIs, and any other service to build models on live data Save time and reduce mistakes by connecting to live data instead of copy/pasting Sharing and collaboration Collaborate in real time - Share each workbook with editor and viewer permissions Govern your data. No more Sharepoint or untraceable emails with .xlsx attachments Provide refresh permissions (without revealing credentials) so business teams can build models off live data Use the Follow feature when presenting to walk team members through your analysis Python Decompose long spreadsheet formulas with Python helper functions to improve readability and prevent costly errors Import popular Python modules like pandas, numpy, scipy, and yfinance, to perform complex analysis in a familiar tool Never write VBA again FeaturesIntegrationsPricingBlogDocsSupport Latest blogs Last Mile AnalyticsSpreadsheet Data GovernanceOpen a Big CSV FileEdit Big CSV FilesWhy is Excel slow?What is the Excel row limit?What is the Google Sheets Row Limit?The Top 5 Python Spreadsheets Use cases Business IntelligenceFinanceOperationsMarketing Integrations SnowflakeRedshiftPostgresS3 Company Contact UsSecurityPrivacy PolicyTerms of Use ©2024 Row Zero, Inc.",
    "commentLink": "https://news.ycombinator.com/item?id=39551064",
    "commentBody": "We built the fastest spreadsheet (rowzero.io)240 points by gamegoblin 18 hours agohidepastfavorite197 comments breckognize 18 hours agoFounder/CEO here. When I worked in AWS S3, I spent a lot of time in Excel. Even as a dev, it was the fastest way to explore data, build models, and share forecasts with business partners. My Excel usage was plagued by slow performance, poor cloud integration, and no first-class Python support. I loved the richness and responsiveness of Excel, but I had to give up too much power to get it. This felt like a false choice, so 3 years ago I started working on it. Today we're launching Row Zero. Row Zero looks and feels like Excel and Google Sheets, but 100-1000x faster. You can easily import gigabytes of CSV, JSONL, and Parquet files or connect directly to Snowflake, Redshift, Postgres, and S3. We also support Python natively. You can call Python functions that return pandas data frames and manipulate the results directly in the spreadsheet. Under the hood, Row Zero is a high-performance columnar engine written in Rust. Running in AWS lets us scale compute up and down and import quickly from S3 and Snowflake. When you open a workbook, we place it in the AWS region closest to you so it feels as snappy as a desktop application. The app is built by 5 ex-principal engineers from Amazon S3, Tableau, and Airtable. We're the team that wrote the file system that powers S3. We've been in beta for a year. Use us to refine big CSVs, build complex financial models, create dashboards, and share large data sets. We're also a killer Snowflake/Postgres client. Give us a try and let us know what you think. reply Terretta 16 hours agoparentStarting from a place of \"I want to start providing this to my employees yesterday\"... I can't see how to adopt this at an info-sec minded financial services firm that would otherwise love to pay you for it. For example, the docs show the product wants static creds for a Postgres database or signed URL for S3 bucket instead of leveraging best-practice service-to-service identity and access management. Maybe you support what I want, and I didn't find it at first glance. This either ... (a) needs to support modern dynamic or token-based authentication (e.g. Oauth2.0 client credentials grant, JWT, or for enterprises ideally CSP native IAM in Azure/GCP/AWS such as share S3 by cross account bucket permissions policy instead of signed URL, etc.), or ... (b) allow firms to operate this themselves so the spreadsheet is run in the firm's security context, no creds are shared and data never leaves. As you are running in AWS, providing this to run in AWS IAM context could solve it, but it's likely worth your time collaborating closely with FS firms that have solved cloud-native infosec at scale for the world's most demanding regulatory environments such as GSIFIs/GSIBs, since if you can do that, it's secure for anyone. Similar for HIPAA or FedRAMP. If your customers can be fully best practice compliant within these regulatory regimes without having to lower their standards or get exceptions to use you, then it's above the bar for pretty much everyone. // Full disclosure: Although using all 3 of AWS, Azure, and GCP, I was an AWS CAB member for half a decade (as principal engineers you likely know what this means), with a native-AWS preference. reply breckognize 15 hours agorootparentFor S3 specifically, we support IAM-based role assumption. If you go to Data > Import from Amazon S3 > Add S3 Bucket, you can grant our AWS account permissions to read an S3 bucket in your account. We also have dedicated hosting options for our Enterprise tier. If there's a specific data source you'd like us to support OAuth integration for, let me know, and we'll add it. reply glitchc 13 hours agorootparent> you can grant our AWS account permissions to read an S3 bucket in your account. That seems like a huge privacy hole. It sounds like you're offering spreadsheet-as-a-service, where you scale up AWS spot instances based on query size. reply Terretta 12 hours agorootparentprevGreat to hear you can do the policy. Are you able to be “NSL-proof”? This means, if you are served a national security letter with a gag order saying to turn over my data without telling me, can you? If you are not NSL proof, are you able to demonstrate who from your firm can, and cannot, by technical guarantee not by policy in the “signed agreement” sense, see my data, and can I see in an audit log any time and every time any of those people do see my data? reply dmurray 12 hours agorootparentIf you need protection from the US security apparatus, you're not the target market. reply Terretta 12 hours agorootparentOn the contrary, that scenario (as well as, \"what if your SaaS provider or CSP is hostile?\"), are great \"clarifying\" questions to understand the security architecture of a product that is very likely to see some incredibly sensitive data. It is possible for the answer to be that a service is NSL proof -- with asterisks, and the asterisks are very interesting to discuss. And no, it's not about the US security apparatus for most firms, although if you take a look at AWS's security teams, you'll see there is a lot of experience exchanged, and AWS does secure the US security apparatus' data. They're quite good at it. reply fragmede 10 hours agorootparent> AWS does secure the US security apparatus' data. with the exception of red teams, AWS isn't securing AWS from US security apparatus attack though. reply fakedang 15 hours agorootparentprevHonestly, is it possible to decouple AWS from the app? >I can't see how to adopt this at an info-sec minded financial services firm that would otherwise love to pay you for it. Else this applies for us too. reply codegeek 17 hours agoparentprev\"We're the team that wrote the file system that powers S3.\" Your funding slide heading right here. reply Kluggy 16 hours agorootparentHe was at AWS for 5 years from 2015 to 2020. It seems unlikely that he wrote the filesystem that powers s3. Improved, absolutely. His partner was a manager, not an engineer, per https://www.geekwire.com/2024/former-aws-engineers-raise-3m-... Personally I don’t like this sort of puffery in funding slides nor announcements. reply gamegoblin 16 hours agorootparentAll our backend engineers were on the S3 filesystem team. I'm Grant Slatton, founding engineer at Row Zero, and before this I designed and led the team that built S3's new filesystem, ShardStore (check out this paper https://www.cs.utexas.edu/~bornholt/papers/shardstore-sosp21...). Our other Row Zero backend engineers (Breck, Erich, Greg) were all on the team. Our frontend guy is ex-Airtable. reply QuinnyPig 7 hours agorootparentOooh! Your reputation precedes you. Please, please, please put an “about us” page on your site. People get leery when there aren’t named humans identified as being behind a product that’s asking for production data access. reply codegeek 15 hours agorootparentprevVCs invest in people and not products/ideas from what I have heard/read. I think it is imp. to mention that you were on the S3 team if you are building something like this. Huge credibility factor. reply giancarlostoro 16 hours agorootparentprevI wont be surprised if someone acquires them. This looks really well engineered and it gets shit done as they say. reply chadash 16 hours agoparentprevWhat I'd love to see is an easy workflow to make this supplant Google docs. Right now, let's say i want a shared spreadsheet. I need to: 1) Signup for an account 2) Create a workbook 3) Click share and put in friend's email 4) They get an invite 5) Now they need to sign up 6) They can edit my doc I'm not a paying customer (or even a user... I just heard of this), so take this with a grain of salt, but what I'd love to see is a super-friction-less way for me to share a doc and make it editable without others needing accounts. Cut out as many steps as possible above. Doing this can be a great tool for marketing... what's better marketing than easily collaborating on a spreadsheet on a site that my friend is already using and that already looks very similar to tools I'm familiar with? reply breckognize 16 hours agorootparentAfter you create a workbook, click the checkbox under Share > Anyone with link can view. Then your friend doesn't need to create an account. reply chadash 13 hours agorootparentThis allows them to view but not to edit reply drivebycomment 14 hours agorootparentprev> make it editable without others needing accounts This requirement brings a very difficult mix of challenges around security, privacy, regulatory compliance and business priorities. As a toy / personal project it could work, but realistically this is unlikely to ever materialize in a way that you imagine. reply warkdarrior 16 hours agorootparentprevThis feature would be the fastest way for the company to have the least number of paying users. As a user, I'd pay for one account and then share new spreadsheets with anyone who asks for one on the Internet. reply chadash 13 hours agorootparentIf I’m the type of user who would ever consider paying, I’m most definitely not the type of person who wants to ask a stranger on the internet to setup a new spreadsheet for me every time I want to use one. I wouldn’t even want to ask a friend or coworker. reply fragmede 10 hours agorootparentFortunately the Internet is real big, so there's definitely going to be a discord or subreddit or private slack where one person has a paid account and just makes new sheets for other people. it doesn't really scale though and it's cheap enough that most people will just make their own instead of being reliant on someone. broke teenagers isn't a market you have to fully satisfy. reply SahAssar 15 hours agorootparentprevThere are other billing models than per user if that's your concern. reply otoburb 14 hours agoparentprevThis looks like something that I could use right away, but I wonder, since we can write Python, does this mean that we can write back to one of the native backends (e.g. Postgres)? Currently, the only way to do this from Excel (e.g. saving a snapshot of an analyst's current dashboard that they just built) is through a macro, which then starts the (understandable) descent into Excel's External Content and Trust Center permissions hell. reply gamegoblin 13 hours agorootparentYou can technically use the Python to write back to Postgres if you are comfortable putting your creds into the code window. The downside of this is your creds would be viewable to anyone you share the workbook with. We have gotten a lot of requests for write-back-to-DB (Snowflake, Postgres, etc) so will be adding first-class support for this feature soon that will use the same connection creds which are stored encrypted in KMS and are not viewable to people you share the workbook with. Would love to chat about your use-case if you want to reach out to us at contact[at]rowzero.io reply al_borland 12 hours agorootparentprevIt’s worth noting that Excel supports Python now as well. https://techcommunity.microsoft.com/t5/excel-blog/announcing... reply andelink 12 hours agorootparentLooks like the Row Zero blog does a brief overview of existing Python spreadsheets, mentioning Excel support: https://rowzero.io/blog/top-python-spreadsheets#top-5-python... Would like to see a more detailed comparison than the one they've got in their blog, though. reply pax 12 hours agoparentprevExcited to see this. I built quite a few data-rich projects / dashboards with gSheets as backend/api - or as input UI, fetched to SQLite. I'd be looking very much forward for: data validation, conditional formatting (including heatmaps) and - what gSheet doesn't offer natively, multiple choices cells via data validation or values from another column/sheet. I was surprised I couldn't way to view a spreadsheet as a DataTable, or back as a DataTable (LE: found later how in documentation). And no sticky/frozen header row, ugh. Cherry on top would be gSheets like SQL flavoured QUERY. LE2: to add the wishlist, a potentially killer feature to/for some, dashboard type of sheet, with minimal/layout options –rows & columns blocks that can host charts & text (headings & paragraphs), with global filters – it would add a feature or two complementary to what gSheets offers. And at some point maybe, comments - per sheet or column only. reply gamegoblin 12 hours agorootparentConditional formatting is high on the to-do list If you're open to it, email us at contact[at]rowzero.io and we would love to do a call to talk about your use case, what features you want, etc. We love doing customer calls and adding delightful features. reply nolongerthere 13 hours agoparentprevThe power of excel is that everyone has it and most people have a ton of familiarity with it and there’s a million results for anything you search Google that you don’t know how to do. Basically, all the network effects, bec you’re right if excel were a new product today it would lose to every single competitor that does it much better. Are you compatible with excel functions? reply breckognize 13 hours agorootparentYes, Excel compatible: https://rowzero.io/docs/spreadsheet-functions If we're missing any formulas you need, message me at breck at rowzero.io and we'll add them (usually within 24 hours). reply lancewiggs 13 hours agorootparentYou'll also need formatting to be the same to get adoption. e.g. I wanted to format numbers as currency, and got USD$x,xxx as a result, and no obvious way to change that to $x,xxx. reply breckognize 12 hours agorootparentSounds like a bug, but I'm having trouble reproducing. If you share repro steps with breck at rowzero.io, we'll get it fixed. reply lancewiggs 10 hours agorootparentI’m in New Zealand if that helps. Generally (please) don’t assume location dictates formatting, while Excel’s assumptions are often incorrect and so custom formatting is a requirement. reply quickthrower2 8 hours agorootparentYou can’t even assume location dictates location! reply croisillon 12 hours agorootparentprevand the translations in the millions localizations not compatible with english formulae reply tiffanyh 13 hours agoparentprevHi. Your product looks great. Would you mind helping me understand the differences between RowZero and something like Equals.com? reply breckognize 13 hours agorootparentEquals and Row Zero are in the same space. They've put more effort into their suite of cloud data connectors, while we've focused on performance and first class Python support. We'll be adding more connectors in the coming months. reply benpacker 13 hours agoparentprevWhy build a custom arrow based columnar engine in Rust instead of using Datafusion or Polars? reply gamegoblin 12 hours agorootparentDatafusion, Polars, and us are all based on Arrow. Datafusion is more targeting database users, Polars is targeting Python programmers/dataframe users, and we are targeting spreadsheet users. They are all ultimately just different UIs on top of Arrow. They're complimentary tools. We can take data out of the spreadsheet and put it into Polars instantly (you can do this in the Python code window if you want), etc. Regarding why not implement our spreadsheet built on top of those: spreadsheet allow for heterogenous types in columns, so that requires a lot of extra infrastructure to manage, whereas Datafusion and Polars require homogenous column types. reply MichaelZuo 17 hours agoparentprevExcel 2010 is several times faster than the latest Excel versions. What are the performance metrics compared to Excel 2010? reply gamegoblin 17 hours agorootparentIt varies depending on what you are doing, but is probably 100x faster on average. Some things are literally 10000x faster because we are internally using algorithms with better asymptotic complexity than Excel, but there are a few edge cases where we are maybe only 10x faster (but we are always working to improve that — half my job is just analyzing flame graphs and grinding out perf improvements) reply breckognize 17 hours agorootparentprevWhen we claim 100x faster than desktop Excel, we're looking at supported row counts, import speeds from Snowflake, and time to drag large XLOOKUPs. We'll do a deep dive on performance in another blog post next week. Stay tuned! reply MichaelZuo 12 hours agorootparentThanks, what are the lower bound numbers you're willing to guarantee in writing? (assuming identical systems) I think putting that on the site would save business customers a lot of evaluation time. reply vcool07 16 hours agoparentprevHow is this different from Power BI ? Or is this software on similar lines as Power BI ? reply breckognize 16 hours agorootparentWe're a spreadsheet-first UI. If you're familiar with the Microsoft suite, we're more like a hosted Excel plus Power Query, but way easier to use. reply david_draco 17 hours agoparentprevHow does it compare to gnumeric in terms of speed? reply gamegoblin 16 hours agorootparentWe haven't benchmarked against gnumeric, but will make a perf blog post in the next week or two (will post on HN too) and will see if we can add it to the comparison mix reply lacoolj 16 hours agoparentprevHow does this compare to sheetsjs? reply LoganDark 13 hours agoparentprevYour spreadsheet experience was plugged by \"no first-class Python support\"? What makes Python special here? reply breckognize 13 hours agorootparentNothing in particular. I'd have been happy with anything that has a real open source community around it, which VBA doesn't. For data tasks, Python's a natural choice. We've also had requests for R, which is on our roadmap. reply ThrowawayTestr 15 hours agoparentprevCan you make it a desktop application? reply breckognize 12 hours agorootparentWe have development desktop builds. A few product questions for you: Would you be ok with a new file format or do we need to save to .xlsx? How important is Python in a desktop version? Would you need integration with conda or virtualenv? How much do you think we should charge? reply emeril 9 hours agorootparentI live in excel in a multibillion dollar company as the \"excel\" guru... though my company doesn't really have big data generally speaking I would love a performant desktop excel replacement - I miss using excel 2003 - it has been downhill ever since... That said, I'm clearly not the target market for this product though I believe there's some degree of frustration with how poorly modern excel works with many files now which this product may address. I would guess that making this a light-weight and performant (not electron) desktop app (ideally portable so installing it wouldn't require admin access) with substantial feature parity and compatibility with excel would be a big deal - I'd install it right away... reply fragmede 10 hours agorootparentprevXLSX is an open format, why invent another? xkcd 927 python in the app needs to be wholly self contained so whatever the backend it uses, it's separate from anything else on the system. charge through the nose for desktop clients because of how much it costs in development and support time. people who want native clients so it's not up on a cloud somewhere for whatever reason can pay for that privilege. reply gamegoblin 5 hours agorootparentI implemented an Excel format processor and the Excel format does not lend itself well to super-high-performance for large datasets unfortunately reply ThrowawayTestr 11 hours agorootparentprev1. A new format would probably be okay, but make the icon for the application green so it looks like an excel file. It can open excel files right? 2. A lot of our sheets use VB/macros so a scripting language is pretty useful. Not sure about the other stuff. 3. No idea, around the price for an office 365 seat? reply ant6n 16 hours agoparentprevDo you sell a software or a service running in a browser? Is the computation happening on my computer or your cloud? reply gamegoblin 16 hours agorootparentAll computation happens in the cloud. This is really nice because we can scale up and down to fit your data size. If you need a supercomputer for 30 minutes, we can get you one. reply ethanwillis 12 hours agorootparentWhat happens if your service is acquired and shut down? Is there any local fallback? reply strongpigeon 16 hours agoprevAs someone who used to work on Excel, awesome work and congrats on the launch! I get that it's easy to bash on VBA, but I'd argue it's what made Excel what it is today (though maybe not the language/runtime per se, but rather the ergonomics). I feel pretty confident saying that probably 5% of the world's economy runs on VBA macros that were started by some eager worker that was tired of doing repetitive tasks and wondered what that \"Record Macro\" button did. I've heard that same story personally from so many users, about how they learned to code by hitting \"Record Macro\", doing something and looking at the resulting code. Their macro then grows and grows and ends up powering the entire business, but becomes an unmaintainable mess. If you add the ability to record macros and maybe a VBA -> Python cross compiler, that would probably be killer for a lot of people. Though honestly, I've seen some stuff in VBA that's probably best left alone (e.g.: a self-rewriting VBA macro). That being said, I'm sure your biggest hurdles are going to be cultural rather than technical. Excel is just so ingrained in so many business. But I genuinely wish you best of luck and am rooting for y'all! reply padjo 16 hours agoparentVBA macros were my route from teenager in a warehouse to a 20+ year career in software development reply cyrialize 13 hours agoparentprevOne of my previous work places with an auditing data set company. We'd collect data sets from various sources (like public filings from the SEC) and the we'd send them over to different research teams to enrich the data in various ways. That company was very Excel heavy - which made sense, we had data entry, accountants, and other people who worked in finance. My CTO told me a story about how one day a member of a research team asked for a new computer. The old computer worked fine, but the employee wrote a VBA script in Excel that would crunch data... and wouldn't finish until 3 days later. This employee wanted a new computer so that he had one to use while the other one was crunching data. We ended up taking his VBA script and putting it into our codebase. reply gamegoblin 16 hours agoparentprevI want to hear more about this self-rewriting VBA macro reply strongpigeon 16 hours agorootparentFrom what I remember (that was almost almost 10 years ago, so a bit foggy now), it was some finance/trading people (the crazier stuff is always from finance people) that were doing some optimization work and found they had better performance by putting back the values in the macro itself and rerunning it. Something to that effect. (Or was it for versioning? I forgot the details honestly) I think they were using file system calls with some tools to rewrite the file directly. But IIRC it's possible to do it just via the API. reply gamegoblin 16 hours agorootparentAs the guy who wrote most of the Row Zero backend engine, I pray to god I am never asked to implement anything like this :) reply datadrivenangel 15 hours agorootparentprevThis is absolutely finance-tier insanity. I bet it made them a lot of money. This is like running into issues with non-converging iterated calculations... reply tacone 15 hours agorootparentprevMe too. Just wondering if it will finally awaken and try to destroy humanity :) reply specialist 13 hours agoparentprevEarly '90s, VBA was awesome. I never understood two of Microsoft's owngoals: 1) The lack of a migration path from workgroup (LAN) to client-server for Access et al. So dumb. SQL Server should have become a first class citizen of Access. Or Access become a viable front-end end to SQL Server. Where swapping JET and MSSQL was a drop-in no-brainer. (Maybe that happened later...) 2) Not unifying tabular data. And then make Excel and Access \"modalities\" (?) for accessing that data. Lotus' Symphony (successor to 1-2-3) was so awesome; hybrid database and spreadsheet. aka The Correct Answer™. And Symphony was on DOS! (Lotus' Improv was even cooler. I wish I knew why it didn't succeed.) I guess all this ML data pipeline Parquet NumPy stuff finally separated tabular data from how it's used. Yay. I haven't used the Microsoft stack in anger since late '90s, when Java emerged, so maybe the .Net/CLR reboot mooted my complaints. I never had the chance to use Borland's tools (Paradox, QuattroPro) in anger, so don't know if they did any better. reply airstrike 16 hours agoprevI'm a bit obsessed about spreadsheets and as someone who's building something similar (but not identical), it feels great to see all these next-gen spreadsheets on HN. Spreadsheets are hard to do and even harder to do right, so congrats on launching—although given your backgrounds I don't think you ever lacked the manpower to let this be a technical challenge ;-) My initial reaction: * It does feel pretty fast * but spreadsheets on the browser always represent an inferior UX * the data tables / formula tables are a solid idea * no self-hosting outside of Enterprise makes switching to this harder than it ought to be * limiting the free plan to 3 sheets feels like a strange decision reply pimlottc 16 hours agoparent> * but spreadsheets on the browser always represent an inferior UX What do you mean by this? Inferior to a standalone app? Inferior to some other design in general? reply airstrike 16 hours agorootparentInferior to a native app. Navigating it with the keyboard is clunky, the UI is never as crisp and responsive, it's harder to save and open files... the list goes on reply dustingetz 16 hours agorootparentIs it true? Google sheets is great, google docs is great, and hyperlinks! How much native app fast UX is due to using local state on disk? The future is not on local disk! reply airstrike 16 hours agorootparentGoogle has the benefit of having all of Google drive around it But even then, the benchmark is Excel. Nearly every Excel user is saving files to disk. Companies like to own data in a shared drive on a network. Maybe making networked drives better is another problem that needs solving, but I don't think spreadsheet applications should disregard that and just hope everyone moves to online. At a minimum you should give users a choice (which Excel/Office does, by the way) And we still haven't talked about navigating the UI with the keyboard. Limiting power users to online-only is like telling vim users they have to use Notepad++. Sure, they can do everything they could in vim, but it's overall objectively worse reply dustingetz 16 hours agorootparent> Companies like to own data in a shared drive on a network. Do they actually like that? Or is that the weight of 30—no, 50—years of legacy momentum? shuffling files around is the worst part of knowledge work! pseudo-files are the worst part of Google Docs, i want a unified graph! reply layer8 16 hours agorootparentThe benefit of files is that they are a consistent, application-independent abstraction. You can copy, move, rename, backup, version, and generally organize them however you want, restrict or grant access, without being constrained by what the respective application supports. Importantly, you can organize files from different applications together without those applications having to know anything about each other. Hyperlinks are no substitute for the object-like, independent nature of files. Applications that are not based on files create their own little separate universe, or rather island, that isn’t really interoperable. reply dustingetz 16 hours agorootparent> files are consistent and application-independent I see diverse, inhomogenous state schemas that are deeply coupled to the originating application (internal data structures serialized to disk!) and have arbitrary legacy structural constraints (\"document\") as well as seams between application silos reply layer8 15 hours agorootparentYou’re talking about the file contents, not about files as objects. Regarding the file contents, how is that different when the data is proprietarily stored in hidden SaaS databases? reply dustingetz 13 hours agorootparentI sense a trap, but i'll bite – APIs and schemas and other logical data models can be remixed, at least in principle. Physical data models (i.e. coupled to storage layout) are too low level to be useful, all you can do is load them back into into their originating app. reply airstrike 16 hours agorootparentprevI agree 100%, but you still need to give them the option so they can transition from legacy to next-gen And we still haven't talked about keyboard navigation! reply dustingetz 15 hours agorootparentOk, you're right – i don't think a smooth transition is possible in the office market – but I also don't think you can disrupt Office from within. Example: the thing that disrupted the New York Times was not a better newspaper, rather Facebook reply airstrike 15 hours agorootparentI agree! Which is why I'm not building the next Excel, but rather something different which offers (or \"will offer\") ~feature parity with Excel spreadsheets but approaches knowledge work and document authoring from an entirely new angle reply jeffbee 14 hours agorootparentprev> the benchmark is Excel That's certainly true among a subset of users who demand Excel power features, but it is not a universal benchmark. People who more highly value collaboration might prefer Google Sheets. There are tons of users and use cases where the choice of local storage is irrelevant or even a drawback. reply wdh505 16 hours agorootparentprevMicrosoft teams has the option to open all shared files in the browser so you can have multiple \"teams\" Microsoft files open at once. This is the direction spreadsheets are going. I agree it feels clunky to me who grew up on excel the application. I memorized a few dozen keyboard shortcuts that are all broken in the \"teams collaboration browser spreadsheet\" sigh. reply _trampeltier 14 hours agorootparentIn Teams you can not work on a Excel Sheet shared in a chat. You have to share it in a Team .. you have to ask the IT to create a Team for you first .. great. reply datadrivenangel 15 hours agorootparentprevTeam's files is sharepoint under the hood, which is why it sucks. Sharepoint would be so good if the UI was faster and more consistent. Also why is the search so impressively bad? reply Cyberdog 12 hours agorootparentprevIndeed. In addition to the UI issues, there's no way this product can be \"the fastest spreadsheet\" when it's browser-based. By definition it runs at least ten times slower than native apps will. reply samstave 16 hours agorootparentprevFor certain apps, it feels much more comfortably mentally compartmentalized when that focus is not in browser... (this is just my opinion), but I find that I typically have so many tabs open - I like to have certain things on not my browser (at times a tab can crash the whole browser) Attempting to import from various sources (urls and upload) and it fails: https://i.imgur.com/YV865bw.png It also stalls for a really long time arttempting to link to large CSV URLS... and it fails on JSON. This 37kb CSV file took over a minute to load: https://i.imgur.com/tHN4Wq0.png It has ONE ROW. I assume im holding it wrong, the local CSV has thousands of rows: https://i.imgur.com/fujf1p5.png --- I had to reload the session to get it to allow import, and this 37... and it took 24 seconds to import the data: https://i.imgur.com/LnFIgL3.png Linking to a URL and hitting import it thinks for a bit or fails.... reply breckognize 16 hours agorootparentWe wrote our own CSV parser to get fast import performance, and we do occasionally encounter novel encodings and weirdness. I sent you an e-mail to get more details. reply nickjj 16 hours agoprevAt the time of this comment (edit: since this comment, they addressed this, check the replies), on your pricing page you have \"SOC 2 Type II Security Compliance\" not being checked off on the free tier but it's checked in the others. The same thing applies to \"HIPAA Compliant and BAA\" except this isn't enabled for free and pro plans. What makes the free tier different here? Are you storing free data in a different area with many less restrictions on who has access to it? How do I know what I upload is safe from being analyzed or sold? Are you using this data to train any data models but only in the free / pro tiers that aren't SOC 2 or HIPAA compliant? reply tptacek 16 hours agoparentGenerally: the free tier won't get you the documentation --- to get the SOC2 report or the BAA, you need a paid plan. Which makes sense as a segmentation strategy. Especially in the case of SOC2, where providers really should charge for that report. reply otterley 16 hours agoparentprevIME working at SaaS providers in my past, there's no practical difference in the underlying implementation; it's market segmentation. It's a great way to help SaaS providers attract more revenue. Customers who care about these compliance regimes are the target cohort and are more likely to pay for the ticked box. reply breckognize 16 hours agoparentprevThere's no difference in how data is stored or processed between the tiers. We updated our pricing page to address the confusion. We only provide the SOC 2 report or BAA for Business accounts. reply nickjj 16 hours agorootparentThanks, I do see a difference now. Both compliance types are checked on the free tier. Can you please answer the question about how our data is viewed and or used internally? reply breckognize 15 hours agorootparentWe do not use customer data. From https://rowzero.io/security: \"Row Zero does not use customer data for any purpose.\" reply eichin 17 hours agoprevhttps://spreadsheets-are-all-you-need.ai/ has a 1.5G (in excel-binary save form) spreadsheet that would make an entertaining benchmark (not a particularly relevant one for your target market, unless you want to get into \"frightening educational tools for AI researchers\" which is not a notably excel-friendly space - just a hilarious one.) reply gamegoblin 16 hours agoparentI have a demo I will post soon that runs MNIST in Row Zero, we should be able to handle all of GPT2 without too much trouble, probably a lot faster than Excel, we may have to try it! reply jcuenod 15 hours agorootparentThis deserves its own Show HN! reply gamegoblin 15 hours agorootparentI will post it next week! It's very fun to play with. Also a good demo of Row Zero's templating capability — I can make a template so when you click the link, you get your own copy you can mutate and play with. reply sdenton4 16 hours agoparentprevA similarly important benchmark is DOOM fps: https://youtu.be/J2qU7t6Jmfw?si=YNuNBDS7ti8gmpqc reply jasongill 17 hours agoprevThis looks really promising. We generate about 1gb of financial CSV files per day for finance/accounting/audit teams and the number one response is \"wow this data is great, but my computer just can't handle it\". The biggest issue that I think I would run into in my organization is that all of the spreadsheet lovers are Excel die-hards and they refuse to even use Google Sheets, not to mention something else that isn't Excel. But maybe they could finally be convinced to stop complaining aboutrequesting huge files with something like this... reply gamegoblin 17 hours agoparentWe try really hard to be Excel-compatible with all our formulas and hotkeys, etc. If there is a formula or hotkey we are missing, let us know and we will add it (usually withImport from Amazon S3. The bucket rowzero-public-datasets has some neat ones. reply chatmasta 17 hours agorootparentOh, cool! I saw that menu option but I didn't click it since I assumed I'd have to enter some details. I'll check it out. Thanks :) reply laborcontract 14 hours agoprevI'm an Excel power(max?)(ultra!?) user and the only thing I want from an alternative to microsoft word on windows is a drop in replacement to their alt shortcuts. Those are the air that I breathe when using Excel and fast isn't fast if you're working slow. reply billylitt 13 hours agoparentRow Zero frontend dev here. Totally understand where you're coming from; we've heard this request from others and are interested in exploring a seamless solution for `alt` reliant power users like yourself. Drop me an email if you'd be interested in user-testing a solution! billy[at]rowzero.io reply cha42 13 hours agoprevDo you have any simd-optim in parsing all those large files ? I have read that you write some of your own parser for perf boost. (I am one of the author of https://github.com/V0ldek/rsonpath) reply gamegoblin 13 hours agoparentNo SIMD in the parser path... yet... :) The main win from writing our own custom parser is writing directly into the final in-memory format from the parse stream without any intermediate allocations or data movement. Awesome project! Adding it to my list of things we may plug into in the future. reply cha42 12 hours agorootparentSealable spreadsheet is also awesome ! Congrats for the delivery. reply voltaireodactyl 6 hours agoprevThis may be way off course but I’m currently looking for a QuickBooks replacement with many of the features you describe. Is building a QB replacement possible within your app, if we can bring our own financial data? reply breckognize 5 hours agoparentI don't know enough about QuickBooks to say we can replace it. But we do have enterprise accounting teams using us to pivot millions of rows of general ledger data. So if you're looking to (e.g.) create summaries of categorized expenses, we're great for that. Where are you importing your financial data from? reply JoiDegn 5 hours agoparentprevWhat are some of the features you are looking for? Just curious because Im working on a Quickbook replacement probably with a different focus but I'd still love to hear what you need. reply fallingsprings 13 hours agoprevOut of curiosity, do you support/have plans to support the broader set of Excel shortcuts that reference the ribbon? (Alt +shortcuts). Lots of excel power users more or less exclusively use the keyboard to navigate, and so have muscle memory for almost everything they do, including more niche operations (Insert Line Chart = Alt N N 1, or Change Column Width = Alt H O W). Will be a very hard sell to banks/other financial services corps if you can't match that aspect of excel. (They will probably also want local files and a native app). reply billylitt 12 hours agoparentGood question - we've heard this feedback before and have some ideas for how to bake alt-shortcuts into the app for the muscle memory crowd. So, not yet, but its very much on our radar. reply phantarch 4 hours agoprevCongrats Row Zero team on the launch! Excited to see all the cool ways people will think to use the product :) reply matrix1010 8 hours agoprevFor people who want to process their XLSX/CSV files locally and want some easy to use ETL tool, maybe take a look Tablesmith: https://tablesmith.io/, it's free to use. reply nojvek 9 hours agoprevOne of the powers of excel is that it is your data on your machine. You can work fully offline, no SaaS monthly crap. Row zero is nice, don’t get me wrong. But the whole idea of me giving them gigabytes of data, or the creds to production database is a big no no. If a 3rd party has your creds, they will eventually get hacked. reply novoreorx 9 hours agoprevNot trying to relate with AI, GPT, or LLM in a spreadsheet product is the best thing to me, kudos! reply gamegoblin 5 hours agoparentWe will add a little tasteful AI stuff later, but it's not the highest priority. Top priority is big data and fast data. AI is relatively easy to layer on later. reply novoreorx 1 hour agorootparentI agree, AI should not be the central focus, but when appropriately integrated, it can be highly beneficial. reply serjester 16 hours agoprevI'd love to see a generative AI integration (hopefully something far better than Google's attempt). I tried to figure out how to write a python function and quickly got lost. Seems promising though. reply gamegoblin 16 hours agoparentIt's very high on the list for post-launch feature additions Copilot for formulas, auto-write python, etc, it'll all land soon! reply crashabr 8 hours agoprevMy current issue with Google Sheets is that it gets really slow when I connect tries to load data from dozens of sheets into a single one using multiple importrange. Does your solution work better for this use case? reply billylitt 5 hours agoparentThis use case is where we shine. Sheets is a great tool, but its performance is limited by your browser's memory, whereas Row Zero compute is happening in the cloud. Give it a try and let us know what you think! reply sidcool 16 hours agoprevCongrats! This looks awesome. I would love to get a sneak peek of the underlying architecture. It takes a lot of confidence to say faster than Google spreadsheets by a 1000x! reply skadamat 17 hours agoprevHonestly, I'm really excited about this next generation of spreadsheet software. - Causal.app - Rows.com - Equals.com - and at least 50 others I've found I'm waiting for someone to create a really high performant spreadsheet engine that runs in WASM to power even more spreadsheet-y applications. The direct manipulation of spreadsheets is super underrated. reply ozim 16 hours agoparentI just wonder how come there is market for these when Microsoft has Excel online. Any company that has O365 has Excel online as well. reply gamegoblin 16 hours agorootparentExcel is really great if you don't have too much data. We love Excel! But we just had too much data for it, so we built the solution we wanted. We can handle 100x more data than Excel (online or desktop), Google Sheets, etc. reply skadamat 14 hours agorootparentprevExcel online still struggles to work with databases well. In classic disruption theory (the real theory by Clayton, not the TechCrunch 'disruption'), these products have less features but are simpler and can win the low-end of the market then move up-market over time. I suspect that people glued to M365 ecosystem are the LAST ones to consider leaving Excel online, but that's okay! reply sidcool 15 hours agorootparentprevExcel online is not a pleasant experience. Google sheets is much better. Row zero seems way better. reply elforce002 14 hours agorootparentGsheets and Excel are hard to beat, so I assume they're focusing on big companies. reply WillAdams 17 hours agoparentprevAre any of these multi-dimensional? Still looking for a replacement Lotus Improv (can't justify Quantrix Financial). reply breckognize 16 hours agorootparentRow Zero supports Python pandas, which handles multi-dimensional data. So you can process your data with Pandas in the code window and then view \"2d slices\" of that data in the spreadsheet UI. Feel free to message me at breck at rowzero.io if you'd like to do a session together to get you started. reply WillAdams 7 hours agorootparentI need something which allows me to interactively re-arrange the date on-the-fly --- if you've never worked w/ Improv it's hard to express. There was the beginnings of an opensource implementation, Flexisheet, or see: https://instadeq.com/blog/posts/no-code-history-lotus-improv... reply jkaptur 12 hours agoprevQuick bug report: I set A0 and A1 to 1, then set A2 =SUM(A0:A1)+A0+A1. As expected, A2 evaluated to 4. Then I right-clicked on the 1 row header and inserted 1 row above. Expected: A3 should become =SUM(A0:A2)+A0+A2 and evaluate to 4 Actual: A3 becomes =SUM(A0:A1)+A0+A2 and remains 4 until it is edited, at which point it evaluates to 3. reply gamegoblin 12 hours agoparentI just tested in Excel and Google Sheets and our behavior matches them here, unless I am misunderstanding the repro steps. For me, A3 becomes =SUM(A1:A2)+A1+A2 and remains 4 as it should be. The action I am taking: A0=1 A1=1 A2=SUM(A0:A1)+A0+A1 (evaluates to 4) Right click A0 and click \"insert row above\" Now I have: A0=empty A1=1 A2=1 A3=SUM(A1:A2)+A1+A2 (evaluates to 4) Thank you for trying to find edge cases! I have put literally hundreds of hours into stuff like this. Let me know if you have different repro steps. reply jkaptur 11 hours agorootparent> Right click A0 and click \"insert row above\" Instead, right click A1 and click \"insert row above\". Google Sheets (and, I'm 99.99% sure, Excel) adjust the range inside the SUM to be three rows high. I've put quite some time into this sort of thing too :) reply gamegoblin 5 hours agorootparentDidn't know who I was talking to, looked you up now, amazing ;) Thanks a lot for the bug report. We actually used to implement the range-extension logic in Excel/Sheets but removed it during a cut-paste refactor due to the complexity (I'm sure you know...) and resolved to add it back when someone asked for it. I actually found edge cases where Sheets and Excel don't do range extension logic the same way. So we don't do the range extension for now, but the 4 not getting recomputed to 3 is definitely a bug, will fix! reply runningamok 14 hours agoprevCongrats! As a sheets user I appreciate that rows are seamlessly added as you scroll past the initial sheet size. reply billylitt 11 hours agoparentAs much as I love clicking \"Add 1000 more rows\", we thought continuous scroll might be appreciated :) reply spacehunt 15 hours agoprevIs there a way to push data into it, rather than have it pull data from data sources? I have some use cases where users wantSSO > Contact US Yeah no thanks. reply CiTyBear 16 hours agoprev [–] Looks really nice. Too bad you take businesses into hostage regarding the SSO. This is even one one the main argument in your Enterprise plan. I know this is common behavior but I find it sad to have such an important security feature proposed only on latest plan. Hope this trend will end soon. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Row Zero is a cloud-based tool enabling business teams to analyze vast data volumes in a spreadsheet interface, connecting to diverse data sources and facilitating real-time collaboration.",
      "The tool integrates with Python for in-depth analysis, enhances data processing speed, and accuracy, targeting business intelligence, finance, operations, and marketing teams for streamlined data analysis."
    ],
    "commentSummary": [
      "Row Zero is a high-performance spreadsheet app, faster than Excel and Google Sheets, developed by experienced engineers, featuring native Python integration and seamless large dataset imports.",
      "Discussions are ongoing about enhancing security through modern authentication methods, improving collaboration tools, and enhancing Python support for users.",
      "Comparisons with Excel and Power BI highlight Row Zero's speed and functionality, with considerations for transitioning from legacy systems and the benefits of native apps over online platforms for data handling efficiency."
    ],
    "points": 240,
    "commentCount": 197,
    "retryCount": 0,
    "time": 1709222264
  },
  {
    "id": 39549194,
    "title": "Distance-Based Workout Tracker: Self-Hosted Web App",
    "originLink": "https://github.com/jovandeginste/workout-tracker",
    "originBody": "I tried some web tools to track my workouts (specifically, running); some (like FitTrackee) came close, but I always found annoyances. So I decided to build my own. Specifically geared towards distance-based workouts, such as walking, running or cycling.",
    "commentLink": "https://news.ycombinator.com/item?id=39549194",
    "commentBody": "Workout Tracker – self-hosted, single binary web application (github.com/jovandeginste)223 points by jovdg 20 hours agohidepastfavorite86 comments I tried some web tools to track my workouts (specifically, running); some (like FitTrackee) came close, but I always found annoyances. So I decided to build my own. Specifically geared towards distance-based workouts, such as walking, running or cycling. eitally 12 hours agoI came into the github not expecting to like this, but you've done a great job so far! I currently use a combination of Garmin Connect (track on watch & head unit), Intervals.icu (for fitness/freshness and more obvious tracking of performance improvement over time via various power & HR metrics), Strava (for segment and route performance change over time, and of course the social aspect... and routing), Komoot (only for cycling routing), and Elevate for Strava (just testing now, but it looks like a crappier version of intervals.icu so far), and Smashrun for running-only tracking. Oh, and Veloviewer for cycling-only performance/history and route visualizations. It's a lot. The problem is that there isn't a single place where you can get both the holistic metrics and the plan in a way that most users will want (which almost mandates a Strava interface). Apps like Veloviewer or Elevate already have this, but both either fall very short functionally or specifically only target a subset of features. When I complete a run or ride, the first thing is to check Garmin to see how it related to, or impacted, my physiological health metrics, since I rely on Garmin for things like sleep quality, training readiness, and load tracking. Then to Strava to see how it went compared the previous times on the route and look at segments, or perhaps to give kudos to someone I was with. Then to intervals.icu to dig into the performance metrics (power, HR). Intervals.icu is great because it handles workout planning, too, but the one big thing it's missing is routes & segments for IRL activities. Veloviewer has this, but it is missing all of the training/planning/fitness features. Imho, the killer app in the space is going to be whoever is able to augment the planning/tracking pieces, which are largely grunt work to develop, with the social bits that Strava has. The existential risk for indy devs is that your product is just a feature for Strava, and you could easily become disintermediated (or they shut down or charge exorbitant rates for API access -- it's already highly rate limited). You're doing great work, but I'll keep paying for intervals.icu, Strava & Veloviewer for now because they all do slightly different things, even if there's significant overlap. reply pcchristie 8 hours agoparentI just can't get past the Intervals design. Wanted to like it but it's an absolute cluster for me. reply mgl 5 hours agoparentprevI use a similar mix but with runanalyze.com reply Solvency 5 hours agoparentprevI exercises like a fiend yet your slavish adherence to logging and scrutinizing these numbers sounds exhausting in comparison. Are you a world class athlete trying to shave .02 seconds off a 10K? If not, try just going outside for a jog or mountain bike with some friends and go home and relax once. I bet your biometrics end up the same, if not better, and you don't even need to check. reply LandR 1 hour agorootparentI wonder how the accurate the biometrics even are on things like smarth watches ? reply tpm 1 hour agorootparentMany (even recreational) athletes use accurate measurements (heart rate chest bands, power meters integrated into bike cranks, ... continuous glucose monitors). reply idiotsecant 3 hours agorootparentprevThat's pretty condescending. For some people the data is not just a means to an end. Its an end itself. reply mattegan 15 hours agoprevSweet! Would love to host this myself and get rid of my Strava subscription. I wrote a Python script that iterates through all of my activities there and downloads the `.gpx` for each - I could share the code if interested! Not sure how you'd integrate it into your app - maybe a \"import from Strava\" page could handle the Strava API auth? I also love this idea of self-hosting some web apps (especially if they're containerized). I setup a `util.` subdomain and have started putting a few things there at different root directories. It's fun! reply jovdg 15 hours agoparentWould you mind cleaning up your script and open-sourcing it? It could be a sort of external component to many such projects... reply doakes 15 hours agoparentprevI'm interested in your Python script! reply xnx 16 hours agoprevI love the minor movement toward self-hosted web apps. I wish they were easy enough to setup as installing an app on Android or iPhone. reply giobox 16 hours agoparentWhile not as easy as an appstore, the modern Docker ecosystem IMO has made self hosting web apps at home super cheap/easy. Use your old PC, a Raspberry Pi, an EC2 instance, whatever you like, a single small Docker Compose file with Watchtower for auto-updating the container image when new one published etc. I've got self-hosted services that have ran untouched for years this way at home from a handful of lines of YML. Docker Compose will also take care of restarting the service on reboot/powercut if thats a concern. Given the ubiquity of container images, you can run largely any web application this way and the container image likely already exists - this project also has a Dockerfile and would run fine this way too. reply easyas124 9 hours agorootparentWhy would you use Docker at all? Just run the application behind a reverse proxy. Docker doesn't get you anything except an extra management headache and an abstraction that happily punches holes in your firewall. reply QuinnyPig 8 hours agorootparentIt may help to view it primarily as a packaging system. reply monksy 7 hours agorootparentprevEasier to upgrade, no extra system deps to mess up, security because you're shelled into a private environment, isolated storage, network routing. reply bogwog 5 hours agorootparentprevBecause \"installing\" an \"app\" becomes as easy as docker run coolapp:latest reply ClumsyPilot 8 hours agorootparentprevApp A requires a version 2 of whatever which conflicts with version 3 of whatever required by app B, and uses same port as app C and doesn’t like the smell of app D. Meanwhile app Z has to be compiled from source with a specific version of python and only installs on Tuesdays, buts it’s Wednesday if mercury is retrograde reply alex_lav 8 hours agorootparentprevI mean...the entire reason docker exists? So that I don't have to understand and manage everyone else's (sometimes insane) build and deploy processes? reply mnutt 11 hours agoparentprevThat was the promise of Sandstorm. \"Easy to install\" is important but sandboxing and data protection _even of the data you give the app itself_ is just as important. I really hope it or something like it succeeds someday. Edit: this app would probably make for a great Sandstorm app. reply imiric 10 hours agorootparentCloudron and YunoHost are Sandstorm alternatives that seem to have more traction. reply ramon156 16 hours agoparentprevHole in the market? Maybe a NAS-type system for self host? Something a bit stronger than a raspberry reply CardenB 8 hours agorootparentThis is basically what HOOBS markets. They've really fallen to the movement of open source though. I think there's still a world in which you can buy an rPi with software preinstalled and updaters packaged without much effort though. If that could take off, you could easily have this type of service. However, I think it falls apart because you need to govern the app store and that is expensive and difficult. reply xnx 15 hours agorootparentprevI do think there's a niche hole in the market for a cloud host that's as easy to use as the major app stores. Fiddling with hardware and network routing is way too technical. reply pcchristie 8 hours agorootparentDoes PikaPods do what you're talking about? I use it for Actual and for Paperless and it's awesome. reply panzagl 15 hours agorootparentprevSynology NASes are a popular choice. reply ClumsyPilot 8 hours agorootparentBut their CPU is potato. They still sell versions with 512 mb ram reply KomoD 12 hours agorootparentprevThere's Umbrel which sells a device like that, or YunoHost + your own device Umbrel is just plug-and-play with an app store, haven't personally tried it but looks very cool. reply jollyllama 13 hours agorootparentprevThey tried this with a device called Helm Personal Server. No, not the Kubernetes manager. reply KomoD 12 hours agorootparentNot surprised it failed, $250-400 + $100/year and the only thing it could do was host email, calendar and file sharing? reply VyseofArcadia 12 hours agoparentprevI think a good question to ask is, what is gained here by being a web app? Couldn't this just be an Android or iPhone (or even desktop!) app? reply kristiandupont 4 hours agorootparentWell that question implies that there is some intrinsic reason not to build it as a web app. That may be the case for you if you prefer Kotlin or Swift or something, but it wouldn't be the case for me. reply jovdg 12 hours agorootparentprevI can access it from any of my devices (laptops, desktop, phone), and my family can use it too. reply xnx 12 hours agorootparentprevWeb app = Always on. No interruption from power outages or wifi problems. Higher bandwidth and more anonymous (if it's going to make requests). Virtual android device in the cloud might be a great model for it since Android has good built-in security and process isolation. reply filleduchaos 11 hours agorootparent> Web app = Always on. No interruption from power outages or wifi problems Web apps don't magically produce their own compute, power and networking. They still need to be run on something. (Web app does not automatically equal SaaS, especially in this case that is explicitly billed as self-hosted.) reply easyas124 9 hours agorootparentUnless you're self-hosting on the same computer you're using, a web app is, by definition, software as a service. But then, either you're just... running a program, or your self-hosted application lives somewhere else that still requires power and network connectivity. I promise, everyone, it is very legal and very cool to just write applications that run without TCP roundtrips. I promise. reply filleduchaos 8 hours agorootparent\"Unless you're self-hosting on the same computer you're using, a web app is, by definition, software as a service\" This sentence makes no sense to me. \"Self-hosting\" and \"software as a service\" are diametrically opposed things. reply xnx 8 hours agorootparentprevExactly. I want the reliability of professional hosting, the ease of a phone app install, and the ownership/control of open source. reply VyseofArcadia 9 hours agorootparentprevI still can't use it if my power or internet are out though, even if the server still has both. (Rephrased to be less mean.) reply xnx 8 hours agorootparentMy little tangent here is about trying to combine the benefits of open source apps that are fully under the users control with the benefits of cloud hosting while trying to avoid the configuration and administration headache of a raw AWS/Azure/Hetzner/Heroku/Digital Ocean/etc. setup. reply weakfish 9 hours agorootparentprevI think this comment is unnecessarily rude reply VyseofArcadia 9 hours agorootparentYou're right. Thanks for calling me out on it. I've toned down the snark. reply easyas124 9 hours agorootparentprevWeb apps are absolutely not always on, and they are absolutely not more anonymous... reply xnx 8 hours agorootparentAWS uptime is better than any server I've run at home, and an AWS IP is less personally identifiable than my home IP. reply vimeh 16 hours agoprevThis is great! Am a fan of this kind of focused, self hosted application. Clean UI, too. Interested to hear your thoughts on HTMX when you get around to eval-ing it reply jovdg 15 hours agoparentI did some evaluation of HTMX a few weeks ago; while I understand the appeal, it does not yet ... vibe with me. Also, the use for HTMX is still very limited in this project. I don't think this will really change, because the project is fairly focused and will remain so for the time being. Maybe there is an opportunity with the statistics and graphs? Like switch the buckets between km, miles, and number of those (1 km, 5 km, etc), of zoom to more years. reply Osiris 13 hours agoprevBy \"Workout\" do mean only exercise that can be tracked via GPS (walking, running, cycling)? It doesn't seem to track any indoor activities like weight lifting. reply jovdg 13 hours agoparentYes, GPX based workouts. Did you look at FitoTrack? reply e12e 8 hours agorootparenthttps://codeberg.org/jannis/FitoTrack ? reply jovdg 2 hours agorootparentYes; could have sworn they had weight lifting by default, but apparently not. They have other indoor activities, however. But elsewhere in the thread, people have suggested many options... reply wredue 13 hours agoparentprevYeah. I was also gleefully looking cause lifting trackers are universally garbage. Diet trackers (Calorie Counters) are also almost universally a pain in the ass, but at least they’re tolerable. There is not a single combined lifestyle experience that is any “good” IMO. reply dominick-cc 13 hours agorootparentFor lifting, check out Gymstro. Free and very good. I recently switched over from FitNotes (also free and good, but quirky). reply switchers 12 hours agorootparentI was about to recommend fitnotes but if you've swapped then I've got something to check out... reply gejose 7 hours agorootparentprevVery curious about what you dislike about lifting trackers. I built a weightlifting tracker primarily for myself a little while ago and also published it. I tried to keep the UX as simple as possible. If you're ever looking to try out another app, give it a shot! https://titangymapp.com. reply anty 2 hours agorootparentTitan looks similar to the workout app that I'm developing: EverBeat! I'm also developing it for myself right now, that's why it is Android only atm. My goal is to employ subtle gamification to get the user/me to the gym more often. I was wondering where you got the data from to know what muscle group is used in each exercise? reply InitialLastName 12 hours agorootparentprevI've only been using it since I saw it recommended a few weeks ago, but for Hacker's Diet[RIP, 0]-style diet tracking MacroFactor has been unbeatable. Nothing dumb or meaninglessly social, just weight+nutrition tracking. In recognition that humans are imperfect and that both sides of CI/CO are more complicated than is apparent, they have built the system around a \"best effort\" vibe; systematic bias gets filtered out through feedback (if you regularly underestimate the calories you actually eat, the system will feed back by lowering your limits). [0] https://www.fourmilab.ch/hackdiet/ reply wredue 11 hours agorootparentCI/CO is what I used to lose weight (back to normal BMI as of today, actually). I won’t lie and say it was “easy”. I had to make real changes, and some were not easy for me (cheese. Peanuts…). But after I bought a scale and weighed my portions, it was readily apparent what was wrong for me. The other things most “experts” (who are trying to sell you their brand of dieting) will sell you about CICO not being “everything” is that some food are awkward. Like celery, which has 5 calories uncooked, 30 cooked. Ultimately, as long as you are weighing your portions, those oddities should generally not be “breaking” a CICO diet. I appreciate this other suggestion, for the app. I will check it out. reply InitialLastName 9 hours agorootparentCongrats on your loss! I used the aforementioned Hacker's Diet (CI/CO with some habit-forming structure) a decade ago to get my BMI from ~33 to ~23 (at which point people started telling me I looked sick, so I backed off). CI/CO definitely works, but both sides of the equation need to track a feedback loop in order to function correctly. On the one hand, your body will reduce its base metabolic rate as you start depriving it of nutrition; on the other, the nutrition labels on food aren't necessarily representative of what your body can get out of them. The other issue is that often the foods people with weight issues eat aren't the same foods that are conducive to weight loss... it's psychologically very difficult to maintain a calorie deficit when many of those calories are taken up by sugar water, for example, where it's much easier when the calories come from nutritionally complete, fresh foods. As usual, proactive lifestyle change is rarely about the facts of what should be done (literally everyone knows they should be eating vegetables and exercising) as much as the psychology (it's difficult to consistently make decisions that add stress to your life). reply ralfhn 8 hours agorootparentprevCheckout Fitbod for lifting. It's pretty good! reply abound 16 hours agoprevThis looks truly excellent, nice work! I've been using RideWithGPS, which is a great tool, but doesn't scratch my \"self-host everything, own my own data\" itch, so I'll definitely be checking this out. I've built (and use daily) my own barebones workout tracker [1], but it's for non-distance-based workouts (i.e. rep-based stuff/lifts), so this compliments that nicely. [1] https://github.com/bcspragu/stronk reply juiiiced 13 hours agoparentHow many things do you self host and how do you manage them all? reply abound 10 hours agorootparent$ kubectl get deployments.appswc -l 25 Minus the header, looks like ~24. I use a single-node Kubernetes cluster running Talos [1]. Running a single-node cluster is kinda dumb architecturally, but adding a new service takes : spec: template: spec: { containers: [{ ports: [...] command: [...] volumeMounts: [...] }] volumes: [...] } And then running: cue export \\ --out yaml \\ --expression 'deployment.' \\ --expression 'service.' \\ kube.cue /.cuekubectl apply -f - Where `kube.cue` sets reasonable defaults (e.g. image is /). The \"cluster\" runs on a mini PC in my basement, and I have a small Digital Ocean VM with a static IP acting as an ingress (networking via Tailscale). Backups to cloud storage with restic, alerting/monitoring with Prometheus/Grafana, Caddy/Tailscale for local ingress. [1] https://www.talos.dev/ [2] https://cuelang.org/ reply reidjs 4 hours agorootparentInterested in how you're using DO as an ingress. I currently run a droplet that's reaching its capacity because I'm running all the services directly on that underpowered machine. I would much rather run them from a local computer. Is it pretty straightforward to set that kind of thing up with tailscale? reply abusaidm 16 hours agoprevThis is a great looking project. The combo of single binary Go-lang and the ability to package assets like css, js and html makes this a killed multi platform distributable one click run. It might be an interesting experiment to have the UI be installable as PWA and thereby not need the electron stack to achieve the common functionality offered by these apps. reply turtlebits 13 hours agoparentThis would a great candidate for https://wails.io/. I've been building a lot of utility desktop apps and the static binary is around 9mb (uses the system webview). reply ggpsv 12 hours agorootparentI'm also eyeing wails for a couple of personal desktop apps that I want to build soon. I built toy app first towards the end of last year to try it out and I was impressed. It felt like a nice middle ground between Electron and Tauri for desktop apps using a web front-end. Looking forward to the release of v3! reply freedomben 13 hours agoprevThis looks super neat and would be very useful. What apps/devices do people use to collec the GPX information? I found a few in the Play Store but they don't look very privacy respecting... Edit: Currently eyeing GPSLogger: https://gpslogger.app/ reply jovdg 13 hours agoparentCheck out FitoTrack reply agilob 16 hours agoprevAlternative https://github.com/aceberg/ExerciseDiary reply matthewhartmans 7 hours agoprevWell done and congratulations on the launch! Looks incredible mate! reply Otek 13 hours agoprevAnyone has good resources on importing such data from apple - I’m using Apple Watch but would love to visualize raw data myself with grafana reply mikestew 10 hours agoparentSibling mentions HealthFit, and there's RunGap as well. I use RunGap simply because HealthFit doesn't do Garmin. With the exception of Garmin, either could be a good choice. reply klaushardt 12 hours agoparentprevHealthFit can export Workouts as .gpx or .fit files. reply nose-wuzzy-pad 14 hours agoprevLooks great! Any chance you plan for it to work with .fit files? Thanks for making this! reply jovdg 14 hours agoparentYes, as stated in the README... I would love to. There seem to be some libraries to parse the .fit file, so when I have more time, I'll take a look at that. reply jovdg 11 hours agorootparenthttps://github.com/jovandeginste/workout-tracker/pull/5 reply tamimio 15 hours agoprevLooks good!! On time for the coming spring! reply shantnutiwari 16 hours agoprevnice. How do you track teh distance (walked or running?) reply jovdg 15 hours agoparentNot sure exactly what you are asking about, so I'll be verbose :-) I use an app called FitoTrack (also FOSS), which records my location while running. It stores my GPS position every few seconds. When I'm done, it auto-exports a GPX file to a folder on my phone, which is synced via syncthing. Then I (manually, for now) upload the file to my self-hosted workout tracker. The GPX file contains some data per measurement point; this is the second point in some random GPX file:56.721055759195581.8200000524520874 2024-02-28T09:28:26ZSo it contains speed (average since previous point), elevation, location and time (offtopic: it took me a while to understand that this elevation is not the actual elevation above sea level; only yesterday I figured that out and fixed it in the code!). The Go GPX library some of this information, and some extras (like max and min elevation, max speed, total up and down, etc. over the whole track). Then I perform some more calculations (like putting the points in buckets per km and per minute), and calculate the estimated location using a geocoder library. Then, finally, to estimate the difference between walking, running, or cycling, I take the average speed and guesstimate from there. This may be wrong some times for some people, and could be improved on. Or maybe I should include an AI here? (just kidding) reply abound 16 hours agoparentprevWithout diving in too deeply, it looks like they use a package called `gpxgo` [1], which has some code for calculating the moving distance of a given GPX file [2]. [1] https://github.com/tkrajina/gpxgo [2] https://github.com/tkrajina/gpxgo/blob/5e7c336e94dac3583a07c... reply wly_cdgr 13 hours agoprevHeck yeah, this is awesome. Any chance of some kind of social features? I want to control my data, but I also do want to be able to share my fitness stats & journey with a specific self-selected group of people: friends, running club mates, etc Thank you for making this! reply jovdg 13 hours agoparentDo you mean some kind of \"share with Social\" button? Or do you mean anonymous access to your recent activities? reply lapetitejort 15 hours agoprev [–] Slightly off topic, but I got tired of looking for a good weight training tracker, so instead I bought a notebook and a pencil and I've been using it for nearly two years. If I want to change up how I long my workouts, I just write them in a different way. I've changed how I log number of sets/reps, weight per set, etc, a few times and I've never had to re-program how the info is displayed. I log how I feel after the workout and use that and the weight from last session to guide what I'll be doing in the current session. I don't need a graph as I can clearly see the values going up or stagnating. I honestly feel like the analog process has helped me keep consistent. It's motivated me to get more notebooks to track stuff like household chores. Sometimes the best app is a piece of paper. reply zhivota 13 hours agoparentI have a Google Keep note where I record the results from the last workout. I figured out I really only need that. I don't need history, I just need to know how long ago was the last workout, how much did I lift / time under tension, so I can make a decision about today's program. I love fancy graphs for my endurance work but for lifting, I feel like this is pretty much all I really need. reply mangateparu 1 hour agorootparentIn a similar vain but with history and progression, I have a Google Sheet where I track all my lifts. Its easy to spot when I should increase my lift. Maybe every 5-6 weeks I need to make a new sheet so it doesn't get too unruly. eg. ExerciseSetsRepsFeb 26Mar 1 ---------------------------------------------------------- Dumbbell Chest Press3 x 865, 8 8 870, 8 8 5 Dumbbell Chest Fly3 x 816, 8 8 820, 8 8 6 ExerciseSets x RepsFeb 28Mar 3 ---------------------------------------------------------- Knee Jumps3 x 8BW, 8 8 8BW, 8 8 8 Pull-ups.3 x AMRAP4, 3, 2.3, 3, 2 KB Swings10M EMOM(216) 10M(2*16) F@7M Bent DB Row3x8.(swap cable row) 60, 8 8 840, 8 8 8 reply salad-tycoon 14 hours agoparentprevI bought the starting strength app, no subscription and has a couple of quality of life improvements like a timer, plate calculator, graphs etc and some actually helpful no nonsense videos and book excerpts. very little distractions. Agree with you though, pen and paper is hard to beat and cheaper. reply drBonkers 13 hours agoparentprev [–] Can I see your paper template? What if you want to graph tonnage? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The individual experimented with different web tools for tracking workouts, particularly running, but found them unsatisfactory.",
      "Subsequently, they opted to develop a personalized tool tailored for distance-based activities such as walking, running, and cycling."
    ],
    "commentSummary": [
      "The author created a self-hosted web app for monitoring distance-based workouts, inspired by platforms like Garmin Connect and Strava.",
      "The discussion revolves around fitness data tracking, debating self-hosting web apps, Docker for hosting, and deployment options.",
      "Users exchange insights on workout tracking apps, tools for self-hosting projects, and the significance of tracking fitness data for long-term lifestyle improvements."
    ],
    "points": 223,
    "commentCount": 86,
    "retryCount": 0,
    "time": 1709214344
  },
  {
    "id": 39557188,
    "title": "Introducing Struct: Feed-Centric Chat Platform",
    "originLink": "https://struct.ai/blog/introducing-the-struct-chat-platform",
    "originBody": "Hi HN! I’m Jason, a product designer at Struct Chat.At Struct, we&#x27;re frustrated by the clutter, distractions, and inefficiencies plaguing existing chat platforms like Slack and Discord.We&#x27;ve built a radical new chat platform that leverages threads, feeds, and AI to help alleviate these problems, and give you back more time in your day.Struct uses a thread-first approach to keep conversations on-topic. It applies AI-generated titles and summaries to help you decide what&#x27;s worth your attention. Threads are then organized in a real-time feed, keeping all your conversations up-to-date and at the ready, eliminating the need for channel hopping.Comprehensive search tools make finding things a breeze, and Strucbot, our AI assistant can answer questions based on what it’s learned from prior threads. It can even proactively respond when it notices repeat requests, providing quick answers so you don’t have to. Structbot is fully GPT-4 enabled, so you can riff with Chat GPT and your peers (generate code, ask questions, all the good stuff) without ever switching apps.Struct is available on Linux, Windows, Mac, and even works as a Slack interface. Give us a try and let us know what you think.",
    "commentLink": "https://news.ycombinator.com/item?id=39557188",
    "commentBody": "Struct – A Feed-Centric Chat Platform (struct.ai)210 points by jdplex 9 hours agohidepastfavorite99 comments Hi HN! I’m Jason, a product designer at Struct Chat. At Struct, we're frustrated by the clutter, distractions, and inefficiencies plaguing existing chat platforms like Slack and Discord. We've built a radical new chat platform that leverages threads, feeds, and AI to help alleviate these problems, and give you back more time in your day. Struct uses a thread-first approach to keep conversations on-topic. It applies AI-generated titles and summaries to help you decide what's worth your attention. Threads are then organized in a real-time feed, keeping all your conversations up-to-date and at the ready, eliminating the need for channel hopping. Comprehensive search tools make finding things a breeze, and Strucbot, our AI assistant can answer questions based on what it’s learned from prior threads. It can even proactively respond when it notices repeat requests, providing quick answers so you don’t have to. Structbot is fully GPT-4 enabled, so you can riff with Chat GPT and your peers (generate code, ask questions, all the good stuff) without ever switching apps. Struct is available on Linux, Windows, Mac, and even works as a Slack interface. Give us a try and let us know what you think. WA 0 minutes ago1. OpenAI and the whole baggage of training on company data should be optional. Titles and summaries might not be that important and could be provided by the user. 2. Never ever will I buy anything from a company that looks like it's hiding where it's from and who's behind it. No, your profile pictures and a bunch of names do not count. This whole website could be scam for all I know. This might be my European (German in particular) point of view, but here, every non-private website must have a full imprint with the companies name and the Terms of Privacy need to have the full address of the responsible entities (usually the company plus a data protection officer). This is a good thing! We need transparency in businesses who collects money and since GDPR is very relevant for my own business, I want to evaluate how easy it is to get the necessary paperwork (data processing agreement) between your company and mine. Right now, you're hiding behind a hey@whatever email address, zero transparency. reply vatican_banker 6 hours agoprevCountless tools have promised to highlight \"what's worth my attention\". None of them have worked for me or my team. What's different on Struct? A few questions I jotted down while watching the video on Struct's landing page: 1. the concept of channels seems to be important on Struct as channels are the starting point of threads/feeds. Could you clarify the concept of channels on Struct? Is it just a concept to group users? Can you also chat on channels? 2. Conceptually how do you handle the fact that only the threads on the realtime feed are visible to the user? Maybe there's a low-signal high-activity thread that takes space and hides the high-signal low-activity thread which results in users missing important information or reminders. 3. Tags are crucial for filtering threads, is there a way to \"police\" the tags? Using tags usually grow into a mess of similar-but-not-the-same collection of text. Think of JIRA tags. 4. How to handle threads created independently by different users but discussing the same topic? 5. Not a question, but I'd be interested in knowing more about private conversations between two parties. It's mentioned only briefly in the video. Hopefully these questions don't come out as overly critical. The tool definitely has potential. reply mrjn 5 hours agoparentGood points. Let me try and address those. 1. Channels for Struct are just groups of people. You engage with a channel, just like you'd engage with a user. You start a thread, and then mention users or channels to define access for that thread. You don't go to a channel to chat. 2. A thread can only take so much space on the feed. The height per thread is fixed. So, both the threads you mentioned could be in the feed, visible to the user. We'd also add a way to archive or mute a thread for some time, that should help with cleaning up that feed. 3. There's a limit on how many tags can be added per thread. Tags help with adding context, and more importantly, very useful for creating custom feeds. We use them for tracking task priorities, which converts our Struct into a lightweight task management system. Nothing to beat Jira, but it's nice to have just one platform for all discussions. We'd allow tags to be merged. That should deal with similar tags growing parallely, creating confusion. And perhaps, we can also limit new tag creation to moderators. Many controls are possible. 4. You can merge similar threads. You can also select a bunch of chats in a thread and fork them into a new thread. We'd maintain links between threads. Coming soon. 5. To do a private conversation (DM), just mention the users you want to DM with. For example, mentioning @struct would invoke a DM between you and the bot. Mentioning yourself would create a \"self-DM\" (useful for TODOs, or just remembering stuff, or as a starter thread which you later mention other users/channels to expand reach). Keep the questions coming. I know the market is filled with chat platforms, and Struct might feel like \"yet another\". But, I've looked and waited for years for other platforms to do what I thought should be obvious, but they didn't. And that's why I decided to build Struct. reply grncdr 2 hours agorootparent> You can also select a bunch of chats in a thread and fork them into a new thread. We'd maintain links between threads. Coming soon. Just want to chime in and say this is an essential feature IMO, I hope it’s prioritized. reply mrjn 1 hour agorootparentAbsolutely! We already have one chat message fork working. Backend supports multiple message forks as well. So, very close. reply 7moritz7 4 hours agoparentprevIn the time you wrote this interview you could also just have tried the app yourself reply JauntyHatAngle 4 hours agorootparentI think that rather avoids one of the best parts of hacker news - getting the vision/justifications from the creator directly. reply olivierduval 1 hour agoprevCongrats for your product ! BTW: I'm old enough to remember the \"forums' days\"... and actually, it really seemed to me that you just rediscovered forum (you know... threads that are bumped when there an update) The only major differences I saw was: - the UI is more \"chat-like\" / \"facebook-like\" - forums where statically structured by an admin... but it looks like there no real way to structure your threads in folder or things like that (important on big orgs with lots of different subjects concurrently) On the other side, mails may be (more or less) structured by \"conversation\" (sadly it's not a strong standard so not reliable) and conversation may be structured by personal folders - there's \"AI\" and that's quite tiring these days. I see the point of having a dynamic summary of a thread... but * either your threads are 'chat-like' so with simple content... and no summary is needed * or your threads are 'mail/forum-like'... and I'm not sure how it will work (sincerly) Sooooooo... I'm not sure to know what to think about it. Is it \"just\" a marketing 'chat-like' with 'AI buzzword' app ? Or did I miss something that make it really fundamentally different from forum threads or email conversation ? reply mrjn 58 minutes agoparentStruct is quite inspired by Discourse. It's a unique blend of forum and chat application. So, you know, every forum, including email, has threads and feed. But, forums are asynchronous, not designed around real-time chat. The challenge when applying concept of threads and feeds to a chat platform is that you need to make it real-time. And that's the hard thing that Struct is tackling -- with the real-time feed design that we have. I disagree a bit about, \"if it's chat, it doesn't need summary\". I've been part of enough conversations which go on and on, to know that's not true. Even with the small team we run, we have threads which have like 100s of chat messages, going back and forth. Having summaries really help. Or, at the very least, you'd appreciate the titles, so your feed would make more sense. You can structure your threads using tags, and create feed around those tags. That'd be the equivalent of \"folders\". (Reminds me of when Gmail came out and people had to learn to map their labels on SMTP folders) The difference in a Struct thread, v/s say a Discourse thread would be this. Struct emphasizes short form, real-time, back and forth communication. Discourse emphasizes long form, well-thought through, one-off posts. Former is chat, latter is forum. This is long topic, but something I think about a lot. Designing Struct is hard exactly because of this balance between structure, knowledge and being real-time. reply olivierduval 32 minutes agorootparentOK, so the point is the \"real time\" interactions Actually, at work, I'm either in 2 situations: - I need a quick answer for a simple problem ==> chat (because it's more about getting a simple info and I need it now if possible, or the sooner) - I need a complete/correct/thoughtfull answer ==> email (because I need someone to really think about it before answering and that person is not available all the time and has other things to do too... so quality of answer is worth waiting) It may happen that complex email need some clarifications... but if the subject is complex, it usually requires a meeting It may happen too that a simple answer lead to a complex question... but then we're back at email. So, I'm not sure that your tool would fit my way of working and/or the places I work for. Moreover, I'm quite sure that \"realtime\" is one of the WORST possible way of working because it's giving other people a way to manage your time at work (interrupting, etc.) reply mrjn 13 minutes agorootparentReal-time platforms cause interruptions, yes. But, that's where we are today with Slack. So, we're starting off from that point. Let's take real-time chat platforms, and reimagine how we can make them more efficient. More useful. Less noisy. That's the idea for Struct. The list of people we email is different from the list of people we chat with. In a team, that means, team comms happen on chat platform. And external comms happen on emails. Within a team using a chat platform, it's unlikely that people would chat, and then say, hold on, let me send you an email. If that happens, that's very rare. What's a lot more likely is you jump from a chat conversation into a video call. Hence, the need for a nice integration with video service (like Slack has huddle). So, for a certain set of people (your team or community), you're going to use the chat platform. And therefore, the chat platform should be built to enable conversations with good retrieval and focus -- so it can be used for complex conversations. Struct hits that pretty well. reply creshal 59 minutes agoparentprev> I see the point of having a dynamic summary of a thread... That was a really cool feature in Discord, for a while: The AI always completely missed the point of the discussion and made hilariously awful summaries that provided entertainment for hours. reply mrjn 52 minutes agorootparentIt's hard to do summaries for chats in channels. Just not enough focus in them to be useful. Could also be the model that they were using. I've tried a few and found GPT4 to be most accurate. GPT4-Turbo is still a question mark. reply lankalanka 1 hour agoprevrunForThread.executeReq.Get.messages: executeReq.client.Do: Get \"https://api.openai.com/v1/threads/thread_*******/messages?li...\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Hope one day new products don't need to depend on OpenAI... reply mrjn 1 hour agoparentArr.. we should clean up the error messaging a bit. Not to hide OpenAI, but just make it appear nicer. I like OpenAI. GPT4 works really well. Some others we tried were cheaper, but the quality wasn't as good. reply pandemic_region 1 hour agoparentprevI'm honestly considering sometimes to ban .ai domains on my router until the whole thing has gone back to normal. But seriously, this thing actually looked very interesting until the point where I read \"... and AI\" :-/ Guess I'm not the target audience. reply n42 7 hours agoprevThis is one of the first Slack competitors that has caught my attention. I really think you're on to something here. I have one thought after watching the video (other than a generally positive feeling about it): It feels, somehow, less personal. Where Slack rooms felt like digital _rooms_, this gave me the feeling of an office bulletin board. I'm imagining being a new employee at a company that is fully bought in on Struct, and somehow I feel like developing personal connections with my coworkers would be more difficult. I don't really get why, that's just my first impression from the demo. Otherwise, as a currently funemployed but past SV eng leader, I would have loved to have had this at my last company while dealing with all of the communication chaos that entails. reply mrjn 7 hours agoparentFounder here. Jason (OP) and I think about this a lot. The balance of cluttering everyone's names on the left panel, v/s making it feel like you're in the same room as these people. If you look at feed based platforms, they tend to get very asynchronous (almost forum like). And so our design for the real-timeness of the feed was to emphasize the fact that you're in this room with these people, and they're talking right now. It's really fun when a new thread pops up, or a new chat emerges in the feed. That is exciting. I think perhaps it would help to show a list of online people, so you feel like starting a conversation. Or, perhaps, just seeing other people participate would help. There's surely room to optimize to get the conversations going. reply n42 4 hours agorootparentyeah, you've hit it on the head – I think what I felt was missing in the demo was a feeling of presence. a hard thing to capture. anyways, awesome job and good luck! reply mrjn 4 hours agorootparentThanks! We'll be brainstorming ways of making user presence felt, while keeping it clean. reply jdplex 6 hours agoparentprevI can imagine a series of useful threads always at the ready for onboarding new people. But I hear you, ensuring new employees to feel warm and welcomed is crucial for every organization. Especially true for orgs that lean more remote. Orgs can still use @random channels, @club-channels and you could set up an @annoucements to have spaces for fun links and bulletin materials, and other ways to get to know your peers. @struct we lean pretty work-focused, we love building. But we should bring the fun too! reply rglover 7 hours agoprevThis is killer. Love the idea of focused threads while still having channels. Really like the design of everything, too. Only question: long-term plans? Before I commit a lot of data to something like this it'd be good to know it's not a VC flip and dump. reply mrjn 7 hours agoparentManish, founder here. We intend to keep Struct running. Struct is built with a small of 3: Backend, Designer and Frontend. It's not that capital intensive. We have a roadmap of features that we're excited to get to, combining both teams and community usecases: - Enable audio / video calls, webinars. - Various spam control systems for community. - Improving the AI bot and recall, making it cheaper to run. - Index the docs shared on Struct, so AI can respond to questions from there as well (obv while maintaining access control) - the list goes on. The thing is. What excites me about Struct is that, it combines a bunch of use cases for us.Struct is replacing two chat platforms (team and community), a task management system, a ChatGPT, my personal TODO list, a publicly indexable knowledge-base for us. It's amazing what you can do when you can trust a system to not forget. reply andoando 1 hour agorootparent3 people for this product is highly impressive. reply vincnetas 4 hours agoprev\"Create a feed for your CEO's threads and stay in sync with your company's latest vision and goals. Jump in the conversation with the right answers and get that promotion you deserve. \" This made me smile with a little sad smile, as i again and again understand it is not important what you do it's important who sees you doing that :) reply mrjn 4 hours agoparentTalking as an engineer who learnt management by building a startup: Do things. But also, make them visible. I remember my days at Google (first job). I used to just build and build... and had a hard time trying to convey why what I was building was important. In fact, my first manager skipped over me, and promoted the entire rest of the team (still hurts). When I switched teams (and manager), they told me I was 2 levels below where I should be at. Long story short: Get into those conversations. Don't be overbearing, but have your presence known. reply iamflimflam1 2 hours agoparentprevIf a tree falls in the forest… If a developer goes the extra mile… reply Brajeshwar 6 hours agoprevThis looks promising. Can you please re-work on the pricing presentation? Honestly, I'm not smart enough to do the math and worry if I might hit an explosion on discussion and pay the price for it. Give examples, present it in a tabular format, perhaps with examples. Please don't make me think. Also, what's with minimum 6-character requirement for a Struct name? Anything from 4-characters should be fine, right -- there are no more international standards beyond 3-characters! Edit: A small request, please make the App Icon solid like all the other macOS Apps. reply mrjn 5 hours agoparentYeah, good point. We have data from orgs who're using Struct. Short story: It's significantly cheaper compared to Slack. Struct charges 9.97 per month. And we only charge for AI usage, not number of users or threads or chats. Base price comes with 200K tokens free, which is enough for most small orgs, if they're only using it to generate thread titles and summaries. In fact, that's the average token usage by Typesense, one of our users. Apache Druid is around 400K tokens per month. 1K tokens cost 3 cents, so 200K tokens would cost $6 (on top of the base price). Both use Struct as a knowledge-base. When we start enforcing monetization, we'd allow users to set a cap on monthly spend, so there're no surprises. Beyond the base price, it can be as cheap as you want. reply Brajeshwar 5 hours agorootparentAwesome. Thanks. Great work. I signed up to try it out and use it personally before deciding for the other team/company. I would really wish to retain org name to 5-characters. ;-) Update: I can actually change the URL from inside Struct. It would still be nice to be able to choose that during the sign-up steps. reply mrjn 5 hours agorootparentYou might have found a loophole, ser. Enjoy the exploit! reply emmanueloga_ 6 hours agoprevBTW I think this is brilliant because summarization is one of those things where AI works really well. Out of curiosity, I wonder if you could share some of the tech you used? Any dgraph or badger under the hood by any chance? :-) How about hosting? Big three or something else? K8s or no? Elastic Search? etc. Edit: Also curious if struct is supposed to online scrape slack/discord or become and standalone chat app now/in the future. reply mrjn 5 hours agoparentThe Stack is this: - Go - Postgres - OpenAI (biggest bill producer is this) - React / Next.JS - Loops.so for Emails - Imgproxy for handling images - Microsoft E5 for embeddings (not OpenAI) - Typesense for Search (also a user of Struct) - Figma (design) - Framer (website) - Server on Hetzner Badger wouldn't have made sense to use instead of Postgres. I'm thinking of using Badger for other purposes. I do use ristretto and roaring bitmaps a lot internally. Struct has a very nice integration with Slack. So, you can actually use Struct as a Slack interface. Essentially, your peers can be using Slack, while you could responding to them via Struct - think Superhuman for Gmail. We have a good integration with Discord too. Both are being used by OSS companies to generate public knowledge bases from their Slack/Discord conversations. [1]: https://github.com/struct-chat/embedding reply leobg 2 hours agorootparentIf OpenAI is your biggest expense maybe I can help. Have reduced LLM cost for several companies by factors of 10 and more. Email in bio. reply nXqd 51 minutes agoprevI really like the UI, it's exactly how I use and share notes within my colleges for years with my current system. Is there anyway that I can bring my own model ( integrating with my data stored on struct ) to answer my questions? reply mrjn 49 minutes agoparentWorth a conversation for sure. Maybe find a time to chat? https://struct.ai/schedule-demo Or, you can jump in here and we can have a discussion: https://chat.struct.ai/join/DXVTmseDdBkeA6mG reply poorman 8 hours agoprevThis is awesome. Discord is the worst for trying to search and follow threads of information. Seems like I'm always answering the same questions over and over again. reply smt88 7 hours agoparentDiscord will inevitably add LLM-based search like Slack AI just announced, and you won't have that problem anymore. reply KTibow 6 hours agorootparentI know Discord is already trying some AI stuff out like Summaries, but AI search doesn't seem like the kind of thing that Discord would add, given their audience and that they took down Clyde AI. reply culopatin 7 hours agoparentprevI don’t see this going after discord but Slack or teams perhaps? reply mrjn 7 hours agorootparentFounder here. Struct is designed for both teams and communities. Because the problems affecting Slack and Discord are the same. We already have a bunch of community features, like publicly indexable knowledge base from threads, public-to-internet channels -- which are being used by many OSS companies (list on home page). Teams is a different product. Companies who use Teams are there because they have tight integration with Microsoft Office, which is the USP there. reply noeltock 3 hours agoprevInteresting how we've almost come full circle back to message boards, maybe time for phpbb/vbulletin to reappear? Next-gen will call it \"Slow Chat\" :) reply mrjn 3 hours agoparentI hear ARPANET was the real deal. j/k reply nextworddev 6 hours agoprevI think this product is useful only 1) if you have a lot of channels aka if you work in an enterprise setting, and 2) you get ping'ed a lot. reply mrjn 6 hours agoparentI'm biased. But, hear me out. We're a team of 3, and we think it's way more useful than Slack would have been. As mentioned elsewhere, Struct is replacing two chat platforms (team and community), a task management system, a ChatGPT, my personal TODO list, a publicly indexable knowledge-base for us. It's nice when you don't have to treat your chat platform as an ephemeral, knowledge void. reply dalberto 8 hours agoprevLooks very cool, planning on giving it a try. Also appreciate transparency around pricing. reply jdplex 8 hours agoparentThanks! Yeah, we've been thinking about fair pricing for a long time. reply stevage 4 hours agoprev> We’re forced into clicking through multiple channels just to catch up on our team’s latest updates. Combine this with the always-growing list of new channels, and information starts to get really scattered. This is one of those problem statements that just doesn't resonate for me at all. I really like things separated into the different buckets so I have a good idea how to find it again later. The idea of combining stuff into one feed is kind of a nightmare for me. reply mrjn 4 hours agoparentIt's really about choice. In Struct, you can create custom feeds, which can filter threads by channels / users/ tags, etc. You have that choice. In Slack, every one of these entities (and permutations for DMs) stands alone. It's set in stone, unchangeable. That itself is a big win. In fact, feeds are everywhere. That's how we consume almost all internet. RSS, News, Facebook, Twitter/X, Instagram. Feeds scale, they are powerful, they work. Instead, Slack has us jumping around channels and DMs for drips of messages, playing Whac-A-Mole. reply stevage 3 hours agorootparentIt seems like you're trying to convince me that I'm using the internet wrong :) I like the channel based structure of Slack. It gives me a confidence that I can always go back and find any message, it won't disappear, or be unfindable. I don't like the \"filtered feed\" model of Facebook and Twitter, because I'm never sure what has been filtered out. And the fact that everyone is getting a different view makes it hard to develop or maintain shared understanding with others. Feeds are fine when it doesn't matter if you miss stuff - jump into Hacker News, read some of the latest posts, then disappear for a bit. Not so good (for me) when missing things matters. reply mrjn 2 hours agorootparentOk. I should clarify. Struct doesn't do anything \"smart\" with your feed. It's the threads matching the boolean filters that you define (all threads has no filters), sorted by updated at timestamp desc order. There might be interest in building a smarter feed which uses recommendation algorithms (and therefore have different views for different people, assuming same channel membership), but that's not what Struct feeds are today. reply stevage 2 hours agorootparentAh cool, thanks for the clarification. reply wingerlang 3 hours agoparentprevIn my experience Slack becomes buckets within buckets, so I still rely on the 'feed' - i.e. the 'threads' aggregator to find which one of the dozen sub-buckets are actually discussing X and Y. Not to mention the infinite buckets with similar topics and names. reply danr4 2 hours agoprevLooks great, but hard to make the swap from slack without apps/bots/api. I'm sure you thought about it, but a slack-compatible bot api will probably really help with adoption. reply mrjn 2 hours agoparentYou looking for this? https://struct.ai/struct-for-slack reply jamestimmins 2 hours agorootparentSlack has a big app marketplace, which is helpful. Do you have plans to add something like that to Struct? reply mrjn 2 hours agorootparentYes. We'd be documenting Struct APIs. And also we want to identify the top 5-7 most popular Slack integrations, and build them ourselves for Struct. reply albru123 3 hours agoprevNice, a culmination of everything I hate about Slack and Teams in a single product! reply vishnumohandas 6 hours agoprevThat's one of the best landing pages I've seen recently. Congratulations on the launch! reply mrjn 6 hours agoparentThanks! Kudos to Jason (OP). One of the best designers I've had a privilege to work with. reply jdplex 6 hours agoparentprevFlying high on this comment! reply maxwell201922 6 hours agoprevThis product is way better than Slack IMO! We have so much information lost in Slack and hard to summarize the key insights with so much data dump! Kudos Struct team, what an exciting product we will test it with our slack integration. reply andoando 8 hours agoprevHello. Just an idea but could be nice to have the app embedded into your website as a demo, so we can interact with it ourselves. reply mrjn 8 hours agoparentGood point. Actually, you can join our Struct org: https://chat.struct.ai/join/DXVTmseDdBkeA6mG Chat there! reply neogodless 8 hours agorootparent* Desktop only reply p2hari 3 hours agoprevDo you have any plans of opening up API access or onboarding plugins and developer ecosystem? reply mrjn 3 hours agoparentYes, big time! I'm biased, but because everything is a thread, Struct APIs are nice and simple to work with. We'll be working on documenting them. reply muralimadhu 6 hours agoprevSomething that has always stopped us from migrating away from Slack is the integrations. We have 10s of integrations on Slack and those are a pain to migrate. Any thoughts or plans on how you would support that? reply mrjn 6 hours agoparentYeah, that's what we're going to focus on next. Identify the top 5-10 integrations to build. Would love to learn what integrations you use, and in what capacity. Based on my interactions, it's really only a few integrations that 80% Slack users use. reply emmanueloga_ 8 hours agoprevI installed the chatbot on my discord but it’s not indexing anything, do I need to sign up for a trial or something like that? It’s my own little discord server so I have a super low volume of messages :-) https://bembem.struct.ai/ reply mrjn 8 hours agoparentManish, founder here. On Discord, the bot only picks up Discord threads, not the chats in a channel. Could that be it? Unless there's some issue with sync. reply emmanueloga_ 6 hours agorootparentThat must be it! I don't really have any threads I don't think. Oh so Struct uses that as boundary to summarize? Makes sense, although a lot of Slack and Discord servers don't really use threads. Like, the golang one in Slack specifically asks people to NOT use threads, out of some usability concern, I think (screen readers or something). reply mrjn 6 hours agorootparentYeah. We now pick up chats from channels in Slack, and convert them into threads. Somepoint we'd do the same for Discord too. Chats in channels are too diverse to act like threads. Jury is out on how good those threads look. reply strogonoff 7 hours agoprevCan’t recommend enough Zulip. First-class topics, tools for organizing messages between streams & topics, open-source self-hostable, no engagement of ML out of the box (but probably possible thanks to Python plugin support), Vim-like keyboard shortcuts. reply kikki 7 hours agoparentI was about to say the same thing - I use Zulip at current employment - and after you get used to it it’s pretty great software that already solves all the problems struct aims to (without the AI nonsense) reply mrjn 7 hours agorootparentFounder of Struct here. I looked at Zulip before starting Struct. And I'm sorry -- I don't think it's the same. Threads in Zulip are really sub-channels, and the idea of a unified feed like \"All Threads\" doesn't exist -- at least, that was my perception of Zulip. And just looking at the site right now, it feels the same as before. I could be wrong. Struct is different. It's a reimagination of what a chat platform would look like if you were to completely give up the idea of chats in a serial log of channels (IRC, Slack, Discord), and embrace threads and feeds whole-heartedly. When everything is a thread, the platform can work remarkably well for users. reply strogonoff 10 minutes agorootparent> the idea of a unified feed like \"All Threads\" doesn't exist I believe it does. You get a feed of all messages; and still organized by a topic, quite conveniently. I don’t use it, though, since from anywhere in GUI I can immediately jump to the next unread message, irregardless of topic or stream, simply by pressing “n” on keyboard (preceded by Esc if I happened to be typing; my draft is saved reliably). reply smt88 6 hours agorootparentprev> unified feed like \"All Threads\" I don't understand why I'd need or want this. I saw it in the video and was horrified. Multiple feeds updating as I'm watching them is just too much going on. When I'm chatting, I don't want to pay attention to 10 different things at once. I'm most productive when I am working on one task (with a single topic), which may require me to refer back to Slack periodically. Struct looks like it has two differentiating features: 1) it surfaces irrelevant distractions in the All Threads channel, and 2) it creates the tl;dr summary. The former seems actively harmful to productivity, and the latter seems like it could be useful. reply mrjn 6 hours agorootparentYou could create a focused custom feed. For example, I have one for \"tasks assigned to me\". That's what I set to when I'm focused working. That's the beauty right -- you can control and filter what you see. As opposed to channel based interactions, where your boundaries are set in stone on channel creation. reply smt88 2 hours agorootparentBut I don't currently use Slack as a task manager and don't want an unstructured task manager with no due dates or tags. reply mrjn 2 hours agorootparentYup. That's the point I'm making. Slack is used as ephemeral messaging system, a knowledge void. Struct aims to proves that more is possible. reply iuffxguy 7 hours agoparentprevLooks cool, but unfortunately another company that treats single sign-on as a luxury feature, not a core security requirement by jacking the price way up to get SAML support. Companies really need to stop doing this. reply JoshTriplett 6 hours agorootparentSingle-sign-on is a large and painful feature to develop, that most people don't need, and that the companies that need it are willing to pay for. It's the perfect candidate for a higher-tier pricing structure. This makes much more sense than tiering on a feature that everyone needs. reply Solvency 5 hours agorootparentSSO in 2017? Sure. In 2024? No way. reply mutant 4 hours agorootparentprevAdd this to sso.tax reply techdragon 8 hours agoprevEDIT: This was an inadvertent copywriting/cms mistake. I'm leaving the original below for context only. >>> OLD POST FOLLOWSAI to 2x your productivity I'm out. Sorry, but this is just obnoxious. reply 9 hours agoprev[deleted] vlw 8 hours agoprev [–] https://xkcd.com/927/ reply mrjn 8 hours agoparentCan you send that via reddit, please? j/k reply rglover 7 hours agoparentprev [–] \"Stop creating new ideas because it makes me uncomfortable.\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Struct Chat, a new platform by product designer Jason, aims to tackle the issues like clutter and inefficiencies present in platforms such as Slack and Discord.",
      "The platform utilizes threads, feeds, and AI to maintain focused conversations, generate titles, summaries, and offers robust search functionalities.",
      "Structbot, an AI assistant powered by GPT-4, enhances user experience by answering questions, proactively responding, and smoothly integrating with Chat GPT on various operating systems and Slack."
    ],
    "commentSummary": [
      "Struct Chat is a novel chat platform enhancing communication efficiency through threads, feeds, and AI to maintain on-topic conversations.",
      "Users raise concerns about transparency, cluttered feeds, and balancing real-time chats with thoughtful posts on the platform.",
      "Integrating AI and features like tag structuring and chat forking, Struct addresses communication organization challenges and supports integrations with Slack, Discord, and OpenAI for customized feeds."
    ],
    "points": 210,
    "commentCount": 99,
    "retryCount": 0,
    "time": 1709254147
  },
  {
    "id": 39551457,
    "title": "Pioneering Neuroscientist Tackles Alzheimer's and Addiction with Ultrasound",
    "originLink": "https://www.youtube.com/watch?v=7BGtVJ3lBdE",
    "originBody": "anyone who's had experience with Alzheimer's disease knows the agony of watching someone fade away as it steals memory and at the end a person's own identity tonight we'll show you an experimental way to try and beat back Alzheimer's it's been tested on just a handful of patients but it caught our attention because of the doctor involved Dr Ali rosai who 60 Minutes first met 20 years ago Dr Rai is a neuroscience pioneer who's developed treatments for Parkinson's Disease and other brain disorders over the last year we followed this master of the Mind as he attempted to delay the progression of Alzheimer's disease and its worst symptoms using ultrasound we saw a Cutting Edge approach to brain surgery with no cutting the story will continue in a moment if we can we should not be doing brain surgery you're a brain surgeon I am but I should be out of a job because brain surgery it's cutting the skin opening the skull it can be barbaric you're going to go right in there it looked like a scene from a sci-fi movie make it a little bit more comfortable a Halo wrapped patient pushed into a tube we're ready to go as a team of doctors manipulate his brain from the other side of the glass gain high modulate power 3 minutes okay we're ready to go Dr oi Rai allowed us to witness his revolutionary attempt to use ultrasound to slow down the cognitive decline in three patients diagnosed with Alzheimer's disease it's never been done before there's no mirical cures here it's advancing medicine with calculated risks and pushing the Frontiers so we're targeting these areas Dr Rai and his team are focused on these red patches in the patient's brain scans the red indicates the densest beta amid protein that gummy protein is believed to play a major role in Alzheimer's by dis erupting communication between brain cells in people with Alzheimer's it accumulates much faster and over time these protein aggregates we call them plaques like plaques into arteries they keep on accumulating and impacting function there are two FDA approved drugs on the market that can help break up that brain plaque at a canab was approved in 2021 followed by lamap last year both are given venly but they work slowly typically you go into the clinic and you get an IV and you have the antibody infusion over 1 to two hours and you have to do it uh once a month or twice a month for 18 months and longer and during those 12 to 18 months the brain is continuing to progress Alzheimer's is not going away it takes so long because the drugs have a hard time getting through something called the bloodb brain barrier this tight filter of cell cells line the blood vessels to keep toxins from leaking into the brain but it also prevents almost all of the medication from getting into we think that that's what's causing the BB disruption by opening this year Dr Rai thought he could solve that problem with ultrasound the same technology that's been used for 70 years to give doctors a view of organs and Fetal development just good you're good come back he chose ultrasound because it easily penetrates the skull and can be focused like sunlight through a magnifying glass to help open the bloodb brain barrier and allow the drugs to rush in this way we're getting the payload the therapeutic payload exactly to the area it needs to go with a high penetration but we got to be careful because we want to be safe about this you don't want to deliver too much don't want to open the blood brain barrier too much because if you open it too much what could happen get bleeding in the brain you can get swelling in the brain you can get many other problems so you have to get it just right we will show you exactly how that worked and the early results in a minute but to understand why one of the country's most accomplished brain surgeons is betting on ultrasound okay open and close your hands for me you have to go back to 2002 when Dr Rai first caught our attention in a story morly safer reported on treating Parkinson's disease look up show me your teeth stick a tongue out very good Dr Rai was among the first to implant a pacemaker type device in the brain which stopped uncontrollable movements suffered by Parkinson's patients it's like traveling through a labyrinth as in the Greek myth and around every quarter you have that bloodthirsty monster that can jump on you so you want to be careful to avoid these areas that kind of implant surgery is now routine for advanced Parkinson's Dr Rai went on to write hundreds of scientific papers secur dozens of patents and present his Parkinson's research to Congress and the White House he could have gone to any big City Research Center but true to form he chose to try something different and move to Morgantown West Virginia where he is the executive director of the Rockefeller Neuroscience Institute it was a fantastic move because we're able to achieve so many things that would have been difficult at other institutions sometimes in the bigger institutions you may not be hungry as much for you may have a thousand different agendas and priorities here we think we have a very Nimble and agile team that can quickly get outcomes like in 2019 this is video Dr rai's team took when they were among the first to use ultrasound to treat Tremors for 15 years Dan wall had been suffering from essential tremor a neurological disorder you okay you got a hat on now okay all right very good ri's team focused ultrasound into a part of the brain called the thalamus to destroy a Pino size patch of tissue doctors believed was responsible for the tremors 180 elements converging right there wall was awake during the procedure touch my finger with your finger after 2 hours the 71 year-old's Tremor was gone I'm feel afraid I'm going to drop it you got it I've got it really good show yeah wow prise the Lord that success helped convinced Dr aai good morning that focused ultrasound could be adapted to patients with other brain disord MERS including Alzheimer's disease my first symptoms that I noticed were that I was having trouble typing at work did you think you had Alzheimer's no I didn't Dan Miller is just 61 years old his wife Kathy began noticing changes four years ago he kind of hit it pretty well and then I noticed he was um having trouble his clothes would be backwards MH and those kinds of things just little things little things yes a scan of his brain revealed what Dan had been hiding Mr Miller had a very large amounts of beta amid the red spots indicated a buildup of those beta amalo proteins the so-called brain plaque a marker of Alzheimer's Dr Rai explained to Miller he couldn't cure him of the disease but he hoped to slow its progression why take part in the trial if it's not a cure I have to explain to you that I was at the point you know like in Dante's Inferno where where it says abandon all hope you who enter here for me it was just you know let's do this you know what do I have to lose and you are infusion sir here's how it worked hours before the procedure Miller was given an IV treatment of attak canab one of those two new drugs to reduce beta ameloid plaque Miller was then fitted with this million dooll helmet similar to the one the team used to treat Tremor patients it directs nearly a thousand beams of ultrasound energy at a Target the size of a pencil Point basically the patient lies on the MRI table and the head goes inside the helmet and the patient is immobilized with a Halo or with a mouthpiece because we don't want movements to cause errors in our targeting in the brain is that comfortable THB up once inside the MRI machine gave Dr Rai a 3D view of the plaque would Target in Dan Miller's brain The Next Step was an IV solution that contained microscopic bubbles when hit with ultrasound energy the bubbles pry open that bloodb brain barrier okay ready we can Sonic in now there we go the bubbles start vibrating they're moving they're moving they start expanding so you can open the barrier temporarily now it's open for 24 to 48 hours and then it reseals so this gives you a tremendous opportunity for 24 to 48 hours with the barrier being open so now therapeutic can get inside the brain you can't hear ultrasound that noise is a signal to tell rai's team the ultrasound is doing its work very nice opening at the blood brain barrier each dot represents an area where all the waves all the ultrasound waves converge and open the blood brain barrier so this this is just one blast if you will one blast getting there and you're hitting one point one point and then it moves to the next one even though patients were awake they told us they didn't feel a thing it all took a couple of hours and they went home when it was over the three patients were given the treatments of ultrasound with infusion once a month over 6 months that's another Target right there the result beta ameloid plaque targeted with ultrasound were reduced 50% more more than areas treated by infusion alone that's the top of the head right there Dr Rai shared the three patients brain scans with us and the red indicates more density of beta ameloid plaques in the brain so you can see as you treat it with ultr closely at the areas outlined in white that were targeted with ultrasound and the drug you get reduction whoa that's after you can see the plaques are very significantly reduced by opening the blood rain barrier just in one area have your lay back Dan Miller and the third patient in the trial had larger areas of their brain targeted with ultrasound and this is his Baseline and then you can see here after 26 weeks there's a very dramatic reduction in the beta ameloid in the areas as outlined by this white Mark and now we're going to look at patient number three and this patient underwent antibody infusion therapy plus ultrasound you can see this area which is really amazing the ultrasound opened the blood brain barrier and the antibody went in faster and cleaned out the plaques what was your reaction when you saw this scan uh I mean my jaw dropped I'm like whoa I was actually even in the clinic seeing patients and the Pet Scan technician called and said oh yeah there's a big change I'm like how do you know we have to analyze it he's like no you can see it on the screen so what did you think when Dr Rai shared the the scans with you it was surreal you can really see it you don't have to be a doctor and understand what's going on there absolutely not even the red is decreasing that's amazing Kathy Miller says she can see it in her husband too who slips up once in a while but hasn't slipped further away he has trouble finding things I'll send him into the kitchen to get something and he's like it's not there and I'm like yes it is I can see it but he can't see it but if that's the worst that's nothing you'll take it I'll take it you feel hopeful about the future I do yes I learned that what I needed to do is accept that the Old Dan is gone and then start working on the new me which has a future Dr rai's team told us there's been no change in the ability of the three patients to do their daily activities since the ultrasound treatments ended in July now that Dr Rai has shown Focus ultrasound can clear beta ameloid plaques faster he has FDA approval to use ultrasound to try and restore brain cell function lost to alzheimer's what's the result of breaking up all those plaques to the damage that's already been done to the brain we don't know if it's going to reverse the damage to the brain because Alzheimer's the underlying cause is still occurring so we have another study that we're looking at with ultrasound first clear the plaques then deliver ultrasound in a different dose to see now if we can reverse it or boost the brain War for people with Alzheimer's when we come back we'll show you Dr rai's new way to use ultrasound to reset the brain and help people suffering from drug addiction the human brain contains a 100 billion neurons that's as many cells as there are stars across the Milky Way Dr Ali Rai has spent 25 years exploring this Frontier of medicine the surgical techniques and therapies he pioneered are in use around the world Dr Rai allowed us to see his latest research over the last year at the Rockefeller Neuroscience Institute in Morgantown West Virginia it includes revolutionary treatments for a brain disease suffered by 24 million Americans addiction the results so far have been lifechanging for the people we met once trapped by drugs the story will continue in a moment looking back I didn't have a chance what do you mean you didn't have a chance I couldn't do anything without having that drug um in my system Jared Buckhalter is the son of a coal miner at 6'3 he was a high school football standout who dreamed of playing wide receiver at Penn State but after a shoulder injury he got hooked on painkillers the very first time that I that I took that first pill um I I knew that I wanted that feeling for the rest of my life what did it feel like it's just pure Euphoria he took us to where he said he often went to buy drugs including heroin everybody in morgant town knows to to come here probably 17 18 years old you know just a kid Buck halter still looks like an athlete it's hard to imagine he was an addict for more than 15 years he told us he does not remember how many times he overdosed and that he couldn't stay clean for more than 4 days at a time I didn't know where I was going to sleep some nights you know my family didn't want me around anymore um I just I did so many things to hurt them that you know it was just too much for them to deal with four years ago a psychologist who' worked with buck halter introduced him to Dr Ali rosai who was gearing up to perform a new kind of brain surgery to treat severe addiction our protocol was people that have failed everything once you've tried everything everything residential programs multiple failers detox multiple times outpatient inpatient multiple overdoses I think classified it as endstage drug user I mean end stage makes you think that this is the end of your life correct um and hearing that at the age of 34 um it was crazy let bring the DBS electrod in Dr Rai thought he might be able to adapt technology he helped develop years earlier to treat Parkinson's disease to treat people with severe addiction we've been able to map out with Neuroscience Imaging there's a spefic specific part of the brain that is electrically and chemically malfunctioning that is associated with addiction so it's not just willpower it's what's happening in the brain it's a brain disease it's an electrical and chemical abnormality in the brain that occurs over time with recurrent use of drugs and this can be any substance as alcohol can be opioids amphetamines cocaine and they all are involving the same part of the brain and so your idea was what with the implant Parkinson's we implant that in the movement part of the brain that is electrically malfunctioning causing shaking in this case we're going into behavioral regulation anxiety and craving parts of the brain Dr Rai has seen the impact of addiction in his community the problem is so severe in Morgantown a vending machine dispenses the overdose antidote Narcan for free you got your baselin the National Institute on drug abuse agre to support Dr rai's attempt to fight addiction with a brain implant in 2019 the FDA gave him a green light to attempt the groundbreaking surgery change the cover please that is Jared Buck halter he agreed to be the first addiction patient in the US to get the implant Dr rai's team interviewed him the day before the surgery the best outcome uh possible would be you know just to cut the Cravings out and and make me felt a little bit better if you know if those couple things happen you know uh that's all I could possibly ask for at that time I was so desperate for a better life um that I was willing to do just about anything and I signed up to do it I think some people might look at this and think an electronic implant in the brain sounds a little creepy people maybe 50 years ago they say a implant in the heart sounds creepy now it's like normal 255 years ago people are saying what are you doing you're putting an implant in the brain for Parkinson's but now it is routine part of standard of care for advanced Parkinson's this is video from the 7-hour procedure I'm ready you are surgery so new it didn't have a name yet Dr Rai opened a nickel-sized hole in Buck Halter's skull then he directed a thin wire with four electrodes deep inside Jared are you okay yes sir all right Jared was awake during the surgery why was that necessary to map the brain we have tiny microphones the size of a hair we put inside the brain and they're going slowly with micro robots they go at increments of a thousand of a millimeter very slow we drive them into the brain and we're listening to the neurons talking to each other in addiction we want to find the area in a reward center so that confirms where we are in the brain once we listen and say okay that's the right sound then we put the final therapeutic pacemaker what does it sound like static electricity which may be electricity to you but is music to my ears music because Dr Rai says it's a signal that he found the right spot in the brain for the implant once in place the wire was connected to a device placed below the collar bone okay the electrical pulses it sends to the brain are intended to suppress Cravings Buck halter said it was painless post surgery the system is adjusted remotely with a tablet computer is needed when they turned the unit on it was an immediate change what was the change just felt better you know just felt like I did prior to ever using drugs but a little bit better and it was at that point that I knew that I was going to have a legitimate shot at doing well in all four patients with severe drug addiction had the implant surgery one had a minor relapse another dropped out of the trial completely but two have been drug-free since their operations including Jared Buck halter who's been clean for 4 years if you hadn't met Dr Rai if you hadn't gone through this implant do you think you'd be sitting here talking to me today you may be talking to my parents you know those that have lost their their loved ones to a drug overdose um um but you wouldn't be talking to me there's there's no doubt about that ah beautiful beautiful the surgery was a success but opening someone's skull is always risky Dr Rai thought he could reach more patients quickly if he used ultrasound he was already using it to treat other brain disorders and was convinced focused ultrasound could Target the same area of the brain as the implant is this brain surgery without a knife it is indeed so this is there's no skin cutting there's no opening the skull so it is brain surgery without cutting the skin indeed now this is just the measuring part right Dr Rai explained how his team would be the first to treat addicts by aiming hundreds of beams of ultrasound to a precise Point deep inside the brain so the area that we're treating is the reward center in the brain which is the nucleus of comass which is right down at the base of to this dark area and then we deliver ultrasound waves to that specific part of the brain and we watch how acutely on the table your cravings and your anxiety changes in response to Ultrasound how is the ultrasound making a change here ultrasound energy is changing the electrical and chemical Millie or activity in this structure in the brain involving addiction and Cravings just re setting them and giving them kind of a fresh start at this point it seems like the brain is being reset or rebooting of the brain and the Cravings are less they're managed anxiety is better so now that allows them to interact with the therapist it's very important to know that this is not a cure but an augmentation of the therapy by reducing the cravings and anxiety that's so overwhelming that the therapist has difficulty working with the patient last February we watched Dr isai used focused ultrasound to treat Dave Martin who told us he's been surrounded by friends and family who use drugs his whole life when did you start using drugs um when I was 7 years old seven yes I did drugs for 37 years what kind of drugs were you using anything I can get my hands on inside the MRI Martin was shown these images of drug use to stoke his Cravings his legs were moving a lot and his very agitated a simultaneous brain scan allowed Dr Rai and his team to immediately spot the area in the nucleus accumbent that was most active I'd like to see the targets one more time 90 watts of ultrasound energy were beamed at a Target the size of a gumdrop ready Sonic all right there we go within minutes we noticed Martin's foot that had been anxiously bouncing was still and he told rai's team that those same images of drugs he was shown earlier we're now not sparking the need for a fix heroin is going down meth is also going down marijuana is down marijuana's down a lot actually good keep on sonicating the day the procedure it was the best day of my life I didn't experience the same effect as like the times before you didn't feel like I need that I no I didn't feel like I needed the The Urge or the desire to use wasn't there anymore So within 15 to 20 minutes of treatment they craving and anxiety melts away and we're seeing this pattern in multiple instances then they can walk away after this there's get off the table and go home and how long does this entire procedure 1 Hour 1 Hour 1 hour have you been around people still using drugs yes yes unfortunately I have um and what happens it didn't even trigger me uh I used to use in ingeniously with with needles and it was a a little while ago not too far back but um this one individual was trying to hit theirself and they couldn't hit and they asked me can can you hit me do you actually put drugs I actually stuck them drew the blood back you know now before when I drew their blood back it would like make make me sweat cuz I couldn't wait to hit myself but this time it was just like God I hope they don't OD and I kill them here you know but I didn't have any urges or desire or anything so Dr rai's team told us Dave Martin did admit to taking one painkilling pill at a party in December still 10 of the 15 patients in the ultrasound clinical trials have remained completely drug-free Dr Oli Rai is trying the same ultrasound therapy on 45 more addiction patients and is already thinking about expanding the use of ultrasound to help people with other brain disorders I want to get a bass here including post-traumatic stress disorder and obesity let do it again this is serious business research never been done before we have to learn more we have to replicate our findings is there any risk at running towards something quickly there's always risk but you cannot advance and make discoveries without risk but we need to push forward and take the risk because people with addiction and Alzheimer's not going away it's here so why wait 10 20 years do it now",
    "commentLink": "https://news.ycombinator.com/item?id=39551457",
    "commentBody": "Neurosurgeon pioneers Alzheimer's, addiction treatments using ultrasound [video] (youtube.com)202 points by gardenfelder 17 hours agohidepastfavorite81 comments jerrysievert 13 hours agomy father had a similar focused ultrasound treatment for tremors. he went from spilling everything that he tried to carry to rock solid with his right hand. it was a life-changing event for him, and now he is impatient for the treatment for his left hand. he was even able to go back to his woodworking hobby, that he thought he'd never be able to do again. they did multiple MRI's to make sure they could hit the exact location, and they appear to have done so. reply stubish 7 hours agoparentWe had somehow missed this treatment for essential tremor, and thanks to this video expect a family member will be visiting a neurologist or surgeon soon to discuss, for the same reasons you describe. Now available in Australia apparently. reply glenngillen 4 hours agorootparentA friend’s mother had this treatment last year (in either Hobart or Melbourne, unsure) and the difference is remarkable. I’ll find out the details. Email me (me @ myHNusername dot com) and I’ll pass them on. reply baranul 5 hours agoparentprevGreat that it worked out for him and your family. These kinds of technological advancements gives hope to many people. reply dghughes 17 hours agoprevI saw this on 60 Minutes I'm amazed this isn't worldwide front page news. Especially just for treating addiction alone. Basically sit in a machine get your head buzzed by ultrasound. All better. reply astockwell 14 hours agoparentCareful, that was literally the exact same selling point of Electroconvulsive therapy 80 years ago. reply caycep 14 hours agorootparentSo...there's a distinction: Electroshock therapy for behavioral modification in schools: bad Modern Electroconvulsive therapy under anesthesia: actually effective and arguably lifesaving for severe clinical depression. reply dartos 9 hours agorootparentLack of human experiments and anesthesia reply ClumsyPilot 8 hours agorootparentprev> Electroshock therapy for behavioral modification in schools: bad Is that just violent punishment / torture by another name? reply cyberax 7 hours agorootparentprevECT therapy is AWESOME. It can treat depression and mood disorders, and it can help with severe epilepsy. It was falsely maligned in the \"One Flew Over the Cuckoo's Nest\", because it can _look_ upsetting. However, even when it was done without anesthesia, it caused amnesia so patients didn't remember the procedure itself. reply smolder 4 hours agorootparentI have someone in my family history that underwent electroshock therapy, and by all accounts was only traumatized and destabilized further by the ordeal. YMMV, I guess. reply cyberax 2 hours agorootparentI remember reading the comparison, that all modern psychiatric treatments can be thought of as banging a misbehaving car engine with a hammer. Sometimes it helps, if the hammer is hitting just the right place. Often it does nothing, letting the underlying disease to progress. And in distressingly many cases, it can harm patients. This absolutely applies to the ECT. reply DANmode 4 hours agorootparentprevSo because the patient cannot recall the traumatic events, they're not thought to be experiencing that trauma, or taking lasting effect from it? reply cyberax 2 hours agorootparentYep. That's also how dissociative sedation works. You don't remember being sedated, and you are not (psychologically) traumatized by medical procedures happening during it. I had it several times for minor oral surgeries, and it's great. And the modern ECT is also done under deep sedation. reply andy_ppp 12 hours agorootparentprevI'm reminded of this particularly beautiful TED Talk... https://www.ted.com/talks/sherwin_nuland_how_electroshock_th... reply vkou 14 hours agorootparentprevIf you've got a better treatment for a case of severe depression that's resisted all other attempts at therapy and medication, we're all ears. If not, I suggest we leave it up to the patients and their doctors to determine whether or not ECT is improving their lives, or not. reply pedalpete 11 hours agorootparentSAINT protocol tCMS from Magnus Medical https://www.magnusmed.com/ Not available to everyone yet. reply SimbaOnSteroids 14 hours agoparentprevLow citation on the original paper, none of the paper that cite Rezai's paper have citations. reply caycep 14 hours agorootparentAlso I think the big originators of FUS thalamotomy/pallidotomy were Bob Gross from Emory and Jeff something something at UVA reply caycep 14 hours agoparentprevit happens to diplomats in Cuba all the time! reply qiine 14 hours agorootparentok you made me laugh reply amelius 13 hours agoparentprevI didn't watch the video, but I assume it is not just ultrasound, but a very focused beam / beams of ultrasound in combination with MRI to hit just the right spot. reply grouchomarx 13 hours agorootparentultrasound is used to temporarily open the blood brain barrier so that drugs can target plaque buildups reply amelius 13 hours agorootparentThat's quite a different mechanism. reply roody15 16 hours agoparentprevMore of a dystopian nightmare. Rather than seeing an individual having agency to make decisions and face consequences … we may be moving to a model of a human as nothing more than a fancy automaton. Ohh so Mr Jim is showing addictive traits and not performing task X well. No problem let’s sap his brain and get him fixed up. Super excited for any breakthroughs with Dementia … but fixing addiction, obesity… or even paranoid wrong think with ultrasounds is scary and perhaps harks back to the era of lobotomies. reply numinoid 16 hours agorootparentIt seems you could do this with any technological/medical advancement - how is this any different from semaglutide for obesity or wellbutrin for addiction? It's just a different lever to pull. Harkening back to lobotomies is a false dichotomy, the environment in which research is done today wouldn't even allow for an outcome like that. reply BizarroLand 16 hours agorootparentI disagree. It's a fair concern. They are literally shaking small parts of the brain until they act differently. It's not as extreme as a lobotomy but its lobotomy adjacent and a little scary. Sure, unlike a lobotomy it probably just jostling areas of the brain and not damaging entire sections with an icepick, but it's still fair to accept that some people (like myself) feel automatic body horror and fear at the idea of parts of my brain being damaged. reply basisword 15 hours agorootparentIf my life were ruined and I was dying from alcoholism or drug addiction I get the feeling I probably wouldn't be thinking about this in a philosophical way. Societal impacts or potential abuses would be the least of my concerns. reply mateo1 10 hours agorootparentHow about this scenario: In 20 years, this treatment is standard, but there's a catch: it's bundled with a wider behavioral modification treatment protocol, whereby they first reduce the cravings for drugs and alcohol, then they start showing you anti-establishment imagery, and zap the parts of your brain that respond to it. In fact the second treatment is also standard issue for all those who are diagnosed with oppositionism, a growing mental health disorder characterized by symptoms such as distrusting government authorities and forbes 500 companies. Treatments show a 90% reduction of symptoms and improving quality of life, for example they no longer clash with authorities or make hate statements as defined by the patriot act II of 2028. Obviously an exaggeration, but there is a real concern. The line is blurry and will be crossed if we let it happen. Taking intravenous drugs for 20 years does some serious damage. It's nice if we develop a treatment for it, but it also shifts the focus away from prevention. People shouldn't be reaching that point, and wouldn't if we were acting on it. reply stonogo 9 hours agorootparentIt's not really a blurry line. \"They\" could require you to take antipsychotic pills when you renew your driver license. \"They\" could chemically castrate you when you register to vote and select the wrong party affiliation. But they can't, because in the US patients can refuse treatment. Panicking about new treatments because \"they\" might someday bundle them with other treatments isn't particularly effective, because you can just decline the treatments you don't want. The days of no-oversight asshole doctors drilling holes in people for being weird are conscripted to the past. If patient rights to refuse treatment are destroyed, then sure, freak out all you want. Meanwhile, research is not a zero-sum game. Treatments and prevention can be, and are, worked on in parallel, often by people with wildly different research backgrounds. Specifically, the resources and personnel involved are not fungible. Discouraging field A because you'd rather have someone work on field B doesn't necessarily mean anyone will work on field B, it just guarantees you don't make progress in field A. reply touisteur 15 hours agorootparentprevEspecially if the alternative is cold turkey, or a good old DT... All my heartfelt deep thanks to everyone who keeps digging for solutions, workarounds... anything that might help there. reply BizarroLand 15 hours agorootparentprevNo, that's fair, my stance very much was from the viewpoint of a generically healthy mind and not from the viewpoint of a damaged mind. If I were in that position I might seriously consider the value of having 99% of me make it to the other side of this misery in exchange for the 1% of my brain that is ruining my life. reply tasty_freeze 12 hours agorootparentprev> They are literally shaking small parts of the brain until they act differently. No, from what other comments have said, they are using ultrasound to open the blood/brain barrier so that drugs can enter the affected area. They aren't indiscriminately shaking the patient's brain and hoping for the best. reply numinoid 14 hours agorootparentprevMy issue is more with framing it as a lobotomy analogue when in reality the similarity ends at them both being brain procedures. My read is that this is more akin to something like rehabilitation in that previously damaged tissue is being worked on to improve function. With regards to the body horror thing, that is legitimate and there is the possibility that we look back on this in 20 years like we do with lobotomy. I personally think it's unlikely considering the differences in how research is regulated compared to the past. In general I think we need to reframe how we look at medical treatments. Changing the brain is literally the point - it's dysfunctional. Whether that be through physical manipulation like this or via pharmacology, something HAS to change functionally or there will be no difference. Until the point that we have nano robots carrying out bodily processes for us it's on our brain and body to adapt to whatever environmental stress it's exposed to, for better (exercising improving health for example) and for worse(trauma causing increased likelihood of addiction etc). This treatment is no different from anything else, all that matters is the positive or negative reaction. reply MisterBastahrd 14 hours agorootparentprevHave you ever considered that nothing is wrong with you because nothing is wrong with you, and not that nothing is wrong with you because you are some stoic or heroic figure who managed to simply resist temptation? I've been drunk plenty of times. I've also not consumed enough alcohol to be in a drunken state in almost 8 years despite having a collection of bourbon worth thousands. A good friend of mine, who drank no more than I did for a long while, could not stop. He kept going until he wrecked his liver and died from organ failure. I didn't have the strength to overcome alcoholism. I was simply never an alcoholic. He was. reply ClumsyPilot 8 hours agorootparentIndeed, I am not better than chain smokers, I just happen to be wired differently that it does not have a grip on me. It’s not like something I deserve. Chocolate on the other hand… reply confoundcofound 15 hours agorootparentprevSomething tells me you are wholly unaware of the damage that addiction and obesity can inflict not just on the person afflicted, but on their family, community, and society as a whole. The benefits/costs equation is so massively lopsided here that you'd be cruel to advocate that people endure years of avoidable torment to satisfy your faulty notion of free will / agency. I can't think of anything more agency-promoting than ridding someone of their addictions. reply BizarroLand 14 hours agorootparentWhat line would you draw between the technology existing and being useful to society and people's fundamental and inalienable right to refuse ever having it used on them? Following their original thread, how would you feel if the government decided that this could be used for criminal correction, or if a company made going through a quick brain cleanse a part of the hiring process, or a college part of it's onboarding, or the military a part of boot camp? Do we clean every spot to flawless similarity or just clean the bad thoughts? If the latter, who gets to decide what the bad thoughts are? reply MisterBastahrd 13 hours agorootparentYour argument is akin to being against needles because the government might use them to perform lethal injections. I don't know if you've ever known any addicts, but they aren't exactly happy about being addicted. Give them a treatment that works and is affordable and most of them will be just happy to get some targeted head buzzing to treat their symptoms. reply BizarroLand 9 hours agorootparentThere is a similarity but I don't think it's quite 1:1. Both of your reference points are an order of magnitude weaker or stronger than mine. As I've said in other responses, my viewpoint is coming from the viewpoint of a generally healthy mind and not wanting that to be taken from me. If I become someone with an unhealthy mind my viewpoint could change. reply MisterBastahrd 9 hours agorootparentHysterical scaremongering which masquerades as lucid, rational thought doesn't go very far here. reply BizarroLand 9 hours agorootparentOnce again, an order of magnitude more than the reality. My response was not by any but the thinnest stretched imagination \"hysterical scaremongering\" reply confoundcofound 5 hours agorootparentYour real problem is with the state, not with technology. reply CommanderData 15 hours agoprevThere needs to be a better way at delivering drugs to the brain. The BBB is essential for the brain from cytokines. An imbalance of immune system function inside the brain can cause havoc long term. I read regarding the BLB (barrier for the Cochlear). A compromised BLB is thought to be the cause for many hearing ailments such as Tinnitus and hearing loss. An experiment was done where blood was injected directly into the cochlear fluids and this resulted in extreme hearing loss in mice. A drug delivery method in a controlled fashion is what has been needed yesterday. reply alexpotato 16 hours agoprevSo the following things are basically happening or exist right now: - we can map the brain structure using fMRI - we can change the structure (or parts of it) using ultrasound - we can (or soon will be) able to read thought patterns via LLMs analyzing brain waves To me, that sounds like we are not far off from being able to take an iterative approach to live brain modification. Both scary and fascinating at the same time. reply lawlessone 16 hours agoparent- we can use magnetic pulses to temporarily alter the conciousness of part of the brain, create the feeling of a \"presence of God\" etc. Note: I am not say there is or isn't a god(s). Just that this is how some people interpret the experience. The LLM thing sounds a bit off though. I think whatever article you read on this may have misinterpreted it. reply Sponge5 1 hour agorootparentThat's called transcranial therapy and what I've heard is that it usually just improves your mood temporarily, which is supposed to help you deal with depression and/or addiction. reply codedokode 15 hours agoparentprev> - we can map the brain structure using fMRI fMRI has poor spatial (about 1mm) and temporal (about 1s) resolution. reply mianos 8 hours agorootparent1mm is so massive, and 1 second is a massive timeframe, in relation to the mechanisms at play. There are so many papers (and retractions at massive scale) around attribution to fMRIs. Maybe it is because it would be a dream to see into the brain that the mainstream media keeps bringing this stuff up? reply keenmaster 6 hours agorootparentprevBetter magnets can improve MRI resolution eventually reply omgwtfbyobbq 6 hours agoparentprevThere have been PD trials with aDBS for the past 5-10 years iirc. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10357172/ reply 4gotunameagain 16 hours agoparentprev> we can (or soon will be) able to read thought patterns via LLMs analyzing brain waves source ? who says that the brain waves we can measure hold information like that in a useful level of granularity ? you can have something that distinguishes \"angry\" from \"happy\" etc, sure, but thought patterns ? reply chriskanan 16 hours agorootparentHere are a couple papers related to this topic, but the primary focus has been on showing that LLM embeddings correlate with neural activity better than alternatives. But given one has neural recordings, one could train a model to take them as input and output what was heard. How well that would work, I think, is an open question. https://www.biorxiv.org/content/10.1101/2023.06.27.546708v1 https://www.biorxiv.org/content/10.1101/2022.07.11.499562v3 reply nickphx 15 hours agorootparentWouldn't the model need to be trained on some valid data before being used to guess/approximate output from input? reply bgnn 12 hours agorootparentyeah. yet people believe AI can do new things without training. reply throwawaymaths 15 hours agoprevcould look great in the short run but go very, very sideways (ultrasound could fragments plaques, possibly below size detection limit, but then nucleate the formation of even more plaques if your'e not fixing the underlying problem): https://www.sciencedirect.com/science/article/abs/pii/S00222... reply seehafer 16 hours agoprevGreat to see Dr. Rezai getting his due. One of the pioneers of neuromodulation; he's been involved in deep brain stimulation since the early days. reply nickpsecurity 16 hours agoparentI’ve seen two papers on stimulation and entrainment of brain waves. Is there any place that has a huge pile of resources on that? Everything from introductory to cutting-edge papers? reply caycep 14 hours agorootparent\"neuromodulation\" mostly means deep brain stimulation since it's by far the most effective and clinically - if you pubmed stuff, there's reams of reams of papers (probably written by someone else than Ali Rezai...). That being said, anything re above neurophysiology is mostly still handwaving, bc honestly, we still don't know exactly how it works. reply nabla9 17 hours agoprev\"Brain washing\" with focused ultrasound is a thing. UEF’s new JPND projects explore focused ultrasound and multimodal interventions to fight dementia https://www.uef.fi/en/article/uefs-new-jpnd-projects-explore... >Focused ultrasound (FUS) is a non-invasive, ground-breaking brain stimulation technique suggested for the treatment of Alzheimer’s disease. FUS promotes transient opening of the blood-brain barrier and enhances the microglial-mediated clearance of beta-amyloid typically accumulating in the brain in Alzheimer’s disease. However, the molecular mechanisms underlying these beneficial effects are poorly known, which limits the full translation of FUS into the therapeutic arena. >The REBALANCE project coordinated by Professor Tarja Malm at the A. I. Virtanen Institute for Molecular Sciences aims to discover the key cellular targets and molecular mechanisms underlying FUS-induced brain cleaning and therapeutic efficacy in Alzheimer’s disease. The project partners have previously demonstrated therapeutic efficacy for FUS stimulation alone and combined with microbubbles. >We hypothesise that FUS, through the activation of mechanosensitive Piezo1 channels, enhances beta-amyloid clearance by microglial cells and facilitates the removal of waste from the brain into the blood stream. This occurs through the improvement of glymphatic flow together with transient increase in the blood-brain barrier permeability. Moreover, increased blood-brain barrier permeability enables drug delivery into the brain, further boosting beta-amyloid clearance,” reply vasco 17 hours agoparentI was waiting for the part where they'd brainwash you using this technique and realized it's wash in the more literal sense of getting rid of accumulated bad stuff rather than changing your thoughts or some sort of amnesia inducing procedure. reply psunavy03 17 hours agorootparent\"We're brainwashing you.\" \"WHAT??\" \"No, I mean we're literally washing your brain.\" reply caycep 14 hours agoparentprev....in short, it's a lesion. reply mike_ivanov 14 hours agoprevI suspect unreported (and drastic) side effects in some cases, particularly when the layout of the patient's brain doesn't perfectly align with the operator's expectations. reply stubish 7 hours agoparentThis will depend on how accurate the machine is. I assume that not only are there restraints to keep the head in the right place, but that the machine targets the beams based on where the head actually is rather than where it expects it to be. I also imagine that ultrasound is also being used to precisely locate the brain or even the targeted section. For the Alzheimers treatments, it seems the damage (if it can be called that?) is subtle enough that it only lasts 24-48 hours, and shouldn't have side effects if that is true (the drugs themselves will of course have side effects). For other treatments, it is essentially pinpoint surgery, so has all the same risks, but less so because pinpoint. I can certainly imagine that destroying a few neurons to treat essential tremor could have drastic side effects. reply serguzest 14 hours agoprevCorrect me if I am wrong. there are already treatments discovered for removing plaques, but it turned out that plaques aren't the real cause of cognitive decline. That's what I read before. reply pedalpete 11 hours agoparentThat's not exactly correct. Amyloid plaque is a hallmark of Alzheimer's and is still believed to be the primary cause of cognitive decline. However, that doesn't mean removing the plaques will replace cognitive abilities. If you think of neurons like plumbing, the plaques clog up the tubes, and the neural connections are no longer firing. Removing the plaques doesn't mean a whole doesn't exist in the tube, it just means you've unclogged it, meaning the contents can now spill out. This isn't to say that amyloid plaques are the only marker in age related cognitive decline. My suspicion (and I do some work in the field) is that many different diseases are being lumped into a single thing that we don't completely understand. There is a lot more which will be learned in the next decade. reply WhitneyLand 4 hours agorootparentMy understanding is it’s still the primary hypothesis as the cause, but this hypothesis is not conclusive and is still being actively debated for multiple reasons. There was a research scandal a while back that called into question some amount of data. And there’s the fact that many attempts at attacking the plaques have not been able to show any significant improvement to patients. reply caycep 14 hours agoparentprevYes, but this is a surgeon. Surgeon like to cut/burn things. This is a new form of lesion therapy, but at its heart, it's...lesion therapy reply seydor 16 hours agoprevit s just amazing to see the amyloid plaque removal reply maherbeg 13 hours agoprevCould this be used for removing calcified plaques or even soft plaques in hearts? reply stubish 7 hours agoparentThat seems a great idea, and could even be perfected with animal testing before getting near a human. I have no idea if the engineering is good enough yet to precisely hit a rapidly moving target. reply bgnn 12 hours agoparentprevUltrasound is routinely used to break kidney stones. With more precision like this, why not? I don't know if there would be risks though. reply novok 15 hours agoprevI wonder if deep loud bass music in clubs leverage something similar inadvertently. reply throwawaymaths 15 hours agoparentthat's infrasound, not ultrasound reply babypuncher 11 hours agorootparentit's also not the sound that is doing the healing here. the sound waves are used to create an opening in the blood-brain barrier so that the drugs which do the actual healing can get where they need to go. i suspect a nightclub that plays music which opens the blood-brain barrier would lead to some peculiar outcomes reply m3kw9 17 hours agoprevMaybe can be used on healthy subjects to prevent Alz reply stubish 6 hours agoparentNo, this removes plaques, and having plaques means you already have Alzheimers. This just slows or stops progression. Thankfully we seem to be developing ways to test for Alzheimers, so we should be able to catch it before any symptoms, and treatments to hold it in stasis. Reversing the damage is a whole different problem. reply mateo1 16 hours agoparentprevI'm pretty sure the BBB is there for a reason, unavoidably there will be some side effects. reply naasking 4 hours agoprevVideo not available in my region. I'm in Canada, come on. reply bookofjoe 15 hours agoprev [–] Havana syndrome? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Dr. Ali Rai, a leading neuroscientist, created an experimental ultrasound treatment to combat beta-amyloid protein in the brain, potentially slowing Alzheimer's disease progression.",
      "The innovative approach demonstrated success in decreasing plaques, enhancing brain function, and treating Parkinson's, essential tremors, and addiction.",
      "Dr. Rai's research signifies a significant advancement in developing enhanced therapies for neurological disorders and addiction, fostering hope for improved treatment outcomes."
    ],
    "commentSummary": [
      "A neurosurgeon is leading innovative treatments for Alzheimer's and addiction using ultrasound technology, showing positive outcomes in tremor patients.",
      "Debates surround the use of focused ultrasound and electroconvulsive therapy for mood disorders, focusing on ethics, patient autonomy, and the advantages of brain-altering tech.",
      "Ongoing research evaluates the effectiveness of deep brain stimulation and ultrasound therapy for Alzheimer's, highlighting promising results in amyloid plaque reduction and cognitive improvement."
    ],
    "points": 202,
    "commentCount": 81,
    "retryCount": 0,
    "time": 1709223892
  },
  {
    "id": 39554874,
    "title": "Preventing System Overload: Defcon's Graceful Feature Degradation",
    "originLink": "https://www.micahlerner.com/2023/07/23/defcon-preventing-overload-with-graceful-feature-degradation.html",
    "originBody": "micahlerner.com Defcon: Preventing Overload with Graceful Feature Degradation Published July 23, 2023 Found something wrong? Submit a pull request! Defcon: Preventing Overload with Graceful Feature Degradation This is one in a series of papers I’m reading from OSDI and Usenix ATC. These paper reviews can be delivered weekly to your inbox, or you can subscribe to the Atom feed. As always, feel free to reach out on Twitter with feedback or suggestions! What is the research? Severe outages can occur due to system overloadDiscussion of managing load from the SRE book here. , impacting users who rely on a product, and potentially damaging underlying hardwareDamage to hardware can show up as fail-slow situations, where performance degrades overtime. This is also discussed in a previous paper review on Perseus: A Fail-Slow Detection Framework for Cloud Storage Systems . It can also be difficult to recover from outages involving overloaded system due to additional problems this type of outages cause - in particular, cascading failures. There are many potential root-causes to a system entering an overloaded state, including seasonal traffic spikes, performance regressions consuming excess capacityThis situation can lead to metastable failures, as discussed in a previous paper review. , or subtle software bugs. As such, limiting the damage caused by overload conditions is a complicated problem. To prevent overload from impacting its products, Meta developed a system called Defcon. Defcon provides a set of abstractions that allows incident responders to increase available capacity by turning off features, an idea called graceful feature degradation. By dividing product features into different levels of business criticality, Defcon also allows oncallers to take a variety actions depending on the severity of an ongoing incident. The Defcon paper describes Meta’s design, implementation, and experience deploying this system at scale across many products (including Facebook, Messenger, Instagram, and Whatsapp) along with lessons from usage during production incidents. Background and Motivation The authors of Defcon describe several alternatives they considered when deciding how to mitigate the risk of system overload. Each of the options is evaluated on the amount of additional resources that the approach would consume during an incident, the amount of engineering effort required to implement, and the potential impact to users. Given that serious overload events happen on a recurring basis (at least once a year), the authors decided to invest engineering resources in an engineering-intensive effort capable of limiting user impact. How does the system work? The core abstraction in Defcon is the knob, which represents for each feature: a unique name, whether a feature is turned on or not, the oncall rotation responsible, and a “level” corresponding to business-criticality. After a feature is defined using this configuration, servers or applications (for example, in Web or iOS devices) import the knob into code and implement code paths that handle cases when the knob is turned off - for example, short-circuiting expensive logic. During testing and incident response, operators change a knob’s state via a command-line or user interface, and Defcon handles replicating this state to impacted consumers (like servers and mobile applications). Knob state is also stored in a database. Defcon’s Knob Actuator Service propagates state changes for two types of knobs: server-side knobs and client-side knobs: Server-side knobs are implemented in binaries running on the servers in data centers. The advantage of server-side knobs is that we can adjust the knobs’ state in seconds without any propagation delays. Client-side knobs are implemented in client code running on phones, tablets, wearables, and so on. The advantage of client-side knobs is that they have the capability to reduce network load by stopping requests sent to the server along side reducing server load due to the request. Client-side knobs (like those in an iOS application) are slightly more complex to update. Under normal conditions, they change via a push (called Silent Push Notification (SPN)) or routine pull (Mobile Configuration Pull) mechanism. To handle extenuating circumstances (like lower latency response to severe outages), Defcon can also instruct clients to pull a broader set of configuration stored in a specific server-location using a process called Emergency Mobile ConfigurationUnder normal operating conditions, a full reset isn’t used because it has the tradeoff of using more resources (in particular networking), which is unfriendly to user mobile plans and device batteries. . Knobs are, “grouped into three categories: (1) By service name, (2) by product name, and (3) by feature name (such as “search,” “video,” “feed,” and so on)” to simplify testing during development and post-release. Testing occurs through small scale A/B tests (where one “experiment arm” of users experience feature degradation, and the “control” arm does not) and during larger exercises that ensure the Defcon system is working (described later in the paper). These tests also have the side effect of generating data on what capacity a feature or product is using, which serves as an input to capacity planning. During incidents, oncallers can also use the output of these tests to understand what the potential implications are of turning off different knobs. The How is the research evaluated? The paper uses three main types of datasets to quantify Defcon’s changes: Real-time Monitoring System (RMS) and Resource Utilization Metric (RUM), which aim to measure utilization of Meta infrastructure. The specifics of which one to use depends on the experiment, as discussed below. Transitive Resource Utilization (TRU), which aims to measure the downstream utilization that a service has of shared Meta systems (like its graph infrastructure described in my previous paper review on TAO: Facebook’s Distributed Data Store for the Social Graph). User Behavior Measurement (UBM), which tracks how changing a knob’s state impacts business metrics like “Video Watch Time”. The first evaluation of Defcon’s impact is at the Product-level. By turning off progressively more business-critical functionality, the system makes greater impact on Meta’s resource usageRepresented with mega-instructions per second (MIPS), a normalized resource representation corresponding to compute. . Entirely turning off critical features (aka “Defcon Level 1”), saves a large amount of capacity, but also significantly impacts critical business metrics. Defcon is next evaluated for its ability to temporarily decrease capacity required of shared infrastructure. As discussed in a previous paper review of Scaling Memcache at Facebook, Meta uses Memcache extensively. By turning off optional features, oncallers are able to decrease load on this type of core system. Next, the research describes how Meta can decrease capacity requirements by turning off knobs in upstream systems with dependencies on other Meta products. For example, turning off Instagram-level knobs decreases load on Facebook, which ultimately depends on TAO, Meta’s graph service. Testing knobs outside of incident response surfaces resource requirements from these interdependencies. The Defcon paper describes a protocol for forcing Meta systems into overload conditions, and testing the impact of turning progressively more business-critical features off. By ramping user traffic to a datacenter, these experiments place increasing load on infrastructure - turning knobs off then alleviates load. Conclusion The Defcon paper describes a framework deployed at scale in Meta for disabling features in order to mitigate overload conditions. To reach this state, the authors needed to solve technical challenges of building the system and to collaborate with product teams to define feature criticality - in some ways, the latter seems even more difficult. The paper also mentions issues with maintainability of knobs. On this front, it seems like future work could automate the process of ensuring that knobs cover features inside of deployed code. Lastly, I’m looking forward to learning more about Defon’s integration with other recently published Meta research, like the company’s capacity management system. Follow me on Twitter or subscribe below to get future paper reviews. Published weekly. Found something wrong? Submit a pull request!",
    "commentLink": "https://news.ycombinator.com/item?id=39554874",
    "commentBody": "Defcon: Preventing overload with graceful feature degradation (2023) (micahlerner.com)201 points by mlerner 13 hours agohidepastfavorite79 comments winrid 2 hours agoOne of the most satisfying feature degradation steps I did with FastComments was making it so that if the DB went offline completely, the app would still function: 1. It auto restarts all workers in the cluster in \"maintenance mode\". 2. A \"maintenance mode\" message shows on the homepage. 3. The top 100 pages by comment volume will still render their comment threads, as a job on each edge node recalculates and stores this on disk periodically. 4. Logging in is disabled. 5. All db calls to the driver are stubbed out with mocks to prevent crashes. 6. Comments can still be posted and are added into an on-disk queue on each edge node. 7. When the system is back online the queue is processed (and stuff checked for spam etc like normal). It's not perfect but it means in a lot of cases I can completely turn off the DB for a few minutes without panic. I haven't had to use it in over a year, though, and the DB doesn't really go down. But useful for upgrades. built it on my couch during a Jurassic park marathon :P reply danpalmer 11 hours agoprevJoining Google a few years ago, one thing I was impressed with is the amount of effort that goes into graceful degradation. For user facing services it gets quite granular, and is deeply integrated into the stack – from application layer to networking. Previously I worked on a big web app at a growing startup, and it's probably the sort of thing I'd start adding in small ways from the early days. Being able to turn off unnecessary writes, turn down the rate of more expensive computation, turn down rates of traffic amplification, these would all have been useful levers in some of our outages. reply tuyguntn 11 hours agoparentit's really great to have such capabilities, but adding them has a cost where only few can afford. Cost in terms of investing in building those, which impacts your feature build velocity and the maintenance reply zerkten 10 hours agorootparentCan you be specific about the cost of building these? I've run into many situations where something was deemed costly, is found out later, and the team ultimately has implement it all while hoping no one groks that is was predicted. \"Nobody ever gets credit for fixing problems that never happened\" (https://news.ycombinator.com/item?id=39472693) is related. reply nostrademons 9 hours agorootparentWhen I was in Search 15 or so years ago, there was actually a very direct cost: revenue. The AdMixer was an \"optional\" response for the search page. If the ads didn't return before the search results did, the search would just not show ads, and Google wouldn't get any revenue for it. Showed the premium that Google of the day put on latency and user experience. I think we lost a few million per year to timeouts, but it was worth it for generating user loyalty, and it put a very big incentive on the ads team to keep the serving stack fast. No idea if it's still architected like that, I kinda doubt it given recent search experiences, but I thought it was brilliant just for the sake of aligning incentives between different parts of the organization. reply spacebanana7 10 hours agorootparentprevThe developer, tester and devops time required to properly implement graceful degradation could easily accumulate to hundreds of hours. Those hours are directly expensive when your developers cost hundreds of dollars a day; and have a material opportunity cost in that their commitment to one particular project delays the delivery of other features. Moreover, any new features would have to be made compatible with the graceful degradation pattern, creating an ongoing cost. reply IggleSniggle 7 hours agorootparentWhen you hire an engineer to build a dam, you expect them to consider piping and subsurface flows such that the foundation isn't swept out in a decade. No matter of the engineer was already paid, retired, etc. My point isn't that we all need to make dams that can hold up for a century. The point is that you hire an engineer because you want someone with the judgement and expertise to apply the correct amount of engineering to any given solution. Over-engineering is on the pathway to correct-sized engineering. It's the experience, discovery, and exploration required to arrive at choosing what things actually do not need to be done. When your manager asks you, \"do we really need to do that?\" It's the expert that can explain why it really is necessary, and the professional who accepts \"we're not going to do that\" as an answer. And if they still feel it would be harmful not to do it, then that's where professional duty kicks in. reply mlyle 6 hours agorootparentprevThere's a lot of levels to the approach. Just spending a few moments to consider whether queues should grow, block, or spill when adding them makes a big difference, along with choices in error handling. You can get a lot of things to gracefully degrade for free if that's a part of your decision-making process. reply jacobcoro 5 hours agorootparentprevCould be as simple as just some feature flags with environment variables reply groestl 2 hours agorootparentI also found that when building a feature iteratively, with feature flags for rollout, a simple feature degradation path often appears natively. reply kortilla 9 hours agorootparentprevEffectively every piece of software written for at most a few thousand people to use concurrently (i.e. 99.99% of software). Consumer apps that scale to hundreds of thousands of users with five 9s+ uptime requirements are very rare. reply rkagerer 9 hours agorootparentprevFor one, it potentially multiplies the testing and regression testing requirements to hit all those additional configurations. reply Banditoz 12 hours agoprevAm I reading the second figure right? Facebook can do 130*10^6 queries/second == ‭130,000,000‬ queries/second?! reply ndriscoll 12 hours agoparentFacebook makes over 300 requests for me just loading the main logged in page while showing me exactly 1 timeline item. Hovering my mouse over that item makes another 100 requests or so. Scrolling down loads another item at the cost of over 100 requests again. It's impressive in a perverse way just how inefficient they can be while managing to make it still work, and somewhat disturbing that their ads bring in enough money to make them extremely profitable despite it. reply akira2501 10 hours agorootparentThis is the company that instead of ditching PHP created a full on PHP to C++ transpiler and then deployed their while site on that for a few years. reply NooneAtAll3 5 hours agorootparent> deployed their while site ?? reply kqr 4 hours agorootparentObviously \"whole\". reply callalex 7 hours agorootparentprevDo you have an ad blocker stopping the requests and causing retries? reply ndriscoll 7 hours agorootparentI do have ublock origin on everything of mine, so conceivably it's reacting to that somehow. I'm no longer at my computer to be able to look more closely at what it's doing. reply yodsanklai 10 hours agorootparentprevCould someone tell me what these hundreds of requests could do? reply ndriscoll 8 hours agorootparentA lot of them appear to be that they've split their javascript into a gazillion files for whatever reason (I suppose because they have several MB of it). But someone or lots of people there did seem to get addicted to dynamic loading. Like I've got 100 or so friends, but my friends page loads them 8-16 at a time as I scroll. Just send all 100 and set the profile pictures to deferred fetch. It'd probably be smaller than the js they have to make it do \"infinite\" scroll. Similarly, after getting to the bottom of their \"infinite\" scroll, my friend feed (which is annoyingly hidden away) gives me... 15 items. Just send me all 15. It's like 1-2 kB worth of data. If you're going to end the scroll after a dozen items, why is it using infinite scroll? reply sd9 12 minutes agorootparentCould be so they can track what you're looking at on the back end reply Solvency 9 hours agorootparentprevTrack you, probably with a thousand layers of redundancy, tech bloat, and decades of mold. reply golergka 11 hours agorootparentprevWasn't the whole point of GraphQL in mitigating this? reply serial_dev 9 hours agorootparentYeah, that's why you have only 100 requests* when you hover over an item instead of 800. (* allegedly, didn't verify it myself) reply zer00eyz 8 hours agorootparentprevNo. Here is the thing, hypermedia is cacheable. React/Graphql not so much. Facebook is now just an application that runs in the browser. As a poor, small developer who doesn't want to hemorrhage money, I tend to want things to be more hypermedia and less app. It saves on complexity and bandwidth and costs. reply bagels 12 hours agoparentprevI can't comment on the numbers, but think of how many engineers work there and how many users Facebook, Whatsapp, Instagram have. Each engineer is adding new features and queries every day. You're going to get a lot of queries. reply bee_rider 11 hours agorootparentWe’ve really wasted an incredible amount of talent-hours over last couple decades. Imagine if we’d worked on, like, climate change or something instead of ad platforms. reply esafak 11 hours agorootparent\"The best minds of my generation are thinking about how to make people click ads. That sucks.\" - Jeff Hammerbacher (2011); early Facebook employee, and Cloudera cofounder. https://www.theatlantic.com/technology/archive/2011/04/quote... reply akira2501 10 hours agorootparentprevThe waste of talent hours is directly connected to climate change. The waste of network bandwidth is as well as the waste of compute cycles to run these \"social\" platforms. That all being said, as humans have free will, imagining what we \"could have done\" if we just _forced_ everyone to do something different is flirting with fascism. reply bee_rider 9 hours agorootparentSure, I wouldn’t suggest forcing everyone to work on something else. We all could have been better, and the government could have tried to incentivize more productive work (they already provide incentives on way or another after all). reply spencerflem 9 hours agorootparentprevI think most devs also would rather work on something good too, but those jobs are rare and pay worse. That's the part that needs fixing reply baby 11 hours agorootparentprevYou make it sound like everybody at Meta works in the ads department. reply bee_rider 10 hours agorootparentIt is an ad company, everyone there works on ads or indirectly works on making a platform for ads. The only exception is people who’ve managed to sneak their way into positions where they don’t contribute anything to the company. Those people are doing society a favor by wasting Facebook’s money. reply bagels 9 hours agorootparentprevWell, a lot of them do work directly on ads. Obviously not everybody, but a significant fraction. And most of the rest are just building things (products, features) to be able to show more ads. reply jojobas 11 hours agorootparentprev98% of Meta's revenue is from ads. Meta is an ads department. reply Scubabear68 11 hours agorootparentprevThey all do. reply goatking 9 hours agorootparentprevfound the meta employee reply scottlamb 9 hours agoparentprev> Am I reading the second figure right? Facebook can do 130*10^6 queries/second == ‭130,000,000‬ queries/second?! That sounds totally plausible to me. Also keep in mind they didn't say what system this is. It's often true that 1 request to a frontend system becomes 1 each to 10 different backend services owned by different teams and then 20+ total to some database/storage layer many of them depend on. The qps at the bottom of the stack is in general a lot higher than the qps at the top, though with caching and static file requests and such this isn't a universal truth. reply avery17 7 hours agoparentprevWhats with people lately writing 10^6 instead of 1 million. Its not that big that we need exponents to get involved. reply gaogao 12 hours agoparentprevThose queries are probably mostly memcache hits, though of course with distributed cache invalidation and consistency fun reply ipaddr 11 hours agorootparentIf it doesn't hit the database is it really a query? reply storyinmemo 12 hours agoparentprevI think about 10 years ago when I was working there I checked the trace to load my own homepage. Just one page, just for myself, and there were 100,000 data fetches. reply vaylian 2 hours agorootparentBy \"homepage\" you mean your Facebook profile? reply bdd 9 hours agoparentprevYes. And that was 4 years ago. Must add that figure does NOT include static asset serving path. reply sebzim4500 12 hours agoparentprevSounds plausible. There are probably many queries required to display a page and Facebook has 2 billion daily active users. reply ipaddr 11 hours agorootparentThis is how information slowly changes. The original numbers from facebook needed to be taken with a grain of salt. 2 billion a day raises it more. Facebook claims to have 2 billion accounts but no where near 2 billion unique accounts. I don't know what facebook calls an active user but it use to mean logged in once in the past 30 days. reply reissbaker 11 hours agorootparentNo, the person you're responding to was correct. Facebook has over 2 billion daily active users [1], and DAU refers to unique users who used the product in a day [2]. 1: https://www.statista.com/statistics/346167/facebook-global-d... 2: https://www.innertrends.com/blog/active-users-measuring-busi... reply ipaddr 10 hours agorootparentIt's directly in the link you provided. \"For example, an active user can be measured as a user that has logged back into her account to interact with the product in the last 30 days.\" Even the marketing material is designed to confuse. reply rcxdude 10 hours agorootparentThat's a monthly active user. A daily active user would be someone who logged into an account in the last day. Generally monthly active user count will be higher than daily active users, but for something like Facebook the difference is about 50% (which is what the second article linked is explaining, if you read more than just cherry-picking a line that matches your preconceptions) And yes, that's a claim that if each user is a separate person, >20% of the world's population interacts with Facebook at least minimally each day. You can add your own interpretation about how many of the accounts are bots or otherwise duplicates, but it's a staggering amount either way. reply ipaddr 10 hours agorootparentprevIt should be their account not her account (or his). Who writes this garbage. reply OrsonSmelles 8 hours agorootparentAlternating or stochastically varying pronouns in your examples used to be a common way to make an effort at inclusive writing, usually preferred aesthetically to constructs like `his/her'. (The style before that was basically to use masculine pronouns for hypothetical people in every single case and deny that there was anything to question about that.) I think I agree that the modern semi-standard of using `they' for examples where gender is irrelevant or unknown is strictly better, but it's hard for me to summon a lot of contempt for someone who goes with a different/older habit. reply zilti 1 hour agorootparentprevHeh. Gave me a chuckle, because DAU also means \"Dümmster Anzunehmender User\" in German (dumbest assumed user, in context of creating idiot-proof software, and a wordplay on GAU, which means grösster anzunehmender Unfall, biggest assumed accident, which comes from fission power plants). And that kinda fits for the kind of people that perceivedly are left on the likes of Facebook and X. reply pests 10 hours agorootparentprevDifferent metrics Daily vs Monthly Active User. MAU vs DAU reply IncreasePosts 12 hours agoparentprevI forgot how to count that low. reply reissbaker 12 hours agoparentprevA custom JIT + language + web framework + DB + queues + orchestrator + hardware built to your precise specifications + DCs all over the world go a long way ;) reply Thaxll 8 hours agoparentprevWe're close to 1 million servers, not 12 racks in a DC. reply AlienRobot 12 hours agoparentpreviirc Facebook has 3 billion users, so that sounds plausible. reply sonicanatidae 12 hours agoparentprevYeah, they allocated ALL of the ram to their DB servers. lol reply dang 11 hours agoprevDiscussed (a tiny bit) at the time: Defcon: Preventing Overload with Graceful Feature Degradation - https://news.ycombinator.com/item?id=36923049 - July 2023 (1 comment) reply mrb 11 hours agoprevOff-topic but: I love the font on the website. At first I thought it was the classic Computer Modern font (used in LateX). But nope. Upon inspection of the stylesheet, it's https://edwardtufte.github.io/et-book/ which was a font designed by Dmitry Krasny, Bonnie Scranton, and Edward Tufte. The font was originally designed for his book Beautiful Evidence. But people showed interest in font, see the bulletin board on ET's website: https://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=... Initially he was reluctant to go the trouble of releasing it digitally. But eventually he did make it available on GitHub. reply gillh 8 hours agoprevAnyone interested in load shedding and graceful degradation with request prioritization should check out the Aperture OSS project. https://github.com/fluxninja/aperture reply mikerg87 10 hours agoprevIsn't this referred to as Load Shedding in some circles? If its not, can someone explain how its different? reply scottlamb 9 hours agoparentThey're the same thing or close to it. \"Load shedding\" might be a bit more general. A couple possible nuances: * Perhaps \"graceful feature degradation\" as a choice of words is a way of noting there's immediate user impact (but less than ungracefully running out of capacity). \"Load shedding\" could also mean something less impactful, for example some cron job that updates some internal dashboard skipping a run. * \"feature degradation\" might focus on how this works at the granularity of features, where load shedding might mean something like dropping request hedges / retries, or individual servers saying they're overloaded and the request should go elsewhere. reply kqr 4 hours agoparentprevThis is the other side of the load shedding coin. The situation is that A depends on B, but B is overloaded; if we allow B to do load shedding, we must also write A to gracefully degrade when B is not available. reply jedberg 10 hours agoprevI'm surprised they don't have automated degradation (or at least the article implies that it must be operator initiated). We built a similar tool at Netflix but the degradations could be both manual and automatic. reply packetslave 4 hours agoparentThere's definitely automated degradation at smaller scale (\"if $random_feature's backend times out, don't show it\", etc.). The manual part of Defcon is more \"holy crap, we lost a datacenter and the whole site is melting, turn stuff off to bring the load down ASAP\" reply velcrovan 12 hours agoprevSeems like whenever I log into FB lately it's pretty much always in a state of “graceful feature degradation”. For example, as soon as I log in I see a bell icon in the upper right with a bright red circle containing an exact positive integer number of notifications. It practically screams “click here, you have urgent business”. I can then leave the web page sitting there for any number of minutes, and no matter how long I wait, if I click on that notification icon it will take a good 20 seconds to load the list of new notifications. (This is on gigabit fiber in a major metro area, so not a plumbing issue.) reply guessmyname 11 hours agoparentHave you tried navigating the website using a web proxy (Charles, Burp Suite, or similar tool) to intercept the HTTP request(s) in order to replay them yourself multiple times to see if the latency is consistent? It’d be interesting to discover that the delay is fabricated using the front-end code or if the back-end server is really the problem. I don’t use Facebook but I asked a friend just now and the response time for the notifications panel to appear is between 500ms-2000ms, which is relatively fast for web interactions. reply philippta 12 hours agoparentprevWithout being able to verify, I would assume it’s designed to behave in this way. The longer you wait the more anticipation builds up, the more gratifying it becomes. reply meowface 11 hours agorootparentI think there's no chance they intentionally want users to wait 20 seconds to see their latest notifications. reply Arainach 11 hours agoparentprevThe initial render of Facebook's UI slows dramatically (I suspect but cannot prove intentionally) if you have adblockers/uBlock Origin/etc. reply paganel 10 hours agoparentprev> it will take a good 20 seconds to load the list of new notifications. Same thing here. Thought it was my (relatively much) slower Internet connection, or maybe that I had something \"wrong\" (what exactly that might have been, I don't know). reply iraqmtpizza 10 hours agoparentprevat least once youtube slowed to a crawl until I cleared the cookie reply OtherShrezzing 12 hours agoprev [–] > if (disableCommentsRanking.enabled == False) This could use some light-touch code reviewing reply bmacho 42 minutes agoparentIt looks funny, but I think it's actually good, and arguably the best possible form of it. reply kqr 4 hours agoparentprevBecause the HN crowd likes learning new things: if `enabled` is a nullable boolean in C# (i.e. has type `bool?`) then this check must indeed be written this way, to avoid confusing null with false. reply jpnc 2 hours agorootparentI thought OP meant to imply that the readability could use some tweaking. You have 'disable', 'enabled' and 'False' used in the same expression so it requires some (more) thinking while reading it and trying to decipher what it's trying to do. reply dvhh 9 hours agoparentprev [–] Some could argue it would be for illustration purpose, and not actual production code reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The paper explores Meta's Defcon system, utilizing graceful feature degradation to avert system overload and possible outages by categorizing product features based on business criticality levels.",
      "Incident responders can manage capacity by deactivating features using knobs, controlling feature status on the server-side and client-side, with testing conducted to assess its effects on resource usage and user interactions.",
      "Future endeavors include automating the knob upkeep process, addressing challenges outlined in the paper."
    ],
    "commentSummary": [
      "The article highlights the significance of graceful feature degradation to prevent system overload, particularly during database outages, stressing the benefits of investing in these capabilities for maintaining functionality and user experience.",
      "Discusses the costs of implementing such features, the testing challenges posed by high uptime requirements, and how Facebook manages excessive requests.",
      "Includes debates on tech firms' focus on advertising, the comparison of daily versus monthly active user counts, and the introduction of a font by Dmitry Krasny, Bonnie Scranton, and Edward Tufte, while website users discuss loading delays and speculate on the causes."
    ],
    "points": 201,
    "commentCount": 79,
    "retryCount": 0,
    "time": 1709239850
  },
  {
    "id": 39553967,
    "title": "Unveiling GGUF: Program Compilation, GPU Initialization, and Hardware Details",
    "originLink": "https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/",
    "originBody": "I ccache not found. Consider installing it for faster compilation. I llama.cpp build info: I UNAME_S: Darwin I UNAME_P: arm I UNAME_M: arm64 I CFLAGS: -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL -std=c11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion I CXXFLAGS: -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi I NVCCFLAGS: -O3 I LDFLAGS: -framework Accelerate -framework Foundation -framework Metal -framework MetalKit I CC: Apple clang version 14.0.3 (clang-1403.0.22.14.1) I CXX: Apple clang version 14.0.3 (clang-1403.0.22.14.1) make: Nothing to be done for `default'. Log start main: build = 2125 (c88c74f) main: built with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.4.0 main: seed = 1707673859 llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/vicki/llama.cpp/models/mistral-7b-instruct-v0.2.Q8_0.gguf (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output. llama_model_loader: - kv 0: general.architecture str = llama llama_model_loader: - kv 1: general.name str = mistralai_mistral-7b-instruct-v0.2 llama_model_loader: - kv 2: llama.context_length u32 = 32768 llama_model_loader: - kv 3: llama.embedding_length u32 = 4096 llama_model_loader: - kv 4: llama.block_count u32 = 32 llama_model_loader: - kv 5: llama.feed_forward_length u32 = 14336 llama_model_loader: - kv 6: llama.rope.dimension_count u32 = 128 llama_model_loader: - kv 7: llama.attention.head_count u32 = 32 llama_model_loader: - kv 8: llama.attention.head_count_kv u32 = 8 llama_model_loader: - kv 9: llama.attention.layer_norm_rms_epsilon f32 = 0.000010 llama_model_loader: - kv 10: llama.rope.freq_base f32 = 1000000.000000 llama_model_loader: - kv 11: general.file_type u32 = 7 llama_model_loader: - kv 12: tokenizer.ggml.model str = llama llama_model_loader: - kv 13: tokenizer.ggml.tokens arr[str,32000] = [\"\", \"\", \"\", \"\", \"' llm_load_print_meta: EOS token = 2 '' llm_load_print_meta: UNK token = 0 '' llm_load_print_meta: PAD token = 0 '' llm_load_print_meta: LF token = 13 '' llm_load_tensors: ggml ctx size = 0.22 MiB ggml_backend_metal_buffer_from_ptr: allocated buffer, size = 7205.84 MiB, ( 7205.91 / 49152.00) llm_load_tensors: offloading 32 repeating layers to GPU llm_load_tensors: offloading non-repeating layers to GPU llm_load_tensors: offloaded 33/33 layers to GPU llm_load_tensors: Metal buffer size = 7205.84 MiB llm_load_tensors: CPU buffer size = 132.81 MiB .................................................................................................. llama_new_context_with_model: n_ctx = 512 llama_new_context_with_model: freq_base = 1000000.0 llama_new_context_with_model: freq_scale = 1 ggml_metal_init: allocating ggml_metal_init: found device: Apple M2 Max ggml_metal_init: picking default device: Apple M2 Max ggml_metal_init: default.metallib not found, loading from source ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil ggml_metal_init: loading '/Users/vicki/llama.cpp/ggml-metal.metal' ggml_metal_init: GPU name: Apple M2 Max ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008) ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003) ggml_metal_init: GPU family: MTLGPUFamilyMetal3 (5001) ggml_metal_init: simdgroup reduction support = true ggml_metal_init: simdgroup matrix mul. support = true ggml_metal_init: hasUnifiedMemory = true ggml_metal_init: recommendedMaxWorkingSetSize = 51539.61 MB ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size = 64.00 MiB, ( 7270.59 / 49152.00) llama_kv_cache_init: Metal KV buffer size = 64.00 MiB llama_new_context_with_model: KV self size = 64.00 MiB, K (f16): 32.00 MiB, V (f16): 32.00 MiB llama_new_context_with_model: CPU input buffer size = 9.01 MiB ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size = 0.02 MiB, ( 7270.61 / 49152.00) ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size = 80.31 MiB, ( 7350.91 / 49152.00) llama_new_context_with_model: Metal compute buffer size = 80.30 MiB llama_new_context_with_model: CPU compute buffer size = 8.80 MiB llama_new_context_with_model: graph splits (measure): 3 system_info: n_threads = 8 / 12AVX = 0AVX_VNNI = 0AVX2 = 0AVX512 = 0AVX512_VBMI = 0AVX512_VNNI = 0FMA = 0NEON = 1ARM_FMA = 1F16C = 0FP16_VA = 1WASM_SIMD = 0BLAS = 1SSE3 = 0SSSE3 = 0VSX = 0MATMUL_INT8 = 0sampling: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000 top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampling order: CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0",
    "commentLink": "https://news.ycombinator.com/item?id=39553967",
    "commentBody": "GGUF, the Long Way Around (vickiboykis.com)200 points by Tomte 14 hours agohidepastfavorite24 comments tbalsam 12 hours agoLlama.cpp I think has a ton of clone-and-own boilerplate, presumably from having grown so quickly (I think one of their .cu files is over 10k lines or so, roughly, ATM). While I haven't seen the model storage and distribution format, the rewrite to GGUF for file storage seems to have been a big boon/boost to the project. Thanks Phil! Cool stuff. Also, he's a really nice guy to boot. Please say hi from Fern to him if you ever run into him. I mean it literally, make his life a hellish barrage of nonstop greetings from Fern. reply qrios 8 hours agoparentThank you for the reference to the CUDA file [1]. It's always nice to see how complex data structures are handled in GPUs. Does anyone have any idea what the bit patterns are for (starting at line 1529)? [1] https://github.com/ggerganov/llama.cpp/blob/master/ggml-cuda... reply thrtythreeforty 6 hours agorootparentThose have to do with dequantization. It involves table lookups and some adjusting math. reply liuliu 12 hours agoparentprevI honestly think have a way to just use json (a.k.a. safetensors) / msgpack or some lightweight metadata serializer is a better route than coming up with a new file format. That's also why I just use SQLite to serialize the metadata (and tensor weights, this part is an oversight). reply andy99 12 hours agorootparentGguf is cleaner to read in languages that don't have a json parsing library, and works with memory mapping in C. It's very appealing for minimal inference frameworks vs other options. reply liuliu 11 hours agorootparentsafetensors can mmap too because the tensor data are just offsets and you are free to align to whatever you want. It is hard to keep metadata minimal, and before long, you will start to have many different \"atom\"s and end-up with things that mov supports but mp4 doesn't etc etc. (mov format is generally well-defined and easy-to-parse, but being a binary format, you have to write your parser etc is not a pleasant experience). If you just want minimal dependency, flatbuffers, capnproto, json are all well-supported on many platforms. reply jart 11 hours agorootparentmmap() requires that you map at page aligned intervals which must be congruent with the file offset. You can't just round down because some gpus like metal require that the data pointers themselves be page aligned too. reply liuliu 11 hours agorootparentYeah, safetensors separates metadata and tensor data. The metadata is an offset reference to the tensor data that you are free to define yourselves. In that way, you can create files in safetensors format but the tensor data itself is paged aligned offsets. reply 3abiton 10 hours agorootparentprevBut usually AWQ get recommended for GPU inference over GGUF reply Mathnerd314 9 hours agorootparentBy who? Only comparison I have seen is that it sucks vs. EXL2 https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacp... reply Eisenstein 7 hours agorootparentprevHaven't heard this. Was this a few months ago? A lot happens in this space over that time span. reply fzzzy 9 hours agorootparentprevI think a binary format is obviously the right answer here. reply andy99 11 hours agoprev> GPT-Generated Unified Format GG is Georgi Gerganov reply kristjansson 4 hours agoparentNothing like a good backronym reply SOLAR_FIELDS 8 hours agoprev“no yapping” gave me a bit of a chuckle. Quick way to ask the response to be brief I guess. reply null_point 8 hours agoprevCool. I was just learning about GGUF by creating my own parser for it based on the spec https://github.com/ggerganov/ggml/blob/master/docs/gguf.md (for educational purposes) reply RicoElectrico 12 hours agoprevAs LLMs have quite minor changes between architectures, would it make sense to just embed the model compiled to some sort of simple bytecode right in the GGUF file? Then, only implement specific new operations when researchers come up with a new model that gains enough traction to be of interest. reply liuliu 12 hours agoparentNot really. We've been on that road before. Embedding computation graph into the file makes changes to the computation graph harder (you need to make sure it is backward compatible). This is OK in general (as we have onnx already), but then if you have dynamic shape and the fact that different optimizations we implemented are actually tied to the computation graph, this is simply not optimal. (BTW, this is why PyTorch just embed the code into the pth file, much easier and backward compatible than a static computation graph). reply sroussey 12 hours agoparentprevYeah, but you want to avoid remote code execution: https://www.bleepingcomputer.com/news/security/malicious-ai-... reply RicoElectrico 12 hours agorootparentThe bytecode would not even need to be Turing-complete. Or maybe it could take inspiration from eBPF which gives some guarantees. What you posted is related to the design oversight of Python's pickle format. reply sroussey 12 hours agorootparentI think ONNX does what you say. reply rahimnathwani 12 hours agoparentprevIt seems like a lot of innovation is around training, no? GGML (the library that reads GGUF format) supports these values for the required 'general.architecture': llama mpt gptneox gptj gpt2 bloom falcon rwkv reply cooper_ganglia 12 hours agoprevI’ve been looking for a good resource on GGUF for the past week or so, the timing on this is awesome! Thanks! reply skadamat 13 hours agoprev [–] This is an excellent deep dive! Love the depth here Vicki reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The message discusses compiling a program, sharing build details, and hardware information.",
      "It loads a model with specific key-value pairs and tensors, initializing GPU processing.",
      "Details about the GPU being utilized are also provided in the message."
    ],
    "commentSummary": [
      "The post covers GGUF, a file format for model storage and distribution in CUDA files, emphasizing its advantages over other formats, especially its compatibility with languages without JSON parsing libraries and its suitability for minimal inference frameworks.",
      "It delves into the concept of using safetensors for file serialization and compares GGUF's performance for GPU inference with formats like AWQ, sparking discussions on technical aspects, potential enhancements, and support for diverse architectures during training.",
      "Overall, the post provides insights into GGUF's strengths and potential for optimization in GPU inference scenarios."
    ],
    "points": 200,
    "commentCount": 24,
    "retryCount": 0,
    "time": 1709235404
  },
  {
    "id": 39549838,
    "title": "Rendering Protein Structures at Atomic Level in Cells with Unreal Engine",
    "originLink": "https://www.biorxiv.org/content/10.1101/2023.12.08.570879v1",
    "originBody": "Skip to main content Home Submit FAQ Blog ALERTS / RSS About Channels Search for this keyword Advanced Search New Results Follow this preprint Rendering protein structures inside cells at the atomic level with Unreal Engine View ORCID ProfileMuyuan Chen doi: https://doi.org/10.1101/2023.12.08.570879 00000000 Comments0 TRiP Peer Reviews0 Community Reviews0 Automated Services0 Blog/Media Links0 Videos0 Tweets Abstract Full Text Info/History Metrics Supplementary material Preview PDF Abstract While the recent development of cryogenic electron tomography (CryoET) makes it possible to identify various macromolecules inside cells and determine their structure at near-atomic resolution, it remains challenging to visualize the complex cellular environment at the atomic level. One of the main hurdles in cell visualization is to render the millions of molecules in real time computationally. Here, using a video game engine, we demonstrate the capability of rendering massive biological macromolecules at the atomic level within their native environment. To facilitate the visualization, we also provide tools that help the interactive navigation inside the cells, as well as software that converts protein structures identified using CryoET to a scene that can be explored with the game engine. Download figure Open in new tab Competing Interest Statement The authors have declared no competing interest. Copyright The copyright holder for this preprint is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under a CC-BY 4.0 International license. Back to top PreviousNext Posted December 11, 2023. Download PDF Print/Save Options Download PDFFull Text & In-line FiguresXML More Info Supplementary Material Email Share Rendering protein structures inside cells at the atomic level with Unreal Engine Muyuan Chen bioRxiv 2023.12.08.570879; doi: https://doi.org/10.1101/2023.12.08.570879 Share This Article: Copy Citation Tools Tweet Widget COVID-19 SARS-CoV-2 preprints from medRxiv and bioRxiv Subject Area Scientific Communication and Education Subject Areas All Articles Animal Behavior and Cognition Biochemistry Bioengineering Bioinformatics Biophysics Cancer Biology Cell Biology Clinical Trials* Developmental Biology Ecology Epidemiology* Evolutionary Biology Genetics Genomics Immunology Microbiology Molecular Biology Neuroscience Paleontology Pathology Pharmacology and Toxicology Physiology Plant Biology Scientific Communication and Education Synthetic Biology Systems Biology Zoology * The Clinical Trials and Epidemiology subject categories are now closed to new submissions following the completion of bioRxiv's clinical research pilot project and launch of the dedicated health sciences server medRxiv (submit.medrxiv.org). New papers that report results of Clinical Trials must now be submitted to medRxiv. Most new Epidemiology papers also should be submitted to medRxiv, but if a paper contains no health-related information, authors may choose to submit it to another bioRxiv subject category (e.g., Genetics or Microbiology). Context and evaluations x Comments 0 Comments bioRxiv aims to provide a venue for anyone to comment on a bioRxiv preprint. Comments are moderated for offensive or irrelevant content (this can take ~24 h). Please avoid duplicate submissions and read our Comment Policy before commenting. The content of a comment is not endorsed by bioRxiv. Share this comments tab (click to copy link)Copied! TRiP bioRxiv partners with journals and review services to enable posting of peer reviews and editorial decisions related to preprints they are evaluating. Reviews are posted with the consent of the authors. Community Reviews bioRxiv aims to inform readers about online discussion of this preprint occurring elsewhere. The content at the links below is not endorsed by either bioRxiv or the preprint's authors. Automated Services A variety of services now perform automated analyses of papers. Outputs from automated tools that summarize and extract information from bioRxiv preprints using AI and other technologies are displayed below. Note these tools can generate errors and the information has not been verified by bioRxiv or the authors. Blog/Media Links bioRxiv aims to inform readers about online discussion of this preprint occurring elsewhere. The content at the links below is not endorsed by either bioRxiv or the preprint's authors. Video bioRxiv partners with conferences and institutions to display recordings of talks and seminars related to preprints. These are posted with the consent of the authors. Tweets bioRxiv aims to inform readers about online discussion of this preprint occurring elsewhere. Recent twitter mentions are provided below. The content of these tweets is not endorsed by either bioRxiv or the preprint's authors. Powered by Powered by Powered by Follow this preprint X You can now receive automatic notifications when a preprint is revised, withdrawn, commented on, peer reviewed, or published in a journal. Select the events you would like to follow below and click \"Submit\". To see all of the preprints you are currently following, please go to the bioRxiv Alerts Page. Sign In to Follow this Preprint Email * We use cookies on this site to enhance your user experience. By clicking any link on this page you are giving your consent for us to set cookies. Continue Find out more",
    "commentLink": "https://news.ycombinator.com/item?id=39549838",
    "commentBody": "Rendering protein structures inside cells at the atomic level with Unreal Engine (biorxiv.org)194 points by Michelangelo11 19 hours agohidepastfavorite48 comments COGlory 17 hours agoMuyuan Chen is one of (maybe the?) primary developer of the sub-tomogram averaging portion of the EMAN2 software package (linked below in another comment). Typically what you do is take a 3D tomogram (think like a scan) using a microscope, but it's extremely noisy. Then you go through and extract all the particles that are identical, but in different orientations, in the tomogram. So if the same protein is there multiple times, you can align them to each other, and average them together to increase the signal. Then you clone back in the higher signal averaged volume at the position and orientation that you found them in originally. The one-line command to go from EMAN2 coordinates to Unreal Engine 5 is kind of crazy. As usual on these (rare) threads, I'm happy to answer any questions about structural biology or cryo-EM. reply dalke 16 hours agoparentDo you know what the author means by \"Current visualization software, such as UCSF ChimeraX6 , can only render one or a few protein structures at the atomic level.\" I haven't used VMD for about 30 years, but even in the 1990s I was using it to visualize the full poliovirus structure (4 proteins in 2PLV * 60 copies, as I recall). It took about 6-10 seconds per update on our SGI Onyx, but again, that was 25 years ago. reply COGlory 16 hours agorootparentI can only guess, but I believe that ChimeraX's rendering pipeline is single threaded (just an empirical guess based on my CPU usage when using it). Additionally, loading that many atom positions requires a huge amount of memory (I routinely use > 32 GB memory just loading a few proteins) and things start to slow down quite a bit. Loading a 60-fold icosahedral virus has used > 100 GB memory on my workstation, and resulted in a 0fps experience. It might render OK from the command line, but now imagine a few dozen of those, plus a cell, plus all the proteins in the cell... reply dalke 15 hours agorootparentOdd. I can't see why. I think we had 128 MB on that IRIX box, and I know I loaded a 1 million atom structure with copies of 2PLV (full capsid plus a bit more to get to a million.) Each atom record has ~60 bytes (x, y, z, occupancy, bond list, resid, segid, atom name, plus higher-level structure information about secondary structure, connected fragments, etc.) We had our own display list, so another (x, y, z, r, color-index) per atom, giving 20 more bytes. We probably used a GL/OpenGL display list for the sphere, and immediate mode to render that display list for each point, so all-in-all about 100 bytes per atom, which just barely fits in 128 MB. That was also all single-threaded, with a ~0.1 Hz frame rate. Again, in the 1990s. I wanted to see what more recent projects have done. Google Scholar found \"cellVIEW: a Tool for Illustrative and Multi-Scale Rendering of Large Biomolecular Datasets\" (2017) at https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5747374/ which says > The most widely known visualization softwares are: VMD [HDS96], Chimera [PGH04], Pymol [DeL02], PMV [S99], ePMV [JAG11]. These tools, however, are not designed to render a large number of atoms at interactive frame-rates and with full-atomic details (Van der Walls or CPK spherical representation). Megamol [GKM15] is a state-of-the-art prototyping and visualization framework designed for particle-based data and which currently outperforms any other molecular visualisation software or generic visualization frameworks such VTK/Paraview [SLM04]. The system is able to render up to 100 million atoms at 10 fps on commodity hardware, which represents, in terms of size, a large virus or a small bacterium. Following that is a section on Related Work: > With their new improvement they managed to obtain 3.6 fps in full HD resolution for 25 billion atoms on a NVidia GTX 580, while Lindow et al. managed to get around 3 fps for 10 billions atoms in HD resolution on a NVIDIA GTX 285. Le Muzic et al. [LMPSV14], introduced another technique for fast rendering of large particle-based datasets using the GPU rasterization pipeline instead. They were able to render up to 30 billions of atoms at 10 fps in full HD resolution on a NVidia GTX Titan Checking up on VMD, in \"Atomic detail visualization of photosynthetic membranes with GPU-accelerated ray tracing\" from 2016: > VMD has achieved direct-to-HMD rendering rates limited by the HMD display hardware (75 frames per second on Oculus Rift DK2) for moderate complexity scenes containing on the order of one million atoms, with direct lighting and a small number of ambient occlusion lighting samples. Those citations are 6-7 years ago, which make me scratch my head wondering why ChimeraX can't handle a picornavirus. reply dekhn 14 hours agorootparentThe author of EMAN2 is incorrect, I don't know why they claimed that. ChimeraX is probably like Chimera and targets a 30fps but can drop below that significantly based on dataset size and rendering quality. It should be using OpenGL with display lists (or some more modern variant on that). The main loop is likely in Python, but if you're just moving a molecule around, the rendering should touch very little python. On a modern machine with an nvidia gaming card it should be fine. For example, in this case I loaded 2PLV with \"open 2PLV\" and on the right side, there's an option to select one of the mmcif assemblies, with select 1 being \"complete icosahedral assembly, 60 copies of chains 1-4\". With the default ribbon rendering, rotating is completely smooth; with all atoms displayed (wireframe or sphere), it's still smooth. Computing a surface for the entire capsid takes well under a second(!) and still renders smoothly. Rotating shows my GPU (nvidia RTX 3080 Ti) at about 50% utilization, and if I exit Chimera, my GPU's releases ~200MB of memory. Chimera was never intended to do high quality rendering of cellular environments with many hundreds of proteins. It was intended for a combination of nice rendering and scripting directly in python. VMD definitely handled some extremely large scenarios faster. A dedicated small C++ using modern OpenGL would be able to do far, far more than Chimera when it comes to simple rendering without any scripting control. reply COGlory 13 hours agorootparentOpening the bio-assembly for 3J31 ate about 8 GB of my VRAM, and 32 of my system RAM, in ChimeraX. Which is actually less than I remember a few years ago. I wonder if the render pipeline has changed a bit. That said, it's still very significant if you have 10-20 viruses attached to a cell, for instance. EDIT - that's also for atoms. Going to 3D maps is significantly more computationally intensive. A typical, sub-tomogram average, or annotation will be in MRC file format, which is horrendously slow with a box size > 1024 pixels or so. reply dalke 14 hours agorootparentprevThanks! That's much more like what I expected. reply timtimmy 12 hours agorootparentprev\"Current visualization software, such as UCSF ChimeraX6, can only render one or a few protein structures at the atomic level.\" Lots of current visualization software is focused on visualizing a single protein structure (for example, ChimeraX). New visualizing and modeling systems are being developed to go up in scale to cellular scenes and even whole cells. For example, systems like le Muzic et al.'s CellView (2015) [1] are capable of rendering atomic resolution whole cell datasets like this in realtime: https://ccsb.scripps.edu/gallery/mycoplasma_model/ [1] : https://www.cg.tuwien.ac.at/research/publications/2015/cellV... reply frodo8sam 17 hours agoparentprevSounds pretty cool, how does EMAN2 deal with dynamic structures? I assume you'll get garbage if sufficiently different conformations get averaged together. Is there some kind of clustering to find similar conformations as is sometimes done in cryo-EM? reply COGlory 16 hours agorootparentYes, at multiple levels. You can do a heterogeneous refinement that takes the structures and solves for n number of averages, trying to use something like PCA to maximize the distances between the averages. Particles get sorted into the average they contribute most constructively to. That works well for compositional heterogeneity or large conformational differences. For minor differences, there's something called the guassian mixture model (lots of software packages have similar, but GMM is EMAN2's version). https://blake.bcm.edu/emanwiki/EMAN2/e2gmm What you can get out of the other end is a volume series, that shows reconstructed 3D volumes along various axes of conformational variability. This quickly turns into a multi-dimensional problem, but it has been very successful in, for instance, seeing all the different states of an active ribosome. reply Vt71fcAqt7 13 hours agoparentprevSlightly off topic but what do you think of the potential growth for GWAS? How much will it improve in the next 10 years? 2x? 10x? reply COGlory 11 hours agorootparentNot an expert in that field. I can speculate wildly that I'm not super optimistic. I can say I've seen the narrative around the field start to give way slightly - shifting from \"genome is all you need\" to \"genome is not enough\". The problem is that many of the interesting or urgent pathologies have no obvious (or weak) associations. Or maybe the noise level is too high. So there's got to be a piece of the puzzle we're missing, or something is getting lost in the noise. Whether a neural network can pull something out of the noise remains to be seen, but if it can't, I'm not super optimistic about our chances. Overall I'd say trying to tackle the problem at the outcome level is probably more promising right now. Even if we can find good associations, we're still lacking therapies. reply BigParm 16 minutes agoprevPeople overlook game engines for plotting and data visualization. You can have your character fly around whatever math/geometry you plotted. It’s great. reply benjismith 16 hours agoprevThis reminds me of the amazing molecular animations of Drew Berry, which he showed in this TED talk: https://youtu.be/WFCvkkDSfIU?si=JNe06VS8TjIrHpqh Which was 12 years ago! After watching that video, I had a much greater appreciation for how our bodies are made up of trillions of tiny protein machines. Fascinating stuff!! reply shagie 16 hours agoparenthttps://xvivo.com/examples/the-inner-life-of-the-cell/ I didn't realize there was also a Powering the Cell: Mitochondria video. The classic one narrated - https://youtu.be/QplXd76lAYQ One of my pandemic YouTube binges was watching Ron Vale videos about kinesin and dynein. https://youtu.be/9RUHJhskW00 https://youtu.be/lVwKiWSu8XE https://youtu.be/FRtqfpO8THU And searching for Ron Vale will bring a number of other videos about molecular machines. reply dekhn 14 hours agorootparentMy friend in grad school was in Ron's group. He built a microscope that visualized individual kinesin molecules and measured their speed using fluorescent labelling. The whole thing was held together with a bunch of scripts written in LabView. Ron had oodles of money and was able to support long-term software development of open source software like MicroManager, which gives a common interface to a wide range of microscopy software. The systems he studies are literally little motors that can attach to biological surfaces and drive around in specific directions, pick up payloads, and then drive to other places. They work in very different way from how humans engineer tiny motors and understanding/engineering their behavior was a major focus in the early 2000s. reply shagie 14 hours agorootparent> My friend in grad school was in Ron's group. He built a microscope that visualized individual kinesin molecules and measured their speed using fluorescent labelling. In vitro motility of yeast cytoplasmic dynein - https://youtu.be/lVwKiWSu8XE?si=Su29neym0wg9DalR&t=627 reply dekhn 13 hours agorootparentYeah, exactly that, but with kinesin instead of dynein (everybody started with myosin, but loss interest, and moved to kinesin and then dynein) and about 10 years earlier. Those little blobs moving along the filaments are ~10-100 nanometers, you wouldn't normally be able to see them, but they managed to tether fluorescent (glowing) molecules to them and those act like point sources of light, which allows for precise localization because the PSF of a point is approximately gaussian and finding the centroid of a gaussian is trivial. reply benjismith 14 hours agorootparentprevNice! Thank you for the links! reply jryan49 7 hours agoparentprevI loved reading this paper https://philpapers.org/archive/NICITC.pdf (was linked on HN a while ago) which actually challenges that viewpoint. Disclaimer: I'm not a biologist :) reply adkaplan 14 hours agoparentprevAppreciate the share. I've seen a compilation of these clips out of context and loved them. Never figured out where they came from. They really are amazing in striking the balance between organic and mechanistic. The Kinesin in particular are cute. https://www.youtube.com/watch?v=wJyUtbn0O5Y&t=2s This video shows it quite well too. EDIT: looks like someone else shared the same. reply mncharity 9 hours agorootparent> The Kinesin in particular are cute. Yes... though regrettably, that render is profoundly misleading. The payload is actually flailing around violently. Dancing in the molecular nanoscale moshpit from hell. Between each glacial step, the payload basically explores its entire tethered configuration space. Picture balloon in a hurricane tied to a mouse clinging to a wire, rather than a donkey towing a barge. The render optimizes for art over education, for pretty over engendering misconceptions. Consider filming a runner, and only showing frames where the arms and legs are in the same unmoving positions, the rigid person quietly floating along over the ground. Or a soccer game render, of floating statues. Not without value, but profoundly misleading. Especially for the poor alien student, deeply unfamiliar with animal life and planetary surfaces. One nice aspect of TFA, is rendering a moment frozen in time. Rendering non-bogus dynamics remains a hard open problem. reply eurekin 18 hours agoprevOne of videos: https://twitter.com/manorlaboratory/status/17630941356392943... Anybody came across the code? reply gilleain 18 hours agoparentThere's a link in the paper to a turorial: https://blake.bcm.edu/emanwiki/EMAN2/unreal_render which uses ChimeraX: https://www.cgl.ucsf.edu/chimerax/ reply dekhn 17 hours agorootparentso it's using ChimeraX to turn a PDB file (protein or DNA structure) into an isosurface and then triangulating the surface into a mesh which is then rendered in unreal. reply COGlory 16 hours agoparentprevhttps://github.com/cryoem/eman2/blob/master/programs/e2spt_m... reply gpnt 17 hours agoparentprevvideos: https://www.biorxiv.org/content/10.1101/2023.12.08.570879v1.... reply gilleain 18 hours agoprevReminds me of the drawings of David Goodsell: https://ccsb.scripps.edu/goodsell/ which are similarly about the 'packed' nature of cells. reply timtimmy 14 hours agoparentI just released a biology education app very much like the preprint for the Vision Pro launch (and soon for iPad/iPhone). I worked with David Goodsell's group to integrate their whole-cell bacteria model and David wrote the content. It looks like this: https://twitter.com/timd_ca/status/1753250624677007492 Our first bit of content is a tour through a 300 million atom bacteria cell for Apple Vision Pro (>60 fps, stereoscopic, atomic resolution). We developed the tech for iPhone, iPad and AVP mobile GPUs (UE5 doesn't support this on the devices we're targeting). iPad: https://twitter.com/timd_ca/status/1592948101144547328 The linked preprint is beautiful, and I love the pipeline. I wonder if it's possible to export to other tools like Blender? The linked preprint is part of a pretty cool field of research into mesoscale modeling and visualization. For me these are a few of the standout papers, projects and works in the area (and there are many more): - le Muzic et al. \"Multi-Scale Rendering of Large Biomolecular Datasets\" 2015 [1] - - Ivan Viola's group helped pioneer large scale molecular visualization. This reference should be in the preprint, IMO. - Maritan et al. \"3D Whole Cell Model of a Mycoplasma Bacterium\" [2] - - This is out of David Goodsell's lab and the model I'm using. - Stevens et al. \"Molecular dynamics simulation of an entire cell\" [3] - Brady Johnston's Molecular Nodes addon for Blender [4] - YASARA PetWorld [5] [1] : https://www.cg.tuwien.ac.at/research/publications/2015/cellV... [2] : https://ccsb.scripps.edu/gallery/mycoplasma_model/ [3] : https://twitter.com/JanAdStevens/status/1615693906137473030 and https://www.frontiersin.org/articles/10.3389/fchem.2023.1106... [4] : https://bradyajohnston.github.io/MolecularNodes/ [5] : http://download.yasara.org/petworld/index.html reply leoncvlt 12 hours agoprevIf you get a kick out of 3D renderings of cells and molecules, you're gonna have a field day with the work done at https://random42.com/. PSA: I started working there as a 3D artist but now lead the interactive department. You'd be surprised at how much a good art direction really makes a difference in scientific visualization. Real-time graphics advanced considerably in the last couple years but it's always a challenge to transport that nice, smooth pre-rendered look over to mobile devices and the web at 60 frames per second (90 on virtual reality headsets, to boot...) reply dr_kiszonka 5 hours agoparentWow - these are stunning! I am curious if you have any \"realistic time\" animations, e.g., where blood circulates with the speed close to the one in the human body. reply vdm 16 hours agoprevVideos: https://www.biorxiv.org/content/10.1101/2023.12.08.570879v1.... reply leptons 14 hours agoparentNice videos but I'm always reminded when watching this type of molecular biology video that it's missing all the water molecules. These proteins and things aren't floating around in empty space. reply protoman3000 12 hours agoprevI would like to ask a question and add before that that I have no intent to judge, discredit or diminish the value of this. It’s merely that I really don’t understand and would like to gain insight. The question is: How is this a scientific contribution? Or, to ask it differently: What makes this a scientific contribution? reply bglazer 11 hours agoparentIt’s more of an engineering accomplishment. I could see this being useful for exploratory data analysis of large protein (mesoscale) complexes. A surprising amount of science starts with a grad student staring at a really complex plot for a really long time, then suddenly going “oh shit thats weird”. That kind of realization is terribly difficult if your visualization tools are fighting you the whole time. reply abraxas 17 hours agoprevthis reminds me of the video from years ago that blew my mind: https://vimeo.com/76306502 reply GeoAtreides 17 hours agoparentrequires login : ( but it might be this video: https://www.youtube.com/watch?v=wJyUtbn0O5Y reply yardshop 16 hours agorootparentIt didn't require one for me. There was a Google login box but I closed it and the video played fine. Maybe a regional thing? reply abraxas 16 hours agorootparentprevThis is an abberviated version of the same yes, I'm surprised though that the vimeo link isn't working without login. I don't have an account with them and was able to play it without issues. reply littlestymaar 14 hours agorootparentSame for me, it says something about the video lacking an age classification. reply throwaway8877 16 hours agoparentprevI have seen this video. The complexity of life is mind blowing. Even more so the fact that we know anything about it. reply mchinen 16 hours agoprevThis is fascinating to watch. I understand that they get the proteins from PDB/ChimeraX, but how much manual process is involved to map and place the individual proteins? The paper says it gets the protein locations from CryoET tomograms, but I'd be surprised if these let you automatically identify which proteins are where, and exactly how to place them, and even less so how the ligomers bind together to form larger structures - for example, in the video the membrane surfaces are very smooth, and look almost textbook picture perfect, which suggests they come from a hardcoded model or are smoothed in some way. One part of the paper mentions subatomic averaging from the tomogram, but another mentions: > From a tomogram (Figure3a), we select the particles and determine the orientation of the crown of the spike, as well as the stalk that connects the spike to the membrane Is this a manual process, where the researcher is using his mental model of how the proteins are fit and placing/rotating individual proteins? Or do the tools they developed let this be automated. Both are impressive! If the former, I'm blown away by the effort it must take to make these kinds of videos. reply COGlory 16 hours agoparentIt's a guided, but automated process. EMAN2 (the software used/partially written by the author), for instance, has a convolutional neural network particle picker. So you can pick a few particles by hand, pick some noise, pick some bad particles, train a neural network, and then inference that network to pick the remaining particles throughout the tomogram. There are a variety of other methods too. There is simple 6 dimensional real-space cross correlation. You can place points by hand, or according to a model. For instance, if you are trying to identify viral spike proteins, and the virus is spherical, and the spike proteins are always on the surface of the sphere, you have a great starting point. So you can say \"place points at n interval along the surface of this sphere\" and then oversample the spherical surface. You then take a reference volume (can be generated a number of ways), and check each point you placed to see how well it matches the reference volume. You can allow for rotations and translations of the reference volume at each point, and if you find points that are too close together, you can merge them automatically. If you have a high contrast, relatively static protein (such as a ribosome), you can do 2D template matching in the tomogram, where you use a central slice (or maybe a collapsed projection) to do cross correlation in 3 dimensions instead of six (X translation, Y, translation, Z rotation). Or you can beef that up with more neural network/YOLO type stuff. EDIT: To expand on this, continuous density like membranes can be roughly modeled just with techniques like thresholding, watershedding, etc. There are some neural nets such as Membrain [0] and tomoseg [1] (also by the author of this paper), but membranes certainly are trickier. I typically segment membranes by hand (and do so rarely). [0] https://www.sciencedirect.com/science/article/pii/S016926072... [1] https://blake.bcm.edu/emanwiki/EMAN2/Programs/tomoseg reply amelius 13 hours agoprevWhat is the meaning of color in these visualizations? Does every functional unit have its own color? reply zmmmmm 12 hours agoprevWould love a way to see these in 3D in VR / MR. reply COGlory 11 hours agoparentChimeraX has a VR functionality. Certain modeling programs still support Nvidia's Stereo3D (Coot, PyMOL, Chimera, ChimeraX, and more) which I still use for modeling. That relies on X11 unfortunately, so I'm looking for a new way to do 3D viewing. reply zmmmmm 7 hours agorootparentthanks! will try that out reply canadiantim 19 hours agoprev [–] Wow, Unreal! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The preprint explores utilizing the Unreal Engine to visualize protein structures at the atomic level within cells, enabling interactive navigation inside the cell environment.",
      "Cryogenic electron tomography enables the identification of macromolecules inside cells, but the visualization at the atomic level is challenging, addressed by using a video game engine.",
      "Tools are presented to convert protein structures from CryoET into scenes for exploration, with the authors sharing the work under a CC-BY 4.0 International license."
    ],
    "commentSummary": [
      "The post delves into rendering protein structures at the atomic level within cells utilizing Unreal Engine, emphasizing the sub-tomogram averaging process in EMAN2 software.",
      "Limitations of current visualization software like UCSF ChimeraX are highlighted, including the challenges posed by handling large structures.",
      "Various tools and techniques for rendering molecular datasets are explored, with a specific focus on ChimeraX's real-time rendering capabilities and the intersection of game engines with scientific visualization."
    ],
    "points": 194,
    "commentCount": 48,
    "retryCount": 0,
    "time": 1709217489
  },
  {
    "id": 39548517,
    "title": "The Musical Legacy of 'Shave and a Haircut'",
    "originLink": "https://en.wikipedia.org/wiki/Shave_and_a_Haircut",
    "originBody": "Toggle the table of contents Shave and a Haircut 10 languages Català Dansk Deutsch Español Français Italiano Nederlands 日本語 Português Suomi Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Special pages Permanent link Page information Cite this page Get shortened URL Download QR code Wikidata item Print/export Download as PDF Printable version In other projects Wikimedia Commons From Wikipedia, the free encyclopedia \"Shave and a Haircut\" in G major. As a door knock Problems playing this file? See media help. \"Shave and a Haircut\" and the associated response \"two bits\" is a seven-note musical call-and-response couplet, riff or fanfare popularly used at the end of a musical performance, usually for comedic effect. It is used melodically or rhythmically, for example as a door knocker. \"Two bits\" is a term in the United States and Canada for 25 cents, equivalent to a U.S. quarter. \"Four bits\" and \"Six bits\" are also occasionally used, for example in the cheer \"Two bits, four bits, six bits, a dollar.\" The final words may also be \"get lost\", \"drop dead\" (in Australia),[citation needed] or some other facetious expression. In the UK, it was often said as \"five bob\" (slang for five shillings), although words are now rarely used to accompany the rhythm or the tune. History[edit] An early occurrence of the tune is from an 1899 Charles Hale minstrel song, At a Darktown Cakewalk.[1] Other songs from the same period also used the tune. The same notes form the bridge in the Hot Scotch Rag, written by H. A. Fischler in 1911. An early recording used the 7-note tune at both the beginning and the ending of a humorous 1915 song, by Billy Murray and the American Quartet, called \"On the 5:15\". In his 1933 novel, Hizzoner the Mayor, Joel Sayre wrote of boats \"tooting the official Malta welcome blast to the tempo of “Shave-and-a-haircut-two-bits, shave-and-a-haircut-two-bits, shave-and-a-haircut-two-bits”, which was soon taken up by every craft in the harbor that had a boiler.[2] In 1939, Dan Shapiro, Lestor Lee and Milton Berle released \"Shave and a Haircut – Shampoo\",[3] which used the tune in the closing bars. In the same year, Rosalind Rosenthal and Herbert Halpert recorded \"Shave and a Haircut, Bay Rum\".[4] Popularity[edit] The tune can be heard on customized car horns,[5][6] while the rhythm may be tapped as a door knock[7][8][9][10][11][12][13][14] or as a Morse code \"dah-di-di-dah-di, dah-dit\" ( –··–· –· )[15] at the end of an amateur radio contact. The former prisoner of war and U.S. Navy seaman Doug Hegdahl reports fellow U.S. captives in the Vietnam War would authenticate a new prisoner's U.S. identity by using \"Shave and a Haircut\" as a shibboleth, tapping the first five notes against a cell wall and waiting for the appropriate response. U.S. POWs were then able to communicate securely with one another via a tap code.[16] The tune has been used innumerable times as a coda or ending in musical pieces. It is strongly associated with the stringed instruments of bluegrass music, particularly the 5-string banjo. Earl Scruggs often ended a song with this phrase or a variation of it. On the television show The Beverly Hillbillies, musical cues signifying the coming of a commercial break (cues which were in bluegrass style) frequently ended with \"Shave and a Haircut\". It is the second most popular bluegrass run, after the G run.[17] \"Shave and a Haircut\" was used in many early cartoons, particularly Looney Tunes cartoons. It was also used as an ending to many cartoon shows, just after the credits. Decades later, the couplet became a plot device to lure-out an intended victim, as used by Judge Doom in the film Who Framed Roger Rabbit, the idea being that toons cannot resist finishing with the \"two bits\" when they hear the opening rhythm.[18] The tune was also featured in early Nokia phones, like the 3310 model, as the That's it! ringtone.[19][20] Usage[edit] The phrase has been incorporated into countless recordings and performances. Notable examples include: Johnny's Theme, the music that opened The Tonight Show Starring Johnny Carson, famously ended with the \"shave and a haircut\" flourish every weeknight for 30 years and 4,531 episodes. \"That's a Lot of Bunk\", a 1920s novelty song composed by Al Wilson, James A. Brennan and Mack Henshaw, and performed by Billy Jones and Ernest Hare, known as \"The Happiness Boys\", closes with the riff.[21] The Crazy Gang sang \"How's your father? Goodbye!\" to the same tune at the end of their 1937 movie O-Kay for Sound.[22] R&B singer and bandleader Dave Bartholomew used the phrase on two of his recordings: \"Country Boy\" (1950) at the very end, and the original version of \"My Ding-a-Ling\" (1952) as a figure introducing each verse.[23] Les Paul and Mary Ford's Capitol recording of \"Magic Melody\" concluded with the phrase minus the last two notes (\"two bits\"). Responding to complaints from disc jockeys, Capitol in 1955 released \"Magic Melody Part 2\"—consisting solely of the missing notes—on a 45, said to be the shortest tune on record.[24] P. D. Q. Bach ends his \"Blaues Gras\" (\"bluegrass\") aria with \"Shave and a Haircut\", sung in Denglisch (mangled German and English): \"Rasieren und Haarschneiden, zwei bitte\" (\"Shave and haircut, two please\", ungrammatical in either language). \"Zwei bitte\" is a Denglisch pun, sounding like \"two bits\" to a speaker of both languages.[25] The melody is also used in The Short-Tempered Clavier.[26] The original version of \"Love and Marriage\" by Frank Sinatra (recorded for Capitol Records in 1955) ends with the tune. \"Unsquare Dance\" (1961) by Dave Brubeck ends with the tune, and also features part of \"Turkey in the Straw\". One of the musical numbers in Mister Magoo's Christmas Carol (1962), \"We're Despicable (The Plunderers' March),\" incorporates the melody into its chorus. The characters sing, \"we're blank-blankety-blank-blank no good.\" Every interview by Nardwuar the Human Serviette ends with the melody of the song, with Nardwuar singing \"doot doot da loot doo\", after which the interviewee is expected to reply with \"doot doo\". The ending theme in the credits of Barney the Dinosaur makes use of it from Seasons 1-3. In a 1960s television comedy sketch called \"The Time Window\", Mike Wallace interviews Victor Borge who is portraying composer and pianist Franz Liszt. During the segment, Borge (Liszt) states that his very first composition were two notes; which he plays on the piano. He next demonstrates that without these two notes \"we would never have had this\", and he plays \"Shave and a Haircut\".[27][28] The animated show Animaniacs makes frequent use of this theme, in particular at the end of the song \"Wakko's America\" with the line \"That's all the capitals there are\". The song \"Gee, Officer Krupke\" from Leonard Bernstein's musical West Side Story ends with the tune. The tune is sampled in several of \"Weird Al\" Yankovic's polka medleys. \"Everything About You\", by Ugly Kid Joe (recorded for Mercury Records in 1992), ends with the tune. The song \"Mi Abuela\" by Wilfred y La Ganga (BMG Ariola, 1990) opens with the tune as a door knock. The tune is played as part of the guitar solo in the song \"Play with Me\" by Extreme, which is also used in the mall chase scene in Bill & Ted's Excellent Adventure. Cassian Andor taps the five-note rhythm to signal Bix Caleen, outside her window, in S1:E7 \"The Announcement\" of the series Star Wars: Andor. There is no two-note response. Uses in other countries[edit] Shave and a Haircut An example of the couplet. Problems playing this file? See media help. The Italian version is Ammazza la vecchia … col Flit! (English: \"Kill the old lady … with Flit!\")—Flit being an old brand of DDT insecticide. This is a humorous popular version of a post-World War II commercial Ammazza la mosca... col Flit (English: \"Kill the fly with Flit!\").[citation needed] The tune is used in Catalan with a different lyric: \"Nas de barraca … Sant Boi\" (English: \"Shack nose … Sant Boi\"). It is also tapped, as a door knock. The Catalan lyrics may come from Blanes, where it was sung twice with Nas de barraca. Sant Boi. Cinc de carmelos pel noi (English: Shack nose. Sant Boi. Five candies for the boy).[29] In Spain, it is sung with the lyrics, Una copita … de Ojén (English: \"A shot … of schnapps\"). In Mexico, it means a vulgar insult with the lyrics, Chinga tu madre … cabrón (English: \"Fuck your mother … bastard\"). In Irish barroom music, the tune is sometimes tagged at the end of a song. The performer sings the first part to the lyrics, \"How is your aul' one?\" (read: \"old one\", a slang term for mother), to which the audience replies, \"Gameball!\" (A slang term meaning A-OK).[30] In Sweden, it is well known as Kvart över elva … halv tolv, which means A quarter past eleven … half past eleven. The twist doesn't work as well in English, as the English time system treats 11:30 as a continuation of eleven instead of as the first half of twelve. Halv tolv thus means half twelve and is the correct Swedish equivalent of half past eleven. In Sweden, the melody was also used in a commercial for the Bronzol brand of candy with the slogan Hälsan för halsen — Bronzol (English: Health for the throat — Bronzol). In Icelandic, the lyrics are Saltkjöt og baunir … túkall (English: \"Salt meat and split peas … two krona\" (króna is the currency in Iceland)). In the Netherlands, the phrase is used when someone leaves with the intention to not return. Die zien we nooit meer, te-rug (English: We shall never see them, a-gain). It is used as a way to make fun of someone/something, if it suddenly disappears from the scene. In Argentina, Carlos Balá, a former children's TV show host, used to include a bit in his routine in which he would whistle the \"shave and a haircut\" part of the tune, prompting the children in the audience to answer \"Ba-lá\" to the rhythm of the two final notes. In the same country in school context to call for silence being sung with the teacher saying the phrase Tapa Tapita (Bottlecap, Small cap) and the students answering Tapon (Plug), followed with the teacher singing the phrase cierro la boca (shutting my mouth) and answering ya está (already done) See also[edit] Banjo roll Oriental riff Bo Diddley beat References[edit] ^ Fuld, James (2000). The Book of World-Famous Music: Classical, Popular, and Folk (5th ed.). New York: Dover Publications. p. 495. ^ Sayre, Joel (1933). Hizzoner the Mayor: A Novel. New York: John Day Company. pp. 28–29. ^ \"Catchy Tune Central Archived 2010-06-12 at the Wayback Machine\", Members.MultiMania.NL. ^ Safire, William (April 3, 1983). \"ON LANGUAGE; PRAY, WHY ME?\". The New York Times. Retrieved May 21, 2019. The Book of World-Famous Music,\" a 1966 work by James J. Fuld, which reveals a 1939 ditty, \"Shave and a Haircut - Shampoo,\" by Dan Shapiro, Lester Lee and Milton Berle, and a similar number in the same year, \"Shave and a Haircut, Bay Rum,\" recorded as a folk melody by Rosalind Rosenthal and Herbert Halpert. ^ Franz, Carl; Havens, Lorena (2006). The People's Guide to Mexico. Avalon Travel Publishing. p. 319. ISBN 1-56691-711-5. ^ Arellano, Gustavo (2008). Ask a Mexican. Scribner. p. 26. ISBN 978-1-4165-4003-8. ^ Thompson, Chuck (2009). To Hellholes and Back: Bribes, Lies, and the Art of Extreme Tourism. Holt Paperbacks. p. 220. ISBN 978-0-8050-8788-8. ^ Stanton, John (September 20, 1948). \"In Mexico City Traffic is Terrific\". LIFE. Time, Inc. ^ Keenan, Joseph John (2004). Breaking Out of Beginner's Spanish. University of Texas Press. ISBN 0-292-74322-X. ^ Axtell, Roger E.; Fornwald, Mike (1998). Gestures: The Do's and Taboos of Body Language Around the World. Wiley. p. 101. ISBN 0-471-18342-3. ^ Axtell, Roger E. (1998). Do's and Taboos of Humor Around the World. Wiley. ISBN 0-471-25403-7. ^ Ruiz Fornells, Enrique; Ruiz-Fornells, Cynthia Y. (1979). The United States and the Spanish World. Sociedad General Española de Librería. ISBN 84-7143-192-0. ^ Wilder, Cora Sarjeant; Sherrier, James (1992). Celebrating Diversity. Ginn Press. ISBN 0-536-58133-9. ^ Partridge, Eric; Dalzell, Tom; and Victor, Terry (2007). The concise new Partridge dictionary of slang and unconventional English, p.571. ISBN 978-0-415-21259-5. ^ King, Thomas W. (1999). Modern Morse Code in Rehabilitation and Education. Allyn & Bacon. p. 77. ISBN 0-205-28751-4. ^ Brace, Ernest C. (May 2, 2008). \"Messages From John\". JohnMcCain.com. Archived from the original on December 1, 2008. Retrieved 2008-11-26. ^ Traum, Happy (1974). Bluegrass Guitar, p.26. ISBN 0-8256-0153-3. ^ \"Quotes from \"Who Framed Roger Rabbit\"\". IMDb. ^ \"NOKIA 3310 ringtone That's it!\". YouTube. ^ \"Shave and a Haircut (Nokia \"That's it!\" ringtone) - Piano Quickie\". YouTube. ^ \"\"That's A Lot Of Bunk\" - Billy Jones & Ernest Hare (1923 Edison)\". YouTube. Archived from the original on 2021-12-21. Retrieved 4 July 2020. ^ O-Kay for Sound, https://archive.org/details/O-kayForSound. Retrieved 2019-02-02. ^ Bartholomew, Dave, \"The King Sides\" Collectables (CD) 2883, 2004 ^ Cleveland, Barry (September 1, 2002). \"It Happened This Month\". OnStageMag.com. Archived from the original on May 27, 2009. Retrieved 2008-11-26. ^ \"Cantata 'Blaus Gras'\". The Peter Schickele/P.D.Q. Bach Web Site. July 3, 2011. Retrieved 2012-12-07. ^ \"The Key of P. D. Q\". ^ \"Victor Borge - As 'Franz Liszt' with Mike Wallace c.1960\". YouTube. Archived from the original on 2021-12-21. Retrieved December 26, 2021. ^ Hencken, John (22 August 1992). \"TV Reviews : Borge's 'Then & Now' Is Mostly Now on PBS\". Los Angeles Times. Retrieved December 26, 2021. ^ Sola i Ramos, Elisa (December 1999). \"PROVERBIS, DITES I FRASES FETES DE BLANES\" (PDF). Servei de Català de Blanes (CPNL). Retrieved 19 March 2016. ^ Martin Dardis. \"Finnegan's Wake lyrics and chords\". Irish Folk Songs. Retrieved 16 February 2019. External links[edit] Description Dutch article on \"Shave and a haircut\" Sheet music for \"At A Darktown Cakewalk\" from the IN Harmony system at Indiana University Retrieved from \"https://en.wikipedia.org/w/index.php?title=Shave_and_a_Haircut&oldid=1211134795\" Categories: Rhythm and meter Riffs Hidden categories: Pages using the Score extension Webarchive template wayback links Articles with short description Short description is different from Wikidata Articles with hAudio microformats All articles with unsourced statements Articles with unsourced statements from October 2019 Articles with unsourced statements from October 2013",
    "commentLink": "https://news.ycombinator.com/item?id=39548517",
    "commentBody": "Shave and a Haircut (wikipedia.org)193 points by bschne 21 hours agohidepastfavorite130 comments miniatureape 21 hours agoA made a little “secret knock” authentication demo. Shave and a haircut is obviously the secret knock used in the demo https://miniatureape.github.io/prohibition/ (Nb: I’m sure this doesn’t work great on some devices) reply jodacola 20 hours agoparentFun! Over a decade ago, my boss and I added a secret knock using Shave and a Haircut just like this to our wayfinder product, a large touchscreen app used on trade show floors. It’s how we got into our admin to maintenance them at the shows and was useful if something was wrong with the onscreen keyboard (or we didn’t have our own keyboard to attach). reply mattbee 19 hours agorootparentI used it for a silly credits screen in football trivia game - summer 2002 I could knock on the screen of a many pub trivia machine to make it show my name :) reply bmacho 1 hour agoparentprevI can't use it on Edge, I tap tap, record, tap tap, do everything I can, but its Scram every time reply toxik 19 hours agoparentprevIt just always says scram no matter what I do, I can record a single knock and it will still say \"scram\". Latest Safari on latest macOS. reply skolskoly 7 hours agorootparentI had an issue, but I figured it out. The rhythm is not the same as the one on the wikipedia page - which I was already familiar with. After looking at my recordings I realized I was playing it straight, but the default knock recording has a swing/triplet feel. straight: DAA DA-DA DA DA, DA DA swung: DAA DA D-DA DA, DA DA I hope my highly rigorous notation is helpful here edit - in hindsight this has nothing to do with your problem reply esposito 8 hours agorootparentprevI couldn't get it to pass on my phone, but \"scram\"/\"get out of here\" is equivalent to a password rejected/401 response. A single knock does not match the expected pattern, so you would expect it to be rejected. reply bmacho 1 hour agorootparentNo, there is a recording function that is supposed to let us in (I think?), but I don't know how reply mkmk 20 hours agoprev> The former prisoner of war and U.S. Navy seaman Doug Hegdahl reports fellow U.S. captives in the Vietnam War would authenticate a new prisoner's U.S. identity by using \"Shave and a Haircut\" as a shibboleth, tapping the first five notes against a cell wall and waiting for the appropriate response. With the mass export of western culture, I wonder if there are any songs, tunes, or patterns like this that would still reliably work as a shibboleth. reply spacebacon 11 minutes agoparentDueling Banjo’s is an equally recognizable call and response. reply gerdesj 10 hours agoparentprevA British or Irish person might kick off with an approved weather discussion (appropriate with strangers, for the use of) opening gambit and await a sanctioned response. Another option is \"fork 'andles\". The correct response is a repeat followed by childish giggling but the rubbish assumed accents are also part of the shibboleth. This one may be gradually dying out. A final option is to surreptitiously start a queue. reply sam_bristow 9 hours agorootparentPretty sure you could identify anyone who grew up in New Zealand just by saying \"Tutira mai nga iwi\". You'd either get an almost immediate involuntary response from a kiwi or just confusion from anyone else. reply pavel_lishin 20 hours agoparentprevI imagine they would have to be hyper-local at this point. You could authenticate someone from a major metropolis, maybe, but not likely the whole of the US. reply kcorbitt 12 hours agorootparentI once interviewed a candidate for an engineering position in Seattle. It quickly turned out that he had fabricated his entire work history and education. My first clue was that he claimed two years of work experience at Costco HQ in Issaquah, Washington but he pronounced it as \"Is-ACK\" (the local pronounciation is \"IS-uh-kwah\"). No one who spent two years in Issaquah would ever pronounce it that way, regardless of the accent you're coming from. His story just got weirder from there. [1]: To this day I'm not sure why -- he actually performed quite well on the tech part of the interview and might have gotten an offer if he hadn't turned out to be so untrustworthy! reply floren 11 hours agorootparentI'd say the Washington shibboleth should be \"Puyallup\" reply monknomo 10 hours agorootparentI once met someone who lived in Seattle a couple years and said \"Puget Sound\" with a hard g reply mauvehaus 6 hours agorootparentprevEnumclaw/EnumClaw? https://devblogs.microsoft.com/oldnewthing/20100401-00/?p=14... reply psunavy03 10 hours agorootparentprevOr Snohomish. Or Sequim. Or Cle Elum. reply chihuahua 9 hours agorootparentLake Keechelus reply chanandler_bong 9 hours agorootparentprev...or Spokane. \"Spo-cane\" is one I've heard not infrequently. reply resolutebat 9 hours agorootparentTIL it's supposed to be \"spoh-KAN\". reply mchaver 18 hours agorootparentprevThe whole of the US would be hard since national level news and culture is what is getting exported, but local knowledge by state/region is a possible source. Like pronunciation of location names pronunciation (like pronunciation of Tilamuk Oregon, Miami Oklahoma, etc.), songs and chants for university sports teams, and jingles and phrases from local commercials. Rock Chalk... reply buildsjets 13 hours agorootparentI enjoy watching recent Seattle arrivals attempt to pronounce Mukilteo, Sequim, and Sekiu. I grew up in Long Island, New York, and there used to be a radio commercial for a local insurance company that bagged on national competitors for not being able to pronounce Ronkonkoma. Other local place names that were difficult for some to pronounce, mostly indigenous language derived, were Quogue, Patchogue, Cutchogue, Yaphank, Massapequa, Secatogue. If you can rattle those off in 1 or 2 seconds, and still throw in a four letter epithet, you can pass as a local. F'n Quogue. reply bitwize 13 hours agorootparentLouisiana has a long list of place names like this. New Orleans street names alone have standard pronunciations, many of which are not obvious especially if you've seen the name in other contexts. Then there are the city names like Plaquemines, Natchitoches, and even New Orleans itself (if you say \"new or-LEENZ\" like Chuck Berry you will be immediately flagged as an outsider). I reminded my wife of Natchitoches every time she complained about pronouncing Massachusetts place names like Leominster when we lived up north. reply munificent 11 hours agorootparentI grew up in New Orleans and now live in Seattle, and all of my intuition for pronunciation is just totally broken. In many places, if you see a \"weird\" (as in non-English-seeming) place name, a reasonable pronunciation guess is to just assume it's from a Romance language and pronounce it vaguely Spanish/French-ish. This works because so many foreign names that are common in the US that aren't obviously Anglo/Germanic are from Spanish, French, or Italian immigrants. New Orleans screws that all up, though, because it has such a complex intertwined cultural history. In New Orleans, place names often have a pronunciation that is explicitly weirder and sort of the opposite of phonetic. You mentioned \"Natchitoches\", which is pronounced locally like \"Nackodish\". There is no reasonable algorithm that would take as input the spelling of a place name in New Orleans and output its pronunciation. So my usual algorithm for pronouncing an unfamiliar name is, \"Guess that it's like a Romance language and if not assume it's completely weird and unrelated to the spelling.\" But Native American-derived names in the Pacific Northwest often confound that. They don't have Romance vowels or emphasis at all (for obvious reasons). And the pronunciation often is very close to the spelling. (I assume that's because the spelling came along so much more recently here in the PNW than on the East and Gulf Coast, and hasn't had as much time to drift.) My dumb algorithm for pronouncing PNW placenames is \"Imagine an American who's never even heard of a European country much less visited one, and have them pronounce the name phonetically.\" And it works surprisingly well! For example, \"Mukilteo\". If you try to throw some Romance flair onto it, you'd get \"Muh-KILL-tey-o\", which isn't right (but does sound charmingly exotic). It's anybody's guess how that would be pronounced if it were a street in New Orleans. Maybe \"Mill-toe\". But if you imagine some hopelessly bored midwestern kid forced to read it out loud in school and not even trying to get it right and they'd go, \"Muh-kill-TEE-oh\" and... that's it. Likewise, I kept wanting \"Anacortez\" to sound like some Spanish explorer \"Anna CorTEZ\". But, no, it's just \"Anna-CORE-tiss\". \"Humptulips\" is literally \"hump tulips\". \"Chimacum\" is \"chim-uh-cum\". \"Snoqualmie\" is \"snow-quall-me\". They all have the most vanilla-sounding pronunciation. I admit that Sequim (\"skwim\") and Puyallup (\"pyoo-A-lup\") are weird. reply mauvehaus 6 hours agorootparent> In many places, if you see a \"weird\" (as in non-English-seeming) place name, a reasonable pronunciation guess is to just assume it's from a Romance language In New England at least you have three options: English, French (Canadian), or indigenous as mangled by a bunch of half-literate English or French speakers 200 years ago. As a result, it's never quite clear if you're dealing with vowels that went through the Great Vowel Shift that divides middle and modern English[0] or not. [0] I am not a linguist, cunning or otherwise. This is an approximation. reply bee_rider 12 hours agorootparentprevWorcester, which will have the advantage of letting us find any secret British. reply mauvehaus 6 hours agorootparentThe town in Ohio is literally spelled Wooster. I'm not sure what that says about us, but it certainly says something. reply card_zero 6 hours agorootparentThat's named for a Revolutionary War general, whose name was Wooster, like 1920s British character Bertie Wooster. There were 169 Wooster families in London in the 1891 UK census. I kind of doubt they all descended from the US, so what this says to me is that some British person filling out a form heard the name \"Worcester\" and very sensibly wrote Wooster. Worcester is a corruption of Wigraceaster which is a corruption of Weorgoran Ceaster, so rigidly sticking to an orthodox spelling is ridiculous and it should be reformed. Like Featherstonehaugh turned into Fanshaw. reply InitialLastName 12 hours agorootparentprevThe pronunciation of Chili, NY is an immediate way to ID anyone from the Rochester area. reply QuercusMax 12 hours agorootparent...of course it's pronounced \"Chye-lie\". I can't talk, though, I come from Ohio where we have a city named \"Versailles\" that's pronounced \"vur-SAYLES\". reply HKH2 2 hours agorootparentAmericans love French. It's their nitch. It probably comes from reading so much Nitchski. reply Fb24k 7 hours agorootparentprevIllinois also has a Ver-Say-Lees as well as a Kay-row (Cairo) reply sgerenser 7 hours agorootparentprevDon’t forget Charlotte, NY (pronounced Shar-LOT). reply mauvehaus 6 hours agorootparentOr Calais, VT is pronounced CAL-iss. reply adrianmonk 11 hours agoparentprevMaybe some TV ad jingle. I bet you could do just the rhythm of \"give me a BREAK, GIVE me a break, BREAK me off a piece of that\" and get three even knocks back as a response. reply sgerenser 7 hours agorootparentFootball cream? reply samatman 12 hours agoparentprevFor the US, \"where does the shortstop play\" is a pretty good one. Some people might get through K-12 without learning that, but surely not many. reply humanistbot 11 hours agorootparentI'm an American and the best I could answer for that is \"on the baseball field.\" I know it is a baseball position. Many other countries play baseball too. (Looked it up, oh I remember now, but wouldn't have been able to answer on demand.) Wikipedia tells me \"baseball is considered the most popular sport in parts of Central and South America, the Caribbean, and East Asia, particularly in Japan, South Korea, and Taiwan. \" I don't think there is such a shibboleth with both good precision and recall. American culture is exported around the world, but also is no longer a monoculture with only 3 broadcast networks. Any shibboleth that is ubiquitous enough in the US will be exported globally. Any part of US culture that is not global probably isn't as universal in the US. Even basic US civics (which would be known by more educated people globally) is far from universal: Only 77% of Americans can link the first amendment to freedom of speech and only 83% can name even one of the three branches of the federal government. And that's of the population of Americans who agree to take a university run political knowledge survey (https://www.annenbergpublicpolicycenter.org/political-commun...) reply spiesd 10 hours agorootparentAgreed on the general diffusion of knowledge; I think it's probably a step in the right direction for various cultures to have some basic understanding of those with which they are not personally familiar. We Americans shouldn't count on the shortstop being a secret to the world, nor should we be willfully ignorant of things that are popularly known elsewhere. Shibboleths, like all language, evolve. Some die off, become ineffective or unuseful. Others spring up. Does tiktok live here, now? I fear that I would fail a modern test, but I try to keep up. reply WalterBright 9 hours agorootparentprevDuring the Battle of the Bulge, the Germans infiltrated the American lines with ersatz officers sowing confusion by giving conflicting orders in perfect English. The GIs hit on the idea of asking them baseball questions. Failing to answer them correctly got the fake officers shot on the spot. My dad (WW2 vet) told me I would have been shot on the spot. My utter disinterest in baseball distressed him. reply StevePerkins 11 hours agorootparentprevI don't give a damn! https://en.wikipedia.org/wiki/Who%27s_on_First%3F#Sketch reply ddingus 10 hours agorootparentWell, that is who plays short stop. The position is a different matter. As if! (the shortstop cares) Great bit, BTW. I loved it as a kid. reply havblue 19 hours agoparentprevI heard it in a jpop song, happy summer wedding by morning musume, about 20 years ago. They actually worked it into the melody of the song. I think they're well aware of it now in the East. reply dan_mctree 12 hours agoparentprevThe Pledge of Allegiance? reply pictureofabear 12 hours agoparentprevNice try ISIS. reply tdeck 13 hours agoparentprevMaybe 800-588-2-300? reply bee_rider 12 hours agorootparentI don’t know that one. Safari seems to have rendered it as a phone number, though. Which makes me wonder, you could probably use as a challenge: 867-5… reply eigen 12 hours agorootparentit is a phone number, Empire Carpets https://www.youtube.com/watch?v=uwJQQux0TF0 reply doomrobo 12 hours agorootparentprevhttps://www.youtube.com/watch?v=uwJQQux0TF0 reply chasd00 19 hours agoparentprevive also heard using the question \"who was mickey's old girlfriend?\" referring to Mickey and Minnie Mouse. reply elihu 8 hours agoprevI've been playing Beyond All Reason (an open-source RTS) lately, and noticed that if I want to build a handful of some unit, I'd unconsciously click five times in a \"shave and a haircut\" pattern. reply esafak 12 hours agoprevPrincipal Ed Rooney gives the finger and two bits in Ferris Bueller's Day Off: https://www.youtube.com/watch?v=Li_jp3mYXLA reply jh3 20 hours agoprevWow, so \"two bits\" is what Roger says in response to Judge Doom in Who Framed Roger Rabbit. I can hear it now, but I never actually knew what was said. reply arp242 20 hours agoparentI never understood that either. It only took me about 30 years to get the joke. reply tanseydavid 17 hours agorootparentThis is the funniest scene in the entire movie IMHO. Roger's compulsive, uncontrollable need to respond with \"Two Bits\" kills me every time. reply MBCook 11 hours agorootparentRight. As a toon Roger can’t not respond. It’s how Doom forces him to reveal himself. reply willcipriano 13 hours agorootparentprevI've seen dozens of memes using the same gag but with a popular song or catch phrase. reply weinzierl 20 hours agoprevRelated: \"na-na na-na na-na\" https://en.m.wiktionary.org/wiki/na-na_na-na_na-na I always wondered if anyone else noticed, how the Jackson 5 smuggled it into ABC. https://youtu.be/ho7796-au8U?t=01m24s reply aeortiz 19 hours agoparentHave you heard it in Queen's \"We are the champions\"? \"No time for losers, for we are the Champions\" ... etc reply adrianmonk 11 hours agorootparentI'm glad I'm not the only one. I noticed that and asked a friend, and they didn't see it. To me, when you consider both how it sounds and the theme of the song, it has to be intentional. reply bee_rider 12 hours agorootparentprevQueen has to be one of the winners in terms of shortness of the message required, you can probably get by with just a STOMP-STOMP reply Biganon 11 hours agoparentprevIn French it's \"na-na na-na nère\" reply pimlottc 10 hours agoparentprev“Batman!” reply harimau777 19 hours agoprevWhen I was in high school jazz band, there was a similar lick that came at the end of a lot of songs \"what makes your big head so hard\". Presumably it originally came from https://en.wikipedia.org/wiki/Caldonia. However, I'm having trouble finding references to it's use as a riff/lick. So maybe it was something that was specific to our band? Edit: An example of this lick would be the end of the Beatle's \"I want to hold your hand\": https://youtu.be/v1HDt1tknTc?t=137 reply cka 7 hours agoprevThere's a common workflow I use at work that involves taking the default 10 times before inputting what I need to use. I do double shave and a haircut quickly instead of counting return presses. reply neilv 6 hours agoparentI have an unintended variation on that... Once a day, I have to press a button on my kitchen scale 4 times, to switch it from grams to fl.oz. It seems that the the fastest tempo at which one can press the button 4 times... matches the \"1-2-3-4\" at the start of the song people might know from \"JK Wedding Entrance Dance\". https://www.youtube.com/watch?v=4-94JhLEiN0 (I have a different association with the sequence 4-3-2-1, reminding me of the Peter Schilling song. https://www.youtube.com/watch?v=wO0A0XcWy88&t=41s ) reply rsaz 19 hours agoprevWow, so this is the where Nardwuar's \"Doot doola doot do...\" comes from! I always wondered how it was so ubiquitous that even people unfamiliar with his interviews knew how to respond. Very interesting. reply nomilk 6 hours agoparentFor Nardwuar fans (and those who've never heard of him), this talk is amazing: https://news.ycombinator.com/item?id=26678512 He is 100/100 for initiative. Surprisingly relevant to HN'er folks like ourselves. reply xxr 18 hours agoparentprevI’m interested in learning where you’re from—I’d assume outside of North America/Western Europe? I take familiarity with it so much for granted that it’s fascinating that someone’s familiarity with it comes from Narduwar. reply rsaz 12 hours agorootparentI am from North America actually, I’m curious where your association with it comes from. I just spend a lot of time on youtube/listening to hip hop so have heard Nardwuar do it many times. I’m vaguely familiar with it otherwise too, but wouldn’t be able to place it anywhere. reply MBCook 11 hours agorootparentI’m American. It was in the movie Roger Rabbit, which came out when I was 5 (40 now). And I know I knew it then. I assume I learned it from looney tunes or some other cartoon. But it was ubiquitous enough everyone knew it. To me it feels a little bit like asking how you know who Superman is. It’s just too engrained. reply kome 19 hours agoparentprevI was thinking the same! reply curiousfab 14 hours agoprevInterestingly Wikipedia says the Morse version of it is Morse code \"dah-di-di-dah-di, dah-dit\" ( –··–· –· ) IMHO more common is to send \"ESE\" (. ... .) - and the other station replies \"EE\" (. .) https://lcwo.net/ext/player?z=MjR%2BfjIwfn42MDB%2BfkVTRSBFRQ... \"ESE\" is a popular Amateur radio call-sign suffix among Morse enthusiasts for that reason. reply scelerat 12 hours agoprevI like the variation where the note for \"hair\" is played a half step flat. In the example on the wikipedia page, the implied chords for \"shave and a hair- cut\" would be (original) G / . C G shave and a hair cut (variation) G / . Eb7 D7 shave and a hair cut reply WalterBright 11 hours agoprevTwo bits meaning 25 cents came from an early practice of cutting a Spanish silver dollar into 8 pieces with four cuts. That's also where \"piece of eight\" as a reference to the Spanish dollar comes from. reply emi2k01 20 hours agoprevWow, this is very interesting. In Mexico, it's used to insult another person. I always thought it was a Mexico-thing. reply amelius 20 hours agoparentFor those wondering, this is explained in the section \"Uses in other countries\". reply higgins 11 hours agoparentprevThis always trips me up when I enthusiastically knock on my friends door to this tune. Oops reply thrtythreeforty 13 hours agoprevIt's used unironically/noncomedically as a final banjo lick in many bluegrass picking songs (the article does mention this). I heard it first there and I've always assumed it originated from American folk music. Interesting that it's quite a bit older/more general. reply huytersd 13 hours agoparentSounds like it comes from a minstrel show so it’s definitely American folk. reply dustincoates 21 hours agoprev> \"Two bits\" is a term in the United States and Canada for 25 cents I don't know about Canada, but the \"is\" should be a \"was\" unless there are contexts I don't know about. It did lead me down a little deeper the Wikipedia rabbit hole, and apparently: > The New York Stock Exchange continued to list stock prices in $1⁄8 until June 24, 1997, at which time it started listing in $1⁄16. It did not fully implement decimal listing until January 29, 2001. That's crazy to imagine we're less than 25 years away from the decimalization of the NYSE. reply harimau777 19 hours agoparentFun fact: The quarter being worth two bits is part of the same \"split a dollar up into eights\" system that resulted in Spanish dollars being called a \"pieces of eight\". A Spanish dollar was worth eight Spanish reals. So a quarter of a dollar would be worth two Spanish reals; hence two bits. Presumably the terminology stuck around even after the switch to US dollars. reply krallja 21 hours agoparentprevEight bits in a (modern) byte, eight bits in a buck. I wonder if there was a conscious or subconscious correction by the team designing the IBM 360 to align with the existing “standard.” reply bschne 20 hours agorootparentseems hard to pin down exactly, but in this Computerphile interview the gist seems to be \"if you get to the point where you want a distinct code for upper- and lowercase characters, digits, and a few punctuation symbols, you land a little north of six bits, an odd number of bits would be annoying to implement in hardware, so let's go for eight.\" https://www.youtube.com/watch?v=vuScajG_FuI&t=184s reply bee_rider 12 hours agorootparentprevI wonder when a bit of memory crossed over a bit of money. Pretty early I guess. reply neom 20 hours agoparentprev2 bit is still used in Canada as an indicator of something less than good, otherwise unused. reply harimau777 19 hours agorootparentIn the US it would still be fairly common for someone to know that \"two bits\" can mean a quarter, but it's not used much in common conversation. I don't know if this is just me, but I particularly associate it with a purposely colloquial or \"old timey\" register of speech. In my head I can envision a carny with a Foghorn Leghorn accent selling me a ticket to the Ferris wheel, \"That'll be two bits, son\". reply kayodelycaon 12 hours agorootparentI'm nearly 40 and spent most of my life in the northern midwest. Never heard \"two bits\" before. reply pavel_lishin 20 hours agorootparentprevI've definitely heard expressions that go something like, \"Why you no-good, two-bit etc etc!\", and never connected that to this until now. reply TeMPOraL 19 hours agorootparentMe neither! I always tentatively assumed it's about computer bits, where two bits would be \"not much\", and that somehow it entered into normie vernacular. So, TIL that this is another case of society and culture in the 19th and 20th century US evolving primarily around the stock exchange :o. reply samatman 11 hours agorootparentI suspect that \"bit\" for binary digit and \"bit\" as in \"two bits\" is a deliberate convergence more than pure coincidence. Tukey coined the term right after the war, which is a cultural high water mark for the term two bits. It doesn't hurt that \"a bit\" also means the least of something one might reasonably consider, this is where the bit as a sliver of the Spanish Dollar comes from in the first place, eight pieces being about as small as it was feasible to divide the coin into. Hence two bits for the quarter. reply bregma 19 hours agorootparentprevTIL Yosemite Sam was the rootinest, tootinest computer geek this side of the Pecos. reply madcaptenor 19 hours agorootparentprevAnd of course bytes are divided into eight bits - but that's just a coincidence, because bits came first and byte sizes weren't standardized until the 1970s. reply dustincoates 19 hours agorootparentprevSame in the US, but I can't imagine anyone would be able to accurately pinpoint its etymology. reply neom 18 hours agorootparentSurprised to hear that, I would have guessed folks over 40 would be accustomed to \"2-bit wh*re\" - my dad used to scream that all the time about his personal banker \"god damn that man he's a f'ing 2 bit wh...\" - pretty sure my grandpa used it regularly also. Maybe my family are not the most refined of people. ;) reply bitwize 12 hours agorootparentBack in the 90s, the anti-Microsoft movement had a slogan that went something like: \"Windows: a 32-bit layer on top of a 16-bit operating system originally for an 8-bit CPU derived from a 4-bit microcontroller, by a 2-bit company that can't stand 1 bit of competition.\" reply beezle 12 hours agorootparentprevLikewise, high school football cheerleaders '2 bits, 4 bits, 6 bits a dollar all for XXXXX stand up and holler\" reply jdminhbg 6 hours agorootparentI was familiar with this as a high schooler but it never occurred to me that 2 bits was 1/4 of a dollar. It was like a nursery rhyme where you don't really think about what the words actually mean. reply al_borland 20 hours agorootparentprevI grew up on the US hearing this usage, but it was primarily in Looney Tunes. reply mc32 20 hours agoparentprevIn other tunes it was bay rum instead of two bits. reply hprotagonist 21 hours agoprevregarding “bits” as a monetary term: it took me a solid decade to get the joke in “Making Money” regarding why Reacher Gilt taught his parrot to say “12.5%” reply messe 20 hours agoparentFor those who don't get it, it's a reference to Robert Louis Stevenson's Treasure Island which features a parrot that repeats \"Pieces of Eight\" (also known as a bit). reply 082349872349872 13 hours agorootparentIf your bird repeats \"Pieces of Seven\" you should check its settings; it might have a parroty error. reply baruz 7 hours agorootparentslow clap reply MrJohz 10 hours agoparentprevI was thinking of a different Pratchett joke: in Making Money, Moist is told to use the barber's knock: \"shave and a haircut, no legs!\" I'm rereading the Moist series again, and it is so incredibly thick with humour and references. I mean, all of the books are, but that series really feels like him at the top of his game. reply hprotagonist 6 hours agorootparent\"the smoking GNU\" reply bombcar 20 hours agoparentprevhttps://wiki.lspace.org/Main_Page Is a great resource for this; so many of the books have deep or obscure British jokes that even Native English (simplified) ((American)) don’t get. reply Sharlin 9 hours agoprevClapping the rhythm of the \"challenge\" part can be very effective in getting the attention of a group of distracted people that you want to address. It’s so well known that almost anyone instinctually claps the response when they hear the challenge. As a sharp percussive sound, clapping is also readily audible over chatter. reply tomcam 7 hours agoprevThe famous Roger Rabbit moment cued up: https://www.youtube.com/embed/6ds6w7SkHyw?&start=54&end=62 reply remarkEon 19 hours agoprevIn the survival/extraction/horror game Escape From Tarkov, this rhythm is used to signal \"friendly\" when there's no other means of communication. reply plasticsoprano 18 hours agoprevThe Blue Devils Drum and Bugle Corps ended many of their segments in the 80s with \"Watch out for that tree\" rhythm from the George of the Jungle theme. Seen here[1] around 10:20 to 10:22. https://www.youtube.com/watch?v=jlYOBu9CzNI&t=618s reply _sys49152 5 hours agoprevim chronically horrific when it comes to interpreting lyrics correctly. ever since ive been small up to present day. for example rage against the machine 'wearing your badge and your trophy wife' always thought this was 'shave and a haircut, to be.' my solution has been to just give up on words in songs altogether. reply schneems 20 hours agoprevIf you do the ending pattern 8 times it becomes “shave and a haircut, two bytes” reply ks2048 7 hours agoprevThis has to be near the top of the list of things people “know of”, but don’t have a word/phrase for. reply nateburke 20 hours agoprevAre there other rhythms that can convey the same or more information in fewer knocks? reply mondobe 17 hours agoparentI would argue that three knocks at regular (quick) intervals is the minimum to convey \"there's someone at the door\" rather than \"something fell down in the other room\". reply ipsum2 20 hours agoprevThere's a tune that's very similar to \"Shave and a Haircut\", but in a minor key, replacing the two eights with a triplet and a chromatic part. (Notes are: G D Db D Eb D; F# G) Does anyone know what its called? reply HKH2 1 hour agoparentIn Australia, that's when the song ends with 'brown bread' (dead). I assume it's British. reply amelius 20 hours agoparentprevIf you scroll down to the section \"Uses in other countries\", there's the same tune that seems to be in minor. reply eikaramba 19 hours agoprevi have a little IOT device[0] which has a starting animation with LEDS when you reset it. i used that tune for the LED animation (without sound) just the timings. funny enough i never knew where it came from just that it was in my head and always associated with looney tunes. Finally i know its origin :) [0] https://notific.at reply amelius 11 hours agoprevHow many catchy tunes would fit that exact pattern? reply unethical_ban 12 hours agoprevWhen I knock on a door, I either use this tune, or the first 7 notes of Super Mario Bros. Weird to know some US peeps don't know it! reply ddingus 10 hours agoprevQUE: ROGER RABBIT STAGE RIGHT [SCREAMS] TWO BITS! reply User23 11 hours agoprevIt's rather striking that a dollar today is worth considerably less than a penny when this was written, at least with respect to grooming services. reply teknico 5 hours agoparentIt’s even more striking once you look into the history of money, and realize what’s going on. End the Fed. reply probably_satan 19 hours agoprevI always thought it was \"match in a gas-tank...to bits!\" reply smugma 16 hours agoprev… two bits! reply chasing 8 hours agoprevSee also: “The stars at night are big and bright…” reply johnea 15 hours agoprev [2 more] [flagged] Sohcahtoa82 13 hours agoparent [–] ...what? Are you okay? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"Shave and a Haircut\" is a famous musical call-and-response couplet used for comedic purposes in various entertainment forms.",
      "The tune is recognized globally, appearing in songs, TV shows, movies, and even serving as an identity verification method for POWs in the Vietnam War.",
      "This melody holds cultural significance across countries, referenced in literature, music, bluegrass genre, works of artists such as Frank Sinatra, and even in shows like Animaniacs."
    ],
    "commentSummary": [
      "The use of the \"Shave and a Haircut\" knock as a covert authentication method is explored, along with its historical importance in verifying origins through shibboleths.",
      "The discussion covers the origins, cultural significance, and references in popular culture of the associated tune, as well as the term \"two bits\" in computing and currency contexts.",
      "The effectiveness of the \"Shave and a Haircut\" rhythm in conveying messages in music, gaming, and communication is highlighted, with mentions of similar catchy tunes and rhythms following this pattern."
    ],
    "points": 193,
    "commentCount": 130,
    "retryCount": 0,
    "time": 1709209869
  },
  {
    "id": 39550124,
    "title": "High-Performance Lock-Free Ring Buffer for Cross-Thread Communication",
    "originLink": "https://ferrous-systems.com/blog/lock-free-ring-buffer/",
    "originBody": "1.0Circular buffers 1.0.1An idealised infinite buffer 1.0.2A bounded circular buffer 1.0.3Contiguous writes/reads 1.1A Hardware Interlude 1.1.1DMA - Direct Memory Access 1.1.2Stackless Operation 1.1.3But why is DMA so important? 1.1.4From embedded to datacenters 1.1.5A fork in the road 1.2Concurrency design 1.2.1High watermark for data 1.3Implementation 1.3.1Writing 1.3.2Reading 1.3.3A note on memory ordering 1.3.4Support for embedded systems Do you need help with coding? Get in touch Get updates This is the story of how Andrea Lattuada (PhD student at ETH Zurich) and James Munns (from Ferrous Systems) designed and implemented (two versions!) of an high-perf lock-free ring-buffer for cross-thread communication. If any of those words look scary to you, don't fret, we'll explain everything from the basics. This post is cross-posted on Andrea Lattuada's blog. This post is for you if you're interested in (safe!) concurrency, systems programming, and cool ways to write efficient systems software. If you've never written a thread-safe data structure, this post may be a great way to get started! 1.0Circular buffers A BipBuffer is a bi-partite circular buffer that always supports writing a contiguous chunk of data, instead of potentially splitting a write in two chunks when it straddles the buffer's boundaries. Circular buffers are a common primitive for asynchronous (inter- or intra- thread) communication. Let's start with a very abstract, idealised view of the circular buffer interface, and then consider real-world constraints one by one, till we get to the BipBuffer design. 1.0.1An idealised infinite buffer A writer (producer) and a reader (consumer) want to communicate, and have access to the same, contiguous, and infinite array. They both keep a bookmark of which part of the array they've (respectively) written and read. They start with these write and read pointers aligned. When the writer wants to send data, it appends it after the write pointer and then moves the pointer to the end of the newly written chunk. The reader inspects the write pointer at its leisure (asynchronously). When the write pointer has advanced further than the read pointer, the reader can consume and act on the available data. Once that's done, it moves the read pointer forwards to keep track of which part of the buffer it has already processed. The reader will never attempt to read past the write pointer, because there's no guarantee there's valid data there (i.e. that the writer has put anything there). This also means that the read pointer can never overtake write. For now, we're assuming an ideal memory system that's always coherent and where writes are visible immediately and sequentially. 1.0.2A bounded circular buffer Computers don't have magic infinite buffers. We have to allocate a finite amount of memory to use for potentially infinite communication between the writer and reader. In a circular buffer, the write pointer can wrap around the boundaries of the buffer when it reaches the end. When new data arrives and the write pointer is close to the end, it splits the write in two chunks: one for the remaining buffer space at the end, and one for the remaining data at the beginning. Note that, if the read pointer is still close to the beginning, this has the potential of clobbering data that hasn't yet been processed by the reader. For this reason, the write pointer is not allowed to overtake read after it has wrapped around. We end up with two possible memory configurations: write leads and read follows (write ≥ read), the valid data (written, but not yet processed by the reader) is in the section of the buffer after read and before write; read leads and write follows (read > write), the valid data is after read, till the end, and from the start of the buffer till write. Note that we disallow read == write in the second case, as this would be ambiguous: while read can catch up to write, after a wraparound write has to stay one step behind read to indicate that we're in case 2 instead of case 1. We repeatedly move from configuration 1 to 2, then back to 1: when read reaches the end of the buffer, it can also wrap around to continue reading at the start. 1.0.3Contiguous writes/reads This is all great, but what if we have chunks of data that should remain contiguous in memory when written to the buffer? Look here, there's a new message to be written, but it doesn't fit in the remaining buffer space after write. If, for whatever reason, we aren't allowed to split this write in two, we're stuck. Maybe we can just wait for read to move forwards, and place our new data in a single chunk at the start of the buffer? Well, in fact, yea. But there's a caveat. We've broken the property in configuration 2 earlier: there's a section of the buffer that's between read and the end of the buffer, but doesn't contain any valid data. If we didn't do anything about it, the reader would keep consuming data, moving read forwards, and it would be oblivious to the fact that at some point it would be reading a section of the buffer that doesn't contain any valid information. 1.1A Hardware Interlude Previously we asked: What if we have chunks of data that should remain contiguous in memory when written to the buffer? But when would we actually require that data be read or written to in a contiguous manner? 1.1.1DMA - Direct Memory Access In embedded microcontroller systems, it is common to have a single core CPU. Instead of having multiple cores, they have a set of features referred to as Memory Mapped Peripherals. These Peripherals act as hardware accelerators for specific behaviors, such as sending or receiving data from a serial port. In order to minimize the amount of time necessary for the CPU to manually copy data from one place to another, these Peripherals can be configured to perform an action completely autonomously, streaming data to or from a section of memory on the CPU. This action of the hardware directly reading from or writing to the memory is called DMA, or Direct Memory Access. Instead of reading or writing one byte at a time to the Serial Port, the CPU can instead start the transfer, and when it is complete, process a chunk of bytes at a time. This allows for less time waiting, and is generally a more efficient method of processing data. A typical usage of DMA (called a DMA transaction) looks like this: The CPU allocates N bytes of memory to be used for DMA The CPU instructs the peripheral, such as a serial port, to receive N bytes of data, and to place those bytes in the memory allocated in step 1 Once the peripheral is configured, the CPU resumes performing other actions, and the Serial Port begins filling data into the memory buffer as it is received When the Serial Port has received all N bytes requested, it notifies the CPU, and stops receiving data The CPU may now process all N bytes requested, and if necessary, repeat the process at step one Although we often only have one CPU core in most microcontrollers, we can think of these DMA actors as their own thread. They are able to operate independently of the main CPU's actions, and read and write memory based on their own needs. In these microcontroller systems, there can be tens or hundreds of these hardware actors, all operating in parallel! 1.1.2Stackless Operation In step one of DMA procedure above, we talked about allocating N bytes of memory. On a non-embedded system, this would generally be done by allocating space on the heap - a Box in Rust, or using malloc() in C. In lightweight or timing critical embedded systems, it is uncommon to have a heap. Instead, all memory must be statically allocated, or allocated through the use of the stack. In these systems, data structures such as Circular Buffers are used to work around these limitations. A fixed amount of space is reserved for use, and a dynamic amount of data within a fixed maximum region is used to simulate a dynamic memory region. Unfortunately, these DMA transactions do not understand the concept of a circular buffer. They are only aware of a pointer to where the memory region starts, and how many bytes to use from the starting pointer. This means that a normal circular buffer where the data region could wrap around would not work for DMA transfers. 1.1.3But why is DMA so important? For operations used with DMA, the speed at which bytes are transferred is often many orders of magnitude slower than the operation of the CPU itself. For a 32 bit ARM CPU, copying 4 bytes from RAM takes a single cycle. In a 64MHz CPU, this means it will take 15.6 nanoseconds to copy these four bytes. A typical serial port configuration is \"115200 8N1\", which means 115,200 baud (or raw bits on the wire per second), with no parity, and 1 stop bit. This means that for every data byte sent, there will be 8 data bits, 1 unused parity bit, and 1 stop bit, to signal the end of the byte, sent over the wire. This means that we will need 40 bits on the wire to receive a 4 data bytes. At 115,200 bits on the wire per second, this means it will take 347,220 nanoseconds to receive the same four bytes, taking 22,222 times as long as it takes our CPU to copy the same amount of data! Instead of making our CPU waste all of this time waiting around, we allow the hardware to manage the simple sending and receiving process, allowing our CPU to either process other important tasks, or go into sleep mode, saving power or battery life. 1.1.4From embedded to datacenters People writing high-performance application for datacenter grade servers have long realised this is also true for the high-grade, power-hungry CPUs they use. Modern, efficient network stacks for servers use similar DMA techniques to offload all of this work to the network card, so that valuable CPU time can be spent running data-crunching applications. 1.1.5A fork in the road Here's where the original BipBuffer design decides to maintain two \"regions\" of valid data, one at the start and one at the end of the buffer: this way it can keep track of which sections of the buffers contain valid data. Have a look at the BipBuffer blog post on CodeProject for details on how this works. The design based on two regions works great in a single threaded environment, but requires swapping the references to two regions when the rightmost one is depleted. This is tricky to do without explicit locking (mutexes) for cases in which the writer and reader reside on different threads. Our use case is communication between two concurrent threads of control: either two actual OS threads, or a main thread of control and an interrupt handler in embedded or a device driver. This is where our design takes inspiration from the BipBuffer, but goes in a different direction. 1.2Concurrency design A common strategy to reduce the amount of coordination that needs to happen between the two threads (writer, reader) is to associate each coordination variable (pointer) with a single thread that has exclusive write access to it. This also happens to simplify reasoning about the design, because it's always clear who's in charge of changing which variable. So, let's start with a simple circular buffer that has the write and read pointers from before. The writer is the only one who ever changes write, and the reader is the only one who increments read. So far so good. Each thread is only concerned with writing to one variable, and reading from the other. 1.2.1High watermark for data Now let's re-introduce the requirement that the data written may need to be contiguous. If there's no space available at the end of the buffer, the writer wraps around and writes the whole contiguous chunk at the start. As we've seen, we need a way to tell the reader which part of the buffer is valid, and which was skipped to be able to write a single contiguous chunk. We're tracking the high watermark of valid data in the buffer, so what about a watermark pointer that gets written when the writer wraps around and leaves empty space at the end? Going back to our idealised infinite buffer from before, here's what things would look like. Whenever the valid region isn't split in two parts (at the beginning and end of the actual buffer) we simply ned to track the write and read pointers, as before. On the other hand, when valid data wraps around the buffer, we leave an artificial \"hole\" in the \"infinite buffer\" representation. The watermark lets us keep track of where the \"hole\" starts, and the end of the physical buffers marks the end. 1.3Implementation We have all the necessary elements for our non-blocking implementation. We start with the write and read pointers aligned at the start of the buffer and the watermark aligned with the end. struct ContiguousAsyncBuffer { buf: *mut u8, len: usize, read: AtomicUsize, write: AtomicUsize, watermark: AtomicUsize, } We use AtomicUsize to let the two threads read and update the pointers concurrently and safely. The writer/sender thread is in charge of write and watermark, the reader/receiver is in charge of read. This is important! Contended writes from multiple threads on the same memory location are a lot harder for the CPU's cache coherence protocol to handle, and will cost latency and throughput. What's more, it's a lot easier to reason about correctness of these concurrent protocols if each of the shared pointers are always written by a certain thread (their \"owner\"). 1.3.1Writing As long as there's enough contiguous buffer space before the end of the physical buffer, as new data arrives (of length write_len) the sender thread moves the write pointer forwards to signal that a new chunk of the buffer is now valid and can be read. // [writer thread] buffer.write.store(buffer.write.load() + write_len) When new data arrives and the write pointer is close to the end, it moves the watermark to its current location, then wraps around. Again, if the read pointer is still close to the beginning, this has the potential of clobbering data that hasn't yet been processed by the reader. For this reason, the write pointer is not allowed to overtake read after it has wrapped around. // [writer thread] if buffer.len.saturating_sub(buffer.write.load()) >= write_len { // not shown: check `read` to make sure there's enough free room buffer.watermark.store(buffer.write.load() + write_len); buffer.write.store(buffer.write.load() + write_len); } else { // not enough space, wrap around // not shown: check `read` to make sure there's enough free room at the beginning of the buffer buffer.watermark.store(buffer.write.load()); buffer.write.store(0 + write_len); } You may have noticed that the writer also pushes the watermark forward when there's room at the end of the buffer. We need to do this because we may have moved it back on a previous wrap-around and we want to avoid the reader now misinterpreting it as a sign that there's a \"hole\" at the end. 1.3.2Reading We end up again with two possible memory configurations: write leads and read follows (write ≥ read), the valid data (written, but not yet processed by the reader) is in the section of the buffer after read and before write; read leads and write follows (read > write), the valid data is after read, till the watermark, and from the start of the buffer till write. This makes the reader thread's logic simple: read till you hit the write pointer, or the watermark, and update the read pointer accordingly. 1.3.3A note on memory ordering Some of you may have noticed that all of our calls to load don't take arguments and our calls to store take a single argument, the new value for the AtomicBool. This isn't valid code, of course. The real signatures take another argument: ordering: Ordering. This instructs llvm on how to emit the proper memory fences and sync instructions to drive the cache coherence and synchronization mechanisms built into the CPUs. The safe thing to do here is to always choose Ordering::SeqCst, \"sequential consistency\", which provides the strongest guarantees. On x86, due to the hardware design, anything other than Ordering::Relaxed is equivalent to SeqCst. On ARMv7/v8, things get more complicated. We recommend reading up on Ordering both in the rust doc and in the documentation for your platform. For the purpose of this post, just assume we used Ordering::SeqCst everywhere. This is often good enough in practice, and switching to a weaker Ordering is only necessary to squeeze out the last bit of performance. In Andrea's implementation of the lock-free ring-buffer, spsc-bip-buffer, some of the orderings are relaxed for performance. This has the downside that it can introduce subtle concurrency bugs that may only show up on some platform (ARM, for example): to be a bit more confident that everything's still fine, Andrea's has continous integation tests both on x86 and ARM. 1.3.4Support for embedded systems In James' implementation of the lock-free ring-buffer, bbqueue, convenience interfaces are provided for statically allocating instances of the ring-buffer. The queue can be split into Producer and Consumer halves, allowing for use of one half in interrupt context, and the other half in non-interrupt (or a different interrupt) context.",
    "commentLink": "https://news.ycombinator.com/item?id=39550124",
    "commentBody": "A lock-free ring-buffer with contiguous reservations (2019) (ferrous-systems.com)188 points by simonpure 19 hours agohidepastfavorite98 comments jamesmunns 18 hours agoOh hey, one of the authors of this post here (James), happy to answer any questions. This post has been discussed here a couple times, but AMA :) edit, the most commented version of this post was the original: https://news.ycombinator.com/item?id=20096946. This is what I'm up to these days: https://onevariable.com/ reply cangeroo 1 hour agoparentELI5? I see a lot of critique in the previous (2019) thread, but no summary in key points. What are the reasons that this is hard? In multiprocessor systems, are memory writes not guaranteed to be sequential? e.g. can data be written out-of-order, after the synchronization bit is written? Or is it more about the use case, that it is optimized for minimal number of instructions, e.g. avoiding additional memory reads? (e.g. by writing data to the buffer, instead writing storing pointers)? Or is it that you're trying to use constant memory (irrespective of number of threads)? Because to me, it seems like a trivial problem to solve, if you have sequential writes, store data separately from the queue, and may linearly scale memory with number of threads. reply Certhas 42 minutes agorootparentSeems to me that the main critique was from someone who didn't understand the specified problem, and complained that this didn't solve a more general one. (... and probably wasn't aware that the assumptions made in the specification can be encoded in the API signature in Rust. This wouldn't be possible in C, which is why C forces you to solve the harder problem or rely on your users to not accidentally misuse the data structure.) reply el_pollo_diablo 52 minutes agoparentprevThere is this snippet of code in the article: if buffer.len.saturating_sub(buffer.write.load()) >= write_len { // not shown: check `read` to make sure there's enough free room buffer.watermark.store(buffer.write.load() + write_len); buffer.write.store(buffer.write.load() + write_len); } I would have to look at the implementation to know for sure, but that part looks incorrect to me. Suppose that the writer has placed a watermark strictly before the end of the buffer, wrapped around, and is about to write a new message; meanwhile, the reader has not reached the watermark yet (and therefore, has not wrapped around), but it has made enough progress to leave room for the writer's new message. In that case, we have write >\"; a MCMP, append-only Vec). It's quite easy to do because it's bounded. However, I could not find a way to make it both unbounded and efficient. Any ideas ? reply duped 4 hours agorootparentIt's very hard to beat locking when the queue needs grow. It's the performance statistics - you're going to grow more often as the app warms up, reach a steady state, and only need to grow under heavier than expected load. And in that case you probably aren't going to see performance dominated by time spent waiting to acquire a write lock. The alternative is to dive into the literature on lock-free mpmc queues, which are kind of gnarly to implement. A lot of the literature handwaves ABA problems with stuff like \"use hazard pointers and RCU\" without correct pseudo code to help you. That's why locking in unbounded queues is popular, imo. It's not actually inefficient, and the alternatives are arcane enough to avoid. No one is going to understand or trust the code anyway. It's worth mentioning that \"lock free\" means \"one task does not prevent other tasks from making progress\" and in the case of a bounded queue, you can trivially accomplish that by busy-waiting when the queue is full. This isn't appropriate if you need consumers to receive events in the order in which they happen, but you can kind of fix that using an atomic counter (to paraphrase Mike Acton, if you have a safe counter, you have a safe queue) and time stamp events/sort on which ones are consumed. reply jayshua 7 hours agorootparentprevNot sure if it could be extended here, but I've seen a lock free hash map that supported lock free reallocation by allocating new space and moving each entry one by one, either when the entry is accessed or in a separate thread concurrently. Accessing an entry during the reallocation would check the new region first, and if not found check the old region. Entries in the old region would be marked as moved and once all entries were moved the old allocation could be freed. reply paholg 14 hours agorootparentprevI'm no expert here, but I wonder if a linked list of bounded logs would work well. So, everyone has a pointer to the current one, and there's an atomic pointer to the next. When the log fills up, you allocate a new one and set the next pointer to it using compare_and_swap. If someone beat you to it, you can walk the list and add yours to the end so that the allocation isn't wasted. This way, most of the time you're using the efficient bounded log. reply jamesmunns 16 hours agorootparentprevI am not sure! Most of the data structures I design are for embedded systems without allocators. On the desktop, I mostly defer to others. I've used tokio's broadcast channel quite a bit before, but it is also bounded. After talking to Eliza from the tokio project, I'm fairly convinced that unbounded queues are a scary thing to have around, operationally :). But again - this is a bit out of my actual expertise! reply SergeAx 5 hours agoparentprevPlease excuse me if my question is dumb, I have little understanding of Rust. You declaring your implementation \"lock-free\", but are using Atomic* Rust primitives at the same time. Those are using CPU atomic instructions, if I googled correctly. Those, in their turn, are using CPU locking mechanism. Turns out that you just shifted locking from language to CPU, right? reply DannyBee 4 hours agorootparentIt's a reasonable question, because lock-free is a confusing term. Lock-free here does not mean \"without any form of synchronization primitive\". That is impossible. Some synchronization must occur. Instead, it is a term of art that means: 1. Thread failure can't cause other threads to get blocked. 2. Forward progress in the algorithm is guaranteed. Basically: threads can't block each other and death of threads does not screw things up. https://en.wikipedia.org/wiki/Non-blocking_algorithm CPU atomics often work by blocking (they have to) but they are non-interruptible instructions and there is a fixed maximum execution time. (So you can still guarantee forward progress if you use them). reply mrcode007 5 hours agorootparentprevYes, this is correct and the most overlooked aspect and reason for the misnomer. Atomics imply locking at the cpu. Depending on the CPU the lock happens either for the entire memory bus (pre Intel P6 on x86) or as part of snoop disable bits on relevant cache lines in the snooping protocol reply DannyBee 4 hours agorootparentIt's not a misnomer, it's just a bad term of art. When it was coined folks were well aware that atomics locked at the CPU level. reply mrcode007 4 hours agorootparent“In discussing the question, he used to liken the case to that of the boy who, when asked how many legs his calf would have if he called its tail a leg, replied, ” Five,” to which the prompt response was made that calling the tail a leg would not make it a leg.” reply loeg 17 hours agoprev> The safe thing to do here is to always choose Ordering::SeqCst, \"sequential consistency\", which provides the strongest guarantees. ... This is often good enough in practice, and switching to a weaker Ordering is only necessary to squeeze out the last bit of performance. If you're going to write lock-free algorithms using atomics, the least you can do is learn about ordering semantics and use the correct abstract semantics for your design's actual needs. It is much easier to do it at design time than to try to figure out if it is safe to relax SeqCst later. (This is one of the major flaws of C++ std::atomic's default SeqCst semantics.) If you aren't going to bother understanding ordering semantics, it is unlikely you can write a safe lock-free algorithm anyway. (It's really hard to do!) reply jamesmunns 16 hours agoparentBack then, there weren't as good references for explaining atomic ordering, and the blog post had gotten long enough. Mentioning SeqCst was a bit of a cop out, though both Andrea and I didn't end up using SeqCst past the inital impl anyway. Today I would have just linked to https://marabos.nl/atomics/, Mara does a much better job of explaining atomics than I could have then or now. reply hinkley 15 hours agorootparentBack then? Do you mean 2019? Or a different “then”? Because there was plenty of material in CS about this subject even in 2010. Java was wrestling with this twenty years ago, and databases long before that. reply anonymous-panda 14 hours agorootparentI think the hard part of it is that x86 only has one atomic ordering and none of the other modes do anything. As such, it’s really hard to build intuition about it unless you spend a lot of time writing such code on ARM which wasn’t that common in the industry and today most people use higher level abstractions. By databases, do you mean those running on DEC Alphas? Cause that was a niche system that few would have had experience with. If you meant to compare in terms if consistency semantically, sure but there’s meaningful differences between database consistency semantics of concurrent transactions and atomic ordering in a multithreaded concept. Java’s memory model “wrestling” was about defining it formally in an era of multithreading and it’s largely sequentially consistent - no weakly consistent ordering allowed. The c++ memory model was definitely the first large scale adoption of weaker consistency models I’m aware of and was done so that ARM CPUs could be properly optimized for since this was c++11 when mobile CPUs were very much front of mind. Weak consistency remains really difficult to reason about and even harder to play around with if you primarily work with x86 and there’s very little tooling around to validate that can help you get confidence about whether your code is correct. Of course, you can follow common “patterns” (eg loads are always acquire and stores are release), but fully grokking correctness and being able to play with the model in interesting ways is no small task no matter how many learning resources are out there. reply gpderetta 14 hours agorootparentNit: x86 has acquire/release and seq_cst for load/stores (it technically also has relaxed, but it is not useful to map it to c++11 relaxed). What x86 lacks is weaker ordering for RMW, but there are a lot of useful lock free algorithms that are implementable just or mostly with load and stores and it can be a significant win to use non-seq-cst stores for this on x86 reply vlovich123 6 hours agorootparentI would have to imagine you mean x86-64 right? I would imagine 32bit x86 doesn’t have those instructions? I’m also kind of curious if a lot of modern code compiled to x86 would see consistency issues running on old CPUs before TSO was formalized (like a p2 multiprocessor server). reply loeg 5 hours agorootparent32-bit x86 has many of the same instructions, including cmpxchg8b (in models dating to the 90s). reply haberman 14 hours agorootparentprevIndeed there is different code generated by seq_cst for stores. Though for loads it appears to be the same: https://godbolt.org/z/WbvEcM83q reply loeg 5 hours agorootparentRe: the godbolt example, note that release semantics are not meaningful for load operations. > If order is one of std::memory_order_release and std::memory_order_acq_rel, the behavior is undefined. https://en.cppreference.com/w/cpp/atomic/atomic/load reply gpderetta 13 hours agorootparentprevYes, seqcst loads map to plain loads on x86. reply foobiekr 13 hours agorootparentprevX86 might but devices connected to it in embedded world have had to be very very aware of this stuff since the 90s. reply vlovich123 6 hours agorootparentEmbedded devices did not necessarily use the c++ memory model, and definitely not in the 90s and were highly likely in order CPUs to boot with no crazy compilers and thus atomics didn’t matter too much anyway (volatile was sufficient). They had a weaker memory model maybe but at the same time multi threading on embedded did not really exist as it was only being introduced into the industry with any real seriousness around that time (threading on Linux started to shake out around the mid 90s). reply foobiekr 5 hours agorootparentSMP systems were widely in use in the 1990s, but you’re correct the dual core MIPS was 2003ish in emedded. reply jamesmunns 15 hours agorootparentprevI meant 2019, and there weren't any materials that I would consider as clear and well defined as Mara's linked docs explaining the different orderings used by C, C++, and Rust (Relaxed, Release, Acquire, AcqRel, and SeqCst). I'm very sure there were discussions and teaching materials then, but none (that I was aware of) focused on Rust, and something I'd link to someone who had never heard of atomic ordering before. reply jcelerier 16 hours agoparentprev> It is much easier to do it at design time Is it? I always worked the second way (starting from seq_cst and then when the core design matured enough and didn't change for a few months, trying to see what could actually be relaxed). I'd be very afraid that in the first case, you start with say relaxed semantics somewhere, them you change the design because the requirements changed, and now you have to go again through all the atomic operations to make sure the assumptions all still hold. reply jamesmunns 16 hours agorootparentBack when this post was written, I would have agreed with you. But Mara's book makes a good case for this: https://marabos.nl/atomics/memory-ordering.html#common-misco... More importantly, when reading code, SeqCst basically tells the reader: \"this operation depends on the total order of every single SeqCst operation in the program,\" which is an incredibly far-reaching claim. The same code would likely be easier to review and verify if it used weaker memory ordering instead, if possible... It is advisable to see SeqCst as a warning sign. reply gpderetta 16 hours agorootparentprevIf you change the design of a lock free algo you very likely have to go through all the atomic operations to make sure that all assumptions hold anyway. reply derefr 13 hours agoparentprev> If you aren't going to bother understanding ordering semantics, it is unlikely you can write a safe lock-free algorithm anyway. I think the implicit suggestion here is that the target audience for this abstraction is actually two separate developers: 1. A developer who doesn’t know much about locking semantics, but can write the simple-but-slow case correctly. 2. Another developer, much later on — probably a senior SRE type — who can revisit this code and optimize it with full understanding of the constraints. (This might also be the same person, years later, with more experience under their belt.) The benefit of the way this library is designed, is that the second developer doesn’t have to completely rewrite everything just to optimize it. The naive dev’s code has already been forced into an “almost but not quite” lock-free mold by the design of the library’s API surface. reply haberman 14 hours agoparentprevI've never actually seen a production lock-free algorithm that uses SeqCst. I have a hard time even imagining an algorithm where SeqCst is the right choice. It seems like SeqCst was chosen as a default to reduce the surprises and gotchas around lock-free programming. But lock-free programming is inherently tricky; if you're trying to avoid surprises and gotchas you should probably use a mutex. reply senderista 7 hours agorootparentIt is the right choice whenever you need linearizability. I can't believe you've never seen a (correct) production lock-free algorithm impl that used SeqCst. Many lock-free algorithms require SeqCst for correctness. Here's a trivial example: hazard pointers. Any thread publishing its hazard pointer must use a StoreLoad barrier (equivalent to SeqCst) to ensure any GC thread scanning the publication list sees its hazard pointer, before it deallocates pointers in the limbo list that didn't appear in the scan. MemSQL actually wrote a blog post on a nasty bug in their database arising from their use of AcqRel for this operation instead of SeqCst: https://www.singlestore.com/blog/common-pitfalls-in-writing-.... reply loeg 5 hours agorootparentThis blog post leaves a lot to the imagination. > Here’s the case that broke our stack: Thread 1, in preparation for a Pop operation, reads the head of the stack. Thread 1 writes the current head to its hazard pointer (using only release semantics, which are weaker than sequentially consistent semantics). Thread 1 reads the head of the stack again. Thread 2 removes head from the stack, and passes it to the garbage collector thread (using sequentially consistent memory semantics). The garbage collector scans the hazard pointers,\\ and (because the assignment was not done with sequentially consistent memory semantics) is not guaranteed to see thread 1’s hazard pointer pointing to the node. The garbage collector deletes the node Thread 1 dereferences the node, and segfaults. My interpretation is that with release semantics for the store, the 2nd read (load) in Thread 1 is actually allowed to be reordered before the release store to the hazard pointer. But they are not very explicit about it. > So if thread 2 removing the pointer happens first, thread 1 will see a different value on its second read and not attempt to dereference it. Thread 1 will see thread 2's remove even with release semantics for that store -- the store has a data dependency on the first load; they cannot be reordered. > If thread 1 writes to its hazard pointer first, the garbage collector is guaranteed to see that value and not delete the node. Yeah, this must be it. Thread 1 fails to notice the GC happened while it was writing its HP because its second load actually happened before the HP store. Folly's hazard pointer implementation uses a release store to update the hazard pointer (here: reset_protection()), but uses some sort of SeqCst barrier between the store and the 2nd load (with acquire semantics): https://github.com/facebook/folly/blob/main/folly/synchroniz... reply senderista 4 hours agorootparentYes, the store to the HP entry must happen-before both the second load of the global pointer in the publishing thread and any load of the HP entry in another (GC) thread. (The first load + store + second load emulates an atomic memory-to-memory copy of the global pointer to the HP entry.) reply gpderetta 2 hours agorootparentprevYou certainly need a #storeload to update the hazard pointer, but do you really need seq_cst? Is a total order of all updates really necessary? Wouldn't, say, an acq_rel exchange be sufficient? I need to read that article, it seems interesting. reply ajross 16 hours agoparentprevCompletely agree, though the more iconoclastic corrolary that goes unspoken there is that putting The Final Word on memory ordering semantics into programming language standards was a terrible mistake. Memory ordering is a hardware behavior. It needs to be specified at the hardware level, and hardware vendors have been very mixed on clarity. And more importantly, lockless algorithms (that rely on memory ordering control) are really, really hard, and demand clarity over all things. And instead we're crippling those poor programmers with nonsense like \"sequentially consistent\"[1] or trying to figure out what on earth \"consume\" means[2]. x86 does this pretty well with their comparatively simple model of serializing instructions. Traditional ARM ISAs did only a little worse by exposing the interface as read/write barrier instructions. Everyone else... meh. But if you really want to do this (and you probably don't) do the analysis yourself at the level of ISA/architecture/hardware, cite your references, and be prepared to handle whatever portability works is needed on your own. [1] A statement about desired final state, not hardware behavior! [2] Nothing, on any hardware you will ever use. Don't ask. reply gpderetta 16 hours agorootparentOn the contrary, fixing the memory model on a widely used language like C++ forced hardware vendors to get their act together and provide more rigorous memory model explanations. For example intel went from Processor Ordering to TSO, and arm started offering explicit acquire/release operations. Java had the opportunity as well, but by initially only providing a stronger, mostly sequentially consistent MO, the hardware vendors managed to get away a little longer. reply ajross 15 hours agorootparentI don't think that's the case with Intel at all, the events are off by a decade at least; do you have a blog or something to cite there? I'd be curious to read it. And as for ARM, \"explicit acquire/release\" is objectively less informative and harder to reason about than the xxxSB instructions were (and yes, I've used both). ARM went backwards to accommodate C++'s nonsense. Again, the language standard writers aren't remotely the experts here, the hardware designers are. That C++ invented its own metaphors instead of listening to the experts is a bug, not a feature. reply gpderetta 15 hours agorootparentThe hardware designers were involved on the standardization process. I don't have citations at hand, I think most of the mailing lists were the discussion re the c++ MO happened have been lost, but (as a lurker trying to learn this stuff) I was following the process closely. The question was, given PO, whether it was at all possible to recover sequential consistently on intel either with mfence or a lock xchg, given the possibility of IRIW. Intel then updated their MO to exclude IRIW, de facto standardizing on TSO. This was early 2000s. I think both ARM and IBM published revisions to their architecture clarifying details around the same time. This spawned a set of academic papers that proved the correctness of the agreed mapping of the C++ memory model to those architecture s. reply ajross 15 hours agorootparent> The hardware designers were involved on the standardization process. That sounds cyclic then. You're saying that Intel's SDM was ambiguous[1] (which it was) and that it was sorted out as part of a standardization process. I'm saying that it doesn't really matter what the SDM said, it mattered whether or not you could reliably write lockless code on x86 using the hardware and docs available at the time, and you could. And further, I'm saying that the standard ended up making things worse by perpetuating arguments like this about what some buggy English text in an SDM said and not about actual hardware behavior. [1] In ways that AFAICT didn't actually impact hardware. I found this, which is probably one of the papers you're citing. It's excellent work in standards-writing, but it's also careful to note that the IRIW cases were never observed on hardware. https://www.cl.cam.ac.uk/~pes20/weakmemory/x86tso-paper.tpho... reply gpderetta 13 hours agorootparentIt didn't impact hardware because intel hadn't taken advantage yet of the additional latitude offered by their original model. Then they closed the hole and they guaranteed no IRIW[1]. But in the meantime if your algorithm was susceptible to this reordering, there was no written guarantee that an mfence would fix it. But most importantly as the model was informal and not self consistent, there was no possibility to write formal proofs of correctness of an algorithm or run it against a model checker. [1] in practice this means no store-forwarding from sibling hyper thread store buffers, something that for example POWER allows and is observed in real hardware. reply bfrog 10 hours agorootparentprevC++ is a bug that can’t be fixed reply vacuity 10 hours agorootparentAlthough, like many bugs, it's also a feature. reply magnat 17 hours agoprev> A typical serial port configuration is \"115200 8N1\", which means 115,200 baud (or raw bits on the wire per second), with no parity, and 1 stop bit. This means that for every data byte sent, there will be 8 data bits, 1 unused parity bit, and 1 stop bit, to signal the end of the byte, sent over the wire. This means that we will need 40 bits on the wire to receive a 4 data bytes. 8N1 means there is 1 start bit, 8 data bits and 1 stop bit (10 bits total), not 8 data bits, 1 unused parity bit and 1 stop bit (also 10 bits total). reply jamesmunns 16 hours agoparentYep, good catch! That's a whoops in my explanation. I don't work at FS any more, so I'm not sure I could PR that change, but you're definitely right :) reply rdtsc 18 hours agoprevMy favorite ring buffer structure is like the one described in https://www.gnuradio.org/blog/2017-01-05-buffers/ > It asks the operating system to give it memory-mappable memory, and map twice, “back to back”. Blocks then get called with pointers to the “earliest” position of a workload within this memory region – guaranteeing that they can access all memory they were offered in a linear fashion, as if they’d actually be dealing with hardware ring buffers. It imposes limitations on hardware and OS support in a way, but I think it's pretty neat. This also used by the kernel BPF ring buffer: https://www.kernel.org/doc/html/latest/bpf/ringbuf.html > ...data area is mapped twice contiguously back-to-back in the virtual memory. This allows to not take any special measures for samples that have to wrap around at the end of the circular buffer data area, because the next page after the last data page would be first data page again, and thus the sample will still appear completely contiguous in virtual memory reply dist1ll 18 hours agoparentUnfortunately that means the chunks are only contiguous in virtual memory. So it won't work with the DMA use case mentioned in the article, which requires contiguous physical addresses. But it's still a nice trick, I like it when people get creative with HW features. reply jnwatson 17 hours agorootparentTheoretically, the same trick could be used to double map addresses coming from an external master via an IOMMU. reply speed_spread 16 hours agorootparentIt is unlikely that cheap microcontrollers where DMA is most helpful will have an IOMMU. reply gpderetta 16 hours agorootparentprevBut the hardware only needs to see on copy of the duplicated memory and you can let it deal with the wraparound. The software can use the convenience of the double mapping. reply gpderetta 16 hours agoparentprevAKA the Magic Ring Buffer. It extremely convenient not having to deal with split messages, especially if you support variables size payloads. reply signa11 3 hours agoparentprevah the vm-trick ! have used it current (and previous) places of work to great effect. reply scottlamb 16 hours agoparentprevThis is a cool trick, and iirc there are a few Rust crates that implement it, including slice-deque. ...but I think there are a few significant downsides, even in userspace: * The most obvious one: you have to write `unsafe`, non-portable code. The Linux, macOS, and Windows implementations are totally different from each other. * The setup and teardown of each ring is a bit expensive (few system calls). For a regular allocation, the malloc implementation typically caches that for you. Here you have to do your own pooling if you might be frequently creating them. * Using whole pages, and two mappings per ring, is wasteful in terms of not only RAM (often no big deal) but also TLB space (which often turns into significant CPU usage). If you just allocate 32 64 KiB rings from the standard allocator, on x86-64 you might be talking about a single 2 MiB huge page mapping. If you do this instead, you're talking about 1024 4 KiB page mappings. reply duped 13 hours agorootparentAny real, production ready ring buffer should be using unsafe. I would consider anything that doesn't to be a toy. In Rust it's basically impossible to do this without MaybeUninit. You could use Option, but then you're paying a massive cost for a very easy to write and audit chunk of unsafe code. reply scottlamb 9 hours agorootparentI don't think it's useful to consider \"uses `unsafe`\" as a boolean. A one-line `unsafe` around `MaybeUninit::assume_init` isn't the same as an `unsafe` module per platform wrapping VM operations. Also, it's not that crazy for a byte-oriented buffer to start with a `vec![0u8; N]` (cheap) and not need `MaybeUninit` at all. Probably doesn't buy you that much though; you still want to be careful to not leak previous bytes. Also, you might be missing the point of my comment if you're responding to one word of \"the most obvious [downside]\" and not the other bullets... reply duped 4 hours agorootparentIt's not an attack on the wording, but the correctness of your first bullet point. `unsafe` is appropriate for the initialization of a ring buffer in Rust. That's true for using `mmap` or anything in \"pure\" Rust using the allocator API to get the most idiomatic representation (which can't be done in safe or stable Rust). It's not one line. It's also not platform dependent, the code is the same on MacOS, Linux, and Windows the last I tried it. The rest of the bullet points are issues with scaling, which sure, are valid. But if your bottleneck is determined by the frequency at which channels get created or how many exist then I would call architecture into the question. A ringbuffer is a heavy hammer to synchronization problems. It's appropriate in many, but not many times in the same application, in my experience. This last month I've written a lock-free ring buffer to solve a problem and there's exactly one in an application that spawns millions of concurrent tasks. reply monocasa 18 hours agoprev> Contended writes from multiple threads on the same memory location are a lot harder for the CPU's cache coherence protocol to handle FWIW, those are the same location according to most cache coherency protocols, since cache coherency generally works on the cache line level. You'd want to split the two contexts to their own cache lines. reply stefanha 14 hours agoparentAnother cache optimization trick some ring-buffer implementations use is to keep a shadow copy of the read or write pointer to avoid frequently fetching the other context's cache line. The latest version of the read pointer is only needed when the writer catches up with their shadow copy and vice versa. reply anonymousDan 11 hours agorootparentYes this is absolutely crucial for performance. reply dnedic 11 hours agoprevBipartite buffers are amazing and criminally underused. For those looking for C and C++ implementations you can check out my libraries: lfbb and lockfee: https://github.com/DNedic/lfbb, https://github.com/DNedic/lockfree (although lockfree contains more data structures as well) reply nyanpasu64 11 hours agoprev>In Andrea's implementation of the lock-free ring-buffer, spsc-bip-buffer, some of the orderings are relaxed for performance. This has the downside that it can introduce subtle concurrency bugs that may only show up on some platform (ARM, for example): to be a bit more confident that everything's still fine, Andrea's has continous integation tests both on x86 and ARM. It might be worth testing/forking the library to test on Loom (https://github.com/tokio-rs/loom), which can model atomic orderings and check for concurrency errors to some degree (though I last used it years ago). TSAN might be able to check for ordering errors in visited execution traces (though I haven't tried using it in the past). reply samsquire 17 hours agoprevI tried to write a lock free ringbuffer with weak atomics, I haven't proved it right with TLA+ yet but I started writing a model in it. I use tagging to avoid the ABA problem. they're all on https://github.com/samsquire/assembly, i tried to write multiple disruptor with multiple consumers, then one with multiple producers then one with multiple consumers AND multiple producers, inspired by LMAX Disruptor. (There's files for each of them and table in the repo. it's not proven yet!) the contention on the same memory address (the read/write index) is the thing that seems difficult to address. One thing I've learned about thread safety: I think if you have thread-owned values then you can be thread safe with a simple semaphore, providing that you have unique, DISTINCT values for each thread. If you have two threads that have this in a hot loop in parallel: // thread 0 if buffer[x].available == 1: // do stuff buffer[x].available = 0 // thread 1 if buffer[x].available == 0: // do stuff buffer[x].available = 1 Due to causality, no matter the interleaving, thread 0 owns the buffer[x].available and body of the if statement when it is 1 and thread 1 owns the body of the if statement buffer[x].available when it is 0. The CMP is a cheap mutex with distinct valued memory locations. Even though thread 1 is writing to buffer[x].available and thread 0 is writing to buffer[x].available it doesn't matter because the causality is mutually exclusive. There is no interleaving of buffer[x].available = x because of the if statement. The buffer[x].available = 0 will never run while buffer[x].available is equal to 0 overwriting or causing a data race when setting buffer[x].available to 1. So the second line cannot happen in parallel. I need to write a TLA model to assert its safety. If you have more than 2 threads, then you need different tokens to provide admissability to the if statement. Remember to use compiler memory barrier asm volatile (\"\" ::: \"memory\"); so you don't need volatile struct values. reply gpderetta 16 hours agoparentThe problem here is the [1] // Do stuff [2] Buffer[X]. available = Y There is no explicit nor implicit ordering between 1 and 2, so the compiler or cpu can reorder them. You need a release barrier between the two. Also while most CPUs preserve the control dependency, not all do (famously Alpha), and certainly not compilers. You would need a consume barrier, except that c++11 consume is only for data dependencies and unimplemented anyway. Edit: with the correct barriers in place, you can prove correctness by similitude to two size 1 SPSC queues used to exchange a mutual exclusion token, with the added quirk that as the queues are never used at the same time, they can actually be physically colocated in memory. reply samsquire 15 hours agorootparentThank you for you and for sharing your knowledge gpderetta, appreciated, TIL. reply loeg 17 hours agoparentprev> The buffer[x].available = 0 will never run while buffer[x].available is equal to 0 overwriting or causing a data race when setting buffer[x].available to 1. In particular, because loads and stores of the same variable cannot be reordered out of program order. Once your algorithm involves other variables, you would (likely) need to be a little careful about loading/storing with acquire/release semantics to prevent reordering other accesses relative to this protocol. > Remember to use compiler memory barrier I would highly recommend using the language atomic types (and barriers if truly needed) instead of gcc inline assembly syntax. reply samsquire 17 hours agorootparentThanks for your reply. This subject is still new to me. My understanding of that syntax is that it is a compiler memory barrier, not a CPU memory barrier because the asm block is empty (no sfence or mfence). reply loeg 17 hours agorootparentHey, no problem. > My understanding of that syntax is that it is a compiler memory barrier, not a CPU memory barrier because the asm block is empty (no sfence or mfence). In C11, you can write compiler-only fences with atomic_signal_fence: https://en.cppreference.com/w/c/atomic/atomic_signal_fence (In practice, though, I think it is rare that you actually want a compiler-only fence. Instead, correct use of acquire/release operations prevents reorderings.) reply samsquire 17 hours agorootparentThank you loeg, I appreciate you and information you brought that TIL. I've been using a compiler fence to force reloads from memory to prevent -O3 from optimising away my variables/structs changing by other threads and keeping data in registers rather than reloading from memory each time. I saw the volatile recommended against from the Linux kernel programmers. such as my thread->running == 1 in my event loops for my threads. https://www.kernel.org/doc/html/latest/process/volatile-cons... reply loeg 17 hours agorootparent> I've been using a compiler fence to force reloads from memory to prevent -O3 from optimising away my variables/structs changing by other threads I would highly recommend using the language standard atomic primitives instead. reply anonymousDan 11 hours agoparentprevRegarding the contention, one thing that's important is to cache a local copy of the shared head and tail variables every time you access them. Then for subsequent operations you can first check the local cached copy to see if you can perform the read or write without needing to check the shared variables. reply samsquire 17 hours agoparentprevWhen you check available, you might have to do it as a (__atomic_load_n(&sender->realend, __ATOMIC_SEQ_CST) and do __atomic_store_n when setting available. rather than just a plain load. reply fritzo 18 hours agoprevSee also the Java LMAX Disruptor https://github.com/LMAX-Exchange/disruptor I've built a similar lock-free ring buffer in C++11 https://github.com/posterior/loom/blob/master/doc/adapting.m... reply GenericCanadian 18 hours agoparentI also wrote an LMAX Disruptor in Crystal: https://github.com/nolantait/disruptor.cr Here is one in Ruby: https://github.com/ileitch/disruptor Both languages are quite readable and I've used these to teach the concepts to beginners. reply bfrog 17 hours agoprevI have to say after looking at various DMA hardware I much much prefer the scatter gather list type than the ring type of DMA. The entire need of a bipbuffer allocation for DMA then goes way. You can have a simple pool of fixed sized blocks to throw at the DMA. Pretty easily done with a free list. I do think the implementation here is cool though, and its nice to see some work in this area. reply 95014_refugee 17 hours agoparentTo make scatter/gather go fast, you either spend a lot of effort caching descriptor lists for pinned buffers (because you expect to see them often), or heavily optimising your VM's v2p translation machinery, or some combination of the two. And then you wind up discovering that you the driver writer aren't actually trusted and so you need to insert at least one if not several IOMMUs between the peripheral and the memory(ies) that they may access, managed by another software component in a different address space. Then someone asks you to make all of this work for clients in VMs. At which point you start wondering why you didn't just allocate a physically contiguous buffer at startup and copy to/from your client buffers using the CPU... Sorry for sounding triggered... 8) reply bfrog 10 hours agorootparentNo need to apologize at all. I haven’t seen these issues. But I’ve worked with this setup without mmu involvement. reply piterrro 17 hours agoprevThis is great article! Very detailed and explains things on a low-level. For a more high-level implementation, I just released yesterday a blog post about ring buffer in Golang: https://logdy.dev/blog/post/ring-buffer-in-golang reply KingOfCoders 4 hours agoprevSounds a little like LMAX architecture. reply ChrisMarshallNY 18 hours agoprevThat watermark is a simple, elegant idea. I haven't really had the need for that kind of thing, in many years, but I like the idea. reply loeg 17 hours agoparentRecently (2022) I designed a ring buffer for use as a 'flight recorder' type tracing system. (I.e., there is typically no reader; the writer needs to write over old records without blocking on any reader. If the reader is triggered, it flips a switch that disables the writer temporarily while the buffer contents are persisted.) In that design I subdivided the ring into several subbuffers (~8). Each subbuffer has its own equivalent of a watermark. That way, the valid portion of the ring always starts at the beginning of one of the subbuffers, and the writer could 'free' the next subbuffer worth of space trivially (without having to scan through old contents record by record). (Any write that did not fit in the current subbuffer was advanced to the start of the next one.) reply liquid153 17 hours agoprevAren't lock free buffers usually just as expensive or more expensive to use as locks. reply loeg 17 hours agoparentNo -- for a lock-free design with a single producer and consumer, it's possible both are typically writing to independent regions of memory. With a lock, both have to write the same cache line to take and release the lock. reply zengid 17 hours agoparentprevNot if your program needs to be realtime or near realtime safe. Locks are controlled by the OS typically, and can have non-deterministic latency. reply loeg 17 hours agorootparentEven ignoring mutexes and OS scheduling, plain spinlocks add contention that would not otherwise exist in a SPSC ringbuffer. https://news.ycombinator.com/item?id=39551575 reply ww520 17 hours agoparentprevLock free has two advantages: the checking code can run in user mode and non-contested access is very cheap with just one instruction. To do it correctly, lock needs to be done in the kernel thus obtaining a lock requires calling into the kernel which is more expensive. I think you meant the memory barrier for syncing cache is just as expensive as the lock version, which is true. reply scaredginger 16 hours agorootparentObtaining an uncontested lock absolutely doesn't require calling into the kernel reply jcelerier 16 hours agorootparentHow can you give hard guarantees that on Windows, Mac, Linux with the OS and/or libc provided locks? reply saagarjha 25 minutes agorootparentBecause not doing that these days would be malpractice for an OS of that caliber. reply tialaramex 8 hours agorootparentprevRust (which is what we're discussing here) actually doesn't promise this in general. But for the three operating systems you mentioned that is in fact what it delivers because as another commenter mentioned it's table stakes. If your OS can't do this it's a toy OS. The Windows and Linux solutions are by Mara Bos (the MacOS one might be too, I don't know) The Windows one is very elegant but opaque. Basically Microsoft provides an appropriate API (\"Slim Reader/Writer Locks\") and Mara's code just uses that API. The Linux one shows exactly how to use a Futex: if you know what a futex is, yeah, Rust just uses a futex. If you don't, go read about the Futex, it's clever. reply gpderetta 14 hours agorootparentprevIf you really really really need such a guarantee, you implement your own. Otherwise you inspect the implementation, but in 2024 a fast-pathed OS lock is table stakes. reply CyberDildonics 17 hours agoparentprevNo, where are you getting that information? reply stmblast 18 hours agoprevAwesome article! Bookmarked. reply LAC-Tech 11 hours agoprev [–] TIL about BipBuffers. I've been struggling with a similar data structure, and to see it already has a name, and a better implementation than what I've been doing, is very welcome. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post explores a high-performance lock-free ring buffer for seamless cross-thread communication, focusing on circular buffers, DMA, concurrency design, and implementation specifics.",
      "Emphasizing contiguous data for efficient communication, it delves into DMA in embedded systems and implements non-blocking asynchronous buffers with atomic pointers for thread safety.",
      "Designed for x86 and ARM platforms, this ring buffer offers user-friendly interfaces for static allocation and can divide into Producer and Consumer halves for diverse scenarios."
    ],
    "commentSummary": [
      "The post covers implementing lock-free data structures like queues and hash maps, emphasizing the use of Atomic* Rust primitives for lock-free designs and the impact of Intel's Software Developer Manual on lockless code.",
      "It delves into memory ordering semantics, bipartite buffers, memory barriers, thread safety, and optimizations for thread performance, stressing the importance of TLA models for multithreading safety.",
      "Discussions include real-time applications, buffer handling techniques, and comparisons between lock-based and lock-free designs in various programming languages, concluding with information on hard guarantees for locks on Windows, Mac, and Linux, and practical locking solutions by Mara Bos."
    ],
    "points": 188,
    "commentCount": 98,
    "retryCount": 0,
    "time": 1709218623
  },
  {
    "id": 39553743,
    "title": "AI and Tech Sectors on Edge: Money Bubble Warning",
    "originLink": "https://www.tbray.org/ongoing/When/202x/2024/02/25/Money-AI-Bubble",
    "originBody": "Money Bubble Search I think I’m probably going to lose quite a lot of money in the next year or two. It’s partly AI’s fault, but not mostly. Nonetheless I’m mostly going to write about AI, because it intersects the technosphere, where I’ve lived for decades. I’ve given up having a regular job. The family still has income but mostly we’re harvesting our savings, built up over decades in a well-paid profession. Which means that we are, willy-nilly, investors. And thus aware of the fever-dream finance landscape that is InvestorWorld. The Larger Bubble · Put in the simplest way: Things have been too good for too long in InvestorWorld: low interest, high profits, the unending rocket rise of the Big-Tech sector, now with AI afterburners. Wile E. Coyote hasn’t actually run off the edge of the cliff yet, but there are just way more ways for things to go wrong than right in the immediate future. If you want to dive a little deeper, The Economist has a sharp (but paywalled) take in Stockmarkets are booming. But the good times are unlikely to last. Their argument is that profits are overvalued by investors because, in recent years, they’ve always gone up. Mr Market ignores the fact that at least some of those gleaming profits are artifacts of tax-slashing by right-wing governments. That piece considers the observation that “Many investors hope that AI will ride to the rescue” and is politely skeptical. Popping the bubble · My own feelings aren’t polite; closer to Yep, you are living in a Nvidia-led tech bubble by Brian Sozzi over at Yahoo! Finance. Sozzi is fair, pointing out that this bubble feels different from the cannabis and crypto crazes; among other things, chipmakers and cloud providers are reporting big high-margin revenues for real actual products. But he hammers the central point: What we’re seeing is FOMO-driven dumb money thrown at technology by people who have no hope of understanding it. Just because everybody else is and because the GPTs and image generators have cool demos. Sozzi has the numbers, looking at valuations through standard old-as-dirt filters and shaking his head at what he sees. What’s going to happen, I’m pretty sure, is that AI/ML will, inevitably, disappoint; in the financial sense I mean, probably doing some useful things, maybe even a lot, but not generating the kind of profit explosions that you’d need to justify the bubble. So it’ll pop, and my bet it is takes a bunch of the finance world with it. As bad as 2008? Nobody knows, but it wouldn’t surprise me. The rest of this piece considers the issues facing AI/ML,  with the goal of showing why I see it as a bubble-inflator and eventual bubble-popper. First, a disclosure: I speak as an educated amateur. I’ve never gone much below the surface of the technology, never constructed a model or built model-processing software, or looked closely at the math. But I think the discussion below still works. What’s good about AI/ML · Spoiler: I’m not the kind of burn-it-with-fire skeptic that I became around anything blockchain-flavored. It is clear that generative models manage to embed significant parts of the structure of language, of code, of pictures, of many things where that has previously not been the case. The understanding is sufficient to reliably accomplish the objective: Produce plausible output. I’ve read enough Chomsky to believe that facility with language is a defining characteristic of intelligence. More than that, a necessary but not sufficient ingredient. I dunno if anyone will build an AGI in my lifetime, but I am confident that the task would remain beyond reach without the functions offered by today’s generative models. Furthermore, I’m super impressed by something nobody else seems to talk about: Prompt parsing. Obviously, prompts are processed into a representation that reliably sends the model-traversal logic down substantially the right paths. The LLMbots of this world may regularly be crazy and/or just wrong, but they do consistently if not correctly address the substance of the prompt. There is seriously good natural-language engineering going on here that AI’s critics aren’t paying enough attention to. So I have no patience with those who scoff at today’s technology, accusing it being a glorified Markov chain. Like the song says: Something’s happening here! (What it is ain’t exactly clear.) It helps that in the late teens I saw neural-net pattern-matching at work on real-world problems from close up and developed serious respect for what that technology can do; An example is EC2’s Predictive Auto Scaling (and gosh, it looks like the competition has it too). And recently, Adobe Lightroom has shipped a pretty awesome “Select Sky” feature. It makes my M2 MacBook Pro think hard for a second or two, but I rarely see it miss even an isolated scrap of sky off in the corner of the frame. It allows me, in a picture like this, to make the sky’s brightness echo the water’s. And of course I’ve heard about success stories in radiology and other disciplines. Thus, please don’t call me an “AI skeptic” or some such. There is a there there. But… · Given that, why do I still think that the flood of money being thrown at this tech is dumb, and that most of it will be lost? Partly just because of that flood. When financial decision makers throw loads of money at things they don’t understand, lots of it is always lost. In the Venture-Capital business, that’s an understood part of the business cycle; they’re looking to balance that out with a small number of 100x startup wins. But when big old insurance companies and airlines and so on are piling in and releasing effusive statements about building the company around some new tech voodoo, the outcome, in my experience, is very rarely good. But let’s be specific. Meaning · As I said above, I think the human mind has a large and important language-processing system. But that’s not all. It’s also a (slow, poorly-understood) computer, with access to a medium-large database of facts and recollections, an ultra-slow numeric processor, and facilities for estimation, prediction, speculation, and invention. Let’s group all this stuff together and call it “meaning”. Have a look at Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data by Emily Bender and Alexander Koller (2020). I don’t agree with all of it, and it addresses an earlier generation of generative models, but it’s very thought-provoking. It postulates the “Octopus Test”, a good variation on the bad old Chinese-Room analogy. It talks usefully about how human language acquisition works. A couple of quotes: “It is instructive to look at the past to appreciate this question. Computational linguistics has gone through many fashion cycles over the course of its history” and “In this paper, we have argued that in contrast to some current hype, meaning cannot be learned from form alone.” I’m not saying these problems can’t be solved. Software systems can be equipped with databases of facts, and who knows, perhaps some day estimation, prediction, speculation, and invention. But it’s not going to be easy. Difficulty · I think there’s a useful analogy between the stories AI and of self-driving cars. As I write this, Apple has apparently decided that generative AI is easier than shipping an autonomous car. I’m particularly sensitive to this analogy because back around 2010, as the first self-driving prototypes were coming into view, I predicted, loudly and in public, that this technology was about to become ubiquitous and turn the economy inside out. Ouch. There’s a pattern: The technologies that really do change the world tend to have strings of successes, producing obvious benefits even in their earliest forms, to the extent that geeks load them in the back doors of organizations just to get shit done. As they say, “The CIO is the last to know.” Contrast cryptocurrencies and blockchains, which limped along from year to year, always promising a brilliant future, never doing anything useful. As to the usefulness of self-driving technology, I still think it’s gonna get there, but it’s surrounded by a cloud of litigation. Anyhow, anybody who thinks that it’ll be easy to teach “meaning” (as I described it above) to today’s generative AI is a fool, and you shouldn’t give them your money. Money and carbon · Another big problem we’re not talking about enough is the cost of generative AI. Nature offers Generative AI’s environmental costs are soaring — and mostly secret. In a Mastodon thread, @Quixoticgeek@social.v.st says We need to talk about data centres, and includes a few hard and sobering numbers. Short form: This shit is expensive, in dollars and in carbon load. Nvidia pulled in $60.9 billion in 2023, up 126% from the previous year, and is heading for a $100B/year run rate, while reporting a 75% margin. Another thing these articles don’t mention is that building, deploying, and running generative-AI systems requires significant effort from a small group of people who now apparently constitute the world’s highest-paid cadre of engineers. And good luck trying to hire one if you’re a mainstream company where IT is a cost center. All this means that for the technology to succeed, it not only has to do something useful, but people and businesses will have to be ready to pay a significantly high price for that something. I’m not saying that there’s nothing that qualifies, but I am betting that it’s not in ad-supported territory. Also, it’s going to have to deal with pushback from unreasonable climate-change resisters like, for example, me. Anyhow… · I kind of flipped out, and was motivated to finish this blog piece, when I saw this: “UK government wants to use AI to cut civil service jobs: Yes, you read that right.” The idea — to have citizen input processed and responded to by an LLM — is hideously toxic and broken; and usefully reveals the kind of thinking that makes morally crippled leaders all across our system love this technology. The road ahead looks bumpy from where I sit. And when the business community wakes up and realizes that replacing people with shitty technology doesn’t show up as a positive on the financials after you factor in the consequences of customer rage, that’s when the hot air gushes out of the bubble. It might not take big chunks of InvestorWorld with it. But I’m betting it does. Updated: 2024/02/29 Contributions Comment feed for ongoing: From: Tristan Louis (Feb 29 2024, at 11:32) As always, a fascinating and thought provoking piece. One of the area where I believe you may need to modulate your thinking is in the proverbial shovels vs. gold framework. Nvidia (and by some extend, your previous employer) are selling shovels (chips or time on system) that allows them to get value right now from those panning for AI gold. Assuming they see the curve well enough, they could adjust to the downward demand in the cycle when the craze subsides. If we were to think in terms of the dotcom era as a comparable bubble/bust cycle (albeit on a much larger scale now), a lot of companies making AI central to their existence will make the same mistakes as dotcoms which made their being an Internet asset central to their existence. In the same ways, the companies that provided tools (eg. The telcos and hardware vendors) or companies that added the new tech to their offerings without jeopardizing their code business (or used the new offering to repackage old ideas into new enhanced ones (eg. Turning the Sears catalog into an Internet ordering system) will survive the incoming crash and potentially amass enough cash reserves to build a longer term large asset. [link] From: Max Pool (Feb 29 2024, at 11:59) Aswath Damodaran (a Professor of Finance at the Stern School of Business at NYU) recently analyzed the magnicifient seven. Nvidia is 55.84% overvalued with five year Expected CAGR Revenue 32.20% and target operating margin 40.00%. Exel: https://pages.stern.nyu.edu/~adamodar/pc/blog/NVIDIA2024.xlsx blog post: https://aswathdamodaran.blogspot.com/2024/02/the-seven-samurai-how-big-tech-rescued.html [link] From: Justin Watt (Feb 29 2024, at 12:33) From one willy-nilly investor to another: aim for average, invest in index funds, and enjoy the ride. But I'm sure you already do that. For further reading/self-soothing, check out The Simple Path to Wealth by JL Collins or The Psychology of Money by Morgan Housel. [link] From: Rob (Feb 29 2024, at 12:40) The UK may be thinking about it, Canada has been using it for I think a couple of years now: https://www.cicnews.com/2023/05/minister-fraser-clarifies-how-ircc-uses-ai-in-application-processing-0537338.html#gs.57gcdt. Basically, when you apply for a Temporary Residence Visa to say stay in Canada with your spouse whilst you wait on IRCC (ie Immigration) to process your application, your application is processed by a LLM. If it turns you down, you have no avenue of appeal to a human, no, all you can do is re-write it and submit it again. I see the day coming when you will have to pay for a LLM to make your applications (to say welfare, immigration, parole board, the tax man) more palatable, much like paying SEO shysters nowadays to show up on Google, or the vigorish for Amazon. Because that has worked out so well. [link] From: philvec (Feb 29 2024, at 12:59) AI specialist here, by both degree and job experience. I was looking long for an opinion like this, particularily on the insufficiency of LLMs alone to model \"meaning\". Not all the statements I 100% agree with, but THANK YOU TB, since my hope for humanity has risen from the dead - as now I know somebody is also aware of the problem! [link] ongoing What this is · Truth · Biz · Tech author · Dad colophon · rights February 25, 2024 · The World (148 fragments) · · Business (2 more) By Tim Bray. The opinions expressed here are my own, and no other party necessarily agrees with them. A full disclosure of my professional interests is on the author page. I’m on Mastodon!",
    "commentLink": "https://news.ycombinator.com/item?id=39553743",
    "commentBody": "Money bubble (tbray.org)181 points by headalgorithm 14 hours agohidepastfavorite206 comments marcinzm 13 hours ago> I kind of flipped out, and was motivated to finish this blog piece, when I saw this: “UK government wants to use AI to cut civil service jobs: Yes, you read that right.” The idea — to have citizen input processed and responded to by an LLM — is hideously toxic and broken; and usefully reveals the kind of thinking that makes morally crippled leaders all across our system love this technology. As someone who actually had to deal with the government recently in the US I disagree. It was impossible to reach a human or otherwise get an answer to my likely not too unusual question. If they had an even half decent LLM then I'd have probably had my answer and action items for me to do within 30 seconds. Instead I've wasted days in various attempts to get some type of answer. I recently needed to fix some issues in something I filled with the government. Email support used to exist but probably cut due to budgets. Chat support used to exist but probably cut due to budgets. Phone support has no waiting queue and require 1 minute of entering numbers to hit the disconnect point (due to not available agents). Physical mail seems an option but I don't know the format or address. Etc. reply rldjbpin 51 minutes agoparentthis idea is at odds with the way public services work in societies with safety nets. it has been, and remains to be, the case that the main purpose of certain parts of public services is to give people employment. there is rarely any meritocracy at scale once you get the job. the reason why we get poor service cannot be completely put down to getting understaffed or lack of budget. while the UK govt has a better public service experience online than many developed countries, this approach I feel is missing the forest for the trees. reply bombcar 13 hours agoparentprevThe only thing I've seen that even gets close to working is physically going to the office in person but hell finding what or where that is. And you can't even do that with Social Security anymore. If it is something you could be legally liable for, I'd at least send a certified letter to whatever address you can find, so that if it becomes a problem later you can at least show you tried. reply marcinzm 13 hours agorootparent> The only thing I've seen that even gets close to working is physically going to the office in person but hell finding what or where that is. They do try to discourage it sometimes. The local passport office has a sign on the door that says \"by appointment only.\" The first thing you hear upon walking in is \"if you don't have an appointment get into line B.\" If you have an urgent mater they will take care of it without an appointment. I wonder how many people turned around upon seeing that sign on the door. Dark patterns left and right to make it harder to get anywhere. reply bombcar 13 hours agorootparentIn the USA, writing a letter to your elected representative (of the appropriate government level, so Senator or Representative for federal, etc) can often eventually get satisfaction, because the government bureaucrats never ignore a letter from them, because it gets their boss yelled at. reply rglover 14 hours agoprev> I kind of flipped out, and was motivated to finish this blog piece, when I saw this: “UK government wants to use AI to cut civil service jobs: Yes, you read that right.” The idea — to have citizen input processed and responded to by an LLM — is hideously toxic and broken; and usefully reveals the kind of thinking that makes morally crippled leaders all across our system love this technology. And this will not just be in government, it will be everywhere. The scariest part is that as people start to spend less time developing a skill set, and instead deferring to AI answers, you will cross a point where this problem can't be fixed (because nobody has the skills to fix it and the AI is trained on the outputs of previous generations of humans). For the \"olds\" who already have a skillset, this will be incredibly lucrative (as those who can afford to pay to fix it will handsomely). But the potential for this to—at best—plateau humanity and at worst, make it regress, is significant. The dark humor in all this: we thought AI would get us the Terminator, but instead it's going to get us rapid degeneration. --- Edit: an addendum, the overall point I'm making is well encapsulated in this talk https://www.youtube.com/watch?v=ZSRHeXYDLko reply PheonixPharts 13 hours agoparentAnyone who works in AI, especially very closely with models, will tell you that it's not capable of really replacing any jobs yet. All these jobs being \"replaced by AI\" are simply being eliminated with the consequences of them being eliminated ignored. Customer service jobs aren't being replaced by AI, companies, like Klarna, are just giving up on customer service and using AI to increase their perceived value rather than reducing it. reply qwertox 13 hours agorootparentAdding a link to Klarna's announcement [0] from two days ago and quoting their summary: - The AI assistant has had 2.3 million conversations, two-thirds of Klarna’s customer service chats - It is doing the equivalent work of 700 full-time agents - It is on par with human agents in regard to customer satisfaction score - It is more accurate in errand resolution, leading to a 25% drop in repeat inquiries - Customers now resolve their errands in less than 2 mins compared to 11 mins previously - It’s available in 23 markets, 24/7 and communicates in more than 35 languages - It’s estimated to drive a $40 million USD in profit improvement to Klarna in 2024 Apparently it affected a Call-Center's (Teleperformance) stock [1]. [0] https://www.klarna.com/international/press/klarna-ai-assista... [1] https://live.euronext.com/en/product/equities/fr0000051807-x... reply Dyac 13 hours agorootparentHow much of it really requires AI though? I bet the majority, if not all of the support that the AI offers could have been done with some of the non-AI chat flow builders if a handful of smart people got together and actually worked the flows out properly to handle the scenarios. reply ben_w 12 hours agorootparentQuite a lot of call centres, are, from a user perspective, a flow chart with a human serving as a voice-to-computer interface. However, I've encountered some pretty weird interactions with customer support over the years, including reportedly \"The iMac can't do anything except browse the internet\" when the demo unit on display behind them was running Nanosaur (a game); \"we only support Microsoft Internet Explorer\" when the customer support team didn't have that installed on their computers; «You need a Windows PC and an Android phone» from the German PostIdent people despite it being obvious they could talk to us while we used a Mac and that they knew this because they raised the issue spontaneously; and \"yes, we will get your internet connection running by the end of tomorrow\" from BT (it took them a month or two, by which time I had already cancelled; apparently someone put the wires in back to front). reply sonicanatidae 12 hours agorootparentMy fav was a Dell Tier 1 insisting that he could not process an RMA without me gathering info from the BIOS screen on a laptop THAT WOULD NOT POWER ON. It only took me explaining it 3 times, then telling him to \"get a fucking person on the phone that understands tech\", which he did and it was processed in minutes. There's poor training, then there's just plain stupid. Clarification: I'd given him the Service Tag, so he knew what device it was. He was insisting that I run the diagnostics and report the results, which is even dumber, in the end. reply sokoloff 12 hours agorootparentprev> a flow chart with a human serving as a voice-to-computer interface Also known as IPoV (IP over voice). reply sn0wf1re 13 hours agorootparentprevIsn't that the point though? There is no need to make chat flow builders and pay the \"handful of smart people\" to figure them out. If AI costs were reaching very high levels perhaps they would try to make non-AI flows for standard processes. But I think that is unlikely given how cheap AI is vs smart people wages. reply Dyac 9 hours agorootparentI think the problem is that building out chat flows, sitting down with customer care and figuring this stuff out, iterating and improving the \"if this then that\" logic within a flow builder tool isn't particularly sexy work. \"LLMs\" and \"AI\" is though. The cost of using generative AI to answer questions is orders of magnitude more expensive than using flows. Plucking a company out of thin air - Landbot [1] offers both flow and generative chats. For $100 per month you can have 2,500 flow chats, or 30 \"AI\" chats. That's nearly a 100x difference in cost. The risks are much higher too - with the flow builders if there's a sudden policy change or whatever then someone can just go into the system and edit it - with AI you'd have to retrain the model somehow. There's also no risk of hallucination with a flow based builder. I'm not saying that Gen AI customer service chatbots don't have a use - what I'm trying to say is that in the real world, business would probably be better served day-to-day with just setting up decent flows in rules based bots. That's unsexy though - it doesn't attract tech talent, it doesn't get people promoted and it doesn't get shouted about in the press. It is, however, probably much better for the environment and the company's P&L (but possibly not their valuation if they're trying to ride the hype train). [1] https://landbot.io/pricing reply kreetx 12 hours agorootparentprevThat's my impression as well: if customer service had been a priority enough, then these smart people would have been gathered already. But there have always been more important things to do. And now with LLMs, you don't have to find these smart people, and those you already have can do something else important. reply ilamont 13 hours agorootparentprevAmazon and others already claim they use AI to handle customer support and marketing copy (https://www.aboutamazon.com/news/innovation-at-amazon/how-am...). Air Canada got burned when an AI chatbot made a mistake (https://news.ycombinator.com/item?id=39378235). Legal firms have been fined for hallucinated precedent citations (https://news.ycombinator.com/item?id=39491510). The technology is clearly flawed. Regardless, a lower cost option (AI) is replacing a higher priced option (human labor). Ten years ago these tasks would have been handled by human employees or specialists. There is certainly a history of this in the corporate world, when expensive union labor in Detroit is replaced by lower cost workers in Mississippi, manufacturing experts are forced to train their replacements in lower-cost countries, or entire engineering and customer service departments are shifted overseas. reply Tagbert 13 hours agorootparentWe use some AI for the first level of customer support. It can provide standard answers to common questions and help with routine action requests, but it is backed up by human operators for anything more complex and customers can opt out of the AI whenever they want. From customer surveys, they tend to like using the AI for simple things because it is faster and they get what they need. The like that they can get a human when they need too. If you aren't providing a human fallback, you aren't doing it right. reply daveguy 13 hours agorootparentSay customers only fall back to a human for 10% of more difficult questions.Then one support agent can field as many customer service questions as 10 employees could before. 90% layoffs is great depression numbers. Maybe only one sector, but the sectors it does impact will be hit hard. reply bilsbie 11 hours agorootparentBeing a consumer myself, I’ve never had a non difficult question. I’m not calling to check my balance or get directions. I’m calling because your system did something wrong and needs to be overridden reply threecheese 10 hours agorootparentSame. Never once have I selected the “Check my balance” tele-option, though I’ve had to listen to it innumerable times. reply eitland 3 hours agorootparentIt used to be quite handy back in the 90ies :-) reply cogman10 13 hours agorootparentprevBingo, and that's the part of AI that I think is overlooked. It might kill some jobs, but it'll never kill an entire sector like the web killed travel agencies. And the thing is, the more the risk the less likely an LLM is going to be trusted to just make a decision. For example, do you think an insurance agency would want an LLM to decide, on it's own, claim approvals? Can you imagine the headache for the insurance agency if the LLM approves the wrong claims or denies the wrong claims? Or for a doctors office, Imagine AI diagnostics without a human. Can you imagine the headache for a hospital when an AI misses cancer? Or diagnoses a false cancer? It's bad enough when humans get that wrong, but now you have to explain to your legal team \"We just left up the practice to the stats gods!\" reply ghaff 12 hours agorootparent>Bingo, and that's the part of AI that I think is overlooked. It might kill some jobs, but it'll never kill an entire sector like the web killed travel agencies. To be slightly pedantic, the Web killed travel agencies for individuals who were mostly interested in booking flights, cruises, and big city hotels once making those bookings became easy on an individual level. Companies absolutely still use corporate travel sites, for reasons good and bad. And there are various types of specialist tour operators, arrangers of private trips, etc. who have on-the-ground knowledge of specific locations--and often won't even book things like air travel for you. (More broadly, the Web forced travel agents of various types to add value above and beyond what a travel portal or a random travel agent could do because they were gatekeepers to the needed systems.) reply mattgreenrocks 12 hours agorootparentprev> For example, do you think an insurance agency would want an LLM to decide, on it's own, claim approvals? Can you imagine the headache for the insurance agency if the LLM approves the wrong claims or denies the wrong claims? You say this, but I sometimes worry that these issues are hand-waved away by decision-makers with, \"oh we'll just have another LLM doing frontline claim support to verify these issues.\" It's the whole XML/violence thing, where the solution to XML-induced pain often ends up being more XML. reply notpachet 13 hours agorootparentprevCan you imagine a lawyer using ChatGPT to draft legal briefs and find relevant case histories? Well... reply milanhbs 12 hours agorootparentprevIn both of your examples I could see the model becoming the default, with humans double-checking, if even that. If things go wrong as you describe, humans are pulled into the loop by the humans wronged (unless they give up before). There is a company making money with auto insurance claims: https://tractable.ai/en/products reply falserum 11 hours agorootparentprevOh, how nice it would be to have AI give medical advice. (From time to time I have some trivial questions, but not enough hundred valued bills, to ask actual doctor) I agree with your point though, before starting on chemotherapy I would definitely shell out some money for opinion of a live doctor. reply ponector 13 hours agorootparentprevThat is not true. Jobs have been already replaced, even before chatgpt. Customer support team of 10 is replaced with chatbot and 2 people. Half of the jobs will be replaced with AI soon. Writers? No need to have huge team, one senior is enough. Developers? Lawyers? Illustrators? Lay off half and replace them with AI tools! reply PheonixPharts 13 hours agorootparent> Customer support team of 10 is replaced with chatbot and 2 people. My experience, having grown up when all customer service reps were people, is exactly what I stated above: this is just giving up on customer service. Anyone who has ever called automated support knows this. When you reduce 10 people to 2 people and some chatbots now you simply have to wait 5x as long for customer support. I worked at startup a few years back that refused to scale customer support so they could be forced to \"automate\" the process. The result? Customers got completely screwed over, but those customers weren't investors so who cares. I can't recall a single time in my life were automated customer support solved my problem, it just kept me busy so that the 5x wait doesn't seem as long since I'm trying to navigate the labyrinth of a customer support decision tree to get my problem solved. reply anthonypasq 12 hours agorootparentive had several instances where automated support solved my problem. Amazon re-shipped me a product that was stolen off my porch without me interacting with a human. The fact you're oblivious to this doesnt mean it isnt happening. reply bilsbie 11 hours agorootparentI’d argue that it’s not automated support at that point. It’s just a feature the software now handles. reply Jensson 11 hours agorootparentIf the software handles it then it is automated, that is kinda the point of software. reply ponector 13 hours agorootparentprevThat is true, it is not about the quality. But replacement is there. And customer is screwed, but who cares? reply bregma 13 hours agorootparentprevI suspect the closer you are to the models the farther you are from the decision making. reply rblatz 13 hours agorootparentprevIt's not an all or nothing situation you can put an AI agent in front of real agents, if the AI agent is able to answer the customers question you do not need to escalate to a real human. That could allow 10 humans to now do the job of say 20. Did AI replace those 10 jobs, I'd argue yes. reply roland35 13 hours agorootparentYes, exactly like how a sewing machine helped replace sewers. One person can do the job of 2. reply pavlov 13 hours agorootparentprevAnd the idea of giving up on customer service isn't anything new. Google and Meta have been extremely successful with this approach. AI is just an excuse for others to follow suit. reply disgruntledphd2 43 minutes agorootparentGoogle and Meta have great customer support for the advertisers that spend loads of money with them. reply wvenable 13 hours agorootparentprevThe consequence of almost all software the loss of jobs. I've personally been responsible for many many jobs disappearing with the software I've written. AI is just another form of software and if it is at all useful, and I believe that it is, then it will eliminate jobs. That should not be particularly surprising -- computers have already eliminated entire categories of jobs. I think the subtle point is that not all humans will replaced -- it's just that a human and AI will be able to do the work of a few humans. Same work, less people. reply arisAlexis 13 hours agorootparentprevNot true at all. Hearsay at best. reply happytiger 13 hours agorootparentprevUnfortunately not only are there plenty of examples in the real world where this isn’t true and people are already being replaced, there’s a larger issue, and that’s the issue of the pace of improvement. People think AI technologies improve in a linear fashion. But there is nothing restricting this area of technology from non-linear progress. Consider the failing prophesies of AGI as a precient example of what’s happening: - It was 2015, and I’m reading articles about AGI being here in 2050 at least. - It’s 2018 and everyone is talking about a few new research papers but secure in their predictions but maybe feeling like it is skewing towards the bottom end of that range (except for a few inspired nerds — my people — who boldly claim 2035). - It’s 2022 and suddenly an AI is blowing people’s minds and we have the fastest growing technical product in the history of the world. Predictions are now 2030 for an AGI. - It’s 2024 and people are debating if AGI has already happened and debating about the definition and many people are calling for an “advanced level” AGI in the next 2-3 years. My point is that predictions of this technology have been terrible. Just like the worst. People have been off on every prediction by orders of magnitude. So now every major tech company is blowing all their money towards AI, realigning their business towards hardware and software solutions and we’re in the middle of an arms race the size of Jupiter towards AI technology, and it’s happening across the world but definitely in both China and the USA where the stock market is going crazy and 25% of the entire markets growth is just nvidia’s massive growth (and those gains are basically powered by the leading ai training solutions). So, the idea that somehow the technology isn’t going to replace people is asinine. This is the biggest, fastest tech wave I have ever seen, it’s growing geometrically, it’s funded by insane amounts of money and has most of the western and eastern world’s technology research focused on it, and has been wildly ahead of predictions from its inception. Let’s get real about this. This is a bomb going off in slow motion and is set to interrupt employment and radically reshape society in a time scale that almost no humans can physically comprehend. But more importantly the trend is accelerating and like most parabolic markets that don’t have physical limitations holding them back (like input materials for a gold rush) it could accelerate to literally crazy levels. There’s no restriction but breakthroughs here. I’m aware of the current reality of the software systems and I know what I’m saying is futurist, but from a trend perspective we are way, way ahead of where we thought we were going to be and the trends point to railgun speed acceleration from here. reply ben_w 12 hours agorootparentPredictions have been wrong in both directions at the same time; myself, I confidently predicted in 2009 that normal people would be able to buy cars which didn't have steering wheels in… 2019. While I'm really impressed with ChatGPT, and am one of the people who regards it as meeting my prior definition of AGI[0], I can still see its current flaws, and do wonder if this is a similar case, where the first 90% needed a major breakthrough but once that was invented anyone could do it and many wanted in on the economic opportunity… but the second 90% turned out to be just as hard, and so was the third, … and you need at least six nines[1] to really replace humans in these roles. [0] All three letters mean different things to different people. To me: it's artificial, it's much too general to count as a narrow AI, and I count it as intelligent because the things it can do were the things I grew up thinking were signs of intelligence, like speak Latin, do algebra, and answer trivia questions, and also things I added later like 'write code' and 'pass medical and law exams'; even though it gets the answers wrong sometimes, I don't think my standard was ever \"must be perfect\" because nobody ever scored 100% on exams at school either. At its best (and it's weird that it even has a best and a worst), the free version of ChatGPT has given me better code than one specific real human I've had to work with, more if you also add in the students. (And at its worst, it gives me stuff that doesn't compile and wouldn't do what I asked even if I fixed the compiler errors). [1] Assuming a driver is making 1 decision per second that has a serious wrong answer, it would take eight nines to have just under one serious accident in a lifetime of 1-hour-each-way commutes, 5 days a week, 50 weeks a year for 40 years. I suspect the actual time between opportunities for serious mistakes is less than that, but probably at least once per minute even on an empty road. For an LLM, I don't know exactly how good they'll really need to be, all I can guess at is that they're not going to have more than one opportunity to seriously mess up per token. reply happytiger 10 hours agorootparentExcellent reply and I apologize for not having time to give you an equally thorough response at this time as I’m slammed. It’s such a worthy reply I apologize. But the question I would ask you is whether you are any barrier to simply scaling up the tokens on the existing technology? I don’t. I see us at the beginning of a ladder where the only input is capacity just like when Intel was young. While breakthroughs can change the path, the truth is we have a fairly predictable step-by-step to much more capable systems without one just by scaling the size of the hardware. Consider this argument: 1) I mean at core the crux of our learning is that predicting the next word may be what human thinking is generally about, and how our brain works, because that’s largely the innovation here. 2) Becoming better at doing that is entirely predictable and we can scale profoundly from current levels with hardware that we are putting into production right now and that we have already invented. 3) Therefore the path to next generation capabilities is relatively (and that’s important I admit) linear. So prior to breakthrough, we have a simple path forward to what would be at peast fairly advanced capabilities of language and media prediction and manipulation. Now your argument about progress is right. Predicting material progress of technology breakthroughs tends to be unpredictable and inherently dangerous, but we are in an accelerating trend and most of the time (big statement here, right?) the appearance of an acceleration trend tends to extend to continuance of the trend in a certain timeframe. At least that’s been the case with the “waves” of technology breakthrough since the Industrial Revolution according. I mean to invoke Smihula's theory of waves in this argument, since I know you’ll understand that. Those last two arguments are statistically supported and quite logical. How smart does it need to be and how statistically probable are excellent points. As an aside… For me, one of the areas I am focused on and thinking about a lot is self-organizing AI agents. Having worked a lot with large scale networks, agents and task specialized networked AI systems get me excited. My brain considers it a blue ocean opportunity. The parallels between human civilization density and current learning about density of population in demographics driving essential human progress makes me believe there will be parallels in AI. The more, the more they will self organize into network effects, and the outcome of this, like human civilization, will be high quality and rapid progress. I am not a believer in one gigantic AI, but networks of networks self organized in a way where they self-optimize around goals and outcomes, and we are really just at the beginning of exploring this direction of the technology. The biggest limiting factor to AI technology at this point is human input and the need for human oversight. While that oversight is definitely necessary, once AI becomes self organizing and self-creating, progress should be profound. Anyone who doesn’t think that’s going to happen needs to understand the nature of intelligence and realize it’s just a matter of time. You can’t go down this path in a meaningful way and repress only certain aspects of digital intelligence in the long term. reply Workaccount2 12 hours agorootparentprevThe core reason is that nobody understands how these models work, and hence what they are/will be capable of. It's a little unnerving to think about, since the approach is pretty much \"Lets build the framework, plant the seeds, and then see what comes out the other side\". If AGI is achieved, it will almost certainly be a surprise rather than be intentional. reply lp4vn 13 hours agoparentprevThere is something very wrong with the current dynamic of the world. Work is deeply devalued while capital has been reaping all the benefits of the increased productivity: if you doubt me, ask anyone who has made any kind of investment if they are making more money from the investment or from their regular job. This is creating a ridiculous wealth disparity and deincentivizing a whole generation to get good with a skillset. I already heard from a lot of young people that working is not worth it, hard to disagree with them when even a basic thing like a piece of land or a house looks out of reach for a regular person. But as you put it unsustainable things are not sustainable, society will regress until the equilibrium is found again. But things didn't need to be like this. reply xetplan 8 hours agorootparentAs opposed to when in the past? This linen of thought is just so lacking gratitude for the time and place you were actually born. You basically won the lottery in the grand scheme of things but still complain. Would it have been better to be born in 1950? Certainly not if you happen to be born in China. How about in 1910 so you hit your 20s right as the depression hits. Or 1920 so you grow up in the depression then go fight in WW2. How about Cambodia in 1970? Yea life would be better if I was 6'4, strikingly handsome with a dead rich uncle that left me all his money too. reply lp4vn 3 hours agorootparentYour commentary is pretty childish. I pointed out a structural issue of the modern world and you came up with a pointless counterpoint about being born in an unfavourable condition in the past. I guess I'm being trolled. reply rglover 13 hours agorootparentprevYou're right, they didn't. > “We can say without exaggeration that the present national ambition of the United States is unemployment. People live for quitting time, for weekends, for vacations, and for retirement; moreover, this ambition seems to be classless, as true in the executive suites as on the assembly lines. One works not because the work is necessary, valuable, useful to a desirable end, or because one loves to do it, but only to be able to quit - a condition that a saner time would regard as infernal, a condemnation.” > - Wendell Berry reply happyjack 12 hours agorootparentThis is so spot on. We (assuming you live in the US) live in a country where the purpose is to work for as little as possible, \"grind,\" then throw money into an index fund and have it support us the rest of our lives. Own property, while having the bottom 50% of society continually support us in the gig and service economy. Look, I'm no Bernie Sanders, but you have to be honest about the morality of it, and the feasibility of it. I don't see the current system lasting. reply agumonkey 11 hours agorootparentprevtechnological progress to that extent is touching ethical issues.. what are we supposed to do if our existence don't depend on us learning / collaborating ? reply bluedino 14 hours agoparentprevOn the other hand, in my country we have the stereotypical lazy government employee who is overpaid and doesn't work hard. The government could then save money and provide better service for menial tasks such as \"what permit do I need to do such and such\" reply toomuchtodo 13 hours agorootparentThis is a fiction. You assume the government will provide better service. Instead (from a historical evidence and performance perspective), cuts will be made, service will degrade, and those who lied to champion the changes will not be held accountable. Who will be held accountable when these promises evaporate? My problem is not with innovation, it is with falsehoods and lack of accountability for those falsehoods. Edit: As PheonixPharts says in another comment: > All these jobs being \"replaced by AI\" are simply being eliminated with the consequences of them being eliminated ignored. Customer service jobs aren't being replaced by AI, companies, like Klarna, are just giving up on customer service and using AI to increase their perceived value rather than reducing it. https://news.ycombinator.com/item?id=39554367 You don't need an LLM to do that. You can ignore your customers just fine without it. Cut out the performance art, go straight to zero without it. It is still mostly a powerful search engine backed by the equivalent of a knowledgeable, not a replacement for human support. If the human is not providing what is needed, that is a system failure, not a human failure. This tech augments the human, it does not replace the human. reply kbolino 12 hours agorootparentAccountability requires power. We have stripped politicians of most of their power, leaving them fiddling with margins and fighting over petty things. \"Politics is Hollywood for ugly people\" they say, with a chuckle, but it's far more true than we accept. Unless and until the executive and the legislature can seriously threaten the financiers, journalists, academics, lobbyists, judges, bureaucrats, etc. again, like FDR could, you can't expect accountability. The politicians are just the faces that implement other people's decisions, and those other people don't even have elections to lose. reply rglover 13 hours agorootparentprev> The government could then save money and provide better service for menial tasks such as \"what permit do I need to do such and such\" Yes, but apply Murphy's Law. They could also automate something like appeals to eminent domain claims and make it impossible for you to fight. Imagine being told the family farm that's been passed down over 5 generations is now going to be claimed by the government and turned into a parking lot for a new \"justice center.\" When you go to appeal, the hyper-efficient but devoid-of-empathy AI bot just says \"sorry, Dave, I'm afraid I can't do that.\" reply glitchc 13 hours agorootparentprevIt certainly looks like that from far away, but get close enough to a real permit application, and you'll see that there are so many edge cases, and every situation is so unique, that the exception always defines the rule. I'm willing to wager that a govt. employee processing, let's say renovation permits, sees no more five applications in their entire service history that are textbook as in can be approved without any required corrections. Extend that to any other application, and you'll quickly see the value of an experienced government employee helping you navigate the bureaucracy. If you haven't yet, then you are likely very young and/or your parents have taken care of everything for you to date. And before you blame the civil servant for the byzantine rulebooks, I want to rush to remind you that those civil servants only interpret the laws/rules. They have as much hand in creating them as you or I. reply candiddevmike 14 hours agorootparentprevIsn't that the dream though? To be overpaid and not have to work hard? reply stavros 13 hours agorootparentNot if you're the employer, which, if you're the taxpayer, you are. reply hgomersall 13 hours agorootparentNo you're not. They're employed by the state which is managed by the politicians you elect. I presume you mean to imply that civil servants are paid by your taxes, but that's not true either if we're talking about a sovereign state - a moments consideration would show that spending must preceed taxation, which is true as a matter of accounting. Really you'd do better to note that state employees allow you to get money enabling you to pay your taxes (but that's also not a very helpful way to look at it). reply bombcar 13 hours agorootparentIf you're trying to apply modern money theory, it only works at the federal level (in the USA), state and local governments are directly affected by taxation which must precede spending (or they have to issue warrants or bonds). This comes back to bite California every time there is a major tax revenue crunch for whatever reason. reply hgomersall 13 hours agorootparentAs I said, a sovereign state. (Though I did edit that clarification so you might have missed it). reply candiddevmike 13 hours agorootparentprevHow does that work with states like IL being massively in debt due to overspending and pensions? reply bombcar 13 hours agorootparentIf \"in debt\" means \"they can keep issuing bonds and people are buying them\" then they're fine. If it means \"they cannot make payments on bonds and cannot issue new ones because nobody wants them\" then the Feds will have to step in, or they'll have to liquidate state assets (including privatizing various governmental functions, selling land and leasing it back, etc), or raise taxes to balance the budget. They literally cannot print money. This cycle has already destroyed a few cities (usually the city gets swallowed by the county). There's a step where they issue \"warrants\" like CA did a few times: https://taxfoundation.org/blog/california-issuing-state-warr... reply otterley 13 hours agorootparentprevAs taxpayers, we most certainly do collectively pay for government services. reply hgomersall 13 hours agorootparentPray show me the accounting for how that works... Edit: here's my offer for the UK case: https://www.ucl.ac.uk/bartlett/public-purpose/publications/2... If you want to argue something that directly contradicts that analysis, I await with anticipation. reply apetresc 12 hours agorootparentI'm not sure I understand your objection. Why are we taxed at all, if not to pay for government expenditures? reply hgomersall 3 hours agorootparentThe obvious question! Because taxing is used to free up the resources necessary for the state to purchase them. Once you force everyone to pay taxes, they have to get hold of the state currency, which should (in a well managed system) only happen through state purchase of resources, notably people's time. That is, the ability of a state to provision itself is driven through its currency which in turn is driven through taxation. reply marcus0x62 10 hours agorootparentprevBecause something something MMT something something. reply eropple 9 hours agorootparentI know you're goofing on it, and tbh I give more credence to MMT than most folks do, but most folks I know who think about MMT still acknowledge that if people believe they're being taxed to pay for things, they still kinda are. MMT is a guiding principle for policy, not something to tell individuals how their relationship with the state actually works (because most folks don't care). reply hgomersall 2 hours agorootparentYou make a good point, which is that even through an MMT lens, tax and spend should be the norm, since the resource provisioning and supply should balance to avoid inflationary pressure. However, I suggest they probably should care given how much policy is guided by an incorrect understanding of the monetary system. The whole concern about deficits and sovereign \"debt\" is the obvious one. In addition, a good understanding of why taxation is necessary helps to understand which taxes might be useful and which are not. Finally (for now!), policy options open up when you understand this stuff properly that make no sense at all through the state-as-a-household view. Politicians need to be held to account and an ignorant population is not able to do that. reply s1artibartfast 13 hours agorootparentprev> a moments consideration would show that spending must preceed taxation, which is true as a matter of accounting. Can you elaborate on that? I can think of numerous examples from history for how governments bootstrap themselves. If your point is as simple as who pays the tax collector, the tax collector can be paid on commission, debt, or with plunder. reply hgomersall 13 hours agorootparentThis is a comprehensive analysis: https://www.ucl.ac.uk/bartlett/public-purpose/publications/2... The general case is more or less the same as the UK with only the details varying. As noted elsewhere, this doesn't apply to non sovereign states. reply throwanem 13 hours agorootparentprevI think you meant 'employer'. reply stavros 13 hours agorootparentI did, thank you! reply throwanem 13 hours agorootparentSure thing. You're still wrong on the merits, though. To pick one example, the folks who work at my local MVA (\"DMV\" in most states) office do not work for me, though they're paid out of my taxes and those of everyone else who earns in Maryland. They don't report to me. Nor should they, because if they did, they would also report to that freak who drives a car plastered with QAnon garbage around Perry Hall. My city councilman and my General Assembly representatives work for me, but the people employed to deliver services managed by the state and city government do not. reply otterley 13 hours agorootparentThe point is that we as collective taxpayers don't want to overpay for government services. And that point is a good one. reply throwanem 13 hours agorootparentIs it? Define 'overpay.' reply kapp_in_life 13 hours agorootparentprevFor who? Certainly not for the person paying. reply doctor_eval 13 hours agorootparentprevYes, but that’s for us tech workers, not the common prole. /s reply AlbertCory 13 hours agorootparentprevNo, the one thing that's predictable is: no matter what they do, service will only get worse. reply callalex 13 hours agorootparentIn the transition from pre-internet to modern times, I have found my experience with both California department of motor vehicles and the tax board has improved significantly. So much less traveling to crappy offices and waiting in hours-long lines. I still wouldn’t call it great, but it has objectively improved. reply bombcar 13 hours agorootparentThe huge increase in service has been that they let you sit on the other side of the wall, basically. Instead of telling a DMV employee what they need to hear to fill out the little computer form, now they allow you to fill out the same form, on the web, and unpaid, too! reply AnimalMuppet 12 hours agorootparentOK, but when I had to sit at the DMV for two hours, I wasn't getting paid for that time, either. So if the options are an unpaid 5 minutes on the computer, or an unpaid 2 hours at their office, one is clearly better for me. reply AlbertCory 13 hours agorootparentprevIt actually IS true that DMV and property tax payments are pretty good in CA now. However, I'm sure that's unacceptable to some folks and they'll soon crapify it. reply MyFirstSass 13 hours agoparentprevChrist this is dark! Imagine a world where everything and everyone will be judged, ranked, evaluated, hired, fired, maybe even as a partner or friend by these AI's. Unfathomably grim even if the alternative is rigid low skill bureaucrats. I find LLM's extremely fascinating but if this is the end game i really hope AI free zones will emerge. You can already see Gen Z being obsessed with face ranking filters, \"looksmaxing\" from data points and using filters day to day. It's dark. reply cjbgkagh 13 hours agorootparentI think these gig economy workers and algorithmic punishments already have an element of that. It’s entirely possible to do a good job of it but it appears that no one chooses to do so - I presume the same would be true for LLM based processes. reply caseysoftware 12 hours agoparentprev> The scariest part is that as people start to spend less time developing a skill set, and instead deferring to AI answers, you will cross a point where this problem can't be fixed (because nobody has the skills to fix it and the AI is trained on the outputs of previous generations of humans). The loss of knowledge/skills was a key bit of Foundation which itself was a retelling of the fall of the Roman Empire. As key skills become rarer, the price goes up.. until you can't hire for those skills at any price. reply userulluipeste 9 hours agorootparentBut... if the price goes up, isn't this going to attract people to that domain of skills? For skills to vanish, there has to be more factors at play, like restrictions to dissemination of knowledge, no? reply willsmith72 13 hours agoparentprevDoesn't that assume that people will forever be better learners than AI? If the \"olds\" learnt a skillset at some point, the data they used to learn the skill is presumably available to the AI too. Why can't the AI learn it too? (Not talking about physical labour which clearly has way less potential to be replaced than knowledge work) reply rglover 13 hours agorootparent> Doesn't that assume that people will forever be better learners than AI? Better creators, not learners. AI can't create, it can only remix what's already been produced by humans. Human progress is created, not learned. The olds who are conditioned to try new things when an existing solution doesn't work still have the capacity to create something new (wholly new, not just remixed new). reply overflow897 12 hours agorootparentAlphaGo and AlphaStar both started out based on human training and then played against versions of themselves to go on and create new strategies in their games. Modern LLMs can't learn/experiment as far as I know in exactly the same way but that may not always be true. reply rglover 11 hours agorootparentYeah, but they had a limited set of rules to work within (they were just hyper-efficient at calculating the possible outcomes relative to those rules). Humans, in theory, only have the rules they believe as there technically are no rules (it's all make-believe). For example, what was the \"rule\" that told people to make a wheel? There wasn't one. The human had to think about it/conceive it, which AI can't (and I'd argue never will be able to) without rules. reply imtringued 1 hour agorootparentprevReinforcement learning is a completely different strategy compared to how most LLMs work. reply ildjarn 14 hours agoparentprevThe future was predicted in The Machine Stops and Mockingbird reply dgfitz 13 hours agorootparentI'd argue more of a mix of Brave New World, 1984, and Atlas Shrugged reply mritchie712 14 hours agoparentprevthat'd be WALL-E reply temp0826 13 hours agorootparentI was thinking Idiocracy. Time to use AI to generate enough blogspam about the benefits of watering plants with gatorade to \"poison the well\" of datasets used for future training. reply cjbgkagh 13 hours agorootparentIt’s your well? That sounds like it’ll just result in Gatorade replacing water for crops even sooner. reply mritchie712 13 hours agorootparentprevI like to think we'd snap out of it eventually (like in WALL-E) reply golergka 13 hours agoparentprevI kind of flipped out, and was motivated to finish this blog piece, when I saw this: “UK government wants to use computers to cut civil service jobs: Yes, you read that right.” The idea — to have citizen data might be put into and processed by a computer — is hideously toxic and broken; and usefully reveals the kind of thinking that makes morally crippled leaders all across our system love this technology. reply __loam 13 hours agoparentprevI'm really worried about the implications of this technology for programming, art, and literacy in general. The skills required to be a good programmer or artist are not what's being sampled by these models, merely the output. There's a real danger here of losing these skills if they can no longer be developed professionally, and that even means no more human training data for newer and better models to be trained on. We'd be stuck with whatever trash current models are putting out. reply cjbgkagh 13 hours agorootparentThe competency pipeline issue predates the threat of AI, I do agree that AI will make it much worse as it’s hard to see a payoff on investment into skills of AI is going to replace you anyway. I feel like a wise old sage knowing things that new generations will never learn and may even be lost to humanity. reply netsharc 12 hours agorootparentprevAs a programmer, I hope programming remains \"sacred\", but I can see the flip side: It's a specific way of making a machine do what you want, repeatedly. I can imagine the AI-fuelled world just entering the machine's capabilities/parameters, and asking an LLM in human language to generate code for it. Sure it might not do it in the most elegant or efficient way, but many programmers also use npm. reply pg_1234 12 hours agorootparentIn this AI-fuelled world where everything is run by 2nd rate AI-generated code, talented programmers will be able to hack the whole world. Forget working for anyone, just make the machines do what you want. reply ben_w 12 hours agoparentprev> And this will not just be in government, it will be everywhere. The scariest part is that as people start to spend less time developing a skill set, and instead deferring to AI answers, you will cross a point where this problem can't be fixed (because nobody has the skills to fix it and the AI is trained on the outputs of previous generations of humans). I think that would require AI development to approximately halt at close to the current level for over a lifetime. Conditional on development halting, I'd agree with you. By analogy, there's this single, very useful, very powerful, set of \"hidden methods that can be used to win all games, get rich, find love, determine the limits of thought itself!\" — mathematics[0]. Do people like learning it? They do not. Calculator much easier. What a calculator does is none of that, calculators are merely arithmetic, but most people can't tell the difference between mathematics and arithmetic. I think LLMs have the same effect on anything that can be expressed in words, and all the various image generator models have this effect on graphical arts. One must be extremely motivated to get past the \"but the computer is better than me\" hump. However, I don't expect AI development to even approximately halt at anything close to the current level. There's a lot of room for self-play in domains like maths and computing where the proofs can be verified, and probably a lot of room for anything that can be RLHF'd, too. And that's also assuming we don't get any brain uploads; regardless of the question of \"is such an upload of a human capable of consciousness\", which absolutely matters, it may still be relevant to the economic issues of AI depending on the cost of running one depending on all the details of such an upload that I can't even begin to guess at at this point (last I heard, https://openworm.org was not actually measuring synaptic weights directly, but rather neural activity? I may be out of date, not my field). Whatever happens, however good it does or doesn't get, I do expect something to go very weird before I reach the current state pension age — close enough that, if that something is \"the machines break\" or \"society breaks\", then there will still be plenty who remember the before times. [0] https://www.smbc-comics.com/comic/secrets-2 reply rglover 11 hours agorootparent> I think that would require AI development to approximately halt at close to the current level for over a lifetime. What I'm getting at isn't AI development halting, but human knowledge/creativity halting [1]. Because the AI is and can only be trained on human knowledge, it's knowledge of reality has an upper bound (whereas, theoretically, humans can know anything or make new discoveries that don't exist in our current knowledge set). If you don't tell the AI that strawberries are a thing/reality, it will never conceive of a strawberry on its own. And arguably, it's not fair to call these things \"AI\" until they can do so. [1] Charlie Munger said \"show me the incentives and I'll show you the outcome.\" Well, in this case, the incentives to use AI > spending the time to learn and develop skills. The outcome here is clear: humans will stop producing new knowledge and by extension, the AI will stop receiving new knowledge to learn. reply carlosjobim 12 hours agoparentprevDealing with an AI will be much better than dealing with a civil servant, in most cases. There is a certain kind of person who becomes a civil servant and many of them will not only have a hostile attitude, but also make it their life's mission to try to do as much damage as possible in the pettiest ways possible to the people they \"serve\". Especially if you're of a sex, ethnicity or age group that they hate. Sometimes the same as theirs. Letting citizens deal with their bureaucratic errands with an online form or portal instead of with a civil servant in their office has been an enormous benefit, in the places that offer this. An AI will fuck things up, being an AI, but it will not necessarily treat people with a hostile attitude and lie to clients to spite them. Unless it's programmed by civil servants, that is. reply rglover 11 hours agorootparent> Especially if you're of a sex, ethnicity or age group that they hate. Sometimes the same as theirs. Well, Gemini just proved that if you're white, you're the ethnicity the AI is told to hate. That's the thing, because it can be programmed by humans means at some point, it will be abused to do something nefarious. And because it only knows what humans tell it about reality, it will always \"think\" within the context it's been given (never in the abstract). reply happytiger 13 hours agoparentprevlol. This is how you end up with that scene in idiocracy where you find out you get your college degrees at Costco. reply zoogeny 12 hours agoprevI disagree with most of this article. My own anecdotal experience is that dozens of non-tech friends, coworkers, etc. tell me that they are using ChatGPT every day. These are people who are telling me how they use it to draft emails, create marketing material, create sales support material, create education material, etc. During every other hype boom I have been through that ultimately failed, those regular Joe types either hadn't even heard of the tech, simply didn't care or were actively hostile to it. Comparatively, with the new generative AI people are talking about how much they love it, how they use it every day, etc. Even the Internet had a bubble that popped (back in the Pets.com days, circa 2001 [1]) and this short-term AI bubble will pop too. I expect the same pattern as the early Internet: an early pop followed by a recovery that leads to massive growth. 1. https://en.wikipedia.org/wiki/Dot-com_bubble reply atomicUpdate 10 hours agoparent> My own anecdotal experience is that dozens of non-tech friends, coworkers, etc. tell me that they are using ChatGPT every day. These are people who are telling me how they use it to draft emails, create marketing material, create sales support material, create education material, etc. My reaction when I hear this is that those people are being paid entirely too much money if an LLM can do their job. I think this is where the real economic impact will come from: when managers realize it's just LLMs generating emails to be summarized by LLMs and it's just bots spamming each other with busy work all day. At some point companies will realize it's all pointless and start trimming these pointless jobs, leaving a lot of people without any actual skills. reply hn_throwaway_99 10 hours agorootparent> My reaction when I hear this is that those people are being paid entirely too much money if an LLM can do their job. That feels like such an unnecessarily cynical view to me. First, parent comment didn't say they are using LLMs to \"do their jobs\". Frankly, I feel that if you're a knowledge worker and aren't using LLMs at least part of the time, you're likely being inefficient. E.g. LLMs don't replace my skill as a software developer, but they sure make it faster to learn new libraries/technologies faster. reply dartos 2 hours agorootparentEvery. Single. Time. I have tried to learn a new library with chatgpt it has been wrong and I just went to read the docs myself. I like that it can figure out my boilerplate, but I wouldn’t trust any info it spits out. reply dartos 2 hours agoparentprevWhat in the article did you disagree with? The author said that AI was and has been obviously useful, but there’s a lot of dumb money flying around in AI land (to try building things like AGI) reply netsharc 12 hours agoparentprevI think the pets.com analogy is apt. Everyone's running around making \"AI-powered\" companies (just like everyone was making online shops in the 90s) and clueless investors are throwing money at them because, \"AI!\"... Nvidia is lucky though, a lot of big companies will want their GPTs in-house to ensure their secrets won't be used to train someone else's GPT, and that means buying a lot of hardware (could be on a cloud data center too, but, same result for Nvidia) reply zoogeny 11 hours agorootparentI'm cautious about picking winners and losers specifically. I remember back in the day all of the search engines: AltaVista, Ask Jeeves, Yahoo, Lycos, Web Crawler, Excite, etc. When Google popped up it was clearly superior to all of them and completely changed the landscape. In fact, there are few Internet companies from the early 2000s that made it up until now intact. The same may end up true for this crop of AI. But anyone who was alive during that time and working should ask themselves: would your career have been better or worse if you started getting familiar with Web Technologies in the early 2000s. What if you saw the impending dot com crash and you decided that the entire Internet was not going to live up to the hype? I don't have a crystal ball but my gut is telling me that 20+ years from now we'll see any short-term market correction around AI as a blip. reply rpeden 12 hours agorootparentprevOne thing to consider is that hardware companies that benefit from a boom don't always do so well in the long term. Lots of investors in Nortel and JDS Uniphase learned that the hard way in the early 2000s. reply __loam 11 hours agoparentprevAm I the only one who is aghast at people using these things to write emails, or worse, educational materials? It feels impersonal and shitty to send someone an AI generated email. Furthermore, they lie all the time. Unless you know what you're talking about, there's a large risk to using it as an educational resource. reply prewett 12 hours agoprevI normally think the entire market is overpriced, which I mostly still do, but I'm not convinced that tech stocks are overpriced, at least the ones he referred to. Nvidia has a forward P/E of 32, which is in line with Microsoft (35), Apple (28), Intel (31). Compare to KO (Coca-Cola) which is 22, which gives the 33% tech premium that one of his linked articles notes. But KO is going to grow at the rate of global growth (3 - 5%); I think it is not unreasonable that all of those companies are going to grow at least 33% more than KO. So I don't think there is a bubble in major tech stocks. It is possible that Nvidia will not retain its current sales over the long term, but given that Nvidia cannot satisfy current demand, that seems unlikely in the next couple of years. Training the future models isn't likely to require less compute, and I think there is a reasonable case to be made that people will need domain-specific training for domain-specific ChatGPTs (or whatever the future is). Which means more training. Yes, I think it is way overhyped, but on the other hand, actual people are using ChatGPTs. I've used it for simple code to get started with an unfamiliar (but popular) library. I talked with a non-technical friend recently who was using it for relationship advice (with predictably unhelpful responses, it can't tell you the issues you are unaware of, but still). If there's an AI bubble, it's in the early stages. In my mind the over-priced aspect of the market is the complete denial that stock prices at 5% interest rates should not be higher than at 1%, all things being equal. At least not if value = profit / costOfCapital as it is supposed to. reply gmm1990 12 hours agoparentAfter this last run up in stocks despite high interest rates I've completely given up on trying to value based on any metrics. reply prewett 11 hours agorootparentI gave up on that because the valuation formulae all divide by a small number with large uncertainty. Instead I use dividend yield, P/E, P/B, and P/S, all relative to historical values for that company. reply bloqs 14 hours agoprevYounger people who may not know: Tim Bray was one of the creators of XML. Also a nice guy on Twitter (at least he was a few years ago) he's probably active here too. reply throw0101c 13 hours agoparent> Younger people who may not know: Tim Bray was one of the creators of XML. Also editor of the JSON RFCs: * https://en.wikipedia.org/wiki/Tim_Bray#JSON * https://datatracker.ietf.org/person/tbray@textuality.com reply 4ndrewl 14 hours agoparentprevTim Bray is still a nice guy on Mastodon https://mastodon.me.uk/@timbray@cosocial.ca [Edited - added Tim's name to help future searchers] reply timeagain 14 hours agoparentprevHis blog posts are one of the main things that keep me coming back here. Insightful, and he often is able to put into words things about the industry that I can only feel in the periphery of my heart. reply hn_throwaway_99 13 hours agoprevWhile I agreed with a lot in this post, I'm also pretty wary of the underlying, unstated idea that average investors can avoid bubbles popping while also somehow taking advantage when things go up - that is, he doesn't really say it in so many words, but he's essentially talking about timing the market. I started my tech career around the turn of the century, and made the mistake of putting a ton of money (at least for me, at the time) into Global Crossing. My thought was that while there were all these \"fluffy\" doomed dot coms at the time, Global Crossing had billions in real, physical infrastructure they built. Obviously I didn't quite understand debt at the time, never mind the actual fraud that Global Crossing committed (I remember thinking \"Wow, stocks really can go to zero and never come back.\") Sure, you could argue I made every newbie investor mistake in the book, but the worse consequence for me was that it \"spooked\" me early in my investing career, such that I became very reticent to want to invest in things when I felt they were overvalued. E.g. I was one of those people who thought there was a giant tech bubble when Facebook bought Instagram for a billion dollars - in 2012... So sure, you may think I'm an idiot, but I can quite guarantee I was far from alone. It was only at the point where I really, truly believed \"I'm definitely not smarter than anyone else in the market\" (and hardly anyone is) that I just put my money in index funds, did regular rebalancing, and otherwise forgot about it. We may be in an AI bubble, we may not, but I've seen way too many \"vastly overvalued\" companies continue to be \"vastly overvalued\" for over a decade (and then only briefly coming down before shooting back up again) to think that Tim Bray has any special insight here. reply ytx 12 hours agoparentMy knee-jerk reaction is to point out that \"even if you bought the S&P at the height of the dotcom bubble, the annualized return over the past 24 years was ~5.5% (not including dividends)\" And while I think this line of thinking is still more correct than not, I wonder how much I (and a lot of other folks in the US) are discounting the possibility of a prolonged period without growth. Despite shocks like in 2000 and 2008, the S&P has spent very little time \"underwater\" over the past 50 years. But that's not the case if you look at something like the Nikkei, which took until this year to get back to its 1990 peak. reply hn_throwaway_99 12 hours agorootparentWhether or not they're discounting the possibility of a prolonged period with little growth, the fundamental issue is that this is essentially unknowable, at least to your average investor. The 2 issues I see with this line of thinking (i.e. comparing it to the Nikkei): 1. You wouldn't want to dump 100% of your money in an S&P 500 index fund. There is a reason to diversify. 2. The point of dollar cost averaging is essentially to reduce the risk of dumping all of your money in (or out) at a bad time. Taking your Nikkei example, I'd be curious to see if you looked at, say, investing the same amount of money on the first of the month over a 2 or 3 year period. The amount of time you'd be under water over the past 4 decades would be much less than just looking at any single instance in time. reply ytx 11 hours agorootparentI don't have monthly data, but as an approximation here's a rough test where you make a one time investment of $1000, either all at once, or equally spaced over 2 or 5 years. This is simulated starting at each year from 1985 to 2005, and we count the number of years underwater starting 5 years after the first year (after $1000 has been put in for all 3 \"strategies\") up until 2023. S&P: once dca_2yr dca_5yr count 25.0 25.0 25.0 mean 0.7 0.8 0.7 std 1.8 1.5 1.1 min 0.0 0.0 0.0 max 8.0 6.0 3.0 Nikkei: once dca_2yr dca_5yr count 25.0 25.0 25.0 mean 11.4 11.5 11.8 std 9.1 9.5 9.4 min 0.0 0.0 0.0 max 29.0 29.0 28.0 So investing at once, the max number of years underwater for the S&P was 8, versus 3 when \"dca\"ing over 5 years. The average number of years underwater (averaged over when you would've invested) is quite low, while for the Nikkei all metrics look much worse. reply hn_throwaway_99 10 hours agorootparentThanks! Curious, how/where did you get the data for this analysis? Also, don't know if you included dividend reinvestment - that has a huge overall impact. reply ytx 10 hours agorootparenthttps://www.macrotrends.net/2593/nikkei-225-index-historical... I did not - adding an optimistic 2% dividend didn't change much for the S&P, but slightly reduced the underwater counts for the Nikkei (max 23, average ~7.5) reply dottjt 13 hours agoparentprevI had the exact same experience with Bitcoin back around 2014. Only now just getting back into investing as I realise it's an important thing to do for wealth preservation. I suppose it's not uncommon for people to have this kind of experience, so I'm just glad I had it young. reply 2024throwaway 13 hours agorootparentBitcoin != investing. I say this as someone who lost money in the 12DailyPro / eGold fiasco of 2005/2006. (read: I am very, very dumb.) reply andsoitis 13 hours agoparentprevIt was only at the point where I really, truly believed \"I'm definitely not smarter than anyone else in the market\" (and hardly anyone is) that I just put my money in index funds, did regular rebalancing, and otherwise forgot about it. This is the way. reply lumost 13 hours agorootparentI had this experience with 3D printing. In 2010 Google/Meta were transitioning to Mobile. It was not at all clear at the time that Mobile would turn them into multi-trillion dollar behemoths. To have bet the farm on FANG at the time would have been extreme. Dropping money into an index fund is generally the right way of going about things. My suspicion is that those who talk about clearing N million on NVidia big bets either had enough money that they could go long with 1MM on a single stock with Y thousands or just got very lucky in their first trading experiences. If someone gets 1/100 luck three times in a row - then they can easily get to 1-10MM portfolios from a ~10k starting point. You'd expect around 1 in one million traders to do this. reply patrickdavey 12 hours agoparentprevCan you explain what you mean by rebalancing in the context of index funds? reply Workaccount2 12 hours agorootparentYou chose a portfolio distribution, 75% NASDAQ 25% S&P. After 1 year, you look at your portfolio, and because of market movements, your portfolio is now 81% NASDAQ and 19% S&P. So you sell some NASDAQ and buy some S&P to rebalance to 75% / 25%. Rebalancing can be any mix of securities or assets (or both). You decide how you want your wealth distributed, and you rebalance to stay within those levels. reply hn_throwaway_99 12 hours agorootparentprevSuppose you have an asset allocation strategy that is 25% US stocks, 25% international stocks, 25% real estate (REITs), 25% commodities (I'm not suggesting you do this, but this was the allocation in Roger Gibson's famous multi-asset allocation strategy paper - google it). To implement this you would want to: 1. Choose 4 different funds to represent each of those classes (e.g. an S&P 500 index fund, an MSCI EAFE fund, etc.). You want to be sure to reinvest dividends. 2. On a specific time period (i.e. once a quarter) you rebalance your portfolio - if anything has gone above 25%, you sell it so that you can buy anything that has fallen below 25%. Many investment platforms let you essentially do this automatically these days. reply andruby 11 hours agorootparentprevNot OP. They probably invest in more than one fund, and want to keep the ratio balanced. Eg: 60% MSCI World, 20% Emerging Markets, 10% Tbonds and 10% Gold. Every few months they would check the ratio and “rebalance” reply hiq 12 hours agorootparentprevhttps://www.bogleheads.org/wiki/Rebalancing reply golergka 13 hours agoparentprevI thought that $10 billion valuation of Facebook in 2010 was crazy and sold of my Bitcoins around 2011-2012 because of similar sentiments. And since around the same time I've been reading stories about how tech is in a bubble on front page of HN. reply lotsofpulp 12 hours agorootparentNotice that all these bubble posts never make the following claim: “The price of X index fund/asset/real estate will be lower at future date Y than today”. reply dado3212 14 hours agoprevArchive while this is down - https://web.archive.org/web/20240229191137/https://www.tbray... reply xenonite 13 hours agoparentOver unencrypted HTTP, the server responds well: http://www.tbray.org/ongoing/When/202x/2024/02/25/Money-AI-B... reply baobabKoodaa 13 hours agoparentprevPSA: publish your static blog content on a CDN, not on a $5 VPS reply sodality2 13 hours agorootparentIf it's static it doesn't (shouldn't) matter. I had over a million hits on my $3/mo VPS and it handled the load perfectly fine. reply baobabKoodaa 11 hours agorootparentHow many times do we have to see a blog post hugged to death by HN before you will change your mind? We literally saw it happen with OP just now. reply sodality2 11 hours agorootparentMy point is that it's typically a design choice when choosing something like Wordpress or a dynamic site, versus a static HTML file that's under 100kb. Though in this case the site's resources are under 400KB, so I can't really be sure. reply mattgreenrocks 13 hours agoprevAbsent GenAI, do the BigTechs of the world have enough growth going on elsewhere to appease Wall St volcano gods sufficiently? They all seem to be hyping GenAI a ridiculous amount, prompting this question. And it makes sense for them to ride the hype train and get something out of it. But it also makes me wonder if that only makes the eventual drop even larger. reply sgt101 11 hours agoprevAs ever the truth is in the middle. - LLM's provide functionality that was very difficult to implement until 2 years ago. - We can decode natural language statements relatively well and relatively easily. - We have an approximate common sense knowledge base. - We can encode statements into human readable text flexibly. (this was never so much of a problem as the first two - but it's still useful). But, these are not magic boxes that can tell our fortunes. So we can do good things if we engineer things well, and there is a lot of synergy with other AI tech that's been evolving in the last ten years. STT and object recognition are both very useful, end to end differentiable reasoners are coming in now as well. ML was becoming important in 2019, 2023 created an inflection and some hysteria, but there's substantial value to be had. reply aejm 13 hours agoprev> I dunno if anyone will build an AGI in my lifetime, but I am confident that the task would remain beyond reach without the functions offered by today’s generative models. This. LLMs are not the path to AGI. At best they’re one of many ingredients. reply neilk 13 hours agoprevI mostly agree but I have a few quibbles with some arguments. For instance, Bray considers the adage \"The CIO is the last to know\". From the 90s until now, developers have always snuck new technology in without management approval. You put Apache on a forgotten Linux box in the corner because it's easy and fun, and a few months later the whole company relies on it. Developers are not rushing to deploy skunkworks generative AI solutions, so, the argument goes, probably generative AI isn't that good. There's a couple of problems with this. 1. Not everything that is good can be deployed skunkworks-style. It might be that AI is only really good with incredibly high up-front costs and extremely specialized developers. Like launching a satellite. You can't do it yourself with stuff you have lying around, and even if you had the money to do it you probably don't have the expertise to do it safely. But it's still extremely valuable! 2. Sometimes we are using this technology to hack up solutions to personal problems! I had a video which I wanted my hearing-impaired father to watch. I could have paid a human or AI-powered service to generate subtitles, but I found that I could do it myself with OpenAI's Whisper, on an old laptop, and then munging text files together in the usual way. I was a little shocked that this worked offline. I could have done it on a plane. This absolutely fits into a hacker workflow. reply wrs 12 hours agoparentYeah, it’s like everyone forgot about Whisper because of ChatGPT. A friend had 200 hours of audio of a retiring expert naturalist’s park tours and wanted to publish them so the knowledge wouldn’t be lost. I turned them it all into incredibly accurate text files overnight using just my M1 laptop, for free. That’s crazy. reply sgt101 11 hours agorootparentAlso effective object recognition & OCR. I got asked about onboarding someone to a fund online and it occurred to me to check out current services before responding - 20 seconds with google document AI made it clear to me that things have moved decisively in the last couple of years. reply IanCal 13 hours agoprev> I kind of flipped out, and was motivated to finish this blog piece, when I saw this: “UK government wants to use AI to cut civil service jobs: Yes, you read that right.” The idea — to have citizen input processed and responded to by an LLM — That's not what the article says, it's about processing responses not responding to people. I don't think there's anything about responding to citizens. And it also doesn't say LLM it says AI. reply frereubu 13 hours agoparentHe's talking about this article: https://www.engadget.com/uk-government-wants-to-use-ai-to-cu... reply IanCal 2 hours agorootparentI know. It doesn't talk about responding to citizens as far as I can tell, can you quote where it does? reply sgt101 11 hours agoparentprevThe UK gov is probably talking about using embeddings to respond to FOI requests. 1000000 documents -> 1000000000 embeddings Citizen question -> GPT normalised question -> 'embeddings match 'embeddingsembeddings recover document fragments with matched embeddings use GPT to create answer from document fragments The question is how successfully this process creates the answers required. Who knows? But, I would not be surprised if it worked pretty well and it might boost productivity to the point where there's a massive saving to be had. Maybe - but that's the fun! reply IanCal 1 hour agorootparentI doubt it. The article mentions generating answers based on sources like Hansard, and dealing with large numbers of consultation responses. Frankly it's shocking they haven't done any ai stuff with consultation responses, lots of freeform text is where you usually want to start doing clustering and analysis. reply INGELRII 14 hours agoprevAswath Damodaran's Nvidia analsysis: Assuming 32% CAGR rate of over 5 years with 40% target operating margin at the end of period Nvidia is now 40% overvalued. https://aswathdamodaran.blogspot.com/2024/02/the-seven-samur... reply lootsauce 14 hours agoparentLogical yet, how overvalued did Tesla get? reply not2b 14 hours agorootparentAs the old saying goes, the market can stay irrational longer than you can stay liquid. So shorting a clearly overvalued stock can be a dangerous move. reply AlbertCory 14 hours agorootparentOops. We posted that at the same time. Short Nvidia only if you hedge with some call options. Or other financial instrument that mitigates your losses. reply AlbertCory 14 hours agorootparentprevthe market can stay irrational longer than you can stay solvent. reply Swizec 14 hours agorootparentprevIn a world where seed and A stage AI startups are getting $100mm rounds and half that money's going to NVIDIA ... eh it's probably not too overvalued. I think there's more oomph left in the bubble. reply bombcar 13 hours agorootparentWhen the valuation is something like \"it looks like it's assuming they'll continue to sell $X billion a year\" it can be reasonablish. When the valuation requires \"they will continue to grow sales X% a year\" is when it quickly becomes impossible, and for a much smaller X than you might realize. reply __loam 11 hours agorootparentprevEven if you do think AI is a bubble, Nvidia's capacity is completely booked for a few years or so if I'm not mistaken. And that's with huge margins. reply bloodyplonker22 13 hours agoparentprevAswath Damodaran looks at valuations with a very one-dimensional lens because he is not an innovator, engineer, or even a business person. Ever since Amazon was in its early days, he has said that Amazon was overvalued and he has always been wrong because Amazon has always found new verticals to build and create more value with. reply INGELRII 12 hours agorootparentDamodaran makes detailed analysis with assumptions in the open. Put your numbers to the Exel sheet and calculate valuations using your own numbers. >Ever since Amazon was in its early days, he has said that Amazon was overvalued and he has always been wrong because Amazon has always found new verticals to build and create more value with. Amazon has been overvalued multiple times. Amazon stock had negative return 10 years between 1999 - 2009. reply woobar 11 hours agorootparent> Amazon stock had negative return 10 years between 1999 - 2009. It did not. You will have to cherry pick very specific days in this time frame (top of the dot com bubble and bottom of the GFC) to get negative returns. But how about 1998-2008 or 2000-2010? Here is how $10K invested in AMZN performed in 10 years [1]: 1998 - 2008 $102,134 1999 - 2009 $25,124 2000 - 2010 $23,625 2001 - 2011 $111,229 [1] https://www.portfoliovisualizer.com/backtest-portfolio?s=y&s... reply lotsofpulp 12 hours agorootparentprevhttps://dqydj.com/stock-return-calculator/ Jan 5, 1999 to Dec 28, 2009, AMZN had an 8.9% annual return. Jan 5, 1999 to Dec 28, 2008 was -0.52% annual return. Jan 5, 2000 to Dec 28, 2009 was 6.88% annual return. But why give a crap about returns during a specific 10 year period? Almost nobody is buying something today to liquidate all of it at a single point in time in the future. reply xetplan 9 hours agorootparentprevThis is seriously might be the worst post I have ever read online. To say it is clueless would be too nice. reply airstrike 13 hours agoparentprev> Last summer, when I valued Nvidia in this post, I found it over valued at a price of $450, and sold half my holdings, choosing to hold the other half. Now that the price has hit $680, I plan to repeat that process, and sell half of my remaining holdings. Well, it looks like he lost a lot of money reply msoad 13 hours agorootparentHe bought it before me at least, so he *made* money. Doesn't matter if didn't predicted the top. He saw a stock that he thought will increase in value, bought some and sold at a higher price. That's what it matters reply Ekaros 13 hours agorootparentprevSo if someone buys a stock and then that stocks goes bankrupt. By that logic they lost no money... Taking money off the table when it is enough for you is the way you guarantee that you make any money... reply airstrike 12 hours agorootparentyeah, but he could have waited a little bit to see signs of a slowdown instead of taking such a radical contrarian view and leaving ~80% upside on the table. better to risk it going down from $450 to $400 before selling than to miss out on the ride to $800 reply OtherShrezzing 13 hours agorootparentprevI hope in both instances the author at least held out the extra couple of weeks until earnings day. reply smeeth 13 hours agorootparentprevNope. Failure to maximize returns is not losing money. reply globular-toast 13 hours agorootparentprevYou can't lose what you never had. reply debacle 14 hours agoprevIs there a resource I can use for understanding how \"dumb money\" impacts the markets? reply lbotos 13 hours agoparentlots of articles talking about \"index investing/passive investing market impacts\" should get you what you are looking for. reply throw0101c 13 hours agoprevJust because some technology may end up changing the world, it does not necessarily mean that it is a good investment: * https://en.wikipedia.org/wiki/Technological_Revolutions_and_... * Via Ben Felix: https://www.pwlcapital.com/investing-technological-revolutio... reply glitchc 13 hours agoprev> Produce plausible output That's basically art. So AI's only really good at producing art. So we're safe... but now I feel bad for the artists. reply frereubu 13 hours agoparentDepends on your audience. One of the things that stuck in my mind from my sculpture degree was a tutor's contention that plenty of (visual) artists produce \"things that look like art\" but aren't worthy of the name. Subjective, yes, but I knew exactly what she meant. The current generation of AI by definition only produces entirely derivative works. All artists are influenced to some extent by their visual history, whether they like it or not, but there are leaps that some of them make which I don't think anything which works solely on previously-generated work could make, and those tend to be the most interesting. (Although I'd agree that artists who really lean into derivation, like Andy Warhol or Hans Haacke, are also interesting). reply benced 12 hours agoprev> As bad as 2008? Nobody knows, but it wouldn’t surprise me. The recency of 2008 has really warped people's brains. 2008 was the 2nd worst financial crisis of all time (maybe it would have been the worst if our fiscal and monetary tools were still at 1929 levels of sophistication). You should be extremely hesitant to declare that anything will even come close to it. reply duderific 12 hours agoparentPlus, that was not a stock bubble, that was a bad loans bubble. Real people borrowed trillions of dollars they then couldn't pay back. This is a completely different phenomenon. Not saying this current bubble won't pop eventually, but the scale would be on a completely different order of magnitude. It's not likely that 170-year-old banks will collapse and be sold off for pennies on the dollar (Lehman Brothers) if this bubble ends. reply andsoitis 13 hours agoprev> but there are just way more ways for things to go wrong than right in the immediate future That is always the case so I wouldn’t over index on that. reply elijahbenizzy 14 hours agoprevThere's an argument that the AI tech bubble is just a continuation from the value-add of automating everything and the productivity/GDP-increase that causes. It is just that investors are so skittish that they'll only let the floodgates open if its \"sexy\", so AI comes in and restarts the money machine. reply bombcar 13 hours agoparentWhat else are investors going to do, sell stocks and buy bonds? reply __loam 13 hours agoparentprevIs that why profitable companies like Google are laying off thousands of workers? reply pyrrhotech 13 hours agoprevIt's obvious that bubbles exist in retrospect, but determining whether current growth and valuations are sustainable in the present is incredibly difficult. As another poster mentioned, we are essentially talking about market timing here. Most investors have been conditioned by many popular talking heads to immediately dismiss the idea of successful market timing - and for the most part, the talking heads are correct. For the average investor, successful market timing is nearly impossible. However, we have many counter-examples of successful market timers over the long term. James Simons' Medallion fund has returned 50%+ CAGR over a multi-decade period and stomping the market, creating many centimillionaires and billionaires in the process. I set out thinking, what's so different about Simons and his crew at RenTec? Why is it so difficult for their success to be replicated? Not one to easily back down from a challenge, I began working on my own algorithms to successfully hedge against market downturns and provide superior absolute and risk-adjusted returns compared to the S&P 500. While I haven't yet seen Simons-level success in live trading, since launching Grizzly Bulls (https://grizzlybulls.com) in January 2022, 6 of our 7 models have outperformed the market on an unleveraged basis: SPX (benchmark): +7% VIX-TA-Macro-MP Extreme: +39.98% VIX-TA-Macro Advanced: +34.38% VIX-TA Advanced: +12.92% VIX Advanced: +9.91% Vix Basic: +5.76% TA - Mean Reversion: +15.46% TA - Trend: +12.97% Of course two years of outperformance also doesn't yet stand the test of time of Simons' remarkable run, but I'm confident that we've discovered alpha here. reply SirLJ 12 hours agoparentThis is great, I am also running my own AI investment robots and this is the future and I believe you can absolutely beat the market and even thinking to use similar approach to other structured data sets and create a startup around the idea... “If you don't find a way to make money while you sleep, you will work until you die.” ― Warren Buffett reply PheonixPharts 13 hours agoprev> Things have been too good for too long in InvestorWorld That's not by accident and it's been at the expense of non-investor world for a long time. What we're seeing is finance capitalism suck surplus value from every piece of the Earth it can. We're still burning more fossil fuels than ever before [0] despite the now visible risk of climate catastrophe. I believe we're likely to see investors continue to become irrationally wealthy while increasing larger and larger pools of people a driven into ever increasing situations of hardship. I see no signs of the madness stopping until both human and planetary resources start to buckle under the pressure and refuse to give yields they once did. The article repeatedly mentions crypto as though it were an obvious bubble, but bitcoin is near record highs and even Sam Altman's bizarre world coin is at extreme record highs, COIN is up 200% in the past year. The bubble won't \"burst\", it's just that increasingly less people will be invited in. 0. https://ourworldindata.org/fossil-fuels reply TapWaterBandit 12 hours agoparentAt this point crypto clearly isn't a bubble. And why would it be? Cryptographically secured money has some advantages over fiat money. This doesn't mean fiat will disappear and all crypto will succeed. But most fiat currencies haven't really \"succeeded\" either. Money is a very \"efficient\" market in this sense because each individual can decide do you want to hold currency X or currency Y or currency Z? And there are pros and cons to each that lead to price discovery between currencies. And I think as time goes on it is becoming more and more apparent that many central banks do not take the management of their countries currency seriously and so people choose alternatives. This is what well-functioning markets look like. Consumer choice and incentives. reply cheema33 2 hours agorootparent> At this point crypto clearly isn't a bubble. And why would it be? Cryptographically secured money has some advantages over fiat money. We've had crypto for how many years? More than a decade, yes? Can it be used as replacement for money yet? Anywhere on the planet? Does it look like it ever will? reply netsharc 12 hours agoparentprevYeah, can't imagine the gigatons of CO2 needed to produce CPU/GPU chips (and other hardware) as well as to run these machines, and for what, so some people have to work less generating a document or applying effects to a movie, or to answer a customer's FAQ, or so you can concentrate less while driving... It really is like that paperclip game where at the beginning you can click once to get 1 paperclip and at the end you're using the resources of a galaxy to generate gazillion paperclips, except on this planet the number we're all irrationally obsessed with wanting to make go up is our bank account total. reply jgalt212 13 hours agoprevWith the socialization of risk there is no good reason to not try to participate in bubbles--as longs you're not the first one rolled. reply zooq_ai 13 hours agoprevIf Tim Bray weren't a renowned progressive / socialist, I would have listened to him about Bubbles and Investing. But he comes from a very liberal skeptical engineer and is always wrong. (note the use of right-wing used for Tax cuts. is Tax breaks that extreme of a position? It is for socialists / progressives). Anyway, here's to think about AI. Intelligence is the most precious commodity of all of humanity. Intelligence captured in bits is easily distributed and scaled. We pay $1000 / hour for intelligent agents and it's easy to see, how a super intelligent system can capture 50% margins on that. (Volume of Data and Proprietary hooks will make switching difficult). But, wait, there is more. Intelligence begets more Intelligence. Every artifact an AI produces, we need more AI to maintain, enhance, distribute it. So, for the first time we have an entity whose demand creates it's own demand spiraling into a vicious positive growth rate. All this means is our AI demand may at 0.000001% of what the demand in 20 years would look like, which makes AI enablers incredibly cheap. I could be wrong, but to dismiss the possibility is exactly what's wrong with today's \"skeptical liberal (with a decel mindset) engineer's\" framework / vision. Build your own mental model reply nkohari 13 hours agoprevI think most people (including many working in AI!) would agree that AI is currently at the peak of the hype cycle and there will be a bloodletting at some point. But I don't really understand how AI being hyped, and NVIDIA's stock being overvalued by extension, could result in a 2008-like market crash. reply Ekaros 13 hours agoparentTech stocks are responsible for most of the gains in stock market in past year. Nvidia is alone responsible for 28%... If that just doesn't seem unsustainable I don't think what is... AI is not pulling up traditional firms just a very small number of tech stocks. So with how much is concentrated on a few tech stocks, downturn in tech could lead to significant correction. reply nkohari 12 hours agorootparentA stock market correction, sure. The market ebbs and flows, though. What happened in 2008 was a catastrophe that caused a multiyear worldwide recession. It's called the Global Financial Crisis for a reason. reply VirusNewbie 13 hours agorootparentprev>Tech stocks are responsible for most of the gains in stock market in past year. Ok, but none of the major tech companies other than Nvidia are AI companies. Sure, some of the pop to MSFT's stock is probably because of the OpenAI deal, AWS, GCP, and Azure is riding some of the wave of new AI investment money coming in, but none of them are first and foremost AI companies selling AI. reply htormey 11 hours agoprevTo summarize what I think the author is trying to say with this article: 1) The stock market is in a bubble due to a decade of low interest rates and tax slashing by “right wing” governments. 2) Big tech in particular has been doing well but this is not sustainable. 3) AI is in a bubble. People are pinning their hopes on it to keep tech and I presume big tech growing. 4) A bunch of references to academic papers from 2000 about why AI is hard. 5) Gen AI requires a lot of compute which generates a lot of carbon and is bad for the environment. Thus his statement: “ I think I’m probably going to lose quite a lot of money in the next year or two. It’s partly AI’s fault, but not mostly. ” Which I disagree with. Because A) I think in the long term (5+ years) the investment in AI will be a positive ROI. B) if the stock market crashes in the short term it’s likely going to be for non AI reasons. 3) His arguments as to why AI isn’t going to pan out long term are a bit weak. Having lived in the Bay Area for over 13 year's, I’ve seen a few cycles: social, mobile, cloud, gig economy etc. The cycle pattern is always the same: a) a big new exciting tech idea comes along. b) investors pile in money. c) 95% or more of the companies they invest in go bust and if the space has legs some companies do really well. How is this any different with the current wave of AI companies? Today the big winners in AI are the incumbents, some examples: Microsoft: is making money being the hyperscaler of choice for AI companies (on prem ChatGPT, mistral, etc), it’s co pilot lines and enterprise subscription products. Nvidia is making bank being the current standard on which all of these companies run their models. They have some recent competition from Groq but are still likely going to be crushing it for the next year or two. Mainly due to precommits from the hyperscaleralers. Meta: seem to have been able to leverage AI to claw back advertising revenue due to Apples crack down by improving targeting. As someone who has raised venture capital to do an AI startup I’d say yes there is a lot of hype in this space. Yes a lot of these startups are going to go out of business but it’s also early days. I also think working AI into this poorly written article about how the stock market is going to crash is a bit of stretch. I’m concerned about a market crash myself but I am more worried about it being caused by a combo of a) the upcoming US election. B) the war in the Ukraine. C) conflict with Iran. D) interest rates in the USA being high. reply ahnick 13 hours agoprev [–] Tim writes a post about a \"Money Bubble\". There is now an alternative form of money that anyone can buy through their 401k if you believe there is a money bubble. He dismisses?(or doesn't even consider it?) since it uses a blockchain, because that's a dirty word in the cloud/SaaS tech circles. sigh reply iydhjdfjtg 13 hours agoparent [–] Money buys things. Cryptocurrency does not. Some may use it as a store of value but it’s not money any more than gold or silver is. reply Etheryte 13 hours agorootparent [–] I'm not sure if this argument makes sense today. You can buy many things using cryptocurrency, from food to cars to houses. reply cheema33 2 hours agorootparent [–] Other than a few token places that you might see on the news, crypto is not really used as money. And never will be. Transaction fees. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article raises concerns about a possible financial bubble burst in AI and tech sectors due to overvaluation and lack of comprehension, reminiscent of the 2008 crisis.",
      "It highlights the benefits of AI while cautioning against excessive investments and discusses challenges in teaching machines human language, along with worries about environmental and financial implications of generative AI.",
      "Companies such as Nvidia are profiting but facing resistance, sparking concerns about job repercussions and society's heavy reliance on AI, emphasizing the need to adapt to new tech cautiously."
    ],
    "commentSummary": [
      "The discussion revolves around the utilization of AI in government services, including its impact on civil service employment and societal consequences.",
      "Participants express concerns about job displacement, limitations, and risks of AI, while noting the accelerating development of AI technology.",
      "Topics cover the potential implications of AI on various industries, investing strategies, market timing, and the environmental impact of AI technology."
    ],
    "points": 181,
    "commentCount": 206,
    "retryCount": 0,
    "time": 1709234256
  }
]
