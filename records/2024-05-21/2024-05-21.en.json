[
  {
    "id": 40421225,
    "title": "Scarlett Johansson Addresses OpenAI \"Sky\" Voice Controversy",
    "originLink": "https://twitter.com/BobbyAllyn/status/1792679435701014908",
    "originBody": "Statement from Scarlett Johansson on the OpenAI situation. Wow: pic.twitter.com/8ibMeLfqP8— Bobby Allyn (@BobbyAllyn) May 20, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40421225",
    "commentBody": "Statement from Scarlett Johansson on the OpenAI \"Sky\" voice (twitter.com/bobbyallyn)1398 points by mjcl 20 hours agohidepastfavorite942 comments anon373839 20 hours agoWell, that statement lays out a damning timeline: - OpenAI approached Scarlett last fall, and she refused. - Two days before the GPT-4o launch, they contacted her agent and asked that she reconsider. (Two days! This means they already had everything they needed to ship the product with Scarlett’s cloned voice.) - Not receiving a response, OpenAI demos the product anyway, with Sam tweeting “her” in reference to Scarlett’s film. - When Scarlett’s counsel asked for an explanation of how the “Sky” voice was created, OpenAI yanked the voice from their product line. Perhaps Sam’s next tweet should read “red-handed”. reply nickthegreek 19 hours agoparentThis statement from scarlet really changed my perspective. I use and loved the Sky voice and I did feel it sounded a little like her, but moreover it was the best of their voice offerings. I was mad when they removed it. But now I’m mad it was ever there to begin with. This timeline makes it clear that this wasn’t a coincidence and maybe not even a hiring of an impressionist (which is where things get a little more wishy washy for me). reply windexh8er 16 hours agorootparentThe thing about the situation is that Altman is willing to lie and steal a celebrity's voice for use in ChatGPT. What he did, the timeline, everything - is sleazy if, in fact, that's the story. The really concerning part here is that Altman is, and wants to be, a large part of AI regulation [0]. Quite the public contradiction. [0] https://www.businessinsider.com/sam-altman-openai-artificial... reply trustno2 12 hours agorootparentAltman wants to be a part of AI regulation in the same way Bankman Fried wanted to be a part of cryptocurrency regulation. reply gds44 12 hours agorootparentWhats really interesting about our timeline is when you look at the history of market capture in Big Oil, Telco, Pharma, Real Estate, Banks, Tobacco etc all the lobbying, bribing, competition killing used to be done behind the scenes within elite circles. The public hardly heard from or saw the mgmt of these firm in media until shit hit the fan. Today it feels like managment is in the media every 3 hours trying to capture attention of prospective customers, investors, employees etc or they loose out to whoever is out there capturing more attention. So false and condradictory signalling is easy to see. Hopefully out of all this chaos we get a better class of leaders not a better class of panderers. reply hoseja 10 hours agorootparentSo great to have twitter so the narcissistic psychopaths can't resist revealing themselves for clout. reply ToucanLoucan 2 hours agorootparentI mean, to whatever extent it matters. All these outrageously rich morons still have tons of economic and social clout. They still have pages upon pages of fans foaming at the mouth for the opportunity to harass people asking basic questions. They still carry undue influence in our society and in our industry no matter how many times they are \"outed.\" What does being outed even mean anymore? It's just free advertising from all the outlets that feel they can derive revenue off your name being in their headlines. Nothing happens to them. SBF and Holmes being the notable exceptions, but that's because they stole from rich people. reply dqft 4 hours agorootparentprev(AI)tman tries to be, (Bank)man fried to be, who is letting Kojima name all these villians? reply askl 10 hours agorootparentprevI always had trouble telling apart those two Sams. Turns out they're the same person. reply ocodo 15 hours agorootparentprevAltman has proven time and again that he is little more than a huckster wrt technology, and in business he is a stone cold shark. Conman plain and simple. reply lawn 14 hours agorootparentYou'd think that Worldcoin would be enough proof of what he is but I guess people missed that memo. reply Intralexical 46 minutes agorootparentI think people tend to assume our own values and experiences have some degree of being universal. So scammers see other scammers, and they just think there's nothing wrong with it. While normal people who act in good faith see scammers, and instinctively think that there must be a good reason for it, even (or especially!) if it looks sketchy. I think this happens a lot. Not just with Altman, though that is a prominent currently ongoing example. Protecting yourself from dark triad type personalities means you need to be able to understand a worldview and system of values and axioms that is completely different from yours, which is… difficult. …There's always that impulse to assume good faith and rationalize the behavior based on your own values. reply ben_w 12 hours agorootparentprevMuch as I dislike crypto, that's more of \"having no sense of other people's privacy\" (and hubris) than general scamminess. It's a Musk-error not an SBF-error. (Of course, I do realise many will say all three are the same, but I think it's worth separating the types of mistakes everyone makes, because everyone makes mistakes, and only two of these three also did useful things). reply lawn 12 hours agorootparentIt's not just about privacy either. Worldcoin is centrally controlled making it a classic \"scam coin\". Decentralization is the _only_ unique thing about cryptocurrencies, when you abandon decentralization all that's left is general scamminess. (Yes, there's nuance to decentralization too but that's not what's going on with Worldcoin.) reply ben_w 10 hours agorootparentTrue decentralisation is part of the problem with cryptocurrencies and why they can't work the way the advocates want them to. Decentralisation allows trust-less assurance that money is sent, it's just that's not useful because the goods or services for which the money is transferred still need either trust or a centralised system that can undo the transaction because fraud happened. That's where smart contracts come in, which I also think are a terrible idea, but do at least deserve a \"you tried!\" badge, because they're as dumb as saying \"I will write bug-free code\" rather than as dumb as \"let's build a Dyson swarm to mine exactly the same amount of cryptocurrency as we would have if we did nothing\". reply lawn 8 hours agorootparent> Decentralisation allows trust-less assurance that money is sent That is indeed something it does. But it also gives you the assurance that a single entity can't print unlimited money out of thin air, which is the case with a centrally controlled currency like Worldcoin. They can just shrug their shoulders and claim that all that money is for the poor and gullible Africans that had their eyeballs scanned. reply ben_w 8 hours agorootparent> But it also gives you the assurance that a single entity can't print unlimited money out of thin air, which is the case with a centrally controlled currency like Worldcoin. Sure, but the inability to do that when needed is also a bad thing. Also, single world currencies are (currently) a bad thing, because when your bit of the world needs to devalue its currency is generally different to when mine needs to do that. But this is why economics is its own specialty and not something that software nerds should jump into like our example with numbers counts for much :D reply bavell 4 hours agorootparent> Sure, but the inability to do that when needed is also a bad thing. When and why would BTC or ETH need to print unlimited money and devalue themselves? reply ben_w 4 hours agorootparentWrong framing, currencies don't have agency. You should be asking when would you need your currency to be devalued, regardless of what it's called or made from. And the answer to that is all the reasons governments do just that, except for the times where the government is being particularly stupid and doing hyperinflation. reply bavell 4 hours agorootparentNot a very convincing answer at all. reply freejazz 7 minutes agorootparentConvincing to who? It's not like crypto is widely used as currency anywhere ben_w 4 hours agorootparentprevWhat would a convincing answer look like? reply pwdisswordfishc 12 hours agorootparentprev> that's more of \"having no sense of other people's privacy\" Sufficiently advanced incompetence is indistinguishable from malice. reply ben_w 10 hours agorootparentIt's not particularly advanced, it's the same thing that means the supermajority of websites have opted for \"click here to consent to our 1200 partners processing everything you do on our website\" rather than \"why do we need 1200 partners anyway?\" It's still bad, don't get be wrong, it's just something I can distinguish. reply Intralexical 1 hour agorootparentIf it fools billions of people and does significant damage to the lives of people, then it's plenty advanced to me, even if it happens through a more simple or savant-like process than something that looks obviously deliberate. I don't think the cookies thing is a good example. That's passive incompetence, to avoid the work of changing their business models. Altman actively does more work to erode people's rights. > It's still bad, don't get be wrong, it's just something I can distinguish. Can you? Plausible deniability is one of the first things in any malicious actor's playbook. \"I meant well…\" If there's no way to know, then you can only assess the pattern of behavior. But realistically, nobody sapient accidentally spends multiple years building elaborate systems for laundering other people's IP, privacy, and likeness, and accidentally continues when they are made aware of the harms and explicitly asked multiple times to stop… reply latexr 6 hours agorootparentprev> that's more of \"having no sense of other people's privacy\" (and hubris) than general scamminess. It’s both. https://news.ycombinator.com/item?id=40427454 reply JoRyGu 13 hours agorootparentprevBecause of course he's got a crypto grift going. Shocking. reply wraptile 15 hours agorootparentprevNot going to lie, he had me. He appeared very genuine and fair in almost all media he appeared like podcasts but many of his actions are just so hard to justify. reply svachalek 14 hours agorootparentHe has a certain charm and seeming sincerity when he talks. But the more I see of him, the more disturbing I find him -- he combines the Mark Zuckerberg stare with the Elizabeth Holmes vocal fry. reply polotics 12 hours agorootparentDo you have a link to a video of Altman's voice shifting from controlled deep to nasal? The videos of Elizabeth Holmes not being able to keep up with the faked deep tone are textbook-worthy... reply jesterson 12 hours agorootparentprevso all psychopaths do, aren't they? reply johnnyanmac 12 hours agorootparentCEO's have been studied to have a disproportionately higher rate of psychopathy. So there's a little correlation. You don't get to the top of a company in this kind of society without having some inherent charm (assuming you aren't simply inheriting billions from a previous generation). reply cutemonster 18 minutes agorootparentprevNot the physically violent ones with impulse control problems. reply kristiandupont 13 hours agorootparentprevI have exactly the same feeling as I think you do. When you reach the levels of success he has, there will always be people screaming that you are incompetent, evil and every other negative adjective under the sun. But he genuinely seemed to care about doing the right thing. But this is just so lacking of basic morals that I have to conclude that I was wrong, at least to an extent. reply wraptile 12 hours agorootparentI feel that this is a classic tale of success getting to you. It almost feels like it's impossible to be successful at this level and remain true. At least, I hadn't seen it yet. reply Intralexical 1 hour agorootparentOr \"success\" itself acts as a filter selecting for those who are ruthless enough to do amoral and immoral things. reply gdilla 3 hours agorootparentprevHe just, to his credit, understands his public persona has to be the non-douchy-tech-bro and the media will eat it up. Much like a politician. He doesn't want to be like Elon or Travis K in public (though he probably agrees with them more than his public persona would imply). reply JohnFen 2 hours agorootparentprevThis is why it's a mistake to go by \"vibes\" of a person when they're speaking to an audience. Pay attention to what they do, not what they say. reply m000 9 hours agorootparentprevAspiring technofeudalist. reply firebirdn99 4 hours agorootparentprevalmost every tech ceo is like that. Could list many examples. It's an effect of capitalism. reply mlindner 11 hours agorootparentprevI'm glad more people are thinking this. It's amazing that he got his way back into OpenAI somehow. I said as much that he shouldn't go back to OpenAI and got downvotes universally both here and on reddit. reply vasilipupkin 15 hours agorootparentprevif this account is true, Sam Altman is a deeply unethical human being. Given that he doesn't bring any technical know how to building of AGI, I just don't see the reason to have such a person in charge here. The new board should act. reply insane_dreamer 14 hours agorootparentI thought we had already established this when the previous board tried to oust him for failing to stick to OpenAI’s charter. This is just further confirmation. > The new board should act You mean like the last board tried? Besides the board was picked to be on Altman’s side. The independent members were forced out. reply silver_silver 14 hours agorootparentprevIt shouldn’t be forgotten that his sister has publicly accused him and his brother of sexually abusing her as a child. reply verisimi 12 hours agorootparentI didn't know about that, strange: https://www.lesswrong.com/posts/QDczBduZorG4dxZiW/sam-altman... reply lrvick 11 hours agorootparent\"Some commenters on Hacker News claim that a post regarding Annie's claims that Sam sexually assaulted her at age 4 has been being repeatedly removed.\" Whelp. Let us see if this one sticks. reply imjonse 14 hours agorootparentprevHe rubs elbows with very powerful people including CEOs, heads of state and sheiks. They probably want 'one of them' in charge of the company that has the best chances of getting close to AGI. So it's not his technical chops and not even 'vision' in the Jobs sense that keeps him there. reply dontupvoteme 7 hours agorootparentAre they really the ones with the best chance now though? They're basically owned by Microsoft, they're bleeding tech/ethnical talent and credibility, and most importantly Microsoft Research itself is no slouch (especially post-Deepmind poaching) - things like Phi are breaking ground on planets that openai hasn't even touched. At this point I'm thinking they're destined to become nothing but a premium marketing brand for Microsoft's technology. reply ornornor 15 hours agorootparentprevHe has “The Vision”… It’s the modern entrepreneurship trope that lowly engineers won’t achieve anything if they weren’t rallied by a demi-god who has “The Vision” and makes it all happen. reply parpfish 13 hours agorootparentI roll my eyes when somebody says that they’re “the idea person” or that they have “the vision”. I’d wager that most senior+ engineers or product people also have equally compelling “the vision”s. The difference is that they need to do actual work all day so they don’t get to sit around pontificating. reply JohnFen 2 hours agorootparentIdeas are a dime a dozen. The value of \"idea men\" isn't their ideas, it's their ability to rally people around them. It's the exact same skill that con men use for nefarious purposes. reply azinman2 15 hours agorootparentprevProbably not wrong. Lots and lots of examples of that being true. reply safety1st 12 hours agorootparentThere is something to it. Someone has to identify the intersection between what the engineering can do and what the market actually wants, then articulate that to a broad enough audience. Engineers constantly undervalue this very fuzzy and very human centric part of the work. I don't think the issue is that Vision doesn't matter. I think the issue is Sam doesn't have it. Like Gates and Jobs had clear, well defined visions for how the PC was going to change the world, then rallied engineering talent around them and turned those into reality, that's how their billions and those lasting empires were born. Maybe someone like Elon Musk is a contemporary example. Just don't see anything like that from SamA, we see him in the media, talking a lot about AI, rubbing shoulders with power brokers, being cutthroat, but where's the vision of a better future? And if he comes up with one does he really understand the engineering well enough to ground it in reality? reply azinman2 3 hours agorootparentI don’t know enough about him or his vision. It doesn’t seem he’s as clear as say Jobs in the past. But I do look at all the amazing things openai has done in a short period of time, and that the employees overwhelmingly backed him with the whole board chaos issue. He also has fundraised a lot money for the company. It appears he’s doing more right than wrong, and openai pulled everyone else’s pants down. reply latexr 6 hours agorootparentprev> if this account is true, Sam Altman is a deeply unethical human being. This isn’t even close to the most unethical thing he has done. This is peanuts compared to the Worldcoin scam. https://news.ycombinator.com/item?id=40427454 reply jcranmer 14 hours agorootparentprevI mean, there's already been some yellow flags with Altman already. He founded Worldcoin, whose plan is to airdrop free money in exchange for retinal scans. And the board of OpenAI fired him for (if I've got this right) lying to the board about conversations he'd had with individual board members. reply JohnFen 2 hours agorootparentWorldCoin is how I first heard of him, and it's what made me think he was a bad actor. I think of it as a red flag, not yellow. reply serial_dev 12 hours agorootparentprevHe must be bringing something to the table as they tried to get rid of him and failed spectacularly. Business is not only about technical know how. reply surfingdino 12 hours agorootparentMicrosoft. They are protecting their investment. reply gdilla 3 hours agorootparentprevbecause so many people ran cover for him, from paul graham to whos-who of silicon valley. reply chx 10 hours agorootparentprevAltman is a known conman. Surely you are aware of Yishan Wong describing how Sam Altman and the Reddit founders conned Conde Nast https://reddit.com/r/AskReddit/comments/3cs78i/whats_the_bes... reply sirsinsalot 9 hours agorootparentWow, Altman in the replies there: > Cool story bro. > Except I could never have predicted the part where you resigned on the spot :) > Other than that, child's play for me. >Thanks for the help. I mean, thanks for your service as CEO. reply xinayder 10 hours agorootparentprev> The thing about the situation is that Altman is willing to lie and steal a celebrity's voice for use in ChatGPT. What he did, the timeline, everything - is sleazy if, in fact, that's the story. Correcting, the thing about this whole situation with OpenAI is they are willing to steal everything for use in ChatGPT. They trained their model with copyrighted data and for some reason they won't delete the millions of protected data they used to train the AI model. reply JohnFen 2 hours agorootparentUsing other people data for training without their permission is the \"original sin\" of LLMs[1]. That will, at best, be a shadow over the entire field for an extremely long time. [1] Just to head off people saying that such a use is not a copyright violation -- I'm not saying it is. I'm just saying that it's extremely sketchy and, in my view, ethically unsupportable. reply akudha 10 hours agorootparentprevWhat is so special about her voice? They could’ve found a college student with a sweet voice and offered to pay her tuition in exchange for using her voice, no? Or a voice actor? Why be cartoonishly stupid and cartoonishly arsehole and steal a celebrity’s voice? Did he think Scarlett won’t find out? Or object? I don’t understand these rich people. Is it their hobby to be a dick to as many people as they can, for no reason other than their amusement? Just plain weirdos reply meat_machine 10 hours agorootparentScarlett voiced Samantha, an AI in the movie \"Her\" Considering the movie's 11 years old, it's surprisingly on-point with depictions of AI/human interactions, relations, and societal acceptance. It does get a bit speculative and imaginative at the end though... But I imagine that movie did/does spark the imagination of many people, and I guess Sam just couldn't let it go. reply mike_hearn 8 hours agorootparentIt's not just that. Originally the AI voice in Her was played by someone else, but Spike Jonze felt strongly that the movie wasn't working and recast the part to Johansson. The movie immediately worked much better and became a sleeper hit. Johansson just has a much better fitting voice and higher skill in voice acting for this kind of role, to the extent that it maybe was a make/break choice for the movie. It isn't a surprise that after having created the exact tech from the movie, OpenAI wanted it to have the same success that Jonze had with his character. It's funny that just seven days ago I was speculating that they deliberately picked someone whose voice is very close to Scarlett's and was told right here on HN, by someone who works in AI, that the Sky voice doesn't sound anything like Scarlett and it is just a generic female voice: https://news.ycombinator.com/item?id=40343950#40345807 Apparently .... not. reply sage76 8 hours agorootparentprev> Is it their hobby to be a dick to as many people as they can, for no reason other than their amusement? Just plain weirdos They seem to love \"testing\" how much they can bully someone. I remember a few experiences where someone responded by being an even bigger dick, and they disappeared fast. reply dvhh 16 hours agorootparentprevSome people might see some parallel with SBF and see how Altman would try to regulate competition without impeding OpenAI progress reply viking123 15 hours agorootparentI always mix up those two in my head and have to think which one is which reply ocodo 15 hours agorootparentOne is in jail, when it should be two are in jail reply garbthetill 13 hours agorootparentI dont like sam, but he moves way smarter than ppl like sbf or Elizabeth holmes. He actual has a product close to the reported specs, albeit still far away from the ultimate goal of AGI i dont see why he should be in jail reply Findecanor 11 hours agorootparentIf his sister's words about sexually abusing her are true, he should be in jail. reply bryanrasmussen 10 hours agorootparentno, in that case he should have been in the Juvenile incarceration system, unless the argument is that he should have been charged as an adult, or that Juvenile abusers should always be charged and sentenced as adults, or that Juvenile sex offenders who were not charged as Juveniles should be charged as adults. Which one? on edit: this being based on American legal system, you may come from a legal system with different rules. reply choppaface 13 hours agorootparentprevShould be in jail for Worldcoin which has pilfered people of their biological identity. I guess you could literally delete Worldcoin and in theory make people whole, but that company treats humans like vegetables that have no rights. reply choppaface 13 hours agorootparentprevsama gets to farm out much of the lobbying to Microsoft’s already very powerful team, which spends a mere $10m but that money gets magnified by MS’s gov and DoD contracts. That’s a huge safety net for him, he gets to steal and lie (as demonstrated w/ Scarlett) and yet the MS lobbying machine will continue unphased. https://www.opensecrets.org/federal-lobbying/clients/summary... reply numpad0 15 hours agorootparentprevMaybe they were the rogue AGI escapes we found along the way reply latexr 6 hours agorootparentprev> The thing about the situation is that Altman is willing to lie and steal a celebrity's voice for use in ChatGPT. He lies and steals much more than that. He’s the scammer behind Worldcoin. https://www.technologyreview.com/2022/04/06/1048981/worldcoi... https://www.buzzfeednews.com/article/richardnieva/worldcoin-... > Altman is, and wants to be, a large part of AI regulation. Quite the public contradiction. That’s as much of a contradiction as a thief wanting to be a large part of lock regulation. What better way to ensure your sleazy plans benefit you, and preferably only you but not the competition, than being an active participant in the inevitable regulation while it’s being written? reply ben_w 6 hours agorootparent> That’s as much of a contradiction as a thief wanting to be a large part of lock regulation. Based on what I see in the videos from The Lockpocking Lawyer, that would be a massive improvement. Now, the NSA and crypto standards, that would have worked as a metaphor for your point. (I don't think it's correct, but that's an independent claim, and I am not only willing to discover that I'm wrong about their sincerity, I think everyone writing that legislation should actively assume the worst while they do so). reply maeln 5 hours agorootparent> > That’s as much of a contradiction as a thief wanting to be a large part of lock regulation. > Based on what I see in the videos from The Lockpocking Lawyer, that would be a massive improvement. A thief is not a lock picker and they don't have the same incentive. A thief in a position to dictate lock regulation would try to have a legal backdoor on every lock in the world. One that only he has the master key for. Something something NSA & cryptography :) reply ben_w 4 hours agorootparentIndeed, but locks are so bad (as demonstrated by LPL) that even a thief would make them better. > Something something NSA & cryptography :) Indeed, as I said :) reply lawn 5 hours agorootparentprev> Based on what I see in the videos from The Lockpocking Lawyer, that would be a massive improvement. If you've watched his videos then surely you should know that lockpicking isn't even on the radar for thieves as there are much easier and faster methods such as breaking the door or breaking a window. reply ben_w 4 hours agorootparentSurely that just makes the thief/locks metaphor even worse? Or have I missed something? reply latexr 5 hours agorootparentprev> Based on what I see in the videos from The Lockpocking Lawyer The Lockpicking Lawyer is not a thief, so I don’t get your desire to incorrectly nitpick. Especially when you clearly understood the point. reply ben_w 5 hours agorootparentYou noticed your confusion but still went on the aggressive, huh. Ah well. \"A is demonstrating a proof of B\" does not require \"A is a clause in B\". A being TLPL, B being that the entire lock industry is bad, so bad that anyone with experience would be a massive improvement, for example a thief. reply latexr 5 hours agorootparentI’m not confused and my reply was not aggressive. I don’t think it will be a good use of time to continue this conversation because discussions should get more substantive as they go on and this was an irrelevant tangent to which I have no desire to get sucked in to. Other people have commented to further explain the point in other words. I recommend you read those, perhaps it’ll make you understand. https://news.ycombinator.com/item?id=40428005 https://news.ycombinator.com/item?id=40428280 reply Intralexical 38 minutes agorootparentprev\"Not consistently candid\", the last board said. Like many people who try to oppose psychopaths though, they don't seem to be around much anymore. reply choppaface 13 hours agorootparentprevAltman doesn’t want to be part of regulation. sama wants to be the next tk. he wants to be above regulation, and he wants to spend Microsoft’s money getting there. E.g. flying Congress to Lake Cuomo for an off-the-record “discussion” https://freebeacon.com/politics/how-the-aspen-institute-help... reply belter 10 hours agorootparentprevThis whole exchange from 1:04:53 to 1:10:22 takes a whole different meaning.... https://youtu.be/P_ACcQxJIsg?t=3891 reply beefnugs 13 hours agorootparentprevthe whole technology is based on fucking over artists, who didn't expect this exact thing? reply surfingdino 12 hours agorootparentIt's not just the artists, anything you do in the digital realm and anything that can be digitised is fair game. In the UK NHS GP practices refuse to register you to see a doctor even when it's urgent and tell you to use a third-party app to book an appointment. You have use your phone to take photos of the affected area and provide a personal info. I fully expect that data to be fed into some AI and sold without me knowing and without a process for removal of data should the company go bust. It is preying on the vulnerable when they need help. reply 4ndrewl 11 hours agorootparentImportant to note the \"The NHS\" is not a single entity and the GP practice is likely a private entity owned in partnership by the doctors. There are a number of reasons why individual practices can refuse to register. Take your point about LLMs though. reply surfingdino 11 hours agorootparentI went to see my GP and the lady at the reception told me they no longer book visits at the reception and I had to use the app. Here's the privacy policy https://support.patientaccess.com/privacy-policy They reserve the right to pass your data to third party contractors and to use it for marketing purposes. There is the obligatory clause on regarding the right to be forgotten, but the AI companies claim it is impossible to implement. reply 4ndrewl 11 hours agorootparentI didn't read that as reserving the right - looks like a standard dpia that is opt-in and limited. However, GP practices are essentially privatised - so you do have the right to register at another practice. reply KineticLensman 12 hours agorootparentprevLast time I booked a blood test it was via the official NHS app , not a third party. reply surfingdino 11 hours agorootparenthttps://www.patientaccess.com/ reply jjgreen 10 hours agorootparentprevApp? What's an app? It's a thing you put on your phone I don't have a phone Well, we can't register you You don't accept people who don't have phones? Could I have that in writing please, ..., oh, your signature on that please ... reply startupsfail 16 hours agorootparentprevMost likely it was an unforced error, as there’ve been a lot of chaos with cofounders and the board revolt, easy to loose track of something really minor. Like some intern’s idea to train the voice on their favorite movie. And then they’ve decided that this is acceptable risk/reward and not a big liability, so worth it. This could be a well-planned opening move of a regulation gambit. But unlikely. reply mbreese 15 hours agorootparentThis is an unforced error, but it isn’t minor. It’s quite large and public. The general public doesn’t understand the details and nuances of training an LLM, the various data sources required, and how to get them. But the public does understand stealing someone’s voice. If you want to keep the public on your side, it’s best to not train a voice with a celebrity who hasn’t agreed to it. reply surfingdino 12 hours agorootparentI had a conversation with someone responsible for introducing LLMs into the process that involves personal information. That person rejected my concern over one person's data appearing in the report on another person. He told me that it will be possible to train AI to avoid that. The rest of the conversation convinced me that AI is seen as magic that can do anything. It seems to me that we are seeing a split between those who don't understand it and fear it and those who don't understand it, but want to align themselves with it. Those latter are those I fear the most. reply kombookcha 10 hours agorootparentThe \"AI is magic and we should simply believe\" is even being actively promoted because all these VC hucksters need it. Any criticism of AI is being met with \"but if we all just hype AI harder, it will get so good that your criticisms won't matter\" or flat out denied. You've got tech that's deeply flawed with no obvious way to get unflawed, and the current AI 'leaders' run companies with no clear way to turn a profit other than being relentlessly hyped on proposed future growth. It's becoming an extremely apparent bubble. reply surfingdino 10 hours agorootparentOn the plus side, lots of cheap nVida cards heading for eBay once it bursts. reply windexh8er 16 hours agorootparentprevI don't think this makes any sense, at all, quite honestly. Why would an \"intern\" be training one of ChatGPT's voices for a major release? If in fact, that was the case, then OpenAI is not aligned with the statement they just put out about having utmost focus on rigor and careful considerations, in particular this line: \"We know we can't imagine every possible future scenario. So we need to have a very tight feedback loop, rigorous testing, careful consideration at every step, world-class security, and harmony of safety and capabilities.\" [0] [0] https://x.com/gdb/status/1791869138132218351 reply Cheer2171 15 hours agorootparentprev> easy to loose track of something really minor. Like some intern’s idea Yes, because we all know the high profile launch for a major new product is entirely run by the interns. Stop being an apologist. reply mmastrac 16 hours agorootparentprevIt makes a lot more sense that he was caught red-handed, likely hiring a similar voice actress and not realizing how strong identity protections are for celebs. reply kergonath 13 hours agorootparentprev> Like some intern’s idea to train the voice on their favorite movie. Ah, the famous rogue engineer. The thing is, even if it were the case, this intern would have been supervised by someone, who themselves would have been managed by someone, all the way to the top. The moment Altman makes a demo using it, he owns the problem. Such a public fuckup is embarrassing. > And then they’ve decided that this is acceptable risk/reward and not a big liability, so worth it. You mean, they were reckless and tried to wing it? Yes, that’s exactly what’s wrong with them. > This could be a well-planned opening move of a regulation gambit. But unlikely. LOL. ROFL, even. This was a gambit all right. They just expected her to cave and not ask questions. Altman has a common thing with Musk: he does not play 3D chess. reply Always42 15 hours agorootparentprevAt first I thought there may be a /s coming... reply thatoneguy 17 hours agorootparentprevAt least in past court cases I’m familiar, you can’t use an impersonator and get people to think it’s the real thing. It’s not like Tom Waits ever wanted to hock chips https://www.latimes.com/archives/la-xpm-1990-05-09-me-238-st... reply acomjean 14 hours agorootparentOr Bette Midler singing for ford. She turned them down. They used a sound alike, she sued and won https://en.m.wikipedia.org/wiki/Midler_v._Ford_Motor_Co. reply dewbrite 14 hours agorootparentThey used a sound-alike and had her sing one of her songs. I believe that's a different precedent, in that it's leveraging her fame. Imo Sky's voice is distinct enough from Scarlett, and it wasn't implied to _be_ her. Sam's \"Her\" tweet could be interpreted as such, but defending the tweet as the concept of \"Her\", rather than the voice itself, is. reply sleepybrett 11 hours agorootparentDoes not the 'her' tweet give away the game. Aas you said, it was a midler impersator singing one of midlers songs. In this case they have a voice for their AI assistant/phone sex toy that is very much like the actress that played a famous ai assistant/phone sex toy. Even if he is taken as meaning the concept it's very very damning. If they had, instead, mimic'ed another famous actor's voice that hasn't played a robot/ai/whatever and used that would that really be any better though? Christopher Walken, say, or hell Bette Midler? reply splatzone 10 hours agorootparentprevThere’s a nice YouTube doc telling the story of this, and Tom Waits’ hatred of advertising - https://youtu.be/W7J01e-OIMA?si=57IJooNwg5oTfh62 reply chii 17 hours agorootparentprev> get people to think it’s the real thing. but did openAI make any claims about whose voice this is? Just because a voice sounds similar or familiar, doesn't mean it's fraudulent. reply tedivm 16 hours agorootparentJust read the top post of the thread you're responding to- > - Not receiving a response, OpenAI demos the product anyway, with Sam tweeting “her” in reference to Scarlett’s film. reply dzhiurgis 16 hours agorootparentTo me reference sounds more to towards omni than her voice reply nickthegreek 16 hours agorootparentThat’s not a gamble they are willing to take to court of law or public opinion. reply altairprime 16 hours agorootparentprevWhat’s “omni”? reply nickthegreek 16 hours agorootparentGTP-4o is the new model, the o stands for Omni. reply barbariangrunge 13 hours agorootparentprevEveryone is so mad about them stealing a beloved celebrity’s voice. What about the millions of authors and other creators whose copyrighted works they stole to create works that resemble and replace those people? Not famous enough to generate the same outrage? reply creato 11 hours agorootparentI think the unique thing about this case is not specifically the \"voice theft\", but that OpenAI specifically asked for permission and were denied, which eliminates most of the usual plausible deniability that gets trotted out in these cases. reply surfingdino 12 hours agorootparentprevWelcome to the world where the \"fuck the creatives\" brigade wants everything for free. reply consf 5 hours agorootparentA mentality that devalues creative work reply al_borland 13 hours agorootparentprevI had to go look at what voice I picked once I heard the news, it was Sky. I listened to them all and thought it sounded the best. I didn’t make any connection to her (Scar Jo or the movie) when going through the voices, but I wasn’t listening for either. I don’t think I know her voice well enough to pick it out of a group like that. Maybe I liked it best because it felt familiar, even if I didn’t know why. I’m a bit disappointed now that she didn’t sign on officially, but my guess is that Altman just burned his bridge to half of Hollywood if he is looking for a plan B. reply ekam 19 hours agorootparentprevSame here and that voice really was the only good one. I don't know why they don't bring the voices from their API over, which are all much better, like Nova or Shimmer (https://platform.openai.com/docs/guides/text-to-speech) reply sanxiyn 19 hours agorootparentI think because it is not text-to-speech. It probably isn't simple to transfer. reply zwily 6 hours agorootparentThe previous chatgpt voice mode uses text to speech, and has different voices than the OpenAI API. Seemed weird to me too. reply belter 6 hours agorootparentprevhttps://x.com/jam3scampbell/status/1791338109709287511 reply andrewinardeer 19 hours agorootparentprevI thought it sounded like Jodie Foster. reply ncr100 18 hours agorootparentScar Jo thought it sounded like herself, and so did people who knew her personally. That is what matters. OWNERSHIP over her contributions to the world. reply smt88 17 hours agorootparentI mostly agree with you, but I actually don't think it matters if it sounded exactly like her or not. The crime is in the training: did they use her voice or not? If someone licenses an impersonator's voice and it gets very close to the real thing, that feels like an impossible situation for a court to settle and it should probably just be legal (if repugnant). reply sangnoir 17 hours agorootparent> The crime is in the training: did they use her voice or not? This is a civil issue, and actors get broad rights to their likeliness. Kim Kardashian sued Old Navy for using a look-alike actress in an ad; old Navy chose to settle, which makes it appear like \"the real actress wasn't involved in any way\" may not be a perfect defense. The timeline makes it clear they wanted it to sound like Scarlett's voice, the actual mechanics on how they got the AI to sound like that is only part of the story. reply aseipp 17 hours agorootparentprevIt is not an impossible situation, courts have settled it, and what you describe is not how the law works (despite how many computer engineers think to the contrary.) reply smt88 12 hours agorootparentCourts have settled almost nothing related to AI. We don't even know if training AI using copyrighted works is a violating of copyright law. Please point to a case where someone was successfully sued for sounding too much like a celebrity (while not using the celebrity's name or claiming to be them). reply ascorbic 11 hours agorootparentMidler vs Ford: https://en.wikipedia.org/wiki/Midler_v._Ford_Motor_Co. reply davidgerard 11 hours agorootparentprevMultiple cases already answering your question in this thread. reply dv_dt 17 hours agorootparentprevAs I understand it (though I may be wrong) in music sampling cases, it doesn’t matter if the “sample” is using an actual clip from a recording or if were recreated from scratch using a new media (e.g. direct midi sequence), if a song sampling another song is recognizable it is still infringing. reply parineum 16 hours agorootparentSampling is not the same as duplication. Sampling is allowed as it's a derivitive work as long as it's substantially different from the original. It's a \"I know it when I see it\" situation so it's not clear cut. reply Findecanor 11 hours agorootparentOh, the day when an artist could sample other artists without attribution and royalties is long gone. The music labels are very hard on this these days. reply toomuchtodo 17 hours agorootparentprevhttps://en.wikipedia.org/wiki/Personality_rights reply randoglando 17 hours agorootparentprev> If someone licenses an impersonator's voice and it gets very close to the real thing, that feels like an impossible situation for a court to settle and it should probably just be legal (if repugnant). Does that mean if cosplayers dress up like some other character, they can use that version of the character in their games/media? I think it should be equally simple to settle. It's different if it's their natural voice. Even then, it brings into question whether they can use \"doppelgangers\" legally. reply jonathankoren 14 hours agorootparentprevThis has been settled law for 34 years. See Tom Waits v Frito-Lay. They literally hired an impersonator, and it cost them 2.5 million (~6 million today). https://www.latimes.com/archives/la-xpm-1990-05-09-me-238-st... reply smt88 12 hours agorootparentThat case seems completely dissimilar to what OpenAI did. Frito-Lay copied a song by Waits (with different lyrics) and had an impersonator sing it. Witnesses testified they thought Waits had sung the song. If OpenAI were to anonymously copy someone's voice by training AI on an imitation, you wouldn't have: - a recognizable singing voice - music identified with a singer - market confusion about whose voice it is (since it's novel audio coming from a machine) I don't think any of this is ethical and think voice-cloning should be entirely illegal, but I also don't think we have good precedents for most AI issues. reply jonathankoren 10 hours agorootparentLet me connect the dots for you. Company identifies celebrity voice they want. (Frito=Waits, OpenAi=ScarJo) Company comes up with novel thing for the the voice to say. (Frito=Song, OpenAI=ChatGpt) Company decides they don’t need the celebrity they want (Frito=Waits, OpenAI=ScarJo) and instead hire an impersonator (Frito=singer, {OpenAI=impersonator or OpenAI=ScarJo-public-recordings}) to get what they want (Frito=a-facsimile-of-Tom-Waitte’s-voice-in-a-commercial, OpenAi=a-fascimilie-of-ScarJo’s-voice-in-their-chatbot) When made public, people confuse the fascimilie as the real thing. I don’t see how you don’t see a parallel. It’s literally best for beat the same, particularly around the part about using an impersonator as an excuse. reply XorNot 15 hours agorootparentprevIf OpenAI commissioned a voice actor to lend their voice to the Sky model, and cast on the basis of trying to get someone who is similar sounding to the Scarlett Johannson, but then did not advertise or otherwise use the voice model created to claim it was Scarlett Johannson - then they're completely in the clear. Because then the actual case would be fairly bizarre: an entirely separate person, selling the rights to their own likeness as they are entitled to do, is being prohibited from doing that by the courts because they sound too much like an already famous person. EDIT: Also up front I'm not sure you can entirely discuss timelines for changing out technology here. We have voice cloning systems that can do it with as little as 15 seconds of audio. So having a demo reel of what they wanted to do that they could've used on a few days notice isn't unrealistic - and training a model and not using it or releasing it also isn't illegal. reply cycomanic 14 hours agorootparentThat's confidently incorrect. Many others already posted that this has been settled case law for many years. I mean would you argue that if someone build a macbook lookalike, but not using the same components would be completely clear? reply XorNot 13 hours agorootparentI ask you what do you call the Framework [1]? Or Dell's offerings?[2] Compared to the Macbook? [3] Look kind of similar right? Lot of familiar styling queues? What would take it from \"similar\" to actual infringement? Well if you slapped an Apple Logo on there, that would do it. Did OpenAI make an actual claim? Did they actually use Scarlett Johannson's public image and voice as sampling for the system? [1] https://images.prismic.io/frameworkmarketplace/25c9a15f-4374... [2] https://i.dell.com/is/image/DellContent/content/dam/ss2/prod... [3] https://cdn.arstechnica.net/wp-content/uploads/2023/06/IMG_1... reply mrbungie 13 hours agorootparentYou're not arguing your way out of jurisprudence, especially when the subject is a human and not a device nor IP. They (OpenAI) fucked up. reply XorNot 12 hours agorootparentThere is not clear jurisprudence on this. They're only in trouble if they actually used ScarJo's voice samples to train the model, or if they intentionally tried to portray their imitation as her without her permission. The biggest problem on that front (assuming the former is not true) is Altman's tweets, but court-wise that's defensible (though I retract what I had here previously - probably not easily) as a reference to the general concept of the movie. Because otherwise the situation you have is OpenAI seeking a particular style, hiring someone who can provide it, not trying to pass it off as that person (give or take the Tweet's) and the intended result effectively being: \"random voice actress, you sound too much like an already rich and famous person. Good luck having no more work in your profession\" - which would be the actual outcome. The question entirely hinges on, did they include any data at all which includes ScarJo's voice samples in the training. And also whether it actually does sound similar enough - Frito-Lay went down because of intent and similarity. There's the hilarious outcome here that the act of trying to contact ScarJo is the actual problem they had. EDIT 2: Of note also - to have a case, they actually have to show reputational harm. Of course on that front, the entire problem might also be Altman. Continuing the trend I suppose of billionaires not shutting up on Twitter being the main source of their legal issues. reply einherjae 11 hours agorootparentAre you a lawyer? reply einherjae 11 hours agorootparentprevGrey laptops that share some ideas in their outline while being distinct enough to not get lawyers from Cupertino on their necks? reply jerojero 13 hours agorootparentprevWell Sam Altman tweeted \"her\" so that does seem to me like they're trying to claim a similarity to Scarlett Johannson. reply minimaxir 17 hours agorootparentprevMore notably for legal purposes, there were several independent news reports corroborating the vocal similarity. reply sangnoir 17 hours agorootparent...and sama's tweet referencing \"Her\" reply mcphage 18 hours agorootparentprevClearly Sam Altman though it sounded like ScarJo as well :-( reply parineum 16 hours agorootparentprevShe doesn't own most (all probably) of her contributions to the world. If the voice was only trained on the voice of the character she played in Her, would she have any standing in claiming some kind of infringement? reply lesostep 5 hours agorootparentIANAL, but – I think it's most likely to be an infringement on her right of publicity (i.e. the right to control the commercial value of your name, likeness, etc.) She doesn't have to own anything to claim this right, if the value of her voice is recognizable. reply dyno12345 16 hours agorootparentprevI'm not sure how much you currently legally own imitations of your own voice. There's a whole market for voice actors who can imitate particular famous voices. reply adolph 16 hours agorootparentShould have renamed it https://en.wikipedia.org/wiki/Sosumi Or https://www.reddit.com/r/todayilearned/comments/9n44b6/til_t... reply KennyBlanken 19 hours agorootparentprevnext [11 more] [flagged] skissane 18 hours agorootparent> > \"This is uncomfortable, but it’s possible we have to allow people to say disparaging things about gay people if we want them to be able to say novel things about physics. [1] Of course we can and should say that ideas are mistaken, but we can’t just call the person a heretic. We need to debate the actual idea.\" > That's just a sample of the morally gross things he's said. If a gay man is saying that he’s willing to listen to homophobes for valuable things they might have to say on unrelated topics, I don’t think that is “morally gross” at all, I think it is evidence of wisdom. I’m not defending what he is accused of doing here re Scarlett Johansson’s voice—prima facie he appears to be in the wrong. I don’t have an opinion on other allegations against him because I haven’t investigated them in sufficient detail to form one. But this particular example of yours is labelling something as “morally gross” which many other people view as an admirable quality reply brosciencecode 18 hours agorootparentprevSam is very clearly in the wrong here, but is your argument for why he sucks seriously that he said “we should not brand people as heretics for disagreeing with my point of view?” reply ethbr1 18 hours agorootparentFreedom of speech, for all those who say things I agree with! reply Barrin92 18 hours agorootparentprevwe should not brand people who make qualified and substantive points as heretics, but his framing is wrong and a common abuse of the notion of the 'marketplace of ideas'. Inherent to the functioning of debate is that debates are settled, just like exchanges in a market. That's their only function. Constant debate of an ever-increasing number of talking points, by definition, renders any notion of progress impossible. So when Sam draws on Galileo as a figure, the conclusion is not, we need to listen to him because he's a human being but, we needed to listen to him because he made a substantiated point. The reverse is obviously silly, nobody in physics argues that we need to debate geocentrism in the year 2024 to advance modern physics. Likewise not debating someone who just makes random homophobic remarks doesn't impede the progress of the sciences, it doesn't even impede the progress in ethics. Even the reverse is true, horrible regimes with no free exchange whatsoever still produced a lot of novel physics. reply tpmoney 16 hours agorootparent>Inherent to the functioning of debate is that debates are settled, just like exchanges in a market. That's their only function. Constant debate of an ever-increasing number of talking points, by definition, renders any notion of progress impossible. Some debates might be settled for some time, but I don't think \"settled\" is inherent in the functioning of a debate, and certainly not any sense of permanence even if they are \"settled\" at the moment. Were that the case, we wouldn't right here and now be debating what is clearly \"settled\" free speech policy of \"you have to allow shitty people to say shitty things\" (see National Socialist Party of America v. Village of Skokie). Or maybe I have that wrong and the settled policy is \"socialist aren't allowed to tell you to resist the draft\" (see Schenck v. United States). I imagine if we went further back in time, plenty of people thought the debate over the legal status of slaves was \"settled\". Similarly the divine right of kings was \"settled\" as well. Or back to more modern times, I imagine the \"settled\" rightful disposition of say Palestine, Taiwan or the Ukraine to depends an awful lot on where you live. reply Barrin92 15 hours agorootparent>plenty of people thought the debate over the legal status of slaves was \"settled\" Are you saying plenty of people don't consider it to be settled now? Not knowing you personally I'd be willing to bet almost any amount you are not genuinely open to be persuaded by a pro-slavery or divine monarchy debate. People weren't argued into their shackles and neither were they argued out of them. Just look at the US DoI. It says \"we hold those truths to be self-evident..\" not \"we invite you to make a pro and con list of everything every so often\" and hash it out again. When debate is productive the most important qualifier is never free, it's always reasoned and on shared moral principles. Borders are so violent exactly because they're truly \"up for debate\". Geopolitics is in a real state of anarchy. The logical endpoint of which is might makes right physical conflict. Mind you European borders today are not \"debated\" that way, and we consider that a win. If someone said \"All European borders must be up for debate!\" you'd be somewhat concerned. I mean, that is precisely why US domestic debate is increasingly breaking down and violent. Not because it's not free enough but because it's too free. Because there's no ethical or rational ground underpinning it. So Sam has it exactly wrong. A community that has no shared conception of the fundamental rights of its people is likely in no condition to debate anything. reply tpmoney 14 hours agorootparent>Are you saying plenty of people don't consider it to be settled now? Clearly some people don't, or the modern slave trade would not exist. Never mind that the US depending on your point of view continues to have slavery in the form of prison labor. And then we can get into what I personally consider hyperbolic descriptions of wage \"slavery\" that others I'm sure do not consider it so hyperbolic. > Mind you European borders today are not \"debated\" that way, and we consider that a win. Up until very very recently I suspect quite a few Irish and British nationals would have disagreed very strongly on that front. Even today I imagine there are some that don't consider it settled. 1998-1999 brought us the war in Kosovo. Famously there are still protests surrounding Basque independence in Spain, and similarly in 2017 there was the Catalan declaration of independence. Heck a good portion of the readers on this site are probably older than the current unified borders of Germany (34 years). I would also remind you that the Ukraine is a European country whose borders and independence are actively and violently up for debate currently. And none of that counts any of the various European colonies that were divested (in sometimes quite violent ways) in the last century, regardless of the border stability of the home country. >I mean, that is precisely why US domestic debate is increasingly breaking down and violent. Not because it's not free enough but because it's too free. Because there's no ethical or rational ground underpinning it. And I see it as almost the exact opposite. I see the breakdown as an increasing unwillingness to ascribe any ethical or rational ground to the opposing side. I think it's compounded by an unwillingness to debate the starting axioms before trying to debate the higher level topics as well, but conveniently if you just assume your opponents have no \"ethical or rational ground underpinning\" their debate, you also don't have to bother with debating those baseline axioms. reply skissane 9 hours agorootparentprev> >plenty of people thought the debate over the legal status of slaves was \"settled\" > Are you saying plenty of people don't consider it to be settled now? Many ultra-conservative Muslims do not consider the issue of slavery to be \"settled\", in they argue that slavery is still permissible under Islamic law, and consider principled abolitionism to be heretical. Examples include Saleh Al-Fawzan, [0] the most senior Islamic scholar in Saudi Arabia, and Daniel Haqiqatjou, [1] a controversial American Muslim with a knack for social media–to say nothing of the leadership of groups such as ISIS and Boko Haram. Now, this is clearly a minority opinion in contemporary Islam–maybe you could even say \"fringe\". Still, if it is fringe, it is a fringe that likely numbers in the millions worldwide – and thus counts as \"plenty of people\". And, so nobody thinks I'm unfairly picking on Islam, it is not the only major religion in which slavery is still defended. In the 1980s, Rabbi Meir Kahane introduced a bill into the Israeli Knesset, providing for (inter alia) the enslavement of the Palestinians. The Israeli political class found his bill so offensive, they attempted to prevent him from introducing it – but the Israeli Supreme Court ruled that in a democracy, a democratically elected member of the legislature could not be denied the right to introduce a bill, no matter how abhorrent its content. His bill included clauses such as \"Non-Jews will be obliged to assume duties, taxes and slavery. If he does not agree to slavery and taxes, he will be forcibly deported\". One MK compared it to the Nuremberg Laws; it was near-unanimously rejected. Yet, before one dismisses the late Rabbi Kahane and his followers as some tiny irrelevant minority, consider that one of his devout supporters, Itamar Ben-Gvir, is currently Israel's National Security minister, [2] and there are at least tens of thousands, possibly even hundreds of thousands, of Kahanists in Israel today. And proslavery views in Judaism are not limited to Kahanism; the American Haredi Rabbi Avigdor Miller (died 2001) was an ardent anti-Zionist (on religious grounds), and hence radically opposed to Kahane's ultra-Zionism – but he also \"defended slavery as an ennobling institution that should not have been abolished\" [3]. And, Miller is still very popular in Haredi Judaism, and while it would be wrong to assume that every Haredi Jew agrees with Miller on this, it appears quite a few (possibly even \"plenty\") of them do. Christianity, too, has its contemporary slavery advocates. The American Calvinist theologian R. J. Rushdoony (died 2001) founded the \"theonomy\" movement, which argues (contrary to most Christians) that Old Testament laws should still be applied in the present day, including the biblical laws for slavery. Rushdoony argued that the Bible \"recognizes that some people are by nature slaves\" and that antebellum American slavery was \"generally benevolent\". [4] And again, while you might want to dismiss the theonomy movement as some minuscule irrelevant fringe, its leadership has close links to the current speaker of the US House of Representatives, Mike Johnson. [5] [0] https://en.wikipedia.org/wiki/Saleh_Al-Fawzan#Controversial_... [1] https://muslimskeptic.com/2023/04/10/slavery-minor-marriage/ [2] https://www.jpost.com/opinion/who-is-itamar-ben-gvir-the-loy... [3] https://www.tabletmag.com/sections/belief/articles/evangelic... [4] https://www.splcenter.org/fighting-hate/intelligence-report/... [5] https://www.thedailybeast.com/the-profound-influence-of-chri... reply ncr100 18 hours agorootparentprev> has a history of not treating people fairly and ethically Grandpa used to say, fallacy often exhibited by powerful people is when they transitively believe their wealth also signifies that the rules don't apply to them. reply selcuka 18 hours agorootparentYour grandpa is/was wise: > The experience of power has been related to several positive consequences but also to negative aspects, such as illusory thinking. The present research supports the notion that power is associated with overconfidence. https://pubmed.ncbi.nlm.nih.gov/38625848/ reply kapildev 19 hours agorootparentprevI can still access the sky voice even though it is supposed to be \"yanked\". reply thorum 19 hours agorootparentThere’s still a Sky option but the actual voice has been changed. reply sneak 17 hours agorootparentprevWhy are you mad? We have no rights to the sound of our voice. There is nothing wrong with someone or something else making sounds that sound like us, even if we don’t want it to happen. No one is harmed. reply elicash 17 hours agorootparentThe law can actually be interesting and nuanced on this: http://law2.umkc.edu/faculty/projects/ftrials/communications... reply ethbr1 17 hours agorootparentI think it's a different argument with respect to famous media celebrities* too. If someone clones a random person's voice for commercial purposes, the public likely has no idea who the voice's identity is. Consequently, it's just the acoustic voice. If someone clones a famous media celebrity's voice, the public has a much greater chance of recognizing the voice and associating it with a specific person. Which then opens a different question of 'Is the commercial use of the voice appropriating the real person's fame for their own gain?' Add in the facts that media celebrities' values are partially defined by how people see them, and that they are often paid for their endorsements, and it's a much clearer case that (a) the use potentially influenced the value of their public image & (b) the use was theft, because it was taking something which otherwise would have had value. Neither consideration exists with 'random person's voice' (with deference to voice actors). * Defined as 'someone for whom there is an expectation that the general public would recognize their voice or image' reply mkehrt 17 hours agorootparentprevAre you sure? You certainly have rights to your likeness--it can't be used commercially without permission. Di you know this doesn't cover your voice? reply wkat4242 18 hours agorootparentprev> maybe not even a hiring of an impressionist If they really hired someone who sounds just like her it's fair game IMO. Johanssen can't own the right to a similar voice just like many people can have the same name. I think if there really was another actress and she just happens to sound like her, then it's really ok. And no I'm not a fan of Altman (especially his worldcoin which I view as a privacy disaster) I mean, imagine if I happened to have a similar voice to a famous actor, would that mean that I couldn't work as a voice actor without getting their OK just because they happen to be more famous? That would be ridiculous. Pretending to be them would be wrong, yes. If they hired someone to change their voice to match hers, that'd be bad. Yeah. If they actually just AI-cloned her voice that's totally not OK. Also any references to the movies. Bad. reply 101008 17 hours agorootparentBut clearly they are advertising as her (no pun intended), which is a gray area. reply wkat4242 17 hours agorootparentYeah that was the bad part. Agreed there. I wonder if they deliberately steered towards this for more marketing buzz? reply confused_boner 18 hours agorootparentprevDiscovery process will be interesting reply crimsoneer 19 hours agorootparentprevBut it's clearly not her voice right? The version that's been on the app for a year just isn't. Like, it clearly intending to be slightly reminiscent of her, but it's also very clearly not. Are we seriously saying we can't make voices that are similar to celebrities, when not using their actual voice? reply ncallaway 19 hours agorootparent> Are we seriously saying we can't make voices that are similar to celebrities, when not using their actual voice? They clearly thought it was close enough that they asked for permission, twice. And got two no’s. Going forward with it at that point was super fucked up. It’s very bad to not ask permission when you should. It’s far worse to ask for permission and then ignore the response. Totally ethically bankrupt. reply ants_everywhere 18 hours agorootparentYes, totally ethically bankrupt. But what bewilders me is that they yanked it as soon as they heard from their lawyers. I would have thought that if they made the decision to go ahead despite getting two \"no\"s, that they at least had a legal position they thought was defensible and worth defending. But it kind of looks like they released it knowing they couldn't defend it in court which must seem pretty bonkers to investors. reply ethbr1 18 hours agorootparent> I would have thought that if they made the decision to go ahead despite getting two \"no\"s, that they at least had a legal position they thought was defensible and worth defending. They likely have a legal position which is defensible. They're much more worried that they don't have a PR position which is defensible. What's the point of winning the (legal) battle if you lose the war (of public opinion)? Given the rest of their product is built on apathy to copyright, they're actively being sued by creators, and the general public is sympathetic to GenAI taking human jobs... ... this isn't a great moment for OpenAI to initiate a long legal battle, against a female movie actress / celebrity, in which they're arguing how her likeness isn't actually controlled by her. Talk about optics! (And I'd expect they quietly care much more about their continued ability to push creative output through their copyright launderer, than get into a battle over likeness) reply justinclift 16 hours agorootparent> They likely have a legal position which is defensible. Doesn't sound like they have that either. reply bonton89 40 minutes agorootparentCopilot still tells me I've commit a content policy violation of I ask it to generate an image \"in Tim Burton's style\". Tim Burton has been openly critical of generative AI. reply XorNot 14 hours agorootparentprevHow is the PR position not defensible? One of the worst things you can generally do is admit fault, particularly if you have a complete defense. Buckle in, go to court, and double-down on the fact that the public's opinion of actors is pretty damn fickle at the best of times - particularly if what you released was in fact based on someone you signed a valid contract with who just sounds similar. Of course, this is all dependent on actually having a complete defense of course - you absolutely would not want to find Scarlett Johannsen voice samples in file folders associated with the Sky model if it went to court. reply ethbr1 14 hours agorootparentIn what world does a majority of the public cheer for OpenAI \"stealing\"* an actress's voice? People who hate Hollywood? Most of that crowd hates tech even more. * Because it would take the first news cycle to be branded as that reply XorNot 12 hours agorootparentIt is wild to me that on HackerNews of all places, you'd think people don't love an underdog story. Which is what this would be in the not-stupid version of events: they hired a voice actress for the rights to create the voice, she was paid, and then is basically told by the courts \"actually you're unhireable because you sound too much like an already rich and famous person\". The issue of course is that OpenAIs reactions so far don't seem to indicate that they're actually confident they can prove this or that this is the case. Coz if this is actually the case, they're going about handling this in the dumbest possible way. reply Sebb767 9 hours agorootparent> they hired a voice actress for the rights to create the voice, she was paid, and then is basically told by the courts \"actually you're unhireable because you sound too much like an already rich and famous person\". There are quite a few issues here: First, this is assuming they actually hired a voice-alike person, which is not confirmed. Second, they are not an underdog (the voice actress might be, but she's most likely pretty unaffected by this drama). Finally, they were clearly aiming to impersonate ScarJo (as confirmed by them asking for permission and samas tweet), so this is quite a different issue than \"accidentally\" hiring someone that \"just happens to\" sound like ScarJo. reply ml-anon 11 hours agorootparentprevIt’s wild to me that there are people who think that OpenAI are the underdog. A 80Bn Microsoft vassal, what a plucky upstart. You realise that there are multiple employees including the CEO publicly drawing direct comparisons to the movie Her after having tried and failed twice to hire the actress who starred in the movie? There is no non idiotic reading of this. reply XorNot 11 hours agorootparentYou're reading my statements as defending OpenAI. Put on your \"I'm the PR department hat\" and figure out what you'd do if you were OpenAI given various permutations of the possible facts here. That's what I'm discussing. Edit: which is to say, I think Sam Altman may have been a god damn idiot about this, but it's also wild anyone thought that ScarJo or anyone in Hollywood would agree - AI is currently the hot button issue there and you'd find yourself the much more local target of their ire. reply ml-anon 9 hours agorootparentThen why bother mentioning an \"underdog story\" at all? Who is the underdog in this situation? In your comment it seems like you're framing OpenAI as the underdog (or perceived underdog) which is just bonkers. Hacker News isn't a hivemind and there are those of us who work in GenAI who are firmly on the side of the creatives and gasp even rights holders. reply jubalfh 7 hours agorootparentprevan obnoxious sleazy millionaire backed by microsoft is by no means “an underdog” reply emsign 14 hours agorootparentprevIt looks really unprofessional at minimum if not a bit arrogant, which is actually more concerning as it hints at a deeper disrespect for artists and celebrities. reply foobarian 17 hours agorootparentprev> But it kind of looks like they released it knowing they couldn't defend it in court which must seem pretty bonkers to investors. That actually seems like there may be a few people involved and one of them is a cowboy PM who said fuck it, ship it to make the demo. And then damage control came in later. Possibly the PM didn't even know about the asks for permission? reply anytime5704 15 hours agorootparentThe whole company behaves like rogue cowboys. If a PM there didn’t say “fuck it ship it even without her permission” they’d probably be replaced with someone who would. I expect the cost of any potential legal action/settlement was happily accepted in order to put on an impressive announcement. reply kuboble 13 hours agorootparentprev> a cowboy PM who said fuck it, ship it to make the demo. Given the timeline it sounds like the PM was told \"just go ahead with it, I'll get the permission\". reply unraveller 4 hours agorootparentKen Segall has a similar Steve Jobs story, he emails Jobs that the Apple legal team have just thrown a spanner in the works days before Ken's agency is set to launch Apple's big ad campaign and what should he do? Jobs responds minutes later... \"Fuck the lawyers.\" reply gibbitz 17 hours agorootparentprevAre we surprised by this bankruptcy. As neat as AI is, it is only a thing because the corporate class see it as a way to reduce margins by replacing people with it. The whole concept is bankrupt. reply ecjhdnc2025 17 hours agorootparent100% this. It’s shocking to me how people cannot see this. The only surprise here is that they didn’t think she’d push back. That is what completes the multilayered cosmic and dramatic irony of this whole vignette. Honestly feels like Shakespeare or Arthur Miller might have written it. reply emsign 14 hours agorootparentprevProblem is they really believe we either can't tell the difference between a human and an AI model eventually, or they think we don't care. Don't they understand the meaning of art? reply ncallaway 17 hours agorootparentprevI don’t think any said anything about being surprised by it? reply Nasrudith 4 hours agorootparentprevThat is wrong on several levels. First off, it ignores the role of massive researchers. Should we start de-automating processes to employee more people at the cost of worsening margins? reply nicce 19 hours agorootparentprevAnd they could have totally get away with it by never mentioning the name of Scarlett. But of course, that is not what they wanted. Edit: to clarify, since it is not exactly identical voice, or even not that close, they can plausibly deny it, and we never new what their intention was. But in this case, they have clearly created the voice to represent Scarlett's voice to demonstrate the capabilities of their product in order to get marketing power. reply visarga 15 hours agorootparent> since it is not exactly identical voice, or even not that close, they can plausibly deny it When studios approach an actress A and she refuses, then another actress B takes the role, is that infringing on A's rights? Or should they just scrap the movie? Maybe if they replicated a scene from the A's movies or there was striking likeness between the voices... but not generally. reply nicce 12 hours agorootparent> When studios approach an actress A and she refuses, then another actress B takes the role, is that infringing on A's rights? Or should they just scrap the movie? The scenario would have been that they approach none. reply mensetmanusman 18 hours agorootparentprevEffective altruism would posit that it is worth one voice theft to help speed the rate of life saving ai technology in the hands of everyone. reply ncallaway 17 hours agorootparentEffective Altruists are just shitty utilitarians that never take into account all the myriad ways that unmoderated utilitarianism has horrific failure modes. Their hubris will walk them right into federal prison for fraud if they’re not careful. If Effective Altruists want to speed the adoption of AI with the general public, they’d do well to avoid talking about it, lest the general public make a connection between EA and AI I will say, when EA are talking about where they want to donate their money with the most efficacy, I have no problem with it. When they start talking about the utility of committing crimes or other moral wrongs because the ends justify the means, I tend to start assuming they’re bad at morality and ethics. reply Intralexical 50 minutes agorootparentThe central contention of Effective Altruism, at least in practice if not in principle, seems to be that the value of thinking, feeling persons can be and should be reduced to numbers and objects that you can do calculations on. Maybe there's a way to do that right. I suppose like any other philosophy, it ends up reflecting the personalities and intentions of the individuals which are attracted to and end up adopting it. Are they actually motivated by identifying with and wanting to help other people most effectively? Or are they just incentivized to try to get rid of pesky deontological and virtue-based constraints like empathy and universal rights? reply Intralexical 1 hour agorootparentprevPlus, describing this as \"speed the rate of life saving ai technology in the hands of everyone\" is… A Reach. reply 0xDEAFBEAD 13 hours agorootparentprev>Effective Altruists are just shitty utilitarians that never take into account all the myriad ways that unmoderated utilitarianism has horrific failure modes. There's a fair amount of EA discussion of utilitarianism's problems. Here's EA founder Toby Ord on utilitarianism and why he ultimately doesn't endorse it: https://forum.effectivealtruism.org/posts/YrXZ3pRvFuH8SJaay/... >If Effective Altruists want to speed the adoption of AI with the general public, they’d do well to avoid talking about it, lest the general public make a connection between EA and AI Very few in the EA community want to speed AI adoption. It's far more common to think that current AI companies are being reckless, and we need some sort of AI pause so we can do more research and ensure that AI systems are reliably beneficial. >When they start talking about the utility of committing crimes or other moral wrongs because the ends justify the means, I tend to start assuming they’re bad at morality and ethics. The all-time most upvoted post on the EA Forum condemns SBF: https://forum.effectivealtruism.org/allPosts?sortedBy=top&ti... reply ncallaway 13 hours agorootparentI’ve had to explain myself a few times on this, so clearly I communicated badly. I probably should have said _those_ Effective Altruists are shitty utilitarians. I was attempting—and since I’ve had to clarify a few times clearly failed—to take aim at the effective altruists that would make the utilitarian trade off that the commenter mentioned. In fact, there’s a paragraph from the Toby Ord blog post that I wholeheartedly endorse and I think rebuts the exact claim that was put forward that I was responding to. > Don’t act without integrity. When something immensely important is at stake and others are dragging their feet, people feel licensed to do whatever it takes to succeed. We must never give in to such temptation. A single person acting without integrity could stain the whole cause and damage everything we hope to achieve. So, my words were too broad. I don’t actually mean all effective altruists are shitty utilitarians. But the ones that would make the arguments I was responding to are. I think Ord is a really smart guy, and has worked hard to put some awesome ideas out into the world. I think many others (and again, certainly not all) have interpreted and run with it as a framework for shitty utilitarianism. reply parineum 16 hours agorootparentprevThis is like attributing the crimes of a few fundamentalists to an entire religion. reply ncallaway 16 hours agorootparentI don’t think so. I’ve narrowed my comments specifically to Effective Altruists who are making utilitarian trade-offs to justify known moral wrongs. > I will say, when EA are talking about where they want to donate their money with the most efficacy, I have no problem with it. When they start talking about the utility of committing crimes or other moral wrongs because the ends justify the means, I tend to start assuming they’re bad at morality and ethics. Frankly, if you’re going to make an “ends justify the means” moral argument, you need to do a lot of work to address how those arguments have gone horrifically wrong in the past, and why the moral framework you’re using isn’t susceptible to those issues. I haven’t seen much of that from Effective Altruists. I was responding to someone who was specifically saying an EA might argue why it’s acceptable to commit a moral wrong, because the ends justify it. So, again, if someone is using EA to decide how to direct their charitable donations, volunteer their time, or otherwise decide between mora goods, I have no problem with it. That specifically wasn’t context I was responding to. reply parineum 4 hours agorootparent> I don’t think so. I’ve narrowed my comments specifically to Effective Altruists who are making utilitarian trade-offs to justify known moral wrongs. Did you? > Effective Altruists are just shitty utilitarians that never take into account all the myriad ways that unmoderated utilitarianism has horrific failure modes. reply ncallaway 1 hour agorootparentSure, I should’ve said I tried to or I intended to: You can see another comment here, where I acknowledge I communicate badly, since I’ve had to clarify multiple times what I was intending: https://news.ycombinator.com/item?id=40424566 This is the paragraph that was intended to narrow what I was talking about: > I will say, when EA are talking about where they want to donate their money with the most efficacy, I have no problem with it. When they start talking about the utility of committing crimes or other moral wrongs because the ends justify the means, I tend to start assuming they’re bad at morality and ethics. That said, I definitely should’ve said “those Effective Altruists” in the first paragraph to more clearly communicate my intent. reply ocodo 15 hours agorootparentprevEffective Altruists are the fundamentalists though. So no, it's not. reply comp_throw7 14 hours agorootparentprev> When they start talking about the utility of committing crimes or other moral wrongs because the ends justify the means, I tend to start assuming they’re bad at morality and ethics. Extremely reasonable position, and I'm glad that every time some idiot brings it up in the EA forum comments section they get overwhelmingly downvoted, because most EAs aren't idiots in that particular way. I have no idea what the rest of your comment is talking about; EAs that have opinions about AI largely think that we should be slowing it down rather than speeding it up. reply emsign 14 hours agorootparentThe speed doesn't really matter if their end goal is morally wrong. A slower speed might give them an advantage to not overshoot and get backlash or it gives artists and the public more time to fight back against EA, but it doesn't hide their ill intentions. reply ncallaway 14 hours agorootparentprevIn some sense I see a direct line between the EA argument being presented here, and the SBF consequentialist argument where he talks about being willing to flip a coin if it had a 50% chance to destroy the world and a 50% chance to make the world more than twice as good. I did try to cabin my arguments to Effective Altrusts that are making ends justify the means arguments. I really don’t have a problem with people that are attempting to use EA to decide between multiple good outcomes. I’m definitely not engaged enough with the Effective Altrusits to know where the plurality of thought lies, so I was trying to respond in the context of this argument being put forward on behalf of Effective Altruists. The only part I’d say applies to all EA, is the brand taint that SBF has done in the public perception. reply ehnto 17 hours agorootparentprevIt didn't require voice theft, they could have easily found a volunteer or paid for someone else. reply consf 5 hours agorootparentAgree, and it'd be more authentic reply avarun 19 hours agorootparentprev> They clearly thought it was close enough that they asked for permission, twice. You seem to be misunderstanding the situation here. They wanted ScarJo to voice their voice assistant, and she refused twice. They also independently created a voice assistant which sounds very similar to her. That doesn't mean they thought they had to ask permission for the similar voice assistant. reply voltaireodactyl 18 hours agorootparentYou seem to be misunderstanding the legalities at work here: reaching out to her multiple times beforehand, along with tweets intended to underline the similarity to her work on Her, demonstrates intention. If they didn’t think they needed permission, why ask for permission multiple times and then yank it when she noticed? Answer: because they knew they needed permission, after working so hard to associate with Her, and they hoped that in traditional tech fashion that if they moved fast and broke things enough, everyone would have to reshape around OAs wants, rather than around the preexisting rights of the humans involved. reply munksbeer 3 hours agorootparent> If they didn’t think they needed permission, why ask for permission multiple times and then yank it when she noticed? One very easy explanation is that they trained Sky using another voice (this is the claim and no reason to doubt it is true) wanting to replicate the stye of the voice in \"Her\", but would have preferred to use SJ's real voice for the PR impact that could have. Yanking it could also easily be a pre-emptive response to avoid further PR drama. You will obvious decide you don't believe those explanations, but to many of us they're quite plausible, in fact I'd even suggest likely. (And none of this precludes Sam Altman and OpenAI being dodgy anyway) reply voltaireodactyl 1 hour agorootparentI actually believe that’s quite plausible. The trouble is, by requesting permission in the first place, they demonstrated intent, which is legally significant. I think a lot of your confusion is attempting to employ pure logic to a legal issue. They are not the same thing, and the latter relies heavily on existing precedent — of which you may, it seems, be unaware. reply KHRZ 17 hours agorootparentprevYou could also ask: If Scarlett has a legal case already, why does she want legislation passed? reply ncallaway 16 hours agorootparentBecause a legal case under the current justice system and legislative framework would probably take hundreds of thousands to millions of dollars to bring a case that requires discovery and a trial to accomplish. Maybe (maybe!) it’s worth it for someone like Johansson to take on the cost of that to vindicate her rights—but it’s certainly not the case for most people. If your rights can only be defended from massive corporations by bringing lawsuits that cost hundreds of thousands to millions of dollars, then only the wealthy will have those rights. So maybe she wants new legislative frameworks around these kind of issues to allow people to realistically enforce these rights that nominally exist. For an example of updating a legislative framework to allow more easily vindicating existing rights, look up “anti-SLAPP legislation”, which many states have passed to make it easier for a defendant of a meritless lawsuit seeking to chill speech to have the lawsuit dismissed. Anti-SLAPP legislation does almost nothing to change the actual rights that a defendant has to speak, but it makes it much more practical for a defendant to actually excercise those rights. So, the assumption that a call for updated legislation implies that no legal protection currently exists is just a bad assumption that does not apply in this situation. reply bradchris 15 hours agorootparentprevShe has a personal net worth of >$100m. She’s also married to a successful actor in his own right. Her voice alone didn’t get her there — she did. That’s why celebrities are so protective about how their likeness is used: their personal brand is their asset. There’s established legal precedent on exactly this—even in the case they didn’t train on her likeness, if it can reasonably be suspected by an unknowing observer that she personally has lent her voice to this, she has a strong case. Even OpenAI knew this, or they would not have asked in the first place. reply staticman2 2 hours agorootparentprevBefore Roe vs Wade was overturned you might have asked if abortion is legal why do abortion rights advocates want legislation passed? The answer is without legislation you are far more subject to whether a judge feels like changing the law. reply minimaxir 17 hours agorootparentprevTo prevent it from happening again, with more legal authority than a legal precedent. reply parineum 16 hours agorootparentprev> If they didn’t think they needed permission, why ask for permission multiple times and then yank it when she noticed? Many things that are legal are of questionable ethics. Asking permission could easily just be an effort for them to get better samples of her voice. Pulling the voice after debuting it is 100% a PR response. If there's a law that was broken, pulling the voice doesn't unbreak it. reply voltaireodactyl 1 hour agorootparentThey are trying to wriggle out of providing insight into how that voice was derived at all (like Google with the 100% of damages check). It would really suck for OpenAI if, for example, Altman had at some point emailed his team to ensure the soundalike was “as indistinguishable from Scarlet’s performance in HER as possible.“ Public figures own their likeness and control its use. Not to mention that in this case OA is playing chicken with studios as well. Not a great time to do so, given their stated hopes of supplanting 99% of existing Hollywood creatives. reply ncallaway 16 hours agorootparentprev> You seem to be misunderstanding the situation here. They wanted ScarJo to voice their voice assistant, and she refused twice. They also independently created a voice assistant which sounds very similar to her. And promoted it using a tweet naming the movie that Johansson performed in, for the role that prompted them to ask her in the first place. You have to be almost deliberately naive to not see that the were attempting to use her vocal likeness in this situation. There’s a reason they immediately walked it back after the situation was revealed. Neither a judge, nor a jury, would be so willingly naive. reply munksbeer 3 hours agorootparentThis is a genuine question. If it turns out they trained Sky on someone else's voice to similarly replicate the style of the voice in \"Her\", would you be ok with that? If it was proven that the voice was just similar, to SJ's would that be ok? My view is, of course it is ok. SJ doesn't own the right to a particular style of voice. reply chromakode 18 hours agorootparentprevSo, what would they have done if she accepted? Claimed that the existing training of the Sky voice was voiced by her? reply sangnoir 17 hours agorootparent> Claimed that the existing training of the Sky voice was voiced by her? That claim could very well be true. The letter requested information on how the voice was trained - OpenAI may not want that can of worms opened lest other celebrities start paying closer attention to the other voices. reply consf 5 hours agorootparentprevThat would be suspicious too reply blackoil 15 hours agorootparentprevMaybe they have second trained on her voice. reply og_kalu 18 hours agorootparentprevVoice cloning could be as simple as a few seconds of audio in the context window since GPT-4o is a speech to speech transformer. They wouldn't need to claim anything, just switch samples. They haven't launched the new voice mode yet, just demos. reply tomrod 18 hours agorootparentprevAnd... No. That is what OpenAI will assert, and good discovery by Scar Jo reps may prove or disprove. reply dragonwriter 19 hours agorootparentprevIf the purpose is to trade on the celebrity voice and perceived association, and its subject to California right of personality law, then, yes, we're saying that that has been established law for decades. reply Last5Digits 18 hours agorootparentThat's not the purpose though, clearly. If anything, you could make the argument that they're trading in on the association to the movie \"Her\", that's it. Neither Sky nor the new voice model sound particularly like ScarJo, unless you want to imply that her identity rights extend over 40% of all female voice types. People made the association because her voice was used in a movie that features a highly emotive voice assistant reminiscent of GPT-4o, which sama and others joked about. I mean, why not actually compare the voices before forming an opinion? https://www.youtube.com/watch?v=SamGnUqaOfU https://www.youtube.com/watch?v=vgYi3Wr7v_g ----- https://www.youtube.com/watch?v=iF9mrI9yoBU https://www.youtube.com/watch?v=GV01B5kVsC0 reply cowsup 17 hours agorootparent> People made the association because her voice was used in a movie that features a highly emotive voice assistant reminiscent of GPT-4o, which sama and others joked about. Whether you think it sounds like her or not is a matter of opinion, I guess. I can see the resemblance, and I can also see the resemblance to Jennifer Lawrence and others. What Johannson is alleging goes beyond this, though. She is alleging that Altman (or his team) reached out to her (or her team) to lend her voice, she was not interested, and then she was asked again just two days before GPT-4o's announcement, and she rejected again. Now there's a voice that, in her opinion, sounds a lot like her. Luckily, the legal system is far more nuanced than just listening to a few voices and comparing it mentally to other voices individuals have heard over the years. They'll be able to figure out, as part of discovery, what lead to the Sky voice sounding the way it does (intentionally using Johannson's likeness? coincidence? directly trained off her interviews/movies?), whether OpenAI were willing to slap Johannson's name onto the existing Sky during the presentation, whether the \"her\" tweet and the combination of the Sky voice was supposed to draw the subtle connection... This allegation is just the beginning. reply Last5Digits 17 hours agorootparentI honestly don't think it is a matter of opinion, though. Her voice has a few very distinct characteristics, the most significant of which being the vocal fry / huskiness, that aren't present at all in either of the Sky models. Asking for her vocal likeness is completely in line with just wanting the association with \"Her\" and the big PR hit that would come along with that. They developed voice models on two different occasions and hoped twice that Johannson would allow them to make that connection. Neither time did she accept, and neither time did they release a model that sounded like her. The two day run-up isn't suspicious either, because we're talking about a general audio2audio transformer here. They could likely fine-tune it (if even that is necessary) on her voice in hours. I don't think we're going to see this going to court. OpenAI simply has nothing to gain by fighting it. It would likely sour their relation to a bunch of media big-wigs and cause them bad press for years to come. Why bother when they can simply disable Sky until the new voice mode releases, allowing them to generate a million variations of highly-expressive female voices? reply om2 14 hours agorootparentprevI haven’t hear the GPT-4o voice before. Comparing the video to the video of Johansson’s voice in “her”, it sounds pretty similar. Johansson’s performance there sounds pretty different from her normal speaking voice in the interview - more intentional emotional inflection, bubbliness, generally higher pitch. The GPT-4o voice sounds a lot like it. From elsewhere in the thread, likeness rights apparently do extend to intentionally using lookalikes / soundalikes to create the appearance of endorsement or association. reply emmp 19 hours agorootparentprevWe can seriously say that, yes. The courts have been saying this in the US for over 30 years. See Midler v. Ford Motor Co. reply Avshalom 19 hours agorootparentTom Waits won a lawsuit against Doritos too. reply bigfishrunning 19 hours agorootparentprevIt could be trained on Scarlett's voice though, there's plenty of recorded samples for OpenAI to use. It's pretty damning for them to take down the voice right away like that reply brandall10 17 hours agorootparentHer statement claims the voice was taken down at her attorney's insistence. reply callalex 18 hours agorootparentprevI think we should all be held to the standard of “Weird” Al Yankovic. In personal matters consent is important. reply bobthepanda 19 hours agorootparentprevthis is correct. in fact the fcc has already clarified this for the case of robocalls. https://www.fcc.gov/document/fcc-makes-ai-generated-voices-r... reply EasyMark 19 hours agorootparentprevSure they could have taken her to court but right now they don't want the bad publicity, especially since it would put everything else in the shadow of such a scandalous \"story\". Better to just back off, let S.J. win and move on and start planning on they're gonna spend all that paper money they got with announcement of a new, more advanced model. It's a financial decision and a fairly predictable one. I'm glad she won this time. reply smugma 19 hours agorootparentShe also won big against Disney. They backed down even though it appeared the contract was on their side. Iger apologized. https://www.bbc.co.uk/news/business-58757748.amp reply mschuster91 12 hours agorootparentProbably (and rightfully) feared that, had Disney stuck with their position, other MCU actors would be much, much harsher in new contract negotiations - or that some would go as far and say \"nope, I quit\". reply __loam 19 hours agorootparentprevPaper money from the model they're giving away for free? reply EasyMark 19 hours agorootparentI mean if you don't think these kinds of positive announcements don't increase the value of the company or parent company then I don't really know how to convince you as it's a standard business principle. reply __loam 16 hours agorootparentI believe there's a difference between building a sustainable and profitable business and pumping the stock. reply ml-anon 18 hours agorootparentprevThere isn’t a positive announcement here, what is wrong with you? This reads like “we got caught red handed” and doing the bare minimum for it to not appear malicious and deliberate when the timeline is read out in court. reply gedy 19 hours agorootparentprevNormally I'd agree if this were some vague \"artist style\", but this was clearly an attempt to duplicate a living person, a media celebrity no less. reply __loam 19 hours agorootparentWhy do you have an issue with them taking someone's likeness to use in their product but not with them taking someone's work to use in their product? reply gedy 18 hours agorootparentBecause this isn't training an audio model along with a million other voices to understand English, etc. It's clearly meant to sound exactly like that one celebrity. I suspect a video avatar service that looked exactly like her would fall afoul of fair use as well. Though an image gen that used some images of her (and many others) to train and spit out generic \"attractive blonde woman\" is fair use in my opinion. reply numpad0 17 hours agorootparentChances are this is. Basically same as LoRA. One of go-to tools for these literally uses Diffusion model and work on spectrograms as images. reply __loam 16 hours agorootparentprevOkay so as long as we steal enough stuff then it's legal. reply citizenpaul 13 hours agorootparentprevAn actress that specifically played the voice of AI in a movie about AI no less. reply threatofrain 19 hours agorootparentprevIs this different from the various videos of the Harry Potter actors doing comedic high fashion ads? Because those were very well received. https://www.youtube.com/watch?v=ipuqLy87-3A reply jacobolus 19 hours agorootparentOne is a company with a nearly $100 billion valuation using someone's likeness for their own commercial purposes in a large-scale consumer product, which consumers would plausibly interpret as a paid endorsement, while the other seems to be an amateur hobbyist nobody has ever heard of making a parody demo as an art project, in a way that makes it clear that the original actors had nothing to do with it. The context seems pretty wildly different to me. I'm guessing if any of the Harry Potter actors threatened the hobbyist with legal action the video would likely come down, though I doubt they would bother even if they didn't care for the video. reply BadHumans 19 hours agorootparentprevIs a billion dollar AI company utilizing someone's voice against their will in a flagship product after they said no twice different from a random Youtube channel making comedy videos? I think so but that could just be me. reply jprete 19 hours agorootparentprevThose are parodies and not meant at any point for",
    "originSummary": [
      "Scarlett Johansson released a statement about the OpenAI situation, which was shared by journalist Bobby Allyn on Twitter on May 20, 2024.",
      "The involvement of a high-profile celebrity like Johansson has drawn significant public attention to the issue.",
      "The specifics of Johansson's statement and the nature of the OpenAI situation were not detailed in the provided text."
    ],
    "commentSummary": [
      "Scarlett Johansson declined OpenAI's request to use her voice for their \"Sky\" feature, but they used a cloned version without her consent in a demo, leading to its removal after legal intervention.",
      "This incident has sparked criticism of CEO Sam Altman for unethical practices and raised concerns about transparency and accountability in tech leadership.",
      "The controversy underscores the necessity of consent and the potential legal and PR risks of using a celebrity's likeness without explicit permission."
    ],
    "points": 1398,
    "commentCount": 942,
    "retryCount": 0,
    "time": 1716244107
  },
  {
    "id": 40419856,
    "title": "Enlightenmentware: A Programmer's Journey Through UNIX, Git, Emacs, and Bazel",
    "originLink": "https://mmapped.blog/posts/28-enlightenmentware.html",
    "originBody": "mmap(blog) Posts About Atom Feed Enlightenmentware ✏ 2024-05-20 ✂ 2024-05-20 UNIX Git Emacs Boost.Graph Bazel Conclusion As programmers, we interact with software tools daily. Most of them can barely get the job done. But occasionally, we discover a piece of software that transcends mere utility. These tools capture our imagination, open new possibilities, and affect how we design our own systems. I call such software enlightenmentware. The most common source of enlightenment for programmers is the programming language they use at work or learn as a hobby. I experienced many jolts of enlightenment from fiddling with programming languages, from masm and C to Prolog and Idris. I won’t focus on languages, however, since the effects of language learning on mind expansion is old news See, for example, Peter Norvig’s Teach Yourself Programming in Ten Years. . In this article, I praise the software that contributed the most to my enlightenment. UNIX unix is user-friendly—it’s just choosy about who its friends are. Anonymous, in the Art of unix Programming by Eric S. Raymond I started looking for my first real programming job around 2008, while studying at university in my hometown of Nizhny Novgorod. Almost all the open positions required knowledge of mysterious things called unix and sockets. My curriculum didn’t offer a course on unix or operating systems in general, so I decided to get a textbook and master the topic myself. The unix Operating System by Andrey Robachevsky et al., also known as the turtle book in Russia because of its cover, introduced me to the magical world of unix-like operating systems. unix became something I could understand, explore, and programmatically interact with. All pieces of the puzzle—the filesystem interface, the process model with environments and permissions, forking, sockets, and signals—fell into place and revealed a coherent, beautiful picture. A search for a working unix installation led me to Mandriva Linux. It was like discovering a parallel universe where you don’t have to pirate software or spend forty minutes installing an ide to compile a c program. Here, people developed software for fun and shared it freely. I couldn’t fathom why anyone would use Windows I became significantly more tolerant since my early university years. Windows (specifically the NT family) is a great operating system. I even have it installed on my gaming pc so that I can buy games I never play. . From that moment on, unix followed me through all stages of my life: the toddler phase of keeping up with the cutting-edge Ubuntu releases, the rebellious teens of compiling custom kernels for my Thinkpad T61p and emerging the @world on Gentoo, the maturity of returning to Ubuntu lts and delaying upgrades until the first dot one release, and to the overwhelmed parent stage of becoming a happy macOS user. unix also became an essential building block in my profession. Most of the software I wrote operates in a unix environment, and I still occasionally consult my copy of Advanced Programming in the unix Environment. Git It is easy to shoot your foot off with git, but also easy to revert to a previous foot and merge it with your current leg. Jack William Bell I encountered version control systems in early 2009; the company I worked for used Rational ClearCase to manage their code. The system versioned each file separately and relied on large configuration files—config specs—to construct a consistent snapshot of the source tree. The tool was utterly confusing and intimidating, so I avoided dealing with it beyond the minimal requirements of my job. About a year later, I joined a shop that used Subversion. This time, I invested in learning upfront and swallowed the entire Version Control with Subversion before making my first commit. Subversion was easy to understand and use; I couldn’t imagine how to improve on it. Still, I perceived it as a tool that you use at work. There was enough friction in setting up a repository to hinder its use for small personal projects. At the time, Google offered hosting on Google Code, but I didn’t feel comfortable sharing my experiments with the world back then. And then I discovered Git. Git was nothing like Subversion. It had a steep learning curve and confused everyone to no end Way before we all got used to ChatGPT, Kim Ødegaard created a service that generates random man pages mocking Git’s dense documentation style. . Still, the confusion was qualitatively different from what I experienced with ClearCase. ClearCase is confusing like a Russian novel: All the characters have strange names, the plot is complex, and it doesn’t end well. Git is confusing like math: It slowly melts your brain and molds it into a tesseract, giving access to higher dimensions. Git removed the friction from using version control; there was no excuse not to version anything of value anymore. Merging branches with Git didn’t cause anxiety disorders. The staging area—confusingly named index—became essential to my workflows. But my favorite feature was the breathtaking beauty of Git’s design, the elegant mix of distributed systems, acyclic graphs, and content-addressed storage. Learning about Git’s internals was so much fun that I became interested in the bits and bolts of other version control systems. I travelled through time from Darcs to Mercurial, BitKeeper, and the ultimate origin, SCCS. I also built a toy one-file version control system while learning Rust. Will Git ever be replaced with something better? Just as it was hard to imagine an improvement over Subversion before Git came along, it’s hard to imagine a significant improvement over Git now. For me, Git’s primary disadvantage is its snapshot-oriented approach that makes merges hard to reason about. Git, Mercurial, and most other tools make it challenging to separate original code from the decisions that the person merging files had to make Jane Street’s tech staff reports similar concerns. See, for example, the Patch review vs. diff review, revisited blog article. . Systems based on patch theory, such as Pijul and Darcs, might address these issues. Emacs While any text editor can save your files, only Emacs can save your soul. Per Abrahamsen I edited my first programs in a friendly blue window of Turbo Pascal 7.0. The environment had little friction: no project or build configuration, no noticeable build time; you type your code and run it. That was a perfect tool for learning. My university used Pascal for introductory programming classes, so I also used Turbo Pascal for my assignments. Later courses introduced C++ and Java, for which we used Visual Studio 6.0 and JBuilder. Although we learned to invoke compilers from the command line, ides dominated my early code-editing experience. At my first programming job, I worked on a remote Solaris workstation over a Citrix connection. Almost everyone in our group used NEdit to edit the code. One day, I noticed a person whose editor looked markedly different from everyone else’s; the background was dark, and the code glowed with bright colors. To me, that was a sign of their superior technical knowledge. I needed to learn how to tweak my editor. The quest for customization led me to Vim (the workstation had Vim 6 installed out of the box). After all, if the goal is to stand out from the crowd, why stop at the color scheme? I went through the Vim tutorial, and it clicked with me immediately. It felt like playing a musical instrument: challenging but fun. It turned a mundane job of fixing bugs into an exercise in skill. I don’t remember exactly when and why I got interested in Emacs Daly journaling is one of the things I wish I started doing earlier. I’m nowhere near Stephen Wolfram’s level, but I enjoy going back and seeing what I was up to a few years ago. . Most likely, it was a result of my obsession with Lisp after reading Structure and Interpretation of Computer Programs and looking for a Lisp to interact with. I remember reading An Introduction to Programming in Emacs Lisp around 2010 and having a great time. I became interested in the editor internals. Using The Craft of Text Editing by Craig A. Finseth as a guide, I explored the source code of various editors to see how they worked: which the data structures they used to represent text buffers, how they interacted with extensions and implemented the undo mechanism. I found Vim’s source code somewhat messy, inconsistent, and hard to understand. Emacs’s source was spotless, well-organized, and well-documented. A dive into Emacs internals also revealed the inherent beauty of its architecture. Emacs is a Lisp machine that provides text editing and window management capabilities. It is a powerful, convenient, and friendly development environment for building dynamic text-driven applications in Emacs Lisp. Almost everything in Emacs is a Lisp object you can inspect, interact with, and access its documentation. That makes Emacs’ documentation system unparalleled once you master it. Emacs’ dynamism makes extending it much easier than any other editor. The feedback loop is airtight: you can immediately try out your code in the context of the editor you use to write it. Although I’m writing these words in Visual Studio Code, I always have my Emacs open I also have a tmux session with multiple nvim instances running. I use these when my pinky gets tired of holding down the Ctrl key. . Many things are easier in Emacs; I don’t think any software will ever completely replace it for me. It’s also my editor of choice if I need to implement an extension. Programming Emacs Lisp is a joy, especially compared to writing Vimscript. Boost.Graph I also must confess to a strong bias against the fashion for reusable code. To me, re-editable code is much, much better than an untouchable black box or toolkit. Donald Knuth, Interview with Andrew Binstock The evening of 2013 New Year’s Eve didn’t go as planned. I was on a cruise ship crossing the stormy Baltic Sea and couldn’t stay on my feet because of the seasickness. Instead of consuming tasty treats with the rest of the passengers, I was lying in bed and reading a book I took for the trip: The Boost Graph Library by Jeremy G. Siek et al. Most algorithm libraries require you to commit to a specific data representation, making integrating them into an existing project prohibitively expensive. That’s especially true for graph algorithms: The vertices and edges are usually implicitly defined and deeply embedded into other data structures, so it’s easier to re-implement the algorithm than to use a generic library. The Boost.Graph library solves this problem elegantly using Alex Stepanov’s ideas on generic programming. It uses a bag of tricks (type traits, property maps, visitors) to implement graph algorithms that can work with any graph representation you throw at them, given that you provide an adapter telling the library how to view your data structures as a graph. Even though I never had a chance to use the library in practice Given that I share Donald Knuth’s attitude opening this section, I would probably not use the library even if I had a chance. I’d rather write one page of interesting code traversing a graph than two pages of boring adapters required to invoke the algorithm. , its design helped me deepen my understanding of stl design and generic programming in general. It also helped me understand the motivation for advanced type-level programming features in other programming languages, such as type families in Haskell. Overall, Boost.Graph is one of the most enlightening pieces of software that I’ve never used. Bazel If make doesn’t do what you expect it to, it’s a good chance the makefile is wrong. Adam de Boor, PMake—A Tutorial I wrote my first Makefile around 2009 while working on a research project in computational mathematics for my degree. I already used make at work, but I didn’t need to understand how it worked. This time, I had to compile a fortran program mixing sources adhering to different language standards: from venerable fortran 77 to hip Fortran 2003. To get a deeper understanding of the tool, I referred to Managing Projects with GNU Make by Robert Mecklenburg. Most books on technology excite me: I become enthusiastic about the subject and want to try it out in practice. The book on make had the opposite effect. The complexity required to make builds correct and ergonomic made me yearn for a better tool One book that made me feel the same way was Modern C++ Design by Andrei Alexandrescu. The book is deep and beautifully written, but the terrifyingly clever and ugly tricks in the second chapter made me question the choice of the programming language. Another one is Autotools by John Calcote. . After my deep dive into make, I often fiddled with build systems at work: I introduced CMake to my first C++ project to replace complex and scarily incorrect Makefile files and replaced an inflexible Ant-based build system in a 500 kloc Java project with Gradle scripts that everyone on the team could contribute to. But all of the tools I tried, including CMake, Ant, Maven, Gradle, SCons, and autotools left me deeply unsatisfied. They were clanky, awkward, and hard to extend and compose. In 2016, I joined Google in Zurich. I heard about Google’s internal build tool, blaze, and couldn’t wait to lay my hands on it. Surprisingly, I didn’t need to fiddle with blaze, nor did I have to understand how it worked. I could copy some build targets and edit the dependency list, and the build worked as expected. blaze made correct and fast builds not just easy, but boring in the good sense. Only a few years later, when I attempted to use Bazel—the open-source version of blaze—for a toy personal project, did I have to understand the underlying model. Bazel was the final piece of the puzzle, together with Haskell’s typeclasses, Flume pipelines interface, and the TensorFlow 1.0 execution model, that made me understand the ubiquitous plan-execute pattern The Build Systems à la Carte article by Andrey Mokhov, Neil Mitchell, and Simon Peyton Jones explains how various build system designs map to Haskell typeclasses. Thomas Leonard’s CI/CD pipelines: Monad, Arrow or Dart? blog post is also a great read on this topic. . Bazel build file is a program that constructs a slice of the build artifact graph. Bazel rules don’t run the build commands; they declare how to transform inputs into outputs, and the Bazel engine figures out the rest. My relationship with the tool reached true intimacy when I helped transition dfinity’s build system to Bazel. Despite all the challenges I faced on the way, Bazel is still my favorite build system. It’s fast, correct, easy to use, and language-agnostic. Paraphrasing Bjarne Stroustup, I think a smaller, simpler, cleaner build system is struggling to get out within Bazel. I hope this core will someday reveal itself to the world and become the standard tool for building all software. Conclusion After presenting my cases, I find it tempting to look for a common theme. What makes a good enlightenmentware? For me, these are the key points: All these tools address a deep problem, and a kind of problem that I face every day, such as making programs on my computer cooperate, managing concurrent work streams, or generalizing a piece of code. They are round: they pack the most volume in the smallest surface area. unix surface area is tiny, but it unlocks much power. Emacs and Git are all over the place, but their core is small, sweet, and easy to appreciate. They invite and encourage you to explore their internals. It’s not only about being free and open-source; mastering them is also well worth the investment. What’s your enlightenmentware? Tell me on Hacker News or Reddit! Similar articles The numeric tower fiasco If composers were hackers Extending HTTPS outcalls→ ©Roman Kashitsyn Source Code",
    "commentLink": "https://news.ycombinator.com/item?id=40419856",
    "commentBody": "Enlightenmentware (mmapped.blog)407 points by zaik 22 hours agohidepastfavorite211 comments dist1ll 20 hours agoI would say the compiler explorer[0] fits the definition perfectly. It may seem like a straightforward piece of software, but it has immensely changed the way people discuss and share knowledge around compilers and performance optimization. I regularly feel the impact on the quality of forum discussions. There's a lot less speculation about if \"call X gets inlined\", or \"Y gets vectorized\". Bold claims can be supported or disproven quickly by sharing a link. And then you have tools like llvm-mca[1] or uiCA[2], if you don't mind going into the weeds. [0] https://godbolt.org/ [1] https://llvm.org/docs/CommandGuide/llvm-mca.html [2] https://uica.uops.info/ reply zem 18 hours agoparentalong those lines, the entire notion of a web playground (a sandbox where users can just write and execute or otherwise process code) has vastly reduced the barrier for checking out a project or experimenting with its behaviour reply ReleaseCandidat 11 hours agorootparent> web playground This _so_ much. Where in the past I've used Jupyter Notebooks for short, one off stuff or to test something, I now do that online for almost any language. Notebooks are still useful to write documentation though. reply roman-kashitsyn 6 hours agorootparentI was thinking about including Mathematica as enlightenmentware. Mathematica 6 (https://www.wolfram.com/mathematica/newin6/) was the first truly interactive system I used (we happen to have a box with a license at the university). It impressed me so much that I still have a lot of warm fuzzy feelings toward Stephen Wolfram and his work. Unfortunately, my relationship with Mathematica didn’t go anywhere: It was too expensive back then, and I never found a good use for it except for double-checking my homework. I tried other computer algebra systems, but they didn’t impress me as much. If you own a Mathematica license and found a good application for it, please let me know! reply coldtea 11 hours agorootparentprevWell, Notebooks main use case is a different purpose, not for trying one-off stuff or checking if some syntax is valid. It's for doing stuff step by step, annotating the steps and/or explaining each result. Web playgrounds are ok for testing some syntax (if you don't have a local REPL/easy way to test), but not for one-off stuff that involves file input or that you want to check against real environment assets. reply ReleaseCandidat 11 hours agorootparent> Well, Notebooks main use case is a different purpose, not for trying one-off stuff or checking if some syntax is valid. I know, but that's what I had used it for too. Like posting some code, for example in a HN post ;) As I've said, I still use it mainly for documentation. reply rnewme 15 hours agoparentprevI would dare say for me the https://pythontutor.com/ even more so than compiler Explorer. (hint: it's not just for python) reply AceJohnny2 13 hours agorootparent> (hint: it's not just for python) I would definitely have overlooked it, and it really needs a better domain name. reply ot1138 7 hours agoparentprevPahole is a related utility for high performance programmers. I've been able to attain orders-of-magnitude performance improvements in trading applications using it. reply delta_p_delta_x 20 hours agoprev> I couldn’t fathom why anyone would use Windows² I saw this sentence, was about to type something in response, and then I expanded the footnote (side note: is it really a 'foot'note if it's not in the footer of the page?): > I became significantly more tolerant since my early university years. Windows (specifically the NT family) is a great operating system. I even have it installed on my gaming pc so that I can buy games I never play. It's pretty rare to see such a balanced perspective with respect to Windows when someone starts off with 'UNIX'. reply 20240519 17 hours agoparentWindows 2000 was great IMO, and peak Windows! XP was peak if hardware compatibility is important. Then it went downhill slowly. UI decisions and telemetry and now needing an internet connection and a MS account to install and now Win11 refuses to install on perfectly good but older hardware. Microsoft cloudifying Office ironically makes going to Linux as a normie fairly easy as Office is the only thing I would miss. And mainly due to it’s dominance rather than it being great. Windows dark side is a shame as MS as a developer’s company is really good. VS, VSCode, Typescript and C# and F# are awesome. And also some changed to Windows are good. reply routerl 23 minutes agorootparent> Windows 2000 was great IMO, and peak Windows! > XP was peak if hardware compatibility is important. It all started with Windows NT 4. It had its own kernel and was enterprise (and network) focused, compared to Windows 98 (which was Windows 95 (which was Windows 3.11 minus the DOS host dependency)) with bolted on network capabilities. Windows 2000 is literally \"Windows NT 5.0\", and Windows XP is \"Windows NT 5.1\". The win95 lineage was decisively killed by Windows Me, after which the NT kernel took over the entire product line. reply GuB-42 10 hours agorootparentprevI put the peak at Windows 7. Windows 7 UI is like Windows XP but prettier thanks to GPU acceleration. Compared to the XP generation, it had better security, 64 bit support out of the box, it was an \"internet age\" version of Windows, but it could still run offline and wasn't too obnoxious with ads, telemetry, etc... It definitely got downhill after that. reply Zecc 8 hours agorootparentI mostly agree. But Windows 10 added virtual desktops[0]. Took them long enough! Its dialog when copying files in nicer as well IMO. [0] Windows 7 supported up to 4 virtual desktops, but not out-of-the-box: https://learn.microsoft.com/en-us/sysinternals/downloads/des... reply timeon 8 hours agorootparentprev> Windows 7 UI is like Windows XP but prettier Isn't 7 UI basically Vista? reply Aeolun 7 hours agorootparentSorta, but Vista was slow and stuttery, so they don’t feel the same. reply pxc 2 hours agorootparentIs Windows 7 fast on Vista-era hardware? reply Affric 16 hours agorootparentprevOccasionally I have to help my father with his windows computer and each time it actually gets worse. Edge. Suggestions. Like I am sure the actual fundamentals of the OS are fairly high quality but holy shit it sucks to use. reply h4kor 12 hours agorootparentI had a similar, frustrating experience last weekend trying to copy data from a phone to a windows machine. Windows is trying to hide all \"technical\" stuff so hard that it becomes impossible to do anything. reply ghostpepper 19 hours agoparentprev> From that moment on, unix followed me through all stages of my life: the toddler phase of keeping up with the cutting-edge Ubuntu releases, the rebellious teens of compiling custom kernels for my Thinkpad T61p and emerging the @world on Gentoo, the maturity of returning to Ubuntu lts and delaying upgrades until the first dot one release, and to the overwhelmed parent stage of becoming a happy macOS user. I started off as a Linux zealot and followed a very similar trajectory. I think it’s a sign of maturity to realize there is no absolute “best” in engineering, just a best solution in a particular problem space, and Windows is the best for a large number of users for a reason. reply aulin 13 hours agorootparentI started as a Linux enthusiast a long time ago, these days in my own time I use macos and I don't miss Linux that much, as long as I'm in the terminal I don't feel a difference. At daily job I'm forced to use Windows, the only thing that's keeping me from changing jobs is WSL2. I'm just not productive with mouse based tools, I need a terminal and powershell doesn't do it for me. Everything feels alien and less usable to me even after years, fonts, window decorations, file manager, UI inconsistencies between different tools. Everything seems slightly hostile and out of place. reply mhh__ 17 hours agoparentprevPedantry: it feels like false balance. Windows is fine, that's it. I use it every day, but it's got so many weird quirks that (that I can't do anything about, being the difference with Linux) it seems ridiculous to call it \"great\". I'm in the other room from my windows laptop. It's late, there's almost nothing running in it, the lid is closed, surprise surprise I can still hear the fans. reply xandrius 12 hours agorootparentFans are probably either a rogue service you can find in the task manager or hardware problem (maybe it needs some new thermal paste or a good air blow to remove dust). That has usually nothing to do with the OS itself. reply klibertp 3 hours agorootparentCounterpoint: I bought a System76 laptop last year. As it came, the CPU fan was never off for longer than a few seconds, even when there was no load at all. The fans are not very loud, but the coil whine just before re-enabling the fan was disturbing. The motherboard's firmware is open, however, so I rebuilt it with a slightly adjusted \"CPU fan curve.\" After flashing it, the fans now go online when there's an actual need, which is to say - rarely. The coil whine still happens, but hearing it once or twice a day is much less irritating than hearing it every 10 seconds. So it's possible the problem is on the OS side (I think we can agree the firmware is part of the OS) and it's sometimes possible to fix the problem in software... as long as you have control over it. reply Narishma 4 hours agorootparentprevAnd what about rogue services that come with the OS itself? Things like Windows Update, Windows Defender, the Phone app thingy, diagnostics policy service, the Xbox game bar or whatever it's called, the .NET optimization thingy and a dozen other things that like to wake up randomly and start consuming resources whenever they feel like it. Most of these things you can only disable temporarily, if at all, without resorting to dubious 3rd party tools. reply andrepd 11 hours agorootparentprevRecent Windows releases are notable for not running software without the consent or wishes of the user :) reply rusk 12 hours agorootparentprev> I can still hear the fans. I could not deal with that I’m sorry. I’d have to turn it off. The only way I could make Windows usable day after day in a previous job was to shut it down every night, and then I had the BIOS configured to start it up again and iterate through my extensive startup list before I got into work. Was quite effective like that. reply Aeolun 6 hours agorootparentMy Ubuntu machine is also noisy. I’m fairly certain it has nothing to do with system activity. reply lelanthran 10 hours agoparentprev> It's pretty rare to see such a balanced perspective with respect to Windows when someone starts off with 'UNIX'. I dunno. I've posted some pretty balanced opinions on OSes: I've frequently criticised Windows, Macs, Gnome, Plasma and more. They all each suck in their own specific ways. Most people acknowledge this.[1] Many people are just like me: we put up with the crap on each system in order to get work done. [1] The exceptions are almost always Mac and Gnome users: trying pointing out that UI can be objectively bad, and the default Mac/Gnome experience fails on more than a few of the objective UI metrics, and you almost always get multiple Mac/Gnome users saying that UI is all subjective. reply Aeolun 6 hours agorootparentTo some extend it is? After all, those users like that objectively bad UI that you are talking about. The fact that they’re idiots doesn’t make it any less subjective. reply DeathArrow 12 hours agoparentprevAfter 20 years of using Linux on the desktop (and FreeBSD, and NetBsd) in parallel with Windows, I gave up. I don't like to always configure things, I don't need 20 different ways to accomplish a task and some of the software I use is not available on Linux. So I went Windows only for the desktop since 4 years. Of course, when I had to do something server side, it was Linux only. Recently I bought a MacBook Pro and the experience is very Windows like. I don't have to mess with the OS and it just works. reply worksonmine 8 hours agorootparentStick to one Linux distribution and you can have the \"one size fits all\" experience you want. Who's forcing you to unixhop and constantly fiddle with your stuff? I'm on Debian and never have to change anything and my setup just works. reply bradstewart 2 hours agorootparentFor me at least, it's personal discipline. If I can fiddle and change stuff, I will. reply pie_flavor 17 hours agoparentprevOn the contrary, footnotes on a webpage belong in the margins. Putting them at the foot of the article in current year is like banging rocks together. reply analog31 16 hours agoparentprevThe main thing keeping me on Windows is touch screen support. I know I'm a freak for wanting it, but we all have our ergonomic preferences. Beyond that, I treat the OS as an appliance. Most of the software that I use is platform independent. reply ctenb 9 hours agoparentprevI think the peak of windows was when they introduced WSL, making windows the ultimate crossplatform dev OS. reply flobosg 11 hours agoparentprev> (side note: is it really a 'foot'note if it's not in the footer of the page?) These are rather sidenotes (or margin notes). reply bitwize 9 hours agoparentprevI'm not the Linux zealot I was as a kid, but I can never see myself going back to Windows. The particular niceties of a Unix environment are ones that I've come to rely on, and I could never go back to managing all my files and data through rat wrestling, the way Windows seems to want you to do. That said, I can see the merits of Windows, especially for normies or video game players. It's just absolute friction town for me to use it. reply hu3 20 hours agoprevDocker is one of these tools for me. The unspeakable amount of my time and headache it saved during my consulting career puts a smile on my face. Docker allows me to quickly iterate over the steps required to run ancient projects. Not having to install 5 different relational database servers on my host OS alone is worth the learning curve. Also crucial for running concurrent, reproducible, Python environments in my laptop. reply XorNot 18 hours agoparentWhile it takes heavily from the UI, podman with user namespaces enabled is the completion of this idea for me. No more sudo requirements, greatly reduced attack surface, isolation as a \"user\" concern. It sits happily in my UI doing exactly what I need it to do for all these use cases. reply udev4096 13 hours agoparentprevImo, making a Dockerfile for an ancient project isn't always easy reply klibertp 3 hours agorootparentIn comparison to what? Making a chroot env for such a project is way harder than dockerizing it. VirtualBox and Vagrant might not be much harder, but are slower to the point of being irritating. I might be missing some alternatives, but among the approaches I tested, Docker is still the easiest way to build and run unfamiliar projects. reply fmbb 12 hours agorootparentprevBut usually easier than making it build locally. reply patrickmay 4 hours agorootparentAnd more easily repeatable. Plus, it avoids polluting the full environment. reply WatchDog 15 hours agoprevI can't really think of a good example that other people haven't mentioned, but I have an anti-enlightenment piece of software, spring framework. Spring actively hindered my ability to understand the simple concept that is dependency injection, and I know I'm not alone. Many a Java developer think that you need a big complicated framework to pass dependencies into your modules, instead of having your modules fetch them for themselves. This isn't a criticism of spring per se, it's fine, it provides value for people, but I think it can lead people to build software that is more complicated and less portable than it needs to be. reply penguin_booze 12 hours agoparentI've had my short stint at using Spring. Often I was dropped into a project where it's already setup and working. When something breaks or I want to extend/modify what was working, I hit a wall in terms of discoverability: how it's been working all this time, and how to find a suitable level of documentation to help me. There are reams of documentation for Spring, but nothing of the kind that'll help me if I'm lost. So, from my perspective, it's write-only framework; it's hard to reason back. reply WatchDog 11 hours agorootparentI spent a lot of time with spring, and became a bit of an expert on it, I'm able to debug and understand most issues that come up with it. Still, I agree wholeheartedly, there is too much magic, it's very difficult for someone to come into a spring codebase without a lot of background experience, and understand how things tie together, and I don't think it's really time well spent acquiring that experience. reply The_Colonel 10 hours agoparentprev> Many a Java developer think that you need a big complicated framework to pass dependencies into your modules, instead of having your modules fetch them for themselves. Not sure what you mean by \"module\" here, but DI means that the dependencies are defined externally to the actual business logic, which kinda contradicts this \"fetch them for themselves\". I think the problem with Spring is that it has too many features, too many different ways to do the same thing, too much configurability. Historically, Spring jumped on most hypes, tried to basically support everything. We ended up with this complexity monster, but this also made it popular. Spring is the Oracle DB of the backend development in the sense that you won't ever get fired for choosing it, it's the safe choice which will support in some way everything you might need to do. reply kookamamie 14 hours agoparentprevI would dare to say Dependendency Injection as a concept is unnecessary and creates more problems than it solves. reply jerf 4 hours agorootparentDependency injection is a basic tool of writing robust, testable code. The alternative is strict hard-wiring of the dependencies, which deprives you of places your code can be tested. But do not confuse \"dependency injection\" with \"massive heavyweight opaque framework with a billion bells and whistles that breaks constantly\". Dependency injection includes things like passing in a handle to the SQL database instead of it being a global variable, which your test suite uses to switch between various test instances of the database instead of the target code being hard coded to some variable, or even hard coded with its own connection credentials. If you're not using dependency injection you are almost by definition using a lot of global variables. I'm as happy or happier than the next programmer to be a contrarian, but, no, the collective wisdom on those is dead on. They're deadly and should be avoided. Dependency injection is the biggest tool for that. Not having dependency injection creates more problems than it solves. However, I'm not sure that most \"frameworks\" for it aren't creating more problems than they solve. Probably one of the classic examples of lopsided accounting, in this case looking at the benefits but neglecting the costs. Anything looks good if you do that. But a lot of \"frameworks\" seem to bring along a suite of middling, sort of convenient benefits at the cost of massive opacity, unreliability, and the bad kind of magic. Not a good trade for a lot of programs. reply throwanem 14 hours agorootparentprevOne might about as sensibly say the same of functions being able to take arguments. If this is meant to illustrate the damage working with Spring does to understanding the concept, then it's an excellent, if mildly horrifying, illustration. reply The_Colonel 10 hours agorootparentprevOne elementary need DI (or perhaps IoC more generally) provides is the ability to mock certain parts of your application for automated tests. While I'm not a fan of mocking too much and prefer higher-level tests (integration/component), mocking is still quite often needed. Is there some alternative to IoC/DI? reply poorlyknit 9 hours agorootparentEffect systems spring to mind but they're rather esoteric (in the Java world). reply WatchDog 12 hours agorootparentprevI'm still very much pro DI as a concept. I don't think DI itself really causes any problems, the solutions designed to save you from a little bit of boilerplate code, cause the problems. reply stonemetal12 3 hours agorootparentAre you pro DI as in \"Dependency Injection\", or pro DI as in \"Dependency Inversion Principle\"? DIP is a good way to build software. When injecting dependencies becomes so complex you need a framework or need a separate concept of DI (sans P) then I think something has gone wrong, incidental complexity has won. reply kookamamie 12 hours agorootparentprevI mean the original statemement from the perspective that only certain languages/environments (Java, etc.) propose DI as a solution. E.g. in my current language of choice, C++, DI is nowhere to be found. reply ReleaseCandidat 12 hours agorootparent> in my current language of choice, C++, DI is nowhere to be found STL, for example when passing explicit allocators. You can even call any higher order function using dependency injection. And of course there are C++ codebases that look like Java - the pattern book works with C++ too. reply WatchDog 12 hours agorootparentprevC++ has constructors doesn't it? reply ReleaseCandidat 12 hours agorootparentAnd higher order functions. reply RamblingCTO 13 hours agoparentprevTotally agree. Spring and actually everything in that realm is plain horrible. DI is awesome on it's own (especially if you do hexagonal architecture), but spring hides behaviour and brings in undocumented changes on updates and things like that (same for any spring related project and hibernate). That's my biggest problem with it. reply 8s2ngy 14 hours agoprevMagit, the git client for emacs, fits the bill perfectly. It is a masterclass in simplicity, effectiveness, and discoverability. It is one of those rare tools that makes you better at the underlying tool it abstracts over; instead of introducing its own jargon and workflow it exposes git's capabilities better than git does. reply beautron 10 hours agoparentI'm sure Magit is lovely. But I can't resist sharing the story of my bad experience with it over a decade ago (which has left me scared away). It was maybe early 2012, and I was excited to try Magit. I got it set up, and called 'M-x magit-init' from a source file I was editing. My understanding was that this would create a new git repo in that source file's directory, ending up with something like \"/home/beautron/myproject/.git\". But something else happened. The git repo was put here instead: \"/home/beautron/myproject/~/myproject/.git\". Note the peculiar \"~\" directory, created inside the project directory. Huh. Weird. Well, let's get rid of this mess and try again. I went to the project directory in my bash shell, typed \"rm -r ~\", and hit enter. Somewhere between my mind firing the signal to hit enter, and enter actually being hit, I realized with horror what this command would do. But it was too late to cancel the brain signal. I didn't lose everything, because I had not typed something worse like \"rm -rf ~\", and somewhere in my home directory tree was a read-only file. So the command only deleted so far as that file, and then paused to ask for confirmation. I estimated I lost about half of everything (the first half of the alphabet was gone from my home directory). The most frustrating thing was not even being sure what all I had lost. On the plus side, this experience improved my regimen around backups. As I was trying to salvage the wreck of my system, I had a separate laptop out on the side, where I was trying to get some help, or maybe just some sympathy, from the #archlinux irc channel on freenode. But the two people who responded to me on the channel were very snarky to me. I felt they thought I was clearly an idiot for having run that command. The irc people refused to believe that Magit created the \"~\" directory. They were convinced I had done that myself, with some other series of stupid commands. (If you had to guess the source of weird \"~\", who would you choose: the established Magit project, or the guy who just deleted half his home directory?) But a short time later I was vindicated! From Magit's github issue 383, Feb 29, 2012: > So if you're editing \"~/Temp/foobar/boo.txt\" and call \"M-x magit-init\" which defaults to \"~/Temp/foobar\", instead of creating a git repo in \"/Users/jimeh/Temp/foobar\" it creates it in \"/Users/jimeh/Temp/foobar/~/Temp/foobar\". Source: https://github.com/magit/magit/issues/383 It was a long night (and I had to leave on a trip the next morning). Now it's fun memory, perhaps with a number of lessons in it. reply pama 6 hours agorootparentSorry to hear. Codes have bugs that evolve over time. I hope you try magit again some day. It is sometimes hard to remember to apply correct quoting in a shell, so if in Emacs you can also use dired for dealing with such errors in less risky ways. reply akho 5 hours agorootparentprevDo you have backups now? reply ziddoap 5 hours agorootparent>On the plus side, this experience improved my regimen around backups. reply jchw 21 hours agoprevDoubt anyone will be surprised by this from me, but, Nix, 1000x. The amount of crazy stuff you can make work with Nix and Nixpkgs is nuts. This weekend someone pinged me wanting a static build of a Rust binary that had some gnarly bindings to C++ libraries. In under 100 lines of Nix, we have everything: static musl-based build, dynamic glibc build. Want an AppImage? `nix bundle` with an AppImage bundler. Want an OCI image? dockerTools.buildImage on top of your derivation. Throw it in GitHub actions using the Determinate Nix Installer action and you get automatic caching of the Nix store using GitHub actions cache; pretty useful since neither musl Rust nor static LLVM are cached in Hydra. Want to share your cache? Pipe a list of Nix store paths to Cachix, then they can pull it down, or add the Cachix GitHub action to automatically pull from and/or push to it for the CI build. So if anyone wanted to re-use your cache from GitHub Actions CI runs, they could, provided they trust you. You can even cross-compile with MinGW, or run it on macOS. It's a hugely complex time sink, but my God, it's great. Whereas I don't generally recommend people go down the NixOS rabbit hole unless they're convinced it's right for them, I definitely think Nix is worth having in your toolbelt, it's ridiculously versatile. reply helpfulclippy 14 hours agoparentI had only a dim awareness that Nix was even a thing before a few weeks ago. Then somehow I decided that I needed to commit to it 100% and make NixOS my daily driver and make myself learn this. It's been a lot of fun. Like, Dwarf Fortress kinds of fun. The kind of fun where when I first looked at it I thought it was insane inscrutable nonsense, and now I kind of wonder what happened to me that it's kind of making sense now. The kind of fun where I keep telling myself I just want to make some tiny little thing work, but actually I find excuses to rabbit hole down a bunch of different pathways and find amazingness under every stone. The kind of fun where I know better than to try to count how many hours I've spent on this now. Except unlike Dwarf Fortress, I feel like things are actually improving over time instead of shambling ever-onwards towards an inevitable downfall. So I guess maybe it's more like the kind of fun the very first time I installed Linux and didn't know my way around anything. I'm surprised how much I enjoy customizing things now. I always thought of my desktops sort of like betta fish before -- like, I've taken care of them, but also known better than to get too attached. Eventually the reformat is gonna come, and I'm never gonna set things up QUITE like I had it before. That's definitely not true now. I could start from scratch and be up and running with my entire suite of applications, themes, add-ons and configurations in no time, because it's all just a git repo of nixfiles. reply pxc 1 hour agorootparent> I had only a dim awareness that Nix was even a thing before a few weeks ago. Then somehow I decided that I needed to commit to it 100% and make NixOS my daily driver and make myself learn this. It's been a lot of fun. Like, Dwarf Fortress kinds of fun. The kind of fun where when I first looked at it I thought it was insane inscrutable nonsense, and now I kind of wonder what happened to me that it's kind of making sense now. The kind of fun where I keep telling myself I just want to make some tiny little thing work, but actually I find excuses to rabbit hole down a bunch of different pathways and find amazingness under every stone. The kind of fun where I know better than to try to count how many hours I've spent on this now. That's absolutely what diving in with NixOS was like for me. Wonderful description. :) > I'm surprised how much I enjoy customizing things now. I always thought of my desktops sort of like betta fish before -- like, I've taken care of them, but also known better than to get too attached. Eventually the reformat is gonna come, and I'm never gonna set things up QUITE like I had it before. That's definitely not true now. Yes. NixOS makes customization feel more worth it than other operating systems can! I first heard it expressed best in this video: https://www.youtube.com/watch?v=17-TRCpDizA reply emporas 19 hours agoparentprevNix is one of the reasons open source wins in the long run. I would have never imagined that package dependencies could be installed and managed across programming language boundaries, kernel boundaries, shell boundaries, dotfile boundaries etc. Given that computer complexity grows exponentially all the time, then at some point everyone will be forced to use something like Nix. I agree with the article with Emacs and Unix/Linux. reply fire_lake 12 hours agorootparentFunctional Programming approaches usually win in the long run. This is a common thread in the article too, although not explicit. reply trashburger 7 hours agorootparentI think you mean declarative. Whether something is FP or OP usually doesn't matter too much (unless you go to the extremes of each end of the sprectrum a la AbstractBeanFactory/IO Functor State). Declarative solutions last longer by virtue of their declarativeness; the underlying implementation can evolve as long as the declarative layer is untouched. reply zarathustreal 7 hours agorootparentIsn’t a class a declaration (of a class type)? Isn’t a function a declaration (of an algorithm)? I’d assert that every programming language is declarative. Especially once you get enough written to constitute a DSL. reply bubblyworld 2 hours agorootparentI think they mean declarative in the sense that you write down want you _want_ in a solution (prolog/datalog/terraform/etc) without specifying exactly how to compute it (day to day programming languages). reply memothon 19 hours agoparentprevWhat's the right way to learn how to use NixOS? There are a few different approaches to doing everyday development (oh use Flakes or just make it a default installed package) that it's tedious to do lots of things I've found. reply atrus 16 hours agorootparentThe No Boilerplate video is what got me started: https://youtu.be/CwfKlX3rA6E?si=hJZ_mm9vaeKI0w8V It's just super nice having everything in a git backed config, for multiple systems. Working with NixOS seems like I'm simultaneously working 10 years in the future and 10 years in the past. reply pxc 14 hours agorootparentprev> What's the right way to learn how to use NixOS? Just commit. Make it your daily driver, engage the community for help, and learn whatever you need to learn to do things the idiomatic way. Then settle in and get comfy. That said, I hear you about the paralyzing number of choices. Here are some okay choices you can fall back on when you get dizzy: - use flakes with direnv for all your projects - when in doubt, just write the damn package - need to use some Python library that depends on another Python library that's not in Nixpkgs? Package 'em. Package all the recursive dependencies, then package the library you wanted to use. - want some program that's not in Nixpkgs? package it - want to reuse a binary built for a traditional distro? package it. Look at examples in Nixpkgs of similar things (e.g., Discord, Steam) - seek community engagement on Discourse and Matrix, not elsewhere (not Reddit, not Discord, not Slack, not IRC) This is the best way to learn NixOS, imo. But if it seems like too much, it's not the only way. reply null_point 4 hours agorootparentprevI found using Nix package manager on my current daily-driver OS was a great way to break the ice. After translating my dotfiles to Nix and figuring out my project-specific development workflow I had given myself a strong foundation for NixOS. Jumping into the deep end and going straight to daily-driving NixOS, is certainly also a good option. reply jchw 17 hours agorootparentprevI don't think there's a right way to do it, you are correct in that learning NixOS is pretty tedious. Re: flakes, my personal opinion is to use flakes. While Flakes are imperfect, they still provide a lot of functionality that Nix doesn't otherwise have. In my mind, it's like Nix's equivalent of \"Go modules\" or something like that. I do feel like people who do not like flakes make many valid points (the boilerplate, the fact that the top-level flake expression is a subset of Nix for some reason, etc.) but the argument isn't that those problems shouldn't be solved, it's that flakes are a sub-optimal design. Since they're so proliferated throughout the ecosystem though, it is quite unlikely that Nix or any prominent fork will outright drop flakes support any time in the near future. For better or worse, Flakes are part of the Nix ecosystem for the foreseeable future. In my opinion, one may as well take advantage of that. If you haven't already, I'd get your feet wet with installing Nix on a non-NixOS machine first, and please feel free to ask questions about Nix in the NixOS Discourse \"Help\" section. I have some recommendations: 1. https://github.com/nix-community/nix-direnv - Since Nix derivations usually wrap around other build systems, the entire derivation is recomputed when any file in it changes; using direnv, you can just get your normal dev tools upon cd'ing into your project directories. This gives you a lot of the benefits of Nix during local development, but with your normal stack, and without needing to globally install anything. Importantly, this works around the problem of needing to rebuild your project every time a file changes; Nix caching isn't granular enough (at least when wrapping around other build systems as it normally does.) 2. If you are trying to build something, chances are you can find inspiration in Nixpkgs. Are you curious how you might package a Bevy game? No problem: literally search \"bevy\" on the Nixpkgs GitHub repo and see what comes up. I found a derivation that does: https://github.com/NixOS/nixpkgs/blob/master/pkgs/games/jump... 3. If you use flakes, you should keep the flake \"schema\" handy. There are a lot of different kinds of flake outputs and there are different ways to specify the same thing, which is somewhat needlessly confusing; keeping the flake schema handy will make it easier to understand what Nix is looking for in a flake, which might make it easier to see what's going on (especially if it's obfuscated.) The most important takeaway here: A command like `nix run flake#attr` will try multiple different attributes. https://nixos.wiki/wiki/flakes#Flake_schema 4. Likewise, I really recommend reading up on what NixOS modules are. NixOS modules are the basis for configurations on NixOS, and having a clear understanding of what is even going on with them is a good idea. For example, you should understand the difference between the Nix language's `import` directive, and using the NixOS modules `imports` attribute to import other NixOS modules. Understanding how the configuration merge works saves a lot of headache, makes it easier to understand how people's configurations works, and also makes it easier to modularize your own NixOS configurations, too. https://nixos.wiki/wiki/NixOS_modules Unfortunately though, there's just no way to make it \"click\", and I can't guarantee that it's worth all of the effort. For me, I felt it was, but yes, there's no one correct way to do it. But please feel free to ask questions if anything seems confusing. reply memothon 16 hours agorootparentThanks for the inspiration. I actually already have nixOS installed on another laptop, but lapsed back to my Ubuntu machine out of a bit of frustration. I'll try it again and see how far I get with these tips. reply SSLy 9 hours agorootparentprevPoint 4 is incredibly under-marketed. Almost all of Nix/NixOS documentation focus on the language-y and build-system-y parts of Nix, and the NixOS modules are usually not talked about. Terrible state of docs doesn't help. reply pxc 3 hours agorootparentKnowing what are the 3 official manuals for the 3 most important projects in the core Nix ecosystem (the manuals for Nix, Nixpkgs, and NixOS, respectively) that together make up the core of the official docs will save newbies a lot of trouble. > the language-y and build-system-y parts of Nix Language-y manual: https://nixos.org/manual/nix/stable/language/index.html Build system-y stuff manual: https://nixos.org/manual/nixpkgs/stable/ > the NixOS modules Using the NixOS module system in the sense of leveraging existing modules is the main topic of the NixOS manual: https://nixos.org/manual/nixos/stable/ Details about the module system mostly live in this section of it: https://nixos.org/manual/nixos/stable/#sec-writing-modules The piece they don't tell you is that as a NixOS user, you generally want to look for a NixOS module that supports your application first. Only if you don't see one should you then directly/manually install a package into, e.g., environment.systemPackages. In other words, search here first: https://search.nixos.org/options And search here second: https://search.nixos.org/packages The landing page that ties these reference docs together and also contains a lot more example-centric and tutorial content is nix dot dev, here: https://nix.dev/ Imo nix.dev is a great entrypoint and quickly getting better. In addition to providing a bit of a map to help you navigate the official docs, it includes the best official starting docs on flakes, and links to the highest-quality external tutorial and expository material about Nix. Make a mental note about the 3 big reference manuals, and bookmark nix.dev, and you have everything you need to learn your way around the official docs. reply xyzzy_plugh 20 hours agoparentprevThis 100%. It's the gift that keeps on giving. reply nicce 20 hours agorootparentYou need to give the first gift, however. Time. And lot of it. reply jauntywundrkind 19 hours agoparentprevI see tons of people very happily using Nix, but also from what I've seen Nix is one of the most opaque hard to understand inscrutable systems on the planet. The language is impossible to learn, and my limited experience has been that there are extremely few good guides to peering under the covers. Nix is one of the most powerful & well-used bits of modern computing, hugely adopted and loved, but my limited understanding (and what's kept me broadly uninterested) is that it is on the wrong side of the Age of Reason trying to overthrow the Age of Magic war. reply jchw 17 hours agorootparentHonestly, I suspect a lot of what makes it opaque is the fact that it hinges a lot on functional paradigms, this definitely made it a lot harder for me to understand. The trouble is that now that I understand it, I'm not sure how they could architect this in a more scrutable way. This is the problem. I think most of us who use and love Nix understand that it is too complicated, but it's too complicated because none of us can figure out how to make it simpler. reply infogulch 16 hours agorootparentI'm also nix-curious and intimidated by the wall. I feel like things would be a lot easier to grok if Nix actually had a static type system or even something opt-in instead of \"the object has whatever fields I happen to put in it, good luck lol\"-typed. reply colordrops 14 hours agorootparentprevThe language is not the hard thing, it's actually straightforward once you get it. It's the crazy amount of convention and the number of layers in the stack that make up the Nix ecosystem that takes so long to grok. reply renewiltord 20 hours agoparentprevWait, this is a pretty good sell. I'm going to give this Nix thing a shot. All the other times it's posted people talk about things I don't care like replicable builds and stuff. reply __MatrixMan__ 20 hours agorootparentMy guess is that if you use it long enough for it to start being useful, you'll find that \"replicable builds\" solves a wider variety of problems than you initially thought it did. At that point, the hard part becomes getting your co-workers to recognize that all of these little problems that they perceive as separate are actually just facets of the the same huge nondeterminism problem. reply renewiltord 19 hours agorootparentI'm sure. It's just not a selling point for me right now. reply pxc 2 hours agorootparentPerhaps that's exactly the problem: 'reproducibility', 'determinism', 'functional'-- these are unfortunately too much 'insider language' to be great sells to most people who aren't already FP or Nix people... even when they're closely related to benefits those prospective newcomers might enjoy! I'm glad you've identified a use case that appeals to you. Have fun! reply cryptonector 1 hour agoprev> ClearCase is confusing like a Russian novel: All the characters have strange names, the plot is complex, and it doesn’t end well. Well done. > Git removed the friction from using version control; there was no excuse not to version anything of value anymore. Merging branches with Git didn’t cause anxiety disorders. The staging area—confusingly named index—became essential to my workflows. But my favorite feature was the breathtaking beauty of Git’s design, the elegant mix of distributed systems, acyclic graphs, and content-addressed storage. I love seeing glowing reviews of Git. reply goeiedaggoeie 1 hour agoparentmany moons ago, before git was released, I put svn in front of clearcase and had people locally commit to that, trap into hooks of svn, check the file out of clearcase commit and recheck-in. the whole engineering group I worked in switched to using the svn and sped up, although user contribution was thrown out in the clear case log. reply nighthawk454 19 hours agoprev> They are \"round\": they pack the most volume in the smallest surface area. unix surface area is tiny, but it unlocks much power. Emacs and Git are all over the place, but their core is small, sweet, and easy to appreciate. I really like this 'round' concept, seems very precise. Maximum interface area / use cases (surface area) with minimum core volume. reply mbwgh 7 hours agoparentIn Ousterhout's \"A philosophy of software design\", this aspect is described similarly via \"deep classes\", as opposed to \"shallow classes\". reply ssivark 15 hours agoparentprevUnfortunately, a sphere of characterized by exactly the opposite property. The article switches the intended meaning of surface and volume to get away with the metaphor, but I’m less than thrilled about the metaphor. reply kergonath 11 hours agorootparentI am not defending this specific metaphor as I am not sure it is really good in this case. But they are right about this specific property of spheres: they have the highest volume/surface ratio (i.e. that’s the way of minimising the surface area of the enveloppe for a given volume). reply infogulch 13 hours agorootparentprev\"encapsulate the most complexity with the smallest API\" seems to fit the metaphor better. reply sssilver 18 hours agoprev> I also must confess to a strong bias against the fashion for reusable code. To me, \"re-editable code\" is much, much better than an untouchable black box or toolkit. I had never come across this particular thought by Knuth, but this hits home so hard. It feels that most of our productivity is a function of how re-editable the code is in the codebase we operate. It’s intriguing to think about what makes code re-editable vs simply reusable. reply donatj 15 hours agoprevWe used SVN at my first job in 2006 and I had the exact opposite experience with it. I never fully understood what I was doing, nothing made rational sense, merges were an absolute nightmare, and somehow I would always ended up corrupting the repo and had to pull a nightly backup to get out of the broken state. Git was a literal breath of fresh air in comparison. I fell in love hard and fast. Everything just made sense, even if our workflow using git am patches seems downright ancient these days. Friends at the time tried to sell me on hg, but I was in love. reply WatchDog 15 hours agoparentI thought SVN was great, easy to use, and very intuitive, that is until you had any merge conflicts. At the time, it worked very well for our small team, I imagine it would work less well for large teams on a single codebase. I miss having a commit serial number. reply ahartmetz 14 hours agorootparentI used kdiff3 and could never understand people complaining so much about SVN merges. Now I use kdiff3 in Git and it's fine, too. What isn't fine (though occasionally improving) is Git's UI and mess of termnology and concepts. reply WatchDog 12 hours agorootparentIt's been a long time since I used it, I don't remember what I was using for diffs, but I suspect it was just whatever the default built in diff support was. It occurs to me that many people these days use git with github exclusively, and have it configured to only allow commits via PR, and only allow either squash or rebase merges, it's kinda SVN with extra steps. reply globular-toast 11 hours agorootparentprevYou have to sacrifice the serial number to get a distributed system. Well worth it IMO. But if you really wanted you could tag every commit on master with the next number (should be easy to do with a hook). reply kaycebasques 11 hours agoprev> These tools capture our imagination, open new possibilities, and affect how we design our own systems. Puppeteer/Playwright fits the bill for me. Learning how to scrape websites with those tools subtly changed how I approach lots of other programming tasks and opened many new possibilities in both personal and work projects. The event model of the web platform has had a deep effect on how I generally think about systems. Some people may throw tomatoes at me for this last one: Google Apps Script. The docs and API aren't great, but it has opened so many new possibilities for automation of things related to my personal and work Google accounts. Re: capturing my imagination, physical computing with RPi/Arduino. I've also gotta admit that GenAI APIs have been an explosion of new possibilities for my line of work (technical writing) Thank you to OP for creating a productive and inspiring topic (enlightenmentware) for us to discuss and share ideas around reply kreyenborgi 10 hours agoparentCan you give some examples of what you've created with Google Apps Script? I've used it a little, but it's one of those things I always spend an hour building up courage to attempt (because of the slow feedback loop, browser-based testing, confusion about different deployment types, permission models and extension types). In such ways it's the opposite of writing Emacs extensions, though I can see how it promises to give something similar in the end. reply kaycebasques 5 hours agorootparentIn a previous job I rigged up the submission of a Google Form to send out an FYI email to relevant stakeholders and to create a GitHub issue (glossing over the details of why this was useful) Apps Script also has an API that essentially allows me to expose my spreadsheet data as a web service that returns the data as JSON For a while I was doing daily journals in Google Docs. I used Apps Script to auto-populate the heading for each day, e.g. an H1 with the text \"21 May 2024\". I've done lots of little auto-population things like this reply susam 6 hours agoprev> But occasionally, we discover a piece of software that transcends mere utility. These tools capture our imagination, open new possibilities, and affect how we design our own systems. For me, it was DEBUG.EXE on MS-DOS. This humble debugger allowed me to peek into interrupt vector tables, inspect the content of ROM, learn how MS-DOS boots from scratch, etc. I fondly remember the days when armed with an assembler, some knowledge of the CPU and the computer architecture, we could plunge into the depths of the system, unravelling its intricacies to our heart's content. reply 20240519 17 hours agoprevI would add Haskell - the rabbit hole to category theory and all the mad stuff Haskell people get into (compilers, proofs etc.) Bitcoin - love or loath it, technically it is a marvel. At the time Bitcoin came out I was musing on the same problem but never could figure out how to avoid double spends and would never have come up with blockchain! reply tsimionescu 17 hours agoparentNow that the blockchain itself is actually not novel, it was already a part of Git at the time of the Bitcoin white paper. The novel part, I believe, was the proof-of-work concept. reply kevindamm 16 hours agorootparentEven proof of work was not novel, there were proposals for fighting email spam with similar techniques. Bitcoin's fortune is combining the right pieces at the right time and getting sufficient buy-in to become relevant and more difficult to ignore. reply 20240519 15 hours agorootparentprevYes I take blockchain to mean proof of work (or proof in general) validating blocks such that you can track time to some extent in a system that cannot be sure of time because of being distributed and nodes not being trusted. reply roman-kashitsyn 10 hours agoparentprevHaskell was one of my most important discoveries; it affected my thinking and approach to software engineering the most. However, as I mentioned in the opening section, I deliberately removed programming languages from candidates for several reasons: 1. They received enough praise already. 2. My presentation would be very biased. 3. The article would be way too long. reply ryukoposting 6 hours agoprevI'll nominate another text editor for this subject: Sam. It's a graphical text editor, but it doesn't resemble any other graphical editor I've ever seen. I've been told it's \"like ed on steroids\" but I can't verify that statement as I've never used ed. Sam made me reconsider the way I think about the mouse. In Sam, the mouse is integral to your workflow, but the way you use the mouse is unlike any other editor I've ever used. Brief strokes blended with keyboard stuff. Sam translates remarkably well to the trackpoint, even though it was designed a decade before laptops existed. Its command language is somehow as simple as sed, but (almost) as powerful as vim. It's really old software now, so it lacked some features I wanted (auto-indent, mainly). The source code is simple enough that I just added auto-indent myself! I used Sam for the last two years of my undergrad. I wrote Python, C++, Nim, and Verilog using Sam. Sam shaped the way I think about computers in so many ways. At a glance it would seem idiosyncratic and weird. Having used it, I consider it to be wonderfully ergonomic and creative. reply roman-kashitsyn 1 hour agoparentI believe Sam is a predecessor of Acme[1][2], which still attracts new users today. One of its notable users is Russ Cox, the tech lead of the Golang team. I haven’t used Acme yet, but I want to try it out one day. If you’re interested in learning more about ed, I highly recommend “Ed Mastery” by Michael W Lucas [3] [1] https://doc.cat-v.org/plan_9/4th_edition/papers/acme/ [2] https://www.youtube.com/watch?v=dP1xVpMPn8M [3] https://www.amazon.de/gp/product/B07BVBSDNZ reply comment_ran 17 hours agoprevOne day, I'm working on a Linux machine with my Emacs open, I'm using a Bazel to clean my today's to-do list project. And I open the browser to find a person who wrote a blog about Boost.graph which I never heard about, but I'm really interested to look at. I finish this writing, save the buffer and =C-c g= to lunch magit to write a commit message \"good day\", then pushed to my git repo. reply bbkane 17 hours agoprevComing from C++ and Python, Go's packaging + deployment tooling really enlightened me. It's SO EASY to depend on things and deploy my apps, I love it! I've also heard Rust gives folks similar warm fuzzy feelings in regards to building and deploying, one day I'll try that too reply nox101 17 hours agoparentI felt that way about node and yet node lead to an explosion of poorly written and designed packages and constant notifications about needing to upgrade project X because it depended on Y which depends on Z and Z has some DoS issue if you pass the wrong regex to it. I don't feel confident that rust won't go the same way when I tried to update the rust docs (https://github.com/rust-lang/docs.rs) cargo build Downloaded 541 crates (55.2 MB) Seriously? 541 crates for a static site generator for docs? rust is clearly off to copy npm in all of it's faults. I have no idea if go is similar in the explosion of dependencies and supply side attack surface area explosion reply steveklabnik 2 hours agorootparent> for a static site generator for docs? docs.rs has a lot more to do than just that. But also, actually building those static pages takes a lot. To do so, it has to actually build every crate, sandboxed, of course. This makes it closer to \"universal CI for the entire ecosystem\" than \"generate a few html pages.\" If you look at the dependencies, they're pretty normal for a website that does this kind of thing. It's roughly 80 dependencies, then 11 used for development only, and a couple more that are build-time only. The larger number is the sum of all transitive dependencies. reply nicce 16 hours agorootparentprev> Seriously? 541 crates for a static site generator for docs? rust is clearly off to copy npm in all of it's faults. I have no idea if go is similar in the explosion of dependencies and supply side attack surface area explosion In Rust, it is design choice. They try to keep the standard library small, and let community create competitive packages. You see the result in those numbers. It is hard judge based on those numbers only. reply kergonath 11 hours agorootparentThe philosophy does not really matter, though. Any one of these dependencies could be a vector for a supply chain attack and all these libraries being updated independently and a synchronously is just asking for 2 dependencies requiring incompatible version of something else. We’ve seen this happening already and it usually ends up in 2 ways: - the node approach: who cares? YOLO! - the particular circle of hell that is Python and its 25 ways of doing virtual environments. Wait, 26, yet another one just dropped. For all its faults (and there are some), a centralised approach like with Boost has some merit. reply nicce 7 hours agorootparentRust, the language itself depends on 220 packages: https://github.com/rust-lang/rust/blob/e8753914580fb42554a79... If you trust nobody, it is hard to use anything. But about your second note, (environment, mismatched dependencies), I would argue that Rust provides the best tooling to solve or identify issues on that area. reply hu3 6 hours agorootparentRust depending on 220 packages is somewhat understandable. After all it runs on many platforms. I counted 16 Windows packages just by glancing over and also many macOS related. But 541 for docs? Surely there's a gradient between trusting no one and trusting 541 packages to generate static files. reply nicce 5 hours agorootparentAre you confusing `docs.rs` with `cargo doc`? It is indeed many packages, but if you look into the dependencies and code, docs is full blown standalone HTTP async server which uses tokio, AWS S3 and Postgresql. It is used to host the docs.rs where is the documentation of every cargo project. Maybe they should feature-gate some functionality and also split the dependencies Static site generator is mostly in the Rust itself: https://github.com/rust-lang/rust/tree/master/src/librustdoc reply nox101 12 hours agorootparentprevThat's the same argument node people make. See how well it's worked out. reply pxc 1 hour agorootparentNPM's got bigger problems than just having lots of small libraries, like for instance allowing circular dependency relations in packages reply vvern 20 hours agoprevMaybe one day the buck2 ecosystem will evolve to be that bazel replacement. It has a much smaller core. Right now it’s lacking in ecosystem support, tooling, examples, and a local build sandboxing (which could be fine if there was an easy to use local implementation of remote build that felt natural). Also no go support is sort of painful, and it’s a bunch of work as i understand it to get something like rules_go to work for buck2. reply ReleaseCandidat 11 hours agoparent> examples As it may be that people don't find them, the official ones are in `examples` https://github.com/facebook/buck2/tree/main/examples language examples in `examples/with_prelude` https://github.com/facebook/buck2/tree/main/examples/with_pr... There is a minmal Go example too: https://github.com/facebook/buck2/tree/main/examples/with_pr... More \"real world\" examples are dtolnay's CXX for Rust and C++ interop: https://github.com/dtolnay/cxx and my own ones (sorry to link these again, but I do not know of any others): C++ with Vcpkg: https://github.com/Release-Candidate/Cxx-Buck2-vcpkg-Example... C++ with Conan: https://github.com/Release-Candidate/Cxx-Buck2-Conan-Example... OCaml: https://github.com/Release-Candidate/OCaml-Buck-2-Examples reply ReleaseCandidat 9 hours agoparentprevAnd a remark: the \"real\" problem when using Buck 2 is the interface to the LSP, as most LSPs only work with the \"native\" project configuration. For C++ generating a `compile_commands.json` is quite easy (see my C++ examples in the other post), not the least because there is no single standard for a project's configuration. reply bbkane 17 hours agoparentprevIt looks like Go is starting to get better support in buck2 - see https://github.com/facebook/buck2/issues/455 reply zokier 12 hours agoparentprevBuck2 is really enticing. If I had too much time on my hands, I'd love to try to make a full bootable Linux distro as monorepo and built with buck2. I do see some sort of convergence between package managers and build tools, nix and buck2 do seem to approach similar problem from different angles. reply Narigo 19 hours agoprevDocker would be on the list for me - for reproducible environments. Probably JUnit as it was my first real testing framework - for being able to use test driven development for hard problems. With programming, I had so many \"aha\"-moments, it's hard to remember them. It's not all about software, but more about understanding the concepts and being able to transfer this knowledge. Being able to pass functions or function pointers. Streaming / piping data instead of a fixed data structure. Interpreted vs compiled languages. How everything we do here only happens through a long list of 0s and 1s and how this clever setup makes us even see graphics on the screen. Or hear audio through a screen reader.... reply mihaitodor 10 hours agoprevStreaming and transforming structured documents at scale used to require some awfully complex machinery such as Apache Camel, Kafka Connect, Flink, etc. I was so happy when I bumped into Benthos https://benthos.dev which can be used as a lightweight replacement in most cases. Bonus: It’s written in Golang, so I don’t have to bother with heavy dependencies and slow start times. reply joe8756438 19 hours agoprevHappy to see emacs on the list, changed my life. Enlightenment hardware: kinesis advantage 2, many non obvious, non ergonomic benefits, like adapting the hardware to _my way_ of working/thinking. Certain games also feel like enlightenmentware. reply dpflug 6 hours agoparentWhat games would you classify as enlightenmentware? reply joe8756438 3 hours agorootparentdwarf fortress ultima-likes minecraft I’m not much of a gamer, but those definitely made a pretty big impact on me as far as what one or a small group of people can accomplish reply fbn79 10 hours agoprevIn my case Cycle JS (https://cycle.js.org) was very enlightenment and pedagogic. It make me realize that software is always and only a matter of data transformation. And those pure data transformations can be keep separated and decoupled from \"side effects\". reply vl 15 hours agoprevI’ll mention Typescript. It elegantly improves almost unimprovable mess of JavaScript. It makes JavaScript development much more productive and pleasant. And it’s surprisingly powerful. reply xandrius 12 hours agoparentI agree but it's a shame we need to put a bandaid over JS instead of having a properly typed language option for the web. Now we start seeing some wasm being used but I still wouldn't use it for the whole project, so TS is the way for now. reply vl 3 hours agorootparentIf you think about it, entire web infrastructure is buzzard and inefficient. Sane design is to have some kind of bytecode for web code and some compact format instead of html. The waste of delivering this data and then executing js is enormous. And we need to pre-process and build websites and web apps before serving anyway! reply ativzzz 5 hours agorootparentprevLosing decades of backwards compatibility and portability is probably not worth it reply igammarays 3 hours agoprevLaravel. I had been developing web apps since I was 12 years old, but Laravel completely changed the speed at which I could deploy and maintain a production-ready app. And the ecosystem of plugins, add-ons and developer tooling is incredible. reply aulin 13 hours agoprev> It turned a mundane job of fixing bugs into an exercise in skill. Emacs does this for me. It's like a toy always there to play with when you're bored with mundane job tasks. reply alceta 12 hours agoprevlazygit (https://github.com/jesseduffield/lazygit) is enlightenmentware for me. It helps me navigate Git commands I forget all the time, like using the reflog to undo things, custom patches, or rebase --onto. It makes working with Git a lot more fun, and I giggle like a little child whenever one of the weirder things work out again. reply xandrius 12 hours agoparentI find VSCode/Codium to be even better at that! And worst case, open up the terminal and do the job there. Even merging which was often an annoying endeavour is quite smooth there. reply hcarvalhoalves 5 hours agoprevCommon Lisp. Not just the language, but the entire runtime: repl, debugger, system loading, quicklisp. It just works. reply maxander 11 hours agoprevI don’t know whether this is a provocative or tedious thing to say, but the quintessential ‘enlightenmentware’ to have come out of the past several years is ChatGPT. Name anything that brings as much functionality with so simple an interface and so elegant a core! (It’s simply a pity that we can’t install it locally or tinker with the internals. :) ) reply ReleaseCandidat 11 hours agoparent> Name anything that brings as much functionality with so simple an interface and so elegant a core! The most important thing of the last years has been LSP support! I can live perfectly fine without LLM autocompletion (although I did use Tabnine long before ChatGPT came out), but not without a LSP. reply jiehong 21 hours agoprevSpeaking of Bazel, I wanted to try it out for a Java project, but it felt a bit more complex than expected. Would you recommend using it even for mono languages projects? reply AlotOfReading 21 hours agoparentBazel is probably at its simplest in a monolingual codebase. Toolchains have a lot of complexity. It's like that Churchill quote about democracy: Bazel is the worst build system except for all those others. reply sanderjd 18 hours agorootparentUsed bazel for years, now using pants[0] and really enjoying it as a tool that is good in the same ways, but better in some smaller ways. 0: https://www.pantsbuild.org/ reply WatchDog 15 hours agoparentprevWhile maven and gradle may lack the architectural purity of bazel, they work much in the same way. Plan and execute steps, building a graph of the required build tasks, only executing the tasks that have changed inputs. With that said, it's quite common to see maven/gradle builds that are misconfigured and execute tasks unnecessarily. reply fire_lake 12 hours agoparentprevI would use choose it for a C++ only project, but that’s because the alternatives are so horrible. reply Tijdreiziger 21 hours agoparentprevWhy not Gradle or Maven? reply colordrops 21 hours agoparentprevIf you haven't already committed, consider Nix instead. reply threePointFive 21 hours agorootparentI'm still trying to understand why people recommend Nix in place of a build system. Nixpkgs stdlib by default expects an autotools project. It will happily integrate with other build systems, as long as you've spelled out your dependencies in both. I've yet to see it generate a Makefile or make any decisions about compilition that weren't spelled out in a \"traditional\" build system. Could you shed some light on what I've missed? reply ris 19 hours agorootparentSo.. it's sort of a battle over territory between build system and package manager. Bazel is there becoming ever more complex and unwieldy in an attempt to provide supposed reproducibility - taking control of the provision of ever more of a project's dependencies (in often very janky ways). But to Nix people it's clear that what people are actually doing here is slowly building a linux/software distribution around their project, but in a very ad-hoc and unmaintainable way. And bazel projects will continue to grow in that direction because until you have control of the whole dependency stack (down to the kernel), you're going to struggle to get robust reproducibility. I don't think many Nix people would suggest actually using Nix as the build system, but probably to use a comparatively simple cmake/meson/whatever build-system and use Nix to provide dependencies for it in a reproducible and manageable way. reply zokier 12 hours agorootparentYou call blaze side janky and ad-hoc but to me (as complete outsider) using monorepo+build tool seems more principled and working more with fundamentals, while nix feels more ad-hoc and trying to fix stuff post-facto. > And bazel projects will continue to grow in that direction because until you have control of the whole dependency stack (down to the kernel), you're going to struggle to get robust reproducibility. This is bit weird statement, considering that it's not where bazel is growing to, but where bazel is growing from. The whole starting point for bazel is having full control (via monorepo) of the dependency stack reply threePointFive 18 hours agorootparentprevThanks for the summary. I've been using Meson + Nix, so the comments about using Nix as a build system have been confusing. I think what I've been seeing though are \"use Nix instead of Bazel\", not \"use Nix as your build system\". reply colordrops 14 hours agorootparentWhat I mean is use a relatively simple build system instead of Bazel, and deal with dependencies and reproducibility through a Nix development environment. reply fire_lake 12 hours agorootparentYou lose out on some of the incremental compilation speed that Bazel offers doing this. I think many in the Bazel space suggest using Bazel inside of a Nix environment. reply __MatrixMan__ 19 hours agorootparentprevI'm not sure why you'd want to generate a Makefile if you're using nix. Unlike make, nix understands the inputs to a build step and won't bother rerunning it unless their inputs have changed. You would lose that you generated a Makefile instead of having nix build whatever it is that the Makefile builds. Otherwise it does the same things as make: this bunch of commands depends on this other bunch of commands... It just makes you express that as a function so it can be smarter about memoization. I've not used it for large complex builds, so maybe there's some itch it fails to scratch at finer granularity which I'm overlooking. I liked this artical about where it shines and where it fails to be a build system: https://www.tweag.io/blog/2018-03-15-bazel-nix/. I've been waiting for the problem to arise that encourages me to learn Bazel so I can use it alongside nix, and it just hasn't yet. reply tithe 15 hours agoprevShells, and how by scripting them you have programmatic access to the entire operating system. reply flatline 17 hours agoprevBoost::graph feels like one of the dustier corners of the Boost libraries. I have used it, it worked, but it took a long time to wrap my head around the design and actually adapt it to my project. It is not great for getting simple things done, but it will get them done, with the power and flexibility as stated in the article. You will likely never see the Boost interfaces poking through whatever facade you end up erecting around it. reply jwrallie 20 hours agoprevI hope the author would consider trying Doom Emacs, he mentioned knowing Vim and using Vscode, so with Evil and lsp support he will feel at home, the configuration has great defaults! reply roman-kashitsyn 10 hours agoparentI tried God Mode and Evil several times, but it never ended well. Mostly because of the muscle memory I accumulated over a decade. Evil is fantastic, but it doesn’t play well with all the packages in the ecosystem. There are adapters for Magit, Compile, etc., but most packages define key bindings that require Ctrl-Meta cords, which causes cognitive dissonance. God mode is a neat idea, but I never committed to it, and I found it confusing at times. Overall, I decided to accept Emacs as it is, with all its quirks, and embrace its way of doing things. I’m not married to it, so I can occasionally cheat by switching to Vim or VS Code without feeling guilty. reply mkesper 9 hours agorootparentI never was able to use evil per se but it works really well in doom emacs (and before that in spacemacs). Oh and use Caps Lock as Ctrl to minify pinky usage! reply srcreigh 16 hours agoparentprevEvil offends my simple sensibilities. I prefer boon for modal editing due to its simplicity and how it fits with emacs so well. reply jovas 18 hours agoprevLaTeX. I LaTeX, Git, and emacs every day. reply nephanth 14 hours agoparentHonestly TeX/LaTeX the engine is a marvel of technology, But everytime i see a \\makeatletter or get a runaway argument it reinforces my belief LaTeX the language was a mistake reply isametry 8 hours agorootparent(La)TeX is an example of a very enlightened _idea_ that offed itself \\footnotemark{} with a spectacularly cursed user interface. It is simply gross to write, and it's difficult for frontends, converters and GUIs to make it much better. Yes yes, I can already hear the cultists chant \"YoU dOn'T wRiTe In LaTeX\" but this mentality is precisely the problem. If I can't write directly in your typesetting system nowadays, then I'm sorry, your system probably sucks. You could unfortunately write an article or thesis quite comfortably in Word or even InDesign, while formatting as you go. (I say \"unfortunately\" because from a business-model and hacker's perspective, these tools suck.) \\footnotetext{not implying that LaTeX is dead, but referring to how it sentenced itself to the academic niche, in which case it might as well be dead…} reply skydhash 28 minutes agorootparentFrom what I’ve seen from Latex GUI applications, there’s no way we can avoid complexity. Most users will do OK with a basic word processor. We do not need a silver bullet for every use case. You select the best one and move on. reply sneilan1 17 hours agoprevI found quicken to be enlightening. It took me two months to master my family’s finances and budgeting and I’ve never looked back. Learning it’s ins and outs and why it is what it is and provides some features but not others was a wonderful learning experience. reply roman-kashitsyn 5 hours agoparentI worked with John Wiegley for a couple of years and discovered one of his projects, Ledger (https://ledger-cli.org/), during our conversations. This tool taught me double-entry accounting and helped me understand finance and blockchains on a deeper level. Unfortunately, I’m too lazy to use the tool on a daily basis, so it’s the second most insightful piece of software I’ve never used :D reply valtism 9 hours agoprevReact is this for me. Before it, I was fumbling around with libraries like ExtJS for my first job, but after I started using it the concept of components that produce a view as a functional output of state really made so much sense to me. It has given me so many powerful primitives to use while coding for the web reply JoeyJoJoJr 7 hours agoparentI would also say Redux. Even though I grew to dislike Redux, understanding the power of reducers was quite mind blowing. And further I would throw in Tanstack/Redux query. reply somishere 19 hours agoprevGreat article. For me it's creative use of tools by others - sometimes myself - in a non-standard way (sometimes that becomes standard!) that brings enlightenment. Be it language, source control, networking, you name it. Aside: assuming the author is reading, minor typo in the first para: But once in a w̶h̶i̶t̶e̶ while reply roman-kashitsyn 6 hours agoparent> But once in a w̶h̶i̶t̶e̶ while Thanks, fixed! It's “occasionally” now. Even Grammarly didn’t find it :( reply vegabook 19 hours agoprevNixOS fits the mold. The confidence it instills feels similar to when I moved from Windows to (Ubuntu) Linux. reply rjmunro 17 hours agoprev> Git was nothing like Subversion. It had a steep learning curve and confused everyone to no end I've actually found git much easier to teach to people who don't know subversion than to people who do. It's still a confusing mess, though. Why do you create a branch with `git checkout -b` rather than something like `git branch -c` (`-c` to checkout new branch)? It looks like the `git switch` command helps a lot, but I never remember to try to use it as I'm used to the old ways, so I never teach it to new people either. I wish I could alias `git branch` and `git checkout` to remind me to use `git switch` but you can't alias over a built in command. reply yjftsjthsd-h 14 hours agoparentI started on mercurial, then git, and I think that was the happy path / easy on-ramp. Actually I still mostly prefer hg, but it has some downsides and the overall ecosystem really prefers git. reply mkesper 9 hours agoparentprevFor teaching to newbies, please use the new commands! Much better to distinguish `git switch` from `git restore` than to use `git checkout` for all possible tasks. reply kevinwang 5 hours agoprevThe only good tools I've ever used are typescript and tqdm (a python progress bar). Wandb and React can get honorable mentions. reply nephanth 14 hours agoprevAgda. Not as in something i'd use everyday, but using it definitely shaped the way i reason about programming and type systems reply Barrin92 19 hours agoprevSmalltalk and as a particular case Pharo is an example of this for me. (https://pharo.org/). When I was in uni a paper that I always came back to was Licklider's 1960s paper on human-computer symbiosis. \"[...] to enable men and computers to cooperate in making decisions and controlling complex situations without inflexible dependence on predetermined programs.\" Experimenting with Smalltalk (and also with Clojure and Emacs) was one of the things to me that genuinely felt like that vision of programs as living, interactive, organic things rather than the formulaic, static and low level programming that I was used to learning. I think it's still such a shame that in daily jobs it's so difficult to convince people of trying these technologies out because it requires such a big shift in how people think about software. https://worrydream.com/refs/Licklider_1960_-_Man-Computer_Sy... reply WillAdams 17 hours agoprevYeah, one of the best programmers I've ever worked with would launch Epsilon (a commercial emacs style editor for various OSs) each morning and then do _all_ of his work from it. The closest I come to that is messing emacs keyboard shortcuts when I'm not using a Mac. I really wish that there were more programs which completely re-examined all aspects of various tasks _and_ incorporated scripting in a fashion which allows folks to take advantage of it. Some of the apps I would consider if putting together such a list: - LyX --- billed as a \"What You See is What You Mean\" Document Processor, v2.4 is looking to be quite promising... - TeXshop/TeXstudio --- the former in particular is _very_ nice for folks who aren't able to devote the effort to learning emacs - pyspread --- have a spreadsheet where every cell can contain a Python program or SVG graphic is _way_ cool --- I just wish it was as flexible as Lotus Improv/Quantrix Financial Modeler - Solvespace --- I wish I could do better with 3D --- usually I fall back to OpenSCAD, esp. now that there's a Python-enabled version: https://pythonscad.org/ though I often use: https://github.com/derkork/openscad-graph-editor - TikzEdt/IPE --- I really wish there was a nice graphical front-end for METAPOST/METAFONT (or that the graphical front-end for Asymptote was more flexible) On the gripping hand, one has to give props to the Krita folks for making scripting a first-class citizen: https://scripting.krita.org/lessons/introduction reply senkora 16 hours agoparent> LyX During college, I time-tracked how long I spent on each homework for each class. I can confidently say that using LyX instead of LaTeX for my math assignments resulted in me finishing them 50% faster. I think that most of the improvement was that the WYSIWYM reduced the cognitive load enough that I could write equation reductions inside the editor without having to write them out on paper first. I highly, highly recommend LyX to anyone who needs to typeset math equations. reply WillAdams 6 hours agorootparentThat also helps folks downstream --- when I did book composition, the cleanest LaTeX manuscript I ever worked on was done by an author who used LyX. reply lejalv 10 hours agorootparentprevHave you seen TeXmacs (https://www.texmacs.org/)? reply bluepoint 9 hours agoprev> Although I’m writing these words in Visual Studio Code, I always have my Emacs open He is writting the blog's text in Code praising Emacs. Makes me a bit sceptical or am I missing something? reply ativzzz 5 hours agoparentYou can learn a lot from Emacs about software development philosophy but then move to something with more batteries included that requires less upkeep like VSCode as you move forward in life, especially once you lose the ability to tinker (mostly lack of time due to things like children) reply zarathustreal 4 hours agoparentprevEmacs is an operating system, VS Code is a code editor. reply tkgally 17 hours agoprevI’m barely a programmer, but I have been using computers for nearly four decades. Among the various tools that have, over the years, captured my imagination, opened new possibilities, and affected how I create things with computers, the current leading enlightenmentware by far is LLMs. Nearly every day I discover something surprising and useful that they can do for me. reply zem 18 hours agoprevblaze is one of mine too, specifically for the realisation that your build setup really feels rock solid when your entire dependency list is spelt out as an explicit DAG in a build file. inferring or otherwise auto discovering dependencies on the fly is seductive, but it always ends up letting me down when things get complex. reply RivieraKid 18 hours agoprevJAGS - allows you to specify a probabilistic model and sample from the posterior distribution reply roman-kashitsyn 1 day agoprevOnce in a white, we discover a piece of software that transcends mere utility. These tools capture our imagination, open new possibilities, and affect how we design our own systems. I call such software enlightenmentware. In this article, I praise the software that contributed the most to my enlightenment. What’s your enlightenmentware? reply boffinAudio 11 hours agoprevI have a couple of these to add as well: VCVRack - simply one of the most mind-expanding things a synthesizer-nerd can play with. (https://vcvrack.com/) ZynthianOS - another example of a simple software solution to a problem nobody realized existed, opening the door to an absolutely astonishing array of Audio processing tools (https://zynthian.org/) reply jdeaton 19 hours agoprevJAX reply dizhn 20 hours agoprevYou have a typo. \"earn to\" when you probably meant \"yearn to\". reply roman-kashitsyn 10 hours agoparentFixed, thanks for reporting! reply bitwize 10 hours agoprevEmacs, Squeak, and Genera (Lisp machine OS) all qualify for me. It's no surprise that all of these examples are what I call \"pervasively programmable\": you can not only extend them with code, but examine and modify the running system by typing code into it, shaping the system to your needs as it runs. Another pervasively programmable piece of enlightenmentware is Ashton-Tate Framework. It wasn't just a spreadsheet but a piece of \"integrated software\" (the 80s term for office suite), sporting spreadsheet, word processing, database, graphing, and serial communications capabilities, all under a unified desktop-GUI-like interface and programmable with the Lisp-like FRED language. If there were ever an \"Emacs for business\", it'd be Framework. It's pretty amazing that a program that powerful existed on 1980s PC hardware. reply brcmthrowaway 17 hours agoprevClosed tab at Bazel. reply mellery451 2 hours agoparentagree - what's more, the author is really talking about blaze, where everything \"just works\" because there are massive dedicated resources maintaining it. He literally admits that he likes the copy-pasta-no-think-about-it: > Surprisingly, I didn’t need to fiddle with blaze, nor did I have to understand how it worked. I could copy some build targets and edit the dependency list, and the build worked as expected. Sure, systems that just work are great, until they break. bazel on the other hand, not so much. Heaven help you if it doesn't support your use-cases - you will wish you had Google to maintain your build. reply zvorygin 16 hours agoparentprevI quit my last job in no small part because of Bazel. I hated it so much. It tortured me. I think Bazel is the kind of really complicated language that invites clever engineers to build incomprehensible balls of spaghetti. And the tooling and docs are really underinvested in. But I got a new job, and to my surprise I've been doing Bazel all day. And I love it. I don't really know why. All this to say, don't make a final judgement yet, there's something brilliant buried underneath that pile of rules and aspects. reply roman-kashitsyn 10 hours agorootparentBazel confused the hell out of me at first, and I think the two-phase execution model (the “plan-execute pattern” as I called it) is to blame. My favorite thing about Bazel is how easy it is to get stuff done if somebody sets up the rules for you. Copy-paste a code snippet and fiddle with the dependency list until it works. But as soon as you go deeper, you get overwhelmed with new concepts, and the documentation doesn’t explain them well enough. I think this huge spike in complexity makes people hate Bazel, especially if their colleagues force it on them, breaking the usual workflows. I don’t love Bazel, but it’s the build system I hate the least. And it taught me a lot. reply ASinclair 14 hours agorootparentprevI work on a couple sets of rules at Google. I enjoy it. Though I agree people really can make balls of spaghetti with it. Everyone’s macros are terrible except for mine . The configuration system is where things can get really out of hand: transitions, selects, flags, etc. reply globular-toast 11 hours agoprevMy road to Emacs was similar to the author's. When I was learning programming everyone was using IDEs. It seemed like they were inseparable from the programming language. I remember thinking I couldn't learn C++ because I didn't have a C++ IDE. After my second or third language I started to think this was ridiculous. It's all just text! And these built in text editors are terrible! It took the best part of a decade for everyone else to realise and move to VSCode, a vastly inferior editor. reply jauntywundrkind 19 hours agoprevI want to thank wmii for being a vanguard force in illuminating the path before me, by being both a fine tiling window manager, while also showing it's guts, exposed as a 9p file-system. Being able to craft together super crappy shell scripts to monitor and manage my windows was amazing. It was a huge turn on to feeling like someone who was really communing with the computer at it's deeper level, rather than just surfing above the application's crust. This feels like the real enlightenment goal: bringing forward the (ab-)natural philosophy that underlies each bit of software, rather than crafting facades of interface atop the core. I have high hopes that general systems research can one day again spark an age of revelation & understanding, that we can form better more earnest symbiosis with machines. wmii was a good example of one way to let bonds grow close & strong. reply komali2 13 hours agoprevI really like the buku terminal bookmark manager. https://github.com/jarun/buku I like that I can just `man buku` when I don't understand something and I can actually find the answer I'm looking for. reply roman-kashitsyn 5 hours agoparentNice! I used https://wiki.systemcrafters.net/emacs/org-roam/ for a while but switched to LogSeq (https://logseq.com/) because org-roam was buggy. I like working with LogSeq, but even after a couple of years of using it, I’m not convinced by the Zettelkasten method. Maybe I’m doing it wrong! reply revskill 16 hours agoprev [–] It is vscode and es6 to me. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The blog post delves into \"enlightenmentware,\" software that significantly shapes a programmer's system design approach.",
      "The author shares their journey with UNIX, version control systems like Git, and their transition from Vim to Emacs, highlighting Emacs' extensibility and Lisp-based architecture.",
      "They emphasize the educational value of the Boost Graph Library, their preference for Google's Blaze and Bazel build systems, and advocate for simple, powerful tools that solve fundamental problems."
    ],
    "commentSummary": [
      "The discussion highlights various software tools and operating systems, focusing on their roles in code verification, optimization, and interactive learning, with tools like Compiler Explorer, Jupyter Notebooks, and Python Tutor noted for their educational benefits.",
      "Users compare Windows, Linux, and macOS for technical tasks, discussing usability and customization, while Docker and Podman are praised for ease of setup and minimal system impact, and NixOS is recognized for its customization and configuration management despite a steep learning curve.",
      "The conversation covers a range of software development tools, including Buck2, Docker, JUnit, and TypeScript, and emphasizes the importance of innovative tool usage and finding the right tools for individual needs and preferences."
    ],
    "points": 407,
    "commentCount": 211,
    "retryCount": 0,
    "time": 1716236582
  },
  {
    "id": 40426701,
    "title": "EU's \"Chat Control\" Surveillance Proposal Rejected Over Privacy Concerns",
    "originLink": "https://mullvad.net/en/why-privacy-matters/going-dark",
    "originBody": "Going Dark: The war on encryption is on the rise. Through a shady collaboration between the US and the EU. State mass surveillance Under the slogan ‘Think of the children’, the European Commission tried to introduce total surveillance of all EU citizens. When the scandal was revealed, it turned out that American tech companies and security services had been involved in the bill, generally known as ‘Chat Control’ – and that the whole thing had been directed by completely different interests. Now comes the next attempt. New battering rams have been brought out with the ‘Going Dark’ initiative. But the ambition is the same: to install state spyware on every European cell phone and computers. On May 11, 2022, EU Commissioner Ylva Johansson presented a legislative proposal under the official name ”Regulation of the European Parliament and of the Council laying down rules to prevent and combat child sexual abuse.” Ylva Johansson made a point of this being her bill: it was she who had devised it – no one else – and if it had not been for her, Europe’s justice system would “go blind” in the hunt to track sexual abuse of children. In Ylva’s world, the EU would “turn into a pedophiles’ paradise” if she didn’t get her way. It was easy to marvel at how, on almost every occasion, Ylva Johansson was keen to point out that this was her proposal. A touch of narcissism? Maybe. But perhaps there was something else behind this self-centered proclamation. Because it would eventually emerge that in fact Ylva Johansson was not alone behind the scenes. Right from the start, there were others involved – actors who would benefit from the bill being passed, but who preferred it not to be known that they were involved in designing it. The rhetoric was clear from day one: it was all about the children, and when it comes to children, there’s nothing we can’t imagine doing to keep them safe. So Ylva Johansson put forward a proposal that meant total surveillance of all EU citizens and as soon as someone opposed it, she pulled out the think-of-the-children card. But those who could see through the bluff quickly gave the proposal (those parts of the bill that dealt with internet surveillance) a shorter and more appropriate name: Chat Control. In brief, Chat Control essentially meant that the communications of every EU citizen would be monitored. Every call, every message and every chat, all the emails, photos, and videos saved in cloud services – all of it would be filtered in real time via artificial intelligence and then checked in a newly established EU center, in close cooperation with Europol. Since the bill was in violation of the European Convention on Human Rights, the EU Charter and the UN Declaration of Human Rights, Chat Control was rejected by one legislative body after another. Both the Council of Ministers and the European Commission’s own legal service warned against the proposal, as did the European Parliament’s Data Protection Board. The UN Human Rights Council described Chat Control as incompatible with fundamental human rights and stated that the proposal would lead to mass surveillance and self-censorship. Former judges at the European Court of Justice said that the proposal was in breach of the EU Charter of Rights and 465 researchers joined forces to warn of the consequences. Faced with massive criticism, Ylva Johansson defended herself. According to her, everyone else had misunderstood the bill. Chat Control was certainly not about mass surveillance and everyone making that claim was simply out to discredit her. Chat Control – total monitoring of all EU citizens. Chat Control is sometimes also called Chat Control 2.0, since existing legislation already makes it possible for tech companies such as Google and Meta to scan their users’ accounts for child pornography material. The fact that there was already a law that allowed tech companies to scan for illegal content – if they chose to – was something Ylva Johansson was not slow to mention. She explained that her draft bill was nothing but an extension of the scanning that had already been going on for ten years. She also referred to the existing legislation when she said that the EU would become a free zone for pedophiles unless her bill went through – as that legislation would expire in the summer of 2024. Time and time again Ylva Johansson was proven wrong by journalists and experts. In fact, nothing prevented the EU from extending the existing law, rather than introducing a new one. And above all: Ylva’s bill was anything but an extension. The differences between the current law and the proposed legislation were extreme. In Ylva Johansson’s EU, scanning would not be voluntary. All messaging services (including encrypted services such as Signal) would be covered by the law and would be forced to scan their users’ images, videos and conversations. That would be a big concern for all those who don’t use Meta or Google to converse because they are in need of secure communication methods. In other words, political opponents, whistleblowers, journalists and their sources, vulnerable people living under secret identities and others, not to mention people with trade secrets, and those in possession of sensitive information important for national security. For example, the European Commission itself uses Signal. Demanding government transparency (either through so-called backdoors or scanning on the computer or phone) would open a Pandora’s box to countries with authoritarian inclinations (and five EU countries have already been caught using spyware to monitor political opponents) and would leave the door wide open for criminals to exploit. But it was not only this that separated the existing legislation from the draft bill that the European Commission wanted to introduce. The previous legislation had only allowed scanning for material that had previously been stamped and registered as child pornography material. Now, AI would be used to find ‘new material’ and would also look for grooming attempts. Quite obviously, Chat Control would therefore send every other citizen of the EU straight into the filtering system. Holiday photos from the beach, nude photos sent between partners, dirty text messages – all the things that no AI system can distinguish between would risk getting caught in a filter that would inevitably drown any new EU center with endless digital heaps of evidence to review. Is this a holiday photo of a child or child pornography? Are these skimpily dressed youngsters 18 or 14? Is this a dirty text message from a wife to a husband or a grooming attempt? But above all, Chat Control would mean a tool that could be used to scan for completely different things. When Ylva Johansson was asked whether it would be possible to communicate safely even after her bill was introduced, she answered “Yes.” And a whole world of experts asked “How?” Ylva replied that she had something nobody else had. A digital sniffer dog that could smell encrypted communication without looking at the content. A sniffer dog that only reacted to child pornography content – never anything else. A group of experts tried to hammer the message home: either encrypted communication is encrypted (so-called end-to-end encryption, which only the sender and the recipient can see) or it’s not encrypted. There’s no ‘seeing the content’ without reading it. But Ylva stood by her claim. She came back to the same argument over and over again. She avoided answering the questions (she obviously didn’t understand how the technology worked) but instead turned the direction of the discussion, saying, for example, that a court order would be required to carry out scanning, which in itself was deliberately misleading. Firstly, her scanning would not require an order from a court – it could be one from another judicial body. And secondly, the key issue was that judicial body making a decision that would force messaging services to monitor all their users. So in other words, when Ylva proclaimed “it requires a court order,” she wasn’t talking about courts and their decisions to monitor people such as suspected pedophiles. She was talking about how a service would be forced to permit surveillance. What was required for a service to be subject to surveillance? Merely that there was a possibility to use the service to spread child pornography or to groom children. Which of course means every messaging service on the planet. As soon as Ylva Johansson was shown to be in the wrong, she shifted her focus. But in the end, she always came back to the final refuge: it’s all about the children. She related anecdotes and referred to figures that pointed to an exponential increase in child pornography material on Facebook, for example – even though Facebook itself stated that 90 percent of all reports come from material previously distributed. The European Commission, led by Ylva Johansson, received criticism from all directions. Police chiefs pointed out that most of the material they receive today involves teenagers sending pictures to each other and that such reports risk leading the police in the wrong direction. Scanning tests carried out by European police on existing material showed that 80-90 percent of all hits were false positives. Now, moreover, ‘new material’ would be scanned – which would obviously mean an impossible administrative burden merely to distinguish between illegal images and holiday pictures from family days on the beach. The error rate would clearly be approaching 100 percent. For a European justice system that even today is unable to follow up all the tips it receives, this would be devastating. And criminals would, of course, turn to illegal messaging services. No children would be helped. At the same time, every EU citizen would have spyware installed on their phones. How did Ylva Johansson deal with this information? Not at all. Instead, like a scratched record, she continued urging everyone to “think of the children.” She also ordered a survey that said 80 percent of the EU population supports Chat Control. The problem? The European Commission used its Eurobarometer series of public opinion surveys in way that opened it to accusations of blurring the line between research and propaganda. When asked to comment on the Chat Control survey, the Max Planck Institute for the Study of Societies concluded that it had a political agenda and consisted of questions that were biased to support the Commission’s plans. Ylva Johansson was employing blatant deception. She used incorrect figures and biased surveys. In interviews, she was populist and evasive. But she was forced to resort to these methods. Because it was never about the children. American tech companies and security services behind the draft bill In September 2023, a major investigative article was published by three journalists: Giacomo Zandonini, Apostolis Fotiadis, and Luděk Stavinoha. After seven months of trying to get the European Commission to release public documents, they finally obtained a piece of material that allowed them to start putting together the puzzle. The puzzle that revealed the true stakes behind Chat Control. The article, which was published in several European newspapers, included a letter in which Ylva Johansson wrote to Julie Cordua, CEO of the American company Thorn: “We have shared many moments on the journey to this proposal. Now I am looking to you to help make sure that this launch is a successful one.” Thorn is an American company, formed by actor Ashton Kutcher, which develops tools that scan for child pornography material. Thorn had sold software worth millions of dollars to the U.S. Department of Homeland Security. Ashton Kutcher himself had held video conferences with European Commission President Ursula von der Leyen, and had given lectures in the EU on how new technologies can scan encrypted content without looking at it. The picture of Ylva Johansson’s digital sniffer dog suddenly became clear. For several years Kutcher lobbied the European Commission (until he was forced to resign as chairman of Thorn’s board after defending his acting colleague Danny Masterson when he was convicted of rape). He held meetings with others at the European Commission and had an extra close relationship with the Commission’s Eva Kaili (until she was convicted of bribery). So here was an American company in direct contact with the European Commission. An American company that just happened to sell the technology that could be used if Chat Control was introduced. In addition, it was all based on a false premise. The technology Kutcher and Johansson talked about did not exist. Expert after expert condemned their talk of sniffer dogs. And here’s yet another seedy aspect to this scandal: in the EU transparency register, Thorn was registered as a charitable organization – despite selling the technology they were lecturing about in the EU. The trick of disguising organizations and corporations as charities would turn out to be a recurring motif. Since the draft Chat Control bill was presented, Ylva Johansson has constantly referred to children’s rights organizations that support her proposal. She has worked with them in a PR context, as a way to show how Chat Control has the support of independent, nonprofit organizations that care about children. A central organization in this work has been the WeProtect Global Alliance. When Zandonini, Fotiadis, and Stavinoha published their article, it turned out that the European Commission had been involved in founding this organization, and that it included representatives from both tech companies and security services in different countries. Ylva Johansson’s colleague in the European Commission, Labrador Jimenez, was on the Board of Directors of WeProtect, together with Thorn’s CEO Julie Cordua, representatives of Interpol, and government officials from the US and the UK (the latter simultaneously pursuing its own monitoring legislation, also using children as the battering ram). Thorn had put a great deal of money into WeProtect. The European Commission had contributed one million euros. In other words, it wasn’t children’s rights organizations that were supporting Ylva Johansson. It was lobbying organizations set up by the European Commission to get the bill through. The Board of Directors of WeProtect also included representatives from the Oak Foundation, who, in addition to their involvement in WeProtect, had also been involved in setting up ECLAG (another charity that supported the Chat Control proposal). ECLAG was launched just a few weeks after Ylva Johansson’s draft bill was presented, and Thorn was also represented on this organization’s board. And there was still another organization: the Brave Movement, an organization formed a month before the proposed Chat Control bill was introduced. Brave was launched with $10 million from the Oak Foundation and a strategy paper discovered by the journalists stated that “once the EU Survivors taskforce is established and we are clear on the mobilized survivors, we will establish a list pairing responsible survivors with Members of the European Parliament – we will ‘divide and conquer’ the MEPs by deploying in priority survivors from MEPs’ countries of origin.” The Oak Foundation also appeared in a article carried out by the Intercept. In 2023, an American organization called the Heat Initiative was formed. On paper, they were a “new child safety group” and the first thing they did was campaign for Apple to “detect, report, and remove” child pornography material from iCloud. Apple responded that this would be something that criminals would be able to exploit and that it could also lead to a “potential for a slippery slope of unintended consequences. Scanning for one type of content, for instance, opens the door for bulk surveillance.” The Heat Initiative did not like this answer and fought back with anti-Apple propaganda on large advertising billboards in American cities under the theme of ‘think of the children.’ But who was behind the Heat Initiative, besides the Oak Foundation? Heat was led by a former vice president at Thorn. The Intercept article also referred to the fact that Thorn was working with Palantir, the big-data company that helped the NSA mass-monitor the whole world and was involved in the Cambridge Analytica scandal where Facebook users’ private messages and data were used to influence the presidential election on behalf of Donald Trump in 2016. In other words, the European Commission was involved in funding and starting up charities with the aim of exploiting existing victims to emotionally influence EU parliamentarians. In close cooperation with the tech company providing the technology that would be used in the implementation of the monitoring. Together with representatives of non-European security services. As part of a larger apparatus, where the same tactics were used to influence developments in the United States. At the same time, the real organizations working to counter sexual crimes against children were wondering why the European Commission was refusing to talk to them. In the same investigative report, Offlimits, Europe’s oldest hotline for vulnerable children, tells how Ylva Johansson would rather go to Silicon Valley to meet companies interested in making huge profits than talk to them. The same is true of the technical experts. Matthew Green, Professor of Cryptography at John Hopkins University, said: “In the first impact assessment of the EU Commission there was almost no outside scientific input and that’s really amazing since Europe has a terrific scientific infrastructure, with the top researchers in cryptography and computer security all over the world.” However, Europol was involved in drafting the law, together with security services from other countries. In July 2022, Europol wrote that it wanted to be able to use scanning and surveillance for purposes other than sexual offenses against children. The European Commission responded that it understood the wish but that it had “to be realistic in terms of what could be expected, given the many sensitivities around the proposal.” Thorn was also clear in understanding that the scanning could later be used for other purposes: “When considering regulation or legislation on encryption it should not be done solely focusing on CSAM. Solutions for detection in encrypted environments are much broader than one single crime,” the company wrote in one document. It was later revealed that Europol was looking for unfiltered access to the scanned material: “All data is useful and should be passed on to law enforcement. There should be no filtering by the [EU] Centre because even an innocent image might contain information that could at some point be useful to law enforcement.” European Parliament: “the commission wanted mass surveillance.” So here was the European Commission, working on legislative proposals together with a Europol that wanted access to all surveillance, regardless of whether it contained something illegal or not – simply because it could be useful to have. In other words, it really wasn’t about the children. When articles were published about the EU Commission’s horrifyingly undemocratic approach, Ylva Johansson’s office at the European Commission responded by advertising on the platform X (formerly Twitter). They targeted advertisements (pro Chat Control) so that decision-makers in different countries would see them, but also so that they would not be seen by people suspected to be strongly against the proposal. The advertising was also targeted on the basis of religious and political affiliation and thus violated the EU’s own laws regarding micro-targeting. Officials at the highest EU level thus used data collected by big tech to try to create illegal filter bubbles designed to push through a mass surveillance proposal. The whole thing ended with Ylva Johansson being summoned to a hearing in the European Parliament. An almost united European Parliament was massively critical of Ylva Johansson and her approach. She was grilled about Thorn’s interference and about the targeted ads and the EU Ombudsman denounced the European Commission’s unwillingness to share public documents regarding the relationship with Thorn (the European Commission had assumed these would be classified because they risked undermining commercial interests …) Ylva Johansson’s answer? “Think of the children.” In November 2023, the European Parliament’s final judgment was delivered. In an almost historic consensus, all the groups in the Parliament stood together and said “No” to the bill. At the press conference, representatives from the Parliament said: “This is a slap in the face of the Commission, what we’ve tabled. The Commission wasn’t focusing on protecting children but wanted mass surveillance.” Patrick Breyer, who has been the most active opponent in the EU Parliament, called it a victory for the children, adding “They deserve an effective response and a rights-respecting response that will hold up in court.” Breyer was referring to the fact that Chat Control would most likely not hold up in court if the bill had been passed. Just a few months later, a ruling from the European Court of Justice ruled that authorities do not have the right to demand access to end-to-end encrypted communications. But the Chat Control proposal wasn’t completely buried just because the European Parliament had taken a clear stance against it. In the EU, two bodies are involved in the adoption of legislative proposals made by the European Commission: the European Parliament and the Council of Ministers. But while the European Parliament was extremely clear and unified in its stance, the Council of Ministers was hopelessly unable to reach an agreement. When new EU elections approached in summer 2024, they had not yet managed to come to a consensus. However, the Council of Ministers also began to hesitate about the technology. Ultimately, it had become evident to most people that Ylva’s digital sniffer dog didn’t exist. There was no technology that could scan communication without looking at it. Parts of the Council of Ministers therefore proposed that scanning should be excluded for politicians, the police and intelligence services, as well as anything classified as ‘professional secrets.’ Obviously, there were politicians who were afraid that their secrets would leak, but who had nothing against mass surveillance of the broader population. Patrick Breyer was clear in his response: “these people are aware that Chat Control involves unreliable and dangerous snooping algorithms – and yet they are ready to unleash them on us citizens.” Even Ylva Johansson finally realized that she was defeated. Did she then go public and announce that Europe would now be blind in the hunt for pedophiles? Of course not. She quickly and easily did what she had previously been completely unable to do: she extended the previous legislation. New attempt at mass surveillance via the Going Dark initiative The fact that the European Parliament rejected Chat Control didn’t mean that attempts to introduce mass surveillance were over. During Sweden’s EU Presidency in spring 2023, a project called Going Dark was initiated. The idea from the Swedish Presidency was initially that a so-called High Level Expert Group would be launched. The task of putting together the group went to the European Commission, which immediately removed the ‘Expert’ label. Instead of a High Level Expert Group, a High Level Group was formed. As the Netzpolitik newspaper put it: “Removing the word ‘expert’ is no small detail: special rules apply to Expert groups, for example when it comes to transparency. Rules that do not apply to High Level Groups.” Once again, the European Commission chose to start the preparatory work linked to mass surveillance without allowing experts to play a serious part in the process. When the group met for the first time, it stated that the group’s purpose was to discuss methods to achieve “access to data for effective law enforcement, based on and guided by the inputs from the EU Member States.” Some challenges were identified as particularly pressing: access to encrypted material (both stored data and communication), data storage, location data, and anonymization (including VPNs and Darknets). Once the group was united, it was divided into three working groups: the first would work with access to data on users’ devices (computer and mobile), the second group would focus on access to data in the services’ systems (messaging apps, for example), and the third group would discuss access to data in transit. According to the minutes of the meeting of the Swedish Parliament’s Committee on European Union Affairs, the group worked “to present effective recommendations for the accession of the new Commission in 2024 and for those recommendations to be implemented.” Future legislative proposals from the European Commission could thus be assumed to be about providing access to data on users’ devices and in the messaging services’ systems, and to data in transit. Patrick Breyer, who had worked hard to counter Chat Control, said the group was just an extension of past offensives and that Going Dark was working to introduce illegal mass surveillance. When he requested documents from the group’s meetings and a list of the attendees, he received a document with the information blacked out as if classified. The European Commission had thus put together a working group aiming to achieve mass surveillance of the broader population while not being transparent about who was part of the group. It was like a scratched record. Gone was the old excuse “think of the children”, but the goal was the same. However, some transparency was obtained through the Swedish Ministry of Justice, which at Mullvad VPN’s request provided both meeting notes and information about the Swedish representatives present at the meetings. The first Going Dark meeting was led by two people. One was Olivier Onidi, who is Deputy Director General directly under Ylva Johansson in the European Commission. Onidi has expressed that the “valuable” thing about Chat Control is “to cover all forms of communication, including private communication”, and he defended Ylva Johansson and Chat Control when he said: “I think it’s totally unfair to point this out as a mandatory inspection of all private communications. That’s not what you have in front of you. This proposal is a huge improvement over the current situation.” Onidi has also been questioned for his meetings with the American company Palantir (notorious for its involvement in US authorities’ illegal mass surveillance). The second person who led the first Going Dark meeting was Anna-Carin Svensson, international chief negotiator at the Swedish Justice Department, who, according to WikiLeaks documents in 2010, allegedly urged the US State Department and the FBI to continue with the current informal exchange of information between the countries instead of signing formal agreements. According to the American representatives at the meeting, it was about withholding information from the Swedish Parliament: “She believed that, given the Swedish Constitution’s requirement to present matters of importance to the nation to the Swedish Parliament, and in light of the ongoing controversy over the newly decided FRA law [FRA, Försvarets radioanstalt, the Swedish National Defence Radio Establishment, is a Swedish government signals agency], it will be politically impossible for the Minister of Justice not to let the Parliament review any data exchange agreements with the United States. In her opinion, the publication of this could also jeopardize the informal exchange of information,” the leaked documents said. According to the documents, Anna-Carin Svensson asked the FBI if they could not continue to make use of the strong but informal arrangements. When the documents leaked, Svensson denied everything and stated: “I cannot be held responsible for how Americans express themselves.” From the Swedish side, the Ministry of Justice was represented at the Going Dark meetings, but so was the Swedish Security Service (Säpo) and the Swedish Police Authority. Together with representatives from the other Member States, they used the High Level Group meetings to discuss how, through legislation, encrypted services could be required to provide data in readable format. Several Member States argued that “the working groups needed to look at solutions that involved ‘legal access through design’.” This was something that pleased American representatives. At the Going Dark meeting on November 21, 2023, a former FBI employee was also present, who said that “solutions for legal access (to data on device) should be prioritized” and that “companies needed to have a responsibility and follow the same rules.” As a former FBI employee, he also expressed “his gratitude for the fact that the issue was being pursued within the EU.” European police chiefs: we cannot accept criminals using secure communications. The Going Dark meetings resulted in an outcry from the assembled police chiefs of Europe. In April 2024 Europol published the challenge “European Police Chiefs call for industry and governments to take action against end-to-end encryption roll-out.” The declaration was a “direct extension of the Going Dark initiative” and, together, the European police authorities were clear that although encryption is “a means of strengthening the cyber security and privacy of citizens … we do not accept that there need be a binary choice between cyber security or privacy on the one hand and public safety on the other. Absolutism on either side is not helpful.” It was as if Ylva Johansson’s sniffer dog had caught the scent again. In the absence of expertise, the Going Dark initiative tried to magic away the fact that end-to-end encryption is absolute – either you have secure communication or you don’t. The assembled police chiefs claimed there were two key factors for achieving online security – which turned out to be direct repetitions of the reasoning in the Going Dark discussions. Number 1: so-called legal access to the tech companies’ stored data. Number 2: real-time scanning of illegal activity in tech companies’ services. Naturally, they said, all this would be done under strong protection and supervision. Stefan Hector, a representative of the Swedish Police Authority, said that “a society cannot accept that criminals today have a space to communicate safely in order to commit serious crimes.” A week later, it was revealed that the Swedish police had been infiltrated and were leaking information to criminals. Although the UN classifies encryption as a human right, the Going Dark initiative and the European police force are fighting to smash end-to-end encryption. Their first move came as a reaction to Meta rolling out exactly such encryption. The echoes from the Chat Control debate are clear. But it is also an echo of an older battle. The Going Dark initiative is really just an extension of the so-called crypto war (the war against encryption) that US authorities have been involved in since the internet began. As Signal’s CEO Meredith Whittaker said in a keynote speech: “Encryption was essential for the commercial internet. But law enforcement and security services saw any network resistant to government surveillance as a threat and a problem.” The US authorities have already tested the backdoors that the European Going Dark initiative is now seeking. Edward Snowden revealed that the NSA spent $250 million a year getting tech companies to install backdoors in their services, which also exposed the risks of backdoors. In 2010, Chinese hackers managed to use a Google backdoor to get into Gmail. The same thing happened in 2005, when state surveillance of Vodafone was exploited by outside actors to bug the Greek Prime Minister, his Foreign Minister, Justice Minister, and a hundred other government officials. And when it comes to client-side scanning, it’s also doomed to fail. Apple, one of the world’s most technologically advanced and wealthy companies, has poured incredible resources into figuring out if it can be done in a secure and private way. But when Apple made its effort public, it took hackers just two weeks to break in. Apple abandoned the attempt and continue to say no to anyone who asks them to try this again – simply because it’s too easy to hack systems where client-side scanning is involved. Snowden’s revelations changed the internet in many respects. Encrypted websites (https) became standard. End-to-end encrypted messaging services like Signal saw a widespread increase in popularity. Apple started using strong encryption in its operating systems. From having virtually free access to people’s internet traffic (if they didn’t use a trustworthy VPN, that is) and from having been able to read people’s messages in plain text, the internet now became more difficult for US authorities to mass monitor. In her lecture, Meredith Whittaker points to an important point: “Strong encryption was an important win. But the result of this win was not privacy. Indeed, the legacy of the crypto wars was to trade privacy for encryption – and to usher in an age of mass corporate surveillance. Because the power to enable – or violate – privacy was left in the hands of companies, not those who relied on their services. Companies that were incentivized to implement surveillance in service of advertising and commerce.” For more than twenty years, so-called commercial mass surveillance has created some of the richest companies in the world. The fact that Meta is rolling out end-to-end encryption doesn’t mean they have abandoned their business model. But it was still sufficient for the European police chiefs, cheered on by the US authorities, to make a joint declaration demanding legal access to the content in secure and private communications. Meredith Whittaker again: “In my view, the ferocity of the current attack on end-to-end encryption, and other privacy-preserving technologies, is very much related to a desire by some in government to return to the less fettered access to surveillance that they see as having lost post-Snowden.” We can see the attack coming in Europe right now. But the movement is based in the United States. Back in 2014, just a year after Snowden’s revelations, FBI Director James Comey spoke of how “the challenges of real-time interception threaten to leave us in the dark, encryption threatens to lead all of us to a very dark place.” The US authorities, which in 2014 had recently been caught spying on the entire world, used a particular expression when they began lobbying to regain access to easily controlling everything and everyone. FBI Director Comey talked about “Going Dark.”",
    "commentLink": "https://news.ycombinator.com/item?id=40426701",
    "commentBody": "Going Dark: The war on encryption is on the rise (mullvad.net)354 points by janandonly 8 hours agohidepastfavorite151 comments miohtama 7 hours agoHere is the latest. The bill could not be passed on Spain’s presidency. The presidency is now on Belgium and Stasi-fans are trying to get this bill passed again, hoping not to cause too much noise this time. The text of the bill was modified a bit, and this time they added an exception, though - Politicians and police are not subject to monitoring, only ordinal citizens messages’ should be wiretapped https://european-pirateparty.eu/chatcontrol-eu-ministers-wan... reply luxcem 6 hours agoparent> Politicians and police are not subject to monitoring, only ordinal citizens messages’ should be wiretapped The inversion of values is frightening, politicians and police should be among the very few under scrutiny. reply oooyay 2 hours agorootparentThis is what struck me as well. This is in the vein of, \"Who watches the watchmen?\" I can understand a world, albeit it sounds chaotic, where nothing is monitored. I'm worried about a world where the only unmonitored people are people with definitive authority. reply dlachausse 6 hours agoparentprev> Politicians and police are not subject to monitoring, only ordinal citizens messages’ should be wiretapped Of course, in 1984, their instruction manual, the top members of the party can turn their telescreens off. reply surfingdino 6 hours agorootparentTime to stash some paper notebooks and pencils while they are still not banned. reply unclebucknasty 6 hours agorootparentprev>Of course, in 1984 Kind of funny to hear people refer to 1984 these days. We're so far past it now, and we did it to ourselves; giving up location data for maps (and mobile phone function for that matter), a Ring doorbell on every front door, participation in social media, etc. But somehow it's all OK, because it's corporations instead of the government (a blurry line itself) on the other end. reply BSDobelix 6 hours agorootparent>We're so far past it now, and we did it to ourselves No we are not, you are not forced to have a smartphone, your are still allowed to have sex and most democracy's don't torture you for speak against it. Don't just fixate on the surveillance aspect in 1984, there's much much more in the book. reply elric 5 hours agorootparentYou're effectively forced to have a smartphone. It pisses me off, because aside from the privacy aspect, it marganalises a lot of people. As with many things, how true this is depends on where you are. But many restaurants refuse to hand out menus, many places require mobile payments, gyms require apps for access, etc. reply BSDobelix 4 hours agorootparent>because aside from the privacy aspect, it marganalises a lot of people. Why not go to the European Court of Justice? And I did not say that there are no parallels to 1984, but I would argue that there are more parallels to Lord of the (key)ring than to 1984 ATM. By the way, the driving force behind this law is also Swedish (Ylva Johansson), so maybe Sweden has some work to do ;) reply 1over137 5 hours agorootparentprevWhoa, really? What country is that? reply dt3ft 4 hours agorootparentSweden. Good luck living there without a smartphone. reply nicce 4 hours agorootparentIt is not only about the smartphone but social media too, to be precise. You are less attractive, \"odd\" or \"strange\" if you are not there. There is huge social pressure. reply hiatus 3 hours agorootparent> You are less attractive, \"odd\" or \"strange\" if you are not there. There is huge social pressure Other people's thoughts of you are not under your control. That people feel pressured to be on these platforms says more about the company they keep than anything else. reply 93po 2 hours agorootparentprevIf someone evaluates whether to socialize with you based off of this, and it's important enough to you to not have or carry a smart phone, then they're probably not super compatible as friends or a partner anyway reply miohtama 2 hours agorootparentprev> you are not forced to have a smartphone In most European countries, you are forced to have smart phone to access banking services, many other online services. Even some government agencies use WhatsApp for communications. reply crtasm 2 hours agorootparentDo you mean there's banks that no longer offer a website interface to your accounts? reply unclebucknasty 5 hours agorootparentprev>you are not forced to have a smartphone Yes. That was my point. We are not forced to (though life would be hard without one). As I said, we did it to ourselves. >Don't just fixate on the surveillance aspect I'm referring to the context of this thread, which is largely around surveillance. That, BTW, is also the context in which references to 1984 most often arise. reply orochimaaru 5 hours agorootparentprevYou have to have a smartphone these days. All 2FA use a smartphone and Authenticator apps. Companies actively deprioritize human agents in favor of automated ones. If this comes to pass, you will live with the risk of your information being leaked out by government incompetence - which they will try their best to cover up and blame you. Edit: No power given to government rarely not become something grotesque. US social security cards were “only for benefits”, now they are some ubiquitous identity number. In recent times, Covid vaccination cards were supposed to “only a patient record” until everyone started demanding them. So if your information starts leaking out your “conformance” will follow one way or another. reply Tijdreiziger 3 hours agorootparentYou can use any TOTP authenticator implementation you want for 2FA. Recently, implementations for PalmOS and J2ME phones featured on HN [1], among others. Password managers such as Bitwarden or 1Password also feature implementations. [1] https://news.ycombinator.com/item?id=40279305 reply Teever 2 hours agorootparentYou've totally missed the point of the post you're replying to. How do I opt out of all of this bullshit? I just want to go back to paper forms and letters mailboxes and stuff. reply Tijdreiziger 2 hours agorootparentUh, by just doing it? You can still file your taxes and apply for benefits on paper, if you’re so inclined. Banks and government departments still have phone lines. You can still send letters to your friends or call them on the phone. reply throw_nbvc1234 6 hours agorootparentprevBecause it's a narrative baked into western culture. People collectively respond to stories/narratives more then pure facts. reply dlachausse 6 hours agorootparentThe fact is that we are shockingly close to the world of 1984. Two minutes hate, newspeak, and our smartphones are telescreens on steroids. Orwell was frighteningly prophetic. reply jenadine 6 hours agorootparentOrwell wasn't prophetic but was a reflection of the world as it already was is 1948. That's what I learnt at school. reply lukan 2 hours agorootparentTo add some details, he worked for the BBC during wartime - that was the inspiration for the job in the truth ministry of the main character in 1984. Basically inventing the truth. reply seanw444 3 hours agorootparentprevThen the message is even more dire: the world has always been a dystopia. And even 70+ years later, with more (and better) education, higher standards of living, and a wealth of dystopias to read and learn from, nothing has changed. reply nalekberov 5 hours agorootparentprev\"than\" not \"then\" reply ruszki 6 hours agorootparentprev> we did it to ourselves Most people don't know that we did it. They still happily click on \"accept all\" and blame it on EU to need to do that. They don't know what the heck is that, and why they should understand what's written there. When they are asked cleanly, and simply whether they want to share their data to thousands of shady companies, about only a quarter choose yes. That's why Facebook had to force everybody in the EU to choose between paying and accepting it. reply globular-toast 5 hours agorootparentEveryone I know did, because I told them. But they did it anyway. Now we wish it was only as bad as Facebook was 15 years ago. Divide and conquer tactics work. Microsoft, Facebook and Apple all use it to great advantage. The funny thing is it was always about free software. Not enough people listened to Stallman. None of this could have happened if people rejected non-free software. reply nalekberov 5 hours agorootparentIt could, and it happened, not long time ago, everyone thought just because xz software was free software, it's 100% safe, but it was not. There are numerous examples. Stallman lives in his own delusional world, and GPL not only solved the problem, it created more burden, that developers decided to use other less restrictive licenses. reply globular-toast 5 hours agorootparent> There are numerous examples Such as? > it created more burden In what way? reply nalekberov 4 hours agorootparent> Such as? You can search the web, but as an appetiser here you go: https://jfrog.com/blog/malware-civil-war-malicious-npm-packa... > In what way? Well, you can't bundle GPL licensed software with less restrictive one such as BSD, which is a big deal, that's why BSD and GNU/Linux are so separated in many ways. That's just one problem. It'c clear that all-or-nothing approach doesn't work in free and open source software world. reply dmm 1 hour agorootparent> Well, you can't bundle GPL licensed software with less restrictive one such as BSD Sure you can. If the result is a derivative work redistribution is subject to the terms of the GPL but you can bundle all you want. reply unclebucknasty 5 hours agorootparentprevMany know by now, but find the trade-off worth it for convenience or whatever they're getting in return. I mean, people are willingly giving up their DNA. But, if it was the government receiving all of this data, they'd be in a panic. This has basis in current day fear-mongering about government power, as well as warnings about authoritarian governments from Orwell and beyond. Not to say there's no reason to be concerned, but the casual mentions of 1984 are hilariously dated and ironic in 2024. More substantively, it also reveals the naivete of those who wish to completely disempower their democratic governments. That is, someone will still retain the power when it's taken from the government. The only question is who and whether everyday people will have a voice in their own governance. One look at corporate power and our deference there is a pretty big hint. reply rusk 6 hours agorootparentprev> We're so far past it now, For me it was more like 2014. Though it was going on long before this … I think it was the pandemonium and ultimately the widespread acceptance … that sealed the deal! reply iamacyborg 6 hours agorootparentI know you meant to write pandemic instead of pandemonium but “the pandemonium” sounds like a fun future event. reply rusk 4 hours agorootparentI meant pandemonium in the wake of the Snowden revelations :) reply consp 6 hours agoparentprev> Politicians and police are not subject to monitoring Isn't that -again- a direct violation of the charter of fundamental rights (article 20)? (all are equal before the law) reply psychoslave 6 hours agorootparentbut some are more equal than others reply hulitu 6 hours agoparentprevWhat they never seem to get is that the status of \"politician\" or \"police\" might not last forever. reply Teever 7 hours agoparentprevUgh. Imagine living in a Bizarro world where the law said that private citizens could not be tapped without a warrant and probable cause but politicians and police must be surveilled to mitigate corruption. reply gpvos 6 hours agorootparentYou'd have to pay politicians and police a lot more then. reply andy_ppp 6 hours agorootparentMaybe you’d just get better people becoming politicians, ones that couldn’t be corrupted? reply moffkalast 6 hours agorootparentThose people don't want to be politicians. reply squarefoot 5 hours agorootparentThis. We should never give power to those who are after it; the mere desire for power is a good clue that said power will be abused. Maybe not always, but it's often the case. Now, how do we vote someone who doesn't run for a seat? Heh, good question! reply Teever 2 hours agorootparentYou make the job more appealing to those kinds of people. I think one of the ways to do that is to make the job less appealing to the people who currently hold it. reply moffkalast 2 hours agorootparentI think the hitchhiker's guide solution is not the worst, don't even tell them that they're doing the job and just take whatever they say and implement it. reply forgetfreeman 5 hours agorootparentprevNobody that actually wants the job is fit by definition. reply hulitu 6 hours agorootparentprevOh, come on. Police, ok, but politicians ? reply noworld 6 hours agorootparentThe corrupted politicians by and large have the money already and have it through things like rent and capital gains, not salary. Paying more as a salary enables more average people to leave their current jobs to take part in politics. reply rcxdude 6 hours agorootparentprevLower-level local politicians are probably the main issue. They're often paid so little it's effectively only a career option for the already-wealthy. And when that's the path towards the high-level, reasonably paid positions, it biases your pool a lot. (Not unlike industries with an expectation of a long period of unpaid internships in high cost-of-living areas) reply mantas 6 hours agoparentprev> Politicians and police are not subject to monitoring Wonder how low would be the bar to become a politician. Signing up for a local council elections definitely makes one a politician, right! reply _heimdall 6 hours agorootparentRobert Kennedy Jr had to make his own political party in some states to get on the ballot. I'd assume the average person in many areas could create their own party with a stack of paperwork. Then track down local laws that define what would be considered a politician, my guess would be something like actively running for an office or being named as a party's candidate or political leader. reply logicchains 6 hours agorootparentprevDon't worry, they'll eventually close that loophole by banning unapproved political parties and candidates, like in China. reply dandanua 6 hours agoparentprev> Politicians and police are not subject to monitoring, only ordinal citizens messages’ should be wiretapped Animal order, at its best. reply andy_ppp 6 hours agorootparentAll animals are equal, but some animals are more equal than others. reply highcountess 5 hours agorootparentprevIt's far simpler than that, it's just the reconstitution of what we call aristocracy from the past, the reversal of the American Revolution and Constitution, the pole-flip of the power relationship between the \"ordinary citizens\" and the powerful/government. It is he same abusive pattern of lying used to manipulate people against the right to self-defense agains a tyrannical government through the supreme law that prohibits the government from infringing on the people's inalienable, God given right to the means of self-defense, as enshrined in the Second Amendment to the US Constitution. \"Think of the children\" the tyrants wail as they demand you give up your ability to defend yourself against the bombs they threaten to use against their own population that refuses to submit to the desires of the ruling class and they are also busy slaughtering children by the tens of thousands. reply 2OEH8eoCRo0 6 hours agoparentprev> Stasi-fans Is this language necessary? reply throwuxiytayq 6 hours agorootparentPerhaps not, but it sure is appropriate. reply blueflow 6 hours agorootparentNo it is not. Comparing the current surveillance capitalism with the Stasi is a trivialization of the former. reply rusk 6 hours agorootparentbut it is important to point out their possible aspirations reply blueflow 6 hours agorootparentYou could get access to your Stasi file and work on getting yourself arranged with the State. It was not some secret algorithm like it is today. reply rusk 4 hours agorootparentFunny how when they're proposing these things they never address FOI issues like this :-) reply 2OEH8eoCRo0 6 hours agorootparentprevMaybe so but I don't like applying labels to people and their complex positions. They're stasi-fans? Why bother listening to them? reply fallingknife 6 hours agorootparentWhy indeed? I already don't bother listening to the authoritarians who favor censorship and surveillance. reply 2OEH8eoCRo0 4 hours agorootparentRight! Someone on a forum labeled them stasi-fans and saved me from thinking for myself. reply baxtr 6 hours agorootparentprevWhat’s wrong with it? It’s basically marketing. It’s the use of a certain language to draw attention to a very important topic. reply blueflow 5 hours agorootparentIts a needless emotionalisation of the topic. It might work with NPCs but people who can see behind it will get irritated. reply baxtr 2 hours agorootparentNeedless in your view. Drawing attention always requires emotionalisation. reply blueflow 2 hours agorootparentHow will that attention be converted into some useful action? reply baxtr 2 hours agorootparentThat’s a valid follow-up question. But for any goal, the first step is attention. reply pseudalopex 4 hours agorootparentprev> NPCs You believe this is better than Stasi fans? reply blueflow 4 hours agorootparentNot really. But any euphemism (like sheep, lemming) i could use to refer to \"people who are incapable of critical thinking\" would, by that use, become a pejorative, so there is no way to win that battle. I picked NPC because it entered youth slang in the last years and understanding of that word is widespread. reply pseudalopex 3 hours agorootparent> Not really. But any euphemism (like sheep, lemming) i could use to refer to \"people who are incapable of critical thinking\" would, by that use, become a pejorative, so there is no way to win that battle. Any euphemism they could use to refer to people who support mass surveillance would, by that use, become a pejorative, so there is no way to win that battle. In your reasoning at least. And none of these are euphemisms really. reply blueflow 3 hours agorootparentNice try but that didn't work out. \"Chat Control Advocates\". Edit: Child Protectors. Cheese Haters. Dickpic Stealers. Now i get all the ideas for funny euphemisms. reply pseudalopex 3 hours agorootparent> Nice try but that didn't work out. \"Chat Control Advocates\". The groups overlap but are not identical. And you could have said some people. But you wanted to show your disdain. > Child Protectors. Cheese Haters. Dickpic Stealers. You believe these are not pejorative? reply blueflow 2 hours agorootparentI think they are more some kind of joke than pejorative. reply muzani 6 hours agoprevThe real danger is criminal profiling. Read a book on criminal profiling as done by the FBI. You hear things like \"the suspect appeared nervous when his eyes saw the murder weapon\" or \"serial killers match two of three: cruelty to animals, obsession with fire-setting, and persistent bedwetting past the age of five\" (aka Macdonald triad). Impulsive killers are in their teens or early 20s, while more careful killers will be at least in their 30s. I'm sure the motives were good - sometimes it's like finding a needle in a haystack, and it saved lives back then. But you have mass surveillance, you can go through every hay in the haystack. Yet likely they won't. They'll settle on a middle ground with these outdated methodologies, and combine it with AI/data, to create some form of data-driven astrology. Someone will be inspired by CSI to ask AI to blow up a blurry photo, and AI might just hallucinate it. There will be experts out there who would oppose this, and these could be shut down by their bosses, the politicians who don't understand how it all works. The Macdonald triad detects the worst criminals, sure, but it mainly detects victims of abuse. Privacy isn't important to the privileged groups; but it's a level of protection for the innocents who could be profiled wrongly. reply donmcronald 6 hours agoparent> some form of data-driven astrology That’s such an apt description of the junk science that’s going to get justified by AI. reply SXX 5 hours agoparentprev> to create some form of data-driven astrology Polygraphs are still heavily used in US even though everyone know it's anti-scientific bullshit. reply belter 5 hours agoparentprevJust use Steganography...For example the next phrase contains the first seven decimal digits of Pi: I view a plane welcoming me aboard reply vindex10 5 hours agorootparentyou meant, Steganography :) reply belter 4 hours agorootparentDamn...of course...corrected! Thank you my Thesaurus as a Service. Do you have an API? reply cowboylowrez 4 hours agorootparentlol i'm always spell checking with google, course probably a small percentage of alternative spellings will probably be hallucinated by google's chatbot but thats just the cost of doing business nowadays. my poor grammar is all me tho. reply sjducb 6 hours agoprevThe debate is framed as privacy vs security. Really it’s internal threat security vs external threat security. Measures to reduce personal security also reduce the security of the traditional armed forces. 1) The armed forces use most of the same networking software and hardware as civilians. They also rely on the same protocols. 2) In a total war scenario, like Ukraine, civilian communication infrastructure becomes military communication infrastructure. See soldiers relying on phones for communication and apps to aim artillery. 3) The vulnerabilities that get built into civilian communications are obvious cyber warfare targets. The framing of privacy vs security tricks the traditional armed forces into thinking that they have the same interests as the NSA. reply maga_2020 6 hours agoparentFamous quote by Benjamin Franklin (from 1755) : “Those who would give up essential Liberty, to purchase a little temporary Safety, deserve neither Liberty nor Safety” ... \"... Franklin was thus complaining of the choice facing the legislature between being able to make funds available for frontier defense and maintaining its right of self-governance--and he was criticizing the governor for suggesting it should be willing to give up the latter to ensure the former. ...\" [0] [0] https://www.lawfaremedia.org/article/what-ben-franklin-reall... reply npteljes 4 hours agoparentprevI think that using a secure layer on top of the insecure layer undermines this argument, similar to how HTTPS is secure while using HTTP with a twist, and using every underlying system in the same way. Or how GPS functionality is regulated for civilians. So especially \"1)\" won't be true. Yes, right now they may be using same or similar things, but then after the new regulation they would be using superior stuff, problem solved. \"3)\" is considerable because that's true, whatever difference there is in comms security, adversaries will have the same power over the civilian comms as their own government. Right now of course this is the case already, but especially after regulating it, will it become prevalent. I think governments are fine with this in general, though, which I deduct from the lack of countermeasures to it. In reality, encryption is power, and the more power individuals have, the less power those have who want to control individuals. Everything else is smoke and mirrors, like the classic \"think of the children\" argument. reply sjducb 3 hours agorootparentRegarding #1. Will there be performance implications when adding the secure layer? Will there be cost implications when adding a secure layer? Will the secure layer add risks to the project? Governments are already bad at delivering defence projects on time, will the extra complexity make it worse? How do we know which layers are insecure? Will there be a published list of vulnerabilities that need to be mitigated by the military? reply palata 5 hours agoprevI believe decision makers really need to understand 3 basic points: 1. End-to-end encryption does exist today (and it is deployed at scale). There is no going back. 2. There is no middle ground: either it is end-to-end encrypted, or it is not. \"Sniffing\" encrypted messages is not a thing, period. 3. Make all the laws you want, criminals will always be able to use end-to-end encryption. Those laws will only prevent honest people from protecting their communications. reply filleokus 4 hours agoparentI strongly agree with 2) and 3), but sadly I think 1) is overplaying our collective hand. I would guess that the largest deployments of end-to-end-encryption today is Whatsapp and iMessage by a quite large margin. E2EE for \"real people\" is provided by the grace of two massive publicly traded companies who have to follow local regulation. If Apple complies [1] with dubious requirements in China, I wouldn't bet against them doing it elsewhere either. Sure, we have Signal, but how many normal users would start delving into side loading if the application simply was banned and not allowed on the marketplaces? We can always use PGP-over-whatever, but that's an argument for 3) - not 1). I think politicians / police (and honestly many normal people) believe that the government have the right to do lawful interception of private communication and see E2EE as a step to far. The US has been wiretapping phones for a century already. We as privacy arguers have a pedagogical challenge of explaining why regulation like this is bad and not equivalent to 1930's style phone wiretapping. [1]: Chinese iOS users have their iCloud data (that for the vast majority includes the decryption keys) on Chinese servers, subject to the Chinese legal system. For the rest of the world the situation is the same, but s/China/U.S, which is arguably problematic as well. reply tga_d 1 hour agorootparentI don't think their first point meant to say that there is no going back on deployed e2ee at scale, I believe they were just providing ground for connecting the next two points: it's so pervasive currently that, even if outlawed, criminals will always have no problem retaining access to it somehow. Even Signal, which relies on fairly centralized infrastructure, still has an open source server implementation that I suspect wouldn't be terribly difficult for a motivated criminal enterprise to deploy privately. Contrast with something like advanced weapon systems, where rarity makes it still viable to control and legislate. reply palata 51 minutes agorootparentYes that was exactly what I meant! E2EE exists out there in so many different forms that it is impossible to make it disappear as a technology. You don't need a world expert to deploy it: it's just a matter of using one of the available libraries. I mentioned that it is deployed as scale because it is not a niche thing anymore: most people in the world have benefited from E2EE already! It's not something one could hide or make disappear. The point is that policy makers need to accept this as a reality: they cannot wish E2EE did not exist, that ship has sailed long ago. reply tempaccount420 12 minutes agoparentprevCan we extend this to client-to-server communication too, not just client-to-client? Why do we allow Cloudflare to terminate so much encrypted traffic? reply FpUser 4 hours agoparentprev>\"End-to-end encryption does exist today (and it is deployed at scale). There is no going back.\" It is useless if the spyware can scrape screen, log your keyboard / etc reply palata 49 minutes agorootparentThis is my second point: there is no middle ground. Either it is useful (because it works), or is it useless. There is no \"it works for the good guys but it does not work for the bad guys\". There is no tradeoff. Either it works for everybody, or it does not work at all. reply andy_ppp 6 hours agoprevAre these people absolutely stupid, we could end up in a potentially catastrophic cyber war at some point and we need to be looking at better more secure systems than making even further holes in what we have! reply jeltz 4 hours agoparentKinda. My guess is that they are very narrowly focused and miss th bigger picture. They worry about organized crime and the war on drugs while forgetting the bigger picture. reply hoseja 6 hours agoparentprevWhat they actually want is to lose that war. Very badly. Or, their sponsors do. reply dlachausse 7 hours agoprevThe war on personal freedom in general is on the rise. reply DoodahMan 6 hours agoparentwhy won't you think of the children? reply ranyume 58 minutes agorootparentmaybe he is reply coldtea 7 hours agoparentprevIt's for your own good. Democracy is under danger, especially by the voters /s reply vundercind 5 hours agorootparentTo be fair, death-by-voter is a fairly common way for democracies to die, so—putting aside what that may or may not justify from a policy perspective, the sentiment isn’t silly. Voters really are one of the greatest dangers to democracy, that’s just true. reply nojvek 4 hours agorootparentWell, we vote for representatives, not for the policies. And there isn't much choice in representatives. Wish we had more choice than Trump or Biden but here we are again after 4 years. There is little churn in Senate as well. Supreme court justice positions are for life. We're not that far from the ways of Monarchs. The powerful will guard their power. reply swayvil 6 hours agoparentprevIt's for your health. The scientists know better than you. reply consp 6 hours agorootparent> The scientists know better than you. No scientists were harmed (or involved) in this farce. reply dlachausse 6 hours agorootparentI’m presuming that was a reference to the “trust the science” that was used to silence any dissent against the official narrative during the COVID pandemic. reply lopis 6 hours agorootparentprevIt's for the sake of the children, as always. reply jillesvangurp 6 hours agoprevTwo people can keep a secret if one of them is dead. Anything involving secret backdoors, intentionally compromised crypto, not so secret master keys, etc. is doomed to leak to a hostile entities abusing this. The weakest part of the system becomes secret weaknesses staying secret. Intentionally compromised encryption is going to be enormously appreciated by North Korea, Iran, China, and all the others one would normally want to keep from looking at secret data related to finances, personal communication, military secrets, industrial R&D, etc. Countries need to get their priorities straight on national security. The enemy is outside of their country, not inside. And they don't play by the rules, generally. reply highcountess 5 hours agoparentIt seems to me that you may be making a mistake in assuming these counties do no in fact have their priorities straight, only that those priorities do not align with most people's interests. The purpose of a system is what it does, not what it consistently and persistently fails to do. Far too long and far too much, people have assumed a good will of our governments because we have intenionlaly been conditioned to accept with blind faith that \"democracy\" is a universal \"good\", never asking oneself why the tiny psychopathic ruling class would be such vehement proponents and rabid advocates of \"Our Democracy\"™, a supposed rule by majority. It appears that not everyone gets as suspicious of things that contain inherent fundamental contradictions. reply gala8y 7 hours agoprevKnowledge is a deadly friend If no one sets the rules The fate of all mankind, I see Is in the hands of fools _King Crimson - Epitaph_ reply BSDobelix 6 hours agoparentBut knowledge prevents that the fate of mankind is in the hands of fools. Knowledge and information need no rules, but humans do. reply croes 7 hours agoprevAnd in the end everyone is amazed why hostile hackers could read confidential messages. reply lolc 6 hours agoparentQuote from the article: > Stefan Hector, a representative of the Swedish Police Authority, said that “a society cannot accept that criminals today have a space to communicate safely in order to commit serious crimes.”[0] A week later, it was revealed that the Swedish police had been infiltrated and were leaking information to criminals.[1] [0] https://polisen.se/aktuellt/nyheter/nationell/2024/april/eur... [1] https://www.svd.se/a/8qwGbx/granskning-poliser-lacker-till-g... reply balder1991 6 hours agoparentprevWe might need a huge leak to open people’s eyes. reply reaperman 6 hours agorootparentThe only leak I can imagine impacting this would be a giant dump of politician’s personal communications and they are exempt from encryption bans in this particular bill. reply balder1991 5 hours agorootparentIDK there are powerful people in society aside from politicians that can influence public opinion too. reply reaperman 5 hours agorootparentVery good point. I should think more deeply before commenting and not rush out off-the-cuff remarks. reply FpUser 4 hours agorootparentprevWell it'll still be a start. A very good one I think reply ranger_danger 3 hours agorootparentprevTons of data leaks all the time but surprisingly nothing seems to happen with it. Not only are companies not being held liable, but for some reason nobody is weaponizing that leaked data either. Imagine framing somebody with manipulated leak data, nobody even questions if it was ever real in the first place! reply croes 3 hours agorootparentIt's a software problem, nothing we can do about it. For leaks the hackers are always to blame, never the poor security of the companies. reply riku_iki 16 minutes agorootparent> It's a software problem, nothing we can do about it. there can be standards/requirements developed how to handle important data, and it will harden system. reply williamcotton 6 hours agoprevThe underlying tension here is the expectation of privacy for signals broadcast outside of one’s property and into the commons. We don’t yet have a firm grasp on how to handle the issue of extending privacy into these shared spaces. There just seems to be two unreconcilable polar extremes at the current moment without a clear path forward. At least in the United States it could mean needing an amendment to the constitution as the 4th doesn’t properly cover these protections. reply strix_varius 6 hours agoparentFundamentally, the tension is between the universal laws of mathematics on which encryption is based and the politicians and police who don't understand them. This quote from the article brings that into stark relief: \"we do not accept that there need be a binary choice between cyber security or privacy on the one hand and public safety on the other. Absolutism on either side is not helpful\" Clearly, Europol misunderstands the fundamental, mathematically binary, nature of encryption as man-made \"absolutism\" that could be dispelled with just enough laws and warrants and wiretaps. reply williamcotton 5 hours agorootparentThe laws of math and physics have very little to do with the determinations of the laws of people. You can’t ban the chemistry of creating alcohol from homegrown grapes but you can ban and enforce against the practice well enough to limit bootlegging. The average person buys from the state regulated dispenser regardless of the possibility of doing otherwise. The same COULD (uh oh, about to be misread and downvoted into oblivion) be said for cryptography, where bans of consumer friendly and easily available messaging applications would be similar to curbing bootlegging. Of course the practice would continue for those who are capable but with consequences. These are issues that must be reconciled. Hiding behind numbers will not work, as we are seeing play out in front of us, so we need constitutional protections. reply vasergen 5 hours agoprevSorry for off-topic, just wanted to say that the price page on mullvad.net is the best one I saw comparing to other SaaS. Everything simple and straight forward! reply Nevermark 5 hours agoprevWide surveillance that aids the government in creating portfolios of “incriminating” circumstantial ”evidence” greatly increases the risk to the innocent, especially the under privileged, if it isn’t counter balanced by an increase in stronger protections for the innocent. Unfortunately, in the US there is no bottom line constitutional protection for the (actually or credibly) innocent. Procedure pre-empts innocence. This impacts everything from permanent property confiscation based on subjective declarations of “suspicion” without any criminal indictment [0], general crime [1], to execution of the likely innocent [2]. Without a fundamental right for the innocent to have convictions vacated, after they are revealed to be highly suspect, procedure pre-empts information, even when the case for innocence is widely considered credible. Executing the (credibly) innocent can even become a strategic politically advantageous performance. [3]. Constitutional protection of the incident would compel the courts to address catch 22 problems for the credibly innocent directly, providing a mechanism for relief when the gears of formal justice otherwise create no balanced recourse. [0] https://www.npr.org/sections/thetwo-way/2014/11/10/363102433... [1] https://en.m.wikipedia.org/wiki/List_of_wrongful_convictions... [2] https://en.m.wikipedia.org/wiki/Execution_of_Nathaniel_Woods [3] https://www.endfmrnow.org/arkansas-governor-denies-clemency-.... —- Telling: what government is also pushing for all internal deliberations, materials and search queries, databases and algorithms leading up to prosecutions to be archived for the defense? Or to open up comprehensive surveillance systems search to the defense team? Centralizing power and information in a way that supports convictions, but not defense, is a recipe for increased motivated, convenient, and incidental injustice. reply FpUser 3 hours agoparent[3] is total madness. I think that highest level politicians who allow it to continue after being made aware should fucking rot in jail. reply bloopernova 5 hours agoprevThey really said they had a foolproof way to read encrypted content to determine if it was \"bad\"? I'm still flabbergasted that anyone supported danny masterson. That the head of an anti-csam org did it is just astonishing to me. reply NeutralForest 5 hours agoprevThis is terribly depressing. How can we, as citizens, protect ourselves and vote with our best interest in mind? reply amelius 6 hours agoprevCan't they just put back doors in our silicon and be done with it? reply ranger_danger 3 hours agoparentYou don't think this is already the case for years? reply BLKNSLVR 4 hours agoprevExecutive Summary: \"They\", as in, whoever will have access to all this gathered information, will not know how to use it. Therefore, it is inevitable that this gathered data will not be used intelligently or responsibly. Law enforcement bodies in any country on the planet just do not possess the smarts to handle it. They don't know how to read, and these politicians want to give them more books. I've mentioned the below a few times on HN because, for me, it was a traumatic incident with an outcome that was essentially an unknown for 8 months of my life: My house was raided by police for suspicion of distribution of CSAM. Skip-to-the-end: They took about $10k worth of my homelab gear, had it for 8 months, and then I was told I could come and pick it up. No charges, no nothing. There are pages and pages of things I've written to document this incident, these are a couple of notables: They didn't know to expect there to be children in the house. At ~7am on a school day. That's the amount of additional investigation they do into the 'subject' prior to raiding their property. ie. Fucking None. I have two teenaged children that have impeccable school attendance and academic results that I'd assume would be easily accessible to law enforcement, had they cared to do any background research. This was a traumatic event for MY CHILDREN caused by quote-unquote Law Enforcement. They said the raid was based on information provided from an overseas (I'm assuming US-based, I'm in Australia) source (private company, law enforcement, intelligence, I have no idea) that identified my household's street address via an IP address, and this \"evidence\" was gathered over a number of years. They asked my eldest \"does your dad use your computer\", he replied \"no\" (truthfully), and they didn't take it, didn't even look at it. I pointed out a laptop on the breakfast table that I said was was my wife's (truthfully) and they didn't take, didn't even look at it (as far as I recall). I specifically asked if this was about \"me, personally\" (since they showed no interest in my wife's laptop), and they repeated their (what became somewhat of a mantra) of \"this specific address, not your neighbour's address, this address\". And yet they had no interest in computers identified as not belonging to me. One of the first things they asked me was \"do you use a VPN?\" I answered \"no\". Which was true then, and is sure-the-fuck not true now (split routing, the fucking works) - as a learning outcome, to protect myself from further blind police incompetence. As part of the phone call when I received the \"good news\" that I could come and pick my shit up (I guess they came out of their way to pick it up from my place, I had to return the favour), the detective in charge of the investigation gave me the impression that she thought that she / the system couldn't be wrong about me. She made a point to say the following three things: - Use of virtual machines is suspicious - MEGA is almost exclusively used for CSAM and they found evidence that I'd downloaded things from MEGA (I think I may have used MEGA to download android ROMs) - They found \"TOR\" installed on one of my computers The last one. Where do I start? It's so incorrect an understanding that it was a 'cherry on top' data point that I didn't really need to confirm how hopelessly out of their depth law enforcement is with technology. To repeat: They don't know how to read, and these politicians want to give them more books. One of the ironies is (because I've got this sort of sense of humour) if I was living in an actual panopticon, I'd have easily been exonerated as I'm far too boring a person without enough spare time to engage in any of that behaviour. In addition to the above, my better half is a teacher. Teachers in Australia have a \"Mandatory Reporting\" responsibility to report possible cases of child abuse based on their observations as a teacher. The suicidally depressing thing about it is, the organisations responsible for visiting the homes and families that have been reported are so under-staffed and under-funded that they only have the time to investigate cases where the child is in immediate life-threatening danger. Combine the two above anecdotes together and you've got a society that's gone wrong and is still applying increasing pressure to the accelerator. Politicians calling for laws against encryption who are not also calling for massively increased budgets to child protection organisations are pushing an agenda that has nothing to do with real, actual protection of children, and they should be called out. reply which 2 hours agoparentThat is disturbing. Were you running an exit node from your home? Or maybe they were watching Tor users and some sort of traffic analysis heuristics they were using gave them a false positive? reply ranger_danger 3 hours agoparentprev> Uses VMs, mega and tor, but no full disk encryption > Talked to the police I think this is a lesson in what not to do in the future. And that equipment they took? Consider it all 100% compromised. reply varispeed 6 hours agoprevCan't help but think these laws are pushed by pervs who would love to be hired as moderators for such filtering systems to watch, store and sell the content the system has fished out for them. reply paganel 5 hours agoprev> But Ylva stood by her claim. She came back to the same argument over and over again. She avoided answering the questions (she obviously didn’t understand how the technology worked) but instead turned the direction of the discussion, saying, for example, that a court order would be required to carry out scanning, which in itself was deliberately misleading. Is there currently a way for us, EU citizens, to vote those Brussels ghouls out of power? No, or at least none that I know of. Is this how a democracy is supposed to work? Certainly not. Which is to say that events like this one should make most of the people see that the EU and the European Commission are certainly not democratic and they certainly do not represent the European electorate, because, as a matter of fact, the European electorate has no power over those people. But the propaganda still coming from those circles is too powerful, so, here we are. reply rasengan 7 hours agoprevThis article reads weird to me. Definitely appreciate the depth of this - thorough research was definitely done. Secondly, I’m definitely quick to blame the US policies for a lot of things. However, this article is mostly about casting shade on the US about the EU’s mass surveillance. I don’t think anyone is to blame for what the EU does other than the EU, and for sure attacking the US isn’t going to prevent the EU from doing this. I wonder if there is a better way to go about bringing awareness and taking action? reply noutella 6 hours agoparentThe article explains how Ashton Kutcher backed non-profit \"Thorn\" was a cornerstone of the Chat Control bill that was to be passed a few years back ; it also explains how Palantir pushed for the bill behind the scenes and how at least one former FBI agent and other members of non-EU security agencies participated in meetings to kickstart that new version of the bill. reply coldtea 7 hours agoparentprev>I don’t think anyone is to blame for what the EU does other than the EU Well, some charade aside, the US usually says jump and EU politicians say \"how high?\" - the EU population be damned reply klabb3 6 hours agorootparentI think the EU has been better in that regard than individual European countries. The Wikileaks diplomatic cables in particular showed US coercion on a country by country basis. In Sweden, Wikileaks showed US diplomats gave a list of laws and executive actions (at the time around IP - the Pirate Bay was based there), with veiled threats about getting gray-listed as a “partner” which can affect trade etc, and they said how high. Now to be fair, at least Sweden was unofficially a NATO/US intel/security collaborator. But EU is in a different position, mostly oriented around trade, and notably lacking in security and military bodies. But the EU has absolutely stood up against US interests, especially their predatory corporations. However, if the EU is collaborating or even aligning with the US on intel gathering, it’s pretty far outside their openly stated mandate, afaik. Especially since the UK left, who were the most hawkish on mass surveillance, it’s creepy to think there are clandestine efforts to push for aggressive monitoring and even worse aligning with the US without oversight. If Mullvad is right, it’s also an absolute failure of MSM to not properly cover such geopolitically crucial issues. > the EU population be damned For sure it’s a concern, but overall many/most Europeans think the EU is a net positive today. Things have changed a lot since the crises of Greece etc. And with increasing geo-political tensions (Russia in the short term and China in the medium term), there’s an argument to establish stronger security and military efforts independent of US-led NATO, which have quite different goals. reply 4bpp 5 hours agorootparent> But EU is in a different position, mostly oriented around trade, and notably lacking in security and military bodies. But the EU has absolutely stood up against US interests, especially their predatory corporations. I think this understanding of the EU's behaviour may be insufficiently cynical. There's one pattern in politics that is very hard to not see everywhere once you have been primed to, which is \"high + low against middle\": the faction that is in power allies with one(s) that is so far away from power as to never become a credible threat to it, in order to put the squeeze on a third faction that is actually a serious contender for the position of the first. A canonical example that's sufficiently historical that it hopefully won't be too incendiary was the practice of early communist states to elevate individuals of peasant/worker background into positions they were unqualified for, as in the famous case of Lysenko - here, high (party brass) supported low (peasants/workers) at the expense of the middle (bourgeois intellectuals, represented in that particular instance by academia, who could have been organised and experienced enough to orchestrate a palace coup). Within the US, the federal government/military/foreign policy complex and tech-based New Money are widely recognised as two distinct power centres, with it at times being unclear if the former can actually fully dictate terms to the latter. Under normal circumstances one would expect the former to champion the interests of its industries on the international stage, and indeed the US is known to have very sharp elbows in this regard (from the famous oil wars in the Middle East via the slightly less famous fruit ones in Central America to the backdoor arm-twisting in copyright matters). The picture for the tech industry looks quite different - far from starting a war or even merely successfully lobbying the EU to drop its regulation, the USG is looking away and whistling. As it happens, out of the four industries mentioned (oil, fruit, media, tech), the tech industry happens to be the one that is by far the most autonomous and misaligned with federal government interests (Apple randomly grandstanding on privacy, everyone wanting to keep their Chinese supply contracts and market access, general abundance of politically engaged progressives and libertarians...). Wouldn't it make sense if what happened was that the USG (high) actually gave the EU (low) a tacit go-ahead for their anti-US-tech measures, and perhaps even indicated to everyone involved that they may let them crack down even harder if the tech industry (contender for high) keeps falling out of line? reply klabb3 1 hour agorootparent> the faction that is in power allies with one(s) that is so far away from power as to never become a credible threat to it, in order to put the squeeze on a third faction that is actually a serious contender for the position of the first. Makes sense. I’m sure it happens. However, it’s an advanced construct and just one out of several incentives in a complex system, so I wouldn’t necessarily blanket attribute it to explain things. That said I also think you’re right that the USG does seem less imperialistically engaged with tech than say oil. That could have other explanations, such as less cozy relationships around subsidies and historical geopolitical interest. I mean, I think it’s entirely possible that there’s enough inertia in these systems to explain why one looks different than another. It doesn’t have to be a delicately played 4D chess by a bunch of boomers who don’t even know what encryption is. Don’t attribute to malice yadda yadda. In either case, from my European perspective, I’m not looking so much what the end goals are for the Americans, but rather how the countries in Europe can stand up to geopolitical winds, ie protect their interests. And in my lifetime, there’s a noticeable increase in alignment and strength, at the expense of a (imo) much less harmful set of compromises between individual countries. It all depends on what are the hot issues of the day. When it’s pollution in the Baltic Sea, or the Greeks treating the euro-wallet as a gift card, then we were all like pissy siblings. But now when the issues are war (Russia), economic hollowing (China) or having big brother deciding what’s best for you (US - although this is old), it’s better to set the differences aside, and band together. reply coldtea 1 hour agorootparent>or the Greeks treating the euro-wallet as a gift card You mean the Germans treating the Euro and ECB as a monetary vehicle to boost their economy and milk the periphery, side-stepping any \"hard rules\" imposed for others when it was convenient for them, explicitly carot-and-whiping the South to de-industrialize over decades, and then strong-arming the indebted states as a means to pad German investors by moving money from the taxpayers to their banks and investment firms, while buying state assets (from airports and roads to utility companies) for themselves (with a few bones thrown to the French)? At the same time imposing stupid austerity policies (against the advice of expert economists) that made recovery impossible and amounted to war-level destruction for the economies involved? All the while cheerfully reviving racist language and imagery (like \"rats\" in the european kitchen, and other such niceties, of which calling the southern economies PIGGS was among the most prominent). reply marginalia_nu 6 hours agoparentprevA big part of the problem is democratic. There's nearly nonexistent political accountability in the EU. If the EU decides something, there's really no effective way for its citizens to do anything about it even if it's a fairly unpopular change. There are too many layers of indirection between the elections and the decision making to hold the responsible politicians accountable to the voters. This in turn makes the EU extremely susceptible to lobbying from special interests inside and outside of Europe. More so than these campaigns, big reason why these laws have been hard to push through is probably Germany and their strong influence in the EU. Since the Germans still have a living memory of the DDR and the Stasi fallout made a significant impression in the public conscience, being seen as moving back in that direction is a really tough sell. 1984 is fiction, the east germans lived that shit. reply jjtheblunt 6 hours agoparentprevIt reads weird perhaps since it’s written by a provider of VPN software. reply boffinAudio 6 hours agoparentprevnext [2 more] [flagged] ranger_danger 3 hours agorootparentGonna need some sources for that, boss reply TacticalCoder 5 hours agoprevThe one thing that makes sick to my stomach is that all around Europe there are criminals committing actual crimes and getting a slap on the wrist from heavily politicized judges. There have been rapists freed because \"in their culture it's different\" (these are documented facts: it happened in several countries, more than once... For example in the UK because an 18 y/o muslim raped a teenager after his religious teacher taught him that \"women are worthless\" the judge relaxed him). It's not about protecting the children. They actually love it when children and teenagers are sexually assaulted: that gives more fuel to put in place a totalitarian state, using the pretext of protecting the kids. It's the same everywhere. In France many crimes are committed by people Macron promised to deport: illegal migrants already caught for a crime. He said, before being elected, that he'd deport 100% of the illegal migrants committing crimes. Instead of that socialist judges are constantly releasing these dangerous criminals in the street. But the actual victims? And victims' families? Zero concern. None. And if a victim dares to fight back and should hurt the illegal migrants: then the whole power of the state falls upon him and he'll get an incredibly harsh sentence. The world is upside down: politicians do not get to have their communication monitored, victims are sent to jail if they dare to defend themselves. And why do they want to wage their war on encryption? To fight me. Because I hate the EU I live in and they want to silence me. And all those like me. The tyranny of the government is a very real thing and anyone longing for more government and more government spending should look deep down in his heart and conscience and wonder if it ever did any good for a country to have ever more government. Meanwhile people shall hate on the libertarians, calling them names (\"ladder pullers\"), but I'll tell you this... Libertarians would never ever vote complete and total surveillance of citizens, while protecting the politicians. But do not worry: the world you deserve for hating on libertarians is coming to you soon enough. reply npteljes 4 hours agoparentThe world is exactly there as it ever was, power structures and power struggles. Two major differences to the past is that there is more people than ever, and that we currently have near-instant global communication. I'm with you on the \"think of the children\" argument. That is 100% what's happening, and I think that it boils down to how the human mind works, particularly on how putting out a fire feels much better than preventing the same fire. \"Libertarian\" means a lot of things, so it's hard to criticize, I especially struggle with the closing thought. Who are these libertarians that we should support, in order to have a better world? reply andersa 6 hours agoprev [–] I thought Chat Control was dead. Is it coming back? The article is far too long. reply noutella 6 hours agoparent [–] Yes, basically it's back under another name after having been knocked off. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The \"Going Dark\" initiative, led by EU Commissioner Ylva Johansson, proposed extensive surveillance measures using AI to monitor all communications under the guise of combating child sexual abuse.",
      "The proposal, known as \"Chat Control,\" faced significant opposition from bodies like the European Parliament’s Data Protection Board and the UN Human Rights Council for violating human rights laws, leading to its rejection.",
      "The European Court of Justice ruled against accessing encrypted communications, highlighting concerns about privacy, mass surveillance, and government overreach."
    ],
    "commentSummary": [
      "A proposed EU bill seeks to increase surveillance by wiretapping citizens' messages, while exempting politicians and police, raising significant privacy concerns.",
      "Critics compare the bill to Orwell's \"1984,\" warning of unchecked authority and the erosion of privacy, especially given the indispensability of smartphones and social media for essential services.",
      "The debate includes issues of end-to-end encryption (E2EE), government overreach, AI misuse, data security, and the balance between privacy and security, reflecting a libertarian perspective skeptical of government intentions."
    ],
    "points": 354,
    "commentCount": 151,
    "retryCount": 0,
    "time": 1716288876
  },
  {
    "id": 40428386,
    "title": "Understanding UI Density: Designing Modern Interfaces for Optimal Information Flow",
    "originLink": "https://matthewstrom.com/writing/ui-density/",
    "originBody": "Interfaces are becoming less dense. I’m usually one to be skeptical of nostalgia and “we liked it that way” bias, but comparing websites and applications of 2024 to their 2000s-era counterparts, the spreading out of software is hard to ignore. To explain this trend, and suggest how we might regain density, I started by asking what, exactly, UI density is. It’s not just the way an interface looks at one moment in time; it’s about the amount of information an interface can provide over a series of moments. It’s about how those moments are connected through design design decisions, and how those decisions are connected to the value the software provides. I’d like to share what I found. Hopefully this exploration helps you define UI density in concrete and useable terms. If you’re a designer, I’d like you to question the density of the interfaces you’re creating; if you’re not a designer, use the lens of UI density to understand the software you use. Visual density We think about density first with our eyes. At first glance, density is just how many things we see in a given space. This is visual density. A visually dense software interface puts a lot of stuff on the screen. A visually sparse interface puts less stuff on the screen. Bloomberg’s Terminal is perhaps the most common example of this kind of density. On just a single screen, you’ll see scrolling sparklines of the major market indices, detailed trading volume breakdowns, tables with dozens of rows and columns, scrolling headlines containing the latest news from agencies around the world, along with UI signposts for all the above with keyboard shortcuts and quick actions to take. A screenshot of Terminal’s interface. Via Objective Trade on YouTube Craigslist is another visually dense example, with its hundreds of plain links to categories and spartan search-and-filter interface. McMaster-Carr’s website shares similar design cues, listing out details for many product variations in a very small space.",
    "commentLink": "https://news.ycombinator.com/item?id=40428386",
    "commentBody": "What UI density means and how to design for it (matthewstrom.com)343 points by delaugust 5 hours agohidepastfavorite181 comments onemoresoop 2 hours agoRelated to UI but not exactly on density. Even Refilling a prescriptions from Wallgreen's seems to have become impossible from a smaller phone such mine - IPhone SE2020 - since the control to choose the pharmacy or add a zip code for searching one is not actionable, the interface automatically scrolls down and when I drag the page I can see that it's there but there's no way to access it. It appears to be some kind of React garbage optimized for larger screens but completely and utterly broken on slightly smaller ones. It's not that I have anything against the technology itself but it broke basic functionality of yesteryear that just worked. And to what avail? All seems broken and ugly these days. And this isn't even about looking under the hood and analyzing the waste this wave of technology has brought. Where are we heading to? Is everything about to get worse and worse? Who is benefitting from all this because the user isn't... reply tracker1 2 hours agoparentI'm with you... I use a 5.9\" phone, but have accessibility/font settings maxed out. There are a lot of sites with limited or broken functionality. Modal dialogs are the absolute worst... they should be configured to just take over the screen on smaller displays, with the region as scrollable. It's easy enough to do, and has been my approach for menus and modals for a long while now. reply DrScientist 2 hours agoparentprevI've seen a lot of React interfaces that don't scale well - and even worse don't allow you to scroll to the bit you can't see! As I assume it's possible to have scalable interfaces in React - what's the common mistake people are doing? reply tshaddox 1 hour agorootparent> As I assume it's possible to have scalable interfaces in React React has almost no opinions on web page layouts or anything related to styling. The only type of web page problem I can think of that's specific to React would be hydration errors, and this doesn't sound like that. The reason that a lot of React interfaces don't scale well is that the vast majority of web pages are very low quality, and React is a popular way to build web pages. reply DrScientist 1 hour agorootparentIt appears to be associated with the use of react based visual components - particularly model type dialogs which when they appear allow enabling scrolling but that can appear partially offscreen. Obviously it's a developer competence issue, but I wondered if there was a React specific trick here - or as you say it's just that popular tech has by definition numerically more low quality/inexperienced developers. reply verelo 45 minutes agorootparentprevI'm glad you said this. As someone that has written his fair share of raw html, php and js, it's a bit misleading to associate react and these issues. I'm writing in nextjs / react these days, and it's amazing...but like everything out there, if used poorly, you get poor results. Designing for reactive UI's and accessibility is a feature. Sometimes features get cut, even when they shouldnt. reply jakubmazanec 2 hours agoparentprevI'm sorry, but what does poorly implemented responsive design have to with React? I agree with you that most websites are very poorly made, but that's not React's fault (as much as it is being constantly shitted on here on HN, which I really don't like), only developers' (and their managers'). Please stop blaming frameworks. reply onemoresoop 1 hour agorootparentI don't blame React per say. But with the proliferation of this technology I've noticed lots of things broken (links, history navigation, layout) that render a lot of things unusable to me and unfortunately I see very few upsides. I'm even willing to admit that React and SPAs are a great technology that enables some (few) use cases that were cumbersome before. But, it clearly seems rushed and applied everywhere with no discernable thought. reply jakubmazanec 40 minutes agorootparentReact was created because we wanted to have more complex web apps. That brings more complexity in development. Empirically, clearly we now see everywhere that most companies don't want to pay for the best version of their websites they can have. reply onemoresoop 11 minutes agorootparentI agree and part of that was delivered, I mentioned earlier that for some use cases SPA tech couldn't be better (well, it could and it keeps on improving and i'm in for it). But it seems that it washed away with good practices, with things that used to work and are utterly broken. Again, Im not blaming React, I just can't help but notice that the brokenness started around the same time React was adopted en masse. reply tshaddox 2 hours agorootparentprevIt's true that React almost certainly wouldn't be responsible for what sounds like a blatant layout bug on a narrow smartphone viewport. React has almost no opinions on things like web page layout or touch interactions. I have to imagine the commenter invoked \"React\" as a sort of scapegoat for the prevalence of low-quality web pages. reply LordDragonfang 1 hour agorootparentprevMaybe not React itself, but the UI frameworks that draw devs to the frameworks, yes. Frameworks like that offer the promise of responsive design to developers without actually helping them understanding how it works, and as a result often fail to deliver on that promise. Splitting everything into 12 grid columns often just hides the actual work that needs to be done. Plain HTML is fully responsive by default, almost all framework components are less-so. reply pants2 3 hours agoprevThis explains exactly why physical restaurant menus are so much better vs mobile site menus. If I'm viewing the menu of a restaurant on my phone, I always look in Google Maps for someone who took a picture of the menu, because it's a dense UI. Every \"mobile friendly\" menu site is able to show maybe 5 items on the page at once, so it takes many pages of scrolling to see everything. reply zeekaran 15 minutes agoparentEven brand new restaurants do this, and it's maddening. I also go straight to Google Maps for the menu. The best restaurant websites have the straight PDF of their menu, and automatically visible from the landing page of the site. Don't make me interact with a burger menu and then several clicks later finally get the menu, grr. reply marcosdumay 2 hours agoparentprevI love how \"mobile friendly\" seems to just mean it will waste some space your phone screen can't afford. reply fbdab103 1 hour agorootparentPlus, if there is wasted space, you can jam an ad in there. reply kcb 1 hour agoparentprevYea I still don't get how little the mobile web takes advantage of pinch to zoom. reply elpakal 37 minutes agoparentprevDefinitely understand the tradeoffs here but you could set your default text size in your OS settings to be smaller to get more volume on your screen. reply Dalewyn 13 minutes agorootparentOne reason I personally prefer proper, physical menus is because they are always physically bigger than my phone or the tableside tablet if a restaurant uses those. Why the sincere fuck must I peck around on a screen for ants when I am paying money to be served? reply BugsJustFindMe 4 hours agoprevThey contort themselves to redefine the word density, when what they should have said is that a good interface for humans maximizes information without losing visual salience (http://www.scholarpedia.org/article/Visual_salience). That is, local density BUT ALSO clear-to-the-human-eye boundaries between information sets. See the famous (still? hopefully?) Kadir&Brady paper \"Saliency, Scale and Image Description\" from 2000 for an explanation of how encapsulating information in something visibly distinct, like whitespace, increases the visual saliency of that information: https://www.robots.ox.ac.uk/~timork/Saliency/ijcv_SalScale.p... reply The_Colonel 3 hours agoparentYou can also encapsulate information with e. g. frames which is what UIs used to do in the 90s. This provides both higher density and saliency. reply bqmjjx0kac 1 hour agorootparentFrames are out, whitespace is in. I wonder if the 30-year fashion cycle applies and it will be cool/retro to have compact, discoverable UIs in the 2030s. reply arjonagelhout 49 minutes agorootparentThe type of visually dense user interface is still around where it’s unavoidable due to the complexity of the data model, e.g. DAWs, game engines and photo editing software. But for simple consumer facing apps or websites, I don’t see it making a comeback, as it is more aesthetically pleasing and more usable to have simpler / sparser user interfaces for less tech savvy people. reply Spivak 2 hours agoparentprevAnd even then it's user/domain specific. What is salient for a user who's an occasional user of your software vs someone who uses your software professionally every day is very different. You stop having to tune for a visual language that's universal-ish and instead have the ability to build-out a denser set of metaphors. The \"design needs to be understandable by the person writing the check who will never use it\" problem is all over enterprise sales leading to software that doesn't need to look like $trendy_consumer_app but has to anyway to get the sale. reply balder1991 2 hours agoparentprevI think the best infographics are kinda like that indeed. reply graypegg 3 hours agoprevI really like the distinction of \"density in time\". JIRA is a really visually dense application, but it's speed, as well as the number of different screens you normally need to click on makes it feel really sparse despite the dense visuals. reply guhidalg 1 hour agoparentCall it what it is: slow. Slow software is bad software. reply swalling 53 minutes agorootparentIt's not just performance, it's also that the number of clicks to perform common actions is way too high because of feature bloat, extreme levels of customizability, and just plain bad design. reply DrScientist 1 hour agoprevSurely some of the reasons for more sparse interfaces is that on mobile: - Peoples fingers are relatively fat and inaccurate. - They are slower that desktop - so you'd break the load into parts - The vertical scroll form factor and screen size limits what you can do. - Things which are massively useful on desktop - like searching in a page or visually scanning a large doc are much harder on mobile. reply ourmandave 1 hour agoparent...relatively fat and inaccurate. A succinct summary of my high school coach's review of me. reply zozbot234 1 hour agoparentprev> Peoples fingers are relatively fat and inaccurate. They're accurate enough to tap on a single OSK key and get it right most of the time. Regardless, tap targets are only a very limited factor in UX design, so there should be plenty of scope for enhancing information density after accounting for that. reply __float 1 hour agorootparentIt's important to note though that the _actual_ touch targets for keys are influenced by what you've already written. You can miss the key from a visual perspective but still end up with the right letter as a result. reply bbminner 1 hour agoparentprev> Things which are massively useful on desktop - like searching in a page or visually scanning a large doc are much harder on mobile Interesting, am I the only one who almost always uses \"search on page\" and glimpses/scrolls over the entire page before reading both on mobile and desktop? Especially if I just came from a search engine, I search for the \"highlighted\" phrase. reply abeisgreat 4 hours agoprevThis is a really good discussion of density in different forms. I’ve always thought mobile UIs could have a density renaissance, would love to see folks questioning some assumptions of these devices - especially when the trend with LLMs is “wait a long time for a potentially incredibly wrong output” it feels like we’re going the wrong way. reply sebastiennight 4 hours agoparentWhen we first released our Chat+RAG feature, users had to wait up to 20 seconds for the response to show. (with only a loading animation). And then we fake-streamed the response (so you're still, technically, waiting 20 seconds for first token, but now you're also waiting maybe 10 additional seconds for the stream of text to be \"typed\")... And, to my enormous surprise, it felt faster to users. (Of course after several iterations, it's actually much faster now, but the effect still applies: streaming feels faster than getting results right away) reply david_allison 3 hours agoparentprevMobile apps are constrained by accessibility (touch target minimum size), so you probably won't see the density renaissance you're hoping for. reply breadwinner 3 hours agoprevHere's a particularly bad example of lack of density: https://investor.vanguard.com/investment-products/mutual-fun... reply while_true_ 2 hours agoparentAt the other end of the spectrum, I've always appreciated the density of FINVIZ: https://finviz.com/# reply KaiMagnus 2 hours agoparentprevWow, the headline is so large, I'm having trouble reading it. Then there's the sticky header, on my screen it takes up 1/5th of the available space. Or the headings, subheadings and tabs that float away (proximity principle from the blog post) and the column of text, that becomes hard to read because of small line length. It clearly looks designed, but they should take a look at this post. reply robertoandred 2 hours agoparentprevAnd a particularly buggy sticky nav there reply tiffanyh 2 hours agoparentprevFont Size vs Density There's a trend of using larger fonts sizes due to Accessibility, but it comes at the cost of reducing density. reply switch007 3 hours agoparentprevThe vanguard (UK) control panel is a hot mess too. I don't find it intuitive at all. I never know where to find important information. reply Ylpertnodi 1 hour agorootparent...it's a feature. reply marcosdumay 1 hour agoparentprevYeah, but it's beautiful. I guess that's the main goal of a page like this. IMO, that's a huge red flag. If the sales information puts beauty before functionality, that's an insurmountably amount of contempt they are showing for you before you even become a customer. On an investment company it's even worse, because contempt is less likely than they just wanting to actively select dumb customers. That's a very strong indication they'll try to steal my money (I have no idea what this one site is tough, it's not my opinion on them, it's what their design choice makes me think). But hey, it's very beautiful! reply jimbokun 1 hour agorootparentBeautiful to who? reply magicalhippo 4 hours agoprevOur \"oldschool\" Windows B2B application is quite UI dense. Without looking overly busy, we've got information that can be viewed at a glance that other web-based systems use 6+ pages to contain. I've seen users struggle to flip between many views in some SPA to figure out if things are right or not in their other system, then come to our system to correlate and looking at one or two windows they see all the same data. I guess it's just the designers, though it seems CSS and HTML lends itself very well to information-sparse pages. As we're transitioning to the web, due to customer demand, this is one aspect which I very strongly want to keep. We'll see how it goes. reply tracker1 2 hours agoparentYou can be every bit as dense in a web based application... you can make it look the same pixel perfect if you want to go that far. I've never been a fan over overly dense applications, unless they are purpose built tools. There's a big difference between PhotoShop and Grubhub. Likewise there should be differences depending on display size and UX... If you're going to have users with finger/touch input, then you don't want things too close.. if it's mostly Desktop/Laptop, you can go much more dense with less issues. Do keep accessibility in mind, some of us zoom up a couple steps on many sites. reply Terr_ 11 minutes agorootparent> You can be every bit as dense in a web based application... you can make it look the same pixel perfect if you want to go that far. That reminds me of the widgets and named-frames of: https://botoxparty.github.io/XP.css/ reply jimbokun 1 hour agorootparentprev> There's a big difference between PhotoShop and Grubhub. Others in this thread have already pointed out the massive difference in information density of a printed menu over most menus rendered on a mobile device. reply magicalhippo 1 hour agorootparentprev> I've never been a fan over overly dense applications, unless they are purpose built tools. Thing is it doesn't look super-dense. It's just space efficient let's say. Our UI components, based on Win32, makes it quite easy to have relatively dense UIs that's don't look cluttered or busy. Like I said I'm sure you can do it using HTML and CSS, it just seems not to be done often. That said it's absolutely a specialized application. At least 99% of our windows/views would make zero sense on a mobile or tablet. > Do keep accessibility in mind, some of us zoom up a couple steps on many sites. Yeah we had to manually implement font scaling, before Microsoft added it to Windows. Certainly something we will support going to the web. reply nogridbag 51 minutes agoparentprevKeep in mind if you're selling to new customers, they may find your web app \"dated\". The new customers likely won't compare the web app to your desktop app, but to other competitors web apps. And these days the expectation for better or worse is that web apps look and feel a certain way (favor whitespace). I'm sure a dense UI can still look clean and modern, but it will take more effort. reply ChrisMarshallNY 4 hours agoprevThis is a great article. Presenting information is an art form. A lot of it depends on what the information is, and also, who the information is for. One of my basic philosophies, is that the UI needs to get out of the way. This means not always using sexy little animations, everywhere (but still using them, if they also work as useful indicators of state transitions), proper contrast, minimizing overhead, like frames and controls, etc. Also, not crowding the display too much. That said, sometimes, we need a dense display, if we have been trained for it. That Bloomberg terminal is probably fine, for many folks, because they have been trained for it, and it's a daily tool. A lot of Tufte's designs need to be presented to experienced users. I remember the first time I looked at the train maps in the Shinagawa Station, in Tokyo. They were confusing AF. After just a couple of days, however, I had them down, and appreciated all that information. I tried using a fancy paid Git client, once, because it was just so pretty. After just a few minutes, though, I nuked it, wrote off the purchase, and went back to ugly old SourceTree. reply eitally 3 hours agoparentThere's also a huge difference between \"presenting information\" and \"presenting actionable information\". If a display is required to complete a task it doesn't really matter what it [aesthetically] looks like. reply jimbokun 1 hour agoparentprev> I remember the first time I looked at the train maps in the Shinagawa Station, in Tokyo. For an example like that, were they confusing because the complexity of all the train routes were inherently confusing to a newcomer, or because it was a poor visualization? reply ChrisMarshallNY 1 hour agorootparentThe former. When I get back to my desktop, I’ll see if I can scare up an image. reply ChrisMarshallNY 42 minutes agorootparentThis is what they look like: https://c8.alamy.com/comp/WAB0FP/japan-honshu-tokyo-yurakach... The numbers in the boxes are the fare (in Yen) required to get to the indicated station from where you are. reply whimsicalism 3 hours agoprevThis trend seems to be a western trend. Here is somewhere where I think we could learn from the apps in Japan and especially China. I frequently feel for any app that I use frequently, i would prefer for it to have many options that I could use to customize its behavior. For instance, Uber reply flobosg 3 hours agoparentRelated: Why Japanese Websites Look So Different – https://news.ycombinator.com/item?id=37722348 reply Nextgrid 2 hours agoparentprev> for any app that I use frequently, i would prefer for it to have many options that I could use to customize its behavior. For instance, Uber The problem is that the era of zero interest rates and (mostly unprofitable) advertising-based business models means the tech industry shifted from making tools to benefit the user to \"tools\" that waste the user's time. Company targets are often measured in \"engagement\" such as screen time, DAU/MAU or pointless metrics about how many times some button was clicked. The zero interest rate era is mostly behind us, but the mentality remains and company targets are still often based on that, so employees are not incentivized to make products more efficient for the user since doing so will reduce the DAU/MAU or whatever metric they're judged on. reply Analemma_ 3 hours agoparentprevAs someone who reads Japanese passably well and uses a handful of Japanese apps and websites, I don't actually agree with this. Japanese apps certainly look more crowded than western ones, but it's mostly with irrelevant garbage, I don't think the density of useful info is actually all that much higher. reply whimsicalism 3 hours agorootparentI mostly was thinking of chinese apps, I’m not very familiar with japanese ones but saw others mentioning it so I put it here. In Chinese apps, I can post photos, message my friends, order food, call an uber, pay transactions, all from the same app reply eddythompson80 2 hours agorootparent> In Chinese apps, I can post photos, message my friends, order food, call an uber, pay transactions, all from the same app I don't see how this statement says anything about UI information density. This is just a super app with monopoly over the market. Last I checked (and it's been a while if I'm being honest) WeChat just looked like a custom launcher for other views/apps that all happen to be hosted and controlled by the one company. That's like saying \"Android is super dense. I can post photos, message my friends, order food, call an uber, pay transactions, all from the same device\" Pretty sure Facebook or Google would love to be that super app for US/Europe/rest of the world. However, you'd probably shout \"monopoly, lock-in, anti-trust, market manipulation\" if any single vendor actually tried and succeeded in that. For good reasons too. reply digging 2 hours agorootparentprev> In Chinese apps, I can post photos, message my friends, order food, call an uber, pay transactions, all from the same app That seems tangential to UI density. You could do all that in a single app and it could still have an extremely sparse UI. reply david_allison 3 hours agorootparentprevChinese is a much more dense language than English, so you need less space for labels. reply flobosg 3 hours agoprev> Tufte's examples of graphics with a low data-ink ratio (left) and a high one (right). Isn't it the other way around? High on the left and low on the right? EDIT: Based on the alt text both images should be swapped. reply ilikescience 3 hours agoparentoh, you're totally right. good catch! i'll fix it in a minute. reply flobosg 3 hours agorootparentThank you for the write-up! I also caught a small typo: > You can form an opinion about the density of these websites simply by looking at an image for a franction of a second. reply ilikescience 2 hours agorootparentfixed that one too! thanks! reply interloxia 1 hour agoprevI like the term value density in this context. Continuing on from the Google/Yahoo example, I would be interested in the author's analysis of not just the landing page, but also the results pages. The search \"value density\" on google, bing, youtube, hn, chat.openai.com etc. are quite different these days. reply pquki4 42 minutes agoprevMissed opportunity to use hacker news as an example reply picture 1 hour agoprevLooks like the author draws a lot from Edward Tufte's The Visual Display of Quantitative Information - I would recommend it to anyone interested in design, it's a very interesting read with a lot of neat visuals. Some complain that it might be a bit heavy on map related examples, but I say that's no problem since maps have pretty much the same goals as effective UX, that being organizing and presenting information to the user accessibly. reply sebastiennight 4 hours agoprevThis article is invaluable. We've done the trick of \"short animations for delaysReticulating splines > Generating witty dialogue > Swapping time and space https://gist.github.com/meain/6440b706a97d2dd71574769517e7ed... reply bluGill 4 hours agoparentprev> while you're waiting for results, how many airlines they're comparing on your behalf. Of course as someone in computers I know that the computer can do all of the actually work faster than my screen can refresh. Even accounting for network latency, all the work is done in less than 1 second - everything else is either inefficient code, or intentional delays to make the problem seem harder than it really is. Both of them are things that anyone with computer training should object to. reply sebastiennight 3 hours agorootparentAha, yes of course I'm only talking about cases where the actual processing time is over 1 second and you can't help but make the user wait (or do something else in app in the meanwhile)... Delaying a 1 sec process to show me 10 sec of ads is one of the many definitions of evil reply bluGill 1 hour agorootparentSometimes the ads are subtle. Tax software isn't showing ads directly - they are just trying to give the idea that taxes are hard and so you should be glad to pay a lot of money for it - even though all the calculations are a few ms for any modern computer once it has the data. However if you think taxes are easy the whole industry goes away. reply BehindBlueEyes 3 hours agoparentprevAnother thing not mentioned in the article is for delays >10s, a loading bar that fills slower at the beginning and faster towards the end feels faster than one filling linearly or accurately reflecting progression. Using load times to convey something while users wait is fair however I would bet shorter loads times always beats however good a filler, unless your business is to trap users in load times to feed them more ads of course, in which case that's a whole other problem. reply zozbot234 1 hour agorootparentThat actually makes a lot of sense, because \"filling slower at the beginning\" provides a worst-case estimation of total completion time. So users are a lot less likely to be negatively surprised by a random, unexpected delay. reply rootusrootus 4 hours agoparentprevI find those sorts of fake delays to be infuriating. Depending on my mood, and where else I think I can get equivalent information, there is a pretty decent chance that about halfway through the BS progress sequence I'm going to navigate to a different site. At best it makes me think the site was designed an implemented by incompetent people. Not a great look. reply threatofrain 4 hours agoparentprevMeh. This is a prescient vision of websites that deliberately take as long as a YouTube video to load because of an ad. reply Signez 4 hours agoprevA rather disappointing read. I was expecting an analysis explaining why there is this trend towards very sparse interfaces, or practical ways of designing interfaces that are denser in the face of design trends that are pushing all product teams to do ever more spacing out. Instead, what I found was a reminder of the ‘laws of design’, which are certainly interesting, but which are only tangentially linked to this drift (in my opinion); and to take the most extreme example of sparse interfaces (the Bloomberg Terminal), without really any concrete elements that could help bring a little density back to our user interfaces. ...not to mention what ends the article, a lunar explanation along the lines of ‘Google's very high stock market valuation compared to Yahoo can be explained by the lack of density of its home page interface’ - really? Come on. reply BehindBlueEyes 3 hours agoparentAgreed. Seems like a long winded lead up to what reads to me like a mildly condescending Gestalt 101, followed with the same examples I've seen in countless other blog posts over the last 15 years and very little in terms of actually discussing design trends. reply btbuildem 2 hours agoprevA good read; it captures a lot of the key points. I'd like to mention consistency -- especially for complex UIs. An expert user who has taken the time to learn the ins and outs of menus and keyboard shortcuts will RESENT you for making what seem like (and often are) superfluous changes to their workflows. reply hgyjnbdet 1 hour agoprevSo is dense information good or bad? I can't work out what the conclusion is from the article, is it a case of context? I don't understand :( reply dugmartin 2 hours agoprevI'm dealing with this now on an accounting app I'm building that runs on both mobile and desktop. I've come to the conclusion that the mobile app and desktop app will need two very different designs. I want the mobile app to be useful for quickly checking a dashboard view and to easily enter transactions while the desktop app needs to be very dense with the choice of a compact view to reduce padding ala Gmail. reply croes 3 hours agoprevI hate it if I need to scroll horizontally just because the content only occupies a third of the page in the middle with great white spaces to the left and to the right of it. I have a wide display for a reason. reply tavavex 1 hour agoparentJust because you have a wide display doesn't mean that all of it can or should be used. For example, lines of text not taking up the whole width is a good thing - if you have to turn your head to read across the screen, it's more straining and takes more time. It's the same reason why newspapers are printed in columns and not across the page. Now, depending on what kind of content you deliver, that empty edge space can be filled with something else (like what Wikipedia does) or just be blank if there's nothing useful you can put there. reply filleduchaos 1 hour agorootparent> It's the same reason why newspapers are printed in columns and not across the page. In fact, arguably the point of a wide display is to create your own columns i.e. putting more windows on the screen, or more panels within a single window in an application like a code editor. What's the point of the real estate if you just put one browser window on there? I only go fullscreen to watch movies or play games, which is somewhat analogous to full-page media spreads in newspapers. reply calfuris 1 hour agorootparentprevIn general, I agree, but there are some things that should be allowed to go wider when the space is present. In particular, horizontal scrolling is evil. Sometimes it is a necessary evil, but forcing it because the wide content doesn't fit in the width that you are using for reflowable text is bad. reply croes 35 minutes agorootparentprevIf I have to horizontally scroll forth and back line by line to read the whole text I'm a lot slower. reply jrd259 3 hours agoprevYou might also want to discuss Fitt’s law. Difference between a diagram (Tufte) and a GUI is that we only look at a diagram, but a GUI we interact with, which means we need to ensure people can actually click/tap/select the element of interest. Higher density makes that harder. reply tiffanyh 4 hours agoprevSlightly OT: seeing those screenshots of Bloomberg Terminal makes me wonder why they haven't picked a better (more readable) monospace font. reply frutiger 2 hours agoparentThe Bloomberg Terminal has evolved from an 80x25 hardware terminal where the application layout was done remotely in our data centers. (Note, users and insiders refer to these as \"functions\" instead of \"apps\", but I'll use \"app\" throughout as it's likely more familiar to the HN audience). When we ported it to a Win32/GDI program, the client was kept intentionally \"dumb\" and so resizing the window led to distorted text rendering. This was necessary to keep the 80x25 cells aligned, as the layout was meaningful. Fast forward to the 2010s and we started offering our app developers a way to implement \"more content\" when resizing rather than just stretching it. Note also that there are plenty of \"form\"-style or \"calculator\"-style apps where there is no more content to show; in those cases resizing just adds more negative space around the UI. Now we are in the 2020s, and most of the apps have adapted to either \"more content\" or \"more negative space\" as needed. There are ~1000 apps on the terminal and they all have their own roadmaps and business deliverables, so UI upkeep cannot always be a priority. Opinions my own, etc. EDIT: the above information is still correct, even if thetag in the OP article is distorted. reply tiffanyh 1 hour agorootparentSuper informative, thanks. Question: you response implies that changing the font would break things (maybe I'm reading too much into your reply). Since monospace fonts are fixed width, wouldn't swapping out 1 monospace font for another monospace font (that's more readable) be seamless? reply frutiger 13 minutes agorootparent> Since monospace fonts are fixed width, wouldn't swapping out 1 monospace font for another monospace font (that's more readable) be seamless? Not sure I exactly understand this suggestion - but if I do, wouldn't it require a different monospace font for each possible configuration of the window size? The \"normal\" window size was set such that each character cell was 9x19 pixels; this makes the window 720x475 (ignoring window borders added by the OS). If the user resizes it to, e.g. 721x475, there is no specific font that can be added to substitute; instead that extra vertical line needs to be inserted somewhere. reply tiffanyh 4 minutes agorootparentWhat I mean is, there's plenty of monospace fonts that can fit within a 9x19 pixel grid. And since the 9x19 size doesn't change, shouldn't you be able to substitute any 9x19 monospace font with another equally sized. (The 9x19 grid doesn't change, which mean the UI wouldn't change, but you could use a monospace font that has more readable letter forms than what's currently in use) reply cptcobalt 3 hours agoparentprevTheir users would revolt, it's a point of pride for B-unit holders to show off how well they can use seemingly inscrutable UIs. reply thrdbndndn 3 hours agoparentprevI'm not even sure if it's just how that font/typeface looks like, or the whole image is stretched horizontally. reply tiffanyh 3 hours agorootparentBased on these two video demos, the font seems to be distorted (normally) https://www.youtube.com/watch?v=2ee-x6IXWK8 https://www.youtube.com/watch?v=BqgqwO11abk&list=PLmUXemGpYY... reply mrob 3 hours agorootparentprevAgreed. Squash the image back to 4:3 aspect ratio and it looks completely normal. reply DeathArrow 1 hour agoprevHigh or low density, what matters to me most is how easy is for me to gather all the information I need and how fast I can act on it. reply jrd259 3 hours agoprevThe example of low and high data-ink (from Tufte) is switched. I wrote the author to suggest it be fixed reply Gbotex 1 hour agoprevtheres so much i dont know reply stevage 3 hours agoprev>The UI was much less visually dense, but more value-dense by orders of magnitude. The results speak for themselves: Google went from a $23B valuation in 2004 to being worth over $2T today — closing in on a 100x increase. Yahoo went from being worth $125B in 2000 to being sold for $4.8B — less than 3% of its peak value. I liked the rest of the article until this nonsense statement. reply burntalmonds 3 hours agoprevThat Yahoo screenshot really took me back. reply ivanjermakov 2 hours agoprev> Actions less than 100 milliseconds apart will feel simultaneous This is not true for some things and people. I would not call those simultaneous, rather bearable. Most users wouldn't mind it. Input delay though is very noticeable. If keyboard or mouse have 100ms delay the user might consider that their device is doing something heavy. And people who got used to fast software, e.g. optimized code editors or games, are even harder to please. reply AlienRobot 1 hour agoprevI love dense UIs, but in my experience they're very hard to design. You can easily make an OK design if you add a lot of space and padding, but to make a dense UI look good you need custom borders and textures, and you need a permanent vision so you can add more widgets later that won't look weird. It's sadly not a paradigm that works in many scenarios. reply kazinator 2 hours agoprevuser-density x ui-density = constant reply kstenerud 3 hours agoprev> The UI was much less visually dense, but more value-dense by orders of magnitude. The results speak for themselves: Google went from a $23B valuation in 2004 to being worth over $2T today — closing in on a 100x increase. Yahoo went from being worth $125B in 2000 to being sold for $4.8B — less than 3% of its peak value. Wait what??? THAT'S how you explain the differences in how their businesses fared - by the density of their UI? reply eviks 3 hours agoparentIf all you've got is a high density hammer, everything looks like a low density nail reply stevage 3 hours agoparentprevYeah it's such a dumb way to end the article. Google in 2004 was a search company. Google in 2024 is an advertising company. reply tavavex 1 hour agorootparentGoogle in 2004 was also an advertising company - it's not like there was a point where you paid for searching directly. 2024 Google is more of an everything company that's primarily funded by advertising - they tap into search and every kind of content imaginable, make hardware, do research. It's about the number of services - in 2004, Google had barely launched Gmail yet. reply divbzero 4 hours agoprevIn addition to varying over time, UI density also varies across cultures. Currently, East Asian websites tend to have higher UI density than Western websites. See, for example, Rakuten’s home page in Japan vs. its home page in the US: https://www.rakuten.co.jp/ https://www.rakuten.com/ reply nolongerthere 4 hours agoprev> There’s an upper limit to information density, which means you can subtract too much ink, or add too much information. The audience matters, too: A bond trader at their 4-monitor desk will have a pretty high threshold; a 2nd grader reading a textbook will have a low one. I think this is the most important line: when taken with the axiom “design for your lowest common denominator” and the general advice given to lawyers in a jury trial “speak, explain at a 3rd grade level” The upper limit for information density has lowered significantly for the vast majority of general users, so unless you can fix that we’re not gonna get our high density UIs back. At least not for general purpose widely distributed applications. reply gonzo41 3 hours agoprevThis is a good article. I always try and articulate these point by talking about how useful and quick and dense phone books where when they existed. And as for fast load times. Might I say that server side rendering may just have a reason to exist again! reply aurareturn 4 hours agoprevI have 1 rule: If you're building an app that people use for work and open every day, you should make it dense. People want to get work done, fast. They don't care about how pretty it is. Otherwise, you should make it sparse. reply Ensorceled 3 hours agoparentMy last gig I built a campaign management interface. The designer kept coming back with something that showed 10 lines per \"screen\". I kept showing them the excel spreadsheet we were replacing that had been configured with 8pt fonts so it showed dozens per screen. It's really hard to switch design patterns from consumer/prosumer to professional. Knowledge workers who \"live\" in the tool want as much data as possible, as quickly as possible. reply stevage 3 hours agorootparentYep. I really can't stand tools made for professionals that paginate lists to 10 items at once. Even my invoicing software does this. I have 200 invoices in the system and can only see 10 at once? Maddening. reply skydhash 2 hours agorootparentThese days, only torrent sites (private) seem to have good UIs. reply eitally 3 hours agoparentprevI'm 100% with you. I ran an Enterprise Apps org within corporate IT for about 15 years and the #1 rule is that the tools must be functional. Unfortunately, this frequently results in technical debt because \"functional\" often means \"complies with whatever business logic the current sponsoring org (Finance, HR, Supply Chain/Procurement, Ops, etc) wants that week, and the result of that is that many internal enterprise apps get wholesale rewrites every 4-5 years because it's easier than refactoring. This set of phenomena is completely foreign to SWEs and PMs who have only ever worked in big tech, on consumer products especially, and the reality is that while some engineering teams in some companies are sometimes doing hard and creative work, the majority of big tech SWEs \"moving protobufs\" is much easier and less complicated than the kind of crap faced by enterprise IT. reply ARandumGuy 3 hours agoparentprevThis is the reason I don't envy the UI designers on products like MS Office. There are professionals who use products in the MS Office suite every day, but there are other users who only need to create the occasional spreadsheet or powerpoint. Even if you think Microsoft has done a poor job with the UI, one can't deny that reconciling the differences between casual and power users is a difficult job. reply smegger001 3 hours agorootparentThis is why Microsoft used to have multiple office suites, there was microsoft office which was everything and the kitchen sink, then you had Microsoft Works which was a stripped down simpler office suite, then you had wordpad for when you need something more than note pad but not a full office suite. reply skydhash 2 hours agorootparentWordpad was good. I don’t remember the full feature suite, but Microsoft could have expanded on it to have something like Google Docs, but native. reply moi2388 3 hours agorootparentprevThey could just have a “quick edit” mode, a “read” mode and a “power user mode”. But that means not having your intern pump out that single electron design so it’ll never happen reply ARandumGuy 2 hours agorootparentI understand why that seems like an appealing solution, but there are some pitfalls with creating multiple distinct UIs: The most obvious pitfall is that more UIs = more work. There's the initial development, but it also increases the work every time you need to add something to the UI, and increases the likelihood of bugs. Microsoft could afford this increased cost, but it's not clear if it would be worth it. Secondly, having multiple distinct UIs makes it difficult for someone to transition from a \"casual user\" to a \"power user\". If someone who has only used the \"simple\" UI switches to the \"power\" UI, they have to re-learn everything from scratch. Additionally, if there are any features limited to the \"power\" UI, it's extremely difficult for a \"simple\" UI user to discover those features even exist. That doesn't mean that creating multiple UIs isn't ultimately the right solution for MS Office. It might be! But doing that comes with downsides, and I can understand why Microsoft doesn't want to go in that direction. reply enriquto 3 hours agorootparentprev> They could just have a “quick edit” mode, a “read” mode and a “power user mode”. Would the density of these interfaces increase or decrease? An actual power-user would master all the shortcuts and would prefer a zen experience where only the document is visible, with no widgets whatsoever. reply djeastm 3 hours agorootparentOnly if they're keyboard-focused. There are power-users who still prefer mouse. reply maxerickson 3 hours agorootparentprevThey sort of do. Hide the ribbon and most users still have all the features they need. reply carimura 3 hours agorootparentprevThe answer is to just put in multiple levels of menus with seemingly-random redundancy! oh wait, they did that. reply skydhash 4 hours agoparentprevDense every time! Youtube and Twitter’s UI’s are so sparse they display the same amount of information on my phone (6.1”) and my desktop (24”). And they waste vertical space like their designer only use square/vertical monitors. I’m clicking with a mouse, I’m not touching the screen with a brick. reply delichon 4 hours agoparentprevTwo basic strategies: Design for the median user or the least common denominator. At web scale we are often driven to least common denominator out of self defense, because lower power users squawk more about confusion than median power users do about efficiency. reply Lutzb 3 hours agorootparentI think about this quite often when creating new applications. Four hypotheses why standard users are often the primary target of design: (1) Power Users are louder but mostly ignored due statistically being not relevant. (2) During design sessions the teams empathize with a standard user, not the power user. I've seen this pattern over and over again. (3) Current web technology makes it difficult to build high density UIs that work well. (4) Mobile UI first. If it works on mobile, we can just use the same UI for the web. All this leads to another problem: Your standard users never become high power users on your platform. In the end platforms become interchangeable. reply Nextgrid 2 hours agorootparent> Your standard users never become high power users on your platform This is considered good, because if they become power users and get their stuff done faster they'll \"engage\" less and we can't have that. Remember that today's career incentives in tech companies means tech is primarily there to drive \"engagement\" and is not there to solve the user's problem. reply rocqua 3 hours agorootparentprevI think its all about new user acquisition. If your design is very dense, then a new user will be scared away. If you NEED growth, you can't afford to scare away users. You need to cater to them as much as possible. You need them to tell their friends \"yes its very easy to get started with\". It leaves no room for a learning curve. And this is also something we have come to expect. So anything with a learning curve feels like a massive investment, and we feel dumb whilst using it and not knowing how, because everything else is so dumbed down that it is instantly intuitive. This has made computers and software very popular and widely used. And I am happy for my (grand) parents that they can use these things now. But it has come at a great cost of productivity for everyone. Because even the designs where we would have invested the learning time to get faster, can't invest that time, because the interface is so simple it becomes limiting. reply Jerrrrry 3 hours agorootparentprev>Design for the median user or the least common denominator. Design for the least, compare against average, and don't cap the best, use those agents as immediate beta-user -> quality/dev feedbook loop, dog-fooding user requirements as a fundamental base, and don't let perfection impede progress. reply duxup 4 hours agoparentprevGenerally that's my experience as well. Business apps that my customers want, crazy dense. Some of these I look at and I'm \"man this is a lot of stuff\" but ... then you get used to it and it works. reply jamil7 3 hours agoparentprevYeah, some tools are built for complex, niche workflows and as a result need to have information and control density. You can't design away the learning curve in these cases and probably shouldn't try to or you'll harm the pro-end of the userbase's UX, meaning someone can't do their job quickly/efficiently enough. reply marginalia_nu 4 hours agoprevAlmost everything wrong with modern web UIs is arguably down to the contradiction of designing for both web and mobile at the same time. These two paradigms are not compatible. If you try to build a responsive UI that caters to both, you'll throw one or more of the groups under the bus. If you want to do both paradigms justice, you need two separate designs altogether. Low density UIs specifically happen when you attempt to present the amount of information a 7\" screen can display onto a 27\" screen. reply mrweasel 4 hours agoparentMore and more I find myself wondering about the use cases companies imagine when developing websites and apps. My bank is trying to implement a new UI for their online banking and they are trying to mimic the UI of their mobile app. The issue I have is that the mobile app is already so dumbed down that it's pointless and I don't use it. When you have limited screen space you need to decide what to show first. For my bank it's the status of a MasterCard credit card I don't own on the first screen of the mobile app. Next is some pointless overview of my accounts. I say pointless, because all context has been removed, in favor of massive amounts of white space. Now they want to replicate this interface, but for larger screens. Most of the screen is white space and you have to click on everything to get details, details that would fit perfectly well, even on a small laptop screen. Also nothing is obviously click-able, because why would you add visual clues that just taints their beautiful white space. For the most part I think that companies would love to ignore desktops and large screens. In some sense they may also be afraid of presenting users with details overviews, either is to make everything seem more friendly, or to discourage usage. reply makeitdouble 3 hours agorootparentThe time where an application is perfectly tailored to the machine you're using and everything is near perfect happened, but I think that ship has sailed. That was the brief moment when the iPhone was the only smartphone to target and was 320x640. Or when the majority computer screens where 1080p at most and the browser would be on more than half the screen estate. But for your bank for instance, if their UI is optimized for a 6\" diagonal window, they'd probably expect you to adjust for that instead of them trying to be perfect on every screen combination that could happen on earth. That's also how I see many support chat apps' choices of spawning a popup when running on a desktop, to reset any browser size the user was trying to use in the first place (users are still free to do whatever they want with the popup, but it's a good indication of what size it's supposed to be) reply bux93 3 hours agorootparentprevAre they trying to mimic the mobile app, or is the web-based online banking app just the mobile app with a slightly different skin? For my bank, it's the latter. reply Normal_gaussian 2 hours agorootparentto expand on this for anyone that isn't aware; most front-end teams are rapidly converging on using systems like React-Native / Flutter / Ionic; which allow you to \"write once, deploy anywhere\" e.g. mobile web, mobile app, desktop web, embedded / PoS, and sometimes tablet and watch. This is possible by abstracting the layout engine (generally to match a web browser) and having UI control running in transpiled JavaScript. In more practical terms, this means its easier to have one team managing your website and mobile app, reducing the number of specialist roles and minimising feature disparity. Generally it saves on duplicate work as well and makes timelines easier to manage. For banks specifically there are a bunch of wins with respect to only having to ensure legal compliance of one application; this applies to banking laws - where certain information must be communicated at certain points, and to public accessibility laws. Most of these frameworks have a lot of tooling around language support and accessibility integrations. So even if these experiences are inferior, it is very much worth it to the providers. reply ghnws 3 hours agorootparentprevFor the vast majority of people, the mobile bank app is far more useful than a desktop oriented web bank. Many (maybe most by now) don't even own a computer. reply mrweasel 3 hours agorootparentI do think you're right. I know a number of people that doesn't own computers anymore and even more who are just doing everything on their phone. One interesting usage I've seen is especially younger people, who have a debit card which doesn't allow an overdraft. They then keep their account at or around zero and only transfer the amount they need to the card account before every purchase. That usage is supported way better than my attempts at managing saving, paying large bills or keeping track subscriptions and other spending for the past week. reply carlosjobim 2 hours agorootparentFrom where do they transfer the money if their account is around zero? reply mrweasel 2 hours agorootparentOne of their other accounts. I don't know how it works else where, but it's not uncommon to have two, three, four or more. I think my dad at one point used 10 to segment his finances. Accounts normally don't cost anything, or very little. Most people have at least two. One of their incoming paycheck and one for their debit card. Most have more. reply magicalhippo 2 hours agorootparentprevI have a non-card account I get my salary deposited into. I also have a separate savings account, and one for my cottage, both which get scheduled transfers from my \"main\" account. When my debit account is low I transfer $100 or so. Allows me to have a sense of spending that cash gave me. reply marginalia_nu 3 hours agorootparentprevWhy do we need one or the other? There's an accessibility element here as well. It's not possible for everyone to use a touch screen. reply safety1st 3 hours agoparentprevI prefer to think that responsive design is still in its relative infancy and as an industry we simply have not completed the shift in mindset that is necessary. When we work stakeholders who are not technical it's clear that they're 1,000 miles away from the mindset, and the average designer is still somewhere from 10-999 miles away. Fluid design is a big step forward and while we still don't have anything near a bible these are some example principles: * Size elements relative to the size of the viewport, some things should be smaller on little viewports, some things should be a little bigger, the CSS units for this are still just coming together. Use formulas and percentages, not pixels. * Think about the correct layout direction for everything on a portrait vs landscape orientation, should those items be in a row or a column? This often requires only one CSS property to change * Hide information density behind an accordion or modal if you need to on small viewports, don't just remove the functionality for all versions of the application. When people strip away functionality, I find design is rarely the true justification -- more commonly they no longer want to pay the maintenance cost of the feature and are using a redesign as an excuse. reply throwup238 2 hours agorootparent> I prefer to think that responsive design is still in its relative infancy and as an industry we simply have not completed the shift in mindset that is necessary. Absolutely. We finally just got container queries! Responsive design using screen width was always a sham. A stop gap to all the work the browser devs and standards bodies had to do to figure out what worked. reply whstl 2 hours agorootparentYou are correct, but I think we're lacking in things even more basic than that. Pretty much every single responsive design I ever worked had either the mobile or desktop version as an afterthought. The mobile was just a \"scaled down\" version that was done last-minute, or vice versa. Is there a sidebar? Hamburger menu it is. Is there a list? Well just put every row on top of each other. Sure, this makes things easier for the both the designer and the developers, but putting just a bit more effort in the \"secondary\" version would be enough to make both versions better. reply troupo 2 hours agorootparentprev> A stop gap to all the work the browser devs and standards bodies had to do to figure out what worked. We now exactly what works. UI didn't suddenly appear in 2024 out of nowhere. What you need are actual tools to build UIs, and not a hodge-podge of hacks thrown in together with no long-term planning, and aimed at displaying only text and a couple of images. Just giving the ability to get an element's size without causing a full-page re-flow and re-layout would give much more power to UIs than any number of relative sizes and container queries. reply LarryDarrell 3 hours agoparentprevI have a 38\" curved wide screen with 1600 vertical pixels. It represents the peak of all the monitors I have bought over the last 25 years. Me in the year 2000 with a 1024x768 17\" CRT would be flabbergasted at the amount of wasted/underutilized space that exists today. Vertical space is precious, and it's why I paid more to have those 1600 pixels. And then Microsoft decides to not only enlarge the taskbar, but to not provide an option to turn it back to normal. I have to resort to hacks to wrangle MS Windows (primarily a Desktop OS the last time I looked) back down to size and reclaim my vertical space. reply rerdavies 2 hours agorootparentYou need to rotate your display to portrait mode. ;-P Serious point: a secondary 1080P display in portrait orientation is actually quite fabulous. Documentation gets parked on the secondary display. Line lengths remain readable, and you get lots of vertical space. reply TheRealPomax 3 hours agorootparentprevAlthough to be fair, \"having to hack Windows to get the UI to be what you want\" has been a staple of being a Windows power user pretty much since XP. reply PaulHoule 3 hours agoparentprevLow density UIs were a scourge before mobile. There has always been a kind of designer who privileges \"negative space\" over everything else and can't imagine any greater luxury than having a 27\" screen and devoting 26\" of it to whitespace. reply JoeAltmaier 3 hours agorootparentFamously derided in the seminal work \"The visual display of quantitative information\". There they give the thought experiment: weigh the ink on your page devoted to data points, compare it to the entire rest of the ink. You want the largest ratio possible. reply rerdavies 2 hours agorootparentWouldn't a low-density UI yield an infinitely large ratio? Not Much Ink / Nothing else. reply ryandrake 2 hours agorootparentprevI think about this every time I browse a web site, and the text is this tiny, 5\" wide strip down the center, leaving 10\" of empty, useless whitespace on both sides. Thanks, designer. I'm glad I bought a 27\" monitor for this shit. reply technojunkie 2 hours agoparentprevExcept that studies show that users expect both similar content and experiences regardless of device. Responsive web design, with progressive enhancement, is the foundational bridge to make this experience work well. Just because someone experiences a less than ideal experience of RWD in one or more places doesn't mean to group all RWD UI experiences together as bad. It's more practical spend less time developing and testing one global component than more than one. reply agumonkey 2 hours agorootparentI think there's a \"theory\" of rotating UX (same features in one way when in one space, another way in another space). But I can't find writings on this. reply technojunkie 2 hours agorootparentBrowser spec creators and browser vendors have been implementing touch and non-touch UI differences in common primitive features since the 2010s. Example, if you use at aelement on desktop browsers and mobile browsers, I would say it falls into a similar idea as rotating UX. There are countless other examples, too. Could there be improvements? Absolutely, and that's where we get to have a voice. reply agumonkey 1 hour agorootparentThat's at the widget level, fair point though, but I was thinking about larger components. reply balder1991 4 hours agoparentprevWhat happens nowadays is that the big screens suffer and get a design made for mobile phones, with giant buttons and a lot of empty space. reply pupppet 4 hours agoparentprevSomehow mobile-first became mobile-only. reply malfist 3 hours agorootparentMy large company just launched a project to encourage previous employees to come back and apply to jobs again. Business decided it would be mobile only, web support is 2 years away at least. There's been a lot of issues because, A, nobody wants to install their previous employeer's spyware app, B, nobody wants to do job applications on a tiny screen, and C, a lot of the step up authentication requires the web browser, but is explicitly disabled for these folks meaning lots of them can't complete login. It's completely bonkers. The worst part of it all is the mobile app requires authentication for everything, including resolving tracking links and shortened urls. These people HAVE to go to the webpage to sign up and get the mobile app, but then can never visit the webpage again. reply makeitdouble 3 hours agoparentprevYou have a good point, though I'd say at the heart of it the issue is that design is just complex and there's no one true way to do it. You point out web vs mobile, but of course as you probably use \"web\" as a shortcut for \"PC\", web also applies to mobile. Then on a 27\" screen you might have one window full screen or 20 windows overlapping and an actual 7\" browser to display the site. And others will be on PC, but with a 13\" touch screen. And others on a 10\" phone but split in half. Then some people will increase text size and your design will need to deal with it. Before you realize it you have dozens of constraints and requirements to think about, start dropping some to satisfy others, and inevitably people will be pissed at the result. reply marginalia_nu 3 hours agorootparentThe biggest problem isn't sizing or font sizes, most designs stretch and scale and deal fairly well with that like even if you kick it old-school and use tables. The big problem is catering to different input methods. A mouse has far greater accuracy than a touch display. You can put two links mere pixels apart on a desktop interface and that is fine because a mouse is more than accurate enough. The smallest interactive element a keyboard-and-mouse user can hit is probably about the size of a single period in a font. There are other issues with doing that, I'm trying to highlight the sort of accuracy you have with a mouse. A mobile user could never hit such a small target. On PC, you can enter text and show the full content of the website at the same time. You can search in the page with a keypress. You can open multiple web pages at the same time. That is not possible on mobile. reply makeitdouble 2 hours agorootparentYes, although clicking links on a page with mere pixels between them is still a PITA for many users. I was reminded of this using the super cheap mouse and acquaintance was using on a dinning table, and getting precision was possible but really frustrating. And they didn't like trackpads. As more and more countries have aging population I'd expect these kind of accessibility issues to be more prominent. Sometimes I feel like using modes (vim style) could help, with the user getting different tradeoffs when \"reading\" and \"manipulating\", if there was an easy enough way to switch between one mode and the other. reply skydhash 3 hours agorootparentprevAnd miraculously base html/css already solved that. There’s a reason desktop UI don’t work on small screens and that’s the same reason you don’t do magazine layout on small books. Html is reflowable because that’s the only way it can work across different screen sizes. But designers like to think their canvas sizes is the norm and do layouts that shouldn’t be done. reply makeitdouble 1 hour agorootparentI'm with you on the size issues, though I don't see html/css as solving it completely. At the end of the day people want magazine layouts, newspaper splash styles, postcard type areas etc. I think even novel writers/editors have a \"best viewed at\" size and layout in mind that gives a perfect pace to their story. Html/css gives the tool to switch layouts and potentially adjust to make the best of the area offered, but it will probably always be a compromise in the eyes of the more opiniated designers, and auto-reflowing content would be more of a necessary evil. Even in our field we have traces of that with our recommended line length, method length, bracket styles etc. reply foobiekr 2 hours agoparentprevI think it’s actually that designers as a group feel like they should not have to understand the product they are designing. This is why the modern uis are so minimal and lacking in features for people who actually use the product in a way that is deeper than trivial. Imagine a current era designer trying to design Photoshop without just copying an existing system. It would be useless. reply agumonkey 2 hours agoparentprevEven the web was already taking a bad direction, too much interactive magazine instead of function/tool paradigm. reply gedy 4 hours agoparentprevI actually support responsive UIs that work for small and large screen sizes - however, in past ~10 years this has been in vogue, I've rarely seen UX, product, or devs who really \"get it\". I.e. columns that wrap or resize, buttons that have labels and icons on desktop, etc. reply antisthenes 3 hours agoparentprev> If you want to do both paradigms justice, you need two separate designs altogether. How do you reconcile this with the fact that you need almost double the effort for 2 separate designs? I'd say you need at least 2x of Designer work and at least 1.5x of front-end Dev work to achieve this. Where will additional resources to support this come from? reply solardev 3 hours agorootparentYou're meeting your users where they're at. If half your visitors are using mobile and half are on desktop, then having two designs isn't 2x the work, it IS the work. Having only one means you're abandoning half your visitors and not doing your job fully. And having both is not really 1.5x to 2x the work, in my experience. Maybe more like 25% to 30%. A lot of components and widgets can be reused, the typography and colors can be similar, and certain screens can keep their layout with just small tweaks. Sure, maybe the initial design takes a bit longer since each screen needs several breakpoints. But that's only a small part of the overall design work anyway. Then on an ongoing basis, your previously established patterns (and components in code) can largely be reused with small tweaks, easily done with modern toolkits like Tailwind or MUI. I don't think it's that big a deal. Web devs have been doing mobile designs for more than a decade now, and the tools have gotten better and better. Honestly, it's way less wasteful than Agile ceremonies or endless meetings. If you want to stay lean and cut cruft, take it from places that don't directly affect the user, not the one place where they actually use your product all day. reply mostlysimilar 3 hours agorootparentprevMaybe the execs get paid less, what about that. reply nottorp 3 hours agorootparentprev> I'd say you need at least 2x of Designer work and at least 1.5x of front-end Dev work to achieve this. That's what you tell your customers at least :) reply marginalia_nu 3 hours agorootparentprevDesign becomes quite a lot easier when it doesn't need to cater to multiple platforms and input paradigms. Testing is easier, there's much less need to make concessions and tradeoffs. If anything, having two designs that look and function well is less work than having one design that looks and functions well on two disparate platforms with completely different affordances. reply carlosjobim 4 hours agoprevSet your browser to open every page in reader view by default and you never have to mind this crap again. Nor ads, nor cookie banners, nor auto-playing video, nor AI chatbot, nor newsletter popups. Hint: You can also force reader mode by pressing CMD+Shift+R reply mrob 3 hours agoprev [–] 100ms is much too long to feel instantaneous. Open a low latency terminal such as xterm (ideally also with a high refresh rate gaming monitor and gaming keyboard), and compare \"sleep 0\" with \"sleep 0.05\". reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Modern websites and applications in 2024 are less visually dense compared to those from the 2000s, meaning they appear more spread out.",
      "UI (User Interface) density involves not just the visual appearance but also the amount of information conveyed over time and how design decisions enhance the software's value.",
      "Examples of visually dense interfaces include Bloomberg’s Terminal, which displays extensive market data, and Craigslist, known for its numerous plain links and simple search features."
    ],
    "commentSummary": [
      "The article addresses the challenges of designing user interfaces (UIs) for various screen densities, particularly with technologies like React, and highlights issues such as poor scaling and scrolling on smaller screens.",
      "It critiques the misuse of frameworks like React, emphasizing the need for better design practices to ensure accessibility and usability across all devices, and discusses the balance between dense information and user comprehension.",
      "The discussion contrasts Western and Asian app designs, critiques unethical UI practices, and stresses the importance of balancing functionality and readability, while also exploring the challenges of responsive web design (RWD) and its impact on user experience."
    ],
    "points": 344,
    "commentCount": 181,
    "retryCount": 0,
    "time": 1716298899
  },
  {
    "id": 40424304,
    "title": "Edward Burtynsky's \"Shipbreaking\" Captures Haunting Beauty in Industrial Recycling",
    "originLink": "https://www.edwardburtynsky.com/projects/photographs/shipbreaking",
    "originBody": "PROJECTS Photographs Books Films The Anthropocene Project In the Wake of Progress Public Art NEWS EVENTS ABOUT Statement Biography CV TIW AVARA CONTACT Burtynsky Studio Gallery Representation SHOP TIW SUBSCRIBE PROJECTS Photographs Books Films The Anthropocene Project In the Wake of Progress Public Art NEWS EVENTS ABOUT Statement Biography CV TIW AVARA CONTACT Burtynsky Studio Gallery Representation SHOP TIW SUBSCRIBE Shipbreaking Urban Mines > < China SHIPBREAKING Artist's Statement “The original idea for the shipbreaking started a long time ago. About four years after the Exxon Valdez oil spill I heard a radio program where they were talking about the danger of single-hulled ships. The insurance companies were refusing to cover them after 2004, which would force all these ships to be decommissioned. Only double-hulled ships would be allowed on the open sea to prevent that kind of catastrophe from happening again. What went off in my mind was, wouldn’t it be interesting to see where these massive vessels will be taken apart. It would be a study of humanity and the skill it takes to dismantle these things. I looked upon the shipbreaking as the ultimate in recycling, in this case of the largest vessels ever made. It turned out that most of the dismantling was happening in India and Bangladesh so that's where I went.” — Edward Burtynsky View fullsize View fullsize View fullsize View fullsize View fullsize View fullsize View fullsize View fullsize View fullsize View fullsize View fullsize View fullsize View fullsize View fullsize Burtynsky's Shipbreaking photographs, like all his works, appear to us as images of the end of time. The abandoned mines and quarries, the piles of discarded tires, the endless fields of oil derricks, and the huge monoliths of retired tankers show how our attempts at industrial \"progress\" often leave a residue of destruction. Nevertheless there is something uncannily beautiful and breathtaking in the very expansiveness of these images―it is as if the vastness of their perspective somehow opens onto the longer view of things. For Burtynsky, nature itself, over time, can reclaim even the most ambitious of human incursions into the land. As long as human needs and desires change, so too will the landscape. © COPYRIGHT INFO",
    "commentLink": "https://news.ycombinator.com/item?id=40424304",
    "commentBody": "Shipbreaking (edwardburtynsky.com)298 points by thunderbong 14 hours agohidepastfavorite135 comments tathagatadg 7 hours agoThis takes me to my childhood. My dad was an electrical engineer in India and worked at a ship repair dockyard. He once came back with a few shelves and cabinets for our kitchen. These were taken out of ships that were getting cut up in their dockyard. They were complete mismatches in aesthetics but it did not matter to his \"why waste such functional ...\" attitude. He was excited about what his \"workers\" could salvage from the ship. Mom didn't care as this was an old house where we have been living for generations and functionality trumped aesthetics. The most intriguing part to me was the wooden cabinet was painted white with something stencil printed in green. My best guess was that was a Cyrillic script, and about twenty five years back, it wasn't easy to decipher what they meant. Those cabinets are still hanging in our old house. Next time I'm there, all I need to do is pull up my phone and translate that text and get a kick out of what the original intention was for the sailors and what my mom is storing in it! reply Waterluvian 6 hours agoparentI can only imagine the conditions his \"workers\" worked in back then given how poor the conditions still are today. I think scenes from ship breaking, electronics recycling, and other recycling efforts sent overseas could fit in, as-is, to a film like Blade Runner. reply boringg 5 hours agorootparentThose industries don't pay well to anyone involved but are a global benefit. Such a strange world we live in. reply paxys 1 hour agorootparent\"Paying well\" is relative. There are lots of industries that are rightly considered exploitative from the western eye, but people who are working these jobs would otherwise be starving or relying on meager government handouts if they went away. reply teachrdan 58 minutes agorootparentThis is plainly true. But there is a middle ground between \"the least amount of money workers will accept before choosing to starve instead\" and \"so much money it's no longer economical to pay workers to do.\" I have no idea what the case was in this specific industry in India. But in many developing countries, first world companies collaborate with government and pay off private muscle to make it impossible for workers to organize and earn anything in that middle ground. (I do not mean to imply that you deny this possibility. But there are many on HN who uncritically believe that if workers take a job, it is therefore a fair wage taken voluntarily.) reply FireBeyond 20 minutes agorootparent> (I do not mean to imply that you deny this possibility. But there are many on HN who uncritically believe that if workers take a job, it is therefore a fair wage taken voluntarily.) Reminds me of the Libertarian Police Department (https://www.newyorker.com/humor/daily-shouts/l-p-d-libertari...): > \"Do we have any leads?” > “Not yet. But mark my words: we’re going to figure out who did this and we’re going to take them down … provided someone pays us a fair market rate to do so.” > “Easy, chief,” I said. “-Any- rate the market offers is, by definition, fair.” > He laughed. “That’s why you’re the best I got, Lisowski. Now you get out there and find those bitcoins.” reply ornornor 44 minutes agorootparentprevNot trying to defend this kind of practices, it just reminds me of something I watched recently about the working conditions in the Victorian era in a UK cotton mill… Atrocious and exploitative by all current standards, and yet people chose this over more traditional agricultural occupations because it paid better, weekly, and there was no uncertainty that you’ll lose an entire season of wages because the harvest was bad. And yet they were working 72h a week, had indentured child labour, average life expectancy was something like 40 years old, injuries and loss of fingers or limbs were regularly occurring. reply s1artibartfast 5 hours agorootparentprevWhy is that strange? At no time in history have jobs paid according to their benefit. reply throw10920 4 hours agorootparentIt's not actually strange - GP is just using that word to emotionally manipulate readers. reply vasco 2 hours agorootparentTo manipulate readers into realizing that industry doesn't pay their workers well? reply s1artibartfast 1 hour agorootparentbasically. To manipulating the reader into thinking that workers being paid well or according to their \"benefit\" is the natural, logistical, or otherwise expected outcome. It is an attempt to switch the roles between what the conventional, familiar, and safe what is unconventional, weird, and strange. Paying workers according to \"benefit\" is not tried and true. It has real challenges and problems. reply almostgotcaught 5 hours agorootparentprevIs there a word for this specific kind of ambivalence where you label a pitiable, miserable set of circumstances \"strange\"? As if it's on the same level as the strong nuclear force? Maybe \"motivated ambivalence\". There's nothing strange about it just as there's nothing strange about cobalt mining conditions in Democratic Republic of Congo: exploitative trade agreements, political corruption, and apathy. reply byteknight 4 hours agorootparentJust because you don't think its strange doesn't mean others don't. Strange is subjective. If he feels its odd we willfully ignore it, then its strange. reply vasco 2 hours agorootparentprevUsing \"strange\" instead of \"bad\" to me indicates someone has enough maturity to recognize that human nature is part of nature, which is gnarly and creates bad things like humans setting up incentives without having to necessarily classify humans or the universe as bad. Is it bad when a lion kills another animal? In a way yes, it's extra death that could be preventable, in another way it's what it is. Is it strange or bad if a human is born dumber than average? What about if a human is born more narcissistic than average and does bad things? It leaves open the possibility of you the writer also being wrong, so it comes across as humbly sharing an opinion. reply frutiger 42 minutes agorootparent> Is it strange or bad if a human is born dumber than average? And by definition, 50% of babies are. reply TrackerFF 10 hours agoprevOne of my previous (job) tasks was to monitor larger vessels, and analyze where they'd end up getting torn apart. Turns out western shipping companies don't like paying western prices for that kind of work, and try to sneak the vessels down to India, Bangladesh, etc. where that kind of work is much cheaper. But with cheaper prices comes a host of issues, from the environmental effects, to human workers actually performing the dangerous work. Sometimes these things can fail spectacularly - like when they try to sail or tow the vessel, end up drifting to land, and create huge oil spills. reply aredox 9 hours agoparentThe trajectory of the last French air carriers - the Foch and the Clémenceau - is a good illustration of the mess it can be, even for former military flagships. Those military ships are of course full of asbestos - more than usual - and heavy metals. The Clémenceau was supposed to be dismantled in Spain, but when the marine nationale saw it being towed to Turkey, they cancelled the contract and got it back. Then another consortium offered to dismantle it in Alang but with special precautions. The boat left, was blocked by NGOs, was blocked by Egypt when it tried to cross Suez, then India refused to accept it. It came back to France after rounding all of Africa. It was eventually dismantled in the UK, after a few more protests (the river Tess had to be deepened, and the hull had to be cleaned up of any invasive organism) https://fr.m.wikipedia.org/wiki/D%C3%A9mant%C3%A8lement_du_p... The Foch was sold to Brazil, and after much of the same drama, was eventually sunk in the Atlantic. reply guidoism 1 hour agorootparentI wonder how different each of the wikipedias are between languages. There's barely any info about the dismantling of the Clémenceau in the English wikipedia. I occasionally read the Spanish language wikipedia for South American history as it is more complete. As a someone fluent in both languages I feel like I have access to more knowledge. How much knowledge is \"hidden\" in other languages on Wikipedia? Would be cool for translators to copy portions between languages. They must do this already, right? reply ornornor 36 minutes agorootparentFor this kind of stuff there is usually much more in the language of origin. I’d suspect the French article have more details on this since it was the French navy. reply spicybbq 5 hours agorootparentprevAlang is an interesting place to look at in Google maps. Make sure to check the street views. https://www.google.com/maps/place/Alang,+Gujarat,+India/@21.... reply teruakohatu 5 hours agorootparentVery interesting. I spotted quite a lot of totally enclosed lifeboats along the main street, including what looked like a business selling enclosed lifeboats. I didn't imagine there was a used market for enclosed lifeboats but a google lead me to Alibaba and they go for hundreds to 10s or 1000s of dollars. reply yohannparis 5 hours agorootparentprevMerci for sharing this tragic part of French history. I wished budgets on such project included the dismantling of it at the end. reply throw383y8 10 hours agoparentprevWell, West should stop dumping their garbage to the rest of the world. It is easy to have strict environmental regulations, if they are not enforced, and negative externalities are exported. reply aredox 9 hours agorootparentThe maritime industry, more than \"the West\", is a prime culprit there. Let's also mention how they skirt taxation, the use of flags of convenience, the lack of protection for crews, the matrioshka shell companies... reply fifilura 9 hours agorootparentAlso the clothes industry! The clothes brands claim to recycle cotton, but use 3rd party \"recycling companies\" to ship the textiles to Africa, where they just burn it in huge waste heaps. With huge environmental issues. https://www.greenpeace.org/africa/en/blog/54589/how-fast-fas... Computer \"recycling\" in Lagos/Nigeria is another topic. reply fifilura 7 hours agorootparentA follow up Swedish newspaper Aftonbladet added a airtags in clothes \"recycled\" by H&M. Guess where they ended up? https://www.aftonbladet.se/nyheter/a/jlME1e/aftonbladet-inve... reply 542458 7 hours agorootparentI’m a bit confused as to how this investigation worked - how can they tell if clothes were ground for fiber with just airtags? reply playingalong 3 hours agorootparentNot sure if they did this, but they could fly over a reporter to last place they observed the AirTag and have them look around. reply fifilura 6 hours agorootparentprevFWIW This is one of the sources, in Swedish though. https://www.aftonbladet.se/nyheter/a/0QxkyA/modets-morker-5-... At the end there they list the fate of 10 different garments they returned to H&M. You can evaluate yourself if you find this environmentally friendly or not. reply fifilura 57 minutes agorootparent(This comment was not meant to be snarky although I realize it may look like it. I just did not have time to write a proper comment/translate to the article earlier. See my later answer in the sibling) reply fifilura 2 hours agorootparentprevTL;DR 1. They travelled to the location (in Togo). 2. They found the guy who imported the jeans. He was about to try to resell them (along with huge bales of clothes), but most of the clothes are not sold. I dont think recycling was at the top of his mind. 3. The last signal was from a place where they use to make a fire of the clothes. 4. Most of the garments travel around the world as waste - not exactly good for the environment. The person from H&M responded with whataboutisms. Which is not exactly legit since they use this in marketing. Edit:formatting reply ornornor 35 minutes agorootparentprevAnd “magic pipes”! reply mschuster91 9 hours agorootparentprev> Well, West should stop dumping their garbage to the rest of the world. Actually the export of ships to be wrecked in Asia already is banned under EU law and international treaties - and sometimes, even company owners can and do land in jail for violating them, as it happened to Georg Eide [1]. The difficulty lies in the fact that many ships aren't registered in the EU countries, but in small countries like Antigua who don't have any incentive to help out European countries enforce their laws, and by many ships being legally hidden between layers of shell companies. It can go as far as there being a dedicated LLC in yet another tax haven per ship, and once the ship is to be wrecked, it isn't the ship itself that's being sold for wrecking (because that would be openly illegal and easy to catch and prove for authorities), but the LLC is being sold, and the lax attitude towards audit and public records requirements in the tax havens makes it very difficult to prove illegal intent. [1] https://www.freitag.de/autoren/julia-lauter/reedereien-lasse... reply withinboredom 3 hours agorootparentReminds me of when I lived on a sailboat. I just bought the LLC that owned it, which always operated at a loss just to keep the ship in working order, which was a tax write off. It even came with accountants who knew how to keep everything in the books “correct.” reply jopsen 9 hours agorootparentprevWe could require that ships they dock in EU harbours are owned by countries that are party to some international anti-ship breaking agreement. But it's a lot of paperwork :) reply ethbr1 6 hours agorootparentLike the Jones Act [0], it's easy to create unintended consequences. [0] https://en.m.wikipedia.org/wiki/Merchant_Marine_Act_of_1920#... reply mschuster91 8 hours agorootparentprevStill would not prevent just selling the LLC at the end of the ship's useful life. And I'm not sure how to effectively police that. reply Teever 8 hours agorootparentFind the names of the people involved, get a warrant and use the full power of that impressive global surveillance system that we've created to fight the global war on terror to surveil them and find evidence of other crimes that they've surely committed and prosecute the fuck out of them for that. reply scarby2 3 hours agorootparentWe do live in a world where it's basically impossible to live without violating one law at one time or another (often unknowingly and without malice) reply s1artibartfast 5 hours agorootparentprevSeems like a lot of work to prevent India from breaking ships it wants to break. reply Teever 3 hours agorootparentMeaningless statement. You can say the exact same thing about any law. reply s1artibartfast 2 hours agorootparentNot every law tries to regulate the behavior of people in other jurisdictions. reply Cthulhu_ 5 hours agorootparentprevBut that might put the EU at an economic disadvantage; it's hubris to believe the rest of the world will bend backwards to meet EU's rules. I mean they do, see cookie banners/GDPR, Apple, etc, but still. I'm of the unsubstantiated opinion that all the laws make the EU a less desireable market. But that's a policy based on morals instead of economics / relentless capitalism. reply mschuster91 4 hours agorootparent> But that might put the EU at an economic disadvantage; it's hubris to believe the rest of the world will bend backwards to meet EU's rules. Actually, it will. RoHS and the push for standardizing phone connectors ended up influencing the whole world and making it a better place for everyone. The EU is a sizable market, and unlike the US with its constant elections and government shutdowns, it's politically relatively stable. No one, as said not even Apple, can ignore the demands of the European Union. reply scarby2 3 hours agorootparentThe EU has consent elections, it's just a proportional representation forces people to work together to get anything done, so you end up with a lot of relatively uncontroversial work happening. There's also little point in trying to score points as you will never have a majority reply mschuster91 2 hours agorootparent> The EU has consent elections The EU parliament is elected every five years, and while most of the member countries elect during the EU parliament term and so it's election year somewhere in the EU every year, national elections usually have negligible impact on the European level (outside of dramatic swings like in Poland or Slovakia between dedicated notorious pro-EU and anti-EU parties). In contrast, the US re-elects the whole House and 1/3rd of the Senate every two years which means that the House is basically only at peace to work for a year at a time (first half year is spent on getting the newbies up to speed, last half year is filled with campaigning), and if changing control over the Senate is even possible depends on if the states whose seats are up for reelection are considered swing states or not. The entire way the US Congress works is not incentivizing bipartisan legislation, and it's outright hostile to the idea that there could be more than two political parties. reply gosub100 4 hours agorootparentprevTo make matters worse, when these ships come under attack or get hijacked, suddenly they want the full backing of Western military forces. Strange they don't call on Panema or Antigua's military forces, since that's where they are registered. reply helsinkiandrew 12 hours agoprevThese are probably taken at the Alang (India) and Chittagong (Bangladesh) Ship Breaking Yards, where they run the ships aground at high tide. Both visible on Google Maps satelite view: https://www.google.com/maps/place/21%C2%B023'21.9%22N+72%C2%... https://www.google.com/maps/place/22%C2%B027'24.5%22N+91%C2%... reply JackFr 3 hours agoparent> where they run the ships aground at high tide. Read an article about this years ago. It seems they had special pilots to do this. Like wetting the bed on purpose, trained ships captains had a very hard time intentionally running ships aground. reply EvanAnderson 10 hours agoparentprevI'm not in a position to look right now, but I'm pretty sure there's video of this happening out on YouTube. It's pretty fearsome seeing these big vessels coming in under power and crashing into the shore. Tons of inertia. reply geetee 7 hours agorootparentI doubt it's the video you were referring to, but it reminded me of when YouTuber bald and bankrupt was in Bangladesh: https://youtu.be/iq_76McFVLo?t=1260&si=WT-yCAefa1vIHXli reply SoftTalker 4 hours agorootparentprevThere are a number of shipbreaking videos and documentaries on YouTube. reply mhuffman 12 hours agoparentprev>where they run the ships aground at high tide. Probably a dumb question but what do they do on the next high tide? Wouldn't it interfere with their work? reply Ekaros 11 hours agorootparentA ship running to shore at full speed has quite a lot of inertia. And then they winch it further. So pretty much only the end is dealing with tides. reply mhuffman 11 hours agorootparentAhhh, ok. So they ram it in and then winch it up closer to work on it. That makes sense. These things are huge and it would seem to take a bit of time to safely break them down. reply t_mahmood 10 hours agorootparentSafely?! Ha ha, unless you mean safe for the ship ... reply bmelton 7 hours agorootparentprevEkaros answer is definitive, but if you didn't have those facilities and wanted to buy more time, you could beach the boat during a high spring tide (which is when the solar tides and lunar tides are in line), which would only occurs twice each lunar month reply pavlov 12 hours agorootparentprevThey just take a break from the work? The hull of the ship that came in on the last high tide is now broken, it's not going to float away. reply jojobas 10 hours agorootparentprevThey start with superstructures etc, making the ships lighter. Then they pull them further up the coast as needed. reply gosub100 4 hours agorootparentprevI've seen the video. They drop an anchor and attach it to shore with a chain. reply mywacaday 10 hours agoparentprevThanks for that, some pretty cool street view panoramas at that location reply psyman 1 hour agoprevI went to my hometown in Pakistan a couple years ago and made my way to one of these yards and recorded this. Enjoy! https://youtu.be/C8VZfLzWYTc?si=0bXNEQDlqj6J4bKn reply aaroninsf 1 hour agoparentAmazing footage. I very much enjoyed! I will share this with my kids as well. reply usful 5 hours agoprevWow Ed Burtynsky on the front page of Hacker News! I worked on his film Anthropocene (https://theanthropocene.org/film/). Also recommend the film Manufactured Landscapes by him, which he also gave a TED talk about (https://www.youtube.com/watch?v=U2Dd4k63-zM&t=930s). reply kratom_sandwich 4 hours agoparentI adore this movie as well as his other movie \"Watermark\"! Any anecdotes you are willing to share from the set? reply morkalork 3 hours agoparentprevManufactured Landscapes was incredible. reply jabl 4 hours agoprevA few comments here mentioned that old ships being broken up have a lot of asbestos. I would have assumed this would have been an issue on very old ships, but some cursory web searching suggests that it's still prevalent even in relatively new ships. IMO rules (https://www.imo.org/en/OurWork/Safety/Pages/Asbestos.aspx ) say roughly that: - Ships built before 2002 may contain asbestos, but it should be \"managed properly\". - Ships built after 2002 can contain asbestos in a few specific applications. - Ships built after 2011 can't contain any asbestos, period. Unfortunately, it seems reality fails to follow even these modest regulations. One can assume that \"managed properly\" doesn't include having workers with no PPE ripping out asbestos insulation and then just leaving it lying around on the beach, as seems to be the norm in 3rd world ship breaking. And not only in breaking up ships, even newly built ships can contain asbestos. E.g. from https://www.marineinsight.com/shipping-news/more-than-65-of-... > John Rendi, General Manager, Environmental Services, Maritec, said: “Although newbuild ships are delivered with an asbestos free declaration, in many cases asbestos has been found onboard during subsequent surveys, or port state inspections.\" reply causal 4 hours agoparentMaritime regulation is hard when it operates in the space between states. Asbestos in particular is such a temptation in manufacturing because it's just so useful and cheap. An argument can probably be made that handled properly it saves more lives than it harms just because it's such a good fire resistant insulator. reply jabl 2 hours agorootparentWhile I don't have any insider info on the behind the scenes wrangling wrt IMO regulations, I would assume that asbestos wouldn't have been prohibited unless satisfactory alternatives weren't available. E.g. mineral wool, glass wool, and Zetex fabric are AFAIU widely used as fireproof insulation materials all over the world, presumably also in 'asbestos-free' ships. reply samatman 4 hours agorootparentprevThis might be true in fact, if asbestos use is limited to applications where it's the obvious best choice. But it's uncomfortable to run a simple utilitarian calculus when, as in this case, the groups it benefits (sailors) and those it harms (shipbreakers) are disjoint. Leads one to want to find a solution which is more straightforwardly positive-sum. reply macintux 3 hours agorootparentI don't think those building/owning the ships care much about the benefit to sailors, just that the ships are less likely to be severely damaged by fire. reply samatman 3 hours agorootparentIt wasn't a comment about motivations, but rather, benefit. Sailors fairly clearly benefit from not being on a ship which catches fire. If you'd like to add the financial balance of owners and builders to the benefit side, feel free. reply causal 3 hours agorootparentprevAgreed. I'm not advocating for use of asbestos given its tendency to end up in places humans breathe. reply xyx0826 12 hours agoprevIf anyone is interested in a game themed shipbreaking/dismantling things, check out Hardspace: Shipbreaker. Instead of oceanic ships one gets to take apart spaceships and sort the salvages like garbage-recycle-compost. https://store.steampowered.com/app/1161580/Hardspace_Shipbre... reply __david__ 2 hours agoparentI've played through this game twice I loved it so much. It's a great podcast game, too. reply hiddencost 10 hours agoparentprevWhich, BTW, is a parable about labor rights and how debt is used to control workers. reply mattlondon 10 hours agoparentprevI wanted to love that game. I gave it a good go, but just could not bring myself to want to play it. I think the story and narrative just put me right off of it from the very outset for some reason. I don't know why but it was a total turn-off. Also the zero-G thing made sense from a setting-perspective, but the slowly-slowly-drifting-around with limited control was just frustrating and infuriating in equal measure. I am sure it is \"realistic\" but then this is a game about being in space tearing space ships apart so who cares about realism? If I was able to do things faster and with more \"arcade\" style movement then I am sure it would have been a blast, but slowly drifting about in treacle was not fun. reply MivLives 9 hours agorootparentI'm a big fan. For me a lot of the fun was mastering that movement system. The way the game is set up it incentives you to be fast and take risks. To get around faster you have to treat yourself the same as the chunks of the ship, using the tether to pull yourself in. You can also magnetize yourself to the hull and spider around like that. With upgrades and practice I was able to get most of the smallest level of ships done in one in game day. reply CaptainOfCoit 8 hours agorootparentprevWhat in particular turned you off, the capitalistic nature of the story? From the Steam page, this is how the game publisher describes the game: > We offer you the privilege of helping turn humanity’s past into its future by salvaging ships in zero-g. Each one is a puzzle, and how you solve it is up to you! Carve your way in, salvage everything, and maximize your profit. Seems relatively vanilla, besides the capitalism part but most people are relatively accepting of that edge nowadays I feel like. reply icambron 1 hour agorootparentIt wastes a fair amount of your time blabbing on about this cartoonishly evil company, but that evil company has no real effect on the gameplay, so it's just distracting and irritating. And it's not some sly or evocative social commentary; it's ham-fisted and over-the-top. It almost feels like listening to the caricatured horribleness of this company is an extra price you have to pay to play the underlying game. The gameplay itself is just great though. reply mattlondon 6 hours agorootparentprevI am not sure really - I think there was a lot of strongly-accented \"yee haw cowboy\" type stuff from the NPC voices? Kinda wildwest in space? That is where my mind goes when I think of it, and I remember a distinct feeling of dislike. reply dippydipdips 5 hours agorootparentI think you're thinking of Breathedge, not Hardspace: Shipbreaker. reply Kerrick 6 hours agorootparentprevDo you really have such a strong aversion towards regional American accents? reply gnramires 7 hours agorootparentprevSounds interesting. I really liked (and like) Homeworld, not only because of the really captivating setting and gameplay, but also because of the stories. They really add meaning to a game: it wasn't just a few ships skirmishing, it was (if I remember correctly?) a civilization searching for their home and fighting for survival, while facing various challenges and contact with other civilizations (or were they other species?). Story and setting makes a ton of difference to how much I enjoy a game... reply Cthulhu_ 4 hours agorootparentprevUnashamed capitalism is a bit of a trope in sci-fi, reminiscent of Starship Troopers (the film) and Helldivers (spreading Democracy in space!); it being over the top is its own kind of criticism. reply Akronymus 11 hours agoparentprevI really like the gameplay of it, but the story actively makes it a worse product, somehow. reply arnsholt 10 hours agorootparentWhat is it about the story that puts you off? I might agree it’s not a masterpiece of interactive storytelling, but given the subject matter I’m hard press to find a different kind of story you could tell with it. reply Akronymus 10 hours agorootparentHow forced and in your face it is, only to then remove the \"working yourself out of debt\" part at the end and just destroying most of the reason to keep playing, at least for me. And everything being unskippable while you are stuck inside the small room unable to do anything but listen to it. reply resolutebat 11 hours agoprevShipbreaking is an absurdly dangerous job, and the \"Brothers\" sequence in Workingman's Death covers this. https://en.wikipedia.org/wiki/Workingman%27s_Death reply t_mahmood 10 hours agoparentI have not keeping track of this anymore, but, from the last time I read on this: A LOT of people go disabled, amputee from freak accidents, on a regular basis, and receive NO support from the owners, and life expectancy go really, really low. The whole business is controlled by a group of people, who have no ethical sense, and environmental concern, and these people are so powerful, no one can do anything to them. There's no pollution control here, so all the harmful chemicals go into the sea, and land, which basically have made the whole seaside area unusable for crops, which mean, people who were farmers can't farm anymore, and have to work in these yards. And die pretty fast. And obviously, as the chemicals are getting mixed in the land, it is affecting the people too. But again these people are so powerful, no one can say anything about it. reply 2rsf 10 hours agoparentprevIs it an inherently dangerous job or is it dangerous because of how it is managed? reply TylerE 10 hours agorootparentBoth. With western automation, rules and PPE it would be no worse than building the things in the first place reply resolutebat 8 hours agorootparentIt's probably worse than building them. When building a ship, we have a pretty good idea of how brand new materials for construction behave and have an exact plan for putting them together. Shipbreaking, on the other hand, involves chopping up enormous chunks of rusty steel of unknown but generally terrible condition (that's why they're getting broken up), meaning they can fail in unexpected ways at any time, plus you're basically flying blind because you don't have the original plans etc. reply csours 5 hours agoprevThere is a building being torn down in Austin that reminds me of shipbreaking every time I drive past it. Specifically it reminds me of the game Hardspace: Shipbreakers. https://www.youtube.com/watch?v=fKX31xawWQk https://www.statesman.com/picture-gallery/news/2024/03/05/fr... reply FriedPickles 12 hours agoprevVICE made a nice video on the shipbreaking here a few years back: https://www.youtube.com/watch?v=JU0DXdAhdsA reply fbdab103 11 hours agoparentThat is wild. Could not believe how many people were wearing sandals or completely barefoot. reply inglor_cz 11 hours agorootparentThe entire industry is wild. Old ships have a lot of asbestos, plastic is burnt in heaps right next to working people. Life expectancy in Alang et al. isn't great. reply refurb 9 hours agorootparentprevThat’s most of the world. The idea of worker safety and protection is the exception, not the rule. reply resolutebat 11 hours agorootparentprevBare feet are the least of your problems when somebody drops a couple of tons of rusty steel on your head. reply jabl 10 hours agorootparentYes, but that doesn't mean that using basic protective gear, like every heavy industry worker in developed countries, wouldn't reduce other kinds of injuries. reply gosub100 4 hours agorootparentprevThe threat of metal dropping on top of one employee doesn't remove the need for foot protection for the rest. reply aloe_falsa 9 hours agoprevPaolo Bacigalupi’s novel “Ship Breaker” deserves a mention here. It’s set in Paolo’s dystopian solarpunk universe, where old tankers are cut up to extract the last tons of fossil fuels from their hold, and it really emphasises how dangerous and unrewarding of a job it is. reply javajosh 3 hours agoparentA harrowing near-future dystopian tale, definitely worth a read. His \"Windup Girl\" is better; \"Ship Breaker\" is like a YA version of Windup Girl, IMHO. His \"Water Knife\" is also quite interesting, imagining a future where climate change has dried up water supply to the southwestern US, and the states have militarized against each other. reply speed_spread 8 hours agoparentprevYep. It's Alang-style shipbreaking, but on the beaches of a near-future Louisiana. reply h0l0cube 9 hours agoprevSomeone posted this hour long documentary on shipbreaking in the comments a while back. Both mortifying and fascinating to see this industry up close. https://youtu.be/5jdEG_ACXLw reply adityapatadia 11 hours agoprevI lived near Alang (India) and visited the shipbreaking operations a lot. For me as a kid, the best part was these ships had olives (in brine) and we could only get them from Alang. Happy to answer any questions you guys might have. reply boppo1 10 hours agoparent- I've heard these breaking yards are very toxic. Do you have any memory of this? - olives in brine? Like leftover food supplies and somehow that was a staple? - What was the visit like? Stand on a platform and watch? Guided tour? Freedom to run amok & don't get in the way/yourself killed? - What were the local opinions on the industry? reply adityapatadia 9 hours agorootparent- There are all sorts of toxins on the ships. Crude oil which drives the ship is one component but the government asks to get rid of it first as long as the ship arrives. Next big one is asbestos which is abundant and not removed by the government. It lies around. I am sure there would be more of them - Yeah olives in brine in a sealed food tin. We did not eat a lot of fish back then but there were food tins of all sorts on those ships. I personally only picked olives. - A visit could be arranged if you knew anyone who managed the shipbreaking. If the ship is not broken yet, they show you how to climb the ship (mostly vertical steel ladders). If the breaking has started, they don't allow you to climb the ship but then you can roam around the site and inspect all the goods removed from the ship and buy it at whatever price u feel. - Local population was not that educated. People did not take the businesses as badly as we treat them online. They feel it's a normal industry like anything else and gives employment so they are mostly fine with it. If some worker loses their life (a few do every year) the families are compensated to the tune of $1000 to $2000 and life goes on. (sad I know) reply SoftTalker 4 hours agorootparent> People did not take the businesses as badly as we treat them online. They feel it's a normal industry like anything else and gives employment so they are mostly fine with it. Same as many jobs in the last century in the west: coal mining, steel mills and other large industrial operations, much heavy construction. Not that it was OK or should be acceptable but it does seem to be the way these things go. reply jl6 10 hours agoparentprevDo you mean there was food still on board when the ships arrived? reply adityapatadia 9 hours agorootparentThere is a ton of food on board when ships arrive. Remember, the ships have crews and they are working when they arrive. In fact you could find all sorts of food items and kitchen equipments that you would find in a professional kitchen (a lot of restaurants in our area just got the kitchen equipment for cheap). reply SoftTalker 4 hours agorootparentI've watched a few shipbreaking documentaries, they show that near the breaking operations there are large flea markets or bazaars where you can buy pretty much anything of any value that has been salvaged from the ships. reply ggm 9 hours agoprevI met a captain whose job was to drive these kinds of vessels up onto the strand at Cox's bazaar. He said it was the saddest job he'd ever had. A lifetime of avoiding irretrievable beaching and then.. get a good line, and ram it up a beach to a final, terminal stop. reply spaceguillotine 2 hours agoprevIf you haven't played Hardspace Shipbreakers yet, you should. I honestly wish they would make it a required play for high schoolers learning about workers rights. Its a similar story just set in space. reply maxglute 4 hours agoprevI think there's some pretty sublime shots of shipbreaking in Burtynsky's manufactured landscapes. Reminds of that scene of Kirk riding his motorcycle in Star Trek 2009 with the foggy silhouette of a titanic starship in the background of middle America ship yard. I love the vibes of seeing the absurd scale of human construction. reply esafak 11 hours agoprevSteve McCurry also has a bunch of shots on this. Unfortunately, I couldn't find a link that conveniently has them all in one placed. https://www.peterfetterman.com/artists/146-steve-mccurry/wor... https://x.com/McCurryStudios/status/575680978997354496/photo... https://www.sundaramtagore.com/exhibitions/steve-mccurry2/se... reply SeattleAltruist 3 hours agoprevGlad to see many comments recognize what a bad thing this is. The photographer is sadly insouciant of how many poor people it kills and the terrible ecosystem damage it creates. Read more about it at https://shipbreakingplatform.org/ reply aaronbrethorst 3 hours agoparentGiven how much of Burtynsky's work centers on the effects of human beings on each other and our world, I don't think it's fair to describe him as being indifferent to human suffering. Instead, I think he maintains an almost journalistic distance from it, and lets his work speak for itself. https://www.edwardburtynsky.com/projects/the-anthropocene-pr... https://www.edwardburtynsky.com/projects/in-the-wake-of-prog... https://www.edwardburtynsky.com/projects/photographs/urban-m... reply datavirtue 2 hours agoparentprevMy understanding is that any recycling advantage is dwarfed by the environmental impact of the ship breaking process. reply bilekas 6 hours agoprev> ―it is as if the vastness of their perspective somehow opens onto the longer view of things. I got some odd feeling of talasaphobia from seeing them, maybe that's just me. I'm also curious how much money India makes in their part for the deconstruction of theses. > You may quote extracts from the website with attribution to www.edwardburtynsky.com reply BLKNSLVR 12 hours agoprevI feel like the music marries well with the somewhat depressingly industrial hardcore work on display in the ship-breaking clips: https://www.youtube.com/watch?v=KVm8G0ipETc (El Rodeo by Kyuss) reply pjmorris 7 hours agoparentOn the topic of music, I first learned of shipbreaking from Mark Knopfler's song, 'So Far from the Clyde.' https://www.youtube.com/watch?v=m9OIucgb_4Q reply BLKNSLVR 6 hours agorootparentTo stray further off topic (because I like this tangent), both of my parents were fans of Dire Straits and I was too young to really get it, but I've been reminded of Dire Straits and Mark Knopfler three or four times in the last couple of weeks, and the more of his body of work I come across the more obvious the consistency of the quality in both music and poetry. With a strong risk of hyperbole: The sound of a guitar string plucked by Mark Knopfler can't be mistaken for any other guitarist. reply pjmorris 6 hours agorootparent> The sound of a guitar string plucked by Mark Knopfler can't be mistaken for any other guitarist. I'd say there's only a mild risk of hyperbole; he really does have a unique sound. reply dfboyd 12 hours agoprevThese could be Battlefield 2042 screenshots of the \"Discarded\" map reply Arch-TK 11 hours agoparentReminded me of BF:BC2 Atacama Desert reply bartekpacia 7 hours agoparentprevSame here. At first I genuinely thought it’s some screenshots from that map. Says much about how advanced games have become graphics-wise. If only everything else didn’t become completely enshittified. reply MrMikardo93 5 hours agoprevI wrote a poem inspired by these images when I was 17 (over a decade ago!) which won second prize in a national competition. Seeing them again here takes me back. reply aaroninsf 1 hour agoprevMy wife and I visited the shipbreaking yards near Chittagong on our honeymoon. It was remarkable. I make field recordings; I was able to do a fair amount of field recording before a nervous foreman invited us to tea, gave us a calendar from the shipbreakers' association, and politely escorted us off the property. Apparently there had been journalists by in months prior doing some investigative journalism into issues of various kinds and while it was pretty clear we were not that, it wasn't worth his job to be wrong. Could not argue. Before we got booted some more junior and affable guys invited us to come through the former hold of a half-dismantled oil tanker and then up on to the fragment of deck remaining and the control tower. Amazing to be where thousands of tons of oil had been in living memory. The specialization of labor and application of manual force to an industrial process on that scale was mesmerizing, but as you can tell from the photographs, was unsafe in innumerable ways. Always wanted to visit the ones in Gujarat as well. reply pqdbr 6 hours agoprevThe last two pictures are the most impactful. reply ZeroGravitas 11 hours agoprevThis presumably inspired some of the scenes in the Star Wars series Andor. reply boppo1 10 hours agoparentPerhaps also BR 2047 reply cies 9 hours agoprevThere was an AlJazeera docu about in the series of \"Workingman's Death\" that had amazing footage on a ship recycling yard in Pakistan. They've since put it on private, sadly. See it being private here: https://www.aljazeera.com/program/working-mans-death/2012/1/... If you are interested in this, you may want to ask them to make it public again as the quality was absolutely stunning. Good interviews. Really getting into the skin of the workers lives there. reply jimmySixDOF 9 hours agoprevShipbreaking, not to be confused with Shipwrecking which was the legendary practice of costal piracy where ships were lured to the rocks by fake lighthouses as depicted by Alfred Hitchcock in the 1939 classic Jamaica Inn. https://m.imdb.com/title/tt0031505/?ref_=nm_flmg_c_40_dr reply aaron695 10 hours agoprev [–] These pics are 2000 (Chittagong, Bangladesh) posted around 2009 as you can see from the low rez. The photographer has a doco - https://www.imdb.com/title/tt0832903/ As others are posting there are better videos / pics out. Good book, not too long, easy read - \"Ship Breaker\" by \"Paolo Bacigalupi\" - https://booko.us/9780316056199/Ship-Breaker I think cyberpunk, Amazon says dystopian romance. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Edward Burtynsky's \"Shipbreaking\" project examines the dismantling of large vessels, inspired by the decommissioning of single-hulled ships after the Exxon Valdez oil spill.",
      "The photographs, taken in India and Bangladesh, portray the industrial process as a form of ultimate recycling, highlighting the environmental impact of industrial activities.",
      "Burtynsky's work suggests a haunting beauty in these scenes, implying that nature can eventually reclaim landscapes altered by human activity."
    ],
    "commentSummary": [
      "The thread discusses the shipbreaking industry, highlighting harsh working conditions, environmental impact, and ethical dilemmas.",
      "Users share personal anecdotes, historical comparisons, and media references, including the game \"Hardspace: Shipbreaker\" and works by photographers and authors.",
      "Broader issues such as global labor exploitation, environmental regulations, and international waste management complexities are also explored."
    ],
    "points": 298,
    "commentCount": 135,
    "retryCount": 0,
    "time": 1716267367
  },
  {
    "id": 40426621,
    "title": "NoTunes: Stop Apple Music from Auto-Launching on macOS",
    "originLink": "https://github.com/tombonez/noTunes",
    "originBody": "Notice The certificate used in noTunes prior to version 3.2 is set to expire on the 14th January 2022. To continue using noTunes please update to version 3.2 or greater. noTunes noTunes is a macOS application that will prevent iTunes or Apple Music from launching. Simply launch the noTunes app and iTunes/Music will no longer be able to launch. For example, when bluetooth headphones reconnect. You can toggle the apps functionality via the menu bar icon with a simple left click. Installation Homebrew brew install --cask notunes Direct Download noTunes-3.4.zip Usage Set noTunes to launch at startup Before Ventura: Navigate to System Preferences -> Users & Groups. Under your user, select \"Login Items\", click the lock on the bottom left and enter your login password to make changes. Click the plus sign (+) in the main panel and search for noTunes. Select it and click \"Add\". Ventura and later: Navigate to System Settings Select General Select Login Items Click the + under Open at Login and select noTunes Toggle noTunes Functionality Left click the menu bar icon to toggle between its active states. Enabled (prevents iTunes/Music from opening) Disabled (allows iTunes/Music to open) Hide Menu Bar Icon Right click the menu bar icon and click Hide Icon. Restore Menu Bar Icon Quit noTunes, run the following command in Terminal and re-open the app: defaults delete digital.twisted.noTunes Quit noTunes To quit the app either: With menu bar icon visible Right click the menu bar icon and click quit. With menu bar icon hidden Quit the app via Activity Monitor or run the following command in Terminal: osascript -e 'quit app \"noTunes\"' Set replacement for iTunes / Apple Music Replace YOUR_MUSIC_APP with the name of your music app in the following command. defaults write digital.twisted.noTunes replacement /Applications/YOUR_MUSIC_APP.app Then /Applications/YOUR_MUSIC_APP.app will launch when iTunes/Music attempts to launch. This can be used to open a website too, for example, YouTube Music. defaults write digital.twisted.noTunes replacement https://music.youtube.com/ The following command will disable the replacement. defaults delete digital.twisted.noTunes replacement License The code is available under the MIT License.",
    "commentLink": "https://news.ycombinator.com/item?id=40426621",
    "commentBody": "NoTunes is a macOS application that will prevent Apple Music from launching (github.com/tombonez)278 points by faebi 8 hours agohidepastfavorite233 comments 9dev 6 hours agoFinally! This is so annoying for Spotify users. Every time the system starts up, and Spotify is configured to start at startup, but you haven't interacted with the window yet, then press the physical play button on the keyboard—instead of starting playback as you'd expect, the system starts Apple Music. It drives me mad. reply tivert 5 hours agoparent> Finally! This is so annoying for Spotify users. Every time the system starts up, and Spotify is configured to start at startup, but you haven't interacted with the window yet, then press the physical play button on the keyboard—instead of starting playback as you'd expect, the system starts Apple Music. It drives me mad. But you're in Apple's ecosystem, you're supposed to use Apple products, happily. The obvious fix for that \"bug\" is to start using Apple Music. reply BelleOfTheBall 3 hours agorootparentAt least Windows had the decency to make their ridiculous straight-to-LinkedIn keyboard shortcut obscure. Applications auto-launching should not be a thing unless the user set them up to do so or they’re essential to the system’s functioning. reply 7thpower 2 hours agorootparentMind blown. reply freetanga 5 hours agorootparentprevAnd buy more hardware, just to be safe… reply pachouli-please 4 hours agorootparentclosing issue as [fixed in m4] reply jkaptur 3 hours agorootparentprevhttps://en.wikipedia.org/wiki/Straw_man reply fragmede 44 minutes agorootparentyou have to follow up the claim that it's a strawman with a counterpoint, otherwise it doesn't count. you can't just say strawman and walk away, if you actually want curious conversation, which is what this site seeks reply talldayo 3 hours agorootparentprevhttps://en.wikipedia.org/wiki/Hyperbole reply freedomben 3 hours agorootparentprevFirstly, as someone who is frequently driven nuts by strawman arguments, thank you for spreading the general message! In this particularly case though, I think you're missing the point. Apple takes an opinionated \"we are right\" approach to design, and it's typically an all-or-nothing thing. If Apple decides to allow choice, then you get choice. If they don't, then you don't. It's one whole, beautiful, integrated system (as they would say). There's a lot of (intentional) friction[1] in place to make keeping a toe in or out very uncomfortable. [1]: Don't think this needs a citation, but if you disagree take a look at the \"buy your mom an iphone\" stories and the internal emails about making iMessage work on Android (Apple/Android families and all that) reply underlines 2 hours agorootparentMaking iMessage work on Android is a non issue for most parts of the world: mobile messaging is fragmented and the US uses SMS/iMessage, while most of the rest of the world uses either whatsapp, LINE or WeChat. reply freedomben 2 hours agorootparentI think you're missing the point. It's not about this single example. The point is that this is how Apple operates. It's embedded in product decisions all over the place. But also, this comes off a bit like a \"if you don't like it, move to another country\" type of response. It's not very reasonable, and it's not very helpful for the majority of people. reply rcleveng 5 hours agoparentprevAlso annoying if you use AirPods for Google Meet and Zoom. Half of the time when I put in my AirPods, here comes Apple Music, stealing focus from what I was trying to do. reply mikepurvis 5 hours agorootparentI get this all the time with bluetooth audio and even Carplay— connect the phone to some external audio sink and boom, I'm listening to whatever is at the top of my music library sorted alphabetically (The Absentee, by Crooked Still). I'm relived to hear that this isn't a problem which goes away if I buy Airpods. reply mastax 4 hours agorootparentI never used to have this problem until recently, when it started happening every time I’d start my car - and even every time I’d stop playing other media in my car. I ended up uninstalling Apple Music. At least they let you do that. reply boxed 6 hours agoparentpreviPhones are even worse. You play a sound in any app, like a short clip in instagram or Safari, and then press play again you get Apple Music. This has been broken since the very first iPhone. They just need to add a stack of 2 entries to fix this for 99% of usages. reply herpdyderp 6 hours agorootparentat least on iOS you can permanently delete the music app (permanently meaning system updates won't restore it) reply rjzzleep 5 hours agorootparentWait, you can do that? Will that fix the play pause issue ? Thank you, this is wonderful. I never thought it was an Music app issue. I always it's just iOS pip being broken. reply nolongerthere 4 hours agorootparentI had never experienced what the gp was experiencing and I can only guess it’s because I’ve had apple music uninstalled ever since they allowed that. reply freedomben 3 hours agorootparent> ever since they allowed that do you remember about when that was? reply dpkirchner 3 hours agorootparentprevYou can, but you may lose the ability to reliably play anything on your car over lightning. At least I do. (I also have to have downloaded music... thankfully I got that U2 album ages ago, ha.) reply kn0where 2 hours agorootparentYes, I had that issue too (years ago, when they first allowed deleting the Music app). I think it might not be a problem in newer cars, but my car radio only had iPod support, not iPhone, and it seemingly needed at least one song in the music library for the connectivity to work at all. I actually bought this 10 minute long silent song on iTunes at the time, so my phone wouldn’t automatically play Weezer’s “Across the Sea” every time I connected my phone to the car: https://music.apple.com/us/album/a-a-a-a-a-very-good-song-si... reply boxed 4 hours agorootparentprevI actually want the app though. Some music is not on Spotify that I have from iTunes music store back in the day. If you can believe it, several Black Sabbath albums aren't on Spotify. reply freedomben 3 hours agorootparentThis phenomenon is what is causing me to seriously contemplate returning to my own music collection. It's getting crazy reply underlines 2 hours agorootparentDitto. And my Spotify library shows cracks more and more. I am a playlist curator by heart, did that with my 100GB MP3/FLAC Music collection for 20 years before switching to Spotify, but now and then you get an empty artist/track name entry and can't even see what song it was, due to the Song rights being lost. reply freedomben 2 hours agorootparentSame. I've come to highly detest the empty artist/track name. Sometimes it's easy to figure out (such as a \"discography\" or \"greatest hits\" playlist where one album or some major-hit single disappears), but more and more often it's been pieces of a playlist I loved that is more and more wiped out. One of my favorite playlists has almost 30% of the songs as empty D-: reply boxed 2 hours agorootparentprevWell.. to be fair, those records aren't on any system anymore. It's a lawyer problem. reply quickthrowman 2 hours agorootparentprevThat doesn’t help me as I use Apple Music to listen to music that isn’t on Spotify. The play button defaulting to Apple Music even when another sound source has been used more recently is one of the few things I dislike about iPhones. reply Angostura 5 hours agorootparentprevThat’s odd, because that’s not the experience I have. I’ve just replicating the behaviour you see and I’m not getting it. Could you give an example? reply bullfightonmars 5 hours agorootparentThis isn’t an Apple Music thing. It’s an app stealing audio focus and them requiring you to manually revert back to what you were listening to previously. reply boxed 3 hours agorootparentAlso that iOS decides to default to Apple Music. Every time I connect my phone to my car for example it defaults to Apple Music. Even though I have used Spotify AND Overcast to play audio lately but almost never Apple Music. At least I could still buy \"A a a a a a very good song\" in iTunes Music Store still so it won't alphabetically run Anno Mundi (or for my wife whatever is the lexiographically first track of that U2 record they pushed on everyone) reply cheshire137 5 hours agoparentprevI modified my touch bar to no longer show the play/pause button because every time I forgot and hit it, I got punished by Apple Music opening and getting stuck in a loop of me force-quitting it and it restarting. reply arrowsmith 3 hours agoparentprevI feel weird reading this thread because I use a Mac all day, every day, and I have never noticed this problem. I honestly had no idea that this was a big frustration for people, and when I saw this thread I thought \"why would I need to prevent Apple Music from opening?\" I can't remember ever being annoyed by the \"play\" button opening Apple Music - the worst it might do is start playing the YouTube video in an open browser tab when I wanted it to play Spotify (it depends on which app i most recently alt-tabbed from), but that's only a minor annoyance. Am I the weird one? My system rarely starts up because I rarely turn it off; why bother? (`uptime` reports that the last time I restarted the computer was 79 days ago.) And I pretty much always have Spotify or YouTube (the two main places I open in the background, so Macos doesn't need to open a new app to start playing. What am I doing right? reply Tyr42 5 hours agoparentprevI never agreed to the EULA, which at least makes it easy to stop. reply RunSet 3 hours agoparentprevSomewhat related: Any veteran Mac user know any way to disable this thing? https://i.postimg.cc/P5XJ1wwJ/image.png reply sbuk 1 hour agorootparentSystem Settings > Control Centre > Now Playing > Show When Active or Don’t Show in Menu Bar reply RunSet 1 hour agorootparentIf only it were so easy. What is shown in the screen shot above is the result of exercising the option you describe. Unfortunately that option only disables the display of the currently playing track title, not the entire section labelled \"Music\". reply troupo 6 hours agoparentprevBeardedSpice is the saviour. reply spectre3d 43 minutes agorootparentFor newer Macs (10.15+) Beardie is a fork that’s been kept current. https://github.com/Stillness-2/beardie reply 9dev 4 hours agorootparentprevHe's doing the lords work here! reply sneak 6 hours agoparentprevThe launch of Apple music also sends your system serial number to Apple, so technically Macs all have a single hotkey to share their hardware identifier and IP with the mothership. reply mvkel 6 hours agorootparentTechnically Apple sent their serial number to you. Either way you can prevent this by selecting \"Don't Send\" when you first set up your Mac reply sneak 6 hours agorootparentThis is false. No matter what you do configuration-wise, macOS sends hardware identifiers to Apple, on each OS update (the ECID and others to the TSS server gs.apple.com), as well as when it registers for APNS. The obtained hardware-identifier-linked certs are then used continuously to access APNS. There is no avoiding it in the first instance (the OS update process), as it is required for updates if you have boot security enabled, and there is no UI to disable APNS. reply nicce 6 hours agorootparentAnd it is actually very useful, at least if you buy AppleCare+ Global repair with capped low price just by giving your computer and no questions asked. I just personally saved about 3000 dollars. reply cjk2 5 hours agorootparentExactly this. Concierge needs to know who the hell you are. reply jeffhuys 6 hours agorootparentprevYou were talking about the hotkey. reply freedomben 3 hours agorootparentGood call. I agree generally with GP, but they definitely did shift the goal posts :-) reply crmd 3 hours agoprevThis is the kind of abuse of power that I wish antitrust regulators would focus on. Apple, Inc owns an operating system and a music app, and leverages its control of the OS to secure an unearned advantage for its music app over its rivals. Apple is not the only perp here, every single large tech company abuses platform power for competitive advantage in unrelated industries. reply jawbah 8 minutes agoprevFor me, this works for the \"play/pause\" button on the keyboard but not if you have your Airpods double-tap or squeeze gesture to also do \"play/pause\" - that also launches Apple Music! Has anyone else reproduced this? reply tombert 4 hours agoprevI've been using macOS full time for about four years. Prior to that I had been running some flavor of Linux for about a decade. I moved to macOS exclusively because I had had extremely bad luck with Asus hardware [1] and I wanted to guarantee that my next laptop had minimally-decent build quality, and I was working for Apple at the time so I had a considerable discount on Macbooks. I still really like my laptop (it's about four years old and the hardware is still ship-shape), but I've grown really frustrated with macOS. It's not horrible, it's still Unix so I still have a decent command line and access to good dev tools, but I feel like I'm constantly trying to work around the direction that Apple is pushing. I can't tell you how many times I've accidentally launched Apple Music, and as apps like this show, I'm clearly not alone. Part of the issue isn't even that it's the default behavior exactly, it's that it includes all this crap I don't want. I don't want Apple Maps and Apple Music and all the other tiny little apps that depend on Apple services pre-installed. I wish there were a \"minimal\" macOS, that gave you a desktop and a terminal and a few other basic utilities [2], and that's basically it. It can't launch Apple Music if Apple Music isn't installed. Once the community fixes up the Linux support on the T2 Macbooks, I'm moving over to NixOS minimal full time. [1] I swore a blood oath to never buy an Asus product again; the laptop literally started to fall apart after only about a year of usage, and it was almost never transported anywhere (it lives right next to my bed). The plastic holding things together started to delaminate and I had to do a ton of surgery on it in the form of Gorilla Glue and clamps. Never again. [2] Disk manager, for example. reply freedomben 3 hours agoparentWe're blood brothers my friend. Never again on Asus (though I do have a traitorous exception for mother boards, though I'm trying to change). I also refuse HP stuff for the same reason. Could be so good, but brittle easily broken parts end up wiping out all the quality stuff. Definitely take a look at Framework laptops (https://frame.work). Unless they lose their way, I don't plan to buy anything else. I've also had great luck with Lenovo T* line (my T580 was one of my favorites of all time). Dell makes some gorgeous stuff as well (though I go with the business line of XPS, I forget what they're called at the moment, maybe Precision?). If you're going with Dell, Ubuntu or a derivative works a little better. I've had friction in the past with Fedora and Arch on Dell because it takes 6 to 9 months for all the drivers to make it upstream to the kernel, so if the model is new within 6 to 9 months you're stuck relying on Dell's build. reply digging 2 hours agorootparentSeconding Framework - mine's not very old, so I can't speak to longevity, but their mission is aligned with my values and I've been pretty happy with it so far. reply tombert 2 hours agorootparentprevYeah, I have a friend that bought a Framework laptop and he's said a lot of good things about it. My Macbook was expensive enough to where I'm hoping to get another couple years out of it, which I actually think will work out great, because that'll give a bit of time to further increase Linux compatibility on the Frameworks. I sort of refuse to use any Linux other than NixOS at this point, so I might have to wait a little longer to get everything, but I think I'm ok with that. reply JohnMakin 4 hours agoparentprev> [1] I swore a blood oath to never buy an Asus product again; the laptop literally started to fall apart after only about a year of usage, and it was almost never transported anywhere (it lives right next to my bed). The plastic holding things together started to delaminate and I had to do a ton of surgery on it in the form of Gorilla Glue and clamps. Never again. I know this is tangential but I've had to swear off high end laptops in general because of stuff like this, and I've seen it with many different brands - they'll jam a bunch of high end hardware into the literal cheapest, most badly designed case ever, and inevitably that is what causes the issues, not the actual hardware (with the exception of MSI boards, which always seem to have some inherent hardware issue). Never again. reply bunderbunder 3 hours agorootparentI think that, for reasons that are various but ultimately just different flavors of the same reason, this seems to be the case for high end anything. My 14 year old Kia Soul isn't in the shop nearly as often as any of the much younger midrange and high-end cars my friends typically drive. My $75 bottom-of-the-line Seiko 5 wristwatch doesn't need nearly as much maintenance as my old boss's Rolex or my dad's Breitling. (And a $5 quartz watch would require still less maintenance and keep better time too.) Our fancy Breville toaster oven seems to be reflowing the solder in its own circuit boards, and my parents' fancy Bosch laundry machines need to be replaced every 5 years, while our cheapo unit works fine and is older than some of my colleagues at work. I'm guessing it's just that trying to pack more fancy features into a product means there's more to break. Or, e.g., if you've got a higher-end laptop then it might have higher-performance parts that generate more heat and degrade the plastic more quickly. Or they might have tried to distinguish themselves by using materials that aren't typically used for a reason. Stainless steel cars, anyone? reply tombert 3 hours agorootparentI agree with everything, but I would like to introduce another variable: economies of scale. There are more-or-less objectively correct ways to do a lot of things reliably, to a point where they’re kind of boring. Since they are somewhat objective, most companies use them as the default and the prices can get lower and lower because they’re making more and more. When a company purposefully tries the differentiate themselves, that inherently means that they cannot benefit from the same economies of scale, which in turn means higher prices have to be baked in. Sometimes that’s fine, I think Apple generally makes solid products for example, but a lot of the time this differentiation is just “different for the sake of being different”. reply dehrmann 3 hours agorootparentprev> midrange and high-end cars my friends typically drive I take it they don't drive Lexuses. reply tombert 3 hours agorootparentprevYeah, this was one of those \"Republic of Gamers\" laptops, which on paper was pretty decent with a nice AMD processor and GPU, but the actual build quality was garbage. I had this idea that I was going to have \"one computer to rule them all\", so something I could edit video on and play games and all that fun stuff. Since then I've come to realize that I would much rather just have some dedicated hardware (e.g. a dedicated mini desktop with a decent GPU) for intensive stuff and keep my laptop a bit more utilitarian, and primarily focus on something with decent battery life and solid Linux driver support. I'm a little disappointed at how bad the 2019-era Macbooks are with Linux, and it's a little frustrating that the Macbook Linux for the M[1|2|3] is substantially nicer and easier to set up than something with an Intel CPU and an AMD GPU. As it stands, I managed to get NixOS installed on my Macbook and it's almost fine, but suspend doesn't work at all, and the audio quality from the speakers is awful, which is a dealbreaker for me right now. I'm hoping that once Apple stops providing updates to Intel Macs, there will be a huge push to fix the kernel on the T2 hardware. reply barbariangrunge 3 hours agoparentprevMy asus zephyrus was unfixable too. Luckily I had an overpriced warranty for it. Not sure what laptop to try next for game dev, I actually went back to a desktop afterwards reply tombert 3 hours agorootparentI don't do game dev, but FWIW those $400-$800 mini gaming PCs on Amazon are surprisingly decent, with extremely low power consumption, and many of which have eGPU support if necessary. I've become extremely conscious about power usage in the last few months and so I've been replacing my servers with them. I started using one as a home theater PC about a week ago, and even the internal GPU has been pretty solid. I was able to do 4k video editing with Lightworks Pro with no trouble; I had trouble getting Stable Diffusion working but that was more of a software-support-on-NixOS thing than \"crappy hardware\" thing. I don't play a ton of new games but I did PS3 and Xbox 360 emulation on there and for the most part the frame rate was pretty stable, and I've read that a lot of people are having decent luck with even relatively modern games like Elden Ring. reply dijit 3 hours agorootparentI recently found that my Razer Chroma X could not fit a modern 40-series GPU. I'm not sure I can recommend eGPU's for gamedev for this reason (among, many others). reply tombert 3 hours agorootparentYeah fair, I've never actually used an eGPU so it's tough for me to directly endorse them. I know people who have had pretty good luck with them but I don't really know many game devs so I don't know how well they would actually work with that. The only thing I do that gives serious use to the GPU is hobbyist video editing, and I don't do that terribly often anymore. I do sometimes run Stable Diffusion, but that's mostly as a goof to generate funny pictures of Keanu Reeves. For that stuff, I think even a relatively cheap GPU does the job well enough. reply dijit 3 hours agorootparentprev1/10 failure rate of Razer Blade 18's in the last 18 months. That said: getting them seems to be horribly hard, so I might regret telling you this. reply projektfu 7 hours agoprevWhat does Apple gain by leaving off the setting to choose whether or not to open Apple Music? Surely their pro audio users don't want it popping up just because they connected headphones. reply Nextgrid 6 hours agoparentLess \"engagement\" in the Music app. Sure, most of that engagement is fake to begin with but why would a project manager intentionally ever admit to that? Same reason ads remain in inconvenient places that are highly prone to accidental clicks even though it would be better for everyone involved if accidental clicks were reduced. reply internetter 6 hours agorootparent> Same reason ads remain in inconvenient places that are highly prone to accidental clicks Not under a PPC model Edit: I'm saying that there is incentive to do this under a PPC model. Its just just \"bumping false engagement\" reply PaulHoule 6 hours agorootparentHuh? My working assumption is “all clicks are click fraud” and whenever you see an advertising supported site where the content jiggles aloud (Anandtech) it is no accident that you were trying to click on a link and the ad moved to be right under your finger at the last moment. reply rchaud 6 hours agoparentprev\"Choice is scary for users\" is the guiding principle of the Apple software ecosystem. reply pornel 6 hours agorootparentChoice is scary for Apple's Services Revenue. Apple believes they own every device, and feel wronged whenever users spend money without Apple taking a cut. reply humzashahid98 5 hours agorootparentThis is plainly true considering how hard Apple fought to suppress alternatives to the \"App Store\". I don't know why your comment was downvoted. reply bunderbunder 4 hours agorootparentI don't think you need to infer that Apple believes they own every device. You don't need to infer anything, really. You just need to observe that every corporation will engage in rent-seeking behavior whenever they're in a position to get away with it. Apple is far from the first mobile device maker to try to make it difficult or impossible to install software through alternative channels. That practice has been in place since before the invention of the smartphone. Google likely would have done the same thing with Android, had it not been far more profitable for them to take a different tactic. Heck, in the USA, back in the analog landline phone era, the Bell system famously did functionally the same thing until regulators stepped in. On that note, I'm annoyed that Apple does it, but I'm even more annoyed that, in the USA, we are absolutely toothless about regulation because we can't shake the pollyannaish belief that we should be able to expect corporations to play nice purely out of the goodness of their nonexistent hearts. reply freedomben 2 hours agorootparent> Google likely would have done the same thing with Android, had it not been far more profitable for them to take a different tactic. I don't think that's fair. You have to consider the culture at Google and pre-Google Android. It might be the case, but at the time it felt like a very natural way for them to go. > On that note, I'm annoyed that Apple does it, but I'm even more annoyed that, in the USA, we are absolutely toothless about regulation because we can't shake the pollyannaish belief that we should be able to expect corporations to play nice purely out of the goodness of their nonexistent hearts. Agreed, though I think this is a cultural problem more than a regulatory problem. We humans are suckers for good marketing, as Apple has masterfully demonstrated over the years. Some of their real life PR/marketing lines are indistinguishable from parody, yet people's faith and trust in Apple is at or near a religious level of devotion. They love the products and they desperately want to believe in the goodness of the creator. Unless/until we figure out how to pull the curtain back so people see through the spin, they will stay powerful. Once we do that, the regulation will follow naturally. reply ziddoap 5 hours agorootparentprevProbably because there is significantly more nuance to be had than the blanket statements would indicate. reply pornel 3 hours agorootparentI've used to excuse such behaviors, saying that Apple has better integration between their products, cares about UI design more than anybody else, or that such integrations are new and don't have proper APIs yet. However, years have passed, and Apple has been systematically giving preferential treatment to their apps and services, while dragging their feet on APIs for 3rd parties. They keep using genuinely useful aspects of the App Store as a shield for anti-competitive abuse and petty rules (only in Apple's imagination allowing apps to mention other platforms is the same thing as allowing malware). They've been spreading FUD and presenting false dichotomies that it's either exactly Apple's way, or chaos and malware. Instead of creating safe and efficient APIs for 3rd party browser engines, they chose not to have competition for Safari instead. Instead of creating APIs for easy centralized subscription management, they conveniently kept it exclusive to their own subscriptions, with pricing that can only be sustained in a duopoly. Apple had plenty of time, and lots of chances to ease off their worst anti-competitive behaviors to keep regulators away, but instead they've doubled down on owning the platform and their users, and are now throwing tamper tantrums and malicious compliance. reply ziddoap 3 hours agorootparentI appreciate the more thoughtful comment! I agree with some of it, and disagree with some of it -- which isn't possible when you try and boil it down into a one-liner. reply humzashahid98 5 hours agorootparentprevCould you tell us what nuance you have in mind? reply ziddoap 4 hours agorootparentThere are literally thousands upon thousands of comments debating both topics (\"owning\" an apple device & app store stuff) on HN alone. Let alone everywhere else from reddit to governments. If it was so simply one-sided, there wouldn't be such lively debate. I don't think we need to rehash those thousands of comments to agree that the debate can't be settled in a single blanket statement? reply humzashahid98 4 hours agorootparentI don't really have the same impression that you have had when it comes to discussions about Apple's approach to HN. This linked comment (which I randomly found on HN's search) summarises what I've seen here: \"The way these discussions usually go is that a bunch of Apple users complain about the restrictions, a bunch of Apple users say they like them, and a few Android users like myself remind the complainers that there are, in fact, other options that don't involve forcing Apple to take away the rules that most iOS users appreciate having.\" https://news.ycombinator.com/item?id=39574040 I do recall a few users opposing allowing alternative app stores (which is the debate that is freshest in my mind) because they prefer to stick to Apple's, and the common rejoinder from the community that nothing is being taken away from those who prefer to stick to Apple's store but that other users who want something different have more choice. I'm still curious about what additional nuance you have in mind, since I honestly can't remember seeing it. reply ziddoap 4 hours agorootparentYou cannot summarize thousands of comments (there's nearly 4000 comments between [1] and [2] alone) in sentence or two and pretend like it is representative of the entire debate and everyone's opinion. Even so, your quote and further commentary proves there is more nuance to be had -- some Apple users are complaining, some Apple users are defending, others are suggesting compromise! The nuance can be found when you examine how those separate groups of people came to their opposing opinions. To say there is no nuance, yet give an example of a more nuanced argument is almost funny. While I have absolutely no interest in debating it, the example that first came to my mind regarding the \"Apple thinks they own every device\" comment was the nuanced debate around security, especially in relation to the target market of the devices. [1] https://news.ycombinator.com/item?id=39132453 [2]https://news.ycombinator.com/item?id=24146987 reply humzashahid98 3 hours agorootparentThanks; I didn't see that debate on security and I can see it being a valid point, especially with the burden of a technically-inclined person having to fix others' (family/friends) devices. I was trying to point to the opinion of the majority that I've seen and what I have seen from most of the community when security is mentioned is that Apple needs stronger security from a technical standpoint rather than controlling what and what is allowed on users' devices. I think both opinions hold validity, and you may be right that there is more nuance. reply jncfhnb 6 hours agoparentprevFirst time users saying “oh I guess this what I should use” reply resource_waste 5 hours agorootparentGood point, I'm not sure I've meet anyone leave Apple's Walled Prison. reply biftek 3 hours agoparentprevPro audio users aren't using the play button on the keyboard (the space bar is the industry standard to toggle playback), nor using bluetooth headphones (way too much latency to do anything meaningful) reply zimpenfish 7 hours agoparentprevProbably umpteen thousands of customer support queries about why Music now isn't starting when they connect their headphones after someone toggled it and forgot (or someone else toggled it and didn't tell them or an MDM profile toggled it or...) reply projektfu 6 hours agorootparentA quick review shows that Apple doesn't mind lots of questions if they don't bother answering them. How to keep a Mac Mini from going to deep sleep so it can be used as a little server? Answer: install Linux. reply RunSet 4 hours agorootparentTo keep a McBook from falling asleep when you close the lid, you need to buy and use a dummy HDMI connector that makes it believe it is connected to an external display. reply jeffhuys 6 hours agorootparentprevOr one of the dozen “prevent deep sleep” apps… reply throwuxiytayq 6 hours agorootparentprevJesus, that's such a feeble, dull, uninspired way to make product decisions. I'm squirming from second-hand embarrassment just thinking that some developer has to live this nightmare. reply zimpenfish 6 hours agorootparent> Jesus, that's such a feeble, dull, uninspired way to make product decisions. At Apple scale, though, it's probably necessary. reply knallfrosch 6 hours agoprevI remember my Macbook 2019 Pro for its bad hardware: horrendous touchbar and terrible ports (no HDMI, no magnetic charging, no USB-A..) But the software wasn't polished either: You can't use it in closed (\"clamshell\") mode, because it shuts down and you can't configure it. You need a 3rd party software to keep it on. But you also can't keep it closed because then you can't reach the fingerprint sensor. You see: iPhones can't use fingerprint recognition because they don't have fingerprint sensors, iPhones use facial recognition. But Macs do have fingerprint sensors, but can not use facial recognition (for example using an IR camera like Windows Hello) It's amazing how MacOS lacks features that everyone knows Windows has supported since XP days. (\"When you close the lid > Power down / Keep running\") reply macintux 6 hours agoparentLest anyone be misled: I assume the comment means that you can’t leave it in clamshell mode without an external monitor. I almost never open mine. There are three alternatives for the fingerprint sensor issue: - Apple Watch can replace most uses for it - One of Apple’s external keyboards has a fingerprint sensor - You can always just use your password reply bzzzt 5 hours agorootparent> One of Apple’s external keyboards has a fingerprint sensor Which only works on the M1 and higher models, not the 2019 Intel MacBooks. reply macintux 4 hours agorootparentGood point. I also didn't realize (or remember) the power requirement for clamshell operation; my laptop is being charged by the monitors it's plugged into. reply hedora 4 hours agorootparentThe “late 2019” intel MacBooks were some of the worst computers ever built: - I had two screens fail, - The battery swelled. - It did not sit flat on my desk. - It warped a pine desk that I used during the pandemic. - At any time after a few months of using it, no fewer than 6 keys would drop or duplicate keystrokes. - It got under 2 hours battery life, when new. - under heavy use, it got about 4 while plugged into the provided charger. - If you charged it using a port on the left side, it’d throttle the CPU to 10% normal speed. - Kernel panics on resume. - They had random monitor incompatibility issues with the LG monitors Apple sold at the time. - My colleagues also had the exact same model, and mine was better than average. - no escape key - the touchbar was over sensitive, so pressing backspace would send a ghost keystroke that launched siri a few times a day. However, you could run them in clamshell mode with the monitor plugged in. It worked out of the box. (Edit: I might have the model year wrong. I used it for a long time before the pandemic. It was the last one before they brought the escape key back.) reply rad_gruchalski 2 hours agorootparent> It did not sit flat on my desk. Because the battery was swollen? > - The battery swelled. Ah, of course. reply freedomben 2 hours agorootparentThat could also explain many of the other issues, such as poor battery life, slow charging, CPU throttling, etc. In general though, I have had a terrible experience with Apple's QA as well. I've had 4 Macbooks over the years and they all had multiple (usually different) things that really sucked and drove me insane. reply hedora 1 hour agorootparentThe battery swelling happened after the other issues (like not sitting flat on the desk). Also, my sample size is dozens of these laptops. My experience was typical. reply strictnein 3 hours agorootparentprev> Which only works on the M1 and higher models, not the 2019 Intel MacBooks. What? Oh man did I waste a lot of time trying to get that setup on my 2019 Intel Macbook. reply robrtsql 6 hours agoparentprevGiven that you mentioned the 3rd party software, you probably know this, but I think even the 2019 MacBook Pro supported working with the laptop closed--but it requires the laptop to be charging, otherwise it will just go to sleep. I, too, downloaded the 3rd party app (\"Amphetamine\"?) to allow it to stay on when closed and unplugged. As far as I can tell, it doesn't work at all. reply CoastalCoder 5 hours agorootparentCirca 2019(?), Amphetamine worked for me. No idea about currently. reply wrboyce 5 hours agorootparentThere is a builtin cli tool “caffeinate”. Prefixed to a command it will keep the machine awake until the command exits, on its own it will keep the machine awake indefinitely. reply resource_waste 5 hours agoparentprevWhy do people pretend people buy Apple products for its features. They are the best company of all time at Marketing. Can we just admit they are really good at mind control and humans are predictable? Maybe it hurts the egos of fans who spent $2000 on a mid CPU with 8GB ram, but the only people who benefit from this open secret are Apple stock holders. reply slater 4 hours agorootparentMust you show up in every Apple/macOS discussion to offer your tediously boring Apple hate? reply jpalawaga 5 hours agoprevit's shocking apple gets a pass on this. Not only for anti-trust reasons (it's giving Internet Explorer), but also that this is starting to remind me of all of the junk/bad settings packaged on Windows PCs eons ago. Honestly, in some ways, a clean windows install feels like it less junk on it than a fresh Mac install. Maybe soon people will be building custom MacOS install media like folks did in the XP days, to trim the fat and make the default settings non-insane. Another example of this is Pages, which INSISTS on being the default CSV viewer, no matter how much I check the \"use this application in the future box.\" My only alternative is to uninstall it completely. reply MatthiasPortzel 5 hours agoparentTo your last example: checking the “Always Open With” box in the right click > Open With menu only changes the default application for that one file. To change the default for all files of that type, you have to open the info panel with command+i, and under “Open With” click “Change All…” (Or, I use the command-line tool duti.) reply dehrmann 3 hours agoparentprev> anti-trust reasons Desktop computing is a dying platform, so it doesn't really matter. reply resource_waste 5 hours agoparentprev> a clean windows install feels like it less junk on it than a fresh Mac install. If you have the choice between windows and mac, go with Fedora. I imagine you arent trying to run any specific software, so go with something that is 10/10 in quality rather than a 6/10 user experience, 10/10 profit maximization. I specify Fedora because Debian-family linux sends people back to Windows. reply bangaroo 5 hours agoprevsuperfocused utilities to provide inexplicably missing functionality in macOS are a time honored tradition, but this may be the absolute peak of that genre of app reply weberer 4 hours agoparentMy nomination for that would go to AltTab. reply krferriter 4 hours agorootparentAltTab and Rectangle bring productivity on macOS up to an acceptable level. I assume anyone using macOS without them is in a constant state of frustration. reply fmbb 3 hours agorootparentI don’t use those tools and am not in a state of constant frustration. I use the Amethyst tiling window manager and multiple desktops. reply Brajeshwar 4 hours agorootparentprevI bought Rectangle Pro because I liked Rectangle. The Pro came with the added benefit of assigning multiple custom window sizes easily without tinkering with the obscure Keys mapping. reply fingerlocks 3 hours agorootparentprevCommand backtick cycles windows by app. Long-press green window dot while holding option for window sizing control. Not frustrated, hust rtfm. reply nolongerthere 4 hours agorootparentprevSomeone gifted me a MacBook a few years ago, I was floored at the usability nightmare the OOTB MacOS experience is. Especially after being inundated with nonstop articles about how superior Macs are by every single tech reporter/blogger my entire childhood. I assumed MacOS was the pinnacle of OS UX, a very big letdown to learn it was far behind Windows in countless ways. reply bangaroo 3 hours agorootparenti mean, as a long-time user who's experienced a ton of ups and downs in the the macOS world, defected to linux briefly, and returned... it is still the best OOTB experience of any computer i've ever owned. the windows experience of 2024 is very similar to the experience of buying a gateway 2000 in the 90s to me. there's tons of bloatware, ads all over the OS, you're constantly being prompted to buy Onedrive and Office (what the hell?) a lot of the Things i've gotten used to on macOS that windows doesn't have, I miss a lot (consistent shortcut keys across apps fro all vendors, the speed and reliability of spotlight as an app launcher/switcher, the ease of previewing files in the OS without launching separate applications, the fact that for all intents and purposes you're in a *nixlike environment and have basically the full suite of tools available to you first class.) every time I go back to my windows 11 desktop after an upgrade I'm baffled by some other weird addition to the OS that I didn't ask for and don't want. In a way, I kind of appreciate that macOS tends to evolve slowly and doesn't do a lot of experimentation. When there's multiple valid ways of solving a problem, they'll leave it to utilities to provide those options. I kinda am okay with that. I used to think apple was extremely opinionated but it seems more and more like their philosophy with macOS is \"leave it to the users to solve this problem.\" i'd also argue there's a ton of windows features missing, which extremely confusingly i have to add manually by installing powertoys (which means microsoft actively builds the solutions to these problems and then just chooses to distribute them in this bizarre, out-of-band way? it makes no sense.) I won't deny there's a bunch of obvious stuff that i cannot understand - window snapping being a key thing, they've gone all in on their weird full-screen approach, which I don't vibe with. The fact that I need to buy an app (bartender) to hide icons in my menu bar in the year 2024 is pretty bonkers to me, and I cannot understand what motivates the complete lack of management tools for this, especially given how out of control it can get on a company-managed machine. finder is in need of a lot of love across the board. i'm also baffled by some of the stuff they choose to include - mission control is one of the single most confusing features they've ever added to the OS and i still have zero understanding of what the intent behind it was. i guess all this to say i feel like having a lot of experience with macOS, i much prefer opening a clean mac to opening a clean windows machine (linux obviously is its own can of worms where it can't really be evaluated by these standards) and i can't at all agree that UX-wise it's a letdown. reply talldayo 3 hours agorootparent> and i can't at all agree that UX-wise it's a letdown. ...even when we're in a thread, discussing the lack of toggle-setting for an annoying default? MacOS deserves some credit; Apple hasn't whittled away it's features too far yet, and the UI is still fairly nice from an outward-facing perspective. They focus well on keyboard-shortcuts and do a generally good job focusing on getting the basic windowing and desktop presentation right. Really though, if you give me the choice between a Windows machine and a Mac I'm rapidly approaching the point I'd only take the Windows machine. Using MacOS, for development purposes in 2024, is a nightmare. Docker doesn't work; Homebrew is hanging on by a thread and acts different on different architectures; the Mac default coreutils play musical-chairs when they're outdated; basic APIs and cross-platform frameworks (eg. OpenGL) were forcibly removed in favor of insular non-replacements. Unless you're an absolute apologist for proprietary software solutions it's just so hard to say MacOS is a particularly great user experience today. For christsake, you type git into the terminal and get a modal asking you to download the 5gb developer toolkit - it's absurd. Part of the problem with setting good defaults is that you're also expected to provide meaningful alternatives. That's the thing about \"default\" options - they implicitly suggest the existence of another possible option. Apple seems to ignore this, and the only interpretation I take away looking at their devices today is that Apple is chronically afraid of their users discovering that they don't need Apple or Apple services to enjoy their Apple hardware. reply bangaroo 2 hours agorootparentI mean I feel like my experience is completely contradictory to yours, and while I can't necessarily understand why, if I was having the experiences you were having I would also be frustrated. I use docker daily in development - I have little to no issue with it, it's basically transparent and I have zero complaints about its performance or behavior. Perhaps I'm not doing things as complex or unique as you are? I don't really know. I have had no problems with Docker whatsoever, bar the initial migration onto the M-series chips that was clunky and awkward for several months (I was pretty much exclusively on linux, personally, until deep in this process.) Since then it's been A-OK for me. Homebrew, similarly, has just been transparent for me - I had a clunky initial 12-months-or-so and at this point I simply don't notice that I'm on a different architecture any longer. I had a lot of complaints early in the migration to apple silicon, at this point problems at least for my use case are solved. As for the system-level APIs, yeah - I can't speak to that. I do web development primarily professionally, and then I spend a substantial amount less time mucking around locally, mostly on stuff that runs in the terminal or as a service. I do not have experience with (nor do I use) OpenGL or other whatever other basic APIs that you're referring to - I haven't really had an issue with that (other than Apple's extremely half-hearted migration into SwiftUI which feels hacky and half-finished.) I absolutely agree that every time I type git for the first time I'm startled by remembering that I have to download the command line dev utils to get rolling with that, I wouldn't necessarily say that I agree this is indicative of a larger problem? The machine doesn't ship with git by default but you absolutely can install git outside the command line utils path if you want to (to the best of my understanding,) that's just the default behavior to unblock you in the event you just want to Get It Done. My personal experience right now really doesn't reflect yours, and transparently I'm not sure why. I've never felt like apple's pushed any of its specific services on me aggressively, but I've also not used a machine where I didn't have some small subscription to iCloud on my account (I use it for some backups, moving stuff between my phone and laptops) and so it's entirely possible I've just never seen that portion of the experience - I may just be blind there. I just genuinely haven't felt that railroaded by the OS. I've never been using chrome and had a popup at the OS level ask me if I want to try safari, for example. I've never opened my laptop and found that suddenly there's some weird LLM-powered assistant on my taskbar that I cannot remove without a registry change. I've never installed a system update and been taken through an onboarding flow that tried to upsell me into an iWork (or whatever it's called these days) subscription and buying more space in iCloud. The stuff I really rely on (like spotlight) consistently works, and works well. I just don't find the experience as miserable as you're describing. Maybe I just am a person who is perfectly comfortable with apple's defaults and that is my blind spot, not sure. reply dpcx 3 hours agorootparentprevHaving never heard of AltTab, I looked it up. I'm still not sure what it does that Cmd+Tab doesn't do. reply ocrow 3 hours agorootparentIt's quite configurable, allowing different key combinations for different window arrangement functions, but the most useful one is that it allows you to bring a single window from another application to the top without rearranging the rest of the stack. This is useful if you are switching back and forth between windows from two different apps each of which has several other windows open, for example a browser and a text editor. reply weberer 2 hours agorootparentprevThere is no way to map Cmd+Tab to Alt+Tab in MacOS. Which makes for an extremely frustrating experience in muscle memory when alternating between your work Mac and your personal Linux/Windows machines. This was the biggest grievance to me. But the Cmd+Tab behavior is also incapable of switching between multiple open windows of the same program. So if you have two Firefox Windows open, you can't switch between them with Cmd+Tab. Cmd+Tab also has the annoying behavior of listing programs that are \"running\" but do not have any open windows. I still can't for the life of me figure out how this would be useful to someone. reply biftek 3 hours agoparentprevAs opposed to what? Linux barely has functioning wifi, bluetooth, audio, and video, and Windows also has countless utilities to decrapify the OS. It's not specific to Mac just a different flavor. reply freedomben 2 hours agorootparentHow long has it been since you used a Linux distro? Fedora has had all those things functioning out of the box for many, many years (at least 10+ for wifi, audio, video, 5+ for bluetooth), as has plenty of other distros. And if you think you have to decrapify your linux install, then you're probably using the wrong distro for you. There are plenty (most) that don't crapify. reply bangaroo 2 hours agorootparenti don't agree with most of the above comment but they did specifically call out windows as the OS that needs the crap extracted reply culi 3 hours agorootparentprevAgreed. I see a lot of people upset at Apple coming up short on many basic things, but I was a Windows user for decades and found that to increasingly be the case with Windows as well. I wonder if people simply have higher expectations from Apple because of its \"polished\" presentation. Windows is known to be less polished but (used to) be more easily customizable and less of a walled garden (imo all that's changing now so there's really no reason to use Windows any more). People are cool with Windows being less polished because its understood that you're supposed to make it your own. After ClassicStartMenu and a number of other important utilities being flagged by Windows Defender or simply not working anymore and after start bar advertisements and un-overrideable keyboard shortcuts to open the Windows Store I didn't see what Windows had left to offer besides legacy compatibility reply nottorp 7 hours agoprevNow I need one to prevent automatic clipboard sharing via handoff without turning off handoff completely. Believe it or not Apple, some people use multiple machines for different purposes and automatically passing my clipboard between them is an annoyance at best. I'm happy with it just being shared while I explicitly remote from one machine to another. reply ljm 6 hours agoparentI think you just need to disable Universal Clipboard? reply nottorp 5 hours agorootparentHow? https://support.apple.com/en-us/102430 Apple only tells me how to turn it on :) reply ljm 4 hours agorootparentIIRC there is a flag in MacOS settings to toggle it. It’s always been disabled by default for me. reply elzbardico 5 hours agoparentprevI went back to Synergy from Symless. Handoff has some very annoying glitches when sharing mouse and keyboard. And Synergy is completely configurable. Not to mention that it is a one-time purchase. reply threecheese 4 hours agorootparentA one time purchase, but they update it quite frequently, and each time we are required to open browser, log into Synergy account, find the download page, get the .dmg, install… to remove the UI nag - on each computer using Synergy. Please PLEASE make an auto-updater :) I think I had to do this twice in one week recently on three computers, with a strong password I had to write on a post-it reply jwells89 5 hours agorootparentprevCan’t speak for others but the reason I use Universal Control is because it’s the only option that doesn’t perform badly without a hardwired connection, thanks to its use of an ad-hoc wireless network to send mouse movements and keystrokes whereas Synergy, ShareMouse, etc make a round trip through the router adding a bunch of variable latency, which IMO is worse than Universal Control’s glitches. reply gregoriol 6 hours agoparentprevOh yes! That would be awesome! I often happen to erase something I copied on one device by copying something on another one... would be much better to have an AirDrop like feature where you can send a text to another device instead of automatic reply boxed 6 hours agorootparentThe fact that macOS doesn't have a built in history for the clipboard is the problem there imo. I use pasteBot and I think macOS is broken without it (which of course means iOS is broken and cannot be fixed by a third party). reply nottorp 6 hours agorootparentI use JumpCut. I still don't want my stuff on one machine erased via the other unless i specifically connect them via vnc. reply jeffhuys 6 hours agorootparentprevIt can. In the EU. AltStore -> Clip. No jailbreak, clipboard manager. reply boxed 3 hours agorootparentThe blog post about it seems to imply you have to do an extra swipe down to put it in the clipboard manager for every copy... reply manmal 6 hours agoparentprevMaybe turning it off and using Paste app instead is worth exploring. reply nottorp 5 hours agorootparentTurning what off? Handoff? But I like the feature where my text messages and voice phone calls can be done on all my devices. I just want to turn off the clipboard. reply Brajeshwar 6 hours agoprevlaunchctl unload -w /System/Library/LaunchAgents/com.apple.rcd.plist Does this not solve for all activities, such as from pressing the right buttons by mistake? reply __alias 6 hours agoparentunfortunately this disables the media keys completely -- the point of the linked software is it still allows it for media players intended -- such as spotify; and blocks the apple media player from hihacking reply nicce 6 hours agorootparentI have just remapped everything with Hammerspoon. You shouldn’t need to do this but it works… reply semi-extrinsic 3 hours agorootparentCan you remap the entire keyboard so it's actually an ISO layout? I've been using Ukelele for this, but it's not optimal so I'm going to switch. Was considering Karabiner Elements, but i like Lua so maybe Hammerspoon is better? reply nicce 3 hours agorootparentIt should be possible, but it will take some lines. Maybe helpful: https://github.com/hetima/hammerspoon-foundation_remapping/t... https://github.com/RobotCharlie/key-remapping-hammerspoon/bl... https://github.com/Hammerspoon/hammerspoon/discussions/3246 About macOS in general if you use hidutil https://hidutil-generator.netlify.app/ https://rakhesh.com/mac/using-hidutil-to-map-macos-keyboard-... reply db3pt0 5 hours agoprevAh, this is a nice find! I've used Santa[1] to accomplish this with a silent block, which always felt like overkill when I was using it for solely that purpose. 1. https://github.com/google/santa reply nico 5 hours agoprevNow I want this for my phone When I plug it into the car, it automatically starts playing the same song on Apple Music I want it to start playing Spotify instead, but that can’t be configured Either I turn off CarPlay altogether, or I let it do what it wants reply philo23 4 hours agoparentI vaguely remember reading about this issue in the past, it’s actually down to the way the software running in the cars radio is using a backwards compatible “iPod” integration. It effectively (against Apple’s recommendations) sends a “play music” signal as soon as you connect your device. Well behaved radios don’t send that signal immediately, and instead wait for you to hit play on your device, or on your cars radio/stalk. reply mnw21cam 4 hours agoparentprevNothing to do with Apple, but I recently bought an old car. It has a radio. Whenever I turn on the engine, it switches on. Actually, no, it doesn't switch on, because I found that behaviour so annoying that I just unplugged the whole thing from the power. It's a fancy sat-nav touchscreen radio that can show a reversing camera, has loads of settings menus, but doesn't have an option to stop it being stupid. What gives? Who in the industry thought that this was what people really wanted? reply CharlesW 4 hours agoparentprev> When I plug it into the car, it automatically starts playing the same song on Apple Music My car (maybe most?) helpfully sends a \"play\" command when my phone connects via Bluetooth. iOS devices play what was last playing, or pass the event to your music app if you've played something else in the meantime. The fix in my case was a Shorcuts automation that gets triggered when the phone connects to your car. I set the volume to max and tell my favorite podcasts app to \"Play\". If I just wanted an app to always be considered the \"last played\" app, I'd do something like add a one second delay and then pause. If I used Spotify, I'd also delete Apple Music. reply cnity 4 hours agoparentprevReminds me of this gem: https://news.ycombinator.com/item?id=32163667 reply hedora 4 hours agoparentprevThat sounds like a bug. Mine autoplays tidal, overcast or nothing, depending on what I was listening to last. I even trained siri to use tidal by saying “play artist on tidal” a few times. (Siri is completely useless for this use case, but it does not open apple music.) reply can16358p 6 hours agoprevThe need for such an app is beyond stupid on Apple's side, and I'm saying this as an Apple Music user. I am an Apple Music user and I have Sennheiser headset that connects both to my iPhone and Mac. Great, no problem up to here. However, when I, say, browse Instagram on my phone, after closing it on my phone, Apple Music opens and starts playing. Every. Single. Time. I don't want a software that prevents Apple Music launch, I don't want to delete Apple Music (even if it was possible) as I sometimes DO use Apple Music on my Mac, I just don't want to to open and start playing the topmost song in my library on my Mac every single time after any inline media on Instagram stops playing on my iPhone. This bug has been present for many macOS and iOS versions for many years, and Apple does nothing about it. After reading it again, I think \"beyond stupid\" is an understatement. Unbelievable. reply devilkin 6 hours agoprevEvery time I switch off my bluetooth Plantronics headset Apple music starts. Why? Because Apple. Another clear sign Apple doesn't really care about it's users, it really cares about the bottom line, and getting people to sign up for their services. reply danhau 5 hours agoparentTo be fair, this could be the headset. Neither iOS nor macOS does this with my bluetooth headset. macOS IO is broken in other ways for me though. It doesn‘t wake up my external monitor after waking from sleep (lid closed, powered, external keyboard). reply koromak 5 hours agorootparentI have my lid open with my externals, I never seen my monitor alone fail to wake, but sometimes all displays refuse to turn on, even the laptop built in. Doesn't happen every time, but maybe 1/10 tries reply ac50hz 2 hours agoprevThis could be a useful alternative: https://github.com/quentinlesceller/macmediakeyforwarder reply mrweasel 3 hours agoprevThat really don't happen to me. Now if it could rip out dictation support that would be wonderful. It happens multiple times a day that macOS pops up \"Would you like to enable dictation\"... No, I would like to never ever talk to my computer. reply tmaly 1 hour agoprevCan we get an iPhone version? I have the same exact problem. reply jrmg 5 hours agoprevWhat’s the technical reason for the launch? Lots of people report Music launching when they connect a headset, but it doesn’t happen to me with my Mac and headset. Do some headsets (like many cars…) issue a ‘play’ command to the device after connection? Not trying to argue that Apple shouldn’t provide a way to stop this - just curious about why it happens at all. reply garciasn 5 hours agoparentMy phone immediately starts playing some song in Apple Music that I haven't listened to in literal years every time it connects to my car's BT. I went through and did whatever it is that's supposed to stop this...but, it doesn't work. So, I sit there listening to 4 seconds of Beck while cursing each and every day. reply jrmg 5 hours agorootparentFWIW, it’s the car that’s issuing the play command. I don’t understand why cars do this. Doesn’t make it less annoying than it being the phone’s ‘fault’ of course. I did manage to stop this by making a shortcut that pauses media after a Bluetooth or CarPlay connection to my car. Sometimes get a second or two of whatever’s playing, but other than that it works. reply kps 4 hours agorootparentprevApparently people work around this with an AAAAAAlbum containing 8 hours of silence. reply jwells89 5 hours agoparentprevI do think it’s something specific to some models of Bluetooth headphones. AirPods don’t do it, and while it’s been a while since I’ve used my Sony XM4’s with a Mac I don’t recall them doing it either. reply vundercind 4 hours agorootparentAll I can figure is some send a “play” command automatically when they connect, and some don’t. reply mateus1 5 hours agoparentprevFrom my personal experience, macOS will default to Apple Music even when there is a Spotify song on pause or a YouTube video open. They’re using “default to” as an effective nudge mechanism towards their moat. reply corney91 4 hours agoparentprevYeah, I think some headsets issue Play when connecting. I had a frustrating moment yesterday on a Macbook: - I was connected to a video call with Bluetooth headphones. - It turns out the headphones were also connected to my phone at the same time, because it started ringing via the headphones and I couldn't hear anything on the video call. - I rejected the call. - The headphones attempted to resume what I was doing by sending the Play command to the Macbook. - iTunes then opened covering up my video call window. - I tried to close with CMD+W, iTunes didn't let me because I wasn't logged in and apparently the login window isn't allowed to be closed by itself. - I manage to close the iTunes window and try to remember what I was saying on the call. This was all due to terrible decisions by both Apple and Sennheiser imo. reply neontomo 4 hours agoprevI had this app for ages but never set it up correctly. Protip: install via brew, change defaults to open spotify instead and hide icon. Instructions here: https://github.com/tombonez/noTunes reply robszumski 3 hours agoprevPlease do Xcode next! reply mdaniel 2 hours agoparentwouldn't $(sudo mv /Applications/Xcode.app/Contents/MacOS/Xcode /Applications/Xcode.app/Contents/MacOS/Xcode.nope) do that, while leaving the actually important SDK bits intact? I hypothesize `sudo chmod a-x` may work, too, but I don't actually know how much the .app bundles care about unix perms reply eagerpace 5 hours agoprevWill it also permanently delete that silly U2 album that keeps coming up too? reply sneak 6 hours agoprevIf you download the free Apple Configurator 2 app, you can create a provisioning profile that disables Apple Music, too. It takes only a moment to install it on the local system, no MDM required. reply aldousd666 3 hours agoprevI need one of these for the news app next. reply pawelduda 6 hours agoprevOh yes, one of my favourites reply stevage 3 hours agoprevSo glad to discover this, thanks! reply pcdoodle 6 hours agoprevI just found out I can't remove the Music app from my own computer (Mac)... reply jeffhuys 6 hours agoparentYou tried nothing and all out of ideas? reply sneak 6 hours agoparentprevYou can disable all boot security and system integrity protection on a Mac and edit the filesystem however you would like. reply PaulHoule 6 hours agoprevReminds me of the last six months before I switched from JPEG to WebP on my web sites. I still had a 2013 Mac Mini that didn’t support WebP in Safari because Apple hadn’t built WebP into the system libraries and they chose not to support my machine. That was enough reason for me to not make the switch (any economic benefit is diminished if you have to keep two copies of each image.) I felt so infantilized because something that is just a userspace thing is locked to the OS. To their credit, Apple did eventually enable WebP for those mac’s but I hate being dependent like that and having to develop software for people who are likewise dependent. reply zimpenfish 7 hours agoprevAh, helpful. I've had to stop using my Bluetooth headphones with the laptop because you can't stop Music from launching. Infuriating that macOS doesn't have a toggle for that behaviour (but I can understand why.) reply jwalton 6 hours agoparentI'm sorry, I feel like I'm missing something here. I have an M1 Macbook Pro running Sonoma 14.1.1 (I need to update...), I have a nice pair of Sony Bluetooth headphones. I turn my headphones on and off multiple times a day, they connect to my Mac, Apple Music doesn't launch. What are you all complaining about? Edit: Just tried connecting my 3M Worktunes, and they didn't launch anything either. reply sbuk 2 hours agorootparentI’m beginning to think the same. Can’t reproduce with Apple (including Beats) or Sony. Would be interesting to know which headphones do exhibit this behaviour. This seems to me to be a headphone manufacturer SNAFU. reply kayodelycaon 6 hours agorootparentprevThere are some Bluetooth audio devices that send the play command when connected. I had a car that did this. It was absolutely infuriating. reply TillE 6 hours agorootparentprevYeah I can't reproduce this with my AirPods Pro either. Simply connecting does nothing. I would assume something is causing a stray input as if you'd pressed the button on your headphones, which can launch Music. reply macintux 5 hours agorootparentI've definitely seen it launch as I put my AirPods Pro into their case (but not consistently), so maybe it is something with the pressure-sensitive stalks. Thanks, I hadn't considered that possibility. reply dukeofdoom 5 hours agoprevSo how do I prevent iTunes starting anytime I want to quickly listen to a music file. Its super annoying since it takes a while, pop ups, and i have to right click to close it. Preview doesn't work for some reason in my system. And i just want to play the track for a couple seconds to know what it is, then move on to the next one to find what I'm looking for. Itynes takes like a minute. And I hate it when apps ignore the x button from closing them. reply jrmg 5 hours agoparenthttps://support.apple.com/guide/mac-help/choose-an-app-to-op... Look at the “Permanently change the app used to open all files of a particular type” section. Or just never open them if it fits your workflow - use Quick Look: select the file, press space, play. reply tootie 6 hours agoprevBeen using this for years. It is great. reply sangupta 4 hours agoparentI discovered noTunes a few months back and it is one of the best installs of all time. It also goes to show how product/developer thinking not always aligns with how users want to use the product. And then, I start thinking why would a developer build such an annoying thing... unless marketing forced them to. reply anArbitraryOne 6 hours agoprevYeah wow what a great operating system that needs a whole other program to keep a particular one from launching. I'm totally going to recommend MacOS to my mother, aunt, uncle, cousin, cybernetic dog and everyone else for the great user experience of being forced to do what apple HQ wants reply manmal 6 hours agoparentWill you be recommending Linux instead? Or Win 11, with ads galore? My question is only half joking, I‘m not aware of a good low maintenance alternative. reply michaljanocko 6 hours agorootparentI installed XFCE desktop on Manjaro (I believe) on my grandma's computer that I bought second-hand a couple years ago for about $60 (i5 6th gen, 8 GB RAM, 120 GB SSD). There have been zero issues. Windows was a nightmare because it was constantly breaking or needing an update and Macs would be wasted money in this case. The usage is pretty light: downloading and sharing photos from a camera, YouTube, email, news, some light spreadsheet accounting, and that's about it. Neither of my grandparents are tech-savvy (my grandpa still uses a feature) and yet they both learned all, that they need, in the afternoon I installed the computer. Zero regrets going for XFCE (or anything that can be stripped down just to the features they need) and buying a cheap computer. Going Windows → Manjaro/XFCE reduced tech support calls to me from 1–2× a month to once per year. reply CoastalCoder 5 hours agorootparentSimilar experience for me with my mom. I set her up with a desktop machine running Pop_OS! with automatic updates. I did some minor hackery to get Windows' FreeCell working on it because she cares a lot about that. I installed tailscale for remote support, but haven't had cause to use it. In general it's working fantastically. The only issue is that the USB WiFi adapter drops out rather frequently. So my one hardware concession will be to replace it with one known to be more reliable under Linux. [0] [0] I know sometimes this can be fixed by choosing a more appropriate driver and/or Bluetooth device settings, but it's not something on which I want to spend a lot of time or headspace. reply MatthiasPortzel 5 hours agorootparentFor most people, reliable WiFi is more important than Apple Music not opening when you hit the play button. reply freedomben 2 hours agorootparentprevWhen XP finally got killed, I set my dad up on Ubuntu LTS with MATE and he ran that way for many years with no issues. Occasionally hardware would fail (like the soundcard) which I would replace with a USB option. All in all though it was great for him. A couple years ago Ubuntu started demanding a \"Pro\" account/license (which shouldn't have been needed, it was the most recent LTS at the time) and making updates a pain for him, so I switched him to Gnome on Fedora (same setup I use) and it took a few minutes of orientation, but now he absolutely loves it. Absolutely I would recommend Linux instead. reply trustno2 6 hours agorootparentprevIt's GNU/Linux. And Copilot/Windows. reply globular-toast 3 hours agorootparentprevMy Gentoo GNU/Linux system is lower maintenance than this. If I don't want an app to launch I simply remove it and it's gone forever. It's a super-basic system that only does what I want and is therefore really easy to maintain. reply myko 6 hours agorootparentprevI set my mom up with chromeOS and haven't looked back reply trissylegs 4 hours agoprev [–] Another frustrating behavior form Apple is: * My macbook will leave bluetooth on while it's closed and asleep. * My BT headphones will connect to my phone and laptop if they're both on me. (It supports 2 simultanous connections) * If I play music or podcasts I can pause through the headphones * If I try to play it again it the \"Play\" action goes the macbook. (Which is effectively off so nothing happens) * I now cant play anything on my phone. Hitting play on the app will fail. * If I open the macbook now it will open Apple Music * Otherwise I can hit the play/pause on my headphones then hit play on my phone it will work again. What a bizarre useless behaviour. Why not just turn bluetooth OFF when my laptop is asleep. (It will also remain connected to wifi hotspot prevent my phone from auto-disabling wifi hotspot which has caught me a few times) reply DrammBA 4 hours agoparentHello, I also hate that bluetooth stays on while my mac is sleeping, a few years ago I found this utility to fix it: https://github.com/odlp/bluesnooze/, my only issue with it is that if I sleep and wake the laptop in quick succession sometimes bluetooth stays off, but I can live with that. reply ohhai 4 hours agoparentprevFix this with bluesnooze: https://github.com/odlp/bluesnooze reply CharlieDigital 4 hours agorootparentAs a MBP user (for dev because the hardware is so good), it annoys me to no end that macOS comes up short in so many basic ways that requires patching with so many third party tools to make it not annoying. Even basic window tiling is just terrible and requires a third party add-on to make it functional. reply inanutshellus 3 hours agorootparentEvery Apple user has this complaint at some point with some feature, and either buckles and just accepts the Apple way or, very very rarely, will go to the effort of installing something to deal with it. Apple's foundational ethos is that they pick a single implementation and run with it, only making it configurable if held at gunpoint. reply bombcar 3 hours agorootparentWhich if I'm honest I'm almost always fine with, as long as the way they ran with is polished as hell. It's when the way they obviously want you to do things is flakey and doesn't work that it becomes super annoying. reply nrvn 3 hours agorootparentprevAs a long term apple customer I would be happy to throw away out of the window every single device and their entire walled ecosystem in face of a decent alternative. Unfortunately (or thankfully, depends on standpoint) the benefits outweigh nuisances. reply ryandrake 3 hours agorootparentI feel the same way. When I look at my aging Apple computer(s) I think about all the annoying OS choices that they made that are difficult or impossible to configure away or work around, and I tell myself, \"That's it. My next computer will not be Apple!\" But, then I look and see the shit that every other manufacturer offers and I can't believe how bad it all is. The hardware is mostly cheap, cheesy, flimsy, plastic garbage, and the software choices are 1. Windows which is so intrusive and user-hostile that I consider it malware and 2. Linux which has been perpetually \"will be ready for prime time about 2 years from now.\" The last time I upgraded Debian on my home lab server, it failed and I had to boot in single user mode to carefully fix it so it would even boot. Fortunately, Apple's hardware almost always 2X+ outlasts its software support, so I won't have to make an actual buying decision for many years. reply gumby 3 hours agorootparentWhen my friends ask why I choose macbook air & iphone my explanation is that they simply \"suck less\". Pretty sad when that's the best way to choose. reply Grimm665 4 hours agorootparentprevAgreed, the Amethyst team is doing saintly work over there. reply stevage 3 hours agorootparentMan I really tried to like Amethyst, but I just...hated it. BetterSnapTool works well for me. reply Grimm665 6 minutes agorootparentI loved BetterSnapTool, but I'm glad my workflow has simplified to the point where allowing my windows to be subjected to the tyranny of a dynamic tiling window manager actually made sense, and now I feel more at home than ever! reply jachee 3 hours agorootparentprevI used Magnet before Big Sur, but have actually since switched to using Stage Manager. With multiple screens, I find that I seldom need more than two active apps simultaneously, and it actually helps me manage my focus better. reply CharlieDigital 3 hours agorootparentIt really depends on the size and resolution of your screens and use case. If you're using wide, high resolution screens, I really want to be able to place multiple windows in different arrangements on one screen for app development. I find myself constantly shuffling windows around on my setup. reply stevage 3 hours agorootparentprevYeah, I thought so too, but I find the exact configuration of which windows I want open and how big I want each one does change a lot. reply throw03172019 4 hours agoparentprevThat is a frustrating experience for audio. However, I do enjoy tapping my external BT keyboard / trackpad to wake my MacBook while it’s in clamshell mode connected to my monitor. reply semi-extrinsic 4 hours agorootparentIf only there was a way for the OS to distinguish input devices from output devices... reply mikestew 3 hours agorootparentThey would have to have categories for Bluetooth devices such as human interface devices like keyboards (let's call them \"HID\"s) and audio devices. I dunno, seems like a lot of hard-to-implement monkey business. reply secstate 3 hours agorootparentLinux definitely does this. What different DEs do with it is sometimes suspect. But BT keyboard definitely wakes my GNOME environment, while it does not stay connected to my headphones. reply freedomben 3 hours agorootparentI can't say with 100% certainty, but I'm highly confident that GP is mocking Apple's implementation. HID has been a standard for a long time and is already widely available on every platform. reply mikestew 3 hours agorootparentOh, I am absolutely 100% mocking Apple's implementation. I have a house full of Apple gear, but that doesn't prevent me from shaking my head from time to time when I come across boneheaded stuff like this. reply Gigachad 3 hours agorootparentprevI wish I could turn this off. I keep bumping the mouse and waking my desktop which then takes a while to suspend again. But for some reason it’s not configurable. reply acrux_ 3 hours agorootparentprevLike the categories they use to show a headphone, keyboard or mouse icon next to a device in the Bluetooth menu? reply freedomben 3 hours agorootparentyes, but with some method for programmatically reading the icon so they can know what type it is reply mikestew 3 hours agorootparentMachine learning would make that task fairly trivial. Much like the TV show Silicon Valley, but instead of \"not hotdog\", \"not headphone\" and then treat it like a keyboard. reply aclimatt 4 hours agoparentprevThis behavior is extremely frustrating. I have a pair that only supports one device, and on the occasion that I connect them to my MacBook and forget to unpair or turn off Bluetooth afterward, they will permanently be usurped by Apple Music -- computer awake or asleep -- and my phone never stands a chance. reply freedomben 3 hours agorootparentI actually think Apple is the reason why newer headphones are starting to support two devices. Apple makes a decision for themselves and it forces entire industries to move that direction. As someone who hates the Apple approach (please give me the Woz machine: more ports, more open, more hackable, more mine), it is painful to watch. Yes I'm still bitter about headphone jacks :-D reply noname120 2 hours agorootparentWhat if you have a personal MacBook *and* a professional MacBook? Well they occupy the two available seats from the “multi-point” system of the Bose QuietComfort 45 and my issue is not solved. reply freedomben 2 hours agorootparentIndeed, that's a good point. And increasingly is the case for people. reply avree 3 hours agoparentprevThieves in San Francisco take advantage of the first one to check if there are goods in cars. reply iknowstuff 3 hours agorootparentWhoa how do we know about this? reply Gigachad 3 hours agorootparentprevHonestly I doubt they bother with that when they can just randomly smash open 10 cars to check them in the time it would take to do a Bluetooth scan. reply avree 2 hours agorootparentThe \"time\" it would take to do a Bluetooth scan? What are you talking about? There are free apps that will instantly show Bluetooth signal when walking past. reply xrisk 2 hours agoparentprevHuh I’m glad I have AirPods, they work perfectly in this scenario. One of those Apple ecosystem things I guess. reply mdavid626 3 hours agoparentprevExactly. Thank you. Apple please fix this. I buy 300$ headphones and >2000$ laptop and can’t listen to music. reply sbuk 1 hour agorootparentOut of interest, what is the model of headphones? Asking as I’ve not seen this with Beats, Apple or Sony. reply xenophonf 3 hours agoparentprev [–] I've gotten in the habit of explicitly disconnecting my headphones from all but the device I'm actually using, typically my phone. I haven't had these kinds of problems since, but what I gain in control, I lose in flexibility. Typical Apple. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "noTunes is a macOS app that prevents iTunes or Apple Music from launching, with a new update (version 3.2 or later) required due to an expiring certificate.",
      "The app can be installed via Homebrew or direct download, and it offers features like launching at startup, toggling on/off via the menu bar, and setting a replacement app or website.",
      "noTunes is available under the MIT License, making it free and open-source for users and developers."
    ],
    "commentSummary": [
      "NoTunes is a macOS app designed to prevent Apple Music from launching automatically, catering to Spotify users frustrated by Apple Music's default behavior.",
      "The discussion reveals broader frustrations with Apple's ecosystem, such as automatic app launches, intrusive features, and limited user control, leading to issues like unwanted autoplay and data sharing concerns.",
      "Users also discuss hardware preferences, recommending alternatives to Asus and HP laptops due to durability issues, and express mixed feelings about macOS, Windows, and Linux, highlighting macOS's lack of built-in clipboard history and poor Bluetooth device management."
    ],
    "points": 278,
    "commentCount": 233,
    "retryCount": 0,
    "time": 1716288150
  },
  {
    "id": 40423082,
    "title": "Chameleon: Meta's Breakthrough in Mixed-Modal AI Integration",
    "originLink": "https://arxiv.org/abs/2405.09818",
    "originBody": "Computer Science > Computation and Language arXiv:2405.09818 (cs) [Submitted on 16 May 2024] Title:Chameleon: Mixed-Modal Early-Fusion Foundation Models Authors:Chameleon Team View PDF HTML (experimental) Abstract:We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents. Subjects: Computation and Language (cs.CL) Cite as: arXiv:2405.09818 [cs.CL](or arXiv:2405.09818v1 [cs.CL] for this version)https://doi.org/10.48550/arXiv.2405.09818 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Armen Aghajanyan [view email] [v1] Thu, 16 May 2024 05:23:41 UTC (26,721 KB) Full-text links: Access Paper: View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CLnewrecent2405 Change to browse by: cs References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=40423082",
    "commentBody": "Chameleon: Meta’s New Multi-Modal LLM (arxiv.org)277 points by gabrielbirnbaum 17 hours agohidepastfavorite39 comments krasin 14 hours agoRelevant thread on /r/locallama ([1]). A relevant quote from the comments: > There's a Twitter thread from one of the authors ([2]). This part seems pretty important: \"The models in this paper were done training 5 months ago. We've progressed significantly since then.\" 1. https://www.reddit.com/r/LocalLLaMA/comments/1ctsala/newly_p... 2. https://x.com/ArmenAgha/status/1791275549815648473 reply themagician 2 hours agoparentThanks for sharing! It's encouraging to see the authors actively improving their models post-publication. Exciting to witness science in action! reply vessenes 14 hours agoprevThere’s some pretty nice fundamental research in here, and I appreciate the publication very much. What stood out to me is their discussion of the difficulties of using softmax against different tokenization spaces; super interesting analysis (they say different modalities compete by upping their own strength relative to other modalities, leading to divergence), and the ultimate fix, (I can’t remember it right now, and leave this as a tease to the interested paper reader). They also noted the problem was most pronounced once they got up to 34b sized. It’s a good reminder training large scale models leads to new interesting problems. I imagine a lot of techniques and know how are not published; all those little bits of experience add up to a lot of competitive advantage in many venues, so once again, thanks to Zuck and co for publishing. reply md_rumpf 13 hours agoparentthe modality competition was one of my favorite insights, too! reply msoad 13 hours agoprevCompared to Mirasol3B[1] this is not supporting audio as a modality. What Google has done with Mirasol3B made the demo of \"Astro\" in Google I/O possible. They do a little of cheating by converting audio to images(spectrogram) and video to 25 photo frames with some sort of attention system to things that change during those frames. So the tokenizer is basically the same for audio and video and images. I believe Meta is going to this direction with multimodality as well. The new GPT voice mode is probably using the same architecture. What's mind boggling is that models perform better at the same parameter size with new modality added to them! It seems obvious that 3D is the next modality. [1] https://arxiv.org/pdf/2311.05698 reply mjburgess 10 hours agoprevAm I reading this correctly: Training time was 4282407hrs. At, conservatively, 200w gpus, that's (4282407*200)/1_000_000_000 GWh ~= 1 GWh. At 10c/kWh that's $100,000 ? So if you have a single eqv GPU at home, it's 500yrs of training time and $100k in energy costs. Or, in practice, 3000 gpus for 2mo. The AI industry has to hope the world doesnt change fast enough for these models to be useless. EDIT: price is $100k reply jsheard 8 hours agoparentNumbers like these really don't bode well for the longer term prospects of open source models, I doubt the current strategy of waiting expectantly for a corporation to spoonfeed us yet another $100,000 model for free is going to work forever. That $100k is conservative too, it doesn't include the cost of buying/renting the hardware, or the compute time spent on experimental training runs, or the cost of data acquisition, labeling and cleaning, or the cost of RLHF fine-tuning. reply mikehollinger 8 hours agorootparent> Numbers like these really don't bode well for the long-term prospects of open source models, I doubt the current strategy of waiting expectantly for a corporation to spoonfeed us yet another $100,000 model for free is going to work forever. I would add “in their current form” and agree. There’s three things that can change here: 1. Moore’s law: The worldwide economy is built around the steady progression of cheaper compute. Give it 36 months and your problem becomes a $25,000 problem. 2. Quantization and smaller models: There’ll likely become specializations of the various models (is this the beginning of the “Monolith vs Microservices” debate? 3. E2E Training isn’t for everyone: Finetunes and Alignment are more important than an end to end training run, IF we can coerce the behaviors we want into the models by finetuning them. That along with quantized models (imho) unlocked vision models which are now in the “plateau of producivity” in the gartner hype cycle compared to a few years ago. So as an example today, I can grab a backbone and pretrained weights for an object detector, and with relatively little data (from a few lines to a few 10’s of lines of code, and 50 to 500 images) and relatively little wall clock time and energy (say 5 to 15 minutes) on a PC, I can create a customized object detector that can detect -my- specific objects pretty well. I might need to revise it a few times, but it’ll work pretty well. Why would we not see the same sort of progression with transformer architectures? It hinges on someone creating the model weights for the “greater good,” or us figuring out how to do distributed training for open source in a “seti@home” style (long live the blockchain, anyone?). reply jsheard 7 hours agorootparentYeah, there's no accounting for breakthroughs in training efficiency. I wouldn't count on Moores Law though, the amount of compute you can put into these problems is effectively unbounded so more efficient silicon just means those with money can train even bigger models. 3D rendering is a decent analogy, Moores Law has made it easy to render something comparable to the first Toy Story movie, but Pixar poured those gains back into more compute and is using it to do things you definitely can't afford to. reply philjohn 4 hours agorootparentprevI wonder if a kind of Seti@Home approach could work - although I'm guessing the limited VRAM in most consumer cards compared to an H100, as well as the much slower \"virtual WAN interconnect\" versus the mellanox goodies that nVidia clusters enjoy would be too big an obstacle? reply jsheard 4 hours agorootparentEven if you could get that to work, how many people would be willing to run their >300W GPUs at full tilt 24/7 in order to contribute to the training cause? You would basically be asking people to deal with the logistics of running a cryptocurrency mining operation but without the prospect of getting paid for it. reply loudmax 3 hours agorootparentDepends on the logistics. If I were confident about the security, I wouldn't mind letting my GPU participate in a distributed effort to significantly improve an open source model. This should be a few dollars a month on my power bill, not dozens or hundreds of dollars, especially if I undervolt. Now, I don't know of any distributed training technique that will make a significant impact on improving a model, and that security component is a big \"if\". But if something promising comes a long, I'd bet lots of people would be willing to donate some GPU time, especially if it were easy to set up. reply mmoskal 4 hours agoparentprevAssuming $30k GPU with 3yr deprecation, it's additionally $1.14/h. Much more than energy. reply TaylorAlexander 10 hours agoparentprevThanks for the figures. I suppose with expenses like that, they will be motivated to research methods of updating models which have already been trained. Edit: I see the price was updated reply hackerlight 10 hours agoparentprev1 GWh is 1 million kWh, multiplied by $0.1 that should give $100k in energy costs? reply mjburgess 10 hours agorootparentYes, thanks. I had assumed I had been off by a factor somewhere. Yet, 100k seems small -- the total cost of production is in the 10mil+ range. reply Etheryte 9 hours agorootparent100k is small, but you only get away with 100k if you nail everything perfect the first time around — something that we all know does not really happen. I think compiling is a good parallel to training, imagine if compiling your whole software project cost 100k if you did it from scratch. Sure there's incremental builds etc, but the cost is steep no matter which way you look at it. reply ljlolel 14 hours agoprevIt also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents. reply aconz2 6 hours agoprev> Recent multimodal foundation models are very widely adopted but still model different modalities separately, often using modality specific encoders or decoder Is this accurate? I thought for example gemini pro used image tokens and gpt4-o similar > without the need for separate image/text encoders but then they say they pre-trained two different tokenizers, so maybe they just mean that the tokens go into the same attention layer? But then I thought that is how all the multi-modal stuff was happening already? two typos stabilitize and multiplicate reply marcinzm 6 hours agoparentThat seems odd since I also don't see how this differs from other approaches being published. Except what everyone else calls an Image Encoder (ie: some type of pre-trained VAE architecture) they call a tokenizer. The Apple MM1 paper used ViT-L for example for it's image encoder and then C-Abstractor for it's image tokenizer. reply huac 6 hours agorootparentthe biggest difference is that existing multimodal models (eg GPT-4V and MM1) trained the text model first, and then added in the image component after text training was done ('late fusion'). MM1 learns a projection into the text space, not discrete tokens, and thus cannot generate images. Other work allows the model during training to learn the 'tokenization' more explicitly. that's more similar to Adept's Fuyu architecture, which I am personally a fan of, but also does not enable generating images out. You can generate images using late fusion as well, though I am not aware of other public work that discloses both early fusion and image generation. reply mountainriver 6 hours agoparentprevVision language models use various encoders to project the image into tokens. This is just a means of a unified encoder across modalities reply elijahbenizzy 14 hours agoprevWhat's cool (and tough to keep up with) with this wave of tech is just how quickly it moves. On the plus side there's a lot of interesting things and it is generally easy to follow/figure out what they did. On the minus side it's a little exhausting, and there's so much money in it feels like the vast majority of it is grifting. To add to that, the people who are trying to catalog it (the AI guy on LI) are the griftiest of them all. I've found the best way to keep up is find one topic you want to learn about, deep-dive, and read all the related papers, then explore breadth-first from there until you find another topic... reply polskibus 12 hours agoparentHow do you keep your knowledge after deep dive? Do you try to use it somehow? I found that reading a lot usually does not contribute to long term proficiency in a given topic, unless followed by non trivial amount of practice. reply elijahbenizzy 4 hours agorootparentYeah so I’m lucky that I work in/adjacent to the space so it doesn’t get buried. Otherwise I think it’s near impossible to retain learning without practice. reply 361994752 14 hours agoparentprevIt's still in the early phase where the bubble is building up. This is necessary if we want a prosperous market. Hopefully, after the bubble bursts, some good companies will remain (which is very likely). reply randmeerkat 14 hours agoparentprev> On the minus side it's a little exhausting, and there's so much money in it feels like the vast majority of it is grifting. It is all grifting. The moment someone creates something that can improve upon itself there will be an intelligence explosion and it won’t need press releases or debates about its intelligence. The current path of research will not lead to that, if there was something to be discovered here it would have been discovered already. It’s just new to the general consumer and there’s a wow factor associated with it like crypto and NFTs before. The truth is tech has lost its momentum and is desperate to find a new trick. reply Kuinox 9 hours agorootparentThe rate of improving is important. If it's as intelligent as the average human, the rate of improving will be slow, very slow compared to what the researcher can do currently. reply elijahbenizzy 14 hours agorootparentprevI think the opposite. There is value in intelligent software, but IMO we’re a long way from AGI. So lots of grifting but some gold along the way. And it’s intellectually interesting/nuanced (cool math, interesting infra), unlike crypto which was more of a massive energy burning waste than anyone likes to admit. reply sanxiyn 14 hours agorootparentIf our standard is cool math, crypto is also full of cool math. (Have you read Vitalik's explanation of Quadratic Arithmetic Programs?) Our standard can't be that low. https://medium.com/@VitalikButerin/quadratic-arithmetic-prog... reply elijahbenizzy 4 hours agorootparentFair enough. More reacting to the concept of a blockchain which is like old news and an extremely inefficient way to do 90% of what people are (we’re) trying to do with it. Will read! reply fngjdflmdflg 2 hours agorootparentprevWhat does grifting mean to you? reply kriro 12 hours agoprevOnly browsed but this is really interesting and I'm glad it was published. I understand why a unified model is an interesting thing to work on but doesn't the discovery of \"modal-competition\" suggest that at least short term it might be even better to train specialized models for each modality and some sort of modality-supervisor (glue code model)? reply huac 6 hours agoparent'sum of the whole is greater than the parts' is a very important line of research to investigate. reply gdiamos 13 hours agoprevDoes Meta plan to open source these models? reply dankle 12 hours agoprevAre they downloadable? reply md_rumpf 13 hours agoprev [–] every 3rd sentsnce is \"the model was not trained on data from meta's products\" reply keyle 13 hours agoparentThat makes sense. You probably don't want to train your LLM against your uncle's dubious claims about the government, flat earthers and the likes content :) reply esafak 2 hours agoparentprev [–] At least they're clear about provenance, unlike OpenAI. https://www.reddit.com/r/ChatGPT/comments/1bfa7s3/openai_cto... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The paper \"Chameleon: Mixed-Modal Early-Fusion Foundation Models\" introduces Chameleon, a model family integrating images and text using an early-fusion, token-based approach.",
      "Chameleon excels in tasks like visual question answering, image captioning, and mixed-modal generation, achieving state-of-the-art performance in image captioning and surpassing Llama-2 in text-only tasks.",
      "It competes with and often exceeds the performance of larger models such as Mixtral 8x7B, Gemini-Pro, and GPT-4V in human evaluations, marking a significant advancement in unified multimodal document modeling."
    ],
    "commentSummary": [
      "Meta's new multi-modal large language model, Chameleon, has made significant advancements in addressing tokenization and competitive dynamics between modalities since its inception five months ago.",
      "The development of Chameleon, which requires substantial computational resources, raises concerns about the sustainability of open-source AI due to high costs and corporate dependency.",
      "Potential solutions to these challenges include improved computing efficiency, model specialization, and distributed training, with ongoing comparisons to other models like Google's Mirasol3B and discussions on unified versus separate modality processing."
    ],
    "points": 277,
    "commentCount": 39,
    "retryCount": 0,
    "time": 1716255473
  },
  {
    "id": 40419325,
    "title": "Understanding Xterm: User Input and Terminal Mechanics (Part 1)",
    "originLink": "https://kevroletin.github.io/terminal/2021/12/11/how-terminal-works-in.html",
    "originBody": "How terminal works. Part 1: Xterm, user input Dec 11, 2021 Motivation Introduction User input strace Printing non-printable stty raw -echo -isig UTF-8 Conclusion Motivation This blog series explains how modern terminals and command-line tools work. The primary goal here is to learn by experimenting. I’ll provide Linux tools to debug every component mentioned in the discussion. Our focus is to discover how things work. For the explanation of why things work in a certain way, I encourage the reader to read excellent articles: The TTY demystified A Brief Introduction to termios and to visit a computer history museum: Teletype ASR 33 Part 10: ASR 33 demo The IBM 1401 compiles and runs FORTRAN II. Please note that I talk solely about Linux (because that is what I use), but many discussed concepts should apply to other Unix-like systems. I’ve chosen the “learn by experimenting” approach because that’s how I’ve learned about command-line tools. In my case, there was no single “click” moment after which I’ve understood all the things. Instead, I’ve learned through a never-ending process of building mental models, proving them to be wrong, and then adjusting those models to reflect new knowledge. Target audience are people who want to start working on command-line tools. The series consists of 4 parts. The first two parts discuss how xterm work. Parts 3 and 4 talk about different features of tty: Part 1: Xterm, user input; Part 2: Xterm, CLI tool output; Part 3: pty, stty; Part 4: pty, sessions. Introduction Let’s start the discussion with an inaccurate diagram that shows a general use case for working with a command-line shell: (1) (2) (3) userxtermbash The user interacts with bash using a terminal emulator xterm. xterm is a GUI app that receives “key pressed” events and writes corresponding characters into a bidirectional filehandle (2). Bash reads those characters from (2) does something and sends the output back to xterm, using the same filehandle (2). Xterm reads, bash outputs from (2) and renders them on the screen. (2) is “just a file” and this communication scheme looks pretty simple. If the user asks bash to execute a command, let’s say cat log.txt then bash spawns cat which uses the same filehandle to send its output to xterm: bash (1) (2) (4) userxtermcat Again, pretty simple. In this unrealistic model (2) is “just a file” xterm and cat exchange plain text. In reality, things are slightly more complicated. Evolution extended the simple scheme of “using a bidirectional filehandle to exchange plain text” to implement additional features: TUI interfaces. The terminal can draw characters at an arbitrary part of the screen; command-line tools can ask capabilities of the terminal and can handle window resize; job control. Shell organizes processes into logical groups which can be paused/resumed or stopped altogether; access control for the filehandle (2). Bash has a feature to spawn background processes. This might lead to a situation when two processes are writing their output into the same filehandle (2) at the same time; there should be some access control mechanism; “fixing” stupid tools which believe that the terminal is just a file with plain text; so that those tools look and feel better. User input Requirement above and 50 years of history led us to this scheme: (1) (2) (3) userxtermttybash The first thing to notice is a “middle man” tty between xterm and bash. We will discuss tty in parts 3 and 4. For now, we will just say that: tty sits between xterm and bash and passes data from one to the other in both directions; depending on its configuration, tty changes data it receives from one side before passing to the other; there is command stty raw -echo -isig which configures tty to pass data “as is without modification”. Using stty raw -echo -isig to disable most effects of tty is our primary strategy to explore how xterm works. Until the part 3, we will ignore the existence of tty and will concentrate on exploring xterm’s behavior. Let’s start by discussing a bi-directional link between a user and xterm. Converting scancodes that come from a keyboard into GUI events happens in two steps. First, Linux handles hardware events and turns them into keycodes that can be read by userland (using device descriptors like /dev/input/by-id/usb-2.4G_2.4G_Wireless_Device-event-kbd). Second, Windows system (X or Wayland) reads Linux keycodes and converts them into its own keycodes, and also assigns a keysym (i.e. a Unicode character). To check how it works, one can use: sudo showkey to explore Linux keycodes (visit this page for more info); xev (or wev for Wayland users) to explore GUI events. For example, when I press the q button on my keyboard, depending on my keyboard layout I see: showkey: keycode 16 xev: keycode 24 (keysym 0x71, q) xev: keycode 24 (keysym 0x6ca, Cyrillic_shorti) xterm receives keypress events and writes data into tty(2): it encodes printable characters using configured encoding (most probably UTF-8); on receiving some key combinations, it executes actions such as copy-paste from clipboard; it encodes other key combinations and non-printable characters (such as arrow keys) using ANSI escape sequences (see post #2 for more details about ANSI sequences). So converting key presses into data written into tty(2) happens in 3 steps, two involving kernel and one in xterm. Now let’s figure out what xterm sends into tty after all those 3 steps. There are two strategies we can use to accomplish this task: strace: trace system calls (we will trace write and read calls, but be aware that there are also aio API); run a command-line tool that will disable tty’s input/output processing using stty raw -echo -isig log its inputs. strace Let’s start with strace because it’s quite a practical approach. In your daily life, if you’ll get stuck with misbehaving command-line tools, you can attach to a running process and observe what your terminal is writing into filehandles and what your shell reads. You don’t need to restart running programs to figure out what is going on. First, here is a little helper to find out PID of a terminal by clicking it with a computer mouse (for users of XWindows system): xpropgrep '_NET_WM_PID(CARDINAL)'awk '{print $3}' Then let’s observe what xterm writes and reads into/from filehandles (please replace -p 22853 with an appropriate PID): sudo strace -f -e 'trace=write,read' -e write=all -e read=all -p 22853 2>&1grep -v EAGAIN For testing, I’ve entered qwe sequence and strace gave me: write(4, \"q\", 1) = 100000 71qread(4, \"q\", 4096) = 100000 71qwrite(4, \"w\", 1) = 100000 77wread(4, \"w\", 4096) = 100000 77wwrite(4, \"e\", 1) = 100000 65eread(4, \"e\", 4096) = 100000 65 That makes sense. xterm sends (writes) q. tty+bash echoes back q to display it so that the user can see what he/she entered. Then a sequence we follows the same pattern. Now, I’ll try arrow keys: the left arrow and then the right arrow: write(4, \"\\33[D\", 3) = 300000 1b 5b 44.[Dread(4, \"\\10\", 4096) = 100000 08.write(4, \"\\33[C\", 3) = 300000 1b 5b 43.[Cread(4, \"\\33[C\", 4096) = 300000 1b 5b 43.[CFor left arrow key, xterm sends \\33[D and receives back \\10. man ascii tells us that 33 Oct is the same 1b Hex and it’s a \\ESC (escape) ASCII control character. 10 Oct is 08 Hex and its BS backspace control character (commonly abbreviated as \\b thanks to C programming language). We will discuss ANSI escape sequences and ASCII control characters soon, for now, we can confirm that using strace helps to observe what xterm is actually doing: it sends qwe\\ESC[D\\ESC[C and receives qwe\\b\\ESC[C. Let’s use strace to observe what bash is doing. sh-4.4$ echo $$ 5944 Entering the sequence qwe gives me symmetrical result from the bash side: It receives qwe\\ESC[D\\ESC[C and sends qwe\\b\\ESC[C back. read(0, \"\\33\", 1) = 100000 1b.read(0, \"[\", 1) = 100000 5b[read(0, \"D\", 1) = 100000 44Dwrite(2, \"\\10\", 1) = 100000 08 I’ve promised to ignore tty for a while, but just to show why it might be useful to strace both a terminal and bash, let’s experiment. Let’s execute cat - command and observe in real-time what xterm is sending to tty and what cat receives. First, let’s get the PID of a shell and then execute cat echo $$ 10519 sh-4.4$ cat - Then in the other terminal window, let’s find out the PID of cat using “parent PID” option of ps: ps --ppid 10519 PID TTY TIME CMD 10560 pts/5 00:00:00 cat In my system, experiment shows that xterm writes characters one by one immediately after I’ve pressed a keyboard button. Yet cat receives the entire line only after I’ve pressed Enter. I can use the Backspace key to erase previously entered characters, which is relatively complicated logic. This logic is part of what tty is capable of. read(0, \"qwe\\33[D\\33[C\", 131072) = 1000000 71 77 65 1b 5b 44 1b 5b 43 0a qwe.[D.[C.We will discuss tty in the 2nd part. For now, let’s just enjoy the success of our debugging approach: we’ve just observed what exactly xterm and bash send to each other and how tty (which sits in the middle) can alter data before sending it to a consumer. The big limitation of such an approach is that reading sequences like \\33[D\\33[C require a certain patience and might be quite hard if applications output a lot of data ¯_(ツ)_/¯. Printing non-printable While playing with strace we’ve encountered sequences like this \\33[D\\33[C which I’ve later written like this: \\ESC[D\\ESC[C. In my daily life, I sometimes encounter different notations, for example \\u001b[D\\u001b[C, \\x1b[D\\x1b[C, or something else. Different software uses different conventions for visualizing non-printable characters. Also, many programming languages have a way to embed non-printable characters into string literals using a sequence of printable characters. But again, conventions for representing non-printable characters using printable ones differ between programming languages. Let’s discover how different software visualizes the ESC (escape) ASCII character: printf \"\\x1b\" > data.txt vi, emacs: ^[ less: ESC code, gedit : on my systems render some nonsense hexdump: 1b (hexdump supports many output formats) od -a : esc (od supports many output formats) strace: \\33 and 1b python: \\x1b open(\"/tmp/data.txt\", \"r\").read() Haskell: \\ESC import qualified Data.ByteString as BS BS.readFile \"/tmp/data.txt\" >>= print nodejs: \\u001b const fs = require('fs') console.dir( fs.readFileSync('/tmp/data.txt', 'utf8') ) To make things more confusing, some popular programming languages support syntax for embedding non-printable characters into string literals, but don’t provide easily accessible function to convert a string into the same notation. For example, using the C programming language, I can easily make a string containing ESC character: char* str = \"\\x1b\"; But the easiest way I know to visualize it using printable characters is to write code like this: #include#includeint main() { FILE* f = fopen(\"/tmp/data.txt\", \"r\"); int c = fgetc(f); while (!feof(f)) { if (isprint(c)) printf(\"%c\", c); else printf(\"\\\\x%x\", c); c = fgetc(f); } return 0; } The moral here is that different tools visualize non-printable characters differently. To make things less confusing it’s helpful to train your eye to recognize magic strings ^[, \\ESC, ESC, esc, 1b, \\x1b, 0x1b, \\u001b, 33, 27. Also, it’s helpful to choose tools you can understand even under stress. stty raw -echo -isig We’ve traced xterm using strace to check what it sends to bash. We can accomplish a similar task without using a tracing tool. The most fool-proof way to do so is to disable the effects of tty and to dump binary data which comes from tty into a file. Then we can explore the content of a file using our favorite tool of choice: stty raw -echo -isig; dd bs=1 of=/tmp/data.txt I prefer to use vi or od: vi /tmp/data.txt od -ac /tmp/data.txt It might be cool to visualize the same data in real-time. One can use this bash one-liner: stty sane -isig -echo -icanon; while true; do od -N 1 -ax -; done Or convert man ascii into a small c program. It executes stty raw -echo on startup, so that tty doesn’t change terminal output and hence the tool shows what terminal sends into tty. Pressing a sequence of a, 1, Ctrl+d, Ctrl+l gives: a 1 EOT (end of transmission) FF '\\f' (form feed) Alt+d gives 2 characters: ESC (escape) d Ctrl+Alt+Shift+d gives: ESC (escape) EOT (end of transmission) which is the same as Ctrl+Alt+d, Shift is just ignored. That behavior of xterm is not set in stone and it is configurable in a terminal-dependent way. Depending on its configuration, xterm might send different things in response to Ctrl+Alt+Shift combination. Here is the discussion about xterm modified keys. UTF-8 Utf8 has a few nice features which I didn’t appreciate enough until recently: Utf8 is a self-synchronizing code. If you take any Utf8-encoded string and randomly chop off the beginning so that you end up in the middle of multi-byte character, then: you’ll be able to detect an error: an attempt to decode an invalid character; you’ll be able to recover from the error by discarding bytes of a broken character and figure out the beginning of the next valid character. ASCII characters (including control character) are valid one-byte Utf8 encoded characters. Combined (1) and (2) give us a nice property that control characters will never appear as part of multi-byte characters. I.e. python code below is correct for any string comprising multi-byte Utf8 characters: \"编程很有趣\".find(\"\") == -1 Let’s understand why this is the case. 编程很有趣 is encoded using 15 bytes; below I’ve represented bytes using decimal numbers: 编 程 很 有 趣 231 188 150231 168 139229 190 136230 156 137232 182 163 All these numbers are greater than 127 Dec. But all ASCII characters (including control characters) are lesser or equal to 127 Dec. So it’s safe to search for single-byte ASCII characters in Utf8 strings without decoding them because all bytes of every valid multi-byte character is guaranteed to be greater than 127 Dec. Error recovery is possible, and it works surprisingly simple. In binary notation, all ASCII characters start with leading 0, all bytes of multi-byte characters start with 1. In addition, for multi-byte characters: only the first byte of a character can start with 11; all continuation bytes (bytes 2, 3, 4) start with 10. You can easily observe these properties in action: 编 11100111 10111100 10010110 程 11100111 10101000 10001011 很 11100101 10111110 10001000 有 11100110 10011100 10001001 趣 11101000 10110110 10100011 Each byte starts with 1 indicating that it’s a part of a multi-byte character. Also, each byte contains an indication if it’s the first byte or a continuation byte. Conclusion Using strace and by disabling tty features, we’ve explored how keyboard input from users reaches command-line tools. We also saw that xterm might send non-printable characters and different tools visualize non-printable characters differently. Also, we’ve improved our mental resilience by getting accustomed to different notations and by trying different tools for visualizing control characters. Finally, we said a few words about Utf8 encoding, which is the most widely used Unicode encoding nowadays. In this blog, post we’ve discussed how xterm handles user input. Next post will discuss how xterm visualizes the output of CLI tools. Stay tuned :)",
    "commentLink": "https://news.ycombinator.com/item?id=40419325",
    "commentBody": "How terminal works. Part 1: Xterm, user input (2021) (kevroletin.github.io)243 points by smartmic 23 hours agohidepastfavorite17 comments tedunangst 18 hours agoThe other three parts aren't linked in the article, but available if you go up a level. https://kevroletin.github.io/ reply rurban 12 hours agoparentI've enjoyed the 4th part, sessions and writing script most. The rest looks like severe security problems lurking underneath reply ggm 20 hours agoprevI used to search for a couple of lines of code in the getty.c program or its ancestors, which permitted ALL CAPS LOGIN because teletypes and early VDU terminals included models without a shift function. It made me ask myself how the password check was able to function (and how strong the crypt function was) if the case wasn't important. They were removed from (at least) FreeBSD sometime in the 2.x series I believe. reply ksherlock 19 hours agoparentI remember AIX doing that when you entered an upper-case login. Probably still does. https://www.ibm.com/docs/en/aix/7.1?topic=s-stty-command#stt... xcase Echoes uppercase characters on input, and displays uppercase characters on output with a preceding \\ (backslash). iuclc Maps uppercase alphabetic characters to lowercase. olcuc Maps lowercase alphabetic characters to uppercase on output. Solaris (Illumos) source code is consistent with this. So for an upper-case only keyboard, uppercase letters are lowercase. To type an uppercase letter, precede it with a \\. The tty layer handles the conversion so programs don't know or care what you're actually typing -- they just read (and write) normal upper and lower case characters. reply matricaria 13 hours agoprev(2021) reply bsder 17 hours agoprev [–] How terminal works? Badly. The terminal protocol is broken in fairly fundamental ways (see: How do I get an event forkey up/down by itself? How do I distinguish Esc by itself from Esc combined with something else) A lot of modern things have adopted the Kitty extensions in order to solve this: See: https://sw.kovidgoyal.net/kitty/keyboard-protocol/ reply zokier 12 hours agoparent> How do I get an event forkey up/down by itself? How do I distinguish Esc by itself from Esc combined with something els How do I drive a screw with a hammer? Badly. It doesn't make hammer a bad tool though. Terminals are a simple tool. The inputs and outputs are lines of text. If your application doesn't fit that model and needs keypress events then terminal is simply the wrong tool for the job. reply M95D 10 hours agorootparentLet's take MC, or any other TUI app. Esc is the common key to use to exit from menus, dialog boxes, various views, etc. If the terminal is the wrong tool, then give example of the right tool. reply zokier 5 hours agorootparentAs I said, terminals are best suited for command line applications; lines of text in, lines of text out. Something that would work on classic ASR-33 or ADM-3A. Dialog boxes, menus, and views are concepts that are more fitted to GUIs. And as such the right tool these days would be Wayland. For example, you get native keyboard scancode events and XKB keymaps, allowing far better input handling capabilities than any layering of hacks on top ANSI escape codes ever can. The whole point of terminals is that they are simple, text based, and as such interoperable and standardized. ANSI escape code extensions are decidedly not text nor simple, and often very non-interoperable to varying degrees. As such they are pretty antithetical to the whole idea of terminals. reply M95D 5 hours agorootparentI am against this view. GUI invites bloat. reply icedchai 3 hours agorootparentprevPerhaps we need a better example? Detecting \"esc\" is simple: it is its own key code (ASCII 27.) Detecting \"esc\" and another key is more difficult because it is not a modifier, like shift or control, so you'll have to do it yourself. reply eviks 9 hours agorootparentprevGUI is reply otabdeveloper4 13 hours agoparentprevIt isn't broken. As far as standards go it is a fairly clean and easy to implement one. (That is the reason for its enduring popularity.) Source: wrote a terminal emulator once. Turned out to be not that difficult. We just need to agree on an xterm-compatible subset of functionality and throw away the rest, which nobody ever uses anyways. Your complaints are just aesthetic complaints, the terminal isn't supposed to be a universal GUI toolkit. reply matvore 12 hours agoparentprevYes, let's take dozens of terminal emulator projects out of maintenance mode so app devs can individually and capriciously overload my shift and escape keys, because modern. reply johnnyjeans 4 hours agorootparentwhat if... we could scrolljack.... the terminal rips bong reply lmz 16 hours agoparentprevAnd yet because of the *nix OSs being widely available it has even displaced superior models e.g. the Windows character cell model [0] is arguably a better fit for modern terminals yet new development goes to compatibility with \"cross platform\" inband signaling and escape sequences. [0]: https://learn.microsoft.com/en-us/windows/console/console-fu... reply Xerox9213 16 hours agoparentprev [–] Kitty is great. I started using it because I like cats. But I am learning more and more about how robust it is. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "This blog series explores the mechanics of modern terminals and command-line tools on Linux, focusing on xterm and tty features through hands-on experimentation and debugging.",
      "It covers user interaction with xterm, shell communication, background processes, access control, and keyboard input processing, using tools like `strace`, `showkey`, and `xev` for debugging.",
      "The series includes practical examples for tracing system calls, analyzing terminal behavior, and understanding non-printable characters, UTF-8 encoding, and ASCII/multi-byte character identification, aimed at those developing command-line tools."
    ],
    "commentSummary": [
      "The article \"How terminal works. Part 1: Xterm, user input\" delves into the functionality of terminal emulators, focusing on Xterm and user input handling.",
      "It covers historical aspects, such as all-caps logins due to early terminal limitations, and the evolution of terminal protocols, highlighting their complexities and limitations.",
      "The discussion includes perspectives on whether terminals should remain simple text-based tools or evolve to support more complex interactions, with mentions of modern solutions like GUI-based systems and extensions in terminals like Kitty."
    ],
    "points": 243,
    "commentCount": 17,
    "retryCount": 0,
    "time": 1716233832
  },
  {
    "id": 40428827,
    "title": "CADmium: Open-Source Browser CAD with Rust and WebAssembly Seeks Community Support",
    "originLink": "https://mattferraro.dev/posts/cadmium",
    "originBody": "CADmium: A Local-First CAD Program Built for the Browser May 21, 2024 We're building a new open-source CAD program. We've gotten pretty far, but we need your help. If you'd like to join the effort, join the Discord! What Does It Take? To build a 3D parametric CAD program, you need a: 2D Constraint Solver B-rep Kernel History Tracker 3D User Interface File Format Let's talk about each one! 2D Constraint Solver This is the 2D engine that can ensure lines stay parallel or perpendicular, can make two circles have the same radius, etc. The go-to approach to solving this problem is to concatenate all the unknowns into a big vector 𝑥 ⃗ x, then express every constraint as a linear equation and assemble them all into a big matrix equation: 𝑀 𝑥 ⃗ = 𝑏 Mx=b Notionally, you can invert 𝑀 M and you're done! 𝑥 ⃗ = 𝑀 − 1 𝑏 x=M−1b In practice many optimizations are made. But this approach has downsides. You can only invert 𝑀 M if it is square, which gives rise to the conventional wisdom that all sketches should be perfectly constrained. If you have too many constraints, 𝑀 M will be too tall and the approach fails, even if the redundant constraints are compatible. If you have too few constraints, 𝑀 M will be too short which means a solution can be found by inserting assumptions. But those assumptions are not always consistent with the modeler's expectations. If you've ever had a sketch feature suddenly fly away to infinity, this is what happened. Another downside is that solving this kind of matrix equation gets prohibitively slow when you have a lot of unknowns, which gives rise to the conventional wisdom that individual sketches should be small and simple. There are many alternative approaches for constraint solving. Let's try to formulate the problem as a 2D physics simulator: Each point has mass 𝑚 m and velocity 𝑣 ⃗ v Each constraint is a spring that exerts a force 𝐹 F on the points it is attached to There's a friction force proportional to velocity Step the simulation forward some small 𝑑 𝑡 dt until convergence Instead of solving the whole problem at once, this formulation makes many small changes, driving the potential energy in the springs to zero. At each time step, the runtime is linear with the number of springs and linear with the number of unknowns, so it may support dramatically more complex sketches than the textbook approach. This type of simulation lends itself to parallelization, so it may be very fast in practice. Maybe this step could happen in a compute shader? In this formulation, overconstrained problems don't complain about being overconstrained: a self-consistent system will solve normally, and in an inconsistent system the springs just fight it out and compromise. Underconstrained problems don't fly out to infinity, they find the nearest valid configuration. Another advantage is that this formulation can support inequality constraints. You could constrain a length to larger than 1 cm and less than 2 cm, and preserve that degree of freedom for later. You could constrain a sketch angle to between 10 and 30 degrees. More speculatively, you could extend this formulation to other types of forces. If a closed polygon should have a particular surface area, that could be accommodated as a pressure force. There are other interesting formulations of the 2D constraint problem and there are certainly disadvantages to the spring-mass-damper approach, but in general we would advocate for solving the problem iteratively rather than in a single monolithic solve step. In the last decade there has been tremendous progress in solving gradient descent problems quickly, and in bringing the power of the GPU to the browser. Our primary goal is to make a CAD experience that feels familiar for most CAD users, but we do believe there is room here for fresh ideas. B-rep Kernel In Mechanical CAD, users need to interact directly with the edges and faces of their parts. Consequently, all parametric CAD programs model parts by directly representing their boundaries in a data structure. A cube is representated as a Solid with six Faces, each with four Edges, each with two Points. This approach is called Boundary Representation, or b-rep. For curved surfaces, it is common to use a generalization of splines called NURBS surfaces, which allow the user artistic control over freeform shapes, and the ability to represent conic sections exactly. Representing shapes this way is hard, and it gets dramatically harder when you try to implement boolean operations like Union, Intersection, and Subtraction. A library that handles this kind of data and boolean operations is called a b-rep kernel, and they are extremely difficult to make. Each of the big four CAD companies has written their own, and it took them decades. Today's proprietary CAD landscape looks like this: Where the company is at the top, the b-rep kernel is at the bottom, and in the middle are some of the CAD programs they offer, arranged loosely by cost. The most important b-rep kernel is Parasolid which powers a lot of the industry including products like Shapr3D and Plasticity. Parasolid is the Cadillac Escalade of b-rep kernels: It is huge, expensive, and it offers every amenity you could ask for as well as a bunch of amenities you didn't ask for. In contrast, the open source CAD landscape looks like this: The only popular open-source b-rep kernel is OpenCascade, which is the Pontiac Aztek of b-rep kernels: It is ugly, barebones, and it might break down on you, but it is drivable and you can get one for free. SolveSpace is a Tuk-Tuk in that it was built by one person in a garage and it gets a lot done with very little, but it only looks like a car if you squint. All this to say: The proprietary kernels are good but expensive and the open-source kernels are free but not good. All popular b-rep kernels are old and written in C++. If you consult the official build instructions for OpenCascade, you see this screenshot: Which looks like it was taken on Windows 2000? Thankfully, there is a new open-source b-rep kernel being developed right now called Truck! Unlike every other b-rep kernel, this one is modern and it is written is Rust. Rust is not categorically better than C++, but it is better in a lot of ways that matter to an open-source project. Its build tooling is powerful, convenient, and well-documented. It has centralized package management. It provides more guarantees around memory safety which in turn makes parallelization easier and safer. Its compiler errors are friendly and helpful, so Rust code is easier to refactor. Importantly, Rust has excellent support for compiling to webassembly so it can be readily run in a browser. It is trivial to include Truck in any Rust project. It runs on any operating system and in a browser. They even provide javascript bindings and examples! Truck is about four years old and it already covers all the basics. It can read and write .step files. It can triangulate surfaces to a fixed tolerance. It has NURBS support. It can compute the Intersection or Union of two Solids*, as well as the Not of a single Solid. It is small and lightweight, it is being developed by a real company, and it is young enough and simple enough that a few motivated people could add major pieces of functionality, in a fork if necessary. For example, the B in NURBS stands for B-Splines, but there is an alternative representation called T-Splines which is better in some ways. The patent on T-Splines is owned by Autodesk, but it just expired a few weeks ago! Support could theoretically be built into Truck! I think that Truck is the Rivian R3 of b-rep kernels: It is smaller than its cousins, it's using a lot of modern technology in an exciting but proven way, and it isn't quite finished yet! At the risk of overextending the metaphor, Rust is the electric motor and C++ is the internal combustion engine. History Tracker Parametric CAD programs store the Feature History of your design. You sketch, extrude, and revolve until your part is done. What makes it \"parametric\" is that you can also rewind the clock to an earlier step, change something about it, then replay your features to get a slightly different part. Abstracted further, you can inject variables as inputs to the model, then change the values and the part will update. Your model has now been \"parameterized\". This approach has been wildly successful, but it's often brittle and there are valid criticisms of the whole paradigm. One approach that has emerged to help address the brittleness of parametric CAD is called the Resilient Modeling Strategy, wherein: RMS is a set of conventions for how parts should be designed. For example, all chamfers and fillets go last because they consume edges. Detail features are allowed to reference Core features, but not each other, and so on. Maybe there is value in enforcing these patterns within the CAD program. It may feel limiting at first, but it may pay huge dividends by making designs actually reusable and transferrable. Another avenue to explore could be adding a feature history to sketches. In today's CAD programs it's common to sketch base features likes circles and rectangles, then use tools like mirror, linear pattern, or sketch fillet to duplicate or modify those features. Then you sketch more base features and use more tools, back and forth. The web of dependencies this builds in a sketch is very hard to understand if you weren't the one who made the sketch, so it is often faster to delete the whole sketch and start over. But if sketch features were also stored and displayed in a feature tree, then the ideas from RMS could be applied to a single sketch. Reference features like projecting an edge probably should come first, and final details like snipping and filleting should probably come last. Again this might feel limiting at first, but putting an operation first in the feature tree doesn't mean you have to start with that feature chronologically as you sit down to model. On the topic of chronological ordering, why not record every user event in an append-only log? If that log were the single source of truth for the file, then any particular Feature History could be reconstructed by moving a time slider. Think: Unlimited Undo/Redo, even after closing and reopening the file: You could imagine rewinding back to an earlier version of the Feature History and forking off in a different direction to try the design a different way. You would end up with a branching tree of different attempts, exactly like a git history: Our goal is to create a CAD application where every valid document state is trivially recoverable. Every false start, every \"final\" deliverable, whether you knew you needed it or not. With that machinery you could maintain different variants of parts and keep a record of every design as it was when you ran downstream processes like toolpath generation or FEA. If building this version control system is akin to building git for Mechanical design, could we also build github for Mechanical design? 3D User Interface We love the idea of doing CAD in a browser. Onshape paved the way here and it's awesome. However, Onshape doesn't really run in a browser—it runs on a GPU enabled cloud instance somewhere in AWS and streams the results to your browser. This is why if your internet connection goes down while you're using Onshape, you literally can't do anything. You can't even rotate the viewport. But CADmium doesn't have to be like that. Given that Truck can compile to webassembly, CADmium can do everything right there in your browser. A Local-First app! We've been using this tech stack: Three.js for the 3D viewport Svelte for state management/reactivity Threlte to bridge the gap between Svelte and Three.js Message passing between the UI and the b-rep kernel, rather than sharing memory Electron for running locally Bog standard everything else: Typescript, TailwindCSS, Vite, etc This kind of stack allows the entire app to be written in a reactive, declarative way, plumbing data changes all the way through to mesh updates without you having manage that complexity. That's important because 3D CAD apps are among the most complex UIs that exist. If you want to make a good one and you only have a small team, the framework had better do a lot of heavy lifting! With this stack we were able to build a proof of concept that works, so we feel that this won't be the limiting factor for CADmium. File Format CADmium will use JSON for everything. The Operation Log mentioned above should be JSON lines. And after you've designed a part, CADmium should support exporting to an even simpler exchange format. Notionally something like: { \"steps\": [ { \"type\": \"sketch\", \"id\": \"Sketch-01\", \"data\": { ... } }, { \"type\": \"extrude\", \"id\": \"Extrude-01\", \"data\": { \"distance\": \"10mm\", \"sketch\": \"Sketch-01\", \"faces\": [0], \"type\": \"new\" } } ] } Which could be converted into a .step or .stl using the CADmium command line interface (CLI): $ CADmium export my_part.cadmium --format stl These two ingredients: A simple, easy-to-understand file format An open-source CLI to work with it Are what's required to enable an ecosystem that can create tremendous new value that we would never be able to build ourselves. Imagine being able to pop open a text editor to change an extrusion depth or a fillet radius. Imagine writing a script that replaces all the M5 screws with M6 screws, without having to read a nasty spec. What would a change like that look like using git-diff? I mentioned above the concept of github for Mechanical design. If such a thing really were built and people really did use it, then it would not be hard to imagine building github copilot for mechanical design. We don't know what that would look like in practice, but we think it's fair to say that large language models work best on simple, open, text-based formats rather than complex, proprietary, binary formats. Conclusion Of the ideas mentioned here, we have no idea which ones are going to work out and which ones will turn out to be duds. But we know that somewhere in this space, there's a huge opportunity for a small group of people to make an outsized impact on the manufacturing industry. These are the things we need help with: Programming in Rust (general improvements) Computational Geometry (patches to Truck) Three.js help (new camera controller, better lighting, post-processing) Finding grant opportunities or wealthy benefactors These are things that we are not touching for now, but would love to revisit later: Venture Capital Toolpath generation (CAM) Finite Element Analysis (FEA) If you find these ideas intriguing, please join the CADmium discord server and chat with us!",
    "commentLink": "https://news.ycombinator.com/item?id=40428827",
    "commentBody": "CADmium: A Local-First CAD Program Built for the Browser (mattferraro.dev)203 points by samwillis 4 hours agohidepastfavorite67 comments samwillis 1 hour agoI'm very excited about what Matt is building, the world desperately needs a good open source parametric CAD package. One where the UI/UX is designed to be as \"easy\" to use a SolidWorks. The biggest reason this hasn't happened so far is the lack of a truly capable parametric kernel, Truck, the kernel that Matt is using looks like an incredible project and exactly whats needed. The only other kennel till now that been close to being whats needed is OpenCascade, but its lacks important feature, is buggy and at times quite unstable. Once Truck (and CADmium) lands stable fillets (surprisingly one of the hardest features to make stable) it will prove itself as the perfect successor to OpenCascade and and the perfect platform to build the future of open source parametric CAD upon. reply LeifCarrotson 34 minutes agoparentMaybe I'm not enough of an evangelist, but I just want a good, non-subscription local-first CAD package. I've recently moved over to Alibre Atom3D, which, while not open-source, costs $200. Once. Then (as long as your host OS doesn't change too much) you can access your designs forever. It runs on the ACIS kernel. \"Browser first\" and (from the readme) \"Beyond that, I will try to monetize by offering a hosted version of the software as a paid product.\" reads to me like this project is doomed to either fizzle or to grow and transform into an open-core, subscription-requiring product, accumulating more complicated dependencies to install and \"security features\" that make it harder and harder to run in truly local fashion. reply spookie 23 minutes agorootparentFeel the same way. Honestly I think there's more money to be had going open source than competing in a sea full of sharks. But hey, that's probably wishful thinking from my part. reply Animats 1 hour agoparentprev> The biggest reason this hasn't happened so far is the lack of a truly capable parametric kernel, Truck, the kernel that Matt is using looks like an incredible project and exactly whats needed. Indeed. This is a great application for Rust. It's something that's complicated, difficult, and has to work right. reply beeboobaa3 1 hour agoparentprev> lands stable fillets (surprisingly one of the hardest features to make stable) Interesting. Any chance you could explain why fillets are hard(er)? reply samwillis 52 minutes agorootparentFor fillets to work well they have to connect three important features of a parametric CAD package: 1. Surface tangency matching - perfectly matching the tangent of the connecting surface on either side of the corner. 2. Edge tangency following - you select an edge, and the fillet should be able to follow along all connected tangent edges. 3. Edge reference tracking - when you modify the model further up the feature tree the kernel needs to keep track of that edge, even if the surfaces that make it drastically change or are split. All three are hard problems on their own, once you connect them all it becomes a great indicator of the capabilities and stability of a parametric kernel. reply phkahler 41 minutes agorootparentFor Solvespace I was looking at using curve offsetting to determine where the fillet touches the surface. This would not produce a \"rolling ball\" fillet in more complex cases, but should be fine for simpler extrusions. It turns out generating an offset curve is another somewhat hard problem in itself. reply samwillis 30 minutes agorootparentYep, it would be difficult to maintain a consistent radius with that method. My assumption (I've not looked) is that you could offset the surfaces from the two sides (say A and B), and then calculate the intersection of an offset of A with the original B. Then do the opposite for the other side. I think that will give you two edges that will produce a consistent radius. Offsetting surfaced and calculating intersections aren't exactly easy problems to solve on their own. Fillets are hard! (Solvespace is awesome BTW! If I had the time (or the expertise) it also makes a brilliant foundation for all this) reply phkahler 20 minutes agorootparent>> Offsetting surfaced and calculating intersections aren't exactly easy problems to solve on their own. Fillets are hard! Yeah, offsetting surfaces is harder than curves ;-) This was my early stab at doing chamfers on extrusions: https://github.com/solvespace/solvespace/issues/453#issuecom... I know how to do it better now, but ugh... not enough time. reply datavirtue 1 hour agoparentprevI love this idea. However, the value of solid works (and other expensive solutions) are the domain-specific assets that are available. SketchUp has a lot of community assets but these still leave you hanging almost immediately. You need to be able to scrounge up drawings of certain parts by part number that can be inserted quickly. This is surmountable but I think it would take at least a decade to reach parity...in a best case scenario where CADmium is readily adopted. I would put in the time to learn CADmium. Having worked in FreeCAD some and SketchUp moreso. I hate that I can't automate things easier or build my drawings from scriptable components--like a XAML file. reply samwillis 1 hour agorootparentI don't think think this is a problem at all, the industry has standardised on STEP for assets, as long as CADmium and Truck have STEP support (which they will) then they have access to all assets. True, there are dynamically configurable assets for SolidWorks, but any community around a cad package will quickly recreate those. reply gen3 2 hours agoprevAs someone who learned CAD on autodesk (inventor) ages ago, the only free modeling software I could grasp after working for a few hours was Onshape. Usability seems to be a real problem in this space, Freecad for example seemed to have a billion ways to do the same thing, but in the end only one of them was the real way, some tutorials leaving me with a model that doesn't conform. Awesome work! It seems like a hard problem Repo: https://github.com/MattFerraro/CADmium reply jnovek 1 hour agoparentUgh Freecad. Love hate relationship. I pretty much only use the Part Design, Sketcher and Spreadsheet workbenches. Those mostly work OK together. I don’t dare branch out because of poor interoperability between benches; I sometimes use the Part bench as well but it’s so fiddly because you had to take it through two internal format changes to make it work in Part Design. Hoping that one of the many open source CAD projects can supplant it eventually. reply nicce 1 hour agoparentprevOnshape is an excellent tool in terms of performance and usability. As total newbie I was able to design fully working robot arm with 3d printed joints, design all the motors, their placements and other stuff and even animate movement. The model even worked on my phone, while it was not very practical... reply loughnane 1 hour agorootparentI had 10y or so experience with CAD when onshape showed up around 2014. You could tell from the beginning that they cared a ton about usability and had their minds wrapped around how people use CAD. It’s only got better since. I highly recommend the cadmium folks just soak themselves in onshape’s UX. reply prokoudine 1 hour agorootparent> You could tell from the beginning that they cared a ton about usability and had their minds wrapped around how people use CAD. Well, what else can you expect from the very same guy who came up with SolidWorks in the 1990s? :) reply IshKebab 1 hour agoparentprevSolveSpace is good. It's the only free CAD software I've found that's remotely usable (yes including FreeCAD). Unfortunately it has some pretty big missing features, notably bevels & filets. reply tredre3 1 hour agorootparentI also like SolveSpace a lot. I use it to design small things all the time. Tiny single file program, very responsive interface. It's also pretty intuitive (if you're already familiar with CAD constraints and parametric modelling concepts). There was a burst of activity 2-3 years ago so I was hopeful we'd get filets and maybe better error messages but it hasn't happened yet. Ondsel is also increasingly good. They've released a new version recently: https://ondsel.com/blog/ondsel-es-2024-2/ Their website login-wall downloads for whatever reason, so go to github: https://github.com/Ondsel-Development/FreeCAD/releases reply alright2565 1 hour agoprevI think this is amazing. I'd love to get to the point where we are with open source EDA with open source physical CAD. > Another downside is that solving this kind of matrix equation gets prohibitively slow when you have a lot of unknowns, which gives rise to the conventional wisdom that individual sketches should be small and simple. I've gotten quite deep into this, and this is really not a problem in practice[1] 1. FreeCAD's main issues with constraint performance come from a redundant & unnecessary GUI layout algorithm, which falls over with just a few hundred constraints. 2. Eigen's sparse QR decomposition benchmarks at 18s for 2200 constraints, which is really not too bad 3. There are sparse QR decomposition libraries that can handle 500k-1M constraints in about 18s. I can't imagine a CAD sketch with more than a few thousand constraints. [1]: https://github.com/FreeCAD/FreeCAD/issues/11498#issuecomment... reply blt 41 minutes agoprev> 3D CAD apps are among the most complex UIs that exist. I agree. > If you want to make a good one and you only have a small team, the framework had better do a lot of heavy lifting! I am skeptical that general-purpose UI frameworks can be a good long-term solution for 3D CAD. At times, a nice UX will need to think about many of the following at once: - the parametric 3D model - its mesh approximation - its hidden-surface projection into lines and patches - the pixels in the frame buffer - UI widgets. The frameworks make too many assumptions about information being easily partitioned and limited in size. Maybe the frameworks will help for prototyping, but if the project grows, I expect at some point they will end up ditching frameworks for the core 3D viewport interactions, and have mostly their own code in between OpenGL (or similar) and mouse/keyboard events. Frameworks for buttons, lists, etc. will probably be fine, but even those tend to be much more dynamic in 3D CAD than in average apps. reply spookie 11 minutes agoparentThe only real solution is imgui reply JeremyHerrman 2 hours agoprevWhat an excellent summary of the CAD kernel landscape. Favorite quote from this: \"The only popular open-source b-rep kernel is OpenCascade, which is the Pontiac Aztek of b-rep kernels: It is ugly, barebones, and it might break down on you, but it is drivable and you can get one for free.\" SO true! reply CrimsonCape 1 hour agoparentI laughed out loud at the tuk-tuk analogy. reply phkahler 48 minutes agoprevAs one of the Solvespace maintainers I have a few comments: 1) You don't want just 2D constraints, 3D is better. If you were writing in C++ I'd say just take our constraint solver (Like Dune3D did). Since you're loving Rust, I can point you to the work of Michael F Bryan who wrote one in Rust and blogged about it here: https://adventures.michaelfbryan.com/posts/constraints-part-... I think his code is over at gitlab. I haven't looked at it in a couple years. He wrote that after I nerd-sniped him ;-) 2) For geometry kernels... I've got 3 classes of bugs I want to squash in the Solvespace kernel and then it should do booleans pretty reliably, but I haven't had the time. Ours is just under 6k LOC so you could learn a lot from it. My email is the same ID at gmail if you want to ping on this topic. Its been a while since I looked at Truck and I thought it had stagnated a bit. This is a really hard problem, which is why there are so few options out there even in the commercial world. Even triangulating a trimmed NURBS shell is tricky. 3) History/feature tree is closely related to the \"topological naming problem\" that FreeCAD has. Solvespace handles this by creating each entity from a set of known things. If you try to recreate (regenerate in our lingo) it will just return a handle to the existing entity rather than creating a new one. In other words, every entity \"came from something\" and that relationship is remembered. Where we handle topological naming it works perfectly. But not everything in solvespace is covered by this. You need to bake this in from the start, it's not something you can easily bolt on afterward. 4) where is the link to try out CADmium? reply WillAdams 27 minutes agoparentI can at least help on 4: Github: https://github.com/MattFerraro/CADmium has the link: https://mattferraro.github.io/CADmium/ reply andybak 2 hours agoprevWeirdly hard to find the actual project urls: https://mattferraro.github.io/CADmium/ https://github.com/MattFerraro/CADmium reply Jhos 1 hour agoparentQuoting directly from that github: \"Status: Early prototype. This tool is not yet an MVP, but is being developed in the open. Please do not share this to HN or Reddit or things like that.\" reply WillAdams 1 hour agoprevWhy in a browser if it's local-first? Solvespace has the benefit of being a single download/executable. It also has a constraint solver which has been used in a couple of projects: CADsketcher as you noted, and Dune 3D: https://github.com/dune3d/dune3d where the author noted: >I ended up directly using solvespace's solver instead of the suggested wrapper code since it didn't expose all of the features I needed. I also had to patch the solver to make it sufficiently fast for the kinds of equations I was generating by symbolically solving equations where applicable. Any relation to: https://github.com/jay3sh/cadmium ? Also, for CAD kernels, Manifold was not mentioned: https://github.com/elalish/manifold/wiki/Manifold-Library --- while I understand it to have many of the same disadvantages as OpenCASCADE, it does seem worth mentioning. Interestingly the kernel was previously discussed here: https://news.ycombinator.com/item?id=35071317 It seems really interesting/promising, esp. the compleat history and editability (I'd love to see that history listed in a pane which could be opened/closed --- add a series of disclosure triangles which would allow hiding finished elements so that one could focus on the current task and it would be a dream come true for me --- if I can puzzle out the 3D stuff, so far I've crashed and burned on all the apps I've tried (BRL-CAD, FreeCAD, Solvespace, Alibre Atom...) --- the only thing I've been successful w/ is OpenSCAD and similar coding tools). reply tgsovlerkhgsel 1 hour agoparent> Why in a browser if it's local-first? Because it means it'll work regardless of whether I have a Linux, Mac, Windows, ChromeOS, iPad, or just try to check some detail of my part on my phone. When I design a part at home, need to make an adjustment at work, and then need to make it using whatever is available in the makerspace, not having to install something is a big win. reply phkahler 30 minutes agoparentprev>> Manifold was not mentioned Manifold is a triangle mesh library. CAD should use NURBS surfaces and only triangulate them for rendering. I had looked at replacing the solvespace triangle mesh code with Manfold, but it's a lot of work and what we really need is to fix the NURBS bugs so we don't need to lean on the mesh code so much. BTW the solvespace constraint solver is also used in the Assembly 3 workbench of FreeCAD ;-) It's really getting around these days. reply PrivateButts 1 hour agoprevI recently messed around with using OpenSCAD's WASM build to make a serial plate generator for Voron printers[1], and the one take away from it is that we desperately need a cad export format that preserves variables and history so that parts can be regenerated with new parameters. OpenSCAD, or some other project built on top of it could get there, but not without some major work. Currently building complex projects in OpenSCAD feels like a form of self harm. [1] https://serial-generator.privatebutts.dev/voron reply ecjhdnc2025 2 hours agoprevOutstanding writing that is clear and convincing. I am a mostly (and increasingly) happy FreeCAD user (though I agree with your framing) but OMG I am happy for you, and I will try to get involved when I can. Best of luck. reply adastra22 1 hour agoprevIt appears to be written in Rust. Why package it as a wasm app inside of electron instead of compiling to native and using wgpu directly? reply roarcher 1 hour agoparentPresumably because there aren't many mature Rust GUI options at the moment. Egui is pretty good for what it is, but it's \"immediate mode\" which comes with some restrictions. Slint seems promising but the developer experience was a little rough when I tried it a couple months ago. Not sure the licensing is compatible with this project either. reply nicce 1 hour agoparentprevYou can have much better OS level integrations with Electron (e.g. filesystem), also assuming that you want to reuse as much code as possible. reply JAlexoid 36 minutes agoprevHopefully it's an easy to use CAD, because my preferred CAD right now on Linux is FreeCAD is way too complex and the user input in the UI horrendously clunky. The good side is that all CADs have a very steep learning curve, with most tutorials made by people who are oblivious to the ignorant masses. The landscape is so complex today, that it's easier to use a vector drawing tool and transform it into a 3D model than to use any CAD to make a cylinder. reply s1mon 1 hour agoprevIt's exciting to see a new entry into this space, especially one that is trying to create a new kernel. Unfortunately, it seems unlikely to be successful. The top kernels in the industry have been in development for decades by armies of CAGD PhDs and programmers, with funding from automotive and aerospace companies. Getting to table-stakes with the feature set will take a long time. I also question what user problems this is trying to solve. CAD users don't necessarily need things to be open source. They may have limited budgets, but open source is not a user facing feature in this space the way it might be for some developers. Plasticity has shown that it is possible to license Parasolid and make money at a very low price point. I've been using parametric CAD tools for 30+ years. I find that Onshape is a pretty amazing solution to many of the issues that exist with Creo or Solidworks. I never loose data. Having the equivalent of Google Docs for collaborative CAD with unlimited undo based on a ground up database instead of a file system is life changing. That said, its modeling capabilities are still playing catch up with other tools. A big downside to Onshape (and Creo and Solidworks) is that it started as 2D sketches to extrude/revovle/loft etc into 3D objects. 3D tools are an add on and afterthought to the fundamentals. Plasticity (and Rhino and Alias) are all much more 3D first. Onshape's FeatureScript is really cool and powerful, but it is not as good for CAD users to build their own tools as say Grasshopper in Rhino. I would focus on what user problems are being solved. How will CADmium be a better CAD tool for a certain market than any of the others paid or free? Even though this is very early days in the development process, it's not too early to try to understand user needs. Open source is not in and of itself a goal. reply spookie 1 minute agoparentNew CAD users would appreciate a good open source alternative. Moreover, the world is a diverse place, not everyone has the same kind of money or quantity. I think CAD users would benefit greatly from good open source alternatives. reply samwillis 48 minutes agoparentprevPeople have said the same about browser engines, that it's impossible to start and new one and catch up due to the unfathomable number of man hours invested. But it's happing, Ladybird browser is making incredible progress. With modern tools, the learnings from older engines and the lack of entering debt it's possible for a small team to build things on par or better than 30yo software. I have no doubt that a small dedicated team could build a new parametric kernel and CAD package, particularly one that's open source. reply syntaxing 54 minutes agoparentprevI mean… Onshape, Creo, and Solidworks use the same kernel so it’s not too surprising the UI/UX is similar (I’m speculating for OnShape but given the company history, they probably are). Rhino and the like are geared towards 3D graphics on top of CAD so it’s not surprising either that they’re a Freeform surface modeler first. That being said, I don’t know about the open source part not being a goal. It’s like KiCAD vs Altium. If they manage to have 90% of what Onshape has to offer but open source, I can imagine people using it. I get the angle of cost though, since Solidworks license is about the order of $3-5K, most companies would rather pay it and use something proven. FWIW, Onshape was created for less than $10M to make a MVP in less than 3 years. I don’t think you need decades of PhDs to make a new CAD program nowadays. reply RobotToaster 34 minutes agoprev> In contrast, the open source CAD landscape looks like this: He missed BRL-CAD, which also does brep. The interface is clunky but the kernel seems advanced. reply spookie 8 minutes agoparentBRL-CAD is a true gem reply rjsw 2 hours agoprevWe spend a lot of time working on the correct type system for STEP, I don't think that using an untyped format like JSON is a good idea. I know that ISO 10303-243 uses JSON but in hindsight I think that was a mistake. reply peppertree 2 hours agoprevPlasticity was open source before it abandoned c3d in favor of Parasolid. The repo can still be found here: https://github.com/nkallen/plasticity reply porphyra 1 hour agoprevI like how the Truck kernel's github [1] says that their choice to use Rust and WGPU is due to \"Trendy Tools\" being a core principle. I'm super excited about this. [1] https://github.com/ricosjp/truck reply jpm_sd 3 hours agoprevWhat a great piece of writing. Clearly explained, nicely illustrated, got me really excited about the project. reply sliptonic 2 hours agoprevIt's great to see others investing time and talent into open-source CAD. At Ondsel, we believe open-source CAD is incredibly important and we're thrilled to see innovation in this area. reply krastanov 2 hours agoparentI am trying to look you up, but I am not sure I completely understand your organization and offering. Is Ondsel to FreeCAD the same as Codeweavers to Wine? reply sliptonic 2 hours agorootparentWe're an open-core company building commercially around FreeCAD. We're leading the development of the new integrated assembly workbench and contributing to many parts of the application. We have our own build of FreeCAD which includes our latest contributions. We're also building an optional web-based file sharing and collaboration layer. reply GiorgioG 2 hours agorootparentprevOndsel is FreeCAD without the sadistic UX. reply ecjhdnc2025 2 hours agorootparentNo, it's not, not really -- because the UX work going into Ondsel is going into mainstream FreeCAD (if it didn't originate there, and a lot of recent work did). I am sure Ondsel will vary somewhat in UX over time (because they will be somewhat ahead of what they upstream) and because a few things may be unavailable for open-source licensing. But most of what has changed in terms of UX is already in 0.22-dev. For example OpenTheme will work (don't know if it is included by default), the \"glass\" tree overlay from RealThunder is there, the overlay panels, the sketcher improvements, the new integrated Assembly workbench etc. Ondsel is a commercial distribution of FreeCAD with a cloud-based engineering suite integrated -- file sharing, cloud compute (for parametric recalculations and STEP export) etc. Here is how they describe themselves -- in some useful detail: https://ondsel.com/handbook/About/about_ondsel/ reply ecjhdnc2025 1 hour agorootparentAside from the cloud bits I think the most notable way they are different at the moment is that they have their new configuration variable sets functionality, which maybe core FreeCAD hasn't agreed on yet (it's also not finished I believe). Ondsel are seemingly using that to integrate with recalculation of designs in the cloud compute facility (think Thingiverse customiser but a bit less basic). I eagerly anticipate that in mainstream FreeCAD, because none of the alternatives are great -- e.g. Spreadsheet is simultaneously cool, powerful and utterly exasperating once it's well-integrated. I am sure we will see it. reply sliptonic 1 hour agorootparentCorrect. Variable sets (varsets) is partially merged into upstream main development branch. Other work is ongoing. Varsets are much more important than just our web service though. Being able to configure and control variants in a design is underappreciated and will be essential to having complex assemblies with multiple copies of identical parts. Imagine a hinge that is reused in many places in the design. The current angle of the hinge should be controlled on a per-copy basis. When you change the angle, you don't want all copies everywhere to reflect the new state. This is a variant. reply ecjhdnc2025 1 hour agorootparentYes -- I've seen the video and spent some time absorbing it and I'm really excited to see that. I really enjoy using spreadsheets and configuration tables but there are some really maddening aspects; I understand global recalculation is kind of unavoidable, but I'm not sure why that has to happen when I change the font of a cell, for example :-) Thank you for all of your work -- I am away from my CAD projects at the moment but I will be digging properly into 2024.2 as soon as I can. reply ecjhdnc2025 1 hour agorootparentprevAs an aside, I would love to see Brodie Fairhall's take on varsets at some point. His video on advanced parametric modelling with the spreadsheets is an epic of the genre. reply bombela 1 hour agorootparentprevIn the realthunder form of freecad, I wrote a macro to give me a form of parametric variant generation. I am so looking forward to this. reply ecjhdnc2025 1 hour agorootparentIn the meantime, have you ever watched the Brodie Fairhall video I mention in a sibling comment? The shapebinder technique near the end of this video is amazing! https://www.youtube.com/watch?v=Yp6cIMA7LsI And there's another clever technique using variant links (which you can take a bit further with BaseFeatures): https://www.youtube.com/watch?v=m9C_ahIVKOI It's funny. People describe FreeCAD as maddening or sadistic, and for sure there are elements of the workflow that are frustrating (I'd be happy if I could do the five things I use the Draft workbench for without ever having to open it), but at the same time, it's so liberating and enthralling once you get your head into its way of thinking. For me it's like QGIS or Inkscape: it's mindblowing that this tool is available to me for free. The trivial things I've been able to do have really changed my life (and I don't mean to overstate that -- these are things I never thought I'd learn and the impacts on my creativity have been striking). reply sliptonic 1 hour agoprevA 2D solver works great for sketches and some kinds of assemblies. However, it is insufficient for many kinds of 3D assemblies. The solver Ondsel released (LGPL) which underlies the new integrated assembly workbench is a true 3D solver. https://github.com/Ondsel-Development/OndselSolver reply syntaxing 1 hour agoprevAbsolutely amazing. I have high hope for this project to be a real OnShape competitor! reply dvh 2 hours agoprevAre boolean operations working yet? reply stirfish 1 hour agoprevInequality constraints! This makes me really excited reply MCLAU155 2 hours agoprevonShape has been great reply daemonologist 2 hours agoparentOnshape is definitely great, but it could go away or have its free version cut down (like Fusion 360) at any time. Having an open source alternative that's less of a slog than FreeCAD would be awesome. reply tjoff 1 hour agorootparentYou know, like they already did previously? It is utterly insane to me how onshape even consider luring folks in again after absolutely ruin their userbase and community. https://www.youtube.com/watch?v=S9hmi1leU2s reply cassianoleal 2 hours agoparentprevOnshape is great but it's neither open source nor local-first. reply xipix 2 hours agoprev [–] Getting negative vibes from the name. Cadmium's a nasty substance. I mean, you wouldn't call an augmented reality app ARsenic, would you? Constructive, I hope, criticism. reply mbonnet 2 hours agoparent [–] sure, why not? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "CADmium is an open-source, browser-based CAD program in development, seeking community support via Discord.",
      "The project aims to use a 2D physics simulator for constraint solving and introduces \"Truck,\" a modern b-rep kernel in Rust, offering memory safety and web compatibility.",
      "CADmium proposes a Resilient Modeling Strategy (RMS) for better design reusability and seeks help with Rust programming, computational geometry, Three.js improvements, and funding."
    ],
    "commentSummary": [
      "CADmium is a new browser-based, local-first CAD program using the Truck parametric kernel, aiming to provide an open-source alternative to commercial CAD software like SolidWorks.",
      "The project has generated excitement due to Truck's potential to overcome limitations of other kernels, but concerns exist about a possible shift to a subscription model and maintaining a local-first approach.",
      "Discussions highlight the complexities of implementing fillets, edge reference tracking, and the potential of Rust and WASM for CAD applications, with significant innovation seen in open-source CAD software like FreeCAD."
    ],
    "points": 203,
    "commentCount": 67,
    "retryCount": 0,
    "time": 1716301196
  },
  {
    "id": 40424982,
    "title": "Erlang/OTP 27: Markdown Docs, Triple-Quoted Strings, and Enhanced Profiling Tools",
    "originLink": "https://www.erlang.org/blog/highlights-otp-27/",
    "originBody": "Erlang/OTP 27 Highlights May 20, 2024 · by Björn Gustavsson Erlang/OTP 27 is finally here. This blog post will introduce the new features that we are most excited about. A list of all changes is found in Erlang/OTP 27 Readme. Or, as always, look at the release notes of the application you are interested in. For instance: Erlang/OTP 27 - Erts Release Notes - Version 15.0. This year’s highlights mentioned in this blog post are: Overhauled documentation system Triple-Quoted strings Sigils No need to enable feature maybe The new json module Process labels New functionality in STDLIB New SSL client-side stapling support tprof: Yet another profiling tool Multiple trace sessions Native coverage support Deprecating archives Overhauled documentation system # The Erlang/OTP documentation before Erlang/OTP 27 was authored in XML, from which the Erl_Docgen application could generate HTML web pages, PDFs, or Unix man pages. The reason for generating PDFs is that the documentation used to be printed as actual paper books. The last time the books were printed were for Erlang/OTP R7 released in 2000. As an example, here is the XML code for lists:duplicate/2 from Erlang/OTP 26: Make N copies of element.Returns a list containing N copies of term Elem. Example:> lists:duplicate(5, xx). [xx,xx,xx,xx,xx] The XML code was stored in separate files, not in the source code. When building the documentation, the function specs from the source code would be combined with the text from the documentation file. It was the responsibility of the writer to ensure that variables mentioned in the documentation body matched the names in the function spec. One thing never said about Erl_Docgen and the old documentation system was that it made writing documentation enjoyable and effortless. That was one thing we wanted to change with the new documentation system. We wanted to make it fun to write documentation, or at least to require less attention to tedious details such as using XML tags correctly. In Erlang/OTP 27, the documentation is written in Markdown and is placed in the source code before the function spec and implementation. Here is the documentation and implementation of lists:duplicate/2 in Erlang/OTP 27: -doc \"\"\" Returns a list containing `N` copies of term `Elem`. _Example:_ ```erlang > lists:duplicate(5, xx). [xx,xx,xx,xx,xx] ``` \"\"\". -spec duplicate(N, Elem) -> List when N :: non_neg_integer(), Elem :: T, List :: [T], T :: term(). duplicate(N, X) when is_integer(N), N >= 0 -> duplicate(N, X, []). duplicate(0, _, L) -> L; duplicate(N, X, L) -> duplicate(N-1, X, [X|L]). ``` The documentation is placed in a triple-quoted string following the -doc attribute. Having the documentation near the spec makes its easy to ensure that the text refers to variables defined in the function spec. Another goal we had was to replace Erl_Docgen with a tool more widely used so that we wouldn’t have to carry the entire burden for maintaining it. We did that by using ExDoc, which is also used by the Elixir language and most, if not all, Elixir projects. An issue that arose is whether it’s advisable to include user documentation within the source code. Wouldn’t this make it much harder to maintain the code? I don’t claim to have a universal response to that concern, but in the case of Erlang/OTP, most actively developed code exists within modules lacking documentation. Typically, OTP applications consist of one or a few modules containing the documented API, while the bulk of the implementation is found in other modules. For example, the interface to the Erlang compiler is found in the compile module, while most of the code being executed resides in one of the other 59 modules of the Compiler application. Similarly, the SSL application comprises 76 modules, of which merely four contain documentation. Another application that is frequently updated is ERTS. However, most of ERTS is implemented in C (and some C++), while much of the actual Erlang code within ERTS is located in modules without documentation. There are, of course, some exceptions to how applications are structured, for example the STDLIB application, where most modules are documented. However, STDLIB is a mature application that is updated relatively infrequently. Triple-Quoted strings # To facilitate writing documentation attributes containing many lines of text, triple-quoted strings as described in EEP 64 have been implemented. Triple-quoted strings come in handy whenever one needs to include multiple line of text in Erlang source code. For example, assume that we want to define a function that outputs some quotations: 1> t:quotes(). \"I always have a quotation for everything - it saves original thinking.\" - Dorothy L. Sayers \"Real stupidity beats artificial intelligence every time.\" - Terry Pratchett ok In Erlang/OTP 26, there are several different ways to do that, but of none of them are particularly satisfying. For example, the text can be put into a single string: quotes() -> S = \"\\\"I always have a quotation for everything - it saves original thinking.\\\" - Dorothy L. Sayers \\\"Real stupidity beats artificial intelligence every time.\\\" - Terry Pratchett\", io:put_chars(S). This works, but is ugly. We must also remember to escape every quote character. A cleaner way is to use multiple strings, one for each line, letting the compiler combine them: quotes() -> S = \"\\\"I always have a quotation for everything -\" \"it saves original thinking.\\\" - Dorothy L. Sayers\" \"\" \"\\\"Real stupidity beats artificial intelligence every time.\\\"\" \"- Terry Pratchett\", io:put_chars(S). That is a little bit nicer, but we’ll need to type more quote characters and we must not forget to add at the end of each string. To make sure that we don’t forget to insert the newlines, we could delegate that mundane chore to the computer: quotes() -> S = [\"\\\"I always have a quotation for everything -\", \"it saves original thinking.\\\" - Dorothy L. Sayers\", \"\", \"\\\"Real stupidity beats artificial intelligence every time.\\\"\", \"- Terry Pratchett\"], io:put_chars(lists:join(\"\", S)), io:nl(). In Erlang/OTP 27, we can use a triple-quoted string: quotes() -> S = \"\"\" \"I always have a quotation for everything - it saves original thinking.\" - Dorothy L. Sayers \"Real stupidity beats artificial intelligence every time.\" - Terry Pratchett \"\"\", io:put_chars(S), io:nl(). The ending \"\"\" determines how much each line in the string should be indented. The same characters that precede \"\"\" are deleted from all lines between the beginning and terminating delimiters. For this particular example, all space characters are removed since all have the same indentation as the terminating \"\"\". Neither quote characters nor backslashes are special in the lines enclosed by the triple-quotes, so there is no need to escape anything. Here is another example to show the versatility of triple-quoted strings: effect_warning() -> \"\"\" f() -> %% Test that the compiler warns for useless tuple building. {a,b,c}, ok. \"\"\". The function returns a string containing a short Erlang function. Assuming that effect_warning/0 is defined in module t, it can be called like so: 1> io:format(\"~ts\", [t:effect_warning()]). f() -> %% Test that the compiler warns for useless tuple building. {a,b,c}, ok. Note that indentation of the Erlang code for function f/0 is retained. For more information, see section String in the Reference Manual. Sigils # Sigils for string literals as described in EEP 66 have been implemented. Continuing with the theme of quotes, let’s explore why sigils were introduced into Erlang, drawing inspiration from the wisdom of ancient Greek philosophers: 1> t:greek_quote(). \"Know thyself\" (Greek: Γνῶθι σαυτόν) ok In Erlang/OTP 26, this can be implemented as follows: greek_quote() -> S = \"\\\"Know thyself\\\" (Greek: Γνῶθι σαυτόν)\", io:format(\"~ts\", [S]). At this point, we get some customer feedback indicating that the modules containing all the quotes are consuming an excessive amount of memory. Each character in a string consumes 16 bytes of memory (on a 64-bit computer). That could be reduced to one byte for each character if a binary were to be used instead of a string. (Actually, one byte for each US ASCII character and two bytes for each Greek letter.) That change should be really easy. Let’s try: greek_quote() -> S = >, io:format(\"~ts\", [S]). That works for the English text, but not for the Greek characters: 2> t:greek_quote(). \"Know thyself\" (Greek: ½ö¸¹ Ã±ÅÄÌ½) What’s wrong? Strings in binary expression are by default assumed to be a sequence of byte-size characters. Therefore, this expression: 1> >. > is syntactic sugar for: 2> >. > It is necessary to specify that the characters are to be encoded as UTF-8 encoded characters by appending an /utf8 suffix: greek_quote() -> S = >, io:format(\"~ts\", [S]). That works because > is syntactic sugar for >. Enter sigils. greek_quote() -> S = ~B[\"Know thyself\" (Greek: Γνῶθι σαυτόν)], io:format(\"~ts\", [S]). The ~ character begins a sigil. It is usually followed by a letter that indicates how the characters in the string should be interpreted or encoded. In this case the character B means that the characters should be put into a binary in UTF-8 encoding, and also that that no escape characters are allowed. After B follows the start delimiter, in this case [. Since no escape characters are allowed, it is necessary to choose delimiters that don’t occur in the string contents. After the contents follows the end delimiter, in this case ]. The B sigil is the default sigil that is used if the letter following ~ is omitted. Thus we get the same binary and the same output if we omit the B: greek_quote() -> S = ~[\"Know thyself\" (Greek: Γνῶθι σαυτόν)], io:format(\"~ts\", [S]). Sigils can also be used to begin a triple-quoted string. Returning to the quotations example from the previous section, a binary literal can be created by inserting ~ before the leading \"\"\": quotes() -> S = ~\"\"\" \"I always have a quotation for everything - it saves original thinking.\" - Dorothy L. Sayers \"Real stupidity beats artificial intelligence every time.\" - Terry Pratchett \"\"\", io:put_chars(S), io:nl(). Here follows a few quick examples to show the other sigils. ~b creates a binary in the same way as ~B, except that backslashes will be interpreted as an escape character. This can be useful if one want to insert control characters such as TAB (\\t) into a string: 1> ~b\"abc\\txyz\". > Here we used the \" character as delimiters as it is not used within the string. ~s creates a string in the usual way. The only useful way it differs from a plain quoted string is that the delimiters can be switched. That way, one can avoid the hassle of escaping quote characters and still get to use control characters such as TAB: 2> ~s{\"abc\\txyz\"}. \"\\\"abc\\txyz\\\"\" ~S creates a string, but does not support escaping of characters within the string, similar to ~B. For more information, see section Sigil in the Reference Manual. No need to enable feature maybe # The maybe expression was introduced as a feature in Erlang/OTP 25. In that release, it was necessary to enable it both in the compiler and the runtime system. Erlang/OTP 26 lifted the necessity to enable maybe in the runtime system. Now in Erlang/OTP 27, maybe is enabled by default in the compiler. In the example from last year’s blog post, the line -feature(maybe_expr, enable). can now be removed: $ cat t.erl -module(t). -export([listen_port/2]). listen_port(Port, Options) -> maybe {ok, ListenSocket} ?= inet_tcp:listen(Port, Options), {ok, Address} ?= inet:sockname(ListenSocket), {ok, {ListenSocket, Address}} end. $ erlc t.erl $ erl Erlang/OTP 27 . . . Eshell V15.0 (abort with ^G) 1> t:listen_port(50000, []). {ok,{#Port,{{0,0,0,0},50000}}} When maybe is used as an atom, it need to be quoted. For example: will_succeed(. . .) -> yes; will_succeed(. . .) -> no; . . . will_succeed(_) -> 'maybe'. Alternatively, it is still possible to disable the maybe_expr feature. With the feature disabled, maybe can be used as an atom without quotes. One way to disable maybe is to use the -disable-feature option when compiling. For example: erlc -disable-feature maybe_expr *.erl Another way to disable maybe is to add the following directive to the source code: -feature(maybe_expr, disable). The new json module # There is a new module json in STDLIB for generating and parsing JSON (JavaScript Object Notation). It is implemented by Michał Muskała who has also implemented the Jason library for Elixir. Jason is known for being faster than other pure Erlang or Elixir JSON libraries. The json module is not a pure translation of the Elixir code for Jason, but a re-implementation with even better performance than Jason. As an example, imagine that we have this file quotes.json with quotes from the film Jason and the Argonauts: [ {\"quote\": \"The gods are best served by those who need their help the least.\", \"attribution\": \"Zeus\", \"verified\": true}, {\"quote\": \"Now the voyage is over, I don't want any trouble to begin.\", \"attribution\": \"Jason\", \"verified\": true} ] The JSON contents of the file can be be decoded by calling json:decode/1: 1> {ok,JSON} = file:read_file(\"quotes.json\"). {ok,>} 2> json:decode(JSON). [#{> => >, > => >, > => true}, #{> => >, > => >, > => true}] By default, for safety, the keys for objects are translated to binaries. Using atoms could open up for denial-of-service attacks if a malicious JSON object would define millions of unique keys. For convenience, it is still possible to convert keys to atoms in a safe way by using a decoder callback. Here is an example: 1> Push = fun(Key, Value, Acc) -> [{binary_to_existing_atom(Key), Value}Acc] end. #Fun This fun converts the key for a JSON object to an existing atom, or raises an exception if no such atom exists. Since this example is run from the shell, we’ll need to make sure that all possible keys are known atoms: 2> {quote,attribution,verified}. {quote,attribution,verified} This would normally not be necessary when JSON decoding is done in an Erlang module, because the atoms to be used as keys would presumably be defined naturally by being used when processing the decoded JSON objects. With this preparation done, the JSON decoder can be called using the Push fun as an object_push decoder callback: 3> {Qs,_,>} = json:decode(JSON, [], #{object_push => Push}), Qs. [#{quote => >, attribution => >,verified => true}, #{quote => >, attribution => >,verified => true}] The json:encode/1 function encodes an Erlang term to JSON: 4> io:format(\"~ts\", [json:encode(Qs)]). [{\"quote\":\"The gods are best served by those who need their help the least.\",\"attribution\":\"Zeus\",\"verified\":true},{\"quote\":\"Now the voyage is over, I don't want any trouble to begin.\",\"attribution\":\"Jason\",\"verified\":true}] ok The encoder accepts binaries, atoms, and integer as keys for objects, so there is no need to customize encoding for this particular example. However, when necessary, it is possible to customize the encoding. For example, assume that we want to store each quotation in a three-tuple instead of in a map: 1> Q = [{~\"The gods are best served by those who need their help the least.\", ~\"Zeus\",true}, {~\"Now the voyage is over, I don't want any trouble to begin.\", ~\"Jason\",true}]. [{>, >,true}, {>, >,true}] The json:encode/1 function does not handle that format by default, but it can be handled by defining an encoder function: quote_encoder({Q, A, V}, Encode) when is_binary(Q), is_binary(A), is_boolean(V) -> json:encode_map(#{quote => Q, attribution => A, verified => V}, Encode); quote_encoder(Other, Encode) -> json:encode_value(Other, Encode). The first clause matches a tuple of size three that looks like a quotation. If it matches, it is converted to the map representation for a JSON object, which is then converted by the utility function json:encode_map/1 to JSON. The second clause handles all other Erlang terms by calling the default encoding function json:encode_value/2 for converting a term to JSON. Assuming that this function is defined in module t, the conversion to JSON is invoked as follows: 2> io:format(\"~ts\", [json:encode(Q, fun t:quote_encoder/2)]). [{\"quote\":\"The gods are best served by those who need their help the least.\",\"attribution\":\"Zeus\",\"verified\":true},{\"quote\":\"Now the voyage is over, I don't want any trouble to begin.\",\"attribution\":\"Jason\",\"verified\":true}] The JSON encoder will call the callback recursively for given term. That can be clearly seen if we modify the second clause of quote_encoder/2 to also print the value of Other: 3> json:encode(Q, fun t:quote_encoder/2), ok. -- [{>, >,true}, {>, >,true}] -- > -- > -- > -- > -- > -- true -- > -- > -- > -- > -- > -- true Process labels # As an help for debugging or observing in general, labels can be now set on non-registered processes using proc_lib:set_label/1. The label is an arbitrary term. The label is shown by the the shell command i/0 and by observer. They can also be found in the dictionary section of a crash dump. Here is an example where five labeled quote-handler processes are started and inspected: 1> F = fun(I) -> spawn_link(fun() -> proc_lib:set_label({quote_handler, I}), receive _ -> ok end end) end. #Fun 2> Ps = [F(I) || I ,,,,] 3> proc_lib:get_label(hd(Ps)). {quote_handler,1} 4> i(). Pid Initial Call Heap Reds Msgs Registered Current Function Stackerl_init:start/2 987 5347 0 init init:loop/1 2 . . . {quote_handler,1} prim_eval:'receive'/2 9erlang:apply/2 233 4006 0 {quote_handler,2} prim_eval:'receive'/2 9erlang:apply/2 233 4006 0 {quote_handler,3} prim_eval:'receive'/2 9erlang:apply/2 233 4006 0 {quote_handler,4} prim_eval:'receive'/2 9erlang:apply/2 233 4006 0 {quote_handler,5} prim_eval:'receive'/2 9 Total642876 1156835 0438 ok The SSH and and SSL applications have been updated to label the processes they create. New functionality in STDLIB # New utility functions for set modules # The three sets modules in STDLIB — sets, gb_sets, and ordsets — have new functions is_equal/2, map/2, and filtermap/2. The is_equal/2 function is useful when one needs to find out whether two sets contain the same elements. Comparing with == or =:= is not always reliable. For example: 1> Seq = lists:seq(1, 20, 2). [1,3,5,7,9,11,13,15,17,19] 2> gb_sets:from_list(Seq) == gb_sets:delete(10, gb_sets:from_list([10|Seq])). false 3> gb_sets:is_equal(gb_sets:from_list(Seq), gb_sets:delete(10, gb_sets:from_list([10|Seq]))). true The map/2 maps the element of a set, producing a new set: 4> Seq = lists:seq(1, 20, 2). [1,3,5,7,9,11,13,15,17,19] #Fun 5> ordsets:to_list(ordsets:map(fun(N) -> N div 4 end, ordsets:from_list(Seq))). [0,1,2,3,4] The filtermap/2 function can map and filter at the same time. Here is an example showing how to multiply each integer in a set by 100 and remove non-integers: 1> Mixed = [1,2,3,a,b,c]. [1,2,3,a,b,c] 2> F = fun(N) when is_integer(N) -> {true,N * 100}; (_) -> false end. #Fun 3> sets:to_list(sets:filtermap(F, sets:from_list(Mixed))). [300,200,100] New timer convenience functions that take funs # In Erlang/OTP 26, the functions in the timer module don’t accept funs. It is certainly possibly to pass in a fun in the argument for erlang:apply/2, but if one makes a mistake it will be only be noticed when the timer expires: 1> timer:apply_after(10, erlang, apply, [fun() -> io:put_chars(\"now!\") end]). {ok,{once,#Ref}} =ERROR REPORT==== 10-Apr-2024::05:56:43.894073 === Error in processwith exit value: {undef,[{erlang,apply,[#Fun],[]}]} Here the empty argument list for the fun was forgotten. It should have been: 2> timer:apply_after(10, erlang, apply, [fun() -> io:put_chars(\"now!\") end, []]). {ok,{once,#Ref}} now! In Erlang/OTP 27, using a fun is much easier: 1> timer:apply_after(10, fun() -> io:put_chars(\"now!\") end). {ok,{once,#Ref}} now! In systems that use hot code updating, using a local fun for a long-running timer is not ideal. The code that defines the fun could have been replaced, and when the timer finally expires the call will fail. Therefore, it is also possible to pass a fun as well as its arguments, making it possible to use use a remote fun that will survive hot code updating: 2> timer:apply_after(10, fun io:put_chars/1, [\"now\"]). {ok,{once,#Ref}} now The apply_interval/* and apply_repeatedly/* functions now also accept funs. New ets functions # The new functions ets:first_lookup/1 and ets:next_lookup/2 simplifies and speeds up traversing an ETS table: 1> T = ets:new(example, [ordered_set]). #Ref 2> ets:insert(T, [{I,I*I} || I{K1,_} = ets:first_lookup(T). {1,[{1,1}]} 4> {K2,_} = ets:next_lookup(T, K1). {2,[{2,4}]} 5> {K3,_} = ets:next_lookup(T, K2). {3,[{3,9}]} 6> {K4,_} = ets:next_lookup(T, K3). {4,[{4,16}]} Similarly, ets:last_lookup/1 and ets:prev_lookup/2 can be used to traverse a table in reverse order. The new function ets:update_element/4 is similar to ets:update_element/3, but makes it possible to supply a default object when there is no existing object with the given key: 1> T = ets:new(example, []). #Ref 2> ets:update_element(T, a, {2, true}, {a, true}). true 3> ets:lookup(T, a). [{a,true}] New SSL client-side stapling support # A new feature in the SSL client in Erlang/OTP 27 is support for OCSP stapling for easier and faster verification of the revocation status of server certificates. With OCSP stapling, the SSL client can streamline the validation of revocation status. Normally the client would have to query the CA (Certificate Authority) using OCSP (Online Certificate Status Protocol) to ensure that the server’s certificate has not been revoked. The basic idea behind OCSP stapling is that the server itself will proactively query the CA regarding the revocation status for its own certificate and “staple” the time-stamped OCSP response from the CA to the certificate. When a client connects, the server passes along its OCSP-stapled certificate to the client. To verify the revocation status, the client only needs to check that the OCSP response was signed by the CA. Here follows an example showing how OCSP stapling can be enabled in the SSL client: 1> ssl:start(). ok 2> {ok, Socket} = ssl:connect(\"duckduckgo.com\", 443, [{cacerts, public_key:cacerts_get()}, {stapling, staple}]). {ok,{sslsocket,{gen_tcp,#Port,tls_connection,undefined}, [,]}} tprof: Yet another profiling tool # In Erlang/OTP 27, the new profiling tool tprof joins the existing profiling tools cprof, eprof, and fprof. Why introduce a new profiling tool? One reason is that cprof and eprof perform similar profiling tasks, but the naming of the API functions are different. It is quite easy to mix up the names when running one tool after the other, and running them after each other is not uncommon. For example, when trying to find a bottleneck in a complex running Erlang system, one approcach is to first use cprof to get a rough idea of the general part of the system where a bottleneck could be located. After that, eprof is run on a limited part of the system trying to narrow it down. Directly running eprof on a large Erlang application could overload it and bring it down. Using tprof, the same function is used for both counting calls and measuring the time for each call. Here is how to count calls when lists:seq(1, 1000) is called: 1> tprof:profile(lists, seq, [1, 1000], #{type => call_count}). FUNCTION CALLS [ %] lists:seq/2 1 [ 0.40] lists:seq_loop/3 251 [99.60] [100.0] ok Note that call counting is always done for all processes. The bulk of the work for lists:seq/2 is done in lists:seq_loop/3, which was called 251 times. Since we asked for 1000 integers, we reach the conclusion that each tail-recursive call to seq_loop/3 creates four list elements at once. That can be confirmed by looking at the source code. To measure the time for each call, we only need to replace call_count with call_time: 2> tprof:profile(lists, seq, [1, 1000], #{type => call_time}). ****** Process-- 100.00% of total *** FUNCTION CALLS TIME (μs) PER CALL [ %] lists:seq/2 1 0 0.00 [ 0.00] lists:seq_loop/3 251 50 0.20 [100.00]50 [ 100.0] ok Call time is only measured the process that called tprof:profile/4 and any process spawned by that process. By replacing call_time with call_memory the amount of memory consumed by each call will be measured: 3> tprof:profile(lists, seq, [1, 1000], #{type => call_memory}). ****** Process-- 100.00% of total *** FUNCTION CALLS WORDS PER CALL [ %] lists:seq_loop/3 251 2000 7.97 [100.00] 2000 [ 100.0] ok The total number of words created is 2000, which make sense since each list element needs 2 words. The number of words consumed per call is 2000 / 251, which is approximately 7.97 or almost 8. That also makes sense since each tail-recursive call creates 4 list elements, or 8 words, and there are 250 such calls. The remaining call creates the final empty list ([]). call_memory tracing was introduced in the runtime system in Erlang/OTP 26, but was not exposed in any existing profiling tool because it didn’t really fit in any of them. It made more sense to enable support for it in a new tool. Multiple trace sessions # Tracing makes it possible to observe, debug, analyse, and measure the performance of a running Erlang system. Over the year, numerous tools using tracing has been developed. In Erlang/OTP alone, several tools leverage tracing for different purposes: dbg, ttb - general tracing tools etop - similar to top in Unix eprof, cprof, fprof, tprof - profiling tools et - event tracer debugger - uses tracing internally when evaluating receive expressions In Erlang/OTP 26 and earlier tracing had some limitations: There could only be a single tracer per traced process. The configuration for which processes and functions to trace were global within the runtime system. Those limitations meant that different tracing tools could easily step on each other’s toes. The treacherous part was that using multiple tracing tools at the same time would seem to work for a while… until it didn’t. In Erlang/OTP 27, multiple trace sessions can be created. Each trace session has its own tracer process and configuration for which processes and functions to trace. To create a trace session and set up tracing, there is the new trace module in the Kernel application. Tools that set up tracing using that module will no longer interfere with each other. Tools that use the old API will share a single global trace session. In the initial Erlang/OTP 27 release, some of the tools using tracing have been updated to use trace sessions. Other tools will be updated in upcoming maintenance releases. We have tried to design the new API in a way to make it relatively easy for maintainers of external tools to migrate their code. Apart from the names of the functions and the first argument (the session argument), the other arguments and their semantics are almost entirely identical to the old API. Quick trace session example # Here is an example to show how the new API is used. First we’ll need a tracer process that prints all trace messages it receives: 1> Tracer = spawn(fun F() -> receive M -> io:format(\"== ~p ==\", [M]), F() end end).Having a tracer process, we can create a trace session: 2> Session = trace:session_create(my_session, Tracer, []). {#Ref,{my_session,0}} Next we turn on call tracing on the current process: 3> trace:process(Session, self(), true, [call]). 1 Make sure that module array is loaded and trace all calls in it: 4> l(array). {module,array} 5> trace:function(Session, {array,'_','_'}, [], [local]). 89 Next create a new array: 6> array:new(10). == {trace,,call,{array,new,\"\"}} == {array,10,0,undefined,10} == {trace,,call,{array,new_0,[10,0,false]}} == == {trace,,call,{array,new_1,[\"\",0,false,undefined]}} == == {trace,,call,{array,new_1,[[],10,true,undefined]}} == == {trace,,call,{array,new,[10,true,undefined]}} == == {trace,,call,{array,find_max,\"\\t\"}} == Note that trace messages are randomly intermingled with the return value of the call. When we are done, we can destroy the session: 7> trace:session_destroy(Session). If we don’t destroy the session, it will be automatically destroyed when the last reference to it goes away. Native coverage support # The Cover tool for determining code coverage has long been part of Erlang/OTP. Traditionally, Cover collected its coverage metrics without the help of any specialized functionality in the runtime system. To count how many times each line in a module was executed, Cover instrumented abstract code for the module by inserting calls to ets:update_counter/3 on each executable line. That worked, but the cover-instrumented Erlang code would always run slower. How much slower depended on the nature of the code being tested. In Erlang/OTP 27, runtime systems supporting the JIT (just-in-time compiler) can now collect coverage metrics in the runtime system with minimal performance overhead. The Cover tool has been updated to automatically take advantage of native coverage support if supported by the runtime system. When running the test suites for most OTP applications, there is no noticeable difference in execution time running with and without Cover. The native coverage support can also be used directly for performing measurements that Cover cannot accomplish, such as collecting metrics for code that is executed while the Erlang runtime system is starting. Here is a quick example showing how we can collect coverage metrics for init, which is the first module executed when starting up the runtime system. First we need to instruct the runtime system to instrument all functions in all modules with extra code to count the number of times each function is called: $ bin/erl +JPcover function_counters The runtime system starts normally. We can now read out the counters for the init module: 1> lists:reverse(lists:keysort(2, code:get_coverage(function, init))). [{{archive_extension,0},392}, {{get_argument1,2},198}, {{objfile_extension,0},101}, {{boot_loop,2},64}, {{request,1},55}, {{to_strings,1},44}, {{do_handle_msg,2},38}, {{handle_msg,2},38}, {{b2s,1},38}, {{get_argument,2},33}, {{get_argument,1},31}, {{'-load_modules/2-lc$^0/1-0,1},30}, {{'-load_modules/2-lc$^1/1-2,1},30}, {{'-load_modules/2-lc$^2/1-3,1},30}, {{'-load_modules/2-lc$^3/1-4,1},30}, {{extract_var,2},30}, {{'-prepare_loading_fun/0-fun-0,3},29}, {{eval_script,2},23}, {{append,1},18}, {{get_arguments,1},18}, {{reverse,1},17}, {{check,2},17}, {{ensure_loaded,2},16}, {{ensure_loaded,1},16}, {{do_load_module,2},14}, {{do_ensure_loaded,2},14}, {{get_flag_args,...},12}, {{...},...}, {...}|...] The returned list of counter values for each function is sorted in descending order on the number of time each function was executed. For more information, see Native Coverage Support in the documentation for the code module. Deprecating archives # Archives is experimental functionality that has existed in Erlang/OTP for a long time. Part of the support for archives is deprecated in Erlang/OTP 27. The reason is that the performance of code loading from archives has never been great. Even worse is that the very existence of the archive functionality degrades the performance of code loading even when no archives are used, and complicates or prevents optimizations aimed at reducing startup time. In Erlang/OTP 27, the following functionality is deprecated: Using archives for packaging a single application or parts of a single application into an archive file that is included in the code path. This functionality will likely be removed in Erlang/OTP 28. The code:lib_dir/2 function. This function was introduced to allow reading files inside archives. In Erlang/OTP 28, the function itself will not be removed, but it will most likely no longer support looking into archives. All functionality to handle archives in module erl_prim_loader. That same functionality is likely to be removed in Erlang/OTP 28. The -code_path_choice flag for erl. In Erlang/OTP 27, the default has changed from relaxed to strict. This flag is likely to be removed in Erlang/OTP 28. In order to use archives in Erlang/OTP 27, it is necessary to use the flag -code_path_choice relaxed. Using a single archive in an Escript is not deprecated # An archive can still be used to hold all files needed by an Escript. However, to access files in the archive (for example, to read templates or other data files), the only supported way guaranteed to work in future releases is to use the escript:extract/2 function.",
    "commentLink": "https://news.ycombinator.com/item?id=40424982",
    "commentBody": "Erlang/OTP 27 Highlights (erlang.org)201 points by asabil 12 hours agohidepastfavorite35 comments aloha2436 7 hours agoReading about Erlang always feels like getting messages from an alternate dimension where we as an industry made much better choices in the 90s about how we write distributed software. reply spinningslate 6 hours agoparentthis. Erlang's concurrency support is one of those things you can't unsee. Going back to sequential-by-design languages (which is pretty much every other industrial quality language bar go[1]) just feels cumbersome: C/C++/C#/Python/...: \"You want concurrency? Sure. We have OS processes, and threads, and this cool new async doohickey. Pick whatever you fancy! Oh, but by the way: you can't use very many processes cos they're _really_ heavyweight. You can have lots more threads, but not too many, and beware corrupting the shared state. Asyc though, you can have _loads_ of things going on at once. Just, y'know, don't mix the colours up\". With Erlang/Elixir it's just: \"You want concurrency? Sure, here's Erlang processes. You can have millions of them. Oh, you need to communicate between them? Yep, no probs, messages and mailboxes. What's that? Error handling? Yep, got that covered too - meet the Supervisors\" -- [1] Counting Elixir as \"Erlang\" in this context given it also sits on the BEAM VM. reply binary132 4 hours agorootparentC++: zero-cost leaky abstraction with unlimited cognitive and development cost Functional programming languages: Unlimited good abstractions of unknown cost I don't feel like there's a great third option. Go is pretty good. reply TwentyPosts 52 minutes agorootparentI hate to be that guy, but if you want \"C++ but also functional programming\" then... Well, then Rust is in fact the language which you're looking for. reply neonsunset 4 hours agorootparentprevC# tasks are lightweight, and I'd expect for per-task overhead to be significantly lower than that of Erlang's. e.g.: var delay = Task.Delay(3_000); var tasks = Enumerable .Repeat(async () => await delay, 1_000_000) .Select(f => f()); Console.WriteLine(\"Waiting for 1M tasks...\"); await Task.WhenAll(tasks); Console.WriteLine(\"Finished!\"); edit: consider suggesting a comparable example in Erlang before downvoting :) reply rdtsc 4 hours agorootparentDo they have isolated heaps and can they be preempted, even if they spin in an infinite loop doing some CPU intensive things? reply neonsunset 4 hours agorootparentTasks are not processes, and that would be a wrong thing to do, and so would be \"isolated heaps\" given performance requirements faced by .NET - you do want to share memory through concurrent data structures (which e.g. channels are despite what go apologists say), and easily await them when you want to. CSP, while is nice on paper, has the same issues as e.g. partitioning in Kafka, just at a much lower level where it becomes critical bottleneck - you can't trivially \"fork\" and \"join\" the flows of execution, which well-implemented async model enables. It's not \"what about x\" but rather how you end up applying the concurrent model in practice, and C# tasks allow you to idiomatically mix in concurrency and/or parallelism in otherwise regular code (as you can see in the example). I'm just clarifying on the parent comment that concurrency in .NET is not like in Java/C++/Python (even if the latter does share similarities, there are constraints of Python itself). reply rdtsc 3 hours agorootparent> and that would be a wrong thing to do, and so would be \"isolated heaps\" - you do want to share memory through concurrent data structures (which e.g. channels are despite what go apologists say), and easily await them when you want to. It depends on the context. In some contexts absolutely not. If we share memory, and these tasks start modifying global data or taking locks and then crash, can those tasks be safely restarted, can we reason about the state of the whole node any longer? > CSP, while is nice on paper Not sure if Erlang's module is CSP or Actor's (it started as neither actually) but it's not just nice on paper. We have nodes with millions of concurrent processes running comfortably, I know they can crash or I can restart various subsets of them safely. That's no small thing and it's not just paper-theoretical. reply neonsunset 2 hours agorootparentWhat value does isolated heap offer for memory-safe languages? Task exceptions can simply be handled via try-catch at the desired level. Millions of concurrently handled tasks is not that high of a number for .NET's threadpool. It's one thing among many that is \"nothingburger\" in .NET ecosystem which somehow ends up being sold as major advantage in other languages (you can see it with other features too - Nest.js as a \"major improvement\" for back-end, while it just looks like something we had 10 years ago, \"structured concurrency\" which is simple task interleaving, etc.). It's a different, lower-level model, but it comes with the fact that you are not locked into particular (even if good) way of doing concurrency in Erlang. reply asabil 59 minutes agorootparentGC determinism is one of the things you get. Another one is non cooperative asynchronous termination. reply neonsunset 23 minutes agorootparentPretty much all efficient GC implementations are inherently non-deterministic, even if predictable. How can this improve predictability of GC impact? reply sidkshatriya 5 hours agoparentprev> where we as an industry made much better choices in the 90s about how we write distributed software. Erlang is a nice piece of software. However, let us not dismiss the massive progress the world of distributed software has made since 1990s _not_ involving Erlang too. Look at the scale at which we _reliably_ access video, audio, email, messaging, e-commerce/trading on distributed systems around the world ! At high reliability too ! Google, Facebook, Amazon, Netflix, Microsoft, NYSE/NASDAQ, ... -- Imagine the millions or even billions of computer systems working, cooperating in various private and public \"clouds\". Apart from a few prominent systems here and there (e.g. erlang at WhatsApp), most of these systems _DONT_ use erlang. For various reasons Erlang has _not_ been chosen by thousands of software architects when they choose to build their next distributed system. Even though erlang lets us build a distributed system with lots of properties out-of-the box easily, let's talk about some failings of Erlang: - Erlang is not statically typed language unlike Java, Rust, C/C++ etc. This means an erlang compiler cannot create code that will run as fast as the aforementioned languages. The compiler simply just does not have that much information available during compile time - Not being statically typed also makes it a bit more difficult to refactor the codebase. Would you be able to refactor a 1 million line Rust code base more easily or a 100,000 line erlang code base (even if you have used Dialyzer). My money is on Rust. - Not being statically typed also means that you cannot verify or mathematically prove properties about your system using various techniques as easily TL;DR -- A small team can build a highly capable system on erlang quite easily in 2024. That small team would probbly take longer if they used Rust/C++/Java because those languages are more low level and take more time for development. But if you can throw some $$ on the project, in the long run a system built in Rust/C++/JVM can run more efficiently (and be maintained more easily) on a fewer machines using specialized code written in Rust/C++/Java etc. In other words it's not everyday you need to build a distributed system -- when you do, it makes sense to specialize and build it on a technology stack that may be a bit lower-level and statically typed. This comment is already too long enough. I like Erlang, it has some nice properties but when building distributed systems other technology stacks can also offer some other great advantages too. reply worthless-trash 5 hours agorootparent> - Not being statically typed also makes it a bit more difficult to refactor > the codebase. Would you be able to refactor a 1 million line Rust code base > more easily or a 100,000 line erlang code base (even if you have used > Dialyzer). My money is on Rust. I have found that refactoring erlang is NOT like refactoring code in other languages, non trivial refactoring in rust is a LOT more complicated however I do understand the fuzzy feelings you get when type-safe code compiles correctly. Most erlang refactoring that I see needing to be done is simply reapplying a different pattern to the gen_server or distributing load differently. I believe if refactoring is a \"complex problem\", the development team had not designed with OTP behaviors in mind. My view may be because I have limited experience in refactoring my erlang due to being a solo developer and my mind is stuck in OTP land, please correct me if you've experienced it differently, I feel that you're perhaps painting the picture a little unfairly there. If programmers need type-safeness for BEAM and I believe Gleam Language supplies the security blanket that other languages provide. From my limited experience it does NOT provide any additional \"speed\" (I expect there are not many compiler optimisations that end up down on the BEAM level) however it does give you that level of confidence that you're not going to be passing garbage data through your functions. I haven't taken anything you have said as a personal (or even against erlang), thank you for the discussion points. reply neillyons 8 hours agoprevReally like the new Erlang 27 docs https://www.erlang.org/doc/readme.html reply victorbjorklund 57 minutes agoparentFinally. I never liked erlang doc style. reply mikl 3 hours agoparentprevIt’s a nice example of cross-polination, they essentially adapted Elixir’s documentation system for Erlang. reply ollysb 6 hours agoparentprevWhat a breath of fresh air, the old docs really were showing their age. reply sergiotapia 5 hours agoparentprevWoah! They copied the excellent hexdocs format, this is great. https://hexdocs.pm/nanoid/readme.html reply adregan 4 hours agorootparentBetter still—they didn’t copy ExDoc, they are using it exactly as it is! It’s really refreshing to see the sharing between Erlang and Elixir. Having been stuck in the js world my whole career, it’s really cool to watch a community that’s rooted in collaboration instead of competition. reply sabzetro 7 hours agoprevHyped about json becoming a first class citizen. As an Elixir engineer and daily Jason user it will be great to rely on stdlib functionality. Jason is great, regardless! reply mindri0t 6 hours agoparentReading the notes it appears it was implemented by Michał Muskała who also implemented Jason (and seems it is faster too) reply asa400 2 hours agorootparentIndeed it is: https://zeroclarkthirty.com/2024-04-21-benchmarking-erlangs-... reply nickpeterson 6 hours agoprevI think I’m going to just break down and learn erlang. I’ve been interested in it for awhile but mostly work in f#. I recently started looking at gleam but I don’t think learning it with no knowledge of OTP is a good idea. reply systems 5 hours agoparentthis link might help you with learn otp with gleam https://github.com/bcpeinhardt/learn_otp_with_gleam reply worthless-trash 4 hours agoparentprevI think you can use it without OTP with very little issues, I did write this, which is a port of the erlang design principals for gleam: https://github.com/wmealing/gleam-otp-design-principals/blob... reply systems 5 hours agoprevJust mentioning gleam https://gleam.run/ which is a typed (and very opinionated) languages that runs on the BEAM reply metadat 3 hours agoprevThe `maybe_expr' meta pattern matching fallback mechanic is nice, and can surely help avoid a lot of boilerplate code while simultaneously encapsulating the logic in a structure which is easy to read and reason about. It's also not a thing in any other programming language I've learned- C, Java/Scala/C#/C++, Go, Javascript, Tcl, Bash(lol), PHP, Forth, ML, and so on. I had to look up it's usage though, because I'm new to both Erlang/BEAM and Elixir. https://chiroptical.dev/posts/2024-03-04-erlang-maybe_expr.h... reply neonsunset 2 hours agoparentYou do realize that Rust, C#/F#, Kotlin/Scala, now Java to an extent, pretty much all FP and many other languages have extensive pattern matching support? reply adregan 1 hour agorootparentIn fairness, it's doing a bit more than \"just\" pattern matching. It's very similar to the `with` expression in elixir: https://hexdocs.pm/elixir/1.16.3/Kernel.SpecialForms.html#wi... You can use it to execute a series of functions returning `either`-ish tuples and build up a railway oriented program. reply cpursley 7 hours agoprevJSON is a big one, great job Erlang folks! reply biorach 7 hours agoprev> The Erlang/OTP documentation before Erlang/OTP 27 was authored in XML ouch reply sph 8 hours agoprev [–] I love the new documentation site: https://www.erlang.org/doc/apps/stdlib/lists#duplicate/2 Looks a lot like Elixir's. The previous one was functional, but a little barebones. A little colour, hyperlinking and syntax highlighting goes a long way. Also, navigation seems to be improved. I always lost my way navigating across Erlang modules to find a specific function. The entire Erlang/OTP ecosystem got a boost of mind share with the explosion in popularity of Elixir, and it's so nice to see it improve at breakneck pace [1], with some cross-pollination between Erlang itself and Elixir. The ideas of Armstrong, Virding and Williams are in many ways far ahead than a lot of mainstream languages, and they were long overdue a revival under the spotlight. Keep up the good work! 1: yet, it's still the most rock-solid platform to build services upon, and you can quote me on that. reply andy_ppp 8 hours agoparentI love that the Erlang guys seem to be so open to Elixir being a big part of their ecosystem even relying on ExDoc for their new documentation system (I think everyone can see it's a big improvement over the brutalist UI that was there before). A lot of software projects would be ego driven rather than pragmatic on something like this. reply cschmatzler 7 hours agorootparentExDoc is arguably one of the nicest documentation systems out there, and Erlang moving to it means two things: 1. The Erlang devs do not need to implement and maintain their own anymore. 2. ExDoc will improve faster since people previously working on Erlang’s documentation system shift to it. It’s really a win for both, and I love it. reply mikl 3 hours agoparentprev [–] It doesn’t just look like Elixir’s docs, Erlang has adopted ExDoc, Elixir’s documentation system. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Erlang/OTP 27 introduces a revamped documentation system using Markdown and ExDoc, enhancing consistency by embedding documentation in source code.",
      "Key features include triple-quoted strings for easier multi-line text handling, sigils for better string literal management, a new JSON module, process labels, and enhanced SSL client-side stapling.",
      "Additional updates include multiple trace sessions, native coverage support, a new profiling tool (tprof), new timer functions, ETS (Erlang Term Storage) enhancements, and the deprecation of archives due to performance issues."
    ],
    "commentSummary": [
      "Erlang/OTP 27 is highlighted for its superior concurrency model, lightweight processes, and robust error handling, making it efficient for distributed software compared to traditional languages like C++, C#, and Python.",
      "The new Erlang 27 documentation, adopting Elixir’s system and integrating with languages like Gleam, is positively received, with the adoption of ExDoc seen as a significant improvement.",
      "Despite its strengths, Erlang's lack of static typing is noted as a potential drawback for performance and maintainability in larger projects."
    ],
    "points": 201,
    "commentCount": 35,
    "retryCount": 0,
    "time": 1716274376
  },
  {
    "id": 40424536,
    "title": "Reimagining Learning: AI, Project-Based Education, and the Future of Personalized Tutoring",
    "originLink": "https://andymatuschak.org/hmwl/",
    "originBody": "Your ideal learning environment # Talks about learning technology often center on technology. Instead, I want to begin by asking: what do you want learning to be like—for yourself? If you could snap your fingers and drop yourself into a perfect learning environment, what’s your ideal? One way to start thinking about this question is to ask: what were the most rewarding high-growth periods of your life? I’ve noticed two patterns in answers to this question: first, people will tell me about a time when they learned a lot, but learning wasn’t the point. Instead, they were immersed in a situation with real personal meaning—like a startup, a research project, an artistic urge, or just a fiery curiosity. They dove in, got their hands dirty, and learned whatever was important along the way. And secondly: in these stories, learning really worked. People emerged feeling transformed, newly capable, filled with insight and understanding that has stayed with them years later. These stories are so vivid because learning isn’t usually like this. People are often telling me somewhat wistfully about experiences that happened years or decades earlier. Learning rarely feels so subordinated to an authentic pursuit. Often if we try to “just dive in”, we hit a brick wall, or find ourselves uneasily cargo culting others, with no real understanding. Why can’t we “just dive in” all the time? Instead it often feels like we have to put our aims on hold while we go do some homework—learn properly. Worse, learning so often just doesn’t really work! We take the class, we read the book… but when we try to put that knowledge into practice, it’s fragile; it doesn’t transfer well. Then we’ll often find that we’ve forgotten half of it by the time we try to use it. Why does learning so often fail to actually work? These questions connect to an age-old conflict among educators and learning scientists—between implicit learning (also called discovery learning, inquiry learning, or situated learning), and guided learning (often represented by cognitive psychologists). Advocates of implicit learning methods argue that we should prioritize discovery, motivation, authentic involvement, and being situated in a community of practice. In the opposing camp, cognitive psychologists argue that you really do need to pay attention to architecture of cognition, long-term memory, procedural fluency, and to scaffold appropriately for cognitive load. In my view, each of these points of view contains a lot of truth. And they each wrongly deny the other’s position, to their mutual detriment. Implicit learning aptly recognizes meaning and emotion, but ignores the often decisive constraints of cognition—what we usually need to make “learning actually work”. Guided learning advocates are focused on making learning work, and they sometimes succeed, but usually by sacrificing the purposeful sense of immersion we love about those rewarding high-growth periods. One obvious approach is to try to compromise. Project-based learning is a good representation of that. By creating a scaffolded sequence of projects, the suggestion is that we can get some of the benefits of implicit learning—authenticity, motivation, transfer—while also exerting some of the instructional control and cognitive awareness typical of traditional courses. But so often it ends up getting the worst of both worlds—neither motivation and meaning, nor adequate guidance, explanation, and cognitive support. In university, I was interested in 3D game programming, so I took a project-based course on computer graphics. The trouble was that those projects weren’t my projects. So a few weeks in, I found myself implementing a ray marching shader for more efficient bump mapping. Worse, because this course was trying to take project-based learning seriously, there weren't long textbook readings or problem sets. I found myself just translating math I’d been given into code. What I ended up with was a project I didn't care about, implementing math I didn't understand. Instead, I suggest: we should take both views seriously, and find a way to synthesize the two. You really do want to make doing-the-thing the primary activity. But the realities of cognitive psychology mean that in many cases, you really do need explicit guidance, scaffolding, practice, and attention to memory support. Learning by immersion works naturalistically when the material has a low enough complexity relative to your prior knowledge that you can successfully process it on the fly, and when natural participation routinely reinforces everything important, so that you build fluency. When those conditions aren’t satisfied—which is most of the time—you’ll need some support. You want to just dive in, and you want learning to actually work. To make that happen, we need to infuse your authentic projects with guided support, where necessary, inspired by the best ideas from cognitive psychology. And if there’s something that requires more focused, explicit learning experiences, you want those experiences to be utterly in service to your actual aims. I’ve been thinking about this synthesis for many years, and honestly: I’ve mostly been pretty stuck! Recently, though, I’ve been thinking a lot about AI. I know: eye-roll! Pretty much every mention of AI in learning technologies gets an eye-roll from me. But I confess: the possibility of AI has helped me finally get what feels like some traction on this problem. I’d like to share some of those early concepts today. Demo, part 1: Tractable immersion # We’ll explore this possible synthesis through a story in six parts. Meet Sam. Sam studied computer science in university, and they’re now working as a software engineer at a big tech company. But Sam’s somewhat bored at their day job. Not everything is boring, though: every time Sam sees a tweet announcing a new result in brain-computer interfaces, they’re absolutely captivated. These projects seem so much more interesting. Sam pull up the papers, looking for some way to contribute, but they hit a brick wall—so many unfamiliar topics, all at once. What if Sam could ask for help finding some meaningful way to start participating? With Sam’s permission, our AI—and let’s assume it’s a local AI—can build up a huge amount of context about their background. From old documents on Sam’s hard drive, our AI knows all about their university coursework. It can see their current skills through work projects. It knows something about Sam’s interests through their browsing history. Sam’s excited about the idea of reproducing the paper’s data analysis. It seems to play to their strengths. They notice that the authors used a custom Python package to do their analysis, but that code was never published. That seems intriguing: Sam’s built open-source tools before. Maybe they could contribute here by building an open source version of this signal processing pipeline. Demo, part 2: Guidance in action # So—Sam dives in. They’ve found an open-access dataset, and they’ve taken the first steps to start working with it. Tools like Copilot help Sam get started, but to follow some of these signal processing steps, what Sam really needs here is something like Copilot, but with awareness of the paper in addition to the code, and with context about what Sam’s trying to do. This AI system isn’t trapped in its own chatbox, or in the sidebar of one application. It can see what’s going on across multiple applications, and it can propose actions across multiple applications. Sam can click that button to view a changeset with the potential implementation. Then they can continue the conversation, smoothly switching into the context of the code editor. Like, what’s this “axis=1” parameter? The explanation depends on context from the code editor, the paper being implemented, and also documentation that came with the dataset Sam’s working with. The AI underlines assumptions made based on specific information, turning them into links. Here Sam clicks on that “In this dataset” link, and our AI opens the README to the relevant line. All this is to support our central aim—that Sam can immerse themselves, as much as possible, in what they’re actually trying to do, but get the support they need to understand what they’re doing. Demo, part 3: Synthesized dynamic media # That support doesn’t have to just mean text. Sam next needs to implement a downsampling stage. This time, guidance includes synthesized dynamic media, so that Sam can understand what downsampling does through scaffolded immersion. Sam doesn’t need to read an abstract explanation and try to imagine what that would do to different signals: instead, as they try different sampling rates, realtime feedback can help them internalize the effect on different signals. By playing with the dynamic media, Sam notices that some of the peaks are lost when the signal is downsampled. These dynamic media aren’t trapped in the chatbox. They’re using the same input data and libraries Sam’s using in their notebook. At any time, Sam can just “view source” to tinker with this figure, or to use some of its code in their own notebook. Demo, part 4: Contextualized study # Now Sam presses on but as they dig into band-pass filters, the high-level explanations they can get from these short chat interactions really just don’t feel like enough. What’s a frequency domain? What’s a Nyquist rate? Sam can copy and paste some AI-generated code, but they don’t understand what’s going on at all. A chat interface is just not a great medium for long-from conceptual explanation. It’s time for something deeper. The AI knows Sam’s background and aims here, so it suggests an undergraduate text with a practical focus. More importantly, the AI reassures Sam that they don’t necessarily need to read this entire thousand-page book right now. It focuses on Sam’s goal here and suggests a range of accessible paths that Sam can choose according to how deeply they’d like to understand these filters. It's made a personal map in the book’s table of contents. So that, for instance, if Sam just wants to understand what these filters are doing and why, there's a 25-page path for that. But if they want to know the mathematical background—how these filters work—there's a deeper path. And if they want to be able to implement these filters themselves, there's an even deeper path. Sam can choose their journey here. When Sam digs into the book, they'll find notes from the AI at the start of each section, and scattered throughout, which ground the material in Sam’s context, Sam’s project, Sam’s purpose. “This section will help you understand how to think of signals in terms of frequency spectra. That’s what low-pass filters manipulate.” Sam’s spending some time away from their project, in a more traditionally instructional setting, but that doesn’t mean the experience has to lose its connection to their authentic practice. Incidentally, I’ve heard some technologists suggest that we should use AI to synthesize the whole book, bespoke for each person. But I think there’s a huge amount of value in shared canonical artifacts—in any given field, there are key texts that everyone can refer to, and they form common ground for the culture. I think we can preserve those by layering personalized context as a lens on top of texts like this. In my ideal future, of course, our canonical shared artifacts are dynamic media, not digital representations of dead trees. But until all of our canonical works are rewritten, as a transitional measure, we can at least wave our hands and imagine that our AI could synthesize dynamic media versions of figures like this one. Now, as Sam reads through the book, they can continue to engage with the text by asking questions, and our AI’s responses will continue to be grounded in their project. As Sam highlights the text or makes comments about details which seem particularly important or surprising, those annotations won’t end up trapped in this PDF: they’ll feed into future discussion and practice, as we’ll see later. In addition to Sam asking questions of the AI, the AI can insert questions for Sam to consider—again, grounded in their project—to promote deeper processing of the material. And just as our AI guided Sam to the right sections of this thousand page book, it could point out which exercises might be most valuable, considering both Sam’s background and their aims. And it can connect the exercises to Sam’s aims so that, aspirationally, doing those problems feels continuous with Sam’s authentic practice. Even if the exercises do still feel somewhat decontextualized, Sam can at least feel more confident that the work is going to help them do what they want to do. Interlude: Practice and memory # Sam ends the day with some rewarding progress on their project, and a newfound understanding of quite a few topics. But this isn’t yet robust knowledge. Sam has very little fluency—if they try to use this material seriously, they’ll probably feel like they’re standing on shaky ground. And more prosaically, they’re likely to forget much of what they just learned. I’d like to focus on memory for a moment. It’s worth asking: why do we sometimes remember conceptual material, and sometimes not? Often we take a class, or read a book, or even just look something up, and find that a short time later, we’ve retained almost nothing. But sometimes things seem to stick. Why is that? There are some easier cases. If you’re learning something new in a domain you know well, each new fact connects to lots of prior knowledge. That creates more cues for recall and more opportunities for reinforcement. And if you’re in some setting where you need that knowledge every single day, you’ll find that your memory becomes reliable pretty quickly. Conceptual material like what Sam’s learning doesn’t usually get reinforced every day like that. But sometimes the world conspires to give those memories the reinforcement they need. Sometimes you read about a topic, then later that evening, that topic comes up in conversation with a collaborator. You have to retrieve what you learned, and that retrieval reinforces the memory. Then, maybe, two days later you need to recall that knowledge again for a project. Each time you reinforce the memory this way, you forget it more slowly. Now perhaps a week can go by, and you’re still likely to remember. Then maybe a few weeks, and a few months, and so on. With a surprisingly small number of retrievals, if they’re placed close enough to each other to avoid forgetting, you can retain that knowledge for months or years. By contrast, sometimes when you learn something, it doesn’t come up again until the next week. Then you try to retrieve that knowledge, but maybe it’s already been forgotten. So you have to look it up. That doesn’t reinforce your memory very much. And then if it doesn’t come up again for a while longer, you may still not remember next time. So you have to look it up yet again. And so on. The key insight here is that it’s possible to arrange the top timeline for yourself. Courses sometimes do, when each problem set consistently interleaves knowledge from all the previous ones. But immersive learning—and for that matter most learning—usually doesn’t arrange this properly, so you usually forget a lot. What if this kind of reinforcement were woven into the grain of the learning medium? My collaborator Michael Nielsen and I created a quantum computing primer, Quantum Country, to explore this idea. It’s available for free online. If you head to quantum.country, you’ll see what looks at first like a normal book. After a few minutes of reading, the text is interrupted with a small set of review questions. They’re designed to take just a few seconds each: think the answer to yourself, then mark whether or not you were able to answer correctly. So far, these look like simple flashcards. But as we’ve discussed, even if you can answer these questions now, that doesn’t mean you’ll be able to in a few weeks, or even in a few days. Notice these markings along the bottom of each question. These represent intervals. So you practice the questions while you’re reading the text, then, one week later, you’ll get an email that says: “Hey, you’ve probably started to forget some of what you read. Do you want to take five minutes to quickly review that material again?” Each time you answer successfully, the interval increases—to a few weeks, then a few months, and so on. If you begin to forget, then the intervals tighten up to provide more reinforcement. You may have seen systems like this before. Language learners and medical students often use tools called spaced repetition memory systems to remember vocabulary and basic facts. But the same cognitive mechanisms should work for more complex conceptual knowledge as well. There are 112 of these questions scattered through the first chapter of the book on that basis. Quantum Country is a new medium—a mnemonic medium—integrating a spaced repetition memory system with an explanatory text to make it easier for people to absorb complex material reliably. We now have millions of practice data points, so we can start to see how well it’s working. This plot shows the amount of time spent practicing, on the x axis, versus a reader’s demonstrated retention on the y axis— that is, how long a reader was able to go without practicing, and still answer at least 90% of questions correctly. These five dots represent the median user’s first five repetitions, for the first chapter. Notice that the y axis is logarithmic, so we’re seeing a nice exponential growth here. Each extra repetition—constant extra input—yields increasing output—i.e. retention. In exchange for about an hour and a half of total practice, the median reader was able to correctly answer over a hundred detailed questions about the first chapter, after more than two months without practice. Now, the first chapter takes most readers about four hours to read the first time, so this plot implies that an extra overhead of less than 50% in time commitment can yield months or years of detailed retention. It’s also interesting to explore the counterfactual: how much would people forget without the extra reinforcement? As an experiment, we removed nine questions from the first chapter for some readers, then covertly reinserted the questions into those readers’ practice sessions a month later. This graph shows what happened. These nine points represent those nine questions. The y axis shows the percentage of readers who were able to answer that question correctly after one month, with no support at all. You can see that some questions are harder than others. One month later, the majority of readers missed the hardest three questions, on the left, about 30% missed the middle three, and about 15% missed the easiest three. Another group of users got practice while reading the essay, like we saw in the video a moment ago—and for any questions they missed, a bonus round of practice the next day. Then these questions disappeared for a month, at which point they were tested. These readers perform noticeably better, though a big chunk of them are still missing some of these questions. Here’s one last group, like the previous one, except they got one extra round of practice a week after reading the book. Then we tested them again at the one month mark, and that’s what you’re seeing here. Each question takes six seconds on average to answer, so this is less than a minute of extra practice in total for these nine questions. But now for all of these questions, at least 90% of readers were able to answer correctly. Of course, some readers have a much easier time than others. This left plot focuses on the bottom quartile of users—the readers who missed the most questions while they were first reading the essay. Notice I’ve had to lengthen the y axis downwards. We can see that without any practice, most of them forgot two thirds of these held-out questions. In-essay practice alone still left roughly half of them forgetting roughly half of the questions. But with one extra round of practice, even this bottom quartile of readers performs quite well. Systems like Quantum Country are useful for more than just quantum computing. In my personal practice, I’ve accumulated thousands and thousands of questions. I write questions about scientific papers, about conversations, about lectures, about memorable meals. All this makes my daily life more rewarding, because I know that if I invest my attention in something, I will internalize it indefinitely. Central to this is the idea of a daily ritual, a vessel for practice. Like meditation and exercise, I spend about ten minutes a day using my memory system. Because these exponential schedules are very efficient, those ten minutes are enough to maintain my memory for thousands of questions, and to allow me to add up to about forty new questions each day. But I want to mention a few problems with these memory systems. One is pattern matching: once a question comes up a few times, I may recognize the text of the question without really thinking about it. This creates the unpleasant feeling of parroting, but I suspect it often leaves my memory brittle: I’ll remember the answer, but only when cued exactly as I’ve practiced. I wish the questions had more variability. Likewise, the questions are necessarily somewhat abstract. When I face a real problem in that domain, I won’t always recognize what knowledge I should use, or how to adapt it to the situation. A cognitive scientist would say I need to acquire schemas. Unless I intervene, questions stay the same over years. They’re maintaining memory—but ideally, they would push for further processing—more depth over time. Finally, returning to this talk’s thesis: memory systems are often too disconnected from my authentic practice. Say I’m studying a topic in signal processing for a creative project. Unless I’m careful, those questions probably won’t feel very connected to my project—they’ll feel like generic textbook questions about signal processing. Demo, part 5: Dynamic practice # Let’s return to Sam now, and see if we can apply some of these ideas about practice and memory. Sam did the work to study that signal processing material, so they want to make sure it actually sticks. They install a home screen widget, which ambiently exposes them to practice prompts drawn from highlights, questions asked, and other activity the AI can access. Sam can flip through these questions while waiting in line or on the bus. Notice that this isn’t a generic signal processing question: it’s grounded in the details of Sam’s BCI project, so that, aspirationally, practice feels more continuous with authentic doing. These synthesized prompts can vary each time they’re asked, so that Sam gets practice accessing the same idea from different angles. The prompts get deeper and more complex over time, as Sam gets more confident with the material. Notice also that this question isn’t so abstract: it’s really about applying what Sam’s learned, in a bite-sized form factor. The widget can also include open-ended discussion questions. Here Sam gets elaborative feedback—an extra detail to consider in their answer. When questions are synthesized like this, it’s important that Sam can steer them with feedback. Future questions will be synthesized accordingly. So far, we’ve been looking at bite-sized questions Sam can answer while they’re out and about, but if they make time for a longer dedicated session, we can suggest meatier tasks, like this one. What’s more, we can move that work out of fake-practice-land and into Sam’s real context—the Jupyter notebook. Notice that the task is still framed in terms of Sam’s specific aims, rather than some generic signal processing problem. Demo, part 6: Social connection # Now, Sam got into this project not as a “learning exercise”, but as a way to start legitimately participating. To start working with BCIs while playing to existing strengths. Just as our AI can help Sam find a tractable way into this space, it can also facilitate connections to communities of practice—here suggesting a local neurotech meetup. So Sam goes to the neurotech event, meets a local scientist, and sets up a coffee date. With permission, Sam records the meeting, knowing the notes will probably be helpful later. And of course, Sam ends up surprised and intrigued quite a lot during this conversation. Our AI can notice these moments and help Sam metabolize them. Here that insight turns into a reflective practice prompt. Design principles # Four big design principles are threaded through Sam’s story. I’d like to review them now, and for each, point out the ways I think AI can help. First, we bring guided learning to authentic contexts, rather than thinking of it as a separate activity. We’re able to make that happen by imagining an AI which can perceive and act across applications on Sam’s computer. And as the audio transcript at the end demonstrated, that can extend to activities outside the computer too. This AI can give appropriate guidance in part because—with permission and executing locally—it can learn from every piece of text that’s ever crossed Sam’s screen, every action they’ve taken on the computer. It can synthesize scaffolded dynamic media, so that Sam can learn by doing, but with guidance. Then, when explicit learning activities are necessary, we suffuse them with authentic context. The AI grounds all the reading and practice Sam’s doing in their actual aims. It helps Sam match the learning activities to their depth of interest. It draws on important moments that happen when Sam is doing, like insights from that coffee meeting at the end, or questions asked while implementing parts of the project, and brings those moments into study activities Besides connecting these two domains, we can also strengthen each of them. Our AI suggest tractable ways for Sam to “just dive in” to a new interest, and helped Sam build connections with a community of practice. Finally, when we’re spending time in explicit learning activities, let’s make sure that learning actually works. Our AI creates a dynamic vessel for ongoing reinforcement it varies over time so that the knowledge transfers well to real situations. And it doesn’t just maintain memory—but increases depth of understanding over time. Two cheers for chatbot tutors # Most discussion of AI and education at the moment revolves around the framing of chatbot tutors. I think this framing correctly identifies something really wonderful about language models: they’re great at answering long-tail questions… if the user can articulate the question clearly enough. And if the user’s trying to perform a routine task, chatbot tutors can often diagnose problems and find ways to get the user unstuck. That’s great. But when I look at others’ visions of chatbot tutors through the much broader framing we’ve been discussing—they’re clearly missing a lot of what I want. I think these visions also often fail to take seriously just how much a real tutor can do. In large part, I think that’s because the authors of these visions are usually thinking about educating (something they want to do to others) rather than learning (something they want for themselves). Now, a sad truth about the world is that postdocs and graduate students are incredibly underpaid, so it’s surprisingly affordable to get an expert tutor for a technical topic I care about. But if I hire a real tutor, as an adult, to learn about signal processing, I’ll tell them about my interest in brain-computer interfaces, and I’ll expect them to ground every conversation in that purpose. My goal isn’t to “learn signal processing”, it’s to “participate in the creation of BCIs”. Chatbot tutors aren’t interested in what I’m trying to do; there’s a set of things they think I should know or should be able to do, and they view me as defective until I say the right things. If I hire a real tutor, I might ask them to sit beside me as I try to actually do something involving the material. They can see everything I’m doing, see what I’m pointing at. If it’s appropriate, I can scoot over, and they can drive for a minute. By comparison, the typical conception of a chatbot tutor lives in a windowless box, can only see whatever’s provided on scraps of paper passed under the door, and can have no effect on the outside world. My goal is to dive in, to immerse myself, to start doing the thing. But these chatbot tutors can’t join me where the real action is. So interactions with them create distance, pull me away from immersion. If I hire a real tutor, we’ll build a relationship. With every session, they’ll learn more about me—my interests, my strengths, my confusions. Chatbot tutors, as typically conceived, are transactional, amnesic. Now, we could fix that as context windows get longer. But that relationship is also important to my emotional engagement. If I view conversation with my tutor as a kind of peripheral participation in the community I’m hoping to enter—an interaction between novice in the discipline and mentor in the discipline—then tutoring will become just part of doing the thing. But if my interaction with my tutor is transactional, that will tend to make my tutoring sessions feel like “learning time”, separate from doing the thing. Finally, people talk about how Aristotle was a tutor for Alexander the Great. But what’s most valuable about having Aristotle as a tutor isn’t “diagnosing misconceptions”, but rather that he’s modeling the practices and values of an earnest, intellectually engaged adult. He’s demonstrating how and why he thinks about problems. His taste in the discipline. The high-growth periods we love transform the way we see the world. They reshape our identity. In my demo earlier, I showed a chatbot, but it didn’t really work like most “chatbot tutors” I see described. It focused all its actions on the user’s interest, rather than bringing its own agenda. It wasn’t trapped in a little text box—it could see and take action in the context of authentic use; it could communicate through dynamic media. It had a deep memory, drawing on everything you’ve ever written or seen. So in some ways, the system I’ve shown is more like a real tutor. But in my ideal world, I don’t want a tutor; I want to legitimately participate in some new discipline, and to learn what I need as much as possible from interaction with real practitioners. I view the role of the augmented learning system as helping me act on my creative interests, ideally by letting me just dive in and start doing, as much as possible. That will often mean scaffolding connections to and interactions with communities of practice. A note on ethics # One theme for this Design@Large series is the ethics of AI and its likely enormous social impacts. Let me say: I’m tremendously worried about those impacts, in the general case. I’m worried about despots locking in their power, about lowering the bar to bioweapons, about economic chaos. I wouldn’t feel comfortable ethically with researching more powerful frontier models. But within the narrower domain of learning, my main moral concern is that we’ll end up trapped on a sad, narrow path. A condescending, authoritarian frame dominates the narrative in the future of learning. I’ll caricature it to make the point: with AI, we can take all these defective kids that don’t know the stuff they’re supposed to know, and_ finally get them to know it_! You know: personalized learning! The AI will let us precisely identify where the kids are wrong, or ignorant, and fix them. Then we can fill their heads to the brim with what’s good for them. The famous “bicycle for the mind” metaphor is better because it has no agenda other than the one you bring. It just lets you reach a wider range of destinations than you could on foot. And it makes the journey fun too, particularly if you’re biking along with some friends. The bicycle asks: where do you want to go? Of course, that question assumes your destination is well-known and clearly charted on some map. But those most rewarding high-growth experiences are often centered on a creative project. You’re trying to get somewhere no one’s ever gone before—to reach the frontier, then start charting links into the unknown. Learning in service of creation. It’s a dynamic, context-laden kind of learning. It’s about more than just efficiency and correctness. More than just faster gears on a bike. That’s the kind of learning I feel an almost moral imperative to help create. I'm an independent researcher, and my work is crowdfunded by my Patreon community. If you find this research interesting, you can become a member to help make more of it happen. You'll get in-depth monthly essays, previews of prototypes, and events like seminars and unconferences. Now for some words of thanks. The ideas in this talk owe much to years of exchanges with Alec Resnick, Bret Victor, Dan Meyer, Joe Edelman, May-Li Khoe, and Michael Nielsen. Thanks also to Ben Reinhardt, Catherine Olsson, Elliott Jin, Laura Deming, Rob Ochshorn, Sara LaHue, and Taylor Rogalski for helpful conversation as I was preparing the talk. I'm grateful to my hosts for this talk, Jim Hollan and Haijun Xia at the UCSD Design Lab. Special thanks to my sponsor-level patrons as of publication: Adam Marblestone, Adam Wiggins, Andrew Sutherland, Andy Schriner, Ben Springwater, Bert Muthalaly, Boris Verbitsky, Calvin French-Owen, Dan Romero, David Wilkinson, fnnch, Greg Vardy, Heptabase, James Hill-Khurana, James Lindenbaum, Jesse Andrews, Kevin Lynagh, Kinnu, Lambda AI Hardware, Ludwig Petersson, Maksim Stepanenko, Matt Knox, Michael Slade, Mickey McManus, Mintter, Peter Hartree, Ross Boucher, Russel Simmons, Salem Al-Mansoori Sana Labs, Thomas Honeyman, Todor Markov, Tooz Wu, William Clausen, William Laitinen, Yaniv Tal In academic work, please cite this talk as: Matuschak, A. (2024, May 8). How might we learn?. UCSD Design@Large. https://andymatuschak.org/hmwl",
    "commentLink": "https://news.ycombinator.com/item?id=40424536",
    "commentBody": "How Might We Learn? (andymatuschak.org)164 points by ColinWright 13 hours agohidepastfavorite53 comments nicklecompte 1 hour agoIt is frustrating to read through demos which feature a hypothetical AI that greatly exceeds the capacities of any actual LLM, and which does not consider the serious risks of learners getting misled by confabulations. It is especially frustrating when I recently tested GPT-4o on a factual question and got 1000 words which were all completely wrong, including fake citations. It is especially frustrating to read this sci-fi daydreaming after talking to a high school science teacher who was forced to use generative AI tutors in their classes this year, even though these tutors are poorly tested and seem to have a ~20% confabulation rate. This particular teacher is technically sophisticated but even they sometimes get confused and misled by the chatbot. Students don't have a chance. I think Matuschak has valuable insights on learning in general. But it seems incomplete to go through this AI thought experiment without discussing how inadequate current AI is to the task. \"Technology will get better\" but what if it takes 50 years? reply yxre 24 minutes agoparentits getting worse. Noticably very bad. I have started double checked with web searches more and more. It went from 100% of the time to only 20% reply mathnmusic 1 hour agoparentprevYou have only tested the current \"FREE\" AIs, right? How do GPT-4 or Perplexity Pro work on the very same tests? reply nicklecompte 1 hour agorootparentNo, I tested the paid GPT-4 last year on similar questions (animal cognition) and it was so bad I decided it was a waste of money. I actually don't care if it's maybe gotten better in the past year, and I'm certainly not spending money to find out. Last I checked the best LLMs still have a 5-15% confabulation rate on simple document summarization. In 2023 GPT-4 had a ~75% confabulation rate on animal cognition questions, but even 5% is not reliable enough for me to want to use it. The high school AI tutor probably wasn't using GPT-4, but the district definitely paid a lot of money for the software. I also hate this entire argument, that AI confabulations don't matter for free products. Unreliable software like GPT-4o shouldn't be widely released to the public as a cool new tech product, and certainly not handed out for free. reply jeffreyrogers 1 hour agorootparentprevI have tried some chemistry problems on the latest models and they still get simple math wrong (mess up conversion between micro and milligrams for example) unless you tell them to think carefully. reply andrepd 44 minutes agorootparentprevIt's always the same response on this website isn't it? No, GP specifically mentioned GPT-4 reply thefaux 3 hours agoprevIn my experience, some key aspects of learning are honest self-assessment (avoiding unnecessary comparisons to others) and learning to appreciate whatever you have wherever you are. Learning music is one of the best areas to learn how to learn. When you start a new instrument or technique, you will not be good relative to experts. That's ok. You just need to focus on what you can do and build from a solid foundation. Listen closely and your weaknesses will become apparent. You can even learn to appreciate your weaknesses as they provide the opportunities for growth and development. Unfortunately, I worry that ai tools will ultimately hinder learning as much as help (at least in the aggregate). My fear is that it will prevent people from exploring and finding their own path, passively following the track laid out by the ai. Inevitably many will compare themselves to ai and find themselves wanting instead of asking what they can do that an ai can't. Teachers can be helpful, but you ultimately are responsible for your own education. It is one thing to follow an individual who has a proven track record, but at least at this stage, I feel like ai tools may be more pied piper than wise sage. reply SirensOfTitan 7 hours agoprevI really enjoy reading Andy's ideas on education--alongside Peter Gray (a psychologist who emphasizes the importance of play for education, https://www.amazon.com/Free-Learn-Unleashing-Instinct-Self-R...) and Piotr Wozniak (invented SuperMemo, https://supermemo.guru/wiki/Main_Page), he has really shaped my perspective on learning. I actually built a last-minute YOLO application to YC on an extremely similar idea--I figure that modern LLMs are capable enough to offload most of the metacognitive aspects of learning onto. Learn drive (a Wozniak term: your natural curiosity) can take you pretty far in a subject, but it's often frustrating to find the right order to learn concepts based on your current understanding and the subject matter. I've previously scoured syllabi on the internet for this, but often what I want to learn isn't really codified in a single course. I started building a prototype of this idea that I've been very slowly working on in my free time that indexes and uses my notes in emacs for RAG against a locally running LLM. I do think these kinds of learning LLMs have to be run locally, though I've recently gotten a little frustrated because I cannot run a capable open model without my machine's fans turning on. reply NeutralForest 4 hours agoparentIf you ever plan on writing on what you're working I'd be interested. I have notes in Markdown or Org forms as well as PDF's I can convert to text easily. I'd like to be able to run a small model locally and tune it with the content i have in the future. reply cloogshicer 6 hours agoprevI really like Andy Matuschak's ideas. What I don't understand though is his obsession with spaced repetition. While I see the value in it, the cost for me personally is also very high. I was surprised to hear that with just 10 minutes of practice he can sustain up to 40 new cards added every day. I did quantum country a while back and I sometimes needed more than an hour to get through all the questions in a single practice session. Maybe my brain is just slower at recalling things. reply wodenokoto 2 hours agoparentMy initial reaction to those numbers are “you’d have to know the stuff in order to go through 40 new cards and a backlog in 10 minutes”, but the thing is, spaced repetition is for remembering, not for learning. We tend to treat anki as a tool for learning, but it isn’t. reply gofreddygo 4 hours agoparentprevI have rejected Anki and its competitors for learning. I found it shallow and a drag It need high initial investment (prep cards, commit to reviewing everyday) with 0 instantaneous results (a week or 2 in and the cards are still fuzzy). These are superficial problems. My deeper beef with this method is the complete absence of emphasizing, discovering or forming connections between cohesive things. We're trying to learn, it's a super power to start seeing patterns in what we learn, it forms buckets that we can put new concepts and information in. Without it, the learning is ... shallow. I found a better way. I map out full concepts to fit on single sheets of printer paper. Frontside has mostly words with lines connecting them or forming groups. The backside is for related drudgery (formulae, dates, numbers, names). I repeat new things everyday till I can reproduce the sheet front and back without any help. And then slowly introduce days of spacing between repetitions. This is way more satisfying, no tech involved, no algorithms, just hard work and way faster. I do not have any evidence of this working long term. The things I put so much effort in learning to reproduce with such accuracy usually is useful in the short term only. So it works for me. reply boxed 4 hours agorootparentI've started doing anki for geography for myself and with my 9yo daughter. We've been doing it for a few weeks and she now knows ~100% of all countries and their flags. Just absolutely domination level learning. I think it's a matter of finding things that fit Anki, and not trying to fit Anki to the thing you want to learn. Geography is a perfect application: we all would be a bit more informed by knowing all countries, seas, etc; and it's something that Anki is very well suited for. I've also added: - the numerical value for letters (A=1, B=2, C=3, etc) which I think will give me greater powers of lexical sorting. We'll see. - NATO phonetic alphabet - multiplication up to 12x12. I neglected/avoided automating that stuff as a kid and my confidence in doing mental arithmetic is still low. Not sure this is a good case for Anki yet...we'll see. - A custom deck with the faces and names of everyone at my work. This feels like a slam dunk. I am terrible with names, so I think this can up my game a lot. In my experience it's hard to find things that feel marginally useful/fun to learn and that works with Anki. But when it does, it's amazing. reply apwell23 17 minutes agorootparent> We've been doing it for a few weeks and she now knows ~100% of all countries and their flags. Just absolutely domination level learning. is this just for fun? reply layer8 1 minute agorootparentIt’s for world domination, obviously. ;) seabass-labrax 3 hours agorootparentprev> the numerical value for letters (A=1, B=2, C=3, etc) which I think will give me greater powers of lexical sorting. We'll see. Another way of doing it could be generating a deck with questions like: \"Q or P. Which comes first?\" I suppose that which technique will be superior depends on whether you usually sort things relative to each other, or relative to their container. If you have a fixed container of files, you could think, \"ah, 'T', that's 20 (out of 26), I should look down 3/4 of the length of the container\". But if the container wasn't evenly divided - for instance, your 'I' for 'Insurance' was a much thicker file than your 'T' for 'Taxes' or whatever - you'd no longer be able to use those numbers directly. What do you think? reply boxed 3 hours agorootparentI think I'll go with the numbers. It's a smaller set of things to memorize and it's kinda like a fun game that I can quiz myself on when seeing license plates when driving (I find driving horribly boring). My dad used to factorize numbers on license plates when I was a kid :P reply seabass-labrax 3 hours agorootparentI can't stop myself from seeing whether the consecutive digits in phone numbers add up to 10 :) reply kiba 3 hours agorootparentprevMultiplication tables is useful enough to memorize. Where I got tripped up is factors that look very similar to each other but has different answer. reply g-w1 4 hours agorootparentprev> My deeper beef with this method is the complete absence of emphasizing, discovering or forming connections between cohesive things. We're trying to learn, it's a super power to start seeing patterns in what we learn, it forms buckets that we can put new concepts and information in. Without it, the learning is ... shallow. You can make connections that give you really deep intuition; it just takes practice making cards. I wrote about it here: https://jacobgw.com/blog/tft/2024/05/12/srs-intuit.html reply jwells89 1 hour agorootparentprevWhat I like about the Anki approach is that it’s very conducive to scheduling and timeboxing. It removes a lot of variability from the process of memorizing things, and for me variability/unpredictability is a point of significant friction and a strong determiner of if I can consistently work on learning something or not. In some cases one can also use decks made by others, which can help avoid wasting time on dry, unnecessarily fluffed up instructional materials with low signal-to-noise ratios like is common in university courses. reply BeetleB 3 hours agorootparentprev> My deeper beef with this method is the complete absence of emphasizing, discovering or forming connections between cohesive things. We're trying to learn, it's a super power to start seeing patterns in what we learn, it forms buckets that we can put new concepts and information in. Without it, the learning is ... shallow. I'm confused why you'd expect spaced repetition to serve this purpose. Did someone claim it would? Yes, it is shallow. It's meant to be shallow. It's not meant to replace other tools to build connections. It's not meant to be a complete solution. You still need to apply the material to learn it. Spaced repetition is for remembering/recall - not understanding. It's useful for people who have already done the work to understand (practice problems, etc), but would like to keep it in memory. If you are taking grad level analysis, and can't remember that a compact set is closed, because it's been 2 years since you took undergrad analysis, then SR will help you. reply bumbledraven 4 hours agoparentprev> What I don't understand though is his obsession with spaced repetition. I think Matuschak wants to be able to recall particular things ~forever, and spaced repetition is the best method known for that. (Nit: Your sentence begins as a genuine expression of puzzlement rather than an outright criticism, but then you refer to Matuschak's \"obsession\", which pre-judges the issue. It would be more consistent to refer to his \"emphasis on\" or \"advocacy of\".) reply cloogshicer 2 hours agorootparentHah, I actually was looking for a better word than 'obsession', I even used the dictionary (English isn't my native language), but couldn't find one. Then I reluctantly picked obsession. 'Emphasis on' is much better, thank you! Edit: Can't edit my original post though, time's up. But I would if I could! reply breadsniffer01 36 minutes agoparentprevI think he’s imagining the perfect tool for himself rather than a tool that could be widely adopted. He mentioned he doesn’t care at all about raising the floor with learning tools i.e. wide accessibility. Which is a kind of a shame given how much AI now allows reply BeetleB 3 hours agoparentprev> I was surprised to hear that with just 10 minutes of practice he can sustain up to 40 new cards added every day. It'd take me over 10 minutes simply to add those 40 cards. But if he's referring only to reviewing, it's believable. Personally, I don't think I can handle more than 10-20 new cards per day. > I did quantum country a while back and I sometimes needed more than an hour to get through all the questions in a single practice session. Are you literally going through all the questions? Isn't the whole point of SR not to do that? If you meant it took you an hour to go through whatever subset is due for that day, my next question would be: Are you reviewing daily? SR algorithms assume you review daily, and that's the only sane way to keep the time low. As an example, out of a 2000 card deck, I had to review only 6 cards the other day. reply cloogshicer 2 hours agorootparentWell, I did go through the entire deck at some point because the SR algo gave them to me. But with 'all' I meant the ones that were given by the algo. reply luqtas 6 hours agoparentprev\"retrieval practice\" is a really efficient evidence based method [0]... maybe that's why they talk about spaced repetition [0] https://journals.sagepub.com/doi/abs/10.3102/003465431668930... reply bumbledraven 5 hours agoparentprevYeah, 10 minutes/day for 40 new questions/day seemed low to me as well. I agree with you that the plausibility of that figure depends on how long it takes to answer questions. Matuschak writes: > In my personal practice, I've accumulated thousands and thousands of questions. > I spend about ten minutes a day using my memory system. Because these exponential schedules are very efficient, those ten minutes are enough to maintain my memory for thousands of questions, and to allow me to add up to about forty new questions each day. It takes a while to formulate (and subsequently edit) a single good question (https://www.supermemo.com/en/blog/twenty-rules-of-formulatin...), but suppose we leave that aside and assume that by \"using my memory system\", Matuschak means answering previously-formulated questions. He says that questions should take only a few seconds each, so suppose it takes 10 seconds on average to answer a question. Then one could answer 40 questions in 400 seconds, which is under 7 minutes. That leaves 3 minutes to review roughly 20 questions of older material. reply keiferski 5 hours agoparentprevIn my experience, the amount of time needed to remember things in Anki will slowly improve. The key word being slowly. A lot of people get frustrated when they have been learning an item for weeks, answered the card over a dozen times, and yet still struggle to remember it. That’s because you need months for the SR algorithm to work properly. You really need to commit to it for the long haul for it to be effective. Personally I can remember random words in obscure languages off the top of my head (instantaneously), purely because I added a card for them years ago and still keep current. reply seabass-labrax 3 hours agorootparentInterestingly, my experience is quite dissimilar at times. I've been using it for foreign language vocabulary acquisition (not multiple languages, just focusing on one language at the moment). I find that certain words just 'go in' easily, and I never have a problem remembering them as long as I review my Anki deck daily. For other words, they don't ever get substantially easier. I can still use Anki to help memorise them, but re-learning them is sometimes necessary. A concrete example from my deck: word added 2024-02-24, three reviews, now with an interval of 3.03 months. No 'again' answers; nice and easy! Another word, added 2024-02-29, not so good: 14 reviews and an interval of 15 days. I think there are several factors contributing to forgetfulness with Anki for me, some of which might overlap with your experience: A: not properly 'learning' the word in the first place. For me, 'learning' means using the word in context, studying its etymology (even if I do not intend to memorise that) and saying it out aloud. If I don't do that at the beginning, it won't really stick until I effectively start all over again. B: 'learning' words on a bad day, or too late in the day. Even if I 'learn' the words properly, I need to have learnt them in Anki when I'm feeling moderately energetic. If I'm exhausted mentally or physically, my rigorous learning strategy doesn't seem to translate into memory. When I notice myself doing Anki reviews much slower than usual due to tiredness, I generally limit the review count and try to catch up when I'm fresh another day. C: not being consistent enough with reviews. Both the time spent on each individual review and the time spent in total are important - 4 seconds per word is a good sign for me, and strictly 20 minutes a day in total. That allows me to keep up a pace of 12 new words a day very consistently. Would love to hear which parts of this sound familiar to you, or what other things you've noticed for yourself! reply alkyon 1 hour agorootparentA common misconception is that you need to make Anki a daily habit. Bad days (when I'm tired, stressed or have a headache) would cause me to fail quite easy words that I could otherwise get spot on. Even if I already started my reviews and I notice any of that, I just cut the session short. It is ok if you do reviews only 80-90% of the time, the algorithm still works fine. Whenever I fail a word, I try hard to find a reason for it. Most of the time it's interference - my answer resembles some similar word that I already know. I make a mental note about it, add this other word to the card. In most stubborn cases, some redundancy is good, I create another card for the same word in another context or just for a derivative of it. Another thing that works for me is adding images (some of my cards just have a picture on the question side), and example sentences with the word in various contexts. reply keiferski 2 hours agorootparentprevFor other words, they don't ever get substantially easier. This should (hopefully) get easier over time. From your dates there, it looks like the cards you have to re-learn often were only originally added 3 months ago? I think this should become easier for you in 6, 9, or 18 months – provided you continue to keep updated on the cards. A. I definitely agree here. If I don't at least know understand a word to say, 30% confidence, I will never learn it, and will forever repeat it without making any progress. Personally I use images and sound to help make words stick in my mind. There's a lot of research about the effectiveness of imagery (see the \"picture superiority effect.\") B. Ditto with #1. In scientific terms, this is called \"encoding.\" Properly encoding things at the beginning has a big effect on your long term retention. C. I do my reviews every morning while on the exercise bike. I use a gamepad to move through them more quickly, although I would technically be better with typing them out. Also, as a side note, I have written a few blog posts on an old Substack about using Anki and AI tools: https://neurotechnicians.substack.com/archive?sort=new You might find the one about Using Images to Remember Things useful. reply kiba 5 hours agorootparentprevI stopped using anki because the stuff I chosen to remember and study are often of no applicable value or they are of use but does very little toward developing the skills I want. I have no doubt that spaced repetition is a biological reality in how our memories work but I haven't found a way to make it resonate for me. reply keiferski 5 hours agorootparentIt depends on what you're trying to learn. There are certainly some things which don't benefit much from the flash card format, but I do think even then, these kinds of complaints are largely a failing of the user, not the app. If you get a little creative and experiment with the format of your cards, you can learn pretty much anything via Anki. Or at least enhance your knowledge of the skill you're ostensibly also practicing in real life. The mistake most people make is simply adopting the \"boring\" flash card format of FRONT-BACK, and not incorporating other more creative types of cards. For example, question-answer, visualization, triggers, or unique images. ChatGPT is pretty useful for this, as you can get it to present the same information in a variety of different formats and contexts. reply throwaway22032 4 hours agoparentprevI love spaced repetition but the issue I have with it is that if you fall off the wagon you have to basically just reset. It becomes insurmountable. The gamification of finishing my queue doesn't work when it always has thousands of entries in. reply criddell 7 hours agoprevFor me, school was mostly frustrating. I loved kindergarten and first and second grades which mostly seemed to be play. I think it was effective for me as far as creativity and socialization goes. From third grade through high school I was bored with most of the material. A lot of it was not interesting and sometimes the pace was too slow. At university, the work load was too high. I think if I had taken six years to complete the 4 year program, I would have been a lot better off. Too often I didn’t have the time to really dig into the material and explore related ideas (side quests). Instead I settled for memorization which was enough to do well on exams. My GPA at graduation did not reflect my command of the material. Like Andy, I think an AI-powered course of learning could be great. The strength, I think, would be its adaptability. If while learning topic A I stumble across an interesting idea, it would have no problem with changing course and running down topic B. reply apwell23 13 minutes agoparent> At university, the work load was too high. I think if I had taken six years to complete the 4 year program, I would have been a lot better off. Too often I didn’t have the time to really dig into the material and explore related ideas (side quests). Instead I settled for memorization which was enough to do well on exams. My GPA at graduation did not reflect my command of the material. This perfectly describes my experience too. My learning style is to 'ride the wave' of curiosity where i am obsessed with something and keep digging into it. Uni learning was antithetical to this learning style with strict schedules and tests. I got mostly As and some Bs by doing what you did but i didn't learn much of anything. My son is somewhat like me and feel kind of sad to see him at a university to earn a living. I feel disappointed that i didn't provide him enough financial freedom to really gain enjoyment from learning istread of grinding at a uni. reply k__ 11 hours agoprevLearning well requires two circumstances for me. 1. I need a goal. 2. The time I need to practice to reach that goal needs to be reasonable. This makes endeavours like learning to speak a language, to play an instrument, or getting buff unsustainable for me. reply input_sh 5 hours agoparent> This makes endeavours like learning to speak a language, to play an instrument, or getting buff unsustainable for me. I think your goals are just far too abstract, you're thinking years into the future instead of something more immediate. Here's how I'd \"atomize\" them: - Learning to speak a language → Being able to answer some basic everyday questions: What time is it? What's the weather like outside? How am I feeling? When's my birthday? - Playing an instrument → Being able to play one popular song of my choice to a reasonable degree. - Getting buff → Being able to look myself in the mirror and see progress. Suddenly all of them are very achieveable within a month or two. I then either lose interest or set myself some \"higher\" goal. Can I up that to three songs? Can I describe my work or hobbies in $targetLanguage? Is there a body part I'm especially interested in improving? Then that becomes my new \"project\" for the next couple of months. Rinse and repeat, all the way until I can speak German (not quite, but I can point to my A2 certificate and call myself an advanced beginner), or play a piano (not quite, but enough to easily impress anyone that never tried), or until I feel good about the way I look (not quite, but it is an indisputable fact that I look better than ever before). reply 082349872349872 9 hours agoparentprevWhat do you consider a \"reasonable\" amount of time? (I probably come from the opposite viewpoint: I consider the few axes along which I've devoted over a decade to learning as being the particular high-dimensional corners making me me, rather than any of my 8 billion other conspecifics) reply throwaway4aday 6 hours agoprevI like a lot of these ideas. Some of this is built in to the ChatGPT desktop app. Other parts of it could be glued together from existing tools. Others are still beyond the capability of LLMs. Lots of people thinking along the same lines and there will be a lot of products taking a shot at this or similar uses. When one sticks it's going to be a big, if somewhat niche, hit. I know I'd use it. reply amadeuspagel 8 hours agoprev> This AI system isn’t trapped in its own chatbox, or in the sidebar of one application. It can see what’s going on across multiple applications, and it can propose actions across multiple applications. Sam can click that button to view a changeset with the potential implementation. Then they can continue the conversation, smoothly switching into the context of the code editor. That would be an incredibly valuable tool beyond learning, a killer feature for an operating system. reply throwaway4aday 6 hours agoparentThat's the ChatGPT desktop app reply runiq 8 hours agoprevThe intro veers dangerously close to this, which I've read with abject horror: https://news.ycombinator.com/item?id=40425306 reply throwaway4aday 6 hours agoparentIt could be misused and leak vital data but so can your browser history and the data that ISPs and social media and search companies have on you. They say all processing will be done locally but you'd have to be a fool to trust that. I'd prefer to see this as a product that you could connect to your computer which would take care of all the processing and storage and have reasonable guarantees on privacy and encryption. I'm sure we'll see something like that, unsure of how successful it would be in the long run. It is a useful feature, being able to \"recall\" everything you've seen on your computer. I get the naming but it's bad branding since recall has negative connotations and is such a common word with many uses. Time Machine was already taken ;P They should have gone with something more generic like Windows History or Microsoft Memory. If they wanted to be cheeky then Tip of the Tongue or Snappy the helpful screenshot as an animated camera would have been better. reply brainzap 6 hours agoprev> If you wanna remember anything create a picture, a pattern, a story, or rhyme. To learn a skill break it down and then rehearse the sub skills. from some podcast I forgot reply yawpitch 8 hours agoprev> what were the most rewarding high-growth periods of your life? Every single one was an extremely life-threatening moment in which I very likely would (and should) have died, but for my very rapid learning. The growth came in not being a corpse. I realize I am not the target audience of this article. reply nonrandomstring 7 hours agoparent\"Target audience\" or not, there's a ton of truth in what you say. Necessity and lack of control in a novel and frightening situation transforms our minds. It isn't sustainable, but it's not meant to be, because the goal is to get through it and get out, by becoming a different, better person. To pick some slightly less dramatic examples than combat or wilderness survival; A person who learned a new language in 14 days because they fell in love with someone who spoke almost no English, but simply had to be with them and make things work. Someone who became an expert bricklayer when stuck in a remote village where that was the only skill they could contribute. A fella named John Taylor Gatto [0,1] became the New York State Teacher of the Year (winning it more than once IIRC) before being fired for reckless unconventionality. He once drove a bus of school kids upstate into the wild, gave each $10 and a bottle of water, told them their assignment was to \"find your way home\", and drove off. Of course all the kids made it and recounted the \"best ever learning experience of their lives\". Today they'd sue for trauma... if they survived. The article I just read sadly describes more scaffolding, more mollycoddling, more \"learning on rails\", but \"Now with added AI!\" [0] https://en.wikipedia.org/wiki/John_Taylor_Gatto [1] https://thesunmagazine.org/issues/186/a-few-lessons-they-won... reply SirensOfTitan 7 hours agorootparentI really enjoyed Gatto's \"The Underground History of American Education,\" it's a refreshing and entertaining rebuke of the authoritarian/scientific management consensus on schooling, which has not changed much over the past century and is not equipped to educate children for the modern era. reply yawpitch 6 hours agorootparentprev> Necessity and lack of control in a novel and frightening situation So, life. > It isn't sustainable, but it's not meant to be, because the goal is to get through it and get out, by becoming a different, better person. So, again, life. reply nonrandomstring 6 hours agorootparentWell yeah, minus the goal being to get out (which will take care of itself... eventually) :) reply jauntywundrkind 3 hours agoprev [–] Killer framing. This is the point, this is the thing. Extremely well set up, beautifully crafted words, on the utmost of topics. Would that we be getting anywhere here! > Subordinated to an authentic pursuit . . . Diving into a brick wall. Alas software usually is the wall, keeps us from grasping true understanding & development. Interface most often is a wrapper high above the core of software. We trap users, keeping them away from authentic & self directed experience. There was a great submission hours before on Enlightenmentware, on softwares that have enlightened us. Letting users into the natural philosophies underlying software, bringing software from \"wizards\" and \"just works\" to an age of reason for users. I think this underlies everything setup here; it's the tales of systems that fomemted implicit learning well! https://news.ycombinator.com/item?id=40419856 > Learning by emersion works nautralistocally when the material has low enough complexity relative to your prior knowledge that you can process it on the fly . And natural participation reinforces everything important giving you fluency when those conditions arent satisfied - which is most of the time - you will need some support. You want to just drive in and you want learning to just work. This is such a beautiful capstone for the bridge IT ought be building. It speaks to the need for general systems research, new (or improved I guess) systems for operating many processes (and their sprawling subprocesses/subroutines/promises) and seeing them run. It necessitates being free to take that live world and tinker and run and rerun experiments. With safety (and the already spoken of visibility). Opening the option to become acquainted with capability and intent. I'm less clear on the guided parts. But my thesis is that software fails to have sufficient starting conditions for most of these good implicit / guided learning loops to begin. We are trapped in a place where everything is arbitrary interface & none of it is learnable at all (to any honest depth). So we have no mental leverage to begin building mental muscles with. This speaks deeply to me, as the shame of our industry & the shining endless journey we should be so excited to be exploring. That we haven't been trying for broader software ecosystems, for more visible and malleable software is a resounding quaking mystery. This is the great open ended quest, is the real journey of what we are doing, and we are not only flat failing to heed the call of this grand adventure, we are worse, to its detriment, building infernal machines that trap us. Even before machine learning, we were already far down the winnowing closing path spoken of in Dune, and some day I hope the sleeping computing world might awaken from this slumber, > Once men turned their thinking over to machines in the hope that this would set them free. But that only permitted other men with machines to enslave them. This talk sets up the greatest call for computing humanism that I have ever read. Throwing off the spheres of control & helping each other reach to the poles is the point of these days is the point, and computing's chance to be the vanguard pushing that forward is colossal, and keeps rising & getting yet more possible. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The text emphasizes the importance of immersive, meaningful activities over traditional learning methods, advocating for a balance between discovery-based and structured learning.",
      "It highlights the potential of AI to provide personalized, context-rich educational support, integrating real-time guidance and dynamic media to enhance learning and memory retention.",
      "Ethical concerns about AI in education are raised, advocating for a student-driven, exploratory approach, as discussed in Andy Matuschak's talk \"How might we learn?\" at UCSD Design@Large."
    ],
    "commentSummary": [
      "The discussion critiques the limitations of current AI, particularly large language models like GPT-4, in educational settings due to high error rates and misleading outputs.",
      "It explores the benefits and drawbacks of spaced repetition systems like Anki for memory retention, noting their effectiveness for memorization but potential shortcomings in fostering deep understanding.",
      "The conversation advocates for more intuitive, human-centric approaches in education and software design, emphasizing the need for technology to empower and connect people, while also addressing concerns about data privacy."
    ],
    "points": 164,
    "commentCount": 53,
    "retryCount": 0,
    "time": 1716269632
  },
  {
    "id": 40426442,
    "title": "Gifski: High-Quality GIFs with Advanced Encoding and Integration Options",
    "originLink": "https://github.com/ImageOptim/gifski",
    "originBody": "Highest-quality GIF encoder based on pngquant. gifski converts video frames to GIF animations using pngquant's fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame. It's a CLI tool, but it can also be compiled as a C library for seamless use in other apps. Download and install See releases page for executables. If you have Homebrew, you can also get it with brew install gifski. If you have Rust from rustup (1.63+), you can also build it from source with cargo install gifski. Usage gifski is a command-line tool. There is no GUI for Windows or Linux (there is one for macOS). The recommended way is to first export video as PNG frames. If you have ffmpeg installed, you can run in terminal: ffmpeg -i video.webm frame%04d.png and then make the GIF from the frames: gifski -o anim.gif frame*.png You can also resize frames (with -Woption). If the input was ever encoded using a lossy video codec it's recommended to at least halve size of the frames to hide compression artefacts and counter chroma subsampling that was done by the video codec. See gifski -h for more options. Tips for smaller GIF files Expect to lose a lot of quality for little gain. GIF just isn't that good at compressing, no matter how much you compromise. Use --width and --height to make the animation smaller. This makes the biggest difference. Add --quality=80 (or a lower number) to lower overall quality. You can fine-tune the quality with: --lossy-quality=60 lower values make animations noisier/grainy, but reduce file sizes. --motion-quality=60 lower values cause smearing or banding in frames with motion, but reduce file sizes. If you need to make a GIF that fits a predefined file size, you have to experiment with different sizes and quality settings. The command line tool will display estimated total file size during compression, but keep in mind that the estimate is very imprecise. Building Install Rust via rustup or run rustup update. This project only supports up-to-date versions of Rust. You may get compile errors, warnings about \"unstable edition\", etc. if you don't run rustup update regularly. Clone the repository: git clone https://github.com/ImageOptim/gifski In the cloned directory, run: cargo build --release Using from C See gifski.h for the C API. To build the library, run: rustup update cargo build --release and link with target/release/libgifski.a. Please observe the LICENSE. C dynamic library for package maintainers The build process uses cargo-c for building the dynamic library correctly and generating the pkg-config file. rustup update cargo install cargo-c # build cargo cbuild --prefix=/usr --release # install cargo cinstall --prefix=/usr --release --destdir=pkgroot The cbuild command can be omitted, since cinstall will trigger a build if it hasn't been done already. License AGPL 3 or later. I can offer alternative licensing options, including commercial licenses. Let me know if you'd like to use it in a product incompatible with this license. With built-in video support The tool optionally supports decoding video directly, but unfortunately it relies on ffmpeg 4.x, which may be very hard to get working, so it's not enabled by default. You must have ffmpeg and libclang installed, both with their C headers installed in default system include paths. Details depend on the platform and version, but you usually need to install packages such as libavformat-dev, libavfilter-dev, libavdevice-dev, libclang-dev, clang. Please note that installation of these dependencies may be quite difficult. Especially on macOS and Windows it takes expert knowledge to just get them installed without wasting several hours on endless stupid installation and compilation errors, which I can't help with. If you're cross-compiling, try uncommenting [patch.crates-io] section at the end of Cargo.toml, which includes some experimental fixes for ffmpeg. Once you have dependencies installed, compile with cargo build --release --features=video or cargo build --release --features=video-static. When compiled with video support ffmpeg licenses apply. You may need to have a patent license to use H.264/H.265 video (I recommend using VP9/WebM instead). gifski -o out.gif video.mp4 Cross-compilation for iOS The easy option is to use the included gifski.xcodeproj file to build the library automatically for all Apple platforms. Add it as a subproject to your Xcode project, and link with gifski-staticlib Xcode target. See the GUI app for an example how to integrate the library. Cross-compilation for iOS manually Make sure you have Rust installed via rustup. Run once: rustup target add aarch64-apple-ios and then to build the library: rustup update cargo build --lib --release --target=aarch64-apple-ios The build will print \"dropping unsupported crate type cdylib\" warning. This is normal and expected when building for iOS (the cdylib option exists for other platforms). This will create a static library in ./target/aarch64-apple-ios/release/libgifski.a. You can add this library to your Xcode project. See gifski.app for an example how to use libgifski from Swift.",
    "commentLink": "https://news.ycombinator.com/item?id=40426442",
    "commentBody": "Gifski: Optimized GIF Encoder (github.com/imageoptim)151 points by cl3misch 8 hours agohidepastfavorite44 comments alin23 40 minutes agoLove this thing! It enabled me to add a “Convert to GIF” for videos in my Clop app (https://lowtechguys.com/clop) in just a few lines of code. The GIF gets encoded at its optimal size by default so I didn’t have to do a second optimization step. I also like that the author spent time to add an easy way to build a static binary, which I need in order to ship gifski inside the app. Compressing all these static binaries so I can ship them in Clop with aheight ? width : height)) # If we havent provided a scale, and the max size is lower than the default, set it to the lower value if [ \"$SCALEPROVIDED\" == true ] then minScale=$SCALE else minScale=$(( SCALESome websites use looping videos and call them GIFs That’s exactly what they’re doing. Just used the web inspector on one of their GIFs. Imgur does the same thing. reply recursive 2 hours agorootparentNot from here. Here's one I got from their front page. https://j.gifs.com/qjOKXp.gif The content-type response header is image/gif. reply jonnybarnes 2 hours agorootparentInterestingly you can change the `.gif` to `.mp4` and you get a video _with_ audio. reply cl3misch 5 hours agorootparentprevwebm seems to be the modern replacement for GIFs in browsers. I would suspect gifs.com supports them. Somehow their site is buggy for me atm so I could not test it. reply NayamAmarshe 4 hours agorootparentI wish WebM could be the standard. GIFs shouldn't even be used in 2024. It's a format of the past, with no upsides. I usually only need GIFs for GitHub readmes. GitHub doesn't support WebM autoplay videos. In texting, Telegram has introduced WebM stickers so there's at least a choice there. reply edflsafoiewq 4 hours agorootparentNote that Github does support the slightly better animated WebP... as long as you change the file extension to .gif :) reply NayamAmarshe 2 hours agorootparentWhoa! Really!? I gotta try that! reply Gormo 3 hours agorootparentprevWebM is just a container format, though it's usually used with VP9 and opus, and is functionally equivalent to using any other kind of actual video. Animated GIFs remain quite useful for non-video animations where you don't want lossy compression, and want to use other GIF features like transparency. Animated PNGs are a potential replacement, and support all of the features of GIFs along with 24-bit color, but software support remains lackluster. reply HeatrayEnjoyer 4 hours agorootparentprevWhere are WebM files used? Safari browsers/Apple don't even support it. reply cornstalks 4 hours agorootparentThey do now. At least on desktop. But it is a more recent feature (within the last few years, I can’t remember when exactly). MP4 with H.264 is still king for compatibility, but WebM with VP9 isn’t as bespoke as it once was. reply edflsafoiewq 4 hours agorootparentAccording to https://caniuse.com/webm, desktop Safari is green since 2022, while iOS Safari is green since two months ago. reply sandreas 5 hours agorootparentprevAka \"gifs with sound\" - I always asked myself who came up with this absurd term ;) reply NayamAmarshe 5 hours agorootparentprevThey do have an option to download GIFs reply dspillett 5 hours agoparentprevPlaying with the quality options with Gifski can yield a significant drop in size sometimes without much drop in quality (sometimes no noticeable drop in quality unless you really look). Perhaps gifs.com are using the same (or similar) algorithm with more size-optimised defaults? reply NayamAmarshe 4 hours agorootparentI'd like to see if we can extract the details from their GIFs and reproduce it with Gifski. reply joaomoreno 3 hours agoprevgifcap.dev compiles it into WebAssembly to get you an in-browser GIF screen recorder. https://gifcap.dev/ reply mrbluecoat 5 hours agoprevLike Ethernet, as long as the two commandments of \"Does it work?\" and \"Is it secure?\" pass, people will keep the default. P.S. The example gif on their site is gorgeous. reply jimfx 4 hours agoparentPretty sure the gif is from the short movie Cosmo Laundromat by Blender Studios, would definitely recommend giving it a watch: https://gooseberry.blender.org reply DeathArrow 5 hours agoprev [–] I kind of dislike the Gif-ification of the internet. I rather watch video or look at static pictures. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Gifski is a high-quality GIF encoder based on pngquant, converting video frames into GIFs with efficient cross-frame palettes and temporal dithering, resulting in thousands of colors per frame.",
      "It is primarily a command-line tool but can be compiled as a C library for integration into other applications, with executables available for download or installation via Homebrew or Rust's cargo.",
      "The tool requires exporting video frames as PNGs, offers resizing and quality adjustment options, and supports dynamic library creation with cargo-c, licensed under AGPL 3 or later with alternative licensing options."
    ],
    "commentSummary": [
      "Gifski is an optimized GIF encoder known for its ease of integration and efficient default settings, reducing the need for extra optimization steps.",
      "Users value Gifski's ability to build static binaries, making it convenient for app inclusion.",
      "The discussion includes a debate on using GIFs versus modern formats like WebM and animated PNGs, with some users favoring the latter for better quality and features, though GIFs remain popular for specific use cases like GitHub readmes."
    ],
    "points": 151,
    "commentCount": 44,
    "retryCount": 0,
    "time": 1716286566
  }
]
