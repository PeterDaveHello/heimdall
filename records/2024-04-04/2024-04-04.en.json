[
  {
    "id": 39918245,
    "title": "Lavender AI Raises Ethical Concerns in Israeli Bombing Campaign",
    "originLink": "https://www.972mag.com/lavender-ai-israeli-army-gaza/",
    "originBody": "Smoke rises after Israeli airstrikes in Beit Lahia, in the northern Gaza Strip, December 28, 2023. (Yonatan Sindel/Flash90) ‘Lavender’: The AI machine directing Israel’s bombing spree in Gaza The Israeli army has marked tens of thousands of Gazans as suspects for assassination, using an AI targeting system with little human oversight and a permissive policy for casualties, +972 and Local Call reveal. facebook twitter email link By Yuval Abraham April 3, 2024Edit In partnership with In 2021, a book titled “The Human-Machine Team: How to Create Synergy Between Human and Artificial Intelligence That Will Revolutionize Our World” was released in English under the pen name “Brigadier General Y.S.” In it, the author — a man who we confirmed to be the current commander of the elite Israeli intelligence unit 8200 — makes the case for designing a special machine that could rapidly process massive amounts of data to generate thousands of potential “targets” for military strikes in the heat of a war. Such technology, he writes, would resolve what he described as a “human bottleneck for both locating the new targets and decision-making to approve the targets.” Such a machine, it turns out, actually exists. A new investigation by +972 Magazine and Local Call reveals that the Israeli army has developed an artificial intelligence-based program known as “Lavender,” unveiled here for the first time. According to six Israeli intelligence officers, who have all served in the army during the current war on the Gaza Strip and had first-hand involvement with the use of AI to generate targets for assassination, Lavender has played a central role in the unprecedented bombing of Palestinians, especially during the early stages of the war. In fact, according to the sources, its influence on the military’s operations was such that they essentially treated the outputs of the AI machine “as if it were a human decision.” Formally, the Lavender system is designed to mark all suspected operatives in the military wings of Hamas and Palestinian Islamic Jihad (PIJ), including low-ranking ones, as potential bombing targets. The sources told +972 and Local Call that, during the first weeks of the war, the army almost completely relied on Lavender, which clocked as many as 37,000 Palestinians as suspected militants — and their homes — for possible air strikes. During the early stages of the war, the army gave sweeping approval for officers to adopt Lavender’s kill lists, with no requirement to thoroughly check why the machine made those choices or to examine the raw intelligence data on which they were based. One source stated that human personnel often served only as a “rubber stamp” for the machine’s decisions, adding that, normally, they would personally devote only about “20 seconds” to each target before authorizing a bombing — just to make sure the Lavender-marked target is male. This was despite knowing that the system makes what are regarded as “errors” in approximately 10 percent of cases, and is known to occasionally mark individuals who have merely a loose connection to militant groups, or no connection at all. Moreover, the Israeli army systematically attacked the targeted individuals while they were in their homes — usually at night while their whole families were present — rather than during the course of military activity. According to the sources, this was because, from what they regarded as an intelligence standpoint, it was easier to locate the individuals in their private houses. Additional automated systems, including one called “Where’s Daddy?” also revealed here for the first time, were used specifically to track the targeted individuals and carry out bombings when they had entered their family’s residences. Palestinians transport the wounded and try to put out a fire after an Israeli airstrike on a house in the Shaboura refugee camp in the city of Rafah, southern Gaza Strip, November 17, 2023. (Abed Rahim Khatib/Flash90) The result, as the sources testified, is that thousands of Palestinians — most of them women and children or people who were not involved in the fighting — were wiped out by Israeli airstrikes, especially during the first weeks of the war, because of the AI program’s decisions. “We were not interested in killing [Hamas] operatives only when they were in a military building or engaged in a military activity,” A., an intelligence officer, told +972 and Local Call. “On the contrary, the IDF bombed them in homes without hesitation, as a first option. It’s much easier to bomb a family’s home. The system is built to look for them in these situations.” The Lavender machine joins another AI system, “The Gospel,” about which information was revealed in a previous investigation by +972 and Local Call in November 2023, as well as in the Israeli military’s own publications. A fundamental difference between the two systems is in the definition of the target: whereas The Gospel marks buildings and structures that the army claims militants operate from, Lavender marks people — and puts them on a kill list. In addition, according to the sources, when it came to targeting alleged junior militants marked by Lavender, the army preferred to only use unguided missiles, commonly known as “dumb” bombs (in contrast to “smart” precision bombs), which can destroy entire buildings on top of their occupants and cause significant casualties. “You don’t want to waste expensive bombs on unimportant people — it’s very expensive for the country and there’s a shortage [of those bombs],” said C., one of the intelligence officers. Another source said that they had personally authorized the bombing of “hundreds” of private homes of alleged junior operatives marked by Lavender, with many of these attacks killing civilians and entire families as “collateral damage.” In an unprecedented move, according to two of the sources, the army also decided during the first weeks of the war that, for every junior Hamas operative that Lavender marked, it was permissible to kill up to 15 or 20 civilians; in the past, the military did not authorize any “collateral damage” during assassinations of low-ranking militants. The sources added that, in the event that the target was a senior Hamas official with the rank of battalion or brigade commander, the army on several occasions authorized the killing of more than 100 civilians in the assassination of a single commander. Palestinians wait to receive the bodies of their relatives who were killed in an Israeli airstrike, at Al-Najjar Hospital in Rafah, southern Gaza Strip, October 24, 2023. (Abed Rahim Khatib/Flash90) The following investigation is organized according to the six chronological stages of the Israeli army’s highly automated target production in the early weeks of the Gaza war. First, we explain the Lavender machine itself, which marked tens of thousands of Palestinians using AI. Second, we reveal the “Where’s Daddy?” system, which tracked these targets and signaled to the army when they entered their family homes. Third, we describe how “dumb” bombs were chosen to strike these homes. Fourth, we explain how the army loosened the permitted number of civilians who could be killed during the bombing of a target. Fifth, we note how automated software inaccurately calculated the amount of non-combatants in each household. And sixth, we show how on several occasions, when a home was struck, usually at night, the individual target was sometimes not inside at all, because military officers did not verify the information in real time. STEP 1: GENERATING TARGETS ‘Once you go automatic, target generation goes crazy’ In the Israeli army, the term “human target” referred in the past to a senior military operative who, according to the rules of the military’s International Law Department, can be killed in their private home even if there are civilians around. Intelligence sources told +972 and Local Call that during Israel’s previous wars, since this was an “especially brutal” way to kill someone — often by killing an entire family alongside the target — such human targets were marked very carefully and only senior military commanders were bombed in their homes, to maintain the principle of proportionality under international law. But after October 7 — when Hamas-led militants launched a deadly assault on southern Israeli communities, killing around 1,200 people and abducting 240 — the army, the sources said, took a dramatically different approach. Under “Operation Iron Swords,” the army decided to designate all operatives of Hamas’ military wing as human targets, regardless of their rank or military importance. And that changed everything. The new policy also posed a technical problem for Israeli intelligence. In previous wars, in order to authorize the assassination of a single human target, an officer had to go through a complex and lengthy “incrimination” process: cross-check evidence that the person was indeed a senior member of Hamas’ military wing, find out where he lived, his contact information, and finally know when he was home in real time. When the list of targets numbered only a few dozen senior operatives, intelligence personnel could individually handle the work involved in incriminating and locating them. Palestinians try to rescue survivors and pull bodies from the rubble after Israeli airstrikes hit buildings near Al-Aqsa Martyrs Hospital in Deir al-Balah, central Gaza, October 22, 2023. (Mohammed Zaanoun/Activestills) However, once the list was expanded to include tens of thousands of lower-ranking operatives, the Israeli army figured it had to rely on automated software and artificial intelligence. The result, the sources testify, was that the role of human personnel in incriminating Palestinians as military operatives was pushed aside, and AI did most of the work instead. According to four of the sources who spoke to +972 and Local Call, Lavender — which was developed to create human targets in the current war — has marked some 37,000 Palestinians as suspected “Hamas militants,” most of them junior, for assassination (the IDF Spokesperson denied the existence of such a kill list in a statement to +972 and Local Call). “We didn’t know who the junior operatives were, because Israel didn’t track them routinely [before the war],” explained senior officer B. to +972 and Local Call, illuminating the reason behind the development of this particular target machine for the current war. “They wanted to allow us to attack [the junior operatives] automatically. That’s the Holy Grail. Once you go automatic, target generation goes crazy.” The sources said that the approval to automatically adopt Lavender’s kill lists, which had previously been used only as an auxiliary tool, was granted about two weeks into the war, after intelligence personnel “manually” checked the accuracy of a random sample of several hundred targets selected by the AI system. When that sample found that Lavender’s results had reached 90 percent accuracy in identifying an individual’s affiliation with Hamas, the army authorized the sweeping use of the system. From that moment, sources said that if Lavender decided an individual was a militant in Hamas, they were essentially asked to treat that as an order, with no requirement to independently check why the machine made that choice or to examine the raw intelligence data on which it is based. “At 5 a.m., [the air force] would come and bomb all the houses that we had marked,” B. said. “We took out thousands of people. We didn’t go through them one by one — we put everything into automated systems, and as soon as one of [the marked individuals] was at home, he immediately became a target. We bombed him and his house.” “It was very surprising for me that we were asked to bomb a house to kill a ground soldier, whose importance in the fighting was so low,” said one source about the use of AI to mark alleged low-ranking militants. “I nicknamed those targets ‘garbage targets.’ Still, I found them more ethical than the targets that we bombed just for ‘deterrence’ — highrises that are evacuated and toppled just to cause destruction.” The deadly results of this loosening of restrictions in the early stage of the war were staggering. According to data from the Palestinian Health Ministry in Gaza, on which the Israeli army has relied almost exclusively since the beginning of the war, Israel killed some 15,000 Palestinians — almost half of the death toll so far — in the first six weeks of the war, up until a week-long ceasefire was agreed on Nov. 24. Massive destruction is seen in Al-Rimal popular district of Gaza City after it was targeted by airstrikes carried out by Israeli forces, October 10, 2023. (Mohammed Zaanoun/Activestills) ‘The more information and variety, the better’ The Lavender software analyzes information collected on most of the 2.3 million residents of the Gaza Strip through a system of mass surveillance, then assesses and ranks the likelihood that each particular person is active in the military wing of Hamas or PIJ. According to sources, the machine gives almost every single person in Gaza a rating from 1 to 100, expressing how likely it is that they are a militant. Lavender learns to identify characteristics of known Hamas and PIJ operatives, whose information was fed to the machine as training data, and then to locate these same characteristics — also called “features” — among the general population, the sources explained. An individual found to have several different incriminating features will reach a high rating, and thus automatically becomes a potential target for assassination. In “The Human-Machine Team,” the book referenced at the beginning of this article, the current commander of Unit 8200 advocates for such a system without referencing Lavender by name. (The commander himself also isn’t named, but five sources in 8200 confirmed that the commander is the author, as reported also by Haaretz.) Describing human personnel as a “bottleneck” that limits the army’s capacity during a military operation, the commander laments: “We [humans] cannot process so much information. It doesn’t matter how many people you have tasked to produce targets during the war — you still cannot produce enough targets per day.” The solution to this problem, he says, is artificial intelligence. The book offers a short guide to building a “target machine,” similar in description to Lavender, based on AI and machine-learning algorithms. Included in this guide are several examples of the “hundreds and thousands” of features that can increase an individual’s rating, such as being in a Whatsapp group with a known militant, changing cell phone every few months, and changing addresses frequently. “The more information, and the more variety, the better,” the commander writes. “Visual information, cellular information, social media connections, battlefield information, phone contacts, photos.” While humans select these features at first, the commander continues, over time the machine will come to identify features on its own. This, he says, can enable militaries to create “tens of thousands of targets,” while the actual decision as to whether or not to attack them will remain a human one. The book isn’t the only time a senior Israeli commander hinted at the existence of human target machines like Lavender. +972 and Local Call have obtained footage of a private lecture given by the commander of Unit 8200’s secretive Data Science and AI center, “Col. Yoav,” at Tel Aviv University’s AI week in 2023, which was reported on at the time in the Israeli media. In the lecture, the commander speaks about a new, sophisticated target machine used by the Israeli army that detects “dangerous people” based on their likeness to existing lists of known militants on which it was trained. “Using the system, we managed to identify Hamas missile squad commanders,” “Col. Yoav” said in the lecture, referring to Israel’s May 2021 military operation in Gaza, when the machine was used for the first time. Slides from a lecture presentation by the commander of IDF Unit 8200’s Data Science and AI center at Tel Aviv University in 2023, obtained by +972 and Local Call. Slides from a lecture presentation by the commander of IDF Unit 8200’s Data Science and AI center at Tel Aviv University in 2023, obtained by +972 and Local Call. The lecture presentation slides, also obtained by +972 and Local Call, contain illustrations of how the machine works: it is fed data about existing Hamas operatives, it learns to notice their features, and then it rates other Palestinians based on how similar they are to the militants. “We rank the results and determine the threshold [at which to attack a target],” “Col. Yoav” said in the lecture, emphasizing that “eventually, people of flesh and blood take the decisions. In the defense realm, ethically speaking, we put a lot of emphasis on this. These tools are meant to help [intelligence officers] break their barriers.” In practice, however, sources who have used Lavender in recent months say human agency and precision were substituted for mass target creation and lethality. ‘There was no “zero-error” policy’ B., a senior officer who used Lavender, echoed to +972 and Local Call that in the current war, officers were not required to independently review the AI system’s assessments, in order to save time and enable the mass production of human targets without hindrances. “Everything was statistical, everything was neat — it was very dry,” B. said. He noted that this lack of supervision was permitted despite internal checks showing that Lavender’s calculations were considered accurate only 90 percent of the time; in other words, it was known in advance that 10 percent of the human targets slated for assassination were not members of the Hamas military wing at all. For example, sources explained that the Lavender machine sometimes mistakenly flagged individuals who had communication patterns similar to known Hamas or PIJ operatives — including police and civil defense workers, militants’ relatives, residents who happened to have a name and nickname identical to that of an operative, and Gazans who used a device that once belonged to a Hamas operative. “How close does a person have to be to Hamas to be [considered by an AI machine to be] affiliated with the organization?” said one source critical of Lavender’s inaccuracy. “It’s a vague boundary. Is a person who doesn’t receive a salary from Hamas, but helps them with all sorts of things, a Hamas operative? Is someone who was in Hamas in the past, but is no longer there today, a Hamas operative? Each of these features — characteristics that a machine would flag as suspicious — is inaccurate.” Palestinians at the site of an Israeli airstrike in Rafah, in the southern Gaza Strip, February 24, 2024. (Abed Rahim Khatib/Flash90) Similar problems exist with the ability of target machines to assess the phone used by an individual marked for assassination. “In war, Palestinians change phones all the time,” said the source. “People lose contact with their families, give their phone to a friend or a wife, maybe lose it. There is no way to rely 100 percent on the automatic mechanism that determines which [phone] number belongs to whom.” According to the sources, the army knew that the minimal human supervision in place would not discover these faults. “There was no ‘zero-error’ policy. Mistakes were treated statistically,” said a source who used Lavender. “Because of the scope and magnitude, the protocol was that even if you don’t know for sure that the machine is right, you know that statistically it’s fine. So you go for it.” “It has proven itself,” said B., the senior source. “There’s something about the statistical approach that sets you to a certain norm and standard. There has been an illogical amount of [bombings] in this operation. This is unparalleled, in my memory. And I have much more trust in a statistical mechanism than a soldier who lost a friend two days ago. Everyone there, including me, lost people on October 7. The machine did it coldly. And that made it easier.” Another intelligence source, who defended the reliance on the Lavender-generated kill lists of Palestinian suspects, argued that it was worth investing an intelligence officer’s time only to verify the information if the target was a senior commander in Hamas. “But when it comes to a junior militant, you don’t want to invest manpower and time in it,” he said. “In war, there is no time to incriminate every target. So you’re willing to take the margin of error of using artificial intelligence, risking collateral damage and civilians dying, and risking attacking by mistake, and to live with it.” B. said that the reason for this automation was a constant push to generate more targets for assassination. “In a day without targets [whose feature rating was sufficient to authorize a strike], we attacked at a lower threshold. We were constantly being pressured: ‘Bring us more targets.’ They really shouted at us. We finished [killing] our targets very quickly.” He explained that when lowering the rating threshold of Lavender, it would mark more people as targets for strikes. “At its peak, the system managed to generate 37,000 people as potential human targets,” said B. “But the numbers changed all the time, because it depends on where you set the bar of what a Hamas operative is. There were times when a Hamas operative was defined more broadly, and then the machine started bringing us all kinds of civil defense personnel, police officers, on whom it would be a shame to waste bombs. They help the Hamas government, but they don’t really endanger soldiers.” Palestinians at the site of a building destroyed by an Israeli airstrike in Rafah, in the southern Gaza Strip, March 18, 2024. (Abed Rahim Khatib/Flash90) One source who worked with the military data science team that trained Lavender said that data collected from employees of the Hamas-run Internal Security Ministry, whom he does not consider to be militants, was also fed into the machine. “I was bothered by the fact that when Lavender was trained, they used the term ‘Hamas operative’ loosely, and included people who were civil defense workers in the training dataset,” he said. The source added that even if one believes these people deserve to be killed, training the system based on their communication profiles made Lavender more likely to select civilians by mistake when its algorithms were applied to the general population. “Since it’s an automatic system that isn’t operated manually by humans, the meaning of this decision is dramatic: it means you’re including many people with a civilian communication profile as potential targets.” ‘We only checked that the target was a man’ The Israeli military flatly rejects these claims. In a statement to +972 and Local Call, the IDF Spokesperson denied using artificial intelligence to incriminate targets, saying these are merely “auxiliary tools that assist officers in the process of incrimination.” The statement went on: “In any case, an independent examination by an [intelligence] analyst is required, which verifies that the identified targets are legitimate targets for attack, in accordance with the conditions set forth in IDF directives and international law.” However, sources said that the only human supervision protocol in place before bombing the houses of suspected “junior” militants marked by Lavender was to conduct a single check: ensuring that the AI-selected target is male rather than female. The assumption in the army was that if the target was a woman, the machine had likely made a mistake, because there are no women among the ranks of the military wings of Hamas and PIJ. “A human being had to [verify the target] for just a few seconds,” B. said, explaining that this became the protocol after realizing the Lavender system was “getting it right” most of the time. “At first, we did checks to ensure that the machine didn’t get confused. But at some point we relied on the automatic system, and we only checked that [the target] was a man — that was enough. It doesn’t take a long time to tell if someone has a male or a female voice.” To conduct the male/female check, B. claimed that in the current war, “I would invest 20 seconds for each target at this stage, and do dozens of them every day. I had zero added value as a human, apart from being a stamp of approval. It saved a lot of time. If [the operative] came up in the automated mechanism, and I checked that he was a man, there would be permission to bomb him, subject to an examination of collateral damage.” Palestinians emerge from the rubble of houses destroyed in Israeli airstrikes in the city of Rafah, southern Gaza Strip, November 20, 2023. (Abed Rahim Khatib/Flash90) In practice, sources said this meant that for civilian men marked in error by Lavender, there was no supervising mechanism in place to detect the mistake. According to B., a common error occurred “if the [Hamas] target gave [his phone] to his son, his older brother, or just a random man. That person will be bombed in his house with his family. This happened often. These were most of the mistakes caused by Lavender,” B. said. STEP 2: LINKING TARGETS TO FAMILY HOMES ‘Most of the people you killed were women and children’ The next stage in the Israeli army’s assassination procedure is identifying where to attack the targets that Lavender generates. In a statement to +972 and Local Call, the IDF Spokesperson claimed in response to this article that “Hamas places its operatives and military assets in the heart of the civilian population, systematically uses the civilian population as human shields, and conducts fighting from within civilian structures, including sensitive sites such as hospitals, mosques, schools and UN facilities. The IDF is bound by and acts according to international law, directing its attacks only at military targets and military operatives.” The six sources we spoke to echoed this to some degree, saying that Hamas’ extensive tunnel system deliberately passes under hospitals and schools; that Hamas militants use ambulances to get around; and that countless military assets have been situated near civilian buildings. The sources argued that many Israeli strikes kill civilians as a result of these tactics by Hamas — a characterization that human rights groups warn evades Israel’s onus for inflicting the casualties. However, in contrast to the Israeli army’s official statements, the sources explained that a major reason for the unprecedented death toll from Israel’s current bombardment is the fact that the army has systematically attacked targets in their private homes, alongside their families — in part because it was easier from an intelligence standpoint to mark family houses using automated systems. Indeed, several sources emphasized that, as opposed to numerous cases of Hamas operatives engaging in military activity from civilian areas, in the case of systematic assassination strikes, the army routinely made the active choice to bomb suspected militants when inside civilian households from which no military activity took place. This choice, they said, was a reflection of the way Israel’s system of mass surveillance in Gaza is designed. Palestinians rush to bring the wounded, including many children, to Al-Shifa Hospital in Gaza City as Israeli forces continue pounding the Gaza Strip, October 11, 2023. (Mohammed Zaanoun/Activestills) The sources told +972 and Local Call that since everyone in Gaza had a private house with which they could be associated, the army’s surveillance systems could easily and automatically “link” individuals to family houses. In order to identify the moment operatives enter their houses in real time, various additional automatic softwares have been developed. These programs track thousands of individuals simultaneously, identify when they are at home, and send an automatic alert to the targeting officer, who then marks the house for bombing. One of several of these tracking softwares, revealed here for the first time, is called “Where’s Daddy?” “You put hundreds [of targets] into the system and wait to see who you can kill,” said one source with knowledge of the system. “It’s called broad hunting: you copy-paste from the lists that the target system produces.” Evidence of this policy is also clear from the data: during the first month of the war, more than half of the fatalities — 6,120 people — belonged to 1,340 families, many of which were completely wiped out while inside their homes, according to UN figures. The proportion of entire families bombed in their houses in the current war is much higher than in the 2014 Israeli operation in Gaza (which was previously Israel’s deadliest war on the Strip), further suggesting the prominence of this policy. Another source said that each time the pace of assassinations waned, more targets were added to systems like Where’s Daddy? to locate individuals that entered their homes and could therefore be bombed. He said that the decision of who to put into the tracking systems could be made by relatively low-ranking officers in the military hierarchy. “One day, totally of my own accord, I added something like 1,200 new targets to the [tracking] system, because the number of attacks [we were conducting] decreased,” the source said. “That made sense to me. In retrospect, it seems like a serious decision I made. And such decisions were not made at high levels.” The sources said that in the first two weeks of the war, “several thousand” targets were initially inputted into locating programs like Where’s Daddy?. These included all the members of Hamas’ elite special forces unit the Nukhba, all of Hamas’ anti-tank operatives, and anyone who entered Israel on October 7. But before long, the kill list was drastically expanded. “In the end it was everyone [marked by Lavender],” one source explained. “Tens of thousands. This happened a few weeks later, when the [Israeli] brigades entered Gaza, and there were already fewer uninvolved people [i.e. civilians] in the northern areas.” According to this source, even some minors were marked by Lavender as targets for bombing. “Normally, operatives are over the age of 17, but that was not a condition.” Wounded Palestinians are treated on the floor due to overcrowding at Al-Shifa Hospital, Gaza City, central Gaza Strip, October 18, 2023. (Mohammed Zaanoun/Activestills) Lavender and systems like Where’s Daddy? were thus combined with deadly effect, killing entire families, sources said. By adding a name from the Lavender-generated lists to the Where’s Daddy? home tracking system, A. explained, the marked person would be placed under ongoing surveillance, and could be attacked as soon as they set foot in their home, collapsing the house on everyone inside. “Let’s say you calculate [that there is one] Hamas [operative] plus 10 [civilians in the house],” A. said. “Usually, these 10 will be women and children. So absurdly, it turns out that most of the people you killed were women and children.” STEP 3: CHOOSING A WEAPON ‘We usually carried out the attacks with “dumb bombs”’ Once Lavender has marked a target for assassination, army personnel have verified that they are male, and tracking software has located the target in their home, the next stage is picking the munition with which to bomb them. In December 2023, CNN reported that according to U.S. intelligence estimates, about 45 percent of the munitions used by the Israeli air force in Gaza were “dumb” bombs, which are known to cause more collateral damage than guided bombs. In response to the CNN report, an army spokesperson quoted in the article said: “As a military committed to international law and a moral code of conduct, we are devoting vast resources to minimizing harm to the civilians that Hamas has forced into the role of human shields. Our war is against Hamas, not against the people of Gaza.” Three intelligence sources, however, told +972 and Local Call that junior operatives marked by Lavender were assassinated only with dumb bombs, in the interest of saving more expensive armaments. The implication, one source explained, was that the army would not strike a junior target if they lived in a high-rise building, because the army did not want to spend a more precise and expensive “floor bomb” (with more limited collateral effect) to kill him. But if a junior target lived in a building with only a few floors, the army was authorized to kill him and everyone in the building with a dumb bomb. Palestinians at the site of a building destroyed by an Israeli airstrike in Rafah, in the southern Gaza Strip, March 18, 2024. (Abed Rahim Khatib/Flash90) “It was like that with all the junior targets,” testified C., who used various automated programs in the current war. “The only question was, is it possible to attack the building in terms of collateral damage? Because we usually carried out the attacks with dumb bombs, and that meant literally destroying the whole house on top of its occupants. But even if an attack is averted, you don’t care — you immediately move on to the next target. Because of the system, the targets never end. You have another 36,000 waiting.” STEP 4: AUTHORIZING CIVILIAN CASUALTIES ‘We attacked almost without considering collateral damage’ One source said that when attacking junior operatives, including those marked by AI systems like Lavender, the number of civilians they were allowed to kill alongside each target was fixed during the initial weeks of the war at up to 20. Another source claimed the fixed number was up to 15. These “collateral damage degrees,” as the military calls them, were applied broadly to all suspected junior militants, the sources said, regardless of their rank, military importance, and age, and with no specific case-by-case examination to weigh the military advantage of assassinating them against the expected harm to civilians. According to A., who was an officer in a target operation room in the current war, the army’s international law department has never before given such “sweeping approval” for such a high collateral damage degree. “It’s not just that you can kill any person who is a Hamas soldier, which is clearly permitted and legitimate in terms of international law,” A. said. “But they directly tell you: ‘You are allowed to kill them along with many civilians.’ “Every person who wore a Hamas uniform in the past year or two could be bombed with 20 [civilians killed as] collateral damage, even without special permission,” A. continued. “In practice, the principle of proportionality did not exist.” According to A., this was the policy for most of the time that he served. Only later did the military lower the collateral damage degree. “In this calculation, it could also be 20 children for a junior operative … It really wasn’t like that in the past,” A. explained. Asked about the security rationale behind this policy, A. replied: “Lethality.” Palestinians wait to receive the bodies of their relatives who were killed in Israeli airstrikes, at Al-Najjar Hospital in Rafah, southern Gaza Strip, November 7, 2023. (Abed Rahim Khatib/Flash90) The predetermined and fixed collateral damage degree helped accelerate the mass creation of targets using the Lavender machine, sources said, because it saved time. B. claimed that the number of civilians they were permitted to kill in the first week of the war per suspected junior militant marked by AI was fifteen, but that this number “went up and down” over time. “At first we attacked almost without considering collateral damage,” B. said of the first week after October 7. “In practice, you didn’t really count people [in each house that is bombed], because you couldn’t really tell if they’re at home or not. After a week, restrictions on collateral damage began. The number dropped [from 15] to five, which made it really difficult for us to attack, because if the whole family was home, we couldn’t bomb it. Then they raised the number again.” ‘We knew we would kill over 100 civilians’ Sources told +972 and Local Call that now, partly due to American pressure, the Israeli army is no longer mass-generating junior human targets for bombing in civilian homes. The fact that most homes in the Gaza Strip were already destroyed or damaged, and almost the entire population has been displaced, also impaired the army’s ability to rely on intelligence databases and automated house-locating programs. E. claimed that the massive bombardment of junior militants took place only in the first week or two of the war, and then was stopped mainly so as not to waste bombs. “There is a munitions economy,” E. said. “They were always afraid that there would be [a war] in the northern arena [with Hezbollah in Lebanon]. They don’t attack these kinds of [junior] people at all anymore.” However, airstrikes against senior ranking Hamas commanders are still ongoing, and sources said that for these attacks, the military is authorizing the killing of “hundreds” of civilians per target — an official policy for which there is no historical precedent in Israel, or even in recent U.S. military operations. “In the bombing of the commander of the Shuja’iya Battalion, we knew that we would kill over 100 civilians,” B. recalled of a Dec. 2 bombing that the IDF Spokesperson said was aimed at assassinating Wisam Farhat. “For me, psychologically, it was unusual. Over 100 civilians — it crosses some red line.” A ball of fire and smoke rises during Israeli airstrikes in the Gaza Strip, October 9, 2023. (Atia Mohammed/Flash90) Amjad Al-Sheikh, a young Palestinian from Gaza, said many of his family members were killed in that bombing. A resident of Shuja’iya, east of Gaza City, he was at a local supermarket that day when he heard five blasts that shattered the glass windows. “I ran to my family’s house, but there were no buildings there anymore,” Al-Sheikh told +972 and Local Call. “The street was filled with screams and smoke. Entire residential blocks turned to mountains of rubble and deep pits. People began to search in the cement, using their hands, and so did I, looking for signs of my family’s house.” Al-Sheikh’s wife and baby daughter survived — protected from the rubble by a closet that fell on top of them — but he found 11 other members of his family, among them his sisters, brothers, and their young children, dead under the rubble. According to the human rights group B’Tselem, the bombing that day destroyed dozens of buildings, killed dozens of people, and buried hundreds under the ruins of their homes. ‘Entire families were killed’ Intelligence sources told +972 and Local Call they took part in even deadlier strikes. In order to assassinate Ayman Nofal, the commander of Hamas’ Central Gaza Brigade, a source said the army authorized the killing of approximately 300 civilians, destroying several buildings in airstrikes on Al-Bureij refugee camp on Oct. 17, based on an imprecise pinpointing of Nofal. Satellite footage and videos from the scene show the destruction of several large multi-storey apartment buildings. “Between 16 to 18 houses were wiped out in the attack,” Amro Al-Khatib, a resident of the camp, told +972 and Local Call. “We couldn’t tell one apartment from the other — they all got mixed up in the rubble, and we found human body parts everywhere.” In the aftermath, Al-Khatib recalled around 50 dead bodies being pulled out of the rubble, and around 200 people wounded, many of them gravely. But that was just the first day. The camp’s residents spent five days pulling the dead and injured out, he said. Palestinians digging with bear hands find a dead body in the rubble after an Israeli airstrike which killed dozens Palestinians in the middle of Al-Maghazi refugee camp, central Gaza Strip, November 5, 2023. (Mohammed Zaanoun/Activestills) Nael Al-Bahisi, a paramedic, was one of the first on the scene. He counted between 50-70 casualties on that first day. “At a certain moment, we understood the target of the strike was Hamas commander Ayman Nofal,” he told +972 and Local Call. “They killed him, and also many people who didn’t know he was there. Entire families with children were killed.” Another intelligence source told +972 and Local Call that the army destroyed a high-rise building in Rafah in mid-December, killing “dozens of civilians,” in order to try to kill Mohammed Shabaneh, the commander of Hamas’ Rafah Brigade (it is not clear whether or not he was killed in the attack). Often, the source said, the senior commanders hide in tunnels that pass under civilian buildings, and therefore the choice to assassinate them with an airstrike necessarily kills civilians. “Most of those injured were children,” said Wael Al-Sir, 55, who witnessed the large-scale strike believed by some Gazans to have been the assassination attempt. He told +972 and Local Call that the bombing on Dec. 20 destroyed an “entire residential block” and killed at least 10 children. “There was a completely permissive policy regarding the casualties of [bombing] operations — so permissive that in my opinion it had an element of revenge,” D., an intelligence source, claimed. “The core of this was the assassinations of senior [Hamas and PIJ commanders] for whom they were willing to kill hundreds of civilians. We had a calculation: how many for a brigade commander, how many for a battalion commander, and so on.” “There were regulations, but they were just very lenient,” said E., another intelligence source. “We’ve killed people with collateral damage in the high double-digits, if not low triple-digits. These are things that haven’t happened before.” Palestinians inspect their homes and try to rescue their relatives from under the rubble after an Israeli airstrike in the city of Rafah, southern Gaza Strip, October 22, 2023. (Abed Rahim Khatib/Flash90) Such a high rate of “collateral damage” is exceptional not only compared to what the Israeli army previously deemed acceptable, but also compared to the wars waged by the United States in Iraq, Syria, and Afghanistan. General Peter Gersten, Deputy Commander for Operations and Intelligence in the operation to fight ISIS in Iraq and Syria, told a U.S. defense magazine in 2021 that an attack with collateral damage of 15 civilians deviated from procedure; to carry it out, he had to obtain special permission from the head of the U.S. Central Command, General Lloyd Austin, who is now Secretary of Defense. “With Osama Bin Laden, you’d have an NCV [Non-combatant Casualty Value] of 30, but if you had a low-level commander, his NCV was typically zero,” Gersten said. “We ran zero for the longest time.” ‘We were told: “Whatever you can, bomb”’ All the sources interviewed for this investigation said that Hamas’ massacres on October 7 and kidnapping of hostages greatly influenced the army’s fire policy and collateral damage degrees. “At first, the atmosphere was painful and vindictive,” said B., who was drafted into the army immediately after October 7, and served in a target operation room. “The rules were very lenient. They took down four buildings when they knew the target was in one of them. It was crazy. “There was a dissonance: on the one hand, people here were frustrated that we were not attacking enough,” B. continued. “On the other hand, you see at the end of the day that another thousand Gazans have died, most of them civilians.” “There was hysteria in the professional ranks,” said D., who was also drafted immediately after October 7. “They had no idea how to react at all. The only thing they knew to do was to just start bombing like madmen to try to dismantle Hamas’ capabilities.” Defence Minister Yoav Gallant speaks with Israeli soldiers at a staging area not far from the Gaza fence, October 19, 2023. (Chaim Goldberg/Flash90) D. stressed that they were not explicitly told that the army’s goal was “revenge,” but expressed that “as soon as every target connected to Hamas becomes legitimate, and with almost any collateral damage being approved, it is clear to you that thousands of people are going to be killed. Even if officially every target is connected to Hamas, when the policy is so permissive, it loses all meaning.” A. also used the word “revenge” to describe the atmosphere inside the army after October 7. “No one thought about what to do afterward, when the war is over, or how it will be possible to live in Gaza and what they will do with it,” A. said. “We were told: now we have to fuck up Hamas, no matter what the cost. Whatever you can, you bomb.” B., the senior intelligence source, said that in retrospect, he believes this “disproportionate” policy of killing Palestinians in Gaza also endangers Israelis, and that this was one of the reasons he decided to be interviewed. “In the short term, we are safer, because we hurt Hamas. But I think we’re less secure in the long run. I see how all the bereaved families in Gaza — which is nearly everyone — will raise the motivation for [people to join] Hamas 10 years down the line. And it will be much easier for [Hamas] to recruit them.” In a statement to +972 and Local Call, the Israeli army denied much of what the sources told us, claiming that “each target is examined individually, while an individual assessment is made of the military advantage and collateral damage expected from the attack … The IDF does not carry out attacks when the collateral damage expected from the attack is excessive in relation to the military advantage.” STEP 5: CALCULATING COLLATERAL DAMAGE ‘The model was not connected to reality’ According to the intelligence sources, the Israeli army’s calculation of the number of civilians expected to be killed in each house alongside a target — a procedure examined in a previous investigation by +972 and Local Call — was conducted with the help of automatic and inaccurate tools. In previous wars, intelligence personnel would spend a lot of time verifying how many people were in a house that was set to be bombed, with the number of civilians liable to be killed listed as part of a “target file.” After October 7, however, this thorough verification was largely abandoned in favor of automation. In October, The New York Times reported on a system operated from a special base in southern Israel, which collects information from mobile phones in the Gaza Strip and provided the military with a live estimate of the number of Palestinians who fled the northern Gaza Strip southward. Brig. General Udi Ben Muha told the Times that “It’s not a 100 percent perfect system — but it gives you the information you need to make a decision.” The system operates according to colors: red marks areas where there are many people, and green and yellow mark areas that have been relatively cleared of residents. Palestinians walk on a main road after fleeing from their homes in Gaza City to the southern part of Gaza, November 10, 2023. (Atia Mohammed/Flash90) The sources who spoke to +972 and Local Call described a similar system for calculating collateral damage, which was used to decide whether to bomb a building in Gaza. They said that the software calculated the number of civilians residing in each home before the war — by assessing the size of the building and reviewing its list of residents — and then reduced those numbers by the proportion of residents who supposedly evacuated the neighborhood. To illustrate, if the army estimated that half of a neighborhood’s residents had left, the program would count a house that usually had 10 residents as a house containing five people. To save time, the sources said, the army did not surveil the homes to check how many people were actually living there, as it did in previous operations, to find out if the program’s estimate was indeed accurate. “This model was not connected to reality,” claimed one source. “There was no connection between those who were in the home now, during the war, and those who were listed as living there prior to the war. [On one occasion] we bombed a house without knowing that there were several families inside, hiding together.” The source said that although the army knew that such errors could occur, this imprecise model was adopted nonetheless, because it was faster. As such, the source said, “the collateral damage calculation was completely automatic and statistical” — even producing figures that were not whole numbers. STEP 6: BOMBING A FAMILY HOME ‘You killed a family for no reason’ The sources who spoke to +972 and Local Call explained that there was sometimes a substantial gap between the moment that tracking systems like Where’s Daddy? alerted an officer that a target had entered their house, and the bombing itself — leading to the killing of whole families even without hitting the army’s target. “It happened to me many times that we attacked a house, but the person wasn’t even home,” one source said. “The result is that you killed a family for no reason.” Three intelligence sources told +972 and Local Call that they had witnessed an incident in which the Israeli army bombed a family’s private home, and it later turned out that the intended target of the assassination was not even inside the house, since no further verification was conducted in real time. Palestinians receive the bodies of relatives who were killed in Israeli airstrikes, Al-Najjar Hospital, southern Gaza Strip, November 6, 2023. (Abed Rahim Khatib/Flash90) “Sometimes [the target] was at home earlier, and then at night he went to sleep somewhere else, say underground, and you didn’t know about it,” one of the sources said. “There are times when you double-check the location, and there are times when you just say, ‘Okay, he was in the house in the last few hours, so you can just bomb.’” Another source described a similar incident that affected him and made him want to be interviewed for this investigation. “We understood that the target was home at 8 p.m. In the end, the air force bombed the house at 3 a.m. Then we found out [in that span of time] he had managed to move himself to another house with his family. There were two other families with children in the building we bombed.” In previous wars in Gaza, after the assassination of human targets, Israeli intelligence would carry out bomb damage assessment (BDA) procedures — a routine post-strike check to see if the senior commander was killed and how many civilians were killed along with him. As revealed in a previous +972 and Local Call investigation, this involved listening in to phone calls of relatives who lost their loved ones. In the current war, however, at least in relation to junior militants marked using AI, sources say this procedure was abolished in order to save time. The sources said they did not know how many civilians were actually killed in each strike, and for the low-ranking suspected Hamas and PIJ operatives marked by AI, they did not even know whether the target himself was killed. Most read on +972 Why do Israelis feel so threatened by a ceasefire? ‘A mass assassination factory’: Inside Israel’s calculated bombing of Gaza Even without a UN veto, Gaza remains hostage to American power “You don’t know exactly how many you killed, and who you killed,” an intelligence source told Local Call for a previous investigation published in January. “Only when it’s senior Hamas operatives do you follow the BDA procedure. In the rest of the cases, you don’t care. You get a report from the air force about whether the building was blown up, and that’s it. You have no idea how much collateral damage there was; you immediately move on to the next target. The emphasis was to create as many targets as possible, as quickly as possible.” But while the Israeli military may move on from each strike without dwelling on the number of casualties, Amjad Al-Sheikh, the Shuja’iya resident who lost 11 of his family members in the Dec. 2 bombardment, said that he and his neighbors are still searching for corpses. “Until now, there are bodies under the rubble,” he said. “Fourteen residential buildings were bombed with their residents inside. Some of my relatives and neighbors are still buried.” Subscribe to The Landline +972's weekly newsletter Sign up Gaza October 2023 war Israeli army Local Call Israeli airstrikes intelligence Israeli Air Force civilian casualties Yuval Abraham is a journalist and filmmaker based in Jerusalem. Our team has been devastated by the horrific events of this latest war. The world is reeling from Israel’s unprecedented onslaught on Gaza, inflicting mass devastation and death upon besieged Palestinians, as well as the atrocious attack and kidnappings by Hamas in Israel on October 7. Our hearts are with all the people and communities facing this violence. We are in an extraordinarily dangerous era in Israel-Palestine. The bloodshed has reached extreme levels of brutality and threatens to engulf the entire region. Emboldened settlers in the West Bank, backed by the army, are seizing the opportunity to intensify their attacks on Palestinians. The most far-right government in Israel’s history is ramping up its policing of dissent, using the cover of war to silence Palestinian citizens and left-wing Jews who object to its policies. This escalation has a very clear context, one that +972 has spent the past 14 years covering: Israeli society’s growing racism and militarism, entrenched occupation and apartheid, and a normalized siege on Gaza. We are well positioned to cover this perilous moment – but we need your help to do it. This terrible period will challenge the humanity of all of those working for a better future in this land. Palestinians and Israelis are already organizing and strategizing to put up the fight of their lives. Can we count on your support ? +972 Magazine is a leading media voice of this movement, a desperately needed platform where Palestinian and Israeli journalists, activists, and thinkers can report on and analyze what is happening, guided by humanism, equality, and justice. Join us. BECOME A +972 MEMBER TODAY",
    "commentLink": "https://news.ycombinator.com/item?id=39918245",
    "commentBody": "'Lavender': The AI machine directing Israel's bombing in Gaza (972mag.com)1205 points by contemporary343 19 hours agohidepastfavorite1090 comments Quanttek 18 hours agoYears ago, scholars (such as Didier Bigo) have already raised concerns about the targeting of individuals merely based on (indirect) association with a \"terrorist\" or \"criminal\". Originally used in the context of surveillance (see Snowden revelations), such systems would target anyone who would be e.g. less than 3-steps away from an identified individual, thereby removing any sense of due process or targeted surveillance. Now, such AI systems are being used to actually kill people - instead of just surveil. IHL actually prohibits the killing of persons who are not combatants or \"fighters\" of an armed group. Only those who have the \"continuous function\" to \"directly participate in hostilities\"[1] may be targeted for attack at any time. Everyone else is a civilian that can only be directly targeted when and for as long as they directly participate in hostilities, such as by taking up arms, planning military operations, laying down mines, etc. That is, only members of the armed wing of Hamas (not recruiters, weapon manufacturers, propagandists, financiers, …) can be targeted for attack - all the others must be arrested and/or tried. Otherwise, the allowed list of targets of civilians gets so wide than in any regular war, pretty much any civilian could get targeted, such as the bank employee whose company has provided loans to the armed forces. Lavender is so scary because it enables Israel's mass targeting of people who are protected against attack by international law, providing a flimsy (political but not legal) justification for their association with terrorists. [1]: https://www.icrc.org/en/doc/assets/files/other/icrc-002-0990... reply surfingdino 15 hours agoparentIt always starts with making a list of targets that meet given criteria. Once you have the list its use changes from categorisation to demonisation -> surveillance -> denial of rights -> deportations -> killing. Early use of computers by Germans during WW2 included making and processing of lists of people who ought to be sent to concentration camps. The only difference today is that we are able to capture more data and process it faster at scale. reply Qem 15 hours agorootparentThere's even books written about it. Shame on IBM for this. I suspect in the future we'll have lots of books like this, for other companies enabling this genocide: https://en.wikipedia.org/wiki/IBM_and_the_Holocaust reply imjonse 15 hours agorootparentThe same author wrote Nazi Nexus, with separate chapters for different US companies' (Ford, GM) dealings with the Nazi regime. It can always be a case of \"let's not bring politics into work\" attitude or the belief that \"tech is a tool only, can be used for good or ill\" but at least in the years leading up to WW2 there was a lot of support for eugenics, antisemitism (Henry Ford was a notorious one) and other Nazi tendencies in the US too. I would not be surprised if many of those working on killer AI today were politically motivated and not just developers caught in projects they don't really have their hearts in. reply zelphirkalt 1 hour agorootparentOnly recently someone here on HN posted a video about some big hall in the US, where nazi supporters gathered in droves. It made it seem like they had significant ideological footing in the US as well. Unthinkable what could have happened, if they had had even more support. Not exactly this video that was linked, but this seems to be about the same gathering: https://invidious.baczek.me/watch?v=r4zRZ7XLYSA reply Sleepful 13 hours agorootparentprevOperation Paperclip et al reply GaryNumanVevo 1 hour agorootparentDon't forget Japanese Unit 731, all the scientists involved were whisked away to the US if they would give up their research on human subjects to the US military and help translate. reply selimthegrim 11 hours agorootparentprevThe weird thing is, I’ve seen this author post factually incorrect things about early Islamic history. I just wish he was more careful about things outside his area of expertise. reply CatWChainsaw 11 hours agorootparentprevIn the future, AI will be so good that it will detect criticism of IBM as you are typing and threaten to lock you out of \"your\" computer unless you delete your work. Either that or genAI will be used to publish a bunch of books telling fantasy stories about how IBM personally arrested Hitler. :) reply ysofunny 11 hours agorootparentas it turns out, there's a better way. already the AI detects criticism of itself. except its response it's to shadowban you meaning you can continue to post but nobody sees your opinion online. eventually, you're \"bubbled\" by AIs.. all your interactions online are surrounded by an AI and you'd think you're interacting with other people when you're just AI-bubbled so to not disrupt the rest of the workers. you'll still see likes, and other interactions with the social media posts you leave behind, but as a flagged critic of the system, all these interactions are merely faked to keep you calm. as the AI advances you'll even see responses, retweets and other interactions.... all AI driven in order to keep you busy while IBM keeps a calm overwatch over all. the end. reply CapeTheory 10 hours agorootparentRidiculous fantasizing - there is simply no way that IBM would be able to build something as good as that. reply ysofunny 9 hours agorootparentthey don't have to, they bought it. or hired it? dunno. for all you know I'm an AI intended to keep you distracted while at the same time you're just an AI bot keeping me occupied with pointless online discussions. even if neither of us is actually an AI, this interaction will surely aid in training some LLM in the end... reply zelphirkalt 1 hour agorootparentMaybe some day in the future this will amount to an \"organic\" way of accidentally building up a simulation of human society, that will be the only thing remaining for some far into the future aliens, who come to visit our planet. And what conclusions they would draw from this. reply TeMPOraL 3 hours agorootparentprevNot today, no. But remember that IBM is critical to SERN due to the importance of IBM 5100 for time travel, so there's a bit of technological back and forth going on within the ~100 year period we happen to be at the center of right now. reply aspenmayer 2 hours agorootparentIs this a John Titor and/or Steins Gate reference? reply TeMPOraL 2 hours agorootparentIt is both, and also a way of acknowledging that GP's comment points out that the main/only ridiculous fantasy in GGP's comment is that IBM specifically is involved, and not the whole AI part. I do regret making the joke now, though, given the wider context of the thread. reply aspenmayer 1 hour agorootparentArt and metaphors are useful tools to illuminate and elucidate. I think you were able to make a good point, and the tonal shift helped situate your comment in juxtaposition to the parents’. My point was not to criticize but to make the reference explicit for those who aren’t familiar with them, and confirm my own assumption regarding your usage, as well. reply more_corn 6 hours agorootparentprevSlow clap. reply goatlover 5 hours agorootparentprevnext [17 more] [flagged] refulgentis 5 hours agorootparentIt's reasonable to view Palestine as a nation and it's reasonable to look at what's going on and see forced starvation of a nation coupled to, as we are discussing here, cruelly relaxed standards for enemy combatants that make a mockery of international law and are de facto indiscriminate by any standard. Sneering about agendas is distasteful in this context. Vast majority of us aren't really keeping score or trying to advance anything at all, just horrified, as horrified as we were by 10/7, while 10% hurl insults at each other and lash out at anything anyone else says. reply flyinglizard 3 hours agorootparentThe entire Palestinian war doctrine is built around attacking Israel, then running for cover of well intentioned Western public. Hamas just needs to survive this to declare victory, and then the clock resets until the next cycle of violence. Hamas assumes Israel will not be allowed to have a decisive win, one where its leadership down to its last junior operatives are hunted down and eliminated. reply thisislife2 1 hour agorootparent> The entire Palestinian war doctrine is built around attacking Israel ... Let me correct that for you - The entire Palestinian resistance is built around the idea of fighting a foreign occupation of their land. Israel is a coloniser state, and the Palestinians are justified in fighting them, even violently, for their land and existence as many other colonial countries have in the past against their oppressors. The denial that some Israelis have about this is the height of political stupidity. One man's terrorist is another man's freedom fighter. The problem with Israel is not PLA or Hamas, but the the right-wing religious fundamentalist Israeli-Jews that have captured power in Israel. Right-wing religious fundamentalists around the world thrive by creating a climate of fear and hate. Unless Israeli Jews and Palestinian muslims hate each other, they cannot be in power. And if you look at the history of the Israeli religious fundamentalist Jews, you will find that they have assassinated their own people, Palestinian politicians and ven foreign diplomats and leaders, to ensure that there is no peace and a climate of fear and hatred flourishes. Why do you think Netanyahu and his predecessor formulated the policy of supporting and financing Hamas when the PLA started exploring diplomatic and non-violent means of peace, and gaining international support? The reality was that that these religious fundamentalist Israeli politicians need some religious fundamentalist violent Palestinian faction to brainwash and radicalise their followers to retain power. So they deliberately create conflict. The Likud party that Netanyahu now heads, was found by religious fundamentalists in Israel who were a jewish terrorist organisation that took please in killing and slaughtering Palestinina men, women and children. Albert Einstein once warned the Israel polity about the dangers of such people capturing power in Israel because of the atrocity them committed against Palestinians in the name of Israel. It is not a co-incidence that Netanyahu and his other religious fundamentalist buddies allowed Hamas to attack Israel, when he and the right- in Israel are in a very politically precarious situation. The current war allows them to create hatred among the new generation of Israelis and Palestinians, so that they can cling on to power. reply flyinglizard 0 minutes agorootparentSounds like you’re endorsing violence, just upset that your side is losing. michaelmrose 2 hours agorootparentprevThis method of elinination can only be carried out via a Holocaust. Killing half the 2M citizens won't eliminate Hamas it will recruit 3 generations as justified in their violence as Israel. The cost of their crimes will be paid by both sides grandchildren or we will witness a final slaughter of 2M people. reply flyinglizard 2 hours agorootparentSorry, but I don’t buy this. West Bank is both more peaceful and less radicalized than Gaza, the difference being that the Israeli military operates in the West Bank but not in Gaza. Gaza was left unchecked to be run by Hamas and we’re seeing the results today. Long term the only solution is systematic deradicalization, but before any of that happens, Hamas needs to he destroyed and the war in Gaza needs to serve as a lesson to why peace is better than war. So far, for all Palestinian suffering, we’re not past that point. reply gizmo 15 minutes agorootparentHamas leadership can be killed, but Israel is effectively waging a was on terror by another name. And terror is a concept you can’t blow up. By inflicting horrific damage on a civilian population Israel creates more people willing to give up their lives in military struggle. michaelmrose 1 hour agorootparentprevSo policing the west bank has left it more peaceful so we can certainly now murder our way to peace in gaza by killing another 30,000 innocents in order to kill another 0.5% of the militants. 30,000 dead innocents can't possibly generate more than 300 soldiers among survivors. For practical purposes the degree of force required to pacify it would kill 98%. Its possible that a better policed Gaza would be more peaceful this doesn't mean present efforts are reasonable. reply flyinglizard 1 minute agorootparentWhere did you take the figure of 0.5%? Consensus is over 12k Hamas militant dead and many more injured. trimethylpurine 3 hours agorootparentprevI'm trying to understand; it sounds like you feel that an AI selecting targets and letting some live is just as bad and indiscriminate as a group forming a charter that reads \"exterminate all Jews.\" (Paraphrasing, but the meaning is not disputed.) That's hard to agree with. I'd rather the group that tries to save some civilians over one that targets all civilians intentionally. Wouldn't you? That said, we can criticize. But we should do it constructively. Provide a better option, militarily, or otherwise. (I don't have one. And anyway I believe this article is baseless.) Short of offering options, we're just picking sides, and to me it looks like you're picking the wrong one. And, that is bias. reply michaelmrose 2 hours agorootparentThe criminal that hides under color of law while killing far more innocents is more odious on multiple fronts. Dead children and austere serious men walking through their blood reeking of offal and rightiousness is a disturbing contrast. The US ought to disassociate ourselves forever from such undepentant criminals. reply ineedaj0b 5 hours agorootparentprevWar is nasty. It won’t stop being reply rixed 4 hours agorootparentYou mean the bombing of Gaza or the October attacks? reply ineedaj0b 2 hours agorootparentI see it as open season. One under resourced side wants a fight with a well resourced side. I don’t expect them to sing songs. I expect them to very violently kill each other till on one side goes, ‘we lost’. reply michaelmrose 2 hours agorootparentprevThey are deliberately targeting civilians based on nebulous association knowing they will kill up to 20 others without even that thin justification. There is absolutely reason to believe such missions could range from 10-50 civilians to one actual soldier. They are claiming kills as justified that never received human vetting. Even the kills that would be lawful are by any reasonable analysis fruits of a poisonous tree. Done at scale its hardly different than running a gas chamber. reply guappa 2 hours agorootparentWell it's like facebook pushing trump content and saying \"well it's not us, it's the algorithm that decided\". Same thing, I think it's to just put blame on something else, even if nobody sensible believes it. reply cess11 4 hours agorootparentprevIBM decided who was jewish, roma, socialist, and so on? IBM:s machines found these people and brought them to the attention of genocidal authorities? reply rdtsc 14 hours agoparentprev> Everyone else is a civilian that can only be directly targeted when and for as long as they directly participate in hostilities, such as by taking up arms, planning military operations, laying down mines, etc. There is some incredible magic that often happens: as soon as anyone is targeted and killed, they immediately transform from civilians to \"collaborators\", \"terrorists\", \"militants\" etc. Of course everything is classified and restricted to avoid anyone snooping around and asking questions. reply skinkestek 13 hours agorootparentIn Norway it is rather the other way: We all know (if we stop and think) that a person can be both a teacher and a terrorist. But according to media here almost every victim except top Hamas brass seems to be referred to by their whatever else they were besides terrorists and the terrorists (or even just soldier) part get hushed down. reply NomDePlum 6 hours agorootparentCan you site an example of this please? It's contradictory to my understanding of what is happening. By that, I mean, when the few remaining police left in Northern Gaza, who had reported to be critical to providing security for aid deliveries (and involved in coordination with Israel) where assassinated recently by Israel, and claimed as high ranking Hamas targets it pretty much cemented my opinion that nothing is true, or believable from Israel in this conflict. How are you defining terrorist here as well? As other than the horrific events of October 7th, and the hostages from that day, the only visible acts of violence and terror associated with Palestine appear to be towards anyone Palestinian, journalists, aid workers and medical staff. reply skinkestek 4 hours agorootparent> How are you defining terrorist here as well? As other than the horrific events of October 7th, and the hostages from that day, the only visible acts of violence and terror associated with Palestine appear to be towards anyone Palestinian, journalists, aid workers and medical staff. You can start with the large scale, multi year campaign of using MLRS ramps to shoot barrages of unguided rockets from Gaza and Lebanon into Israel. That is indiscriminate - or even targeting civilians directly. But because Israel has gone to extreme lengths to counter it there are few causalities these days and combined with medias extreme one-sidedness that means we don't even hear when they hit a hospital in Israel last year. Cynically speaking, Iron Dome has been an expensive PR disaster for Israel, but that is what one get for caring about ones own citizens and not being allowed to just do counter battery fire until the enemy stops. reply NomDePlum 3 hours agorootparentMy understanding of what you have said: Israel has had rockets fired at it. It was frightening for the population (understandably), but it didn't affect us much. So I agree that's an impact of terrorism. But, it's really saying we haven't been impacted since October 7th is it not? Not a criticism, and a good thing. My response is just related to earlier questions. Which are now reopened. reply YZF 6 hours agorootparentprevNot sure what's the point of your last paragraph. Clearly there have been many documented visible \"acts of violence\" towards the IDF in Gaza. There have also been rockets fired from Gaza into Israel for weeks since Oct 7th and even in recent days. Plenty of \"visible\" acts of violence. By the way, Hamas also killed Palestinians they suspected of collaborating with Israel during this time. Hamas Police is Hamas. Hamas is a terrorist organization (e.g. where I live in Canada). I.e. everyone in Hamas is a terrorist, at least in Canada, the US, the EU, and I'm pretty sure in Israel. They earned that by indiscriminately attacking civilians and according to organizations like Amnesty International committing crimes against humanity. Soldiers, even if they commit war crimes, are not generally labelled as terrorists. I know sucks to be a terrorist. They fight by different rules so they get different names (they wear uniforms etc.). reply NomDePlum 5 hours agorootparentWhat are the documented, visible acts of violence towards the IDF this year? I'm not questioning there haven't been any but it's difficult to imagine a more aggressive, punitive force at present. Everything I hear about them is them being the aggressor/instigator and in a high level of cases executioner. On the execution of civilian police I don't find it acceptable to label civilians as you want and then execute them. Those are obvious war crimes. In the case I'm talking about the group of police were some of the last able to assist aid getting through and had been doing that in coordination with the IDF. I find the reporting we get (UK) very IDF/Israel based, with no real perspective from Palestinians, but still it's clear that the deaths and suffering in the current conflict day to day are pretty much all a result of Israel's deliberate actions. It's not excusable what is happening. Does it matter what you are called if you are deliberately committing war crimes? If nothing else what will Israel be like as a place to live in with so many people who have deliberately and consciously decided to kill, starve, maime and persecute so many others? How will Palestine and Israel ever recover? reply cess11 3 hours agorootparentThe resistance groups in Palestine publish videos of their operations pretty much daily, and the bigger ones publish text messages about their operations and political commentary several times daily. If you follow their communications you'll see a lot of sniping, light artillery and RPG:d vehicles. The mainstream israeli position is to hurry up and get it over with, there are daily protests demanding a change in government to one that, unlike Netanyahu who is perceived to use the military campaign to stay in office and avoid prosecution for corruption, would make a quick prisoner deal and then end the palestinian resistance as soon as possible. Edit: And if you want to take a look at how IDF/Israel presents itself you'd look for soldiers on TikTok (preferred by israelis) or Facebook (preferred by foreign fighters), and Telegram channels like dead_terrorists. You'll notice some pretty stark differences. Should probably also mention that you'll come across very NSFW, quite traumatising material. reply cess11 3 hours agorootparentprevThe palestinians have a right to violent resistance to the occupation. On the Gaza strip they're denied international relations and trade so they can only make very primitive military equipment, which means that to reach an effect at all they pretty much have to fire unguided rockets into Israel. When they tried non-violent protest against the occupation, the \"March of Return\", by demonstrating at the border they were systematically mutilated by the IDF. There is an alternative, sure, prepare for a year and then invade Israel. Which they did, after decades of \"mowing the lawn\" as the israelis call it. The terror organisation classification of Hamas isn't as much about the political party or its affiliated militia as manufactured consent to relations with Israel and traditions among colonial states. The modern 'West' usually calls its enemies terrorist, like it did during the Mau Mau uprising. This is why so few states agree with this classification. You don't have to like Hamas but compared to the PA they're not very corrupt, and since they stopped doing suicide bombings they've been quite successful as a resistance movement. Since several years back they've also been quite good at unifying and coordinating the political parties and militias on the Gaza strip in preparation for and during periods of israeli military aggression, including with their main competitor in Palestinian Islamic Jihad, socialists from PFLP/DFLP/Fatah movement, Iran's Mujahideen movement and so on. Hamas isn't just a political party with a militia, it's also a charity movement. To most people it seems weird to call people terrorists because they take care of their vulnerable neighbours and run soup kitchens and the like. reply Tainnor 19 minutes agorootparentPeople can have different opinions on the way Israel is conducting this war. I know I am conflicted. But Hamas is not a legitimate resistance movement. It is a fundamentalist, oppressive, terrorist regime. You do not stand to gain anything by associating with them. reply WJW 37 minutes agorootparentprevNobody calls them terrorists because they run soup kitchens. People call them terrorists because they take children hostage and kill civilians. Destroying the Israel and killing its inhabitants is literally in their founding charter, and they act upon it whenever they get the chance. That is why they are terrorists. reply cess11 28 minutes agorootparentNo, they got the designation because they used suicide bombings in the nineties. But OK, so you'd call Israel a terrorist state then? And consider Israel the bigger problem due to the scale of their actions? The Hamas charter is from 2017. Do you have any specific complaints about its contents? reply WJW 20 minutes agorootparent> they got the designation because they used suicide bombings in the nineties Did you completely miss their actions on October 7th? They didn't stop that kind of thing after the nineties. reply gryzzly 20 minutes agorootparentprevwho else in the world has the right to violent resistance in your opinion? do you believe kidnapping people and murdering civilians is part of that right? I can’t believe my eyes reading \"hamas is a charity movement\". Have you seen what they do to their own people? Do you know what Sinwar is popular for in Gaza when he was just making it in the Hamas hierarchy? I wonder if you also believe that Russia has reasons to attack Ukraine? It would explain your world view pretty well. reply fakedang 2 hours agorootparentprev> You don't have to like Hamas but compared to the PA they're not very corrupt, If your society's two choices are a.) lots of corruption, and b.) less corruption but with terrorism, then you've pretty much shown that you're incapable of self-governance as a people. > and since they stopped doing suicide bombings they've been quite successful as a resistance movement. Since several years back they've also been quite good at unifying and coordinating the political parties and militias on the Gaza strip in preparation for and during periods of israeli military aggression, including with their main competitor in Palestinian Islamic Jihad, socialists from PFLP/DFLP/Fatah movement, Iran's Mujahideen movement and so on. Sounds like if Israel didn't exist, these guys would just be fighting against Fatah instead. Or fighting between themselves. reply cess11 2 hours agorootparentWhat do you mean by \"terrorism\", exactly? Yeah, possibly. In the West Bank militia groups have been fighting PA forces recently due to them harassing and killing militia men and generally assisting the IDF in the occupation. After the 2006 election the PA tried to oust Hamas from the Gaza strip and got violently expelled. On the other hand, over the decades since 2006 Hamas has co-existed with lots of political movements in the Gaza strip and helped make sure their militias continued recruiting and exercising. It has been a politically repressive environment for sure, in large part because you can't survive as a political movement under occupation without developing a serious paranoia. reply elygre 1 hour agorootparentprevThis is probably obvious, but just to make sure: This is the opinion of skinkestek, and not an objective truth. As another Norwegian, I do not share this opinion. reply notsafetocomm 12 hours agorootparentprevMaybe it's because the overwhelming majority of the people being killed are actually just regular people? reply zmgsabst 12 hours agorootparentnext [39 more] [flagged] Gabriel54 11 hours agorootparentTrying my best to assume this comment in good faith... Low compared to what? For reference, in the recent war in Ukraine (post 2022), there have been approximately 11,000 Ukrainian civilians killed and approximately 70,000 Ukrainian soldiers killed [1]. [1] https://en.wikipedia.org/wiki/Casualties_of_the_Russo-Ukrain... reply _djo_ 11 hours agorootparentNot to get into the debate about that other war, but there have almost certainly been many more Ukrainian civilians killed than the 11 000 formally confirmed deaths. That's just the number that can be properly verified, mostly in Ukrainian-held territory, and nobody is entirely certain how many have died in the Russian-occupied regions. Ukraine claims a much larger number have died, including more than 25 000 in Mariupol alone, for instance, but that can't be independently verified because it's still Russian-held. reply NomDePlum 6 hours agorootparentThis is at least as true in Gaza too is it not? And over a much shorter time frame, with a significantly smaller population. The expectation is that there are at a minimum 15/20% dead under the bombed and decimated buildings. It could well be much, much higher, even double, or triple is not infeasible, given the large scale untargeted bombing, population dispersement and recognised IDF tactics that don't allow for groups to even consider searching and rescue operations in most cases but leave possible survivors buried under the rubble to die slowly and horribly. The current numbers are just not even close to verifiable given the circumstances, but are statistically clearly far worse in terms of civilians on all measures. reply skinkestek 4 hours agorootparentOne significant difference is that Ukrainian authorities goes really far to evacuate civilians while Hamas goes really far to prevent civilians from evacuating. Earlier it has been said that based on previous reporting from previous incidents we can roughly trust the total numbers Hamas release evem if they are obviously wrong in that they claim every death to be an innocent civilian. reply cess11 2 hours agorootparentHow would Hamas go about evacuating civilians? And if they could, why would they help Israel displace the palestinian population from palestinian territory? The only resistance groups in the Gaza strip that might have militia units for women that I know about are PFLP and DFLP, and I forgot which one I've seen a video of militia women from. They are probably very small and not deployed at the moment. This means that kids and women aren't in the 'brigades', and that a majority of killed palestinians are \"civilian\" for sure. Israel claims that every male they kill is a combatant, and israeli pundits and politicians routinely equate terrorist and palestinian and say things like 'there are no innocent civilians in the Gaza strip'. Neither is true, many they kill are elderly or obviously unaffiliated with the 'brigades'. Of course, this is irrelevant, since Israel is starving the entire population of the Gaza strip and kills or maims pretty much anyone they see in the areas where they operate, sometimes even other IDF soldiers or hostages. reply NomDePlum 3 hours agorootparentprevAddressing your first paragraph: All reporting I've seen has made it clear that any movement of population in Gaza has been subject to IDF and more broadly Israeli control. Every reported case I have seen appears to have been demanded by Israel, and the Palestinians have had no choice in it. I'm not sure we're you think Hamas are involved in this at all? It's all been forced displacements by Israel. And none of it has been willing. Where people have stayed and not moved often they have died, even with not the slightest involvement with Hamas. Addressing your second paragraph: Not quite sure if your point, but the numbers of deaths/casualties are broadly (my interpretation) seen as being as accurate as are available, and likely to be a significant undercount of the real number. I have to say the continual questioning of, what by a number of significant indicators, looks to be an undercount of the total number of people deliberately killed by Israel, in such a short period is appalling. It's highly likely that when we say 30,000 it's a wrong figure because it's 40,000 Israel deliberately killed. If it's wrong and it's only 20,000 Israel killed it doesn't matter. They should still stop the killing. reply sir0010010 8 hours agorootparentprevAlso Ukraine is a large country, civilians who were not drafted have mostly evacuated westward, and Poland and other countries have taken in other countless Ukrainian refugees. Meanwhile, Egypt has built barbed wire fences to prevent Palestinians from crossing the border and taking refuge there. And no other countries are presently accepting large numbers of Palestinian refugees at all. reply NomDePlum 6 hours agorootparentThe barbed wire fences were equally built by Israel, are they not? Israel definitely controls that border. The current geopolitical outcome of Egypt accepting large numbers of Palestinian is that Israel does what it is doing now to Palestine, to Egypt at some point in the future. reply ineedaj0b 5 hours agorootparentSadly, whoever takes the fleeing will likely have tremendous headaches in the future -to their own government- because of the people. Jordan took a large number in the past and they were terrible guests -attempting to overthrow the sovereign government- and got expelled. Black September left a very bad taste in nearly everyone’s mouth. Governments friendly towards Palestine are very much against physically taking them after those events reply NomDePlum 4 hours agorootparentLikely didn't state it clearly enough in my original response, but that's my take too. Also, why would Egypt let in those that might provoke conflict and war with Israel? It's a fools mission, and one which Egypt was quite clear on in November/December it didn't want to embark on. reply skinkestek 4 hours agorootparentprevI think the Egyptian fence has been built by Egyptians. That border is even more tightly controlled than the Israeli one AFAIK. Also, some useful conext: The Israeli border wasn't always like today. It has been progressively tightened step by step as nutjobs on the Arab side used whatever leeway they had to stage suicide attacks and smuggle in rocket parts. reply NomDePlum 4 hours agorootparentThat response is just random propaganda. Can you provide some actual verifiable facts please, or at the very least, something based on your experience and insight? reply doktrin 4 hours agorootparentprevEgypt is categorically not closing their border to Palestinians out of fear of an Israeli invasion. reply NomDePlum 4 hours agorootparentEgypt currently definitely is, other than in very individual cases. From Google: ================= Egypt, however, has warned against an influx of refugees. It facilitates humanitarian aid into Gaza, but has said a mass exodus of Palestinians out of Gaza into Egypt is a red line, saying it fears Israel might never let the Palestinians go back. ===================== And Egypts real fear is that the Palestinians in Egypt will try to take back Palestine. Which wouldn't be very good for Egypt and engage them in a war with Israel. reply doktrin 2 hours agorootparentYour own quote directly contradicts your assertion. reply bawolff 11 hours agorootparentprevUsually you would compare it to other instances of urban combat. E.g. you might compare it to ukrainian battles that took place in cities, but you wouldn't compare it to ukrainian battles that took place in the middle of nowhere where no civilians were. https://en.wikipedia.org/wiki/Civilian_casualty_ratio has some things to compare against. Part of the problem is it is often hard to identify who is a civilian, and often different battles will categorize them differently. For example, in the iraq war us was accused of significantly undercounting civilian casualties. All this makes it hard to do direct comparisons. reply zmgsabst 11 hours agorootparentprevA similar anti-terrorist war featuring large amounts of urban conflict, eg Iraq (3:1) or Afghanistan (4:1.1) — since much of Ukraine is designated armies across open fields. Numbers from: https://en.wikipedia.org/wiki/Civilian_casualty_ratio reply throwaway6734 11 hours agorootparentprevUkraine's troops are uniformed and fighting along a front, not trying to blend in with civilians in an urban area reply NomDePlum 5 hours agorootparentWhat indication is there that there is any significant \"blending in\" of Hamas fighters? I've seen lots of unsubstantiated claims but no real evidence of this. Even the more obvious cases were it might be true such as recently the claims of killing Hamas fighters at hospitals appear to be highly questionable. When you assassinate paraplegics and claim they are Hamas fighters how much credibility can you have? What the article we are commenting on here makes a clear, believable, claim to is that Israel is more than happy to kill significant numbers of civilians to kill a single \"off-duty\" Hamas fighter (or associate of). And particularly favours killing Hamas fighters and their family/neighbours. reply gryzzly 9 minutes agorootparentAn innumerable amount of videos of armed hamas fighters launching RPGs, shooting and attacking without uniform. Self-made live videos from October 7th massacre, using israeli civilians Facebook accounts without uniforms. \"assassinate paraplegics\" – just pure hate – 100% reader manipulation. skinkestek 4 hours agorootparentprev> What indication is there that there is any significant \"blending in\" of Hamas fighters? I think it is rather well documented. Go to r/combatfootage on reddit to see for yourself. reply NomDePlum 4 hours agorootparentSorry, that is garbage. Reddit/YouTube/Twitter/TikTok are social media, not accurate reference points. Can you point me to something credible please? reply dnate 3 hours agorootparentWikipedia has a good article about it: https://en.wikipedia.org/wiki/Use_of_human_shields_by_Hamas reply cess11 1 hour agorootparentOf what use is a human shield if your enemy would just shoot the person in front of you? The IDF commonly uses palestinians as human shields though. reply guappa 1 hour agorootparentprevedit war, completely unreliable for now. reply oa335 10 hours agorootparentprevHamas’s Oct 7th attack also had a 2:1 civilian to soldier death ratio. “The latest death toll from the attack is now 767 civilians, 20 hostages and 376 members of the security forces, giving a total of 1,163. One person remains missing.” https://www.barrons.com/news/new-tally-puts-october-7-attack... reply YZF 6 hours agorootparentSince we're quoting might as well: \"Under the cover of thousands of rockets fired from Gaza, they killed indiscriminately in streets, houses, kibbutz communities and at a rave music festival. It took more than three days of heavy fighting for the Israeli army to regain control, and left the country deeply traumatised by violence unseen since the country's formation in 1948. Police are still working to assess the scale of the sexual violence that was reported alongside the killings.\" I'm pretty sure \"security forces\" includes police and possibly firefighters and even ambulance drivers. What I found in the IDF site is 282 soldiers: https://www.gov.il/en/departments/news/swords-of-iron-idf-ca... So the ratio is more like over 3:1. More importantly your statement not true (\"civilian to soldier\"). reply oa335 5 hours agorootparent3:1 ratio = wanton indiscriminate terroristic destruction Thus 2:1 ratio = ? reply SZJX 3 hours agorootparentprevSo even if the ratio is slightly lower (also debatable whether every death they count as “combatant” is accurate), this makes a fundamental difference in terms of IDF’s behavior compared to that of who they designate as “terrorists”? reply wahnfrieden 8 hours agorootparentprevMuch of that was Israeli friendly fire reply oa335 8 hours agorootparentIndeed, both at kibbutz’s and the music festival https://youtu.be/3cPeRSVgUpQ reply weweersdfsd 4 hours agorootparent\"The Electronic Intifada\" doesn't sound like a very balanced and unbiased source. reply cess11 2 hours agorootparentYou can find interviews with commanders and soldiers about it on israeli television as well. If you watch footage from Nova you'll see large patches of obvious damage from Hellfire missiles, and footage from kibbutzes commonly show damage way beyond what you'd expect from handgrenades or RPG:s. reply lukan 11 hours agorootparentprevWell, depends how exactly you classify people as \"combatant\". https://www.haaretz.com/israel-news/2024-03-31/ty-article-ma... reply tmnvix 11 hours agorootparentprevThis could only be possible if you are assuming all males killed are Hamas militants. In other words, absurd. reply sillystuff 10 hours agorootparentprevnext [4 more] [flagged] tptacek 6 hours agorootparentI don't know about the \"Israeli Zionist European settler Jews\", but a plurality of all Israeli Jewish people are indigenous to MENA, so summing the population up that way is a little bit racist. I may be misreading you, though. reply tsimionescu 4 hours agorootparentI believe the poster above was referring to the biggest representatives of the extremist minority of Israel who are cheering this conflict on in those terms. Not all of Israel is pro-genocide, but those that are are more likely than not to be in that group. reply tptacek 4 hours agorootparentIronically: the extremist minority in Israel is probably more likely to be of MENA origin, not less. Ben Gvir is from Iraq, for instance. The Mizrahim are generally more conservative, and the Ashkenazim are generally more liberal. reply SZJX 3 hours agorootparentprevAnd again, what would be the reason why a lot of people trying to live a “normal” life feel compelled to take up arms? Many will call it resistance in an occupied territory in a lot of other contexts. By this logic when the Nazis killed members of the resistance, of course they were also fighters in addition to whatever day jobs they had. reply Aerroon 4 hours agorootparentprevAnother one is when you label any 15+ year old male as \"military age\" and treat them as combatants. reply jiggawatts 12 hours agorootparentprevOn the flip side, in this war many of the Gaza combatants are either irregular forces or militants deliberately wearing civilian clothing. So if some guy in a track suit and flip-flops uses an anti tank grenade launcher, discards the empty tube, walks away, and gets lit up, then the next day the Internet is awash with videos of the “IDF murdering a civilian!” For reference, I think both sides are in the wrong in this conflict, and Israel more than Gaza. However, the Internet is full of armchair international law experts that are being played like a fiddle by Hamas’ propaganda arm. Speaking of international laws of combat: no protections apply to non-uniformed combatants pretending to be civilians. None. They can be tortured, executed on the spot, whatever. If you want protections to apply to you, then wear a uniform or never go anywhere near a gun. reply Quanttek 4 minutes agorootparent> Speaking of international laws of combat: no protections apply to non-uniformed combatants pretending to be civilians. None. They can be tortured, executed on the spot, whatever. Speaking of \"armchair international law experts\", this is completely wrong. BLUF: Failing to distinguish does not deprive you of fundamental guarantees of humane treatment, including the prohibition of torture and summary execution - both of which are war crimes. The individual obligation to distinguish is linked to Prisoner of War (POW) status - those who do not distinguish, do not get the protections of that status. That is the only consequence of the failure to distinguish. All those persons who are not POWs are automatically civilians, as made clear by the residual clause in Article 4(4) Fourth Geneva Convention (GC IV). While civilians can be interned for \"imperative reasons of security\", they are entitled to their own detailed treatment obligations (Articles 79-135 GC IV). In any case, even if they are somehow not entitled to that treatment, the fundamental humane treatment guarantees of Art 27 GC IV [1] and Art 75 Additional Protocol I [2] (which, as customary law, applies to all parties to a conflict) nonetheless apply. If we argue that it is a non-international armed conflict (which knows neither POW status nor the obligation to distinguish), Common Article 3 [3] similarly obligates humane treatment. Humane treatment is also a norm under customary law [4]. Under these rules, you cannot torture people and you cannot summarily execute people [4]. Read the provisions yourself. In fact, summary execution and torture are actual war crimes [5]. If you want to punish a person, you need to give them a fair trial (IHL does not prohibit the death penalty). You seem to be hinting at the Bush-era \"illegal enemy combatant\" theory but even the Bush Admin never argued that those persons are not entitled to humane treatment (it was mostly about fair trial rights), and the US (as its lone defender) has long since abandoned the position. Whether Hamas is actually subject to such an obligation to distinguish is highly controversial. On one level is the issue of conflict classification, since POW status and the obligation to distinguish only exist in the law of international armed conflict (IAC). If we accept that there is an IAC (e.g. because of the military occupation), then the question still arises if Hamas somehow \"belongs\" to the State of Palestine or if they should just be seen as civilians directly participating in hostilities or as being in a parallel non-international armed conflict between Hamas and Israel. In turn, if we accept that there is an obligation to distinguish applicable to Hamas, then Israel also needs to treat Hamas fighters that distinguished as POWs (and, as set out above, if they failed to distinguish, as civilians). [1]: https://ihl-databases.icrc.org/en/ihl-treaties/gciv-1949/art... [2]: https://ihl-databases.icrc.org/en/ihl-treaties/api-1977/arti... [3]: https://ihl-databases.icrc.org/en/ihl-treaties/gciv-1949/art... [4]: https://ihl-databases.icrc.org/en/customary-ihl/v1/rule87 https://ihl-databases.icrc.org/en/customary-ihl/v1/rule89 https://ihl-databases.icrc.org/en/customary-ihl/v1/rule90 [5]: https://ihl-databases.icrc.org/en/customary-ihl/v1/rule156 reply singleshot_ 11 hours agorootparentprevWhile perfidy is a violation of the law of war, summary execution is not a generally-acceptable penalty under IHL. reply bandrami 6 hours agorootparentHuh? Summary execution has always been the punishment for perfidy under the laws of war. reply elygre 58 minutes agorootparentDo you have a reference for that? Even as perfidy is a war crime, we do not generally allow for summary execution for war criminals. reply ein0p 11 hours agorootparentprevChildren and women do not shoot up tanks reply andsoitis 11 hours agorootparent> women do not shoot up tanks There’s quite a bit of literature, history, statistics on women terrorists as well as soldiers. reply goatsi 7 hours agorootparentIn the linked article the only check the IDF was still using on the target list provided by the AI was discarding any and all targets it selected who were women, as they don't believe Hamas would use them as fighters. reply gryzzly 17 minutes agorootparentprevif you think that \"children\" are everyone who is under 18 and believe \"they don’t shoot tanks\", then you should familiarise yourself with Iran and its proxies war practices https://skeptics.stackexchange.com/questions/53791/during-th... reply leptons 11 hours agorootparentprevnext [9 more] [flagged] ein0p 11 hours agorootparentYou are aware you’re parroting war propaganda, right? I mean sure, this does happen in some cases I’m sure, for that matter I have seen the IDF _on video_ use Palestinians as human shields. But the entire article is about the fact that nobody is even looking if there are civilians there before dropping bombs, and 20K+ of women and children are now dead as a result. reply leptons 10 hours agorootparentnext [4 more] [flagged] Sporktacular 9 hours agorootparent\"When people cite deaths from numbers supplied by Hamas, they are parroting terrorist propaganda.\" No. Stop it. Numbers provided by the Health Ministry historically have been largely verified. The 30 000+ figure is not in question. All you have is a propaganda talking point doubting its source, not questioning methodology or providing a reasoned alternative figure. The fact Israel can't, and doesn't care enough to, provide numbers of its own IS THE PROBLEM. Just parrot 'terrorist figures' and you have a license to continue. \"Women and kids have killed\" as a license to continue killing women and kids. Except you don't and this ugly tribalist brutality will never be forgotten. reply leptons 7 hours agorootparentWhen a country is run by literal terrorists, there are no figures that can be trusted. I'm not sure why that's difficult to understand. How many did Hamas themselves kill because they tried to leave an embattle area because Hamas wanted to use them as human shields?? You want to blame Israel for everything, but HAMAS IS THE PROBLEM. Without Hamas using the entire country as a staging ground for terrorism (and continuing to do so), none of this would have happened. reply ein0p 6 hours agorootparentWhich country do you mean? Because Israel has easily 10x the body count. reply p3rls 10 hours agorootparentprevnext [4 more] [flagged] ein0p 10 hours agorootparentThat too is war propaganda though. reply wahnfrieden 8 hours agorootparentprevYou're repeating Zaka lies. reply gryzzly 15 minutes agorootparentYou are an antisemitic liar. Spreading hate is not OK. wavefunction 9 hours agorootparentprevYou completely forgot about the humans in your \"human shields\" after the first clause of your sentence. reply kQq9oHeAz6wLLS 11 hours agorootparentprevnext [13 more] [flagged] ein0p 11 hours agorootparentDoesn’t mean it’s true. Remember the source. A toddler can’t even lift an RPG reply bloaf 11 hours agorootparentThe Palestinians have a well documented history of using children in combat. See for example: https://en.wikipedia.org/wiki/Use_of_child_suicide_bombers_b... reply mandmandam 4 hours agorootparentFrom your own link: > At the height of the phenomenon, Avraham Burg, former chairman of the Jewish Agency for Israel, speaker of Israel's Knesset and interim President of Israel, stated his view that, given Israeli indifference to the tortured lives of Palestinian children under occupation, suicide bombings come as no surprise. ... Palestinians have been tortured from birth for decades. Acting surprised when they fight back is either disingenuous or stupid, or both. reply klipt 8 hours agorootparentprevDoes child soldier only mean toddlers though? I thought it was more commonly 16, 17 year olds. Presumably recruiting 17 year olds is against international law. But for a guerrilla fighting group that doesn't care about international law, a 17 year old is almost the ideal recruit. They can fight almost as effectively as an 18 year old can, and if they die in combat you can also claim, technically truthfully, that the enemy is deliberately targeting children. reply ein0p 7 hours agorootparentDoes Israel care about international law? Because I’m seeing a lot of dead and wounded toddlers. reply sgjohnson 5 hours agorootparentIf Israel didn't care about international law, all of Gaza would now be a smoking crater and the death toll would be an order of magnitude higher. reply mandmandam 4 hours agorootparentIt seems that you're claiming the only way to violate international law is to bomb all their people at once. This is very wrong. Targeting hospitals without clear proof of military activity is against international law. As is collective punishment, using starvation as a weapon of war, and many other things which are currently being livestreamed to the world. If you understand a topic this poorly, why not read a little about it before 'contributing' to the discussion? https://www.theguardian.com/world/2024/mar/30/uk-government-... When even Tory lawyers are telling their government that they are arming a genocidal state and need to stop immediately or be liable for war crimes... reply sgjohnson 4 hours agorootparentI'm convinced that there's no way to fight a war without pissing someone off. Even more so, it's impossible to fight an _assymetric_ war without pissing a lot of people off. > Targeting hospitals without clear proof of military activity is against international law. What's \"clear proof\" is in the eye of the beholder. > As is collective punishment, using starvation as a weapon of war I'm actually willing to give Israel the benefit of doubt here, as they actually have no experience whatsoever in coordinating large scale aid deliveries, even if delays somewhat further their goal of punishing palestinians. > UK government lawyers say Israel is breaking international law, claims top Tory in leaked recording So what? Every major player on the world stage breaks international law whenever they damn well please, UK, US, Israel, China, Russia, they all do, to the point where in my eyes \"x is breaking international law\" is a completely worthless statement. Nobody is really going to do anything about it if you're on the right side of the new iron curtain. This might be very cynical from me, but honestly, can you name the last time when the words “international law” carried any real weight? reply ein0p 2 hours agorootparentIt's far less complicated than you make it out to be. I'm on whichever side that doesn't deem it acceptable to deliberately kill kids by the thousand. Whatever else you type does not change that determination. reply sgjohnson 2 hours agorootparent> I'm on whichever side that doesn't deem it acceptable to deliberately kill kids by the thousand. Unfortunately, that side doesn’t exist in this world. Civilians have died in every war ever known to man. Whether they are kids or not is completely irrelevant. reply YZF 6 hours agorootparentprevIsrael does care about international law. A lot. The IDF has lawyers involved in many targeting decisions and other decisions. There is a department dedicated to it: https://www.idf.il/en/mini-sites/military-advocate-general-s... podcast: \"The Lawyer who Advises the IDF on Law of War issues in Gaza\" https://law-disrupted.fm/idf-lawyer-advises-war-gaza-militar... Now whether the right-wing soldier who has seen his friends murdered in a music festival and has been sent in with a tank into Gaza care? Very likely less so. Does every member of this right wing government care? Likely less. However the independent court system in Israel can also enforce the various conventions Israel is a signatory to (e.g. the Geneva convention). The situation where young soldiers are in Gaza armed to the teeth fighting an enemy that blends in with civilians is the creation of Hamas. If Israel follows international law 100% you will still see a lot dead toddlers in this kind of warfare. So just that fact doesn't prove much. reply Avicebron 11 hours agorootparentprevI wonder if we track this sentiment how far back it would go, I'd suspect it goes back about as far as there have been public complaints about child deaths. reply CommieBobDole 18 hours agoparentprevIt's also interesting (and I guess typical for end-users of software) how quickly and easily something like this goes from \"Here's a tool you can use as an information input when deciding who to target\" to \"I dunno, computer says these are the people we need to kill, let's get to it\". In the Guardian article, an IDF spokesperson says it exists and is only used as the former, and I'm sure that's what was intended and maybe even what the higher-ups think, but I suspect it's become the latter. reply NomDePlum 8 hours agorootparentI read that article and feel your interpretation is very misleading/wrong. The Guardian article makes it clear prior to those denials that those higher-up appear to not to care how accurate it is and appear to be making a conscious choice to accept the fact it is highly flawed on the basis that it might kill some of whom they would legitimately claim as valid targets. It's clear from the operational details discussed in the article the critical target number is largely number of kills, regardless of whether they are any actual material threat, or not. Cull predominantly the male population and their family members, not assassinate active threats is the overall impression I got of the Israeli strategy. I must add that anyone claiming the use of AI and inference models in this way is in anyway justifiable needs to seek help. The claim of 90% accuracy is almost certainly over claiming by over 100%. reply solarpunk 15 hours agorootparentprev20 second turnaround from target acquisition to strikes seems to guarantee it's become the latter. reply hluska 13 hours agorootparentDo you have enough military experience to say this? Or are you just guessing? I’ll guarantee that it’s the latter. reply JohnKemeny 12 hours agorootparentI'm guessing the point they're making is that there's no human in the loop, which can confidently be claimed, even without military experience. reply londons_explore 10 hours agorootparentI'd bet there is a human in the loop, but the human isn't informed. Ie. \"Your job is to press this button whenever that red light comes on\". reply NomDePlum 8 hours agorootparentThere is testimony to that exact occurrence in the article in the Guardian: https://www.theguardian.com/world/2024/apr/03/israel-gaza-ai... Quote: \"I would invest 20 seconds for each target at this stage, and do dozens of them every day. I had zero added-value as a human, apart from being a stamp of approval. It saved a lot of time.” reply lozenge 11 hours agorootparentprev\"I'm sure that's what was intended\" Intended by who? You don't kill 13,000 children by accident. reply kobayashi 10 hours agorootparentnext [16 more] [flagged] lozenge 10 hours agorootparentThe median age in Gaza in 2014 was 18. According to researchers in the US, 50%-61% of buildings have been damaged or destroyed. If Israel didn't kill tens of thousands of children it would be a statistical miracle. reply nsguy 9 hours agorootparentNot sure how we got to \"tens of thousands of children\" when even the Hamas reports don't claim that. I think 12,300 is the last claim I saw a little bit back which is not \"tens of thousands\" but I agree is a large number. You're assuming the children are evenly distributed and buildings are attacked randomly and evenly. None of which is true. Many buildings have been demolished with explosives or bulldozers while empty. Many buildings were damaged during combat with walls being breached to provide access. In many cases buildings were attacked in areas that have been ordered to evacuate. (yes not all cases). There's no doubt many children have been killed. We've all seen terrible photos of individual children dead. I don't think it's \"tens of thousands\" and I'm not sure we'll have accurate numbers. Other than that as you note there's a large number of children and so they've definitely and unfortunately impacted. reply theoldlove 8 hours agorootparentThe article under discussion says that even by internal Israeli calculations in the early days of the war on a typical day “another thousand Gazans have died, most of them civilians.” Given the population demographics of Gaza, I do not think it’s a stretch you might get to 20,000 minors killed. The article also questions your claim that the IDF was careful to protect civilians by eg destroying empty building. On the contrary, the IDF accepted extremely high civilian deaths to kill even one suspected minor militant. reply nsguy 7 hours agorootparentI didn't claim that the \"the IDF was careful to protect civilians by eg destroying empty building\". I made no particular observation about the IDFs efforts or lack thereof to protect civilians. I'm claiming many of the destroyed buildings have been empty. This really has nothing to do with the article. I'm addressing the parent's logic/argument. There have been many examples of buildings destroyed while empty, for example to make up the new boundary area Israel has created from the border. The attempt to calculate casualties via the number of destroyed buildings is a very poor methodology. Your statement and mine are not mutually exclusive- it's possible the IDF targeted buildings that had many civilians in them to get at one target. I don't know the demographics of those civilians. I don't know whether what happened in the early days of the war continued throughout the war. I would agree that early in the war Israel very likely accepted a larger number of civilian casualties to get at Hamas targets. I think the new thing potentially in this article is questioning the degree to which the targets were valid military targets. reply killerdhmo 9 hours agorootparentprevWhat do you possibly get about trying to hairsplit the number OF DEAD CHILDREN. Oh, yes, you’re right. It wasn’t 20,000 so we shouldn’t say “tens of thousand” it was just twelve thousand three hundred children. “Impacted” is definitely one word for it. Murdered another. reply nsguy 8 hours agorootparentHopefully I get you to be more accurate and specific when discussing this loaded emotional topic. Words literally kill. I think using defusing language is conductive to de-escalation. Words are important. And you're right those are different words. The word murdered has a specific meaning, e.g. \"to kill (a person) unlawfully and unjustifiably with premeditated malice\". The word impacted means: \"strongly or directly affected by something especially in a negative way.\" The word \"child\" also has a definition. In this context it usually refers to people under the age of 18. I'll turn the question back at you, what are you trying to get out of claiming \"tens of thousands\" or \"murdered\"? I think the word murdered is IMO more appropriate to the children Hamas killed in Israel on Oct 7th and civilians it has killed in Israel over the years and less appropriate for the children killed in Gaza by IDF action during a war. That said my heart breaks for every single child, whoever they are, that lost their lives. It's not their fault. This war didn't need to happen. We should try our best to protect children, here and everywhere, and I do recognize that at some level you're trying to do so, thanks for that. reply rideontime 6 hours agorootparentYour pedantry isn't even correct. Say it with me: \"One point two tens of thousands of dead children.\" reply Aliabid94 10 hours agorootparentprevThe US and even Israeli governments consider these numbers as accurate. The Health Ministry provides the social ID numbers of every death so the deaths would be impossible to fake. reply nsguy 10 hours agorootparentI don't think the US or Israel made any statements regarding the accuracy of the composition of the numbers. Only that the total ballpark numbers are more or less accurate. If you have a link to the US government statement about the breakdown accuracy of the numbers I'd be interested to look at that. Also this is extrapolating from previous rounds of violence and this one is very different. Do you have a link to the officially published name, sex, age, ID, of all the victims so far, by date, in the conflict? When large buildings are demolished by bombs and can't be excavated because of active violence what's the process for figuring out who potentially was or wasn't in that building? with all the refugees moving around the chaos how do you know if someone who used to live there is dead or sheltering somewhere else out of contact? Hamas reports everyone as civilians (or doesn't make a distinction). reply theoldlove 7 hours agorootparentThe official list is published on the Gaza Health Ministry’s telegram account. The most recent list is available at https://t.me/MOHMediaGaza/5261. reply nsguy 7 hours agorootparentIs the only way to download it by installing this telegram app on my machine? EDIT: While trying to find this from another source I found this: https://news.sky.com/story/israel-hamas-war-health-system-co... \"Of the 21,703 identified fatalities whose details have been shared by the Hamas-run health ministry, 13,207 were women, children or elderly (61%).\" \"Until recently, however, the ministry had been reporting a figure of 72%.\" \"Mr al Wahaidi told Sky News that this was a \"media estimate\". He was not able to explain the basis for this estimate or who had produced it.\" \"Since speaking to Sky News, he has stopped using this figure in his reports for the health ministry. It continues to be used by the government media office, a separate branch of Gaza's government.\" ... \"media estimate\". Another by the way is that I think we should assume that given the significant bombing campaigns are more or less over the more recent casualties likely include less randomness and more targeted. I would guess that the ratio of women/children/elderly is much lower in more recent stages of the war. reply elihu 7 hours agorootparentprevThey have released those at points in the past; I'm not sure if they're doing it on an ongoing basis, and their numbers include a large number of unidentified casualties. I think those usually make up about a third or so if I remember correctly. Not included are a large number of missing people, and all the people who die of secondary causes like maltutrition, disease outbreaks, and poor access to proper medical care. Nobody really knows what the real casualty figures are, but I'm inclined to take the Gaza Health Ministry's numbers seriously -- as do Israel and the US, as you already mentioned. reply lobocinza 8 hours agorootparentprevHow it would be impossible to fake just because they provide the social ID number? reply runarberg 10 hours agorootparentprevThis link again. People keep posting it and it keeps—rightly—getting flagged. This source is a schoolbook example of lying with statistics, and so far HN seems prerry aware of that. reply nsguy 9 hours agorootparentWouldn't it be better to discuss it and for you to take it down point by point and show everyone why it is \"lying with statistics\"? I only have a B.Sc. and my only observation was the sample size seemed to small and there might have been some cherry picking of the date ranges. You might be correct but we're not getting to discuss that and what are the proper statistical methods one can apply here if we had the data (and I don't think we have data). This link being wrong however doesn't negate the fact that the only numbers we have come from Hamas that is 1) in most western countries is considered a terrorist organization 2) is one of the sides in this war 3) clearly benefits from distorting the numbers 4) shouldn't be considered a reliable source for anything. The numbers discussion seems to always come back to in previous rounds of violence those same numbers seemed more or less in the ballpark though there have been major revisions in the past re: the ratio of combatants. (I don't have the link handy but I can find it if you dispute this last bit). reply pixl97 16 hours agorootparentprevhttps://en.wikipedia.org/wiki/Computer_says_no https://en.wikipedia.org/wiki/Computers_Don%27t_Argue reply GuB-42 12 hours agorootparentprevIdeally, that would be \"Computer says we shouldn't kill these people, let's not\". reply jiggawatts 12 hours agorootparentIt’s a very powerful drug to be able to shrug your shoulders and say you were just doing as you were told. reply hluska 13 hours agorootparentprevnext [34 more] [flagged] dotnet00 12 hours agorootparentWhen this latest series of attacks started there was still some room to charitably interpret what the IDF had to deal with, but we've had months of constant action and very obvious suffering and death that the IDF has been imposing upon Gaza, either intentionally or through sheer apathy. They've long since lost the \"oh but think critically\" excuse. The amount of suffering they are inflicting is not at all justified, it has gone far beyond just a tit-for-tat retaliation. reply erickf1 10 hours agorootparentSince when has war ever been tit-for-tat. Since the beginning of time, if a nation where to attack and make war with another nation, they had to measure carefully the consequences if they were to lose... because losing meant losing everything. This is no different. Russia attacking Ukraine is no different. No one is tracing and counting every bullet Russia fires and certainly no world court is saying anything at all to Russia… or to Turkey, or Iran, or Syria, or anyone else… except for Israel. Gaza attacked Israel and like any nation that goes to war, they will either win or lose. Consider carefully before declaring war against your enemy…. Everything is at stake. reply tga_d 8 hours agorootparentTo be clear, is the position you're arguing that governments that commit war crimes should not be held to account? reply underlipton 8 hours agorootparentprev>Since when has war ever been tit-for-tat. Since we had a global war which almost destroyed civilization. https://en.wikipedia.org/wiki/Proportionality_(law)#Internat... reply gizmo 9 hours agorootparentprevYou're right that the violence and devastation Israel has laid upon Gaza has historic precedence. Genghis Khan completely destroyed cities that he considered cultural enemies, to give one example. You'll find many instances of genocide in the history books, even post WW2. Israel is not unique in that regard either. But why does it matter? Does historical barbarity justify present day barbarity? It doesn't, and we all know it doesn't. It's not the case that Israel is held to a higher standard. Russia has been widely condemned, blocked from international finance, and faces severe sanctions. On top of that, western countries have given substantial military and intelligence aid to Ukraine. Russia had reasons for their invasion of Ukraine (just like Israel has its reasons), but so what? Russia being angry at Ukraine is not a justification for destroying the country. I also take issue with your characterization that Gaza \"started it\". This is a 70+ year conflict with many chapters of violence in it, and innocent Palestinians make up the bulk of the casualties. Israel is not the victim here. Finally, it's not accurate to talk about Israel/Gaza as two nations at war. Israel controls everything coming in and going out of Gaza, electricity, water, etc. Palestine is not an independent country or a state, it's part of Israel except its citizens have no rights. reply throwaway331337 8 hours agorootparent> has historic precedence. Genghis Khan completely destroyed cities Are you even serious? Genghis Khan? Read about literally any modern war fought. Read about WW2, about bombings of North Korea, that might open your eyes: https://en.wikipedia.org/wiki/Bombing_of_North_Korea Why look even that long ago? Look up \"Battle of Mosul\" (check images in Google) — its 2017, very recent. Look up Ukraine war. All looks the same. But no one cries genocide (because it isn't). War always comes along with major destruction of cities, because the enemy uses buildings as a fortification. > many instances of genocide It's not a genocide, it's just a regular war. Stop abusing the term for god's sake. Look up typical civilian-to-combatant death ratio in major wars, you'll be surprised. reply zmgsabst 12 hours agorootparentprevnext [24 more] [flagged] the-smug-one 11 hours agorootparent>When people say you’re not “thinking critically”, they’re saying you’re trying to portray one of the modern conflicts with the lowest civilian deaths (versus combatants) as a crime against humanity while ignoring numerous others — eg, genocides in Niger or Myanmar, and forced expulsions in Armenia/Azerbaijan. Why is it that this is always mentioned? As if ignorance of one crime against humanity makes us incapable of criticizing the other? And where exactly are the public spokespeople from our governments talking about how any of these genocides are justified as the killers have a \"right to defend themselves\"? Not to talk about how an attack that killed 4000 people justifies killing 25000 non-combatants. > They’re not interested in a tit-for-tat retaliation: they’re intending to destroy the political and military structures that made the attack possible. A smaller country can’t cry “that isn’t fair!” when they start a fight and get beaten — this isn’t a scuffle between kids at school. This is not even comparable to what is occurring when the world is condemning Israel's actions. If Israel was interested in removing the political structures that made Hamas's attack supported by Gaza then they could've stopped the settlement of the west bank, supported the stability of the Palestinian state, and countless of other actions which would have lowered the risk of creating terrorists in Gaza. reply chgs 11 hours agorootparentPeople hold israel to a higher standard as it’s a modern western democracy and not some tinpot banana republic or military junta. reply myth_drannon 9 hours agorootparentWell Turkey is a democracy(and a NATO member), Armenia is too. It's the fixation with Jews that compels such scrutiny of everything that Israel does. reply chgs 3 hours agorootparentTurkey only gets away with what it does due to its strategic importance at the entrance to the Black Sea. reply flyinglizard 10 hours agorootparentprevIf you followed the conflict you’d have seen that Israeli concessions (especially the unilateral withdrawal from Gaza) have only made the Palestinians more radical. The overwhelming states goal of Palestinians is the destruction of Israel and any Israeli concession leads towards that. reply gizmo 9 hours agorootparentIsrael doesn't want peace because they're winning. Every year Palestinians lose territory, and the land that remains becomes more and more uninhabitable. reply flyinglizard 4 hours agorootparentIsraelis don’t want peace because every time it was attempted, it only led to more violence and misery. The suicide bombings around Oslo accords in the 90’s, the second intifada of 2000, the terror enclave that became of Gaza after the disengagement in 2005; peace is a dangerous pipe dream. reply gizmo 30 minutes agorootparentThe last time Israel “attempted” peace was before the majority of Palestinians were even born. The only peace talks that had any chance of working were held by Rabin who got promptly murdered. Israel also never stopped killing Palestinians and refuses to return the land they have unlawfully occupied. reply yawaramin 11 hours agorootparentprev> We all should have worked harder at solving the problem, but a genocidal militant group Yes, 'we all' should have worked harder when Benjamin Netanyahu actively funded Hamas and expended all possible efforts to prevent a viable Palestinian state. Genius thinking right there. reply Sporktacular 10 hours agorootparentprev\"the political and military structures\" Then why no aid? Why no water, electricity, economy or medical infrastructure either? \"inevitable\" Yes, they made us do it. See what they made us do? \"Niger or Myanmar, Armenia/Azerbaijan.\" Hey everyone, look over there! No one will take blind defenders seriously. reply dotnet00 11 hours agorootparentprev\"We all should have worked harder\" is such an absurd thing to be saying alongside that sorry excuse you've presented. The entire point of human rights and rules of war is that there are certain rights the people of even small countries that started the fight are entitled to. You don't just get to excuse relentlessly bombing hospitals and aid workers. \"We thought it was a military target, but we will not disclose why, nor will we disclose what we're doing to not make this mistake in the future\" is not a get out of jail free card for genocides, especially when it never seems to come with any actual signs of improvement. Campaigns to stop genocides in other places having been unsuccessful does not justify smaller genocides taking place elsewhere. That's not critical thinking, that's whataboutism. Particularly considering that not only is America's supposedly democratic leadership not condemning the atrocities, they're actively offering the aid to continue it while claiming to want peace. Being from India, I can relate to the troubles with islamic terrorism that Israel has faced, which is why I mentioned having initially been sympathetic. But if India engaged in this large scale indiscriminate slaughter of muslims, it'd have been rendered a pariah on a similar tier as Russia. As it stands it's already constantly accused of being undemocratic and violating the rights of Muslims, despite never having undertaken deliberate, remorseless government sanctioned slaughter of this scale. It took far less for the current Indian prime minister to be banned from Western nations when he was chief minister of a state. All he had to do was fail to stop a much less deadly riot and get repeatedly exonerated from accusations of wrongdoing by several courts. reply zmgsabst 11 hours agorootparentOkay — what specific rules of war do you believe have been broken? …what specific atrocities? I was responding to the demand for “tit-for-tat” and claims this was unusually brutal; neither of those are true. You’re now making different, non—specified claims in emotionally charged language. Be specific; think critically. reply dotnet00 11 hours agorootparentI've pointed out two things, bombing hospitals and bombing aid workers. There's also targeting children, having no qualms about the collateral damage when they bomb houses to get at single targets and so on. Using systems like the one described in the article to offload further responsibility, such that if by some miracle Western nations do try to introduce the IDF to the concept of accountability, they can just blame the computer and promise to do better. I'm using emotionally charged language because these are supposed to be emotional topics. \"Critical thinking\" on its own is just a pathway to justifying extreme inhumane cruelty. reply JumpCrisscross 10 hours agorootparent> bombing hospitals and bombing aid workers Yes on aid workers. The hospitals, unfortunately, appear to have been used for military purposes by Hamas. That makes them valid targets. reply temporarely 10 hours agorootparentI was watching one of those ww2 movies with typical evil Germans (Nazis) in it and there was this scene where the SS officer dude is about to burn down a hamlet or something because of \"partisans\" hiding in there. We, Americans, are being forced to change our morality and humanity to suite an \"indispensable ally\". (Both of which are definitely open to question). That said, how cute that they are blaming both the 10/7 event (\"intelligence failure\") and their daily killing (for our viewing pleasure) of civilians, on AI. I think \"intelligence failure\" is accurate but not in the sense that was offered. It is an intelligence failure of a people to recognize that they are on the wrong side of humanity and history. You can't blame that on AI. I think it a cultural failing - an overly inflated and exaggerated sense of historic grievances, a conceit regarding God Almighty's affections, and a misread of the Global Room, and clear contempt for the \"nations\" watching. https://www.reuters.com/world/middle-east/chef-jose-andres-s... reply JumpCrisscross 10 hours agorootparent> there was this scene where the SS officer dude is about to burn down a hamlet or something because of \"partisans\" hiding in there And we fire bombed multiple German cities, the British with an explicit policy of killing German civilians who lived near factories. Many things are horrible and either permitted explicitly by international law or, by convention and precedent, technically illegal but widely tolerated. reply temporarely 10 hours agorootparentNazi Germany is nothing like Occupied Palestine. Nazi Germany had millions of people's blood on its hand at that point. That is why it was \"widely tolerated\". And your bring this up is a case in point of intelligence failure I alluded to. People may seem to \"tolerate\" mandated group think but we're still mostly Human beings and have empathy and can tell the difference between Nazi Germany and Palestine. At some point the silent consensus will be quite vocal. Germans are still paying for their crimes in WW2 ... Something to keep in mind. reply JumpCrisscross 9 hours agorootparent> Nazi Germany had millions of people's blood on its hand at that point. That is why it was \"widely tolerated\" This is a myth. We didn’t fight Germany to stop the Holocaust. (That said, I agree on the moral unequivalence. Hamas aren’t the Nazis. But not every German was a Nazi, either. We ultimately draw lines on even collective punishment.) If we don’t want to use Dresden, take Vietnam. Or Cambodia. Or Afghanistan or Ukraine or the Uyghurs or Kashmir. (Or Sudan, right now. Or Eritrea in ten minutes.) It sucks. But the international laws with relevance are the ones that aren’t being systematically violated by every regional power. We aren’t changing our standards to suite Israel, this is just the first conflict in a generation we’ve bothered to pay attention to. reply lozenge 11 hours agorootparentprev“I have ordered a complete siege on the Gaza Strip. There will be no electricity, no food, no fuel, everything is closed,” - Israel defence minister reply tmnvix 11 hours agorootparentprevDenial of aid. Collective punishment. reply Sporktacular 10 hours agorootparentAlso war crime. reply pvaldes 11 hours agorootparentprev> what specific rules of war do you believe have been broken? Basically every single one. We will end much faster if you just read the laws. And this is not \"a belief\" or a \"lets debate for a year more if this is or not a genocide while sipping tea and killing faster\". The ship of good faith has parted many months ago. reply forgetfreeman 10 hours agorootparentprevYour comment is a stellar example of the Bullshit Asymmetry Principle in action. You'd have us wade through untold reams of international law for specific references, a task that would likely take hours, just to rebut your glib denial of the current state of play. Oh well, I've got some time to kill... Shit That Should Land Israel's Government and Military Apparatus In The Hague, Abridged: Per ICC Article 8: (https://www.icrc.org/en/doc/resources/documents/misc/5nsf46....) - (i) Intentionally directing attacks against the civilian population as such or against individual civilians not taking direct part in hostilities; - (ii) Intentionally directing attacks against civilian objects, that is, objects which are not military objectives; - (iv) Intentionally launching an attack in the knowledge that such attack will cause incidental loss of life or injury to civilians or damage to civilian objects or widespread, long-term and severe damage to the natural environment which would be clearly excessive in relation to the concrete and direct overall military advantage anticipated; - (v) Attacking or bombarding, by whatever means, towns, villages, dwellings or buildings which are undefended and which are not military objectives; World Bank report on destroyed civilian infrastructure: https://www.worldbank.org/en/news/press-release/2024/04/02/j... - (iii) Intentionally directing attacks against personnel, installations, material, units or vehicles involved in a humanitarian assistance or peacekeeping mission in accordance with the Charter of the United Nations, as long as they are entitled to the protection given to civilians or civilian objects under the international law of armed conflict; The Israeli army has a storied history of bombing the shit out of aid workers that goes back decades, everything from shelling UN aid warehouses with white phosphorous munitions to calling in artillery strikes on aid convoys. This behavior is well-documented and certainly not limited to the current conflict. - (xxv) Intentionally using starvation of civilians as a method of warfare by depriving them of objects indispensable to their survival, including wilfully impeding relief supplies as provided for under the Geneva Conventions; Do we really need to review the current state of play with relief aid in Gaza given it hasn't been 5 days since an Israeli patrol greased a convoy of aid workers? Shit's gotten so bad foreign governments have taken to air dropping aid. Additionally as I'm sitting here combing through the Geneva Conventions there are a few things that stand out: - Part I, Article 3, 2) seems to be in olay between shelling the fuck out of aid workers, bombing hospitals out of existence, and the several documented attacks on emergency response vehicles. - Part II, Article 24 on child welfare also seems like an unambiguous faceplant given this: https://twitter.com/UNLazzarini/status/1767618985397272831?s... I'm certain there's more here but you aren't getting more of my time than the initial hour I budgeted to the task of putting together this reply. Have fun with the supplemental reading... reply bjourne 10 hours agorootparentprevIt's because actions speak louder than words. The bombings Gaza has suffered is worse than Dresden during WWII. There is a famine in north Gaza. 30 000+ Palestinians have been killed. There is live footage of Palestinians waving white flags but still getting sniped by Israeli soldiers. The same soldiers that laugh while blowing up mosques and running over corpses with their bulldozers. When someone show you who they really are you should believe them. reply ofiryanai 4 hours agorootparentDresden wasn't full of guerilla army disguised as civilians, and there weren't even the tools to target specifics as we have today. Any civilian was evacuated from north Gaza 5 months ago, secured by IDF under Hamas attacks. Those who stayed there insist on sticking to a war zone and risk to be labeled as terrorists. blowing up mosques and houses is nothing compared to raping innocents with no one to defend them miles away. And while they hold hundreds of the same innocents underground and keep provoking Israel's destruction, I see no reason to care about their precious houses. Israel is judged by Western morals in a barbaric medieval war that it was dragged into. reply goatlover 5 hours agorootparentprevThe firebombing of Tokyo was around 100,000 dead. reply Sporktacular 10 hours agorootparentprevSome have. Most of the IDFs current critics haven't. They get the news and conclude reasonably that the IDF deserves criticism over how it's conducting itself. For example: \"The army also decided during the first weeks of the war that, for every junior Hamas operative that Lavender marked, it was permissible to kill up to 15 or 20 civilians\" When a computer generates a target list of thousands, that's how tens of thousands of innocent people die. reply bawolff 15 hours agoparentprev> That is, only members of the armed wing of Hamas (not recruiters, weapon manufacturers,.. I think the loop-hole here is that a weapon manufacturing facility is almost certainly a military strategic target, and international law allows you to target the infrastructure provided the military advantage gained is porportional to the civilian death. So you can't target the individuals but according to international law its fine to target the building they are in while the individuals are still inside provided its militarily worth it. reply klipt 8 hours agorootparentBut presumably if you can target the building e.g. at night when nobody is there, that's preferable to targeting it during the day when there may be more civilian workers. reply Quanttek 47 minutes agorootparentExactly! The key difference is that the worker still count as civilians in the calculus that considers whether an attack is proportional (anticipated military advantage vs expected civilian effects) and whether the attacker took all feasible precautions to avoid and minimize civilian loss, including attacking at night, using tailored weaponry, giving a warning, … reply firejake308 12 hours agoparentprevPractical AI did a podcast episode about the dangers of using AI models as a shield to hide behind in justifying your decisions. The episode was titled \"Suspicion Machines\" and based on the libked article [1], and I think it's worth a read/listen. [1]: https://www.wired.com/story/welfare-state-algorithms/ reply wahnfrieden 8 hours agorootparentIt's also an incentive to use it - accountability evasion. reply throwaway7351 14 hours agoparentprevBy the standards discussed in the article, anyone with a beef with Israel could justify targeting possible a majority of buildings in Israel. After all, most of the population is required to serve in the IDF. reply BurningFrog 12 hours agorootparentBoth Hamas and Hezbollah are routinely doing exactly that. reply goatlover 5 hours agorootparentprevWhat do you think all those rockets being intercepted by the Iron Dome are targeting? reply shmatt 15 hours agoparentprevGitmo is still open, if the US isnt participating in those laws, I don't see how any of its allies are expected to reply BLKNSLVR 9 hours agoparentprevThis happens in wartime: https://www.abc.net.au/news/2024-04-03/world-central-kitchen... https://www.abc.net.au/news/2024-04-02/israeli-strike-that-k... Pretty disgraceful (which itself feels a disgracefully unimpactful thing to say regarding people losing their lives whilst doing charity work). reply 2-3-7-43-1807 6 hours agoparentprevthey don't need lavender to kill civilians. they'll do it intentionally. https://edition.cnn.com/videos/world/2023/10/31/wolf-idf-spo... reply jibe 14 hours agoparentprevThat is, only members of the armed wing of Hamas (not recruiters, weapon manufacturers, propagandists, financiers, …) can be targeted for attack It seems wrong that you can't target weapon manufacturers, can you cite a source? Weapon manufacturers contribute to the military action, and destroying weapon manufacturers contributes to military advantage. reply Quanttek 12 hours agorootparentYou can target the manufacturing plants since they are military objectives but you cannot target the workers. If any war-sustaining activity would make you, as a person, a target, pretty much anyone could be bombed: farmers, bankers, power plant engineers, truck drivers, ... For a source, you can check out the Red Cross document I linked. Specifically, Ctrl+F for \"continuous combat function\" and read the commentary on recommendation V. The Guidance is considered authoritative in legal circles. reply quandrum 12 hours agorootparentprevIn the case of Hamas, the US and Israel are the primary weapon manufacturer, as unexploded ordinance is the primary source of their explosives. https://www.nationalreview.com/corner/hamas-is-using-unexplo... reply zip1234 9 hours agorootparentIt is not the primary source of their weapons, nor is it the primary source of their explosives. It might be \"a primary source\" now as your linked article mentions, but certainly not THE primary source. Hamas is primarily given weapons by Iran. reply cess11 1 hour agorootparentThey claim to be producing a lot of them locally in the Gaza strip, based on russian or iranian design. They've proudly showed videos of factories producing al-Ghoul rifles, Tandem anti-tank grenades, light artillery grenades, rifle ammunition. This is an explicit ideal in the 'Resistance Axis', to develop the ability to produce military equipment locally and not be dependent on brittle trade routes or smuggling. The West Bank seems to get rifles from several sources, both american style that probably comes from PA or IDF and russian style, probably smuggled through Jordan from Iraq, Iran, Russia. They produce IED:s locally, quite crude ones, not the directed type with concave copper plates favoured by iraqi militias. Eventually they'll learn to make those too, I'm sure. reply nickff 13 hours agorootparentprevThis is a very 'anti-war' opinion by a lawyer affiliated with the Red Cross, not some sort of treaty or other convention. As an example, the Geneva Convention's scope of protection is much narrower. reply Quanttek 11 hours agorootparentWhile the DPH Guidance has it's controversial parts (Rec IX), the guidance on interpreting \"directly participating in hostilities\" is quite authoritative. And that should be emphasized: the Geneva Conventions allow the targeting of military objectives, combatants (i.e. members of armed forces) and \"civilians directly participating in hostilities\". The Guidance just interprets the latter and arguably widens the scope, because - without the invention of \"continuous combatant function\" - you could attack e.g. members of Hamas' armed wing during an attack and in preparation of one. Now you can attack them at any time. reply Aloisius 4 hours agorootparentEm. From the foreword: > First, the interpretive Guidance is an expression solely of the ICRC's views. While international humanitarian law relating to the notion of direct participation in hostilities was examined over several years with a group of eminent legal experts, to whom the ICRC owes a huge debt of gratitude, the positions enunciated are the ICRC's alone. Second, while reflecting the ICRC's views, the interpretive Guidance is not and cannot be a text of a legally binding nature. The purpose and of the Interpretive Guidance is to provide recommendations, as the document itself states, in an attempt to persuade states. It does not claim to be authoritative. reply Quanttek 40 minutes agorootparentI did not assert that it would be legally binding. However, it is considered to be quite authoritative by lawyers, including military lawyers. The two most controversial parts concern the idea of \"continuous combatant function\" to define members of an armed group, which some want to see defined more narrowly or more broadly (latter: US), and recommendation IX. However, the criteria for direct participation on hostilities are widely accepted as the authoritative interpretation by States and scholars of that term in the Geneva Conventions. Of course, the document itself would not make a statement on its authoritative nature since, despite the broad consultation with experts, they cannot predict the wider reaction. reply sgjohnson 5 hours agoparentprev> That is, only members of the armed wing of Hamas (not recruiters, weapon manufacturers, propagandists, financiers, …) can be targeted for attack - all the others must be arrested and/or tried. In theory, yes. In practice--in which make believe world is this true? reply usaar333 6 hours agoparentprevI don't understand your point here. They are targeting militants with the system: > Formally, the Lavender system is designed to mark by all suspected operatives in the military wings of Hamas and Palestinian Islamic Jihad (PIJ), including low-ranking ones, as potential bombing targets. Obviously any judgement is probabilistic. reply tootie 10 hours agoparentprevThat's not exactly a prediction. It was was standard operating procedure for Warsaw Pact nations. They used human intel which was possibly even worse because it could manipulated out of malice. reply abtinf 10 hours agoparentprevIt would be difficult to deliberately design a set of rules that would prolong war and human suffering even longer than what you’ve described. reply snird 17 hours agoparentprevnext [11 more] [flagged] anigbrowl 16 hours agorootparentThis is demonstrably untrue (it happens of course, but is not ubiquitous as comabt footage demonstrates) and in any case does not obviate the LOAC. reply KittenInABox 16 hours agorootparentprevI am okay with the notion that war is dirty and that Hamas will try to pretend to be civilians or whatever. I have some questions: - if the weapons always wait for them at target locations, who is transporting the weapons at any given target location and why is that not the focus? - if we know they try to use infrastructure like hospitals, the IDF clearly knows where Hamas is aiming to shelter in, why not militarily occupy hospitals but then otherwise allow them to run without delay vs sniping anyone who shows up in a window? - how is it possible the IDF allowed premature babies to die in their incubators once medical staff left a hospital in Gaza (aka unoccupied facility for IDF to sweep through), such that once medical staff returned they were presented with their rotting bodies left untouched in the incubators? - how were Israeli unarmed civilians waving white t-shirts get mistaken for armed Hamas combatants and shot dead when trying to escape from their capture? - how did the World Kitchen convoy, which had provided the IDF their route and time and coordinates with clearly labeled trucks, get shot with targeted missiles from above? reply theoldlove 7 hours agorootparentOn the death of the premature babies see https://www.nbcnews.com/news/world/abandoned-babies-found-de... reply _a_a_a_ 16 hours agorootparentprev> It is very convenient to criticize it when you're not in it Israel 'criticised' Hamas for their monstrous attack six months ago, started a war over it. Perhaps you're saying Israel should have just accepted it? You know, perhaps this whole mess Israel is now involved in is a product of its own behaviour, and killing of loads more Palestinians is not likely to bring peace but further hate and evil. reply _a_a_a_ 16 hours agorootparent3 downvotes in 16 minutes, well I never! (but oddly no other response) reply ben_w 15 hours agorootparentBecause you're arguing against a totemic representation with rhetoric rather than helpful points. Also, weirdly, looks like you're arguing against both sides' totemic representations in this way, so you're going to annoy everyone. reply _a_a_a_ 15 hours agorootparentI found that putting solid facts in front of Israeli supporters just got me downvotes. Anyway >> It is very convenient to criticize it when you're not in it > Israel 'criticised' Hamas for their monstrous attack six months ago, started a war over it. Perhaps you're saying Israel should have just accepted it? That is not rhetoric but a straightforward question. reply ben_w 15 hours agorootparentThat's totes rhetoric: what you wrote there is a sarcastic false dichotomy to make a point seem obvious — a leading question. Ironically, given I now know which side you think you're taking, that bit you're proud enough of to quote, defends Israel's behaviour by suggesting the only possible alternative to their current actions was to simply accept Hamas' previous attack. reply _a_a_a_ 14 hours agorootparentI was pointing out the hypocrisy of telling the world they should look away from what's being done in Palestine because war is dirty, but should sympathise with Israel after Hamas's awful attack, because war is dirty. You don't care about Palestinians but you expect the world to care about Israel, despite committing obvious warcrimes. I very much want peace for both sides, a stable society and good life for everyone, Arab and Israeli. You clearly don't share that wish. You have no morals, no moral authority, and what you are doing is putting Israel's future at greater risk than Hamas could, but you're so shortsighted and self-centred you're blinded to that. reply mrs6969 16 hours agorootparentprevAnd you can justify any kind of civillian killing and genocide just like this. Anyone can do anything, lets just target them all. reply _blk 13 hours agoparentprevnext [3 more] [flagged] Comma2976 13 hours agorootparent>civilians as shields For as much as the IOF likes to market this expression, I only ever see them do it in the actual sense, like chaining literal 12 year-olds to the front of armored vehicles https://www.btselem.org/ota/104/all https://en.wikipedia.org/wiki/Human_shields_in_the_Israeli%E... reply smashah 13 hours ago [flagged]rootparentprevnext [1 more] There's no justification for committing a holocaust. reply kjkjadksj 15 hours agoparentprevnext [2 more] [flagged] ben_w 15 hours agorootparent> Ai flagging targets sure beats the cold impunity of the strategic air campaign during wwii and vietnam by far. Only on a single iteration of a Nash game. If it tempts the politicians and generals to target larger areas because of the lower costs… https://en.wikipedia.org/wiki/Base_rate_fallacy reply ghufran_syed 5 hours agoparentprevnext [5 more] [flagged] cad31 1 hour agorootparent“Israel seems to trying much harder to avoid civilian casualties than” how? Their intelligence are doing 20 second of vetting on people some dat",
    "originSummary": [
      "The Israeli military deployed the AI program \"Lavender\" to pinpoint targets in Gaza, leading to a high number of civilian casualties, including women and children, due to the system's high error rate and lack of human oversight.",
      "Despite ethical concerns, the military persisted in using AI for targeting, causing extensive destruction and loss of innocent lives, particularly when employing \"dumb bombs\" and abandoning bomb damage assessments, raising uncertainty about civilian casualties.",
      "Critics argue that Israel's targeting strategies are counterproductive, risking Hamas' increased support, as the far-right government intensifies attacks on Palestinians, silencing dissent and heightening policing measures, prompting support for platforms like +972 Magazine for coverage."
    ],
    "commentSummary": [
      "The debate on using AI in guiding Israel's strikes in Gaza raises concerns about targeting individuals linked to terrorists and breaching international humanitarian law.",
      "Discussions explore historical US support for eugenics and Nazi ideas, Hamas's role, ethics in targeting civilians in war, disputes on casualty figures, and challenges in distinguishing combatants from civilians.",
      "Reflections are made on Israel-Palestine conflict, the ethics of military actions, propaganda usage by both sides, and the intricate dynamics in Gaza, highlighting international law, ethical aspects of AI in warfare, and the complexities in the conflict zone."
    ],
    "points": 1205,
    "commentCount": 1089,
    "retryCount": 0,
    "time": 1712155830
  },
  {
    "id": 39919401,
    "title": "Tips and Tools to Uncover Shell Company Owners",
    "originLink": "https://gijn.org/stories/tracking-shell-companies-secret-owners/",
    "originBody": "Image: Shutterstock Stories • Topics » Investigative Techniques » Reporting Tools & Tips Tips for Linking Shell Companies to their Secret Owners by Rowan Philp • April 1, 2024 Related Resources How to Investigate Money Laundering Organized Crime Chapter 1 – Money Laundering Video: GIJC23 – The New Organized Crime GIJN’s Elections Guide for Investigative Reporters — Revised for 2024 Explore the Resource Center → Share Tweet Facebook Print LinkedIn WhatsApp Telegram Reddit Republish For investigative journalists, the search for the actual owners of shell companies and trusts can sometimes seem as elusive and “fuzzy” as popular searches for UFOs. But there are powerful tools out there that can help newcomers to this complex field track breadcrumbs to people who go to great lengths to hide assets that the public should know about. While often partly obscured by secrecy jurisdictions, shell companies always need paperwork in order to be incorporated. Shell corporations are companies that don’t actually do any business, but which are often incorporated for strategic reasons, such as legal tax avoidance for legitimate corporations — and also to hide the identity and assets of the individuals who truly control illegitimate or sanctioned enterprises. Digging into shell companies and their ultimate benefit owners (UBOs) is a specialized investigative area, represented by networks such as the International Consortium of Investigative Journalists (ICIJ) and the Organized Crime and Corruption Reporting Project (OCCRP). The advanced techniques they use, from tracking bank transfer routes to deferred corporate prosecution agreements, could fill several books. However, in a solo presentation at the recent 2024 NICAR data journalism summit in the US, Karrie Kehoe, deputy head of data and research at ICIJ, shared several tips, tools, and places to start that almost any reporter can try to track the person at the top of a shady empire — and their overseas assets. Kehoe invited attendees to first ask themselves: What words might appear in paperwork that must be filed for these front companies? And to then experiment with possible variations of those words that could be used in corporate registries and databases. For instance, might the name of a director or owner, like Ian, appear as “Iain”? Karrie Kehoe, deputy head of data and research at ICIJ. Image: ICIJ While often partly obscured by secrecy jurisdictions — such as the British Virgin Islands, Panama, Cyprus, or Cayman Islands — shell companies always need paperwork in order to be incorporated. Kehoe also invited reporters to think carefully about the people typically paid to fill out these forms, also known as “formation agents.” It’s their job to create companies for wealthy clients in friendly legal jurisdictions, which can offer country registration fees as low as US$14. For extra fees, formation agents can also provide clients with bank accounts, “straw man” director names, and secretarial services. But they can’t get around the basics on those forms: an official business address, real names of at least some directors, and documents about the nature of the business. Searching for foreign assets of a Middle East king in 2021, Kehoe and her colleagues found that the monarch secretly owned 14 luxury homes in the UK and the US via a network of front companies in tax havens. They then got dramatic confirmation when the secret client’s home address was listed by formation agents in registration documents as the kingdom’s “royal palace.” Remember, she stressed, that you are always seeking the ultimate beneficial owner, rather than the director or owner names you may find early in your search. “You could be sitting in the Netherlands, and you get your order off to a formation agent in, say, Hong Kong, and he starts setting up shells for you in the Cook Islands, Singapore, and other places,” said Kehoe, explaining one possible scenario. “From there, they will provide secretarial services, an address for you, and straw men — in other words, figureheads. One could be listed as the director of 100 companies, and that person could be a taxi driver in Azerbaijan. He has nothing to do with this — they may get a small bit of money; sometimes they are relatives or just people they know; sometimes they are strong-armed. But he is a body the formation agent needs.” Shell companies represent one of several kinds of “proxy” methods that people use to disguise their wealth, as investigative journalist Will Neal recently explained at OCCRP. For more on this, see GIJN’s detailed resource for Researching Corporations and their Owners, which includes access portals for annual reports, and links to country-specific registries, from Germany to Hong Kong. Here are some of the places-to-start tips Kehoe shared at NICAR24. Start with a quick company or person name search in OpenCorporates. Drawing on company records from more than 140 government registries, OpenCorporates is a vast, open database that reporters love. “The first thing I do is go to this wonderful, searchable database – it has over 220 million companies listed there,” explained Kehoe. “It is wonderful for some jurisdictions; it struggles for others. For instance, you’d need to go somewhere else for Irish shell companies. If you want to look up a whole lot of companies, they do have API access too.” OpenCorporates is a vast, open database drawing on company records from more than 140 government registries. Image: Screenshot, OpenCorporates Consider a subscription to a corporate risk database if you hit a wall with open source tools. Kehoe showed one major shell company name that got no hits on the excellent OpenCorporates database, but which got a detailed profile from a simple name search in the paid-for Sayari corporate risk platform– including helpful red flag icons to show sanctions listings, and national flags to show countries associated with listed directors. “This is a database I use all the time — it’s subscription-based, but they also have some free trials,” she said. “There are also other great paid-for corporate databases like Orbis and Factiva.” Put yourself in the shoes of billionaires and oligarchs. Kehoe said it helps to refine database and registry searches for places and interests that appeal to the egos of oligarchs and UBOs. “The thing about super wealthy people is that they’re very predictable, and, if you’re a billionaire, there is only so much stuff you can buy,” Kehoe noted. “They often want luxury properties in London and the South of France; they want mega-yachts and jets and art and sports teams.” Simply identifying and searching for the wealthiest neighborhoods in certain jurisdictions, she said, can help narrow searches for hidden assets. Paperwork tends to poke holes in secrecy — so keep digging. Use vetted investigative data in the ICIJ Offshore Leaks Database. Many reporters don’t realize that the vast, searchable Offshore Leaks database — with vetted information on 810,000 offshore companies, foundations, and trusts in 200 countries — is freely available for any reporter, anywhere, to use, and to explore for new stories in their regions. “Please use it,” she said. “We know there are more stories in all that data, and we want reporters to find them.” Compiled by ICIJ, the database has been built from previous leaks and collaborative investigations into secret offshore assets and their hidden owners, such as the Panama Papers, the Pandora Papers, the Bahamas Leaks, and the Paradise Papers projects. “You can just put addresses into the search box, or the names of people or companies,” she explained. Reporters can also filter their search by the major leak investigations, which focus on different regions and jurisdictions — so a filter for Pandora Papers, for instance, will highlight more Asia data than Panama Papers. Kehoe added one note of caution: “Just because someone’s name appears in this database does not mean they are guilty or associated with any wrongdoing at all. It just means they’re listed in an offshore jurisdiction.” (See ICIJ’s video tutorial series for advanced search techniques for this database.) Flag potential criminal links in OCCRP’s follow-the-money archive. OCCRP’s Aleph database has information on 439 million public entities in 141 countries. “It also has persons of interest datasets and gazettes,” said Kehoe. “Gazettes are huge in Europe — if a company is struck off [from company registers], it has to be listed in a gazette.” (Limited companies are typically struck off, or removed, from official registers for regulatory non-compliance, and are prohibited from further trading.) She added: “(Aleph) also has lots of court records, which is important for many countries, because most countries don’t have a court records tool like Pacer in the US.” OCCRP’s Aleph database has information on 439 million public entities in 141 countries. Image: Screenshot, OCCRP Aleph Experiment with different spellings — and check against Google Maps. Using the name of her own hometown in Ireland as an example, she said reporters searching for people or assets in Dún Laoghaire should also try spellings such as “Dún Laoire” and “Dunleary.” “If you try different spellings for places, you’d be surprised what you come up with,” she said. “Leaks data comes from many different languages, so if you want to find an address in Russia, there’s no point entering the word “street.” I recommend going to Google Maps, and, in this case, taking the Cyrillic phrase, and putting the English translation of that phrase into the search field of databases.” Even more important: use alternate spellings for the names of directors listed in corporate registries. This can be as simple as trying traditional alternatives, such as “Ann” and “Anne.” “But remember that when looking at Russian names, in particular, the spellings often change – “i”s become “y”s, “x”s become “ks”s etc,” Kehoe explained. Tip: Enter addresses you find on OpenStreetMap, and check to see whether the director’s listed address is the mansion you’d expect, or a tiny office or apartment that could signal another false front. Cross-search the “nuggets” you find in other free portals. Kehoe suggested the following sites as other useful, free resources to search for hidden cross-border business connections: Open Ownership, the UK-based Register of Overseas Entities, and Tenders Electronic Daily (TED). “TED is amazing — it involves public contracts for 27 EU member states, and it goes back a long time,” said Kehoe. “But it can be a little tricky as it has many different languages together.” Tip: Image search for society photos of the oligarch or UBO you’re investigating with the names listed in datasets and corporate registries as company directors. Try a family connections tool to track oligarch assets. “Russian oligarchs are the world masters of shell companies — they’ve been doing this since the 1990s, and reporters are always playing catch up,” said Kehoe. “The first thing to do is to check if they are sanctioned.” She suggested a website called RuPEP, which profiles thousands of “politically exposed persons” and sanctioned individuals in Russia, Belarus, and Kazakhstan, as well as their relatives, and their association with legal entities. She added: “These kinds of databases often give you nuggets of new information, but it’s a great place to start. This has profiles for a lot of oligarchs, and the research is amazing,” said Kehoe. “It gives you family members, which is important because a lot of the assets are in family members’ names.” Paperwork tends to poke holes in secrecy — so keep digging. “Be persistent,” Kehoe advised. “Ultimate beneficial owners really do trust their formation agents to keep them safe from scrutiny, and share a lot of stuff with them. And so, somewhere down the line, they send in a scan of their passport or their utility bill, and their home addresses often pop up in leaks and databases that way.” Rowan Philp is GIJN’s senior reporter. He was formerly chief reporter for South Africa’s Sunday Times. As a foreign correspondent, he has reported on news, politics, corruption, and conflict from more than two dozen countries around the world. Related Resources How to Investigate Money Laundering Organized Crime Chapter 1 – Money Laundering Video: GIJC23 – The New Organized Crime GIJN’s Elections Guide for Investigative Reporters — Revised for 2024 Explore the Resource Center → Related Stories From Real Estate to Racehorses: Tracking Hidden Assets Around the World Follow the Money — and Enablers — When Investigating Kleptocracies and Mafia States Following the Money, from Laundromats to Central Banks GIJN Toolbox: Hunting for Secret Money and Financial Conflicts of Interest More Stories → This work is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International License Republish this article Republish our articles for free, online or in print, under a Creative Commons license. Read other stories tagged with: corruption databases financial crime follow the money investigating offshore companies NICAR 2024 offshore assets shell companies Republish this article This work is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International License Material from GIJN’s website is generally available for republication under a Creative Commons Attribution-NonCommercial 4.0 International license. Images usually are published under a different license, so we advise you to use alternatives or contact us regarding permission. Here are our full terms for republication. You must credit the author, link to the original story, and name GIJN as the first publisher. For any queries or to send us a courtesy republication note, write to hello@gijn.org. Copy to clipboard Read Next GIJC23 Reporting Tools & Tips From Real Estate to Racehorses: Tracking Hidden Assets Around the World by Emily O'Sullivan • November 1, 2023 When reporters know how to follow the money, hidden wealth can often be uncovered in real estate, planes, yachts, artwork, and even racehorses. GIJC23 Reporting Tools & Tips Follow the Money — and Enablers — When Investigating Kleptocracies and Mafia States by Rowan Philp • October 23, 2023 Veteran journalists explained how an industry of enablers is supporting a growing group of kleptocracies and mafia states around the world, and that following the money is a great way to track them. GIJC23 Following the Money, from Laundromats to Central Banks by Laura Oliver • September 25, 2023 How did three reporters investigate complex financial stories featuring Nigerian billionaires, Eurasian oligarchs, and the head of Lebanon’s central bank? Reporting Tools & Tips GIJN Toolbox: Hunting for Secret Money and Financial Conflicts of Interest by Rowan Philp • January 24, 2022 In this edition of GIJN Toolbox, we profile three brand new — or newly expanded — tools to dig into financial secrecy and hidden gains from corruption or crime. Our list includes a user-friendly database to search for sanctions and conflict-of-interest red flags, a site that uses an algorithm to detect hidden bank accounts, and a newly expanded database on the true owners of offshore companies.",
    "commentLink": "https://news.ycombinator.com/item?id=39919401",
    "commentBody": "Tips for linking shell companies to their secret owners (gijn.org)954 points by chippy 17 hours agohidepastfavorite425 comments nocoiner 17 hours agoThese are great techniques and helpful advice, but note that they are basically of zero use whatsoever in the case of (for instance) a Delaware LLC. The amount of information regarding beneficial ownership that’s out there varies by jurisdiction and entity type - and again, these tips are great ways to dig into those and quite likely come up with some interesting findings in a lot of cases - but in most cases a Delaware LLC is basically a black box. reply binarymax 16 hours agoparentI didn't know this until I searched for myself in the linked OpenCorporates site - was surprised to see my Delaware LLC not listed. How/why is this true? IMO it should be straightforward to find the owner/director of a US based corporate entity. reply lobochrome 10 hours agorootparentBecause the US, after bullying countries like Switzerland, Lichtenstein and Luxembourg to clean house, is now left as the single biggest western tax haven in the world. reply greesil 10 hours agorootparenthttps://www.theguardian.com/world/2019/nov/14/the-great-amer... reply grugagag 8 hours agorootparentThis article is 4 years old, I wonder what are the new figures… reply sealeck 54 minutes agorootparentprevSwitzerland, Lichtenstein and Luxembourg aren't exactly innocent and the US has done a good job in pushing for measures such as the end of banking secrecy. Other measures such as the OECD minimum tax benefit everyone, and hopefully the US will also push for the implementation of OECD Pillar 2 (to prevent profit shifting). reply cqqxo4zV46cp 19 minutes agorootparentYes, but the parent poster’s point still stands reply roenxi 8 hours agorootparentprevStarting with the fairly tame observation that taxing commercial activity results in less of it; I'd imagine the country that is the biggest tax haven has significant commercial advantages. I'd imagine if we looked back at the height of the British Empire's commercial successes we'd find London was hoovering up commercial activity from all over the place. And once a lot of finance is flowing through a country, I'd expect some sort of interplay with political influence to happen. reply ImHereToVote 3 hours agorootparentIf you don't tax commercial activity and wealth, you end up taxing work. This kills societies. reply discreteevent 1 hour agorootparent> If you don't tax commercial activity and wealth, you end up taxing work. This kills societies. In general this is true but if you are one of the few tax havens in the world then you have so much money going through that a very small percentage tax adds up to enough money to keep your citizens quiet. This only works locally though. Globally, other societies are getting screwed. reply dclowd9901 4 hours agorootparentprevI mean, like it or hate it, I guess having the world’s most powerful military has its benefits. reply throwawayqqq11 4 hours agorootparentLove it when the system works for you and you can shift assets away from any resposibility. Hate it when you are the 6/7 (actually much more) of mankind on the other side of the gun. Sorry, but i hate these shrug \"but it has some benefits\" reactions. Without a perceived moral high ground, your military is useless, ask putin, and your displayed lacking critical stance is the ever green substrate for that. Sorry again. Whenever i see such pro/indiffernt US foreign policy posts, i see the next war coming and i feel urged to reply. reply ImHereToVote 2 hours agorootparentIf it's any consolation, the US military isn't the military for the American people. It's a gun for hire for the biggest political donors. This is the case to such an extent that the military leaders are completely in the dark as to why decisions are made. https://www.youtube.com/watch?v=FNt7s_Wed_4 reply csomar 2 hours agorootparentprevI am not sure how and why everyone is ignoring the elephant in the room here. These laws are being \"forced\" by different \"international\" organizations at pretty much every country (except partially China and countries like Iran and North Korea that don't give a damn). Many countries could make money accepting financial refugees. That's how Hong Kong and Singapore built their wealth city states (good governance was just a part of it). Even if we are being moralistic and transparent; for some countries, being tax-free is their only competitive advantage. Instead we are now to assume that tax-free jurisdictions are evil and should be abolished. Might as well just restart colonialism; at least colonial countries got to be part of the big trade. reply Shaanie 54 minutes agorootparentTax havens is a race-to-the-bottom thing, just like not caring about emissions. Or using slaves, if you want a more extreme example. If your country's only advantage is to steal tax money from other countries, or provide cheap energy by burning coal, or cheap labor by using slaves, then it's perfectly fine to isolate that country to force it to stop. We obviously shouldn't just accept that everyone is worse off just so that a country with no other advantage can thrive. reply tomrod 12 hours agorootparentprevBanks require additional paperwork for Delaware-registered LLCs because they can't search the company structure directly at the Secretary of State website (this was explained to me last week). reply sealeck 14 hours agorootparentprevA lot of jurisdictions do maintain a beneficial owners database (e.g. Companies House in the UK maintains a list of all companies) but often these are not public. For example the ECJ recently made these illegal in the EU (see e.g. https://www.ft.com/content/e4b31a4e-a79d-40f7-8a19-c1e451a95...). reply Animats 9 hours agorootparentThat may clash with the European Directive on Electronic Commerce for businesses which do anything online. If you just have a shell company to own your yacht, that may be entitled to privacy, but if you do transactions online within the EU, the entity has to be properly identified.[1] [1] https://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX... reply throwaway22032 12 hours agorootparentprevWhy should it be true? reply collegeburner 5 hours agorootparentprevi am surprised by your opinion it is fundamentally and deeply inconsistent to support consumer privacy except for joe six pack’s LLC for his general contracting business financial privacy is hugely more important than google knowing whether i buy tide or downy reply autoexec 42 minutes agorootparent> inconsistent to support consumer privacy except for joe six pack’s LLC Those aren't the same at all. An LLC is a privilege created under state laws and it's perfectly reasonable for those states to require records that the public can access in exchange for that privilege and the benefits it offers. Having consumer privacy is good for the American public. Having access to know who owns an LLC is also good for the American public. Perfectly consistent. reply Scubabear68 10 hours agoparentprevPrecisely right. There are firms that specialize in listing companies in Delaware and become the official “agent” for the firm. Figuring out who actually controls it is nearly impossible. reply 1vuio0pswjnm7 8 hours agoparentprevAre there are other sources for details on ownership of Delaware LLCs. https://www.delawarebusinessincorporators.com/blogs/news/ben... \"A. 3. Under the Corporate Transparency Act, who can access beneficial ownership information? FinCEN will permit Federal, State, local, and Tribal officials, as well as certain foreign officials who submit a request through a U.S. Federal government agency, to obtain beneficial ownership information for authorized activities related to national security, intelligence, and law enforcement. Financial institutions will have access to beneficial ownership information in certain circumstances, with the consent of the reporting company. Those financial institutions' regulators will also have access to beneficial ownership information when they supervise the financial institutions. [Updated January 4, 2024]\" https://www.fincen.goc/boi-faqs That includes prosecutors and courts. After an authorised person obtains the information on beneficial ownership from the FinCen database, can they transfer it to their agents. If they share the information with others then will they, in every instance, bind those agents to appropriate obligations of confidentiality. What is the penalty if the information gets leaked, accidentally. If it is leaked, then what incentives are there to pursue enforcement. Arguably, for those affected, it's too late. The information has become public. There is no way to put the genie back in the bottle. https://www.ecfr.gov/current/title-31/subtitle-B/chapter-X/p... This reporting requirement is relatively new. Maybe it really changes nothing. We shall see. But the number of parties who potentially have access to information about Delaware LLCs has increased, IMHO. At this point, is it even possible to know who might have it. reply yieldcrv 16 hours agoparentprevfrom the article: > While often partly obscured by secrecy jurisdictions — such as the British Virgin Islands, Panama, Cyprus, or Cayman Islands ….. I wish people were in a place to see where the ICIJ is misguided, like they play into a sentiment that is widely shared but heavily misunderstood For example, the Cayman Islands was assumed to be super secret and shady and then the results of their information sharing agreement came out and it turns out the Delaware was waaay more heavily used and way more secretive. Jumping the US to the least transparent jurisdiction…. Up from spot number 2. All this offshore money hiding stigma is capturing the minds of people that are being intentionally mislead. Not by the ICIJ theyre just as misled. just the collective apparatus of a more powerful state that does protectionist things for its own industries against smaller nations states. The US has difficulty bullying its constituent member states, and directs all of that angst outwards to anyone competitive, while the US is a bigger market participant in the same behaviors the whole time! for both foreigners looking to hide ownership and money, and citizens, the US onshore offers a catalogue thats at parity or superior to financial services microstates, and flies under all scrutiny reply asdfman123 13 hours agorootparentPolitics would be so much better if we focused on these kinds of arguments instead of people defending their identities as business owners or employees. reply readyman 10 hours agorootparent>instead of people defending their identities as business owners or employees In a political/economic context, those are not \"identities\" but distinct roles in a power structure. Their political and economic interests are different and often fundamentally opposed to the extent that they're canonically considered two disting classes: working class* and capitalist class. It is ridiculous to consider them mere \"identities.\" reply FireBeyond 15 hours agorootparentprev> the Cayman Islands was assumed to be super secret and shady ... it turns out the Delaware was waaay more heavily used and way more secretive Ugland House in the Caymans. 10,000sq ft, 5 stories. Registered offices of forty-two thousand companies. Corporation Trust Center at 1209 North Orange Street, Wilmington, Delaware, United States, \"home\" to over 285,000 Delaware corporations. reply balderdash 8 hours agorootparentWhy is that weird? Company are required to have registered agents in that state (every legitimate Delaware LLC that isn’t based in Delaware (think most of corporate America) has to have someone do this for them (mostly so they can be sued) reply refurb 5 hours agorootparentIndeed. I don’t think people understand what a Registered Agent is. Basically it’s a requirement to have someone available as a point of contact for the LLC - typically name, phone and address for mail. This is to ensure that should the LLC need to be contacted, someone is available. It can be a hassle if you list the owner or an employee as people come and go and companies can be bought and sold. You can list the owner, but it is there responsibility to make sure it’s constantly up to date. Registered Agents offer a service to act as the Agent on behalf of someone else. The Registered Agent will make sure the phone and address is always up to date and there is always someone on the other end to respond. It’s basically a “go between” as a service. So why would anyone be surprised that 40,000+ businesses all use the same agent? reply yieldcrv 14 hours agorootparentprevthat's a decent example, there are other many addresses and other ways of counting straight from ICIJ itself \"US lands top spot as world’s biggest enabler of financial secrecy in new index\" https://www.icij.org/investigations/pandora-papers/us-lands-... > Meanwhile, the Cayman Islands, which ranked first in 2020, dramatically dropped to No. 14 in this year’s index, after disclosing new data on the financial services it provides to foreigners. in reality, it should have always been No. 14 or assumed to be operating the way it was, and it was just assumed to be far bigger and shadier than reality. where no information is assumed to mean bad information. that's just not the case. there are robust domestic ways to avoid claims on assets and money even from tax authorities, stigmatizing the entire offshore industry is just protectionist mentality. reply dade_ 8 hours agoparentprevRecently there was a book written about Delaware and this problem. What’s the Matter with Delaware?: How the First State Has Favored the Rich, Powerful, and Criminal—and How It Costs Us All https://press.princeton.edu/books/hardcover/9780691180007/wh... reply jgalt212 16 hours agoparentprevvery true, but you can glean some clues regarding any entities that came before the Delaware LLC, or another state LLC that has a similar name, or does transactions with the Delaware LLC. Or you can search on those you think may be involved with the Delaware target. Their names often show up in other less opaque filings. So, you man not find the smoking gun, but you can sure gin up a lot of smoke. reply lsllc 15 hours agorootparentIn fact many states require LLCs created in other states to register as a foreign entity if they do any business in that state (incl. having a presence there). For example in Massachusetts [0]: Pursuant to M.G.L. Chapter 156C, §48, a foreign limited liability company doing business in the Commonwealth must submit to the Corporations Division within ten days after it commences doing business in the Commonwealth, an application for registration as a foreign limited liability company ... So even if you form your LLC in Delaware, if you live/work/conduct business in Mass, then you must also register there -- it costs $500/yr and the statute requires that a non-trivial amount of information about the company be supplied including the names/addresses of \"managers\" for the entity as well as the registered agent. I don't know what the privacy laws for foreign entities in registered in MA are (if any), but I suspect not the same as say Delaware. If you have a footprint in more than one state, you may be required to file as a foreign entity in multiple states. [0] https://www.sec.state.ma.us/divisions/corporations/filing-by... reply nocoiner 16 hours agorootparentprevFor sure. By way of further example, if you’re looking into land owned by an LLC (or other entity), the LLC had to be conveyed that land at some point - searching the county grantor/grantee index of deeds will turn up who owned the land before and may be suggestive as to who owns the LLC (or maybe not - maybe ownership of the LLC was conveyed instead of the real property interests specifically to avoid showing up in the county records - it’s a fun puzzle). reply dimal 12 hours agoparentprevHow on earth is this legal? reply codexb 11 hours agorootparentThere's no way to solve this problem short of public, government registration of all cash and assets, which has been done before, but which most people are wary of, and for good reason. There are so many ways that a person can be the beneficiary of a corporation that is not technically \"owned\" by them, depending on how you define \"own\". Does the corporation issues stock? Do they have investor agreements? Is it just a loan agreement? You'd have to register all those different documents. Follow that all the way down and you eventually have to register all assets and cash. In the end, there's very little legitimate legal reason to have to know precisely who controls an asset or cash, so long as someone is responsible for the public obligations of taxes, unless there has been some crime. reply wpietri 9 hours agorootparentI'd say flip that around. If people own things in their own names, they're welcome to reasonable privacy. But the corporation is a special government program to enable certain kinds of collective action because we get broader economic benefits from things like the ability to limit liability. If people want special government support for financial activities in the broader economy, which is what corporations are, then I say key information should default to being public. If people don't like that, well, they don't have to use the government program. But like any government program, I think voters should have enough information to be sure that it's not being misused. reply codexb 6 hours agorootparentIt's not like companies didn't exist before the US created LLC's in the 1970's. You don't need legal, registered corporations to have companies. Companies would still exist. And even without explicit limitations on liabilities created by LLCs, any reasonable court is going to limit the liability of a company. It's trivial to create private agreements and EULA's that do effectively the same things as LLC law. The thing you're failing to realize is that the concept of \"ownership\" and \"benefit\" aren't black and white. There are stockholders, investors, beneficiaries, members, clients, subscribers -- they all have at least some level of benefit from a company or organization, and depending on how it is organized, they may have some direct control or liability of it as well. What you are asking for is mandatory violations of privacy with no legitimate purpose. It's the kind of \"if you've got nothing to hide, then you shouldn't be worried\" type of mentality that typically isn't tolerated elsewhere. reply solidsnack9000 7 hours agorootparentprevShould the names and addresses of all shareholders of all public corporations be public record? What about private corporations? reply mitthrowaway2 10 hours agorootparentprevThere's plenty of reason. For example, it's often very important whether deals are made between \"arms-length\" parties, especially where a valuation is involved. If a corporation is allowing me to buy their property for $1, it matters whether that is the fair value, or if the corporation just secretly belongs to my brother and we're avoiding taxes on asset transfer. reply solidsnack9000 7 hours agorootparentThe fair value can be assessed independently of that, though -- and frequently, it is anomalies in values relative to fair value that lead to investigations that reveal the non-arms-length nature of a deal. reply mitthrowaway2 7 hours agorootparentIt's often very difficult to assess fair value when it's not determined by an arms-length transaction. (That's one of the challenges faced by command economies). And even when it can be done, it takes a lot of work to do. Even the price of oil, or shares in a company, can fluctuate on a daily and regional basis. In an individual transaction it's even more difficult -- is this a loss-leader? Or a free sample? -- and when it's a unique, non-fungible good, still more difficult. The price of used assets sold at auction are all over the map. And for some assets there is very little active price discovery in the marketplace. That cryptocurrency that went up in price 1000x shortly after it changed hands, was that just a lucky call? Is that patent really worth $500k? That commercial property that's been without a tenant for nearly a decade, is it really worth an imputed rent of $40k/month when nobody has been willing to rent it at that price? ... But if these two companies signed a contract that values it at that figure as collateral for a loan, then maybe that's the market price? As they say, location location location, right? Etc. reply solidsnack9000 6 hours agorootparentI think you've got something backwards. The arms-length principle is not literally a requirement to be unrelated. It is a requirement to act that way. If one brother sells another brother a moving truck for 40000 USD, it is an arms-length transaction. If he sells it for 1 USD, that is not. It's a non-arms-length transaction because of the value, not because they are brothers! In some of the cases you mention, it's difficult to assess fair value even when it's determined by an arms-length transaction, so the sudden disclosure of a relationship would not make it at all straightforward to take anyone to court over it. The commercial property without a tenant for nearly a decade isn't worth 40k USD / month. When you say \"...these two companies signed a contract that values it at that figure as collateral for a loan...\", you are describing fraud. That could arise not because of any plan of fraud prior to the loan agreement but because of one formed during it! reply nmfisher 7 hours agorootparentprevI don't agree with this at all. Beneficial ownership/shadow directorships/controlled entities/etc etc are well established legal concepts, and many jurisdictions have very stringent requirements to disclose and register those interests. Some - like Delaware, apparently - do not. That's a regulatory/political decision that they have chosen to make. Would it eliminate shell companies? No (and at the end of the day, dedicated fraudsters will just forge documentation to lie about control/ownership anyway). But it would definitely reduce their usage, which I believe would be a net benefit. reply dimal 10 hours agorootparentprevI suppose when you look at it from this angle, it sounds reasonable. But then when I consider that corporations are essentially eternally existing legal persons which behave like sociopaths, and are able to get away with any crime (even occasionally killing people by accident) as long as they pay a small fine, it makes me a little upset to think that a corporation can do all this and the beneficiaries can be hidden from us. reply solidsnack9000 7 hours agorootparentImagine if all beneficiaries in corporations had to be publicly known. That means all shareholders, and by extensions all holders of mutual funds, &c (since they hold shares in a trust and the trust holds shares in the corporation, they are the ultimate beneficiary, &c) and thus all participants in retirement funds, &c. It would mean no privacy for anyone. reply mathgradthrow 11 hours agorootparentprev>good reason. skeptical of the \"good\" qualifier. reply codexb 11 hours agorootparentHistorically, registration of assets has been used by government to seize and tax the property of those they opposed. reply JumpCrisscross 11 hours agorootparentprev> How on earth is this legal? It massively simplifies reporting requirements. If you’re forming a Delaware entity, you file and go. reply IG_Semmelweiss 7 hours agorootparentprevThe right to privacy, defined broadly as \"the right to be let alone\" (in speech / commerce) is enshrined in the US constitution, specifically 1st, 4th, and 14th amendments. Other constitutions do not protect this privacy. Its easy to forget that the US founding fathers wrote anonymously - including the federal papers. Time will show which constitution outlasts all others. reply readyman 10 hours agorootparentprevCapitalists choose the laws reply doublerabbit 12 hours agorootparentprevFirst state of America. reply BLKNSLVR 7 hours agoprev\"Shell corporations are companies that don’t actually do any business\" This, I think, is a fairly key differentiator between valid and invalid use of incorporation. If a company doesn't do any business, then it shall not have a right to exist as it has no reason to exist, as the reason companies exist is to do business. One may argue there are other reasons a company may exist, but I'd argue those reasons only exist as an unintended consequence of the ability to exist as 'shields' or 'cut-outs' as discovered by those familiar with the peculiarities of international law and accounting/finance. reply Hermel 6 hours agoparentThere are various valid use cases for companies without business. Examples include: - International holding companies: if there is Coca Cola France and Coca Cola Germany that economically belong together, you might not be able to just merge them into one entity for legal reasons (both countries might require you to have a locally incorporated presence). So to ensure that both always have the same owners, you create an international holding company that owns both of them. - Investment funds: investment funds (especially passive ones) are companies whose only business is to own shares in other companies. There is no \"real\" operating business. - Feeder funds: sometimes, the law requires foreign investment funds to create a local shell company to be allowed to accept investments from local retail investors. In this case, the only purpose of the shell company is to fulfill local regulatory requirements with regards to the legal form if the investment vehicle and to provide investors with someone local that they can hold liable in case things go wrong. There is no real business in such companies. In fact, it is often regulation that requires you to create shell companies. If you want to get rid of shell companies, you should start by removing regulation that requires the creation of shell companies with no real business except to satisfy the regulators. reply goodlinks 3 hours agorootparentAre you not giving more examples of how shell companies are created to get around regulation here? I.e. an indicator not really following the spirit of the regulation, only the letter? reply aelhaji 7 hours agoparentprevNo, that statement is a subjective assessment. Any company, including shell companies, will have business activity on paper. In other words, there are invoices, financing or a combination. This is why it’s difficult to define shell companies. A more realistic proxy is to look at the ratio of employed people by the particular entity to revenue or some other financial metric. Most shell companies don’t have many employees while the financial figures might be huge. Again, this might be a bona fide structure as part of an international holding but at least you have an objective filter as a starting point. reply boringg 7 hours agorootparentHasn't there been a lot of talk about AI enabling the 1 person billion dollar company recently? reply lIIllIIllIIllII 6 hours agorootparentprevThis isn't my area but I feel like they might be kind of easy to define. They're wrappers around their inner companies. I'm sure nothing has been done because pretty much every corpo lobbyist represents a company that uses them. reply adgjlsfhk1 5 hours agorootparentthe problem here is that most companies are wrappers around inner companies. reply defrost 7 hours agoparentprev> If a company doesn't do any business, then it shall not have a right to exist as it has no reason to exist, as the reason companies exist is to do business. One typical use of shell companies in mineral exploration is to obfuscate regions of interest from the prying eyes of competitors. If some wants to gather lease ownership of a large number of small leases in (say) a province of Canada then it's a matter of public record that mining|exploration leases change ownership in public records. The end goal here is to publicly declare ownership after exploration results (geochimistry, prelim drilling, etc) have been assesed by third party technical reports and put a prospect on the stock market to attract investors. This gets a bit complex when someone else owns a band of rights dead centre through your ROI. This is a use of shell companies that's distinct from hiding assets from taxation, it's obfuscation for the purpose of getting ducks lined up before going public. reply lIIllIIllIIllII 6 hours agorootparentI'm not sure if corporations need a right to privacy... reply defrost 6 hours agorootparentIt's obfuscation not a right to keep a secret .. databases such as https://www.spglobal.com/marketintelligence/en/campaigns/met... get around such things through the power of cross referencing. Initially with a lot of manual trawling through microfiche records, later with computer assistance and digital records, recently with fully automated approaches. reply rfrey 6 hours agorootparentprevWhy is the lease information a matter of public record? reply defrost 6 hours agorootparentIt's regulation in a great many countries about the globe that mining lease data is recorded and made available to \"the public\", eg: in the US this is largely handled by the BLM: https://www.doi.gov/ocl/blm-lands-leasing As to the existantial nature of your question, you might try The Evolution of Resource Property Rights by Anthony Scott, perhaps look back to Roman Doctrine and how their laws carried forward in Western civilisation, look to Chinese history, etc. There's a brief narrow overview of some of that here: https://www.pheasantenergy.com/mineral-rights-history/ and a whole lot has been written about \"The Commons\", etc. reply rfrey 5 hours agorootparentSo is it a problem that corporate secrecy allows people to do an end-run around the intent of these regulations? reply defrost 5 hours agorootparent? The intent of the regulations is to record who is searching for what and where (ie. can be contacted and are responsible for property damage, spills, destruction, etc) to have one prospecter per parcel, etc. Can you point out a country where it is explicit that the intent is that (say) Rio Tinto has to directly list head office on every lease in the country and cannot spin off a copper division, an iron subgroup, a rare earth exploration subsidiary, etc? reply Mengkudulangsat 7 hours agoparentprevSometimes we need to incorporate a shell just to have cleaner legal agreements. A typical investment firm may have dozens of special purpose vehicles just to manage shareholders. reply epa 6 hours agoparentprevNot that simple - as in, a company that not yet does business but may. That may be financed for that. reply agys 15 hours agoprevNot shell companies but made me remember They Rule…! https://theyrule.net reply pksebben 11 hours agoparentholy cow. This is one of the best writeups[1] of \"the mess\" I've ever seen. Thanks for sharing. 1 - https://theyrule.net/so_what reply hirsin 9 hours agorootparentI felt it quite underwhelming, until I realized it only looked at a handful of companies. Somehow only three of Microsoft board are on other boards? Wow, how puritan of them! Ah, not quite... reply pksebben 4 hours agorootparentI'm with you - it would be better with more data (and the creator has stated he wants to expand to 500 companies, apparently that used to be the number prior to the most recent update which was a while ago). It's the writeup he has in the \"why\" section that blew me away. A very sober and deep analysis of why we are where we are and what might be done about it. reply tennisflyi 13 hours agoparentprevJust like those Reddit power mods reply stemlord 8 hours agoparentprevHas this been updated? I thought it stopped functioning several years ago reply VirusNewbie 12 hours agoparentprevI did the apple board and the majority of people weren't on any other boards... reply er4hn 11 hours agorootparentThis data appears to be 2021 top 100 companies. Lisa Su is solely listed as a board member of Cisco for example, since AMD had yet (I think?) to eclipse Cisco's market cap at the time. reply bluerooibos 14 hours agoparentprevJesus. Perhaps showing my ignorance but I'm surprised by the amount of overlap these board members have across major companies. reply ensignavenger 11 hours agorootparentSome of these people are investors or represent investor groups. As such, the group they represent will own stakes in multiple companies, and often have board seats on many of the companies. Others will be an top executive at one company, and also serve on the board as an advisor of a few other, non-competing companies (usually non-competing). Those are probably the most common scenarios where some one would serve on multiple boards. reply poorlyknit 10 hours agorootparentDoes that change anything though? reply ensignavenger 8 hours agorootparentI am not sure what the concern is in the first place, so I am not sure if it changes anything or not. I was just adding my own perspective on the situation. reply refurb 5 hours agorootparentprevSure. If Vanguard has board seats it wouldn’t be odd for that person to sit on lots of boards of companies Vanguard invests in. reply tristor 14 hours agorootparentprevThey Rule is outdated, but the unfortunate truth is that there's been even more consolidation since it first went online. This is partly due to the consolidation of liquidity through institutional investors. It's one of the consequences of passive investment strategies being dominant among retail investors. reply bozhark 11 hours agorootparentprevCompanies, state dept., politicians, revolving door reply mdekkers 10 hours agorootparentprevIt’s a big club, and you ain’t in it! https://m.youtube.com/watch?v=Nyvxt1svxso reply balderdash 8 hours agoprevI find it ironic in an interesting way that on HN there is such a strong privacy bent generally, but on the topic of financial privacy it seems to be the reverse reply Staple_Diet 8 hours agoparentThere's a marked difference between an individual's right to privacy and that of a business. Shell companies are financial instruments designed to obfuscate ownership, usually for tax evasion purposes. Therefore increased transparency is, for the most part, in the public's interest. reply balderdash 7 hours agorootparentWhy is it in the public interest to know who owns a company? Sure for specific groups of people, transparency is in order (elected politicians etc.) but wouldn’t financial disclosure laws work better? Why isn’t it the general public’s prerogative to know that I own 5% of a shell company (LLC) set up specially for the purpose of pooling an investment in a new venture? shell companies are just companies, suppose the vast majority of which are formed for perfectly normal reasons, reply taway789aaa6 6 hours agorootparent\"Corporations are people too, my friend.\" If they can have \"free speech\" and are \"legal persons\" then the public _surely_ has an interest in knowing who is benefiting (the ultimate beneficial owner) from that corporation's actions. reply diego_sandoval 6 hours agorootparentprevIt's in the public interest only because of the particular way that taxes work. If sleeping with someone was a taxable event, would that mean that it's OK to spy on people's bedrooms? After all, it's in the public interest. In principle, I think that taxing only land ownership and pollution is the theoretically fair way to do taxation. And land ownership doesn't require obtaining any data that isn't necessarily known by the government in the first place. reply IG_Semmelweiss 7 hours agorootparentprevThe founding fathers published anonymously to obfuscate ownership of their penmanship. Would it be in the public interest that their identities were revealed and the lot be killed by the King ? reply AnarchismIsCool 5 hours agoparentprevIndividuals should have financial privacy, but corporations are not people (fight me). Trying to equate individual liberties with those corporations is a massive source of societal issues but shills keep advocating for it because it's very much in the corporations interests to have those rights, which when wielded by an entity with such vast resources, puts said entity on a very uneven playing field with any actual humans. As such, we need to level that playing field. reply refurb 5 hours agorootparentBut corporations are just groups of people. And by removing the privacy of corporations you’re just removing the privacy of people within it. reply AnarchismIsCool 4 hours agorootparentThey're superficially groups of people but the legal reality that makes them exist is the fact that they're legally separate from the people who comprise them. To put it differently, they're not groups of people, they're a distinct legal \"entity\" that can then be fractionally owned by people while shielding them from whatever it's doing. This gives it special powers, you can't arrest a corporation or send it to jail. It can act against the will of some of its fractional owners, it can own its own property, and it can legally participate in electoral politics, beyond how any or all of its owners are legally able to participate. reply refurb 3 hours agorootparentSure, but that doesn’t change the fact that eliminating privacy for LLCs is eliminating privacy for individuals. reply IG_Semmelweiss 7 hours agoparentprevits ironic, and sad reply boringg 7 hours agoparentprevI think the people commenting on this thread are a subsection of HN and probably not the same ones with strong libertarian viewpoints that people sometimes associate with HN. I would agree there is a weird disconnect here. reply patrakov 13 hours agoprevThe https://opencorporates.com/ site is also useful for finding companies registered using a stolen identity or a fake address that matches yours. In the past, I was a victim of some bad guys registering a shell company using my address in a not-yet-built house. I reported that to the tax officers back then. reply dsign 1 hour agoprevI see lots of people wanting to go after shell companies here, and with good reasons. But if you live in a country where the state takes away 65% of the money the employer sets apart to pay you, and then your means are precariously sufficient to live in a small one-room apartment and you struggle to sustain an elderly parent who doesn't receive any support whatsoever from that same state that takes your money away for \"social security\" ... Yeah, then you start looking for \"shells\". This is the financial equivalent of the cherished right of Americans to have guns. reply Mashimo 1 hour agoparentWhat country is that? reply beryilma 13 hours agoprevShell companies, in a sense, have been used to evade local laws and ordinances, even by semi-public institutions. IIRC, Harvard University, for example, used LLCs and law firms to buy land secretly in Brighton and Cambridge, MA against the policies of local governments. It would be good to know the real institutions behind such transactions... reply bozhark 11 hours agoparentIf they can get around it, it’s not against policies. Policies need to change reply globalnode 7 hours agoprevfinancial privacy is the way you get corruption - i dont care if you dont want anyone to know you spent a bazillion dollars on your yacht, too bad. companies especially dont deserve special consideration in this respect. as a community we could expose corruption and greed at the highest levels if financial privacy ceased to be a thing. edit: ofc this is what the whole article is about, ive calmed down and read it now :D... still, dont know why these things are allowed to go on reply faeriechangling 7 hours agoparentThe issue with financial information being more out in the open is it also strengthens the most powerful incumbents, most notably states. Credit scores are a good example of a financial privacy violation used to keep the poorest poorer. When the data leaks and people suffer waves of identity theft a slap on the wrist is issued and 5 free years of credit monitoring are handed out. After year 6, well it's the consumers responsibility to pay for that, not the financial data hoarders. reply danparsonson 7 hours agoparentprevPrivacy, like all tools, is neither good nor bad, but depends on the wielder. Same goes for messaging privacy and crime fighting for example. reply hackernewds 7 hours agoparentprevsoooo no crypto? reply globalnode 7 hours agorootparentcrypto currency? afaik thats not private but trackable. crypto for general privacy? sure, we have a right to communicate in private what i dont like is financial privacy reply travoc 6 hours agorootparentPut your money where your mouth is and publish all of your bank and credit card statements. reply globalnode 2 hours agorootparentbanks have all this info - its already published to 3rd parties via rewards schemes and tbh i wouldnt be surprised if the banks sell it as well. retailers resell it... none of this data is private. in effect i am publishing it already. but as you say, i would gladly have it all posted online for anybody to see if it meant financial transparency for all. edit: perhaps i wouldnt like my bank balance published... that may make some individuals targets. in thinking about it perhaps i would prefer financial \"transaction\" transparency. then we would have to guard against huge cash transactions tho somehow. its clearly a difficult topic but its also the source of a lot of the worlds corruption and misery. reply zrn900 6 hours agoprevNot only those - majority shareholders must always be mentioned when any company is being talked about for any reason. Shareholders should not be able to hide behind a smoke curtain while profiting from destructive corporations. Every company must be named along with its majority shareholders before its ceo. reply karol 2 hours agoprevDon't burn other people's shell companies - start your own. reply Projectiboga 11 hours agoprevThere is a new national law for all corps to have their beneficial owners disclosed to the Department of the Treasury by the end of 2024, and all new firms within 30 days. reply nostromo 10 hours agoparentThe law has been declared unconstitutional by a federal judge though, and will likely be overturned entirely on appeal. reply doctor_eval 10 hours agorootparentWhat part of the constitution? Not American, don’t know how to find out, but quite intrigued. reply aftbit 10 hours agorootparentFrom FinCen's website at https://www.fincen.gov/boi. More can be found about the case by searching the title given below. Red Alert Alert: Notice Regarding National Small Business United v. Yellen, No. 5:22-cv-01448 (N.D. Ala.) Updated March 11, 2024 On March 1, 2024, in the case of National Small Business United v. Yellen, No. 5:22-cv-01448 (N.D. Ala.), a federal district court in the Northern District of Alabama, Northeastern Division, entered a final declaratory judgment, concluding that the Corporate Transparency Act exceeds the Constitution’s limits on Congress’s power and enjoining the Department of the Treasury and FinCEN from enforcing the Corporate Transparency Act against the plaintiffs. The Justice Department, on behalf of the Department of the Treasury, filed a Notice of Appeal on March 11, 2024. While this litigation is ongoing, FinCEN will continue to implement the Corporate Transparency Act as required by Congress, while complying with the court’s order. Other than the particular individuals and entities subject to the court’s injunction, as specified below, reporting companies are still required to comply with the law and file beneficial ownership reports as provided in FinCEN’s regulations. FinCEN is complying with the court’s order and will continue to comply with the court’s order for as long as it remains in effect. As a result, the government is not currently enforcing the Corporate Transparency Act against the plaintiffs in that action: Isaac Winkles, reporting companies for which Isaac Winkles is the beneficial owner or applicant, the National Small Business Association, and members of the National Small Business Association (as of March 1, 2024). Those individuals and entities are not required to report beneficial ownership information to FinCEN at this time. Update [March 11, 2024]: This notice was updated on March 11, 2024, to reflect that a Notice of Appeal has been filed regarding this case. reply agensaequivocum 10 hours agorootparentprevThe constitution grants the federal government enumerated powers. Since it's not an enumerated power, it's reserved to the states and is therefore unconstitutional. reply aardvarkr 10 hours agorootparentIt falls under the the foreign affairs and national security powers, the Commerce Clause, and the Taxing Clause (plus the Necessary and Proper Clause in support). reply jandrese 10 hours agorootparentprevProbably the First Amendment using the Citizens United precedent. reply pugworthy 10 hours agoparentprevI'm skeptical. There are many laws on the books against illegal business practices. I doubt everyone looking to shield their corporate beneficial owners will go, \"Curses! If only it wasn't for this new law!\" reply saalweachter 10 hours agorootparentIt's like KYC laws. It's both preventative, and it also becomes another easy to prove charge against people circumventing it. reply kazinator 11 hours agoparentprevProbably backed by those who benefited from it, as another way to to screw young up-and-comers. reply readyman 10 hours agorootparentany anyone who understands how capitalism works would understand this will never change under capitalism reply syklep 10 hours agorootparentWhy call it capitalism as opposed to a free market? 'Capital' stands for money but the 'market' pressure is why these dark patterns emerge. reply kazinator 9 hours agorootparentBecause it's not a free market with those with deep pockets manipulate the rules that get enshrined into law. reply autoexec 31 minutes agorootparentWhich means there can never be a free market since there will always be people able to bribe and manipulate in order to get their way. It doesn't matter how much oversight and transparency you have, those things can be slowly chipped away overtime and replaced with laws that favor the ultra rich at the expense of everyone else. I'd love to be wrong about that, but I've never seen an example of a country where wealth had no power to influence law. reply readyman 8 hours agorootparentprevBecause capitalism is an entirely different concept. Capitalism is an economic system of relations that prioritizes the right to exercise one's existing capital to accumulate more of it, hence the word capitalism. So-called \"free markets\" is a utopian concept that is not a requirement of capitalism and of course has never existed in any honest sense of the idea. reply kazinator 8 hours agorootparentFree markets arguably exist in the plant and animal kingdoms. reply readyman 7 hours agorootparentSure, I can accept that because it is a concept fundamentally at odds with civilization. reply kQq9oHeAz6wLLS 10 hours agorootparentprevIt's way better under socialism, where there are no up and comers. reply readyman 8 hours agorootparentThis seems to presume that one's wealth is the only measure of individual value anyone cares about, which is just absurd. One can be wealthy beyond belief and still be an up-and-comer in many aspects of their life, and under socialism those aspects would almost certainly have more meaning. I would personally prefer wealth have less meaning and these other things have more, which makes socialism attractive to me. reply kazinator 5 hours agorootparentSocialism is thoroughly materialistic. The socialist state regards itself as owning the productive capacity of its individuals, and tends to the distribution of wealth. The pariah in a socialist society is he who has the capacity for work, but instead lives off the state. Or he who dodges taxes. Socialism is concerned with distribution of resources to satisfy human needs, and bring about an equitable society. But those needs are material needs, and the equality is material. It's not about redistributing spiritual or intellectual resources to make everyone equally spiritual or smart! It's purely about keeping everyone warm, dry, in good health and with a full stomach. The only non-materialistic aspect of socialism is the meta level: choosing equitable materialism over inequitable. The idea that all people deserve to be reasonably well off is a different form of materialism from \"I want me and my family to be well off, screw the rest\" that represents much of capitalism. Socialists regard themselves as supremely virtuous due to clinging to this idea of equality. It is almost religious. There are obvious links between equality in socialism and in Christianity. The proper socialist is like a disciple of Jesus who has renounced everything spiritual, and just remembers the stories about healing the sick, feeding the multitude, and \"render unto Caesar the things that are Caesar's\". reply spxneo 16 hours agoprevthis area is super murky with legitimate use cases for shell companies. im wondering why this is an area that hasn't been solved by eager engineers. seem like there is a large gap to fill for people needing turnkey IBCs but one problem might be using offshore data centres. I often see advertisements for incorporating offshore but there is no legitimate way to know which are authentic and which are just skimming on top of what you can do by yourself. It's quite fascinating enough that I need to hire a CFO or someone specifically familiar with IBCs reply snarf21 16 hours agoparentGenuinely curious: What are the legitimate use cases for shell companies that hide who owns them? reply amluto 16 hours agorootparentHiding one’s address is nice so one can avoid listing one’s address in the public record. And the services used to conceal one’s address are largely the same services that hide one’s name. Also, Delaware corporations are very popular for many legitimate reasons even for businesses with no personnel in Delaware. But you still need an agent for service of process in Delaware. I wish the states would allow designating the Secretary of State as the agent for service of process and paying a nominal fee for them to forward documents electronically. This would keep relevant information available to law enforcement and the courts, but it would avoid the need for paying mildly sketchy registered agents for their mildly sketchy services. It’s not clear to me that there’s any sort of bright line between shell companies and any other sort of corporation, anyway. reply sealeck 14 hours agorootparentYou can also rent a mailbox at a serviced office who will scan all the mail and email it to you instead of setting up an offshore shell company? reply amluto 13 hours agorootparentI think you have the dichotomy wrong. A registered agent is, among other things, a services mailbox. And a “shell” company, whatever that is, isn’t necessarily offshore. Many US companies, even very ordinary ones, incorporate in Delaware for various, mostly good, reasons. IIRC even YC strongly recommends this. Unless the company actually has an office in Delaware, it will use the services of a registered agent in Delaware to satisfy the requirements of Delaware. Then the company will register to do business in whatever state it’s in. Or it could break the law and not register. The only funny business here is that, at least traditionally, there is no requirement to inform Delaware of the beneficial ownership structure of the company. This seems to be changing — the US is pushing back against companies with anonymous ownership. The big question, to me at least, is why anyone expects bad actors to fill out fancy new forms correctly. reply refurb 4 hours agorootparentprevThat doesn’t protect you against title lookups. You can go online, find a plot and see who owns it. reply IncreasePosts 16 hours agorootparentprevPublic figure who doesn't want randos knowing where their house is. Non-public or public figure who doesn't want their name associated with certain business activities that they partake in. reply williamcotton 16 hours agorootparentBoth seem counter to the notion of a republic and the accountability needed for private property. From what I can gather most houses of public figures are already known. reply Terr_ 15 hours agorootparent> Both seem counter to the notion of a republic and the accountability needed for private property. Imagine that you own and run Acme Critical Publishing, which publishes exposés of crimes and ethical lapses of the sitting President. In speeches, he starts rambling about your company as an example of Horrible Very Bad People, and the next thing you know some supported of his casually looked up your home address online and now there are burning lower-case-t's on your lawn... I'd say the republic and accountability are both suffering in that scenario. It's one of those \"tools that can be used for for good or evil\" things, and simply prohibiting the tool isn't necessarily the best way to maximize the good while minimizing the evil. reply seabass-labrax 15 hours agorootparentIn your story, there are two failures; firstly, the President's failure to exhibit due care in the content of his speeches (an increased responsibility due to his lofty station), secondly, the lack of a police response to disperse ominous gatherings before they become violent. Neither of these failures have anything to do with the anonymity of company directors. The President could know you personally, and still leak your address. Similarly, there could be a mob vandalising your property even if you didn't run Acme Critical Publishing, because that kind of thing happens in riots sometimes. In some countries there is almost no anonymity on the public record, but this doesn't seem to negatively affect the level of violence in their societies compared to otherwise comparable locations. Therefore I would opine that 'enjoying safety' and 'enjoying privacy' are two very different and mostly orthogonal issues. reply arunabha 9 hours agorootparent> firstly, the President's failure to exhibit due care in the content of his speeches (an increased responsibility due to his lofty station) There is no such legal requirement, right? What's holding the president accountable? > the lack of a police response to disperse ominous gatherings before they become violent. It's really not possible for the police to do this. Regardless, GP's comment was about one or a small number of people showing up to cause trouble. In reality, the police can pretty much do nothing to prevent this. If they could, there really wouldn't be any robberies or murder. reply autoexec 18 minutes agorootparent> It's really not possible for the police to do this. You try burning t's on the front lawns of politicians and see how quickly the police do exactly that. You seem to think it's a perfect crime and you could never be caught or held accountable. That's an insane view, but you're welcome to test it for yourself and see what happens. I think you might be surprised. A lone person (or even a small group) could show up and cause trouble on anyone's property at any time. Why don't most people do it? Probably because police exist and there would be consequences. Should everyone have their addresses hidden from all people at all times? Why should a company have that protection and not you? What makes them so special? I think it's far better to understand that transparency is important to our democracy and our freedom and that means accepting a certain amount of risk sometimes. The fact that someone, somewhere, might one day find out where you live is a risk almost all of us face right now. There's no reason for a privileged class of people who want to influence our lives and government while also hiding themselves from the public and accountability. reply Terr_ 9 hours agorootparentprev> disperse ominous gatherings [...] a mob [...] riots Hold up, I didn't say anything about a riot, let alone a physical gathering. Just the contents of a speech where your hypothetical publishing company has become fingered as a target. It could very well be a video on a campaign-blog. > the level of violence in their societies The analogy isn't about violence, it's about kinds of damage to you (and to the republic) when the privacy is pierced, and violence just happens to be the most illustratively-obvious form of that. reply xtracto 11 hours agorootparentprevSomething like this just happened in Mexico: the president exposed the personal contact info (phone iirc) of some journalist that wrote a piece critical of his government: https://www.nytimes.com/2024/02/22/world/americas/mexico-pre... So, imagine if Trump exposed the contact info of people behind one of those groups showing anti-trump ads. His minions could use it as a signal to attack them. reply williamcotton 11 hours agorootparentOk, so we only allow the government the privilege of knowing who owns what… then we worry about the government abusing this information for political gain? reply IncreasePosts 15 hours agorootparentprevWhy is public knowledge of who owns what home required for accountability in a republic? A functioning government should have no problem enforcing whatever laws and codes exist regardless of whether the owner is directly known or shielded through some LLC. reply singleshot_ 10 hours agorootparentBecause our system of property, inherited from English common law, has a concept of a beneficial purchaser for value without notice. If someone gives you money for property and they are not on notice that there are conflicting claims, their claim is valid. Such a system requires a way to establish constructive notice. A taker for value should have known that the land he was buying belonged to another if its ownership was properly recorded at the time of the purported sale. Mileage may vary in different jurisdictions. Recording office May vary according to property type under state law; e.g., DMV might record motor vehicle titles while the county recorder in the county of the property probably records real estate. reply williamcotton 12 hours agorootparentprevSo we should rely on the government being the only party with access to what you would make privileged information? A key aspect of public accountability is tautologically dependent on public information. We are supposed to keep our own elected officials as well as those granted the right to private property in check or else fraud and corruption would reign supreme. We have been operating under these conditions since the advent of the democratic republic and to our good fortune. reply codexb 11 hours agorootparentprevWhat about a \"republic\" (ie. a nation governed by laws) requires public registration of assets? reply vundercind 10 hours agorootparentIt also doesn’t require the government granting the privilege of incorporation. reply solidsnack9000 7 hours agorootparentNo, it doesn't; but it turns out that you end up with a much poorer society without it. The king did not come up with incorporation merely to amuse the incorporators; it was to tax them and take loans from them, and that's because there was enormous economic activity they could undertake once there was a liability shield. reply williamcotton 11 hours agorootparentprevA monarchy is also a nation governed by laws. I’ve answered your question in another response. reply dheera 16 hours agorootparentprevOr even non-public figure not wanting randos knowing where their house is. I'm not a public figure and I don't want where I sleep posted all over the internet. I don't have the budget for security if some Twitter fool decides to give me a death threat because of a comment. reply excitom 15 hours agorootparentI remember when you could find someone's home address in the white pages of the phone book. reply klyrs 14 hours agorootparentBack in the day, I looked up a minor celebrity in my hometown and just... knocked on his door one afternoon and had a nice chat. Can't wait to explain this to my grandchildren. reply avmich 15 hours agorootparentprevI guess sending threats then required more of sending a physical mail, which could be used with better effect in dealing with threats? reply bongodongobob 9 hours agorootparentprevAnd you could opt out if you wanted. reply deadbabe 16 hours agorootparentprevIf someone really wants to know where you live, they will find you. If the information in your bio is actually yours, there’s already plenty to work with. :) reply rrr_oh_man 16 hours agorootparentA lock is not to keep out bad guys who want to get in, but to keep good guys honest. Same with privacy. Why not install a camera on your toilet while we’re at it? Or do you have something to hide? reply csa 14 hours agorootparent> A lock is not to keep out bad guys who want to get in This is definitely not true. Locks are to keep randos from invading your space — drunk, drugged, and/or mentally unhealthy people end up in the oddest places sometimes. My buddy had a guy high on meth open and climb through a laundry room window and start wandering through his house. I’ve twice had drunk people knock on my door loudly and try to let themselves in (different cities) and swear that they were at the right address. These were all in decent/nice areas (some not so nice areas a mile or so away, but still…). Same with privacy corps… You don’t want some rando who is irrationally angry at your business or at you to be able to find you easily. You don’t have to be as famous as Barbra Streisand in order to be a person of interest to mentally unhealthy randos. Just my 2 cents… reply rrr_oh_man 14 hours agorootparentTotally agree, and I don’t feel this invalidated my point reply deadbabe 15 hours agorootparentprevI’ll do everything I can to keep sickos the hell away from me. Too many stalkers or people DMing inappropriate shit. But I never forget that in the end I’m never truly safe from someone trying to do serious harm. reply codexb 11 hours agorootparentprevIt's not always nefarious. If I'm an investor, I might make an agreement with an LLC that I'll invest $1000 in their business, but I get 99% of their income and ownership control, or even 100%. There's no requirement that we register or publicly disclose our private agreement. The real question is, what legitimate reason do you have to know who owns a particular thing or asset. If you see a car parked somewhere, do you have a legal right to know who owns it? What about a lemonade stand? What legitimate legal reason do you have to compel people to register their assets? reply BLKNSLVR 10 hours agorootparentThe government responsible for the jurisdiction in which your private agreement occurs should have a register of this information. It doesn't need to be available to the public at large, but should be available to those responsible for the jurisdiction that could be affected by that agreement. Re: Car parked somewhere. If it has a numberplate, then there's a government agency that can track it's ownership (pending whether it's been registered to a company that then has some opaque international ownership structure). Re: Lemonade stand. Minimum level of asset value for registration? reply codexb 6 hours agorootparentBut why does the government need this information, for everyone, and everything? Again, what is the legitimate reason? I understand tax liability, sure, but so long as someone is responsible for paying the taxes on a thing, why is it important who has control of the thing. reply Veserv 4 hours agorootparentBecause it is important to know who has control of the thing if they control the thing to park illegally, scam people, or dump toxic waste into the river. It does not need to be public, but the responsible party should be identifiable and accountable. To the extent that harm is minimal or can be efficiently mitigated if no responsible party can be identified, it is probably socially acceptable to sacrifice a little bit of accountability for convenience and reduced friction. But at scale it is ridiculous to allow the intentional and calculated ablation of responsibility against legitimate legal grievances. reply BLKNSLVR 6 hours agorootparentprevGut feel response: Because of the power differential between an employee who's given the responsibility of X versus a director/owner who threatened to fire said employee if they don't do legally questionable / flat out illegal thing in regards to X. 99.99999% of the time it's OK. It's those instances when it's not OK that it becomes societally important to demonstrate consequences for illegality. I think it'd become a problem that governments around the world would prioritise if every individual conducted their private affairs through a shell company structure. I said elsewhere that doing so is going to be a potential retirement project of mine. reply vundercind 10 hours agorootparentprevLLC’s a privilege granted by the public. It’s a creation of the voters’ government. It absofuckinglutely should come with a right for us to know who’s benefiting from our permitting such an arrangement to exist. Don’t like it, don’t incorporate. reply jiriknesl 4 hours agorootparentYour post looks a lot like: “Individuals have no rights except those given by the government.” Would you use the same logic for everything else? reply solidsnack9000 7 hours agorootparentprevIf they didn't incorporate, we'd all be much poorer. reply matt_s 16 hours agorootparentprevOutside of privacy, I could imagine it might be beneficial to structure a set of companies as a primary company and a bunch of shell companies to separate accounting and legal matters. This way if a sub-company is in a dangerous industry or has large liabilities in its normal business that you would want the primary company to be insulated from that. IANAL so I don't know if this is a use case, there are probably ways to do this w/o privacy. reply amluto 14 hours agorootparentAs an example, it’s extremely common for financial companies of all sorts to create a company to hold an asset or group of assets. This can be done for all manner of reasons, including difference in beneficial ownership. A VC fund, PE fund, etc will often have a “manager” (the possibly big-name company that operates it) and a bunch of limited partners for that specific fund. If you own a piece of a BlackRock fund, for example, you don’t own BlackRock itself. And the corporate structure reflects this. reply toofy 14 hours agorootparentprevThis is exactly why we’ve been unable to hold anyone responsible for misuse of their powers in so many instances. reply abound 16 hours agorootparentprevI've seen shell companies used during acquisitions to make it less obvious that FAANG is buying a company. reply inetknght 16 hours agorootparentI'd argue that's not a legitimate use for a shell company. Hiding the entities behind the company should not be legal. reply abound 15 hours agorootparentFor clarity, in the instance I witnessed, the company being acquired was 100% aware that it was FAANG acquiring them, it's just that all the legal paperwork had the acquiree being consumed by a random shell company that was a wholly owned subsidiary of the FAANG company. I think the goal is just not to leak information ahead of time, and perhaps to insulate the FAANG company in case anything goes sideways mid-deal. reply jandrewrogers 16 hours agorootparentprevCommon case is privacy and safety for public figures. Some places have laws that specifically allow some public officials to have their otherwise public records sealed for privacy and safety reasons. Aggressive harrassment by activists, conspiracy theorists, and other malcontents is definitely a thing when you become high profile. A related case is investors in or owners of a category of otherwise legal business whose relatives are targeted by nasty people for that fact alone. This includes businesses like coal and defense. Shell companies help shield themselves and their extended family from association with a business that attracts undue drama. reply LastTrain 15 hours agoparentprevThe real question is - do the legitimate reasons outweigh the negatives? The stated reasons are pretty flimsy - are they worth not knowing who owns 1/4 of Manhattan? reply spxneo 13 hours agorootparentThere's all sorts of legitimate ways for instance there are traders who are watching companies doing mergers and acquisitions and you want to make it not so obvious. There's family offices that do not want to advertise but still need to manage their wealth. Holding companies with investment portfolios in non-liquid assets, isolate and manage risks etc. People in the comments are conflating layering with simply legit use cases like using foreign jurisdiction and shell companies for corporate/investment strategies. reply wpietri 8 hours agorootparentI understand why rich people want to keep secrets from the general public. I just don't think \"rich people want it\" is a justification for much of anything. reply CPLX 16 hours agoparentprevA common case that doesn’t seem illegal or immoral to me at least is people assembling groups of related properties in order to combine them. So like if you want to buy all the buildings on a block, or something like that, and want to pay fair market value for all instead of being gouged for the last few. I suppose that’s arguable either way on policy grounds but it seems reasonable to me. reply kwhitefoot 15 hours agorootparentOne man's gouging is just another's operation of a genuinely free and transparent market. What is a fair market price? How does it differ from the market price, and who gets to make the distinction? reply CPLX 15 hours agorootparentI mean we are used to having at least some privacy when engaged in business negotiations. Do you forward your pay stubs from the prior job to a new employer when engaged in salary negotiations? reply wombatpm 5 hours agorootparentNo need TheWorkNumber and ADP take care of sharing all that information. reply yieldcrv 15 hours agoparentprevyou dont need to use an intermediary to incorporate offshore its the same process as incorporating in any US state, where you need an agent of service or registered agent if you dont live there pick the country just like you would pick a state and some countries have states too, the US is actually one of the weirder countries as you cant incorporate at the national level whereas in st kitts & nevis - another federation - you can do both, a st kitts & nevis entity has one set of transparency and regulations, and nevis has a different set of regulations and generally seen as more favorable its not really a taboo topic like ICIJ and some socioeconomic classes of people make it out to be. its a catalogue with offerings domestically and worldwide reply singleshot_ 10 hours agorootparentThe Corporation for Public Broadcasting might be a counter example to your “states-only” comment about corporate registration — except that congress incorporated it; it did not incorporate itself. There are also federally chartered banks, although that’s not exactly the same thing. (Now I’m starting to wonder why my BizOrgs casebook was so short…) reply yieldcrv 10 hours agorootparentyes the only incorporation at the federal level is by Congress at the moment even federal agencies register new entities with their states of choice I’m always curious what a federal incorporation statute would look like, I bet it would suck as the loudest states’ representatives and delegates would want transparency reply spxneo 13 hours agorootparentprevif the article was about Tor, comments would be overwhelming supportive of privacy. I have a hunch about why they would be against corporate privacy and it is probably tied to socioeconomic reasons and the media they are exposed to (aka reddit) reply yieldcrv 11 hours agorootparentthe bias probably does stem from that, tor is egalitarian everyone can use it, while people perceive business vehicles as expensive and advantageous only for the wealthy reply p1necone 13 hours agoprevCan someone do this with userbenchmark? I'm dying to know what unhinged maniac runs that site. reply Stagnant 11 hours agoparentThere is no mention of any company name on userbenchmark's site so there is nothing to search. I guess not selling anything and not accepting donations helps to keep the owner private. reply indus 9 hours agoprevIsn’t all of the data available to the government especially in the US via federal banking system? Every dollar transaction in the world is reported back to banks in Wall St. reply nick7376182 17 hours agoprevI've heard that you can create two LLCs which own each other, and be completely anonymous. This should be possible for regular people to do at a small nominal cost. Not sure if it works in practice! reply LoganDark 12 hours agoparentHow exactly would you pull that off without having to create an LLC owned by yourself first? This sounds like just something you heard once, I would love to know more if it's actually a thing. reply hackable_sand 11 hours agorootparentAwhile ago I did some lay research to see if I could form a Corp that owns itself. Iirc the legislation actually accounts for recursion. I do not remember if they require the owners to be human though. Either way I was doubly disappointed... reply BLKNSLVR 10 hours agorootparentSounds like something in Accelerando. reply mihaitodor 11 hours agoprevThis podcast interview with Brooke Harrington gave me some good insights into how people manage offshore wealth: https://www.preposterousuniverse.com/podcast/2023/05/22/237-... reply ryandrake 17 hours agoprevI don't even know why shell companies and companies with secret ownership are even allowed. Well, I mean we all know the real reason: because it benefits rich people and they make the rules. But, what would a politician disingenuously cite as the public interest aligned justification for having these entities? Are there legitimate non-nefarious uses? reply tensor 17 hours agoparentYes, privacy. Corporations are often used for trusts and investments. I wouldn't call those nefarious at all, though yes they are mostly only useful for more wealthy individuals. These structures all still pay taxes and are 100% legal. Arguments about tax law being good or bad are really a separate issue, and those should be addressed directly by updating the law, not erasing privacy for wealthy. As much as the wealthy rightly get shit on for various things, they are still entitled to the same rights as everyone else. reply adra 16 hours agorootparentThe paying tax and 100% legal is only tested when brought to court, which seems that be chronically underfunded conveniently and I laugh loudly at the belief that 100% of all corporations are in legal compliance. You said that the rich are entitled to the freedoms as anyone else but shrugs off that only the rich have the effective means to make use of these instruments. They are quite capable of being completely invisible to public scrutiny by holding all their assets as an individual. They choose to take steps to leverage opaque often intentionally complicated corporate layering schemes to minimize risk, skirt (legally or not) tax, or to layer the sources of bad money. Whatever the reason for engaging in these games, I believe they are no longer \"living life like every citizen deserves privacy\" (note your comment spoke of total privacy from oversight which no citizen pretty much anywhere actually has). I'm at least happy that my home of Canada is starting to chip away the corporate veil. reply solidsnack9000 7 hours agorootparentEveryone who owns shares in a retirement fund benefits from the privacy afforded by these instruments. If every beneficial owner of every corporation had to be publicly registered, that'd be an enormous proportion of the regular public! Think about what it would mean that anyone could, at any time, look up all the investing choices of anyone in the country. reply tensor 15 hours agorootparentprevYour home in Canada just recently demanded blind trusts all report their members personal information to the government. Far from engaging only the rich, that demand hit millions of every day citizens with shared bank accounts with their parents or kids. Those are setup for a variety of reasons, one of which is to circumvent probate tax! Yep, by everyday non-rich people! However, note that the CRAs information collection did not make your personal bank account information public. Would you prefer that it had? I used the example of a blind trust here because \"non-blind\" trusts are one of the common uses of non-commercial corporate entities. Also, I never once said \"total privacy from oversight\", all these corporate entities already supply financial and ownership information to the government, whose job it is to hold them to account. This is not the same and making information public so a rabid mob of people can enact vigilante justice, or whatever people like you hope comes out of it. reply no_wizard 12 hours agorootparent>Your home in Canada just recently demanded blind trusts all report their members personal information to the government. Far from engaging only the rich, that demand hit millions of every day citizens with shared bank accounts with their parents or kids. Those are setup for a variety of reasons, one of which is to circumvent probate tax! Yep, by everyday non-rich people! How does this hurt anyone though? That is not clear at all. The thing you're upset about here is that blind trust members information is reported to the government, but with no clear statement how this hurts anyone, wealthy or not. Chances are, the information turned over the government has anyway. If this is a legal mechanism to circumvent probate taxes, then its not a problem. If not, well, even regular people should pay their taxes, no? reply Sleepful 11 hours agorootparent> even regular people should pay their taxes, no? Taxes aren't an ethical or moral topic, they are a legal topic. If you can avoid a tax through some legal structure, you are within your rights to do so and you can't judge this as some sort of shady business. Taxes are mostly used to create incentives and collect money, if people are allowed a legal structure to avoid a probate tax, then that might be an incentive on purpose. Just because you did something to pay less taxes does not mean that you are some bad actor exploiting a loophole. For example, taxes are only paid on profit, so companies are incentivized to spend their money and pay less in taxes. No one sees this as a legal loophole that needs to be fixed, it is very much intentional. Also you are making the false dichotomy here of \"regular people\" as something different from \"somewhat versed in financial entities people\". That's weird. reply Mashimo 1 hour agorootparent> If you can avoid a tax through some legal structure, you are within your rights to do so and you can't judge this as some sort of shady business. Sure you can. Legal tax avoidance gets judged all the time. See the Jimmy Carr tax \"scandal\" that was legal, and yet he received public pushback. reply IG_Semmelweiss 6 hours agorootparentprev>>> How does this hurt anyone though? That is not clear at all. Allow me to make it clear. https://www.smithsonianmag.com/history/1938-nazi-law-forced-... reply diordiderot 13 hours agorootparentprevPeople should pay their probate tax reply solidsnack9000 7 hours agorootparentSay a couple lives in a house. They have a son, who lives with them. The wife passes away. 1. Should her husband pay tax on her share of the house when it passes to him? Assuming these are not wealthy people, this may force the sale of the house. 2. Should the house go through probate? Should everyone move out while that happens? The old man wants to leave the house to his son. 3. Should the son pay tax on the house when his father passes away? 4. Should he move out while the house goes through probate? It is because of cases like these that we have ways to avoid going through probate. reply Sleepful 11 hours agorootparentprevSounds like something said by a tax collector :) reply jen20 15 hours agorootparentprev> 100% legal is only tested when brought to court A tangent, but this is in and of itself completely nuts. One should be able to read the text of the law, as written, and understand what you are and are not permitted to do - end of story. There should be no need to look at precedents. Courts only recourse if a law is unclear should be to send it back to an elected legislature for refinement. reply lostlogin 16 hours agorootparentprev> they are still entitled to the same rights as everyone else. Has there ever been a time when the wealthy had less rights than the poor? The speeding ticket fine which is charged as a percentage of ones wage comes to mind. https://www.theguardian.com/world/2023/jun/06/finnish-busine... reply vlovich123 16 hours agorootparentThat’s not really less rights though. That’s like saying the rich have fewer rights under progressive taxation. And no, generally the more rich and/or powerful you are the more rights society provides you. Even countries which strive for more equality simply try to shore up the most egregious instances but there’s always a difference. That’s because not all rights matter equally to everyone. For example, rich and poor both don’t have the right to sleep on a park bench but in practice that right is only particularly relevant to one party. reply sdeframond 16 hours agorootparentprevDo you mean that fining more for richer people is unfair? That's an interesting view. One could argue that a flat fine allows rich people to break the law more, because they can afford it. Which IMHO seems pretty unfair. reply lostlogin 15 hours agorootparentI agree with it and think there should be more penalties like it. reply criddell 16 hours agorootparentprevI'm still not seeing it. What are some specific benefits for hiding the benefitting owner of a company? How would society be worse off if the person or people behind a company could always be known? reply arcastroe 15 hours agorootparentIf you recently won the lottery. You might fear for your safety should your name and address become public knowledge. Some states allow you to claim anonymously, while others don't. For those that don't, you may be able to claim under an LLC, with your name and address \"hidden\". Edge case, but I think it's legitimate. reply IG_Semmelweiss 6 hours agorootparentprevHere you go https://www.smithsonianmag.com/history/1938-nazi-law-forced-... reply h1fra 16 hours agorootparentprevIf privacy was the only reason they would allow any company in any country to be anonymous and identifiable when requested with legitimate reasons (like Domain Names) and accessible to journalists. reply BadHumans 16 hours agorootparentWhy would it be accessible to journalist? Being a journalist doesn't suddenly make you ethical and responsible. reply struant 3 hours agorootparentprevIt is trivial for a wealthy person to be treated like everyone else. They just have to ditch their money. Unless they want do that, they can endure extensive scrutiny. reply BLKNSLVR 9 hours agorootparentprev> These structures all still pay taxes and are 100% legal Any offshoring calls that into question. > they are still entitled to the same rights as everyone else No, they're entitled to more because they can afford the services of those who can setup shell company structures to hide their wealth and their identities. reply sealeck 14 hours agorootparentprev> As much as the wealthy rightly get shit on for various things, they are still entitled to the same rights as everyone else. We're not talking about individual assets though here – we're talking about the distribution of resources in our society and the people who own these resources have power over lots of other people. Essentially - have US$1bn is structurally different to owning US$1 million. reply maxerickson 11 hours agorootparentprevI think they are proposing that rights would change for everyone, not just for the wealthy? Also, \"It's 100 legal\" is a pretty hilarious rejoinder to someone discussing changing what is legal. reply wredue 16 hours agorootparentprevCorporations are not people. People deserve privacy. Corporations do not necessarily also deserve privacy. Who runs a corporation 100% should not be a secret, ever. reply Aloisius 11 hours agorootparentIf people deserve privacy and corporations are run by people, then logically the people who run corporations deserve privacy. I'd argue instead that the public's need to know sometimes outweighs an individual's right to privacy. reply wredue 8 hours agorootparentNo. You do deserve privacy, however, you cannot expect it in all circumstances. You cannot expect privacy when walking down a sidewalk, you cannot expect to own a company without anyone knowing that. reply IG_Semmelweiss 7 hours agorootparentPrivacy is not deserved. Privacy is a right granted in the US constitution. reply janalsncm 3 hours agorootparentThere is no explicit right to privacy in the Constitution. People have inferred that right from things like the third and fourth amendments. But in any case, even if privacy was broadly recognized and protected (it isn’t, otherwise the government wouldn’t be allowed to buy your information from data brokers) that still leaves open the question at hand. reply hobs 16 hours agorootparentprevIt is illegal for both the rich and the poor man to steal a loaf of bread. The idea that the wealthy could ever have \"less rights\" than the hoi polloi is an absolutely hilarious thought. If any billionaire wants all the \"extra\" rights of being a normal citizen I'd be glad to swap anytime. reply janalsncm 3 hours agorootparentA more prescient example might be the fact that both everyday folks and billionaires are free to say whatever they want on social media in their real name. However only one of them needs to worry that medical bills will bankrupt them if they say the wrong thing. reply mkmk 17 hours agoparentprevI'm not very familiar with shell companies, so I'm curious – why isn't this just a matter of privacy for those involved? I'm generally pretty understanding of pro-privacy-oriented behavior. reply truckerbill 16 hours agorootparentIt's about the usage. No single dogma ever makes sense without context. These are abused by many to hide taxable wealth. It's in the public interest to understand what very powerful people are doing , because the public are usually getting the short end of the stick- this should override any ideology regarding 'privacy', 'freedom' and other fuzzy words (not that some interpretation of these things isn't also important) reply Veserv 16 hours agorootparentprevNot all are bad, but they are frequently abused to not only protect privacy, but to protect against legitimate legal need. It is like hiring a contractor on your house who gives you a false name so they can skip out of town when you figure out they cheated you. There is no legitimate purpose for that degree of privacy. They wanted privacy for the express purpose of providing you no legal recourse. Pseudonymity is perfectly valid, but there needs to be efficient, effective, and crystal clear means to pierce it and find the actual humans making the decisions with intent when there is a legitimate and well-supported legal need. reply ensignavenger 16 hours agorootparentCan you cite any research on just how frequent you mean by \"frequently\". It certainly happens sometimes, but I have found no evidence that is is the norm. reply Veserv 15 hours agorootparentWhy does that matter? I said that pseudonymity is perfectly fine, there just needs to be a straightforward and effective means of piercing it when there are legitimate legal purposes. Are you arguing that because it is not a majority of cases that they should be impervious even if there are legitimate legal purposes? There are enough high profile cases of shell corporations being used for unsavory and explicitly illegal behavior, to the extent that it is literally a meme, such as the crimes unveiled in the Panama Papers [1] that simple and robust mechanisms to prevent abuse are warranted even if abuse is uncommon. It is not like being able to pierce the privacy with a court order is even some new mechanism, you can already do that, it is just expensive, time-consuming, and difficult if they fight. [1] https://en.wikipedia.org/wiki/Panama_Papers reply ensignavenger 14 hours agorootparentThe cost of providing such mechanisms and properly securing them against abuse have to be weighed against the benefit. In order to know the benefit, we need to know the size of the problem we are seeking to solve, and whether or not the proposed system would actually solve the problem. So yes, knowing the size of the problem matters a lot, and is a good first step to running a cost benefit analysis. reply Veserv 12 hours agorootparentExcellent, since you wish to run a cost-benefit analysis you can first start by citing the research you used to conclude that the abuses of the current system are small. That would give us a baseline for analysis. It is then easy for you to take the first step in the cost-benefit analysis you wish to do by estimating the costs. The proposal is that a court order demanding disclosure, which are already routinely issued, can not be stalled indefinitely through the application of lawyers. So, all you need to do is identify the balance of cases where disclosure is fought and then see how often the disclosure results in illegitimate harm to the disclosed party versus how often it results in the discovery of legitimate harms caused by the disclosed party. The disclosures in the Panama Papers alone resulted in 1.2 billion dollars of recovered taxes [1]. So you can compare the estimated costs against the benefits of preventing a singular incident for now. If you can present credible evidence that the harms of requiring disclosure on legitimate court orders is in excess of that, then a broader analysis of the problem size if warranted. [1] https://www.icij.org/investigations/panama-papers/panama-pap... reply speff 11 hours agorootparentThey didn't volunteer to do your homework. You were originally asked for a citation on > frequently abused to not only protect privacy, but to protect against legitimate legal need Shell corporations are used for unsavory purposes being a meme is not proof of this as was implied by your other comment. Also 1.2B in taxes is pocket change so it doesn't really help your point. reply itopaloglu83 17 hours agorootparentprevI don’t know any other case where someone can own an entity like a house, car, estate, etc. where they get to hide who benefits from it. reply toomuchtodo 16 hours agorootparentYou can hide ownership in real estate using land trusts in states that support them. You can fund an LLC with a vehicle to hide ownership of the vehicle by VIN or plate search. In both cases, startup costs are a few hundred dollars, and ~$100-200/year upkeep per asset. Trusts in general are simply good estate planning compared to probate costs, at least on the topic of real estate holdings and titling. reply ensignavenger 16 hours agorootparentNew Mexico has fairly decent LLC privacy and you don't have to pay an annual fee to upkeep your LLC at all. Just need to have a registered agent, which can be had for under 100/year. reply toomuchtodo 16 hours agorootparentAppreciate the info! reply itopaloglu83 15 hours agorootparentprevCould you elaborate on reasons to do so? Using LLC to limit liability makes sense, but I’m not familiar with hiding identity behind a corporation. reply toomuchtodo 14 hours agorootparentPrivacy. My use case is obfuscating my life from data brokers. Law enforcement and the tax folks still know where to find me. reply itopaloglu83 11 hours agorootparentPrivacy makes sense. Thank you. reply mcguire 16 hours agorootparentprevThose are essentially shell companies. reply arrosenberg 17 hours agorootparentprevOligarchs using transnational legal arbitrage to avoid paying taxes isn't a matter of privacy. reply tensor 17 hours agorootparentNor is it due to the existence of shell companies. Rather it's due to poor enforcement and loopholes in tax law. You also don't need to make any of the information public to enforce it anymore than it's required to make your personal earning information public to enforce you paying taxes. reply arrosenberg 16 hours agorootparentIt's hard to take that position seriously. The people using the loopholes are the ones that pushed to put them in the law and degraded enforcement budgets. One hand washes the other and it takes place in the dark. Let's put a little sunshine on it and see what happens. We might find some very interesting networks exist... reply digging 16 hours agorootparentprevI'm quite big on privacy rights for indibviduals but I also feel that when corporate wealth (thus, influence on culture and politics) reaches a certain scale, radical levels of transparency become very important for the health of our society. For an extreme example, the public deserved (and still does) to know who were the individuals at Shell devising a decades-long misinformation campaign about climate change -- and that the bullshit they were hearing about climate change being fake was driven by nothing but greed. To address inevitable replies: No, I don't know exactly where the line is (or lines are - it should probably be a tiered system), and I recognize defining those lines is itself a position of immense power. Those are solvable problems though and don't make the idea bad. (It could still be a bad idea, but for other reasons.) reply ensignavenger 16 hours agorootparentprevIt is very much a matter of privacy for most folks doing it. And you don't have to particularly be rich to benefit from it. reply micromacrofoot 16 hours agorootparentprevbecause it's a lot easier to have privacy when you're rich the default for normal people is for all of this data to be public unless you can either navigate bureaucracy (costs time and money) or pay someone to do it for you (costs money) reply tetromino_ 15 hours agoparentprevAs a working class individual, you can get excellent financial privacy. You can stick your $10k in savings in cash in a glass jar behind your bed, and neither the government nor big banks nor investigative journalists get to know how much is in thar jar, and they have no say what you spend that money on and when. A more wealthy individual has no such privacy. Their wealth is not truly wealth and not truly theirs, it is fundamentally a trust-us IOU from a bank or a stockbroker which is shared with all kinds of parties, a publicly visible number on the screen which at any second could turn zero or negative on the orders of a corrupt official or due to a buggy algorithm or mistyped name on a sanctions list. The more wealthy individual yearns for a glass jar - but no jar is big enough to hold the sums the more wealthy individual operates with. Hence, shell companies. reply jhp123 14 hours agorootparentThe largest glass bottle seems to be 1700L. $1 million is only about 11 liters of $100s. So you could store about $100 million in a glass jar. reply janalsncm 3 hours agorootparentAt 5% inflation $100M loses almost $14000 of purchasing power per day. reply JumpCrisscross 11 hours agoparentprev> Are there legitimate non-nefarious uses? Have you formed an entity? In Delaware it’s file and go. You don’t need a lawyer nor to fill out a bunch of paperwork. That’s efficient. Where we demand disclosure is when that entity touches money through the banking system. reply CPLX 16 hours agoparentprevNot every corporation is some wealthy shadow network. The owners of an abortion clinic, or a store that sells fur coats, or a therapy practice for the criminally mentally ill, all might have good reasons why they’d like additional privacy. reply atomicfiredoll 14 hours agorootparentIt could also be a programmer who is within their legal right to start their own company, but who doesn't want their current employer harassing or singling them out for it. Further common situations involve trying to keep details out of the public record because they can be abused by bad actors; ones who may be looking to spam you, engage in a frivolous lawsuit, or personally harass you/your family. At least these are some of the scenarios mentioned by companies that do asset protection. Edit: In regards to harassment, think about the abuse retail, fast food, or other customer-facing employees endure for perceived slights. It feels easy to understand why average small business owners would want privacy and to keep things in legal channels. Personally, I think the government knowing who's in charge (Corporate Transparency Act) is a good halfway point, but it's not unrealistic to be concerned about leaks or abuse with that system. reply ebiester 17 hours agoparentprevGame theory: Any country can do it, and the company in which the corporation is registered is not going to suffer. So what is their incentive not to allow it? reply dartos 17 hours agorootparentIdeally it should be votes, but that doesn’t really work out like you’d want :( reply 165 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Investigative journalists utilize tools from networks like ICIJ and OCCRP to reveal the connections of shell companies to their secret owners through techniques like tracking bank transfers and utilizing resources like OpenCorporates.",
      "Formation agents are crucial in creating shell companies for wealthy clients by providing services like bank accounts and director names for a fee, while journalists uncover hidden assets and potential criminal links using databases such as ICIJ Offshore Leaks.",
      "They delve into investigating money laundering, organized crime, and corruption, particularly in countries like Russia and Belarus, by following the money trail and employing various reporting tools and tips."
    ],
    "commentSummary": [
      "The gijn.org discussion delves into the challenges and benefits of using shell companies for financial privacy, tax avoidance, and legal protection.",
      "Participants debate topics such as ownership transparency, tax policies' impact, and ethical implications of complex corporate structures.",
      "Key points include shell companies' roles in acquisitions, legal compliance, tax strategies, wealth protection, and the impact of government oversight and societal inequalities in financial affairs."
    ],
    "points": 954,
    "commentCount": 425,
    "retryCount": 0,
    "time": 1712161332
  },
  {
    "id": 39923404,
    "title": "Local Perplexity Clone Running on Gaming GPU Shows Impressive Performance",
    "originLink": "https://github.com/nilsherzig/LLocalSearch",
    "originBody": "The video demo runs a 7b Model on a normal gaming GPU. I think it already works quite well (accounting for the limited hardware power). :)",
    "commentLink": "https://news.ycombinator.com/item?id=39923404",
    "commentBody": "I've built a locally running Perplexity clone (github.com/nilsherzig)449 points by nilsherzig 12 hours agohidepastfavorite85 comments The video demo runs a 7b Model on a normal gaming GPU. I think it already works quite well (accounting for the limited hardware power). :) BrutalCoding 15 minutes agoThat’s a great project you pulled off. From the time I starred it (10-12h ago I think), and upon re-checking this post, you gained 500+ stars lol. Visualized in a chart with star-history: https://star-history.com/#nilsherzig/LLocalSearch reply nilsherzig 11 hours agoprevHappy to answer any questions and open for suggestions :) It's basically a LLMs with access to a search engine and the ability to query a vector db. The top n results from each search query (initialized by the LLM) will be scraped, split into little chunks and saved to the vector db. The LLM can then query this vector db to get the relevant chunks. This obviously isn't as comprehensive as having a 128k context LLM just summarize everything, but at least on local hardware it's a lot faster and way more resource friendly. The demo on GitHub runs on a normal consumer GPU (amd rx 6700xt) with 12gb vRAM. reply keefle 2 hours agoparentWonderful work! is it possible to make it only use a subset of the web? (Only sites that I trust and think are relevant to producing an accurate answer), and are there ways to possibly make it work offline on pre installed websites? (wikipedia, some other wikis and possibly news sites that are archived locally), and how about other forms of documents? (books and research papers as pdfs) reply nemoniac 2 hours agorootparentLlocalsearch uses searxng which has a feature to blacklist/whitelist sites for various purposes. reply nilsherzig 42 minutes agorootparentalso a great idea to expose this to the frontend. thanks :) reply kidintech 2 hours agorootparentprevSeconded. I tried to do this many years ago for my dissertation and failed, but this would be a dream of mine. reply robertlagrant 2 hours agorootparentWould it not be possible to create a search engine that only crawls certain sites? reply kidintech 2 hours agorootparentI was most interested in the offline aspect of it, which I wouldn't know where to even start with if I were to fork. How do you parse and efficiently store large, unstructured information for arbitrary, unstructured queries? reply stavros 48 minutes agorootparentYou put it in a search server, like ElasticSearch or Meili. reply nilsherzig 43 minutes agorootparentprevuhhhh both ideas are great, would you like to turn them into github issues? i will definitely look into both of them :) reply FezzikTheGiant 4 hours agoparentprevIf you're open to it, it would be great if you could make a post explaining how you built this. Even if it's brief. Trying to learn more about this space and this looks pretty cool. And ofc, nice work! reply Nischalj10 1 hour agorootparenta primer - https://github.com/nilsherzig/LLocalSearch/issues/17 reply nilsherzig 38 minutes agorootparentguys, i didn't thought there would be this much interest in my project haha. I feel kinda bad for just posting it in this state haha. I would love to make a more detailed post on how it works in the future (keep an eye on the repo?) reply mark_l_watson 8 hours agoparentprevYour project looks very cool. I had on my ‘list’ to re-learn Typescript (I took a TS course about 5 years ago, but didn’t do anything with it) so I just cloned your repo so I can experiment with it. EDIT: I just noticed that most of the code is Go. Still going to play with it! reply nilsherzig 1 hour agorootparentThanks :). Yea only the web part is typescript and I really wouldn't recommend to learn from my typescript haha reply d-z-m 24 minutes agoparentprevany plans to support other backends besides ollama? reply koeng 9 hours agoparentprevWhat is the search engine that it uses? reply nilsherzig 9 hours agorootparentsearxng, which is a locally running meta search engine combining a lot of different sources (including Google and co) reply mmahemoff 4 hours agorootparentThis might be more of a searxng question, but doesn't it quickly run up against anti-bot measures? CAPTCHA challenges and Forbidden responses? I can see the manual has some support for dealing with CAPTCHA [1], but in practical terms, I would guess a tool like this can't be used extensively all day long. I'm wondering if there's a search API that would make the backend seamless for something like this. 1. https://docs.searxng.org/admin/answer-captcha.html reply visarga 3 hours agorootparentAs a last resort we could have AI work on top of a real web browser and solving captchas as well. Should look like normal usage. I think these kinds of systems LLM + RAG + Web Agent will become widespread and the preferred method to interact with the web. We can escape all ads and dark UI patterns by delegating this task to AI agents. We could have it collect our feeds, filter, rank and summarize them to our preferences, not theirs. I think every web browser, operating system and mobile device will come equipped with its own LLM agent. The development of AI screen agents will probably get a big boost from training on millions of screen capture videos with commentary on YouTube. They will become a major point of competition on features. Not just browser, but also OS, device and even the chips inside are going to be tailored for AI agents running locally. reply manojlds 2 hours agorootparentIf everyone consumes like that what's even the incentive for content creators? reply nilsherzig 1 hour agorootparentprevI didn't run into a lot of timeouts while using it myself, but you would probably need another search source if you plan to host this service for multiple users at the same time. There are projects like flareresolverr which might be interesting reply ivolimmen 5 hours agoparentprev\"normal consumer GPU\"... well mine is a 4GB 6600.. so I guess that varies. reply nilsherzig 1 hour agorootparentSorry it wasn't my intention to gatekeep, but my 300€ card really is on the low end of LLM Things reply wg0 22 minutes agoprevIn five year's time - by 2030, I foresee that lots of inference would be happening on local machines with models being downloaded on demand. Think docker registry of AI models which is pretty much Hugging Face already there. This all would be due to optimisations within model inference code and techniques, hardware and packaging of software like the above. Don't see billion dollar valuations for lots of AI startups out there to materialise into anything. reply openquery 14 minutes agoparent> I foresee that lots of inference would be happening on local machines with models being downloaded on demand Why? It's much more efficient to have centralized special purpose hardware to run enormous models and then ship the comparatively small result over the internet. By analogy, you don't have a search engine running on your phone right? reply Sammi 1 minute agorootparentYou currently can't have a search engine running locally on your phone. Google search is possible the single largest c++ program every built. And nevermind the storage needs... But in a few years we might be able to have LLMs running on our phones that work just as well if not better. reply vachina 9 minutes agorootparentprevA more appropriate analogy would be driving your own car vs. taking the bus. reply arflikedog 2 hours agoprevA while back you commented on my personal project Airdraw which I really appreciated. This looks awesome and you're well on your way to another banger project - looking forward to toying around with this :) reply gardenhedge 0 minutes agoparentDid you just happened to see this post today and notice the username? reply nilsherzig 1 hour agoparentprevUhh yes I was really impressed by your project :) reply gardenhedge 2 minutes agoprevCompletely locally running search engine.. that queries the Internet reply gorbypark 53 minutes agoprevI if a quick poke through the source and it seems like there’s not much reason this couldn’t run on macOS? It seems that ollama is doing the inference and then there’s a go binary doing everything else? I might give it a go and see what happens! reply adr1an 1 hour agoprevThis is so cool! And the fact that you can use Ollama as 'llm backend' makes it sustainable. didn't see how to switch models in the demo, that might be worth to highlight in readme.. reply adr1an 29 minutes agoparentI have a 'feature request', can we manage which sites are being used by some categories in the frontend? For example, if I build a list of websites and out them under \"coding\", then I'd like to use those to answer my programming questions. Meanwhile, I'd like to add an \"art\" category for museum's homepages so that I can ask which year was XYZ painting from. And so on. The current implementation looks like the inter-operability with searing is more static... IDK if searxng has an API to switch those filters or if they can be managed already through 'profiles'.. that kind of thing.. reply hubraumhugo 2 hours agoprevExcellent work! Cool side projects like that will eventually help you get hired by a top startup or may even lead to building your own. I can only encourage other makers to post their projects on HN and put them out into the world. reply nilsherzig 1 hour agoparentYea it's also quite fulfilling to see people likening something you've put some work into :) reply noisy_boy 1 hour agoprevWould be good if the readme mentions minimum hardware specs to get a reasonably decent performance. E.g. I have a ThinkPad X1 extreme i7 with MaxQ graphics, any hopes of running this on it without completely ruining the performance? reply nilsherzig 1 hour agoparentYou could run the LLM using your CPU and normal (non video) ram. But that's a lot slower. There are people working on making it a lot faster tho. The bottleneck is the transfer speed between the ram Sticks and the CPU. Just taking a guess, but I wouldn't expect more than a couple tokens (more or less like syllables) per second. Which is probably to slow, since it has to read a couple thousand per search result. It's hard to provide minimum requirements, since there are so many edge cases. reply oysterpingu 1 hour agoprevAwesome project! As I newbie myself in everything LLM, where should I start looking to create a similar project than yours? Which resources/projects are good to know about? Thank you for sharing! reply nilsherzig 1 hour agoparentI think the easiest entry point would be the python langchain project? It has a lot more documentation and working examples than the golang one I've used :) If you could tell me more about your goals, I can probably provide a more narrow answer :) reply xydac 9 hours agoprevThis is cool, haven't run this yet but seems really promising. Am thinking how this can be a super useful to hook with internal corporate search engines and then get answers from that. Good to see more of these non API key products being built (connected to local llms) reply siborg 40 minutes agoprevExciting project. Trying to install it but running into some issues with searxng. Anyone else? reply nilsherzig 36 minutes agoparentplease tell me about the problem or open an issue :) reply keyle 11 hours agoprevImpressive, I don't think I've seen a local model call upon specialised modules yet (although I can't keep up with everything going on). I too use local 7b open-hermes and it's really good. reply nilsherzig 11 hours agoparentThanks :). It's just a lot of prompting and string parsing. There are models like \"Hermes-2-Pro-Mistral\" (the one from the video) which are trained to work with function signatures and outputting structured text. But at the end it's just strings in > strings out, haha. But its fun (and sometimes frustrating) to use LLMs for flow control (conditions, loops...) inside your programs. reply davidcollantes 9 hours agorootparentGot a link for that one? I have found a few with Hermes-2-Mistral in the name. reply keyle 10 hours agorootparentprevWow, I didn't know about \"Hermes 2 Pro - Mistral 7B\", cheers! reply nilsherzig 10 hours agorootparentIt's my go to \"structured text model\" atm. Try \"starling-ml-beta\" (7b) for some very impressive chat capabilities. I honestly think that it outperforms GPT3 half the time. reply peter_l_downs 9 hours agorootparentSorry to repeat the same question I just asked the other commenter in this thread, but could you link the model page and recommend a specific level of quantization for the models you've referenced? I'd love to play with these models and see what you're talking about. reply BOOSTERHIDROGEN 9 hours agorootparentIt's from nous research https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B Q5 is minimum. reply peter_l_downs 9 hours agorootparentThank you — from that page, at the bottom, I was able to find this link to what I think are the quantized versions https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B-... If you have the time, could you explain what you mean by \"Q5 is minimum\"? Did you determine that by trying the different models and finding this one is best, or did someone else do that evaluation, or is that just generally accepted knowledge? Sorry, I find this whole ecosystem quite confusing still, but I'm very new and that's not your problem. reply d-z-m 47 minutes agorootparentTalking GGUF, Usually the higher you can afford to go wrt. quantization(e.g. Q5 is better than Q4, etc), the better. A Q6_K has minimal performance loss from the Q8, so in most cases if you can fit a Q6_K it's recommended to just use that. TheBloke's READMEs[0] usually have a good table summarizing each quantization level. If you're RAM constrained, you'll also have to make trade-offs about the context length. e.g. you could have 8 GB RAM and a Q5 quant with shorter context, vs Q3 with longer, etc. [0]:https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF reply BOOSTERHIDROGEN 4 hours agorootparentprevIt's the best balance if you have limited compute performance. reply peter_l_downs 9 hours agoparentprevI'm just starting to get into downloading and testing models using llama.cpp and I'm curious which model you're actually using, since they seem to come in varying levels of quantization. Is this [0] the model page for the one you're using, or should I be looking somewhere else? What is the actual file name of the model you're using? [0] https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GG... reply windexh8er 9 hours agoparentprevHave you looked into tools like CrewAI [0]? [0] https://www.crewai.io/ reply viksit 10 hours agoparentprevcurious what hardware you use? and is any of this runnable on an m1 laptop? reply keyle 10 hours agorootparentAbsolutely, 7B will run comfortably on 16GB of RAM and most consumer level hardware. Some of the 40B run on 32GB, but it depends on the model I found (GGUF, crossing fingers help). I ran this originally on a M1 with 32GB, I run this on an Air M2 with 16GB (and mac mini M2 32GB), no problem. I use llama.cpp with a SwiftUI interface (my own), all native, no scripts python/js/web. 7b is obviously less capable but the instant response makes it worth exploring. It's very useful as a Google search replacement that is instantly more valuable, for general questions, than dealing with the hellscape of blog spam ruling Google atm. Note, for my complex code queries at $dayjob where time is of the essence, I still use GPT4 plus, which is still unmatched imho, without running special hardware at least. reply nilsherzig 10 hours agorootparentprevDepends on your m1 specs, but should definitely be able to run a 7b model (at least with some quantization). reply fnetisma 10 hours agoprevThis is really neat! I have questions: “Needs tool usage” and “found the answer” blocks in your infra, how are these decisions made? Looking at the demo, it takes a little time to return results, from the search, vector storage and vector db retrieval, which step takes the most time? reply nilsherzig 10 hours agoparentThanks :) Die LLM makes these decisions on its own. If it writes a message which contains a tool call (Action: Web search Action Input: weight of a llama) the matching function will be executed and the response returned to the LLM. It's basically chatting with the tool. You can toggle the log viewer on the top right, to get more detail on what it's doing and what is taking time. Timing depends on multiple things: - the size of the top n articles (generating embeddings for them takes some time) - the amount of matching vector DB responses (reading them takes some time) reply dcreater 4 hours agorootparent> Die LLM You mean the? The German is bleeding through haha reply rzzzt 3 hours agorootparentWolfenstein 3D did it first! And then The Simpsons as well. reply hackernewds 5 hours agoprevwhat does this have to do with Perplexity? it should reference the underlying models used instead reply vishnumohandas 5 hours agoparentThe UX is comparable. reply firtoz 4 hours agoprevExcellent work! I plan to use it with existing LLMs tbh, but great to see it working locally also! Thank you so much for sharing. I love the architecture. reply pants2 9 hours agoprevIt says it's a \"locally running search engine\" - but not sure how it finds the sites and pages to index in the first place? reply nilsherzig 9 hours agoparentYea I guess that's misleading, I should probably change that. I was referring to the LLM part as locally running. Indexing is still done by the big guys and queried using searxng reply lavela 1 hour agorootparentWhat would be your current recommendation on how to create a vector db from local files that would work with LLocalSearch? reply nilsherzig 9 hours agorootparentprevJust to clarify, it wasn't my intention to be misleading reply nilsherzig 1 hour agoprevUhh sorry guys, I was asleep and now the project has like 1k stars haha I will try my best to catch up with everyone <3 reply nikolayasdf123 1 hour agoprevcool to see Go here reply ldjkfkdsjnv 9 hours agoprevThe big secret about perplexity is they havent done much beyond using off the shelf models reply msp26 21 minutes agoparentPretraining and even finetuning (to a good extent) is overrated and you can create plenty of value without it. reply ggnore7452 5 hours agoparentprevI've been working on a small personal project similar to this and agree that replicating the overall experience provided by Perplexity.ai, or even improving it for personal use, isn't that challenging. (The concerns of scale or cost are less significant in personal projects. Perplexity doesn't do too much planning or query expansion, nor does it dig super deep into the sources afaik) I must say, though, that they are doing a commendable job integrating sources like YouTube and Reddit. These platforms benefit from special preprocessing and indeed add value. reply KuriousCat 7 hours agoparentprevHow did they secure funds in that case? reply code51 1 hour agorootparentSimple, looking at the mirror and saying \"Google-killer\" firmly 3 times everyday. reply basbuller 5 hours agorootparentprevThat is probably exactly why they got funding. You can sell it as focus on adding new features and leveraging the best available tools before reinventing the wheel. They do train their own models now, but for about a year they just forwarded calls to models like gpt3.5T. You still have the option to use models not trained by perplexity. reply KuriousCat 5 hours agorootparentI still don't get it. What was the USP here? What is the allure in it for the investors? reply hackernewds 5 hours agorootparentprevwhich is why their engagement and model responses suck. the other competitors are far better C.ai and Pi comes to mind reply basbuller 5 hours agorootparentWait, are you directly comparing Perplexity and C.ai or Pi? Perplexity is a search engine, Pi is a chatbot, and C.ai is roleplay? Their value propositions are very different reply pcthrowaway 9 hours agoprevNow if someone can hook this into Plandex (also shared today - https://news.ycombinator.com/item?id=39918500) to make a tool that enables you to collaborate with AI without any of your code leaving your computer, that would be amazing! reply darby_eight 8 hours agoprevPerplexity seems to be a chatbot competitor. reply ml-anon 2 hours agoprev [–] Did you really make a perplexity clone if you didn’t spend more time promoting yourself on Twitter and LinkedIn than on the engineering? reply nilsherzig 1 hour agoparent [–] Ah damn I forgot about getting some VC money reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A video demo showcases a 7b Model operating on a gaming GPU, delivering excellent performance despite hardware constraints."
    ],
    "commentSummary": [
      "Users are exploring the development and capabilities of the locally running meta search engine, searxng, on GitHub, focusing on optimizing hardware, managing websites, and balancing performance and resources with specialized text generation models.",
      "Comparisons are drawn between searxng and other AI platforms like Perplexity, C.ai, and Pi, while recommendations are provided for collaboration efforts and attracting venture capital investment."
    ],
    "points": 449,
    "commentCount": 86,
    "retryCount": 0,
    "time": 1712179666
  },
  {
    "id": 39917509,
    "title": "Enhanced Static Analysis Features in GCC 14",
    "originLink": "https://developers.redhat.com/articles/2024/04/03/improvements-static-analysis-gcc-14-compiler",
    "originBody": "Improvements to static analysis in the GCC 14 compilerRed Hat Developer /** * Creates and dispatches an event trigger * @param {String} evt - The name of the event */ function sendCustomEvent(evt){ if(document.createEvent && document.body.dispatchEvent){ var event = document.createEvent('Event'); event.initEvent(evt, true, true); //can bubble, and is cancellable document.body.dispatchEvent(event); } else if(window.CustomEvent && document.body.dispatchEvent) { var event = new CustomEvent(evt, {bubbles: true, cancelable: true}); document.body.dispatchEvent(event); } } var digitalData = {\"page\":{\"attributes\":{\"queryParameters\":\"\",\"taxonomyAudience\":[\"9175\"],\"taxonomyBusinessUnit\":[\"9625\"],\"taxonomyCampaign\":[],\"taxonomyLifecycle\":[],\"taxonomyProduct\":[\"9985\"],\"taxonomyProductLine\":[],\"taxonomyProject\":[\"10495\"],\"taxonomyPromotion\":[],\"taxonomyRegion\":[]},\"category\":{\"contentSubType\":\"\",\"contentType\":\"\",\"keyPage\":false,\"keyPageType\":\"\",\"pageType\":\"\",\"primaryCategory\":\"\",\"subCategories\":[]},\"pageInfo\":{\"breadCrumbs\":[],\"cms\":\"RHD CMS 8\",\"contentID\":\"284912\",\"destinationURL\":\"\",\"errorMessage\":\"\",\"errorType\":\"\",\"language\":\"en\",\"pageID\":\"284912\",\"pageName\":\"Improvements to static analysis in the GCC 14 compiler\",\"pageStatus\":\"published\",\"referringDomain\":\"\",\"referringURL\":\"\",\"syndicationIds\":[],\"sysEnv\":\"\",\"title\":\"Improvements to static analysis in the GCC 14 compiler\"},\"listing\":{\"browseFilter\":\"\",\"query\":\"\",\"queryMethod\":\"\",\"refinementType\":\"\",\"refinementValue\":\"\",\"resultCount\":\"\",\"searchType\":\"\"}},\"user\":[{\"profile\":[{\"profileInfo\":{\"accountID\":\"\",\"daysSinceLastPurchase\":\"\",\"daysSinceRegistration\":\"\",\"eloquaGUID\":\"POPULATE ELOQUA ID\",\"keyCloakID\":\"\",\"loggedIn\":false,\"profileID\":\"\",\"registered\":false,\"socialAccountsLinked\":[],\"subscriptionFrequency\":\"\",\"subscriptionLevel\":\"\",\"userAgent\":\"\"}}]}],\"event\":[]}; ( function( w, d, dd, undefined ) { // Check to see if the data object is on the page if not go no further. if(!dd) { return; } var bc = window.location.href.replace(/^https?\\:\\/\\/([a-z._-]|[0-9])+(:?[0-9]*)?(\\/pr\\/[0-9]+\\/export)?\\//,'').replace(/\\/$/,'').split('?')[0].split('#')[0].split(/\\//), primaryCategory= '', subCategories = [], pageType = '', pageName = '', contentType = '', contentSubType = ''; if (bc.length === 1 && bc[0] === \"\") { primaryCategory = \"home page\"; pageName = \"home page\"; subCategories.push('cms'); pageType = 'home'; } else { switch(bc[0]) { case 'about': pageType = bc[0]; primaryCategory= 'about_us'; break; case 'articles': pageType = 'article'; pageName = bc[bc.length - 1]; primaryCategory = 'article'; break; case 'events': if (bc.length > 1) { pageType = 'event'; primaryCategory = bc[0]; subCategories = bc.slice(1); } else { pageType = 'events'; primaryCategory= 'event-list'; } break; case 'products': pageType = bc[0]; if (bc.length > 1) { primaryCategory = bc[0]; subCategories = bc.slice(1); } else { primaryCategory = 'product-list'; } break; case 'topics': pageType = \"topic\"; if (bc.length >= 2) { primaryCategory = bc[0]; subCategories = [bc[1]]; pageName = subCategories[0]; contentType = \"topic\"; if (bc[2]) { contentSubType = \"topic-all\" } else { contentSubType = \"topic-landing\" } } break; case 'learning': let path = getLearningPathUrlData(window.location); if (path) { primaryCategory = path.primaryCategory; pageType = primaryCategory; subCategories = path.subCategories; pageName = path.pageName; } break; default: pageType = bc[0]; primaryCategory = bc[0]; if (bc.length > 1) { subCategories = bc.slice(1); } break; } } dd.page.attributes.queryParameters = window.location.href.split('?')[1] ? window.location.href.split('?')[1].replace(/=/g,':').split('&') : \"\"; dd.page.category.primaryCategory = primaryCategory; dd.page.category.subCategories = subCategories; dd.page.category.contentType = contentType || \"\"; dd.page.category.contentSubType = contentSubType || \"\"; dd.page.category.pageType = pageType || \"\"; dd.page.pageInfo.destinationURL = window.location.href; dd.page.pageInfo.pageName = pageName || bc[0] || \"\"; dd.page.pageInfo.language = document.documentElement.lang ? document.documentElement.lang : \"\"; dd.page.pageInfo.breadCrumbs = [dd.page.category.primaryCategory, dd.page.category.subCategories[0] || \"\"]; dd.page.pageInfo.sysEnv = ( w.innerWidthel); const [rootPath, secondPath, thirdPath, fourthPath] = paths; const secondPathParams = secondPath ? secondPath.split(':') : []; // [learn, openshift, foundations-openshift] const fourthPathParams = fourthPath ? fourthPath.split(':') : []; // [resources, build-and-deploy-applications-openshift] const [primaryCategory] = secondPathParams; const subCategories = [...secondPathParams.slice(1), thirdPath]; // [openshift, founations-openshift, resource] const pageName = fourthPathParams[fourthPathParams.length - 1]; // build-and-deploy-applications-openshift return { primaryCategory, subCategories, pageName, }; } function sendSocialLinkEvent(sprov){ var ddSocialLinkEvent = { eventInfo: { eventAction: 'link', eventName: 'social account link', socialAccount: sprov, socialAccountsLinked: dd.user[0].profile[0].profileInfo.socialAccountsLinked, timeStamp: new Date(), processed: { adobeAnalytics: false } } }; dd.event.push(ddSocialLinkEvent); sendCustomEvent('socialLinkEvent', ddSocialLinkEvent); } function getCookie( name ) { var value = \"; \" + document.cookie; var parts = value.split( \"; \" + name + \"=\" ); if ( parts.length == 2 ) { return parts.pop().split( \";\" ).shift(); } } function deleteCookie(name) { document.cookie = name + '=;path=/;expires=Thu, 01 Jan 1970 00:00:01 GMT;'; }; } )( window, document, digitalData ); //Attach event listner for search-complete window.document.addEventListener(\"search-complete\", function (e) { // Check to see if the data object is on the page if not go no further. if(!window.digitalData) { return; } if (typeof e.detail.invalid === \"undefined\") { var calcResultCount = e.detail.results.hits ? e.detail.results.hits.total : e.detail.results.numFound; var ddSearchEvent = { eventInfo: { eventName: \"internal search\", eventAction: \"search\", listing: { browseFilter: e.detail.filterStr || \"internal search\", query: e.detail.term, queryMethod: \"system generated\", resultCount: calcResultCount, resultsShown: e.detail.from, searchType: window.digitalData.page.category.primaryCategory || \"\", refinement: [], }, timeStamp: new Date(), processed: { adobeAnalytics: false }, }, }; window.digitalData.event.push(ddSearchEvent); window.digitalData.page.listing = ddSearchEvent.eventInfo.listing; window.sendCustomEvent(\"ajaxSearch\"); } }); { \"analyticsTitle\": \"Improvements to static analysis in the GCC 14 compiler\", \"blogAuthor\": \"David Malcolm\", \"cms\": \"RHD CMS 10\", \"pageCategory\": \"articles\", \"contentID\": \"284912\", \"contentType\": \"article\", \"dataObject\": \"digitalData\", \"destinationURL\": \"https://developers.redhat.com/articles/2024/04/03/improvements-static-analysis-gcc-14-compiler\", \"gated\": \"false\", \"queryParameters\": \"\", \"severityLevel\": \"\", \"siteLanguage\": \"en\", \"siteName\": \"rhd\", \"pageStatus\": \"Published\", \"pageSubType\": \"\", \"subsection\": \"2024\", \"subsection2\": \"04\", \"subsection3\": \"03\", \"pageTitle\": \"Improvements to static analysis in the GCC 14 compilerRed Hat Developer\", \"pageType\": \"article\", \"taxonomyAudience\": [ \"9175\" ], \"taxonomyBusinessUnit\": [ \"9625\" ], \"taxonomyCampaign\": [], \"taxonomyCheatSheetTags\": [], \"taxonomyControlTags\": [], \"taxonomyEventCategories\": [], \"taxonomyLifecycle\": [], \"taxonomyProduct\": [ \"9985\" ], \"taxonomyProductCategories\": [], \"taxonomyProductLine\": [], \"taxonomyProject\": [ \"10495\" ], \"taxonomyPromotion\": [], \"taxonomyRegion\": [], \"taxonomyStage\": [], \"taxonomyTopic\": [ \"8935\", \"34941\", \"8715\" ], \"taxonomyVideoResourceTags\": [], \"productCategory\": \"\", \"productCertifiedVendor\": \"\", \"productComponents\": \"\", \"productPublicationStatus\": \"\", \"productSBR\": \"\", \"productTags\": \"\", \"products\": \"\\u003Ca href=\\u0022/taxonomy/term/9985\\u0022 hreflang=\\u0022en\\u0022\\u003ERed Hat Enterprise Linux\\u003C/a\\u003E\" } { \"@context\": \"https://schema.org\", \"@graph\": [ { \"@type\": \"Article\", \"headline\": \"Improvements to static analysis in the GCC 14 compiler\", \"name\": \"Improvements to static analysis in the GCC 14 compilerRed Hat Developer\", \"about\": [ \"C\", \"Compilers\", \"Linux\" ], \"description\": \"I work at Red Hat on GCC, the GNU Compiler Collection. For the last five releases of GCC, I\\u0027ve been working on -fanalyzer, a static analysis pass that tries to identify various problems at compile-time, rather than at runtime.\", \"image\": { \"@type\": \"ImageObject\", \"url\": \"https://developers.redhat.com/sites/default/files/styles/share/public/blog/2021/04/gcc_1x.png?itok=6l1b8Lo9\" }, \"datePublished\": \"2024-04-03T07:00:00+0000\", \"dateModified\": \"2024-04-03T07:00:00+0000\", \"isAccessibleForFree\": \"True\", \"author\": { \"@type\": \"Person\", \"name\": \"David Malcolm,\" }, \"mainEntityOfPage\": \"https://developers.redhat.com/articles/2024/04/03/improvements-static-analysis-gcc-14-compiler\" }, { \"@type\": \"ImageObject\", \"url\": \"https://developers.redhat.com/themes/custom/rhdp_fe/images/metatags/2023_Global_Shared_image__A.png\", \"height\": \"630\", \"width\": \"1200\" }, { \"@type\": \"Organization\", \"@id\": \"https://developers.redhat.com/#organization\", \"name\": \"Red Hat\", \"sameAs\": [ \"https://www.facebook.com/RedHatDeveloperProgram/\", \"https://twitter.com/rhdevelopers\" ], \"url\": \"https://developers.redhat.com/\", \"image\": { \"@type\": \"ImageObject\", \"url\": \"https://developers.redhat.com/themes/custom/rhdp_fe/logo.svg\" }, \"logo\": { \"@type\": \"ImageObject\", \"representativeOfPage\": \"False\", \"url\": \"https://developers.redhat.com/themes/custom/rhdp_fe/logo.svg\", \"width\": \"540\", \"height\": \"174\" }, \"address\": { \"@type\": \"PostalAddress\", \"streetAddress\": \"100 East Davie Street\", \"addressLocality\": \"Raleigh\", \"addressRegion\": \"North Carolina\", \"postalCode\": \"27601\", \"addressCountry\": \"USA\" } }, { \"@type\": \"Person\", \"name\": \"David Malcolm\", \"url\": \"https://developers.redhat.com//author/david-malcolm\" }, { \"@type\": \"WebPage\", \"@id\": \"https://developers.redhat.com/articles/2024/04/03/improvements-static-analysis-gcc-14-compiler#webpage\", \"description\": \"I work at Red Hat on GCC, the GNU Compiler Collection. For the last five releases of GCC, I\\u0027ve been working on -fanalyzer, a static analysis pass that tries to identify various problems at compile-time, rather than at runtime.\", \"publisher\": { \"@type\": \"Organization\", \"@id\": \"https://developers.redhat.com/#organization\" }, \"isAccessibleForFree\": \"True\", \"inLanguage\": \"en-US\" }, { \"@type\": \"WebSite\", \"@id\": \"https://developers.redhat.com/#website\", \"name\": \"Red Hat Developer\", \"url\": \"https://developers.redhat.com/\", \"potentialAction\": { \"@type\": \"SearchAction\", \"target\": { \"@type\": \"EntryPoint\", \"urlTemplate\": \"https://developers.redhat.com/search?t={search_term_string}\", \"inLanguage\": \"en-US\" }, \"query\": \"https://developers.redhat.com/search?t={search_term_string}\", \"query-input\": \"required name=search_term_string\" }, \"publisher\": { \"@type\": \"Organization\", \"@id\": \"https://developers.redhat.com/#organization\" }, \"inLanguage\": \"en-US\" } ] } body[unresolved][unresolved]{opacity:1;} Skip to main content .view_mode_link_group h3 a, .view_mode_column_with_title_outer h3 a { color: #06c; text-decoration: none; font-weight: bold; } .view_mode_link_group h3 a:hover, .view_mode_column_with_title_outer h3 a:hover { text-decoration: underline; } .desktop-col-span-2, .desktop-col-span-3, .desktop-col-span-4 { display: flex; flex-wrap: wrap; align-items: flex-start; gap: 16px; } .desktop-col-span-2>div, .desktop-col-span-3>div, .desktop-col-span-4>div { flex: 1 auto; min-width: 150px; max-width: 215px; } .column_with_right_border { border-right: 1px solid var(--pfe-theme--color--ui--border--lighter, #d2d2d2); } li.view_mode_feature { display: grid; align-items: start; grid-template-columns: auto; grid-template-rows: auto auto; grid-template-areas: \"title\" \"feat\"; } li.view_mode_feature[data-feature-image-url] { display: grid; align-items: center; grid-template-columns: 64px auto; grid-template-rows: auto auto; grid-template-areas: \"icon title\" } li.view_mode_feature[data-feature-text][data-feature-image-url] { display: grid; align-items: start; grid-template-columns: 64px auto; grid-template-rows: auto auto; grid-template-areas: \"icon title\" \"icon feat\"; } li.view_mode_feature a { grid-area: title; } li.view_mode_feature .field--name-field-link-image { grid-area: icon; } li.view_mode_feature .field--name-field-link-image > div { padding-right: 12px; display: grid; } li.view_mode_feature[data-feature-text][data-feature-image-url] .field--name-field-link-image > div { display: block; } li.view_mode_feature .field--name-field-link-image img { width: 100%; height: auto; object-fit: contain; object-position: top left; } li.view_mode_feature div.field--name-field-feature-text { grid-area: feat; } li.view_mode_feature_callout { display: grid; grid-template-columns: auto; grid-template-rows: auto auto; grid-template-areas: \"title\" \"feat\"; } li.view_mode_feature_callout[data-feature-image-url] { display: grid; grid-template-columns: auto; grid-template-rows: auto auto auto; grid-template-areas: \"image\" \"title\" \"feat\"; } li.view_mode_feature_callout h3 { grid-area: title; color: #fff; margin-top: 1rem !important; } li.view_mode_feature_callout img { grid-area: image; max-width: 100%; height: auto; object-fit: contain; object-position: top left; } li.view_mode_feature_callout div.field--name-field-feature-text { grid-area: feat; } .column-feature { background-repeat: no-repeat; background-position: center; background-size: cover; background-color: #000; margin: -32px -16px -32px -16px; padding: 32px 16px; color: #fff; } .column-feature-light { background-repeat: no-repeat; background-position: center; background-size: cover; margin: -32px -16px -32px -16px; padding: 32px 16px; } .column-feature-light li.view_mode_feature_callout h3 { color: var(--pfe-navigation__dropdown--Color,#151515);; } @media only screen and (max-width: 767px) { li.view_mode_feature_callout img { display: none; } .desktop-col-span-2, .desktop-col-span-3, .desktop-col-span-4 { -ms-flex-direction: column; flex-direction: column; } .desktop-col-span-2>div, .desktop-col-span-3>div, .desktop-col-span-4>div { max-width: 100%; } .column_with_right_border { border-right: none; border-bottom: 1px solid var(--pfe-theme--color--ui--border--lighter, #d2d2d2); } } Products Featured Red Hat Enterprise Linux A secure, stable, and supported operating system.Red Hat OpenShift A cloud-native application platform.Red Hat Ansible Automation Platform Simple IT automation anyone can use.Podman Desktop Graphical tool for container and kubernetes development. View All Red Hat Products Linux Red Hat Enterprise Linux Red Hat Universal Base Images (UBI) Red Hat Enterprise Linux for SAP Microsoft SQL Server on Red Hat Enterprise Linux CentOS Linux Java runtimes & frameworks JBoss Enterprise Application Platform Red Hat build of Quarkus Red Hat build of OpenJDKView allKubernetes Red Hat OpenShift Red Hat OpenShift Service on AWS (ROSA) Microsoft Azure Red Hat OpenShift Red Hat OpenShift Dedicated Red Hat OpenShift AI Integration & App Connectivity Red Hat Service Interconnect AMQ Broker Red Hat Build of Apache CamelView allAutomation Red Hat Ansible Automation Platform Red Hat Ansible Automation Platform via AWS Marketplace Red Hat Ansible Lightspeed with IBM watsonx Code Assistant Ansible automation for applications and servicesView allDeveloper tools Red Hat Developer Hub Red Hat Trusted Software Supply Chain Podman Desktop Red Hat OpenShift Dev SpacesView all Developer Sandbox for Red Hat OpenShiftTry Red Hat products and technologies without setup or configuration fees for 30 days with this shared Openshift and Kubernetes cluster. Try at no cost Technologies Featured LinuxKubernetesAutomationApp Dev Platform View All Technologies Programming Languages & Frameworks Java Python C, C#, C++ System Design & Architecture Red Hat architecture and design patterns Microservices Event-Driven Architecture Databases Developer Productivity Developer productivity Developer Tools GitOps Secure Development & Architectures Security Secure coding Platform Engineering DevOps DevSecOps Ansible automation for applications and services Automated Data Processing AI/ML Data Science Apache Kafka on KubernetesView All Technologies Start exploring in the Developer Sandbox for freeTry Red Hat&#039;s products and technologies without setup or configuration. Try at no cost Learn Featured Kubernetes & Cloud NativeLinuxAutomationJavaAI/ML View All Learning ResourcesInteractive Lessons and Learning Paths Red Hat Enterprise Linux Red Hat OpenShift Red Hat Ansible Automation Platform Red Hat OpenShift AI Java Developer Sandbox Activities Get Started Try Hands-On ActivitiesE-Books GitOps Cookbook Podman in Action Kubernetes Operators The Path to GitOpsView All E-booksTutorials Kubernetes Application DevelopmentCheat Sheets Linux Commands Bash Commands Git systemd CommandsView All Cheat SheetsAPI Catalog API Catalog Legacy DocumentationRed Hat LearningBoost your technical skills to expert-level with the help of interactive lessons offered by various Red Hat Learning programs. Explore Red Hat Learning Events Developer Events DevNationJoin developers across the globe for live and virtual events led by Red Hat technology experts. Explore All Events Tech Talks Getting Gitops GitHub Makeover OpenTelemetry on KubernetesView All Tech TalksDeep Dives Quinoa: A modern Quarkus UI with no hassles Event-driven autoscaling through KEDA and Knative IntegrationView All Deep Dives Red Hat SummitExplore the Red Hat Summit 2023 virtual content hub and uncover insights that can fuel your future. Get on-demand access to the keynotes, sessions, and technical presentations from Red Hat Summit 2023 that are most relevant to your career. View the content hub Developer Sandbox Developer Sandbox (free) Try hands-on activities in the Developer SandboxAccess Red Hat’s products and technologies without setup or configuration, and start developing quicker than ever before with our new, no-cost sandbox environments.Explore the Developer Sandbox IDE Featured Developer Sandbox activities Deploy a Java application on Kubernetes in minutes Ready to start developing apps?Explore the free Developer SandboxBlogVideos Search SearchAll Red Hat:root { --rh-custom-column-top-margin: 41px; --pfe-navigation--content-max-width: 1400px; } @media(max-width: 1200px){:root { --rh-custom-column-top-margin: 0; } }site-switcher { --developers--logo: var(--developers--logo--you-are-here); --developers--backgroundColor: var(--background--you-are-here); --developers--label: var(--label--you-are-here); --developers--margin-top: 0; --rhdc--logo: initial; --rhdc--backgroundColor: initial; --rhdc--label: initial; --rhdc--margin-top: initial; }Article Improvements to static analysis in the GCC 14 compiler April 3, 2024CCompilersLinuxDavid MalcolmTable of contents:I work at Red Hat on GCC, the GNU Compiler Collection. For the last five releases of GCC, I've been working on -fanalyzer, a static analysis pass that tries to identify various problems at compile-time, rather than at runtime. It performs \"symbolic execution\" of C source code—effectively simulating the behavior of the code along the various possible paths of execution through it. This article summarizes what's new with -fanalyzer in GCC 14, which I hope will be officially released sometime in April 2024. Solving the halting problem? Obviously I'm kidding with the title here, but for GCC 14 I've implemented a new warning: -Wanalyzer-infinite-loop that's able to detect some simple cases of infinite loops. For example, consider the following C code:void test (int m, int n) { float arr[m][n]; for (int i = 0; ivoid test (void) { char buf[10]; strcpy (buf, \"hello\"); strcat (buf, \" world!\"); }The analyzer emits this message:: In function 'test': :7:3: warning: stack-based buffer overflow [CWE-121] [-Wanalyzer-out-of-bounds] 7strcat (buf, \" world!\");^~~~~~~~~~~~~~~~~~~~~~~ 'test': events 1-2| 5char buf[10];| ^~~||(1) capacity: 10 bytes6strcpy (buf, \"hello\");7strcat (buf, \" world!\");| ~~~~~~~~~~~~~~~~~~~~~~~||(2) out-of-bounds write from byte 10 till byte 12 but 'buf' ends at byte 10:7:3: note: write of 3 bytes to beyond the end of 'buf' 7strcat (buf, \" world!\");^~~~~~~~~~~~~~~~~~~~~~~ :7:3: note: valid subscripts for 'buf' are '[0]' to '[9]'I've been unhappy with the readability of these messages: it describes some aspects of the problem, but it's hard for the user to grasp exactly what the analyzer is \"thinking.\" So for GCC 14, I've added the ability for the analyzer to emit text-based diagrams visualizing the spatial relationships in a predicted buffer overflow. For the above example (which you can try here in Compiler Explorer) it emits the diagram shown in Figure 1. Figure 1: Visualizing buffer overflows in GCC 14.This diagram shows the destination buffer populated by the content from the strcpy call, and thus the existing terminating NUL byte used for the start of the strcat call. For non-ASCII strings such as this:#includevoid test (void) { char buf[11]; strcpy (buf, \"&#12469;&#12484;&#12461;\"); strcat (buf, \"&#12513;&#12452;\"); }It can show the UTF-8 representation of the characters (Figure 2). Figure 2: Visualizing non-ASCII strings in GCC 14.This demonstrates that the overflow happens partway through the &#12513; character (U+30E1). (Link to Compiler Explorer). Analyzing C string operations I've put some work into better tracking C string operations in GCC 14's analyzer. One of the improvements is that the analyzer now simulates APIs that scan a buffer expecting a null terminator byte, and will complain about code paths where a pointer to a buffer that isn't properly terminated is passed to such an API. I've added a new function attribute null_terminated_string_arg(PARAM_IDX) for telling the analyzer (and human readers of the code) about parameters that are expected to be null-terminated strings. For example, given this buggy code:extern char * example_fn (const char *p) __attribute__((null_terminated_string_arg (1))) __attribute__((nonnull)); char * test_unterminated_str (void) { char str[3] = \"abc\"; return example_fn (str); }Here, the analyzer correctly complains that str doesn't have a null terminator byte, and thus example_fn will presumably read past the end of the buffer:: In function 'test_unterminated_str': :10:10: warning: stack-based buffer over-read [CWE-126] [-Wanalyzer-out-of-bounds] 10return example_fn (str);^~~~~~~~~~~~~~~~ 'test_unterminated_str': events 1-3| 9char str[3] = \"abc\";| ^~~||(1) capacity: 3 bytes10return example_fn (str);| ~~~~~~~~~~~~~~~~||(2) while looking for null terminator for argument 1 ('&str') of 'example_fn'...| (3) out-of-bounds read at byte 3 but 'str' ends at byte 3:10:10: note: read of 1 byte from after the end of 'str' 10return example_fn (str);^~~~~~~~~~~~~~~~ :10:10: note: valid subscripts for 'str' are '[0]' to '[2]'&#9484;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9488;&#9474; read of 1 byte &#9474;&#9492;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9496; ^ &#9474; &#9474; &#9484;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9516;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9516;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9488;&#9484;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9488; &#9474; [0] &#9474; ... &#9474; [2] &#9474;&#9474; &#9474; &#9500;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9524;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9524;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9508;&#9474;after valid range&#9474; &#9474; 'str' (type: 'char[3]') &#9474;&#9474; &#9474; &#9492;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9496;&#9492;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9496; &#9500;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9516;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9508;&#9500;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9516;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9508; &#9474;&#9474; &#9581;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9524;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9582; &#9581;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9524;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9582; &#9474;size: 3 bytes&#9474; &#9474; over-read of 1 byte &#9474; &#9584;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9583; &#9584;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9472;&#9583; :2:1: note: argument 1 of 'example_fn' must be a pointer to a null-terminated string 2example_fn (const char *p)^~~~~~~~~~ Again, you can try this example in Compiler Explorer here. Taint analysis The analyzer has a form of \"taint analysis\", which tracks attacker-controlled inputs, places where they are sanitized, and places where they are used without sanitization. In previous GCC releases this was too buggy to enable by default, with lots of false positives, so I hid it behind an extra command-line argument. I've fixed many bugs with this, so for GCC 14 I've enabled this by default when -fanalyzer is selected. This also enables these 6 taint-based warnings: -Wanalyzer-tainted-allocation-size-Wanalyzer-tainted-array-index-Wanalyzer-tainted-assertion-Wanalyzer-tainted-divisor-Wanalyzer-tainted-offset-Wanalyzer-tainted-size For example, here's an excerpt from CVE-2011-2210 from the Linux kernel:extern struct hwrpb_struct *hwrpb; SYSCALL_DEFINE5(osf_getsysinfo, unsigned long, op, void __user *, buffer, unsigned long, nbytes, int __user *, start, void __user *, arg) {/* [...snip...] *//* case GSI_GET_HWRPB: */ if (nbytes : In function 'sys_osf_getsysinfo': :55:21: warning: use of attacker-controlled value 'nbytes' as size without upper-bounds checking [CWE-129] [-Wanalyzer-tainted-size] 55if (copy_to_user(buffer, hwrpb, nbytes) != 0)^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 'sys_osf_getsysinfo': event 1| 28long sys##name(__SC_DECL##x(__VA_ARGS__))| ^~~||(1) function 'sys_osf_getsysinfo' marked with '__attribute__((tainted_args))'+--> 'sys_osf_getsysinfo': event 2| 28long sys##name(__SC_DECL##x(__VA_ARGS__))| ^~~||(2) entry to 'sys_osf_getsysinfo''sys_osf_getsysinfo': events 3-6| 52if (nbytes31')... |......55if (copy_to_user(buffer, hwrpb, nbytes) != 0)| ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~||(5) ...to here| (6) use of attacker-controlled value 'nbytes' as size without upper-bounds checking:11:13: note: parameter 3 of 'copy_to_user' marked as a size via attribute 'access (write_only, 1, 3)' 11extern long copy_to_user(void __user *to, const void *from, unsigned long n)^~~~~~~~~~~~The issue is that the attempt to sanitize nbytes was written asif (nbytessizeof(*hwrpb)) With a fixed version of that conditional, the analyzer is silent. I'm continuing to work on running the analyzer on the kernel to look for vulnerabilities (and fix false positives in the analyzer). Try it out! We're still fixing bugs, but we hope that GCC 14 will be ready to officially release (as 14.1) sometime in April 2024. With my \"downstream\" hat on, we're already using the prerelease (GCC 14.0) within Fedora 40 Beta. Finally, you can use the excellent Compiler Explorer site to play with the new compiler. Have fun!Recent ArticlesGetting started with MongoDB and Quarkus: Beyond the basics Improvements to static analysis in the GCC 14 compiler How to add debug support for Go stripped binaries Customize AWS cloud images with RHEL image builder and Packit Secure JBoss EAP apps with Microsoft Entra ID & OpenID Connect Related ContentImprovements to static analysis in the GCC 13 compilerHow the GNU C Library handles backward compatibilityGCC5 and the C++11 ABIA platform interface for the GNU C LibraryNew C++ features in GCC 13A leanerin libstdc++ for GCC 13 What’s up next? Ready to level up your Linux knowledge? Our Intermediate Linux Cheat Sheet presents a collection of Linux commands and executables for developers and system administrators who want to move beyond the basics. Get the cheat sheet LinkedIn YouTube Twitter FacebookProducts Red Hat Enterprise Linux Red Hat OpenShift Red Hat Ansible Automation Platform See all products See all technologies Build Developer Sandbox Developer Tools Interactive Tutorials API Catalog Operators Marketplace Quicklinks Learning Resources E-books Cheat Sheets Blog Events Communicate About us Contact sales Find a partner Report a website issue Site Status Dashboard Report a security problemRED HAT DEVELOPERBuild here. Go anywhere.We serve the builders. The problem solvers who create careers with code.Join us if you’re a developer, software engineer, web designer, front-end designer, UX designer, computer scientist, architect, tester, product manager, project manager or team lead.Sign me upRed Hat legal and privacy links About Red HatJobsEventsLocationsContact Red HatRed Hat BlogDiversity, equity, and inclusionCool Stuff StoreRed Hat Summit Red Hat legal and privacy links Privacy statement Terms of use All policies and guidelines Digital accessibility{\"path\":{\"baseUrl\":\"\\/\",\"scriptPath\":null,\"pathPrefix\":\"\",\"currentPath\":\"node\\/284912\",\"currentPathIsAdmin\":false,\"isFront\":false,\"currentLanguage\":\"en\"},\"pluralDelimiter\":\"\\u0003\",\"suppressDeprecationErrors\":true,\"ajaxPageState\":{\"libraries\":\"ckeditor_indentblock\\/indentblock,claro\\/media_library.theme,claro_media_library_theme\\/claro_media_library_theme,codesnippet\\/prismjs,codesnippet\\/prismjs.plugin.normalize-whitespace,codesnippet\\/prismjs.style.default,core\\/drupal.states,core\\/internal.jquery.form,datalayer_metatag\\/cpx_cookie,datalayer_metatag\\/cpx_reporter_component,datalayer_metatag\\/cpx_user,media\\/filter.caption,red_hat_jwt\\/keycloak-js-external,red_hat_learning_paths\\/red_hat_learning_paths_styles,red_hat_unified_footer\\/red_hat_unified_footer,red_hat_universal_navigation\\/red_hat_universal_navigation_account,red_hat_universal_navigation\\/red_hat_universal_navigation_all_sites,rhd_eloqua_embed\\/eloqua,rhd_zendesk\\/zendesk_modal,rhdp_fe\\/add-to-any,rhdp_fe\\/article_js,rhdp_fe\\/code-snippet,rhdp_fe\\/codesnippet-prism,rhdp_fe\\/jira-issue-collector,rhdp_fe\\/rhd_sso,rhdp_fe\\/rhdp_fe-theme,simple_recaptcha\\/simple_recaptcha,system\\/base,views\\/views.module\",\"theme\":\"rhdp_fe\",\"theme_token\":null},\"ajaxTrustedUrl\":{\"form_action_p_pvdeGsVG5zNF_XLGPTvYSKCf43t8qZYSwcfZl2uzM\":true,\"\\/articles\\/2024\\/04\\/03\\/improvements-static-analysis-gcc-14-compiler?ajax_form=1\":true},\"simple_recaptcha\":{\"sitekey\":\"6LfI4RApAAAAAF0FZDNh0WxeTb5vW26PPphSO9CR\",\"explicit_render\":true,\"form_ids\":{\"rh_zendesk_support_form\":\"rh_zendesk_support_form\"}},\"ajax\":{\"edit-submit\":{\"callback\":\"::createTicket\",\"wrapper\":\"progress\",\"progress\":{\"type\":\"throbber\",\"message\":\"Processing form...\"},\"event\":\"mousedown\",\"keypress\":true,\"prevent\":\"click\",\"url\":\"\\/articles\\/2024\\/04\\/03\\/improvements-static-analysis-gcc-14-compiler?ajax_form=1\",\"httpMethod\":\"POST\",\"dialogType\":\"ajax\",\"submit\":{\"_triggering_element_name\":\"op\",\"_triggering_element_value\":\"Send\"}}},\"red_hat_jwt\":{\"client_id\":\"rhd-web\",\"cookie_name\":\"rh_jwt_rhd_web\",\"scopes\":\"api.dxp_portals.developers\",\"leeway\":\"0\",\"realm\":\"redhat-external\",\"sso_host\":\"https:\\/\\/sso.redhat.com\",\"user_integration\":1,\"user_plugin\":\"internal_registration_only_user_auth\",\"use_external_js\":1,\"use_internal_js\":0,\"use_in_admin\":0,\"debug\":0,\"store_external_id\":0,\"external_id_jwt_property\":\"\",\"stateless_auth\":0,\"drupal_authentication\":1},\"rhd_admin\":{\"admin-logout-too\":false},\"user\":{\"uid\":0,\"permissionsHash\":\"e0060b822870eed63cba12ef5ee18bccda6560e16fe33f622dae789afb3bc314\"},\"rhd\":{\"contentGateway\":{\"baseUrl\":\"https:\\/\\/developers.redhat.com\",\"fileBaseUrl\":\"\\/\\/developers.redhat.com\\/content-gateway\\/file\\/\"},\"swel\":{\"url\":\"https:\\/\\/api.developers.redhat.com\\/swel\"},\"developer_sandbox\":{\"apiUrl\":\"https:\\/\\/registration-service-toolchain-host-operator.apps.sandbox.x8i5.p1.openshiftapps.com\\/api\\/v1\",\"cheApiUrl\":\"https:\\/\\/registration-service-toolchain-host-operator.apps.sandbox.x8i5.p1.openshiftapps.com\\/api\\/v1\",\"rhodApiUrl\":\"https:\\/\\/registration-service-toolchain-host-operator.apps.sandbox.x8i5.p1.openshiftapps.com\\/api\\/v1\",\"rhoamApiUrl\":\"https:\\/\\/registration-service-toolchain-host-operator.apps.rhoam-ds-prod.xe9u.p1.openshiftapps.com\\/api\\/v1\",\"recaptchaSiteKey\":\"6Lc_164lAAAAAPvrC0WO-XDljvZ2DZ3UQ38A4XR0\"},\"red_hat_universal_navigation\":{\"red_hat_environment\":\"prod\",\"red_hat_universal_navigation_environment\":\"prod\",\"red_hat_environment_calc_nav\":\"prod\"}}}if ((\"undefined\" !== typeof _satellite) && (\"function\" === typeof _satellite.pageBottom)) { _satellite.pageBottom(); } Report a website issue Your nameYour e-mail addressSubjectMessageType of request/issue Download Issue - Logged in User can&#039;t get file Content is wrong or missing from site Learning Path trouble : Broken Instruqt Learning Path trouble : Missing Content Product Issue Trouble registering for an event I cannot log in I cannot renew my subscription I cannot sign up to the Red Hat Developer Program Other Problem Page URLCountry/Territory Canada China India United StatesAmerican Samoa Australia Bangladesh Bhutan Brunei Darussalam Cambodia Christmas Island Cocos (Keeling) Islands Cook Islands Fiji Guam Heard Island and McDonald Islands Hong Kong Indonesia Japan Kiribati Korea, Republic of Lao People&#039;s Democratic Republic Macao Malaysia Maldives Mongolia Marshall Islands Myanmar Nauru Nepal New Zealand Niue Norfolk Island Northern Mariana Islands Palau Papua New Guinea Philippines Samoa Singapore Solomon Islands Sri Lanka Thailand Timor-Leste Tokelau Tonga Tuvalu Taiwan Vanuatu Viet Nam Afghanistan Aland Islands Albania Algeria Andorra Angola Armenia Aruba Austria Azerbaijan Bahrain Belarus Belgium Benin Bermuda Bosnia and Herzegovina Botswana Bouvet Island British Indian Ocean Territory Bulgaria Burkina Faso Burundi Cameroon Cape Verde Central African Republic Chad Comoros Congo Congo, The Democratic Republic of the Cote d&#039;Ivoire Croatia Curacao Cyprus Czech Republic Denmark Djibouti Egypt Equatorial Guinea Eritrea Estonia Ethiopia Faroe Islands Finland France French Polynesia French Southern Territories Gabon Gambia Georgia Germany Ghana Gibraltar Greece Greenland Guadeloupe Guernsey Guinea Guinea-Bissau Holy See (Vatican City State) Hungary Iceland Iran, Islamic Republic of Iraq Ireland Isle of Man Israel Italy Jersey Jordan Kazakhstan Kenya Kuwait Kyrgyzstan Latvia Lebanon Lesotho Liberia Libyan Arab Jamahiriya Liechtenstein Lithuania Luxembourg Macedonia, The Former Yugoslav Republic of Madagascar Malawi Mali Malta Martinique Mauritania Mauritius Mayotte Moldova, Republic of Monaco Montenegro Morocco Mozambique Namibia Netherlands Netherlands Antilles New Caledonia Niger Nigeria Norway Oman Pakistan Palestinian Territory, Occupied Pitcairn Poland Portugal Qatar Reunion Romania Russian Federation Rwanda Saint Barthelemy Saint Helena Saint Martin (French part) Saint Pierre and Miquelon San Marino Sao Tome and Principe Saudi Arabia Senegal Serbia Serbia and Montenegro Seychelles Sierra Leone Sint Maarten Slovakia Slovenia Somalia South Africa South Georgia and the South Sandwich Islands South Sudan Spain Sudan Svalbard and Jan Mayen Swaziland Sweden Switzerland Syrian Arab Republic Tajikistan Tanzania, United Republic of Togo Tunisia Turkey Turkmenistan Uganda Ukraine United Arab Emirates United Kingdom Uzbekistan Wallis and Futuna Western Sahara Yemen Zaire Zambia Zimbabwe Anguilla Antigua and Barbuda Argentina Bahamas Barbados Belize Bolivia Brazil Cayman Islands Chile Colombia Costa Rica Cuba Dominica Dominican Republic Ecuador El Salvador Falkland Islands (Malvinas) French Guiana Grenada Guatemala Guyana Haiti Honduras Jamaica Mexico Micronesia, Federated States of Montserrat Nicaragua Panama Paraguay Peru Puerto Rico Saint Kitts and Nevis Saint Lucia Saint Vincent and the Grenadines Suriname Trinidad and Tobago Turks and Caicos Islands United States Minor Outlying Islands Uruguay Venezuela Virgin Islands, British Virgin Islands, U.S.Red Hat Account NumberRed Hat Username",
    "commentLink": "https://news.ycombinator.com/item?id=39917509",
    "commentBody": "Improvements to static analysis in GCC 14 (redhat.com)319 points by dmalcolm 20 hours agohidepastfavorite102 comments quincepie 14 hours agoTo me fanalyzer is one of GCC killer features over clang. It makes programming C much easier by explaining errors. The error messages also began to feel similar to Rust in terms of being developer friendly. reply mr_00ff00 13 hours agoparentI know Rust (esp on HN) is very hyped for its memory safety and nice abstractions, but I really wonder how much Rust owes its popularity to its error messages. I would say the #1 reason I stop learning a technology is because of frustrating or unclear errors. EDIT: Getting a bit of topic, but I meant more because I love C and would love it more with rust level error messages. reply darby_eight 9 hours agorootparentClang already had decent error messages by the time rust stabilized. There's simply not much you can do at runtime to explain a segfault. reply GrumpySloth 9 hours agorootparentNot when you called templated functions and were greeted with compile-time template stack traces. Or you called overloaded functions and were presented with 50 alternatives you might have meant. The language is inherently unfriendly to user-friendly error messages. reply darby_eight 9 hours agorootparentRust doesn't have templates, mister c plus plus user. Perhaps you might include an example of such a user-unfriendly message? reply GrumpySloth 8 hours agorootparentI’m talking about C++. You wrote that Clang already had friendly error messages. While they were less unfriendly than GCC, calling them friendly is a stretch. Rust having traits instead of templates is a big ergonomic improvement in that area. reply estebank 3 hours agorootparentFunnily enough, trait bounds are still a big pain in the neck to provide good diagnostics for because of the amount of things that need to be tracked that are cross cutting across stages of the compiler that under normal operation don't need to talk to each other. They got better in 2018, as async/await put them even more front and center and focused some attention on them, and a lot of work for keeping additional metadata around was added since then (search the codebase for enum ObligationCauseCode if you're curious) to improve them. Now with the new \"next\" trait solver they have a chance to get even better. It still easier than providing good diagnostics for template errors though :) (althought I'm convinced that if addressing those errors was high priority, common cases of template instantiations could be modeled internally in the same way as traits purely for diagnostics and materially improve the situation — I understand why it hasn't happened, it is hard and not obviously important). reply dist1ll 13 hours agorootparentprevThat's what makes me wary of modifying my NixOS config. A single typo and you get an error dump comparable to C++03 templates. reply pxc 12 hours agorootparentThat's definitely the most painful part of iterating on Nix code for me, even in simple configs. You eventually develop an intuition for common problems and rely more on that than on deciphering the stack traces, but that's really not ideal. reply lynx23 5 hours agorootparentprevActually, thats a reason why I never even touched Nix. Besides, being functional and all the hype, but the syntax and naming of the language feels ad-hoc enough for me to never have caught on... reply Quekid5 12 hours agorootparentprev... but you do get an error. That's a lot better what you typically get with C or C++. Assuming it's valid systax, of course. This is a veering off topic, but I do agree that Nix-the-language has a lot of issues. (You might suggest Guix, but I don't want to faff about with non-supported repositories for table stakes like firmware and such. Maybe Nickel will eventually provide a more pleasant and principled way to define Nix configurations?) reply nh2 11 hours agorootparentMy favourite Nix error message is infinite recursion encountered, at undefined position reply Quekid5 10 hours agorootparentHaha, reminds me of some Scheme interpreter that would just say something like 'missing paren' at position 0 or EOF depending on where the imbalance was :) ... but, yeah... I'm pretty sure there could be some hints as to whereabouts that infinite recursion was detected. reply giovannibonetti 12 hours agorootparentprevArguably Rust got good error messages by learning from Elm: https://elm-lang.org/news/compiler-errors-for-humans reply estebank 11 hours agorootparentElm is acknowledged as being the initial inspiration for focusing on diagnostics early on, but Rust got good error messages through elbow grease and focused attention over a long period of time. People getting used to good errors and demanding more, is part of the virtuous circle that keeps them high quality. Making good looking diagnostics requires UX work, but making good diagnostics requires a flexible compiler architecture and a lot of effort, nothing more, nothing less. reply darby_eight 9 hours agorootparentprevRust's eye towards errors predates Elm entirely. reply hardwaregeek 13 hours agorootparentprevYeah Rust is popular because it's a practical language with a nice type system, decent escape hatches, and good tooling. The borrow checker attracts some, but it could have easily been done in a way with terrible usability. reply darby_eight 9 hours agorootparent> The borrow checker attracts some, but it could have easily been done in a way with terrible usability. Why would anyone use the resulting language over C? What you're describing is C with a slightly friendlier compiler. reply Ar-Curunir 9 hours agorootparentI have never heard C as being described to have a good type system. reply darby_eight 8 hours agorootparentYou could try to argue this is the only source of rust's popularity.... or you could admit that the borrow checker is in fact a reason why folks use Rust over C. reply jonathankoren 12 hours agorootparentprev> I would say the #1 reason I stop learning a technology is because of frustrating or unclear errors. Overly verbose error messages that obscure more than illuminate are chief complaint against C++. Honestly, they can just sap all the energy out of a project. reply cogman10 12 hours agorootparent\"You violated a template rule. Here's a novel on everything that's broken as a result\" It's why the Constraint system was important for C++. reply Quekid5 13 hours agorootparentprevThe hard problem with C is that it's hard to tell if what the programmer wrote is an error. Hence warnings... which can be very hit or miss, or absurd overkill in some cases. (Signed overflow being a prime example where you really either just need to define what happens or accept that your compiler is basically never going to warn you about a possible signed overflow -- which is UB. The compromise here by Rust is to allow one to pick between some implementation defined behaviors. That seems pretty sensible.) reply uecker 12 hours agorootparentFor signed overflow I use -fsanitize=signed-integer-overflow . reply Quekid5 12 hours agorootparentGood. I wonder how many people do and also if their compilers support it. (One would hope so, of course. I assume clang and GCC do.) ... but the question is really what you ship to production. Btw, possible signed overflow was just an example of things people do not want warnings for. OOB is far more dangerous, obviously... and the cost for sanitizer in that case is HUGE... and it doesn't actually catch all cases AFAIUI. reply gpderetta 1 hour agorootparentFor OOB you can enable bound checking in the C++ standard library. That's relatively cheap. Of course it won't help with C raw pointers and C array. reply chc4 13 hours agoparentprevI have had the exact opposite experience: clang constantly gives me much better error messages than GCC, implementations of some warnings or errors catch more cases, and clang-tidy is able to do much better static analysis. reply kolbe 7 hours agorootparent\"Copilot explain this error\" has made this whole discussion irrelevant for me. reply estebank 3 hours agorootparentAn issue is immediacy: problems are better the earlier they are pointed out (why online errors are better than compile errorswl, which are better than CI errors, which are runtime errors). Having to copy paste an error adds a layer of indirection that gets in the way of the flow. Another is reproducibility and accuracy: LLMs have a tendency to confidently state things that are wrong, and to say different things to different people, the compiler has the advantage of being deterministic and generally have better understanding of what's going on to produce correct suggestions (although we still have cases of incorrect assumptions producing invalid suggestions, I believe we have a good track record there). If those tools help you, more power to you, but I fear their use by inexperienced rustaceans being misled (an expert can identify when the bot is wrong, a novice might just end up questioning their sanity). Side note: the more I write the more I realize that the same concerns I have with LLMs also apply to the compiler in some way and am trying to bridge that cognitive dissonance. I'm guessing that the reproducibility argument, ensuring the same good error triggers for everyone that makes the same mistake and the lack of human curation, are the thing that makes me uneasy about LLMs for teaching languages. reply someplaceguy 13 hours agoparentprevClang has a similar tool, the Clang Static Analyzer: https://clang-analyzer.llvm.org/ reply darby_eight 9 hours agoparentprevI'm quite surprised that clang doesn't have static analysis! That doesn't seem right, but I don't program much in C anymore. reply bluGill 9 hours agorootparentIt does. However it catches some different things* reply bvrmn 1 hour agoprevIt's really great. Shear amount of work is huge. It seems difficulty level is on par with introducing fat pointers/array views into stdlib and C standard. reply perihelions 16 hours agoprev36 more comments in this other thread: https://news.ycombinator.com/item?id=39918278 (\"GCC 14 Boasts Nice ASCII Art for Visualizing Buffer Overflows (phoronix.com)\", 2 hours ago) reply Davidbrcz 15 hours agoprevI wish there was a better output format for the analysis, because this is hell for screen readers. reply dmalcolm 15 hours agoparentFWIW I implemented SARIF output in GCC 13 which is viewable by e.g. VS Code (via a plugin) - though the ASCII art isn't. You can see an example of the output here: https://godbolt.org/z/aan6Kfxds (that's the first example from the article, with -fdiagnostics-format=sarif-stderr added to the command-line options) I experimented with SVG output for the diagrams, but didn't get this in good enough shape for GCC 14. reply 1udfx9cf8azi0 16 hours agoprevif (nbytessizeof(*hwrpb)) But I think the correct fix is: if (copy_to_user(buffer, hwrpb, sizeof(*hwrpb)) != 0) It never makes sense to copy out of the hwrpb pointer any size other than sizeof(*hwrpb). reply pwagland 16 hours agoparentRight, but the size of the buffer is given, it doesn't make sense to stomp over end of the callers buffer either, so you can't use pass in something longer than `nbytes` either. reply 1udfx9cf8azi0 15 hours agorootparentThat's what the original check is for: if (nbytessizeof(*hwrpb), your version causes the kernel to only write part of the buffer, and then when the app accesses fields at the end of the struct, it would read uninitialized data, which is very bad. Recall that the API is intended to be used like this: struct hwrbp buf; getsysinfo(GSI_GET_HWRPB, &buf, sizeof(buf), /* .. */); At first glance, it might seem unnecessary to pass the buffer size at all, because in theory the user and kernel should agree on what the sizeof(struct hwrbp) is. But the reason it is passed is because there are various reasons why the separately compiled kernel and user binaries might disagree (e.g., incorrect compiler flags, wrong header file being used, struct has changed between different versions, etc.), and it's useful to detect that. So you can make an argument that the most conservative check is: if (nbytes != sizeof(\\*hwrpb)) return -1; After all, if the user and kernel disagree on the correct size of the struct, then something is wrong! But allowing nbytesNo, because if nbytes > sizeof(*hwrpb), your version causes the kernel to only write part of the buffer, and then when the app accesses fields at the end of the struct, it would read uninitialized data, which is very bad. > I would agree with you if the kernel had some other mechanism to pass the size of the buffer that was actually filled to the client (like e.g. the read() syscall does) but the getsysinfo() API doesn't return that data, so the kernel must either fill the buffer entirely or return failure. As you mention, this struct is versioned. Userspace can tell how much of the struct was filled by checking the size field (hwrpb->size). > But allowing nbytesThe original less-than check was deemed incorrect It was only deemed incorrect because of an information leak. Not because it's a valid use-case for user space to copy smaller portions of *hwrpb into user space. https://github.com/torvalds/linux/commit/21c5977a836e399fc71... reply noam_k 17 hours agoprevVery cool stuff! I haven't done much C development lately, so I'm curious how often `strcpy` and `strcat` are used. Last I checked they're almost as big no-nos as using goto. (Yes, I know goto is often preferred in kernel dev...) Can anyone share on how helpful the c-string analyses are to them? reply jandrewrogers 16 hours agoparentThe use of goto is unambiguously correct and elegant in some contexts. Unwavering avoidance of goto can lead to unnecessarily ugly, convoluted code that is difficult to maintain. It usually isn't common but it has valid uses. While use of functions like `strcpy` are less advisable, there are contexts in which they are guaranteed to be correct unless other strong (e.g. language-level) invariants are broken, in which case you have much bigger problems. In these somewhat infrequent cases, there is a valid argument that notionally safer alternatives may be slightly less efficient for no benefit. reply xedrac 15 hours agorootparent> The use of goto is unambiguously correct and elegant in some contexts. For C, absolutely. For C++, it's likely a footgun. reply jandrewrogers 14 hours agorootparentIt has fewer use cases in C++ but it still has use cases where the alternatives are worse. reply sirwhinesalot 16 hours agorootparentprevstrcpy and friends don't really have any benefits beyond just being there. The \"safer\" versions are still unsafe in many cases, while being less performant and more annoying to use. Writing a strbuffer type and associated functions isn't particularly hard and the resulting interface is nicer to use, safer, and more efficient. reply bvrmn 1 hour agorootparentI argue strview (non-owning) is almost always what is needed. Most of string operations are searching and slicing. reply sirwhinesalot 40 minutes agorootparentYou also need a strview. Not really relevant for avoiding strcpy and strcat though. reply i80and 16 hours agoparentprevSome usage of goto is still idiomatic in C if used in ways logically equivalent to structured programming constructs C lacks. It requires some care, but I mean, it's C. (I'm not however fond at all of longjmp) reply sirwhinesalot 17 hours agoparentprevThere's nothing wrong with simple usages of goto. The strxcpy family on the other hand is complete garbage and should never be used for any reason. I'm horrified that they're used in the kernel at all. All of those functions (and every failed attempt at \"fixing\" them) should have been nuked from orbit. reply Ysraes 7 hours agorootparentThis is the approach taken in git https://github.com/git/git/blob/master/banned.h reply rdtsc 16 hours agorootparentprev> There's nothing wrong with simple usages of goto Indeed a like a few gotos here and there for doing cleanup toward the end of the function. reply sirwhinesalot 16 hours agorootparentOr to break out of nested loops. The problem is with unstructured goto spaghetti making the code impossible to follow without essentially running it in your head (or a debugger). Goto + Switch (or the GCC computed goto extension) is also a wonderful way to implement state machines. reply laweijfmvo 17 hours agorootparentprevWhat's wrong with `strncpy`? reply i80and 17 hours agorootparentstrncpy won't always write a trailing nul byte, causing out of bounds reads elsewhere. It's a nasty little fellow. See the warning at https://linux.die.net/man/3/strncpy strlcpy() is better and what most people think strncpy() is, but still results in truncated strings if not used carefully which can also lead to big problems. reply sirwhinesalot 16 hours agorootparentSpeaking of strlcpy, Linus has some colorful opinions on it: > Note that we have so few 'strlcpy()' calls that we really should remove that horrid horrid interface. It's a buggy piece of sh*t. 'strlcpy()' is fundamentally unsafe BY DESIGN if you don't trust the source string - which is one of the alleged reasons to use it. --Linus Maybe strscpy is finally the one true fixed design to fix them all. Personally I think the whole exercise is one of unbeliavable stupidity when the real solution is obvious: using proper string buffer types with length and capacity for any sort of string manipulation. reply jjav 13 hours agorootparent> the real solution is obvious If it were obvious it would have been done already. Witness the many variants that try to make it better but don't. > using proper string buffer types with length and capacity Which you then can't pass to any other library. String management is very easy to solve within the boundaries of your own code. But you'll need to interact with existing code as well. reply sirwhinesalot 12 hours agorootparent> If it were obvious it would have been done already. Witness the many variants that try to make it better but don't. Every other language with mutable strings, including C++, does it like that. It is obvious. The reason it is not done in C is not ignorance, it is laziness. > Which you then can't pass to any other library. String management is very easy to solve within the boundaries of your own code. But you'll need to interact with existing code as well. Ignoring the also obvious solution of just keeping a null terminator around (see: C++ std::string), you should only worry about it at the boundary with the other library. Same as converting from utf-8 to utf-16 to talk to the Windows API for example. reply jjav 5 hours agorootparent> The reason it is not done in C is not ignorance, it is laziness. Of course not. C has been around since the dawn of UNIX and the majority of important libraries at the OS level are written in it. Compatibility with such a vast amount of code is a lot more important than anything else. If it were so easy why do you think nobody has done it? > Ignoring the also obvious solution of just keeping a null terminator around That's not very useful for the general case. If your code relies on the extra metadata (length, size) being correct and you're passing that null-terminated buffer around to libraries outside your code, it won't be correct since nothing else is aware of it. reply sirwhinesalot 44 minutes agorootparent> If it were so easy why do you think nobody has done it? People have done it, there are plenty strbuf implementations to go around. Even the kernel has seq_buf. How you handle string manipulation internally in your codebase does not matter for compatibility with existing libraries. > That's not very useful for the general case. If your code relies on the extra metadata (length, size) being correct and you're passing that null-terminated buffer around to libraries outside your code, it won't be correct since nothing else is aware of it. You can safely pass the char* buffer inside a std::string to any C library with no conversion. You're making up issues in your head. Don't excuse incompetence. reply lelanthran 4 hours agorootparentprev> you should only worry about it at the boundary with the other library. If this was a mitigation, it would solve all problems with nul-terminated strings i.e. do strict and error-checked conversions to nul-terminated strings at all boundaries to the program, and then nul-terminated strings and len-specified strings are equivalently dangerous (or safe, depending on your perspective). The problem is precisely that unsanitised input makes its way into the application, bypassing any checks. reply sirwhinesalot 37 minutes agorootparentIt's impossible to avoid \"sanitizing\" input if you have a conversion step from a library provided char* to a strbuf type. Any use of the strbuf API is guaranteed to be correct. That's very different from needing to be on your toes with every usage of the strxcpy family. reply jandrese 3 hours agorootparentprevFor me the \"real\" solution looks something like this: ssize_t strxcpy(char* restrict dst, const char* restrict src, ssize_t len) Strxcpy copies the string from src to dst. The len parameter is the number of bytes available in the dst buffer. The dst buffer is always terminated with a null byte, so the maximum length of string that can be copied into it is len - 1. strxcpy returns the number of characters copied on success, but can return the following negative values: E_INVALID_PARAMETER: Ether dst or src are NULL or len = n || ret = 0 && cpThus, I suspect, if you don't call that \"f...f...frob my C program\" function known as setlocale, it will never happen. Of all the footguns in a hosted C implementation, I believe setlocale (and locale in general) is so broken that even compilers and library developers can't workaround it to make it safe. The only other unfixable C-standard footgun that comes close, I think, are the environment-reading-and-writing functions, but at least with those, worst-case is leaking a negligible amount of memory in normal usage, or using an old value even when a newer one is available. reply kazinator 2 hours agorootparentI see that in Glibc, snprintf goes to the same general _IO_vsprintf function, which has various ominous -1 returns. I don't think I see anything that looks like the detection of a conversion error, but rather other reasons. I would have to follow the code in detail to convince myself that glibc's snprintf cannot return -1 under some obscure conditions. Defending against that value is probably wise. As far as C locale goes, come on, the design was basically cemented in more or less its current form in 1989 ANSI C. What the hell did anyone know about internationalizing applications in 1989. reply aulin 4 hours agorootparentprevexcept no one does that return code check and worse they often use the return code to advance a pointer in concatenated strings reply lelanthran 4 hours agorootparentprevI actually do use `snprintf()` and friends. reply sirwhinesalot 17 hours agorootparentprevIt doesn't guarantee that the output is null terminated. Big source of exploits. reply jlokier 15 hours agorootparentprev`strncpy` is commonly misunderstood. It's name misleads people into thinking it's a safely-truncating version of `strcpy`. It's not. I've seen a lot of code where people changed from `strcpy` to `strncpy` because they thought that was safety and security best practice. Even sometimes creating a new security vulnerability which wasn't there with `strcpy`. `strncpy` does two unexpected things which lead to safety, security and performance issues, especially in large codebases where the destination buffers are passed to other code: • `strncpy` does NOT zero-terminate the copied string if it limits the length. Whatever is given the copied string in future is vulnerable to a buffer-read-overrun and junk characters appended to the string, unless the reader has specific knowledge of the buffer length and is strict about NOT treating it as a null-terminated string. That's unusual C, so it's rarely done correctly. It also doesn't show up in testing or normal use, if `strnlen` is \"for safety\" and nobody enters data that large. • `strncpy` writes the entire destination buffer with zeros after the copied string. Usually this isn't a safety and security problem, but it can be terrible for performace if large buffers are being used to ensure there's room for all likely input data. I've seen these issues in large, commercial C code, with unfortunate effects: The code had a security fault because under some circumstances, a password check would read characters after the end of a buffer due to lack of a zero-terminator, that authors over the years assumed would always be there. A password change function could set the new password to something different than the user entered, so they couldn't login after. The code was assumed to be \"fast\" because it was C, and avoided \"slow\" memory allocation and a string API when processing strings. It used preallocated char arrays all over the place to hold temporary strings and `strncpy` to \"safely\" copy. They were wrong: It would have run faster with a clean string API that did allocations (for multiple reasons, not just `strncpy`). Those char arrays had the slight inconvenience of causing oddly mismatched string length limits in text fields all over the place. But it was worth it for performance, they thought. To avoid that being a real problem, buffers tended to be sized to be \"larger\" than any likely value, so buffer sizes like 256 or 1000, 10000 or other arbitrary lengths plucked at random depending on developer mood at the time, and mismatched between countless different places in the large codebase. `strncpy` was used to write to them. Using `malloc`, or better a proper string object API, would have run much faster in real use, at the same time as being safer and cleaner code. Even worse, sometimes strings would be appended in pieces, each time using `strncpy` with the remaining length of the destination buffer. That filled the destination with zeros repeatedly, for every few characters appended. Sometimes causing user-interactions that would take milliseconds if coded properly, to take minutes. Ironically, even a slow scripting language like Python using ordinary string type would have probably run faster than the C application. (Also Python dictionaries would have been faster than the buggy C hash tables in that application which took O(n) lookup time, and SQLite database tables would have been faster, smaller and simpler than the slow and large C \"optimised\" data structures they used to store data). reply lelanthran 15 hours agoparentprev> Last I checked they're almost as big no-nos as using goto. I don't think so. Gotos are fine, strcat and strcpy without a malloc with the correct size in the same scope is a code smell. reply saagarjha 16 hours agoparentprevgotos are fine if used judiciously. strcpy and strcat are “fine” in that they work when you know your code is correct and you have big problems if you don’t. But this describes most of C, unfortunately. reply dmit 11 hours agorootparent> gotos are fine if used judiciously Is there a language feature that is not? :) reply randomdata 16 hours agoparentprev> Last I checked they're almost as big no-nos as using goto. Huh? Why is goto a no-no? It is there for good reason. I think we all agree with Dijkstra that, in his words, unbridled gotos are harmful, but C's goto is most definitely bridled. I doubt any language created in the last 50+ years has unbridled gotos. That's an ancient programming technique that went out of fashion long ago (in large part because of Dijkstra). reply bluGill 15 hours agorootparentLanguages other than C give you options for flow control so that you don't need goto for that. It is a spectrum, if you only use goto to jump to the end of a small function on error it is okay, though I prefer something better in my language. I've seen 30,000 line functions with gotos used for flow control (loops and if branches) - something you can do in C if you are really that stupid and I think we will all agree is bad. This 30,000+ line function with gotos as flow control was a lot more common in Dijkstra's day. reply randomdata 14 hours agorootparentWe all agree that you shouldn't write bad code. Not using goto, not using any language construct. But when unbridled gotos were the only tool in the toolbox, bad code was an inevitability in a codebase of any meaningful size. Not even the best programmer was immune. This is what the \"Go to statement considered harmful\" paper was about. It was written in 1968. We listened. We created languages that addressed the concerns raised and moved forward. It is no longer relevant. Why does it keep getting repeated in a misappropriated way? reply bluGill 9 hours agorootparentIn 1968 they had better languages and programmers were still using goto for control in them despite better options. reply randomdata 3 hours agorootparentOf course. The ideas presented in said paper went back at least a decade prior, but languages were still showing up with unbridled gotos despite that. But that has changed in the meantime. What language are you or anyone you know using today that still has an unbridled goto statement? reply lelanthran 15 hours agorootparentprev> Languages other than C give you options for flow control so that you don't need goto for that. The idiom `if (error) goto cleanup` is about the only thing I see goto used for. What flow control replaces that other than exceptions? reply sirwhinesalot 14 hours agorootparentJumping out of nested loops. Implementing higher level constructs like yield or defer. State machines. Compiler output that uses C as a \"cross-platform\" assembly language. All of them are better served with more specialized language constructs but as a widely applicable hammer goto is pretty nice. I don't expect C to have good error handling or generators any time soon but with goto I can deal with it. reply nickpsecurity 13 hours agorootparentCompiling HLL constructs in some of those scenarios ultimately produces a jump statement. So, it makes sense that a higher-level version of a jump would be helpful in the same situations. reply cozzyd 10 hours agorootparentprevRAII + destructors Though gcc supports cleanup functions, just not very ergonomically. reply randomdata 14 hours agorootparentprev> What flow control replaces that other than exceptions? defer has gained in popularity for that situation. reply jjav 13 hours agorootparentprev> 30,000 line functions with gotos The problem there is the 30K line function, not the goto! reply bluGill 9 hours agorootparent30k functions are a problem but they are manageable if goto isn't used in them. I prefer not to but a have figured them out. reply jjav 5 hours agorootparentWow! Longest single function I can think of having written is ~200 lines. I always feel bad when editing it but there's no useful way to break it down so I let it be. But a single 30,000 line function? Wow. reply umanwizard 16 hours agorootparentprevgoto used in certain idiomatic ways (e.g. to jump to cleanup code after an error, or to go to a `retry:` label, or to continue or break out of a multiply nested loop) is fine. What's annoying is bypassing control flow with random goto spaghetti. reply saagarjha 16 hours agoprevVery nice. I’m glad to see these all have detailed reports explaining what’s wrong! reply mgaunard 14 hours agoprev-Wstringop-overflow is the first warning I disable because of all the false positives. I doubt the analyze variant would fare any better. reply bregma 10 hours agoparentIsn't sort of like pulling the battery out of your carbon monoxide detector because the constant beeping is giving you a headache and making you sleepy? reply gpderetta 7 minutes agorootparentNo. -Wstringop-overflow is really broken with a huge amount of false positives. At $JOB we disable it on a line by line basis, but I'm not sure it is worth the effort. reply aulin 4 hours agoprev [–] now we want a GCC language server! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article delves into enhancements in static analysis within the GCC 14 compiler, emphasizing the -fanalyzer static analysis pass, showcasing new capabilities like identifying infinite loops and tackling security weaknesses through taint analysis.",
      "It focuses on technical improvements rather than promoting Red Hat products, mentioning Compiler Explorer and upcoming GCC versions.",
      "The conclusion provides links to Red Hat's blog and store, covering diversity, equity, and inclusion topics."
    ],
    "commentSummary": [
      "A discussion on enhancing static analysis in GCC on redhat.com delves into comparing error messages across languages such as C, Rust, and NixOS, pinpointing challenges and enhancement suggestions.",
      "The significance of precise error messages, warning accuracy, tool recommendations, and concerns regarding Language Model models are explored in the debate, emphasizing addressing vulnerabilities and boosting coding standards.",
      "The conversation also scrutinizes the usage drawbacks of functions like strcpy and strlcpy in C, along with the utilization of \"goto\" statements and flow control in older programming languages, advocating for leveraging modern language features for improved code quality."
    ],
    "points": 319,
    "commentCount": 102,
    "retryCount": 0,
    "time": 1712152468
  },
  {
    "id": 39922006,
    "title": "Terraform's Breakthrough: Carbon Neutral Natural Gas Production",
    "originLink": "https://terraformindustries.wordpress.com/2024/04/01/terraform-makes-carbon-neutral-natural-gas/",
    "originBody": "Terraform makes carbon neutral natural gas We did it! After two years of hard work we hold in our hands hard proof that the incredible team at Terraform can make synthetic natural gas from sunlight and air, as reported in TechCrunch. Last Wednesday March 27, 2024, we commissioned our integrated power-to-gas demonstrator. The system converts electricity and air into affordable fossil-free natural gas using proprietary subsystems, developed in-house, for hydrogen electrolysis, direct air capture, and methanation. This marks a major milestone in the technical and commercial development of our technology and another step on the path to our positive vision for the future where everyone has access to plentiful cheap, reliable energy, harvested from renewable carbon and power, with lower ecological and climate impact. Our core innovation is the utilization of proven and well-understood industrial processes implemented within innovatively simple and cost-effective proprietary technology systems adapted to the daily variations of renewable power for producing cheap sustainable natural gas from sunlight and air. These design and process innovations are responsible for driving production costs down for both their green hydrogen and atmospheric CO2 capture to levels previously thought unattainable. Combining super-cheap solar power with low-cost green hydrogen and CO2 capture are the key to producing commodity-grade electro-fuel alternatives at competitive commodity prices. Our elite team of engineers have made significant progress towards our stated goals of radically cheaper green hydrogen and direct air capture CO2. We follow a simple playbook. Adapt a known industrial process to ultra-cheap solar power by a) radically reducing capex to compensate for lowered utilization and b) developing processes that are robust to variations in power due to weather, as well as diurnal and seasonal power shifts. First, our innovative electrolyzer converts cheap solar power into hydrogen with current production costs at less than $2.50 per kg of H2. How? To first order, our electrolyzer capex costs 97% methane (CH4). Our reactor uses no rare or expensive catalysts nor cherry-picked data – it handily operates for hours in steady state. The Terraformer combines these three subsystems into one modular chemical plant, skid-mounted for deployment alongside a solar PV array. It is particularly cool to develop and build all three of our sub-systems in our castle in Burbank, CA, right across from the long-time home of the fabled Lockheed Skunk Works. In-house production & manufacturing enables faster iteration, lowered supply chain risk, and the successful demonstration of the complete techno-economic model for producing cost-effective fossil-free natural gas. While I generally prefer understatement, it is hard to overstate the significance of this achievement, which has been two years in the making. Humanity today depends on traditional oil and gas to provide them with a cheap and easily transportable source of energy-dense hydrocarbons, to the tune of 100 million barrels per day, which unfortunately also generates ~50 gigatons per year of CO2 emissions. The Terraformer unlocks an industrial-scale source of renewable hydrocarbons that breaks our dependence on fossil fuels and lays the foundation for another century of spectacular growth, broad-based wealth creation, and global environmental restoration. What’s next? We’re taking reservations for our first production Terraformers, and ramping production to meet demand. Share this: Twitter Facebook Like Loading... Related April 1, 2024 terraformindustries",
    "commentLink": "https://news.ycombinator.com/item?id=39922006",
    "commentBody": "Terraform makes carbon neutral natural gas (terraformindustries.wordpress.com)314 points by jseliger 14 hours agohidepastfavorite264 comments carbonguy 13 hours agoAs a \"carbon industry\" observer, this is pretty exciting news. I've had my eye on Terraform Industries for a while and love what they're doing; they're one of the few groups that actually seem to understand the implications of what it will take to shift to a carbon-neutral economy, and their core insight about the economics of atmospheric fuel synthesis is one of those \"obvious when you hear it\" ideas: solar electricity is trending ever-cheaper, so rather than trying to maximize efficiency in an expensive piece of kit you can make cheap 'inefficient' equipment and get lower overall costs, which in turn unlocks scale. Their recent post on \"Terraformer Environmental Calculus\" is a great read, if you are interested in this space: https://terraformindustries.wordpress.com/2024/02/06/terrafo... Congratulations to the team! reply joe_the_user 13 hours agoparentIt looks good. I wish their headline was \"natural gas from solar power\" 'cause many things labeled \"carbon neutral\" wind-up being conventional petrochemicals plus some worthless \"offsets\" baloney. reply tgtweak 12 hours agorootparentIn theory you can use any power source - hydroelectric, geothermal, nuclear, wind - but the benefit to solar is that you can fully utilize it when the sun is shining and store the output (compressed natural gas) for storage, transportation and use off-hours. You could also technically use this as a grid-battery, taking in excess grid energy when it is cheap and converting it into natural gas that can be run back through a gas peaking plant that spins up to meet peak demand. You could also look into SOFC fuel cell plants [1] to convert the stored natural gas into electricity at 60% and heat at 30% (the heat is high temperature which is good for cogeneration or as a direct heat source). There would need to be some very large spreads in margin on those to make up for the fact you're likely double-dipping on inefficiencies when going from electricity in -> natgas production -> storage -> generation -> electricity out. On that same note though - in some free and open energy markets it is not unheard of to buy at $200/MWh at peak on-demand - plenty of margin for arbitrage there - as the tesla megapack facilities have demonstrated in Australia. In comparison a 4MWh megapack facility (2MW in/2MW out) is priced at $1.9M before installation [2] [1]https://assets.bosch.com/media/en/global/stories/sofc/solid-... [2]https://twitter.com/SawyerMerritt/status/1643488856946122754... (updated for M/Mega - thanks) reply martinald 10 hours agorootparentYes exactly. In the UK power is becoming increasingly negative in pricing when there is high wind + solar output. This is actually increasingly alarmingly rapidly (and there is 3-5GW of offshore wind, plus loads of utility scale solar coming online). Prices are then very high when wind and solar is low - which happens to be when demand is the highest (cold weather snaps in winter which tend to result in very low windspeeds). National Grid is already paying £1bn/yr to turn off wind farms when supply is too high (plus paying a fortune for new nat gas peakers, which are limited by law to run for 10 days a year max). It's projected that curtailment payments to wind farms will reach £4bn/yr. While some of this will be rectified with more transmission capacity (there is a 4GW offshore HVDC link being built between scotland and england), if the claims of terraform are true and hold up at scale, I think this is the actual breakthrough people have been looking for. These could be connected to substations near wind farms (which also happen to be near major gas interconnectors from the north sea) and generate when power prices were low or negative, which will be a large amount of the time. They'd then get paid not only for the arbitrage in gas prices but also they would be able to take (most/all) of the curtailment payments national grid is paying the wind farms. To be clear batteries do not work particularly well for a market like the UK. Batteries work well for overnight storage of solar, they do not work well for northern climates like the UK that require weeks of storage of power to cover low renewable output in winter. That's not to say there isn't loads of batteries being constructed right now, there is, but it's to cover very short term movements in supply and demand - the much harder problem is covering days or weeks of low output. reply arrowsmith 3 hours agorootparentI don't get it, why does it cost billions to turn the wind farms off? Why isn't it free? reply japanuspus 3 hours agorootparentBecause some of the windfarms still run on legacy contracts where they were incentivized by a guaranteed selling price for the power they produce. They are reimbursed for the loss of income they suffer due to curtailment. reply throwaway2562 2 hours agorootparentCorrect. The UK regulator (Ofgem) and thus UK consumers are being taken for a ride by windfarmers, who made these deals a precondition of building farms. We will be paying them £2.5bn a year to not generate electricity by 2030 https://www.nsenergybusiness.com/features/examining-challeng... ‘A wholly unsatisfactory state of affairs’ indeed https://www.ref.org.uk/ref-blog/372-why-are-unsubsidised-win... reply _Microft 12 hours agorootparentprev> On that same note though - in some free and open energy markets it is not unheard of to buy at $200/mWh at peak on-demand - plenty of margin for arbitrage there - as the tesla megapack facilities have demonstrated in Australia. In comparison a 4mWh megapack facility (2MW in/2MW out) is priced at $1.9M before installation [2] m means milli, M is mega. reply mschuster91 11 hours agorootparentprev> You could also technically use this as a grid-battery, taking in excess grid energy when it is cheap and converting it into natural gas that can be run back through a gas peaking plant that spins up to meet peak demand. The loss in such a cycle is abysmal, alone from thermal loss (not to mention the loss during compression and decompression) - even straight fuel cells are at 60% round-trip, compared to batteries with >>90% efficiency. reply pydry 10 hours agorootparentBatteries are good at smoothing out daily or even weekly variations but do not make any sense whatsoever for seasonal power storage. It's ridiculously cost ineffective to charge a battery in July only to discharge it in December. 60% roundtrip is cost effective if you're synthesizing when the sun is blazing and the wind is blowing hard and burning it when wind, solar and batteries have all tapped out. Thats especially so if the equipment has low capex which it seems like this does. Unlike batteries that makes it cost effective to overbuild and idle it most of the year. reply ben_w 13 minutes agorootparent> It's ridiculously cost ineffective to charge a battery in July only to discharge it in December Indeed. This is why the cheapest solutions in most places are a mix of a mere few days off storage plus a target production level that is a little higher than you need on an average day in winter. While this doesn't work above the arctic circle (you could do it with a power line somewhere sunnier or a synthetic fuel, and possibly also geothermal or nuclear etc., devil is in the details for all options) overproduction + 35-90 hours of batteries is sufficient for most people and places: https://tonyseba.com/wp-content/uploads/2020/11/RethinkingEn... reply cbsmith 3 hours agorootparentprevIs that more effective than the traditional, \"store the energy as kinetic potential energy by pushing water uphill, so you can let it go down the hill later\" approach. reply danmaz74 24 minutes agorootparentIt's not more effective where you have the right conditions for pumped storage, but those conditions aren't very common around the world. reply pydry 15 minutes agorootparentI'm curious about where you read this. I see this idea that pumped storage geography is rare pop up a lot on Hacker News but I don't know where it's coming from and it rarely seems to come with citations. If you look at this map, you'll see that unlike, say, dam-appropriate geography, it's actually pretty common: https://www.energytransitionpartnership.org/uploads/2023/05/... reply genman 3 minutes agorootparentFigure 2 doesn't agree with you. It's common but not geographically equally distributed. pydry 1 hour agorootparentprevFor storing energy cheaply for months at a time and transporting it across large distances, yes. Pumped storage has ~90% roundtrip efficiency, good at storing energy for days or weeks but maxes out easily. The energy density of water pushed uphill is very low. I think we should be pushing a lot more water uphill, but I see it as an alternative to or competitor to grid-scale batteries and a complement to syngas. Syngas production will probably be most useful if built next door to a wind or solar farm and used to siphon off energy which is currently curtailed when the grid is maxed out. It can then be easily stored in enormous quantities and easily transported by ship to anywhere in the world that needs it. reply cbsmith 3 hours agorootparentprevThat's basically what this is, minus the \"worthless\" part. That's the part you'd have to read past the headline to be sure of anyway. reply toddmorey 9 hours agoparentprevI really wonder about unintended consequences. It's exciting to be able to store solar as methane because we can \"plug\" this new synthetic methane easily into existing infrastructure. (But we have to get better at leak management!) However, you almost always go through huge underground methane pockets when drilling for oil. So oil drilling stations vent / flare methane when they can't \"off site\" it, like when natural gas pipelines are at capacity. In those moments, the price of methane actually drops below zero--I've seen it at -$1.20 per MMBtu as recently as this year! Essentially you are paying someone to get rid of the stuff for you. So... if we flood the market with new, cost-effective synthetic methane... will companies just flare more of it as we drill for oil? reply rndmize 6 hours agorootparentClimate town just did a piece on natural gas leaks and how its a much more serious problem that previously considered - https://www.youtube.com/watch?v=K2oL4SFwkkw - certainly soured me a bit on natural gas in general, at least until there's better regulation in place. reply jvanderbot 8 hours agorootparentprevI believe the intent is to offset oil production with the methane. Somehow. Maybe making other hydrocarbons from it? reply throwaway4220 9 hours agorootparentprevWell it’s still carbon neutral though right reply _aavaa_ 9 hours agorootparentCarbon molecules yes. But methane was a significantly higher (80x) greenhouse gas potential than CO2. So it's definitely not neutral in that regard. reply NortySpock 8 hours agorootparentRight, but the point is, once this costs nearly-the-same as methane extracted from the ground, it's not worth it to pull methane out of the ground! We'd stop having an incentive to add more CO2 to the atmosphere! We'd be able to get to net-zero carbon / methane emissions without having to substantially change our living conditions. Cities or states would \"just\" bottle up some liquid methane for the winter months (or summer months) and seasonal energy usage changes become much easier to manage. (I'm aware that would involve creating more infrastructure.) Sounds great to me. reply toddmorey 7 hours agorootparentYes but the issue is we almost always encounter & have to remove lots of natural underground methane to get to the oil below it. If the oil drills lose incentive to sell the methane off, they just burn it on site as waste. Horrible I know. So synthetic methane also needs to reduce crude oil demand I’m thinking. reply cbsmith 3 hours agorootparentI don't think it does, though that would be great. Burning the methane on site as waste is still significantly better than releasing it into the air (converting methane to mostly CO2 is still better than not doing it). This technology doesn't need to solve global warming. Even if it just buys us some more time, it is fantastic news. reply yetihehe 3 hours agorootparentIf you could get that methane and use it for something productive, what would you propose? I'm looking for ideas, some process that has relatively easy to transport equipment (no expensive big buildings which have to be demolished when oil field is depleted), energy intensive and makes some valuable product with that energy. If someone has any wild/interesting ideas in this space, I'd like to hear it. reply _aavaa_ 7 hours agorootparentprev> once this costs nearly-the-same as methane Problem #1 is finding people to pay for it until then. Problem #2 is that this will make fore expensive energy at the end, efficiency being one problem and capital cost of those idle gas turbines being another. We'll have to wait and see if these ever plan any role beyond a demonstration project or two, but I'm skeptical it'll compete on price. reply jseliger 13 hours agoparentprevOther discussion: https://news.ycombinator.com/item?id=39896560 reply groby_b 13 hours agoparentprevIt's an excellent way forward because it's not only carbon-neutral, it can also \"fall back\" to pure CO2 capture should we ever get a decent enough grid & storage mechanisms to afford that. Really exciting work! reply throwaway_5753 8 hours agoparentprevWhat is going on with the section numbering in that blog and infographic? Super interested in the content I can't focus when we start with section 7 then jump to 9 then 13 then back to 8. reply jvanderbot 8 hours agoparentprevThis all strikes me as a nice opening to an apocalypse movie. Pulling CO2 and upgrading to methane - wouldn't that amplify GHG effects? reply strken 7 hours agorootparentMethane has a short half-life in the atmosphere, so its cumulative effect after a few decades is close to CO2 unless you're constantly outputting more, and that's assuming you're not burning it. In the case where you're making methane from atmospheric CO2 and then burning it, it's just returning the same CO2 back, which per the article is carbon neutral. reply CorrectHorseBat 3 hours agorootparentMore like a few centuries, methane's global warming potential after 500 years (GWP-500) is still 7.95. The issue with burning is that methane can leak during transport. reply strken 2 hours agorootparentI stand corrected: did the maths wrong. Centuries it is. reply julienchastang 12 hours agoprevTheir minimalist website is awesome: https://terraformindustries.com/ complete with plain text chemical equations. \"Why does our website look like this? At TI we believe we can change the world by displacing fossil hydrocarbon production at global scale. Like our website, our machines are simple so we can build millions of them as quickly as possible. Our website embodies our cultural commitment to allocating resources where they solve the most important problems.\" reply abraae 10 hours agoparentI particularly love this: > Are you an excellent recruiter? We get a lot of inbounds. To help us qualify your ability to match our needs, please send us your strongest candidate, a singular champion, as an exemplar of your talent hunting skill. After a lifetime of doing HR software this sparks thoughts about a scoreboard/ranking system for agencies with exactly this kind of \"you've got one chance - don't blow it\" scenario. Maybe once you've provided 5 great candidates you're allowed a dud or two. reply basil-rash 8 hours agorootparentTo me that just sounds like “we don’t want to pay you anything but do all the work for us anyways”. Is it supposed to be some tongue in cheek way of saying “fuck off if you’re a recruiter we don’t give a shit”? Or are they instead asking for a historical example of a good hire you facilitated? It’s akin to saying “Oh you’re a systems software engineer? Prove it by designing and implementing our entire system for us, we won’t pay you for it, we just want to know you can do it”. reply dylnuge 5 hours agorootparentIt's not akin to that at all. External recruiters are typically paid for placement (i.e. they get a commission paid on a hired employee after they're hired, typically 20–30% of base salary). There are good ones, but a non-trivial portion are just cold-email spamming everyone whose LinkedIn profile looks even remotely like it might match the jobs they're recruiting for and forwarding on anyone who replies. For a company/hiring manager, one great candidate a month is far more valuable than a few dozen ones who aren't even close to matching the role. They're not saying \"do the work for us\", they're saying \"if you want us to take your sourcing seriously, send us quality, not quantity.\" reply abraae 7 hours agorootparentprevI don't think it's a fuck off message at all. That would be just \"fuck off\". Why encourage recruiters if you didn't want to deal with them? To me it says \"don't just shotgun me with every rando candidate you have - I'll give you one chance, make it count\". It's exactly the sort of thing you might say to a pushy recruiter you met at the bar who wants in to your business. To succeed, they must a) have a good candidate and b) understand your business (so they know that candidate is right for you). And I'd say there's an enormous difference between what a software engineer does (spend months or years designing and implementing a system that is for a single \"customer\") to what a recruiter does (spend 1 hours of conversations/emails in qualifying a candidate who can be shopped out to any of a number of customers.). reply basil-rash 4 hours agorootparentBut if the recruiter has already gone through the work of vetting the candidate, why would the company not just talk to the candidate directly and “cut out the middleman”, so to speak? What guarantee does the recruiter have that providing the company this holy grail candidate will be met with anything more than “wow good eye, thanks!” reply abraae 2 minutes agorootparentThat's not usually how business works. Many reasons, but talent is still hard to find and employers don't want to piss off someone who can find staff for them. And it's likely against the agreement the recruiter has with the candidate for the candidate to go direct to the employer. diffeomorphism 2 hours agorootparentprevThat sounds hilariously out of touch. Like \"15 years experience required, rockstar developer, entry level position, passion not salary\". No, you are not that special and I will send a \"champion\" to a much better employer, thanks. reply lrem 3 hours agoparentprevThis is basically https://motherfuckingwebsite.com/ But you know what, http://bettermotherfuckingwebsite.com/ They already have a style element in there. The whole thing becomes readable by just adding a width property. Much better when centered. Done. reply cqqxo4zV46cp 11 hours agoparentprev“Like our machines, our website looks awful on a phone.” Someone could’ve spent literally 5 minutes making this look reasonable on the world’s most popular web browsing device form facto, whilst still retaining the site’s retro virtue signalling aesthetic, AND it wouldn’t have taken away from their ‘core mission’ or whatever. If you don’t care, don’t have a website at all. reply ihumanable 9 hours agorootparentOn my phone the website looks essentially identical to how it looks on my desktop browser. Does it look different on yours. It's just a plaintext website with white text on a black background and it's fast to load and easy to read for me. What's wrong with it? reply Lvl999Noob 2 hours agorootparentprevI only looked at the linked article and the homepage so maybe diagrams and such look worse, but those pages look fabulous on phone to me. I wish more sites looked like this one. reply mulmen 10 hours agorootparentprevSometimes the children really are wrong. reply tsukurimashou 35 minutes agorootparentprevusing a phone to browse the web is really sad reply dullcrisp 10 hours agorootparentprevWho left you in charge? reply kalkr 9 hours agorootparentprevman wtf does virtue signaling even mean anymore reply hackerlight 7 hours agorootparentprevUnstated benefit of the website is it repels the right people. reply rbliss 13 hours agoprevReally exciting for Terraform to hit this mark. Hopefully true, and not an April 1st joke. Casey Handmer (founder) is a pretty interesting guy. He also helped with the initial analysis of the crackle on the Vesuvius challenge scrolls that contributed to the breakthrough of reading the first passages from the scrolls. See https://caseyhandmer.wordpress.com/2023/08/05/reading-ancien... Highly recommend checking out more articles on the Terraform Industries blog and Casey's personal blog. reply oaktowner 9 hours agoparentYeah, kind of a terrible day to publish something like that (through no fault of their own, of course!). reply mattjaynes 12 hours agoprevFor more, I'd recommend Jason Carman's recently released videos on Terraform: Overview and tour (~20min): https://www.youtube.com/watch?v=NngCHTImH1g Deeper Interview (~40min): https://www.youtube.com/watch?v=ekEdq6PhC0Q reply consumer451 7 hours agoparentThank you so much for these recommendations! This is excellent and I highly recommend everybody watches both, starting with shorter one. Casey, the CEO is super interesting and inspiring. reply kartoffelmos 13 hours agoprevHow will this address methane leakage, which is a substantial source of athmospheric greenhouse gas emissions? Converting Co2 to methane is not carbon neutral if/when leaked... reply skrebbel 13 minutes agoparentPut a terraformer next to every leak to suck it back out of the air! reply rich_sasha 12 hours agoparentprevNatural gas is already stored in enormous quantities, and shipped around the world. Leakage appears to happen mostly in transit, or at poorly maintained installations. My guess is, it's a solvable problem. reply Moto7451 11 hours agorootparentSolvable, probably. Has oversight and a commitment to fix the issue? Not yet. https://youtu.be/K2oL4SFwkkw?si=zbr_fr5TDK2EHT72 (Topical and recent take from a favorite YouTube channel of mine) The industry provided (self reported) estimates of linkage is a little over 1%. The realistic value is over 2% and is at the point that coal and natural gas are likely equally bad for the environment given our current infrastructure. Carbon neutral is a useful feature but doesn’t solve that problem. I will say I am a fan of carbon neutral methane in place of the efforts to move to hydrogen combustion (this is a thing) and hydrogen for fuel cells since there isn’t a commercially viable carbon neutral version of that yet. Making existing methane infrastructure cleaner and less leaky is better, in my mind, in the path to solar/wind/nuclear electrification than trying to capture the emissions of coal or retool petroleum infrastructure into hydrogen. reply imbusy111 11 hours agorootparentWe have a bunch of new methane monitoring satellites, so it is very observable and enforceable. reply Moto7451 8 hours agorootparentI agree we can do it. It just has to be done. Aka give the EPA some teeth and require actual monitoring and enforcement rather than a self reported fantasy. reply ajross 10 hours agorootparentprev> Carbon neutral is a useful feature but doesn’t solve that problem. Uh... yes it does? A fossil-fuel free fuel production process that loses 2% to the atmosphere is still fifty times lower-impact (when considering output energy -- obviously production costs are different) than one that pulls 100% out of the ground. reply Moto7451 8 hours agorootparentSee the video I linked or the other commenters with references that Methane is a higher contributor to global warming than CO2 by 30x. It’s not about the carbon removed, it’s about the methane leaked. reply ajross 8 hours agorootparentEven granting that number (and the 2% loss above), Terraform still wins. It's just math, sorry. I mean, sure: this might be bad on balance. But it's starting from a position of overwhelming assumed advantage. You need to come to the table with analysis actually showing it's bad, and all you have is \"it's only about twice as good and not 50x better\". reply kamray23 1 hour agorootparentprevSolvable, yes, but at least in Europe it is currently dirtier than anthracite coal due to leakages in lifetime emissions. Solvable but not solved, and we really should be looking for solutions. reply MetaWhirledPeas 5 hours agorootparentprev> Natural gas is already stored in enormous quantities, and shipped around the world. Leakage appears to happen mostly in transit And if you can make natural gas from solar that means there will be more sources of natural gas and thus less transit. Right? reply BWStearns 11 hours agoparentprevDoesn't methane get reacted away in a relatively short (≈years) period of time? reply marax27 1 hour agorootparentIt kind of does, but calculating its actual lifetime or warming effect is tricky. This post explains it quite nicely: https://climateer.substack.com/p/methane-lifetime reply stephen_g 11 hours agorootparentprevNo, that’s the problem. The GWPs usually quoted are over 20 years (GWP 81.2) and 100 years (27.9), but over 500 years it’s still around a GWP of 8. Depends on your definition of a ‘long time’ but it’s not like it reaches the low single digits even after 500 years… reply realreality 11 hours agorootparentprevAtmospheric methane oxidizes into CO2 and water vapor. Then the CO2 hangs around for much longer... https://en.wikipedia.org/wiki/Atmospheric_methane#Natural_si... reply Taniwha 5 hours agorootparentyes eventually, but until it does it is a far worse greenhouse gas than CO2 reply daedrdev 10 hours agorootparentprevIt converts to more CO2, and is also worse than CO2 during the short time reply zdragnar 10 hours agorootparentWhich is irrelevant if the input carbon was atmosphere sourced anyway. reply wolfram74 8 hours agorootparentno, it isn't. Over the course of a few decades, a kilogram of methane has the greenhouse impact of ~50 kilograms of CO2 because of the different absorption modes methane has that CO2 doesn't. If terraform leaks more than a few percent of their product into the atmosphere from carelessness (who cares about tracking such a low value commodity?) Then this is worse than just leaving the CO2 in the atmosphere. I really wish they'd address this fact more. Some possibilities: -The hydrogen half is still a great way to make hydrogen for industrial processes -The methane can be used on site of production for organic chem feedstocks -Many new rockets are using methane as their fuel, using it at point of origin instead of transporting through leaky pipelines and trucks. Any analysis along these lines would be reassuring that this isn't going to be a net-negative, climate wise. reply tombert 13 hours agoparentprevYou beat me to it, because I was going to say the same thing; wouldn't it only be carbon neutral if you had some means of guaranteeing that there's no leaks? It seems like converting CO2 to methane could actually make things worse... reply bamboozled 12 hours agorootparentIf we're converting CO₂ that already exists in the atmosphere, how could it make things worse? Isn't problem making new CO₂ from burning fossil fuels, not converting existing Co2 into other things? reply arcanemachiner 12 hours agorootparentMethane is a 30x stronger greenhouse gas than CO2. So if 4% of the converted methane leaked into the atmosphere, you would be worse off (from a climate heating perspective) than if you had done nothing. https://www.frontiersin.org/journals/plant-science/articles/... reply bamboozled 12 hours agorootparentOk thanks. Well we're super fucked then because I've read that all the freaking / coal seam gas infra leaks million of tons of methane every year. reply aaronax 12 hours agorootparentprev30x strong on what measurement? Per molecule? How much CO2 is Terraform Industries pulling from the atmosphere to create that 4% of methane that might be leaking? reply mojomark 10 hours agorootparent>>...on what measurement? By CO2 equivalence in the context of greenhouse gas potential [1]. And, the 30X factor is only valid if you look over scales of atmospheric persistence of 100yrs. If you look at scales of 10years (the amount of time methane persists, the GHG potential strength a over 80X. Liquid (Cryogenic) natural gas tankers and storage emit \"boiloff\" gas. Some of this can be burned in the 'dual fuel' propulsion engines when combined with a small amount of diesel \"pilot\" fuel, but not all and I'm uncertain at what quantity. Even engines that burn NG gave methane \"leak by\" that escapes into the atmosphere. It's not great, and no, nobody is enforcing containment via satellites at the LNG shipping level (despite a comment to the company trary above). Hydrogen production is a much better option for dense energy storage option. 1.) https://climatechangeconnection.org/emissions/co2-equivalent... reply ordu 10 hours agorootparentprevSome of Terraform's production will leak, the most of it will be burned, so the real outcome is a little more methane in athmosphere. reply jtsiskin 12 hours agorootparentprevYes if you're considering the terms 'carbon neutral' at face value; but in terms of global warming, methane traps a lot more heat per carbon atom reply slashnode 12 hours agorootparentprevI think the point is that methane is a more potent greenhouse gas than CO2, so if leaked, the net greenhouse impact is greater than that of the original CO2 that went into the process reply joking 12 hours agorootparentprevBecause methane is worse than co2 as a greenhouse gas reply tgtweak 12 hours agorootparentprevEven if it all leaks out, you're still back at 0 essentially. reply wmf 12 hours agorootparentNo, because methane is worse than CO2. Anyway, replacing fossil methane with synthetic is strictly better so the leaks are irrelevant. reply rstat1 10 hours agorootparent>>No, because methane is worse than CO2. Should've just stopped here. Being synthetically produced doesn't magically make it not a potent greenhouse gas. reply boxed 3 hours agorootparentThe big source of methane is cows anyway. This system needs to scale to ridiculous levels to come even close to all the cows. reply api 12 hours agorootparentprevMethane has a much shorter atmospheric half life than CO2-- years as opposed to millennia. It does end up getting oxidized into CO2 and H2O, just not nearly as quickly as when it's burned. Leaks would happen to a small degree, but since a leak represents money drifting away there's a strong incentive to fix them. Methane leaks of any size are fairly easy to detect. There's been an effort to put up satellites for this purpose. If using this technology helps us to phase out fossil fuels, it would be a huge net win. This could effectively let us repurpose all our existing natural gas storage, transport, and generation infrastructure into a battery to store surplus renewable or off-peak nuclear energy. This could also allow renewable energy to be shipped as LNG, allowing the gigantic amounts of solar power in places like the Sahara to be harnessed and exported. The only other way to do this is extremely long distance superconducting or incredibly high voltage transmission lines that would probably be more expensive and very vulnerable. reply martinald 10 hours agorootparentActually there's a project underway to build a 4000km HVDC line from Morocco to the UK: https://xlinks.co/morocco-uk-power-project/ I thought it was a pipe dream but has got some fairly big backers now and does seem to be moving along. reply cool_dude85 11 hours agorootparentprev>Leaks would happen to a small degree, but since a leak represents money drifting away there's a strong incentive to fix them. This is true of existing natural gas infrastructure, and yet... reply purple_ferret 10 hours agoparentprevIn a similar vein, what if producing natural gas this way makes it cheap enough that companies choose to just vent it off when pursuing other fossil fuels. reply krastanov 9 hours agorootparentWouldn't putting the execs of these companies in jail be an easy legal solution? reply darby_eight 9 hours agoparentprevI don't see them patching over refinery methane leaks guerilla-style if that's what you're asking. reply mkobit 12 hours agoparentprevI've been watching a YouTube channel named Climate Town that recently did a video talking about natural gas, and leakage was a focus point in the video. It's like a documentary-style comedy channel, and I quite enjoy both the content and the format. It reminds me a bit of Jon Stewart and John Oliver. Video link: https://youtu.be/K2oL4SFwkkw reply Latty 13 hours agoparentprevThis was my thought, methane is much worse than carbon when not burned, but just released. Any claims of carbon neutrality that rely on assuming perfect storage and transport without leakage are fantasy. If anything, when you are calling it \"easily transportable\" at the same time, as they do, you are actively misleading. You can't have both: it's either easily transportable and you are accepting a bunch of methane released (and thus terrible for climate change), or it's carbon neutral and you are baking in the cost of making sure it doesn't leak in transport/storage (and thus not easily transportable). They are having their cake and eating it too by claiming both. reply __MatrixMan__ 12 hours agorootparentI'd be interested in a quantitative analysis here. Methane is much worse, but sunlight has broken most of it down after a decade or so. CO2 is comparatively forever. Presumably there's a point where the lines cross and leaking green methane is still a win. I guess it just comes down to where those lines cross and whether we deem that an acceptable goal. reply CrazyStat 12 hours agorootparentSunlight breaks methane down into CO2, so I don’t think there’s much of a win there. reply __MatrixMan__ 12 hours agorootparentBut that methane \"started\" as CO2 in the atmosphere, so after the breakdown happens you're carbon neutral. Carbon capture and storage are still relevant topics, but \"stop making the problem worse\" is a good start. If we all switched to still-leaky synthetic methane today, things would continue getting worse only until the atmospheric breakdown rate equalled the leak rate. That's still a decade of things getting worse, but it's possible that the alternatives are even more problematic. I'm not saying it's the right or wrong path, I haven't done that analysis, I'm just saying that approaches to it could use a bit more pragmatism. reply scq 12 hours agorootparentprevOf course there is. CO2 and methane are not equally bad, methane is about 100x worse initially. After about 60-70 years the lines cross and the impact of a tonne of methane released is less than a tonne of CO2 released at the same date. reply CrazyStat 12 hours agorootparent> After about 60-70 years the lines cross Methane's global warming potential is estimated to be ten times CO2 after 500 years [1]. > and the impact of a tonne of methane released is less than a tonne of CO2 released at the same date. A ton of methane released into the atmosphere breaks down into 2.75 tons of CO2 [2]. There is no possible way that this statement can be true. [1] https://www.ipcc.ch/report/ar6/wg1/downloads/report/IPCC_AR6..., Table 7.15 (page 1017) [2] Simple stoichiometry: CH4 (weighing ~16 g/mol) breaks down into CO2 (weighing ~44 g/mol) at a 1:1 ratio. 44/16 is 2.75. reply __MatrixMan__ 8 hours agorootparentWow that table is over my head, I spent a fair bit of time trying to unwind the acronyms but I gave up. Can you help me understand how a gas with a lifetime of 11.8 years is having a different impact on the climate at 500 years than it did at say... 11.8 years? That's 488.2 years of being in the same state as where it started prior to the carbon capture that made the CH4. reply CrazyStat 6 hours agorootparent1. 11.8 years is a halflife, not a \"all the methane is gone after 11.8 years\" lifetime. 2. Methane doesn't just warm the atmosphere up a little bit and then disappear with no side effects. In addition to carbon dioxide, methane decomposition creates ozone and water vapor, which are both greenhouse gases. The additional heating effects of these decomposition byproducts are also included in the global warming potential calculations. 3. We care about cumulative effects over time. GWP is \"how much additional heat will the atmosphere absorb because of this gas over X amount of time\", scaled relative to carbon dioxide (so CO2 always has a GWP of 1). Methane's GWP-20 is about 80, which means that if I release one ton of methane today, over the next 20 years it will absorb about as much heat as if I had released 80 tons of CO2 instead. The longer the time frame the less bad methane looks, because it mostly decomposes, but even over a 500 year time frame releasing 1 ton of methane absorbs as much additional heat as if you had released 10 tons of CO2 instead. GTP is similar to GWP except it's about how much global average temperatures will rise instead of how much heat is absorbed. 4. If you can create methane out of atmospheric CO2 for free, you can subtract 2.75 from each of the GWP numbers for methane (since you remove 2.75 tons of CO2 to create one ton of methane). This is essentially what the table is showing on the CH4-non fossil line (notice each of the GWPs on this line is 2.8 less than on the CH4-fossil line). ----- Imagine I had a magical gas called timelockium. It is not a greenhouse gas (no radiative forcing), but after exactly 10 years it decomposes to an equal mass of CO2 with no other byproducts. The GWP-10 for this gas would be zero: over the first ten years, releasing a ton of timelockium is equivalent (in terms of heat absorbed by the atmosphere) to releasing zero tons of CO2. The GWP-20 for this gas would be 0.5: over the first twenty years, releasing a ton of timelockium is equivalent to releasing 0.5 tons of CO2. This is because it does nothing for the first ten years, and then for the next ten years it is just CO2 [1]. For longer time frames, the GWP of timelockium would approach 1. Over 500 years, emitting a ton of timelockium would be nearly equivalent (0.98) to emitting a ton of CO2. --- Now I have another magical gas, decayium. It is equivalent to CO2 for 10 years and then magically disappears. Again it has no other side effects or byproducts. The GWP-10 of decayium would be 1--over the first 10 years it's identical to CO2. Over the next ten years it contributes nothing to warming, so the GWP-20 would be 0.5. For longer time frames the GWP of decayium would approach 0. the GWP-500 would be 0.02. --- Superdecayium is like decayium except much worse. It's equivalent to 100x as much CO2 for the first ten years and then magically disappears with no side effects or byproducts. The GWP-10 is 100. The GWP-20 is 50. The GWP-500 is 2. This last scenario is more analogous to methane, except methane chemistry is much more complicated, with gradual decay and byproducts that are also greenhouse gases. Like superdecayium, methane's GWP decreases over longer time intervals, but even over 500 years it is still worse than an equivalent mass of CO2. --- [1] For the sake of simplicity I'm ignoring CO2 dynamics here, assuming it's just static in the atmosphere. reply __MatrixMan__ 5 hours agorootparentThat was very well explained, thank you. At 11.8 years it seemed like it would be worth considering because the total amount of anthropogenic CH4 would find equilibrium relatively soon, and that would be better, at some point, than continuing to emit new CO2 year after year. But at 80 years... all of that infrastructure that the synthetic methane people are excited to reuse... It'll have been decommissioned by then anyway. We might as well just hold out for synthetic gasoline or double down on electric everything (both, probably). (This is all assuming that the leak problem is unsolvable. Not sure about that.) ---- You seem to know quite a bit about this stuff, so I have an unrelated question: Things are \"simple\" in this case because a degree of climate temperature increase provides a basis for comparison between different gasses. But sometimes I find myself thinking about tradeoffs between climate heating and other ecological harms. Like, I should probably get a dishwasher because they use less water than washing by hand, but what's the carbon footprint of manufacturing a new dishwasher? I suppose you could still standardize on a degree of heating, but you'd need to figure out how much wasted fresh water is equivalently harmful to a degree of heating. That's always going to be subjective to some degree, but not all subjects are created equal. I'd much rather just let some ecologists build consensus around a number and then take that number myself as an axiom. Does such a thing exist? reply solid_fuel 12 hours agorootparentprevBut that CO2 is then theoretically recaptured to make more fuel, right? At least if the carbon in the methane is sourced from the atmosphere in the first place. reply trueismywork 9 hours agorootparentprevYou are conflating carbon neutral with global warming neutral. reply pfdietz 13 hours agoprevOne can make natural gas from CO2 captured from air and with hydrogen electrolysed form water. But then one has to ask: why not just use hydrogen directly and skip the inefficiency and cost of direct air capture of CO2 and of making methane? If one is dead set on making a carbon-containing synfuel, why methane and not something more storable, like methanol or higher hydrocarbons? Or, why not use the hydrogen to deoxygenate biomass (including waste biomass like paper) instead of laboriously collecting completely oxidized CO2 from air? reply jillesvangurp 3 hours agoparentBecause the economics of storing and transporting hydrogen are just not great. You need a lot of energy and infrastructure to do it. And while the density per kg is great. The density by volume is absolutely terrible because hydrogen is the smallest and lightest molecule. In liquid and gas form it's nearly 3x the volume of methane (for the same amount of energy). If you are transporting it in compressed gas form (like is common to supply hydrogen vehicles), it's about 18x the volume of diesel. Needless to say, this doesn't scale for things like supplying fuel stations. In liquid form, it's better but we're still talking 3x that of methane and 4-5x that of diesel. And to keep hydrogen liquid, you typically boil some of it off continuously: it doesn't keep for very long. Methane has the same problem except the temperatures and pressures are such that it's not an enormous amount. Liquid hydrogen has to be chilled to a few degrees Kelvin. If you ship it around, you'll lose some non trivial amounts over the whole journey. Most hydrogen produced today is used onsite to produce something else for this reason. Moving it around just adds cost and complexity. Terraform seems to have made some progress with hydrolizers. But before people start popping champagne bottles, their synthetic gas still is an order of magnitude more expensive than natural gas. So, the value proposition of replacing one of the cheapest (but dirty) fuels with a clean alternative that is much more expensive is limited. And of course the process of using the captured carbon, releases all of it back to the atmosphere. For most use cases, switching to synthetic gas would push those use cases into the deeply unprofitable region. I.e. you'd be considering many other alternatives before committing to that. Green hydrogen at the price levels they are citing is ballpark getting to a stage where it could replace grey hydrogen and be worth the extra cost just to clean up existing uses of hydrogen. So, things like fertilizer production and other chemical processes. It's still more expensive but the difference could be bridged with subsidies and incentives. reply pfdietz 1 hour agorootparentThe economics of storing hydrogen for long periods is just fine, compared to the economics of storing energy in batteries for long periods. reply pbmonster 1 hour agorootparent> compared to the economics of storing energy in batteries for long periods Which doesn't say much, since practically nobody is doing that. reply jillesvangurp 1 hour agorootparentAlso the need for this long term storage is never quantified by those we need it. Put a twh number on it and the problem melts away. We're actually producing batteries by the twh per year now. reply carbonguy 13 hours agoparentprev> why not just use hydrogen directly and skip the inefficiency and cost of direct air capture of CO2 and of making methane? Broadly speaking, one key reason is that we've already got the infrastructure in place for using methane (and other hydrocarbons) whereas we do not have this for hydrogen. Another point is that this really isn't an either-or proposition: if people want hydrogen, then the Terraform electrolyzer can in principle provide it. reply ajb 13 hours agorootparentIn the UK, our gas infrastructure actually used to be used for hydrogen[1], so a lot of it should still work if we were to switch back. [1] Actually \"town gas\" which was a mixture of hydrogen and carbon monoxide. But I imagine not including the CO would not be a problem. reply _aavaa_ 9 hours agorootparentNeglecting the CO will absolutely be a problem. The material properties required for handling hydrogen are very different than those for other gasses, especially methane. Increasing the concentration of hydrogen significantly increases the difficulty. reply rbliss 13 hours agoparentprevAnother point others haven't made, which Terraform Industries points out: that using solar to make Natural Gas is going to be the cheapest form of natural gas production. You can effectively short circuit the existing fossil fuel industry and pull the hydrocarbons from the air instead of the ground to stay carbon neutral. No need to re-invent industry. reply __MatrixMan__ 12 hours agorootparentBut if the main problem with industry is that it leaks then maybe you do still have to reinvent it. reply NewJazz 12 hours agorootparentSeriously, how do you know whether the supply chain is truly using one process to synthesize, vs. Adding/cutting with extracted fuel. reply __MatrixMan__ 11 hours agorootparentOn the demand side, I imagine you'd be drawing fuel gas from a rig that's like, in your neighborhood, being powered by the solar panels on your roof. So you'd probably notice if there were tanker trucks showing up to fill it. On the supply side, you could use things like methane sniffing satellites (https://www.nature.com/articles/d41586-024-00600-z) to find cases of extraction and tax (at first) or arrest (eventually) those participating in it. reply konschubert 11 hours agorootparentprevIt doesn’t matter. This isn’t driven by consumer choice, it’s driven by production cost. At least that’s the idea. reply andrewla 13 hours agoparentprevOther commenters have pointed out that hydrogen may not be feasible, but the other direction -- methanol or higher hydrocarbons, seems way more interesting. Storage and density are orders of magnitude easier, and since we're synthesizing it from atmospheric component we don't have to be concerned with contaminants, especially sulfur compounds, that plague higher hydrocarbon use. Even better in a lot of ways would be to move to amorphous carbon; generating coal from atmospheric CO2 would be a huge win in transportability especially around safety and reliability dimensions. reply marcosdumay 7 hours agorootparentIt's quite hard to create heavy hydrocarbons in a controlled fashion. That company is already full of technical risks. Adding an extra optional one would be crazy. That said, if not them, somebody will probably crack that one problem in time. But it's best done as an independent development. reply papercrane 13 hours agoparentprevMy guess is that the answer is taking advantage of existing NG infrastructure. reply outworlder 9 hours agoparentprev> why not just use hydrogen directly and skip the inefficiency and cost of direct air capture of CO2 and of making methane? Because hydrogen sucks. Specifically, storing it sucks. You'll either compress or liquefy. That costs energy. It leaks and embrittles containers. One of the best ways of storing it in a stable manner is to combine with carbon atoms. Voila, natural gas. reply pfdietz 8 hours agorootparentYou repeat some of the anti-hydrogen memes. Hydrogen is actually quite storable, certainly much more easily stored than energy in batteries. The cost of underground storage caverns for hydrogen is two orders of magnitude cheaper than the cost of batteries storing the same energy. reply aldonius 8 hours agorootparentThat's great, but how do I take one of these underground caverns around with me? :P More seriously, I'm sure hydrogen will make a valuable contribution to the stationary grid-level energy storage situation. I'm not convinced about other applications. reply pfdietz 1 hour agorootparentYou don't, just as you don't take massive takes of compressed methane around with you (I know about CNG cars; they're not competitive with BEVs or liquid fueled cars.) Methane and hydrogen are competing in stationary applications. reply woeirua 7 hours agorootparentprevGuess what the cost of an underground storage cavity is for natural gas? Very cheap, because they're everywhere. reply pfdietz 1 hour agorootparentYes, but hydrogen has the advantage of not requiring CO2 to either be acquired or recovered. reply sollewitt 13 hours agoparentprevI would guess because natural gas has hundreds of years of us handling it, and we already have a lot of things from power generators to domestic driers that consume it. reply pydry 13 hours agoparentprevHydrogen is much more difficult to store. It escapes easily from any container you put it in and causes hydrogen embrittlement for any metal you try to use to contain it. Apparently it can be stored in abandoned salt mines and we probably will use them for that eventually but you dont always have one of those handy. reply pfdietz 13 hours agorootparentAll those points are overstated. reply groby_b 13 hours agoparentprevBecause we kinda know how to do natgas storage/transport/use at large scale, while we're not quite there on the hydrogen economy. Higher complexity hydrocarbons likely have higher synthesis costs, and if you want to enter the market, you probably want your pricing as competitive as possible. Same for the biomass approach. It is ultimately an approach striving for as much simplicity as possible. That's firmly baked in their culture, too - see their home page :) reply pfdietz 13 hours agorootparentWe use all that because natural gas is cheap. If hydrogen were cheaper than natural gas, we'd use that instead, at least for the industrial processes, and we'd switch away from combustion of a gas in the distributed residential/commercial applications. reply groby_b 12 hours agorootparentAnd if wishes were horses... ;) Kidding aside: That's the whole point. This is a project that can work without retooling infra all over the world. It's a drop-in replacement to get to carbon-neutral fuel. This solves problems right now, with limited investment. This isn't about perfection. This is specifically about \"works right now, cheap, no impact on the surrounding infrastructure\". (Also, strong doubts on \"we'd use hydrogen instead\", because storage is a beast. Until that's solved, there is no chance we'd use that) reply pfdietz 12 hours agorootparentI don't believe their cost figures (look above; their 250/t for CO2 doesn't jibe), and I think electrification of most residential/commercial uses will be the superior solution (it's more efficient even if the electricity is produced by burning natural gas). Industrially, hydrogen is just fine. After all, industry already manipulates vast quantities of hydrogen. The fossil fuel uses that are the most difficult to displace involve liquid fuels for transportation; natural gas has limited use there. reply airstrike 14 hours agoprevI'm more used to seeing the other Terraform on HN so I would have perhaps titled this \"Terraform Industries...\" reply playingalong 14 hours agoparent\"Terraform stinks\" seems to match both. reply _Microft 14 hours agorootparentNatural gas doesn’t stink. It’s the additives that are there for exactly the reason to be able to smell it if there is a leak. reply leesec 14 hours agorootparentprevWhy the hatred buddy? Does anyone on Hackernews like technology anymore? reply LastTrain 13 hours agorootparentIs not liking some software and making a mild joke about it considered hate now? reply trueismywork 9 hours agorootparentYes reply k8sToGo 13 hours agorootparentprevRust or bust! reply dvfjsdhgfv 14 hours agorootparentprev> \"Terraform stinks\" seems to match both. As compared to what, IaC-wise? reply marcosdumay 12 hours agorootparentLooks like a bad joke made by somebody confused about the composition of farts. reply whirlwin 13 hours agorootparentprevCDKTF. Or in other words, shit camouflaged by a pleasant clown outfit reply Borg3 57 minutes agoprevHmm, this is interesting, but I have few doubts about it. First, Metane is worst gas to come by. Its super greenhouse gas. Second, everyone screems about carbon neutral, but do those people understand carbon cycle on this planet? Its essential for plants. Less CO2 in atmosphere, plants growth is slower. If you start to compete with plants for it, it might have unforseen consequences. I hope someone will put some research on it. This is usual human aproach. Lets fix one thing, worry about consequences later. reply popol12 37 minutes agoparentWe've spend the last 2 centuries burning fossil fuels from the ground, significantly increasing the atmospheric CO2 levels. Capturing some of it back won't make a difference for plants. Also, when you burn methane, you release CO2 again. So it's not like there's a finite amount of CO2 to pump from the atmosphere, it will renew itself. reply maerF0x0 13 hours agoprevWhat are some applications where its worth taking the highest grade energy (electricity) and turning it into a lower grade (eg liquid fuels), rather than electrifying the application? Electricity to motion is significantly more efficient per input energy than burning fuel->heat-> gaseous expansion->drive a piston->convert to rotation chain. What am I missing about this? reply shawndrost 13 hours agoparentElectricity (during sunlight hours) is not the highest grade of energy. California threw away ~32TWh of electricity last year, with renewables at only 19% penetration. You can't give power away at noon in April; this fact is a significant barrier to building new solar. (See \"Learning is not enough\" and other academic research on that topic.) We also use fossil fuels as feedstocks for fertilizers and plastics, so there are very important power-to-X applications which don't involve inefficient combustion. Something like Terraform would probably have to exist in order to transition away from fossil fuels. (Source: I'm CEO at a startup with a very different take on the same problem.) reply boringg 8 hours agorootparentSo the argument is take excess solar power turn it into a fuel for use at another time. It's another take on energy storage. Question for me is how do you make the business model work. Is it a bet that lower cost to produce or bank that carbon tax on traditional fuels makes it more cost competitive? Or in your case is it that downstream users are looking to clean up their supply chain so will look into a contract for that benefit? To your point everything can't be electricity or alternatively fuels have different use cases. And it's certainly important to have a diversity of power sources especially as fuel has different attributes then electricity. reply EasyMark 6 hours agorootparentwe aren't very good at storing electrical energy just yet, but we have a couple hundred years of experience storing and using hydrocarbons. I'm sure you can read the pdf someone posted up above and get their take on it. reply gimmeThaBeet 6 hours agorootparentprevExactly, the tl;dr is energy storage, but I feel like it's sort of the Amazon approach. There was an interview where they asked Bezos why books as Amazon's first product, and his answer was basically: \"I read internet usage was going up by 2300 percent a year, so I decided to try and find a business that would make sense in that context.\" \"Cheap and ubiquitous solar power is coming, what products does the world need in that context to move away from extracting fossil fuels?\" Like a lot of things, current technology probably isn't there yet. But philosophically, if you wait for tech to catch up with your vision of the future, you might find yourself behind. reply OkayPhysicist 13 hours agoparentprevChemical fuels are dramatically easier to store and transport than electricity. Batteries are heavy and expensive compared to a tank. Time-dependent surplus electricity (which is basically a guaranteed situation on renewable-heavy grids) is basically free, which makes efficiency concerns approximately moot. Instead of letting excess capacity go to waste, you use it to create chemical fuel, which can be used either just as storage to be later burned in a peaker plant, or you can use it as fuel in mobile settings (trucks, planes, ships, etc) where energy density is important. reply spenczar5 13 hours agoparentprevYou’re missing density of storage. Gasoline and natural gas are in the region of 50 MJ/kg, while batteries are more like 10MJ/kg or lower. That 5x factor is what stops us from having electric airplanes. reply timerol 11 hours agorootparent10 MJ/kg is way overstated for batteries. Current Li-ion is around 1 MJ/kg (278 Wh/kg) reply maerF0x0 12 hours agorootparentprevI actually think the reason we don't have electric airplanes has to do with traction vs propulsion. Eg: spinning a jet engine to propel air backwards is very different than spinning a motor that is (through a series of solid objects) directly connect to the ground. and while a battery is only 1/5th density, the motors on a tesla deliver 3x the range per energy compared to a prius. (not true break even, but impressive that one of the most efficient hybrid ICE cannot compare KWh for KWh to a battery + electric) reply XorNot 9 hours agorootparentNo it's just energy density. Hydrocarbon fuels have more. There's also the benefit that getting lighter as you expend fuel increases range (batteries don't meaningfully change mass when they discharge). Nothing about electric powertrains causes any problems here: it's just hydrocarbon fuel is more energy dense. It's not inconceivable you could build a hybrid electric aircraft if a suitably high power hydrocarbon fuel cell was developed, since removing the combustion stages from a jet engine would simplify the design considerably. reply michael1999 12 hours agoparentprev- heating a zillion legacy homes using existing gas pipelines - running all the existing fertilizer and other chemical plants - gas peakers/backups for resiliency. Gas storage is much cheaper than batteries. Even if all new construction is electric, we have decades of infrastructure built around gas. Replacing the furnace in every German house with a heat pump just isn't going to happen in 10 years. reply megaman821 13 hours agoparentprevSeasonal storage of energy is likely only practical in fuel form. Also, there is no easy pathway to electricity to high temperature heat. reply maerF0x0 12 hours agorootparentI'm guessing you mean high temperature + high energy (ie lots of watts). Cause lasers can do high temp rn, right? reply trueismywork 9 hours agorootparentYes reply fransje26 2 hours agoparentprevApplications that need heat. Metallurgy, for instance. reply pbmonster 1 hour agorootparentYou can actually electrify much of metallurgy, and this is already happening. Things like cement kilns are much more difficult, on the other hand. reply abdullahkhalids 13 hours agoparentprevThe only way a technology like this will help get us to carbon zero is using it in places where there is not much hope of replacing electricity. While small elective airplanes have been demonstrated, it doesn't seem like we will be able to build, in the next couple of decades, large electric airplanes that can substantially replace current fuel powered ones. This would require the additional step of converting methane to jet fuel, but that is also a technology under development. That said, I personally think, in practice technology like this will only delay getting to net zero, because the existence of this will disincentivize investments in electrification. I recall Sun Tzu's claim that a force completely surrounded will fight fiercely, but if you give a way out, it will look to escape or retreat. reply orr94 8 hours agorootparent> I personally think, in practice technology like this will only delay getting to net zero, because the existence of this will disincentivize investments in electrification. I recall Sun Tzu's claim that a force completely surrounded will fight fiercely, but if you give a way out, it will look to escape or retreat. I think you’re misapplying Sun Tzu’s lesson. An attempt to completely and immediately replace the fossil fuel industry will be met with fierce resistance, and thus be more likely to fail. Whereas a more gradual approach (like carbon neutral-ish green hydrogen) gives much of the industry around fossil fuels an opportunity to survive longer, and perhaps gives you a better chance of success. Is this actually true? Maybe, maybe not. But that would seem to be the implication of Sun Tzu’s strategy. reply DarmokJalad1701 11 hours agorootparentprevYou could theoretically run jet engines on directly on methane (liquid or not) - though I am not sure how much work would be required to refit an existing one for this purpose. Methane is already used for running the turbo-pumps in rocket engines like SpaceX's Raptor or Relativity's Aeon-R. reply Plasmoid 4 hours agorootparentYou'd be talking about liquid methane to get the energy density you want. Handling a cryofuel would be quite hard and you'd be redesigning everything to ensure it kept working. reply trueismywork 9 hours agorootparentprevElectricity/energy consumption of developing world is going to be at least twice today's value. Better this than coal reply DenseComet 13 hours agoparentprevEnergy density is much higher, which is important for applications such as aviation. reply yodelshady 13 hours agoparentprevThere are ships that move over one billion kWh of LNG around at a time. At $100 to store a kWh of electrical energy, along with approximately ten times the cost to account for structures that can support and move the extra weight (that's AFTER allowing for less energy demanded in total)... shall I leave this as exercise to the reader? reply maerF0x0 12 hours agorootparentOk, so here you're referring to moving a lot of energy w/o wires/transmission lines? Is that a commonly needed thing? reply Danieru 6 hours agorootparentSouth Korea, Japan, China say hello. Much of the world by population and gdp is a ship ride away from their major energy suppliers. Japan imports about half their LNG from Australia. Next largest is Qatar. There are no power lines between Australia and Japan. reply logtempo 9 hours agorootparentprevcheck climate town last video on youtube. reply rini17 13 hours agoparentprevSeasonal storage. Large amounts of gas can be stored underground. reply marcosdumay 12 hours agoparentprevIf you are talking about thermodynamics, chemical energy is exactly of the same grade as electricity. reply maerF0x0 7 hours agorootparentNo thermodynamics. Like usefulness. Electricity can be uses for a lot of things. a mass heated to 50c is not, relatively, as useful. Plus low temp waste heat is abundant eg: from computing, or refrigeration, or exhaust pipes of engines. reply usrusr 5 hours agoparentprevTime travel. It's very expensive to move electricity into the future. Batteries are very cheap when you look at dollars per watt of onboarding/offboarding the one way time machine, but when you look at it dollars per Wh it does not look as rosy anymore. Still worthwhile for short hops. But for longer trips, you want to look at $/Whh and then batteries are atrociously expensive. Seasons are a thing and time travel is hard, even if it's only one way. reply dr_kretyn 13 hours agoparentprevLack of sockets is likely the main one. Then, lack of the long long cable to the nearest socket. reply willio58 13 hours agoprevWhile this is impressive to some degree I just today watched Climate Town’s “Natural Gas is Scamming America” (https://youtu.be/K2oL4SFwkkw) and the entire natural gas industry is tainted beyond belief for me now. It’s not like they were a positive thing in my view before, but on the whole natural gas seems to be just as if not more detrimental than coal for climate change mainly due to how much is leaked out but also how much energy it takes to ship natural gas to other countries. The idea of creating carbon neutral natural gas seems great, but can we maybe avoid holding energy in one of the most climate-change inducing gases out there? reply tedivm 11 hours agoparent> but also how much energy it takes to ship natural gas to other countries. Something like this would reduce the need to transport it to other countries, since you can manufacture it anywhere you want. Right now we're limited to where we can pull fossil fuels out of the ground, which means that it has to be transported from one place to another. That's not the case with atmospheric extraction. reply gorjusborg 12 hours agoparentprevIf the natural gas is captured from the atmosphere, a spill will be bad, but no worse than before the capture happened, right? reply luhn 12 hours agorootparentThey're capturing CO2 from the atmosphere, if they leak methane back out to the atmosphere that's much, much worse. Over a 20-year period, methane has ~80x the warming potential of CO2. reply logtempo 9 hours agorootparentprevwhat kind of natural gas? reply yurishimo 3 hours agorootparentThis is important to consider. The video is mostly a commentary on how CO2 is being handled while the methane released from fracking is largely ignored when evaluating if a well is carbon neutral. The video also argues that methane is much worse for the environment as a whole than CO2. I'm sympathetic to the argument, but I have a hard time believing that climate scientists have not already taken this into account with their calculations _somewhere_. Maybe I'm wrong, but at this point, the US is so far gone down the fracking/natural gas hole, I'm afraid backing out of it is almost untenable. The best we can hope for is better capture, storage, and utilization strategies. If we can find a way to create a machine similar to the Terraform Industries model, but focused on methane capture, perhaps we can reabsorb/store that as well. reply asmor 12 hours agorootparentprevBeing realistic, we're going to put any CO2 we manage to pull out of the atmosphere (if we can do it at cost at all) on a balance sheet. We already do this in various places. For instance, if you dedicate land to growing a forest in Germany to offset your carbon emissions, it is added to the emission trade balance of the country no matter if you intend for it or not - you can't offset. Hamburg Airport tried to do this as a publicity stunt, and only later noticed they're doing nothing for the CO2 bottom line. reply Latty 13 hours agoparentprevIndeed, and they claim it is \"easily transportable\": it seems to be the two are mutually exclusive, if you are going to shove it into the existing systems that leak methane habitually, it's horrific for climate change. reply logtempo 9 hours agoparentprevI feel the same vibe too about this. If methane leak is an issue, H2 leakage will be a bigger issue. It's far more prone to leakage and far more corrosive. https://link.springer.com/article/10.1007/s10311-021-01322-8 an interesting paper for you :) reply asmor 12 hours agoparentprevGreen hydrogen and other \"e-fuels\" are an attempt to keep the gas industries infrastructure alive and relevant. If you see someone break down the math it makes zero sense, it just sounds nice. reply dzhiurgis 11 hours agorootparentKeeping existing infrastructure is the whole beauty of it. Ships, pipelines, factories, supply chains that you can keep using if/until other technology replaces it. reply jefftk 8 hours agoprevWill the oil industry will copy the meat industry and say this is misleading consumers because it's actually artificial gas? reply ein0p 2 hours agoprevThere's got to be a catch to all this. If there was no catch, this would be the hottest startup in the world by far. reply sebstefan 1 hour agoparentI'm going to post my favorite graph, it's called the \"MAC graph\". It shows, in itemized investments, how much it will cost to reach net-zero emissions for the U.S. Instead of time, the X axis is how much you are willing to invest, in $/Ton of CO2 The types of investments are sorted by their respective cost ; the later they appear on the X axis, the lower their name is on the Y axis You can observe by yourself here that direct carbon capture is all the way to the bottom of the list https://www.edf.org/sites/default/files/2021-07/MACgraph_sim... Each dollar invested in Terraform would net you 100x the results if you invested it in wind, solar, nuclear, electric vehicles or heat pumps Here's Hank Green talking about this https://www.youtube.com/watch?v=1dRgCsZ1q7g reply skeledrew 13 hours agoprevI see this, and I like how it sounds on the surface. But I can't help but raise an eyebrow when I see \"carbon neutral\". Got me wondering how they'll be delivering the gas, if they won't be using trucks or building other infra that uses fossil fuel-consuming equipment. If they actually have resolved this end-to-end, super great! But otherwise, I feel like they're still ignoring crucial externalities. reply r3trohack3r 13 hours agoparent> Got me wondering how they'll be delivering the gas, if they won't be using trucks or building other infra that uses fossil fuel-consuming equipment. Sounds like you’ve answered your own question. Time for you to get to work on an end-to-end natural gas powered supply chain that can run on the natural gas they’re pulling out of thin air. Lots of fun problems for you to solve. A lifetime of fulfilling work ahead. reply skeledrew 6 hours agorootparentHah, interesting problems but already have my hands+brain full working on problems in my main area of interest. reply orthecreedence 12 hours agoparentprevOne thing comes to mind: don't transport it. It's sitting right next to a solar power plant...one that shuts down at night. Why not store the gas locally and use it as a battery when the sun isn't shining? reply triceratops 12 hours agoparentprevYou can run the trucks on liquified natural gas. Or run a natural gas power plant next to the Terraform module that can charge an electric truck's battery. reply SamBam 12 hours agoparentprevCouldn't these systems be built much closer to municipalities' distribution centers (compared to existing NG wells), and so require even less transportation than what currently exists? reply leesec 13 hours agoprevNatural gas is just the beginning, they'll be able to make any hydrocarbon in time reply _Microft 13 hours agoparentCasey is also very bullish on Musk and his Mars plans. I wouldn‘t be surprised if half of the plan is to have the tech ready to build the methane generation infrastructure for Musk‘s Mars colony. reply leesec 13 hours agorootparentIn fact he started this company after looking into how they're going to make gas when they get to Mars reply standeven 2 hours agoprev“… our electrolyzer capex costs The best part about using nuclear for fuel generation The main point of this operation is to utilize free surplus energy from solar and wind to store fuel for days where solar and wind can't produce enough. Free-as-in-beer surplus, since the energy would otherwise be wasted or sold at negative prices, like what we have been seeing lately in certain markets. Nuclear can always produce electricity, so converting to fuel has no benefits it's just a loss compared to using the electricity directly. Also, nuclear electricity is never free but always very expensive because you need a large amount of very highly educated people and expensive infrastructure to deal with it. >you can build a DAC fuel generator way out in the middle of nowhere. If things go very badly with a nuclear reactor, there's no such thing as a \"middle of nowhere\" that's far enough away. The fallout from Chernobyl made certain foods as far away as the arctic circle unsuitable for human consumption. reply rgmerk 10 hours agoparentprevIntermittent, yes. Expensive, no. It's just not. It's a bit more expensive in the US because, hey, protectionism, but solar panels are now by far the cheapest source of electricity on Earth if you don't care when that electricity is delivered, and cheaper again if you're happy to have it in the Sahara or northern Australia. Solar power is now so cheap it is hard to conceive of any way nuclear power could ever compete against it where its advantages (24-hour all-year power) aren't relevant. Terraform's secret sauce is that they are designing their equipment to be cheap so it doesn't matter that they're only utilising it maybe 30% of the time. reply rhelz 13 hours agoprevFraking has made natural gas so cheap, I can't see these being used for anything but virtue signaling. They claim a breakthrough in converting hydrogen to natural gas, but natural gas is by far the cheapest source of H2. And, given that methane is 80 times more potent of a greenhouse gas than CO2 is, is it really a good idea to be manufacturing it? Inevitably there would be leakage, and it wouldn't take much to leak enough gas to more than compensate for any C02 sucked out of the air. reply mikeatlas 13 hours agoparentTheir claim is that this method is still yet cheaper than drilling/fracking. reply pfdietz 13 hours agorootparentThe cost of natural gas on the Henry Hub is somewhere around $300/ton. A ton of natural gas requires 2.75 tons of CO2, so the cost of CO2 capture has to be well below $110/ton (and that ignores the cost of the hydrogen and the equipment for doing the methane synthesis.) They must be assuming large increases in natural gas prices or large CO2 taxes. I think it will be much easier to get the price/BTU of H2 down below the current price of natural gas than it would be to get synthetic methane down that cheap. (If they are assuming large CO2 taxes then it's probably a better business model to just collect CO2 from the air and sequester it.) reply aaronblohowiak 8 hours agorootparentI went to double-check your math and I don't see $300/ton on Henry hub.. the units are a little weird as it looks like the price is per million btu which is... 50 pounds? of lng, so to get the per-ton price we take the Henry hub price and multiply by 40 (2000 pounds per ton divided by 50 pounds per million btu)? with a 52-week high of 3.63, this would get to $177 per ton, which seems short of your $300/ton? Anyway, I ask because id love to learn where I went wrong with the math (my result makes your point stronger, not contradicts it...) reply pfdietz 1 hour agorootparentI think you're closer to right. I started with a wrong figure that I didn't double check. Thanks for catching the error. reply riku_iki 13 hours agorootparentprev> so the cost of CO2 capture has to be well below $110/ton they say $250/t in the article, but could you expand how you came to \"A ton of natural gas requires 2.75 tons of CO2\"? Where 1.75t of CO2 is disappearing in result? reply pfdietz 13 hours agorootparentMolecular weight of methane (CH4): 16 Molecular weight of CO2: 44 One molecule of CO2 is needed to get the carbon atom to make one molecule of methane. 44/16 = 2.75 The 1.75t of CO2 that went missing is the oxygen, which obviously isn't in the methane. reply riku_iki 12 hours agorootparentThank you, I am an idiot in chemistry ) reply rhelz 11 hours agorootparentprev> Their claim is that this method is still yet cheaper than drilling/fracking. They claim the green way to go is converting hydrogen into methane....check out the link for a company claiming the green way to go is converting methane to hydrogen: https://www.pnnl.gov/news-media/new-clean-energy-process-con... I rather suspect that taking an energy detour through methane either way is a red herring. I mean....the OP says that their whole process is powered by solar energy. So they are presupposing that solar energy is going to be WAAAYYYY cheaper than methane. Why not just use the solar power directly? reply rdedev 13 hours agoparentprevThe US is already extracting a lot of natural gas and leaking a ton of it. The main culprit of these leaks are those regulatory bodies who outsource their jobs onto the companies. Fund them better and enforce heftoer penalties and the issue of leaks can be minimized drastically reply noutella 13 hours agoparentprevLeakages are indeed an enormous problem. It’s estimated than more than 2% of the gas leaks ! reply alexb_ 13 hours agoparentprev> They claim a breakthrough in converting hydrogen to natural gas, but natural gas is by far the cheapest source of H2. Are you serious? Water is right there! reply jfengel 13 hours agorootparentWater is right there, but the hydrogen is really closely bound to the oxygen, and extremely hard to get out. It's considerably easier to get H2 out of methane. Unfortunately, that process also yields CO2, so it's not helping the greenhouse gas situation. Hydrogen proponents suggest a route where we start with \"blue hydrogen\" from natural gas. Then, when we've got a good H2 infrastructure going and a lot of excess electricity from solar, we can switch over to \"green hydrogen\" from water. Skeptics point out that this is incredibly stupid, and that \"hydrogen proponents\" tend to be closely in bed with the fossil fuel industry. It looks an awful lot like an excuse to delay the elimination of fossil-fuel replacements like wind and solar. reply pfdietz 13 hours agorootparentThere is this really dumb internet meme, that hydrogen is just there as a plot by the fossil fuel industry to keep selling fossil fuels. What this ignores is that hydrogen must still be made even in a post-fossil fuel economy. It's not optional. Production of ammonia requires hydrogen, and without ammonia-derived nitrogen fertilizer billions of people will starve. About half the nitrogen atoms in your body came from synthetic ammonia. The meme is really weird. In all other applications, we assume that fossil fuels will be displaced, by law and force if necessary. But somehow SMR will always be used to make hydrogen; the technology will somehow be immune to the forces that will be deployed against all other fossil fuel uses. It's really crazy when examined closely. reply _aavaa_ 9 hours agorootparentIt's a meme because most of the people peddling hydrogen as a solution don't look at replacing existing uses of hydrogen (e.g. ammonia or as a process chemical) but instead try to sell it for dead on arrival ideas like fuel cell cars, blending in the natural gas pipelines, or burning it directly in homes for heat. There's an order of magnitude more companies and hype around those use cases than there are around actually important ones. reply pfdietz 8 hours agorootparentThe important word is \"just\". In this meme, all uses of hydrogen are tarred with this brush. reply _aavaa_ 7 hours agorootparentAlright, people get overzealous by painting 100% of the projects that way when only 90% of them deserve it. reply XorNot 9 hours agorootparentprevNatural gas was also pitched exactly this way in 2008 or so. There's now a mountain of natural gas infrastructure being built, the the US is a major exporter, and for some reason people think the industry is going to just \"switch off\" that infrastructure when asked to. This is 30+ year design lifetime infrastructure. Investments were made on that basis: no one is going to turn off anything. Which is the best argument in favor of otherwise ludicrously inefficient power-to-gas storage schemes: if you could, by some miracle, undercut fracking extraction, then at the very least you'd only have to bankrupt the well-operators - not the pipeline, export terminals etc. But I'm extremely skeptical this is possible and it will be fought against dirtily (see the anti-wind power campaigners). reply pfdietz 8 hours agorootparentPower-to-hydrogen is often attacked for being inefficient, but the alternative presented by the anti-hydrogen people for dealing with seasonal variations and long dark-calm periods is to just overbuild solar and wind massively -- in which case, most of the time power from these is just being curtailed. Apparently using this excess power with 0% efficiency is preferable to using it with nonzero efficiency making hydrogen. reply XorNot 6 hours agorootparentThe issue is that hydrogen makes no sense except for transportation fuel, where it also makes no sense. There are other options for stationary energy storage, and hydrogen is amongst the worst. i.e. if we can tolerate high losses, iron-air flow batteries are a much more reasonable option[1] The thing is you pay for all of that pretty heavily - it's all more expensive with other drawbacks, but it's not nearly the complete pain that handling hydrogen is. [1] https://essinc.com/iron-flow-chemistry/ reply pfdietz 1 hour agorootparentFor long term energy storage, minimizing the capital cost of energy storage capacity is paramount. Round trip efficiency is not. On that relevant metric, hydrogen is very hard to beat, particularly if proper geology is available (salt formations for solution mined storage cavities). Costs less than $1/kWh are possible. Artificially heated geothermal may be competitive on that metric, but its RTE is likely to be even lower. reply elproxy 2 hours agorootparentprevYup, or things like sulphur thermal storage[1]. Also, hydrogen is pushed heavily by the fossil fuel industry, as it will provide another out for all their methane reserves (via steam reforming). [1]https://www.solarpaces.org/why-solar-sulphur-cycle-ideal-sea... reply rhelz 12 hours agorootparentprev100% of anything doesn't really go away. We still use horse-drawn vehicles for some purposes. But we didn't have to get rid of 100% of the horse-drawn vehicles to get rid of the mountains of horse shit which used to accumulate on city streets. reply _aavaa_ 9 hours agorootparentprevSkeptics also point out that the blue part of blue hydrogen is carbon capture and storage, and nobody seems to be willing to pay for the extra cost of that. reply rhelz 11 hours agorootparentprev> Are you serious? yes :-) Think about it: why did Saturn V use hydrogen and oxygen? Because burning hydrogen produces more energy by weight than any other chemical reaction. If putting hydrogen and oxygen together releases the most energy, then splitting water into hydrogen and oxygen would also take the most energy. Any other chemical reaction which yielded H2 would take less energy. reply 1 more comment... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Terraform has created a system producing synthetic natural gas from sunlight and air, rendering it carbon neutral through hydrogen electrolysis, air capture, and methanation technologies.",
      "This development is a crucial step in offering cost-effective, fossil-free natural gas, mitigating environmental impact significantly.",
      "The technology adapts industrial processes to renewable energy, enabling efficient production of green hydrogen and CO2 capture, with production system reservations now available to the public."
    ],
    "commentSummary": [
      "Terraform Industries is pioneering carbon-neutral natural gas production using solar power, tackling energy storage and transportation issues in the natural gas sector.",
      "Discussions analyze trade-offs between energy sources such as methane and hydrogen, technology like carbon capture, and the shift to sustainable energy options in different industries.",
      "The conversations delve into methane leakage concerns, efficacy of long-term energy storage, and their relation to global warming and greenhouse gas emissions."
    ],
    "points": 314,
    "commentCount": 264,
    "retryCount": 0,
    "time": 1712172900
  },
  {
    "id": 39920644,
    "title": "Google Introduces Jpegli: A Faster, More Efficient JPEG Coding Library",
    "originLink": "https://opensource.googleblog.com/2024/04/introducing-jpegli-new-jpeg-coding-library.html",
    "originBody": "opensource.google.com Events Projects Programs and services Documentation About Blog Google Open Source Blog The latest news from Google on open source releases, major projects, events, and student outreach programs. Introducing Jpegli: A New JPEG Coding Library Wednesday, April 3, 2024 The internet has changed the way we live, work, and communicate. However, it can turn into a source of frustration when pages load slowly. At the heart of this issue lies the encoding of images. To improve on this, we are introducing Jpegli, an advanced JPEG coding library that maintains high backward compatibility while offering enhanced capabilities and a 35% compression ratio improvement at high quality compression settings. Jpegli is a new JPEG coding library that is designed to be faster, more efficient, and more visually pleasing than traditional JPEG. It uses a number of new techniques to achieve these goals, including: It provides both a fully interoperable encoder and decoder complying with the original JPEG standard and its most conventional 8-bit formalism, and API/ABI compatibility with libjpeg-turbo and MozJPEG. High quality results. When images are compressed or decompressed through Jpegli, more precise and psychovisually effective computations are performed and images will look clearer and have fewer observable artifacts. Fast. While improving on image quality/compression density ratio, Jpegli's coding speed is comparable to traditional approaches, such as libjpeg-turbo and MozJPEG. This means that web developers can effortlessly integrate Jpegli into their existing workflows without sacrificing coding speed performance or memory use. 10+ bits. Jpegli can be encoded with 10+ bits per component. Traditional JPEG coding solutions offer only 8 bit per component dynamics causing visible banding artifacts in slow gradients. Jpegli's 10+ bits coding happens in the original 8-bit formalism and the resulting images are fully interoperable with 8-bit viewers. 10+ bit dynamics are available as an API extension and application code changes are needed to benefit from it. More dense: Jpegli compresses images more efficiently than traditional JPEG codecs, which can save bandwidth and storage space, and speed up web pages. How Jpegli works Jpegli works by using a number of new techniques to reduce noise and improve image quality; mainly adaptive quantization heuristics from the JPEG XL reference implementation, improved quantization matrix selection, calculating intermediate results precisely, and having the possibility to use a more advanced colorspace. All the new methods have been carefully crafted to use the traditional 8-bit JPEG formalism, so newly compressed images are compatible with existing JPEG viewers such as browsers, image processing software, and others. Adaptive quantization heuristics Jpegli uses adaptive quantization to reduce noise and improve image quality. This is done by spatially modulating the dead zone in quantization based on psychovisual modeling. Using adaptive quantization heuristics that we originally developed for JPEG XL, the result is improved image quality and reduced file size. These heuristics are much faster than a similar approach originally used in guetzli. Improved quantization matrix selection Jpegli also uses a set of quantization matrices that were selected by optimizing for a mix of psychovisual quality metrics. Precise intermediate results in Jpegli improve image quality, and both encoding and decoding produce higher quality results. Jpegli can use JPEG XL's XYB colorspace for further quality and density improvements. Testing Jpegli In order to quantify Jpegli's image quality improvement we enlisted the help of crowdsourcing raters to compare pairs of images from Cloudinary Image Dataset '22, encoded using three codecs: Jpegli, libjpeg-turbo and MozJPEG, at several bitrates. In this comparison we limited ourselves to comparing the encoding only, decoding was always performed using libjpeg-turbo. We conducted the study with the XYB ICC color profile disabled since that is how we anticipate most users would initially use Jpegli. To simplify comparing the results across the codecs and settings, we aggregated all the rater decisions using chess rankings inspired ELO scoring. A higher ELO score indicates a better aggregate performance in the rater study. We can observe that jpegli at 2.8 BPP received a higher ELO rating than libjpeg-turbo at 3.7 BPP, a bitrate 32 % higher than Jpegli's. Results Our results show that Jpegli can compress high quality images 35% more than traditional JPEG codecs. Jpegli is a promising new technology that has the potential to make the internet faster and more beautiful. By Zoltan Szabadka, Martin Bruse and Jyrki Alakuijala – Paradigms of Intelligence, Google Research Labels: codec , JPEG , Jpegli    Popular Posts Google Summer of Code 2023 accepted contributors announced! Google Summer of Code 2024 Celebrating our 20th Year! Rust fact vs. fiction: 5 Insights from Google's Rust journey in 2022 Magika: AI powered fast and efficient file type identification Mentor organizations announced for Google Summer of Code 2024 Archive GOOGLE PRIVACY TERMS",
    "commentLink": "https://news.ycombinator.com/item?id=39920644",
    "commentBody": "Jpegli: A new JPEG coding library (googleblog.com)307 points by todsacerdoti 16 hours agohidepastfavorite200 comments underlines 12 hours agoJPEGLI = A small JPEG The suffix -li is used in Swiss German dialects. It forms a diminutive of the root word, by adding -li to the end of the root word to convey the smallness of the object and to convey a sense of intimacy or endearment. This obviously comes out of Google Zürich. Other notable Google projects using Swiss German: https://github.com/google/gipfeli high-speed compression Gipfeli = Croissant https://github.com/google/guetzli perceptual JPEG encoder Guetzli = Cookie https://github.com/weggli-rs/weggli semantic search tool Weggli = Bread roll https://github.com/google/brotli lossless compression Brötli = Small bread reply billyhoffman 11 hours agoparentGoogle Zürich also did Zopfli, a DEFLATE-compliant compressor that gets better ratios than gzip by taking longer to compress. Apparently Zopfli = small sweet breat https://en.wikipedia.org/wiki/Zopfli reply occamrazor 1 hour agorootparentZopf means “braid” and it also denotes a medium-size bread type, made with some milk and glazed with yolk, shaped like a braid, traditionally eaten on Sunday. reply codetrotter 11 hours agorootparentprevThey should do XZli next :D And write it in Rust reply tialaramex 10 hours agorootparentAll of the data transformation (codecs, compression etc.) libraries should be in WUFFS. That's exactly what it's for, and unlike the C++ this was written in, or indeed Rust, it's able to provide real compile time safety guarantees for the very affordable price of loss of generality (that is, you can't use WUFFS to write your video game, web browser, word processor, operating system or whatever) For example in C++ array[index] has Undefined Behaviour on a bounds miss. Rust's array[index] will panic at runtime on a bounds miss, at least we know what will happen but what happens isn't great... WUFFS array[index] will not compile if it could incur a bounds miss. Show the compiler why index will be a value that's always in-bounds when the index occurs. reply haneefmubarak 10 hours agorootparentIt appears that XZ actually is in WUFFS! https://github.com/google/wuffs/tree/main/std/xz reply nigeltao 7 hours agorootparentYeah, it's just a coincidence (†), but I started working on Wuffs' LZMA and XZ decoders last December. It works well enough to decode the Linux source code tarball correctly (producing the same output as /usr/bin/xz). $ git clone --quiet --depth=1 https://github.com/google/wuffs.git $ gcc -O3 wuffs/example/mzcat/mzcat.c -o my-mzcat $ ./my-mzcatThe suffix -li is used in Swiss German dialects Seems similar to -let in English. JPEGlet Or -ito/-ita in Spanish. JPEGito (Joint Photographers Experts Grupito) Or perhaps, if you want to go full Spanish GEFCito (Grupito de Expertos en Fotografía Conjunta) reply Someone 1 hour agorootparenthttps://en.wikipedia.org/wiki/List_of_diminutives_by_languag... lists many more that “could be seen as diminutives”, at least some of which a were fairly recently used in forming new words (examples: disk ⇒ diskette, computer ⇒ minicomputer) reply sa-code 4 hours agorootparentprevOr JPEGchen in high German reply 7bit 2 hours agorootparentOr JPEGle in Swabian German. -le as in left, not as in Pebble reply cout 4 hours agoparentprevInteresting, I was expecting there to be some connection to the deblocking jpeg decoder knusperli. reply JyrkiAlakuijala 3 hours agorootparentThat would give additional savings. reply bArray 1 hour agoprev> In order to quantify Jpegli's image quality improvement we enlisted the help of crowdsourcing raters to compare pairs of images from Cloudinary Image Dataset '22, encoded using three codecs: Jpegli, libjpeg-turbo and MozJPEG, at several bitrates. Looking further [1]: > It consists in requiring a choice between two different distortions of the same image, and computes an Elo ranking (an estimate of the probability of each method being considered higher quality by the raters) of distortions based on that. Compared to traditional Opinion Score methods, it avoids requiring test subjects to calibrate their scores. This seems like a bad way to evaluate image quality. Humans can tend towards liking more highly saturated colours, which would be a distortion of the original image. If it was just a simple kernel that turned any image into a GIF cartoon, and then I had it rated by cartoon enthusiasts, I'm sure I could prove GIF is better than JPEG. I think that to produce something more fair, it would need to be \"Given the following raw image, which of the following two images appears to better represent the above image?\" The allowed answers should be \"A\", \"B\" and \"unsure\". ELO would likely be less appropriate. I would also like to see an analysis regarding which images were most influential in deciding which approach is better and why. Is it colour related, artefact related, information frequency related? I'm sure they could gain some deeper insight into why one method is favoured over the other. [1] https://github.com/google-research/google-research/blob/mast... reply Permik 18 minutes agoparentOne other thing to control for is the subpixel layout of their display which is almost always forgotten in these studies. reply jug 11 hours agoprevTheir claims about Jpegli seem to make WebP obsolete regarding lossy encoding? Similar compression estimates as WebP versus JPEG are brought up. Hell, I question if AVIF is even worth it with Jpegli. It's obviously \"better\" (higher compression) but wait! It's 1) a crappy, limited image format for anything but basic use with obvious video keyframe roots and 2) terribly slow to encode AND 3) decode due to not having any streaming decoders. To decode, you first need to download the entire AVIF to even begin decoding it, which makes it worse than even JPEG/MozJPEG in many cases despite their larger sizes. Yes, this has been benchmarked. JPEG XL would've still been worth it though because it's just covering so much more ground than JPEG/Jpegli and it has a streaming decoder like a sensible format geared for Internet use, as well as progressive decoding support for mobile networks. But without that one? Why not just stick with JPEG's then. reply lonjil 11 hours agoparent> Their claims about Jpegli seem to make WebP obsolete regarding lossy encoding? Similar compression estimates as WebP versus JPEG are brought up. I believe Jpegli beats WebP for medium to high quality compression. I would guess that more than half of all WebP images on the net would definitely be smaller as Jpegli-encoded JPEGs of similar quality. And note that Jpegli is actually worse than MozJPEG and libjpeg-turbo at medium-low qualities. Something like libjpeg-turbo q75 is the crossover point I believe. > Hell, I question if AVIF is even worth it with Jpegli. According to another test [1], for large (like 10+ Mpix) photographs compressed with high quality, Jpegli wins over AVIF. But AVIF seems to win for \"web size\" images. Though, as for point 2 in your next paragraph, Jpegli is indeed much faster than AVIF. > JPEG XL would've still been worth it though because it's just covering so much more ground than JPEG/Jpegli and it has a streaming decoder like a sensible format geared for Internet use, as well as progressive decoding support for mobile networks. Indeed. At a minimum, JXL gives you another 20% size reduction just from the better entropy coding. [1] https://cloudinary.com/blog/jpeg-xl-and-the-pareto-front reply ksec 8 hours agorootparent> I would guess that more than half of all WebP images on the net would definitely be smaller as Jpegli-encoded JPEGs of similar quality. That was what I expected a long time ago but it turns out to be a false assumption. According to Google with data from Chrome. 80%+ of images on the web are bpp 1.0+. reply ksec 8 hours agoparentprevSharing similar view. I even go as far as to say jpegli ( and the potential with XYB ICC ) makes JPEG XL just not quite good enough to be worth the effort. The good thing is that the author of XL ( Jyrki's ) claims there are potential of 20-30% bitrate savings at the low end. So I hope JPEG XL encoder continues to improve. reply jug 53 minutes agorootparentYes, I agree and I think there is a hurdle in mucking with file formats alone because it always affects interoperability somewhere in the end. I think this also needs to be accounted for - the advantages need to outweigh this downside because it is a downside. I still kind of want JPEG XL but I'm starting to question how much of it is simply due to me being a geek that want tech as good as possible rather than a pragmatic view on this, and I didn't question this as much before Jpegli. reply JyrkiAlakuijala 45 minutes agorootparentIt can be a question when your uncle's/daughter's/etc phone is full of photos and they asks for advice on how to make more space. It can be a question of if the photo fits as an email attachment etc. 'Zillions' of seconds of aggregate latency waiting time is spent each day on waiting for web sites to load. Back-of-the-envelope calculations can suggest that the value of reducing waiting time can be in hundreds of billions over the time of deployment. Bandwidth cost to users and energy use may also be significant factors. reply JyrkiAlakuijala 5 hours agorootparentprevYou can always use JPEG XL lossless JPEG1 recompression to get some savings in the high end quality, too — if you trust the quality decision heuristics in jpegli/guetzli/other jpeg encoder more than the JPEG XL encoder itself. We also provide a ~7000 lines-of-code libjxl-tiny that is more similar to jpeg encoders in complexity and coding approach, and a great starting point for building a hardware encoder. reply ksec 3 hours agorootparent>JPEG XL lossless JPEG1 recompression This reminded of something. I so wish iOS 18 could support JPEG XL out of the box rather than Safari only. I have 80GB of Photos on my iPhone. Vast Majority of them were sent over by WhatsApp ( JPEG ). If iOS could simply recompress those into JPEG XL I would instantly gain ~10GB+ of storage. reply JyrkiAlakuijala 3 hours agorootparentWhat happens if you recompress them losslessly manually to JPEG XL? reply themerone 5 hours agorootparentprevIt's not just about the compression ratio. JPEG XL improvements in generational loss are reason enough that it should be the default format for the web. reply Mr_Minderbinder 2 hours agoprevIt would help if the authors explained how exactly they used the Elo rating system to evaluate quality, since this seems like a non-standard and rather unusual use case for this. I am guessing that if an image is rated better than another that counts as a \"win\"? Finally, writing \"ELO\" instead of \"Elo\" is incorrect (this is just one of my pet peeves but indulge me nevertheless). This is some guy's name not an abbreviation, nor a prog rock band from the 70's! You would not write \"ELO's\" rating system for the same reason you wouldn't write \"DIJKSTRA's\" algorithm. reply summerlight 2 hours agoparenthttps://github.com/google-research/google-research/blob/mast... Looks like this is the implementation. reply aendruk 12 hours agoprevLooks like it’s not very competitive at low bitrates. I have a project that currently encodes images with MozJPEG at quality 60 and just tried switching it to Jpegli. When tuned to produce comparable file sizes (--distance=4.0) the Jpegli images are consistently worse. reply JyrkiAlakuijala 10 hours agoparentI believe that they should be roughly the same in a photography corpus density at quality 60. Consider filing an issue if some image is worse with jpegli. reply ruuda 12 hours agoparentprevWhat is your use case for degrading image quality that much? At quality level 80 the artifacts are already significant. reply egorfine 1 hour agorootparentThumbnails. I typically serve them at 2x resolution but extremely heavily compressed. Still looks good enough in browser when scaled down. reply aendruk 12 hours agorootparentprevThumbnails at a high pixel density. I just want them up fast. Any quality that can be squeezed out of it is a bonus. reply londons_explore 9 hours agorootparentJPEG has a fixed macroblock size (16x16 pixels), which negatively affects high resolution low bitrate images. If you must use JPEG, I suspect you might get better visual quality by halving the resolution and upsampling on the client. By doing so, you are effectively setting the lower and right halves of the DCT to zero (losing all high resolution info), but get to have 32x32 pixel macroblocks which lets you better make use of low frequency spacial patterns. reply egorfine 1 hour agorootparentOh, that's interesting. I typically serve thumbnails at 2x resolution and heavily compressed. Should I try to instead compress them less but serve at 0.5x resolution? reply lonjil 11 hours agorootparentprevI recently noticed that all the thumbnails on my computer are PNG, which I thought was funny. reply Brian_K_White 8 hours agorootparentprevI apologize that this will seem like, well it IS frankly, more reaction than is really justified, sorry for that. But this question is an example of a thing people commonly do that I think is not good and I want to point it out once in a while when I see it: There are infinite use-cases for everything beside one's own tiny personal experience and imagination. It's not remarkable that someone tested for the best version of something you personally don't have a use for. Pretend they hadn't answered the question. The answer is it doesn't matter. They stated a goal of x, and compared present-x against testing-x and found present-x was the better-x. \"Why do they want x when I only care about y?\" is irrelevant. I mean you may be idly curious and that's not illegal, but you also stated a reason for the question which makes the question not idle but a challenge (the \"when I only care about y\" part). What I mean by \"doesn't matter\" is, whatever their use-case is, it's automatically always valid, and so it doesn't change anything, and so it doesn't matter. Their answer happened to be something you probably agree is a valid use-case, but that's just happenstance. They don't have to have a use-case you happen to approve of or even understand. reply mgraczyk 14 hours agoprev> Jpegli can be encoded with 10+ bits per component. How are the extra bits encoded? Is this the JPEG_R/\"Ultra HDR\" format, or has Google come up with yet another metadata solution? Something else altogether? Ultra HDR: https://developer.android.com/media/platform/hdr-image-forma... reply lonjil 13 hours agoparentIt's regular old JPEG1. I don't know the details, but it turns out that \"8 bit\" JPEG actually has enough precision in the format to squeeze out another 2.5 bits, as long as both the encoder and the decoder use high precision math. reply actionfromafar 13 hours agorootparentWow, this is the first time I heard about that. I wonder if Lightroom uses high precision math. reply donatzsky 13 hours agoparentprevThis has nothing to do with Ultra HDR. It's \"simply\" a better JPEG encoder. Ultra HDR is a standard SDR JPEG + a gain map that allows the construction of an HDR version. Specifically it's an implementation of Adobe's Gain Map specification, with some extra (seemingly pointless) Google bits. Adobe gain Map: https://helpx.adobe.com/camera-raw/using/gain-map.html reply mgraczyk 13 hours agorootparentThanks, I was on the team that did Ultra HDR at Google so I was curious if it was being used here. Didn't see anything in the code though so that makes sense. reply JyrkiAlakuijala 12 hours agoparentprevUltra HDR can have two jpegs inside, one for the usual image and another for the gain-map. Hypothetically, both jpegs can be created with jpegli. Hypothetically, both Ultra HDR jpegs can be decoded with jpegli. In theory jpegli would remove the 8 bit striping that would otherwise be present in Ultra HDR. I am not aware of jpegli-based Ultra HDR implementations. A personal preference for me would be a single Jpegli JPEG and very fast great local tone mapping (HDR source, tone mapping to SDR). Some industry experts are excited about Ultra HDR, but I consider it is likely too complicated to get right in editing software and automated image processing pipelines. reply Zardoz84 3 hours agorootparentWhat is the point of that complexity if JPEG XL can store HDR images ? reply JyrkiAlakuijala 3 hours agorootparentThe main idea why Ultra HDR is done like that is that the content creator (photographer) can control the local tone mapping. I think. reply simonw 15 hours agoprev> High quality results. When images are compressed or decompressed through Jpegli, more precise and psychovisually effective computations are performed and images will look clearer and have fewer observable artifacts. Does anyone have a link to any example images that illustrate this improvement? I guess the examples would need to be encoded in some other lossless image format so I can reliably view them on my computer. reply n2d4 15 hours agoparentYou can find them in the mucped23.zip file linked here (encoded as PNG): https://github.com/google-research/google-research/tree/mast... reply simonw 14 hours agorootparentThanks - I downloaded that zip file (460MB!) and extracted one of the examples into a Gist: https://gist.github.com/simonw/5a8054f18f9ea3c560b628b16b00f... Here's an original: https://gist.githubusercontent.com/simonw/5a8054f18f9ea3c560... And the jpegli-q95- version: https://gist.githubusercontent.com/simonw/5a8054f18f9ea3c560... And the same thing with mozjpeg-a95 https://gist.githubusercontent.com/simonw/5a8054f18f9ea3c560... reply modeless 13 hours agorootparentYou shouldn't compare the same quality setting across encoders as it's not standardized. You have to compare based on file size. reply IshKebab 14 hours agorootparentprevThey're far too high quality to tell anything. There's no point comparing visually lossless images (inb4 \"I am amazing and can easily tell...\"). reply deanresin 7 hours agorootparentRight? I had all 3 open and quickly flipped over them saw no difference. Maybe I'm just uncultured. reply pseudosavant 12 hours agorootparentprevPerhaps try quality settings in the 70 range, and comparable output file sizes. 95 will be high-quality by definition. reply simonw 12 hours agorootparentThere are some 70s and 65s in the gist: https://gist.github.com/simonw/5a8054f18f9ea3c560b628b16b00f... reply tedunangst 14 hours agorootparentprevWhat are the file sizes for those two? reply simonw 14 hours agorootparentThe zip file doesn't have the originals, just the PNGs. reply edflsafoiewq 14 hours agorootparentprevEdit: I'm dumb. reply tedunangst 14 hours agorootparentI would hope the jpegs compress better than png does. reply masfuerte 14 hours agorootparentprevYou said linked but like a fool I went looking for a zip in the repository. This is the link: https://cloudinary.com/labs/cid22/mucped23.zip (460MB) reply n2d4 14 hours agorootparentI can't blame you, my comment originally didn't have the word \"linked\", I edited that in after I realized the potential misunderstanding. Maybe you saw it before the edit. My bad. reply masfuerte 14 hours agorootparentHa ha! No worries. I thought it had changed but I frequently skim read and miss things so I wasn't sure. reply JyrkiAlakuijala 14 hours agoparentprevhttps://twitter.com/jyzg/status/1622890389068718080 Some earlier results. Perhaps these were with XYB color space, I don't remember ... reply andrewla 14 hours agoparentprevAs an aside, jpeg is lossless on decode -- once encoded, all decoders will render the same pixels. Since this library produces a valid jpeg file, it should be possible to directly compare the two jpegs. reply nigeltao 7 hours agorootparent> all decoders will render the same pixels Not true. Even just within libjpeg, there are three different IDCT implementations (jidctflt.c, jidctfst.c, jidctint.c) and they produce different pixels (it's a classic speed vs quality trade-off). It's spec-compliant to choose any of those. A few years ago, in libjpeg-turbo, they changed the smoothing kernel used for decoding (incomplete) progressive JPEGs, from a 3x3 window to 5x5. This meant the decoder produced different pixels, but again, that's still valid: https://github.com/libjpeg-turbo/libjpeg-turbo/commit/6d91e9... reply JyrkiAlakuijala 5 hours agorootparentMoritz, the author of that improvement, implemented the same for jpegli. I believe the standard does not specify what the intermediate progressive renderings should look like. I developed that interpolation mechanism originally for Pik, and Moritz was able to formulate it directly in the DCT space so that we don't need to go into pixels for the smoothing to happen, but he computed it using a few of the low frequency DCT coefficients. reply JyrkiAlakuijala 14 hours agorootparentprevIt is approximately correct. The rendering is standards compliant without pixel perfection and most decoders make different compromises and render slightly different pixels. reply lxgr 15 hours agoprevThey'll do literally anything rather than implementing JPEG XL over AVIF in Chrome, huh? I mean, of course this is still valuable (JPEG-only consumers will probably be around for decades, just like MP3-only players), and I realize Google is a large company, but man, the optics on this... reply whywhywhywhy 14 hours agoparentIf you do creative work countless tools just don’t support webp, AVIF or HEIF. It’s so prominent running into files you can’t open in your tools that I have a right click convert to PNG context menu reply jiggawatts 14 hours agorootparentThey don’t support it because Chromium doesn’t. Because Chromium doesn’t support it, Electron doesn’t. Because Electron doesn’t, Teams and other modern web apps and web sites don’t either, etc… If Google just added JPEG XL support instead then it would be… a supported alternative to JPEG. You’re saying working in that is a waste of time because… it’s not supported. reply modeless 13 hours agorootparentThere's a lot more to format support than Chromium. There's a pretty strong meme out there on the Internet that webp is evil despite being supported in all browsers for years because there's still a lot of software out there that never added support and people get annoyed when an image fails to open. reply lxgr 13 hours agorootparentI don't think it's evil, but I just don't think it's very good either. And a graphics format better be damn good (i.e. much, not just a little bit, better than what it's hoping to replace) if it aspires to become widely supported across applications, operating systems, libraries etc. reply jug 11 hours agorootparentprevAt least now with Jpegli, this will surely be the nail in the coffin for WebP? The article has 35% compression improvements over JPEG mentioned and that's at least as much as usually thrown around when discussing WebP. reply redeeman 13 hours agorootparentprevmaybe proprietary software just isnt so good? reply n2d4 14 hours agorootparentprevChromium does support WebP and AVIF, yet parent's tools don't. reply The_Colonel 11 hours agorootparentMaybe because WebP and AVIF are actually not that great image formats. WebP has been critiqued as a very mediocre improvement over JPEG all the way since its introduction. These formats are in Chromium because of Google politics, not because of their technical merit. reply dchest 14 hours agoparentprevSome authors of this are also the authors of JPEG XL. reply lxgr 14 hours agorootparentI saw that. It's the tragedy of Google in a nutshell: Great things are being worked on in some departments, but organizational dysfunction virtually ensures that the majority of them will not end up in users' hands (or at least not for long). reply ksec 8 hours agorootparentThis is work from Google research outside US. You could even call it a different company with the same name. It is Google US who made those AOM / AVIF decisions. reply out_of_protocol 7 hours agoparentprevWhy no blame on Mozilla for ignoring format as well? reply refulgentis 14 hours agoparentprevI have no love for Google, at all. It's really hard to say this in public, because people are treating it like a divisive \"us or them\" issue that's obvious, but the JPEG-XL stuff is _weird_. I've been in codecs for 15 years, and have never seen as unconstructive behavior like the JPEG-XL work. If I had infinite time and money and it came across my plate, we'd have a year or two of constructive work to do, so we didn't just rush something in with obvious issues and opportunities. It turned into \"just figure out how to merge it in, and if you don't like it, that's malfeasance!\" Bread and circus for commentators, maybe, but, actively preventing even foundational elements of a successful effort. reply lxgr 14 hours agorootparentTo be honest, at Google scale, if there's an objectively good new codec with some early signs of excitement and plausible industry traction, and even Apple managed to deploy it to virtually all of their devices (and Apple isn't exactly known as an \"open codec forward\" company), not integrating it does seem like either malfeasance or gross organizational dysfunction to me. reply refulgentis 14 hours agorootparentCompletely hypothetical scenario: what if the technical reaction was so hostile they invested in it themselves to fix the issues and make it sustainable? In light of the recent security incident, I'd see that completely hypothetical situation as more admirable. reply lxgr 13 hours agorootparentHm, are you suggesting they're currently in the process of reimplementing it in a safer and/or more maintainable way as part of Chrome? In that case, that would just be extremely bad messaging (which I also wouldn't put past Google). Why agitate half of the people on here and in other tech-affine parts of the Internet when they could have just publicly stated that they're working on it and to please have some patience? Public support by Google, even if it's just in the form of a vague \"intent to implement\", would be so important for a nascent JPEG successor. reply refulgentis 11 hours agorootparentSee comment on peer (TL;DR: I agree, and that's the substance of the post we're commenting on) reply LeoNatan25 12 hours agorootparentprevYour posts here seem of the “just asking questions” variety—no substance other than being counterculture. Do you have any proof or resemblance of any logical reason to think this? reply refulgentis 11 hours agorootparentIt's a gentle joke, it happened, that's TFA. (ex. see the other threads re: it started from the JPEG XL repo). I use asking questions as a way to keep contentious discussions on track without being boorish. And you're right, it can easily be smarmy instead of Socratic without tone, a la the classic internet sarcasm problem. Gentle note: I only asked one question, and only in the post you replied to. reply F3nd0 14 hours agorootparentprevWhatever are you referring to? JPEG XL had already been merged into Chromium, prior to being removed again (without a proper reason ever given). As far as I know, the JPEG XL developers have offered to do whatever work was necessary for Chromium specifically, but were never taken up on the offer. Same thing with Firefox, which has had basic support merged into Nightly, and a couple more patches gathering dust due to lack of involvement from the side of Firefox. Mozilla has since decided to take a neutral stance on JPEG XL, seemingly without doing any kind of proper evaluation. Many other programs (like GIMP, Krita, Safari, Affinity, darktable) already support JPEG XL. People are not getting upset because projects don’t invest their resources into supporting JPEG XL. People are getting upset because Google (most notably), which has a decisive say in format interoperability, is flat out refusing to give JPEG XL a fair consideration. If they came up with a list of fair conditions JPEG XL has to meet to earn their support, people could work towards that goal, and if JPEG XL failed to meet them, people would easily come to terms with it. Instead, Google has chosen to apply double standards, present vague requirements, and refuse to elaborate. If anyone is ‘preventing even foundational elements of a successful effort’, it’s Google, or more specifically, the part that’s responsible for Chromium. reply ksec 8 hours agorootparentprev>\"just figure out how to merge it in, and if you don't like it, that's malfeasance!\" It isn't not accepting it or hostile. That is completely not true. They actively push against JPEG XL, despite all the data, prior to even 1.0 suggest it is or could be better than AVIF in many cases. To the point where they even make up false benchmarks to downplay JPEG XL. Companies were even willing to paid ( wont name ) and put resources into getting JPEG XL because they see it to be so good. But they still refused. It is at this point people thought something doggy is going on. And then not only did Google not explain themselves. They were even more hostile. So why the extra hate? Well partly because this is a company who gave us an over promised WebP and underdelivered. reply ur-whale 14 hours agoparentprev> They'll do literally anything rather than implementing JPEG XL over AVIF in Chrome, huh? Before making that kind of claim, I would spend some time looking at the names of the folks who contributed heavily to the development of JPEG XL and the names of the folks who wrote jpegli. reply lxgr 14 hours agorootparentBy \"they\" I mean \"Google, the organization\", not \"the authors of this work\", who most likely have zero say in decisions concerning Chrome. reply JyrkiAlakuijala 14 hours agorootparentChrome advised and inspired this work in their position about JPEG XL. Here: https://www.mail-archive.com/blink-dev@chromium.org/msg04351... \"can we optimize existing formats to meet any new use-cases, rather than adding support for an additional format\" It's a yes! Of course full JPEG XL is quite a bit better still, but this helps old compatible JPEG to support HDR without 8-bit banding artefacts or gainmaps, gives a higher bit depth for other uses where more precision is valuable, and quite a bit better compression, too. reply lxgr 13 hours agorootparent> \"can we optimize existing formats to meet any new use-cases, rather than adding support for an additional format\" Only within pretty narrow limits. Classic JPEG will never be as efficient given its age, in the same way that LAME is doing incredible things for MP3 quality, but any mediocre AAC encoder still blows it out of the water. This is in addition to the things you've already mentioned (HDR) and other new features (support for lossless coding). And I'd find their sentiment much easier to believe if Google/Chrome weren't hell-bent on making WebP (or more recently AVIF) a thing themselves! That's two formats essentially nobody outside of Google has ever asked for, yet they're part of Chrome and Android. reply IshKebab 13 hours agorootparentprevDespite the answer being yes, IMO it's pretty clear that the question is disingenuous, otherwise why did they add support for WebP and AVIF? The question applies equally to them. reply therealmarv 14 hours agoprevoh, probably we will get it soon in ImageOptim then https://imageoptim.com/ reply vladstudio 14 hours agoparentThanks in advance! reply atdt 9 hours agoprevWhen using Jpegli as a drop-in replacement for for libjpeg-turbo (i.e., with the same input bit-map and quality setting), will the output produced by Jpegli be smaller, more beautiful, or both? Are the space savings the result of the Jpegli encoder being able to generate comparable or better-looking images at lower quality settings? I'd like to understand whether capitalizing on the space efficiency requires any modification the caller code. reply JyrkiAlakuijala 5 hours agoparentThe output will be smaller after replacing libjpeg-turbo or mozjpeg with jpegli. You don't need to do any code changes. reply pizza 8 hours agoparentprevI think the main benefit is a better decorrelation transform so the compression is higher at the same quality parameter. So you could choose whether you want better accuracy for the same quality parameter, or lower the quality parameter and get better fidelity than you would have otherwise. Probably to get both most of the time, just use JPEGXL reply Waterluvian 15 hours agoprevThis is the kind of realm I'm fascinated by: taking an existing chunk of something, respecting the established interfaces (ie. not asking everyone to support yet another format), and seeing if you can squeeze out an objectively better implementation. It's such a delicious kind of performance gain because it's pretty much a \"free lunch\" with a very comfortable upgrade story. reply terrelln 13 hours agoparentI agree, this is a very exciting direction. We shouldn’t let existing formats stifle innovation, but there is a lot of value in back porting modern techniques to existing encoders. reply xfalcox 15 hours agoprevDoes anyone have compiled this to WASM? I'm currently using MozJPEG via WASM for a project and would love to test replacing it by Jpegli. reply simonw 15 hours agoparentA WASM demo of this would be fantastic, would make it much easier for people to try it out. Maybe a fork of https://squoosh.app/ ? The code for that is https://github.com/GoogleChromeLabs/squoosh reply jeffbee 12 hours agoparentprevJust for my own edification, why would there be any trouble compiling portable high-level libraries to target WASM? reply tedunangst 12 hours agorootparentMaybe it uses threads. reply theanonymousone 15 hours agoprevIs that from Google Zürich? reply JyrkiAlakuijala 15 hours agoparentYes! reply veselin 15 hours agorootparentWhen I saw the name, I knew immediately this is Jyrki's work. reply 082349872349872 14 hours agorootparentI'm waiting for huaraJPEG... reply actionfromafar 13 hours agorootparentwhat is that? reply mensi 13 hours agorootparenta much ruder but just as stereotypically Swiss German thing as the \"-li\" suffix ;) reply thaliaarchi 14 hours agorootparentprevI’ve never heard of a Jpegli bread, but Zöpfli and Brötli sure are yummy :) reply JyrkiAlakuijala 14 hours agorootparentI thought clarity was more important. Otherwise it would be called ... Pumpernikkeli. reply lars_thomas 13 hours agoprevSorry for perhaps missing it but it states \"It provides both a fully interoperable encoder and decoder complying with the original JPEG standard\". Does that mean that jpegli-encoded images can be decoded by all jpeg decoders? But it will not have the same quality? reply lonjil 13 hours agoparentJpegli encoded images decode just fine with any JPEG decoder, and will still be of great quality. All the tests were done with libjpeg-turbo as the decoder. Using Jpegli for decoding gives you a bit better quality and potentially higher bit depth. reply lars_thomas 13 hours agorootparentThanks, sounds great! Not sure if you are part of the research team but a follow up question nevertheless. Learning from JpegXL, what would it take to develop another widely supported image format? Would the research stage already need to be carried out as a multi-corporate effort? reply lonjil 13 hours agorootparent> Not sure if you are part of the research team but a follow up question nevertheless. I am not. > what would it take to develop another widely supported image format? Would the research stage already need to be carried out as a multi-corporate effort? I believe JXL will be very successful sooner or later, it already has a lot more support than many other attempts at new image formats. But in general, the main way to get fast adoption on the web is to have Chromium's codec team be the main developers. reply JyrkiAlakuijala 13 hours agorootparentprevMulti-corporate effort would likely need to start by first agreeing what is image quality. Image quality folks are more cautious and tradition-centric than codec devs, so quite an initial effort would be needed to use something as advanced and risky as butteraugli, ssimulacra or XYB. With traditional objective metrics it would be very difficult to make a competing format as they would start with a 10–15 % disadvantage. So, I think it is not easy and would need substantial investment. reply 01HNNWZ0MV43FF 13 hours agorootparentprevSort of like the quality vs. speed settings on libx264, I suppose jpegli aims to push the Pareto boundary on both quality and speed without changing the decode spec reply edent 13 hours agoprevI'd love to know what \"Paradigms of Intelligence\" means in this context. reply larodi 13 hours agoparentI would've loved to see side-by-side comparison... after all we talk visuals here, right? So as this old saying goes: a hand to touch, an eye to see. Not underestimating the value in this, but the presentation is very weak. reply p0nce 15 hours agoprevWonder how it compares to guetzli, which is good albeit slow (by Google also!). reply JyrkiAlakuijala 15 hours agoparentI believe guetzli is slightly more robust around quality 94, but jpegli likely better at or equal at lower qualities like below 85. Jpegli is likely about 1000x faster and still good. reply ronjouch 7 hours agorootparentThat's my experience, yes. Just tested it on a 750kB 1080p image with detailed areas and areas with gradients. Highly unscientific, N=1 results: - Guetzli at q=84 (the minimum allowed by Guetzli) takes 47s and produces a 403kB image. - Jpegli at q=84 takes 73ms (MILLIseconds) and produces a mostly-indistinguishable 418kB image. \"Mostly\" because: A. it's possible to find areas with subtle color gradients where Guetzli does a better job at keeping it smooth over a large area. B. \"Little specks\" show a bit more \"typical JPG color-mush artifacting\" around the speck with Jpegli than Guetzli, which stays remarkably close to the original Also, compared to the usual encoder I'm used to (e.g. the one in GIMP, libjpeg maybe?), Jpegli seems to degrade pretty well going into lower qualities (q=80, q=70, q=60). Qualities lower than q=84 are not even allowed by Guetzli (unless you do a custom build). I'm immediately switching my \"smallify jpg\" Nautilus script from Guetzli to Jpegli. The dog-slowness of Guetzli used to be tolerable when there was no close contender, but now it feels unjustified in comparison to the instant darn excellent result of Jpegli. reply JyrkiAlakuijala 5 hours agorootparentThank you for such a well-informed report! With guetzli I added manually overprovisioning for slow smooth gradients. If you have an example where guetzli is better with gradients you could post an issue with a sample image. That would help us to potentially fix it for jpegli, too. reply donatj 10 hours agoparentprevI was wondering the same thing. We have a system that guetzli's some of our heavily used assets but it takes SOOOOOO long reply ctz 14 hours agoprevFrom the people who bought you WebP CVE-2023-41064/CVE-2023-4863... reply vanderZwan 14 hours agoparentNo, these are not the WebP people. These are Google's JXL people. reply JyrkiAlakuijala 14 hours agorootparentI designed WebP lossless and implemented the first encoder for it. Zoltan who did most of the implementation work for jpegli wrote the first decoder. reply tambourine_man 14 hours agoprevIs there an easy way for us to install and test it? reply ronjouch 7 hours agoparentIn Arch it's packaged in libjxl as /usr/bin/{c,d}jpegli . See https://archlinux.org/packages/extra/x86_64/libjxl/ , section \"Package Contents\" at the bottom. reply miragecraft 11 hours agoparentprevI'm currently using it via XL Converter. https://codepoems.eu/xl-converter/ reply aendruk 14 hours agoparentprevnixpkgs unstable (i.e. 24.05) has it at ${libjxl}/bin/cjpegli reply cactusplant7374 15 hours agoprev> These heuristics are much faster than a similar approach originally used in guetzli. I liked guetzli but it's way too slow to use in production. Glad there is an alternative. reply LeoNatan25 12 hours agoprev> 10+ bits. Jpegli can be encoded with 10+ bits per component. Traditional JPEG coding solutions offer only 8 bit per component dynamics causing visible banding artifacts in slow gradients. Jpegli's 10+ bits coding happens in the original 8-bit formalism and the resulting images are fully interoperable with 8-bit viewers. 10+ bit dynamics are available as an API extension and application code changes are needed to benefit from it. So, instead of supporting JPEG XL, this is the nonsense they come up with? Lock-in over a JPEG overlay? reply spider-mario 10 hours agoparentThis is from the same team as JPEG XL, and there is no lock-in or overlay. It’s just exploiting the existing mechanics better by not doing unnecessary truncation. The new APIs are only required because the existing APIs receive and return 8-bit buffers. reply littlestymaar 15 hours agoprevWhy is it written in C++, when Google made Wuffs[1] for this exact purpose? [1]: https://github.com/google/wuffs reply vanderZwan 14 hours agoparentThis is pure speculation, but I'm presuming Wuffs is not the easiest language to use during the research phase, but more of a thing you would implement a format in once it has stabilized. And this is freshly published research. Probably would be a good idea to get a port though, if possible, improving both safety and performance sounds like a win to me. reply nigeltao 11 hours agorootparentYeah, you're right. It's not as easy to write Wuffs code during the research phase, since you don't just have to write the code, you also have to help the compiler prove that the code is safe, and sometimes refactor the code to make that tractable. Wuffs doesn't support global variables, but when I'm writing my own research phase code, sometimes I like to just tweak some global state (without checking the code in) just to get some experimental data: hey, how do the numbers change if I disable the blahblah phase when the such-and-such condition (best evaluated in some other part of the code) holds? Also, part of Wuffs' safety story is that Wuffs code cannot make any syscalls at all, which implies that it cannot allocate or free memory, or call printf. Wuffs is a language for writing libraries, not whole programs, and the library caller (not callee) is responsible for e.g. allocating pixel buffers. That also makes it harder to use during the research phase. reply Sesse__ 13 hours agoparentprevWuffs is for the exact _opposite_ purpose (decoding). It can do simple encoding once you know what bits to put in the file, but a JPEG encoder contains a lot of nontrivial machinery that does not fit well into Wuffs. (I work at Google, but have nothing to do with Jpegli or Wuffs) reply nigeltao 11 hours agorootparentEncoding is definitely in Wuffs' long term objectives (it's issue #2 and literally in its doc/roadmap.md file). It's just that decoding has been a higher priority. It's also a simpler problem. There's often only one valid decoding for any given input. Decoding takes a compressed image file as input, which have complicated formats. Roughly speaking, encoding just takes a width x height x 4 pixel buffer, with very regular structure. It's much easier to hide something malicious in a complicated format. Higher priority means that, when deciding whether to work on a Wuffs PNG encoder or a Wuffs JPEG decoder next, when neither existed at the time, I chose to have more decoders. (I work at Google, and am the Wuffs author, but have nothing to do with Jpegli. Google is indeed a big company.) reply littlestymaar 4 hours agorootparentThanks for the answer! reply Hackbraten 13 hours agorootparentprevThe first paragraph in Wuffs's README explicitly states that it's good for both encoding and decoding? reply Sesse__ 12 hours agorootparentNo, it states that wrangling _can be_ encoding. It does not in any way state that Wuffs is actually _good_ for it at the current stage, and I do not know of any nontrivial encoder built with Wuffs, ever. (In contrast, there are 18 example decoders included with Wuffs. I assume you don't count the checksum functions as encoding.) reply Blackthorn 14 hours agoparentprevGoogle is a big company. reply ramrunner0xff 15 hours agoparentprevthis is a valid question why is it being downvoted? reply SunlitCat 13 hours agorootparentMaybe because the hailing for yet another \"safe\" language starts to feel kinda repetitive? Java, C#, Go, Rust, Python, modern C++ with smartpointer,... I mean a concepts for handling files in a safe way are an awesome (and really needed) thing, but inventing a whole new programming language around a single task (even if it's just a transpiler to c)? reply vinkelhake 13 hours agorootparentOne of the advantages with wuffs is that it compiles to C and wuffs-the-library is distributed as C code that is easy to integrate with an existing C or C++ project without having to incorporate new toolchains. reply littlestymaar 13 hours agorootparentprev> Maybe because the hailing for yet another \"safe\" language starts to feel kinda repetitive? Ah yeah, because the endless stream of exploits and “new CVE allows for zero-click RCE, please update ASAP” doesn't feel repetitive? > I mean a concepts for handling files in a safe way are an awesome (and really needed) thing, but inventing a whole new programming language around a single task (even if it's just a transpiler to c)? It's a “single task” in the same way “writing compilers” is a single task. And like we're happy that LLVM IR exists, having a language dedicated to writing codecs (of which there are dozens) is a worthwhile goal, especially since they are both security critical and have stringent performance needs for which existing languages (be it managed languages or Rust) aren't good enough. reply asicsarecool 14 hours agoprevAwesome google. Disable zooming on mobile so I can't see the graph detail. You guys should up your web game reply politelemon 14 hours agoparentMany sites do this out of a misguided notion of what web development is. FWIW Firefox mobile lets you override zooming on a site. reply pandemic_region 13 hours agorootparent> FWIW Firefox mobile lets you override zooming on a site. What in heavens name and why is this not a default option? Thankee Sai! reply modeless 13 hours agorootparentChrome also has an option for this and it's great reply _carbyau_ 5 hours agorootparentprevHaving just searched a little for this, to turn this on you need about:config. Apparently about:config is still not available on Firefox Mobile main release it seems. Supposedly available on Dev/Beta/Nightly or similar - unverified statement though. Annoying. reply kbrosnan 4 hours agorootparentThere is no need to go to about:config. Firefox 3 dot menu -> Settings -> Accessibility -> Zoom on all websites reply kloch 15 hours agoprev> 10+ bits. Jpegli can be encoded with 10+ bits per component. If you are making a new image/video codec in 2024 please don't just give us 2 measly extra bits of DR. Support up to 16 bit unsigned integer and floating point options. Sheesh. reply JyrkiAlakuijala 15 hours agoparentWe insert/extract about 2.5 bits more info from the 8 bit jpegs, leading to about 10.5 bits of precision. There is quite some handwaving necessary here. Basically it comes down to coefficient distributions where the distributions have very high probabilities around zeros. Luckily, this is the case for all smooth noiseless gradients where banding could be otherwise observed. reply vlovich123 14 hours agorootparentDoes the decoder have to be aware of it to properly display such an image? reply spider-mario 14 hours agorootparentTo display it at all, no. To display it smoothly, yes. reply JyrkiAlakuijala 14 hours agorootparentFrom a purely theoretical viewpoint 10+ bits encoding will lead into slightly better results even if rendered using a traditional 8 bit decoder. One source of error has been removed from the pipeline. reply qingcharles 3 hours agorootparentHas there been any outreach to get a new HDR decoder for the extra bits into any software? I might be wrong, but it seems like Apple is the primary game in town for supporting HDR. How do you intend to persuade Apple to upgrade their JPG decoder to support Jpegli? p.s. keep up the great work! reply JyrkiAlakuijala 1 hour agorootparentI tried to reach to their devrel person Jen Simmons here: https://twitter.com/jyzg/status/1763141558042243470 I didn't follow up and I don't know if she read it or understood the proposal. reply Sesse__ 12 hours agorootparentprevIdeally, the decoder should be dithering, I suppose. (I know of zero JPEG decoders that do this in practice.) reply lonjil 11 hours agorootparentJpegli, of course, does this when you ask for 8 bit output. reply vlovich123 13 hours agorootparentprevHow does the data get encoded into 10.5 bits but displayable correctly by an 8 bit decoder while also potentially displaying even more accurately by a 10 bit decoder? reply JyrkiAlakuijala 13 hours agorootparentThrough non-standard API extensions you can provide a 16 bit data buffer to jpegli. The data is carefully encoded in the dct-coefficients. They are 12 bits so in some situations you can get even 12 bit precision. Quantization errors however sum up and worst case is about 7 bits. Luckily it occurs only in the most noisy environments and in smooth slopes we can get 10.5 bits or so. reply lonjil 13 hours agorootparentprev8-bit JPEG actually uses 12-bit DCT coefficients, and traditional JPEG coders have lots of errors due to rounding to 8 bits quite often, while Jpegli always uses floating point internally. reply Retr0id 15 hours agoparentprevIt's not a new codec, it's a new encoder/decoder for JPEG. reply JyrkiAlakuijala 13 hours agorootparentI consider codec to mean a pair of encoder and decoder programs. I don't consider it to necessarily mean a new data format. One data format can be implemented by multiple codecs. Semantics and nomenclature within our field is likely underdeveloped and the use of these terms varies. reply whywhywhywhy 15 hours agorootparentprevThis should have been in a H1 tag at the top of the page. Had to dig into a paragraph to find out Google wasn’t about to launch another image format supported in only a scattering of apps yet served as Image Search results. reply Retr0id 14 hours agorootparentIt is. (well, h3 actually) > Introducing Jpegli: A New JPEG Coding Library reply Timon3 15 hours agoparentprevFrom their Github: > Support for 16-bit unsigned and 32-bit floating point input buffers. \"10+\" means 10 bits or more. reply johnisgood 15 hours agorootparentWould not \">10\" be a better way to denote that? reply mkl 11 hours agorootparentThat means something different, but \"≥10\" would be better IMHO. Really there's an upper limit of 12, and 10.5 is more likely in practice: https://news.ycombinator.com/item?id=39922511 reply JyrkiAlakuijala 5 hours agorootparentI decided to call it 10.5 bits based on rather fuzzy theoretical analysis and a small amount of practical experiments with using jpegli in HDR use where more bits is good to have. My thinking is that in the slowest smoothest gradients (where banding would otherwise be visible) it is only three quantization decisions that generate error: (0,0), (0,1) and (1, 0) coefficient. Others are close to zero. I consider these as adding stochastic variables that have uniform error. On the average they start to behave a bit like a Gaussian distribution, but each block samples those distributions 64 times so there are going to be some more and some less lucky pixels. If we consider that every block would have one maximally unlucky corner pixel which would get all three wrong. log(4096/3)/log(2) = 10.41 So, very handwavy analysis. Experimentally it seems to roughly hold. reply johnisgood 54 minutes agorootparentprevYeah, >=, my bad. reply bufferoverflow 8 hours agoparentprevFor pure viewing of non-HDR content 10 bits is good enough. Very few humans can tell the difference between adjacent shades among 1024 shades. Gradients look smooth. 16 bits is useful for image capture and manipulation. But then you should just use RAW/DNG. reply kaladin-jasnah 15 hours agoprevI wonder if Google will make a JPEG library with https://github.com/google/wuffs at some point. reply greenavocado 15 hours agoparentGoogle has a direct conflict of interest: the AVIF team. reply jeffbee 12 hours agoparentprevDid you mean other than the jpeg decode that is already in wuffs? reply jbverschoor 15 hours agoprevGoogle sure did a shitty job of explaining the whole situation. JpegXL was kicked out. This thing is added, but the repo and code seems to be from jxl. I'm very confused. reply JyrkiAlakuijala 15 hours agoparentIt was just easiest to develop in libjxl repo. All test workers etc. are already setup there. This was done by a very small team... reply jauntywundrkind 15 hours agoprevnext [11 more] [flagged] jsnell 15 hours agoparentFirst, this is work by the team that co-designed JPEG XL. Second, as far as I can tell this is a JPEG encoding library, producing JPEG files that any existing JPEG decoder will be able to read. Nobody is being asked to support or maintain anything new. reply CharlesW 15 hours agorootparentNot to mention that it's 35% more efficient than existing encoders, and can support 10+ bits per component encoding while remaining compatible with existing decoders. That's pretty amazing. reply amluto 15 hours agorootparentIt would still be nice to see a comparison to JPEG XL. reply edflsafoiewq 15 hours agorootparenthttps://giannirosato.com/blog/post/jpegli/ Note that that's from almost a year ago, don't know if anything changed. reply deaddodo 15 hours agorootparentprevYeah, this is just that. They took JPEG XL denoising and deartifacting algorithms and front loaded them into a 10-bit JPEG encoder. There's more to it, of course, but it's essentially just an improved encoder versus a new format. reply JyrkiAlakuijala 14 hours agorootparentNo denoising or deartifacting was used. They are an option for future improvements at low quality JPEG decoding. reply jbverschoor 15 hours agorootparentprevThe sourcecode is on jxl's main repo. I'm confused to say the least reply jsnell 15 hours agorootparentWhat's so confusing about it? This team did a lot of work to create JPEG XL. For whatever reason, Chrome is not agreeing to ship it, which will make it a lot harder for it to be widely adopted. So they're now applying the same techniques to classic JPEG, where the work they did can provide value immediately and not be subject to a pocket veto by Chrome. reply jbverschoor 12 hours agorootparentThe confusing part is that the code is in the jxl repository. Is it Google's code, JXL's, JXL's people under Google? reply lonjil 11 hours agorootparentAll the developers involved are part of the JXL team at Google Zürich, as far as I can tell. reply udkl 9 hours agoprevnext [3 more] [flagged] Dylan16807 1 hour agoparentAre you asking this just because you saw the word \"compression\" from google, or is there a better connection I'm missing. reply batch12 9 hours agoparentprevIt's April 3rd, confirmed reply publius_0xf3 12 hours agoprevCan we just get rid of lossy image compression, please? It's so unpleasant looking at pictures on social media and watching them degrade over time as they are constantly reposted. What will these pictures look like a century from now? reply adamzochowski 11 hours agoparentPlease, keep lossy compression. Web is unusable already with websites too big as it is. What should happen: websites/applications shouldn't recompress images if they already deliver good pixel bitrate. Websites/applicates shouldn't recompress images just to add own watermarks. reply npteljes 2 hours agoparentprevIt's not the social network's job to preserve image quality. That would be mixing up the concerns. reply pizlonator 13 hours agoprev [–] Very interesting that new projects like this still use C++, not something like Rust. reply mkl 11 hours agoparent [–] When your aim is maximum adoption and compatibility with existing C++ software, C++ or C are the best choice. When you're building on an existing C++ codebase, switching language and doing a complete rewrite is very rarely sensible. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google has launched a new JPEG coding library named Jpegli, providing a 35% enhancement in compression ratio at high-quality settings.",
      "Jpegli is engineered to be quicker, more effective, and aesthetically appealing compared to standard JPEG, incorporating adaptive quantization heuristics and enhanced quantization matrix selection.",
      "Testing indicates that Jpegli can compress high-quality images more effectively than traditional JPEG codecs, potentially leading to a faster and visually stunning internet experience."
    ],
    "commentSummary": [
      "Google Zürich introduces Jpegli, a new JPEG coding library, focusing on enhancing image quality and compression efficiency using Swiss German naming conventions.",
      "Discussions include comparing image quality with various codecs, evaluating formats like AVIF and JPEG XL, and debates on Google's choice not to include JPEG XL in Chrome.",
      "Users consider integrating Jpegli with WebAssembly to broaden its usage, emphasizing optimization of current formats rather than creating a new one, supporting high bit-depth encoding."
    ],
    "points": 307,
    "commentCount": 200,
    "retryCount": 0,
    "time": 1712166696
  },
  {
    "id": 39915594,
    "title": "PyTorch's ipex-llm: Intel-Compatible LLM Library",
    "originLink": "https://github.com/intel-analytics/ipex-llm",
    "originBody": "Important bigdl-llm has now become ipex-llm (see the migration guide here); you may find the original BigDL project here. 💫 IPEX-LLM IPEX-LLM is a PyTorch library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max) with very low latency1. Note It is built on top of Intel Extension for PyTorch (IPEX), as well as the excellent work of llama.cpp, bitsandbytes, vLLM, qlora, AutoGPTQ, AutoAWQ, etc. It provides seamless integration with llama.cpp, Text-Generation-WebUI, HuggingFace transformers, HuggingFace PEFT, LangChain, LlamaIndex, DeepSpeed-AutoTP, vLLM, FastChat, HuggingFace TRL, AutoGen, ModeScope, etc. 50+ models have been optimized/verified on ipex-llm (including LLaMA2, Mistral, Mixtral, Gemma, LLaVA, Whisper, ChatGLM, Baichuan, Qwen, RWKV, and more); see the complete list here. Latest Update 🔥 [2024/03] bigdl-llm has now become ipex-llm (see the migration guide here); you may find the original BigDL project here. [2024/02] ipex-llm now supports directly loading model from ModelScope (魔搭). [2024/02] ipex-llm added inital INT2 support (based on llama.cpp IQ2 mechanism), which makes it possible to run large-size LLM (e.g., Mixtral-8x7B) on Intel GPU with 16GB VRAM. [2024/02] Users can now use ipex-llm through Text-Generation-WebUI GUI. [2024/02] ipex-llm now supports Self-Speculative Decoding, which in practice brings ~30% speedup for FP16 and BF16 inference latency on Intel GPU and CPU respectively. [2024/02] ipex-llm now supports a comprehensive list of LLM finetuning on Intel GPU (including LoRA, QLoRA, DPO, QA-LoRA and ReLoRA). [2024/01] Using ipex-llm QLoRA, we managed to finetune LLaMA2-7B in 21 minutes and LLaMA2-70B in 3.14 hours on 8 Intel Max 1550 GPU for Standford-Alpaca (see the blog here). More updates ipex-llm Demos See the optimized performance of chatglm2-6b and llama-2-13b-chat models on 12th Gen Intel Core CPU and Intel Arc GPU below. 12th Gen Intel Core CPU Intel Arc GPU chatglm2-6b llama-2-13b-chat chatglm2-6b llama-2-13b-chat ipex-llm Quickstart Install ipex-llm Windows GPU: installing ipex-llm on Windows with Intel GPU Linux GPU: installing ipex-llm on Linux with Intel GPU Docker: using ipex-llm dockers on Intel CPU and GPU For more details, please refer to the installation guide Run ipex-llm llama.cpp: running ipex-llm for llama.cpp (using C++ interface of ipex-llm as an accelerated backend for llama.cpp on Intel GPU) vLLM: running ipex-llm in vLLM on both Intel GPU and CPU FastChat: running ipex-llm in FastChat serving on on both Intel GPU and CPU LangChain-Chatchat RAG: running ipex-llm in LangChain-Chatchat (Knowledge Base QA using RAG pipeline) Text-Generation-WebUI: running ipex-llm in oobabooga WebUI Benchmarking: running (latency and throughput) benchmarks for ipex-llm on Intel CPU and GPU Code Examples Low bit inference INT4 inference: INT4 LLM inference on Intel GPU and CPU FP8/FP4 inference: FP8 and FP4 LLM inference on Intel GPU INT8 inference: INT8 LLM inference on Intel GPU and CPU INT2 inference: INT2 LLM inference (based on llama.cpp IQ2 mechanism) on Intel GPU FP16/BF16 inference FP16 LLM inference on Intel GPU, with possible self-speculative decoding optimization BF16 LLM inference on Intel CPU, with possible self-speculative decoding optimization Save and load Low-bit models: saving and loading ipex-llm low-bit models GGUF: directly loading GGUF models into ipex-llm AWQ: directly loading AWQ models into ipex-llm GPTQ: directly loading GPTQ models into ipex-llm Finetuning LLM finetuning on Intel GPU, including LoRA, QLoRA, DPO, QA-LoRA and ReLoRA QLoRA finetuning on Intel CPU Integration with community libraries HuggingFace tansformers Standard PyTorch model DeepSpeed-AutoTP HuggingFace PEFT HuggingFace TRL LangChain LlamaIndex AutoGen ModeScope Tutorials For more details, please refer to the ipex-llm document website. Verified Models Over 50 models have been optimized/verified on ipex-llm, including LLaMA/LLaMA2, Mistral, Mixtral, Gemma, LLaVA, Whisper, ChatGLM2/ChatGLM3, Baichuan/Baichuan2, Qwen/Qwen-1.5, InternLM and more; see the list below. Model CPU Example GPU Example LLaMA (such as Vicuna, Guanaco, Koala, Baize, WizardLM, etc.) link1, link2 link LLaMA 2 link1, link2 link ChatGLM linkChatGLM2 link link ChatGLM3 link link Mistral link link Mixtral link link Falcon link link MPT link link Dolly-v1 link link Dolly-v2 link link Replit Code link link RedPajama link1, link2Phoenix link1, link2StarCoder link1, link2 link Baichuan link link Baichuan2 link link InternLM link link Qwen link link Qwen1.5 link link Qwen-VL link link Aquila link link Aquila2 link link MOSS linkWhisper link link Phi-1_5 link link Flan-t5 link link LLaVA link link CodeLlama link link Skywork linkInternLM-XComposer linkWizardCoder-Python linkCodeShell linkFuyu linkDistil-Whisper link link Yi link link BlueLM link link Mamba link link SOLAR link link Phixtral link link InternLM2 link link RWKV4link RWKV5link Bark link link SpeechT5link DeepSeek-MoE linkZiya-Coding-34B-v1.0 linkPhi-2 link link Yuan2 link link Gemma link link DeciLM-7B link link Deepseek link link StableLM link link Footnotes Performance varies by use, configuration and other factors. ipex-llm may not optimize to the same degree for non-Intel products. Learn more at www.Intel.com/PerformanceIndex. ↩",
    "commentLink": "https://news.ycombinator.com/item?id=39915594",
    "commentBody": "PyTorch Library for Running LLM on Intel CPU and GPU (github.com/intel-analytics)290 points by ebalit 23 hours agohidepastfavorite91 comments vegabook 20 hours agoThe company that did 4-cores-forever, has the opportunity to redeem itself, in its next consumer GPU release, by disrupting the \"8-16GB VRAM forever\" that AMD and Nvidia have been imposing on us for a decade. It would be poetic to see 32-48GB at a non-eye-watering price point. Intel definitely seems to be doing all the right things on software support. reply riskable 19 hours agoparentNo kidding... Intel is playing catch-up with Nvidia in the AI space and a big reason for that is their offerings aren't competitive. You can get an Intel Arc A770 with 16GB of VRAM (which was released in October, 2022) for about $300 or an Nvidia 4060 Ti with 16GB of VRAM for ~$500 which is twice as fast for AI workloads in reality (see: https://cdn.mos.cms.futurecdn.net/FtXkrY6AD8YypMiHrZuy4K-120... ) This is a huge problem because in theory the Arc A770 is faster! It's theoretical performance (TFLOPS) is more than twice as fast as an Nvidia 4060 (see: https://cdn.mos.cms.futurecdn.net/Q7WgNxqfgyjCJ5kk8apUQE-120... ). So why does it perform so poorly? Because everything AI-related has been developed and optimized to run on Nvidia's CUDA. Mostly, this is a mindshare issue. If Intel offered a workstation GPU (i.e. not a ridiculously expensive \"enterprise\" monster) that developers could use that had something like 32GB or 64GB of VRAM it would sell! They'd sell zillions of them! In fact, I'd wager that they'd be so popular it'd be hard for consumers to even get their hands on one because it would sell out everywhere. It doesn't even need to be the fastest card. It just needs to offer more VRAM than the competition. Right now, if you want to do things like training or video generation the lack of VRAM is a bigger bottleneck than the speed of the GPU. How does Intel not see this‽ They have the power to step up and take over a huge section of the market but instead they're just copying (poorly) what everyone else is doing. reply Workaccount2 19 hours agorootparentBased on leaks, it looks like intel somehow missed an easy opportunity here. There is an insane demand for high VRAM cards now, and it seems the next intel cards will be 12GB. Intel, screw everything else, just pack as much VRAM in those as you can. Build it and they will come. reply dheera 16 hours agorootparentExactly, I'd love to have 1TB of RAM that can be accessed at 6000 MT/s. reply talldayo 14 hours agorootparentOptane is crying and punching the walls right now. reply yjftsjthsd-h 13 hours agorootparentDoes optane have an advantage over RAM here? reply watersb 12 hours agorootparentOptane products were sold as DIMMS with single-DIMM capacity as high as 512 GB. With an Intel memory controller that could make it look like DRAM. 512 GB. It was slower than conventional DRAM. But for AI models, Optane may have an advantage: it's bit-addressable. I'm not aware of any memory controllers that exposed that single-bit granularity; Optane was fighting to create a niche for itself, between DRAM and NAND Flash: pretending to be both, when it was neither. Bit-level operations, computational units in the same device as massive storage, is an architecture that has yet to be developed. AI GPUs try to be such an architecture by plopping 16GB of HBM next to a sea of little dot-product engines. reply wtallis 10 hours agorootparent> But for AI models, Optane may have an advantage: it's bit-addressable. That's an advantage over NAND but not over DRAM. Fundamentally, DRAM is also bit-addressable, but everybody uses DRAM parts with memory cells organized into a hierarchy of groupings for reasons that mostly apply to 3D XPoint memory. reply ponector 18 hours agorootparentprevI don't agree. Who will buy it? A few enthusiasts who wants to run LLM locally but cannot afford M3 or 4090? It will be a niche product with poor sales. reply bt1a 18 hours agorootparentI think there's more than a few enthusiasts who would be very interesting in buying 1 or more of these cards (if they had 32+ GB of memory), but I don't have any data to back that opinion up. It is not only those who can't afford a 4090 though. While the 4090 can run models that use less than 24GB of memory at blistering speeds, models are going to continue to scale up and 24GB is fairly limiting. Because LLM inference can take advantage of splitting the layers among multiple GPUs, high memory GPUs that aren't super expensive are desirable. To share a personal perspective, I have a desktop with a 3090 and an M1 Max Studio with 64GB of memory. I use the M1 for local LLMs because I can use up to 57~GB of memory, even though the output (in terms of tok/s) is much slower than ones I can fit on a 3090. reply michaelbrave 7 hours agorootparentRight now I have a 3090TI so it's not worth it for me to upgrade to a 4090, but I do run into Vram constraints a lot, especially with merging stable diffusion models, especially as the models get larger (XL-Cascade-etc). As I move toward running multiple LLMs at a time I run into similar problems. I would gladly buy a card that ran a touch slower but had massive Vram, especially if it was affordable, but I guess that puts me into that camp of enthusiasts you mentioned. reply Dalewyn 16 hours agorootparentprev>models are going to continue to scale up and 24GB is fairly limiting >24GB is fairly limiting Can I take a moment to suggest that maybe we're very spoiled? 24GB of VRAM is more than most peoples' system RAM, and that is \"fairly limiting\"? To think Bill once said 640KB would be enough. reply hnfong 16 hours agorootparentIt doesn't matter whether anyone is \"spoiled\" or not. The fact is large language models require a lot of VRAM, and the more interesting ones need more than 24GB to run. The people who are able to afford systems with more than 24GB VRAM will go buy hardware that gives them that, and when GPU vendors release products with insufficient VRAM they limit their market. I mean inequality is definitely increasing at a worrying rate these days, but let's keep the discussion on topic... reply Dalewyn 15 hours agorootparentI'm just fascinated that the response/demand to running out of RAM is \"Just sell us more RAM, god damn!\" instead of engineering a solution to make due with what is practically (and realistically) available. reply dekhn 13 hours agorootparentI would say that increasing RAM to avoid engineering a solution has long been a successful strategy. i learned my RAM lesson when I bought my first real linux PC. it had 4MB of RAM, which was enough to run X, bash, xterm, and emacs. But once I ran all that and also wanted to compile with g++, it would start swapping, which in the days of slow hard drives, was death to productivity. I spent $200 to double to 8MB, and then another $200 to double to 16MB, and then finally, $200 to max out the RAM on my machine-- 32MB! And once I did that everything flew. Rather than attempting to solve the problem by making emacs (eight megs and constantly swapping) use less RAM, or find a way to hack without X, I deployed money to max out my machine (which was practical, but not realistically available to me unless I gave up other things in life for the short term). Not only was I more productive, I used that time to work on other engineering problems which helped build my career, while also learning an important lesson about swapping/paging. People demand RAM and what was not practically available is often available 2 years later as standard. Seems like a great approach to me, especially if you don't have enough smart engineers to work around problems like that (see \"How would you sort 4M integers in 2M of RAM?\") reply watersb 12 hours agorootparent> I spent $200 to double to 8MB, and then another $200 to double to 16MB, and then finally, $200 to max out the RAM on my machine-- 32MB! Thank you. Now I feel a log better for dropping $700 on the 32MB of RAM when I built my first rig. reply nl 11 hours agorootparentprevWhile saying \"we want more efficiency\" is great there is a trade off between size and accuracy here. It is possible that compressing and using all of human knowledge takes a lot of memory and in some cases the accuracy is more important than reducing memory usage. For example [1] shows how Gemma 2B using AVX512 instructions could solve problems it couldn't solve using AVX2 because of rounding issues with the lower-memory instructions. It's likely that most quantization (and other memory reduction schemes) have similar problems. As we develop more multi-modal models that can do things like understand 3D video in better than real time it's likely memory requirements will increase, not decrease. [1] https://github.com/google/gemma.cpp/issues/23 reply xoranth 15 hours agorootparentprevPeople have engineered solutions to make what is available practical (see all the various quantization schemes that have come out). It is just that there's a limit to how much you can compress the models. reply michaelt 12 hours agorootparentprevThere has in fact been a great deal of careful engineering to allow 70 billion parameter models to run on just 48GB of VRAM The people training 70B parameter models from scratch need ~600GB of VRAM to do it! reply hnfong 3 hours agorootparentprevWhat makes you think people aren't trying to engineer a solution that uses less RAM? There are millions (billions?) of dollars at stake here, and obviously the best minds are already tackling the problem. Only plebs like us who don't have the skills to do so bicker on an internet forum... It's not like we could realistically spend the time inventing ways to run inference with fewer resources and make significant headway. reply whiplash451 12 hours agorootparentprevBy the same logic, we’d still be writing assembly code on 640KB RAM machines in 2024. reply loudmax 17 hours agorootparentprevI tend to agree that it would be niche. The machine learning enthusiast market is far smaller than the gamer market. But selling to machine learning enthusiasts is not a bad place to be. A lot of these enthusiasts are going to go on to work at places that are deploying enterprise AI at scale. Right now, almost all of their experience is CUDA and they're likely to recommend hardware they're familiar with. By making consumer Intel GPUs attractive to ML enthusiasts, Intel would make their enterprise GPUs much more interesting for enterprise. reply mysteria 17 hours agorootparentThe problem is that this now becomes a long term investment, which doesn't work out when we have CEOs chasing quarterly profits and all that. Meanwhile Nvidia stuck with CUDA all those years back (while ensuring that it worked well on both the consumer and enterprise line) and now they reap the rewards. reply Wytwwww 15 hours agorootparentCurrent Intel and its leadership seems to be much more focused on long term goals/growth than before, or so they claim. reply resource_waste 16 hours agorootparentprevI need offline LLMs for work. It doesnt need to be consumer grade, it doesnt need to be ultra high either. It needs to be cheap enough for my department to expensive it via petty cash. reply antupis 16 hours agorootparentprevIt would be same playbook that NVIDIA did CUDA where was market 2010 when it was research labs and hobbyists doing vector calculations. reply Aerroon 16 hours agorootparentprevIt's about mindshare. Random people using your product to do AI means that the tooling is going to improve because people will try to use them. But as it stands right now if you think there's any chance you want to use AI in the next 5 years, then why would you buy anything other than Nvidia? It doesn't even matter if that's your primary goal or not. reply talldayo 18 hours agorootparentprev> Who will buy it? Frustrated AMD customers willing to put their money where their mouth is? reply jmward01 14 hours agorootparentprevMicrosoft got where they are because the developed tools that everyone used. The got the developers and the consumers followed. Intel (or AMD) could do the same thing. Get a big card with lost of ram so that the developers get used to your ecosystem and then sell the enterprise GPUs to make the $$$. It is a clear path with a lot of history and it blows my mind Intel and AMD aren't doing it. reply zoobab 1 hour agorootparent\"Microsoft got where they are because the developed tools that everyone used.\" It's not like they don't have a monopoly on pre-installed OSes. reply resource_waste 16 hours agorootparentprev>M3 >4090 These are noob hardware. A6000 is my choice. Which really only further emphesizes your point. >CPU based is a waste of everyone's time/effort >GPU based is 100% limited by VRAM, and is what you are realistically going to use. reply alecco 16 hours agorootparentprevAFAIK, unless you are a huge American corp with orders above $100m Nvidia will only sell you old and expensive server cards like the crappy A40 PCIe 4.0 48GB GDDR6 at $5,000. Good luck getting SXM H100s or GH200. If Intel sells a stackable kit with a lot of RAM and a reasonable interconnect a lot of corporate customers will buy. It doesn't even have to be that good, just half way between PCIe 5.0 and NVLink. But it seems they are still too stuck in their old ways. I wouldn't count on them waking up. Nor AMD. It's sad. reply ponector 12 hours agorootparentParent comment requested non-enterprise, consumer grade GPU with tons of memory. I'm sure there is no market for this. However, server solutions could have some traction. reply alecco 1 hour agorootparentHobbyists are stacking 3090s with NVLink. reply glitchc 18 hours agorootparentprevI think the answer to that is fairly straightforward. Intel isn't in the business of producing RAM. They would have to buy and integrate a third-party product which is likely not something their business side has ever contemplated as a viable strategy. reply monocasa 18 hours agorootparentTheir GPUs as sold already include RAM. reply glitchc 14 hours agorootparentYes, but they don't fab their own RAM. It's a cost center for them. reply monocasa 13 hours agorootparentIf they can sell the board with more RAM for more than their extra RAM costs, or can sell more GPUs total but the RAM itself is priced essentially at cost, then it's not a cost center. reply RussianCow 12 hours agorootparentprevThat's not what a cost center is. There is an opportunity for them to make more money by putting more RAM into their GPUs and exposing themselves to a different market. Whether they physically manufacture that RAM doesn't matter in the slightest. reply glitchc 10 hours agorootparentI agree with you, it makes good business sense. No doubt. I'm merely positing that Intel executives are not used to doing business as integrators. Their core business has always been that of a supplier. reply chessgecko 19 hours agoparentprevGoing above 24GB is probably not going to be cheap until gddr7 is out, and even that will only push it to 36gb. The fancier stacked gddr6 stuff is probably pretty expensive and you can’t just add more dies because of signal integrity issues. reply frognumber 19 hours agorootparentAssuming you want to maintain full bandwidth. Which I don't care too much about. However, even 16->24GB is a big step, since a lot of the model are developed for 3090/4090-class hardware. 36GB would place it lose to the class of the fancy 40GB data center cards. If Intel decided to push VRAM, it will definitely have a market. Critically, a lot of folks will also be incentivized to make software compatible, since it will be the cheapest way to run models. reply 0cf8612b2e1e 18 hours agorootparentAt this point, I cannot run an entire class of models without OOM. I will take a performance hit if it lets me run it at all. I want a consumer card that can do some number of tokens per second. I do not need a monster that can serve as the basis for a startup. reply hnfong 16 hours agorootparentA maxed out Mac Studio probably fits your requirements as stated. reply 0cf8612b2e1e 15 hours agorootparentIf I were willing to drop $4k on that setup, I might as well get the real NVidia offering. The hobbyist market needs something priced well under $1k to make it accessible. reply rnewme 17 hours agorootparentprevHow comes you don't care about full bandwidth? reply frognumber 8 hours agorootparentMostly because I use this for development. If a model takes twice as long to run.... I'll live. Worst-case, it will be mildly annoying. If I can't run a model, that's a critical failure. There's a huge step up CPU->GPU which I need, but 3060 versus 4090 isn't a big deal at all. Indeed, the 24GB versus 16GB is a bigger difference than the number of CUDA cores. reply Dalewyn 16 hours agorootparentprevThe thing about RAM speed (aka bandwidth) is that it becomes irrelevant if you run out and have to page out to slower tiers of storage. reply zoobab 19 hours agoparentprev\"It would be poetic to see 32-48GB at a non-eye-watering price point.\" I heard some Asrock motherboard BIOSes could set the VRAM up to 64GB on Ryzen5. Doing some investigations with different AMD hardware atm. reply stefanka 18 hours agorootparentThat would be an interesting information. Which MB works with with which APU with 32 or more GB of VRAM. Can you post your findings please? reply LoganDark 15 hours agorootparentprevWhen has an APU ever been as fast as a GPU? How much cache does it have, a few hundred megabytes? That can't possibly be enough for matmul, no matter how much slow DDR4/5 is technically addressable. reply zoobab 40 minutes agorootparent\"APU ever been as fast as a GPU\" Ryzen5 has both CPU+GPU on one chip, the BIOS allows you set the amount of VRAM. They share the same RAM bank, you can set 16GB of VRAM and 16GB for the OS if you use a 32GB RAM bank. reply sitkack 20 hours agoparentprevWhat is obvious to us, is an industry standard to Product Managers. When is the last time you have seen an industry player upset the status quo? Intel has not changed that much. reply OkayPhysicist 15 hours agoparentprevThe issue from the manufacturer's perspective is that they've got two different customer bases with wildly different willingness to pay, but not substantially different needs from their product. If Nvidia and AMD didn't split the two markets somehow, then there would be no cards available to the PC market, since the AI companies with much deeper pockets would buy up the lot. This is undesirable from the manufacturer's perspective for a couple reasons, but I suspect a big one is worries that the next AI winter would cause their entire business to crater out, whereas the PC market is pretty reliable for the foreseeable future. Right now, the best discriminator they have is that PC users are willing to put up with much smaller amounts of VRAM. reply haunter 18 hours agoparentprevFirst crypto then AI, I wish GPUs were left alone for gaming. reply talldayo 18 hours agorootparentAre there actually gamers out there that are still struggling to source GPUs? Even at the height of the mining craze, it was still possible to backorder cards at MSRP if you're patient. The serious crypto and AI nuts are all using custom hardware. Crypto moved onto ASICs for anything power-efficient, and Nvidia's DGX systems aren't being cannibalized from the gaming market. reply azinman2 18 hours agorootparentprevDidn’t nvidia try to block this in software by slowing down mining? Seems like we just need consumer matrix math cards with literally no video out, and then a different set of requirements for those with a video out. reply wongarsu 17 hours agorootparentBut Nvidia doesn't want to make consumer compute cards because those might steal market share from the datacenter compute cards they are selling at 5x markup. reply talldayo 10 hours agorootparentNvidia does sell consumer compute cards, they're just sold at datacenter compute prices: https://www.newegg.com/p/pl?d=nvidia+quadro Nvidia's approach to software certainly deserves scrutiny, but their hardware lineup is so robust that I find it hard to complain. Jetson already exists for low-wattage solutions, and gaming cards can run Nvidia datacenter drivers on headless Linux without issue. The consumer compute cards are already here, you just aren't using them. reply baq 15 hours agorootparentprevThey were. But then those pesky researchers and hackers figured out how to use the matmul hardware for non-gaming. reply belter 19 hours agoparentprevAMD making drivers of high quality? I would pay to see that :-) reply UncleOxidant 16 hours agoparentprev> Intel definitely seems to be doing all the right things on software support. Can you elaborate on this? Intel's reputation for software support hasn't been stellar, what's changed? reply whalesalad 19 hours agoparentprevstill wondering why we can't have gpu's with sodimm slots so you can crank the vram reply amir_karbasi 19 hours agorootparentI believe that the issue is that graphic cards require really fast memory. This requires close memory placement (that's why the memory is so close to the core on the board). expandable memory will not be able to provide the required bandwidth here. reply frognumber 19 hours agorootparentThe universe used to have hierarchies. Fast memory close, slow memory far. Registers. L1. L2. L3. RAM. Swap. The same thing would make a lot of sense here. Super-fast memory close, with overflow into classic DDR slots. As a footnote, going parallel also helps. 8 sticks of RAM at 1/8 the bandwidth each is the same as one stick of RAM at 8x the bandwidth, if you don't multiplex onto the same traces. reply riskable 19 hours agorootparentIt's not so simple... The way GPU architecture works is that it needs as-fast-as-possible access to its VRAM. The concept of \"overflow memory\" for a GPU is your PC's RAM. Adding a secondary memory controller and equivalent DRAM to the card itself would only provide a trivial improvement over, \"just using the PC RAM\". Point of fact: GPUs don't even use all the PCI Express lanes they have available to them! Most GPUs (even top of the line ones like Nvidia's 4090) only use about 8 lanes of bandwidth. This is why some newer GPUs are being offered with M.2 slots so you can add an SSD (https://press.asus.com/news/asus-dual-geforce-rtx-4060-ti-ss... ). reply wongarsu 17 hours agorootparentprevGPUs have memory hierarchies too. A 4090 has about 16MB of L1 cache and 72MB of L2 cache, followed by the 24GB of GDDR6 RAM, followed by host ram that can be accessed via PCIe. The issue is that GPUs are massively parallel. A 4090 has 128 streaming multiprocessors, each executing 128 \"threads\" or \"lanes\" in parallel. If each \"thread\" works on a different part of memory that leaves you with 1kB of L1 cache per thread, and 4.5kB of L2 cache each. For each clock cycle you might be issuing thousands of request to your memory controller for cache misses and prefetching. That's why you want insanely fast RAM. You can write CUDA code that directly accesses your host memory as a layer beyond that, but usually you want to transfer that data in bigger chunks. You probably could make a card that adds DDR4 slots as an additional level of hierarchy. It's the kind of weird stuff Intel might do (the Phi had some interesting memory layout ideas). reply magicalhippo 8 hours agorootparentprevIsn't part of the problem that the connectors add too much inductance, making the lines difficult to drive at high speed? Similar issue to distance I suppose but more severe. reply riskable 19 hours agorootparentprevYou can do this sort of thing but you can't use SODIMM slots because that places the actual memory chips too far away from the GPU. Instead what you need is something like BGA sockets (https://www.nxp.com/design/design-center/development-boards/... ) which are stupidly expensive (e.g. $600 per socket). reply monocasa 17 hours agorootparentYou could probably use something like CAMM which solved a similar problem for lpddr. https://en.wikipedia.org/wiki/CAMM_(memory_module) reply chessgecko 19 hours agorootparentprevYou could, but the memory bandwidth wouldn’t be amazing unless you had a lot of sticks and it would end up getting pretty expensive reply justsomehnguy 19 hours agorootparentprevLook at the motherboards with >2 Memory channels. That would require a lot of physical space, which is quite restricted on a 50 y/o standard for the expansion cards. reply Hugsun 22 hours agoprevI'd be interested in seeing benchmark data. The speed seemed pretty good in those examples. reply captaindiego 21 hours agoprevAre there any Intel GPUs with a lot of vRAM that someone could recommend that would work with this? reply Aromasin 21 hours agoparentThere's the Max GPU (Ponte Vecchio), their datacentre offering, with 128GB of HBM2e memory, 408 MB of L2 cache, and 64 MB of L1 cache. Then there's Gaudi, which has similar numbers but with cores specific for AI workloads (as far as I know from the marketing). You can pick them up in prebuilds from Dell and Supermicro: https://www.supermicro.com/en/accelerators/intel Read more about them here: https://www.servethehome.com/intel-shows-gpu-max-1550-perfor... reply goosedragons 21 hours agoparentprevFor consumer stuff there's the Intel Arc A770 with 16GB VRAM. More than that and you start moving into enterprise stuff. reply ZeroCool2u 18 hours agorootparentWhich seems like their biggest mistake. If they would just release a card with more than 24GB VRAM, people would be clamoring for their cards, even if they were marginally slower. It's the same reason that 3090's are still in high demand compared to the 4090's. reply DrNosferatu 21 hours agoprevAny performance benchmark against 'llamafile'[0] or others? [0] - https://github.com/mozilla-Ocho/llamafile reply VHRanger 20 hours agoparentYou can already use intel GPUs (both ARC and iGPUS) with llama.cpp on a bunch of backends: - SYCL [1] - Vulkan - OpenCL I don't own the hardware, but I imagine SYCL is more performant for ARC , because it's the one intel is pushing for their datacenter stuff [1]: https://www.intel.com/content/www/us/en/developer/articles/t... reply donnygreenberg 18 hours agoprevWould be nice if this came with scripts which could launch the examples on compatible GPUs on cloud providers (rather than trying to guess?). Would anyone else be interested in that? Considering putting it together. reply antonp 21 hours agoprevHm, no major cloud provider offers intel gpus. reply belthesar 21 hours agoparentIntel GPUs got quite a bit of penetration in the SE Asian market, and Intel is close to releasing a new generation. In addition, Intel's allowing for GPU virtualization without additional license fees (unlike Nvidia and GRID licenses), allowing hosting operators to carve up these cards. I have a feeling we're going to see a lot more Intel offerings available. reply VHRanger 21 hours agoparentprevNo, but for consumers they're a great offering. 16GB RAM and performance around a 4060ti or so, but for 65% of the price reply _joel 21 hours agorootparentand 65% of the software support, less I'm inclined to believe? Although having more players in the fold is definitely a good thing. reply VHRanger 21 hours agorootparentIntel is historically really good at the software side, though. For all their hardware research hiccups in the last 10 years, they've been delivering on open source machine learning libraries. It's apparently the same on driver improvements and gaming GPU features in the last year. reply frognumber 19 hours agorootparentI'm optimistic Intel will get the software right in due course. Last I looked, it wasn't all there yet, but it was on the right track. Right now, I have a nice NVidia card, but if things stay on track, I think it's very likely my next GPU might be Intel. Open-source, not to mention better value. reply HarHarVeryFunny 19 hours agorootparentprevBut even if Intel have stable optimized drivers and ML support, it'd still need to be supported by PyTorch/etc for most developers to want to use it. People want to write at high level, not at CUDA-type level. reply VHRanger 18 hours agorootparentIntel is supported in Pytorch, though. It's supported from their own branch, which is presumably a big annoyance to install, but they do work reply HarHarVeryFunny 13 hours agorootparentI just tried googling for Intel's PyTorch, and it's clear as mud as to exactly what's run on the GPU and what is not. I assume they'd be bragging about it if this ran everything on their GPU the same as it would on NVDIA, so I'm guessing it just accelerates some operations. reply anentropic 21 hours agoparentprevLots offer Intel CPUs though... reply tomrod 22 hours agoprev [–] Looking forward to reviewing! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The bigdl-llm project has shifted to ipex-llm, a PyTorch library tailored for executing LLM on Intel CPU and GPU with minimal latency.",
      "ipex-llm, based on Intel Extension for PyTorch, brings seamless integration with various tools and models, offering optimizations and finetuning options for LLM models on Intel hardware.",
      "Over 50 models are optimized on ipex-llm, supporting low-bit inference, self-speculative decoding, and compatibility with community libraries, detailed in the documentation with benchmarks, code samples, and installation guides; performance might vary depending on usage and configurations, especially on non-Intel devices."
    ],
    "commentSummary": [
      "GitHub discussions highlight Intel's CPUs and GPUs potential in the GPU market compared to AMD and Nvidia, emphasizing higher VRAM capacity at competitive prices for AI workloads.",
      "The conversation suggests Intel should focus on boosting VRAM capacity in their GPU launches, optimizing memory performance, and meeting the consumer demand for high RAM capacity at budget-friendly prices.",
      "Participants express optimism about Intel's software support and value proposition, foreseeing a promising future for Intel GPUs against other competitors."
    ],
    "points": 290,
    "commentCount": 91,
    "retryCount": 0,
    "time": 1712140105
  },
  {
    "id": 39916144,
    "title": "Enhancing Development with LiveView and Svelte",
    "originLink": "https://blog.sequin.io/liveview-is-best-with-svelte/",
    "originBody": "Back to posts LiveView is best with Svelte Anthony Accomazzo • Apr 1, 2024 • 12 min read We’re Sequin. We turn 3rd-party API data (e.g. Salesforce, AWS) into Kafka topics. As an infrastructure company, we questioned if we really need a SPA. So, we started with LiveView, which helped us move fast but left us wanting more. This post is about that journey. Phoenix's LiveView has polarized our team. Compared to SPA, there are components and features that we’re able to build 2-3x faster. Conversely, there are components and features that are frustrating to build or feel very counterintuitive. Said another way, LiveView makes a lot of things easy. But it also makes some easy things hard. This created tension. Do we keep forging down this path? Or do we give in and convert our app to a SPA? Fortunately, we found a companion library called LiveSvelte. LiveView enables a development experience with Svelte that’s unlike any other fullstack paradigm I’ve used. The team agrees: this is a killer way to build. To appreciate the LiveView+Svelte paradigm, I’ll start by explaining how LiveView works and what makes it different. Then, I’ll detail the friction we encountered with a pure LiveView approach. At that point, you’ll be able to appreciate what LiveSvelte offers. What is LiveView LiveView offers a very unique way to build web applications. In a traditional server-rendered web application, the server is stateless. The client requests a page and the server renders it. All client actions route back to the server, which re-renders the next page. In a SPA, the client is in charge of building pages. It leverages a backend API to read and write data. Client apps are stateful (e.g. useState in React). In LiveView, the server is in charge of rendering the page. But it’s stateful. Actions in the frontend are handled by the backend, but the server incrementally updates the DOM, much like in a SPA. At a high-level, the reason a SPA is complex is because distributed systems are complex. Supporting a client JS app is supporting a microservice (and one that runs in a hostile, untrusted environment, no less!) In theory your frontend app uses a backend REST API that could be used to support lots of different services and clients. In reality, the needs of your frontend app are unique. So your backend routes and controllers explode with functions that serve the needs of a single client. If nothing else, this complexity just means shaving a lot of yaks. Each request requires a fair bit of plumbing on both the frontend and the backend. Callstacks can easily exceed half a dozen layers: onMount await api.fetchUsers parseResponse Router.handle(/api/users) AuthPlug.verify_cookie UsersController.index Users.list_for_org ApiHelpers.prepare_response The promise of LiveView is that you get to create rich client-side experiences without the frontend microservice. You're back to the much simpler world where you can query the database in the function adjacent to the function that renders your table rows. If a new row comes in, you just need to push it to your table, and LiveView will update the client for you. But in addition, you also get to enjoy building an app using the stateful paradigm of frontend frameworks. It's much easier and faster to build rich interaction patterns this way vs prior backend paradigms where you'd need to \"rebuild the world\" on each request. Where LiveView makes easy things hard There's a lot of good stuff in LiveView. But there are also real thorns. There are two primary areas that we struggled with LiveView: Client-side state is inevitable There is a (literal) speed of light limitation with this approach: your server can only be so close to your users. Invariably, you’re going to need to do some stuff client-side. Animations, tooltips, showing/hiding DOM elements, disabling form fields, etc. For example, there’s a form in our app with two interdependent dropdowns. Selecting an option in the first dropdown allows our server to generate the list for the second dropdown. To get the best UX, you want to disable the second dropdown immediately after the first dropdown changes. Then, when it’s repopulated by the server, you can re-enable it: Simulating 1000ms of roundtrip latency between the client and the server. To pull this off, as far as we could tell, you need to use two independent concepts in LiveView: Use the JS module to disable the second dropdown when the first dropdown changes. Use a hook to register an event listener on the second dropdown. Then, send an action to re-enable the second dropdown from the backend. And for slightly more complex interaction patterns, you’ll need to incorporate a third concept, LiveView state. For example, maybe you only want to re-enable the second dropdown in certain conditions. The way these three concepts fit together is not obvious (we’re still not sure this is the right pattern!) So, while the server is in charge of a lot of DOM changes, it can’t command all of them. You use JS and hooks to sprinkle in JavaScript where needed. These tools feel side-chained to core LiveView, and therefore their patterns of use are not obvious. And the more JS and hooks you use, the more of your DOM state now exists outside of LiveView. This is a stark contrast to a paradigm like React. In React, it’s state and actions all the way down. With that core concept, you can do most anything. And there is no blurry line between DOM state and component state. React can take that approach because there’s no latency between client-side actions and client-side state. This means you can let React’s state paradigm handle every action and transition. Because all of LiveView’s state is server-side, it has to contend with the latency between client-side actions and server-side state. This means that while LiveView state looks like other frontend frameworks, the model is actually quite different. Take input fields, for example. In React, a character can’t be inserted into an input field without routing through state. This unlocks a powerful programming model, where your component re-renders – and therefore responds – to every keystroke. It gives the state and action paradigm a lot of reach, where you can use one core concept (useState) to solve a huge space of problems. In LiveView, it’s more accurate to say that the input field is changed by the user, and then a short while later LiveView finds out about it and reacts to it. With no latency, it looks a lot like React. But with increased latency, it’s quite a different paradigm. In frontend frameworks like React, you need to contend with server-side latency all the time. But when a high-latency action is going to take place is clear (i.e. you’re fetching from a server). In LiveView, the boundary is murkier. Three components LiveView has three different types of components: LiveViews, LiveComponents, and Components. LiveViews and LiveComponents are like stateful components in React, whereas Components are like functional components. Importantly, a LiveView will always be the uppermost parent component. You render LiveComponents and Components as children underneath a LiveView. In React, it's easy to switch between stateful and functional components–just add or remove useState hooks. The API for both are the same (they both accept props in the same way). And outside state, they have an identical feature set. For example, they can both register and respond to DOM events in the same way. The ease of switching between component types is important. As an app matures, you’re constantly factoring out components. You’re figuring out which bits should be reused, what should be generalized, where state should live, etc. In LiveView, all three components are very different. As a result, refactoring a LiveView into a LiveComponent is surprisingly cumbersome. In particular: The syntax for rendering and passing props to LiveViews and LiveComponents is different. The lifecycle of LiveViews and LiveComponents are different. The communication options between LiveViews and LiveComponents are different. For example, you send to LiveViews but send_update to LiveComponents. LiveComponents are not processes, and so can't interact with the rest of the system like LiveViews can. That last point is what makes LiveComponents so different and so frustrating. The limitations make sense: A LiveView is a process. That's one of the best parts about a LiveView, they're \"just processes\" and so they can fit into your Elixir/OTP system like every other process. For example, you can use pub/sub in a LiveView to subscribe to system-wide changes. A LiveComponent is not its own process, they are modules invoked by a LiveView. The parent LiveView process holds the state for all subcomponents. So, a LiveView has a pid, state, and an inbox; a LiveComponent does not. This means the LiveView also has to handle all message routing for its child LiveComponents. This is in keeping with Elixir/OTP design principles: processes are the building blocks. To give LiveComponents the same powers of independent state management and action handling, they would each need to be their own process. Still, for the life of me, I really struggled with LiveComponents. So often, I wanted to send my LiveComponent an event/action but didn't have a good way to do it. You end up using send_update, which is an awkward API. We couldn't decide: do we send actions via send_update, or do we use it to patch state? If we use it to patch state, how do we tell in our update clause whether we're mounting or updating? The elusive “LiveView way” LiveView often made us feel like we were “missing something.” The “LiveView way” feels elusive. Perhaps LiveView is in an uncanny valley. It shares a lot in common with contemporary frontend frameworks. So, our “React brains” and intuitions would kick in, driving us to use old patterns–but those would often lead to a dead end. More alienness would have forced us to recognize the differences and to approach problems differently. You can do a lot with just LiveView state and actions. But there are limits, and when you hit them you need to switch paradigms. It has components to help you organize and reuse code. But due to differences between JavaScript and Elixir, LiveView can’t really offer the same isomorphic component trees without a ton of abstraction, and so has LiveViews and LiveComponents. This is what makes LiveSvelte so promising. As you’ll see, it shifts more responsibility to the frontend. It embraces the fact that the frontend will have its own state. And it lets you take advantage of all the maturity of contemporary JavaScript component frameworks. LiveView + Svelte LiveSvelte lets you render Svelte components from LiveView. It's an awesome paradigm. There’s a couple different ways to render Svelte from your LiveViews, but the most basic way looks like this: # LiveView component defmodule Web.SyncLive.Form do def render(assigns) do assigns = assigns |> Map.put(:encoded_collections, Enum.map(assigns.collections, &encode_collection/1)) |> Map.put(:encoded_errors, encode_errors(assigns.changeset)) ~H\"\"\"\"\"\" end end This is an Elixir module, the LiveView. Inside the render, we first take our Elixir data structures and encode them for the frontend. We like the pattern of explicitly encoding Elixir structs and such as plain maps before passing to Svelte, like this: defp encode_collection(%Collection{} = collection) do %{ \"id\" => collection.id, \"slug\" => collection.slug, \"name\" => collection.name } end We’re able to set props on the Svelte component. Those are passed down as you’d expect to the component: // Svelte component export let resource; export let credential_options = []; export let errors = {}; export let live; One of the props that LiveSvelte sets for us is the live prop. To communicate from the Svelte component back up to the LiveView, we can call live.pushEvent. For example, check how easy it is to send the server changes to the form: // ... $: { live.pushEvent(\"form_updated\", { form }, () => {}); } This is a reactive block in Svelte. It will be executed whenever the variable form is changed. (Kind of like a useEffect, where form is the dependency.) The LiveView can handle and respond to the pushEvent using typical Elixir message handling semantics: # In the LiveView # ... @impl LiveView def handle_event(\"form_updated\", %{\"form\" => form}, socket) do params = decode_params(socket, form) {:noreply, merge_changeset(socket, params)} end defp merge_changeset(socket, params) do changeset = Collection.create_changeset(socket.assigns.resource, params) assign(socket, :changeset, changeset) end We first decode the params from the frontend, reversing any encoding/mapping we did on the way out. Then, merge_changeset/2 updates our changeset. If there are any validation errors in the changeset, those will make their way back to the frontend via the errors prop. So, you have data flow from Elixir down to the component via props. The LiveView process can update props at any time to cause the Svelte component to re-render. Any other communication can happen via the websocket. The boundary between the two is very clear–just as clear as in any SPA. What's most game-changing, though, is that you have a backend, stateful process that is collaborating with a frontend, stateful process. And it's so fun and productive. The three powerhouse properties: The backend controls the props on the frontend component. The frontend and the backend are stateful. You have a private, bi-directional communication channel between the two where either side can initiate a message to the other. #1 is made possible thanks to LiveView’s rendering paradigm: re-renders on the server are automatically pushed and applied to the client. This lets the server update props on the component just like a JS parent component can! #2 is possible because a LiveView is a process. Processes are how Elixir encapsulates and reduces state. #3 is made possible by the persistent websocket that LiveView gives you, wired to the frontend. Consider the differences between this paradigm and a SPA: First, all browser routing happens via the backend. This is a great simplifier. (In a regular SPA you have to maintain two sets of routes, one for the browser and one for your API.) Second, the backend is stateful. It knows what route you’re on. Which resource you’re working with. Each action it handles can be far more incremental, as it’s applying a state change to itself vs rebuilding state from scratch. Third, communication between the frontend and backend is private and coupled, as it should be. You’re not “polluting” your server’s public routes with a bunch of RPC calls that support a single component. When you see a pushEvent in the client, you know exactly where the handler for that is – in the collaborating Elixir module. Fourth, functionality is split across just two files. Sure, the backend module will call out to your backend functions (e.g. fetch data from database) and the frontend will import components and styles. But roundtrips between the two aren’t routing through a stack of API modules, routers, and controllers. Fifth, communication between frontend and backend is far less ceremonious. The backend can simply update props to inform frontend changes. And the frontend can pushEvent without needing handlers for expired tokens, timeouts, or outages. It’s binary: either the websocket is open which means the server is open for business, or it’s not in which case LiveView helpfully shows the user a global “disconnected” banner. In the simplest terms, the frontend microservice is eliminated. What you end up with feels like such a great split of responsibilities with very little boilerplate. All your business logic is on the backend – how you load data, which data to load, how to sort and filter the data, your validators, etc. Your frontend code is stupid simple. In Svelte, it’s all (1) if/end blocks to conditionally render stuff (2) animations and (3) a few dead simple pushEvent functions back to the server. That last part has been blowing my mind. The typical SPA frontend is full of so much logic, usually map, reduce, and filter in order to process server data, prepare data for display, or prepare data for the server. In a LiveSvelte app, all this can just happen server-side. The LiveView can prepare data exactly as the Svelte component needs it. This keeps complexity in your server language, in your server's data structures, and in your server's test suite. The backend LiveView and the frontend Svelte component aren't so much coupled as they are two halves: the LiveView only renders that Svelte component, and that Svelte component is only ever rendered by that LiveView. In contrast to a “regular” LiveView, this paradigm: Embraces state and state transitions in the frontend. Creates a clear boundary layer between the frontend and backend. Leverages Svelte’s component paradigm, which like other contemporary JS frameworks is very mature and familiar. In general, lets great frontend frameworks do what they do best! A pure LiveView approach doesn’t let you tap into this huge ecosystem. (For example, Svelte comes with great animation primitives.) By moving more into the frontend, we no longer felt like we were straddling an awkward middleground. We chose LiveSvelte because React didn't have a similarly complete LiveView library. The joy of working with Svelte has been a very happy bonus. Because LiveView does the heavy lifting with state management, our state management in Svelte is very simple. For basic state and reactivity, Svelte is the lightest and fastest frontend framework I've worked with. We also prefer its templating features to React's, namely getting to use if/else instead of ternary operators and its conditional property setting. Further, Svelte 5 is around the corner, and we're bullish on its runes. We think it makes Svelte even easier to pick up and reason about, meaning everyone on the team is empowered to traverse the stack. I’m now convinced LiveView shines brightest as a backend-for-frontend. By rendering frontend components, incrementally updating them, maintaining a stateful backend process, and providing a websocket API, it creates a tremendously productive platform for frontend applications. If you’re using LiveView and resonated with any of the friction I highlighted, you need to give this a try. If you’ve never used LiveView, you’ll find that this paradigm lowers the learning curve. This is because you’re able to use a lot of the JavaScript framework primitives you’re used to. Update 4/3/2024: Join the discussion on Hacker News.",
    "commentLink": "https://news.ycombinator.com/item?id=39916144",
    "commentBody": "LiveView Is Best with Svelte (sequin.io)274 points by keturakis 22 hours agohidepastfavorite137 comments furyofantares 18 hours agoA pattern sometimes used in multiplayer video games is there's a bunch of code that is by default run on both the client and the server. The client code runs as a prediction of the server state. When the server state is received it slams the client state. For games \"prediction\" is an apt description of this, because the client can make a good guess as to the result of their input, but can't know for sure since they don't know the other players' inputs. But this paradigm can also be used to simply respond immediately to client input while waiting for the official server state - say by enabling/disabling a dropdown, or showing a loading spinner. There's also plenty of client state that's not run on the server at all. Particle systems, ragdolls - stuff that doesn't need to be exactly the same on all clients and doesn't interact with other player inputs / physics. If we're gonna have a persistent server connection I don't see a reason this wouldn't work in a reactive paradigm. reply POiNTx 17 hours agoparentThat's what I did with: https://territoriez.io/ It's a clone of https://generals.io/ It's built with LiveSvelte. It doesn't need any predictive features as it's basically a board game and runs at 2 ticks per second. It does use optimistic updates to show game state before it actually updates on the server. The server overrides the game state if they're not in sync. All game logic is done inside Elixir. To do predictive correctly, you'd need to share logic between the server and the client. Otherwise you're writing your logic twice and that's just a recipe for disaster. One possible solution which I didn't investigate, but should work, is to write all game logic in gleam (https://gleam.run/). Gleam is compatible with Elixir, AND it also can compile to js, so you could in theory run the same code on the server and the client. Now this is a big mess to understand, you could say \"why don't write it all in js and be done with it\" and you'd make a very good point and I'd probably agree. The main advantage you get is that you can use the BEAM and all it's very nice features, especially when it comes to real time distributed systems. It's perfect for multiplayer games, as long as you don't take into account the frontend :) reply rytill 17 hours agorootparentHey, about your clone of https://generals.io - why not make it a better designed version of that game instead of just a clone? Perhaps you have plans to. There are things about the original that kind of suck. 1) Spawning closer to the center is just strictly significantly worse, corners are best. It's essentially an auto-lose to spawn anywhere but an edge or corner in a FFA. 2) Getting literally all of someone's armies when you kill them is so good that it pushes players to just try to all-in the first person they meet every single time. There is no way to \"fast expand\" once you've met another player because of the first factor, and also that 40-50 armies on a neutral castle is very high. I think the \"early-game\" can be much better designed. 3) Perhaps you should be able to change your capital, which could solve problem 1. 4) There are many ways you could keep the simplicity of play, but boost the richness and depth of the actual gameplay. For instance, more chokepoints that would allow actual strategic use of territory and army positioning. Different tiles which have different advantages to owning. Borrowing from something like civ - forests or hills which have a defensive boost when you're inside. Rivers which attacking across is disadvantageous. Just some feedback from someone who enjoys games like Chess and Starcraft, and thinks the core gameplay loop of generals.io is really fun, but believes that it is seriously lacking in strategic depth. reply POiNTx 16 hours agorootparentIt's a little bit different from generals.io but the points you mention are very valid. I originally started working on it as a showcase of LiveSvelte + I like generals.io a lot and thought it could be improved visually + UX wise. Then afterwards started thinking a bit about the game design and what I could improve, but eventually burned out on the project. Was also at a time when I wasn't working but am working again. Maybe I'll pick it up at some point, but also open to other people working on it if they're interested. I'd be open to partner up with someone to eventually make it monetized. It's not open source as I wanted to add paid features (cosmetic features, not pay to win). A solution for most of these issues is a modifier system. I have a basic one in place and wanted to experiment with different modifiers. A modifier can be anything, like prevent spawning in the center, to increasing rewards for capturing cities, to even allow crossing the border of the map into the other side, like pacman (which could address point 1). Then the goal would be to see which modifiers stick with the player base, and make them the default, while still allowing for custom games with different modifiers. Generals has a similar system in place and they sometimes make the modifier the default for the day, which I like a lot. reply djbberko42 17 hours agorootparentprevAs someone who started using Elixir this year (for work) this is really cool. I have had some ideas for some SPAs and games just like these io ones and it'd be great to use Elixir for them. Do you think a top down fps io game would be plausible with this setup? There would need to be at least 60 ticks per second I'd think. reply POiNTx 17 hours agorootparentDefinitely possible. Tick rate isn't the problem, Elixir is very performant. Just the predictive elements are an issue if you're working with 2 different languages. I'd go with Gleam from the start and look into compiling to js. reply spiderice 15 hours agorootparentDon't you lose a lot of the niceties of Elixir when switching to Gleam, just because Gleam is a younger project? LiveView would be the big one I'm thinking of. Do you see that as a worthy trade? reply POiNTx 15 hours agorootparentYou can still use LiveView/Phoenix/Elixir, but have your game logic be in Gleam. I haven't used it though so I could be wrong here. A little bit more about it here: https://katafrakt.me/2021/10/13/app-with-elixir-business-log... You'd call Gleam code like this inside Elixir: `:game.move(game, player_1, :left)` And you'd receive an Elixir map `%Game{}` which you can then use in LiveView. If that makes sense. reply buzzerbetrayed 13 hours agorootparentCool! I'm going to have to look more into Gleam. I saw it hit v1.0.0 on ElixirForum last month, but I figured it was an alternative to Elixir rather than something that played so nicely with it. reply ElFitz 16 hours agorootparentprev> To do predictive correctly, you'd need to share logic between the server and the client. > > One possible solution […] is to write all game logic in gleam […] Rust, with Uniffi, can also be a good candidate. You’d be targeting WebAssembly. reply btown 4 hours agoparentprevFighting games dial up these low-latency conflict resolution considerations to 11, and there’s an entire subfield of techniques for writing “netcode” to reconcile real-time event streams. https://arstechnica.com/gaming/2019/10/explaining-how-fighti... is an incredible walkthrough of this! Discussion: https://news.ycombinator.com/item?id=34399790 and https://news.ycombinator.com/item?id=26289933 When it comes to server persistence, as in an MMO setting, you add I/O bottlenecks into the mix. https://prdeving.wordpress.com/2023/09/29/mmo-architecture-s... is a fascinating read for that end of things. Discussion: https://news.ycombinator.com/item?id=37702632 reply pests 12 hours agoparentprevAs a player of a game (Overwatch) that simulates ragdoll physics locally and non-deterministic, I wish they did it correctly. What ends up happening is someone dies and their body flies off or gets stuck in a hilarious pose... and no one else saw it, nor can you rewatch it in the replays as every client renders it differently. Unless you catch it with a live recording then it's lost forever. With a sometimes goofy game like Overwatch it's sad knowing no one else is seeing it. reply wldcordeiro 11 hours agorootparentIn Counter Strike I've seen it be an actual issue too where a player's view is blocked by a body and their teammates don't have the same issue while spectating them. reply chrisweekly 7 hours agoparentprevI've always heard of and referee to this pattern as \"otimistic updates\" - tho my frame of reference is 25y in webdev, w/ almost no exposure to game dev. reply dkersten 17 hours agoparentprevIsn’t that just optimistic updates? This has been common in client side logic for a long time, no? reply furyofantares 15 hours agorootparentI was more talking about the method to achieve optimistic updates, rather than the concept of optimistic updates in general. That is, having the client and server code be identical where applicable, with client code being run in a prediction context. I'm not a web expert but the optimistic updates I've seen in web stuff is more like, I'm gonna fetch this url and here's the data I expect back. Nothing wrong with that, but it's achieved in a different way, where the server is all about providing data and the client is all about managing state. The OP is talking about maintaining a persistent connection to a server which is doing most of the state management. They detail things this does well (makes the server easier to write) and things it does poorly (makes optimistic updates harder) and a solution to the things it does poorly. So I'm drawing a parallel to other systems where state is managed on the server and must be predicted on the client. reply dkersten 4 hours agorootparentThe way I’ve always done it is that I have client side event handlers that perform the UI-visible side of the logic client side, but also asynchronously request the server to do it. In simple cases, that logic simple sets a toggle or whatever. In slightly more advanced cases it might modify something such as appending to a list. But in some cases it could be performing more complex logic such as applying a filter to a list. Sometimes this logic is only done client side (if the filtering is view-only for example) and sometimes both client and server. Basically, if you remove all of the network requests, it would still look like it’s working, more or less. Of course only logic that is needed for data to be correctly displayed in the UI is needed client side. Performing logic that is only used server side (even if it has later UI effects — only effects that the user expects immediately need local representation) is unnecessary client side. Eg even in a game, you might not want to run, for example, bot AI speculatively on every client but rather just the movement prediction like for other players, while everything else may be needed for rendering. reply mb7733 14 hours agorootparentprevHaven't used it in years, but Meteor.js worked like this. Even to the point where it had a client-side database implementation that mirrored (parts of) the server-side database. It would apply the updates optimistically to the client side DB, and the much of code that sat between the database could be shared between client and server. Neat stuff, even if overall Meteor wasn't my favourite thing to work with. reply OJFord 15 hours agoparentprevYou need to be smart about this though, I don't want it to look like a live/interactive form has worked for example - updating derived data on the page - if it's then going to turn out that actually the server had an error or there was network trouble, and it all gets undone (or worse I close the page/navigate away not realising at all). reply willsmith72 13 hours agorootparentyou would be in the minority then, most people these days want (and expect) live feedback if the action's result is predictable enough e.g. throttle your network and upvote a hn comment. you're not sitting there waiting with a spinner while the server responds, it's all in the background. the hn implementation isn't great though - if the upvote request fails, the optimistic update isn't rolled back, and you have no knowledge that it failed. for hn, who cares, it's just a lost upvote, but for most modern web apps you would show the user that the action failed reply OJFord 13 hours agorootparentIf you're (would be) 'sitting there waiting with a spinner' then that endpoint is too slow, regardless of what the frontend does in the meantime. reply willsmith72 13 hours agorootparentdepends on the definition of \"slow\". unless you have servers and databases everywhere (overkill for 99% of apps), your endpoint will be probably be >400ms for some people somewhere, enough to feel as a user. that's without accounting for patchy reception (in a tunnel?), network blips, server blips (overworked?), etc. i'm not saying everything should be optimistic, but for something like a hn upvote, i dont care if my public wifi freaked out and took 3 seconds for that 1 request, and i think more people are like that than not reply debussyman 18 hours agoprevI love LiveView + Svelte! (I gave the talk at ElixirConf 2022 on how to combine them, but the live_svelte contributors have done the work to make it a reality) IMO there is always a need for client side state, especially for apps with rich UX. I also live in NYC where network connectivity is not a given, especially in transit. One super powerful feature that the authors don't cover is being able to use Phoenix's pubsub, so that server-side state changes that occur on other servers also get pushed reactively to any client. It's pretty typical to have multiple web servers for handling mid/high levels of traffic. reply matlin 16 hours agoparentMobile networks in general also produce a lot of latency and often warrants a local first caching system on the client. If you want a LiveView like user experience but resilience to flaky network conditions, we just added Svelte 5 bindings to Triplit[1] which let's you write queries client-side and have them sync with a server over web-sockets. Probably not attractive to Elixir devs but if Svelte is your central focus, Triplit is a good option. [1] https://www.triplit.dev/docs/frameworks/svelte reply logicallee 18 hours agoparentprevHow usable are LiveView pages while offline? (Due to intermittent lack of network connectivity.) reply victorbjorklund 18 hours agorootparentLiveview does not work at all while offline. If you just experience a short temporary drop it can reconnect you automatically. reply POiNTx 18 hours agorootparentprevOut of the box, they don't work offline. But there's recently been a project showing it's possible to create a PWA with CRDT's and LiveSvelte: https://github.com/tonydangblog/liveview-svelte-pwa reply realusername 17 hours agorootparentprevIt's the same as any other web page, it doesn't work when you receive or send data from the server. Liveview isn't that special, the liveview paradigm works best for what would already be online actions in a normal page. reply inopinatus 12 hours agoprev> If a new row comes in, you just need to push it to your table, and LiveView will update the client for you. Don’t do this in line-of-business apps where those rows are interactive. The cognitive latency readily induces users into clicking the wrong thing, emailing the wrong customer, refunding the wrong transaction etc. My preferred UX instead is a sticky banner conveying “the data has changed, click here to refresh”. Or, in a pinch, ensure that new rows are rendered append-only with no change in scroll position. reply EmilStenstrom 12 hours agoparentThis CAN work, if you animate in the new lines slowly and (maybe) disable clicks while new data is coming in. reply bogwog 19 hours agoprevSo instead of managing state on the client, you manage state on the client and the server? That doesn't seem like an improvement, even if it saves you from having to build yet another API. reply riskable 18 hours agoparentIt's never that simple. In web applications there's always these types of states: * States that the client needs to keep track of * States that the server needs to keep track of Then on top of those there's two more kinds of states that overlap but they're not quite the same thing: * States that only need to exist in memory (i.e. transient) * States that need to persist between sessions There's a seemingly infinite number of ways to manage these things and because \"there's no correct way to do anything in JavaScript\" you either use a framework's chosen way to deal with them or you do it on an ad-hoc basis (aka \"chaos\" haha). In the last sophisticated SPA I wrote I had it perform a sync whenever the client loaded the page. Every local state or asset had a datetime-based hash associated with it and if it didn't match what was on the server the server would send down an updated version (of whatever that thing was; whether it be simple variables, a huge JSON object, or whole images/audio blobs). Whenever the client did something that required a change in state on the server it would send an update of that state over a WebSocket (99% of the app was WebSocket stuff). I didn't use any sort of sophisticated framework or pattern: If I was writing the code and thought, \"the server needs to keep track of this\" I'd have it send a message to the server with the new state and it would be up to the server whether or not that state should be synchronized on page load. IMHO, that's about as simple a mechanism as you can get for managing this sort of thing. WebSockets are a godsend for managing state. reply uxcolumbo 4 hours agorootparentInteresting. What kind of app was it? How did you decide whether the server needed to keep track it? I’m assuming that was predetermined? What about new requirements, which would need to be tracked by the server? Could the app deal with this dynamically without code changes? reply MatthiasPortzel 18 hours agoparentprevEventually, some form of this paradigm is going to win. In practice, applications need state on both the client and the server. The server needs the authoritative state information (since the client is untrusted), but the client needs to be able to re-render in response to user interaction without a round-trip. reply kevinak 19 hours agoparentprevSome things just end up being a better experience fully client-side. Don't go all in on it - just do it when it makes sense. Another thing I like about this is the ability to be able to use Svelte as a templating language rather than Heex. reply pjmlp 19 hours agoparentprevIt is just a new generation rediscovering ColdFusion, Web Forms, JSF, PHP, Spring, Rails... reply ahallock 13 hours agorootparentSounds pretty reductive and dismissive. Have you actually used LiveView and compared the DX? I've used a a few of the technologies you listed, and LiveView is a different animal. reply ramon156 18 hours agorootparentprevWhat does php spring and rails have to do with this? I'm confused haha reply pjmlp 18 hours agorootparentKeeping sessions on both sides. reply dns_snek 18 hours agorootparentprevCould you elaborate? I don't see many similarities between those and LiveView. The difference between traditional technologies that render HTML server-side and LiveView, is a persistent connection to the server which allows it to re-render templates in response to server-side events and patch client-side HTML without writing any Javascript. reply pjmlp 18 hours agorootparentMostly based on WebSockets and Server Push. As one example of such approaches, .NET has SignalR since 2013. And WebForms could use designer tooling since 2001, and then there was ASP.NET AJAX Control Toolkit. reply pdimitar 17 hours agorootparentBut it doesn't have the BEAM VM. The runtime matters a lot, and LiveView wouldn't work as well if it wasn't running under the BEAM. reply jddj 12 hours agorootparentThis isn't particularly convincing, as the pattern -- loosely: keep a websocket open and keep the formerly clientside app state serverside while passing events and pushing diffs -- seems to appear successfully outside of the BEAM vm. For example, blazor serverside (.net) and laravel livewire (php). I haven't checked but I'm sure there will be a python one too reply pdimitar 12 hours agorootparentYou are not convinced because you don't know the details. But I am not paid to advocate or to even try to convince. You've been informed now -- from here on it's on you as to whether to remain biased or to expand your horizons. reply jddj 12 hours agorootparentI've written elixir apps, also read Armstrong's thesis, etc etc. Beam is undoubtedly an excellent piece of engineering. I don't want to engage more than that, as the horizons comment seems bizarre in the context of the thread reply pdimitar 11 hours agorootparentNo need to get defensive. Your comment came across as curmudgeon-y and biased, I called you out for it, you could have agreed to disagree which I would respect. But no, you had to go out of your way to try and strike. Well, OK. But HN is not the place for that and I refuse to engage further. reply pjmlp 17 hours agorootparentprevTomato, Tomato, it has another VM. reply pdimitar 17 hours agorootparentIt's your right to delude yourself that the runtime does not matter. That's not a discussion I am willing to have though, especially when exactly 100% of my 22+ years of programming experience have demonstrated, time and again, that the runtime inevitably ends up making a lot of difference (sometimes all the difference even). Or, when you don't have a runtime -- as is the case with Rust, kinda sorta I mean because technically `tokio` can be classified as a runtime -- then you rely on a stricter compiler. Both strategies work pretty well. I am not shitting on C# / .NET, they are solid as hell. But some things the BEAM VM just does better and everyone who worked with old-school VMs (in my case the JVM) and the BEAM VM can tell you that. But again, you do you, think what you will. ¯\\_(ツ)_/¯ reply OJFord 14 hours agorootparentprevThis saying really doesn't work in writing, I just read ..err.. 'tomato, tomato' reply dns_snek 17 hours agorootparentprevNot quite, I'd say that SignalR is comparable to Phoenix Channels. It's a communication protocol that can use different transport protocols, Websockets being one of them. LiveView builds on top of Phoenix Sockets. When the page is loaded for the first time, it starts a lightweight server-side process on the BEAM VM. This long-lived process renders the HTML template (which can consist of multiple SPA-style components) and keeps all of the relevant \"props\" (called assigns) in-memory. Every time a new event is received, either client-side (e.g. button click) or server-side (typically via a PubSub subscription), that long-running process will update its assigns (props) to reflect the new state. The framework then intelligently re-renders parts of the HTML template which depend on those modified assigns, and sends a minimal HTML diff to the client. All of this can be done without writing any custom JS. Basic client-side events are usually set up with special HTML attributes like `phx-click=my_custom_event` and automatically wired up by LiveView JS bundle to be received by that long-running process. reply open592 17 hours agoparentprevThis is no different than any other optimization performed in any other software ecosystem. There is no reason to do this unless you have to. And the reason you would have to is because of performance and user experience. When your web application requires the following: - Large amount of user interaction (requires client side JavaScript) - Large amount of experimentation (bundle is not static, changes per request) You are going to want to split up the logic on the server and client to reduce the amount of JavaScript you're sending to the client. Otherwise you have a MB JavaScript bundle of all the permutations you're application could encounter. This may be fine for something like a web version of Photoshop where the user understands that an initial load time is required. But for something like Stripe, Gmail, etc. where the user expects the application to load instantly you want to reduce the initial latency of the page. You can move everything to the server, but like GitHub experienced you then encounter a problem where user interaction on the page suffers because user actions which are expected to be instant, instead require a round trip to the server (speed of light and server distribution then becomes an issue). You can lazy load bundles with something like async import, but then you run into the waterfall problem, where the lazy loaded JavaScript then needs to make a request to fetch it's own data, and you're left making two requests for everything. If you encounter all these problems then you end up reaching for solution which make it easier to manage the complex problem of sharing logic/state between the client and the server. reply klabb3 19 hours agoparentprevAre you comparing with PWAs? Or what’s the baseline? Because with PWAs you have both too. State is accessed from the client with requests and you have to manage caching and stale state as well, to ensure performance and not drifting out of sync across components, no? Last I checked out a modern “performant” graphql stack – it was horrifyingly complex and full of knobs. reply akira2501 15 hours agoparentprev> build yet another API. It appears that's one of their major problems. They're not \"building\" APIs. They're just slapping \"one shots\" onto the side of a router whenever the need arises. This speaks to a complete lack of a planning and design phase. I guess if you want to build something without any plan whatsoever, this might be a way to \"improve\" that process, but there's a much simpler one that doesn't require your team to become polarized over a framework. reply matlin 16 hours agoparentprevYeah that is the crux of most modern three-tiered architectures (clientserverdatabase) these days. We're working on simplifying this with a new concept: a full-stack database. If you run an identical query engine on client and server and then sync data between them, a client can just write a normal query on the client and you get an optimistic response instantly from a local cache and then later a response from the authoritative server. Frankly, this is where most highly interactive apps (like Whatsapp, Linear, Figma, etc) end up anyway with bespoke solutions we're just making it general purpose. reply bogwog 11 hours agorootparentThat sounds a lot like CouchDB and PouchDB. I haven't used the latter, but CouchDB provides a standard REST API (with authentication) out of the box. You'll probably need to add a custom API for more complex stuff, integrating with other services, etc. But all the boring CRUD stuff is already built into the database. But more stuff like this is always welcome! reply realusername 17 hours agoparentprevYou already manage two kinds of state when you are building a React app: - State coming from the server as a result of user actions - Local state which isn't intended to be shared to the server, usually UI stuff. reply fizx 18 hours agoprev> There is a (literal) speed of light limitation with this approach: your server can only be so close to your users. The next step is to compile your server to WebAssembly and ship it to your clients. You can then use it to optimistically render responses while waiting for the real server to return. Sounds a little crazy, but we've actually pulled it off for a project, and its magic. reply tyre 17 hours agoparentWhat about persistence? To me that’s the main purpose of the backend and running a server on the client doesn’t fix that. You still have the network latency of persistence, which is the ultimate server state that should win. reply postalrat 18 hours agoparentprevIt sounds over-engineered. Lots of over-engineered stuff gets pulled off for no good reason. reply paulgb 17 hours agorootparentThere is a good reason: it becomes much easier to think about state transitions, because optimistic and “verified” updates both follow the same code path. I've built a turn-based game that worked this way, where essentially every player and the server contains a copy of the same state machine, and the server determines the order of state updates. Like OP said, once you have the framework in place, it's magic. reply thibaut_barrere 14 hours agorootparentprevIt's over-engineered unless done properly, it which case it becomes a detail. I wouldn't be surprised to see some automatic conversion of server-side code to front-end by LiveView in the future for events where this behaviour is applicable, actually! reply kevinak 18 hours agoparentprevWhy wouldn't you just use a Service Worker for this? reply AirMax98 18 hours agorootparentThere's a couple different ways to skin this cat, but WebAssembly is definitely the \"coolest\". I'd imagine even a Service Worker is overkill compared to just inlining an optimistic response in whatever rest client you're using. Kind of similar to the content of this article, I wish people were more upfront with their reasoning. Doing something because \"it's rad\" is definitely fine by me! reply ffsm8 15 hours agorootparent> WebAssembly is definitely the \"coolest\" As always: when I doubt, it was likely because of CV-driven development reply kevinak 14 hours agorootparentprevI just mutate the data object when using SvelteKit. Blasphemy, I know. But it works, and it works well. :P reply bogwog 11 hours agoparentprevThat sounds a lot like the \"client side prediction\" that modern multiplayer games do. reply CSSer 18 hours agoparentprevWow, that sounds cool! Can you share any more details? reply willsmith72 13 hours agoparentprevlocal-first with a client-side sqlite db sounds way simpler for what seems like the same purpose reply knallfrosch 18 hours agoparentprevWait, what's the difference to a fat client then? reply riskable 18 hours agorootparentEase of distribution. reply bcardarella 19 hours agoprevWe use Svelte along with LiveView in BeaconCMS. There are certainly good use cases for wanting something that has more granular control of the UI on the client but I would caution teams from just going all-in on Svelte + LV for all things. Even with Phoenix using LiveView isn't always the answer as sometimes dead render pages are perfectly fine. Don't all-or-nothing everything. As the article points out there are some good use cases for deviating from the 'LiveView Way'. I would argue that if you have 1,000ms round trips then there is something else to consider but geographically located servers could be unavailable to your team for a number of reasons (i.e. cost) so adding some client-side state management could be your solution. reply POiNTx 19 hours agoprevI created LiveSvelte, let me know if you have any questions :) reply wturner 19 hours agoparentIs there any reason you can think of to still use JS Hooks if using LiveSvelte? reply POiNTx 19 hours agorootparentWith hooks you are able to render your html server side while still having some js functionality. Technically you can also render server side with LiveSvelte as it supports SSR out of the box, but this SSR uses Node and that introduces a slight performance decrease (around 3ms I believe). I've been looking at Bun to see if it would help but it's still unclear to me. Heex rendering is just way faster. If you don't care about SSR for certain components (for example modals, they don't need SSR generally), then I don't see a clear advantage for hooks. reply victorbjorklund 18 hours agorootparentprevIf you just need one simple thing in your app that uses JS it might be overkill to bring in LiveSvelte only for that. reply POiNTx 16 hours agorootparentThat's true. And another downside is that once you're in the Svelte environment you can't use your Phoenix components inside those Svelte components. I've thought about adding a library of default Svelte components which mirror the core components you get from Phoenix out of the box. But then again you lose forms and changesets etc, it's just annoying. Where I see LiveSvelte fit is where you really need a lot of complex client side state. Sprinkle it in, but keep using phoenix components as your default, even with hooks reply baskind 19 hours agoprevNice solution! In my app, I use reusable Stimulus controllers alongside LiveView, and it works seamlessly as well. On a general note, while it's a pleasure to build with LiveView, the more I use it in real-life scenarios, the more I realize the benefits of stateless HTTP frameworks like Hotwire, which feel more performant and resilient to reconnections, and avoid the need to place more servers close to users for stability. reply MatthiasPortzel 19 hours agoparentWhen Stimulus/Turbo was first announced, I was really hoping it would help with the problems that the author describes. Unfortunately, Stimulus doesn’t actually provide an elegant way to keep state on the client. “A stimulus application’s state lives as attributes in the DOM.” This means that it’s not better than vanilla JS or jQuery. Edit: I haven’t used Stimulus for a real project; it’s possible their values and change callbacks are a better experience than I originally imagined. reply sph 19 hours agoparentprevYeah I use Stimulus and Live View together as well. It is the right level of complexity, while I feel Svelte deals with a lot of stuff which is not even an issue when paired with LV. All you need is vanilla JS or a thin layer on top of it, not an entire framework. You will not have to write a lot of JS after all. My production app has no more than 200 lines of JS, and I could probably get rid of a couple Stimulus controllers. Live View is that good. I also made a very hacky Stimulus-Live View hook adapter, so my Stimulus controller can send events directly to the LV process. EDIT: Live View does not require you to run geo-distributed servers at all, unless you have bought into the fly.io kool aid a little too much. And it deals with disconnections beautifully. I didn't even have to do anything to support zero-downtime updates. The client loses connection to the WebSocket and reconnects to the new version, restores the state, all that out of the box automatically. What more do you need? reply baskind 15 hours agorootparentI have the same view of an app implemented with Rails+Turbo and a duplicate in Elixir+LiveView. Performance is comparable when I am close to the server (elixir is slightly faster), but when on another continent any content changes/navigation over websockets suddenly feel very laggy, while navigation over HTTP in the supposedly slower Ruby+Rails is actually consistently fast. I’ve only recently discovered this as I went travelling to another continent, so will do more perf testing. But the nature of the always-connected websockets hasn’t been a pleasurable one for me: for instance, a LiveView process crashes and you lose all the data in a big form, unless you write glue code for it to restore. And the experience of seeing the topbar getting stuck in the loading state the second your internet connection is spotty or if you are offline just gives me anxiety as a user. reply klabb3 19 hours agoprevGenerally speaking this is the model I’ve always been wanted to build apps in. Event oriented, bidirectional realtime updates with server, ordered events, local and remote state... I didn’t know about LiveView and never used erlang-family languages, but definitely they’re onto something. The traditional request-response model is many times causing a lot of subtle problems with consistency and staleness. A wishful (probably also controversial) thought: if the last decade was about integrating FP concepts into mainstream languages, then I hope the next decade will be oriented around integrating stateful message-oriented (reactive?) programming into the mainstream full stack. reply floodfx 17 hours agoparentThere are LiveView backends written in Javascript (http://liveviewjs.com), Go (https://github.com/canopyclimate/golive), Java (https://github.com/floodfx/undead), and Python (https://github.com/ogrodnek/pyview). Disclaimer: I wrote or helped write the first three. reply cess11 18 hours agoparentprevYou can use asdf to install erlang/BEAM/OTP and elixir, takes a few minutes if you have some previous experience with the tool. Either way it'll probably take about two hours to have your first rudimentary Phoenix chat application loaded in a browser if you follow some guides and tinker around a bit. reply square_usual 15 hours agorootparentI would strongly recommend using mise instead of asdf. It's a drop in replacement that is just flat out better for most people. reply karmajunkie 7 hours agorootparentcurious, what makes it better? asdf has always just worked for me, but this is the first time i’ve heard of mise, so i’m wondering what i’m potentially missing out on reply cess11 4 hours agorootparentMaybe it works better with tmux? reply nmk 19 hours agoparentprev> I hope the next decade will be oriented around integrating stateful message-oriented (reactive?) programming into the mainstream full stack Also known as MVC before Rails decided to redefine the term. reply andrewflnr 16 hours agorootparentNo, MVC is at best orthogonal to message-passing. reply klabb3 18 hours agorootparentprevMaybe? Most good ideas have been out there for decades. Getting the execution right is what’s hard. reply DonHopkins 19 hours agorootparentprevJava redefined the term MVC a long time after Smalltalk originally defined the term in the 70's. Rails didn't redefine the term, it much later copied the (poorly) redefined term from Java and tweaked it a little. Smalltalk MVC and Java/Rails MVC are EXTREMELY different. Java and Ruby's MVC are quite similar (and loosely based on a superficial misunderstanding Smalltalk's MVC). reply kevinak 19 hours agoprevGreat project! Superb timing here. Just released our Svelte Radio episode about this: https://www.svelteradio.com/episodes/phoenix-liveview-and-sv... reply xrd 19 hours agoparentI'm so excited about this, I've never been able to rectify LiveView and Svelte. I love hearing about the philosophies of Svelte builders so this will be an extra special episode. reply atonse 19 hours agoprevI've wanted something like this for Vue/React too with LiveView, because then you get access to this massive ecosystem of great components to put in your phoenix apps while still utilizing LiveView. (the LiveView component ecosystem is tiny). So maybe this can be the start of a general bridge that can bridge LV with React or Vue components too? And make it easy to put these in a page and interact with LV events, etc. reply dugmartin 18 hours agoparentI created a useLiveView() hook a couple of years ago for a couple of personal side projects. It was pretty straightforward and let me have bi-directional state. The only big downside is you also need to setup server side React rendering for the initial load if you care about SEO. Maybe I should dig it up and post it in a gist somewhere (I don't want to maintain an open source library for it). reply yurishimo 19 hours agoparentprevThere is an adapter already for Inertia.js and Phoenix. https://github.com/devato/inertia_phoenix reply bezieio 14 hours agoparentprevI've recently developed a LV/React bridge and am using it on a production app, but nothing open sourced at the moment. I'll open it up at some point soon and try and post back here! reply udkl 9 hours agoprevThe cleanest way to handle the backend and frontend charade I've seen until now is using https://github.com/hyperfiddle/electric which is a clojure DSL on top of react reply uxcolumbo 4 hours agoparentThis needs to be higher up ;) Also worth checking out Dustin’s talk about optimistic UIs. https://youtu.be/v-GE_P1JSOQ?si=3tZeAZqoroN1Vubp And quick tutorial: https://electric.hyperfiddle.net/ Have you used electric clojure? reply submain 19 hours agoprevI am not familiar with LiveView, so I'm curious. Looks like it processes UI actions server side. So, are all client interactions sent through the websocket? I remember years/decades ago we used to do that with ASP.NET, where every single component interaction was handled by the server. How is this different / better? reply bcardarella 19 hours agoparentI never used ASP.net so I cannot offer a comparison about what is \"better\" but your assumption is correct that diffs are through a WS and merged client-side. What this has resulted in is actual order of magnitude of implementation time reduction over the crazier SPA complexity available today. Less time to build, less cost to the company, less bugs in the long run, and a single place to manage and reason about state. It's a win. reply bornfreddy 18 hours agorootparentBut isn't there a delay in UI responses because of latency then? reply cmoski 2 hours agorootparentYou keep frontend UI changes that don't affect state in the frontend. If you don't need the server then you don't waste your user's time with trips to the server. If you need server, you can do things on the frontend at the same time you send the request, e.g. hide something, add/remove a class. reply cess11 18 hours agorootparentprevSure, it's not for a smooth user experience over 2G connection, if that's your audience you'd use ordinary template rendering with Phoenix, or use it for a JSON API and build a JS client that talks to it. reply bornfreddy 4 hours agorootparentI can see the next big thing coming: no latency! Render in client! :) reply cess11 4 hours agorootparentSeems maybe messy to have templating in the client but I'd take a look if someone has done it. reply Muromec 17 hours agoparentprevVanilla live view does exactly that, but I think the point of trowing svelte (or any other frontend) into the mix is to keep some updates frontend-only. It seems to go against the wisdom of the last few decades, but the network latency seems to permit it now. Throw some edge computing to the mix and maybe it's all a good idea. reply cess11 18 hours agoparentprevThis is run on the BEAM VM, so a LiveView is a lightweight process hooking up the view to the bells and whistles of the BEAM, including relatively easy Pub/Sub, soft-realtime monitoring and so on. Some people think it's kind of nice to have one programming language for everything, including queues, cache, database queries, client layout, business logic. reply victorbjorklund 18 hours agoparentprevYou could do that but in general no not in production. If you just have a UI update that server doesnt need to know about (for example opening a menu) then you do it with javascript. reply sebastianconcpt 16 hours agoprevHave you guys considered htmx before going that way? If you can share pro/cons I'd love to hear. reply _acco 8 hours agoparentAuthor here. We did not. We briefly tried Alpine, which I think is comparable? I think Alpine is cool, but it didn't really stick with the team. I think that's because we were still writing our components in LiveView and sprinkling in Alpine. A big unlock with LiveSvelte was getting to move so much into `.svelte` files, but not converting the whole thing to a SPA. Working in a `.svelte` file gives you a lot of niceties that an Alpine-decorated LiveView component won't (prettier, intellisense, etc). This approach could totally work with other paradigms, like Alpine and HTMX. I think the key is using LiveView as a backend-for-frontend, so writing all your components in `.htmx` files or whatever. reply matt_s 16 hours agoprev> LiveView makes a lot of things easy. But it also makes some easy things hard. Having been involved with web applications for a couple decades there were some weird things LiveView did that weren't easy for me to pick up. Maybe its changed in the last 12-18 months but initial versions were built odd for some seemingly easy types of XHR things one would want to do. reply fearthetelomere 19 hours agoprevThis looks super promising... I very much relate to the dropdown example, and I've found that complicated UX patterns can be extremely awkward to implement and maintain in LV. One example from my experience that was prickly to implement in LV was graying out a chat message you sent if it didn't get acked or persisted by the channel/server for any reason. Can't wait to try this out in my next project! reply nvegater 19 hours agoprevI honestly don’t understand how’s this different from react server components with nextjs but with less features? It seems like the same thing but with an even clearer border between server/client components and therefore missing all the optimizations (like streaming). Like islands architecture but made out of different technologies, is my understanding correct ? I would appreciate some feedback :) reply bcardarella 19 hours agoparentLiveView has streaming. I would argue that if you're stuck in the React way of thinking about application design then it's not worth trying to sell you on what LiveView is doing. But there are multiple case studies out there showing that it results in far less build times than React for no compromise on user experience. reply terandle 17 hours agoparentprevYeah I wish this article had covered how this solution compares to React Server Components as it kind of looks like spaghetti of different techs compared to how next.js streamlines the same problem space into one consistent mental model. reply gr4vityWall 7 hours agoprevI have the impression Meteor solves this problem really well. You get optimistic UI by default without any extra work. reply stsrki 19 hours agoprevEssentially, it works the same as Blazor Server. Or am I wrong? reply jonnycat 18 hours agoprevThis is great. LiveView is truly amazing and greatly speeds up development, but as the post describes, there are a couple of rough edges. They're all solvable, but sometimes there aren't clear or well-established patterns for how, so it can feel a bit ad hoc. While I'm not currently using Svelte for this kind of thing, I'm really glad to see people formalizing some of the issues + solutions. reply atonse 11 hours agoparentMy main complaint with LiveView is communication between components in a tree (like callbacks and sending data back). It’s still janky at a core syntax level between send, send update, etc. I feel this is something only the core team can fix because it probably requires some kind of use of macros etc. reply jmull 19 hours agoprevI guess it's fun to build things, but this mashup is pretty messy. If you like Svelte (I do) you're probably going to find sveltekit to be a lot simpler and more useful. reply amsterdorn 18 hours agoparent+1, SPA should only be used as a last resort, not sure why the article compares only to it. SvelteKit is much simpler and well-documented and addresses the issues mentioned. reply todotask 18 hours agoprevI wonder what happened when attempting to open this website, as it caused my Firefox browser to hang. I've never experienced such a problem before. reply mrcwinn 8 hours agoprevIf you find the need to use Svelte with LiveView, I assure you, you’re doing it wrong. reply digdugdirk 18 hours agoprevFor someone not in the web dev space, this looks like an interesting way to get started. If one has Python and Rust experience, what would be a recommended \"first principles\" path to get started in understanding web development with LiveView and Svelte? reply dinkleberg 18 hours agoparentI would advise against that path (unless you’re looking at a long time horizon). Phoenix is great, and it makes a lot of really hard things simple, it also introduces you to a lot of things that don’t exist anywhere else (relies heavily on OTP). But then that approach has almost nothing to do with svelte or any other SPA tool so that is something else you’d have to learn. Personally I’d start with either Phoenix and avoid any SPA tools until you get really comfortable and have hit the boundaries of what is possible with Phoenix. Or alternatively, I’d start with sveltekit and not think about Phoenix and save exploring that for another time. Phoenix is super cool, but I’d suggest starting with SvelteKit, as you can build full stack apps and the same principles apply if you want to move to react or vue or anything else. reply kevinak 18 hours agoparentprevStart with Phoenix and add live_svelte when you need to. No need to juggle both at the same time when you're starting out. reply rc_mob 19 hours agoprevI'm a noob but it looks like this overlaps with nextjs a lot? reply PKop 18 hours agoparentLiveView and Elixir/Erlang have stateful processes on the server unlike pure stateless HTTP request/response. This fundamental difference is what makes LiveView unique. Next.js isn't passing messages over a websocket nor holding stateful processes on the server. reply kimi 17 hours agoprev> What's most game-changing, though, is that you have a backend, stateful process that is collaborating with a frontend, stateful process. ...given that managing state is the thorniest of issues, what could go wrong with this approach? reply lacoolj 13 hours agoprevWhen I watch people using Nextjs, it makes me cringe. \"This is just PHP but less clear what happens where\" And yet for some reason the thought of Phoenix + LiveView + Svelte makes me want so badly to try it. Just the thought of playing with it has me giddy. This must be a mental disorder I'm experiencing. Dissociative Framework Disorder. reply krainboltgreene 17 hours agoprevMy own company has found significant advantage by using LiveView with Alpine, specifically in CSP mode since we can't allow `eval` to happen on the client. When I originally looked at LiveSvelte it seemed very new and untested. It also had some unsolved implications in strict CSP environments. Glad to see that it's become very useful! Our own pattern (LiveAlpine?) is as follows: Does the component need HTML? Then use an HTML Component. Does the component have server-side state? Then use a Live Component. Does the component need client-side behavior and/or state? Then also define an Alpine component. Does the component need to receive client-side events from the server or make HTTP requests? Then also define a Phoenix Hook. reply scop 13 hours agoparentThank you for providing your pattern. reply enraged_camel 18 hours agoprevThe biggest benefit to this would be gaining access to all the UI component libraries in the JavaScript ecosystem. The lack of such components in LV is one of the things that slows us down the most. reply gardenhedge 11 hours agoprevArticle misses a comparison to newer frameworks like Remix which are server side driven reply peterisdirsa 19 hours agoprevIs LiveView same as Vaadin in Java world? reply cess11 18 hours agoparentNot really, Vaadin is mainly a UI component lib and some convenience annotations, while LiveView is designed to allow soft-realtime UI views over WebSocket. reply moomoo11 16 hours agoprev [–] Anyone else use this in production with a significant user base or business usage? How’s ur experience? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article explores using LiveView with Svelte in web development, showcasing how it enables server-side rendering with stateful components similar to SPAs without needing a separate frontend microservice at Sequin.",
      "The team at Sequin found LiveView to be both efficient and frustrating, eventually leading them to discover LiveSvelte as a complementary library for a smoother development process.",
      "LiveSvelte combines state and state transitions in the frontend seamlessly, providing a clear separation between frontend and backend, harnessing Svelte's lightweight and fast frontend framework ideal for LiveView users."
    ],
    "commentSummary": [
      "The discussion examines using LiveView and Svelte for multiplayer video games, covering predictive client code and server synchronization concepts.",
      "It explores the challenges, advantages, and game design enhancements while discussing Elixir's performance for game logic and the impact of runtime environments on web apps.",
      "The conversation also mentions tools and frameworks for state management, showcasing the potential and complexities of leveraging LiveView and Svelte in web development."
    ],
    "points": 274,
    "commentCount": 137,
    "retryCount": 0,
    "time": 1712144477
  },
  {
    "id": 39917547,
    "title": "Seafloor Mapping Project: 25% Complete, 6 Years Left",
    "originLink": "https://eos.org/articles/new-seafloor-map-only-25-done-with-6-years-to-go",
    "originBody": "Ten years have passed since Malaysia Airlines Flight 370 vanished after departing from Kuala Lumpur, Malaysia, with 239 people aboard. Military and scientific personnel spent months combing unexplored reaches of the Indian Ocean, and though they failed to find the Boeing 777, the quest highlighted the fact that beneath the waves, vast areas of Earth’s ocean are still a mystery. Seabed 2030 is a long-term mapping project attempting to fully chart the seafloor and reveal all features 100 meters or larger by 2030. But with just a quarter of the job now done, it still faces tremendous challenges. “Maps enable us to manage how we utilize marine space, [guide] ocean commerce and use of resources, mitigate our impacts, and achieve an improved understanding of threats such as tsunami, storm surges, and underwater landslides.” The General Bathymetric Chart of the Oceans (GEBCO) and Nippon Foundation, a Japanese nonprofit that funnels profits from motorboat racing to philanthropic causes, established Seabed 2030 in 2017. The aim was to use ocean mapping to support United Nations Sustainable Development Goal 14, namely, to “conserve and sustainably use the oceans, seas and marine resources for sustainable development.” At the time, only 6% of the ocean floor had been mapped to modern standards, according to the project. This newsletter rocks. Get the most fascinating science news stories of the week in your inbox every Friday. SIGN UP NOW “Maps enable us to manage how we utilize marine space, [guide] ocean commerce and use of resources, mitigate our impacts, and achieve an improved understanding of threats such as tsunami, storm surges, and underwater landslides,” said hydrographic surveyor Jamie McMichael-Phillips, Seabed 2030’s project director. “Essentially, we cannot manage what we don’t know.” New Discoveries An oft-repeated observation is that we know more about the surface of Mars than Earth’s seafloor. But that has been changing with recent discoveries. Off the coasts of Central and South America, researchers with the Schmidt Ocean Institute, a Seabed 2030 partner, found four seamounts in January 2024, including a massive one covering 450 square kilometers that is 2,681 meters tall, which is about 5 times the height of New York City’s One World Trade Center. At a depth of 1,150 meters, the giant seamounts were well hidden beneath the waves. To find these features, researchers aboard the institute’s R/V Falkor (too) looked for clues on the surface of the ocean. Satellite altimetry data can show variations in the height of the ocean surface caused by gravity anomalies. A slight depression could point to a trench, and a rise could reflect the presence of a seamount. “The most significant aspect of our findings is the validation of the satellite altimetry data that we used to target these seamounts,” said John Fulmer, lead technician aboard the Falkor (too). “As all of our findings were in international waters, there is no apparent political significance to speak of, but the ease with which we were able to identify potential seamounts is a testament to the importance of collective and open data resources at our disposal.” Bringing New Tools to Bear Crowdsourcing and the use of open-source data are crucial to the Seabed 2030 project. It relies on data donations from a range of sources: scientists and philanthropists; offshore survey companies; and fishing, cruise, and cargo vessel operators. In November 2023, for instance, Seabed 2030 announced the donation of a data set covering 8,000 square kilometers of remote areas that are difficult to reach. U.K.-based remote sensing company ARGANS (Applied Research in Geomatics, Atmosphere, Nature and Space) provided the high-resolution bathymetric data derived from satellite observations, one of the main data-gathering methods along with aircraft-based lidar sensing and sonar readings. In another example, the Japan Coast Guard donated to GEBCO a large bathymetry data set covering areas around Japan and Antarctica, according to Haruka Ogawa, a researcher in the coast guard’s Hydrographic and Oceanographic Department. All Seabed 2030 data are uploaded to the free-access GEBCO grid of the seafloor. The grid shows that 24.9% has been mapped to the Seabed 2030 resolution, leaving about 75% to be done over the next 6 years if the project’s goal is to be met. That’s a massive undertaking considering challenges such as permanent ice cover near the poles and the fact that about half of the global ocean is deeper than 3,200 meters. Because vessels are usually needed for the job, it makes for a slow, costly endeavor with a bias toward areas with expertly equipped ships, said Katleen Robert, a seafloor and habitat mapping researcher at Memorial University of Newfoundland, in Canada. She said she sees autonomous vehicles, both underwater and surface, as having the largest impact on how much of our oceans we will map in the near future. “These will act as force multipliers and enable us to acquire higher-resolution data and repeat data sets to monitor changes over time,” said Robert, who is not directly involved with Seabed 2030, although expedition data she was involved with are supporting it. In 2023, Seabed 2030 partnered with U.S. company Saildrone to promote the use of uncrewed hydrographic vessels. Powered mainly by wind and solar, Saildrone’s 20-meter surveyor drone can gather multibeam sonar data to depths of 7,000 meters for a fraction of the operating cost of crewed ships. One surveyor found a 1,000-meter seamount off California during a months-long surveying mission that saw 35-knot winds and 5-meter swells, according to the company. “We know that we will be much closer to a fully mapped planet than the 6% that had been mapped when the project started.” Other researchers are hoping other forms of artificial intelligence will accelerate the mapping mission. In addition to surface and underwater exploration vessels, researchers at the Japan Agency for Marine-Earth Science and Technology (JAMSTEC) are deploying deep learning techniques to enhance the resolution of existing topographic data. The approach using convolutional neural networks is “surprisingly effective” and can be used as a complementary method to direct depth sensing, said Eiichi Kikawa, a data scientist at JAMSTEC. He added that a paper on the research has been submitted for publication. It’s too early to tell whether these tools could help Seabed 2030 meet its goal in 6 years, but the project is relying on more partnerships to increase coverage of the seafloor. “We are aware of the magnitude of the challenge that still remains and are working with the global community to push to complete the job by 2030,” McMichael-Phillips said. “However, with the progress that has already been made, and with the time left until the end of the decade, we know that we will be much closer to a fully mapped planet than the 6% that had been mapped when the project started.” —Tim Hornyak (@Robotopia), Science Writer Citation: Hornyak, T. (2024), New seafloor map only 25% done, with 6 years to go, Eos, 105, https://doi.org/10.1029/2024EO240154. Published on 2 April 2024. Text © 2024. The authors. CC BY-NC-ND 3.0 Except where otherwise noted, images are subject to copyright. Any reuse without express permission from the copyright owner is prohibited. Related Tagged: bathymetry, crowdsourced science, mapping, Oceans, open science, seafloor",
    "commentLink": "https://news.ycombinator.com/item?id=39917547",
    "commentBody": "New seafloor map only 25% done, with 6 years to go (eos.org)238 points by Brajeshwar 20 hours agohidepastfavorite130 comments tbihl 18 hours agoIt's crazy to read this, for the same reason that maps used to be military secrets; accurate bathymetric data is a huge deal for modern battlespace preparation. And it'll probably be impactful in all sorts of exciting ways just like Google Earth was when it released. Hopefully it doesn't experience too much interference from the usual suspects, like with the requirements that GPS maps be offset from reality. Given the large extraterritorial sea claims pushed by the same, it could interfere with a large geographic area. reply medellin 11 hours agoparentCan you explain “GPS maps offset from reality” or what would i have to look up to learn more? I see for china some into about random offsets but couldn’t find anything else. reply tbihl 10 hours agorootparentOthers have already answered, but yes, I'm referring to China. Check out Google maps in any Chinese city; I've never seen one without the offsets. You can read about the general spec for the obfuscation and also about fines and arrests for illegal mapping. reply rdlw 11 hours agorootparentprevI think the comment you replied to is just talking about the government of China. Try looking at China on Google Maps in satellite view. reply bragr 11 hours agorootparentSpecifically look along the border with other countries where the offset is obvious. https://en.wikipedia.org/wiki/Restrictions_on_geographic_dat... reply ysofunny 11 hours agorootparentprevthe mathematics of cartographic projections possibly but this information was really a military secret not that long ago. same as cryptography is right now! which annoys me personally becuase I wish to learn more/better about the mathematics behind information technology and cryptography. but this is considered 'sensitive' understanding so the quantity of people that are ever taught this stuff is tightly controlled; not to say that it's difficult stuff to begin with and any efforts to make it more widespread and accessible are often derailed in education comitees and so on. but I should stop before I get any more paranoid reply AlotOfReading 10 hours agorootparentThe details of specific implementations are sometimes considered secret, but broadly speaking the mathematics of both projections and cryptography are extremely public. For example, GCJ-02 (the projection being obliquely referenced by the GP) is just WGS-84 + \"noise\". For cryptography, you can read any of the excellent textbooks and educational resources available (e.g. [0]). [0] http://www.cs.umd.edu/%7Ejkatz/imc.html reply ysofunny 9 hours agorootparenti'm not supposed to say this, but my above paranoia is but a coping mechanism to deal with the harsh truth that I just cannot for the life of me completely understand any of those ideas no matter how well they're explained reply mxkopy 8 hours agorootparentIt would make for a good sci fi novel, but yeah. Cryptography is cutthroat in the sense that if you’re even a little imprecise people can and will break your scheme. At some level of development the material will just filter out people from learning it. reply 8note 10 hours agorootparentprevThis information is part of why the US missiles can hit their targets precisely and why Soviet ones could not. you lose accuracy dead reckoning when you don't have good corrections for the earth's gravity at your current location reply pyinstallwoes 37 minutes agorootparenthow much does earth's gravity fluctuate by location? reply naavis 15 minutes agorootparentEnough for consumer-grade Hario coffee scales to include a geographic location setting, so that you can weigh your coffee beans accurately. reply mschuster91 18 minutes agorootparentprevQuite a lot, at least at the weight and speed scale a missile operates on. The biggest source of gravity force difference is the distance between the equator / pole [1], and altitude (both in terms of altitude in reference to the planet core as well as altitude in reference to ground, e.g. due to the mass of a mountain range) also adds jitter [2]. On top of that, dead-reckoning is disturbed by aerodynamic effects such as wind, as well as drift from the gyro compass itself... a fine-detailed gravity effect map can be used to compensate for both of that, and adding imagery maps and radar/laser distance measuring is even better. To add even more context... that is the reason why Germany's Chancellor Scholz is refusing to deliver TAURUS long range missiles to Ukraine. TAURUS owes its significant increase in precision over Storm Shadow / SCALP (UK/FR equivalent) to that kind of sensor aids, but the assistance dataset aka the photogrammetry, gravity and height maps of the flight route must be individually created and loaded into the missile as part of the mission preparation. As you can imagine this kind of precision information is stuff that we really don't want to get leaked to Russia so we can't (or don't want to) deliver the systems for programming to Ukraine where it might get stolen/diverted to Russia, so Scholz is afraid that the necessary direct cooperation of German Bundeswehr soldiers with Ukrainian soldiers might be seen as an act of war of Germany by Putin. (Complete horse dung if you ask me, but I can understand where he's coming from) [1] https://www.wired.com/story/do-you-weigh-more-at-the-equator... [2] https://physics.stackexchange.com/a/652759 reply wolverine876 11 hours agoparentprev> accurate bathymetric data is a huge deal for modern battlespace preparation Their resolution is \"all features 100 meters or larger\", so better than nothing but not useful for manuevering your submarine. reply tbihl 10 hours agorootparent100m isn't especially big. Significantly less than turning radius for most ships, and laughably small compared to stopping distance. Having said that, maneuvering is only one problem among many. reply wolverine876 8 hours agorootparentObstacles smaller than 100m can wreck your sub. reply _carbyau_ 6 hours agorootparentWell yeah. But trees can wreck my face too. I'd still want a good contour map if I were to go wandering into the wilderness. A good map can tell you about the possibilities of the environment, but it's always inferior to looking at it yourself. reply chmod775 4 hours agorootparentprevSo can obstacles smaller than 50cm. What resolution should we aim for? reply elevaet 14 hours agoprevThe name of the ship doing the work is Falkor (too), which is named after the ship RV Falkor, which in turn was named after the Luckdragon: > The retrofitted vessel was renamed R/V Falkor, after the Luckdragon in the fantasy novel The Neverending Story https://en.wikipedia.org/wiki/RV_Falkor_Too https://en.wikipedia.org/wiki/RV_Falkor reply TheFinalDraw 9 hours agoparentSchmidt Ocean Institute (which owns/operates Falcor (too)) is an awesome nonprofit supporting a lot of really fascinating and impactful oceanographic and environmental research. One of my favorite oceanography documentarians, Leo Richards, produced a truly beautiful video for Schmidt Ocean Institute and put it up on his YouTube channel, Natural World Facts, which I highly recommend for anyone with an interest in the ocean or scientific research in general: https://youtu.be/Uh3fNYVwDXM?si=QnzFTJFJ5hIhXWoJ reply a_bonobo 3 hours agorootparentThere's another nonprofit in this space, Inkfish. They have a highly advanced deep sea research vessel called the Pressure Drop, and another called the Hadal Exploration System. Do you know who owns Inkfish? Gabe Newell of Valve fame! reply a_bonobo 6 hours agoprevThe Nippon Foundation that funds this is a strange beast, established to make it legal to bet on speed boats in Japan. A percentage of speed boat betting winnings enables the Nippon Foundation. The first two chair-people were... colorful. Leading to some 'colourful activities': >In 2001, the Peruvian Congress determined that the foundation financed forced sterilizations in that country (1990-1999). First chairman: 1: https://en.wikipedia.org/wiki/Ry%C5%8Dichi_Sasakawa >Ryōichi Sasakawa (笹川 良一, Sasakawa Ryōichi, May 4, 1899, in Minoh City, Osaka – July 18, 1995, in Tokyo)[1] was a Japanese suspected war criminal, businessman, far-right politician, and philanthropist. He was born in Minoh, Osaka. In the 1930s and during the Second World War he was active both in finance and in politics, actively supporting the Japanese war effort including raising his own paramilitary units. He was elected to the Japanese parliament during the war. After Japan's defeat he was imprisoned for a time, accused of war crimes, and then found financial success in various business ventures, including motorboat-racing gambling events (Kyōtei) and ship building. He supported anticommunist activities, including the World Anti-Communist League Second chair-woman: 2: https://en.wikipedia.org/wiki/Ayako_Sono > Sono is considered to be a conservative. She was considered to be an advisor to Prime Minister Shinzo Abe. She has drawn controversy for advocating for a system similar to South Africa's apartheid for Japan's immigrants.[1][2][3] She has also advocated for women to quit their jobs after becoming pregnant.[4] The third is less colorful (at least his Wikipedia is highly curated), but the son of the first chairman, so who knows.... reply toomuchtodo 18 hours agoprevProject: https://seabed2030.org/ Related: https://news.ycombinator.com/item?id=38231626 (“HN: Why Is It So Difficult to Map the Ocean?”) reply araes 17 hours agoparentThe \"Explore the Global Map\" portion's pretty interesting, as there are several regions where the difference is rather clear. Gulf of Siam and the Caribean both have fairly clear areas where the low res old data and the higher res new collection can be examined. Unfortunately, if you zoom in too much though it goes back to the old Maxar data. The Azores and the Mid-Atlantic Ridge work is also kind of interesting. Much larger number areas that are \"almost islands\" than I thought there were. The old GIS data seems smeared out and makes them look shallow, yet the newer maps show pretty distinct near surface ridges. reply BurningFrog 16 hours agoprevSince nuclear submarines and intercontinental data cables are militarily very important, I assume the US and other militaries have mapped vastly more than the civilian world has access to. reply nine_k 16 hours agoparentSubmarines don't go deeper than couple kilometers, usually less than 500 m. Vast areas of the ocean are way deeper, and that depth likely has little operational importance for submarines. Cable-laying is another thing, but only along the lines that connect interesting points on the shore. reply knute 15 hours agorootparentEven at submarine depths, the oceans are poorly mapped. In 2005, the USS San Francisco collided with an uncharted (but suspected) seamount at 160m depth. [0] https://en.wikipedia.org/wiki/USS_San_Francisco_(SSN-711)#Gr... reply fletchowns 8 hours agorootparentprevManned submarines, perhaps, but what about unmanned submarines? reply rocqua 28 minutes agorootparentDo those have military relevance at those depths? reply peppertree 17 hours agoprevThe most interesting part is the funding came from Japanese motorboat racing profit. Imaging if we could funnel profit from NFL to benefit science or local communities. reply tacocataco 16 hours agoparentI often think about what could be if artists and scientists were free to explore their crafts and passions. What are we missing out on because it's not profitable right now? reply random_kris 16 hours agorootparentWell if it isn't profitable it isn't desirable by society enough? reply wongarsu 16 hours agorootparentOften you have chains of invention, where it's not obvious from the outset that a specific outcome can be achieved by making that specific discovery or invention. Other issues are cases where the profit to society is clear, but difficult to capture by any specific group (the reason why infrastructure is mostly built by governments), and cases where discoveries happen by chance because somebody was given the chance to \"screw around\" (for example the person who found the xz vulnerability probably couldn't have justified why he put engineering time into investigating a 0.5 second delay). reply snapcaster 13 hours agorootparentprevDo you actually truly deep down believe this? You don't look around and see a single valueable thing that isn't profitable? reply wolverine876 11 hours agorootparentOr see highly profitable things that aren't valuable, or that are even highly damaging? reply kouru225 14 hours agorootparentprevYou gotta think about profitability across a larger timescale. Some things aren’t profitable now, but will be later, and if we get started on them now we can make that later come sooner. reply malfist 16 hours agorootparentprevThat's a very bad take. Things have value even if they're not profitable. Do you think the pharaohs built the pyramids because they were profitable? Look how important they are to the world culture. reply gus_massa 11 hours agorootparentThe pyramids were a power demonstration. Probably indirectly profitable. Some unespoken threat like Fake> I ordered them to build the pyramid. If you revolt I will order them to pillage your village. Have a nice day and please send 20% more goats. reply roywiggins 10 hours agorootparentThe most familiar thing to compare the pyramids to is probably European cathedrals. They were huge public works projects requiring a lot of labour, both skilled and unskilled, and done for religous, political, and probably economic reasons. reply stuartjohnson12 14 hours agorootparentprevInteresting choice of example as far as ROI calculations go reply richwater 8 hours agorootparentprev> world culture Do people in 2024 really care about the pyramids? reply rvba 3 hours agoprevHow strong are the sonars used by those ships? Do they hurt / kill fish and other sea creatures? I read that they made whales deaf. Obviously military does not care about that. reply mikece 17 hours agoprevIs the NSA a secret funder of this project? I suppose having accurate bathymetric data might have business or scientific applications but the only immediate use that comes to mind is undersea warfare. reply marsRoverDev 17 hours agoparentI would put good money on the US Navy already having a map that is superior to this. reply mikece 17 hours agorootparentFor certain parts of the world it's more than a certainty -- I've seen them! (Before my computer career I was a navigator in the US Navy.) Where we don't yet have hyper-accurate bathymetric data there are boats like the USS Jimmy Carter which have lots of tricks and capabilities that we'll never learn about. reply maxglute 14 hours agorootparentWithout divulging anything, as a map enthusiast, are there any public images of maps submariners use to navigate? It is just like bathymetric/contour maps with similar notations? Do they look cool? reply tbihl 10 hours agorootparentI'm going to hazard a guess, by your use of 'map' rather than 'chart', that you're not very familiar with nautical charts. A lot of what they do is chart depths, including broad contours which get more precise in shallower areas and closer to land (not always the same thing). Check out https://fishing-app.gpsnauticalcharts.com/i-boating-fishing-... reply maxglute 5 hours agorootparentThanks, I've \"enjoyed\" nautical charts before, not for use, but the design. I just like seeing different kinds of maps/schematics and seeing their symbology or notation. Want to see something for submersible navigation, I imagine it's mostly countrs and depth indicators, but what about stuff like currents, thermocline layers, landmarks... like is there a big geologic underwater arch somewhere that subs pass under for shits and giggles. What do charts show subs that operate at 500m or vehicles designed to deeper. Things like that. reply reaperman 17 hours agorootparentprevWas this already public info? I feel weird about people sharing classified info on the forums I frequent. (Even information that I don’t agree should be classified) reply crote 17 hours agorootparentIt's public knowledge that the US Navy has its own fleet of surveying vessels, concluding that they actually use them as well isn't much of a stretch. reply vundercind 16 hours agorootparentI’ve had no access to classified anything ever and could have told you the US navy has remarkably good maps of several key parts of the ocean in which friendly or enemy boomer subs are likely to operate. Like, of course they do. If there are any unusually-good ocean maps they have, it’s those, at least. reply mikece 17 hours agorootparentprevIt's also public knowledge that the USS Jimmy Carter is a \"very special\" submarine. I have no direct knowledge of what it can do (I was a surface warfare type) but splicing into fiberoptic cables has been rumored many times by many sources. reply colmmacc 8 hours agorootparent\"Blind Man's Bluff\" is a great book about the work of espionage submarines and includes many anecdotes from other boats that were based at Kitsap. Tapping cables is definitely something that's been done through history, but more of the stories were about things like recovering the remains of enemy test missiles for physical intelligence. Now that that fiber traffic is routinely encrypted anyway, I wouldn't be surprised if more of USS Jimmy Carter's missions are recovering North Korean rockets. reply swarnie 13 hours agorootparentprevYou paid for it, why not? reply jjk166 17 hours agoparentprevThe main reason why these parts of the ocean are not mapped is because ships don't go there. In proximity to inhabited land and along seaways that ships regularly travel there are much better maps readily available. Not a lot of warfare is going to happen in a place where there are no assets to defend and no targets to attack and which isn't on the way to or from a location with either. reply rocqua 25 minutes agorootparentThose boring places would be great places to hide misske carrying subs no? And hence other subs might want to engage there. reply ecommerceguy 16 hours agoparentprevGoogle Earth, Formerly Keyhole EarthViewer, funded in large part by the CIA. https://www.businessinsider.com/the-cias-earthviewer-was-the... reply sandworm101 15 hours agorootparentKeyhole being a very old reference to the shape drawn on the earth by geostationary sat orbits. reply adolph 14 hours agorootparentThe KH series of satellites were in low earth orbit, not geostationary. Where did you get the idea the keyhole name came from a shape? An NRO history has all early satellite reconnaissance as being under the “TALENT-KEYHOLE” designation: https://www.nro.gov/Portals/65/documents/foia/docs/HOSR/SC-2... reply sandworm101 13 hours agorootparenthttps://ar.inspiredpencil.com/pictures-2023/geosynchronous-o... Many geosync orbits trace an asymmetrical 8 on the ground that looks like a keyhole. That's where the name comes from, from programs older than the TK sats. reply ianburrell 7 hours agorootparentKeyhole reconnaissance satellites predate geostationary satellites. Geostationary reconnaissance satellites are rare, and not like Keyhole ones. reply adolph 9 hours agorootparentprev> from programs older than the TK sats. What was that older than Corona? reply maxglute 14 hours agorootparentprevWonder if subsea map will ever be released like Google Maps. It's something I always wanted to see. I suppose it's more of national security threat since it allows adversaries to plot up subsurface routes, and can't exactly blur out features without giving away importance like on Google maps. Unless they're deliberately tampered to cause confusion... maybe that's enough motivation to release. Get foreign subs to bonk on undersea cliffs. reply maxwell 17 hours agoparentprevhttps://en.wikipedia.org/wiki/Deep_sea_mining reply fburnaby 11 hours agoprevDoes anyone here know if and how the IHO's S-100 data formats relate to this? reply sandworm101 17 hours agoprevHow quickly we forget the ties between mapping and exploitation. The British Empire didn't map the world to preserve it, nor will these maps be used to protect the seafloor. This is about demarcation ahead of future development and resource extraction. Sure, some interesting areas will be saved, but thanks to these more accurate maps such areas can now be defined as narrowly as possible. reply tomrod 17 hours agoparentNah. We aren't creating rutters to navigate tough waters for enforcing colonialism. Sometimes the past doesn't predict the future with much confidence. reply itishappy 16 hours agorootparent> Since the ISA's inception in 1994, the Authority has approved over two dozen ocean floor mining exploration contracts in the Atlantic, Pacific and Indian Oceans, with the majority of contracts for exploration in the Clarion-Clipperton Zone between Hawaii and Mexico, where polymetallic nodules contain copper, cobalt and other minerals used to power electric batteries. To date, the Authority has not authorized any commercial mining contracts as it deliberates over regulations amid global calls for a moratorium on deep sea mining. https://en.wikipedia.org/wiki/International_Seabed_Authority reply refulgentis 16 hours agorootparentIs there some additional context that got cut out? I'm not sure this adds anything to the observation that there's a distinction between this and British empire colonialism. reply itishappy 15 hours agorootparentWhat drove colonialism? It wasn't a desire to meet new and interesting cultures. It was land for development and resources for extraction. What's driving our current exploration binge? Land for development and resources for extraction. Regardless of intent, they share many of the same incentive structures. That being said, I'm quite surprised by the partner list from the project website. I may be discounting conservation from my analysis. https://seabed2030.org/our-partners/ reply refulgentis 15 hours agorootparent+1 on partners. Riffing on that: my priors entering are \"oh I've been reading an article in the Economist, every 2 months, for 2 years about how scaling green energy requires deep sea bed mining, and it's incredibly non-invasive because they just sit in coalesced lumps down there\" From that perspective, comparing it to subjugating human populations seems like a non-sequitur, though one I respect. reply itishappy 14 hours agorootparentFair point. Colonial empires have some emotional baggage that I completely glossed over... The good news is that the impact of sea bed mining on the human population of the deep ocean appears negligible! The bad news is that the environmental impact is still quite uncertain, and the lack of resident humans could make advocacy more challenging. From the Wikipedia article I linked about the ISA: > The ISA is funded by UNCLOS members and mining contractors and led by Secretary-General Michael Lodge, a British barrister who oversees a 47-member administrative body and has come under criticism for close ties to the mining industry and support for deep sea robotic exploration to develop renewable energy. Also, I feel compelled to respond to to the comment that it's \"incredibly non-invasive,\" as I'm not sure that's how it actually works out. In theory, we can simply pluck the nodules from the seabed. In practice this requires sending a multi-ton robot vacuum cleaner down to suck up and sift through the top layer of the seafloor, pump the chunks back to the surface for processing, and spit the silt back overboard. Here's a video of how we currently mine shallow-water deposits by hand. (Deep sea nodules are a bit different, they sit on the surface, so less disruption to the seabed is required. Less isn't none though, and surface is where life concentrates.) https://www.youtube.com/watch?v=aYAw26nSzBA In this light I'm actually quite enheartened by the partner list of the Seabed2030 project. reply Teever 17 hours agorootparentprevWe're going to be using these maps to expand deep sea mining that will certainly result in the poisoning of innocent people in remote regions of the world whn their food sources become contaminated from whatever we dredge up. It's colonialism and exploitation of a different kind. I'm not expecting you to single handedly put a stop to it or anything but it's not unreasonable to expect that you can recognize and acknowledge it instead of pretending it doesn't exist and saying that those sort of things only happened a long time ago. reply non-chalad 7 hours agorootparentSo compete with eco-friendly dredging, and show them a better way? reply nerdponx 5 hours agorootparentprev\"You gotta tell people... The oceans are dying, plankton's dying... it's people. Soylent Green is made out of people.\" reply LargoLasskhyfv 4 hours agorootparentYäss, Yäss. By employing alcalic hydrolysis and growing fungi from that sludge. Maybe with some fried bugs & worms added. Soylent Green Sesame? reply ramesh31 14 hours agorootparentprevMaps exist to define ownership. Everything else is tertiary to that. Lewis and Clarke weren't \"blazing a trail\" for the pioneers. They were measuring the land that the US government had just purchased. reply rocqua 20 minutes agorootparentThese are charts, they exist mainly to aid in navigation with secondary benefits to exploration for resource extraction. Besides, maps are very much about taxation, balistics, and administration. Much more than just ownership. reply JumpCrisscross 12 hours agorootparentprev> Maps exist to define ownership ...you've never used a map in a different way? When we draw maps of the galaxy, are we imminently planning on dividing it up? reply ramesh31 11 hours agorootparent>you've never used a map in a different way? Sure. But the USGS doesn't publish precise up to date topological maps of every square inch of the United States for the purpose of guiding backpackers with compasses. They do it to divide up political boundaries, delimit resource rights, and guide commercial activity. This goes for just about any other large scale mapping effort (on earth). reply BurningFrog 16 hours agoparentprev\"Exploiting\" the natural resources of the oceans, if allowed, will be a huge benefit to humanity. Just like we've benefitted enormously from \"exploiting\" land resources. reply nine_k 16 hours agorootparentBut for steel, gold, silver, salt, gems, fuel, fertilizers, uranium, lithium, semiconductors, stone, cement, bricks, plastics, aluminum, glass, and chalk for schools, what did exploitation of land mineral resources ever give to us? reply SCUSKU 15 hours agorootparentThe aqueduct? reply tomoyoirl 16 hours agorootparentprevwell, the Bronze Age, for one And regular conductors … reply micromacrofoot 15 hours agorootparentprev> what did exploitation of land mineral resources ever give to us the entirety of modern society, good and bad... the computer or phone you're typing this with, your job, etc reply Night_Thastus 15 hours agorootparentThey're making a joke, referring to a skit by Monty Python. reply micromacrofoot 14 hours agorootparentI see, that explains why it wasn't funny reply albumen 13 hours agorootparentIf you mean, \"I didn't get the joke because I'm not familiar with the cultural reference\", fair enough. If you mean, \"it's Monty python, therefore not funny\", you must be fun at parties. Now go away before I taunt you a second time. reply sleepingreset 16 hours agorootparentprevwhat makes you so certain we can coordinate our resources and act in unprofitable ways to ensure long-term survival? genuine question reply BurningFrog 16 hours agorootparentCertainty is hard, but we've done pretty well with it on land, so I think leaving the 3x larger mineral resources untapped would be madness. Not saying things will be perfect, of course. They never are. reply godelski 16 hours agorootparentFWIW, that land allocation came through a lot of blood allocation. Of course, wars are fought very differently these days but I wouldn't be so quick to dismiss the \"transitionary period\" reply ToValueFunfetti 16 hours agorootparentFortunately the merpeople are already extinct; no genocide necessary this time. More seriously, I'd expect greater resource availability to result in fewer wars over it. That's pretty consistent with human history. reply godelski 15 hours agorootparentWe fought many wars over lands that no one occupied. In fact, we've fought many wars over maritime territories before too. Wars are far from exclusively fought via inhabiting force and invading force. How do you think those boundaries were developed in the first place? People don't grow out of the ground and we all originated from Africa. In fact, not even the whole of Africa! reply samus 15 hours agorootparentprevDid we really do so well on land? We are already looking at overtaxing our planet's carrying capability. 21st century will be pretty rough as the developing countries' lifestyle will catch up and they won't accept the richer countries telling them to be more sustainable unless the latter become so as well. Unfortunately, our level of consumption and environmental destruction is already unsustainable. reply BurningFrog 12 hours agorootparentDid we really do so well on land? We grew from 50 million hunter gatherers to 8000 millions mostly living longer better lives, and have access to Hacker News. So for me that's a very strong Yes. reply sleepingreset 10 hours agorootparentI am not asking you if human progress is speeding along and accelerating. That's an obvious yes. I am asking you: are we speeding into a brick wall? If so, can we steer the car? Who steers the car? Does ANYONE steer the car? Or do market forces just decide where we go. How do we ant colony our way out of this bitch before we all die? reply BurningFrog 8 hours agorootparentNobody has steered humanity's car so far, and it's gone well. I'll give you my favorite answer. You probably won't like it. For reasons I don't claim to know, humanity always expect the world to end. It's in all the religions. Christians has expected the Last Judgment any day now for 2000 years. In this century we don't express that urge in religious term. It manifests in worries that pollution or overpopulation or nuclear war will bring the end of times. I've seen enough of these predictions over the last 50 years to stop worrying too much about the new ones that keep popping up. Of course non of this proves we won't speed into one of these brick walls. None of us knows the future, and that's certainly a possible one. reply samus 4 hours agorootparentIt worked so far because our desire to grow never before threatened to exceed the carrying capacity of the whole planet. Growth is not stopped by people, but by environmental boundaries. So far the cost of failure was never the near-extinction of humanity. When civilizations exceed the carrying capacity of their environment, the results aren't pretty. Civilizational collapse is real. It has happened many times to flourishing cultures and might happen again. Most cultures never recover*. The difference is that previously there was always another culture that was not pulled into the abyss as well and could pick up the leftovers. reply samus 12 hours agorootparentprevAs I indicated above, this seems to be an unsustainable state of affairs and an example of the mentality of kicking the can down the road. Let's hope we can continue living longer and better lives. reply perihelions 16 hours agoparentprevThe US government actually has an ongoing push right now to claim parts of the Arctic Ocean, for mining purposes, on the basis of bathymetric data. https://www.bloomberg.com/news/articles/2023-12-22/us-claims... (\"US Claims Huge Chunk of Seabed Amid Strategic Push for Resources\", https://archive.is/9Uyg4 ) https://www.ncei.noaa.gov/products/us-extended-continental-s... (\"U.S. Extended Continental Shelf Data\") - \"The mission of the U.S. Extended Continental Shelf (ECS) Project is to establish the full extent of the U.S. continental shelf, consistent with international law. \" - \"Data collected for the project include bathymetric, subbottom, gravity, magnetic, seismic , and geologic sample data from the U.S. coastal waters to the deep ocean. U.S. ECS project data are in the public domain.\" reply dragonwriter 15 hours agorootparent> The US government actually has an ongoing push right now to claim parts of the Arctic Ocean as territorial waters. No, it doesn't. (Beyond that part of the Arctic Ocean already generally recognized as US territorial waters.) “Continental shelf” is not “territorial waters”, it is a whole different legal category. The project to identify the extent of what qualifies under international law as continental shelf beyond the 200nm presumed limit is not about extending territorial waters. reply perihelions 15 hours agorootparentAlright, I've edited out \"territorial waters\" from my comment. reply dragonwriter 15 hours agorootparentThe correct term for the legal category which they are trying to determine the extent of is “continental shelf”, hence the name of the project (under international law this is legal category is determined by the greater of 200nm or the natural extent of the physical continental shelf.) reply acchow 13 hours agoparentprevIf I'm understanding the history correctly, the British Empire eventually actively fought to end the slave trade because they realized there was something even more profitable than colonialism and extraction: trade. reply ricardoplouis 13 hours agorootparentChasing profits didn't cause slavery to end, it was the abolitionists who were standing up for human rights. Historically massive social change doesn't come from the top down like you are describing. It would be akin to saying LBJ was a civil rights hero for his legislation. Sure, he signed it, but the civil rights movement of 60s is why it was possible in the first place. Hardly can one deserve credit for playing a forced hand. reply dr_dshiv 17 hours agoparentprevTotally agree. We need to know where we can establish kelp farms. reply jjk166 17 hours agoparentprevBy that argument, anyone who drinks a gin and tonic today must be trying to protect themselves from malaria. reply eschneider 17 hours agorootparentIt's worked for me so far. reply Waterluvian 17 hours agorootparentprevMy house has a threshold to hold the thresh in the room I do all my threshing. reply giantg2 17 hours agorootparentI do my threshing where I do my winnowing. reply deadbabe 17 hours agoparentprevAre those resources more valuable just sitting at the bottom of a deep sea? reply crote 17 hours agorootparentThe Statue of Liberty is just sitting there too - it would be far more valuable recycling all that copper and putting it to use. Could there perhaps be another reason we haven't melted it down yet? reply tacocataco 17 hours agorootparent\"Not like the brazen giant of Greek fame, With conquering limbs astride from land to land; Here at our sea-washed, sunset gates shall stand A mighty woman with a torch, whose flame Is the imprisoned lightning, and her name Mother of Exiles. From her beacon-hand Glows world-wide welcome; her mild eyes command The air-bridged harbor that twin cities frame. \"Keep, ancient lands, your storied pomp!\" cries she With silent lips. \"Give me your tired, your poor, Your huddled masses yearning to breathe free, The wretched refuse of your teeming shore. Send these, the homeless, tempest-tost to me, I lift my lamp beside the golden door!\" It's remarkable how people forget how we all got here. reply pimlottc 16 hours agorootparentDon't forget about the people who were already here, too reply Retric 17 hours agorootparentprev> The Statue of Liberty is just sitting there too - it would be far more valuable recycling all that copper and putting it to use. The US makes a several orders of magnitude more money from the Statue of Liberty where it is than they would get from its scrap value. https://www.nps.gov/stli/planyourvisit/fees.htm reply BobaFloutist 16 hours agorootparentThe same math could probably be done for sustainable stewardship of our oceans, just with more steps. reply JumpCrisscross 12 hours agorootparent> same math could probably be done for sustainable stewardship of our oceans, just with more steps Could it? We can measure the cash flow the Statue of Liberty produces relative to its commodity value. A pristine deep sea certainly has value. But it's difficult to argue that every square inch of it is more valuable than the commodities on and below it. Particularly when you start trading off extraction there against terrestrial mining. reply dopidopHN 12 hours agorootparentHow fish were fished from place X 40 years ago. How much now. Make the difference. reply s0rce 17 hours agorootparentprevSeems highly unlikely, tourism economic value must exceed the scrap value of the copper... reply sandworm101 17 hours agorootparentprev>> another reason we haven't melted it down yet? Because it was gift. It was supported in the US locally by donations, but it was conceived and built in France as a gift to a younger nation. reply sockaddr 17 hours agorootparentprevMy first reaction is: No But if you rephrase it as \"Are those resources more valuable just sitting at the bottom of a deep sea instead of mixing them into our bodies and environment?\" my reaction is now: Yes reply hex4def6 16 hours agorootparentBut then, also: \"And what if the (human) environment is damaged / contaminated less by deep undersea extraction vs land-based mines\"? and also: \"What if an abundance of these materials enables vastly cheaper energy storage batteries, making solar / wind energy overnight storage practical, reducing our reliance on cheap fossil fuel energy generation\"? To be clear, I'm not sure if either of those hypotheticals are true, but I have a feeling, as with many things in life is \"It's complicated\". Being good stewards of our resources with careful management / regulation is the answer, rather than unfettered exploitation or outright bans. reply dopidopHN 12 hours agorootparentprevThose bottom are brittle ecosystem that thrive in silence and total darkness. It would be nice to document and observe first, rather than barging in to get the riches as fast as possible once again. ( riche in that point being « nodules » it’s a fun resource ! ) reply pygy_ 16 hours agorootparentprevExtracting these resources means destroying the ecosystems that live on top of them. So, yes, they are better left to their own devices. reply caycep 13 hours agoprevqueue Red October quotes :) reply TypicalHog 17 hours agoprevDon't know the resolution they're working with, but they should defo prioritize the MH370 potential crash area. reply johnp_ 14 hours agoparentIt says in the article > Seabed 2030 is a long-term mapping project attempting to fully chart the seafloor and reveal all features 100 meters or larger by 2030. There is an area of interest, \"Area 3: Indian Ocean: Ninetyeast Ridge\" around 30S 87E, which specifically mentions MH370: https://seabed2030.org/wp-content/uploads/2023/06/AtlanticIn... Though that is quite a bit (~830 km / ~1250 km from the center of the square around 30S 87E) off to the (north)west of the most recent search proposal from mh370search.com: https://www.mh370search.com/2024/03/16/mh370-a-new-hope/ edit: Got the distance wrong. Corrected. reply xandrius 17 hours agoparentprevWhy? reply luxuryballs 17 hours agoprev [–] If we just discovered these giant mountains then what are the chances we will find a new “deepest point”? reply s0rce 17 hours agoparent [–] My guess is low, the deepest points are specific trenches related to intersecting tectonic plates and are of relatively higher scientific interest so many have been mapped. Its unlikely there is just a giant hole somewhere random. The trenches are way deeper than the ocean floor https://education.nationalgeographic.org/resource/ocean-tren... and the deepest on e is quite deep, maybe there is a slightly deeper point in the deepest trench? I'm not a ocean floor expert though. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Seabed 2030 is a project with a mission to completely map the seafloor by 2030, with only 25% of the mapping completed as of now.",
      "The project utilizes data contributions and collaborations to make new discoveries like large seamounts, leveraging satellite data and cutting-edge technologies.",
      "To expedite the mapping process, Seabed 2030 incorporates autonomous vehicles and artificial intelligence, aiming to back sustainable development objectives and enhance awareness of marine challenges."
    ],
    "commentSummary": [
      "The discussion encompasses seafloor mapping, military use, historical aspects, and ethical dilemmas of deep-sea mining.",
      "It explores the significance of precise cryptography, gravity effects on missile precision, and environmental impacts of seabed mining.",
      "Additionally, it addresses the US Navy's mapping prowess, submarine secrecy, and challenges in ocean floor mapping accuracy."
    ],
    "points": 238,
    "commentCount": 130,
    "retryCount": 0,
    "time": 1712152626
  },
  {
    "id": 39918500,
    "title": "Plandex: Open-Source AI Coding Engine",
    "originLink": "https://github.com/plandex-ai/plandex",
    "originBody": "Hey HN, I&#x27;m building Plandex (https:&#x2F;&#x2F;plandex.ai), an open source, terminal-based AI coding engine for complex tasks.I built Plandex because I was tired of copying and pasting code back and forth between ChatGPT and my projects. It can complete tasks that span multiple files and require many steps. It uses the OpenAI API with your API key (support for other models, including Claude, Gemini, and open source models is on the roadmap). You can watch a 2 minute demo here: https:&#x2F;&#x2F;player.vimeo.com&#x2F;video&#x2F;926634577Here&#x27;s a prompt I used to build the AWS infrastructure for Plandex Cloud (Plandex can be self-hosted or cloud-hosted): https:&#x2F;&#x2F;github.com&#x2F;plandex-ai&#x2F;plandex&#x2F;blob&#x2F;main&#x2F;test&#x2F;test_pr...Something I think sets Plandex apart is a focus on working around bad outputs and iterating on tasks systematically. It&#x27;s relatively easy to make a great looking demo for any tool, but the day-to-day of working with it has a lot more to do with how it handles edge cases and failures. Plandex tries to tighten the feedback loop between developer and LLM:- Every aspect of a Plandex plan is version-controlled, from the context to the conversation itself to model settings. As soon as things start to go off the rails, you can use the `plandex rewind` command to back up and add more context or iterate on the prompt. Git-style branches allow you to test and compare multiple approaches.- As a plan proceeds, tentative updates are accumulated in a protected sandbox (also version-controlled), preventing any wayward edits to your project files.- The `plandex changes` command opens a diff review TUI that lets you review pending changes side-by-side like the GitHub PR review UI. Just hit the &#x27;r&#x27; key to reject any change that doesn’t look right. Once you’re satisfied, either press ctrl+a from the changes TUI or run `plandex apply` to apply the changes.- If you work on files you’ve loaded into context outside of Plandex, your changes are pulled in automatically so that the model always uses the latest state of your project.Plandex makes it easy to load files and directories in the terminal. You can load multiple paths: plandex load components&#x2F;some-component.ts lib&#x2F;api.ts ..&#x2F;sibling-dir&#x2F;another-file.tsYou can load entire directories recursively: plandex load src&#x2F;lib -rYou can use glob patterns: plandex load src&#x2F;**&#x2F;*.{ts,tsx}You can load directory layouts (file names only): plandex load src --treeText content of urls: plandex load https:&#x2F;&#x2F;react.dev&#x2F;reference&#x2F;react&#x2F;hooksOr pipe data in: cargo testplandex loadFor sending prompts, you can pass in a file: plandex tell -f \"prompts&#x2F;stripe&#x2F;add-webhooks.txt\"Or you can pop up vim and write your prompt there: plandex tellFor shorter prompts you can pass them inline: plandex tell \"set the header&#x27;s background to #222 and text to white\"You can run tasks in the background: plandex tell \"write tests for all functions in lib&#x2F;math&#x2F;math.go. put them in lib&#x2F;math_tests.\" --bgYou can list all running or recently finished tasks: plandex psAnd connect to any running task to start streaming it: plandex connectFor more details, here’s a quick overview of commands and functionality: https:&#x2F;&#x2F;github.com&#x2F;plandex-ai&#x2F;plandex&#x2F;blob&#x2F;main&#x2F;guides&#x2F;USAGE...Plandex is written in Go and is statically compiled, so it runs from a single small binary with no dependencies on any package managers or language runtimes. There’s a 1-line quick install: curl -sL https:&#x2F;&#x2F;plandex.ai&#x2F;install.shbashIt&#x27;s early days, but Plandex is working well and is legitimately the tool I reach for first when I want to do something that is too large or complex for ChatGPT or GH Copilot. I would love to get your feedback. Feel free to hop into the Discord (https:&#x2F;&#x2F;discord.gg&#x2F;plandex-ai) and let me know how it goes. PRs are also welcome!",
    "commentLink": "https://news.ycombinator.com/item?id=39918500",
    "commentBody": "Plandex – an AI coding engine for complex tasks (github.com/plandex-ai)234 points by danenania 18 hours agohidepastfavorite77 comments Hey HN, I'm building Plandex (https://plandex.ai), an open source, terminal-based AI coding engine for complex tasks. I built Plandex because I was tired of copying and pasting code back and forth between ChatGPT and my projects. It can complete tasks that span multiple files and require many steps. It uses the OpenAI API with your API key (support for other models, including Claude, Gemini, and open source models is on the roadmap). You can watch a 2 minute demo here: https://player.vimeo.com/video/926634577 Here's a prompt I used to build the AWS infrastructure for Plandex Cloud (Plandex can be self-hosted or cloud-hosted): https://github.com/plandex-ai/plandex/blob/main/test/test_pr... Something I think sets Plandex apart is a focus on working around bad outputs and iterating on tasks systematically. It's relatively easy to make a great looking demo for any tool, but the day-to-day of working with it has a lot more to do with how it handles edge cases and failures. Plandex tries to tighten the feedback loop between developer and LLM: - Every aspect of a Plandex plan is version-controlled, from the context to the conversation itself to model settings. As soon as things start to go off the rails, you can use the `plandex rewind` command to back up and add more context or iterate on the prompt. Git-style branches allow you to test and compare multiple approaches. - As a plan proceeds, tentative updates are accumulated in a protected sandbox (also version-controlled), preventing any wayward edits to your project files. - The `plandex changes` command opens a diff review TUI that lets you review pending changes side-by-side like the GitHub PR review UI. Just hit the 'r' key to reject any change that doesn’t look right. Once you’re satisfied, either press ctrl+a from the changes TUI or run `plandex apply` to apply the changes. - If you work on files you’ve loaded into context outside of Plandex, your changes are pulled in automatically so that the model always uses the latest state of your project. Plandex makes it easy to load files and directories in the terminal. You can load multiple paths: plandex load components/some-component.ts lib/api.ts ../sibling-dir/another-file.ts You can load entire directories recursively: plandex load src/lib -r You can use glob patterns: plandex load src/**/*.{ts,tsx} You can load directory layouts (file names only): plandex load src --tree Text content of urls: plandex load https://react.dev/reference/react/hooks Or pipe data in: cargo testplandex load For sending prompts, you can pass in a file: plandex tell -f \"prompts/stripe/add-webhooks.txt\" Or you can pop up vim and write your prompt there: plandex tell For shorter prompts you can pass them inline: plandex tell \"set the header's background to #222 and text to white\" You can run tasks in the background: plandex tell \"write tests for all functions in lib/math/math.go. put them in lib/math_tests.\" --bg You can list all running or recently finished tasks: plandex ps And connect to any running task to start streaming it: plandex connect For more details, here’s a quick overview of commands and functionality: https://github.com/plandex-ai/plandex/blob/main/guides/USAGE... Plandex is written in Go and is statically compiled, so it runs from a single small binary with no dependencies on any package managers or language runtimes. There’s a 1-line quick install: curl -sL https://plandex.ai/install.shbash It's early days, but Plandex is working well and is legitimately the tool I reach for first when I want to do something that is too large or complex for ChatGPT or GH Copilot. I would love to get your feedback. Feel free to hop into the Discord (https://discord.gg/plandex-ai) and let me know how it goes. PRs are also welcome! CGamesPlay 4 hours agoLove the idea of this, and very excited to see how it pans out. That said: I hate the code review UI. Just dump the changes as `git diff` does and let me review them using all the code review tools I use every day, then provide revision instructions. Building a high-quality TUI for side-by-side diffs should not be the thing you are spending time on, and there already exist great tools for viewing diffs in the terminal. reply danenania 4 hours agoparentThanks for the feedback! I actually had a ‘plandex diff’ command working at one point, but dropped it in favor of the changes TUI. I could definitely bring it back for people who prefer that format. reply retendo 3 hours agorootparentYou could have a mode for people „who know what they are doing“ and just auto approve all the changes plandex makes and let users handle the changes themselves. I would actually prefer that, because I could keep using my IDE to look at diffs and decide what to keep. reply wanderingmind 10 hours agoprevCongrats on the launch. Can you please compare and contrast Plandex features with another similar solution like aider[1] which also helps solve similar problem. [1] https://github.com/paul-gauthier/aider reply anotherpaulg 6 hours agoparentThanks for mentioning aider! I haven't had a chance to look closely at plandex, but have read the author's description of differences wrt aider. I'd add a few comments: I think the plandex UX is novel and interesting. The idea of a git-like CLI with various stateful commands is a new one in this space of ai coding tools. In contrast, aider uses a chat based \"pair programming\" UX, where you collaborate with the AI and ask for a sequence of changes to your local git repo. The plandex author highlights that it makes changes in a \"version-controlled sandbox\" and can \"rewind\" unwanted changes. These capabilities are all available \"for free\" in aider, because it is tightly integrated with git. Each AI change is automatically git committed with a sensible commit message. You can type “/diff” to check the diff, or \"/undo\" to undo any AI commit that you don't like. Or you can use \"/git checkout -b \" to start working on a branch to explore a longer sequence of changes, etc. All your favorite git workflows are supported by invoking familiar git commands with \"/git ...\" inside the aider chat, or using any external git tooling that you prefer. Aider notices any changes in the underlying repo, however they occur. reply danenania 10 hours agoparentprevThanks! Sure, I posted this comment in a Reddit thread a couple days ago to a user who asked the same question (and I added one additional point): First I should say that it’s been a few months at least since I’ve used aider, so it’s possible my impression of it is a bit outdated. Also I’m a big fan of it and drew a lot of inspiration from it. That said: Plandex is more focused on building larger and more complex functionality that involves multiple steps, whereas aider is more geared toward making a single change at a time. Plandex has an isolated, version-controlled sandbox where tentative changes are accumulated. I believe with aider you have to apply or discard each change individually? Plandex has diff review TUI where changes can be viewed side-by-side, and optionally rejected, a bit like GitHub’s PR review UI. Plandex has branches that allow for exploring multiple approaches. aider has cool voice input features that Plandex lacks. aider’s maintainer Paul has done a lot of benchmarking of file update strategies. While I think Plandex’s approach is better suited to larger and more complex functionality, aider’s unified diff approach may have higher accuracy for a single change. I hope to do benchmarking work on this in the future. aider requires Python and is installed via pip, while Plandex runs from a single binary with no dependencies, so Plandex installation is arguably easier overall, especially if you aren't a Python dev. I’m sure I’m missing some other differences but those are the main ones that come to mind. reply wanderingmind 10 hours agorootparentThank you. Branches to explore different approaches is a really good idea, since LLMs are most powerful when they are used as a rubber duck to generate boilerplate templates and this can help get multiple perspectives. Going to test it soon. reply parentheses 34 minutes agoprevThis looks so damn good! Can't wait to try it in the morning! reply liampulles 2 hours agoprevI appreciate in the copy here that you are not claiming plandex to be a super dev or some such nonsense. I really dislike the hype marketing in some other solutions. reply ldelossa 13 hours agoprevShow me one of these things do something more complex then a front end intern project. reply danenania 13 hours agoparentHere's a prompt I used to build the AWS infrastructure for Plandex Cloud with Plandex: https://github.com/plandex-ai/plandex/blob/main/test/test_pr... reply chmod2 12 hours agorootparentIt's not something I would consider a complex job. A simple prompt to chatgpt could even produce a working CDK template. reply danenania 12 hours agorootparentHere's another one, for the backend of a Stripe billing system: https://github.com/plandex-ai/plandex/blob/main/test/test_pr... It seems like more examples demonstrating relatively complex tasks would be helpful, so I'll work on those. I'm certainly not trying to claim that it can handle any task. The underlying model's intelligence and context size do place limits on what it can do. And it can definitely struggle with code that uses a lot of abstraction or indirection. But I've also been amazed by what it can accomplish on many occasions. reply IshKebab 13 hours agoparentprevI agree, these things seem to do okish on trivial web projects. I've never seen them do anything more than that. I still use ChatGPT for some coding tasks, e.g. I asked it to write C code to do some annoying fork/execve stuff (can't remember the details) and it did a decentish job, but it's like 90% right. Great for figuring out a rough shape and what functions to search for, but you definitely can't just take the code and expect it to work. Same when I asked it to write a device driver for some simple peripheral. It had the shape of an answer but with random hallucinated numbers. I've also noticed that because there is a ton of noob-level code on the internet it will tend to do noob-level things too, like for the device driver it inserted fixed delays to wait for the device to perform an operation rather than monitoring for when it had actually finished. I wonder if coding AIs would benefit from fine tuning on programming best practices so they don't copy beginner mistakes. reply danenania 13 hours agorootparentI used a web project in the demo because I figured it would be familiar to a wide range of developers, but actually many nontrivial pieces of Plandex have been built with the help of Plandex itself. That's not to say it's perfect or will never make \"noob-level\" mistakes. That can definitely happen and is ultimately a function of the underlying model's intelligence. But I can at least assure you that it's quite capable of going far beyond a trivial web project. It's also on me to show more indepth examples, so thanks for calling it out. I'd love it if you would try some of the projects you mention and let me know how it goes. reply timfsu 12 hours agoprevCongrats on the launch, I'm excited to give it a try. I'm curious how you're having it edit files in place - having built a similar project last summer, I had trouble with reliably getting it to patch files with correct line numbers. It was especially a problem in React files with nested div's. reply danenania 11 hours agoparentThanks! I tried many different ways of doing it before settling on the current approach. It's still not perfect and can make mistakes (which is why the `plandex changes` diff review TUI is essential), but it's pretty good now overall. I was able to improve reliability of line numbers by using a chain-of-thought approach where, for each change, the model first summarizes what's changing, then outputs code that starts and ends the section in the original file, and then finally identifies the line numbers from there. The relevant prompts are here: https://github.com/plandex-ai/plandex/blob/main/app/server/m... reply nico 11 hours agorootparentAmazing work. Loved the video and looking forward to trying it Can a user ask plandex to modify a commit? Maybe the commit just needs a small change, but doesn’t need to be entirely re-written. Can the scope be reduced on the spot to focus only on a commit? reply danenania 11 hours agorootparentThanks! There isn't anything built-in to specifically modify a commit, but you could make the modification to the file with Plandex and then `git commit --amend` for basically the same effect. reply visarga 13 hours agoprevThis approach works. I just built a SPA in 3 days with GPT-4 of which about 50% was generated. My only tooling was a bash script to list all the files in the repo (with some exceptions), including a README.md planning the project, a file list, and at the end I type my task. I run about 10-15 rounds with it. At the beginning I was using GPT more heavily, but in the middle I found it easier to just fix the code myself. The context got as big as 10k tokens, but was not a problem. At some point I might need to filter the files more aggressively. But surprisingly all that is needed for a bare-bone repo-level coding assistant is a script to list all the files so I could easily copy paste the whole thing into the chatGPT window. reply danenania 13 hours agoparentYes, well said. Doing exactly this kind of thing for months with ChatGPT is what convinced me the idea could work in the first place. I knew the underlying intelligence was there--the challenge is giving it the right prompts and supporting infra. reply Aeolun 10 hours agorootparentDo you have any of the issues where ChatGPT tends to forget the first parts of it’s context window? It could have the information explicitly spelled out, but if it weren’t in the last 2K tokens or so it’d just start to hallucinate stuff for me. reply danenania 9 hours agorootparentPlandex uses gradual summarization as the conversation gets longer (the exact cutoff point in terms of tokens is configurable via `plandex set-model`). So eventually, with a long enough plan, you can start to lose some resolution. That said, assuming you use the default gpt-4-turbo model with a 128k context window, you'd need to go far beyond 2k tokens before you'd start seeing anything like that. We don't know what ChatGPT's summarization strategy is since it's closed source, but it does seem to be quite a bit more aggressive than Plandex's. reply nico 11 hours agoparentprev> a script to list all the files so I could easily copy paste the whole thing Just in case you are using a Mac, you can pipe the output of your script to pbcopy so that it goes directly into your clipboard script.shpbcopy reply sanmon3186 7 hours agoparentprevWhat’s your experience with API cost? I've also tried something similar, but I often end up using up my balance too quickly. reply CGamesPlay 4 hours agorootparentI can generally have these tools solve a simple issue in about 0.1 USD, or \"complex\" issues in 1-2 USD (complex generally just means that I'm spending time prompt engineering to get the model to do the right thing). reply ugh123 11 hours agoparentprevDo you have any boilerplate part of your prompt you can share? reply rglover 14 hours agoprevThis is something I've been thinking a lot about (a way to set context for an LLM against my own code), thank you for putting this out. Looks really polished. reply danenania 13 hours agoparentThanks! Please let me know how it goes for you if you try it :) reply _bry-guy 10 hours agoprevWow, this is phenomenal! I can't wait to dig in. This is almost exactly the application I've been envisioning for my own workflow. I'm excited to contribute! reply danenania 10 hours agoparentThank you! Awesome, I'm glad to hear that! Looking forward to your thoughts, and your contributions :) reply FezzikTheGiant 2 hours agoprevCurious to know how you built this. Is it GPT-4 or a fine-tuned model. How much does it cost? reply mbil 13 hours agoprevLooks really interesting. Is it wrapping git for the rollback and diffing stuff? If I were a user I'd probably opt to use git directly for that sort of thing. reply danenania 13 hours agoparentYes, it does use git underneath, with the idea of exposing a very simple subset of git functionality to the user. There's also some locking and transaction logic involved to ensure integrity and thread safety, so it wouldn't really be straightforward to expose the repo directly. I tried to build the backend so that postgres, the file system, and git would combine to form effectively a single transactional database. reply asadalt 15 hours agoprevIn demo it modified UI components, is there any model that can look at the rendered page to see if it looks right? Right now all these wrappers just blindly edit the code. reply razster 14 hours agoparentI paired mine with VSCode and used the live view addon for that folder. So far so good. reply danenania 15 hours agoparentprevPlandex can't do this yet, but soon I want to add GPT4-vision (and other multi-modal models) as model options, which will enable this kind of workflow. reply asadalt 14 hours agorootparentWell I have built similar project that lives in github action, communicates via issues and sends PR when done. 4-vision isn't there yet. It can mostly OCR or pattern recognize the image if it's popular or has some known object. It cannot detect pixel differences or css/alignment issues. reply dr_kiszonka 9 hours agoprevHi! Is it possible to tell Plandex that the code should pass all tests in, e.g., `tests.py`? reply danenania 9 hours agoparentHey! Not in an automated way (yet). But you can get pretty close by building your plan, applying it, and then piping the output of your tests back into Plandex: pytest tests.pyplandex load plandex tell \"update the plan files to fix the failing tests from the included pytest output\" reply splatzone 11 hours agoprevThis is really cool. I tried it and ran into a few syntax errors - it kept missing closing braces in PHP for some reason. It seems it might be useful if it could actually try to execute the code, or somehow check for syntax errors/unimplemented functions before accepting the response from the LLM. reply danenania 11 hours agoparentThanks! Was this on cloud or self-hosted? If cloud and you created an account, feel free to ping me on Discord (https://discord.gg/plandex-ai) or by email (dane@plandex.ai) and let me know your account email so I can investigate. If you have an anonymous trial account on cloud, please still ping me--I can track it down based on file names. There is definitely some work to do in ironing out these kinds of edge cases. \"It seems it might be useful if it could actually try to execute the code, or somehow check for syntax errors/unimplemented functions before accepting the response from the LLM.\" Indeed, I do have some ideas on how to add this. reply jayloofah 17 hours agoprevWhat is the cost of planning and working through, let's say, a manageable issue in a repo? Does it make sense to use 3.5/Sonnet or some lower cost endpoint for these tasks? reply danenania 17 hours agoparentIt's hard to put a precise number on it because it depends on exactly how much context is loaded, how many model responses the task needs to finish, and how much iteration you need to do in order to get the results you're looking for. That said, you can do quite a meaty task for well under $1. If you're using it heavily it can start to add up over time, so you'd just need to weigh that cost against how you value your time I suppose. In the future I do hope to incorporate fine tuned models that should bring the cost down, as well as other model options like I mentioned in the post. You can try different models and model settings with `plandex set-model` and see how you go. But in my experience gpt-4 is really the minimum bar for getting usable results. reply ukuina 9 hours agoprevCongrats! Looks great, and I can't wait to try it. Do you support AzureOpenAI with custom endpoints? Are any special settings necessary to disable telemetry or non-core network requests? reply danenania 9 hours agoparentThanks! It doesn't yet support custom endpoints, but it will soon. I'd recommend either joining the Discord (https://discord.gg/plandex-ai) or watching the repo for updates if you want to find out when this gets released. If you self-host the server, there is no telemetry and no data is sent anywhere except to your self-hosted server and OpenAI. reply lprubin 18 hours agoprevLooks interesting. Can you go into more detail about why you like this better for large/complex tasks compared to GH Copilot? reply Cieric 18 hours agoparentNot the author, but I'm in a discord with him, I believe the main selling point here is that it allows you to manage your updates and conversations in a branching pattern that's saved. So if you can't get the AI to do something you can always revert to a prior state and try a different method. Also it doesn't work on a \"small view of the world\" like Copilot from when I was using it could only insert code around your cursor (I understand that copilot pulls in a lot of context from all the files you have open, but the area it can modify is really small). This can add/remove/update code in multiple files at once. But it'll also just show you a diff first before it applies and you can select some or all of the changes made. reply danenania 13 hours agorootparentYes, couldn't have said it better myself! reply brap 13 hours agoprevThis seems very interesting, but I think the interface choice is not good. There would have been much less friction if this was purely a GitHub/GitLab/etc bot. reply vertis 13 hours agoparentI disagree, having used Sweep extensively, I've found the GitHub Issue -> PR flow to be incredibly clunky with a lack of ability to see what is happening and what has gone wrong. reply danenania 13 hours agoparentprevI see where you're coming from and I do plan to add a web UI and plugin/integration options in the future. I personally wanted something with a tighter feedback loop that felt more akin to git. I also thought that simplifying the UI side would help me stay focused on getting the data structures and basic mechanics right in the initial version. But now that the core functionality is in place, I think it will work well as a base for additional frontends. reply ENGNR 13 hours agorootparentI haven't tried it yet, but I think making it fast iteration and simple initially is the right way to go. Nice one sharing this as open source! reply ahstilde 12 hours agoprevthis looks neat i can't wait to try it out. reply danenania 4 hours agoparentThanks! Let me know how it goes :) reply aksyam 14 hours agoprevLove this. Super excited AI-SWEs, will give it a try. reply danenania 4 hours agoparentAwesome, thank you! reply j45 13 hours agoprevCongrats on the launch. reply danenania 13 hours agoparentThank you! reply BirbSingularity 18 hours agoprev [–] It's pretty annoying that every project like this lately is just a wrapper for OpenAI API calls. reply CharlieDigital 14 hours agoparentOpenAI API is simply a utility. The question is given this utility, how does one find the right use case, structure the correct context, and build the right UX. OP has certainly built something interesting here and added significant value on top of the base utility of the OpenAI API. reply aerhardt 12 hours agoparentprevI’m moving an inordinate amount of data between the ChatGPT browser window and my IDE (a lot through copying and pasting) and this demonstrates two things: 1) ChatGPT is incredibly useful to me and 2) the worflow UX is still terrible. I think there is room for building innovative UXs with OpenAI, and so far what I’ve seen in Jetbrains and VSCode isn’t it… reply danenania 12 hours agorootparentThat was also my experience and thought process. reply danenania 18 hours agoparentprevSupporting more models, including Claude, Gemini, and open source models is definitely at the top of the roadmap. Would that make it less annoying? :) reply codeapprove 16 hours agorootparentNot affiliated with the project but you could use something like OpenRouter to give users a massive list of models to choose from with fairly minimal effort https://openrouter.ai/ reply danenania 16 hours agorootparentThanks, I need to spend some time digging into OpenRouter. The main requirement would be reliable function calling and JSON, since Plandex relies heavily on that. I'm also expecting to need some model-specific prompts, considering how much prompt iteration was needed to get things behaving how I wanted on OpenAI. I've also looked at Together (https://www.together.ai/) for this purpose. Can anyone speak to the differences between OpenRouter and Together? reply kekebo 13 hours agorootparentI can't speak to the differences of Openrouter to Together but the Openrouter endpoint should work as a drop-in replacement for OpenAI api calls after replacing the endpoint url and the value of $OPENAI_API_KEY. The model names may differ to other apis but everything else should work the same. reply danenania 13 hours agorootparentAwesome, looking forward to trying it out. reply j45 13 hours agorootparentprevWould love to hear any feedback from people who have gotten to know OpenRouter, as well as any similar tools. reply npace12 15 hours agorootparentprevI think Mistral-2-Pro would work really well for this, judging by the great results I've had with it on another heavy on tool calling project [1] [1] https://github.com/radareorg/r2ai reply danenania 15 hours agorootparentThanks, I'll give it a try. Plandex's model settings are version-controlled like everything else and play well with branches, so it will be fun to start comparing how all different kinds of models do vs. each other on longer coding tasks using a branch for each one. reply p1esk 13 hours agorootparentFor challenging tasks, I typically get code outputs from all three top models (gpt4, opus, and ultra), and pick the best one. It would be nice if your tool could simply this for me: run all three models and perhaps even facilitate some type of model interaction to produce a better outcome. reply danenania 12 hours agorootparentDefinitely, I'm very interested in doing something along these lines. reply mritchie712 14 hours agorootparentprevhttps://github.com/ollama/ollama reply Aeolun 10 hours agorootparentprevI think OpenAI is still the best of the bunch. Kind of feel like the others are kind of there to make people realize OpenAI works the best. Maybe when Gemini 1.5 is released? reply _ink_ 12 hours agoparentprevBut the open source models have Open AI compatible APIs, so as long as you can set the API endpoint you can use whatever you want. reply bottlepalm 11 hours agoparentprev [–] Every program is a wrapper around a CPU, so annoying. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Plandex is an open-source AI coding engine designed for complex tasks, featuring seamless integration with the OpenAI API, version control, sandboxing, and diff review.",
      "Written in Go, it simplifies file loading, task execution, and prompt submission in the terminal with a quick, one-line install option.",
      "Users can engage by offering feedback, contributing through Discord, or submitting pull requests for the project."
    ],
    "commentSummary": [
      "Plandex is an open-source AI coding engine for complex tasks, emphasizing handling edge cases and failures, written in Go as a CLI tool.",
      "Users can run tasks in the background, manage running tasks, and connect for streaming, with integration with git, version control, sandboxed updates, and a diff review interface.",
      "Discussions explore using Plandex for automated code editing and testing, mention the project Sweep as a language model akin to GH Copilot, and share experiences with various models and tools for coding tasks."
    ],
    "points": 234,
    "commentCount": 77,
    "retryCount": 0,
    "time": 1712157011
  },
  {
    "id": 39916102,
    "title": "White House Proposes Coordinated Lunar Time for Moon Missions",
    "originLink": "https://www.bbc.co.uk/news/science-environment-68722032",
    "originBody": "White House wants Moon to have its own time zone Published 22 hours ago Share Related Topics Artemis IMAGE SOURCE, ISLAM DOGRU/ANADOLU VIA GETTY IMAGES By Esme Stallard Climate and science reporter, BBC News The White House wants US space agency Nasa to develop a new time zone for the Moon - Coordinated Lunar Time (LTC). Because of the different gravitational field strength on the Moon, time moves quicker there relative to Earth - 58.7 microseconds every day. This might not seem like much, but it can have a significant impact when trying to synchronise spacecraft. The US government hopes the new time will help keep national and private efforts to reach the moon co-ordinated. Prof Catherine Heymans, Scotland's Astronomer Royal, told BBC Radio 4's Today programme: \"This fundamental theory of gravity in our Universe has an important consequence that time runs differently in different places in the Universe. \"The gravity on the Moon is slightly weaker and the clocks run differently.\" Time is currently measured on Earth by hundreds of atomic clocks stationed around our planet which measure the changing energy state of atoms to record time to the nanosecond. If they were placed on the Moon, over 50 years they would be running one second faster. \"An atomic clock on the Moon will tick at a different rate than a clock on Earth,\" said Kevin Coggins, Nasa's top communications and navigation official. \"It makes sense that when you go to another body, like the Moon or Mars, that each one gets its own heartbeat,\" he said. IMAGE SOURCE, BETTMANN/GETTY IMAGES Image caption, An early atomic clock \"maser\", in the mid-1950s But Nasa is not the only one trying to make lunar time a reality. The European Space Agency has also been developing a new time system for a while. There will need to be agreement between countries and a centralised co-ordinating body - currently this is done by the International Bureau of Weights and Measures for time on Earth. At the moment on the International Space Station, Coordinated Universal Time is used because it remains low in orbit. Another element that countries will have to agree on is where the new time frame begins from and to where it extends. How will we get humans back to the Moon? The US wants LTC to be ready by 2026 in time for its manned mission to the Moon. Artemis-3 will be the first mission to return to the Moon's surface since Apollo 17 in 1972. It is set to land at the lunar south pole, which is thought to hold vast stores of water-ice in craters that never see sunlight. Locating and directing this mission requires extreme precision down to the nanosecond, errors in navigation which could risk spacecraft entering the wrong orbits. But Artemis-3 is also one of numerous planned national missions to the Moon as well as private endeavours. If time is not co-ordinated between them it could lead to challenges in sending data and communication between spacecraft, satellites and Earth. Related Topics Artemis Nasa The Moon Space exploration More on this story Nasa delays crewed return to the Moon's surface Published 9 January Tom Hanks predicts first baby born on the Moon Published 24 September 2023 Around the BBC BBC Future",
    "commentLink": "https://news.ycombinator.com/item?id=39916102",
    "commentBody": "White House wants Moon to have its own time zone, Coordinated Lunar Time (CLT) (bbc.co.uk)211 points by rwmj 22 hours agohidepastfavorite238 comments KboPAacDA3 19 hours agoThe key reason is the Coordinated part. By letting the Moon handle it's own timekeeping, it reduces the hassle of getting regular updates from Earth clocks. Earth-based atomic clocks will continue to measure its own precise second lengths and take measurements of quasars to get accurate wall-clock time. Moon-based atomic clocks can have their own separate network of atomic clocks and measure the same quasars to get the same wall-clock time. reply ivan_gammel 15 hours agoparentMaybe not just that. We generally need to establish the time of events on another celestial body and process that time locally. Having separate clocks is trivial, but recording correctly sequence of events that happened a few light-seconds away is interesting, when taking all effects into account. Should we record the local time of event? The observer time (=local time+distance+gravitational effects)? reply jenny91 18 hours agoparentprevDo we have any atomic clocks on the moon at the moment? reply mikeyouse 18 hours agorootparentLooks like \"no\" but a few years ago they launched / tested the \"deep space atomic clock\" which seems like it'll be the basis for future space-based clocks. https://en.wikipedia.org/wiki/Deep_Space_Atomic_Clock reply zokier 14 hours agorootparentprevLN-1 that landed recently might have an atomic clock onboard. But the details are scarce so I'm not completely sure. reply kspacewalk2 18 hours agoparentprevStep 1 of N for Lunar Independence. Free Luna! reply euroderf 15 hours agorootparentCertainly lunar clocks should feature on the first series of lunar postage stamps. reply willvarfar 20 hours agoprevThe U in UTC stands for Universal. If we can't use UTC for whole universe, or even for the nearby solar system, should we rename it Coordinated Global or Earth Time (GTC or ETC) instead? reply andrewla 17 hours agoparentWhat you are describing is not just impractical, it is actually impossible. Someone linked to \"falsehoods programmers believe about time\" but I think maybe \"falsehoods non-physicists believe about time\" might be more apt. There is no such thing as a universal frame of reference is the somewhat depressing outcome of Einstein's theories. reply Out_of_Characte 16 hours agorootparentI dont think the GP is asking for a universal reference frame in the same way physics describes it. I think he's asking for a universal programmatic reference frame to describe the offsets that sattelites and lunar stations have. Even though time runs faster on sattelites, and the signal delay differs based on position. Because the vectors of sattelites and the moon is constant, as well as the moons gravity, you could calculate what those offsets should be and synchronise clocks anyway. To put it another way, you normalise the clock drift and the delay, whats left over is the same clock drift you'll need to account for when you have a quartz clock in your computer and a time server that runs an atomic clock. The point being, clock drift can become a huge issue unless you define a specification that allows any equipment to sync up with any other clock in Time And Relative Dimensions In Space. reply jzemeocala 12 hours agorootparentNice Dr. WHO reference reply willvarfar 3 hours agorootparentprev(a bit late, but anyway, ... I wasn't suggesting we should have a true universal time, I was suggesting that we rename the mislabelled universal coordinated time that we already have to something else... :) ) reply Retric 16 hours agorootparentprevYou can select a rest frame based on the velocity of all observable galaxies adjusted for the expansion of the universe and the impact of local sources of gravity. As far as we can tell any observer would come to similar conclusions about that reference frame would be. It’s just not a particularly useful reference. reply nyrikki 16 hours agorootparentThere is no objective concept of 'now' even at distance scales of Alpha Centauri with reasonable curvature and speeds. If you want to stick to special relativity, the Andromeda paradox will show the problem with simultaneity with distance, and Einstein's train thought experiment about trains will show the problem of simultaneity with high speed. The order of events, timing of events, and even causal structure of events are not fixed. Even the Conventional Celestial Reference System is only quasi-inertial, only inertial like over small periods of time. The incredible smoothness of the earth approximating a co-rotating frame with a high error tolerance is the only reason we can treat time as absolute on the surface of the Earth. But to derive the new definition of the Kilogram requires lots of corrections as do modern atomic clocks. Don't confuse the ability to choose a reference frame with the Newtonian concept of universal reference frames. When making measurements SR is a framework that allows you to pick any reference frame that is convenient, but the speed of causality(light) prevents the existence of a universal reference frame. reply Retric 14 hours agorootparent> The order of events, timing of events, and even causal structure of events are not fixed. You can calculate observed order of events for any reference frame from any reference frame. Thus they are fixed once everyone agrees to using a given reference frame. It’s not much more complicated than seismic detectors recording the seismic waves from an earthquake at different times based on their location. Sure detector X see earthquakes in order A, B, C and detector Y sees them in order C, B, A they would both agree that detector Z should see them in order A, C, B based on it’s location, the timing of the events, and wave propitiation speeds. It only seems unintuitive because you aren’t used to accounting for such issues. reply nyrikki 12 hours agorootparentIn SR, you have the spacetime interval, it is more complex in GR. For constant velocity motion in flat spacetime: Δτ^2Δs^2 = c^2Δt^2-Δx^2-Δy^2-Δz^2 The spacetime interval is invariant, but notice it is also a scaler that combines space and time, and note that time is multiplied by c^2. Also note Δt is the time as observed but an inertial observer that is present at both events. You have three types of intervals here represented by one scaler. Timelike: Δτ^2 > 0 Spacelike: Δτ^2The faster you 'move' through space Therein lies the rub. To \"move\" proves nothing about who is \"faster\" because there is no absolute frame of reference. You may think that I am moving at 0.8c, but maybe it is just me slowing down to a standstill while you are still receding at 0.8c. This might be a valid interpretation of your observation of me \"moving at 0.8c\" if... (and only if...) there were an absolute frame of reference. But there ain't. AIUI it is the flavors of acceleration - including accelerating, decelerating, and gravity - that tinker with time. Which is why I still can't quite wrap my mind around the Twin Paradox, because it is usually explained in terms of speed, not periods of acceleration. reply lupire 9 hours agorootparentYou just explained it yourself. Time slows down for people who are accelerating (not in freefall). reply user_7832 18 hours agorootparentprevWhat is the frame of reference, or is there even one, in this case? If you have one spacecraft moving in one direction at 0.5c flying by Earth, is the craft at 0.5c and experiencing \"slower\" time, or is Earth experiencing slower time? Or do they both experience the same thing as they're not accelerating wrt each other? reply aidenn0 18 hours agorootparentIf they just pass by each other without changing speeds, each sees the other as experiencing slower time. For the case where one leaves and comes back, see the Twin paradox[1] which is resolved by the simple fact that to change speeds one must accelerate (or that the one who returns must have experienced at a minimum two different frames of references; one to leave and one to return). 1: https://en.wikipedia.org/wiki/Twin_paradox reply kijin 17 hours agorootparentprevThere is no independent frame of reference by which we can tell whose time is slower. From the spacecraft's frame of reference, Earth's time is slower. From Earth's frame of reference, the spacecraft's time is slower. Both are right. When we say that time is slower or faster on a spacecraft, the Moon, or an exoplanet of Christopher Nolan's, we are implicitly prefixing the statement with \"From Earth's frame of reference...\" reply bumby 17 hours agorootparentprevCan you explain why the momentum vector must be constant in magnitude? reply notfish 17 hours agorootparentThere isn’t really a “why”, other than the need to match observed reality. We know from observations that light moves at a constant speed, even when the observer is moving near the speed of light, and we know that this observation is true regardless of your frame of reference. In order for physics to remain consistent while accounting for the constant speed of light, other things need to flex between the two reference frames: namely, time (time dilation) and length (Lorentz contraction). reply bumby 14 hours agorootparentIn other words, \"to make the math work out\". That's kinda what I was poking at, trying to understand if that is some fundamental truth or if it is the result of some underlying mechanism that is more fundamental. reply adolph 16 hours agorootparentprev> light moves at a constant speed While in the same medium, right? The speed of light is a universal constant in a vacuum, like the vacuum of space. However, light can* slow down slightly when it passes through an absorbing medium, like water (225,000 kilometers per second = 140,000 miles per second) or glass (200,000 kilometers per second = 124,000 miles per second).* https://www.space.com/15830-light-speed.html#section-speed-o... reply lxgr 15 hours agorootparentLight propagation in a medium is a quite different thing from light in vacuum. For example, the speed of light in a medium is not the \"speed limit\" of things in the same medium, and particles in it can actually move faster than light: https://en.wikipedia.org/wiki/Cherenkov_radiation In other words, the speed of light in vacuum plays a special role in both a vacuum and a medium. reply idontpost 17 hours agorootparentprevNo, and if you could you would win a Nobel prize. But a century of experimental and observational data proves that it is. At this point it's generally just taken as a fact that the speed of light is constant for all observers. The explanation given above falls out as a direct mathematical consequence. reply irobeth 10 hours agorootparentto hear Lenny Susskind say it, \"light moves at the speed of light\" reply aidenn0 18 hours agorootparentprevAll of those things fall out from the speed of light being independent of the speed you are moving at (i.e. regardless of how fast or slow or what direction you are going, you will always get the same answer when you measure the speed of light in a vacuum). The easiest one to explain is probably the most mind-bending: wheter or not an event is \"simultaneous\" depends on the frame of reference. You are sitting in a spaceship that has a large empty cargo bay. There is a lightbulb in the exact center of the cargo bay. If you turn the bulb on, it will hit the front and the rear of the cargo bay at the exact same time. This is true regardless of the speed the spaceship is traveling at (because the speed of light will always be measured as the same value). Now think of what happens from the frame of reference of someone on a planet as the spaceship is passing by. Since the spaceship is moving forwards, and the light moves in both directions at the same speed, the light will hit the back of the cargo bay slightly before the light hits the front of the cargo bay. The faster the spaceship is moving, the bigger this difference. reply johnla 19 hours agorootparentprevIt will forever boggle me but the way I sort of rationalize it is that speed basically borrows energy from time. Something like that. So the faster you go, the slower the time goes. So if you go the speed of light, you're taking so much time away that when you make a roundtrip, you're like 50 years into the future. That's a totally wrong explanation but it helps me sort out the effects and what to expect. If you fly quickly around the Earth, your watch is a tiny bit different than the stationary clock. reply magicalhippo 18 hours agorootparentAFAIK one can also consider the 4D velocity vector to have a constant lenght. Thus the faster you move in the spatial dimensions, the slower you move in the time dimension to maintain the constant length. reply bumby 17 hours agorootparentWhy must the velocity vector have a constant length? reply magicalhippo 15 hours agorootparentAs the sibling comment alludes to, it's otherwise they wouldn't behave according to what we measure. For a deeper \"why\", see Feynman[1]. edit: it's related to how spacetime with a finite speed of light is represented mathematically. There's some discussion here[2] which might shed some light. [1]: https://fs.blog/richard-feynman-on-why-questions/ [2]: https://en.wikipedia.org/wiki/Imaginary_time#In_cosmology reply CydeWeys 16 hours agorootparentprevNow that's the billion dollar question, isn't it. But it's true. reply numpad0 15 hours agorootparentprevSo to preoccupy your mind with simulation hypothesis and inter-node bandwidth estimation for the cluster we’re on. edit: wait, if the Vector4.magnitude == 1, are dense objects actually just fast/hot? reply bloopernova 17 hours agorootparentprevThat's a helpful explanation, thank you. reply andrewla 17 hours agorootparentprev> speed basically borrows energy from time This is close to the main intuitions of special relativity as a geometric theory. I would phrase is more as \"speed borrows space from time\". In general relativity (and special relativity if you look at it geometrically) every reference frame moves at 1 second / second in their own coordinates, but an observer in a different frame will see you move in their coordinates -- the 4-vector of your time / position keeps the same magnitude, so since you are moving faster in space, your time is moving slower. reply Y_Y 18 hours agorootparentprev> speed basically borrows energy from time This sounds cool, but I don't think it has explanatory value. It's true that moving close to the speed of light will mess with your intuitions about the passage of time and give you things like the Twin Paradox. But there's nothing special about time there, it also makes things shorter by Fitzgerald contraction. You can't be in the future, for example. The point of relativity is the equivalence of reference frames, it's not like the space traveller's clock is wrong and the stayhome's clock is correct, it's that humans who never travel at speeds close to the speed of light don't expect that two good clocks could ever disagree. reply everforward 14 hours agorootparentprevThe underlying reason is causality. Causality is the concept that an event can only be caused by another event in its past (not its future), and events can only impact their own future. Causality cannot propagate faster than the speed of light, ergo for a given event you can express the area of space that could have caused that event as a function of time (it's a circle with radius equal to the amount of time in the past you're looking times the speed of light; e.g. 10s before an event, that event could have been caused by another event anywhere within 10s*the speed of light). If time is constant, objects moving at a significant fraction of the speed of light break causality. They're moving fast enough that causality propagating the opposite direction of their velocity (things happening in the future) reach them faster than they should, and causality propagating in the same direction as them reaches them slower than it should (because they're moving away from it at a significant portion of the speed causality is approaching them at). E.g. lets say there are 2 massive celestial bodies A and B. A is not moving at all, B is moving at 50% the speed of light. Let's say they pass close enough to gravitationally interact, but are half a light-year away from each other. Once they pass each other, causality from B will propagate to A at the speed of light, like normal. Causality seems fine. But causality will propagate from A to B much, much slower because of the relative velocity. If B is 1 light-year away, it would actually take something like 1.3 years for causality to reach it. In other words, A is within B's causality radius (B can cause effects on A), but B is not within A's causality radius (or rather, it is when the event happens, but it won't be there by the time causality gets there). That's a problem for something like gravity. B's gravity can influence A, but A's gravity can't be the cause of events on B, because of the speed of causality. Thus the only valid event is B's gravity on A, accelerating A without decelerating B, meaning we would have actually created energy (at least until causality catches up). To maintain causality, and preservation of energy, something has to happen to B such that A interacts with B at the same time B interacts with A. The answer is to make movement through time and movement through space inversely correlated. If B is moving fast enough that light takes 30% longer to get there, B's time has to slow down by the same amount so that causality can be simultaneous and not create energy. Causality essentially requires movement through space and movement through time to add up to some constant. As movement through space increases, movement through time decreases and vice versa. It's basically a formula like (current speed/speed of light) + rate of passage of time = 1. That's the underpinnings of the idea that FTL travel will allow time travel. If (current speed/speed of light) is greater than 1, the passage of time has to be negative or flowing backwards to maintain causality. I.e. you are moving so quickly that you can catch up to and interact with causality propagating through the universe. Somewhere out on the edges of the universe, the causality of the meteor that killed the dinos is still propagating outwards. Perhaps if we could move fast enough to reach that wave of causality, we would be able to interact with it somehow. It's all theoretical, and hand-wavy, and trippy, but interesting in concept. reply akdev1l 19 hours agorootparentprevthis phenomenon is called “time dilation” you can search for it. Not trying to be a dick, just not qualified to explain this myself reply nerdjon 19 hours agorootparentHonestly that helps, just having a name to look for instead of somehow making a google search out of this will help. reply 7thaccount 19 hours agorootparentThere is also time dilation from motion through space (motion through space + motion through time is equal to the speed of light, so as you speed up, your motion through time slows down) as well as time dilation due to gravity (like when near a black hole I think). I'm also not a physicist, but read some popsci books in my youth like \"A brief history of time\" from Stephen Hawking (it's written for normal people). That's probably the best book you'd want. He later wrote a more modernized version called \"a briefer history of time\" as well. They both cover time dilation from a high level iirc. Of course, to truly understand you'd need to learn the math. I'm looking forward to doing that during retirement in a few decades. reply c54 19 hours agorootparentprevFloatHeadPhysics does a really good job of explaining these kinds of things https://youtu.be/TJmgKdc7H34 reply trey-jones 18 hours agorootparentThis is actually the guy that finally helped me break through into understanding (at least a little what's going on). Specifically about how the movement through space lengthens distances between objects (including atoms) and causes information (including light) to travel further, along the hypotenuse of a triangle, \"in order to do stuff\". \"Doing stuff\" includes observing things because of the light travel distance, but also biological processes, which involve eg. electrons moving from one atom to another. I didn't watched the linked video here (more relevant perhaps), but this was the one for me: https://youtu.be/Vitf8YaVXhc reply ldargin 17 hours agorootparentprevIt's a bit easy to understand that in the context of gravity, because gravity bends light down. If you shine a flashlight on Earth, that light is in free-fall, bending down. Time follows the same path, because the speed of light is fixed. reply corndoge 19 hours agorootparentprevhttps://en.wikipedia.org/wiki/Time_dilation reply belter 19 hours agorootparentprevMaybe this one: \"Does the Past Still Exist?\" - https://youtu.be/GwzN5YwMzv0 reply nerdjon 19 hours agorootparentI will watch that tonight, thank you. reply dmoy 19 hours agorootparentprev> I have seen references to this but I am constantly confused by how this is actually a thing. Shit, I studied this for a whole semster in physics back in the day, and even back then I was constantly confused by it. It is some unnaturally (to humans) weird stuff. reply hoorayimhelping 18 hours agorootparentprevA Brief History of Time deals with this subject very well. It's a very easy and quick read, meant for a wide audience, not a technical one. It's readable in an afternoon. The concept that time is relative to the observer is where the theory of relativity gets its name from. reply TaylorAlexander 17 hours agorootparentprevHighly recommend the best-selling book A Brief History of Time! reply somenameforme 18 hours agorootparentprevThe concept of time dilation in general is actually pretty easy to explain, with the use of a relativistic starship! Imagine we create a ship that could constantly accelerate at 1g per second. What happens if you're on that ship, and start to approach the speed of light (relative to an observe back on Earth)? Well, some quite weird stuff - but you would not actually slow down! The speed of light is not a speed limit, like most people think of it. A human could easily travel billions of light years in a single lifetime. But it is true that nothing can ever be perceived as traveling faster than the speed of light. So how you can you travel billions of light years in a few decades, yet never be perceived as going faster than the speed of light - one light year per year? Simple - the universe, like a simulation filled with spaghetti code, starts to cheat, and changes the rate at which things start to move through time. An observer back on Earth would see your ship start to accelerate towards the speed of light, but then hit an insurmountable asymptote just before it. So if you traveled a million light years, they would see your trip taking a million years. But by contrast, only about 26 years would pass for you. So if you traveled a million light years out in our 1g accelerating ship, and then a million light years back, it would take you 52 years, but 2 million years would have passed on Earth. There's a calculator for such trips here. [1] This whole effect is called time dilation. Gravitational time dilation is just a special case of general time dilation, and is essentially the time dilation factor driven by the velocity needed to escape the gravitational well created by a body. So - more massive objects result in greater time dilation. It leads to interesting things like the core of the Earth actually being younger than its surface! --- A clear way this can be seen in real life (and to also emphasize this is in no way whatsoever an optical illusion) is with particle accelerators. Many emergent or unstable particles tend to decay rapidly. Yet when we accelerate them to speeds near light, we end up being able to observe them for orders of magnitude longer than their decay when at rest. It's because of time dilation. From an at rest observer, time starts to move more slowly for something moving rapidly. All of this should also be taken with a general 'for illustrative purposes' asterisk. I'm leaving out lots of things, like how as you approached the speed of light you'd start to experience length contraction. It's essentially another way that the universe cheats to ensure that everybody always perceives the speed of light as a constant. --- This has amusing and interesting social implications for Earth as well, if and when we become able of developing such technology - the rich and powerful, seeking immortality, will undoubtedly seek to thrust themselves into the future. Great setting for some sort of a sci-fi series, not only for those on Earth, but also for those setting out into a future that may not be exactly what they were hoping for. [1] - https://spacetravel.simhub.online/ reply AnonHP 17 hours agorootparent> A human could easily travel billions of light years in a single lifetime. Serious questions follow. How and using what technology? Why are we still struggling to go to the moon or Mars by taking weeks or months? reply somenameforme 16 hours agorootparentAccelerating 1g constantly is much more difficult than it sounds. You'd need a practically infinite fuel source or some way to generate fuel while traveling. That's why the EM-drive was so tantalizing. If we could [near] perpetually generate even the smallest amount of thrust, the implications would be unimaginable. But for now this seems impossible. reply Traubenfuchs 18 hours agorootparentprevIt‘s pretty simple. In places with high gravity, time goes slower. In fast moving containers (car, plane, spaceship), mass+gravity are also increased. This is the reason why shining a light forward on a moving train doesn‘t make the photons the speed of light + the speed of the train. The speed of the light emitted from the flashlight is slowed down by the mass/gravitation increase through the movement of the train. This way, travelling to the future is pretty easy by the way. Just travel in a vehicle almost at the speed of light and the outside worlds time will move faster (relatively), since you are slowed down. reply chgs 19 hours agorootparentprevThe length of a second varies at different points on earth too, and varies far more when in low earth orbit. UTC is still good enough though, even with the number of seconds in a year varying by a non-predictable amount. reply torstenvl 13 hours agorootparentprev> on Earth . . . time runs slightly slower due to the gravitational time dilation effect When people say \"time runs slower\" in a location, that means that time spent there subjectively feels longer than it really is in the outside world. \"Time runs slower at the cabin, a weekend there makes the city and office life seem like a long-ago time and place.\" That's the opposite of what happens in gravitational time dilation. You spend a subjective two hours in Gargantua and decades pass on the outside. reply dotancohen 18 hours agorootparentprevThen, as GP implied, the U in UTC is a misnomer. Considering replacing it with a better name is logical considering the development of technology and our improving ability to go out and explore. reply philistine 17 hours agorootparentprevThat's why if we ever make a Moon time, what should change is the length of the second, not the whole makeup of the calendar otherwise. reply lsllc 16 hours agorootparentThe definition of a second is as follows [0]: The second is defined by taking the fixed numerical value of the caesium frequency ∆ν, the unperturbed ground-state hyperfine transition frequency of the caesium 133 atom, to be 9 192 631 770 when expressed in the unit Hz, which is equal to s−1 But with the Moon's lower gravity, time flows very slightly faster (a moon-second is 0.99999999999848 of an earth second according to a sibling post). I'm no physicist, but my assumption is that the SI definition of a second would still be true on the moon (at least relative to the moon's reference frame). So it would make sense to me to leave a second defined as a SI second, regardless of your reference frame because that's how humans, science & technology measure time passing. I think the bigger calendar problem with space travel is the differences in the length of a day (or sol), as we know it's 24hrs on earth, Mars is 24h 37m (not sure about the moon since it's tidally locked its ~27x24hrs). The drift of days between Mars and Earth is going to be far more noticeable than a tiny fraction of a second due to relativity. [0] https://www.npl.co.uk/si-units/second reply pavon 16 hours agorootparentprevThat might be convenient for the purpose of timekeeping, but then you would also have to redefine all sorts of physical constants, which could be problematic. Oops, our lander crashed because we were doing calculations using Moon seconds, but the gravitational constant G was in Earth seconds. Okay, the difference probably isn't enough to matter for that case, but still any physics calculations where that precision is needed (say interferometry?) you'd have to keep track of which measurement system you are using. In some ways it would be worse than US -> SI because they would be close enough that it wouldn't be immediately obvious you made a mistake. I'd much rather have SI be universal than our calendar. reply philistine 15 hours agorootparentThe space shuttle had a four-second window to start its entry burn to reach the runway back on Earth. I don't think the discrepancy in a second on Earth versus the Moon is going to mean the difference between a crash or a success. No space-faring vessel had any tolerance as low as .0000001 reply vnchr 17 hours agorootparentprevThat’s great until we occupy a smaller moon and need to reduce the length of a second for that moon’s time dilation. reply ta1243 20 hours agoparentprevYou can't use any time for observers in different reference frames as the length of a second will be different. UTC is generally fine as far as normal time goes as we're all in roughly the same reference frame on Earth, and indeed anywhere in the Solar System. It would still drift by noticeable milliseconds per year, but we can cope with leap seconds just fine with UTC and that's a larger drift. You'd probably want to avoid leap seconds and use TAI though, but we should avoid leap seconds anyway IMO. reply greggsy 20 hours agoparentprevGTC is easily confused with Galactic Time, which will likely cause mass havoc a rewrites in dead languages like Imperial C and Scala sometime after the year 48,000AD. reply randomdata 15 hours agorootparentNot to mention that universal, by definition, refers to earth already. Universal does not relate to universe any more than carpet relates to car. reply kijin 17 hours agorootparentprevA true visionary must plan for job security for millennia to come. Next on the todo list: code up a Kwisatz Haderach to do your all your data modeling work while you slack off. On a more serious note, Galactic Time probably won't be useful because relativity introduces too much irreconcilable error at that scale. We'll be back in the early days of the railroad when every station had their own clock, adjusted to local noon. reply zokier 20 hours agoparentprevTerrestrial Time is a thing: https://en.wikipedia.org/wiki/Terrestrial_Time > TT is indirectly the basis of UTC, via International Atomic Time (TAI). Nobody is going to bother renaming UTC at this point, misnomer or not. Besides, if they stop adding leap seconds to UTC (as is planned) the whole point of UTC goes away anyways. reply belter 20 hours agoparentprev\"Falsehoods Programmers believe about Time\" - https://hn.algolia.com/?q=Falsehoods+Programmers+believe+abo... reply hbrav 18 hours agoparentprevI'm afraid it becomes even worse when you learn that the concept of 'simultaneous' is ambiguous. reply GuB-42 16 hours agoparentprevWe can use (Earth based) UTC on the moon, or on the whole universe for that matter. The problem is that on the moon, due to relativistic effects, a UTC second is not really a second, it is slightly longer, and that makes it inconvenient for conducting experiments on the moon. So for moon-based operations, moon time will be used, it can then be converted to UTC for \"universal\" operations. It is not the first time we use different clocks. In fact, we already have plenty: UT0, UT1, TAI, etc... That's because the Earth rotation is not that great at timekeeping, atomic clocks do better, but we still want our days to be aligned with the sun, so depending on whether you are more interested in the Earth rotation or in precisely counting seconds, you will use different clocks, with UTC being a compromise you can always convert to. GPS time is one such example, it is a few seconds off UTC because it doesn't account for leap seconds. All that is not even accounting for local (earth) time zones. reply ant6n 15 hours agorootparentAren’t seconds different based on latitude, as well as elevation? Should we perhaps have many more time zones than the 30 or so we have now, all requiring occasional leap-ms to sync up to UTC? reply gadders 17 hours agoparentprevGo back to GMT. Embrace tradition. reply foxyv 15 hours agoparentprevTime dilation is a bit of a PITA. https://en.wikipedia.org/wiki/Gravitational_time_dilation#:~.... reply randomdata 15 hours agoparentprev> should we rename it Coordinated Global or Earth Time (GTC or ETC) instead? No, why? Universal is already defined as referring to earth (the world). From the Oxford Languages dictionary: u·ni·ver·sal – of, affecting, or done by all people or things in the world reply shawncampbell 17 hours agoparentprevETC. Time, etcetera. I like that. There's something poetic about it. reply faitswulff 20 hours agoprev> But Nasa is not the only one trying to make lunar time a reality. The European Space Agency has also been developing a new time system for a while. Great, lunar time hasn’t even been established and we already have two time zones. reply kayge 17 hours agoparentJust wait until one of them pushes for Moonlight Savings Time... reply SketchySeaBeast 20 hours agoprevIs \"time zone\" even the correct term when we're looking at a different frame of reference? \"Welcome to Hawaii, each day here is 23.4 hours.\" I thought the article would be about how the lunar day is 29.5 Terran days. reply itishappy 20 hours agoparentFascinating question! I wonder if GPS satellites use their own special \"time zones\" too? Apparently so! https://www.ipses.com/eng/in-depth-analysis/standard-of-time... reply onion2k 19 hours agorootparentThat page is a bit out of date. The last GPS week reset was in April 2019, when it rolled over to 0 again. reply lxgr 18 hours agorootparentThe GPS week is a calendar issue and not really relevant to this \"time zone\" (more accurately: time scale?) discussion, though. It's also a non-issue for most applications since it's usually possible to figure out the current year with a precision of at least 9.8 years. reply philip1209 20 hours agoparentprevIt's a contraction rather than an offset. An interesting question is what the frame of reference for this time is - if the Moon's orbit changes significantly and the contraction factor changes, then this timezone shouldn't be adjusted - almost \"de-pegging\" it from Earth's time, right? reply ISL 20 hours agorootparentIt isn't just the speed/shape of the orbit. The gravitational potential difference is a major factor. reply MasterYoda 14 hours agoprevFun fact!? TIL: UTC = Coordinated Universal Time but the abbreviation is not CUT that would be \"logical\", but UTC. \"It came about as a compromise between English and French speakers. - Coordinated Universal Time in English would normally be abbreviated CUT. - Temps Universel Coordonné in French would normally be abbreviated TUC. The International Telecommunication Union (ITU) and the International Astronomical Union wished to minimize confusion and designated one single abbreviation for use in all languages. UTC does not favor any particular language. In addition, the advantage of choosing UTC is that it is consistent with the abbreviation for Universal Time, UT\" So the reason it's called UTC is because the French and British couldn't agree, who could have guessed? :) reply gglitch 20 hours agoprevAt some point will ISO-8601 have to be updated to include a field for difference in speed of time? reply withinboredom 17 hours agoparentMy time library considers measuring time from different frames of references: https://github.com/withinboredom/time From the earth frame of reference, the lunar day is different than from a lunar frame of reference. reply gglitch 18 hours agoparentprevMaybe it would be more precise to include a field for something like \"gravity offset\" reply layer8 19 hours agoparentprevThere is no difference in speed of time. Time always passes one second per second. reply achates 19 hours agorootparentNot true, on Fridays after lunch time can sometimes pass as slowly as 3 seconds per second. reply seanhunter 17 hours agorootparentI've been in some 15min standups but actually take 30mins wallclock time, during which interval years pass. reply aurizon 18 hours agorootparentprevI know some bores so good that 100 seconds passes, and I shake my watch ~ 1 second has passed, so there is a subjective localised field around some people.. reply c54 18 hours agorootparentprevTo a local observer yes, but to someone moving at a different speed no! FloatHeadPhysics does a good job explaining some of this https://youtu.be/OpOER8Eec2A reply layer8 18 hours agorootparentNo, time still passes at one second per second for any observer. reply lxgr 18 hours agorootparentOnly if you disregard observers in different frames of reference interacting with each other, which you shouldn't when you need high precision and it comes to projects spanning Earth, Earth orbit, and the Moon. GPS wouldn't work without accounting for relativity, for example. reply kijin 17 hours agorootparentYou're missing the joke. It's still one second per second, only that everyone's second looks different. reply layer8 16 hours agorootparentExactly. reply lxgr 16 hours agorootparentYeah, I guess I don't see what's funny about that statement. Unlike \"sometimes a second is longer than a second\" (not literally true but it makes some sense in the context of relativity), this one just seems like a tautology to me. reply kijin 4 hours agorootparentYeah, it might not be funny, but the tautology draws attention to the fact that there is no privileged frame of reference. In other words, the only thing we can say without qualification is that a second is just a second in the same frame of reference. All other statements must be heavily qualified. Even things like \"A's second is longer than B's\" are only valid in some frames of reference and not others. reply lxgr 18 hours agorootparentprevEinstein would beg to differ! reply icapybara 18 hours agorootparentI think Einstein would agree. Time is relative but you always experience 1 second per second no matter where you are or how fast you’re going. reply lxgr 18 hours agorootparentBut you can observe somebody or something in an accelerating frame having experienced time at a different rate. In the twin paradox thought experiment, one of the twins really has aged slower than the other (or, from their point of view, the entire earth has aged faster than themselves). In that sense, relativity has effects more tangible than distortion of observations across a large distance. reply dudeinjapan 18 hours agorootparentprevEinstein's watch would beg to differ with Einstein! reply notatoad 16 hours agorootparentprevone earth second per earth second, or one lunar second per earth second? reply WillAdams 20 hours agoprevThis was actually discussed in Heinlein's _The Moon is a Harsh Mistress_ --- and dismissed as idiocy, since being in synch with earth for communications was more important, and anyone who needed to would check a table to determine if it was lunar day or night on the surface. reply oaththrowaway 18 hours agoparentI loved that book reply chias 18 hours agoprevThere's a typo in the headline here on HN: the initialism for Coordinated Lunar Time is LCT, not CLT. (this isn't my opinion on whether we'll end up following suit with UTC and so on, the article itself calls it \"LCT\", and \"CLT\" does not appear on the page at all) reply fullstop 14 hours agoparentYes, I searched for the CLT but could not find it. reply benced 16 hours agoprevSo what would the prefix for the IANA code be? Right now, it's continent (Asia/Kolkata, America/Chicago etc). Would it just be \"Space/Moon\"? One can imagine the standard would have to be revised many many times if humans became interplanetary. reply akira2501 15 hours agoparent> So what would the prefix for the IANA code be? I would prefer if IANA/Eggert were not involved on this one. They've already made an absolute mess of the Earth based timezone database. > Would it just be \"Space/Moon\"? More problematically, how would you define it's coordinates and country code, which are fields in the tzdb for each timezone? reply ivan_gammel 16 hours agoparentprevIf we will standardize time on solar system level, we’ll probably have another component of time, rather than a flat list of time zones. E.g. Mars should have its own set of zones. Maybe that means having an identifier of coordinate system, which can be omitted in local use cases. reply joering2 16 hours agoparentprevSolar_System/The_Moon reply simcop2387 16 hours agorootparentI'd suggest Sol/Luna or Sol/Terra/Luna, that way it's easier to do Sol/Jupiter/Europa reply throw0101b 20 hours agoprevSomewhat related: * https://en.wikipedia.org/wiki/Timekeeping_on_Mars reply bradley13 20 hours agoprevWhy? Just use UTC, there is zero reason for anything else. ETA: There is a minor relativistic issue due to the reduced gravity. If the figures I found online are correct, we are talking about 0.02 seconds per year. Surely such a small difference can just be \"smeared\" away periodically? reply thfuran 19 hours agoparent0.02 seconds per year is about 55,000 nanoseconds per day, which is kind of a big deal as far as the high precision CPU timers are concerned. reply AdamH12113 19 hours agorootparentI'm not sure about the larger issues surrounding the lunar time standard, but 55 microseconds per day is about 0.64 ppb. Even good OCXOs drift more than that. A standard crystal oscillator like the kind used in computers drifts by multiple seconds per day. Unless your oscillator costs more than your computer, that kind of accuracy is going to be beneath the noise floor. reply MeImCounting 16 hours agorootparentThe issues arent arising from cellphones and personal computers. Precise time may not be necessary for building web services but it is the basis of precise navigation systems and other scientific endeavors such as astronomy and quantum computing. Since navigation and science will probably be way up on the todo list above twitter I think nanosecond accuracy lunar time is clearly justified. reply thfuran 16 hours agorootparentprevYou're right that CPUs probably weren't the best example. My point was more that there are everyday things that operate at timescales where nanoseconds matter. 55 us per day is larger than the effects of either special or general relativity on GPS, but both need to be accounted for in order for the system to be useful. reply alexdns 20 hours agoparentprevTime dilation - the moon time runs at different pace due to gravity reply gumby 17 hours agoprevSo UTC (\"Coordinated Universal Time\") isn't in fact universal after all? Not even solar-system wide much less galactic. Some marketing department must have been involved. reply timetraveller26 18 hours agoprevOh great, hopefully I will be retired before I have to worry about inter planetary timezone on my django app. reply palmfacehn 20 hours agoprevWould it not be simpler for everyone involved to just use UTC? reply sp332 20 hours agoparentNo, you would have to calibrate the clocks to run slower to match earth seconds when they are in the moon's weaker gravity. Or constantly sync them to an Earth time source with over two seconds latency. reply queuebert 20 hours agorootparentNTP has several methods for dealing with large latency that could possibly be adapted. reply sp332 19 hours agorootparentYup, or you could... not do that. Which would be a lot simpler. reply lxgr 19 hours agorootparentprevLatency is different from a constant frequency offset. reply masswerk 18 hours agorootparentprevBut, using your own frame of reference, you now need to convert to and fro on each cross-access. I don't really see the advantage. Notably, there is no such thing as an ontological second, it's an abstract measurement unit (loosely) bound to the rotation and revolution of the Earth. Moreover, there is no provision for such a cross-reference of basic progression of time in any Date/Time system currently available. Wouldn't be adding a constant coefficient easier? (Notably, we have modified basic constants several times, already.) reply hansvm 20 hours agoparentprevIt would be even simpler to use a standard without leap seconds (or at least just without negative leap seconds). The drift happens slowly enough that nobody will notice the qualitative elements of 8am changing slightly over their lifetime. reply layer8 19 hours agorootparentThe drift is speeding up long-term though. reply hansvm 16 hours agorootparentI'm optimistic we'll have better synchronization mechanisms in the distant future than \"8am == sunrise, +/- a few hours\". That said, interestingly, the quadratic nature of that problem (the earth slowing down progressively faster over time) means that the gap between when we have to care about my solution (because we accumulate too many leap seconds per lifetime) and the leap second solution (because you can only insert a little over 365 per year with the current protocol) is much smaller than the gap between now and when we would have to care about my solution. If you're not too picky, the two solutions \"expire\" around the same time. The max rate of leap second insertion only admits an hour of drift per decade, which is slow enough that I still think most people wouldn't care, and they might like if our time handling is simple enough that we can't repeat the recent xz supply chain issue in hundreds of thousands of lines of increasingly complex datetime nonsense. reply Jeff_Brown 20 hours agoparentprevThat was my thought. Who cares what time it feels like on the moon, if the people making plans and decisions about it are at, say, Cape Canaveral? Once we get humans on the moon, they'll probably want their own time zones -- but in that case this solution still won't be enough, because they'll probably want the time to depend on their lunar longitude, just like it does on Earth. reply TrueGeek 20 hours agorootparentSure, go ahead and ignore what people on the moon feel because Earth is more important. It's all fun and games until people start throwing rocks. reply bluGill 20 hours agorootparentNo human has walked on the moon in my lifetime (last human left December 1972). I'm somewhat against going back because I like to tease old people about this (which isn't a good reason I know, but it is fun). Even if/when humans again walk on the moon it will just be a short trip and they will head back to earth (Likely China, but India is realistic, other countries that could realistically send someone to the moon don't seem very interested in trying) Their missions will be planed from Earth, and they will otherwise want to match earth time so they can contact people back on Earth. If in the distance future we put colony on the moon, the people will drift apart from earth and so I expect the kids born on the moon will not care about earth time so much and may eventually demand a switch to moon time. OTOH, I think what they really want out of moon time (light cycles are too long to be useful humans) probably isn't what we think today. reply williamcotton 19 hours agorootparentIn this distant future I imagine that getting to Earth from the Moon is pretty easy and very common. I doubt people move off of an Earth-centric time. Of course this would be very different for planets or solar systems far from Earth. reply bluGill 15 hours agorootparentI would think it is more like going from North America to Australia. Possible, you are likely to know someone who did - but it isn't common because between cost and the travel time is just isn't worth it. reply layer8 19 hours agorootparentprevEarth will win because it has more rocks. Although I guess an equilibrium will necessarily be reached after a sufficient amount of rock-throwing. reply aftbit 18 hours agorootparentWho can throw rocks better, a tribe with a giant pile of boulders at the bottom of a valley, or a single man with a pile of fist-sized stones at the top of a mountain? reply layer8 18 hours agorootparentMaybe the one with more rocks to convert into energy via rock fusion? reply TrueGeek 18 hours agorootparentprevNo, no, you forget about the gravity well. Read up on your Heinlein! reply aftbit 18 hours agorootparentOr the Expanse series. Although there the Moon is firmly in Earth's political control, while the rock-throwers are further out in the asteroid belt. reply a-priori 18 hours agorootparentprevThe Moon will win because it has the high ground. reply tailspin2019 18 hours agorootparentprevI’ll just leave this here: https://youtu.be/N_DjsmkD1fw reply black6 19 hours agorootparentprevHarsh reply zokier 20 hours agorootparentprevIts not about how anyone feels, but about making sure that we have accurate time reference on moon for stuff like positioning and networking, the same way GPS is used on Earth and also relies on precision timekeeping. reply prpl 20 hours agoparentprevTAI feels so left out in its moment to shine. reply ISL 20 hours agorootparentTAI doesn't help. Observers on Earth and the Moon will forever disagree about the other's duration of a second (unless they account for the relativity corrections that make them appear to be different). reply yellow_lead 20 hours agoprevFuture bug report: App fails to run with clock set to Coordinated Lunar Time reply hackeraccount 17 hours agoparentNever wanted a bug reported so bad. reply tgsovlerkhgsel 18 hours agoprev> over 50 years they would be running one second faster. That's even less than leap seconds on earth, so for practical purposes, I suspect the two will be kept in sync somehow. reply elzbardico 20 hours agoprevWouldn't be easier to just calibrate the clocks and use UTC? reply ISL 19 hours agoparentTo do so is to redefine the second on the moon. That redefines the Watt, the Joule, the Newton, the Ampere, the Pascal, the Becquerel, etc. It even makes a voltmeter read incorrectly. Moon-meters would be different from Earth-meters. Careful. reply adrian_b 15 hours agorootparentIt would have been easy to avoid the redefinitions of all other units by defining the second based on the frequency of an atomic clock that works in a null gravity field. This would have required to apply a relativistic correction to the measured frequency of any atomic clock, but it would have provided unit definitions independent of the position in the Universe. However, before contemplating the idea of time keeping on other celestial bodies it was decided to define the second based on an atomic clock that works on the surface of the geoid. I believe that this was a big mistake, because it ties all the SI units to the Earth and especially because it does not really avoid the use of relativistic corrections. Now the precision of the atomic clocks is so great that for most of them it is necessary to apply relativistic corrections depending on the altitude of the laboratory. reply brummm 18 hours agorootparentprevIt would redefine the meter too since the speed of light is defined as a constant and the meter derives from it. reply looperhacks 17 hours agorootparentThe meter is defined by the speed of light in a vacuum, that doesn't change on the moon reply ZiiS 16 hours agorootparentMeter is defined as how far the light travels in a vacuum in 1/299792458th of a second. The actual speed of the light in a vacuum is fixed, so unless you have identical seconds the distance of the meter changes. If you have identical seconds in lower gravity, then you get less of them. reply seanhunter 16 hours agorootparentprevGood luck landing a craft built by multiple teams with possibly differing definitions of Newtons. Kinda feels like agreement on the definition of the Newton would be the difference between \"guidance and propulsion systems worked perfectly\" and \"debris was scattered across the landing zone\".[1] [1] Not an astrodynamicist, but have done enough problems with inclined planes, pulleys, springs etc to know it's quite important to get the magnitude of the forces right. reply doublerabbit 19 hours agorootparentprevHow many moon metres equate to 10 space football fields? reply 8338550bff96 19 hours agorootparentThe size of football fields might need to change as well to accommodate the change in play style. You probably don't want quarterbacks dunking a touchdown in one flying leap from the 50-yard line. reply kube-system 19 hours agorootparent> You probably don't want quarterbacks dunking a touchdown in one flying leap from the 50-yard line. That’s exactly what I want! takeMyMoney.jpg reply dmd 19 hours agorootparentprevJon Bois has you covered reply non-chalad 19 hours agorootparentprevIs that moon yards, or space yards? reply saalweachter 18 hours agorootparentIs it space feet, or space survey feet? reply Findecanor 16 hours agorootparentIt has been speculated that humans evolved in lower gravity would grow taller, getting longer finer features than humans on earth. If an independence movement on Luna gets hold, it might want to differentiate Luna from Earth, and introduce a \"lunar foot\" based on lunar people. reply zokier 20 hours agoparentprevThe actual policy has pretty clear explanation why LTC is needed: > Additionally, the navigation accuracy a system can achieve with signals from multiple space-based assets, such as a person navigating on Earth with signals from Global Positioning System satellites, depends on the synchronization of those assets with each other. At the Moon, synchronizing each lunar asset with an Earth-based time standard is difficult — due to relativistic effects, events that appear simultaneous at the Earth (e.g., the start of a broadcast signal) are not simultaneous to an observer at the Moon. ... > Precision applications such as spacecraft docking or landing will require greater accuracy than current methods allow ... > Beyond these operational challenges, the direct use of UTC at the Moon (i.e., without correction) as the local time scale would have cascading effects for applications that require precise metrology. International System of Units (SI) core unit definitions, including the meter and kilogram, rely on the SI definition of time. Due to relativistic effects, a non-SI unit would introduce uncertainty in core unit definitions. These types of errors will have undesired impacts, such as reducing the accuracy of mapping and inertial navigation products reply aftbit 18 hours agorootparent>due to relativistic effects, events that appear simultaneous at the Earth (e.g., the start of a broadcast signal) are not simultaneous to an observer at the Moon. Isn't this true on Earth as well, just to a slightly lesser degree? Why are the synchronization mechanisms used to correct for drift in LEO suddenly unable to cope when used on Luna? reply jacoblambda 16 hours agorootparentIt's true for exactly the same reason. You have - TAI: i.e. Atomic time which is basically the aggregate of a bunch of major atomic clocks to get as close as possible to \"true time\" on earh. - UT: i.e. Universal Time on earth with the different UT0, UT1, etc providing different levels of correction based on where you are on earth. - UTC: i.e. coordinated time that is the same anywhere on earth. This is derived from TAI but receives leap seconds when it is sufficiently out of sync from UT1. - GPS time: i.e. the specific time standard kept on GPS satellites based on their orbit. This time is derived from UTC(UNSO) which is a specific UTC clock at the US Naval Observatory and the offsets are recorded by the satellites. So you have your GPS time and your UTC time. What this proposal is doing is effectively the same thing. i.e. creating a new coordinated time standard for the moon (LTC) that tracks an offset of how far it's drifted from UTC so you can effectively coordinate. And eventually when the moon gets it's own GPS (which it will eventually), you'll have GPS(LTC), i.e. GPS time relative to LTC. reply masswerk 18 hours agorootparentprevHowever, wouldn't the inclusion of a coefficient solve this easier in a much more general and flexible way than defining a frame of reference for each individual case? reply rob74 20 hours agoparentprevMy thought exactly - the fact that time runs slightly faster on the Moon than on Earth is fascinating, but the pragmatic approach would be to just compensate for the deviation. reply lxgr 19 hours agorootparentHow would you? Periodic leap seconds, or redefine the second (which would have impacts on many other units of measurements derived from it – are you prepared to also introduce lunar meters, for example)? reply rob74 19 hours agorootparentThe compensation for the relativistic effects just makes sure that the second on the Moon is the same as the second on Earth. According to the article another commenter linked (https://www.ipses.com/eng/in-depth-analysis/standard-of-time...) this is already routinely done on GPS satellites: > the only corrections made on atomic clocks located on satellites are very small adjustments to ensure that they remain perfectly synchronized with atomic clocks installed on the Earth (usually to correct drifts due to relativistic effects). this takes care of the effect mentioned in the article. However the more relevant (and debatable) question is if \"Moon time\" should also observe the leap seconds (which are introduced to account for variations in Earth's rotation, so have nothing to do with the Moon). reply lxgr 18 hours agorootparentThis only works on the GPS satellites because they provide the GPS time for Earth's benefit, not their own. For all internal operations requiring high precision, they'd actually have to keep their own time reference (or calculate a dynamic offset from \"GPS time\") or they'd get unexpected results since the atomic clocks they carry run fast with regards to the SI definition of a second. (It could be the other way around, i.e. the clocks running correctly with regards to the satellite's frame of reference and an offset being applied to the signal, but I suspect skewing clocks to fit the Earth-based frame is easier.) reply marcosdumay 20 hours agoparentprevAnd suddenly all of physics changes slightly. reply non-chalad 19 hours agorootparentHow often has this happened in the past? Has it had any effect? reply withinboredom 17 hours agorootparentGo back a couple hundred years before there were measures. A bolt from one foundry was a different size than the bolts in another foundry. It wasn't until the early 1900s that things were actually attempted to be standardized. Even until the '50s/'60s (and '70s in some countries), you'd often have to go to your local blacksmith to get them to make something as simple as a bolt. reply swader999 16 hours agoprevThe article title cracks me up. As if anyone in the Whitehouse even thinks about this. Why don't they just say NASA. reply saghm 17 hours agoprevGiven that \"UTC\" means \"coordinated universal time\" due to a compromise (https://en.wikipedia.org/wiki/Coordinated_Universal_Time#Ety...), would this end up being \"LTC\" rather than \"CLT\"? reply lainga 17 hours agoparentWhen the French get someone up there, we can consider it. reply skeeter2020 20 hours agoprevmost important: What impact will this have on the timezone picker for my (yet to be widely used) SaaS app? reply martin_a 20 hours agoparentThere will probaby be at least three JS frameworks to take care of that, don't worry. reply qntty 20 hours agoprevWith daylight savings time too, I would hope reply junon 20 hours agoparentIf memory serves DST is mostly a historical thing for farming and other daylight-centric work due to the Earth's rotational tilt in relation to the sun, right? Unless we have farmers on the moon where this is a consideration I would hope not, given that it's a source of confusion and complexity in timekeeping software. reply aftbit 18 hours agorootparentFarming? Where does this myth come from? Farmers hate DST. The cows want to be milked according to the sun, not to a clock. https://agamerica.com/blog/myth-vs-fact-daylight-saving-time... reply engineer_22 17 hours agorootparentprevFrom https://en.wikipedia.org/wiki/Daylight_saving_time : >It is a common myth in the United States that DST was first implemented for the benefit of farmers.[38][39][40] In reality, farmers have been one of the strongest lobbying groups against DST since it was first implemented.[38][39][40] The factors that influence farming schedules, such as morning dew and dairy cattle's readiness to be milked, are ultimately dictated by the sun, so the clock change introduces unnecessary challenges.[38][40][41] >DST was first implemented in the US with the Standard Time Act of 1918, a wartime measure for seven months during World War I in the interest of adding more daylight hours to conserve energy resources.[42][41] Year-round DST, or \"War Time\", was implemented again during World War II.[42] After the war, local jurisdictions were free to choose if and when to observe DST until the Uniform Time Act which standardized DST in 1966.[42][43] Permanent daylight saving time was enacted for the winter of 1974, but there were complaints of children going to school in the dark and working people commuting and starting their work day in pitch darkness during the winter, and it was repealed a year later. reply junon 17 hours agorootparentInteresting, I stand corrected :) thanks reply engineer_22 17 hours agorootparentNo problemo, I was curious also! reply danesparza 20 hours agoparentprev:-/ reply yencabulator 16 hours agoprev> Locating and directing this mission requires extreme precision down to the nanosecond, errors in navigation which could risk spacecraft entering the wrong orbits. I don't see how time on the lunar surface helps with that any more than time on Earth surface does... reply mywittyname 19 hours agoprevI hope this prompts Tom Scott to do a follow up on his brilliant video on the complexities of timezone. reply aftbit 18 hours agoparentIsn't he done with videos now? reply gen3 18 hours agorootparentNope, just not producing on a schedule now reply tmaly 14 hours agoprevI hope they don't plan on a Moon Light Savings time. reply daviddever23box 16 hours agoprevMight be an excellent opportunity to re-think how we handle timekeeping in general, especially as Local Solar Time will vary wildly by dint of the moon's orbital patterns. reply NovemberWhiskey 19 hours agoprevIs it just me, or is the headline terrible? They don't want the moon to have its own timezone; they want it to have its own time standard. i.e. CLT (which will probably end up being LTC for the same reason that UTC is neither TUC or CUT) replaces UTC for lunar operations and any putative lunar timezones would be offsets from that. reply pulvinar 17 hours agoprevI can see NIST setting up station WWVM: \"At the tone, 693 hours, 21 minutes, Coordinated Lunar Time.\" reply jgtor 18 hours agoprevCan't wait for a 2026 manned moon mission. Get to watch a live moon landing like my grandparents generation! reply idlewords 18 hours agoparentNo one is landing on the moon in 2026. SpaceX is two years behind schedule building the lander, and there's not enough room in the calendar to develop and then operationalize the necessary refueling technology. The earliest feasible landing date is probably in 2028. reply withinboredom 17 hours agorootparent> is two years behind schedule building the ladder I initially read it as that, and didn't even blink. Elon's promises are so ridiculous and inept that being two years behind on building a ladder seems entirely plausible. reply baq 20 hours agoprevWhat an amazing problem to have, both as a bureaucrat somewhere in a dark office and as the whole humanity...! reply ordu 19 hours agoprev> Because of the different gravitational field strength on the Moon, time moves quicker there relative to Earth - 58.7 microseconds every day. So it is beneficial to move computers there because they would make more computation per year. Though 58.7us/day is ~21ms/year which is 6.8e-10, one needs a really big scale to make it meaningful. reply icapybara 19 hours agoparentDownloading the result of your multi-year computation would take a long time too, so that needs to be factored in. reply fullstop 16 hours agoprevThey need to run this stuff past a panel of 14 year old boys. reply diebeforei485 14 hours agoprevIs this dominated by gravitation or relative motion? reply j-j-j-j 18 hours agoprevThat would be a big improvement in the remote work economy. reply pseingatl 20 hours agoprevSame for Mars, called a \"sol.\" See, marslegalcode.org. reply mikhailfranco 18 hours agoprevDoes the IRS claim jurisdiction on the Moon or Mars? (asking for an American friend) Elon - renounce citizenship before blast-off! reply jacoblambda 16 hours agoparentIn all seriousness, it'd be the same as with antarctic stations where which station you are at determines the rules. reply danesparza 20 hours agoprevI'm OK with this. Just remove daylight savings shifts here on Earth. reply DrFalkyn 20 hours agoprevCoordinate Lunar International Time or CLI- reply dotnet00 20 hours agoprevJust another symbolic gesture from the White House to try to pretend that they're actually serious about achieving Artemis' goals as they continue to cut NASA's budget while raising the allocation to the waste of material that is SLS, doing nothing more than accomplishing their goal of killing science programs and purchasing votes and future 'advisor' roles at Boing. reply ajaimk 20 hours agoprevLet's get rid of DST first; then we can talk about moon time... reply spacephysics 20 hours agoprevMaybe they’re just now doing this cause we never actually went to the moon! /s reply joncrane 20 hours agoprevGreat. Yet another time zone I have to worry about when writing my scripts. reply beanjuiceII 19 hours agoprevCoordinated Lunar International Time ackshulally reply queuebert 20 hours agoprevWhatever time standard we settle on will need to be re-synchronized periodically, since, according to general relativity, time measured in Earth's gravity well will be running slightly slower than clocks on the moon with its lower gravity. GPS satellites already correct for this. reply queuebert 18 hours agoparentNo, I did not RTFA. Hangs head in shame. As an American, I did not expect my government to talk about relativity in a press release. reply SamBam 20 hours agoprev [–] Odd to frame it as the \"White House wants...\" I know the White House is a metonym for the US government, but it usually refers to something at least slightly in the political wheelhouse, like foreign policy or taxes. Why not just say NASA here? reply dragonwriter 20 hours agoparentIts literally a directive from the White House Office of Science and Technology Policy; White House is not being used in a figurative sense for “the US government broadly” (which , incidentally, I’ve never seen any major Western media do; Washington, sure, but not the White House) but in a literal, institutional sense. https://www.whitehouse.gov/ostp/news-updates/2024/04/02/whit... reply atonse 19 hours agorootparentBut isn't everything NASA does (by extension of being in the executive branch) what the \"white house wants\"? Unless this was triggered by an executive order, which is kind of exceptional to the normal process. reply dragonwriter 18 hours agorootparent> But isn't everything NASA does (by extension of being in the executive branch) what the \"white house wants\" In some abstract philosophical sense, perhaps, but that's different than a directive from an actual White House office, which this is. > Unless this was triggered by an executive order, which is kind of exceptional to the normal process. Policy directives from the White House Office of Science and Technology Policy are also an exception to the normal NASA-internal decision-making process. reply Dalewyn 18 hours agorootparentprevNASA is an Independent Agency[1], which basically means they are a part of the Executive Branch but not part of the White House. [1]: https://en.wikipedia.org/wiki/Independent_agencies_of_the_Un... reply unethical_ban 19 hours agorootparentprev>Unless this was triggered by an executive order Not executive order I don't think, but yes, from the White House's offices directly. This didn't originate from NASA administration. https://www.whitehouse.gov/wp-content/uploads/2024/04/Celest... reply SamBam 20 hours agorootparentprevAh, fair enough, that wasn't anywhere in the article. reply tekla 20 hours agorootparentIts literally the first line reply SamBam 15 hours agorootparentIt absolutely isn't the first line that it's a directive from the White House Office of Science and Technology Policy. It just says \"The White House wants...\" Again, \"The White House\" is often a metonym for the US government. [1] I'm not sure why it didn't bother mentioning the Office of Science and Technology Policy. If I had realized that's where it came from, instead of from NASA, I wouldn't have mentioned it. 1. \"Metonymy is a figure of speech where the name for one object or concept is substituted for another, related one (as in the White House for the US Government)\" https://www.dictionary.com/compare-words/metonymy-vs-synecdo... reply tekla 11 hours agorootparentOnly relevant if you simply ignore how the American Govt works. Yes the White House is used as a \"metonymy\" for the Govt as a while, but with SOME critical thinking, its obvious this is coming from the executive branch, since the legislative and judicial branches are irrelevant for the purposes of directives since they pass legislation/judicial judgements, not yell about directives reply samatman 15 hours agoparentprev [–] \"The White House\" is used as a metonym for the Presidency, not the entire US government, or even for the executive branch: it would be incorrect to describe something from the various departments as originating from the White House. USG has something called the Executive Offices of the Presidency, that's included in \"the White House\". This is one of those councils, they're all hosted at whitehouse.gov reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The White House has tasked NASA with developing a new time zone, Coordinated Lunar Time (LTC), for the Moon due to the different gravitational field strength causing time to pass faster there.",
      "LTC aims to synchronize spacecraft missions to the Moon, as atomic clocks on Earth run faster on the Moon, posing a timing challenge for missions.",
      "Various countries and organizations are collaborating to establish LTC by 2026 for the US manned Moon mission, crucial for effective communication and navigation in space."
    ],
    "commentSummary": [
      "The White House is contemplating the introduction of a unique time zone, Coordinated Lunar Time (CLT), for the Moon to lessen reliance on Earth clocks, potentially utilizing atomic clocks for recording celestial events.",
      "Discussions are ongoing about renaming Coordinated Universal Time (UTC) and establishing a universal time standard, exploring time dilation in Special and General Relativity, impacting space travel and gravitational interactions.",
      "The challenges of standardizing time across various reference frames, adjusting physical constants, and the necessity for precise timekeeping in lunar activities are key points in understanding the complex concepts of time dilation, gravity, and the speed of light."
    ],
    "points": 211,
    "commentCount": 238,
    "retryCount": 0,
    "time": 1712144206
  },
  {
    "id": 39917551,
    "title": "Revolutionary 'Mini Liver' Grown in Lymph Node Could Offer Hope for Liver Failure",
    "originLink": "https://www.nature.com/articles/d41586-024-00975-z",
    "originBody": "NEWS 02 April 2024 ‘Mini liver’ will grow in person’s own lymph node in bold new trial Biotechnology firm LyGenesis has injected donor cells into a person with liver failure for the first time. By Max Kozlov Twitter Facebook Email Researchers injected a mouse’s lymph node with liver cells (green), converting the organ into a ‘mini liver’. Credit: Lagasse Lab A person has received an experimental treatment for the first time that, if successful, will lead them to grow an additional, ‘miniature liver’. The procedure, developed by the biotechnology firm LyGenesis, marks the beginning of a clinical trial designed for people whose livers are failing, but who have not received an organ transplant. First pig liver transplanted into a person lasts for 10 days The approach is unusual: researchers injected healthy liver cells from a donor into a lymph node in the upper abdomen of the person with liver failure. The idea is that in several months, the cells will multiply and take over the lymph node to form a structure that can perform the blood-filtering duties of the person’s failing liver. “It’s a very bold and incredibly innovative idea,” says Valerie Gouon-Evans, a liver-regeneration specialist at Boston University in Massachusetts, who is not involved with the company. The person who received the treatment, on 25 March, is recovering well from the procedure and was discharged from the clinic, says Michael Hufford, chief executive of LyGenesis, which is based in Pittsburgh, Pennsylvania. But physicians will need to closely monitor them for infection because the person needs to take immunosuppressive drugs so that their body doesn’t reject the donor cells, says Stuart Forbes, a hepatologist at the University of Edinburgh, UK, who is not affiliated with LyGenesis. Organ crisis More than 50,000 people in the United States die each year with liver disease. In the end stage of the disease, scar tissue that has accumulated prevents the organ from filtering toxic substances in the blood, and can lead to infection or liver cancer. A liver transplant can help, but there is a shortage of organs: about 1,000 people in the United States die every year waiting for a transplant. Thousands more aren’t eligible because they are too ill to undergo the procedure. A person received donor liver cells on 25 March that were injected into one of their lymph nodes.Credit: LyGenesis LyGenesis has been trialling an approach that could help people in this situation — and make use of donated livers that would otherwise go to waste because a person on the transplant waiting list with a compatible health profile hasn’t materialized in time. The company’s strategy delivers the donor cells through a tube in the throat, injecting them into a lymph node near the liver. Lymph nodes, which also filter waste in the body and are an important part of the immune system, are ideal for growing mini livers, Hufford says, because they receive a large supply of blood and there are hundreds of them throughout the body, so if a few are used to generate mini livers, plenty of others can continue to function as lymph nodes. The treatment has so far worked in mice1, dogs and pigs2. To test the therapy in pigs, researchers restricted blood flow to the animals’ livers, causing the organs to fail, and injected donor cells into lymph nodes. Miniature livers formed within two months and had a cellular architecture resembling a healthy liver. Researchers even found cells that transport bile, a digestive fluid produced by the liver, in the mini livers of the pigs. In this case, they saw no build-up of bile acid, suggesting that the mini organs were processing the fluid. Hufford says there’s reason to think that the organs won’t grow indefinitely in the lymph nodes. The mini organs rely on chemical distress signals from the failing liver to grow; once the new organs have stabilized blood filtering, they will stop growing because that distress signal disappears, he says. But it’s not yet clear precisely how large the mini-livers will become in humans, he adds. The company aims to enrol 12 people into the phase II trial by mid-2025 and publish results the following year, Hufford says. The trial, which was approved by US regulators in 2020, will not only measure participant safety, survival time and quality of life post-treatment, but will also help to establish the ideal number of mini livers to stabilize someone’s health. The clinicians running the trial will inject liver cells in up to five of a person’s lymph nodes to determine whether the extra organs can boost the procedure’s success rate. A stop-gap measure However, mini livers might not relieve all of the complications of end-stage liver disease, says Forbes, who has formed his own company to tackle liver disease using genetically modified immune cells that stimulate repair. One of the most serious of these is portal hypertension, in which the build-up of scar tissue compresses blood vessels in the liver and can cause internal bleeding. Pig brains kept alive outside body for hours after death Hufford acknowledges that the mini livers are not expected to address portal hypertension, but the hope is that they can provide a stopgap until a liver becomes available for transplant, or make people healthy enough to undergo a transplant. “That would be amazing, because these patients currently have no other treatment options,” Gouon-Evans says. LyGenesis has ambitions beyond mini livers, too. The company is now testing similar approaches to grow kidney and pancreas cells in the lymph nodes of animals, Hufford says. If the liver trial is successful, Gouon-Evans says, it would be worth investigating whether a person’s own stem cells could be used to generate the cells that seed the lymph nodes. This technique could create personalized cells that capture the diversity of cells in the liver and don’t require immunosuppressive drugs, she says. doi: https://doi.org/10.1038/d41586-024-00975-z References Komori, J., Boone, L., DeWard, A., Hoppo, T. & Lagasse, E. Nature Biotechnol. 30, 976–983 (2012). Article PubMed Google Scholar Fontes, P., Komori, J., Lopez, R., Marsh, W. & Lagasse, E. Liver Transplant. 26, 1629–1643 (2020). Article Google Scholar Download references Reprints and permissions Latest on: Regeneration Diseases Biotechnology Jobs Faculty Positions, Aging and Neurodegeneration, Westlake Laboratory of Life Sciences and Biomedicine Applicants with expertise in aging and neurodegeneration and related areas are particularly encouraged to apply. Hangzhou, Zhejiang, China Westlake Laboratory of Life Sciences and Biomedicine (WLLSB) Faculty Positions in Chemical Biology, Westlake University We are seeking outstanding scientists to lead vigorous independent research programs focusing on all aspects of chemical biology including... Hangzhou, Zhejiang, China School of Life Sciences, Westlake University Faculty Positions in Neurobiology, Westlake University We seek exceptional candidates to lead vigorous independent research programs working in any area of neurobiology. Hangzhou, Zhejiang, China School of Life Sciences, Westlake University Head of the histopathology and imaging laboratory GENETHON recruits: Head of the histopathology and imaging laboratory (H/F) Evry-Sud, Evry (FR) Genethon Seeking Global Talents, the International School of Medicine, Zhejiang University Welcome to apply for all levels of professors based at the International School of Medicine, Zhejiang University. Yiwu, Zhejiang, China International School of Medicine, Zhejiang University",
    "commentLink": "https://news.ycombinator.com/item?id=39917551",
    "commentBody": "'Mini liver' will grow in person's own lymph node in bold new trial (nature.com)194 points by Brajeshwar 20 hours agohidepastfavorite47 comments gamepsys 19 hours ago> The approach is unusual: researchers injected healthy liver cells from a donor into a lymph node in the upper abdomen of the person with liver failure. The idea is that in several months, the cells will multiply and take over the lymph node to form a structure that can perform the blood-filtering duties of the person’s failing liver. The first treatment was performed on March 25th, so we have several months to see how this will go. Initial recovery looks good. Of course this is huge because we don't have enough livers for everyone that needs one. If we can grow a new one with a little help than that will greatly increase the number of people with failing livers we can help. reply thimkerbell 19 hours agoparentWell this is good news. But how much of a whole liver does a recipient need, now, to get good enough liver function? I thought they were famous for growing back. reply sandworm101 18 hours agorootparentSurprisingly little. Most of the liver stand by at idle ready to filter intermittent massive doses of things, such the stuff arriving in your bloodstream after a big meal. This is what makes many liver diseases so evil. A person can live basically free of symptoms for many years as their liver struggles and is progressively damaged by some substance (alcohol). Until one day the amount of remaining functional liver dips slightly below the minimum needed to sustain daily functions. After decades of drinking, over the course of a month a functional alcoholic can go from being a happy drunk to a patient dying in hospital in need of a new liver. reply raisedbyninjas 16 hours agorootparentSo alcoholics should do a periodic stress test of binge eating to get an early warning. reply ajuc 16 hours agorootparentJust do blood tests for liver diseases markers every year. Even if you're not an alcoholic - there's many other diseases that result in similar damage. For example if you have asthma and especially if you have colitis ulcerosa or crohn's disease - you're at much higher risk of PSC - an autoimmune liver disease which does basically the same thing as decades of drinking. reply boredemployee 15 hours agorootparentTangentially, I just found out in a routine test that I have a condition called \"gilbert's syndrome\" [0]. It's harmless tho. It came from my parents, it seems. My dad died of liver cancer, but I don't know if he was diagnosed with that syndrome and if they were related. [0]. https://www.mayoclinic.org/diseases-conditions/gilberts-synd... reply rrrrrrrrrrrryan 4 hours agorootparentI've got Gilbert's as well - found out about ten years ago during some routine blood work. The only thing I've noticed is that dehydration hits me a little harder than most people. If I don't drink enough water throughout the day I get fatigued, and if I don't drink enough water after a night out I get pretty gnarly hangovers. This is just comparing me to my peers though, and might not be due to my Gilbert's at all, but the fact that these things have been linked to Gilbert's in studies means it's possible they're related. Anyway, staying hydrated never hurts, so thought I'd pass that along in case it helps you too. reply arcticbull 9 hours agorootparentprevNot to mention non-alcoholic fatty liver disease and its evolution, non-alcoholic steatohepatitis. reply adamredwoods 15 hours agorootparentprevPSC is a rough disease with the bile ducts. I'm surprised we haven't any treatment for it yet. reply hinkley 14 hours agorootparentprevOne of our dogs had days to live by the time we found out he had a congenital defect that caused a fibrous liver. In retrospect some parts of his personality may have been symptoms, but we were wrecked. reply derefr 10 hours agorootparentWhat aspects of his personality would you suspect? reply hinkley 8 hours agorootparentVery easily satisfied by activity. Afterward I was less convinced he was a chill dude and more concerned he was easily exhausted. reply _a_a_a_ 16 hours agorootparentprevI'm rather unclear about the speed of regrowth. From one source (the web?) It was a matter of years, yet talking to a herpetologist (edit: hepatologist. Blame voice recognition) in a pub, she said it can regrow 80% of itself in a matter of weeks. Assuming a healthy liver, how fast is regrowth? Assuming a cirrhosed liver , how would that affect regrowth time? TIA. reply username135 7 hours agorootparentWhen a liver has cirrhosis, its scarred beyond functional repair which means no growth. reply Jaepa 15 hours agorootparentprevThey are to an extent. Cirrhosis is permanent. Other conditions such as fatty liver are reversible-ish. The issue is that the liver is basically fully functional until after you’ve destroyed ~88% of it. For a tortured analogy it’s a lot like a DB. Most of them time you’re well under your total capacity. Even short spikes in queue length aren’t an issue. But once you hit the tipping point there are cascading failures. reply skissane 11 hours agorootparent> Cirrhosis is permanent Cirrhosis has been observed to reverse in some individuals. It is still permanent for most patients, and we shouldn’t give people with it false hope, but its reversal isn’t impossible. Most commonly cirrhosis reversal is observed after chronic viral hepatitis is cured by antiviral treatment. There are even a handful of clinical case reports of alcoholic cirrhosis reversing after extended abstinence from alcohol, even though that is a rather rare outcome. I remember reading a case report (sorry can’t find it right now), of a Japanese alcoholic diagnosed with liver cirrhosis, who then was severely disabled by a stroke, and had to be moved to a nursing home. He never drank again because he physically couldn’t drink without nursing staff assistance, and they weren’t going to give him any alcohol. Roughly 20 years later, his cirrhosis had disappeared. Of course, this is a rare outcome (he likely had favourable genetics, plus rarely does an alcoholic find relapse physically impossible), but it shows it can happen. This is why there is a lot of hope that some of the liver disease drugs currently in the clinical pipeline may be able to reverse cirrhosis (and if not them, maybe their future successors). However, current trials are focusing on fibrosis not cirrhosis. And they run into the difficulty that what counts as cirrhosis seems reasonably clear in everyday clinical practice, but how to reliably detect its existence/progression/improvement/reversal to a clinical trial standard of certainty is difficult and a matter of dispute (No I don’t have cirrhosis, as far as anyone knows, but my aunt had it-albeit asymptomatic, it was only discovered on autopsy after she suddenly and unexpectedly died from a cardiac arrest while asleep one night-which is actually very common, majority of people with liver disease die from heart disease before the liver disease kills them, or even causes any observable symptoms-but I do have intermittent hypochondria, and in some of my past hypochondriac episodes I have become rather obsessed with reading papers on this and related topics.) reply sp332 19 hours agorootparentprevIf the liver gets full of hard scar tissue, it can physically constrain the hepatocytes from making more liver tissue. https://www.news-medical.net/news/20200824/Large-animals-wit... reply bschmidt1 14 hours agoparentprevGoodbye fatty liver hello fatty lymph nodes! The question is how often will they have to get bile drained, and what does it do to body shape? reply lokar 18 hours agoparentprevAnd there is nothing like kidney dialysis for the liver. reply a1o 18 hours agoprev> Hufford says there’s reason to think that the organs won’t grow indefinitely in the lymph nodes. The mini organs rely on chemical distress signals from the failing liver to grow; once the new organs have stabilized blood filtering, they will stop growing because that distress signal disappears, he says. But it’s not yet clear precisely how large the mini-livers will become in humans, he adds. Well that looks promising, hopefully they stay as mini-livers. reply smeej 18 hours agoparentI immediately imagined people with much larger liver masses in their necks and got to wondering how safe it would be. Is there risk of impingement on things like arteries or the spine were they to continue to grow? What's the margin of safety between how big they're expected to get and how big would be dangerous? (I know nearly nothing about any of the systems involved, so I won't be surprised if these end up being comically naïve questions. I just still wonder!) reply engineer_22 17 hours agorootparentThey are using lymph nodes in the abdomen. Worth-while to consider your questions though. reply smeej 16 hours agorootparentOh you're right. I misread the part about a tube in the throat and thought they were ending up in the neck. Though, with lymph nodes as connected as they are, why do the liver cells stay where they're planted, so to speak? reply tialaramex 10 hours agorootparentAlmost all human cells are programmed to remain in a specific place and to commit suicide if they discover they're misplaced. This way when things go a little bit wrong and a few cells are somewhere they shouldn't they just die, and there are more where those came from. https://en.wikipedia.org/wiki/Anoikis The lymph system's own cells deliberately lack this programming, they're part of the immune system and are supposed to wander about on their own. But these liver cells presumably don't think they're actually lymph cells, so they aren't entitled to just wander off. reply exe34 17 hours agoparentprevIt's pretty amazing, a decentralized liver. I wonder how many other organs could be decentralized like this for backup. reply harvie 17 hours agorootparentAre we talking brain here? reply exe34 16 hours agorootparentI was thinking mini-kidneys, mini-hearts? The latter might involve difficult coordination, but maybe that's not necessary if you've got individual hearts running at lower blood pressure. It all just reminds me of Neal Asher's stories, where creatures build nano factories in their bodies as and when needed, including local fusion \"nodes\". reply inglor_cz 14 hours agorootparentThe people who are running this trial have similar plans with mini-kidneys and mini-pancreases, yes. It is mentioned at the end of the article. Personally, I would like to see mini-brains. \"This idea comes from my ass\" could become literally true. reply BobaFloutist 9 hours agorootparentHeart would be great because it's such a single point of failure. Brain would be nice for similar reasons, but also...a little more existentially challenging. reply exe34 1 hour agorootparentI think there's a lot of parts in the brain that involve day to day coordinations that could be moved out. Fine motor control stuff in the cerebellum probably wouldn't hurt if it's closer to the muscles, but the cerebrum might be harder to split up and scatter because of latency and bandwidth. On the other hand, we could go from dual core in the cranium to multicore distributed - like octop(uses|i) have one brain per arm and I believe another to rule them all. reply Kalanos 19 hours agoprevinside a lymph node is the very last place i would expect a transplant. they are teaming with immune cells that should be hostile to an inter-species transplant. there are no red blood cells for oxygen, just lymph fluid. how would it recruit blood vessels to grow? it would be painful; swollen glands. why not orthotopic? reply sp332 18 hours agoparentYears ago, someone noticed that hepatocytes injected into a mouse's abdomen would migrate into the lymph nodes and start making liver cells. I don't even know why they were doing that in the first place, but all this research follows from that observation. reply Kalanos 15 hours agoparentprevteeming* reply idontwantthis 18 hours agoprevDoes anyone know if they could use the patient’s own liver cells instead of a donor’s? Is every cell in the dying liver so unhealthy that they couldn’t harvest any? reply turblety 18 hours agoparentThis is my thought too. How many cells are actually need to do this. If it's just a few, surely if a patients kidney is in 80% bad shape, then they can take a few cells from the 20% good and do this? I guess if there is a genetic problem with the patients own liver, then it would make sense to use another persons cells. Ahh just read the last paragraph: > If the liver trial is successful, Gouon-Evans says, it would be worth investigating whether a person’s own stem cells could be used to generate the cells that seed the lymph nodes. This technique could create personalized cells that capture the diversity of cells in the liver and don’t require immunosuppressive drugs, she says. reply caconym_ 18 hours agorootparentThis was my question too, so thanks for doing the legwork. Wondering (in complete ignorance of this general topic) why they didn't jump straight to that approach, though... edit: I guess maybe if you start with a healthy liver, the variable of whether the liver disease in question could affect the viability of transplanted cells is taken out of question? Presumably the immune response/immunosuppressant stuff is better characterized. reply jakewins 15 hours agorootparentThat last paragraph is different though, it’s using stem cells to make liver cells, not using the patients liver cells directly.. I have no idea why not, but my wife does a lot of work growing different cell types from stem cells, and my understanding is that that’s still like.. they think they are making cell types a, b or c, but it’s a lot of uncertainty. What they really do is convert the stem cells into cells that express m various markers/pass various tests that the “real” cell type also express.. but it’s really hard to know that it’s actually a 1-1 match. Just yesterday she was lamenting they were making astrocyte cells, and many of the cells in the colony instead became.. something else, unclear what, maybe not even a cell type that exists in humans normally? Either way: using healthy liver cells from a healthy liver would be a way to ensure you actually really have liver cells, and not something that just sorta duck-types as one reply datascienced 9 hours agorootparentprevTDD! They are passing the easier test first. reply idontwantthis 18 hours agorootparentprevShoot I swear I RTFA! reply jojobas 9 hours agoprevThis sounds uncomfortably close to being a metastase. reply m3kw9 17 hours agoprevHow does it know to stop growing up to a optimal size? reply fwipsy 16 hours agoparentThis is covered in the article. As I understand it, the liver grows in response to chemicals in the blood which it also filters. So it will stop growing when it's large enough to filter all the markers. reply r2_pilot 16 hours agoparentprevThe existing liver stops sending distress signals once the stress is lessened on it, and the new liver uses the lack of signal to stop growing. reply trallnag 19 hours agoprevCan we please grow lymph nodes and vessels next? I need them reply Kalanos 19 hours agoparentis there no stem cell therapy/ clinical trial for this condition? reply martibravo 19 hours agoprev [–] Grey's Anatomy predicted this. [1] [1] https://mirm-pitt.net/from-the-lab-of-dr-eric-lagasse-to-abc... reply isodev 18 hours agoparent [–] Shonda Rhimes always gives us high quality content. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "LyGenesis has injected liver cells into a person's lymph node, aiming to develop a 'mini liver' to assist with liver failure, offering a temporary solution for transplant patients.",
      "The innovative technique, proven successful in animals, targets individuals unsuitable for standard liver transplants, with plans to expand to kidney and pancreas cells within lymph nodes, potentially eliminating the necessity for immunosuppressive medications.",
      "This pioneering method could revolutionize cell therapy by producing personalized cells mirroring liver diversity, showcasing a promising alternative to conventional transplantation."
    ],
    "commentSummary": [
      "A new trial involves injecting healthy liver cells into a person's lymph node to grow a \"mini liver\" for treating liver failure.",
      "Research is ongoing on using stem cells to create personalized liver cells for transplant, facing challenges in ensuring the correct cell type and addressing safety concerns.",
      "Interest in using lymph nodes for decentralized organ transplants and creating mini-organs for backup function is gaining attention in the medical field."
    ],
    "points": 194,
    "commentCount": 47,
    "retryCount": 0,
    "time": 1712152642
  },
  {
    "id": 39915473,
    "title": "Redis Fork Redict 7.3.0: Stable, Community-Driven Alternative",
    "originLink": "https://redict.io/posts/2024-04-03-redict-7.3.0-released/",
    "originBody": "Redict 7.3.0 is now available Drew DeVault April 3, 2024 The Redict community is pleased to announce the release of Redict 7.3.0, the first stable version of our copyleft fork of Redis® OSS 7.2.4.1 You can download the release on Codeberg, or download one of our official container images from registry.redict.io. We have written comprehensive documentation detailing our compatibility with Redis® OSS 7.2.4, which also provides detailed documentation for various migration scenarios, such as for users of the official Redis® containers on Docker Hub, downstream package maintainers, and so on. We have tested Redict thoroughly with a variety of migration scenarios, but we may have missed a detail that pertains to your use-case. If you have any issues or questions with respect to the migration process, please join our community spaces to get help. Why Redict? # You may be wondering why Redict would be of interest to you, particularly when compared with Valkey, another Redis® fork that was announced on Thursday. In technical terms, we are focusing on stability and long-term maintenance, and on achieving excellence within our current scope. We believe that Redict is near feature-complete and that it is more valuable to our users if we take a conservative stance to innovation and focus on long-term reliability instead. This is in part a choice we’ve made to distinguish ourselves from Valkey, whose commercial interests are able to invest more resources into developing more radical innovations, but also an acknowledgement of a cultural difference between our projects, in that the folks behind Redict place greater emphasis on software with a finite scope and ambitions towards long-term stability rather than focusing on long-term growth in scope and complexity. We will happily pull useful changes from software with permissive licenses, such as Valkey, to improve Redict; such is the value of permissive software and the key advantage of free software generally. However, we will do so at a more conservative pace, so that our users can enjoy stability first and shiny new features second. We are also going to focus on establishing and maintaining a good relationship with downstream distributions, prioritizing their needs with respect to tasks such as de-vendoring Lua and jemalloc. Redict also has social and political aims which differ from other forks. In short, we believe in an approach which is built from an independent, grassroots, and community-focused means of building our software. We are not governed by the consensus of a small group of commercial interests, but rather by an independent and community-driven consensus. Importantly, we have also chosen to protect our software from further exploitation by applying the Lesser GNU General Public License (LGPL) to our work. Our choice of license prevents the hard work of our contributors from being incorporated into the now-proprietary Redis® software, and from any future attempts to create proprietary distributions of Redict. However, our choice of the LGPL balances this concern with the needs of commercial users – we have selected this license in part to ensure that cloud providers can continue to offer Redict to their customers without being subject to onerous compliance regimes. Note: Check out our About the license page for more information about the license of Redict. We’ve made these choices because we believe they are essential in providing for a future which is based on free software, and in which the rug cannot be pulled out from under our users and contributors again. We believe it is essential to make these choices now, at the onset of our fork, especially in response to the crisis that the Redis® community is faced with at the hands of its commercial stewards. If you don’t want your investment in this software to risk another artificial crisis in the name of profit, if you want to enjoy the protection of copyleft and a guarantee that your software will remain free, then we encourage you to adopt Redict for your needs. We have also taken this opportunity to re-evaluate our infrastructure and double down on using free software. Rather than continuing to use the proprietary GitHub forge, we have elected to use the non-profit, free software Codeberg as our home, and we run our continuous integration on SourceHut, which is also free software. Moreover, rather than gathering on Discord, we have chosen instead to set up our community on Matrix and IRC. We believe that the Redis® license change provides us an opportunity to focus on our values as members of the free software community, to exercise solidarity, and to lend our strength to re-enforce the broader free software ecosystem. As such, we felt it important to choose free software solutions for our infrastructure needs. Acknowledgements # I’d like to extend a personal “thanks” to everyone who was involved in bringing this fork to life. In particular, Micke Nordin and Hugo, for their work on the Redict containers; Lucas Dohmen, for his extensive work on the documentation and website; and Anna, for her work forking and maintaining hiredict; as well as everyone who contributed small patches here and there, and everyone who helped with the rapid turn-around and testing of Redict’s release candidates. Shoutout to @janWilejan for designing our logo, and to everyone else who submitted their artwork for consideration. Thanks are also due, of course, to all of the many contributors who worked on Redis® OSS, commercial contributors and independent contributors alike, whose work forms the foundation of our codebase, as well as all of those who worked on the extensive Creative Commons documentation that was adapted for Redict. We also extend our gratitude to the community downstream of Redis® OSS, including downstream distributions, cloud services, and countless users, all of whom nourished its growth as free software. What’s next? # We focused on a very conservative set of changes for the initial release, to maximize backwards compatibility and ease the transition for new users. Going forward, we do have some plans to make conservative improvements. Among the planned changes are: Modernizing the build system (muon is the leading candidate) Forking the ecosystem, in particular Redis® client libraries (this is a great way for you to help!) De-vendoring dependencies such as Lua and jemalloc Lucas is also planning to invest heavily in Redict’s documentation, such that we become the reference of choice for all participants in this ecosystem. Anna has some changes planned for hiredict as well (our fork of the official Redis® C client library), including build system improvements and better conformance with Unix norms. We will also be happy to consider improvements from any community member – come join us! We will welcome you as equals – independent and commercial users alike! Redis is a registered trademark of Redis Ltd. Any rights therein are reserved to Redis Ltd. Any use by the Redict project is for referential purposes only and does not indicate any sponsorship, endorsement or affiliation between Redis and the Redict project. ↩︎",
    "commentLink": "https://news.ycombinator.com/item?id=39915473",
    "commentBody": "Redict 7.3.0, a copyleft fork of Redis, is now available (redict.io)191 points by drewdevault 23 hours agohidepastfavorite165 comments crabmusket 21 hours agoEveryone's discussing the license and the hosting, but I think this is the truly interesting differentiator: > In technical terms, we are focusing on stability and long-term maintenance, and on achieving excellence within our current scope. We believe that Redict is near feature-complete and that it is more valuable to our users if we take a conservative stance to innovation and focus on long-term reliability instead. This is in part a choice we’ve made to distinguish ourselves from Valkey, whose commercial interests are able to invest more resources into developing more radical innovations, but also an acknowledgement of a cultural difference between our projects, in that the folks behind Redict place greater emphasis on software with a finite scope and ambitions towards long-term stability rather than focusing on long-term growth in scope and complexity. It'll be interesting to see what Valkey's future is with the maintainers having some lofty goals, and expressing frustration that they weren't able to move fast enough or be innovative enough under Redis. As a small-time user of Redis I kind of like the idea that I could just have what I've got now, but with a promise that someone's looking after it. I don't feel the need for millions of transactions per second, a timeseries database, etc. reply reconditerose 17 hours agoparentHey, I am one of the maintainers of Valkey, I'll try to answer. I think there is a few things I would like to see in the mid to short term. We're trying to make a lot of the core datastructures more efficient (both in terms of memory and performance) as well as the main dictionary. Valkey 8.0 (or whatever first major version we have) will have lower overhead per key-value pair. Multithreading performance is nice, but as you mentioned most people don't need it. It gives folks a lot of runway if they need to scale but don't want to use clustering, and can also be a \"quick fix\" for certain types of P99 or higher latency spikes. Clustering is also really hard to use today, and a lot of the current folks want to fix that. It's a huge community pain point. Observability is another pain point I would like to improve. (Disclosure, I work at AWS as well) We see a lot of customers ask about \"why did we see a performance issue\", and Redis really doesn't provide a lot of introspection to diagnose those issues. Another big area, which maybe will work for redict, is we want better integration with other OSS projects, especially CNCF projects like envoy, open-telemetry, K8s. There are a lot of self-developed projects floating around, we're hoping we can pull all of this together to make a more cohesive project for people to use instead of what we see today. I think another big issue will be clients. I'm concerned Redis will try to inject a poison pill into the clients they own to make it so they can only talk to \"official\" Redis versions. Ultimately a small community will struggle to maintain a lot of clients, so I think we need a larger (and likely commercial) investment into keeping clients open. We will likely passively support redict with our clients, so they'll get that for free. We want to do all the cool feature stuff to (timeseries, JSON, bloom, RAG), but I would like to keep the core pretty clean. reply fmajid 17 hours agorootparentThank you for your work on Redis, Madelyn, and I'm sorry for the utterly unjustified abuse that's been thrown your way simply because you work at AWS. One feature that would be very helpful would be some form of analytics and a memory profiler. At one point I'd written a python library to iterate over a RDB to identify what the top keys consuming most memory were, but Redis has added a lot of complexity and new data types since, so I doubt it still works. reply crabmusket 12 hours agorootparentprevThanks for your work Madelyn and I wish you and the team the best of good fortune! reply drewdevault 16 hours agorootparentprevWe're excited to see valkey innovate in these areas! We wish you the best of luck and wish that we could have merged our forks, but alas. >Ultimately a small community will struggle to maintain a lot of clients, so I think we need a larger (and likely commercial) investment into keeping clients open I'm not sure that this is true, at least not the commercial part. Redis clients are pretty simple (compared to Redis itself, at least). Redict is leading an effort to encourage community forks of the official Redis clients, as well as sending patches downstream to third-party clients. We set as an explicit goal to fork the ecosystem as well; we've started this work with hiredict and don't intend to stop. I think there was some mention at some point about our two forks working together on maintenance of the protocol specification independently of Redis Ltd, which would be a good way to ensure that the clients remain broadly compatible with both of our forks. reply reconditerose 16 hours agorootparent> I'm not sure that this is true, at least not the commercial part. Redis clients are pretty simple (compared to Redis itself, at least). I'll get on my soap box then. The redis client ecosystem is fractured and awful, and it's mostly Cluster's fault. Getting cluster right is really hard, and most clients do it inconsistently and get something wrong, if they tried at all (hiredict doesn't for example). > I think there was some mention at some point about our two forks working together on maintenance of the protocol specification independently of Redis Ltd, which would be a good way to ensure that the clients remain broadly compatible with both of our forks. Yeah, I'm still fully aligned with this. reply drewdevault 16 hours agorootparent>I'll get on my soap box then. The redis client ecosystem is fractured and awful, and it's mostly Cluster's fault. Getting cluster right is really hard, and most clients do it inconsistently and get something wrong, if they tried at all (hiredict doesn't for example). Fair point, I think it's prudent to recognize this as a weakness in the ecosystem. But I would also say that applies generally of cluster, sentinel, etc, in all respects. I know this is a focus area for valkey for that reason. reply 8organicbits 22 hours agoprevBeing copyleft, Redict can merge any contributions to Valkey. However, Valkey cannot merge any of the Redict commits (unless the contributor actively dual licenses them). Being non-open source Redis can merge any contributions made to Valkey but not from Redict. So if you don't want your code to end up in Redis, contribute to Redict. Interestingly, there have only been two commits from a single developer to the Redis repo in the last two weeks since the license change. A huge decrease. reply reconditerose 17 hours agoparentAs someone who has extensively worked on a Redis fork at AWS and worked with the company for 6 years, I wouldn't worry about Redis merging stuff from Valkey. We believe they refused a huge number of PRs because we believe it would have messed with their internal features. (We can't prove this, because they never discussed it publicly). One of the main reasons I started contributing to Redis was to help my team at AWS get out of the business of merging conflicts. > Interestingly, there have only been two commits from a single developer to the Redis repo in the last two weeks since the license change. A huge decrease. It's just a guy from Redis. It's not even one of the three former maintainers (Oran, Yossi, or Itamar). The number of open PRs is also dramatically down (it was around 550 when I last looked before the fork, since I've gotten a lot of notifications from old PRs of people refusing the CLA). reply dewey 23 hours agoprevTime will tell if the version on Codeberg (https://codeberg.org/redict/redict) can compete with the fork on Github (https://github.com/valkey-io/valkey) in terms of visibility and contributions. reply forty 23 hours agoparentValkey is under the Linux foundation umbrella, and I assume will be developed by the same people who were previously making redis. I could not find who is behind predict. reply dewey 23 hours agorootparentYou can see it here: https://codeberg.org/org/redict/members reply drewdevault 22 hours agorootparentprevIt's worth noting that the Linux Foundation is a commercial consortium and the companies behind Valkey contribute over $1M to its annual budget. reply rmbyrro 22 hours agorootparent> contribute over $1M to its annual budget That on top of developer hours dedicated to various projects, which are probably worth multiple millions. reply pietroppeter 21 hours agoprevI think what are seeing here is the true power of an open license. There are now two forks with different approaches and two dedicated and competent teams and we will see not only who wins, but if any or both win (for some definition of win). reply Brian_K_White 20 hours agoparentTo me that's the big one, that even you an individual has the ultimate option to define \"win\" however you want. Not only are you not subject to the conflict of interest between users and sellers in a commercial product, you aren't even subject to the popularity contest tyranny of the majority in a non-commercial product. If someone takes the last open version of something and forks it and never does anything further to it and no one else ever uses it, but it does what they want, they win. They win at life no matter what anyone thinks about that fork. reply zwaps 23 hours agoprevIf you have a commercial use-case, there is also a non copyleft fork available here https://www.linuxfoundation.org/press/linux-foundation-launc... reply tormeh 23 hours agoparentMost commercial projects can safely use any of the Redis forks, whether Redis itself, Redict or Valkey. For Redict, you have to provide the source code of Redict - and Redict only! - in the case that you distribute it to customers. Redis has harsher terms, but only if you provide Redis-as-a-service. If you aren't a cloud provider and you don't modify the code of your chosen Redis variant, this is all a big nothingburger and neither of the licenses impose any important restrictions on you. We desperately need all open questions regarding AGPL and SSPL to be clarified in court so this fearmongering can stop. It's really really bad for open source. reply verdverm 22 hours agorootparentOpen source seems to be doing fine without the clarity. Even fields that have nothing to do with software are adopting the mindset. The movement only gains in steam reply yunohn 21 hours agorootparentMost popular OSS projects use Apache and MIT in my experience, precisely because GPLs are problematic for commercial usage and contributions from employed people. reply xorcist 20 hours agorootparentThat has been a popular opinion since FreeBSD was a complete operating system and Linux and bunch of disparate tarballs. Yet here we are. There have been other examples where copyleft products have competed with more permissively licensed products as commercial products. The results are overwhelmingly in favor of the former. Empirically, cooperation works much better on a level playing field where your company's work won't be included in a competitors closed fork. reply drewdevault 21 hours agorootparentprevThis is a common misconception and source of fear/uncertainty/doubt regarding copyleft licenses. It's true that, the stronger the copyleft license, the more obligations it imposes on companies, and the AGPL is perhaps the most onerous of all and thus the least attractive to businesses. But, copyleft exists on a spectrum, and there are thousands of big-ticket FOSS projects that tens of thousands of businesses depend on and make use of that have a copyleft license. Linux itself is GPL, and pretty much everyone depends on it. The license for Redict is one of the weaker copyleft licenses and should not pose any onerous compliance obligations on the most common commercial use-cases for Redict. reply hobs 20 hours agorootparentprevDon't know why this is downvoted, I have endlessly heard corp lawyers say this to my face while I worked there. I have had a \"popular\" open source library I contributed significantly to campaigned by Microsoft to change the license to MIT, and the founder and creator decided to relicense without getting any sign offs because they felt that nobody would get mad enough to sue them. reply Brian_K_White 19 hours agorootparentThe only thing stopping a commercial product from using copyleft software is themselves, not the copyleft. Lawyers entire job is to make convincing arguments for any position you want, by artfully speaking only true facts, no matter what the position is and no matter what the facts are. Of course a lawyer can and will say that you \"can't\" let any gpl software pollute the companys product \"because the gpl prevents it\", instead of saying that the company doesn't want to pay the license fee for the software. The facts of the gpls terms may be true, and the lawyer may present them for their argument, but that still doesn't make the overall assertion true. They can use gpl software all they want. They just don't want to. And good for them. That is better than simply stealing it which many do. It's also merely an assertion that \"most\" open source software uses apache/mit/bsd wthout some numbers and citation. But that sentence could be parsed more than one way. They might have only been saying that most of the projects that use apache/mit do so for commercial compatibility reasons. reply Macha 22 hours agorootparentprev> If you aren't a cloud provider and you don't modify the code of your chosen Redis variant, Or get hosted redis services from someone other than Redis Labs. That isn't fearmongering about the SSPL, it is literally the point of the SSPL. reply tormeh 14 hours agorootparentSure, but that's left for the hosted service providers to figure out. No one can sue you for using AWSs offering. reply drewdevault 23 hours agoparentprevLGPL is a pretty weak copyleft license and was chosen specifically because it is amenable to almost all present-day commercial use-cases for Redis. You don't have to publish your changes to Redict in most situations, commercial or not. Check out the FAQ here: https://redict.io/docs/license/ reply zwaps 23 hours agorootparent\"Tip: Commercial and non-commercial users of Redict are not required to publish or otherwise “open source” the source code of Redict, including any private modifications they make to the source code\" And very next section: \"If you compile Redict’s source code into an executable form and distribute this executable form to others you are required to include a copy of the Redict source code and any modifications to it licensed under the LGPL.\" Let's be real: Especially in the EU, the definition of distribution of derivative works is typically so strong that it's super risky to use any copyleft license and I personally (e.g. consulting for large companies) see huge issues with compliance whenever this happens. In other words, it doesn't even matter what the current intention of using this license is regarding commercial use (but not distribution?)... I would not recommend to use this commercially for multiple reasons including risk, career but also the fights to be had with legal compliance. reply dwheeler 22 hours agorootparentThat is absurd. LGPL is widely understood. Compliance is generally pretty easy. The Linux kernel has many users, and it is GPL. reply Macha 22 hours agorootparentprev> In other words, it doesn't even matter what the current intention of using this license is regarding commercial use (but not distribution?)... I would not recommend to use this commercially for multiple reasons including risk, career but also the fights to be had with legal compliance. In a non-technical firm with no relevant expertise outside the software devs, maybe. But in every software company I've worked for, legal has been more concerned with the AGPL, or Oracle's proprietary licenses, than LGPL which has always been explicitly approved. reply stavros 21 hours agorootparentprevIt's a massive stretch to read \"distributing the executable to people\" as \"distributing derivative works by somehow using this service in your network\". The language is very clear. reply dartos 13 hours agorootparentWhat would you consider a website? Is minimized JavaScript considered an executable? What about html and css? reply stavros 13 hours agorootparentHow would you compile Redict's source code into a website? And if you somehow did, it would be an executable, yes. reply drewdevault 23 hours agorootparentprevNote the careful use of private modifications as not requiring disclosure of the source code. You only have to include source in the very specific situation of building Redict and providing it in compiled form to customers directly. Like, if you give someone the ELF file. If you run it on their behalf as a cloud service or something you have no obligation to provide source, which is the most common commercial use-case of Redis. LGPL is a very well understood license, even in the EU, and is already in use for many projects that are widely depended on commercially. Consider ffmpeg as one prominent example, which is used by virtually all multimedia software in the industry. It is very easy to comply with the LGPL, and your legal department works for you, not the other way around. reply frant-hartm 22 hours agorootparent> legal department works for you, not the other way around. Not sure what planet you live on. Unless you are one of the execs, Legal (and other compliance departments like HR) work pretty much against you. They exist to protect the company and the exec team. reply elzbardico 20 hours agorootparentLegal may sometimes be stubborn dicks, they can be overly conservative and afraid, but basically they are there to protect you from doing stuff that would create problems for the company and usually lead to you being fired or even being arrested in consequence. And they are working from the context of potentially fighting against a hostile legal challenge from people like them working for other companies, agencies, the law or the government. Yeah, they know that we are talking is just common-sense, but the law machinery not always work according to common-sense, and they know it far better than you. Be cooperative with them, and they will usually try to help you. And don't try to teach them their work, after all, they don't try to teach you how to architect and code. reply marcinzm 21 hours agorootparentprevIn my experience, legal is perfectly supportive assuming you're sane, reasonable and not a dick to them. They have expertise and constraints (ie: client contracts, bandwidth, etc.) that you are not aware of but that's different than being your enemy. If your attitude to them is that they're your enemy then they will be your enemy because why would they treat someone who is clearly antagonistic to them as a friend? reply drewdevault 22 hours agorootparentprevThey exist to serve the bottom line, and if the bottom line is best served by evaluating and approving the LGPL license (a trivial task, as it is broadly understood and the compliance requirements are negligible for most users), then that's what they will do. And in any case, no one is categorically opposed to the LGPL. Unless you think that no one is using Linux in industry, given that it uses a stronger license (GPL), which is patently absurd. reply dani0854 11 hours agorootparent> And in any case, no one is categorically opposed to the LGPL. That's not exactly true, take android as an example, which has a policy of \"no GPL in user space\", if I recall correctly. I do however believe that due to drivers and other things, GPL is beneficial in Linux kernel, but that rather an exception. And also Linux is GPLv2, which is a big difference to GPLv3 (and so to LGPLv3). reply tormeh 23 hours agorootparentprev> Especially in the EU, the definition of distribution of derivative works is typically so strong Maybe I'm naive but how strong can it be? And is there anything that can't be solved by liberally decorating your software with links to the git repo of Redict? reply fmajid 17 hours agorootparentprevI've had to go over open source package licenses as part of due diligence when my startups were acquired, but LGPL was never an issue, unlike AGPL. reply yurytom 23 hours agoprevWhat about Valkey? We have 2 big forks now reply JoshTriplett 22 hours agoparentValkey has the support of various Redis developers who moved over, keeps the same license as the original Redis did, and stayed on GitHub to keep the workflow that Redis developers and contributors are used to, so I expect it'll end up winning out. GitHub is proprietary and not ideal, but when trying to get developers on board after a fork, using GitHub and using the same license as the original avoids spending innovation tokens / weirdness budget unnecessarily. reply KronisLV 22 hours agorootparent> innovation tokens / weirdness budget I find it amusing that you called it \"weirdness budget\", but then again that pretty accurately describes the feeling I get when I see someone using a fairly niche DB instead of something like PostgreSQL, or something like NixOS instead of a regular Ubuntu LTS or RHEL-like. Not that it's a bad thing, there's plenty of specific use cases out there for sure. reply JoshTriplett 20 hours agorootparentExactly. Spend your weirdness budget wisely, for the things that are really important to differ on. It's fine to spend it, but spend it where you're getting substantial value in exchange for it. reply swed420 20 hours agorootparentprevReminds me of this from a comment yesterday https://boringtechnology.club/ reply KronisLV 18 hours agorootparentYep, that's what the innovation tokens are probably a reference to, the talk occasionally gets brought up on the site. Such a cool talk, I agree with most of what's said there, albeit sometimes even certain \"boring\" technologies might have a bunch of complexity to them. reply drewdevault 22 hours agorootparentprevCodeberg was chosen over other candidates because it has a workflow similar to GitHub, to ease the transition for the existing community. In my opinion, we're going through a big shake-up anyway and there's no better time than now to consider changes like this. We did discuss moving it to GitHub or another platform entirely, but as a community we decided to stay on Codeberg. Changing the license was an absolutely essential requirement, and this is a crucial time to evaluate and commit to that change. As far as we're concerned, not being copyleft was a bug that was exploited by Redis Ltd, and a fork which doesn't fix that bug isn't addressing the underlying problem. reply GrumpySloth 20 hours agorootparent> As far as we're concerned, not being copyleft was a bug that was exploited by Redis Ltd Disclaimer: I don’t have anything against the relicensing to LGPL. I think it’s your right and I root for you. That said, correct me if I’m wrong, but, as far as I understand, what Redis Ltd did, they could do regardless of the license. Copyleft wouldn’t have stopped them, given the CLA. Moreover I wouldn’t call that exploitation. To people outside of Redis Ltd who don’t want to be Redis Ltd customers this move is indistinguishable from them just closing down business and stopping development of Redis. Would that be exploitation? Are they obliged to provide free work on Redis indefinitely? They can’t retroactively change the licence of previous versions of Redis, so they can’t actually take anything away. The existence of the 2 forks is proof of that. reply drewdevault 20 hours agorootparent>That said, correct me if I’m wrong, but, as far as I understand, what Redis Ltd did, they could do regardless of the license. Copyleft wouldn’t have stopped them, given the CLA. Redis never had a CLA and Redis Ltd does not hold the copyright for the work, it's held in aggregate by all contributors. Redis Ltd did use a CLA for their products surrounding Redis, like RedisJSON, but Redis itself did not use a CLA. >To people outside of Redis Ltd who don’t want to be Redis Ltd customers this move is indistinguishable from them just closing down business and stopping development of Redis. Redis Ltd was only ever responsible for about 20% of the development of Redis. If they wanted to shut down operations in good faith they would just hand it over to the other 80% to manage. Instead they used their trademark to try and do a hostile takeover of the IP. reply 8organicbits 22 hours agoparentprevThree. KeyDB forked before the recent shake-up. https://github.com/Snapchat/KeyDB reply fmajid 21 hours agorootparentKeyDB is great, with major performance improvements, but it has also diverged from Redis and lacks most of the newer features added to Redis since the fork. reply dewey 23 hours agoparentprevThis is directly addressed in the blog post: https://redict.io/posts/2024-04-03-redict-7.3.0-released/#wh... reply yurytom 18 hours agoparentprevWe will basically see who wins the race and gets adopted the most. reply gigatexal 20 hours agoprevJust to be 100% I can still use Redis for free in my projects in production so long as I don’t sell a hosted version of it right given this new Redis license? reply dartos 20 hours agoparentTo be 100% you should email redis and/or ask a lawyer. Not randos on HN reply Y-bar 20 hours agoparentprevThat is the stated intent and spirit of the new license terms, but as others have said, ask a lawyer to be 100% certain. reply drewdevault 20 hours agoparentprevI agree with dartos, but I will state for the record that you can use Redict for free in production even if you do sell a hosted version of it. reply caymanjim 19 hours agoprevWhat's the track record of other projects that have gone too commercial and had their code forked like this? The only other example I can think of offhand is MySQL and MariaDB. I don't know what the market share of either is now. Do people still use MySQL? Does it generate profit for Oracle? I think Redis Ltd. is vastly overestimating the value of their product. Redis is incredibly popular, but the vast majority of users are just looking for a simple in-memory key-value store for lightweight database use cases, caching, etc. I've used it in some way in just about all my projects for the past ~15 years. The thing is, I don't care that it's Redis. I don't care about most of its features. I could have subbed in memcached or any number of other solutions. It would have been trivial and had no impact on my system. I have no doubt that there are some power users who need advanced features of Redis, but I also have no doubt that Redict will be better, and that there will be companies who provide commercial support for it. I'm just going to use Amazon ElastiCache for big projects, and continue not caring at all about what it is behind the scenes. And I'll s/redis/redict in my docker-compose.yml for small/personal projects, and that'll be the end of it. I can't imagine a scenario where Redis Ltd. is relevant or profitable 10 years from now. Oracle can afford to lose money on MySQL forever, and treat it as a loss-leader for acquiring new Oracle DB customers or at least new MySQL service contracts. Redis Ltd. has one product and few people need support for it or care much about it vs. alternatives. Edit: Or Valkey instead of Redict; either way, which exemplifies the degree to which I don't care. reply evanelias 18 hours agoparentMySQL never changed its license. The fork happened due to concerns of ownership and direction, not a license change or \"going too commercial\". MySQL is still widely used, including by a large portion of publicly-traded tech companies. That said, most newer startups seem to be choosing Postgres instead. MariaDB is also somewhat widely used, but not nearly as much as MySQL. And MariaDB's commercial enterprise (which was VC-backed and went public via a SPAC) is not doing well. reply drewdevault 17 hours agorootparent>MariaDB is also somewhat widely used, but not nearly as much as MySQL. I believe this is factually false. Consider just one datapoint: https://repology.org/project/mysql/versions https://repology.org/project/mariadb/versions reply evanelias 16 hours agorootparentYour belief is incorrect. The links you have provided do not provide any data on actual use of these databases in the industry. I've been working in the MySQL/MariaDB ecosystem for 21 years and am the creator/maintainer of a MySQL and MariaDB specific schema management utility used by hundreds of companies and with over 1.6 million downloads to date. I can tell you conclusively, MySQL usage in the industry significantly exceeds MariaDB's. While I personally enjoy both systems and do hope MariaDB adoption increases, this doesn't change the facts on the ground. And unfortunately MRDB is a penny stock, Azure is dropping their managed MariaDB offering, Vitess has dropped MariaDB support entirely, AWS Aurora is compatible with MySQL and not MariaDB, Percona Server is based on MySQL and not MariaDB, and so forth. reply drewdevault 16 hours agorootparentI'll defer to your on the ground expertise as to the relative popularity of each, but I will point out that we're both working with anecdata here. I might also suggest that MySQL is losing a lot of ground to Postgres and I don't expect that trend to reverse in any case. In any case it's not very relevant to this thread because you're quite right in pointing out that MySQL uses a free software license (GPL). reply evanelias 16 hours agorootparentYes, I mentioned the losing ground to Postgres aspect in my original comment. But that affects both MySQL and MariaDB, and still does not cause MariaDB to somehow be more popular than MySQL. As for both working with anecdata, I mean sure, but what else is there? I'm citing my own business's direct experience with MySQL users and customers significantly outnumbering MariaDB users and customers, and seeing clear signs of that trend also being true at several much larger businesses (AWS, Azure, Percona, PlanetScale). I suspect your view may be skewed by Linux distros / package managers having replaced MySQL with MariaDB many years ago. But even that is starting to change, see https://lwn.net/Articles/960630/ for example. reply drewdevault 19 hours agoparentprevJust passing by with a quick nit to pick: >And I'll s/redis/redict in my docker-compose.yml for small/personal projects, and that'll be the end of it FYI we're publishing to registry.redict.io, so s:redis:registry.redict.io/redict:g https://redict.io/docs/redis-compat/containers/ Not that it detracts from your point in any way :) reply zvr 18 hours agorootparentTHANK YOU, for making images based on scratch, rather than Debian or Alpine. It makes redistribution so much easier from a license compliance perspective. And double thanks for providing an SPDX document for the contents of the image. reply drewdevault 18 hours agorootparentNo problem :) reply kerkeslager 20 hours agoprevThis is all fine and good, but the big question for me personally is when we can expect to see cloud providers (DigitalOcean and AWS are the ones I'm using) provide hosted versions of Redict OR Valkey with some sort of upgrade path from Redis. I'm a good full-stack developer and a mediocre server administrator, so self-managing hosting is usually not something I'd prefer to do. reply hiccuphippo 19 hours agoparentAWS has ElastiCache, which is compatible with current redis, but I wonder in which direction they'll go. reply qwertox 22 hours agoprevI mostly use Redis in combination with RedisJSON, and RedisInsight is a nice way to check what data is stored. I'm only using it for a handful of small documents which mirror the state of some devices. These options (Redict, Valkey) don't seem to support JSON as a data type, so I'd like to know if there is some server specifically made for dealing with JSON documents. Something like a very lightweight MongoDB server which can be managed via a browser and where the data can be inserted/updated/removed via HTTP calls. reply cacois 22 hours agoparentLightweight is your problem here, I think, but I've used CouchDB successfully in similar situations. However, its not in-memory like Redis is. reply nurple 19 hours agorootparentBig fan of couch, wish it was more popular. Its http interface basically removes the whole http translation layer of code most projects put in front of the db. For such a small need, you might also look at PouchDB. Inspired by couch but simplified to allow it to run in-browser. reply drewdevault 22 hours agoparentprevRedict is binary compatible with Redis Modules, including RedisJSON, out of the box, so you can keep using it no problem. reply sgerenser 23 hours agoprevHopefully they don’t get legal pressure from Redis Labs over name similarity. Didn’t OpenTF have to change to OpenTofu for that reason? reply lolinder 20 hours agoparentThe difference is that TF is a widely-used acronym for Terraform, while Redict is a distinct word. That doesn't mean Redis won't try to put pressure, but it does mean it's not as obvious that they'd win if it came down to it. reply rmbyrro 22 hours agoprevI respect their choice of license. Totally agree they shouldn't let Redi$ take their work after what Redi$ have done. But still let any kind of project use it, including cloud vendors. Downside is that Valkey won't be able to use Redict code, though. reply sitkack 19 hours agoprevI don't see much info on the governance of the project. What is the stance on incorporating Rust into the codebase? reply drewdevault 19 hours agoparentIt's essentially a do-ocracy in practice, though we have discussed the possibility of putting together a foundation to steward it, particularly if we start to get money involved. There are currently five people who have admin rights with respect to their various competencies, and anyone who establishes trust with the community and gets stuff done will also be promoted to their level of competence, as it were. Though I hesitate to describe it like this, I think of \"admin\" more as a clerical role than an authoritarian one. You're good at code review? Everyone generally trusts you? Then merge rights are just a tool to help you do your work better. I don't think anyone is going to be very excited about introducing Rust unless there's a compelling reason to, but feel free to bring it up on the issue tracker for discussion and see if you can form a consensus on the matter with the rest of the community. reply sitkack 19 hours agorootparentGreat responses! I wish you the best of luck and when I am able to be involved with OSS again, I will gladly help out with Redict. I think the LGPL is a good choice. Areas where I would personally want to see Rust in a project like this, a) parsing and talking with the network b) extension mechanism moved to Wasm (Wasmtime for execution) but that plugins would be moved into a Wasm container. It would also be a nice property if the core of the project maintained a compile to Wasm compatibility so that Redict could be run everywhere. reply drewdevault 18 hours agorootparentThanks! It would be great to have your help. I think your Rust/wasm goals are a little bit dramatic for the goals we established among ourselves, but by all means start a conversation about it. Good support for compile-to-wasm is probably something that we'd be down to have upstream, though. reply gadders 23 hours agoprevI don't get the point of this - people are upset that Redis is trying to make money from companies like AWS and Google? reply prmoustache 23 hours agoparentPeople are upset because Redis is not open source anymore. That is all. reply gadders 23 hours agorootparentIs this like a Richard Stallman ideological thing? (EDIT: Genuine question - I'm trying to understand if this is a license purity issue or something else). reply Macha 22 hours agorootparentPeople like having choice. The SSPL provides less choice than open source licenses. It came about because users were utilising their choice in way the project leaders didn't like, because they didn't directly financially benefit from it. But without that choice, arguably Redis would not have reached where it is today. If it had launched as some proprietary single-vendor cloud service, people would have kept using memcached, or their relational DB or whatever for a lot of Redis use cases where it might be nice, but not essential improvements over the competition. So it's shutting the door behind them, accusing the users of taking advantage of the very thing that let Redis get to where they are today. (Especially so for Redis, where Redis Labs started as just another hosted provider unassociated with the open source projects) reply wink 21 hours agorootparent> But without that choice, arguably Redis would not have reached where it is today. I'd honestly love to know how many people actually use any of the features added to Redis in the last... 5 or 8 years. I'm not saying they were useless, but Redis used to be a pretty fine piece of software that could have easily been done for many use cases, 10 years ago. reply prmoustache 22 hours agorootparentprevWhat is commonly known as open source nowadays is a license that follows the definition adopted by the Open Source Initiative. SSPL is considered as not approved because it encumber unrelated programs to the one licensed by the SSPL: https://web.archive.org/web/20230411163802/https://lists.ope... So it is a combination of idealogical issue as well as being an annoyance to people who adopted it because it was released under the BSD in the first place. reply nurple 19 hours agorootparentprevFor some people, yes. But for the majority, I'd say no. In my mind a lot of the outrage is just generated through FUD that the big corps create when their ability to place themselves in a position to create false scarcity (and hence \"value\") is threatened. A big clue to these types of people are if they say anything about money or profit: e.g. \"OSS devs need to make money too\" For the RMS believers, OSS is a more fundamental attempt to change mankind at a time before the greedy asshats could capture and restrict things. The birth of the electronic age, and software in particular was viewed as a golden opportunity to capture the value we created for EVERYONE. This is a HUGE reason you see so many OSS devs that will work thanklessly on code for years or decades for no pay, they are the doctors-without-borders of the tech world, they really give a shit about freeing humanity from usury and corporate value capture. It's been really interesting to watch as the internet was captured, in a space where the cost of reproduction is literally zero, they've still been incredibly successful in strategically shunting a lion's share of the value for themselves where they then proceed with leveraging the artificial scarcity to capture that value monetarily. Considering this in the face of what we've been taught about today's capitalist society, owning the means of production is really only a small part of the greedy antisocial playbook of those who market in false scarcity. Don't think for a second that this isn't an ideological war, one that will be fought with all the information weapons at the disposal of those who stand to lose in a free and open society. reply yau8edq12i 22 hours agorootparentprevYes, it's an ideological type of thing. Meanwhile, most of the same people don't mind using closed standards like HDMI or kinda proprietary software like Android. reply diggan 21 hours agorootparentI'm sure there are plenty of people who do mind using HDMI and/or Android but in lack of other realistic alternatives for one or more reasons, end up with the pragmatic choice of using those things anyways. reply prmoustache 21 hours agorootparentprevI don't think your snarky remark about hdmi or android is pertinent. Regardless is stuff is open or proprietary, nobody like when the terms of a contract change without their consent. People/companies have adopted redis under a specific license, which really is kind of a binding contract, then one day under a new release the terms have changed making it incompatible with their intended use. It is only natural that an alternative, and in this case a fork appears. reply kryptiskt 23 hours agoparentprevRedis Labs didn't start Redis, they didn't contribute most of the code. They just own the trademark. They have about as much right to extract money from AWS and Google for Redis as I have, all they are doing is that they are hijacking an open source project to make themselves rich. They're not a victim of the cloud providers, they are a leech trying to make a score while fucking over all the other contributors to Redis, who have done 80% of the work they are trying to extract a ransom for. reply dkdbejwi383 22 hours agorootparentHmm, so anyone can just start a company with the name of an open source project and try to monetise it? Like someone could start e.g. \"Rust Labs\" and sell a commercial version of Rust? I don't know much about the genesis of Redis or Redis Labs, who key people and dates are, etc. I guess this obfuscation is part of the problem. reply Macha 22 hours agorootparent> Like someone could start e.g. \"Rust Labs\" and sell a commercial version of Rust? No, because the Rust trademark guidelines prohibit that (https://foundation.rust-lang.org/policies/logo-policy-and-me...) In Redis Labs case, they acquired the Redis trademark from the original author, who they employed for a few years after the project was well established. reply M2Ys4U 22 hours agorootparentprevRedis Labs acquired the trademark from the original owner. Nobody could start \"Rust Labs\" without the agreement of the Rust Foundation, because they own the Rust trademark. reply prionassembly 21 hours agorootparentprevThere's some point in which good faith rules apply, probably even in court (although not decisively). Presumably Redis Labs works with the developer community and, critically, promotes the technology, which adds great value in terms of network effects. This is sort of the situation with Mozilla.org/com, right? (Say you like something like Elm -- wouldn't it be better to have a relatively closely aligned commercial entity that puts significant and effective effort in making it widely used, which in turns makes it easier to find an Elm job or sell Elm-like solutions as a consultant). reply bheadmaster 22 hours agorootparentprevNot really. As mentioned in the article, Redis has a Contributor License Agreement [0] that you have to sign if you contribute to Redis codebase, which basically gives the company behind it ownership behind everyone's contributions: You grant to Redis and to the recipients of the software distributed by Redis a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare derivative works of, publicly display, publicly perform, sublicense, and distribute Your Contribution and such derivative works. Usually, projects don't have such agreements. E.g. Linux, the kernel, would have a hard time re-licensing under different license, because of how many people actually hold the copyright to the code they contributed, and would have to agree beforehand. [0] https://github.com/redis/redis/blob/unstable/CONTRIBUTING.md reply Macha 22 hours agorootparentThe CLA is sort of a red herring in the case of permissively licensed software though. It becomes relevant for the SSPL case, where Redis don't want to be bound by the same rules as others. Like since Rust is MIT licensed, you could make a closed source fork of Rust. The trademark guidelines would prevent you calling it Rust or anything too close, but you could describe it as Rust(TM) compatible and any of the other legally permitted uses of other people's trademarks. reply fmajid 17 hours agorootparentprevThere's some context in the comments to: http://antirez.com/news/121 reply 8organicbits 21 hours agorootparentprevYou can sell a commercial version of Rust (MIT, BSD, and Apache licensed), but you'd need to change the name for trademark reasons. reply kerkeslager 19 hours agorootparentprev> Hmm, so anyone can just start a company with the name of an open source project and try to monetise it? Like someone could start e.g. \"Rust Labs\" and sell a commercial version of Rust? I'm not a lawyer, so take the following with that grain of salt. In the specific case of Rust, no, because as another user pointed out, their licensing prohibits it. If my understanding of the licenses is correct, the X-11, BSD 3-clause and BSD 4-clause licenses also prohibit this. The MIT, BSD 2-clause and ISC licenses don't appear to prohibit this. Your post mentions a few issues which I believe are legally separate: 1. Naming your company after an open-source project. I believe this is perfectly legal under the latter listed licenses, and happens in practice (for example, a brief search yields that React is MIT-licensed, and \"React Labs\" is a company). 2. Selling a commercial version of an open-source project. This is legal, and in fact a license isn't considered open source by OSI or free software by FSF if it disallows selling a commercial version. Whether this will be profitable is a separate question--generally people won't be willing to pay for something if they can just get it for free. There are two ways around this that I can think of: a) providing services and development around the open-source project, and b) extending the open source project with closed source code. The latter business model is prohibited by copyleft--you can only sell closed-source extensions to copyleft software if you have rights to the copyright (i.e. you created the code yourself) so attempting to do this with an existing copyleft-licensed project would be prohibited. 3. Enforcing trademark on the name of an open source project. My understanding is that enforcing a trademark created after a open source project started using it isn't possible, not because of licensing terms, but because of \"priority of use\" or \"first to use in commerce\"[1]. That is, if an open source project \"Foo\" exists already, I can't create \"Foo Labs\" and then sue the Foo project for using my name--on the contrary, the Foo project could probably sue Foo Labs. Redis Labs avoids this liability because they obtained rights for the trademark from the original Redis developer (I'm not sure what terms they obtained rights to the Redis name under--if they have exclusive rights they could sue anyone using the Redis name, but contribution to the project over time would likely make this complicated). There's a separate issue which is that \"Foo\" and \"Foo Labs\" are arguably different trademarks--Foo Labs can't inherently sue anyone using the Foo name, but they could likely sue someone who started a \"Foo Labs\" if they were the first ones to trade under that name. [1] https://www.avvo.com/legal-answers/does-prior-art-apply-to-t... reply gadders 22 hours agorootparentprevThis makes more sense as an explanation. Are there any instances where an SSPL license or similar is warranted? reply Macha 22 hours agorootparentIf you want to release your project for the first time as SSPL, go right ahead. You may have difficulty getting adoption however. It's the \"build your userbase on open source, then lock them in\" that bothers people. reply KingOfCoders 23 hours agoparentprevThere is open source and there is non open source. Both is fine. Open source does not make a difference who uses it. reply Y-bar 19 hours agorootparentThat’s unfortunate. I have a clause forbidding Anish Kapoor from forking or using my code. reply diggan 23 hours agoparentprevSlightly disingenuous question, but I'll bite anyways... No, people are not upset Redis is trying to make money. People are upset that something that used to be FOSS is no longer FOSS, and are trying to protect themselves from future pain by doing something that is very common in FOSS, which is forking a project. Just because someone is trying to make sure the software they rely on continues to be FOSS, doesn't mean that they are out to actively hurt the original creator(s) of the original project. I don't know how you could possibly read the situation like that either. reply drewdevault 23 hours agoparentprevSubstantial portions of Redis were written by AWS and Google. People are upset because Redis is a collaborative project with many people working on it and Redis Ltd wants the sole right to commercialize the work of an entire community. LWN has a good overview: https://lwn.net/SubscriberLink/966631/8bc9d155d4e2afb3/ Redis Ltd is only responsible for about 20% of the work. reply gadders 23 hours agorootparent>>Substantial portions of Redis were written by AWS and Google Employees on their own time, or paid to do it? So I'm clear though - if I wrote a SAAS product tomorrow that under the covers used Redis, I'm OK, but if I spun up a bunch of servers and offered managed Redis as a product, I would need to pay? reply xorcist 20 hours agorootparentThat's what many people seem to think, but it has not been legally challenged yet. We won't know for sure until Google or AWS decides to test the limits, in your jurisdiction. You should probably ask Redis, Inc. what their intention is. Keep in mind, though, the run the risk of being re-rug-pulled. They have changed the license once and they can do it again. reply drewdevault 22 hours agorootparentprev>So I'm clear though - if I wrote a SAAS product tomorrow that under the covers used Redis, I'm OK, but if I spun up a bunch of servers and offered managed Redis as a product, I would need to pay? That's the pitch of the license, essentially, but in practice the SSPL is an utter nightmare to comply with (to the point of being deliberate) and it mainly exists to put a veneer of open source on a proprietary product. In actual practice you really just cannot offer Redis as a service unless you are Redis Ltd, which is the actual point of the license in the end. reply umanwizard 22 hours agorootparent> In actual practice you really just cannot offer Redis as a service unless you are Redis Ltd Isn’t this exactly what the person you’re replying to was saying? reply drewdevault 22 hours agorootparentYes, I realize I worded this poorly. I emphasized this point because this is what makes it non-free/not meet the Open Source Definition; you cannot retain exclusive right to commercialization. But I realize that was not actually something the parent commenter was asking about. reply sotillan 23 hours agorootparentprevYeah, I think a key point is that Salvatore Sanfilippo, the original developer, maintainer and principle contributor[1] for 11 years, was not a founder of Redis Labs. He joined as an employee in 2015 but resigned in 2020. Perhaps he has equity in Redis Labs, but it's not clear he stands to profit at all from this switch. [1] https://github.com/redis/redis/graphs/contributors reply sanxiyn 23 hours agorootparentprevAWS and Google contributed those under BSD license, being fully informed that Redis Ltd can commercialize like this. It makes perfect sense for AWS and Google to fork immediately after the switch, but for contributions before the switch, there is no basis for AWS and Google to complain Redis Ltd, either legally or morally. reply bunnyfoofoo 22 hours agorootparentLegally, they can't complain, but morally it is icky. Redis is a leech. reply sanxiyn 22 hours agorootparentIf you don't want to be leeched, don't contribute to BSD licensed projects. reply baq 23 hours agorootparentprevPerhaps true but besides the point, actually - the same companies will happily take your code and make a saas offering out of it on their oligopolistic infrastructures, making it economically unviable to compete with. ...which, given LGPL, will be worse now as they'll simply not share their modifications because that's the legally safest option. reply KingOfCoders 23 hours agorootparentAnd well, why not? That is the idea. It's open source. reply soygem 21 hours agoprevfix hare reply endisneigh 21 hours agoprevI read the post and it’s not clear why it’s not MIT licensed. Why not allow attempts to “create proprietary distributions?” That’s what open source would allow, no? I honestly do not see this as being different than Redis. Do BSD or MIT and be done with it. It seems needlessly ideological. Everyone wants to call their stuff open source but have strings attached. reply wyldfire 21 hours agoparent> Everyone wants to call their stuff open source but have strings attached. The term 'Open Source' is well-defined and includes licenses like the one redict picked (LGPL 3.0). It is incorrect to try and lump this in with source-available licenses that are IMO anathema to Open Source. reply endisneigh 21 hours agorootparentThe OSI doesn’t represent everyone. It’s simply a fact that MIT for instance allows things like Redict and this discussion to take place, as well as other things. It is maximally permissive. reply xorcist 20 hours agorootparentThey don't, but they were established for a singular reason, to shepherd the term \"open source\". A term that was chosen on terms of having little previous use. They did not succeed with the trademark application because the term was deemed too descriptive, not from a lack of trying. So we should probably respect their term if we want to use it. There are plenty of other terms you can choose instead, such as \"permissively licensed software\" which also has a de facto established meaning. reply wyldfire 20 hours agorootparentprev> The OSI doesn’t represent everyone. Sure, sure. But the term Open Source has a meaning. Ultimately terms have meaning based on their usage in the language. For 'Open Source', its usage happens to reflect the same meaning enshrined by OSI. > It is maximally permissive. It appears that there are now several redis forks, with several licenses. For the most part: everybody wins. If you prefer the MIT license, maybe you would prefer to contribute to the BSD-licensed Valkey. And if not, you can fork redis too. reply falcor84 21 hours agoparentprevAs another comment mentioned, copyleft would prevent commits to this project from being merged into Redis, intentionally making this a \"hard fork\". I do think it's ideological, but it's doesn't seem needless to me - there is a real ideological battle to be fought here. reply endisneigh 20 hours agorootparentI don’t understand why that is a bad thing. Perhaps a closed source fork will end up being superior through incentives only available through closing the source. Humanity still can use an open source fork. More options must be superior, no? reply yjftsjthsd-h 21 hours agoparentprev> That’s what open source would allow, no? That's what a permissive license would allow, which is a subset of Open Source. > I honestly do not see this as being different than Redis. Redis doesn't give you the 4 freedoms; in point, \"The freedom to run the program for any purpose\". > Everyone wants to call their stuff open source but have strings attached. This literally is open source. reply skywhopper 21 hours agoparentprevDo you feel the same way about Linux and Git? Anyway, the Redict team are doing their thing and letting Redis and Valkey be. You’re the one insisting on Redict doing things the way you would like, so who exactly is being ideological? reply endisneigh 21 hours agorootparentIsn’t Linux GPL? What’s the relevance? Edit: Linux was always GPL, Redis was not, so I don’t see the relevance. reply ksec 21 hours agorootparentBecause \"Everyone wants to call their stuff open source but have strings attached\" implies GPL ( or CopyLeft, Non-BSD / MIT license ) have strings attached. So the parent was asking isn't Linux Open Source but with strings attached? And if so are you happy with Linux? Although I am assume what you meant was that Redis was originally a BSD / MIT, and re-licensing it to LGPL seems ideological. But I could be wrong. reply endisneigh 21 hours agorootparentCopyleft is a string. The default would be complete permissiveness. If someone gave you something the correct presumption would be that you can do as you like with it, unless - and what follows are strings, like copy left. It isn’t inherently bad, but it is what it is. My happiness with Linux is irrelevant because Linux to my knowledge was not re licensed to be more restrictive. reply ksec 20 hours agorootparentLet's just recap >\"I read the post and it’s not clear why it’s not MIT licensed. Why not allow attempts to “create proprietary distributions?” That’s what open source would allow, no? \" Your \"That's what Open Source would allow\", clearly implies anything but MIT / BSD license are not Open Source. Hence why people ask about Linux, which is Open Source but copy left aka non MIT / BSD. >Edit: Linux was always GPL, Redis was not, so I don’t see the relevance. Your first sentence implies GPL are not open source, the relevance here is why the parent ask \"do you think Linux as Open Source\" or happy with it. And: >My happiness with Linux is irrelevant because Linux to my knowledge was not re licensed to be more restrictive. Your happiness with Linux contradict with the first \"quoted\" sentence to what a lot of people think you meant. Just in case, I am on the BSD / MIT camp but what you wrote create a lot of confusion. reply drewdevault 21 hours agorootparentprev> Copyleft is a string. The default would be complete permissiveness. I have answered a similar line of questioning before. Arguing strongly for permissive licenses is arguing for a kind of passive freedom which presents as freedom from obligations. Copyleft is the sort of active freedom which presents as a guarantee of rights. reply ksec 20 hours agorootparentThis is off topic. I am going to assume this isn't impersonation, but Drew what happen to your old HN account? ( I had it on RSS feed and hasn't shown any update for ages I thought you left HN ) reply drewdevault 20 hours agorootparentI don't see any reason why this is off-topic. Naturally the license change is a distinguishing feature of Redict and a reasonable subject for discussion. I lost the password to my old account. reply ksec 8 hours agorootparent>I don't see any reason why this is off-topic. Naturally the license change is a distinguishing feature of Redict and a reasonable subject for discussion. I meant off topic as in asking you question about your account. Not the Open Source discussions which is of course perfectly valid. Sorry for the confusion. reply endisneigh 21 hours agorootparentprevI disagree - Redict is only possible because of the permissive license to begin with. A hypothetical license that said no forks allowed wouldn’t allow Redict, for instance. reply drewdevault 21 hours agorootparentA hypothetical license that said \"no forks allowed\" would not be an open source license. If Redis used the LGPL we could still fork it, we'd just have to use the LGPL for our fork; likewise anyone can fork Redict so long as they use the LGPL for their fork.* * Technically you do have more options than this with the LGPL but this is a layman's explanation. reply endisneigh 21 hours agorootparentI did not say it would be an open source license. My point is simply that Redict takes something more permissive and makes it restrictive, which is true. reply drewdevault 21 hours agorootparentI don't think that's true. It's only true if freedom == freedom from obligation, which is a naive view of freedom. But if freedom == guarantees of rights, then Redict is more free. This is how freedoms actually work in practice: freedom of the press is guaranteed by restricting the government from censorship, workers rights are guaranteed by restricting the freedom of businesses to exploit them, etc. In practice all freedoms require someone to give something up. Even with permissive licenses, you are giving up the sole right to your IP and the sole to commercialize your software. reply endisneigh 21 hours agorootparentWith all due respect I do not think that makes any sense. You’re creating this metaphor with the government, but this isn’t the government. In the end freedom is a spectrum and you are free to do more with MIT than LGPL. Even if someone forked MIT and made it closed source that does not in any way stop or restrict those who prefer the project pre-fork. Indeed, that is the very logic that has made Redict possible to begin with. This is the illustration of why MIT is maximally free. Suppose the EU said code must be closed source, and you must distribute encrypted binaries. In this hypothetical it wouldn’t be possible to use Redict at all. This is another example of why the MIT, and permissiveness in general, is superior. —- As an aside your freedom of the press example is yet another reason for permissiveness. The government isn’t permissive by default, which is why “freedom of the press” is even a thing. reply drewdevault 21 hours agorootparentGovernments, businesses, they're all just institutions and I see no reason not to use them as illustrative examples. The MIT maximally enables exploitation; copyleft maximally enables freedom. From the point of a proprietary fork of a permissively licensed project onwards, users of the proprietary fork enjoy fewer freedoms than before. You are advocating for freedoms for the few (business owners making proprietary forks of free software for profit) at the expense of the freedom of many (everyone else). Redict would also be possible if Redis used a copyleft license. The space for Redict to exist is not afforded to us because of the use of a permissive license in some way that Redict denies to anyone else through the use of a copyleft license. reply endisneigh 20 hours agorootparentI really don’t understand you. Even if there was a closed source fork, users would still be free to use the open source one. There is no restriction or diminishing of options. In fact, now there are even more options and thus more freedom. In any case we can agree to disagree. reply drewdevault 20 hours agorootparentI think you're missing the reciprocal nature of the arrangement here. Since you seem to assert that it's acceptable to release software under a proprietary license (and I agree with you, most of the time), you must believe that the authors of a work have a right to distribute it under whatever terms they like. So: is it appropriate for someone to say \"I am willing to offer my labor to this collaborative project, and release the source code for free, and allow people to build from my work, under the condition that they extend the same rights back to everyone else\"? It's much more generous than what proprietary software offers, after all. Why is copyleft subject to more scrutiny than proprietary software for you? You suggest that permissive licenses are better because they allow permissive derivatives of the software, which suggests that permissive derivatives are good. But copyleft derivatives are... not good? Copyleft offers more freedoms than proprietary software. Moreover, this line of reasoning completely disregards the social contract. Many people worked on, used, popularized Redis under the presumption that they were participating in a collaborative effort from which all participants would benefit, only -- surprise -- now the trademark holder has changed the terms so that only they benefit from a product of which, by objective measures, 80% was made by the community. reply benterix 21 hours agorootparentprevYes, Linux and Git are GPL-ed, so the parent is asking if you also feel about them in the same way as about this Redis fork (\"needlessly ideological\", \"have strings attached\" etc.). reply forgotpwd16 21 hours agorootparentprevRedict is LGPL, a less restrictive GPL allowing linking to projects with other licenses. reply endisneigh 21 hours agorootparentYes, but Redis was even less restrictive to begin with. reply RUnconcerned 21 hours agorootparentprevThe GPL has more strings attached than the LGPL. reply treprinum 22 hours agoprev [–] Why would any startup ever get idealistic again and release their product under open license when big boys can just fork it and destroy their business? I think the dual AGPL/commerical licensing will be the choice of anyone with still some idealism left. reply marcinzm 22 hours agoparentThere's nothing idealistic involved. Startups want users more than they want revenue. Thus they open source it, use VC money to cover the loses and then eventually try to squeeze those users for money. Redis Labs also did not in any way make Redis. They came in later to exploit the already open source project for their own benefit. reply willvarfar 21 hours agorootparentYeap it's interesting. Redis starts as a hobby project by Salvatore Sanfilippo (aka antirez) who eventually gets sponsored by VMWare as the project grows in popularity. Then, another company that is offering a hosted Redis and support hires antirez and so becomes the 'offical' Redis company. In 2020 antirez leaves and goes and writes a novel (called \"Wohpe\") instead. https://en.wikipedia.org/wiki/Redis_(company)#History Antirez hasn't been involved in Redis in a long time. This is a common enough pattern when open-source projects leave their roots and then, eventually, alienate theiropen source base. Perhaps the time is ripe for Antirez to come back and shepherd one of the forks, a bit like mariadb? reply treprinum 22 hours agorootparentprevOk, I was mistaken then. I thought Redis Labs folks created Redis initially. reply fmajid 21 hours agorootparentNo, they didn't, although they misleadingly claimed to be the \"Home of Redis\" for a number of years. Then they hired Salvatore Sanfilippo (antirez), the author of Redis, and eventually purchased the copyright from him. Very sleazy outfit that fully deserves all the scorn poured on them. reply danielovichdk 20 hours agorootparentIf one sells a copyright to another, how can that be considered sleazy ? Someone made the sell - to make money I presume - and another bought it, to perhaps make more money ? Seems not that out of reach ? I don't understand the bitterness around this debate, it seems most people are frustated that they have to pay for something that was free, but at the same time, they probably made money from what was free. I totally understand, with todays mess of maintainers not getting paid in full, that at some point, someone wants money for their work. Be it a person, a company or you doesn't matter. But right has to be right. I think we will see a lot of this the next 10 years, simply because we cannot afford maintainers not detecting backdoors in a code-review from some bad actor. reply fmajid 17 hours agorootparentThe sleaze was in falsely claiming to be the \"Home of Redis\" at least 2 years before actually becoming so by hiring antirez (I remember seeing their huge billboards on Bryant & 9th in San Francisco, and it rankled). reply drewdevault 19 hours agorootparentprevTo clarify, they did not purchase the copyright -- they purchased the trademark, and are using it to attempt to capture the practical value of the copyright. The copyright is by far the more valuable of the two, but they do not actually own it. Of the actual valuable object (the copyright, i.e. the codebase), about 80% was written by people outside of the employ of Redis Labs né Redis Ltd. reply fmajid 17 hours agorootparentAre you sure? I'd asked antirez to clarify 6 years ago, and he said he had transferred the Redis IP to Redis Labs, without being more specific (see the comments on http://antirez.com/news/121 ) In any case, congratulations on the very fast release of Redict, rewriting references to Redis is not a simple s/Redis/Redict/g sed job. How do you feel about possible competition with the Valkey project, apart from the licensing differences? reply drewdevault 16 hours agorootparent>Are you sure? I'd asked antirez to clarify 6 years ago, and he said he had transferred the Redis IP to Redis Labs, without being more specific Well, trademarks are a form of IP. But in any case the copyright was never antirez's to transfer to anyone. In the absence of a CLA with a copyright assignment, every contributor retains the copyright to their contributions, and licenses them to everyone else (Redis Ltd included) under the terms of the BSD license. Legally speaking, Redis Ltd's SSPL code is, in effect, a fork of the Redis BSD code, in the same way that Redict is. >In any case, congratulations on the very fast release of Redict, rewriting references to Redis is not a simple s/Redis/Redict/g sed job. How do you feel about possible competition with the Valkey project, apart from the licensing differences? Thanks for the congratulations! As for Valkey, so far they've put a lot of time and energy getting the various corporate stakeholders on board with their fork and getting some marketing out (which is no small feat, to be sure, I shudder to imagine the number of lawyers involved), but they still have a lot of boots-on-the-ground work to do getting their fork up and running so it may be a while before our projects are interacting directly. From the limited communication we've had with them, it seems likely that we'll be able to collaborate insofar as reducing incompatibilities is concerned, maybe co-maintenance on the protocol specification, but not much more. Redict does plan on pulling useful patches from Valkey once they get the code going, though it's unfortunately not possible for them to pull from us unless they're on board with switching to a copyleft license -- and we encourage them to do so :) reply tsimionescu 20 hours agoparentprevI don't know why people think AGPL changes anything here. The very first of this wave of moves to non-FOSS licenses, and the creation of the SSPL license we are discussing here, was Mongo moving away from the AGPL. What these companies want is licenses which prevent AWS and other cloud providers from competing with them on their specific technology, regardless of how much those same companies are contributing to the technology. Redis is the most extreme example here - by all accounts I've read, Amazon was a major contributor, not just throwing bug fixes here or there. But that doesn't help Redis Labs, they want money, not code. And the AGPL would have done less than nothing to help stop Amazon from running their own Redis service: Amazon was already doing everything (or at least most things) that the AGPL would have required of them. SSPL is just a figleaf. No one sane redistributes third party SSPL code without having a contract with the company, it's essentially proprietary in all but name. But it allows the company to maintain this air of open source legitimacy. reply ilc 20 hours agorootparentI don't see a fig leaf. All I see is dick. reply nurple 18 hours agorootparentprevMongo didn't move away from the AGPL to keep SaaS providers from capturing value from the project, they did it so that they could capture more for themselves. reply tsimionescu 17 hours agorootparentWell, I don't think they expected this change to increase the market, so they wanted to capture value from other providers to themselves. But even that was not guaranteed to happen, the only thing they can guarantee with this move is that others will capture less value. reply lolinder 20 hours agoparentprevThere are lots of loosely related forms of idealism, and plenty of projects will continue to be released under mainstream FOSS licenses because the ideals of the project don't include \"make money for our investors\" but do include \"make this cool thing widely available because we think people will like it\". For the vast majority of open source projects, being used by (and, importantly, receiving contributions from!) the \"big boys\" is a Good Thing and something to be aspired to. reply eloisant 22 hours agoparentprevOpen Source doesn't have to come from a startup with a business model based that one open source project. reply margorczynski 21 hours agoparentprevAnd that's a good thing as it will provide a steady mechanism to fund the project. Of course there is the risk that it will result in an \"open-core\" model where the open source part is artificially slowed down to promote the commercial offering. reply mattmanser 21 hours agoparentprev [–] They'll just put that clause in from the start. And if it grew organically, well OpenSearch is not exactly thriving. One of my clients are using it and it's a massive PITA to be on it. No idea how badly it affected enterprise revenue for ElasticSearch though, but ES is going to win that fight in the long term. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Redict 7.3.0 is now released, offering a stable version of a copyleft fork of Redis® OSS 7.2.4.1, emphasizing stability and community-driven development.",
      "Protected by the Lesser GNU General Public License, Redict aims to prevent exploitation and promote free software solutions in infrastructure.",
      "Planned updates involve modernizing the build system, forking the ecosystem, and improving documentation, welcoming community contributions to establish itself as a reliable alternative to Redis®."
    ],
    "commentSummary": [
      "Redict 7.3.0, a copyleft fork of Redis, is sparking community discussions on stability and long-term maintenance.",
      "Debates center on licensing practices, commercialization worries by Redis Labs, LGPL licenses, trademarks, contributors' rights, and the balance between open source principles and commercial concerns.",
      "Comparisons between open-source projects, licensing terms, and the evolving open source landscape are part of the conversation."
    ],
    "points": 191,
    "commentCount": 165,
    "retryCount": 0,
    "time": 1712139001
  },
  {
    "id": 39921964,
    "title": "MOnSter 6502: Transistor-Scale Replica of MOS 6502",
    "originLink": "https://monster6502.com/",
    "originBody": "The MOnSter 6502 A dis-integrated circuit project to make a complete, working transistor-scale replica of the classic MOS 6502 microprocessor. Video FAQ Sixty-Five-Oh-What? The 6502 is the famous processor found at the core of such influential computer systems as the Apple ][, the Commodore PET, the Atari 400 and 800 home video game consoles, the BBC Micro, and the Tamagotchi digital pet. Slight variations of it were found in the Commodore 64, the Atari 2600, and the original Nintendo Entertainment System. What's with the name and capitalization? MOnSter 6502 is a play on the original manufacturer and device name (MOS 6502) as well as acknowledging its large size. How big is it? It's a four layer circuit board, 12 × 15 inches, 0.1 inches thick, with surface mount components on both sides. How many components are there on the board? In total, there are 4769 components on the board. There are 3218 transistors and 1019 resistors that comprise the \"functional\" part of the 6502. In addition to these, there are also LEDs sprinkled throughout that indicate the values of various control lines, registers, and status bits, as well as additional transistors and resistors (not counted in those \"functional\" totals) that are necessary to drive those LEDs. As of the second revision of the board, the design statistics are as follows: Components that correspond 1:1 with transistors in the original 6502: Total active transistors: 4237 3218 enhancement mode n-channel MOSFETs 2588 discrete 630 located on 164 quad transistor array chips (26 of these 656 transistors are not used) 1019 resistors (the original 6502 used depletion mode MOSFETs in place of resistors) 998 Additional parts present only in the MOnSter 6502: 313 LEDs (The first revision board, shown in the video, has only 167 LEDs) 270 extra MOSFETs for driving the LEDs 20 filter capacitors 32 bus capacitors 2 zero-ohm jumpers for net tie reasons 8 current limit resistors 314 resistors for the LEDs 36 diodes for ESD protection 2 jacks for 5 V power: main and alternate location 1 rectangular connector for the 40-pin \"ICR\" ribbon cable Total parts: 4769 Are you nuts? Probably. What is the current status of the project? Headed towards a full public launch; delayed a bit by COVID. The first full-scale prototype was publicly demonstrated at the 2016 Bay Area Maker Faire. We brought it up to the stages of successfully running various programs in assembly, BASIC, and Forth. Our second prototype, a year later, fixed a few issues that we encountered when bringing up the first version (removing all of the patch wires) but also added over 100 additional LEDs, including one for every instruction decode line on the 6502's decode ROM. Since the 6502 is just a microprocessor (CPU), about half of our work on the project (and most of our work since the first prototype) has actually been building up the capabilities around this processor — adding interfaces for a keyboard, monitor, and programming — so that you can actually use it. In the process of doing so, we have built up a single-board computer (a tiny motherboard) that uses the 6502 as its CPU, such that you can either use a socketed (vintage) 6502 IC, or the MOnSter 6502 through a cable. More, we have been working towards a public launch, currently planned for mid-2023, building the MOnSter into a shadowbox enclosure complete with a tiny hidden \"motherboard\" that the CPU sits in, so that it can run demonstration programs even without being plugged into a separate computer. Does it run at the full speed of an original 6502 chip? No; it's relatively slow. The MOnSter 6502 runs at about 1/20th the speed of the original, thanks to the much larger capacitance of the design. The maximum reliable clock rate is around 50 kHz. The primary limit to the clock speed is the gate capacitance of the MOSFETs that we are using, which is much larger than the capacitance of the MOSFETs on an original 6502 die. Can you hook it up inside an Apple ][ and run Oregon Trail? No, not directly. It's neat to think of plugging the MOnSter 6502's in-circuit emulator (ICE) in-circuit replica (ICR) cable directly into a socket inside an Apple ][, but that wouldn't actually work. The Apple ][ design relies on a number of clever tricks that derive timing for video generation and peripheral control from the main clock signal — all of which will fail if you need to run at a slower speed. There are some ways to get around limitations like these. For example, the Replica I computer (an Apple I clone) uses a Parallax Propeller chip to emulate a system clock and some of the timing-dependent external processing. The RetroShield 6502 is another example of a software-implemented Apple I compatible that can use a hardware 6502 processor. We are most excited about the new SmartyKit Apple I replica, which is specifically designed to run at a slower clock speed. You should also be able to put a slower clock into Ben Eater's 6502 computer in order to \"plug in\" the MOnSter. So what can it do? It can act as an in-circuit hardware emulator for a 6502 integrated circuit, in any circuit that can run at a relatively low clock rate. We're currently running it on a custom 6502 development board (the single-board computer that we mentioned earlier) and doing things like running running BASIC, much as you would on an Apple ][. We have also plugged it into the CPU sockets of a few people's homebrew 6502 computers (including the Cactus and RetroShield 6502) that could run at variable clock speed. We are interested in finding other applications where it could be substituted for an original 6502. How long did it take? This has been a ~8 year project, thus far. The primary design work was done over six months, from July 3 to December 1, 2015. There have been several stages of reviews and revisions since then, and a few generations of the \"motherboards\" that we have run the MOnSter in. Is it truly a \"discrete 6502?\" Not in the strictest sense. However, it really depends upon how picky you would like to be. The MOnSter 6502 uses the original dynamic NMOS logic design, implemented at the individual transistor level. Dynamic NMOS requires a large number of \"transmission gate\" transistors that are used to switch currents. For various technical reasons, only a 4-terminal MOSFET can make an effective NMOS transmission gate. Unfortunately, individually packaged 4-terminal MOSFETs are no longer commercially available. However, they do still make arrays of 2 or 4 MOSFETs on a single chip with a separate substrate pin. We used the 4-pack version — These are the quad transistor array chips that we mentioned earlier. Because these transistors do share a pin, there are (strictly speaking) integrated circuits in the MOnSter 6502. However, one might credibly argue that it is a discrete transistor design since there are not (for example) any logic gate chips in the circuit. What are those little gold squares around the border? The 40 square pads around the edge of the circuit board correspond to the 40 pins of the original 6502 integrated circuit. They're also designed to look like the wire-bond contact pads on an IC die — where the IC would be physically connected inside (for example) a 40-pin DIP package. These pads are functional contact points on the MOnSter 6502; you can hook up to them with alligator clips to probe or communicate with any pin. How much power does it draw? It draws up to roughly 2 A at 5 V, or 10 W total. The LEDs are a significant part of the power consumption. Can I see it in person? Due to the ongoing pandemic we haven't been taking the MOnSter to shows lately, but you can come visit us by appointment at Evil Mad Scientist Laboratories in Sunnyvale California for a demonstration. Please use our contact form to ask about available demonstration times. How big is the MOnSter 6502 compared to the original 6502 die? The original device was 153 × 168 mils (3.9 × 4.3 mm) or an area of 16.6 square mm. Given that ours is 12 × 15 inches, that makes it about 7000 times actual size. How big would the MOnSter 6502 be if it were made with through-hole parts instead of surface mount parts? About 19 square feet (1.7 square meters). How big would a 68000 microprocessor be at the scale of the MOnSter 6502? Also about 19 square feet (1.7 square meters). Are you going to make a 68000 next? No. How big would a modern CPU be at this scale? The Apple A8X, found in the iPad Air 2, contains about 3 billion transistors. (This is comparable to the number of transistors in modern desktop computer CPUs as well.) At the scale of the MOnSter 6502, that would take about 885,000 square feet (over 20 acres or 8 hectares) — an area about 940 ft (286 m) square. Are you going to make a dis-integrated modern CPU next? No. Are you going to make one out of vacuum tubes next? No. Are you going to make one out of relays next? No. (Aren't we already doing enough to fight Moore's law?) Can I buy one? Hopefully in the near future! We are working towards a public launch, currently planned for Mid 2023; please Sign up for our mailing list if you would like to receive updates as we get further along. Is it expensive? It is definitely not cheap to make one of these. We are currently estimating the cost at between $2k - $4k. While the circuit board itself is large and a little bit expensive, the cost is actually dominated by the component and assembly costs of an extremely large number of tiny components, most of which are individually not that expensive. Add to that the setup and test costs of building complex things like these in small batches, and you can immediately see how it adds up. Is there going to be a soldering kit version of this? No. (But on the other hand, \"Anything is a soldering kit if you're brave enough!\") Credits The MOnSter 6502 is a continuing work in progress, designed by Eric Schlaepfer, in collaboration with Evil Mad Scientist Laboratories. Together, we're the folks that brought you the \"Three Fives\" discrete 555 timer, the XL741 discrete 741 op-amp, and the book Open Circuits. Special thanks to Ken Shirriff, visual6502.org, Chuck Peddle, and Bunnie Huang. Stay up to date! Please join our mailing list for occasional future updates.",
    "commentLink": "https://news.ycombinator.com/item?id=39921964",
    "commentBody": "MOnSter 6502: a working transistor-scale replica of the classic MOS 6502 (monster6502.com)181 points by harporoeder 14 hours agohidepastfavorite70 comments dang 12 hours agoRelated. Others? Complete working transistor-scale replica of the classic MOS6502 microprocessor - https://news.ycombinator.com/item?id=33841901 - Dec 2022 (38 comments) MOnSter 6502 - https://news.ycombinator.com/item?id=26507525 - March 2021 (31 comments) The MOnSter 6502: transistor-scale replica of classic MOS 6502 microprocessor - https://news.ycombinator.com/item?id=17969472 - Sept 2018 (81 comments) A working, transistor-scale replica of the MOS 6502 microprocessor - https://news.ycombinator.com/item?id=14386413 - May 2017 (44 comments) The MOnSter 6502 - https://news.ycombinator.com/item?id=11703596 - May 2016 (74 comments) reply lloeki 3 hours agoprev> Does it run at the full speed of an original 6502 chip? > No; it's relatively slow. The MOnSter 6502 runs at about 1/20th the speed of the original, thanks to the much larger capacitance of the design. The maximum reliable clock rate is around 50 kHz. The primary limit to the clock speed is the gate capacitance of the MOSFETs that we are using, which is much larger than the capacitance of the MOSFETs on an original 6502 die. Now I'm curious but in way over my head. Could the speed be improved by using MOSFETs with better capacitance? Can the full 1.023 Mhz be attained by throwing money at it or are there physical limitations at that scale? reply junon 3 hours agoparentI'm not an expert but a hobbyist. Take what I say with a grain of salt. > Could the speed be improved by using MOSFETs with better capacitance? Not using MOSFETs would make it faster, in theory. MOSFETs have a delay time and are considered slow. But it appears they want to be true to how the original was made. There definitely exist mosfets indistinguishable from dust - I made the mistake of designing with a few at one point. So probably, yes. Another thing I noticed is that the distance between the components on the board is quite high. They might have done that to replicate the original layout, or maybe I'm just not seeing it correctly. But everything - including the traces on the board - has some capacitance. > Can the full 1.023 Mhz be attained by throwing money at it or are there physical limitations at that scale? Hard to answer without measuring, though I'd imagine it's possible. 1MHz isn't that fast, but if you have high capacitances or they don't switch fast enough (e.g. MOSFETs) then your signals are going to get muddy. EDIT: to be clear, not ragging on the project. This stuff is beyond cool. reply skissane 31 minutes agorootparentThis is basically a second generation computer, whose heyday was in the first half of the 1960s – after vacuum tubes, but before the first integrated circuits. It makes me wonder, what transistor family would give you the best performance for this (very unusual nowadays) use case? Obviously not MOSFETs. From what I understand, germanium transistors were most common for 2nd generation computers, because (at the time) silicon transistor technology wasn't sufficiently mature to be used for that application. reply sebcat 47 minutes agorootparentprev> Not using MOSFETs would make it faster, in theory. My understanding is that the MOS 6502 was MOS, so MOSFET. Not using MOSFETs would make it less of a replica. Implementing the replica in an integrated circuit instead of discrete transistors would lower capacitance. The delay time is a consequence of the capacitance. reply junon 31 minutes agorootparentI'm aware of all of this (I even mentioned the MOSFET vs not in my comment), I was just answering the OPs questions. reply codeflo 1 hour agorootparentprevThe real question is why would you. This is an educational project that visually demonstrates how a CPU operates. It could clock at 0.5 Hz for that purpose. reply junon 32 minutes agorootparentWhy do anything? reply zokier 1 hour agoparentprevits unfortunate they don't have any details on what components were used so its difficult to say how much room for improvement there is still. reply ooterness 13 hours agoprevI love this art/engineering project. The whole concept of extra-large unintegrated circuits is just so amusing. The team behind this project sells kits for two classic ICs, the 741 op-amp [1] and the 555 timer [2]. Sadly, they've said the Monster6502 is too big and complex to make a practical kit. [1] https://shop.evilmadscientist.com/tinykitlist/762 [2] https://shop.evilmadscientist.com/tinykitlist/652 reply floating-io 10 hours agoparentThere's also the fact that they were recently acquired [1], unfortunately (IMO). I've said it before: I would totally buy one of those just to hang on my wall. It's a truly awesome project, and I love blinkenlights! [1] https://www.evilmadscientist.com/2024/bantam-tools/ reply lnx01 11 hours agoparentprevThe Apple A8X, found in the iPad Air 2, contains about 3 billion transistors. (This is comparable to the number of transistors in modern desktop computer CPUs as well.) At the scale of the MOnSter 6502, that would take about 885,000 square feet (over 20 acres or 8 hectares) — an area about 940 ft (286 m) square. reply MBCook 7 hours agorootparentI wonder how slow it would have to run and how many kilowatts it would use. reply xcv123 6 hours agorootparentprevThe Apple A8X is 10 years old. The current iPhone processor (Apple A17) contains 19 billion transistors. So now we have 126 acres in a pocket. https://en.wikipedia.org/wiki/Apple_A17 reply allenrb 9 hours agoparentprevUnintegrated or, perhaps… Dis-integrated? reply Max-q 52 minutes agorootparentThe correct term is discrete circuit. We used to have circuits, and when integrated on one piece of silicone, we called them \"integrated circuits\", while the old ones became \"discrete circuits\". reply tombert 13 hours agoprevWait, so if I understand this correctly (what I don't know about hardware is a lot!), is this basically a \"macroprocessor\"? As in, it functions the same way as a vanilla 6502, but done at a larger scale? Actually pretty cool that it's able to get 1/20th the speed when it's this big! reply kazinator 12 hours agoparentIt's a discrete CPU. This is how processors were made before microprocessors. Before 1970, we had discrete component mainframe machines, in fact running at multi-MHz speeds (e.g. CDC 6600). https://en.wikipedia.org/wiki/CDC_6600 60 bit, 10 MHz processor, introduced in 1964. Article says that was 10 times faster than other contemporary machines; but that still leaves those at around a solid megahertz. Discrete logic circuits can be fast because they have a lot of area over which to dissipate heat and can guzzle current. reply magicalhippo 12 hours agorootparentThe main issue isn't the CPU as such, it's memory. You can reasonably make a usable CPU using relays even, but a non-trivial amount of memory takes up sooo much space. reply jfoutz 9 hours agorootparentI learned this in Minecraft. I’d wired up ttls in college for intro to ee for cs majors, which was fun. But ram was a beast. I guess I’ll build another tower and upgrade to a full 1k. Ram takes so much space. reply vintermann 4 hours agorootparentThe Virtual Circuit Board game/sandbox at Steam offers an external memory, presumably for this reason. I remember a Minecraft mod from way back that let you do \"modular\" redstone, going \"into\" blocks to build redstone inside them, which you could then reuse as a single block. I wonder if it's been updated. reply userbinator 8 hours agorootparentprevDiscrete logic circuits can be fast because they have a lot of area over which to dissipate heat and can guzzle current. Moreover, most of them used either ECL or TTL, which are logic families much faster than P/N/CMOS. This being a transistor-level replica of an NMOS 6502, it can't go much faster. reply mschuster91 11 hours agorootparentprev> Discrete logic circuits can be fast because they have a lot of area over which to dissipate heat and can guzzle current. ... but not too fast either, because the long trace lengths are antennas so you'll end up in EMF emission regulations hell on the one side, and signal integrity issues on the other side. On top of that come losses from inductive and capacitive coupling. reply dhosek 13 hours agoparentprevYep. The bummer is that the Apple ][ relies on the clock speed of the 6502 to handle some of its functionality (most notably video handling), so you can’t just run a cable to the CPU socket on the Apple motherboard. I wonder if any of the other classic 6502 machines (Commodore, Atari, Acorn, BBC, etc.) would work? reply tombert 13 hours agorootparentAgain, I know basically nothing about hardware, so this is likely a dumb question: could you conceivably get an oscillator/clock chip that also operates at 1/20th the speed, wire that into the Apple ][, and then wire in this giant CPU? reply rogerbinns 11 hours agorootparentIt is well worth understanding how these older computers worked. All the components were doing double or triple duty, so that together all the tasks like refreshing memory, reading/writing memory, generating video, generating audio, and I/O worked. This involved interleaving bus access and similar \"tricks\". There are an excellent series of talks at CCC titled \"The Ultimate X talk\" [0] where you are shown exactly how. I recommend The Ultimate Acorn Archimedes Talk [1] which introduced the ARM chip, but also shows how shared bus access and optimisation was the core principle of performant designs at the time. [0] https://media.ccc.de/search/?q=the+ultimate+talk [1] https://media.ccc.de/v/36c3-10703-the_ultimate_acorn_archime... reply pulvinar 13 hours agorootparentprevDue to the way the video is interleaved with CPU cycles, you'd have to do something like divide the 1 MHz CPU clock by 20, and then when writing carefully only put data on the data bus for one of those 20 cycles. And the disk accesses would surely fail due to the CPU not keeping up, among other problems. http://www.apple-iigs.info/doc/fichiers/TheappleIIcircuitdes... reply anyfoo 12 hours agorootparentThinking more and more about it in this thread (yes, I'm procrastinating doing something else), I'm beginning to come to the conclusion that the way with the least modifications to the computer itself is to replace the DRAM with SRAM, and to \"just\" slow the video circuit down as well (including color burst, horizontal scan, etc.), but to attach a custom buffered display instead of a regular monitor to the computer. However, I wouldn't be surprised in the slightest if any other of the integrated circuits don't work at the slower speed for some reason. (Filters etc. you'd just replace/retune.) reply duskwuff 13 hours agorootparentprevUnfortunately, no. The CPU would work, but memory retention would be unreliable (because the DRAM is now going ~20x too long between refreshes), and the composite video output would fail to display on a monitor (because it's the wrong frequency). reply tombert 13 hours agorootparentWell now I'm wondering; it might not directly run an Apple ][, but what about the Apple ][ compatibles like the Laser 128 [1]? It doesn't use the same ROM as Apple, they licensed Microsoft Basic directly from Microsoft and then added the missing functionality themselves. I guess someone would have to see how similar it is to a vanilla Apple ][. [1] https://en.wikipedia.org/wiki/Laser_128 reply anyfoo 12 hours agorootparentAgain, the answer is almost certainly no, unfortunately. A Laser 128 isn't that fundamentally different to an actual Apple II. In fact, from a wider point of view, they are \"almost identical\". DRAM needing timely refresh is universal to DRAM itself: It's because DRAM is literally made of a transistor and a capacitor holding your bit, and if you don't access it every so often, that capacitor discharges eventually. The most realistic path to \"slow DRAM\" would be to replace DRAM with SRAM altogether, which is effectively made of transistors only, and holds its content as long as just power is applied. It's not that hard to replace DRAM with SRAM: Apart from not needing refresh, SRAM behaves similar to DRAM. It's just more expensive (and usually faster, which is also not a drawback), and the DRAM refresh, while not doing anything useful, wouldn't hurt. But as for video output, that's just a consequence of how Apple II's (most microcomputers of that era, actually) video output works: By literally controlling the electron gun in the CRT, with only some analogue electronics in between. The monitor however wants to move the electron beam at a certain speed (or, if you had a fancy Multisync monitor, a set or range of speeds). But even if that wasn't the case, the phosphor on the screen glows only for a short amount of time, so draw your picture too slow, and you will only see small parts of it. The latter can be demonstrated by just pointing a camera at a CRT. In all likeliness, the camera's frequency will be slightly off from the CRT's frequency, so that you capture different parts of the beam's path at each frame, simulating what it would look like if you'd slow down the beam itself: https://pub.mdpi-res.com/sensors/sensors-22-01871/article_de... Would it be possible to decouple the video output circuit from the microprocessor's timing? Absolutely! And this is in fact what virtually any modern computers has been doing for many decades by now: Even if you still can find analogue VGA with a CRT, that VGA card had its own clock that is completely independent of the CPU's clock. But that's just not how simple home computers of that era usually operated. reply duskwuff 12 hours agorootparentprevFrom what I can tell from documents like [1], the Laser 128 was very similar (possibly even identical) to an Apple II on a hardware level. It'd have the same issues running at a higher/lower clock speed than it was designed for. [1]: http://www.applelogic.org/files/VTECHL128.pdf reply CamperBob2 11 hours agorootparentprev1.023 MHz is basically the Apple ]['s fine structure constant. Change that, even by a tiny amount, and the whole universe comes unglued. Video, disk access, you name it. There were accelerator boards, but they ran at integer multiples of the original clock speed for that reason. reply jameshart 13 hours agorootparentprevYou’d need to run an NTSC screen at 3Hz reply basementcat 13 hours agorootparentYou’ll need additional circuitry to \"upscan\" your 3Hz display to 60Hz. There was a moment many years ago when I considered constructing a mechanical 6502. I quickly learned that transistors are fabricated in silicon for economic reasons. reply anyfoo 12 hours agorootparentThe fun path would be to modify the display to run at 3Hz (and also implying the much slower horizontal scan rate), but due to the phosphor coating's short persistence, you'd at best see only a small sliver of the full picture at any time. Exactly like you see if you point a video camera to a CRT whose frequency does not match up exactly. But if you'd take a long exposure photo of the CRT, it would likely work! (EDIT: Might fry your phosphor, though... as for any other components, like filters or components that may get damaged by the now only slowly changing current, I just assume you replaced or retuned that as part of the conversion process.) reply userbinator 8 hours agorootparentThere are CRTs with long-persistence phosphors too; they found applications in radar and the like: https://en.wikipedia.org/wiki/Phosphor#Standard_phosphor_typ... https://tubetime.us/index.php/2015/10/31/crt-phosphor-video/ http://www.labguysworld.com/crt_phosphor_research.pdf The P10 phosphor there claims \"Persistence from several seconds to several months\"(!) reply basementcat 13 hours agorootparentprevThis is true of the Commodore 64 as the VIC-II chip timing is closely coupled to the NTSC color burst (note below joke about \"up-scanning to 60 Hz\") Also disk I/O and RS-232 are bit-banged. reply anyfoo 12 hours agorootparent> Also disk I/O and RS-232 are bit-banged. You could easily slow that down as well. The big problem is really the CRT (and possibly other ICs in the computer that don't like the extreme slowdown). As noted below, if you went through the arduous process of replacing and retuning components to be able to slow the beam down, your phosphor coating is still too fast, and probably doesn't like having the beam passing only slowly over it. (You thought regular burn-in was bad!) However, replace the CRT with something else, like an LCD with some extra buffering circuit, and it could work. Yes, the color burst will be at a much lower frequency as well, but just demodulate color at this lower frequency, then. reply dhosek 9 hours agorootparentYeah, I’m thinking that the solution would be something that takes the analog monitor signal and uses it to control a digital display. reply geon 12 hours agorootparentprevStill, you could compile your own custom microsoft basic like Ben Eater did on youtube recently. reply anyfoo 12 hours agorootparentThe software is really not an issue. I saw the MOnSter 6502 live running BASIC, and it was MS BASIC as far as I recall. It just wasn't simply hooked up as the CPU of an Apple II or similar. reply cancerhacker 10 hours agoprevIt’s mentioned in the page, but the emulation at http://www.visual6502.org/JSSim/expert.html is worth a visit. reply playa1 4 hours agoprevProjects like this are a great way to appreciate how much can fit into an IC. Reminds me of the Megaprocessor project. https://www.megaprocessor.com/index.html reply pnw 12 hours agoprevI signed up for the mailing list when it was first on HN a few years ago, but haven't seen many signs of progress. Website still says mid 2023 for a launch. 6502 was my first CPU so I'm totally down to buy one, even though it's going to be pretty expensive. I'd be pleased if they could make them for less than $5k. reply pclmulqdq 6 hours agoprevBeautiful work, and a great choice of processor to emulate. As I understand it, they also tried to get close to the layout of the actual chip, not just make a functional equivalent circuit. The next time I have some time, I would love to do a discrete RISC-V, but I know how big a project this sort of thing is. reply th4tg41 11 hours agoprevWonder if it's feasable and how much work it would be to record a run of eg Super Mario Land and play the processor instructions back on this thing hanging on the wall. Would be a nice conversation piece. reply th4tg41 11 hours agoparentPlus a small LCD in the frame that displays the level and a progress bar for that level. reply th4tg41 9 hours agorootparentDot matrix display. Progress bar is underneath the Level indicator. Plus presence sensor. Not PIR. And RTC in/on whatever plays the instructions to always seemlessly pick up on the loop. reply henry_bone 11 hours agoprevI wonder if it's correct right down to the errata. Was there undocumented instructions on the 6502? If so, I wonder if it supports those. reply zik 11 hours agoparentYes, there are some undocumented instructions [1]. They say it's a transistor-for-transistor replica so it should be 100% compatible with them. [1] https://www.masswerk.at/nowgobang/2021/6502-illegal-opcodes reply namuol 6 hours agoprevSee also: https://gigatron.io/ Gigatron is a retro 8-bit computer with no microprocessor. Instead, its processing is done through thoughtful application of “TTL” logic chips. It’s another project that helps you “see” the processor’s internals. Kits aren’t for sale anymore through the official website, but you can find unpopulated PCBs and the components shouldn’t be hard to find. reply apantel 13 hours agoprevThis is the Japanese Cloisonné of circuit boards. Beautiful, well-done. reply warbled_tongue 13 hours agoparentI'm always slightly disappointed that I can't just Buy Now one of these passion projects for my wall. Truly wonderful art. reply VectorLock 2 hours agorootparentTake my money please! reply saulpw 9 hours agorootparentprevIt's a shame not everything is for sale. reply johnwbyrd 11 hours agoprevFun project, but fairly old news at this point. reply djmips 2 hours agoparentOld to you but not to a lot of people! https://xkcd.com/1053/ reply KingOfCoders 4 hours agoprevI'd love to have a Z80 version. reply grishka 13 hours agoprevA macroprocessor. reply dylan604 12 hours agoprevThis would be a great piece of art for a very niche space, like my office =) The fact that it actually works is just bonus. reply tocs3 12 hours agoprevI would like to see a mechanical version of something like this. reply hyperthesis 13 hours agoprev [–] what nm node is this? when would it have been state of the art? reply offmycloud 4 hours agoparentOne millimeter is 1,000,000 nanometers. So that's roughly the 1950s. :) reply Palomides 13 hours agoparentprev [–] maybe around 1956 or so I think it's a question of mm rather than nm! reply dboreham 10 hours agorootparentBefore my time but isn't 1956 too early for Silicon transistors? I'd have put it around 1963. Before DTL chips, introduced in 1964. Non-military TTL came along in 1966. reply actionfromafar 12 hours agorootparentprev [–] https://en.wikipedia.org/wiki/Bendix_G-15 was much slower. reply anyfoo 12 hours agorootparent [–] And had much larger \"nodes\", given that those tubes are much, much larger and farther apart than the SMT transistors on this 6502 replica. It's not even close. But I think IBM's SLT from the 1960s might come close: https://en.wikipedia.org/wiki/Solid_Logic_Technology Those little square \"chips\" are not actually integrated circuits, but effectively small PCBs with surface mounted discrete components (including transistors). reply NikkiA 11 hours agorootparent [–] The early PDPs used discrete CPUs, so they'd probably be the closest examples, the 12-bit PDP-5 is the smallest those CPUs went though, which isn't hideously more complex than a 8-bit CPU. reply anyfoo 8 hours agorootparent [–] I thought about CPUs with either discrete transistors, or with e.g. 74xx ICs. But the former intuitively seemed too big to me (because I was assuming they use large transistor packages instead of SMT), and the latter too small (because a single 74xx logic chip can pack a lot of transistors). So this “hybrid” of having SMT components in a can by IBM seemed close. Happy to learn more! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The MOnSter 6502 project aims to replicate the MOS 6502 microprocessor, famous for its role in significant computer systems.",
      "Comprising 4769 elements, such as 3218 transistors and 1019 resistors, the replica operates at a reduced speed compared to the original chip and includes new features like LEDs.",
      "Despite not being purchasable yet, the project, ongoing for about 8 years, is gearing up for a full public debut in 2023, offering demonstrations at Evil Mad Scientist Laboratories in Sunnyvale, California."
    ],
    "commentSummary": [
      "The MOnSter 6502 is a working transistor-scale replica of the MOS 6502 microprocessor, running slower due to increased capacitance, highlighting older computer technology and transistor use in early CPUs.",
      "Conversations revolve around integrating the 6502 chip into classic computers such as the Apple ][, focusing on overcoming memory and video output hurdles with discussions on adjusting clock speeds, displays, and emulating the processor.",
      "This project is regarded as a blend of art and historical technological progress, showcasing a unique combination of creativity and historical significance."
    ],
    "points": 181,
    "commentCount": 70,
    "retryCount": 0,
    "time": 1712172698
  }
]
