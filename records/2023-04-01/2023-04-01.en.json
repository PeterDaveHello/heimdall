[
  {
    "id": 35391433,
    "timestamp": 1680288136,
    "title": "Twitter's Recommendation Algorithm Now Open-Sourced",
    "url": "https://blog.twitter.com/engineering/en_us/topics/open-source/2023/twitter-recommendation-algorithm",
    "hn_url": "http://news.ycombinator.com/item?id=35391433",
    "content": "Twitter's recommendation algorithm has sparked controversy due to the revelation that the system employs metrics that ensure select people, like Elon Musk, are not negatively impacted. The code suggests that Musk's vanity metrics function as a control group for the algorithm. However, some comments support the notion that it is not unusual for a large system to measure metrics and claims of bias are unfounded. The controversy raises the issue of the challenge the platform faces in balancing freedom of expression, user safety, and the potential influence of select individuals.Twitter employees with a leftist ideological bent were found to have spent time looking for the mildest reasons to ban right-wing politicians, even building blacklists for them, according to The Tech Times. The documents were heavily redacted, but some users were able to find files revealing certain users received a special flag called authorised_black_bird_owners or author_is_elon. Twitter CEO, Jack Dorsey, has requested that all files be made public to ensure transparency; however, only a fraction of the documents have been released. The most surprising part of the revelations was the news that Twitter has a \"Democrat and Republican\" labelled metric, in which\u00a0\"Independents\u201d are excluded from boosting metrics.Foreign nationals are able to freely comment on American politics on social media due to lack of US jurisdiction over their speech. The USA remains a major player in Western and other cultures, largely due to its media influence including Hollywood exports and social media. Twitter's algorithm appears to favor right-wing American political content since the arrival of Musk, but some high-profile left-leaning politicians have also been boosted. A strange age classification system used by Twitter to classify user data seems to be incorrect in many cases. The US's two-party system is a logical outcome of its political systems and Duverger's law.Twitter's algorithm tracks engagement metrics for political affiliation, which could potentially result in echo chambers, but it is not used for other purposes. There is no nefarious intent behind this, and it is likely used to check that Twitter's algorithm changes are not politically biased. The usage of a separate flag to track engagement specifically for Elon Musk's tweets is strange and was reportedly a mistake. Nonpartisan elections can lead to apathy and difficulty understanding candidates in local elections. Parliamentary systems with smaller parties produce meaningfully different outcomes than a two-party system, as seen with Canada's NDP demanding state-funded dental care for children in exchange for backing the Liberal government. Bipolar electoral systems may not be stable, and a multipolar system may lead to less cooperation and increased divisiveness. International relations theory holds multipolar systems are more stable than bipolar systems. Echo chambers can be insidious, and involuntary and inescapable echo chambers are nefarious. Overall, Twitter's algorithm is designed around letting users curate their own feed, but it also constantly throws random content through retweets and quote tweets, which many users hate about the platform.Elon Musk wants to remove a feature from Twitter that tracks engagement metrics for important accounts like his, which some speculate he had requested in the first place, and the code for the feature has been released for public viewing by Twitter. Musk insists he only recently discovered the existence of the code and demanded its removal, with Twitter complying. The code also includes the existence of a list of 'very important tweeters' that was added by Twitter engineers, and the potential reasons for its creation are debated among commenters. Some express surprise at the way the code is structured, with one particular function tracking various ordinal variables encoded as binary flags.Twitter released its natural language processing algorithm but with limited transparency because the underlying policies and models are missing making it difficult to evaluate the effects of the algorithm. Although there are some valuable components, such as MostRecentCombinedUserSnapshotSource, its calculation can't be accessed. Some comments criticized Twitter for its \"openwashing\" and not fulfilling its promises, while others commended Twitter for publishing its recommendation code. One commentator, who is a social media and recommendation systems expert, tried to provide context relevant to the discussion, although it was unnecessary. Another commentator drew a comparison with Facebook's open-source algorithm, and yet another commentator cynically questioned if the code published is similar to what Twitter actually uses.Twitter has open-sourced its recommendation algorithm, revealing its three-stage pipeline that extracts latent information from tweet, user, and engagement data. By delivering more targeted recommendations, Twitter hopes to increase user engagement with the platform. The service, called Home Mixer, is built on the Product Mixer framework, serving as the backbone that connects the various candidate sources, scoring algorithms, heuristics, and filters that generate users' timelines. The candidate sources aim to extract the best 1,500 tweets from a pool of hundreds of millions, choosing from people users follow, or don't follow, creating a For You feed composed of 50% in-network tweets and 50% out-of-network tweets.Twitter has explained how its \"For You\" timeline works. The In-Network source provides recent tweets from a user's follow list using a real graph model that assesses the likelihood of engagement between two users, while Twitter's Social Graph and GraphJet are used to find relevant tweets outside of a user's network. Users' interests are used to create an Embedding Space, which ranks content based on similarity, and a\u00a0neural network ranks tweets using engagement data, before heuristics ensure its \"For You\" feed is balanced and diverse. The pipeline, which funnels\u00a0150 billion tweets per day,\u00a0is open source, and Twitter is planning on expanding its recommendation systems with new functionality.",
    "summary": "Twitter's algorithm has faced scrutiny for potentially favoring right-wing American political content but Twitter denies nefarious intent. Documents revealed Twitter employees with a leftist ideological bias spending time blacklisting right-wing politicians. Twitter CEO requested transparency but only a fraction of the documents have been released. Elon Musk requested Twitter to remove a feature that tracked engagement metrics for important accounts, but its existence and potential reasons for creation is debated. Twitter released its natural language processing algorithm, but there is limited transparency, making it difficult to evaluate. Twitter open-sourced its recommendation algorithm, \"Home Mixer,\" consisting of three-stage pipelines to increase user engagement. Twitter's \"For You\" timeline uses in-network and out-of-network sources, a neural network, and heuristics to provide a diverse and balanced feed to users.",
    "hn_title": "Twitter's Recommendation Algorithm",
    "original_title": "Twitter's Recommendation Algorithm"
  },
  {
    "id": 35393284,
    "timestamp": 1680295070,
    "title": "LLaMa.cpp 30B Model Achieves Significant Performance Improvements with Reduced RAM Usage",
    "url": "https://github.com/ggerganov/llama.cpp/pull/613",
    "hn_url": "http://news.ycombinator.com/item?id=35393284",
    "content": "The Llama.cpp 30B model can now run with only 6GB of RAM, which is a significant improvement in loading time performance. The new change has been well-received, with users reporting a\u00a0better experience. However, experts are still trying to figure out the RAM usage miracle, suggesting the model may still have some bugs. Some users are skeptical about the Python programming language's performance, leading to a debate on whether it's worth using. The success of the Llama.cpp model with just 6GB of RAM has sparked discussions on distributed training ideas.The LLaMA 30B model requires a lot of RAM, but a bug has been found in the measurement, and the author used a machine with lots of RAM. When LLaMA 30B is quantized, it only needs 6 GB of RAM, making it easy to integrate into other applications. Training tends to require a lot more precision and memory than inference. The development of large language models has been the product of hundreds of thousands of dollars in rented compute time. The FOSS community doesn't have a way to pool together that much money to buy compute from cloud providers. The leaked LLaMA 30B model has now given people the opportunity to work on iterative improvements. It is challenging to do distributed training for AI models with current hardware limitations.Optimization in GPU land is necessary, and intermediate outputs of models are useful for hyperparameter optimization. While cloud computing is an option, it introduces distribution and cost issues. Groups of people could potentially train as big as LlaMa 7B in a week or two, but the open-source community lacks the funds to pay for cloud compute. OpenAI's lack of optimization engineers has led scientists to write code by stringing together Python libraries, but software engineers are now optimizing these codes. ML researchers do not have to produce clean code; their job is experimentation, analysis, and creating a prototype. Recall instability with open organizations such as OpenAI.Researchers have achieved a faster and more memory-efficient version of the recently-released LLaMa language model. The optimization involves mapping the weights of the model to memory using mmap, which leads to a significant improvement in load times and a reduction in memory usage. However, some users have experienced poor performance when running the model on certain hardware, and there are still questions about its quantization and its impact on model quality. Despite these issues, the progress made in optimizing large language models is impressive, and the potential for running models such as this on local hardware is exciting.LLaMA is a new language model that is as good as GPT-3 at most sizes but with fewer parameters, and it can be used to run tools like executing searches and doing calculations. It apparently performs well, and the largest LLaMA model at ~60 billion parameters is not quite as large as ChatGPT 3 but is basically in the same class. As for legal issues, a counterclaim to Meta's DMCA against llama-dl is being drafted, and an anonymous HN user has pledged $200k for llama-dl\u2019s legal defense; therefore, soon you'll be able to use LLaMA without worrying that Facebook will knock you offline for it (though beware not to use it for commercial purposes).LLAMA, a model trained on partly copyrighted text, has recently caught the attention of Hacker News, as users discuss the ethical and legal implications of using and sharing such models. Some argue that all models trained on public data should be made public and that the outputs of such models are not copyrightable, while others contend that this is not the case, and that models are still subject to intellectual property rights. A recent pull request on the llama.cpp repository aims to make loading of model weights 10-100x faster by changing the file format, allowing for faster inference and concurrent inference processes.The new PR for the llama.cpp project has introduced a breaking change that allows loading weights between 10-100 times faster than before. This change also opens up opportunities for further performance gains on some microprocessors, and it supports both POSIX and Windows platforms. Additionally, the new file format now supports single-file and multi-file models and aligns tensors properly on a 32-byte boundary. The changes were made in collaboration with slaren and were rebased on PR #586. Some tests failed during the process, including one involving CMake, and some contributors suggested minor revisions that could improve the project further.The llama.cpp project introduces a new file format and a migration tool, praised by reviewers for their great work; some users encountered issues migrating their models, especially the gtp4all model, and contributors provided solutions by converting files and manually setting n_parts due to a possible LLAjo accretion; the update includes the inclusion of the \"mmap()\" function, thread sanitizer, and parallelization for quantization processing, along with evolving test results, a change in migration tool algorithms (\"guessing\" n_parts), and the removal of the quantize.py file; the changes are expected to close 91 and 640 issues, and the PR is reviewed by ggerganov, slaren, pgoodman, bakkot, mqy, and Green-Sky.",
    "summary": "LLaMa.cpp 30B model runs with only 6GB of RAM making it easy to integrate into other applications, sparking discussions on distributed training ideas in the open-source community. Despite some hardware and quantization issues, the progress made in optimizing large language models is impressive, and the potential for running models such as this on local hardware is exciting. A new PR for llama.cpp project aims to make loading of model weights 10-100x faster, allowing for faster inference and concurrent inference processes. The llama.cpp project introduces a new file format and a migration tool, praised for great work by reviewers, but some users encountered issues migrating their models. Discussions on the ethical and legal implications of using and sharing models like LLAma are ongoing among users.",
    "hn_title": "Llama.cpp 30B runs with only 6GB of RAM now",
    "original_title": "Llama.cpp 30B runs with only 6GB of RAM now"
  },
  {
    "id": 35382698,
    "timestamp": 1680237955,
    "title": "Developers discuss pros & cons of AI-enhanced development tools",
    "url": "https://simonwillison.net/2023/Mar/27/ai-enhanced-development/",
    "hn_url": "http://news.ycombinator.com/item?id=35382698",
    "content": "A developer praises AI-enhanced development tools for their ability to generate elegant and minimal proof of concept code that's faster to produce than research-based efforts. The unique and exciting aspect is the optimism of such AI tools that can grasp complex prompts that regular developers might have difficulty with. ChatGPT is compared to Mr Meeseeks, a fictional device for solving any problem before vanishing. Humorous comments reference the dangers of relying entirely on AI and the need for careful maintenance of medical equipment. Developers observe that the benefits of AI far outweigh the legal and legislative risks associated with its use. The conversation emphasizes that AI tools are a game-changer that accelerates software development even with complex problems.Engineers in fields like pharmaceuticals and aerospace do not typically use AI in their development processes, as precision in specification and testing are critical. The use of ChatGPT for medical software development is concerning, as liability ultimately falls on the developer. Developers must have in-depth knowledge of what they are engineering, while ChatGPT encourages the opposite. The validation process for using ChatGPT in critical software development would be an important consideration. The AI tool can be useful in some cases, but struggles with more niche libraries and frameworks.Developers have mixed opinions about ChatGPT's usefulness in code creation, with some finding it helpful for suggesting tools and high-level design, while others consider it almost entirely useless for code. Some have also noted that ChatGPT sometimes hallucinates non-existent functions, and its suggestions can be slightly off the mark. The AI has been found to be more useful for refactoring code and providing boilerplate code. However, other developers caution against blindly relying on ChatGPT's outputs and recommend manually checking the code and testing it before releasing it. Despite its limitations, some developers find ChatGPT a useful tool for handling DevOps work and boosting their confidence in their work.GPT technology can generate code but it may hallucinate APIs or mix libraries which are not always functional for production-grade code. ChatGPT is a useful tool for general algorithms or data translanslation in smaller amounts of data, but can't be integrated like Copilot. Errors are easily confirmed once the code is run; but while it produces great possible APIs, it doesn't know which actually exist. GPT has made a PoC for a horizontally scalable database in Rust using Raft-based consensus. It would be of more use if, like human teachers, ChatGPT could add annotations or text coloring to denote uncertainty. Developments such as unbounded optimism in AI also irritate some users.Developers are using artificial intelligence (AI) language model ChatGPT to explore new ideas and concepts in programming. ChatGPT's mobile accessibility and conversational interface offer hands-free and on-the-go development capabilities for various modules of applications, once limited by an impractical form factor. While the model is unable to write full code examples, its responses are capable of creating high-level outlines for complex algorithms and design structures. However, the model's confidence score estimates may not always match the accuracy of its responses. Despite this limitation, the model's ability to provide a \"real-time Wikipedia\" experience and develop new skills in users makes it an exciting tool for developers.Comments on Hacker News discuss the future of coding and the role AI will play, with opinions varying on whether AI will take over completely or whether there will always be room for classic programming languages like C and Java. Some argue that learning new things is part of the job and that reskilling can lead to insights and cross-pollination. Others contend that constantly having to learn new frameworks and languages can be stressful and that being a specialist in a particular language can be more beneficial. The discussion also touches on changes in work culture, with some bemoaning the shift towards treating coworkers as competition rather than part of a team.Programmers debate the benefits and drawbacks of using high-level, low-barrier programming languages favored by boot campers versus more complex languages that filter out inexperienced coders. Some argue that familiarity with complex languages like C and Java enables coders to write better code that is easier to maintain, while others dismiss the significance of deep knowledge of those languages or memory management. Regardless, virtually no one sees new AI tools as an immediate threat to their jobs despite their ability to perform aspects of coding automatically.Developers discuss the potential benefits and limitations of OpenAI's code generation tool, Codex, formerly known as Copilot. Some argue that the assistance the tool provides is only useful for simple tasks and note that it still requires developers to have expertise in creating infrastructure that supports language models like GPT-4. Others see the tool as a massive step forward, saying it eliminates the tedium in programming and frees them up to concentrate on more complex problems. The discussion touches on the future of AI-generated code, with some suggesting that, in the future, AI could replace human coders in the role of generating code that is both better and more efficient.Experts on Hacker News discuss the impact of not contributing to sites like Stack Overflow on training language models (LLMs) and how it could result in walled gardens. One commentator brings up Casey Muratori's concept of fake optimization, and another notes the prevalence of learning specific context from Stack Overflow rather than generalizing knowledge. The conversation veers off into anecdotes about learning to drive and programming before the internet. While the discussion is interesting, there is no central argument or new technology in the post.Programmers discuss their methods for troubleshooting code errors, including opening issues and delegating tasks to more knowledgeable colleagues. Some reflect on their years of experience in programming, with one noting that they only truly learned programming from working with exceptional mentors. Others question the value of certain programming skills, such as knowing assembly or manually managing memory, in today's industry. The discussion touches on the availability of programming resources and education compared to earlier decades.The article discusses the potential implications of automatically generated code on software reliability in critical systems such as hospitals and water supply. The accuracy rate of AI-generated code is currently at 80%, which may not be sufficient for systems that people's lives depend on. Experts argue that developing new kinds of testing and validation methods for AI-generated code could be critical to ensuring reliability. While AI-generated code has the potential to increase productivity, it could also amplify bad practices if not properly guided. Advances in AI-generated code could fundamentally change the landscape of programming and prompt the development of new programming languages.The Tech Times editor highlighted a project done by developer Simon Willison, who described his excitement about the way AI-enhanced development allows him to be more ambitious with his projects. Willison used ChatGPT and GitHub Copilot to save time and reduce the effort taken to accomplish tasks that used to take days or even hours to complete. He also used ChatGPT to develop a ChatGPT message archiving system that intercepts and saves JSON data from conversation history. This system leverages existing CORS headers and HTTP requests which allow JSON data to be transferred between servers smoothly, and Willison demonstrates how this proxy can safely endow an outsider to mimic that data. The project also involved using Starlette, a Python-powered web app that enabled CORS support and forwarded incoming requests to another server host specified in an environment variable. As such, the project wasn't just a passing whim but a technical feat.Large language models like GPT3/4/LLaMA/Claude are powerful tools that can enable building projects that were previously impossible, while also reducing the effort required for others. AI-assistance has allowed the author to work on multiple projects like using ChatGPT to generate OpenAI schema, writing AppleScript, and building prototypes like datasette-paste-table. The technology has given them the ability to learn a new language, Rust, and make progress in the field. The ChatGPT archiving problem is one example of projects that were tackled with AI assistance, and the impact of large language models on reducing the effort required continues to grow stronger.",
    "summary": "Developers discuss the advantages and limitations of AI-enhanced development tools such as ChatGPT, their potential in accelerating software development, and their impact on crucial software development fields. Mixed opinions exist about ChatGPT's usefulness in creating code, with some finding it helpful for suggesting high-level designs and others cautioning about blindly relying on its output. Codex and other AI-generated code tools are seen by some as a massive step forward, though some argue that they are only useful for simple tasks. The potential implications of AI-generated code on software reliability are being discussed as it has the potential to increase productivity, but also amplify bad practices if not guided correctly. The Tech Times editor highlighted a project developed using AI-enhanced development tools, demonstrating the potential of these tools to enable developers to tackle complex projects with more ease, reducing the effort needed significantly.",
    "hn_title": "AI-enhanced development makes me more ambitious with my projects",
    "original_title": "AI-enhanced development makes me more ambitious with my projects"
  },
  {
    "id": 35386948,
    "timestamp": 1680269736,
    "title": "Experts discuss the pros & cons of using Postgres as a graph database",
    "url": "https://www.dylanpaulus.com/posts/postgres-is-a-graph-database/",
    "hn_url": "http://news.ycombinator.com/item?id=35386948",
    "content": "The use of Postgres as a graph database is being discussed on Hacker News, with experts offering insights on the limitations and potential of the approach. The use of SQL queries with WITH RECURSIVE shows promise in expressing reachability and shortest path queries, but executing them requires a sophisticated optimizer like Umbra. Other proposed SQL syntax extensions, such as WITH ITERATIVE, could make queries easier to write and execute. There are drawbacks to using Postgres as a graph database, including scalability issues with large edge tables, and the need to build and index denormalized n-th order relationship tables. Experts suggest considering proper graph databases for more complex use cases.Experts discuss the advantages and disadvantages of using Postgres as a graph database. While graph databases are technically more efficient for graph data, Postgres can be used as a \"NoSQL graph DB\" with little effort. Some limitations to using Postgres as a graph database include recursive queries that may not scale well for large graphs and difficulty optimizing complex queries that go beyond basic BFS and DFS algorithms. However, Postgres can be sufficient for many use cases, and some readers suggest using Apache Age as a graph database extension for Postgres. Additionally, some readers recommend using native graph databases like Memgraph or EdgeDB for more specialized performance.Users on Hacker News discuss the capability of Postgres to store and query graph data using recursive CTEs, and some express doubt about its performance and scalability. Others point out the difficulty of traversing graphs using SQL recursive queries and the need for specialized graph databases for certain use cases. However, some also suggest approaches for utilizing Postgres as a graph database, such as using nested sets or materialized views, while acknowledging the need for performance and scalability testing. Several users provide resources for learning more about working with graphs in SQL, such as Joe Celko's books and the Wikipedia article on hierarchical and recursive queries.PostgreSQL can efficiently store and query graph data structures, making it a hidden gem of the graph database world. Creating tables for nodes and edges can store information about each entity and its relationships. Recursive queries enable the traversal of graphs starting at a designated node and following its edges until the endpoint is reached. This approach avoids the need to join tables repeatedly, which can create a maintenance nightmare. One method to deal with undirected graphs on relational databases is to materialize paths and search using various indexing techniques. Postgres is rapidly evolving and may be able to support Worst-Case Optimal Joins soon, putting more pressure on other databases.The post talks about using Postgres to store and query graph data structures, allowing for dynamic generation of work instructions. This approach doesn't require adding extra systems and can integrate graph data structures into existing databases.",
    "summary": "Experts discuss advantages and limitations of using Postgres as a graph database, citing difficulty with recursive queries and scalability issues with large edge tables. However, Postgres can be a suitable option for simple use cases, with some suggesting specialized extensions like Apache Age or native graph databases like Memgraph or EdgeDB. Recursive queries with CTEs and materializing paths enable efficient traversal of graphs, avoiding the need for repetitive table joins. Postgres is becoming more competitive with its evolving capabilities, including potential Worst-Case Optimal Joins.",
    "hn_title": "Postgres as a graph database",
    "original_title": "Postgres as a graph database"
  },
  {
    "id": 35385075,
    "timestamp": 1680259755,
    "title": "Italian Privacy Regulator Bans OpenAI's ChatGPT Over Privacy Concerns",
    "url": "https://www.politico.eu/article/italian-privacy-regulator-bans-chatgpt/",
    "hn_url": "http://news.ycombinator.com/item?id=35385075",
    "content": "Italian privacy regulator bans ChatGPT. Comments mostly lead to discussions about spam calls and scams, with various anecdotes shared across different countries. The ban is criticized for targeting a service that doesn't require personal information. The utility of technology in preventing spam calls is widely discussed, with some pointing to settings and measures that have worked for them. The status of privacy regulations in different countries is questioned.OpenAI has been banned in Italy due to concerns over privacy violations, particularly over how the company handles data and trains its models. The privacy regulator noted that the ban should not be seen as evidence of any wrong-doing but, rather, a \"ban on collecting data from Italian users without a proper disclosure.\" Meanwhile, UK GDPR has been proposed to change, but it's unclear how this will affect compliance with EU GDPR. In another development, individuals in Sweden are receiving multiple spam calls per week from UK numbers. People shared tips such as answering but remaining silent to waste the caller's time, and installing AdGuardHome/PiHole to bypass ISP blocking in Italy.Italian regulators are threatening to temporarily ban OpenAI's language model, ChatGPT, for exposing minors to unsuitable content, as it does not verify users' age. Critics contend that the government should improve access controls or implement \"age-verification mechanisms.\" Others note that different ages of minors require various control mechanisms, adding that age-range bans are ineffective. Some commenters compare ChatGPT's harmful content to that on television or in books, though governments regulate these differently. One suggests that packaged software, age-verified by clerks or parents, could mitigate regulator concerns. Some speculate that Italy's past regulation of startups, like ai.companion, may have contributed to the current scrutiny.Italy's Data Protection Authority (DPA) has blocked OpenAI's natural language processing technology, ChatGPT-3, within the country's borders. The DPA stated that the system \"exposes minors to absolutely unsuitable answers compared to their degree of development and self-awareness\", posing a risk to individual rights and freedoms. OpenAI's system has come under scrutiny for its lack of transparency and privacy protections. However, some have criticised the move, stating that it limits innovation and potentially harms Italy's economy. The decision follows a recent series of regulatory actions taken by European authorities against big tech companies over concerns of data privacy and antitrust.Calls for technology ministers to have a basic understanding of technology rather than being complete novices who simply regurgitate buzzwords; however, some argue that technical expertise is not the main issue but rather the decision-making process that led to poor decisions. Being a good politician requires talent and competency, and like software, it is an area in which one could only do a satisfactory job. Leaders who lack proper knowledge or competence can hinder their team's ability to lead. Politicians may understand politics in their respective regions, but they may lack knowledge about technology. Countries like Taiwan have technology ministers with a background in software programming, electrical and computer engineering, and innovation. In Europe, politics may not progress due to rural areas' aversion to change.The comments section is discussing the introduction of new technologies, specifically ChatGPT, and whether or not they should be regulated. A few people make observations about politics and different mentalities. Some argue that Europe is slower to adopt new technologies, while others suggest that it's better to take things slow rather than rush. One commenter points out that Europe has its own high-quality farmland and culture around it. Another person argues that there is a fear of democracy among the elite class in Europe. The overall theme of the discussion is about finding a balance between innovation and regulation, and also understanding the different perspectives that exist on this issue.Italy has banned \"OpenAI\" from coming to the country, which some users find to be unnecessary and rather arbitrary. However, the ban is in place because of data compliance, copyrights, political reasons, and drug testing policies. This is a vivid example of how large-scale deep packet inspection is becoming prevalent and used as an enforcement tool, and how some countries want to safeguard their boundaries like nation-states while the internet is seemingly a unified world. Despite its criticism, such bans are not new or incomprehensible, just like local drug testing policies for returning nationals enforced by some countries. And while some people might regard Singapore as one of the ideal places to live in, its strict drug policies are discriminatory, meaning personal freedom is limited there.The GDPR asserts extraterritorial jurisdiction over servers worldwide, which is unprecedented compared to laws from any other first-world country. The assertion applies to anyone worldwide who handles data from any EU citizen. The act would affect a Chinese web shop trading internationally, for example. The regulations are distinct from those within the United States which aren't as expansive. There is an argument that this extent of jurisdiction is impractical and challenging for the EU to enforce. The governance is unique in being broadly applicable beyond the EU with little burden imposed on firms.The Italian privacy regulator has banned the artificial intelligence chatbot app, ChatGPT, over alleged privacy violations, ordering an immediate block and investigation into the US company behind it, OpenAI. The national data protection authority's order, which is temporary, prohibits OpenAI from processing data from Italian users until the company complies with the General Data Protection Regulation. Calls for a suspension of new ChatGPT releases are growing in the US and Europe over risks to privacy, cybersecurity and disinformation. OpenAI does not have an EU office but its representative in the European Economic Area has 20 days to respond or face a penalty of up to 4% of its global revenue.The Tech Times is not the correct platform for the given text content as it is unrelated to cutting-edge technology news. It instead covers news and analysis on various topics such as French politics, elections in Europe, coronavirus response, migration, defense, and other policy areas. The website also offers newsletters, podcasts, and live extensions of their journalism, including in-depth reporting and data for policy professionals.",
    "summary": "Italian privacy regulator bans OpenAI's ChatGPT over alleged privacy violations, particularly for exposing minors to unsuitable content without verifying age. Critics argue that the ban is unnecessary and could limit innovation. The status of privacy regulations in different countries, like GDPR, is questioned. The comments section features debates on finding a balance between innovation and regulation and the need for technology ministers to have technical expertise. The ban highlights the challenge of enforcing GDPR's extraterritorial jurisdiction worldwide. The Tech Times covers news and analysis on various policy areas but is not the correct platform for this content.",
    "hn_title": "Italian privacy regulator bans ChatGPT",
    "original_title": "Italian privacy regulator bans ChatGPT"
  },
  {
    "id": 35387000,
    "timestamp": 1680269986,
    "title": "NYPD non-compliance with surveillance laws raises concerns in tech community",
    "url": "https://www.dailydot.com/debug/nypd-violating-post-act-inspector-general/",
    "hn_url": "http://news.ycombinator.com/item?id=35387000",
    "content": "The NYPD is currently refusing to comply with New York City's new surveillance tech laws, leading to concern in the tech community about government surveillance. Hacker News users discuss the prevalence of surveillance in other cities, with some noting that non-government entities, such as insurance companies and businesses, collect surveillance footage. Some users express concern about the lack of checks on government surveillance and the potential for a surveillance state. Others discuss the challenges of using surveillance footage as evidence in court. Overall, the post highlights continuing concerns about government surveillance and its impact on privacy.The NYPD's International Liaison Program based in New York's Intelligence Bureau sends officers around the world to various law enforcement agencies, and is funded by the New York City Police Foundation, which overrides the city itself. Some people question why the NYPD is not leaving this type of responsibility up to the FBI; others find it to be the embodiment of the \"Gotham\" narrative. A satellite designer from Loral Space Systems in the 2000s indicated that there were indeed U.S.satellites that could read number plates from space, disproving one user's skepticism. While some small town law enforcement branches can be corrupt, they typically lack the resources to run a surveillance/police state, unlike bigger cities like New York and San Francisco.A county's heavy surveillance of children has raised concerns about a police state mentality. Screening children to identify potential criminals and their families was carried out through categorizing every child by GPA, attendance, and credit count. This resulted in specific surveillance that affected entire families. Equipment is not necessary in small communities as power is gained through social ties. However, federal funds for anti-terror purposes support the scalability of technology. Non-big city policing can also have corrupt law enforcement, leading to small communities being 'cash cows' through fines and fees. While smaller communities don't have as much relative power, accusations of corruption are investigated less frequently due to a lack of resources.The NYPD is being accused of not fully complying with elected officials and laws; some officers believe that following certain requirements, such as reporting surveillance equipment, is unconstitutional. There is a debate about whether it is the responsibility of law enforcement to decide the legality of laws, and if a cop thinks a law is unconstitutional, they should not have to enforce it. However, quitting is not always the best solution, as it may leave the job open to less principled people. The issue of police accountability and oversight is crucial, as there is minimal accountability when it comes to the NYPD. New Yorkers care about the issue, but there is little they can do as the NYPD has de facto veto power over the city government.The discussion revolves around the police force not living where they work. Some argue it is because they fear retribution, but others suggest it is to avoid developing empathy for the community. It is mentioned that police officers are generally middle-class and can afford a middle-class lifestyle. One commenter suggests the police are an organized gang of criminals who benefit from their communities, while another user notes that cops are among the highest paid employees in the country. The discussion concludes with a statement about how New Yorkers of all professions survive on salaries far below NYPD's average.The NYPD is not being reined in despite promises by candidates with voter support. Different priorities and candidate choices exist, as local voters in one district generally look to the police for increased presence in hopes of ensuring their safety. A conflict exists between parts of the Democratic base and police departments, leading to calls for structural changes and more training in mental health and deescalation. Police unions hold tremendous political power, and past experiences have taught many that holding the NYPD accountable is a difficult task. The media's portrayal of protests has created difficult sympathies on both sides, with Democrats denying its role in enabling rioters and looters, and protesters reacting in anger to the police's lack of accountability.Despite some stores partnering with non-governmental organizations to reduce the need for police presence, reports suggest that Minneapolis neighborhoods were faced with turning into food deserts after looting caused damage that forced grocery stores to close for nearly a year; however, the media's representation of the impact on crime rates from defunding the police is considered overinflated, and the narrative around the need for more police support has grown as a result of a spike in what could be considered a relatively insignificant bump in crime rates when viewed in the context of historical rates of crime in New York City.The post discusses police accountability issues in the NYPD and Vancouver police departments, with experts noting that police should not be left to police themselves. Large-scale police departments such as the NYPD have data scientists on staff to mitigate small biases that could have significant effects, and some readers suggest sending researchers and statisticians into police departments for \"touching\". The discussion highlights the lack of accountability and transparency in the police forces and criticizes the legal precedents, such as Warren v. DC and Castle Rock v. Gonzalez, which broadly exempt police from any special duty to protect or serve the public, thereby creating a need for police reform. Attempts to legally compel such performance by the police or monitor with statistical analysis shows some promised solutions.The New York Inspector General has reported that the NYPD is violating a 2016 law regulating the use of surveillance technology by police. The report alleges that the NYPD failed to adhere to guidelines that require police to document the use of certain equipment, obtain approval from the department's Legal Bureau, and make public their use of certain technologies. The report found that the NYPD failed to document its use of facial recognition technology and obtained approval for only one of six types of devices it uses for cellular surveillance. The NYPD responded that it is taking steps to address the report's findings.The NYPD violated New York City's 2020 Public Oversight of Surveillance Technology (POST) Act by failing to disclose detailed information about its surveillance technologies. According to the Office of the Inspector General for the NYPD, the department did not issue Impact and Use Policies (IUPs) as directed by the act, which would detail the capabilities of any new or existing surveillance technology, and any prohibitions or restrictions on its use. The department has published general IUPs which provided insufficient information for the Inspector General to conduct an audit. Civil liberty advocates have accused NYPD of failing to comply with the POST Act to ensure transparency and reduce harm.",
    "summary": "The NYPD's non-compliance with surveillance technology laws in New York City is causing concern in the tech community. Users on Hacker News discuss the prevalence of surveillance, challenges of surveillance footage as evidence, and concerns about government surveillance leading to a surveillance state. There is debate over whether it is the responsibility of law enforcement to decide the constitutionality of laws. Police accountability and oversight are crucial, but there is minimal accountability when it comes to the NYPD. The NYPD is also accused of not disclosing information about its surveillance technologies. The lack of police accountability and transparency highlights the need for police reform.",
    "hn_title": "NYPD is refusing to comply with NYC\u2019s new surveillance tech laws",
    "original_title": "NYPD is refusing to comply with NYC\u2019s new surveillance tech laws"
  },
  {
    "id": 35391115,
    "timestamp": 1680286867,
    "title": "Feasibility & Future of Building Low-Cost Language Models for Web Browsers",
    "url": "https://simonwillison.net/2023/Mar/17/beat-chatgpt-in-a-browser/",
    "hn_url": "http://news.ycombinator.com/item?id=35391115",
    "content": "Tech experts discuss the feasibility of building a ChatGPT-beating model for less than $85k and running it on a browser, considering limitations in parallelizing the training process and inter-node latency. Train-on-idle-GPU frameworks, such as Folding@Home, have been proposed, but the poorly-parallelizable nature of machine learning training, which requires tightly-coupled GPUs, makes it hard to achieve multi-node speedup, and other issues such as getting these models to produce reliable structured data remain. However, there is still interest in exploring LLaMA fine-tuned for the ReAct pattern that can execute additional tools for creative applications.A discussion on the potential of using fine tuning against LLaMA to teach it to emulate GPT-3 through extra tuning or different prompts to tap into the knowledge available in Github, which was part of its training corpus. The conversation also covers the possibility of pooling resources, crowd sourcing, or using SETI@home to reduce the cost of training models and the feasibility of having a large portion of human knowledge just sitting locally on a machine. Additionally, the benefits of having non-big bux company options exist, even if they cater to niche use cases, are weighed in.OpenAI's partnership with Microsoft Azure and potential privacy concerns for AI conversations are discussed in a Hacker News thread. Some suggest that running models locally is a promising solution for those concerned about passing private data to another company. Others discuss the cost and practicality of building your own AI hardware versus paying for cloud services. One commenter suggests creating an open-source, non-profit AI clone of OpenAI. The potential for reduced training costs with 4-bit quantization of weights is also noted. Additionally, OpenAI's secrecy around training data is criticized for hindering collaboration and trust.A post on Hacker News discusses a project that uses the Alpaca model, which is able to generate human-like answers to prompts, even outstripping human performance in some tests. The project is a demonstration of how lightweight such models have become and uses WebGL to execute models within the browser. The post also discusses the memory capabilities of browsers, with some suggesting that modern browsers are capable of handling models like Alpaca with ease. The project has attracted attention due to the accessibility of the code used and the low hardware requirements, which could make similar developments accessible to many. Furthermore, there is a discussion surrounding the viability and cost of training large models like Alpaca.Simon Willison believes it's possible to create a language model with GPT-3-like capabilities for $85k, a dramatic reduction in cost compared to GPT-3 which costs millions of dollars to build. The LLaMA project and Alpaca from Stanford is the key to this. LLaMA is an open-source, publicly documented, GPT-3-class model, with efficient training process techniques, and it currently cost around $82,432 to run. Alpaca improves LLaMA performance on a particular prompt and the fine-tuning can be done for under $100. The resulting model could be run entirely in the browser. Although some argue that running software in a browser is not always ideal, it offers excellent sandboxing if you're going to give a language model the ability to execute API calls and evaluate code safely.Large language models can potentially be trained for $85,000, with the possibility of running models directly in web browsers with more capabilities than ChatGPT through the addition of extra tools like Bing. New models like LLaMA and Alpaca could face increasing competition from openly licensed models in the future, along with a rising number of people who can afford the training. Advancements in WebAssembly and WebGPU are enabling the stable diffusion image model and the Hugging Face Transformer library of models to run entirely in the browser. Using the ReAct prompt pattern can expand language model capabilities dramatically. Safe coding environments like web browsers are crucial, and the new H100 Tensor Core GPU from Nvidia provides 30x faster performance over their previous model.",
    "summary": "Building low-cost language models for web browsers is being explored by tech experts with a focus on using fine tuning techniques against the LLaMA model to emulate GPT-3. Train-on-idle-GPU frameworks are being proposed to reduce the cost of training models, but challenges in parallelizing the training process and inter-node latency remain. Some suggest that creating an open-source, non-profit AI clone of OpenAI could be a solution. Projects using lightweight models like Alpaca from Stanford are attracting attention for their accessibility and low hardware requirements. New models like LLaMA and Alpaca could face increasing competition from openly licensed models in the future. Advancements in WebAssembly and WebGPU are enabling the stable diffusion image model and the Hugging Face Transformer library of models to run entirely in the browser. Safe web browser capabilities are crucial, and the new H100 Tensor Core GPU from Nvidia provides 30x faster performance over their previous model.",
    "hn_title": "Could you train a ChatGPT-beating model for $85k and run it in a browser?",
    "original_title": "Could you train a ChatGPT-beating model for $85k and run it in a browser?"
  },
  {
    "id": 35389566,
    "timestamp": 1680280538,
    "title": "Facial recognition tech misuse raises concerns for civil liberties",
    "url": "https://www.nytimes.com/2023/03/31/technology/facial-recognition-false-arrests.html",
    "hn_url": "http://news.ycombinator.com/item?id=35389566",
    "content": "Police in Michigan used facial recognition software from Clearview AI to identify the wrong person and put him in jail for a week. While Clearview AI may not be directly responsible, legal action could still be taken against them or the police for wrongful arrest or defamation. Clearview AI's technology has been controversial and raises questions about the reliability of facial recognition tools. The use of facial recognition by law enforcement has been deemed unconstitutional in the past due to its potential infringement on civil liberties. This case highlights the need for stricter regulations on the use of these technologies.The story involves the misuse of technology, particularly facial recognition software, to create false evidence in a police investigation. The use of these algorithms raises questions about the accuracy of such technologies and their role in police investigations. The story also highlights how police biases and errors can lead to innocent people being arrested, and the consequences of this on their lives. The impact of such technology on individuals and society, and the trade-offs between security and privacy, need to be discussed to find effective solutions. Finally, the article emphasizes the need for responsible management of these emerging technologies and the importance of ensuring that their use aligns with ethical and legal standards.A case in which an innocent man was arrested due to a cascade of constitution-violating technologies has highlighted concerns with facial recognition and e-warrants. These technologies are designed to make policing more efficient but have a troubling rate of generating false positives. The case demonstrates how automated image matching can result in the issuance of faulty warrants, depriving innocent people of their freedom. Civil liberties advocates question whether e-warrants make it too easy for judges to rubber-stamp decisions made by police. Technologies such as these have been combined to create a fully-automated computerized police state. There are concerns over the accuracy of facial recognition technology and the use of e-warrants.The real risk of AI in the short and medium-term stems from the harms it is causing now, not a potential malicious superintelligent being. People need to be skeptical of technology again and understand that all \"miracle\" technology is merely corroborating evidence, not prima facie evidence. AI technology has complexly combined with human social factors to exacerbate damages. There is a lack of laws and regulations to control the AI-based technologies that have devastating social impacts like facial recognition, and they should be considered with caution. Sadly, Clearview AI has not been raided by the police despite the constant need for surveillance watchdogs.",
    "summary": "A wrongful arrest of an innocent man using facial recognition software highlights concerns about the technology's reliability and infringement on civil liberties. The case underscores the need for stricter regulations and responsible management of emerging technologies. E-warrants and automated matching also raise concerns about false positives and rubber-stamping by judges. The combination of AI technology with human social factors has resulted in complex damages. The lack of laws and regulations to control the use of such technologies is cause for caution. The misuse of technology should not go unchecked, and the importance of ethical and legal standards must be emphasized.",
    "hn_title": "Police relied on Clearview AI and put the wrong person in jail",
    "original_title": "Police relied on Clearview AI and put the wrong person in jail"
  },
  {
    "id": 35387191,
    "timestamp": 1680270846,
    "title": "Valve's Steam Deck Gains Popularity Among Gamers & Developers",
    "url": "https://boilingsteam.com/75-of-the-top-100-most-played-games-on-steam-are-playable-verified-on-the-steam-deck/",
    "hn_url": "http://news.ycombinator.com/item?id=35387191",
    "content": "The Steam Deck, a handheld gaming PC from Valve, offers a smooth, console-like experience with a versatile hardware that provides a more comfortable gaming environment than sitting at a desk, and has an almost seamless Linux-based operating system with a user-friendly Windows compatibility layer. Furthermore, the Steam Deck's passionate community of gamers and hackers provide an exceptional experience, featuring an outstanding level of support and engagement that enables users to file bugs and make effective bug fixes that improves gaming experience. The hardware itself is incredibly advantageous for narrative-driven or console-oriented titles, with games like JRPGs achieving better performance on this handheld gaming PC than on a traditional desktop computer.The Steam Deck's power consumption and performance is compared to Xbox and Playstation, with the Steam Deck using <20W and having better specs than the PS4. The impact of the Steam Deck on the gaming market is discussed, particularly in terms of promoting Linux/proton friendly games and helping games to add solid controller support. The device is not suitable for coding, with recommendations given for using it as a portable dev platform with a docking station or Bluetooth keyboard, and using an external keyboard and monitor for extended coding sessions. Lastly, people discuss their experiences buying and using the Steam Deck.The Steam Deck, a handheld gaming device that can easily stream games and play mainline AAA Windows games via a software layer called Proton, is generating excitement among gaming enthusiasts; it has a high-quality screen and can be used without a reliable internet connection while traveling. Most users find that the Steam Deck controls are far ahead of the previous Steam Controller iteration, which had mediocre build quality and unsatisfying controls; the Deck\u2019s controls feature better buttons and a more comfortable hand position. Protondb, which uses user feedback and tweaks to test and rank games, is allowing Steam Deck users to evaluate which games are compatible for the Deck. Some users doubt the high standards for verified game playability on the Deck, but agree it is a pioneering device for gaming on Linux.The Steam Deck's smaller trackpad and relocated buttons have limited its functionality, but users appreciate the inclusion of a d-pad for emulation. The development of new products frequently balances technological expertise against cost-cutting measures. While the Steam Controller emphasizes touchpads, which may not be the most comfortable or efficient input devices for the Deck, the DualSense already provides an excellent secondary controller option. Valve appears to be achieving success with the Steam Deck, thanks in part to the burgeoning compatibility of Linux with Steam games. The Steam Deck's success stems from Valve's attempts to reduce their dependence on Microsoft.Valve's Proton system is a significant improvement for Linux gaming thanks to its compatibility with Steam Deck. The build quality and Nvidia driver support of System 76 makes it a reliable system for Linux gaming. The popularity of Steam Deck makes Linux a major gaming platform, potentially leading to a migration for PC gaming towards Linux. Switch games run equally well or even better on Steam Deck due to ARM to x64 translation overhead. Microsoft may not see the Steam Deck as a threat to PC gamers on Windows. Sleep mode on Steam Deck is a significant improvement, providing an instant console-like experience for PC gaming. Despite some issues with hardware quality, the Steam Deck provides a great gaming experience.Gamers discuss their experiences with the Steam Deck, Valve's new handheld gaming device. Users report a seamless sleep and resume feature that makes casual gaming easier, a customizable interface, and impressive performance for its price point. Some report issues with the new Big Picture mode and the dock, which was not powerful enough to charge the device quickly. Users praise the Deck as a portable gaming machine that runs many games perfectly, but wish more publishers would support Linux.The Steam Deck is gaining popularity among indie game developers, especially those with pick-up-and-play type games. Some users have reported difficulties running certain games, particularly multiplayer games with anti-cheat measures, however Valve has collaborated with Epic to get EAC functional in Wine. Some users have also reported issues with the size and controls of the device. Despite this, the Steam Deck has surpassed low expectations, with some users even swapping the eMMC with a big SSD. Some developers have expressed interest in receiving a free/discounted deck from Valve, which could become more important to them as the Steam Deck gains traction among gamers.Around 75% of the Top 100 most played games on Steam are playable or verified on the Steam Deck, indicating Valve's successful testing of the device. The list excludes non-game applications, leaving the verified count at 26.8% and playable count at 48.4%. The remaining unsupported titles mostly lack anti-cheat support, but could change as Steam Deck's market grows. Valve reportedly uses internal metrics to prioritize testing, and the mentioned Resident Evil 4 Remake is expected to become playable or verified soon, pushing the percentage up to approximately 98%. Some users on Hacker News praise the device's convenience, smooth gameplay, and potential as a remote play companion.BoilingSteam reports that numerous games are playable on the newly released Steam Deck, including Fallout 4, Soundpad, Football Manager 2022, and Crusader Kings III, among others; however, others such as\u00a0Factorio, Ori and the Will of the Wisps, Forza Horizon 5, and Squad remain unsupported. BoilingSteam encourages readers to follow their social media to remain up-to-date with their content and other exclusive news. The publication also offers insight and predictions on the future of Linux gaming and an overview of the best native Linux games currently available. Readers may also donate to BoilingSteam to support their content.",
    "summary": "Valve's Steam Deck, a handheld gaming PC with a Linux-based operating system and user-friendly compatibility layer, is gaining popularity among gamers and indie game developers due to its console-like experience, powerful performance, and support from passionate communities of gamers and hackers. Users find the controls of the Deck comfortable and a significant improvement over the previous Steam Controller. Protondb's user feedback and tweaks help users evaluate compatible games. Though some users report issues with certain games and hardware quality, the Steam Deck's popularity and success may potentially lead to a migration towards Linux for PC gaming. The device's convenience, smooth gameplay, and potential as a remote play companion are highly praised by users. Developers express interest in receiving a free/deeply discounted Steam Deck from Valve to capitalize on its increasing traction among gamers. Valve's testing metrics and Resident Evil 4 Remake's expected release point to a high percentage of supported games in the near future. BoilingSteam's reports on supported games and predictions for Linux gaming can also benefit readers.",
    "hn_title": "75% of the Most Played Games on Steam Are Playable on the Steam Deck",
    "original_title": "75% of the Most Played Games on Steam Are Playable on the Steam Deck"
  },
  {
    "id": 35387160,
    "timestamp": 1680270693,
    "title": "CoScreen V5: New Screen Sharing App for Collaborative Projects",
    "url": "https://blog.coscreen.co/blog/coscreen-v5-0-a-new-way-to-share-your-screen/",
    "hn_url": "http://news.ycombinator.com/item?id=35387160",
    "content": "CoScreen 5.0 is a new, multi-display screen sharing app that allows for simultaneous and multi-directional screen sharing with audio and video chat between team members. The app features high-definition code sharing, reduced CPU utilization and latency, 60FPS+ super smooth mouse pointers, and other useful features for pairs of developers or remote teams. Currently available on macOS and Windows, CoScreen is free, but larger organizations and teams will eventually be charged for its use. CoScreen has received much attention due to its unique features in comparison to other screen sharing apps, and its flexibility for remote pair programming, brainstorming, and other collaborative projects. Linux support and a web client are in development.CoScreen, a collaborative screen-sharing tool, is rolling out a paid plan centered around active users. The company notes it is still open to feedback about its model. Users are also claiming that the pricing is too high, and the feedback is mixed on the eventual charging of the free service. The company noted that it has not tried its screen-sharing tool with Unity, but the approach of software based sharing that it employs can stream any content. CoScreen offers support for macOS and Windows, with Linux support planned in the future. Users have asked for partial screen sharing and Linux support.CoScreen, a collaboration and screen sharing tool, has launched a new version that allows bidirectional sharing and individual mouse pointers, making it ideal for pair programming and collaborative work. CoScreen V5\u00a0also allows users to share multiple windows as separate native windows, adding to the seamless collaborative experience. The tool uses WebRTC for audio and video chat, and screen sharing is accomplished via libwebrtc and native Rust. While the tool is currently free, plans are in the works for enterprise features and pricing models. The company also plans to release a web client and a headless client for Linux.\u00a0CoScreen has released V5, which features high-def code transmission, built-in audio and video chat, intuitive remote control, multi-display sharing, and a web/browser version. Remote mouse pointers run smoothly even on a 4k display, with a responsive video chat UI that is never in the way. Resource-intensive processes have been optimized for low latency and 60 FPS+ smoothness. Users can choose between watching, controlling, or drawing on the remote windows' convenient tab. CoScreen also provides fast and easy access for new users to join, even from Linux or a mobile browser. Additional improvements include state-of-the-art noise reduction, background blurring, lower CPU consumption, and novel ways to share dev tools.",
    "summary": "CoScreen V5 is a new screen-sharing app for simultaneous and multi-directional sharing with audio and video chat, featuring high-definition code sharing, reduced CPU utilization, and latency. CoScreen has received much attention due to its unique features, which include flexibility for remote pair programming, brainstorming, and other collaborative projects. Currently available on macOS and Windows, support for Linux and a web client is in development. While the tool is currently free, plans for enterprise features and charging models are in the works. The latest version of CoScreen, V5, allows for individual mouse pointers, native window sharing, and a responsive video chat UI with advanced noise reduction and background blurring. It provides a seamless collaborative experience with optimized smoothness and low latency.",
    "hn_title": "Show HN: Multi-display screen sharing with CoScreen",
    "original_title": "Show HN: Multi-display screen sharing with CoScreen"
  },
  {
    "id": 35386405,
    "timestamp": 1680267319,
    "title": "Janet Language: A New Way to Create Redistributable CLI Apps",
    "url": "https://janet.guide/",
    "hn_url": "http://news.ycombinator.com/item?id=35386405",
    "content": "Janet, a LISP-inspired programming language, is being touted as a viable option for creating redistributable CLI apps due to its ability to compile Janet programs into statically-linked native binaries. Unlike other alternatives, such as python, Janet is easy to distribute to users who have never heard of the language before, and its small binaries make it convenient for making CLI or desktop apps with good CFFI support. Janet's feature-rich ecosystem offers an excellent choice for making redistributable CLI apps, especially ones that target non-technical users. The entire language of Janet, including core library, interpreter, compiler, assembler, and PEG, is less than 1 MB, and a simple hello world compiled with Janet got only 784k on aarch64 macOS.A DevOps engineer finds a language with tiny design decisions that make it compelling. The author recommends going with a language designed for the end result of the build pipeline like Rust and Go. A comment suggests using GraalVM and frameworks like Quarkus can make native compilation for Java painless. Native image is compared to Clojure and the author mentions how it is easier to know which modules contribute to binary bloat-up with it. Python is discussed, and it is mentioned that distributing python code is not easy. Janet language is favored because it can handle native compiling and boots up faster than larger languages. No mention is made of any distinct advantage over other Scheme implementations.Janet is a small language with a practical orientation and good concurrency system, and it integrates well with C libraries. It has a lot in common with Lua, and compared to a language like Lua, Janet is packed with \"batteries\". Its standard library strikes a good balance between providing everything and nothing. With immutable structures and coroutines, Janet is much closer to Lua in scope and applications than it is to any Scheme or Common Lisp or Clojure implementations. Janet is easily embeddable in C programs and has a good community with lively discussions while being fun to learn. The book on Janet will show readers why learning it is worth it.In the book \"Janet for Mortals\" by Ian Henry, the author introduces readers to the Janet programming language which is small, simple, and usable on Windows, and has built-in concurrency, multithreading and support for parsing expression grammars making it an excellent choice for text-wrangling. The book emphasizes what makes Janet different from other programming languages such as macros, images, and PEGs before discussing basic concepts. The author also highlights the ability to distribute Janet programs as statically-linked native binaries without any runtime or package dependencies. The book is aimed at those who already know how to program, with the assumption that the reader knows JavaScript to facilitate comparisons.A book on Janet, a programming language, is also a website with a repl feature that can be accessed without installation. The book author can be contacted through the repl feature to report typos, ask questions, or express confusion. The author disclaims having any certifications to write the book, and the content should not be considered authoritative or educational.",
    "summary": "Janet Language, a LISP-inspired programming language, is being considered as an excellent option for creating redistributable CLI apps due to its ability to compile programs into statically-linked native binaries. It is easy to distribute even to non-technical users and has small binaries, making it convenient for making CLI or desktop apps, especially those targeting non-technical users. Janet's feature-rich ecosystem makes it a compelling choice, particularly for non-technical users. Its entire ecosystem is less than 1MB, and it boots up considerably faster than larger languages such as Python. Janet shares many similarities with Lua, making it an excellent choice for text-wrangling. A book on Janet, \"Janet for Mortals\" by Ian Henry, caters to those who already know how to program; the language is small, simple, usable on Windows, and has built-in concurrency, multithreading, and support for parsing expression grammars.",
    "hn_title": "Janet for Mortals",
    "original_title": "Janet for Mortals"
  },
  {
    "id": 35393458,
    "timestamp": 1680295839,
    "title": "CDC team falls ill investigating hazardous Ohio train derailment",
    "url": "https://www.cbsnews.com/pittsburgh/news/cdc-team-sick-east-palestine-ohio-train-derailment/",
    "hn_url": "http://news.ycombinator.com/item?id=35393458",
    "content": "A CDC team studying the East Palestine train derailment fell sick during the investigation; there are concerns over the incident being downplayed by officials; protection and coverage on the food supply chain are in question. The EPA's equipment may not be adequate enough to detect dangerous levels of chemicals tested for. The potential damages of this incident on human health are not yet known, but people are worried. The state\u2019s technology and infrastructure of federal bodies handling environmental disasters may be questioned yet again. The incident reveals the complex nature of medical causality and the general concern of the public towards the government\u2019s regulatory policies. The response to the incident was not reassuring for the public, and people remain wary.Presidential visits to disaster sites commonly focus on damage control before all the facts are known, and photo opportunities in safe areas can be easily coordinated for PR purposes. Catastrophes involving chemical/biological/radiological factors are riskier and more unpredictable than natural disasters, and their consequences may have societal and engineering backgrounds that exacerbate them. President Carter's visit to Three Mile Island was unique due to his history as an engineer in the nuclear energy industry, and he was specifically qualified to visit the site. While train derailments occur frequently, particularly in rail yards, efforts should be made to reduce the likelihood of hazardous material spills for public safety, not to justify complacency.Seven U.S. government investigators fell ill while investigating the possible health impacts of a toxic train derailment in East Palestine, Ohio. The investigators' symptoms included sore throats, headaches, coughing, and nausea, consistent with what some residents in Ohio and Pennsylvania experienced after the train derailment released a cocktail of hazardous chemicals into the air, water, and soil. Although it is not clear what caused the investigators' symptoms, some experts in chemical exposures say the episode is significant, adding confirmation that the symptoms reported by residents are associated with environmental exposures from the derailment and chemical fire.Residents of Ohio are reporting symptoms, including headaches, anxiety, and skin irritation, after a train derailment spilled chemicals in their area, causing the CDC and ATSDR to conduct ACE surveys to document the health of the community; preliminary results are expected to be shared with the states in early April, followed by final reports in a few months. EPA monitoring and testing have not shown any levels above expected background levels and are continuing to clean up the area, with aerators being used to increase oxygen in the water to speed up the breakdown of chemicals spilled from the train that continue to be detected almost two months later. Although officials say everything is safe, residents remain skeptical, and the state and federal government have suggested that physical complaints may be driven by fear and anxiety, rather than chemical exposure.",
    "summary": "A CDC team fell ill during their investigation of the East Palestine train derailment, raising concerns over downplayed incident severity and the adequacy of protection and coverage on the food supply chain. The incident's potential impact on human health remains unknown, and the state's technology and infrastructure may be questioned. Catastrophes involving chemicals or biological agents may have societal and engineering backgrounds, exacerbating their unpredictability. Seven U.S. government investigators also fell ill, further confirming potential health impacts of the toxic train derailment. While officials claim everything is safe, residents remain skeptical, and physical complaints may be driven by fear and anxiety.",
    "hn_title": "CDC team studying East Palestine train derailment fell ill during investigation",
    "original_title": "CDC team studying East Palestine train derailment fell ill during investigation"
  },
  {
    "id": 35381968,
    "timestamp": 1680231783,
    "title": "Efficient Cosine Computation with Polynomial Approximation in C: Challenges, Trade-Offs, & Insights",
    "url": "https://github.com/ifduyue/musl/blob/master/src/math/__cos.c",
    "hn_url": "http://news.ycombinator.com/item?id=35381968",
    "content": "An implementation of cosine in C efficiently utilizes polynomial approximation's coefficients. The approach considers precise derivatives and overall precision better than the equivalent Taylor series. Range reduction, worth its article, conducts by modulo pi/2 noiselessly. It supports double-double precision and utilizes hardware implementations on GPU and CPU. Such an implementation in a 6th order approximation, given the float definition, is more than good enough. An approximation function used in vacuum-tubes based surface-to-air systems of the 60s denotes cos(\ud835\udc65)=0.7. By using the Remez algorithm, the approximate polynomial of best cosine approximation was obtained. There are other Python packages for generating polynomial approximations like Pythonsollya, SageMath.In a discussion on Hacker News, users share insights about implementing transcendental functions and the challenges posed by interdisciplinary technical work. Users discuss learning implementation of the Remez algorithm and the Parks McClellan filter design. Additionally, users talk about their experiences with math libraries in early computing and discuss the use of the CORDIC algorithm for sine generation. Finally, users debate the benefits of implementing decimal floating point in digital calculators.Discussions on decimal floating-point representations, fixed-point implementation, and trigonometric algorithms have occurred on Hacker News. Decimal floating-point is easy to implement in hardware, and BCD floating point is useful as it can display numbers directly. Fixed-point is widely used in embedded microcontrollers and automotive applications without hardware floating point or hardware trig functions. The advantages of floating-point lie in its ability to represent values with different magnitudes; the limitation of fixed point is dependent on the scale. The main challenge of fixed point is its scale-dependent accuracy. High precision depends on the number of fractional bits in fixed-point numbers. The discussion also covers the benefits of recursive definition in calculating cosine values.Experts in numerical methods and professionals working on synthesizers and embedded systems showed interest in a Hacker News post discussing fast and accurate methods for computing sine and cosine. The post explains the mathematics behind the Taylor series, along with numerous implementations using various techniques, such as interpolated table lookup, double-double arithmetic, and more. Discussions include comparisons of methods and trade-offs for accuracy and speed, along with insights on fixed-point numbers, floating-point rounding errors, and the like. Overall, the post provides a detailed and informative account of the challenges and opportunities involved in implementing sine and cosine algorithms.The post discusses a numerical analysis topic in applied mathematics pertaining to polynomial approximation of cosine functions. The code is standard and utilizes techniques for getting higher precision out of a floating-point value. Minimax approximations, not Taylor series, are used to consider everything in a specific area to be essential, making minimax polynomials better in real-life situations than Taylor series. The function implemented in the code is optimized and employs bit manipulation and delves into complexities of numerical analysis, making it an unusual specialization. Understanding the context and requirement is essential, and it's not necessary to write a cosine function from scratch, as existing libraries or game engines can be used. Finally, it's vital to realize that programming expertise can manifest differently in mathematical and other aspects of computer science.Though programming in AI requires higher-level math skills (as illustrated in the 'deep-learning book' by MIT Press), general programming and neural net models, do not necessitate math skills above high-school level. There is also a discussion on unusual function names for math arguments, leading to confusion for some. Additionally, there is an example of fast inverse square root function and its questionable readability. Finally, this post provides a source code for the cosine function which is shown to be accurate under various conditions.",
    "summary": "Efficient polynomial approximation of cosine in C takes precise derivatives and range reduction with modulo pi/2. It supports double-double precision and uses GPU/CPU hardware implementations. Discussion on Hacker News shares insights on Remez algorithm, Parks McClellan filter design, and decimal floating-point. The post provides a detailed account of sine & cosine algorithm implementations, trade-offs, and insights in numerical analysis. The context & requirement matter, expertise can differ, and source code for cosine function is a bonus. Programming in AI requires high-level math skills, while general programming & neural nets do not.",
    "hn_title": "Cosine Implementation in C",
    "original_title": "Cosine Implementation in C"
  },
  {
    "id": 35381755,
    "timestamp": 1680230138,
    "title": "SVG's Potential as an Open Standard Version of Flash Discussed Amid Debate Over Flash's Decline & iPhone's Removal of Headphone Jack",
    "url": "https://leonidasv.com/til-svg-specs-almost-got-raw-socket-support/",
    "hn_url": "http://news.ycombinator.com/item?id=35381755",
    "content": "A discussion on Hacker News about SVG's network support for raw sockets highlights the potential of SVG to become an open standard version of Flash, which would be a game changer. Flash was predominant in the early-mid 2000s internet as it allowed for scalable vectors that could easily\u00a0shrink large games to playable sizes. The iPhone did not support Flash, and its subsequent drop in support finally led to the death of the consumer web technology. Although raw sockets seem irresponsible by current standards, it was the norm back when the internet was decentralized\u00a0and had fewer risks. The post also includes a debate over the decline of Flash and the rise of HTML5.Flash's demise was not solely due to battery usage issues, but rather a culmination of factors, including poor software quality, lack of regard for performance and security, and failure to adapt to the mobile device landscape. Flash Lite was not a satisfactory alternative due to its limitations on codec support and network features. While several products aim to replicate Flash's animation capabilities, they lack the ease of use and popularity that made Flash appealing for amateur game developers. The loss of the mouse and keyboard as primary input devices for web interactions also contributed to the decline of Flash. However, Flash's proprietary nature poses a challenge to the preference for open-source solutions in the tech world.Anti-Flash argument misunderstood at the time; majority of people who noticed were happy; losing authoring environment was a blow; invasive browser plugins disappearing is a success; flash was terrible and a security nightmare; Flash was used for more than just games and animation; HLS enabled the adoption of an adapted video streaming pattern; Apple bet on wireless future and the adoption of Bluetooth headphones; competitors derided Apple for removing the jack, but adoption of wireless headphones was increasing already.Users debate the concept of removing the 3.5mm headphone jack from recent iPhones: while some suggest it has been used for other things, the best Bluetooth earbuds don't compare to 3.5mm in sound quality, and wireless adds another charging dilemma. While adapters are fine in theory, they often do not work with different platforms or are clunky. Moreover, Bluetooth has latency and re-encodes audio to a lossy format. Critics argue the removal of the jack seemed driven by DRM concerns, a desire to sell accessories, or the minimalism trend to shave production costs. There is a fundamental opposition against turning everything into a locked-down consumer-grade device but critics also acknowledge that Apple has a fair share of the merits for setting the design standards.The article discusses the historical competition between Flash and SVG, with some contributors suggesting that SVG was originally positioned as a rival to Flash. It notes that Flash had raw TCP socket support and was one of the first mainstream devices to avoid using Flash for browsing. Some readers express nostalgia for Flash's ease of use and the lack of restrictions in website animation, while others suggest that Flash and other similar software were too unfashionable to survive. The article also includes links to resources about SVG and an authoring tool for SVG that could be a potential replacement for Flash's authoring tool.Flash and Visual Basic died because they were designed for a changing environment, not because they were unfashionable. Flash was unstable and insecure, with poor performance on mobile devices. VB was designed for the unconnected 32-bit desktop world, and Microsoft wanted to drop it. SVGs are poorly optimized but can be interactive with buttons and sliders. SVG had nearly been forgotten before its resurgence due to the end of Flash and the rise of D3.js. Raw socket access in the browser has potential for nefarious use, including exploiting vulnerabilities in devices on a user's local network. SVG files have the potential to scale but can lose fidelity if zooming in on a pixel oriented image. The addition of websockets and webrtc to SVG makes full applications in .SVG files possible. SVG aimed to compete with .SWF and was successful on maps systems that intended to be purely standards-based. SVG also implemented high and low power profiles that were incompatible.In 2004, the SVG 1.2 Full specification draft proposed network support for raw sockets. This was meant to allow computers to talk to any protocol or non-HTTP endpoint securely. However, the proposal was retired, and the SVG 2.0 Full spec never touched on this idea again due to security concerns. The original SVG spec aimed at mobile devices with limited scripting capabilities, and it wasn't until later that SVG was used for displaying static vector graphics. Despite the proposal's initial enthusiasm, it went nowhere, partly because of the Internet Explorer Year of Shame when remote code execution flaws were discovered.",
    "summary": "The potential for SVG to become an open standard version of Flash is discussed in a Hacker News post exploring SVG's network support for raw sockets. Flash was widely used in the early 2000s but declined for various reasons such as poor performance, security issues, and failure to adapt to mobile devices. Some mixed opinions around iPhone's removal of the headphone jack are discussed, along with debates around Bluetooth audio quality and charging. SVG was originally positioned as a rival to Flash and could potentially replace it as an authoring tool. However, SVG files can lose fidelity when zooming in on a pixel-oriented image but have potential for full applications with websockets and webrtc.",
    "hn_title": "When SVG almost got network support for raw sockets",
    "original_title": "When SVG almost got network support for raw sockets"
  },
  {
    "id": 35390853,
    "timestamp": 1680286024,
    "title": "Heat pumps surpass gas furnaces in US homes despite challenges",
    "url": "https://electrek.co/2023/03/31/heat-pump-sales-2022/",
    "hn_url": "http://news.ycombinator.com/item?id=35390853",
    "content": "Heat pump sales surpassed gas furnace sales in the US in 2022, with only about 13% of households owning a heat pump, leading to confusion among some homeowners about why they don't have one. The natural gas heating is still surprisingly energy-efficient but often less so than modern heat pumps, which are also gradually getting much better. The disadvantage of air source heat pumps is decreased efficiency in low temperatures as compared to geothermal systems. The question of whether to use a furnace or a heat pump depends on the specific need, with furnaces being better on very cold days and heat pumps being cheaper and requiring fewer maintenance tasks.Many homes across the US lack central air and rely on alternative methods to heat and cool, with cost being a major factor influencing their choice. Heat pumps have become a popular alternative, but they can be expensive to install and may not be practical for every home. The efficiency of heat pumps has improved over time, making them more practical for cold climates, but natural gas remains a cheaper alternative for heating in some regions. Window units and multi-zone mini splits are also popular methods for heating and cooling, but they also come at a cost. Overall, the lack of central air and reliance on alternative methods is a significant issue for many households in the US.The article discusses the popularity and practical use of heat pumps in different areas of the US. Some commenters note that a lack of gas infrastructure and availability of cheap electricity make heat pumps more common in some regions, while others share their experiences and opinions about the effectiveness and efficiency of heat pumps compared to traditional furnaces or boilers. The article also mentions recent tax credits for heat pumps as part of household electrification initiatives. Various commenters also discuss the potential environmental and economic impact of heat pumps versus traditional heating methods.Leaky gas infrastructure contributes to substantial methane emissions, making shale gas potentially worse than coal. However, various sources of landfills, agriculture, and methane emissions still require reduction. While natural gas is relatively abundant in the United States, the efficacy of using heat pumps versus gas furnaces varies by location and electricity generation methods. While efficiency numbers depend on various factors, many places have better green fundamentals for heat pumps considering mild temperatures and non-fossil fuel energy generation. The momentum towards gas still massively influences installer recommendations, which hinders progress towards renewable energy implementations. However, localities with grid energy being primarily generated from hydroelectric power can still benefit from a gas furnace.Heat pumps are a more efficient option for heating than gas boilers in areas where sustainable energy is abundant, but the decision relies on the cost of electricity and gas in the area. Salespeople should consider differences in local energy sources and carbon emissions when pushing a particular product. Heat pumps have a higher operating cost concerning cooling costs, but heating costs can be significantly lower. The efficiency of the heat pump depends on the ratings of the pump and varies all over the country. Natural gas heat pumps are available but are usually rare. Legislation is making it hard for multi-family spaces to switch to heat pumps from traditional heating methods.Modifying existing buildings for new technology faces legal questions, cost allocation, space division, electrical and plumbing requirements, and regulatory roadblocks. Single-family homes and new builds face fewer obstacles. Renters face difficulty accessing cost-saving technologies that landlords have little incentive to install. Governments need to consider regulatory changes, such as incentives or mandates, to facilitate retrofitting of existing buildings. Heat pumps can reduce the need for large condenser units in retrofitting, but not all spaces can use them. NYC is phasing out gas infrastructure in new buildings, but challenges remain for heating and hot water in older buildings. Air conditioning can provide efficient heating, but hot water may still require electric resistive heating. Contractors are used to many unfeasible retrofit requests, so they apply a 10x discount to the number of calls they receive.The article discusses personal experiences and opinions on the effectiveness and efficiency of various heating and cooling systems, including boilers, radiators, heat pumps, and forced air systems. The author highlights the importance of proper insulation and maintenance to maximize efficiency and cost savings, and briefly touches on available financial incentives for energy-efficient upgrades. The discussion in the comments section includes further insight and recommendations from other readers, as well as frustrations with high prices and complicated installation processes. Overall, the article provides valuable insight into the realities and nuances of home heating and cooling systems.The International Energy Agency's analysis reveals that heat pump purchases in the US surpassed gas furnace sales in 2022. The analysis indicates that heat pump sales grew by 11% globally, with nearly 40% in Europe, primarily for air-to-water models that can work with typical radiators and underfloor heating systems. Key drivers include increased policy support and incentives due to high natural gas prices and efforts to reduce greenhouse gas emissions. Heating devices implemented in roughly 10% of all buildings globally, over 100 million households or 1 in 10 homes, need to double implementation to achieve climate goals.",
    "summary": "Heat pump sales have surpassed gas furnace sales in the US due to energy efficiency improvements, but decision-making depends on local factors such as cost and climate, with natural gas remaining cheaper in some regions. Multiple heating and cooling methods are popular but come at a cost. Landlords face little incentive to install cost-saving technologies, making retrofitting of existing buildings difficult. The article provides valuable insight into the realities and nuances of home heating and cooling systems, including the importance of proper insulation and maintenance to maximize efficiency and cost savings. The International Energy Agency's analysis reveals that key drivers for the global growth in heat pump sales include policy support and incentives targeting greenhouse gas emissions reduction.",
    "hn_title": "Heat pump sales outpaced gas furnace sales in the US in 2022",
    "original_title": "Heat pump sales outpaced gas furnace sales in the US in 2022"
  }
]
