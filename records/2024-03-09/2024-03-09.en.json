[
  {
    "id": 39640992,
    "title": "Building a Safe Home Lab: Essential Guide",
    "originLink": "https://linuxblog.io/home-lab-beginners-guide-hardware/",
    "originBody": "Home Lab Beginners guide (Hardware) March 8, 2024 by Hayden James, in Blog Linux Until recently, and for well over the past decade, my wife and I have been nomads. Moving from the Caribbean to Miami, New York, Las Vegas, Vancouver, and now back home. This has meant that for many of those years, my home office basically comprised of a few laptops and screens. These days, we are more settled; my wife has completed her studies, works full-time, and my full-time job remains remote, via a home office supporting Linux servers and hosting them. After a short trip to Antigua, I had a brief meeting with a tech friend of mine, Yves Ephraim. He was an Engineer with Cable & Wireless for 20 years (until 2003). Yves left C&W shortly before I started there in 2004. He’s a champion of the internet and technological developments in Antigua & Barbuda. We sat in his home office, where he told me a bit about his current projects, as well as his dabbling with IPv6, web hosting, mail, BGP4, among other things. Most of which he does via his very capable home lab. Table of Contents What is a Home Lab? May we inspire each other to great home labs Home lab location, it’s all about location. Network vs. Server Racks vs. Cabinets? Recommended Home Lab Hardware To replace or not to replace: ISP Cable Modem Selecting Your Home Lab Rack Home Lab UPS (Uninterruptible Power Supply) Universal Home Lab Rack Shelf Rack Mount Home Lab Power supply Rack Mount Home Lab Cooling Fans Home Lab Routers and switches Home Lab Patch Panels and Network Cables Home Lab Servers Summary and Conclusion What is a Home Lab? My current Home lab in a 12u rack as of May 2020 Think of a home lab as a place where you can fail in the privacy of your own home. As Thomas A. Edison said: “I have not failed. I’ve just found 10,000 ways that won’t work.” I consider myself an expert at failure. But seriously, I would like to fail a lot more, and a home lab will create endless opportunities for me to fail. Of course, all the while seeking success, but you got that already. In general, a lab is a place where you can safely perform experiments. Most of you reading this article are techies and sysadmins. As you know, trying out new things on production equipment never ends well. Shush… it’s OK, I know, I know, you didn’t think that one command would take everything offline. This risk is the reason why we build ourselves a sandbox environment to dabble, test, and fail in, all from the comfort of our own homes. To date, I host my test labs on servers in North America and Europe. I’m creating a customized network and server home lab; to fill those areas where I would like to become more familiar. I will also be using my home lab for remote backups, network monitoring and alerting of remote servers, and wired UAP APs, among other things. May we inspire each other to great home labs As I told Yves, our chat had me revisiting the idea. In fact, I think my exact words were, “Yves, I see your home lab, and I’ll raise you mine.” As always, I’m certain there’s a lot more to learn through discussion. In light of this fact, I wanted to share this journey with you, starting with the hardware selection process for my home lab. Of course, over the years, I’ve had equipment lying around, but that stops today. Well, sort of. So, if you have ever been interested in building a home lab or if you already have one, then let’s mingle, share and dabble together. Where do we begin? Not to sound cliché, but let’s start from the ground up, or wall forward. What I’m trying to say is, we need storage space, physical storage space, for our new home lab. Home lab location, it’s all about location. Location, Location, Location. It’s all about location! Please excuse the lack of originality – I’m a sysadmin, not a writer. I’ll try my best to get you to the end of this article. Location is critical for several reasons. The choice between the home office, living room, closet, attic, basement, or garage depends on a range of important variables. These include room temperature and ventilation, workable space around your equipment, ease and distance of network cable runs, foot traffic, 24-hour ease of access, power, noise levels from your home lab, and more. Here’s a quick list of Pros vs. Cons I’ve compiled to get us thinking about all the possible home lab locations. Choose wisely: Home office Pros: Proximity to work area/desk/devices, fewer cable runs, and you can watch lights flash all day. Cons: no home office or you already spend too much time in your home office. Living room Pros: Usually cool, lots of space/setup options, blinky-blinky sci-fi movie nights, and counts as family time. Cons: divorce, foot traffic, could get damaged, or damaged during the divorce. Closet Pros: Easily accessible, stealthy, and you get to say: “Look at what I have hiding in my closet.” Cons: Poor ventilation (excess heat), lack of space, and one less closet = unhappy wife. Basement Pros: Usually cooler temps, and volunteering to do laundry (maybe con?). Cons: don’t have a basement, flooding, spiders, or no access when injured. Attic Pros: Less noise, easier cable runs. Cons: can get hot depending on where you live, roof leaks, humidity/condensation, and creepy at night. Garage Pros: Less noise in the house, completely out of sight, wife won’t even notice. Cons: A Bug’s Life in your lab, excess heat (if no AC), dust, could require longer cables, or gets wrecked when parking the car. For my lab, I’m going to build it out in my home office. I have laptops, a desktop, a server, and other devices in my office, so that this location will require shorter cable runs. When selecting your location, consider the coolest area of the room/space, avoid direct sunlight and consider reserving space for future expansion. It’s wise if you draw/sketch out a home network diagram or use network design software. Keep in mind your floor plan and how you will accomplish the cable runs. For example, most of the homes on this island have 8″ concrete interior walls. Think about these things before, not after. In other words, map everything out, make notes and create lists. Network vs. Server Racks vs. Cabinets? Next, we need to decide how we will store the equipment (modems, routers, switches, servers, patch panels, UPS systems, power strips, cooling fans, etc.). Network cabinets and racks are often confused with Server cabinets and racks. Routers, switches, patch panels, and the like are usually much shallower than servers. As such, Network Cabinets and Racks are usually not as deep as Server Cabinets and Racks. Also, networking devices often produce less heat than servers. You will find some network cabinets will have glass doors that may not leave enough ventilation for servers. After deciding the depth and ventilation requirements for your home lab, there are a couple of other things to consider. A cabinet is an enclosed space with door(s) and/or removable sides, whereas a rack is a semi or fully open (4 sides open) frame. To help you decide whether to use a cabinet or rack, consider the following: If you are installing large, heavy servers, then the extra stability of cabinets or four-sided racks should be considered. If you need frequent access to the sides or rear of equipment, then an open rack or cabinet with removable sides would work well. If your equipment requires extra cooling, an enclosed cabinet will need more attention to cooling and ventilation. If the room is prone to dust, the extra protection of a cabinet will go a long way in keeping it out of your equipment. If you are installing in a general living area frequented by house guests, consider an enclosed cabinet that can often look neater in appearance when locked. However, a well-maintained open rack with tidy power and network cable runs can look just as neat! If restricted access/security is required, many enclosed cabinets often offer lock and key access control for better security. Recommended Home Lab Hardware UPDATE: I’ve updated the article. Some of the devices mentioned have been discontinued or are not readily accessible anymore. I’ve updated with upgrades I’ve made and other recommended hardware and accessories. It’s been nearly four years since my original homelab build. Now that you have already measured your equipment’s maximum depth and considered all the above advice, it’s time to buy your first piece of hardware. Or not. You could have probably skipped to this section if you prefer more em’, creative methods for hosting your home lab. However, most of the above advice applies even if you plan to set up your home lab in your existing living room cabinet, on a bookshelf, under your desk, or in a cardboard box. No, seriously… Image: Fancy cardboard box cabinet. NOTE: You can get most of the equipment listed below via Craigslist, Universities, and other local second-hand options in some countries. Prices on this page reflect Amazon’s current pricing, which is subject to change. As an Amazon Associate member, I’ve included paid links to items, for which I’m compensated. For all other items not listed on Amazon, if the links become broken over time, please let me know, as it always seems to happen often, which is another benefit of using Amazon’s links. Let’s move on to the recommended home lab hardware. To replace or not to replace: ISP Cable Modem Before we delve a bit deeper into my home lab recommendations, if you are a beginner to switching and routing, then a lot of the hardware below may seem to be a bit of an overkill. You can get your feet wet by merely replacing your ISP’s cable modem (ISP or Internet Service Provider), you know, that thing with all the flashing lights that you frequently have to unplug to reset to regain internet access. Yup, that one, the mini heater in your living room. Most US ISPs charge monthly rental for that thing. Buying your own could make some financial sense. As well as giving better performance, reliability, and security. Bet you were already sold with the financial bit eh? But it gets better; most of these modems provided by ISPs are usually of poor quality, used, and lack features. Lastly, your ISP’s Modem/Router combos often cannot independently update firmware versions as new exploits emerge. This means you are at the mercy of your ISP to push new patched firmware. If you have a great ISP and this does not apply to you, stick with your modem. The only reason to replace it in such circumstances will be to have access to additional network features and create a home lab where you can test away! I replaced the modem/router combo from my ISP. I went with the Motorola MB7420 16×4 Cable Modem. Update: 3 years later, there are much faster models, for example, the Motorola MB7621 32×8. If you would like to keep things simple, then go with a Modem + Wi-Fi router combo. Otherwise, continue if you would like to build out a home lab to manage Firewall, NAS storage, VPN, VLANs, PoE (Power over Ethernet), Virtualization, remote network infrastructure monitoring/alerting, wired APs, web servers, backup servers, the list goes on. Update: This modem, similar to most I’ve used, gives off quite a bit of heat. When deciding placement, I would suggest toward the top or leave space below and above. That said, I love this modem! Unlike the ISP’s modem, it shows the quality of your connection, including Freq. (MHz), Pwr (dBmV), SNR (dB), corrected and uncorrected, and even allows manual downstream frequency setting if needed by your ISP. Selecting Your Home Lab Rack A rack unit (abbreviated U or RU) is a unit of measure defined as 1 3⁄4 inches (or 44.45 mm). It’s the unit of measurement for the height of 19-inch and 23-inch rack frames and the equipment’s height. The height of the frame/equipment is expressed as multiples of rack units. A typical full-size rack is 42U high. Racks and cabinets are then listed as 1U, 2U, 3U, 4U, and so on. Here’s an example below of a 6U rack (labeled): StarTech 12U Wall Mount Rack – 13.5″ Deep / 19″ wide / 125lbs Capacity – $158 (specs) – During my search for a wall-mounted rack, I almost purchased the 6U version of this rack, then at the last minute, I switched to the 12U version for possible future expansion. Samson SRK-12 Universal Equipment Rack Stand – $230 (specs) – This was my first choice; this is an audio equipment rack that is fully compatible with shallow-mount equipment. One advantage is that it’s supported on wheels, so you don’t have to fix it to a wall or floor. However, I’m 6′ 2″, and I want to mount my rack about 4′ off the ground, so as you will see, I have also purchased a 2U cooling fan (intake), which I will place at or to the bottom of my setup. If your room is air-conditioned 24/7 and you don’t mind the low-to-floor setup, this may be the rack for you. Here’s a great example of how clean this Rack can look when set up. NavePoint 9U Basic IT Wall Mount Network Rack Locking Glass Door – $235 (specs) – This is a very beautiful-looking Network Cabinet. The issue for me is that I only use the AC in my office in the summer, and I don’t want the heat ever to be an issue; thus, for my home setup, an enclosed cabinet wasn’t right for me. Update: This is a very solid wall-mount rack. I can confirm that what you see is what you get. No issues to report. It does exactly what it’s supposed to. Home Lab UPS (Uninterruptible Power Supply) At home, we have a backup generator that runs on propane. It can run for up to a week, which is useful in the tropics. Other than hurricane season, power cuts are extremely rare here. So far, for 2020, we have had one outage due to a lightning strike on a nearby electrical post. It takes a minute or so to failover to the backup generator. So the rack mount UPS solution I’m looking for is something that will support between 100W and 200W for a couple of minutes. Long enough to fail over to the backup gen or to shut down all the equipment cleanly. In the future, I may expand this with larger batteries, but with no way of returning anything I purchase, I’m a bit wary of investing in a larger backup battery only to find out it’s a dud. I will test for a year first, and so here’s what I decided on: CyberPower OR500LCDRM1U UPS, 500VA/300W, 6 Outlets, AVR, 1U Rackmount – $220 (specs) – A small 1U rackmount UPS coming in at 20lbs, which should give me around 10 to 15 minutes of runtime. Ultimately, you will have to decide whether you even require an uninterruptible power supply at home. From experience, UPS batteries are almost always a disappointment, and this is largely because of the cost vs. return. If you really need to have a UPS, you could spend thousands just to have a long enough runtime. If you work from home or host critical services hosted from home – you probably shouldn’t – then it may be worthwhile. My advice, get a UPS that provides just enough runtime for everything to shut down cleanly and only use the UPS for the most essential and efficient devices in your home lab. With all things considered, I’ll have to come back to this section before recommending any other UPS models. Update: Surprisingly, the battery performance of this UPS so far is above my expectations. I ran it down to around 50%, which gave me just over 30 mins of run time, including powering 2 PoE APs and a 27″ monitor. As mentioned later in this article, pay attention to your power consumption. This was something I considered, and it’s paid off thus far. The APs for example, max power consumption is just 6.5W per AP. The max load is about 20% to 30% of UPS capacity. Universal Home Lab Rack Shelf Did you notice that the CyberPower 1U UPS does not have rack mounts? Of course, you could rest it on top of other equipment; however, I wouldn’t recommend that because we should keep the batteries as cool as possible and leave space between equipment to reduce the amount of trapped heat. Lastly, for this setup, it will look more professional if placed on a ventilated 1U rack shelf: AC Infinity Vented Cantilever 1U Universal Rack Shelf, 10″ Deep, for 19” – $30 (specs) – These shelves are available from 6″ to 16″ deep. The backup battery is 9.5″. Pay attention to the depth of your rack/cabinet and equipment before deciding what you need. Update: Very sturdy, great for keeping equipment cool and cable management thanks to the vents and slots. Rack Mount Home Lab Power supply The aforementioned CyberPower UPS comes with six outlets – four backup /w surge + two surges only. Currently, under my desk, I’m using seven outlets, and this is before ordering a replacement ISP modem, router, switch, servers and does not consider any future expansion. Right above the UPS, I will be mounting the following: ADJ Products AC POWER STRIP (PC-100A) – $30 (specs / alternative) – will plug this into the 1U CyberPower UPS. Update: This is a very convenient piece of hardware. It plugs directly into the UPS and allows easy on/off switching of other equipment and devices. Rack Mount Home Lab Cooling Fans Maintaining an optimal temperature for rack-mounted equipment prevents overheating, ensures consistent performance, and extends their lifespan. The plan is to mount an intake fan at the bottom rack unit position (remember hot air rises). After testing, I will decide whether I’ll need an exhaust fan at the very top of the rack. AC Infinity CLOUDPLATE T7-N, Rack Mount Fan Panel 2U, Intake for 19” Racks – $130 (specs) – an intelligent cooling fan system providing quiet high airflow cooling. The system is housed in a solid aluminum 2U case that’s less than 4 inches deep. It includes a thermostat for automatic fan speed during varying temperatures. A max airflow rating of 220 CFM will provide useful cooling to server, network, and other equipment. AC Infinity CLOUDPLATE T1-N, Rack Mount Fan Panel 1U, Intake for 19” Racks – $150 (specs) – A more compact 1U design that moves less air (up to 60 CFM) if you are low on free rack units. AC Infinity CLOUDPLATE T2, Rack Mount Fan Panel 1U, Top Exhaust for 19″ racks – $150 (specs) – This 1U top exhaust cooling fan can move up to 300 CFM. At 13.5″ deep it will be either a tight squeeze or will need modification for my rack. So I have not ordered this until I can examine it closer. Update: At the lowest fan speed, this cools rack temps by about 2 to 3 degrees Fahrenheit. I’ll update my thoughts again in the summer. Update 2: As of September 2022, two of the four fans have failed. I’ve removed it from the rack and will try to clean it with compressed air. Over the past 2 years of use, they are filled with dust! I still recommend buying these units, but I also recommend cleaning them out more often than I have! Update 3: As of September 2023 – 3 of the 4 fans have failed again. Home Lab Routers and switches After many weeks of research, online comparisons, reading reviews, watching YouTube videos, and countless hours spent on /r/homelab, I can safely say that the following three companies’ devices will fully cover all of your routing and switching needs: Cisco > Ubiquiti > TP-Link. Sorry TP, nothing personal you are just 3rd, but not last; is that better? For my setup, I’m going more small-business than a home office as far as equipment goes. I want to be able to have future access to additional features. The following were my selections, followed by the runner-ups. Routers Multi-WAN Load balancing: Peplink Balance 20X [CAT 7]Futureproof Gigabit Dual WAN Router – $500 – Simultaneous Dual-Band (2.4GHz / 5GHz), Wi-Fi 5 1x WAN Port, 4 x LAN Port, 1x Embedded Cellular Modem with Redundant SIM Slots, 1x USB Interface 1 Gbps Router Throughput. Small Biz features: Ubiquiti EdgeRouter ER-10X, 10 Port Gigabit Router with PoE Flexibility – $220 (specs) – (10) Gigabit RJ45 Ports, PoE Passthrough on Port 10, Dual-Core, 880 MHz, MIPS1004Kc Processor, 512 MB DDR3 RAM, 512 MB NAND Flash Storage, Internal Switch, Serial Console Port. Home lab Value: TP-Link ER7206 Multi-WAN VPN Router – $150 (specs) – 1 Gigabit SFP WAN Port + 1 Gigabit WAN Port + 2 Gigabit WAN/LAN Ports plus1 Gigabit LAN Port. Supports up to 100× LAN-to-LAN IPsec, 50× OpenVPN, 50× L2TP, and 50× PPTP VPN connections. Switches Enterprise features: Ubiquiti EdgeSwitch ES-10XP, Managed 10-Port Gigabit Switch with PoE – $215 (specs) – (8) Gigabit RJ45 Ports, (2) SFP ports, 24V passive PoE output on all RJ45 ports, 10 Gbps Total Non-Blocking Throughput, 20 Gbps Switching Capacity. Home lab Value: TP-Link 8-Port Gigabit Ethernet Easy Smart Switch (TL-SG108E) – $40 (specs) Unmanaged Pro, Plug and Play, Shielded Ports. There’s also Netgear, pfSense, and many, many other options, but you want me to complete this article, don’t you? Of course, this means that you will still have to research the recommendations on your own, as not all of my picks may fit your requirements. Update: Just what I needed. The router runs modified Debian. How cool! It’s packed with enterprise features. I’m excited to learn full command line management. Home Lab Patch Panels and Network Cables In hindsight, I should have gone with the 24 port patch panel. Instead, I went with this 16 port patch panel because, well, I liked how centered and less busy it looked. I know. I don’t want to hear it. But look at that thing! If it isn’t already obvious by the 16 ports, it’s the image just above. The fastest internet plan available from my ISP is 160Mbps down / 30Mbps up. For my home lab, I won’t be needing CAT8 or CAT7 anytime soon, and I don’t need high LAN traffic. So I purchased 250′ of regular CAT6 reel and some different lengths Cat6 as I’m ok with the 1000Mbps limit. Yeah, I also like my coffee black. If you want smoother network traffic, then, by all means, go for at least CAT6a. However, make sure you go with a shielded patch panel. Going CAT7 cable or Cat6a is a waste of money if your RJ45 connectors and patch panels don’t support it. “Keep it simple stupid!” No, I’m not talking to you; that was me speaking to myself while selecting the patch panel and cables. Update: High quality! Easy to punch down. All my patch connections worked on the first try. Home Lab Servers For this article’s purpose, I won’t recommend any specific servers because this will vary a ton depending on what you will be hosting. NAS storage, VMs, web servers, backup servers, mail servers, ad blockers, and all the rest. For my requirements, I purchased a ThinkCentre M73 and ThinkCentre M715q; both used off eBay. As far as full-fledged servers go, some good options are Dell, HP, Cisco, and Lenovo. For home labs, these are often best purchased used from Craigslist or eBay. You can use Wikipedia to search the specs of older generations, for example, Dell PowerEdge model history. Update: AMD has raised the bar. I’m most impressed with the CPU performance of the M715q. They both run quiet and cool, with Ubuntu Server and Windows 10. Summary and Conclusion Image: your setup, should be your setup. As traditional or as custom as you’d like! When writing this guide, I tried my best to convey my passion, thoughts, and considerations throughout the selection process. It’s important that you find your passion when it comes to building your home lab. What will you use it for? How will it benefit you and others? If you have a plan, and if your lab will be something you enjoy learning, failing, and succeeding with, it will pay for itself in one way or another. That said, be careful with the power consumption! You don’t want to end up with… mortgage of $1,100, a power bill of $1,200, the feeling when walking into your home lab… NOT priceless. Plan with room to expand, and feel free to start small. You don’t have to start with a rack, cabinet, patch panels, and all the other gear. Get your feet wet, replace your ISP modem, then build from there. Avoid making rash decisions on hardware; instead, sleep on it. Then, post a question or two in the new community forums, to get valuable feedback from other techies and sysadmins. You will find that the more you research, it will open up additional ways to get things just right or very wrong. More importantly, have fun! Share your home lab goals with your spouse, colleagues, kids, and that neighbor across the street who still only waves after five years. I hope this article was useful and inspiring and opens a dialogue between us. Feel free to contact me. I love hearing, supporting, and learning from you guys! Update 1 – March 10th, 2020: As of today, I’ve almost completed the setup of my new Home lab, and it’s now fully functional. I’m pleased with all of the items purchased. For this update, I’ve added to the article some highlights, tips, and findings below each hardware item in italics. One thing I missed in the original article and Home lab setup was the coaxial cable. The existing cable turned out to be about three feet shorter than required. Tomorrow, I’ll replace it with a longer cable. As you can see, I’ve also replaced the ISP modem with the Motorola model listed below. Unfortunately, the Ubiquiti EdgeRouter and EdgeSwitch turned out to be about an inch too wide. I failed to consider this because I measured their combined width and then compared that measurement to the rack’s interior width instead of the width of the rack shelf. Oops! Now, so that the switch doesn’t remain sitting on the router, I’ve just ordered two Ubiquiti rack mounts. Better cooling and aesthetics, but also the rack mounts will allow positioning the router and switch directly below the 16 port patch panel pictured below. Check back in a week or two for another update. My Home lab – a work in progress (everything pictured is listed below) Update 2 – March 25th, 2020: I Will follow up with details and feedback shortly. Here’s an updated status photo: Update 3 – April 19th, 2020. Over the past few weeks, I’ve added a few things. A Netgear 4G LTE modem (and antenna) for internet auto fail-over from ISP1 (cable internet) to ISP2 (4G internet). 1U Blanking Panels to improve the efficiency of the cooling fans. The previous two vented 1U blanks are now at the top of the rack. It will be used to mount a monitor. (see to-do’s below) Quick access 1u mess cover to prevent accidental power switch toggling. Rackmount kits for the EdgeRouter ER-10X and EdgeSwitch ES-10XP. Cleaned up all wires and cables. Labeled network cables but didn’t label the patch panel, of course! Added VLANs, restricted bandwidth of guest devices as well as some other home devices, hardened firewall. Installed Zabbix server for additional monitoring of 150+ remote servers. Added a domain to static IP, which has a couple of public-facing subdomains. Removed or smoked out some of the logos and text for a cleaner look. (aka. OCD) Added x2 UAPs and x2 beacons. Outlet savers so I could make use of all available power outlets. Added push notification for when ISP1 goes down since switching to backup ISP2 is completely seamless. To-do’s: Add a separate 21″ monitor to display Zabbix hosts/dashboard. Dabble with pfSense. Reset everything to defaults and reconfigure with lessons learned. Add indoor PoE surveillance cameras (currently only outdoor cams). Set up KVM desktop switch to be able to switch easier between the two servers and desktop. Check rack temperatures from June to August to determine if the setup is as cooling efficiently as planned. Latest home lab photo: Update 4 – May 1st, 2020: Added a separate 21″ monitor to display remote hosts/dashboard using Raspberry Pi and Zabbix. Update 5 – June 7th, 2020: Added keyboard and mouse for walk-by access to Zabbix UI. Swapped out mesh security cover with tinted plexiglass. At this point, my home lab is complete. Is it ever? :) Update – August 19th, 2021: A month ago a friend asked if I could set up something similar for their home. They wanted a simple whole-home VPN setup. Pictured below is the result. All together uses less than 40 watts via a 700VA UPS. StarTech.com 6U Wall Mount Network Equipment Rack – 14 Inch Deep AC Infinity Vented Cantilever 1U Universal Rack Shelf, 10″ Deep C2G 12-Port Patch Panel VCE UL Listed CAT6 RJ45 Keystone Jack Inline Couplers Vilros – Raspberry Pi 4 2GB Basic Kit – running Unifi Controller Brume (GL-MV1000) – Edge Gateway + VPN (wireguard) Edgeswitch 10xp StarTech.com 8 Outlet Horizontal 1U Rack Mount PDU Power Strip CyberPower SL700U Standby UPS System, 700VA/370W, 8 Outlets, 2 USBs Update – October 7th, 2022: Added update on the AC Infinity CLOUDPLATE T7-N, Rack Mount Fan Panel 2U (Intake). Make sure to clean the dust from these units every couple of months. It took 2 years without any maintenance for 2 of the 4 fans to become disabled. I will update if they work again once cleaned. Update – December 13th, 2022: We moved to a new home in February. So I removed the home lab – cables and all. Finally had the time to set up. I’ve made very small changes because I really like it as is. Changes: Mounted at a more ergonomic height. Flipped some devices around (for easier access to unplugging/power buttons) Replaced the EdgeRouter with a new Peplink Balance 20x for ISP load balanced using the “fastest response” method. Update: May 3rd, 2023: Purchased a 1500VA Cyberpower UPS and offloaded 3 PC monitors and a gaming PC. So that the in-rack 1u UPS only connected to the servers and network devices. This allows for up to 3 hours of runtime on battery if needed. Update: Sept 24th, 2023: Added information above on failure of the AC Infinity CLOUDPLATE T7-N, Rack Mount intake fans. Added a link to my Peplink Balance 20x Router Review. Published: Feb 25th, 2020Last updated: March 8th, 2024 Additional related articles: 5 Network Devices for work-from-home and Small Business. Finding Linux Compatible Printers. Home Lab inspiration – A letter from a reader. After 10 Yrs of Linux, I Switched to Windows. What next?",
    "commentLink": "https://news.ycombinator.com/item?id=39640992",
    "commentBody": "Home Lab Beginners guide (linuxblog.io)546 points by ashitlerferad 20 hours agohidepastfavorite343 comments buybackoff 19 hours agoThe article is good, but it is intimidating buy its size and scope. A homelab could be a single NUC on a desk. I have one with 64GB RAM and it could fit so so many things. NUCs are very efficient to run 24/7, but become noisy with sustained CPU load. For that one could grow the setup with OptiPlex or Precision Tower (can haz ECC) SFF from eBay. These Dell SFFs are very good, quite small yet proper desktops/servers with proper quiet fans, could fit 10G Mellanox 3 cards (also from eBay for $40), and you could stack one on top of another horizontally. Don't go older than OptiPlexes with 12th gen CPUs, electricity and space they take could become a constraint. The used ones with i5-12500 are already very cheap. With LGA1700 one could put i9-14900 (non-K) there if needed. reply cybrox 18 hours agoparentIn addition to that, I don't think people should overthink the rack as an essential component. I like racks as much as the next guy and homelab racks look really cool but for usability and learning, just go with whatever you like. I personally have 4 boxes stacked in a corner that are connected to a \"rack\", which is just two rack side panels bolted to the back of a workbench that I screw components into, connected to NAS and multiple Raspberry Pi's on a shelf and I really like the mess and have learned a lot. Just use what you have and expand as you need. Racks are cool once you're into the hobby enough to care about style points. reply NelsonMinar 17 hours agorootparentFrom what I've seen on reddit's /r/homelab, the neat thing about racked systems is you can buy very cheap used hardware that are discards from enterprise uses. There's very little market for an old server or rackmount network switch so you can get very good bargains. Personally I buy new cheap mini PCs and consumer networking stuff. That's another way to get bargains, since consumer hardware is so cheap. But I'm a bit jealous of the enterprise capabilities like proper managed switches or remote managed servers. reply mech422 16 hours agorootparentThe down side to the old enterprise servers is the power draw. I had 2 Dell blade server chassis and 1 HP blade server chassis that used Xeon based blades. The electric bill was UNREAL with 10+ blades running at once. I do use Dell PowerConnect manged switches from EBay, but tbh for what I do, TP Link switches work really well and are cheap. They support VLANs, MAC Lans, SNMP, SSH/Web configuration, etc etc - basically fully managed switches. And they have been rock solid as 'edge' switches for each room in the house. The powerconnects basically work as top-of-rack switches for pulling all the different rooms/rack into 1 feed to the cable modem. reply blackfawn 15 hours agorootparentAgreed. Power draw and often noise on enterprise equipment is ridiculous in a home setting. I try to stick to the small form factor (SFF) or ultra-small form factor (USFF) computers whenever I can, which is what the author is using. I used to use Raspberry Pi boards for some things but always ran into problems w/ microSD cards dying, even with log2ram and buying industrial cards. I haven't had a single issue after switching to tiny x64 machines w/ real SSDs in them. I picked up a HP managed switch that I thought I might use but it draws more power than the ThinkCentre M93p Tiny computers do and it's actively cooled w/ super noisy 1U fan(s) so I'm going to ditch it. reply mech422 13 hours agorootparentYeah - I skipped rPi because back when I started replacing stuff ARM still had some gotcha's with stuff that hadn't been ported. I do use the rPi's for the 3d printers though. I've found the TP Link stuff to be good for home switches. cheap, quiet, and basically fully featured/managed. Stuff like SNMP, VLAN/Mac Lan, port mirroring/monitoring, etc. Also SSH and web interfaces... I have 2-3 currently, 1 per room feeding back to the PowerConnects (NOISY) and haven't had a single issue in 3-5 years of use :-) reply doawoo 11 hours agorootparentOne of the coolest additions to my home lab recently was an Orange Pi 5 plus/pro - 8 core ARM 64-bit cpu and 32GB of RAM all with awesome power usage and zero noise, plus an up to date kernel. I skipped the rpi as well a while back but these newer boards are worth lookin at! :) reply mech422 10 hours agorootparentOh nice! - I'll have to take a look Thanks! reply nvy 14 hours agorootparentprevWhich x64 machines have you been using? I'm looking for a successor to PC Engines APUs reply blackfawn 14 hours agorootparentI've been sticking w/ very compact units, mainly ThinkCentre M series \"Tiny\" and Dell OptiPlex \"Micro\" form factor, which are both practically identical in size. Both use a laptop power supply. You can get current versions for many hundreds of dollars or generation(s) old ones for as cheap as $30 USD. My most recent ones were older 3rd & 4th gen i5 & i7 CPUs but they still blow current Raspberry Pi, etc. out of the water, have M.2 for network, etc. & SATA for storage. A Dell I have that's a generation newer than these adds NVMe too. It's hard finding these small PCs that have multiple Ethernet but I just added a gigabit Ethernet M.2 card to the one. reply transpute 11 hours agorootparentprevOn paper, these look promising, no direct experience. Also on Amazon US/CA. $170, N100, 16GB LPDDR5, dual 2.5GbE, https://aoostar.com/products/aoostar-n-box-pro-intel-n100-mi... $210, N100, dual 3.5\" SATA, M.2 NVME, dual 2.5GbE, https://aoostar.com/products/aoostar-r1-2bay-nas-intel-n100-... $250, Ryzen 5700U, https://aoostar.com/products/aoostar-cyber-amd-ryzen-7-5700u... reply mech422 13 hours agorootparentprevI love the Odroid H series [0]. cheap, quiet and only like 4w idle draw 0) https://ameridroid.com/products/odroid-h3 reply vially 13 hours agorootparentprevI was also looking for a successor to the PC Engines APUs and came across https://teklager.se/en/ that lists some possible alternatives that you might find interesting. Personally I was looking to build a router so I ended up buying a fanless N100 based mini PC from Aliexpress (e.g.: search term is \"N100 firewall appliance\") and have been very satisfied with it so far (Proxmox homelab with OPNsense running as a VM). reply buybackoff 13 hours agorootparentprevPower draw also comes with terrible noise. They were designed to be in a server room, not a living/bed room. reply EvanAnderson 13 hours agorootparentprev> The down side to the old enterprise servers is the power draw. It depends on the generation of the gear. Stuff coming off-lease in the last 3 - 4 years has been of a generation where lower power draw became a major selling point. In Dell land, for example, the power draw differences are dramatic between the Rx10, Rx20, and newer series. (Basically, throw Rx10 servers in the trash. Rx20 servers are better but still not great.) reply bayindirh 11 hours agorootparentprevWhat homelab people who do not work with/in a data center daily doesn't realize is how noisy and power hungry is the equipment they procure. They got a managed switch, nice. It has VLANs, stacking, etc. nice. How many 48 port switches you can possibly stack in a home? You got a Sun ZFS7420. Great. That's a rack with two controllers with 1TB RAM each. That thing screams. An entry level 7320 is maybe workable, but you can't do 7420 at home. Same for HPE/Dell/Netapp storage boxen. Doing Infiniband is home is another thing I can't understand. Fiber cables are extremely expensive. Copper is range limited and thick, plus unless you do HPC, its latency and speed doesn't mean much. Go fiber Ethernet. Much more practical. I understand the curiosity and willingness to play, but it's not that practical to play with these at home (noise, space, heat, power wise). I also buy high end consumer hardware. TP-Link has 4/8 port smart managed SOHO switches for example, they even have PoE. Power your Raspi cluster via PoE with less cables and get some respectable computing power with no noise. Neat! No? Dell's Rack servers are generally not noisy at idle, but a 2U server under some real load is unbearable. What's funny that an Intel N95 has the same power with an i7-3770K with better SIMD support while being 4x more efficient, and that thing is not slow by any means. reply isopede 9 hours agorootparentI don't think fiber cables are expensive. I ran CAT6 in my house and at the same time pulled an OS2 single-mode 30m fiber cable to my office for core switching. The 30m fiber cable was $17 on Amazon, plus two 10G-LR modules at $18 each. The equivalent CAT6 is more power hungry, has less range, the cable is the same damn price, and the 10GBase-T SFP->RJ45 transceivers are twice the price of the LR transceivers at $50 each. reply bayindirh 9 hours agorootparentInfiniband uses its own cables with dedicated/integrated transceivers. It's not compatible with Ethernet and can't be swapped (unless you plan to talk plain Ethernet with your IB card, which is possible). A 5m, 200gbps cable is $1,149.00 a pop [0]. Mind you, this is neither the longest, nor the fastest cable. [0]: https://esaitech.com/products/mellanoxmfs1s00-h005v-200gbps-... More cables: https://esaitech.com/search?type=product&options%5Bprefix%5D... reply internet101010 13 hours agorootparentprevThe caveat to this is noise. People who have never seen a rack server in person don't understand how loud they really are. PSUs, case fans, cpu fans (if applicable) all gotta go. You kind of have to go into it knowing you are going to spend an additional $100-200 replacing it all, and that's assuming you will not have to 3D print a custom bracket to house the new fans. reply doublepg23 16 hours agorootparentprevI feel like it’s a harder sell than it used to be because post-Ryzen there’s been real increases in CPU performance. Modern NUCs and prebuilt also having native NVMe is a boon too. All depends on what you’re going for though. reply geek_at 17 hours agorootparentprevI did just the same. Built myself a little movable 6 node rack out of wood. With 3d printed powerbrick holders on the sides and a switch with a 10g uplink so all servers can use the full 1g nic. Warms my home office in the colder months and can be put in the garage on the warmer months. Photo front: https://pictshare.net/qi7zv2.png Photo back: https://pictshare.net/llloiw.png Photo of the 3d printed power brick holders: https://pictshare.net/kgffy6.png [edit] Specs: - 80 Watts during normal load - 50 cores - 9TB nvme storage - 300GB RAM - 3 of them are running proxmox - 3 of them Docker swarm reply E39M5S62 15 hours agorootparentI can't quite see which Thinkcentre's those are, but the M920x can fit a 10GbE PCIe NIC in it and provide it enough power. I have one on my desk with 2x2TB NVMe, 32GB RAM and a 10GbE fiber connection to it. reply Semaphor 15 hours agorootparentprevIn my experience, homelab means high powered or at least a lot of compute. People like me who have tiny, power saving devices are in the selfhosted communities. There's no definition like that, but it happens to turn out that way. reply didntcheck 14 hours agorootparentYou can add \"data hoarder\" to the Venn diagram too! reply Semaphor 10 hours agorootparentFair enough, kinda works with neither group. Usually not enough compute for homelab, but care way too much about storage for selfhosted stuff. reply m463 9 hours agorootparentprevI didn't care about racks either, but I got a 6u desktop rack and mounted a couple of things in it and turns out I love it. - shelf with two mini systems - power strip with individual switches - power strip always-on outlets front/back - ethernet switch - \"broom\" thing to feed cables through - many-port USB 3 hub other systems are under the desk. It's really convenient. I have hooked some systems to power switches, others are always on. It is organized, takes up space but doesn't fall over, and gives me desktop access to ports if I need them. That said, many years ago I had a server setup where I really organized everything, zip ties for all cables for neatness, etc. I turned out to be a pain every time I rewired something (cut zip ties, etc). I eventually learned there's a neat and organized vs flexibility balance you have to maintain. reply nucleardog 16 hours agorootparentprevRacks are nice from an organization perspective as well, but agreed it’s an unnecessary complication and expense and limits your options. I run a half dozen Lenovo ThinkCentre tiny PCs I picked up off eBay super cheap. Sure, I could go buy some rack mounts for them all… to the tune of $750. It would look really nice. That’s about it. If I’d decided that was a “requirement” I never would have done any of this. For anyone considering diving in to any of this stuff, don’t let the pursuit of “perfect” stop you before you’ve even started. My PCs are quite content sitting in a pile with some scraps of wood shoved between to help with airflow. reply mech422 16 hours agorootparentprevWhen I started using SBC's for my home setup, I skipped rPi's. At the time, being ARM based was a bit limiting (IIRC, at the time some stuff still hadn't been ported to ARM). So I went with odroid h2/h3 [0]. X86 based, lots of memory to work with, and much cheaper then a nuc - and uses much less power then my old Dell/HP blade servers from EBay :-) 0) https://ameridroid.com/products/odroid-h3 reply idatum 17 hours agorootparentprevIn the spirit of \"your setup, should be your setup. As traditional or as custom as you’d like!\" as mentioned in article, I went with a vertical mount rack. It's not enclosed so it has enough ventilation. The advantage with a vertical rack for me was space savings. Key parts of my home lab are near the low voltage panel (lucky to have in newer house) which does have foot traffic from other household members. Much less risk of someone bumping into the rack. reply OJFord 15 hours agorootparentprevOr as a compromise, you can buy strips of appropriately spaced/sized square-punched metal angle; before I moved I built a shelf unit designed so that the depth of the shelf was correct (19\" between posts plus a bit for the sheet metal thickness) for a rack, I had access to the side so I had a combination shelf & rack. People do similarly with Ikea tables ('lack rack'). Nice thing about the shelves was that I could buy a short strip (it was maybe 12U) and if I'd outgrown it I could've just removed a shelf and added another strip. reply m463 9 hours agorootparentThing is, you can get pre-built racks on amazon for 65W there. Or hungry PCI things. reply CTDOCodebases 11 hours agorootparentprevBMC is not a requirement these days as there are options like using a piKVM or AMT/vPRO on certificied Intel systems. Personally I turn IPMI off. The difference at idle can be up to 20w. Not to mention the outdated interfaces are a sercurity risk reply smallmancontrov 19 hours agoparentprevI've been thinking about upgrading my current \"homelab\" (just an old PC), and I've been waiting for a small form factor with an integrated power supply and a bunch of NVMe slots. Does that exist yet? reply buybackoff 18 hours agorootparentFor my homelab, I bought OptiPlex 7000 with 3 NVMe slots (one PCIe 4.0, one 3.0, one 3.0 short 2242), 1 PCI x16, 1 PCI x4, 3 SATA and 3 DisplayPorts for 300 EUR near 2 years ago. It was so good actually that I've made it my main PC. I already had 3 monitors and this SFF just worked fine with them. No games of course. I also have Precision Tower with Xeon/64GB ECC RAM and 1 native NVMe slot. I use PCI-to-NVMe adapters there (2 slots), it works well with ZFS mirror. reply Andrex 14 hours agorootparentOptiPlex seemed to be the best option when I was browsing around. reply CharlesW 18 hours agorootparentprev> …I've been waiting for a small form factor with an integrated power supply and a bunch of NVMe slots. Does that exist yet? Have you considered using a NAS as your homelab? I run a few containers (no VMs yet, although I can) and just do that on my QNAP. The TBS-h574TX (https://www.qnap.com/en-us/product/tbs-h574tx) has five E1.S/M.2 slots. The HS-453DX-8G has two SATA 3.5-inch drive bays and two M.2 2280 SATA SSD slots (https://www.qnap.com/en/product/hs-453dx), and the TBS-h574TX. Or you can just throw SATA SSDs into any NAS. reply smallmancontrov 18 hours agorootparentThese NASbooks are sooooo close to what I want! My problem is that none of them have an internal power supply. They all require a power adapter that's decidedly less easy to source than an IEC cord. I have had 3 separate experiences where I give someone instructions to grab a box and the power adapter... and they show up with the box and without the power adapter (or with a power adapter in a box of similar power adapters and no volt/amp/size rating on the box to disambiguate). In short, the internal power supply is something I actually care about. reply bombcar 18 hours agorootparentIt's harder and harder to find small computers with internal power, because they've found that nobody counts the power brick in the \"size\" (remember the original Xbox?). The best bet might be to wait until they've all finalized on USB-C. reply smallmancontrov 17 hours agorootparentYep, that's probably how it has to be. Hopefully USB4NET can save us from \"gigabit ethernet is enough for everyone,\" too. reply buybackoff 18 hours agorootparentprevIt's easier to turn an OptiPlex into NAS. Actually you could pass-through entire SATA stack as a PCI device into TruNAS running in Proxmox VM so it sees SATA ports natively, with all goodies such as SMART. reply znpy 17 hours agorootparentIf you just need to run a few containers, you might want to take a look at TrueNas (Scale) rather than Proxmox. reply belthesar 13 hours agorootparentDepending on the use case, this is definitely an option. I personally prefer using Proxmox under my Container Orchestration hardware because, as it seems to always eventually do, my home lab has become production, and being able to do software patches of the underlying system without service outages is really desirable. I've been agonizing and going back and forth between various potential solutions for this for a couple months now, with the most recent attempt being \"Deploy k3s, with a fully separate control plane from the worker nodes, and run all of those atop a Proxmox cluster because being able to migrate VMs across nodes and also stand up isolated workloads to test out other solutions makes changing things in the future far less risky\". That said, I was running the Homelab on effectively a single Unraid hyperconverged NAS/compute layer before then, and that worked wonders for quite a long time. I wouldn't consider Unraid my first choice now, given the advancements in TrueNAS SCALE in the time since I chose it as the solution, but the concept of NAS + Container Orchestrator as hyperconverged homelab today is quite attractive. reply natebc 16 hours agorootparentprevMaybe give this thing a look? it's newish and a little expensive but pretty dang well equipped. (not internally powered though) https://store.minisforum.com/products/minisforum-ms-01 reply buybackoff 9 hours agorootparentIt's a mobile CPU (with H). Unless it's fanless it will sound like a rocket lunch. Or it will be thermal-throttled with any sustained load. And it looks too expensive. reply jay-saint 9 hours agorootparentprevAfter massively overthinking things for my homelab / firewall, I am ordering one this weekend, just deciding which cpu. reply unregistereddev 13 hours agorootparentprevThere are SFF PC's with integrated power supplies that have a few PCIe slots. PCIe -> NVME adapters are readily available (not my website or article, but posting a link to a review site seemed more genuine than posting Newegg or Amazon links to specific products): https://thunderboltlaptop.com/best-nvme-pcie-adapters/ reply buybackoff 11 hours agorootparentThe cheapest from Amazon works well. Just make sure it matches PCI Gen and slot overall for throughput, e.g. PCI 3.0 x4 are fine for NVMe 3.0, but for NVMe 4.0 more is needed. reply jhickok 18 hours agorootparentprevI think you could DIY that, but I found getting a few little PCs can be stuffed in a drawers with some holes drilled in the back. Haven't complained about suffocation yet! I bought some Bee-Link mini PCs with pretty impressive specs. No idea if I am now being spied on by China. reply thethindev 18 hours agoparentprevI actually did just that and setup my homelab with my \"old\" laptop [1]. [1] https://thin.computer/index.php/2024/03/08/self-hosting-my-s... reply znpy 17 hours agoparentprevIntel NUCs are so *insanely* good for homelabbers. I got a few recently, and they're just great, particularly in terms of power usage. I hooked mine to tasmota-powered smart plugs, and they idle at something like 6W ... Granted, I always tune hardware (from bios config) and software (tuned profile) for low power... But long story short, my nucs rarely spike over 30W. Literally half of one of those old tungsten-based lightbulbs. reply mech422 16 hours agorootparentPersonally, I thought the NUC style boxes were really expensive. I like odroid H2/H3 SBCs[0] - they have much less compute power, but support large memory (DDR4 up to 64G), SSD/NVME, dual on-board nics (up to 2.5Gbps), X86 based and just sip on power (2 watts idle, 15w stress, 18w cpu+gpu stress). 0: https://ameridroid.com/products/odroid-h3 reply buybackoff 12 hours agorootparentNUCs may be expensive, until you see a 40% discount on Amazon :) I've got mine this way. Now they are discontinued so one may expect to get them cheap on eBay. Anyway, I'm talking about the box itself and MB/CPU only. Vendors/people often sell a good box with bad RAM/disks at a very low price. reply Arcanum-XIII 16 hours agorootparentprevI got a Ryzen 7 5800H with 32gb of ram and a 1tb hard drive for 320€... I don't have the dual on board nic, but beside that, it's quite great cpu wise with Proxmox — multiple VM on it, doing transcoding, DNS, dealing with download and search, Home Assistant. Of course transcoding 4k live is costly, but that's an exception. I can use NVMe and SATA hd, that's nice. And I can update the ram and use ECC. Not bad all in all ! Still, I got a Dell SFF with an I5 and 16gb of ram for less than 100€ — updated it with a 4 2,5gbps NIC, will be good for OPnsense... If I don't switch to a Melanox Connect 3. reply mech422 13 hours agorootparentVery Nice :-) I don't really need that much horsepower (My stuff is like 90% idle - git server, etc.) but I _really_ appreciate the lower noise and power usage compared to my old Xeon blades reply Andrex 14 hours agorootparentprevI didn't know ODROID was in this market, that's awesome! Definitely checking them out next time I need hardware, the H3 looks perfect. reply mech422 13 hours agorootparentI sound like a shill - I have no affiliation, just a customer but I _LOVE_ the H series :-) plenty of power for my needs, silent, low power ... and CHEAP! I especially love the large RAM capacity as it lets me run a lot of docker/vm/etc without swapping. Most of my stuff is idle like 90% of the time (stuff like git servers, etc) so the CPU is fine for me. I've had mine for a few years now (I had the first gen..) and never any problems! reply Andrex 13 hours agorootparentDon't worry, I'm actually (the original?) ODROID shill! https://news.ycombinator.com/item?id=32619740 :) Glad to hear they never stopped doing what they were good at, sounds like they've gotten even better! reply mech422 13 hours agorootparentOne of us...One of us :-) I really hope more people hear of the H2/H3 stuff...I'd love for Odroid to keep making/improving them! :-D reply mech422 12 hours agorootparentprevOh hey - that post was pretty cool :-) Amazing how much a kind gesture sticks with you! Glad to see stuff like that these days :-D reply Wuzado 11 hours agorootparentprevAny power tuning tips? reply giantrobot 16 hours agorootparentprevNot only do NUCs sip power but they're embarrassingly fast for most \"I want to play with servers\" jobs. RAM is cheap and 32GB goes a long ways. reply diffeomorphism 16 hours agorootparentprevHow do they compare to SFF (like thinkcentre tiny) or other minipcs (e.g. Minisforum um790)? Power usage seems to be fairly similar (5-7W) but maybe there are less obvious perks? reply fsckboy 13 hours agoparentprev> A homelab could be a single NUC on a desk then a better name than homelab would be your \"NUC/NOC-nook\". with regard to TFA, I don't trust \"labs\" with neat wiring. reply Scene_Cast2 17 hours agoparentprevYep, agreed. When living in a high cost of living area (i.e. where lots of tech professionals tend to live), space is limited. I'm annoyed at the lack of ECC-capable sub-ITX NAS boards. I have a Helios4, but I've no idea what I'll migrate to when that dies. reply epakai 8 hours agorootparentIf you want cheap, but non-performant, there are AMD R-series like found in Dell/Wyse thin clients. They're pretty weak machines though 2-4 cores, but support ECC DDR3 single channel. reply kjkjadksj 17 hours agorootparentprevN100 boards are ddr5 compatible which has internal ecc reply mrob 17 hours agorootparentInternal ECC doesn't protect the data in transit. It also doesn't have error reporting, so you could be operating with the error correction already operating at maximum capacity without knowing it, leaving no error correction available for unusual events. It's not a substitute for traditional ECC. reply sitkack 16 hours agorootparentDDR5 ECC is an internal mechanism just to make DDR5 viable due to the speed and node size. It doesn’t increase reliability, in enables not having a reduction in reliability over DDR4. reply NelsonMinar 17 hours agoprevA bit of a tangent but I want to sing the praises of Proxmox for home servers. I've run some sort of Linux server in my home for 25 years now, always hand-managing a single Ubuntu (or whatever) system. It's a huge pain. Proxmox makes it very easy to have multiple containers and VMs on a single hardware device. I started by just virtualizing the one big Ubuntu system. Even that has advantages: backups, high availability, etc. But now I'm starting to split off services into their own containers and it's just so tidy! reply caconym_ 16 hours agoparent+1, Proxmox is awesome. I set up a cluster with two machines I wasn't using (one almost 10 years old, one fairly modern with a 5950X) and now I don't have to worry about that single Debian box with all my services on it shitting the bed and leaving me with nothing. VMs are useful on their own, but the tools Proxmox gives you for migrating them between machines and doing centralized backup/restore are incredibly freeing. It all works really well, too. Most recently I set up a Windows VM with PCIe GPU passthrough for streaming (Moonlight/Sunshine) games to the various less-capable machines in my house, and it works so well that my actual gaming PC is sitting in the corner gathering dust. My only complaint is I wish there was a cheaper paid license. I would love to give back to them for what I'm getting, but > $100/yr/cpu is just too much for what amounts to hobbyist usage. I appreciate the free tier (which is not at all obnoxious about the fact that you aren't paying), but I wish there could be a middle ground. reply znpy 15 hours agorootparent> My only complaint is I wish there was a cheaper paid license. I would love to give back to them for what I'm getting, but > $100/yr/cpu is just too much for what amounts to hobbyist usage. I don't use proxmox anymore at home (1) but as an user I would have loved to pay, say, $5/month and just get access to the updates repository (the stable ones) or maybe something similar. $100/year/cpu was just too much for me as an hobbyist. [1]: not because I don't like it, for life reasons I had to downsize that hobby a bit and I don't need proxmox anymore. reply caconym_ 14 hours agorootparentMy thoughts exactly. Anyone from Proxmox reading? Could it be that you're leaving money on the table here? reply jtriangle 2 hours agorootparentHard to tell really. I do wish they had cheaper access to their enterprise repos, perhaps a no-support option... reply cmehdy 17 hours agoparentprevProxmox is incredibly helpful especially for homelab beginners familiar with Linux. You can easily create and destroy various things as you learn. Only thing that's not so easy to use is storage in general : it's very difficult to truly understand the consequences of various choices and therefore it's kinda difficult to properly set up the backbone of a NAS unless you're fairly comfortable with zfs, lvm, lvm-thin, re-partitioning things, etc.. reply NelsonMinar 9 hours agorootparentOne thing Proxmox is missing (IMHO) is a simple storage sharing system. I ended up just running the kernel NFS server on my Proxmox hypervisor, then sharing disks via a virtual-only LAN to the guest images. Other folks run an NFS server or a full NAS system in a VM under Proxmox. Proxmox' official solution for this is Ceph but that seems like overkill for a small home setup. reply Spivak 6 hours agorootparentHyperconverged Ceph is both trivial to set up and amazing. Would highly highly recommend it, it makes everything so much easier when your VMs are backed by RBDs. reply dusanh 1 hour agorootparentAny good tutorials for dummies like me? reply radiowave 16 hours agorootparentprevAnother handy feature of it is snapshots, very useful before trying out some modification. reply didntcheck 14 hours agoparentprevAs someone still in monolithic home server tech debt I've been meaning to migrate too. The part I always wonder about is storage - said server is a NAS, serving files over SMB and media over Plex. From what I hear some people mount and export their data array [1] directly on Proxmox and only virtualize things above the storage layer, like Plex, while others PCI-passthrough an HBA [2] to a NAS VM. I suppose one advantage of doing the former is that you might be able to directly bind mount it into an LXC container rather than using loopback SMB/NFS/9p, right? I also hear some people just go with TrueNAS or Unraid as their bare-metal base and use it for both storage and as a hypervisor, which makes sense. I might have to try the Linux version of TrueNAS now that it's had some time to mature I also rely on my Intel processor's built-in Quicksync for HW transcoding. Is there any trouble getting that to run through a VM, except presumably sacrificing the local Proxmox TTY? [1] As in the one with the files on, not VHDs. I default to keeping those separate in the first place, but it's also necessary since I can't afford to put all data on SSDs nor tolerate OS roots on HDDs. Otherwise a single master ZFS array with datasets for files and zvols or just NFS for VM roots would probably be ideal [2] Which I hear is considered more reliable than individual SATA passthrough, but has the caveat of being more coarse-grained. I.e. you wouldn't be able to have any non-array disks attached to it due to the VM having exclusive control of all its ports reply kungp 2 hours agorootparent> I suppose one advantage of doing the former is that you might be able to directly bind mount it into an LXC container rather than using loopback SMB/NFS/9p, right? Yep, I do this and it's very simple to give LXCs access to parts of my storage array. reply haunter 8 hours agorootparentprev>I also hear some people just go with TrueNAS or Unraid as their bare-metal base and use it for both storage and as a hypervisor, which makes sense. I might have to try the Linux version of TrueNAS now that it's had some time to mature Unraid is especially meant for that. But TrueNAS Scale is also good, it has Docker and Kubernetes support too. There is also openmediavault, it's a bit more Linux-y than the Unraid or TrueNAS Scale but a lot of people love it reply 2024throwaway 17 hours agoparentprevHow exactly does running a VM in proxmox give you high availability? reply wutwutwat 17 hours agorootparentProxmox comes with baked in support for live vm migration[0], high availability[1] and clustering support[2] if you run multiple host nodes. I'm assuming that's what OP is referring to 0: https://pve.proxmox.com/pve-docs/pve-admin-guide.html#qm_mig... & https://pve.proxmox.com/pve-docs/pve-admin-guide.html#_guest... 1: https://pve.proxmox.com/pve-docs/pve-admin-guide.html#_how_i... 2: https://pve.proxmox.com/pve-docs/pve-admin-guide.html#chapte... reply karolist 17 hours agorootparentprevIt doesn't on its own but it simplifies things hard to roll by hand that does, like observation nodes, consensus algorithms, Ceph storage, live migrations etc. reply astockwell 17 hours agorootparentprevSome defense against the class(es) of availability problems that occur from OS + upwards. No defense against the class(es) that occur below that (metal). reply unpopularopp 17 hours agoparentprev>Proxmox makes it very easy to have multiple containers and VMs on a single hardware device I just wish there was something like Unraid's Docker support but in a FOSS distro. Proxmox with LXC is just not the same. Unraid is good, love the UI and easyness too but the proprietary paid Linux is just something I don't like. reply xxpor 17 hours agorootparentFWIW https://tteck.github.io/Proxmox/ has a docker LXC container. A bit meta but works fine for my use cases. But I agree with your general point. reply NelsonMinar 9 hours agorootparentprevMost folks who need Docker in Proxmox just set up a VM for it. It's a bit goofy but works fine. Portainer is a popular UI frontend for Docker. I like LXC alright but I do miss the ecosystem of DockerHub. reply whalesalad 16 hours agorootparentprevProxmox is ultimately just debian linux. You can install portainer on the host. Or create a VM running portainer. reply haltcatchfire 17 hours agorootparentprevEspecially now with the rumors of Unraid moving from pay-once to an annual subscription model. reply BXlnt2EachOther 4 hours agorootparentI believe this was confirmed and they aim to switch by end of the month. Those interested in Unraid who prefer the old model may wish to pick up a perpetual license now. https://unraid.net/blog/pricing-change disclaimer - I'm not an Unraid user. reply k8sToGo 16 hours agoparentprevCombining that with Proxmox Backup Server is awesome. reply aaronax 17 hours agoparentprevAnd a huge win that it makes it so easy to have distributed storage. Ceph is built-in and works well. Being able to add disks as needed is bliss (no RAID planning up front). And the rebuild situation is better than any RAID. Performance is adequate for homelab even on a 1gbps network. I have about 10 SSDs mixed 1TB and 2TB across 5 hosts (old Optiplex units). reply didntcheck 15 hours agorootparentThat sounds interesting, I'm only tangentially familiar with Ceph so forgive these stupid questions: So you have the Ceph \"array\" running on the Proxmox nodes, and then are your VHDs like objects on Ceph? Do you also use Ceph for direct data storage or are all your files and data within VM's VHDs? reply tick_tock_tick 13 hours agoparentprevWhat's the main value vs using something like K3? I've mostly just been defaulting to k3 since I use k8s at work so lots of my knowledge maps over. reply globular-toast 16 hours agoparentprevObviously you could have just run containers on that Ubuntu system. And nothing stopping you backing it up either. But you might as well start with a hypervisor on a new server, though. It gives you more flexibility to add other distros and non-Linux stuff into the mix too (I run an OPNsense router, for example). I use xcp-ng rather than Proxmox. reply AnarchismIsCool 19 hours agoprevI know it's becoming the new \"I use arch btw\" meme but if you go down this route I highly recommend using nix as the distro. Ideally you can just leave these systems running once they're working and using nix means all your system state is logged in git. No more \"wait, how did I fix that thing 6mo ago?\" or having to manually put the system back together after a Ubuntu dist upgrade explodes. Every change you've made, package you've installed, and setting you've configured is in the git log. I find myself referring to my git repo frequently as a source of documentation on what's installed and how the systems are set up. reply cloudripper 18 hours agoparentI use nix btw. I totally agree. It (nix and nixos) is/are such a pain so often. But, wow... when you get things to work it is glorious. Quickly and easily syncing system/user/application configurations across multiple devices is incredible. The declarative approach means that the often laborious effort it takes me to get things to work pays off when I finally figure it out and realize I never have to do that again - and since it's declarative, I now have a documented functional configuration/example to grow and expand from. My current project is to learn Nix development environments by building \"Linux from Scatch\" from Nix. I am making progress slowly, but every success is cemented and replicatable. LLMs definitely help distill the sparse/scattered documentation, but I am finding a good amount of grit is still required. All in all, I am very much enjoying Nix and hope to utilize it more in my homelab. reply ninetyninenine 17 hours agorootparentI just feel it doesn't have to be this way. We can get the benefits of nix without the hardness of nix. It's just someone hasn't taken the time to make this \"thing\" yet. It's annoying enough that I won't use nix personally. I've used it at work so I'm quite knowledgeable with it. reply transpute 4 hours agorootparent> It's just someone hasn't taken the time to make this \"thing\" yet. DietPi [1] has one massive (15KLoC) bash script [2] for system config. [1] https://dietpi.com [2] https://github.com/MichaIng/DietPi/blob/master/dietpi/dietpi... reply Arelius 17 hours agorootparentprevIMO, this thing is Nix, it just needs better UI and a lot better documentation. The problem I've realized is that for many core Nix developers, the source is the documentation. Which doesnt scale. reply AnarchismIsCool 16 hours agorootparentLuckily the ecosystem is literally getting better by the hour right now. There's been a huge uptick in interest and lots of people are writing really good documentation and starter templates. reply yonatan8070 13 hours agorootparentprev> sparse/scattered documentation I've been hearing more and more from people switching over to NixOS, so hopefully the docs will get better as well reply nonrandomstring 17 hours agorootparentprevI got into Guix and then failed. Now I realise it's because I didn't fully understand the nix philosophy underneath. Can you recommend any good tutorial points for an intermediate/experienced unix person? reply _peeley 12 hours agoparentprevSame, I originally had a bunch of RasPi's in my lab running differing versions of Raspbian until I got tired of the configuration drift and finally Nixified all of them. Writing a single Nix Flake and being able to build declarative SD card installation images for all of them makes managing a bunch of different machines an absolute dream (tutorial here[0], for those interested). The only issue is remotely deploying Nix configs. The only first-party tool, nixops, is all but abandoned and unsupported. The community driven tools like morph and deploy-rs seem promising, but they vary in terms of Flakes support and how much activity/longevity they seem to have. [0] https://blog.janissary.xyz/posts/nixos-install-custom-image reply eddieroger 19 hours agoparentprevI wish Nix had a shallower learning curve (or that I was smarter). It's been on my list of wishes to play with, but that list is long, and getting Nix over the hump hasn't been something I've been able to do yet. The concept makes sense to me, but when I see a new package or app I want to play with, I can get there with docker-compose and a VM more quickly than I can wrap my head around doing it in Nix. reply haswell 18 hours agorootparentI’m going through the NixOS journey right now, and it’s a combination of easier than I thought, but also more frustrating than it should be. Getting a basic system going seemed about as easy as Arch (if you consider Arch easy). But adding flakes and home manager to the mix is where things have run off the rails a bit for me. I think a lot of the steepness of the learning curve comes from trying to go all in on flakes/home manager up front, and a lot of the online discussions and quick starts being oriented towards this goal. I’ve since scaled back my aspirations a bit and I’m focusing on just understand core NixOS with a monolithic config file until it makes more sense to my brain. If you haven’t get, check out vimjoyer’s channel on YouTube. His content has been the most helpful for me by far, and it got me a basic setup going that I’m now iterating on. What I do love is that I feel like I can iterate with confidence. I have no idea what I’m doing for the most part but I know I can get myself back to a working state. I think it will be worth the time investment in the long run, but yeah, it’s been a bit weird and the lack of a canonical approach makes it hard to break into the ecosystem. reply JamesSwift 18 hours agorootparentprevIts just a matter of repetition. It will eventually click if you go back enough times (at least it did for me after like 4 tries). The biggest thing to getting your mind adjusted to the syntax is to understand that nix is just 1 big JSON blob. All the syntax is there to facilitate merging/updating that ball of JSON in a deterministic way. You might try to generate your docker containers using nix as a first step. Not sure if it will be more approachable for you, but at least its 1 less new thing to adjust to. reply AnarchismIsCool 19 hours agorootparentprevIf you get the base system up you don't have to exclusively use their containerization system. I have a mix of nix environments and k3s deployments that are saved off in another git repo. There are advantages to purity but practically speaking you can just do what's convenient. reply shepherdjerred 17 hours agoparentprevI want to love arch, but it is way too complicated. I _love_ complicated things, but arch just didn't seem worth it. My home lab journey was: manual installs -> Ansible -> Docker + Docker Compose -> Kubernetes (I told you I loved complicated things) I kept everything source controlled. This repo documents my long journey: https://github.com/shepherdjerred/servers Docker and Docker Compose are nearly perfect. Here's an example of what that looked like for me: https://github.com/shepherdjerred/servers/blob/839b683d5fee2... I mostly moved to Kubernetes for the sake of learning. Kubernetes is very cool, but you have to put in a large amount of effort and learning before it becomes beneficial. Ansible is nice, but I didn't feel like it met all of my needs. Getting everything to be idempotent was quite hard. reply jjeaff 16 hours agorootparentI use a single to set everything up, including docker and docker compose. most applications are running in docker compose. but ansible allows me to handle all the hard drive mounts and other stuff that can't be run inside docker containers. reply shepherdjerred 14 hours agorootparentThat's not a bad idea! I didn't mind manually installing Docker, configuring fstab, etc., since it was such little work. reply icedchai 16 hours agoparentprevNearly every time I do an Ubuntu dist upgrade, there is some relatively easy to solve but critical problem. A couple examples from a few years back: default route was lost, network interfaces were renamed. In both cases, remote connectivity was obviously lost. Good thing it was in my basement... reply kiney 18 hours agoparentprevAnsible code in git solves the same problem for ANY distro reply NewJazz 18 hours agorootparentOnce you stop templating yaml you will never go back. Seriously as someone who has used ansible a lot and nix a bit, I'd take NixOS over Ansible+Debian/Fedora any day. reply k8svet 14 hours agorootparentprevNo, no it doesn't. It doesn't solve the same problem, at all, certainly not holistically like NixOS. reply nonameiguess 17 hours agoparentprevFor what it's worth, I personally do use Arch for my homelab and still achieve cattle not pets because the Arch install media comes with cloud-init so I simply keep a cloud-config with the modules to build a server from scratch on a separate USB stick with a drive that has \"cidata\" on a label and plug that into a machine along with the install media before hitting the power button. I don't know if Nix would have been easier but cloud-init just lets you run scripts that you presumably already know how to write if you're the kind of person who does this sort of thing. No need for a DSL. Someone is going to chime in with \"boo yaml\" and sure, but a person interested in running a homelab is almost certainly a person who manages a bunch of servers for work too and there's no escaping cloud-init, Ansible, Kubernetes, or something using yaml that you're not going to have a choice about. Assuming the point of a homelab is at least partly to practice what you do on the job without the possibility of destroying something owned by someone else (which is what this guy says his reason was), you may as well get used to the tools you're going to have to use anyway and not make your home setup too much different than the environment you manage at work. Of course, I know a lot of people on Hacker News are not like me and this guy and use \"homelab\" to mean they want to self-host media servers, chat, and photo sharing and what not for their family and friends, not have a mini data-center as a practice ground for the real data center they manage in their day job. reply simoncion 18 hours agoparentprev> ...or having to manually put the system back together after a Ubuntu dist upgrade explodes. I avoid this by running Gentoo Linux on my Linux machines. I've heard people say good things about Arch, but I've never used it. Ubuntu is just bad... I say this as someone who used Ubuntu for a few years waaay back when (and was _really_ pleased that I finally could recommend a reliably, easy-to-use Linux distro to nontechnical folks), but swore off of it after the second or third in-place upgrade (in a row) that left my system nonfunctional in ways that required the sysadmin knowledge I'd gained from going through the well-documented Gentoo setup process (and from tinkering over the years) to repair. Friends don't let friends use Ubuntu. reply bombcar 18 hours agorootparentAgreed - I'm willing to use Ubuntu LTS on servers that are basically glorified LAMP machines (LCMP?) - but for home servers, nothing has beat Gentoo, not only for the full understanding of \"what I have\" but also having direct control over what I've got running, including quite a bit of customizability if needed. Gentoo on ZFS is kinda nice. reply znpy 15 hours agoparentprevI actually went the complete opposite way and I run RHEL (Red Hat Enterprise Linux - with the developer subcription) or RockyLinux at home. Because: > \"wait, how did I fix that thing 6mo ago?\" Such things just don't happen. That thing (RHEL/RockyLinux) just doesn't break. I do update software once every six (or more) months, reboot and it just works. I have notes on how I did stuff, if necessary. I used to keep ansible playbooks but they were just an overkill. Does it even make sense to write a playbook for a system that I'll reinstall in three or four years ? Probably not. One of my main systems at home has been running RHEL 8 for 3-4 years, still happily getting updates. I plan on replacing the whole box, with another one running RockyLinux 9. And I'm moving from RHEL to RockyLinux because I just don't want the yearly annoyance of having to renew the Developer Subscription license. That's how reliable it is: I have to look up my notes once a year, and it's too much. I run most of my stuff in containers anyway, and podman is just great. reply k8svet 14 hours agorootparent>I have notes on how I did stuff, if necessary. I used to keep ansible playbooks but they were just an overkill. Does it even make sense to write a playbook for a system that I'll reinstall in three or four years ? Probably not. Yea, sure, except what if the one-time configuration was in a Nix file that will continue being exactly as useful for years? Folks applying the Ansible mindset to NixOS are missing the point. NixOS is *not like a runbook*. It's declarative, it's prescriptive, it's not some god-forsaken state machine that tries to paper over the horrors of modern computing. It IS the definition of my computers, not some glorified pile of scripts and conditional checks encoded in fking jinja. The same as it's been mostly for 8 years now. reply znpy 8 hours agorootparentx doubt reply k8svet 8 hours agorootparentWhich part? Like, that's what NixOS is. My config repo is declarative, describes in its history every machine I've owned. You could rebuild them at any point in time and get an identical exact machine image out. Idk, this is why it's hard to talk about nix with people that think they know it but then also openly doubt things that you will one day take for granted if you take the time to learn it. reply Affric 16 minutes agorootparentWhen people say Nix is declarative do they mean like SQL/Prolog? reply lloeki 16 hours agoprev> Location For a few years I put mine inside an IKEA FRIHETEN sofa (the kind that just pulls†). Pros: - easy access - completely invisible save for 1 power wire + 1 fiber (WAN) + 1 eth (LAN)†† - structure allows easy cable routing out (worst case you punch a hole on the thin low-end) - easy layout/cable routing inside - noise reduction for free - doubles as warming in winter a seated butt is never cold - spouse enjoys zero blinkenlights - spouse didn't even notice I bought a UPS + a disk bay Cons: - a bit uncomfortable to manipulate things inside - vibrations (as in, spinning rust may not like it) when sitting/seated/opening/closing - heat (was surprisingly OK, not worse than a closet) - accidental spillage risks (but mostly OK as design makes it flow around, not inside the container, worst case: put some raisers under hardware) - accidental wire pull e.g when unsuspecting spouse is moving furniture around for housecleaning (give'em some slack) † https://www.ikea.com/us/en/images/products/friheten-sleeper-... †† that I conveniently routed to a corner in the back then hidden in the wall to the nearest plugs, so really invisible in practice. reply jcul 1 hour agoparentThis is amazing. Do you have an actual photo with your servers inside or anything? I would be really worried about ventilation and heat / fire risk. Reminds me of the lack rack. https://archive.is/Uf2k3 reply neilv 16 hours agoparentprevI had a similar thought about the IKEA KIVIK sofa (non-sleeper variants), which has wide boxy armrests that are open to the underside, and could fit towers of little SFF PCs, or maybe rackmount gear sideways. There's also space underneath the seat, where rackmount gear could fit. One of the reasons I didn't do this was I didn't want a fire with sofa fuel right above/around it. I wasn't concerned about the servers, but a bit about the UPS. Sheet metal lining of the area, with good venting if the UPS battery type could leak gases, I'd feel a bit better about, but too much work. So the gear ended up away from upholstery, in rack/shelving, where I could keep an eye on it. reply zer00eyz 19 hours agoprevThe whole home lab scene is great. Everyone has some set of goals... low power, interesting processors, data ownership, HA, UPS/whole home UPS... and the home is the only common intersection of all these overlapping interests, and bits of software. Even more fascinating is the types of people it attracts, from professionals playing to people far outside the industry. I have really taken the plunge and it recaptures some of the magic of the early internet (at least for me). reply AnarchismIsCool 19 hours agoparentThe community is absolutely amazing. It's incredibly active on Reddit and Lemmy. People are always quick to help you find solutions and give you advice on setting things up using best practices. It's an absolute gem if learning this stuff is interesting to you. reply zer00eyz 18 hours agorootparentThe home lab community is a very leaky abstraction ;) Serve the home, and level 1 on YouTube... It intersects with proxmox community, home assistant community, all the NAS distros... The spread out and over lapping nature of these groups really adds to the early internet vibe! reply bazmattaz 12 hours agorootparentprevAbsolutely. As a prolific Reddit user I have a a love hate relarelationship with the site but the home lab and self hosted communities have restored my faith in internet communities reply tills13 17 hours agorootparentprevwhich lemmy instance / community do you recommend? reply AnarchismIsCool 12 hours agorootparentI like selfhosted@lemmy.world but there are probably plenty of others reply isopede 14 hours agoprevI have a rather extensive homelab, painstakingly set up over time. It works great, I love it. Few questions for you guys: - My real problem is disaster recovery. It would take me forever to replicate everything, if I could even remember it all. Router configurations, switch configurations, NAS, all the various docker containers scattered across different vlans, etc. I mapped out my network early on but failed to keep it up to date over time. Is there a good tool to draw, document, and keep up-to-date diagrams of my infra? - Backup and upgrading is also a persistent problem for me. I will often set up a container, come back to it 6 months later, and have no idea what I did. I have dozens of containers scattered across different machines (NUCs, NAS, desktops, servers, etc). Every container service feels like it has its own convention for where the bind mounts need to go, what user it should be run as, what permissions etc it needs. I can't keep track of it all in my head, especially after the fact. I just want to be able to hit backup, restore, and upgrade on some centralized interface. It makes me miss the old cattle days with VM clone/snapshot. I still have a few VMs running on a proxmox machine that is sort of close, but nothing like that for the entire home lab. I really want to get to a point, or at least move towards a solution where I could in theory, torch my house and do a full disaster recovery restore of my entire setup. There has to be something simpler than going full kubernetes to manage a home setup. What do you guys use? reply rlabrecque 14 hours agoparentI think the core problem you have is a lack of consistency. I'd try to trim down the number of different types of device/deployment methods/etc you have. It's too hard to scale wide as a single person or two, but you can go very deep in one stack and both know it very well and also get into a spot where it's kind of like either everything works or nothing works if it's all exactly the same, which means ultimately everything will have to work well. reply macromaniac 9 hours agoparentprevI accidentally wiped the drives on my server last year, but it wasn't so bad due to my setup. My strat is having a deploy.ps1 script in every project folder that sets the project up. 80% of the time this is making the vm, rcloning the files, and installing/starting the service if needed. Roughly 3ish lines, using custom commands. Takes about 100ms to deploy assuming the vm is up. Sometimes the script gets more complicated, but the general idea is that whatever system I use under the covers, running deploy.ps1 will set it up, without internet, without dependencies, this script will work until the heat death of the universe. After losing everything I reran the deploys (put them into a list) and got everything back. I'm with you on some of the routing setups, mine aren't 100% documented either. I think my router/switch config is too complex honestly and I should just tone it down. reply drojas 12 hours agoparentprevOh I wish companies like Framework or System76 launched a reproducible manufacturing process where you code your hardware similarly to how Nix/Guix manages builds. Disaster recovery would be much easier. Perhaps Super Micro Computer can do this already but they target data centers. One can only dream. reply sandreas 13 hours agoparentprevI think the complexity of your infrastructure is not helpful - so I would start in reducing it. I personally use one single server (Fujitsu D3417-B, Xeon 1225v5, 64GB ECC and WD SN850x 2TB NVMe) with Proxmox and an OpenWRT Router (Banana Pi BPI-R3). The Proxmox draws ~12W and the OpenWRT around 4.5W in Idle and with NodeJS I can run MeshCommander on OpenWRT for remote administration. Since I use ZFS (with native encryption), I can do a full backup on an external drive via: # create backup pool on external drive zpool create -f rpoolbak /dev/sdb # create snapshot on proxmox NVMe zfs snapshot -r \"rpool@backup-2024-01-19\" # recursively send the snapshot to the external drive (initial backup) # pv only is there to monitor the transfer speed zfs send -R --raw rpool@backup-2024-01-19pvzfs recv -Fdu rpoolbak An incremental backup can be done by providing the -I option and two snapshots instead of one to mark the starting point and the stop of the incremental backup: # create new snapshot zfs snapshot -r \"rpool@backup-2024-01-20\" # only send everything between 2024-01-19 and 2024-01-20 zfs send -RI --raw rpool@backup-2024-01-19 rpool@backup-2024-01-20pvzfs recv -Fdu rpoolbak That is easy, fast and pretty reliable. If you use this `zfs-auto-snapshot` you can rewind the filesystem in 15 minute steps. This is also pretty helpful on rewinding virtual machines[1]. Recently my NVMe (a Samsung 980 Pro) died and restoring the backup via zfs (the backup shell command in reverse order from rpoolbak to rpool) took about 2 Hours for 700GB and I was back online with my server. I was pretty happy with the results, although I know that ZFS is kind of \"experimental\" in some cases, especially encryption. 1: https://pilabor.com/series/proxmox/restore-virtual-machine-v... reply buybackoff 12 hours agoparentprevI think people mix \"homelab\" and \"homeprod\". Here there are some other comments mentioning homeprod. To me what you describe sounds more like a homeprod setup. But I like the \"lab\" part. When one could just relax and experiment with lots of stuff. I do have an ECC instance for data, to be backed up to a remote S3. I could torch it. It still remains a lab. I could just buy a new hardware and restore. But it's only about the valuable data (isolated to a couple of containers), not \"restore of my entire setup\". reply tegiddrone 13 hours agoparentprevI've been trying to re-bootstrap my own homelab and I'm sort of gridlocked because I know I will forget or yes \"cant keep it all in my head\"... so I've been spending a lot of time documenting things and trying to make it simple, vanilla, and community supported when possible. As a result I have no homelab or \"homeprod\" as some others have coined here :) I also found a friend who I am convincing of the same issues and so we may try mirroring our homelab documentation/process so that the \"bus factor\" isn't 1. reply bazmattaz 12 hours agoparentprevI’m quite a homelab novice and I’m always tinkering so I have the same problem as you. So I spent time creating a robust backup solution. Every week I backup all the docker volumes for each container to an external NAS drive. Then I also run a script which backs up my entire docker compose file for every single container in one gigantic compose file. The benefit of this is that I can make tweaks to containers and not have to manually record every tweak I made. I know that on Sunday at midnight it all gets backed up reply steveh777 13 hours agoparentprevI just use Ansible to \"orchestrate\" the containers. I only run one of each, configured to restart on fail, so I don't need anything smarter than that. reply pl4nty 10 hours agoparentprevk8s is a lot easier for homelabs than it used to be, and imo it's quicker than nix for building a declarative homelab. templates like this one can deploy a cluster in a few hours: https://github.com/onedr0p/cluster-template here's my home assistant deployment as a single file: https://github.com/pl4nty/homelab/blob/main/kubernetes/clust... I deliberately nuked my onprem cluster a few weeks ago, and was fully restored within 2 hours (including host OS reinstalls). and most of that was waiting for backup restores over my slow internet connection. I think the break even for me was around 15-20 containers - managing backups, config, etc scales pretty well with k8s. reply adhamsalama 13 hours agoparentprevMaybe something like Nix/NixOS can help you? reply lolinder 19 hours agoprevAs an alternative perspective, this is my home lab: * Location: Sitting on a shelf in my basement office. Ventilation is okay, the WiFi is fine but not great. * Hardware: An old PC I picked up at a neighborhood swap meet. I added some RAM taken from another old PC and bought a hard drive and WiFi card. * Software: Debian stable and podman/podman-compose. All my useful services are just folders with compose files. I use podman-compose to turn them into systemd units. If the stuff in the article is the kind of thing you're into, that's awesome, go ham! But you absolutely do not need to ever, and you certainly do not need to do it right away. I run a bunch of services that my family uses daily on this thing, and we use less than half the 16GB RAM and never get over 5% CPU usage on this old, ~free PC. reply drewzero1 17 hours agoparentMy favorite home lab setup was when I had an old (originally XP, switched it to Ubuntu) laptop and a couple of USB drives in the top drawer of a scavenged file cabinet. I cut a hole in the back for the wires and it never generated enough heat for airflow to be an issue. I hosted my personal website on it, including a webcam of my aquarium, hooked it up to my doorbell, XMPP, used it for document storage... anything I needed a server for. I switched to an old Mac Mini for NAS after I moved and haven't had much time to mess with it since. reply c-hendricks 18 hours agoparentprevSimilar, my homelab is an old gaming rig shoved in a corner running a bunch of docker compose services via systemd. reply SystemOut 18 hours agoparentprevThis could almost be me. The main difference is my \"server\" is an off-lease Dell Micro PC that I maxed out the RAM on running ProxMox with a mix of VMs with everything stored on a Synology NAS all sitting in my basement office closet. I have tailscale setup so I can access it remotely. reply kjkjadksj 17 hours agorootparentI see this a lot with certain home labs. A ton of ram for vms. Why not just run on the metal? reply drewzero1 17 hours agorootparentEasier to mess with or reinstall OS remotely if you're not sitting in front of it, I'd imagine. Also can be easier to back up the whole system or take a snapshot of the system state, or nuke it and start over if you really screw it up. I've dabbled with VMs for test environments but generally just install on bare metal. reply icedchai 15 hours agorootparentprevI have about a dozen VMs running on a couple of large-ish servers (16 cores, 128 gigs RAM, 8 TB storage) Many of these are small, experimental environments (8 gigs RAM max, few hundred gigs storage) Running all those on metal would be expensive and annoying. reply neilv 16 hours agoprevIf your homelab gear is in a non-tech-nerd living space, also think about noise, lights/displays, and otherwise being discreet. As an apartment-dweller, for a long time I had it in a closet. Once I moved it to living room, my solutions included: * For discreet, IKEA CORRAS cabinet that matched my other furniture. I had rackmount posts in it before, but got rid of them because they protruded. * For noise, going with gear that's either fanless, or can be cooled with a small number of Noctua fans. (I also replace the fans in 1U PSUs with Noctuas, which requires a little soldering and cursing.) I tend to end up with Atom servers that can run fanless in a non-datacenter except for the PSU. * Also for noise, since my only non-silent server right now is the 3090 GPU server, I make that one spin up on demand. In this case, I have a command I can run from my laptop to Wake-on-LAN. But you could also use IPMI, kludge something with a PDU or IoT power outlet, find a way to spin down the 3090 and fans in software, make Kubernetes automate it, etc. * For lights, covering too-bright indicator LEDs with white labelmaker tape works well, and looks better than you'd think. Black labelmaker tape for lights you don't need to see. * For console, I like the discreet slide-out rack consoles, especially the vintage IBM ones with the TrackPoint keyboards. If I was going to have a monitoring display in my living room, I'd at least put the keyboard in a slide-out drawer. * I also get rid of gear I don't need, or I'd need more than twice the rack space I currently have, and it would be harder to pass as possibly audiophile gear in the living room. * For an apartment, if you don't want to play with the router right now (just servers), consider a plastic OpenWRT router. It can replace a few rack units of gear (router, switch, patch panel), and maybe you don't even need external WiFi APs and cabling to them. reply hedgehog0 15 hours agoparentI’m a math grad student and want to play with LLMs as well, so I have been thinking about getting a 3060 or 3090, due to budget. However, I’m using a MacBook Pro from 2011, thus not very convenient for these kinds of things. What do you think the lowest budget or spending that I should have, given my “requirement”? Or would you say that vast.ai or similar websites would suffice? reply neilv 13 hours agorootparentYou might try to find forums where people are doing the kind of LLM work you want to do, and see what they're recommending for how to do that work. Personally, for my changing needs, my GPU server is already on its 4th GPU, after I kept finding I needed more VRAM for various purposes. The RTX 3090 seems to be a sweet spot right now, and my next upgrade right now would probably be a 2nd 3090 with NVLink, or some affordable older datacenter/workstation GPU with even more VRAM. Other than the GPU, you just need an ordinary PC with an unusually big PSU, maybe a big HDD/SSD (for data), and maybe a lot of RAM (for some model loading that can't just stream it). https://www.neilvandyke.org/machine-learning/ reply tanelpoder 12 hours agoprevI recently bought an old Mac Pro 2013 (trashcan) with 12 cores/24 threads and 128 GB ECC RAM for my upgraded \"always-on\" machine - total cost $500. Installed Ubuntu 22.04, works out of the box (23.10 had some issues). Unfortunately it's hard/impossible to completely suspend/disable the two internal AMD Radeon GPUs to lower power consumption. I got it down to ~99W consumption when idle, when using \"vgaswitcheroo\" to suspend one of the GPUs (and it set the other one to D3hot state). My Intel NUC consumes almost nothing when idle (my UPS reports 0 W output while its running, even with 4 NVMe disks attached via a Thunderbolt enclosure). I don't want a 100W heat generator running 24x7, especially when I'm away from home, so will need to stick with the NUC... reply buybackoff 12 hours agoparentGPUs are bad with power. My RTX 3060 still consumes around 20W doing nothing, no display or real load whatsoever. More than the rest combined when idle. I wanted to sell the entire box, but made that machine on-demand (it's mostly off). It's also noisy for it's relatively powerful overall. Yes, NUCs are great for 24/7 and light/spiky loads. reply bovem 19 hours agoprevSince last year I’ve been configuring and maintaining my homelab setup and it is just amazing. I’ve learned so much about containers, virtual machines and networking. Some of the self hosted applications like paperless-ngx [1] and immich [2] are much superior in terms of features than the proprietary cloud solutions. With the addition of VPN services like tailscale [3] now I can access my homelab from anywhere in the world. The only thing missing is to setup a low powered machine like NUC or any mini PC so I can offload the services I need 24/7 and save electricity costs. If you can maintain it and have enough energy on weekends to perform routine maintenance and upgrades. I would 100% recommend setting up your own homelab. [1] https://docs.paperless-ngx.com/ [2] https://immich.app/ [3] https://tailscale.com/ reply bazmattaz 12 hours agoparentDefo invest in a mini pc. I got a powerful HP elitedesk that sips 7w on idle and runs 38 containers reply gorkish 16 hours agoprevIMO one of the biggest frustration I currently have with homelab-scale gear (and edge compute in general) is how everything has massively regressed to only offering 2.5gbit connectivity at best. Try to find one of these hundreds of small form factor products that has anything faster than a 2.5Gbit nic and despair! What good are these machines if you cant even get data in and out of them?! The list of hardware with 2x10GBe or better connectivity is amazingly short: - Minisforum MS-01 - GoWin RS86S*, GW-BS-1U-* - Quotom Q20331G\\* From major manufacturers, only the following: - Mac Mini (1x10gbaseT option, ARM, ) - HPE Z2 Mini\\* (1x10GbaseT option) Please someone, anyone, build a SFF box with the following specs: Minimum of 2 NVMe Minimum of 2 additional storage devices (SATA or more NVMe) Minimum of 2 10GbE Dedicated or shared IMPI/OOBM Minimum of 8 Core x86 CPU Minimum of 64GB RAM Make sure the nics have enough PCIe lanes to hit their throughput limits, give the rest to storage. Stop with the proliferation of USB and thunderbolt and 5 display outputs on these things, please already! reply skazazes 16 hours agoparentYou should check out the Xeon D offerings from Supermicro. They make some M-ITX motherboards with embedded Xeon D chips that sport dual 10GB Ethernet. Used one in a freenas system at one point for years Looks like the X10SDV-TLN4F[1] has everything you mentioned besides a second m.2 slot. Although its 8 cores they are a few generations old and low power (45w TDP on x86). For true compute its not exactly fast but for something like a high performance file server in 1U connected to a disk shelf, they are really nice. [1] https://www.supermicro.com/en/products/motherboard/x10sdv-tl... reply gorkish 15 hours agorootparentI actually do run a production edge cluster with these exact boards; they are just too old. The price/performance is so far off the mark WRT modern hardware they are simply not worth using anymore. I have started hitting CPU instruction set issues with current software due to the ancient microarchitectures in these lowend parts. On one hand they are able to still sell them because nobody is competing, but what is the reason for the lack of competition? I have a theory about Broadcom buying and blocking up the low end PCIe switch market to purposely hold back this segment, but I'm not sure how significant that is. reply jmbwell 15 hours agorootparentprevI use one of these. For a system that’s honestly mostly idle, it’s plenty for proxmox with Plex and pihole and whatever else, and it’s good on power and heat and noise. I made the mistake of getting Seagate 10TB rock tumblers, but they’re the only source of real noise. WD would probably have been quieter. I have this and my switches and a ham radio base station all racked up on top of the cabinets in my laundry/utility room, out of the way but not hidden away, so I can still notice if the fans are too loud or the wrong lights are blinking or not blinking or there’s weird smells etc. reply CharlesW 16 hours agoparentprev> Stop with the proliferation of USB and thunderbolt… If you don't need it built-in, external adapters are a good way to get to 10GbE and even 25GbE. https://www.qnap.com/en-us/product/qna-t310g1s https://www.sonnettech.com/product/thunderbolt/ethernet-netw... reply gorkish 15 hours agorootparentBy \"good\" I assume you mean \"possible\" because there is nothing really good about a connectivity solution that is both technically inferior and vastly more expensive than it ought to be. I'm not opposed to Thunderbolt though; let me know when someone ships a 8+ port thunderbolt interdomain/multi-root PCIe switch. That would be a good solution. reply Borealid 16 hours agoparentprevI understand that more is better in some sense, but why the emphasis on having two (2) 10GbE instead of one for a single device that isn't a router? What's the second link for? Just more bandwidth, or is there some crucial use case I'm missing? reply gorkish 15 hours agorootparentIn my experience, a second nic is important to build anything with resilience; it's not just for routing. I see it as the most important hardware to \"double up\" on. If you are building a small cluster, for instance, you generally want to have one interface for the backend interconnect and one interface for client traffic. Or maybe you want to be able to upgrade or change your networking without taking traffic offline -- you'll need two physical connections for that, too. Of course you can still build routers and everything with a single interface (which is why I noted the single port hardware in the first place), but if you are going to design something specific to the mini-server market, the cost difference between a single port and dual port controller is negligible enough that there's no reason not to include 2 or even 4 ports. The GoWin machines simply toss in an OCP nic slot and call it a day. That would be a fantastic solution to see adopted by other vendors. reply jmbwell 15 hours agorootparentprevIf a home lab is for tinkering, it’s something to play with first of all, but it also helps facilitate re-patching things or moving things around, which in many cases you can do one port/cable at a time without interrupting services. I once moved a server from one room to another by moving its redundant power cables one at a time to a UPS on a cart, then swapping its short network cables for longer ones one a time, wheeling the thing to the other room, then “reversing” the procedure in the new location. Zero downtime! Zero point really, but fun in the way I reckon a home lab is meant to be. reply Andrex 13 hours agoprevThis article has such a focus on hardware, when that's like... 1% of my homelab decision making. Was hoping this article would delve into hosting your own email, websites and services and the steps to expose them to the public internet (as well as all the requisite security considerations needed for such things). A homelab is just a normal PC unless you're doing homelab stuff. reply Shawnecy 13 hours agoparentThis is always my area of interest, and I rarely see it covered. Maybe all the people who do it just understand it so well that's it just an after thought for them? I've tried on numerous occasions to host my own services at home and use dynamic DNS to map to an IP, but when things don't work, it's hard to know if it's the hardware (consumer modems, routers, etc.), dirty ISPs who want to force you into a business tier to open up anything, some combination of the two, or something else altogether. Are the problems so one-off and specific that they all require bespoke problem-solving? I would love it if anyone could share foolproof resources on the subject. reply bazmattaz 12 hours agorootparentDid you try r/self hosted on Reddit. Thats where you’ll get support for something like that reply treflop 12 hours agorootparentprevI think it is because most of those things are just regular sysadmin things and homelab more refers to setting up hardware or software that you wouldn’t normally find at work. As for your actual problem, a lot of ports are blocked by ISPs because a LOT of computer worms have used them to spread malware in the past. However you usually can test to see if the port is open as well as try alternative uncommon ports like 47364 to see if they work. Some ISPs, although I believe mostly mobile ones, also use something called CGNAT where your Internet IP is actually an internal network IP within a part of the ISP, and you can usually test for this by checking if your IP is in a private range. reply haunter 19 hours agoprev>For this article’s purpose, I won’t recommend any specific servers because this will vary a ton depending on what you will be hosting. NAS storage, VMs, web servers, backup servers, mail servers, ad blockers, and all the rest. For my requirements, I purchased a ThinkCentre M73 and ThinkCentre M715q; both used off eBay. This is the way. HP, Lenovo, and Dell are all making identical small form factor PCs that are more than enough for most people at home. They are also very silent and power efficient. I have a Lenovo M720 Tiny and a Dell Optiplex 3080 Micro (they are virtually almost the same). You can change parts, there are ample ports available and you can pretty much run any OS you want. reply Andrex 14 hours agoparentI've been pretty happy with the EliteDesk G3 I got off eBay a couple months ago. I actually use it for light work (mostly spreadsheets and emails) too. I think I paid less than $130 including shipping and sometimes it (anecdotally) has better performance than my home PC which is a full tower I bought new 4-5 years back. reply bazmattaz 12 hours agorootparentI love my G3. I found with a beefy i7 8 core cpu that sips 7w at idle. These machines are gold reply laweijfmvo 15 hours agoparentprev> For my requirements, I purchased a ThinkCentre M73 and ThinkCentre M715q; both used off eBay. This deserves a shout out. These things are everywhere on eBay and they Ryzen ones rock. reply justinlloyd 17 hours agoprevThe notion of a homelab is really, more of a suggestion. I have, what you might call, a \"homeprod.\" \"Be aware, the video server and sync server will be offline between the hours of 8PM and 10PM on Friday, March 8th for hardware upgrades. Expected downtime should not be more than 20 minutes but please plan around this disruption to essential services. Other services should not be affected during this work.\" Like someone else mentioned in this thread: It was really best described as a \"home network with more than one computer.\" But you can't really put that on your C.V. under \"experience.\" \"Improved SLA uptime on client's network to five nines. Upgraded storage controllers in a live environment without disruption to established services. Deployed k8s cluster into a production environment serving thousands of hours of compressed video files with real-time format transcoding.\" I will freely admit, my home network probably needs to be pared down a bit. The problem is the salvaged hardware and good deals at the liquidation auctions accumulates faster than I can cart it off to the street corner. There's also far too many laptops floating around the house doing nothing useful that could be donated away. A lot of the homelab is salvage or bought at (in-person) liquidation auctions. There's some new stuff, but I rarely need the latest and greatest for my setup. In my home network diagram I try to document everything that touches the network so I can stay on top of the security patches, and what has access to the outside world and in what way. https://justinlloyd.li/wp-content/uploads/2023/11/home-netwo... reply MrVitaliy 18 hours agoprevSurprised author didn't mention https://labgopher.com/ for finding used enterprise hardware at low cost on ebay. This hardware is cheap and typically out-of-service for enterprise, but great for home labs. reply bombcar 17 hours agoparentThe big problem with off-lease enterprise hardware is the power usage can quickly outrun the original costs - but that may not be an issue for everyone. The nice thing about off-lease enterprise hardware is there is so much of it, and it's enterprise. You can easily find replacement parts, and working on that stuff is a joy, everything is easily replaceable with no tools. reply mech422 16 hours agorootparentTHIS! My Dell and HP blade chassis/server EAT power (10+ Xeon blades). I actually shut them down and started moving to SBC based stuff like odriod H2/H3's. Basically silent and just sips power. All much less heat (I live in a desert, so thats important)! 0: https://ameridroid.com/products/odroid-h3 reply bombcar 15 hours agorootparentFor what it's worth, we're just starting to get to where enterprise stuff that cares about power usage is coming off-lease. Some of the newest stuff actually sips power relatively when not under load. My biggest (current) problem is finding something that supports a ton of SATA drives and still is low power (though at some point even the power usage of spinning rust can become noticeable). I live in the icy wilderness, so excess power to heat isn't a problem for most of the year. reply mech422 13 hours agorootparentyeah - I don't have a need for a lot of drives, but its hard to come by...but the odroids have a PCIE expansion slot, so you might be able to add a add-on controller... Given how huge drives are now, 1 nvme and 1 sata/spinning rust is basically all I'd ever need (I don't do video/music/etc. ) Edit: Oh shit...just checked, it does NOT have a PCIE slot..I was sure it did :-( reply kjkjadksj 17 hours agorootparentprevFor the price of an 8th gen off lease pc on ebay you can buy a new alder lake N minipc with 6 tdp. reply hollerith 17 hours agorootparentSome people even use an Alder-Lake-N miniPC as a \"user-facing machine\" (i.e., as the only computer in the room). I do, and I love it so far (after about 45 days of using it) because it doesn't have a fan to make noise and gradually become noisier as the machine ages. My guess is that if I was used to a much faster machine, then the machine's slowness would bother me, but it in fact does not bother me. (The machine has an NVMe interface, and I have a fast SSD in there. I have the N100 CPU because I couldn't find the N200 or the i3-N305 for immediate delivery from a non-Chinese manufacturer.) reply tivert 17 hours agoprev> Attic > Pros: Less noise, easier cable runs. > Cons: can get hot depending on where you live, roof leaks, humidity/condensation, and creepy at night. Where do attics not get hot? I live in northern climate, and ours regularly gets hotter than 140F during sunny summer days. And I say hotter, because the remote temp sensor I have up there maxes out at 140F. I wouldn't be surprised if it actually gets up to 150F or more. reply zeroping 17 hours agoparentMy attic is insulated at the roof sheathing, not the ceiling. My attic is basically 'inside' the house, from an airflow perspective. It peaks around 87°F in late June. Not ideal, but workable. reply tills13 17 hours agoparentprevConditioned attics are the new rage. Once it's conditioned, you can move other mechanical devices up there like HVAC etc to reduce the amount of ducting your home needs (especially if it's single-story). It's nice because that loose, shredded insulation is garbage and so easy to mess up (walking on it can change it from R40 to R20, for example). With a conditioned attic you use standard batting in between the roof members -- impossible to mess up. reply duped 16 hours agorootparentSounds like a nightmare for your energy bills. At least with interior ductwork the losses go to heating/cooling your home instead of your attic, which even with insulation is going to act like a giant heatsink that you get nothing out of. reply tills13 13 hours agorootparentI don't think so -- you _are_ paying to condition that part of your house, sure, but it becomes just another part of the overall envelope and acts like a buffer between the weather and the actually livable parts of your home. Anecdotally, in my home in the summer months, when unconditioned the attic is 50 degrees on a summer day it literally radiates heat down through the ceiling on the floor below into the rooms there. The attic acts like a heat battery -- long after the sun goes down the attic is radiating heat into the upper floors of my house. Not sure why it retains heat so well it's got louvres and vents ... reply duped 12 hours agorootparentWhen you have ductwork in the attic your heating/cooling efficiency is hurt by the amount of heat lost through the roof and out into the environment. When the ductwork is all in interior walls, there's no efficiency lost because those losses go into the rooms the system was trying to heat/cool already. Where I live, we have extreme ranges of temperature (both hot and cold) and almost all ductwork is on the interior for this reason. You'd be spending a fortune in the winter heating an attic when it's getting lost to -17C air temps outside... It sounds like your attic needs more insulation. reply ardaoweo 14 hours agoprevTiny PC's make excellent low-power servers for lots of purposes, but personally in the end I just decided to build my own desktop PC from cheapish previous gen parts to have more flexibility. Now I have an AM4 consumer board based system with 8 core Ryzen 7, more ECC ram than I will ever need, 4 enterprise grade SSD's with ZFS, intel NIC, and RTX 3060 for some AI experimentation. Thanks to efficient PSU choice the whole thing idles at just 30W running Linux, about 10 of which is due to the dedicated GPU. I should be even able to game on it through GPU passthrough to Proxmox VM, though I haven't tried it yet. For networking I can recommend Fujitsu Futro S920 thin clients, especially for EU users. They are dirt cheap here, and with aftermarket riser & intel NIC they are great routers with opnsense/pfsense. Apart from that, I have a couple of raspberry pi's running dedicated tasks, and space heaters (e-waste tier enterprise desktops) running BOINC during coldest months. reply Jun8 8 hours agoprevThere are a ton of resources about HW aspects of home labs for beginners but not so much for what to run on them and why. There are lists like https://github.com/awesome-selfhosted/awesome-selfhosted but they are confusing for absolute beginners like me. Are there any good SW project guides you know? reply gomoboo 7 hours agoparentI’m running a scraper for Dune movie tickets on mine. Before that I didn’t really have anything to do on it besides learning about Proxmox, VMs, networking, and cloud-init. Maybe try to run something you’d usually spin up a VPS for. reply NortySpock 5 hours agoparentprevAdmittedly, it makes way more sense once you have more than one person in your household. I ended up running most things in docker on a Raspberry Pi 4. Started with - PiHole to cut down on ads and try to keep the kids from clicking malware links - added network attached storage to make it easier to share files with the family - added Jellyfin to share music with people in the house (kids gotta grow up listening to good music) - added syncthing to be able to ditch dropbox and still keep multiple computer file folders synced - ran a Minecraft server, then a Terraria server, then shut those down since we moved on to other games - turned on an ntp server since ddwrt router wasn't getting the time from the internet reliably - added librespeed and ittools for quick wifi-speed-testing and to convert docker-run commands to docker-compose commands - needed a webpage with links to start keeping track of all of this, so lighttpd it is... - smokeping and blip were fired up when the internet got flaky and I needed more data to explain to the ISP what I was seeing - Got a 3D printer, and Octoprint made it easy to start and monitor prints without having to pull a microSD card out of the socket every time. - Figured, while we're here, that I might as well explore the smart-home thing with HomeAssistant -- now we have a handful of lightbulbs and smart power switches that mean you don't have to get up to change the lights quite as often (not everything is on a smart control, just a few out-of-the-way things. Oh, yeah, handy during Christmas-time to turn the tree lights on and off without having to fumble behind the tree for the little switch.) - That, of course, lead to an MQTT broker (mosquitto), which is kind of a budget pub-sub broker - And now that I have docker logs with things I need to debug, Dozzle serves those logs in a handy webpage. Not secure, but it's not like this is enterprise server. - Then I wanted to play around with timeseries data (just to learn), so (running out of RAM at this point, so I couldn't just fire up Grafana) kept digging until I found VictoriaMetrics, which was quite happy to catch metrics (fired at it from Benthos) and visualize some 3 years of hourly Beijing weather data. (Seriously, I'm still amazed how lightweight, simple, and easy to use VictoriaMetrics is) - currently thinking of kicking the tires on the NATS message broker and see if I can get the multi-node clustering to work. No real purpose other than see what it's like and dream about silly developer projects. - also thinking of exploring SeaweedFS -- might be nice to have horizontally scalable storage, as each of my nodes are scattered around the house depending on where I can put an Ethernet drop. - after a few years going along this journey, it has the perk of having helped me get my current job, which is pretty cool. So... it's mostly a plaything, a sandbox, but a handful of things run \"in production\" at home and make life around the house slightly more convenient and less risky. By being choosy with the software and a bit aggressive on the container RAM limits, I've got 27 containers running on a Raspberry Pi. Hey, gotta take my bragging rights somewhere. reply tonymet 17 hours agoprevI'll go against the grain and recommend running your lab on Windows 11 Pro and Hyper V (often included with reclaimed PCs). Hyper-V is a very capable hypervisor and Windows hardware compatibility is excellent. Remote mgmt can be done over RDP, SSH or WMI Nearly everyone has a spare PC lying around collecting dust, and within an hour it can be running a dozen VMs . Reclaimed HP & Dells are cheap and include a Windows Pro license. otherwise they can be had for $20 I recommend Alpine for the guest OS. It's fast to set up, natively supports Hyper-V hosts and compact (about 1/10th the size of a debian guest). https://learn.microsoft.com/en-us/windows-server/virtualizat... https://wiki.alpinelinux.org/wiki/Hyper-V_guest_services reply arh68 13 hours agoparentI absolutely agree. Windows Pro + Hyper-V + Linux = super good. And I'm a Mac guy, if anything. Remote Desktop is really good, too. I use it all the time from my MacBook (though I mostly SSH, naturally). WinSCP + midnight commander are all the extras I need. If you have enough memory/CPU on your workstation/gaming machine, Just 1 PC works great. reply theideaofcoffee 15 hours agoparentprevThis is how I have my setup running, a moderately-outdated, but still beefy (E5-2699v4, 512GB ram, etc), Dell Precision tower, which originally started as a design machine with a handful of Autodesk and EDA tools. I wanted to try out proxmox, but I was too lazy to want to move all of those installed things off onto another host, reinstall, then move it back into a Windows VM. I suppose I could still run proxmox in a HyperV instance and then VMs within there, but it feels like too many layers of indirection. HyperV is perfect for what I want to do, run a dozen or two sandbox and experimental VMs. I can pass through hardware, assign raw volumes, partition out network interfaces for routers and other shenanigans and it's stable too. And with RDP, I just use the mac version of the remote desktop client to use it interactively, which also works surprisingly well even over tailscale on another continent! reply tonymet 15 hours agorootparentsounds like an awesome setup. this is what I tell my friends. Just run Hyper-V. They can easily be checkpointed and moved to another hypervisor. Use what you got! reply buybackoff 16 hours agoparentprevYou could use this built-in OEM license from a Windows VM in Proxmox. Just search for that on Proxmox forum. Hyper-V is very limited in hardware support, you won't be able to pass-through stuff. And you always pay for VM layer, while in Proxmox you could use containers which give almost the host performance. reply haunter 16 hours agorootparentHyper V Server 2019 (not the same as Hyper V inside Win 10/11) is free and has GPU pass through https://learn.microsoft.com/en-us/windows-server/virtualizat... https://www.microsoft.com/en-us/evalcenter/evaluate-hyper-v-... reply tonymet 16 hours agorootparentprevthat's changed in recent years. you now have access to hardware and the gpu for inference, for example. reply buybackoff 16 hours agorootparentMaybe in the Server version. I haven't seen anything useful in Windows Pro. I could not setup SR-IOV even with Powershell after wasting a lot of time on searching for help. reply daemin 14 hours agoparentprevHow do you handle the fact that Windows 11 is a desktop OS and it tends to want to do whatever it feels like restarting for updates and such. I would imagine it's a bit of a pain to have the system reboot at random, killing your VMs etc. reply arh68 13 hours agorootparentWindows 10 hardly restarts a lot. There's no pain. If I reboot windows, my Debian VM comes right back up, with all the docker-compose services running (a la `restart: always`). Like nothing ever happened. So I just don't think this is a problem. But that's just my experience... reply tonymet 13 hours agorootparentprevI manage this by scheduling restarts. Maybe 2-3x / month. fwiw my linux machines get periodic kernel & firmware updates that also need restarting reply haunter 16 hours agoparentprevNot sure why are you downvoted, Hyper V is perfectly fine Hyper V Server 2019 is also free you don’t have to pay for anything https://www.microsoft.com/en-us/evalcenter/evaluate-hyper-v-... And you can install a full GUI too https://gist.github.com/bp2008/922b326bf30222b51da08146746c7... reply tonymet 16 hours agorootparentthat's an interesting option. does it boot into cmd.exe by default? reply haunter 15 hours agorootparentYes that only but you can install a GUI + a web interface reply FredPret 17 hours agoprevIt'd be so cool if we could have permanent routable IPv6 addresses for every machine. Home Labs would become 1000x more useful, and I bet Digital Ocean and similar would see an exodus of small site hosters like me reply hardwaresofton 13 hours agoprevShameless plug, but if you're looking for awesome software to run in your home lab, I've got just the thing: https://awsmfoss.com The quality of software out there these days is insane -- there are projects that cover so many crucial functions (tracking expenses, playing music, managing data, etc), that it's becoming really hard to track them all. I've been working on converting the site to a directory and it's not quite there yet -- but some people do like the massive wall of projects. reply quaffapint 19 hours agoprevUnless you simply want to play with all the various bits of hardware, a standard PC tower will be more power efficient and save a lot of room. I use that with unRaid and a simple router and switch and it's power efficient, doesn't make much noise, cheaper, and takes up much less room. reply justinclift 19 hours agoparent> a standard PC tower will be more power efficient How much does it average in power draw (watts) from the wall? reply NegativeK 18 hours agorootparentI can't speak to if it's more or less efficient, but I have one PC taking backups that's running at 43W idle. A different PC based server, which is a much more highly specced retired desktop, is running at 72W idle. reply justinclift 18 hours agorootparentFor standard PC's (ie with ATX based power supply) the 43W one seems pretty decent at idle. The small form factor gear though can have much more efficient power supplies, so some of those reportedly idle as low as 6w. That's total power draw at the wall. Much more efficient than a standard desktop PC. --- That aside, apparently that newer 12 volt power supply standard being pushed by Intel is good for lower power idle too. Hopefully that gains traction and reduces everyone's power bills. :) https://hackaday.com/2021/06/07/intels-atx12vo-standard-a-st... reply treflop 17 hours agoprevI run a more mundane setup using the most possible boring choices (lol) and it's been reliable. I occasionally restart random things and pull network cables to fuck with it to make sure that it can recover. * Hardware: A regular but small (SFF) computer + Synology NAS * Software: Ubuntu distro + Docker containers bare * Configuration: Git-versioned Dockerfiles and docker-compose.ymls * Storage: The NAS with Synology Hybrid RAID * Backup: TBD... haven't figured out a cheap and automatic way to backup 20 TB of data off-site I run cameras, a media server shared with friends, a shared place to dump project files, a DNS server (just in case the Internet goes down so that everything local still works) and supporting services. reply albertzeyer 9 hours agoprevIt's linked there, but I found this interesting: To get some more inspiration, visit: https://www.reddit.com/r/homelab/ https://www.reddit.com/r/minilab/ My \"homelab\" for now is a single Raspberry Pi (I want it silent and not take much power when idling). But from there, I can power-on my desktop, via wakeonlan. reply jprd 16 hours agoprevI have an obscene amount of retired Chromeboxen that I have flashed with Mr.Chromebox[0] EFI and running as either K8s, Proxmox or independent Linux/Windows nodes at any given time. These are mostly 1st-gen i7-4600U Haswell's, upgraded to 16GB RAM and 128G+ SSDs. I ripped them out of their cases, added heatsinks to the new SSDs, and then \"racked\" them by wedging the front-bottom mainboard corner (if plugs are oriented to the rear and the heaviest plug, AC, at the rear-bottom for balance) into a slotted piece of super-strong and right-angled cardboard that came from inside some re-inforced shipping box (I think it was an X-mas tree). I assembled all of this juuuuuuust before COVID while learning K8s and craving the physical plan aspect, as I had just transitioned to a remote SaaS role from one that involved multiple physical DCs and travel. My \"go small lab\" initiative turned into some sort of Tribble fable. [0] https://mrchromebox.tech/ reply runjake 17 hours agoprevIf you're old and grumpy[1] and don't want a homelab but paradoxically want homelab functionality, I can't recommend the Firewalla[2] enough. And heck, it would fit right into a homelab situation, as well. It allows you to go deep into settings and configure anything to your heart's content. But, if you want to ignore it for a year and not mess with it, it will run as smoothly as an Eero or a similar consumer product like that. I have a Purple model[3]. I initially balked at the price, but it performs well, and there hasn't been a single day in the past couple years where I've regretted it or looked at something else. It supports just about any router feature. Has robust and easy to configure device/family profiles. The quality of the Firewalla software and the mobile app is excellent. --- 1. I manage a quite large WAN during the day. I don't want to do it at night, too. 2. https://firewalla.com/ 3. https://firewalla.com/products/firewalla-purple reply kmos17 15 hours agoparentI've seen it mentionned on hn before and was intrigued as a replacement/simpler solution to a home pfsense setup. It seems there are issues with it though: https://old.reddit.com/r/firewalla/comments/18wvlqa/steer_cl... The software is open source apparently? https://github.com/firewalla/firewalla Anyone running this on their own hardware? O",
    "originSummary": [
      "The blog post highlights the significance of establishing a home lab for safe tech experimentation.",
      "It covers selecting hardware, setting up a location, choosing networking gear, and servers for the lab.",
      "Recommendations are given for equipment such as modems, UPS systems, cooling fans, routers, switches, and server brands."
    ],
    "commentSummary": [
      "The article explores creating a home lab with compact computers like NUCs or Dell SFFs for space efficiency and quiet operation.",
      "Users suggest OptiPlex PCs and Odroid SBCs for home labs, discussing networking, storage, container orchestration, OS options, and disaster recovery.",
      "It delves into experiences with hardware, software tools, and community assistance for managing VMs and containers at home."
    ],
    "points": 546,
    "commentCount": 343,
    "retryCount": 0,
    "time": 1709905633
  },
  {
    "id": 39643136,
    "title": "Introducing Hatchet: The New Open-Source Task Queue",
    "originLink": "https://github.com/hatchet-dev/hatchet",
    "originBody": "Hello HN, we&#x27;re Gabe and Alexander from Hatchet (https:&#x2F;&#x2F;hatchet.run), we&#x27;re working on an open-source, distributed task queue. It&#x27;s an alternative to tools like Celery for Python and BullMQ for Node.js, primarily focused on reliability and observability. It uses Postgres for the underlying queue.Why build another managed queue? We wanted to build something with the benefits of full transactional enqueueing - particularly for dependent, DAG-style execution - and felt strongly that Postgres solves for 99.9% of queueing use-cases better than most alternatives (Celery uses Redis or RabbitMQ as a broker, BullMQ uses Redis). Since the introduction of SKIP LOCKED and the milestones of recent PG releases (like active-active replication), it&#x27;s becoming more feasible to horizontally scale Postgres across multiple regions and vertically scale to 10k TPS or more. Many queues (like BullMQ) are built on Redis and data loss can occur when suffering OOM if you&#x27;re not careful, and using PG helps avoid an entire class of problems.We also wanted something that was significantly easier to use and debug for application developers. A lot of times the burden of building task observability falls on the infra&#x2F;platform team (for example, asking the infra team to build a Grafana view for their tasks based on exported prom metrics). We&#x27;re building this type of observability directly into Hatchet.What do we mean by \"distributed\"? You can run workers (the instances which run tasks) across multiple VMs, clusters and regions - they are remotely invoked via a long-lived gRPC connection with the Hatchet queue. We&#x27;ve attempted to optimize our latency to get our task start times down to 25-50ms and much more optimization is on the roadmap.We also support a number of extra features that you&#x27;d expect, like retries, timeouts, cron schedules, dependent tasks. A few things we&#x27;re currently working on - we use RabbitMQ (confusing, yes) for pub&#x2F;sub between engine components and would prefer to just use Postgres, but didn&#x27;t want to spend additional time on the exchange logic until we built a stable underlying queue. We are also considering the use of NATS for engine-engine and engine-worker connections.We&#x27;d greatly appreciate any feedback you have and hope you get the chance to try out Hatchet.",
    "commentLink": "https://news.ycombinator.com/item?id=39643136",
    "commentBody": "Hatchet – Open-source distributed task queue (github.com/hatchet-dev)458 points by abelanger 16 hours agohidepastfavorite137 comments Hello HN, we're Gabe and Alexander from Hatchet (https://hatchet.run), we're working on an open-source, distributed task queue. It's an alternative to tools like Celery for Python and BullMQ for Node.js, primarily focused on reliability and observability. It uses Postgres for the underlying queue. Why build another managed queue? We wanted to build something with the benefits of full transactional enqueueing - particularly for dependent, DAG-style execution - and felt strongly that Postgres solves for 99.9% of queueing use-cases better than most alternatives (Celery uses Redis or RabbitMQ as a broker, BullMQ uses Redis). Since the introduction of SKIP LOCKED and the milestones of recent PG releases (like active-active replication), it's becoming more feasible to horizontally scale Postgres across multiple regions and vertically scale to 10k TPS or more. Many queues (like BullMQ) are built on Redis and data loss can occur when suffering OOM if you're not careful, and using PG helps avoid an entire class of problems. We also wanted something that was significantly easier to use and debug for application developers. A lot of times the burden of building task observability falls on the infra/platform team (for example, asking the infra team to build a Grafana view for their tasks based on exported prom metrics). We're building this type of observability directly into Hatchet. What do we mean by \"distributed\"? You can run workers (the instances which run tasks) across multiple VMs, clusters and regions - they are remotely invoked via a long-lived gRPC connection with the Hatchet queue. We've attempted to optimize our latency to get our task start times down to 25-50ms and much more optimization is on the roadmap. We also support a number of extra features that you'd expect, like retries, timeouts, cron schedules, dependent tasks. A few things we're currently working on - we use RabbitMQ (confusing, yes) for pub/sub between engine components and would prefer to just use Postgres, but didn't want to spend additional time on the exchange logic until we built a stable underlying queue. We are also considering the use of NATS for engine-engine and engine-worker connections. We'd greatly appreciate any feedback you have and hope you get the chance to try out Hatchet. kcorbitt 15 hours agoI love your vision and am excited to see the execution! I've been looking for exactly this product (postgres-backed task queue with workers in multiple languages and decent built-in observability) for like... 3 years. Every 6 months I'll check in and see if someone has built it yet, evaluate the alternatives, and come away disappointed. One important feature request that probably would block our adoption: one reason why I prefer a postgres-backed queue over eg. Redis is just to simplify our infra by having fewer servers and technologies in the stack. Adding in RabbitMQ is definitely an extra dependency I'd really like to avoid. (Currently we've settled on graphile-worker which is fine for what it does, but leaves a lot of boxes unchecked.) reply ako 11 hours agoparentFunny how this is vision now. I started my career 29 years ago at a company that build exactly this, but based on oracle. The agents would run on Solaris, aix, vax vms, hpux, windows nt, iris, etc. Was also used to create an automated cicd pipeline to build all binaries on all these different systems. reply throwawaymaths 1 hour agorootparentAlso basically has existed as an open source (pro version has web dashboard and complex task zoo) drop-in library (no sidecar dependencies outside of postgres) in Elixir for years called Oban. reply abelanger 15 hours agoparentprevThank you, appreciate the kind words! What boxes are you looking to check? Yes, I'm not a fan of the RabbitMQ dependency either - see here for the reasoning: https://news.ycombinator.com/item?id=39643940. It would take some work to replace this with listen/notify in Postgres, less work to replace this with an in-memory component, but we can't provide the same guarantees in that case. reply kcorbitt 10 hours agorootparentBoxes-wise, I'd like a management interface at least as good as the one Sidekiq had in Rails for years. Would also need some hard numbers around performance and probably a bit more battle-testing before using this in our current product. reply jaggederest 10 hours agorootparentprevI come to this only as an interested observer, but my experience with listen/notify is that it outperforms rabbitmq/kafka in small to medium operations and has always pleasantly surprised me. You might find out it's a little easier than you think to slim your dependency stack down. reply bevekspldnw 11 hours agoparentprevYou can do a fair amount of this with Postgres using locks out of the box. It’s not super intuitive but I’ve been using just Postgres and locks in production for many years for large task distribution across independent nodes. reply renegade-otter 7 hours agorootparentI wrote about one simple implementation: https://renegadeotter.com/2023/11/30/job-queues-with-postrgr... reply bevekspldnw 5 hours agorootparentLooks very similar to my solution. :-) reply BenjieGillam 14 hours agoparentprevNot sure if you saw it but Graphile Worker supports jobs written in arbitrary languages so long as your OS can execute them: https://worker.graphile.org/docs/tasks#loading-executable-fi... Would be interested to know what features you feel it’s lacking. reply kcorbitt 10 hours agorootparentThat's interesting! Would that still involve each worker node needing to have Nodejs installed to run the process that actually reads from the queue? That's doable, but makes the deployment story a little more annoying/complicated if I want a worker that just runs Python or Rust or something. Feature-wise, the biggest missing pieces from Graphile Worker for me are (1) a robust management web ui and (2) really strong documentation. reply BenjieGillam 9 hours agorootparentYes, currently Node is the runtime but we could bundle that up into a binary blob if that would help; one thing to download rather than installing Node and all its dependencies? A UI is a common request, something I’ve been considering investing effort into. I don’t think we’ll ever have one in the core package, but probably as a separate package/plugin (even a third party one); we’ve been thinking more about the events and APIs such a system would need and making these available, and adding a plugin system to enable tighter integration. Could you expand on what’s missing in the documentation? That’s been a focus recently (as you may have noticed with the new expanded docusaurus site linked previously rather than just a README), but documentation can always be improved. reply doctorpangloss 13 hours agoparentprevWhy does the RabbitMQ dependency matter? It was pretty painless for me to set up and write tests against. The operator works well and is really simple if you want to save money. I mean, isn’t Hatchett another dependency? Graphile Worker? I like all these things, but why draw the line at one thing over another over essentially aesthetics? You better start believing in dependencies if you’re a programmer. reply eska 12 hours agorootparentIntroducing another piece of software instead of using one you already use anyway introduces new failures. That’s hardly aesthetics. As a professional I’m allergic to statements like “you better start believing in X”. How can you even have objective discourse at work like that? reply doctorpangloss 12 hours agorootparent> Introducing another piece of software instead of using one you already use anyway introduces new failures. Okay, but we're talking about this on a post about using another piece of software. What is the rational for, well this additional dependency, Hatchet, that's okay, and its inevitable failures are okay, but this other dependency, RabbitMQ, which does something different, but will have fewer failures for some objective reasons, that's not okay? Hatchet is very much about aesthetics. What else does Hatchet have going on? It doesn't have a lot of history, it's going to have a lot of bugs. It works as a DSL written in Python annotations, which is very much an aesthetic choice, very much something I see a bunch of AI startups doing, which I personally think is kind of dumb. Like OpenAI tools are \"just\" JSON schemas, they don't reinvent everything, and yet Trigger, Hatchet, Runloop, etc., they're all doing DSLs. It hews to a specific promotional playbook that is also very aesthetic. Is this not the \"objective discourse at work\" you are looking for? I am not saying it is bad, I am saying that 99% of people adopting it will be doing so for essentially aesthetic reasons - and being less knowledgable about alternatives might describe 50-80% of the audience, but to me, being less knowledgeable as a \"professional\" is an aesthetic choice. There's nothing wrong with this. You can get into the weeds about what you meant by whatever you said. I am aware. But I am really saying, I'm dubious of anyone promoting \"Use my new thing X which is good because it doesn't introduce a new dependency.\" It's an oxymoron plainly on its face. It's not in their marketing copy but the author is talking about it here, and maybe the author isn't completely sincere, maybe the author doesn't care and will happily write everything on top of RabbitMQ if someone were willing to pay for it, because that decision doesn't really matter. The author is just being reactive to people's aesthetics, that programmers on social media \"like\" Postgres more than RabbitMQ, for reasons, and that means you can \"only\" use one, but that none of those reasons are particularly well informed by experience or whatever, yet nonetheless strongly held. When you want to explain something that doesn't make objective sense when read literally, okay, it might have an aesthetic explanation that makes more sense. reply necovek 4 hours agorootparentThere is some implicit context you are missing here. Tools like hatchet are one less dependency for projects already using Postgres: Postgres has become a de-facto database to build against. Compare that to an application built on top of Postgres and using Celery + Redis/RabbitMQ. Also, it seems like you are confusing aesthetic with ergonomics. Since forever, software developers have tried to improve on all of \"aesthetics\" (code/system structure appearance), \"ergonomics\" (how easy/fast is it to build with) and \"performance\" (how well it works), and the cycle has been continuous (we introduce extra abstractions, then do away with some when it gets overly complex, and on and on). reply danielovichdk 1 hour agorootparent\"Since forever, software developers have tried to improve on all of \"aesthetics\" (code/system structure appearance), \"ergonomics\" (how easy/fast is it to build with) and \"performance\" (how well it works), and the cycle has been continuous\" Fast,easy,well,cheap is not a quality measure but it sure is a way to build more useless abstractions. You tell me which abstractions has made your software twice as effective. reply eska 10 hours agorootparentprev> You can get into the weeds about what you meant by whatever you said. I am aware. >When you want to explain something that doesn't make objective sense when read literally, okay, it might have an aesthetic explanation that makes more sense. What an attitude and way to kill a discussion. Again, hard for me to imagine that you're able to have objective discussions at work. As you wish I won't engage in discourse with you so you can feel smart. reply danielovichdk 11 hours agorootparentprevI fully agree with you. 'But I am really saying, I'm dubious of anyone promoting \"Use my new thing X which is good because it doesn't introduce a new dependency.\"' \"Advances in software technology and increasing economic pressure have begun to break down many of the barriers to improved software productivity. The ${PRODUCT} is designed to remove the remaining barriers […]\" It reads like the above quote from the pitch of r1000 in 1985. https://datamuseum.dk/bits/30003882 reply blandflakes 11 hours agorootparentprevAnd you better start critically assessing dependencies if you're a programmer. They aren't free; this is a wild take. reply notpushkin 34 minutes agoprevCongrats on the launch! You say Celery can use Redis or RabbitMQ as a backend, but I've also used it with Postgres as a broker successfully, although on a smaller scale (just a single DB node). It's undocumented, so definitely won't recommend anybody using this in production now, but seems to still work fine. [1] How does Hatchet compare to this setup? Also, have you considered making a plugin backend for Celery, so that old systems can be ported more easily? [1]: https://stackoverflow.com/a/47604045/1593459 reply jerrygenser 15 hours agoprevSomething I really like about some pub/sub systems is Push subscriptions. For example in GCP pub/sub you can have a \"subscriber\" that is not pulling events off the queue but instead is an http endpoint where events are pushed to. The nice thing about this is that you can use a runtime like cloud run or lambda and allow that runtime to scale based on http requests and also scale to zero. Setting up autoscaling for workers can be a little bit more finicky, e.g. in kubernetes you might set up KEDA autoscaling based on some queue depth metrics but these might need to be exported from rabbit. I suppose you could have a setup where your daemon worker is making http requests and in that sense \"push\" to the place where jobs are actually running but this adds another level of complexity. Is there any plan to support a push model where you can push jobs into http and some daemons that are holding the http connections opened? reply jsmeaton 2 hours agoparenthttps://cloud.google.com/tasks is such a good model and I really want an open source version of it (or to finally bite the bullet and write my own). Having http targets means you get things like rate limiting, middleware, and observability that your regular application uses, and you aren’t tied to whatever backend the task system supports. Set up a separate scaling group and away you go. reply abelanger 14 hours agoparentprevI like that idea, basically the first HTTP request ensures the worker gets spun up on a lambda, and the task gets picked up on the next poll when the worker is running. We already have the underlying push model for our streaming feature: https://docs.hatchet.run/home/features/streaming. Can configure this to post to an HTTP endpoint pretty easily. The daemon feels fragile to me, why not just shut down the worker client-side after some period of inactivity? reply jerrygenser 14 hours agorootparentI think it depends on the http runtime. One of the things with cloud run is that if the server is not handling requests, it doesn't get CPU time. So even if the first request is \"wake up\", it wouldn't get any CPU to poll outside of the request-response cycle. You can configure cloud run to always allocate CPU but it's a lot more expensive. I don't think it would be a good autoscaling story since autoscaling is based on http requests being processed. (maybe can be done via CPU but that's may not be what you want, it may not even be cpu bound) reply tonyhb 10 hours agoparentprevYou might want to look at https://www.inngest.com for that. Disclaimer: I'm a cofounder. We released event-driven step functions about 20 months ago. reply alexbouchard 11 hours agoparentprevThe push queue model has major benefits has you mentioned. We've built Hookdeck (hookdeck.com) on that premise. I hope we see more projects adopt it. reply leetrout 14 hours agoprevJust pointing out even though this is a \"Show HN\" they are, indeed, backed by YC. Is this going to follow the \"open core\" pattern or will there be a different path to revenue? reply abelanger 12 hours agoparentYep, we're backed by YC in the W24 batch - this is evident on our landing page [1]. We're both second time CTOs and we've been on both sides of this, as consumers of and creators of OSS. I was previously a co-founder and CTO of Porter [2], which had an open-core model. There are two risks that most companies think about in the open core model: 1. Big companies using your platform without contributing back in some way or buying a license. I think this is less of a risk, because these organizations are incentivized to buy a support license to help with maintenance, upgrades, and since we sit on a critical path, with uptime. 2. Hyperscalers folding your product in to their offering [3]. This is a bigger risk but is also a bit of a \"champagne problem\". Note that smaller companies/individual developers are who we'd like to enable, not crowd out. If people would like to use our cloud offering because it reduces the headache for them, they should do so. If they just want to run our service and manage their own PostgreSQL, they should have the option to do that too. Based on all of this, here's where we land on things: 1. Everything we've built so far has been 100% MIT licensed. We'd like to keep it that way and make money off of Hatchet Cloud. We'll likely roll out a separate enterprise support agreement for self hosting. 2. Our cloud version isn't going to run a different core engine or API server than our open source version. We'll write interfaces for all plugins to our servers and engines, so even if we have something super specific to how we've chosen to do things on the cloud version, we'll expose the options to write your own plugins on the engine and server. 3. We'd like to make self-hosting as easy to use as our cloud version. We don't want our self-hosted offering to be a second-class citizen. Would love to hear everyone's thoughts on this. [1] https://hatchet.run [2] https://github.com/porter-dev/porter [3] https://www.elastic.co/blog/why-license-change-aws reply echelon 1 hour agorootparent> 2. Hyperscalers folding your product in to their offering [3]. This is a bigger risk but is also a bit of a \"champagne problem\". Champagne for them, not for you. Better put some venom and claws in your license. reply MuffinFlavored 14 hours agoparentprev> path to revenue There have to be at least 10 different ways between different cloud providers to run a distributed task queue. Amazon, Azure, GCP Self-hosting RabbitMQ, etc. I'm curious how they are able to convince investors that there is a sizable portion of market they think doesn't already have this solved (or already has it solved and is willing to migrate) reply Kinrany 14 hours agorootparentThere will be space for improvement until every cloud has a managed offering with exactly the same interface. Like docker, postgres, S3. reply leetrout 13 hours agorootparentprevI am curious to see where they differentiate themselves on observability on the longer run. Comparing to rabbitmq it should be easier to see what is in the queue itself without mutating it, for instance. reply MuffinFlavored 8 hours agorootparenthttps://www.rabbitmq.com/docs/management reply leetrout 8 hours agorootparentSure, but to see what is in the queue you have to operate on it, mutating it. With this using postgres we can just look in the table. reply Aeolun 5 hours agorootparentprev> I'm curious how they are able to convince investors that there is a sizable portion of market they think doesn't already have this solved Is there any task queue you are completely happy with? I use Redis, but it’s only half of the solution. reply wodenokoto 1 hour agoparentprevWasn’t the first Dropbox introduction also a show HN? I don’t think this is out of place reply welder 1 hour agoprevRelated, I also wrote my own distributed task queue in Python [0] and TypeScript [1] with a Show HN [2]. Time it took was about a week. I like your features, but it was easy to write my own so I'm curious how you're building a money making business around an open source product. Maybe the fact everyone writes their own means there's no best solution now, so you're trying to be that and do paid closed source features for revenue? [0] https://github.com/wakatime/wakaq [1] https://github.com/wakatime/wakaq-ts [2] https://news.ycombinator.com/item?id=32730038 reply wodenokoto 1 hour agoprevSince these are task executions in a DAG, to what degree does it compete with dagster or airflow? I get that I can’t define the task with Hatchet, but if I already want to separate my DAG from my tasks, is this a viable option? reply serbrech 2 hours agoprevI wish that this was just a sdk built on top of a provider/standard. Amqp 1.0 is a standard protocol. You can build all this without being tied to a product or to rabbitMQ, with a storage provider and a amqp protocol layer. reply bluehadoop 15 hours agoprevHow does this compare against Temporal/Cadence/Conductor? Does hatchet also support durable execution? https://temporal.io/ https://cadenceworkflow.io/ https://conductor-oss.org/ reply abelanger 15 hours agoparentIt's very similar - I used Temporal at a previous company to run a couple million workflows per month. The gRPC networking with workers is the most similar component, I especially liked that I only had to worry about an http2 connection with mTLS instead of a different broker protocol. Temporal is a powerful system but we were getting to the point where it took a full-time engineer to build an observability layer around Temporal. Integrating workflows in an intuitive way with OpenTelemetry and logging was surprisingly non-arbitrary. We wanted to build more of a Vercel-like experience for managing workflows. We have a section on the docs page for durable execution [1], also see the comment on HN [2]. Like I mention in that comment, we still have a long way to go before users can write a full workflow in code in the same style as a Temporal workflow, users either define the execution path ahead of time or invoke a child workflow from an existing workflow. This is also something that requires customization for each SDK - like Temporal's custom asyncio event loop in their Python SDK [3]. We don't want to roll this out until we can be sure about compatibility with the way most people write their functions. [1] https://docs.hatchet.run/home/features/durable-execution [2] https://news.ycombinator.com/item?id=39643881 [3] https://github.com/temporalio/sdk-python reply bicijay 13 hours agorootparentWell, you just got an user. Love the concept of temporal, but i can't justify the overhead you need with infra to make it work for the upper guys... And the cloud offering is a bit expensive for small companies. reply mfateev 10 hours agorootparentDo you know about the Temporal startup program? It gives enough credits to offset support fees for 2 years. https://temporal.io/startup reply Aeolun 5 hours agorootparentIf you are expecting to still be small after 2 years that just delays the expense until you are locked in? reply Kinrany 14 hours agoprevWith NATS in the stack, what's the advantage over using NATS directly? reply abelanger 4 hours agoparentI'm assuming specifically you mean Nex functions? Otherwise NATS gives you connectivity and a message queue - it doesn't (or didn't) have the concept of task executions or workflows. With regards to Nex -- it isn't fully stable and only supports Javascript/Webassembly. It's also extremely new, so I'd be curious to see how things stabilize in the coming year. reply rapnie 3 minutes agorootparentI recently found Nex in the context of Wasmcloud [0] and ability for it to support long-running tasks/workflows. Impression that indeed Nex needs a good time to mature still. There was also a talk [1] about using Temporal here. For Hatchet it may be interesting to check it out (note: I am not affiliated with Wasmcloud, nor currently using it). [0] https://wasmcloud.com [1] https://www.temporal.io/replay/videos/zero-downtime-deploys-... reply sroussey 14 hours agoprevAh nice! I am writing a job queue this weekend for a DAG based task runner, so timing is great. I will have a look. I don't need anything too big, but I have written some stuff for using PostgreSQL (FOR UPDATE SKIP LOCKED for the win), sqlite, and in-memory, depending on what I want to use it for. I want the task graph to run without thinking about retries, timeouts, serialized resources, etc. Interested to look at your particular approach. reply toddmorey 15 hours agoprevI need task queues where the client (web browser) can listen to the progress of the task through completion. I love the simplicity & approachability of Deno queues for example, but I’d need to roll my own way to subscribe to task status from the client. Wondering if perhaps the Postgres underpinnings here would make that possible. EDIT: seems so! https://docs.hatchet.run/home/features/streaming reply abelanger 15 hours agoparentYep, exactly - Gabe has also been thinking about providing per-user signed URLs to task executions so clients can subscribe more easily without a long-lived token. So basically, you would start the workflow from your API, and pass back the signed URL to the client, where we would then provide a React hook to get task updates automatically. We need this ourselves once we open our cloud instance up to self-serve, since we want to provision separate queues per user, with a Hatchet workflow of course. reply toddmorey 15 hours agorootparentAwesome to hear! reply rad_gruchalski 15 hours agoparentprevIf you need to listen for the progress only, try server-sent events, maybe?: https://en.wikipedia.org/wiki/Server-sent_events It's dead simple: an existence of the URI means the topic/channel/whathaveu exists, to access it one needs to know the URI, data streamed but no access to old data, multiple consumers no problem. reply SCUSKU 15 hours agoprevLooks pretty great! My biggest issue with Celery has been that the observability is pretty bad. Even if you use Celery Flower, it still just doesn’t give me enough insight when I’m trying to debug some problem in production. I’m all for just using Postgres in service of the grug brain philosophy. Will definitely be looking into this, congrats on the launch! reply abelanger 14 hours agoparentAppreciate it, thank you! We've spent quite a bit of time in the Celery Flower console. Admittedly it's been a while, I'm not sure if they've added views for chains/groups/etc - it was just a linear task view when I used it. A nice thing in Celery Flower is viewing the `args, kwargs`, whereas Hatchet operates on JSON request/response bodies, so some early users have mentioned that it's hard to get visibility into the exact typing/serialization that's happening. Something for us to work on. reply 9dev 12 hours agoparentprevI case you’re stuck with Celery for a while: I was hit with this same problem, and solved it by adding a sidecar HTTP server thread to the Python workers that would expose metrics written by the workers into a multithreaded registry. This has been working amazingly well in production for over two years now, and makes it really straightforward to get custom metrics out of a distributed Celery app. reply kamikaz1k 5 hours agorootparentAny chance you could share more specifics about your solution? reply 9dev 1 hour agorootparentHere you go: https://stackoverflow.com/questions/75652326/celery-spawn-si... Plus some adjacent discussion on GitHub: https://github.com/prometheus/client_python/issues/902 Hope that helps! reply fuddle 12 hours agoprevLooks great! Do you publish pricing for your cloud offering? For the self hosted option, are there plans to create a Kubernetes operator? With an MIT license do you fear Amazon could create a Amazon Hatchet Service sometime in the future? reply abelanger 11 hours agoparentThank you! > Do you publish pricing for your cloud offering? Not yet, we're rolling out the cloud offering slowly to make sure we don't experience any widespread outages. As soon as we're open for self-serve on the cloud side, we'll publish our pricing model. > For the self hosted option, are there plans to create a Kubernetes operator? Not at the moment, our initial plan was to help folks with a KEDA autoscaling setup based on Hatchet queue metrics, which is something I've done with Sidekiq queue depth. We'll probably wait to build a k8s operator after our existing Helm chart is relatively stable. > With an MIT license do you fear Amazon could create a Amazon Hatchet Service sometime in the future? Yes. The question is whether that risk is worth the tradeoff of not being MIT-licensed. There are also paths to getting integrated into AWS marketplace we'll explore longer-term. I added some thoughts here: https://news.ycombinator.com/item?id=39646788. reply beerkat 11 hours agoprevHow does this compare to River Queue (https://riverqueue.com/)? Besides the additional Python and TS client libraries. reply abelanger 5 hours agoparentThe underlying queue is very similar. See this comment, which details how we're different from a library client: https://news.ycombinator.com/item?id=39644327. We also have the concept of workflows, which last I checked doesn't exist in River. I'm personally very excited about River and I think it fills an important gap in the Go ecosystem! Also now that sqlc w/ pgx seems to be getting more popular, it's very easy to integrate. reply moribvndvs 13 hours agoprevOne repeat issue I’ve had with my past position is need to schedule an unlimited number of jobs, often months to year from now. Example use case: a patient schedules an appointment for a follow up in 6 months, so I schedule a series of appointment reminders in the days leading up to it. I might have millions of these jobs. I started out by just entering a record into a database queue and just polling every few seconds. Functional, but our IO costs for polling weren’t ideal, and we wanted to distribute this without using stuff like schedlock. I switched to Redis but it got complicated dealing with multiple dispatchers, OOM issues, and having to run a secondary job to move individual tasks in and out of the immediate queue, etc. I had started looking at switching to backing it with PG and SKIP LOCKED, etc. but I’ve changed positions. I can see a similar use case on my horizon wondered if Hatchet would be suitable for it. reply abelanger 11 hours agoparentIt wouldn't be suitable for that at the moment, but might be after some refactors coming this weekend. I wrote a very quick scheduling API which pushes schedules as workflow triggers, but it's only supported on the Go SDK. It also is CPU-intensive at thousands of schedules, as the schedules are run as separate goroutines (on a dedicated `ticker` service) - I'm not proud of this. This was a pattern that made sense for the cron schedule and I just adapted it for the one-time scheduling. Looking ahead (and back) in the database and placing an exclusive lock on the schedule is the way to do this. You basically guarantee scheduling at +/- the polling interval if your service goes down while maintaining the lock. This allows you to horizontally scale the `tickers` which are polling for the schedules. reply moribvndvs 11 hours agorootparentThanks for the follow-up! I’ll keep an eye on the progress. reply herval 13 hours agoparentprevwhy do you need to schedule things 6 months in advance, instead of, say, check everything that needs notifications in a rolling window (eg 24h ahead) and schedule those? reply moribvndvs 12 hours agorootparentWell, it was a dumbed down example. In that particular case, appointments can be added, removed, or moved at any moment, so I can’t just run one job every 24 hours to tee up the next day’s work and leave it at that. Simply polling the database for messages that are due to go out gives me my just-in-time queue, but then I need to build out the work to distribute it, and we didn’t like the IO costs. I did end up moving it Redis and basically ZADD an execution timestamp and job ID, then ZRANGEBYSCORE at my desired interval and remove those jobs as I successfully distribute them out to workers. I then set a fence time. At that time a job runs to move stuff that should have ran but didn’t (rare, thankfully) into a remediation queue, and load the next block of items that should run between now + fence. At the service level, any items with a scheduled date within the fence gets ZADDed after being inserted into the normal database. Anything outside the fence will be picked up at the appropriate time. This worked. I was able to ramp up the polling time to get near-real time dispatch while also noticeably reducing costs. Problems were some occasional Redis issues (OOM and having to either a keep bumping up the Redis instance size or reduce the fence duration), allowing multiple pollers for redundancy and scale (I used schelock for that :/), and occasionally a bug where the poller craps out in the middle of the Redis work resulting in at least once SLA which required downstream protections to make sure I don’t send the same message multiple time to the patient. Again, it all works but I’m interested in seeing if there are solutions that I don’t have to hand roll. reply tonyhb 9 hours agorootparentI built https://www.inngest.com specifically because of healthcare flows. You should check it out, with the obvious disclaimer that I'm biased. Here's what you need: 1. Functions which allow you to declaratively sleep until a specific time, automatically rescheduling jobs (https://www.inngest.com/docs/reference/functions/step-sleep-...). 2. Declarative cancellation, which allows you to cancel jobs if the user reschedules their appointment automatically (https://www.inngest.com/docs/guides/cancel-running-functions). 3. General reliability and API access. Inngest does that for you, but again — disclaimer, I made it and am biased. reply herval 10 hours agorootparentprevCouldn’t u just enqueue + change a status, then check before firing? I don’t see why you’d need more than a dumb queue and a db table for that, unless you’re doing millions of qps reply kbar13 12 hours agoparentprevcan you explain why this cannot be a simple daily cronjob to query for appointments upcoming nextand send out notifications at that time? polling every few seconds seems way overkill reply moribvndvs 12 hours agorootparentSure: https://news.ycombinator.com/item?id=39646719 reply kevinlu1248 16 hours agoprevWe're building a webhook services on FastAPI + Celery + Redis + Grafana + Loki and the experience with setting up every service incrementally was miserable, and even then it feels like logs are being dropped and we run into reliability issues. Felt like something like this should exist already but I couldn't find anything at the time. Really excited to see where this takes us! reply tasn 16 hours agoparentThat's exactly why we built Svix[1]. Building webhooks services, even with amazing tools like FastAPI, Celery and Redis is still a big pain. So we just built a product to solve it. Hatchet looks cool nonetheless. Queues are a pain for many other use-cases too. 1: https://www.svix.com reply krawczstef 10 hours agoprevCan you explain why you chose every function to take in context? https://github.com/hatchet-dev/hatchet/blob/main/python-sdk/... This seems like a lot of boiler plate to write functions with to me (context I created http://github.com/DAGWorks-Inc/hamilton). reply abelanger 5 hours agoparentWe did it because there are methods that should be accessed which don't map to `args` cleanly. For example, we let users call `context.log`, `context.done` (to determine whether to return on cancellation) or `context.step_output` (to dynamically access a parent's step output). Perhaps there's a more pythonic way to do this? Admittedly this is a pattern we adapted from Go. reply kamikaz1k 5 hours agorootparentProbably just have it attached to self, like self.context But nbd IMHO reply krawczstef 3 hours agorootparentyep nbd, but :/ reply krawczstef 3 hours agorootparentprevyou could just make them optional arguments that you inject if they're declared. Happy to chat more. With Hamilton we could actually build an alternative way to describe your API pretty easily... reply topicseed 16 hours agoprevWhat specific strategies does Hatchet employ to guarantee fault tolerance and enable durable execution? How does it handle partial failures in multi-step workflows? reply abelanger 15 hours agoparentEach task in Hatchet is backed by a workflow [1]. Workflows are predefined steps which are persisted in PostgreSQL. If a worker dies or crashes midway through (stops heartbeating to the engine), we reassign tasks (assuming they have retries left). We also track timeouts in the database, which means if we miss a timeout, we simply retry after some amount of time. Like I mentioned in the post, we avoid some classes of faults just by relying on PostgreSQL and persisting each workflow run, so you don't need to time out with distributed locks in Redis, for example, or worry about data loss if Redis OOMs. Our `ticker` service is basically its own worker which is assigned a lease for each step run. We also store the input/output of each workflow step in the database. So resuming a multi-step workflow is pretty simple - we just replay the step with the same input. To zoom out a bit - unlike many alternatives [2], the execution path of a multi-step workflow in Hatchet is declared ahead of time. There are tradeoffs to this approach; it makes it much easier to run a single-step workflow or if you know the workflow execution path ahead of time. You also avoid classes of problems related to workflow versioning, we can gracefully drain older workflow version with a different execution path. It's also more natural to debug and see a DAG execution instead of debugging procedural logic. The clear tradeoff is that you can't try...catch the execution of a single task or concatenate a bunch of futures that you wait for later. Roadmap-wise, we're considering adding procedural execution on top of our workflows concept. Which means providing a nice API for calling `await workflow.run` and capturing errors. These would be a higher-level concept in Hatchet and are not built yet. There are some interesting concepts around using semaphores and durable leases that are relevant here, which we're exploring [3]. [1] https://docs.hatchet.run/home/basics/workflows [2] https://temporal.io [3] https://www.citusdata.com/blog/2016/08/12/state-machines-to-... reply spenczar5 15 hours agorootparentWhat happens if a worker goes silent for longer than the heartbeat duration, then a new worker is spawned, then the original worker “comes back to life”? For example, because there was a network partition, or because the first worker’s host machine was sleeping, or even just that the first worker process was CPU starved? reply abelanger 11 hours agorootparentThe heartbeat duration (5s) is not the same as the inactive duration (60s). If a worker has been down for 60 seconds, we reassign to provide some buffer and handle unstable networks. Once someone asks we'll expose these options and make them configurable. We currently send cancellation signals for individual tasks to workers, but our cancellation signals aren't replayed if they fail on the network. This is an important edge case for us to figure out. There's not much we can do if the worker ignores that signal. We should probably add some alerting if we see multiple responses on the same task, because that means the worker is ignoring the cancellation signal. This would also be a problem if workloads start blocking the whole thread. reply spenczar5 10 hours agorootparentRight, I meant inactive duration, of course. Cancellation signals are tricky. You of course cannot be sure that the remote end receives it. This turns into the two generals problem. Yes, you need monitoring for this case. I work on scientific workloads which can completely consume CPU resources. This failure scenario is quite real. Not all tasks are idempotent, but it sounds like a prudent user should try to design things that way, since your system has “at least once” execution of tasks, as opposed to “at most once.” Despite any marketing claims, “exactly once” is not generally possible. Good docs on this point are important, as is configurability for cases when “at most once” is preferable. reply sigmarule 14 hours agorootparentprevI think the answer is no but just to be sure: are you able to trigger step executions programmatically from within a step, even if you can't await their results? Related, but separately: can you trigger a variable number of task executions from one step? If the answer to the previous question is yes then it would of course be trivial; if not, I'm wondering if you could i.e. have a task act as a generator and yield values, or just return a list, and have each individual item get passed off to its own execution of the next task(s) in the DAG. For example some of the examples involve a load_docs step, but all loaded docs seem to be passed to the next step execution in the DAG together, unless I'm just misunderstanding something. How could we tweak such an example to have a separate task execution per document loaded? The benefits of durable execution and being able to resume an intensive workflow without repeating work is lessened if you can't naturally/easily control the size of the unit of work for task executions. reply abelanger 13 hours agorootparentYou can execute a new workflow programmatically, for example see [1]. So people have triggered, for example, 50 child workflows from a parent step. As you've identified the difficult part there is the \"collect\" or \"gathering\" step, we've had people hack around that by waiting for all the steps from a second workflow (and falling back to the list events method to get status), but this isn't an approach I'd recommend and it's not well documented. And there's no circuit breaker. > I'm wondering if you could i.e. have a task act as a generator and yield values, or just return a list, and have each individual item get passed off to its own execution of the next task(s) in the DAG. Yeah, we were having a conversation yesterday about this - there's probably a simple decorator we could add so that if a step returns an array, and a child step is dependent on that parent step, it fans out if a `fanout` key is set. If we can avoid unstructured trace diagrams in favor of a nice DAG-style workflow execution we'd prefer to support that. The other thing we've started on is propagating a single \"flow id\" to each child workflow so we can provide the same visualization/tracing that we provide in each workflow execution. This is similar to AWS X-rays. As I mentioned we're working on the durable workflow model, and we'll find a way to make child workflows durable in the same way activities (and child workflows) are durable on Temporal. [1] https://docs.hatchet.run/sdks/typescript-sdk/api/admin-clien... reply topicseed 15 hours agorootparentprevThank you for the thorough response! reply Yanael 5 hours agoprevLooks very promising. Recently, I built an asynchronous DAG executor in Python, and I always felt I was reinventing the wheel, but when looking for a resilient and distributed DAG executor, nothing was really meeting the requirements. The feature set is appealing. Wondering if adding/removing/skipping nodes to the DAG dynamically at runtime is possible. reply leafmeal 3 hours agoparenta little late now, but I wonder if https://github.com/DataBiosphere/toil might meet your requirements reply mfrye0 12 hours agoprevI've been looking for this exact thing for awhile now. I'm just starting to dig into the docs and examples, and I have a question on workflows. I have an existing pipeline that runs tasks across two K8 clusters and share a DB. Is it possible to define steps in a workflow where the step run logic is setup to run elsewhere? Essentially not having an inline run function defined, and another worker process listening for that step name. reply abelanger 5 hours agoparentThis depends on the SDK - both Typescript and Golang support a `registerAction` method on the worker which basically let you register a single step to only run on that worker. You would then call `putWorkflow` programmatically before starting the worker. Steps are distributed by default so they run on the workers which have registered them. Happy to provide a more concrete example for the language you're using. reply acaloiar 14 hours agoprevA related lively dicussion from a few months ago: https://news.ycombinator.com/item?id=37636841 Long live Postgres queues. reply pyrossh 16 hours agoprevHow is this different from pg-boss[1]? Other than the distributed part it also seems to use skip locked. [1] https://github.com/timgit/pg-boss reply abelanger 15 hours agoparentI haven't used pg-boss, and feature-wise it looks very similar and is an impressive project. The core difference is that pg-boss is a library while Hatchet is a separate service which runs independently of your workers. This service also provides a UI and API for interacting with Hatchet - I don't think pg-boss has those things, so you'd probably have to build out observability yourself. This doesn't make a huge difference when you're at 1 worker, but having each worker poll your database can lead to DB issues if you're not careful - I've seen some pretty low-throughput setups for very long-running jobs using a database with 60 CPUs because of polling workers. Hatchet distributes in two layers - the \"engine\" and the \"worker\" layer. Each engine polls the database and fans out to the workers over a long-lived gRPC connection. This reduces pressure on the DB and lets us manage which workers to assign tasks to based on things like max concurrent runs on each worker or worker health. reply hinkley 13 hours agoprevIt’s been about a dozen years since I heard someone assert that some CI/CD services were the most reliable task scheduling software for periodic tasks (far better than cron). Shouldn’t the scheduling be factored out as a separate library? I found that shocking at the time, if plausible, and wondered why nobody pulled on that thread. I suppose like me they had bigger fish to fry. reply abelanger 11 hours agoparentThis reminds me of: https://news.ycombinator.com/item?id=28234057 If you're saying that the scheduling in Hatchet should be a separate library, we rely on go-cron [1] to run cron schedules. [1] https://github.com/go-co-op/gocron reply lelanthran 3 hours agoparentprevHonestly, I'm doing something like that right now, just not in a position to show. All I want is a simple way to specify a tree of jobs to run to do things like checkout a git branch, build it, run the tests, then install the artifacts. Or push a new static website to some site. Or periodically do something. My grug brain simply doesn't want to deal with modern way of doing $SHIT. I don't need to manage a million different tasks per hour, so scaling vertically is acceptable to me, and the benefits of scaling horizontally simply don't appear in my use cases. reply CoolCold 5 hours agoprevFrom your experience, what would be a good way for doing Postgres Master-Master ? My understanding that Postgres Professional/EnterpriseDB based solutions provide reliable M-M and those are proprietary. reply treesciencebot 13 hours agoprevLatency is really important and that is honestly why we re-wrote most of this stuck ourselves but the project with the gurantee of 25ms Distributed > Built on PostGRES Not what people usually mean by distributed, caveat emptor reply peterisdirsa 6 hours agoprevThis is not a viable product, it's a feature reply radus 15 hours agoprevYou've explained your value proposition vs. celery, but I'm curious if you also see Hatchet as an alternative to Nextflow/Snakemake which are commonly used in bioinformatics. reply nextworddev 16 hours agoprevI’m interested in self hosting this. What’s the recommendation here for state persistence and self healing? Wish there was a guide for a small team who wants to self host before trying managed cloud reply abelanger 15 hours agoparentI think we might have had a dead link in the README to our self-hosting guide, here it is: https://docs.hatchet.run/self-hosting. The component which needs the highest uptime is our ingestion service [1]. This ingests events from the Hatchet SDKs and is responsible for writing the workflow execution path, and then sends messages downstream to our other engine components. This is a horizontally scalable service and you should run at least 2 replicas across different AZs. Also see how to configure different services for engine components [2]. The other piece of this is PostgreSQL, use your favorite managed provider which has point-in-time restores and backups. This is the core of our self-healing, I'm not sure where it makes sense to route writes if the primary goes down. Let me know what you need for self-hosted docs, happy to write them up for you. [1] https://github.com/hatchet-dev/hatchet/tree/main/internal/se... [2] https://docs.hatchet.run/self-hosting/configuration-options#... reply cybice 13 hours agoprevWhy Hatchet might be better than Windmill: Windmill uses the same approach in PostgreSQL, very fast and has an incredibly good UI. reply dalberto 15 hours agoprevI'm curious if this supports coroutines at tasks in Python. It's especially useful for genAI, and legacy queues (namely Celery) are lacking in this regard. It would help to see a mapping of Celery to Hatchet as examples. The current examples require you to understand (and buy into) Hatchet's model, but that's hard to do without understanding how it compares to existing solutions. reply Kluggy 16 hours agoprevIn https://docs.hatchet.run/home/quickstart/installation, it says > Welcome to Hatchet! This guide walks you through getting set up on Hatchet Cloud. If you'd like to self-host Hatchet, please see the self-hosted quickstart instead. but the link to \"self-hosted quickstart\" links back to the same page reply abelanger 15 hours agoparentThis should be fixed now, here's the direct link: https://docs.hatchet.run/self-hosting. reply rheckart 12 hours agoprevAny plans for SDKs outside the current three? .NET Core & Java would be interesting to see.. reply abelanger 11 hours agoparentNot at the moment - the biggest ask has been Rails, but on the other hand Sidekiq is so beloved that I'm not sure it makes sense at the moment. We have our hands very full with the 3 SDKs, though I'd love for us to support a community-backed SDK. If anyone's interested in working on that, feel free to message us in the Discord. reply fcsp 11 hours agoprev> Hatchet is built on a low-latency queue (25ms average start) That seems pretty long - am I misunderstanding something? By my understanding this means the time from enqueue to job processing, maybe someone can enlighten me. reply abelanger 11 hours agoparentTo clarify - you're right, this is a long time in a message/event queue. It's not an eternity in a task queue which supports DAG-style workflows with concurrency limits and fairness strategies. The reason for this is you need to check all of the subscribed workers and assign a task in a transactional way. The limit on the Postgres level is probably on the order of 5-10ms on a managed PG provider. Have a look at: https://news.ycombinator.com/item?id=39593384. Also, these are not my benchmarks, but have a look at [1] for Temporal timings. [1] https://www.windmill.dev/blog/launch-week-1/fastest-workflow... reply mhh__ 11 hours agoparentprevIt's only a few billion instructions on a decent sized server these days reply spenczar5 10 hours agorootparentDamn, I want one of these 100GHz CPUs you have, that sounds great. I think you mean million :) reply jlokier 4 hours agorootparentYou'd be surprised. 1 billion instructions in 25ms is realistic these days. My laptop can execute about 400 billion CPU instructions per second on battery. That's about 10 billion instructions in 25ms. Ihat's the CPU alone, i.e. not including the GPU which would increase the total considerably. Also not counting SIMD lanes as separate: The count is bona fide assembly language instructions. It comes from cores running at ~4GHz, 8 issued instructions per clock, times 12 cores, plus 4 additional \"efficiency\" cores adding a bit more. People have confirmed by measurement the 8 instructions per clock is achievable (or close) in well-optimised code. Average code is more like 2-3 per cycle. Only for short periods as the CPU is likely to get hot and thermally throttle even with its fan. But when it throttles it'll still exceed 1 billion in 25ms. For perspective on how far silicon has come, the GPU on my laptop is reported to do about 14 trillion floating-point 32-bit calculations per second. reply mhh__ 1 hour agorootparentprevMy ipad has 8 cores executing about 4 to 6 billion instructions a second these days (3GHz at a most ipc of about two) reply zwaps 13 hours agoprevYou say this is for generative AI. How do you distribute inference across workers? Can one use just any protocol and how does this work together with the queue and fault tolerance? Could not find any specifics on generative AI in your docs. Thanks reply abelanger 13 hours agoparentThis isn't built specifically for generative AI, but generative AI apps typically have architectural issues that are solved by a good queueing system and worker pool. This is particularly true once you start integrating smaller, self-hosted LLMs or other types of models into your pipeline. > How do you distribute inference across workers? In Hatchet, \"run inference\" would be a task. By default, tasks get randomly assigned to workers in a FIFO fashion. But we give you a few options for controlling how tasks get ordered and sent. For example, let's say you'd like to limit users to 1 inference task at a time per session. You could do this by setting a concurrency key \"\" and `maxRuns=1` [1]. This means that for each session key, you only run 1 inference task. The purpose of this would be fairness. > Can one use just any protocol We handle the communication between the worker and the queue through a gRPC connection. We assume that you're passing JSON-serializable objects through the queue. [1] https://docs.hatchet.run/home/features/concurrency/round-rob... reply zwaps 6 hours agorootparentGot it, so the underlying infrastructure (the inference nodes, if you wish) would be something to be solved outside of Hatched, but it would then allow to schedule inference tasks per user with limits. reply hannasm 5 hours agoprevSeems like this summary should be in the README reply Nukesor 7 hours agoprevHey @abelanger, I got a few feature request for Pueue that were out of the scope as they didn't fit Pueue's vision, but seem to fit hatchet quite well (e.g. complex scheduling functionality and multi-agent support) :) One thing I'm missing from your website however, is an actual view from how the interface looks like, what does the actual user interface look like. Having the possibility to schedule stuff in a smart way is nice and all, but how do you *overlook* it? It's important to get a good overview of how your tasks perform. Once I'm convinced that this is actually a useful piece of software, I would like to reference you in the Readme of Pueue as a alternative for users that need more powerful scheduling features (or multi-client support) :) Would that be ok for you? reply abelanger 5 hours agoparentPueue looks cool, it's not an alternative to Hatchet though - looks like it's meant to be run in the terminal or by a user? We're very much meant to run in an application runtime. Like I mentioned here [1], we'll expand our comparison section over time. If Pueue's an alternative people are asking about, we'll definitely put it in there. > Having the possibility to schedule stuff in a smart way is nice and all, but how do you overlook it? It's important to get a good overview of how your tasks perform. I'm not sure what you mean by this. Perhaps you're referring to this - https://news.ycombinator.com/item?id=39647154 - in which case I'd say: most software is far from perfect. Our scheduling works but has limitations and is being refactored before we advertise it and build it into our other SDKs. [1] https://news.ycombinator.com/item?id=39643631 reply Fiahil 14 hours agoprevHow does this compare to ZeroMQ (ZMQ) ? https://zeromq.org/ reply vector_spaces 14 hours agoparentNot the OP or familiar with Hatchet, but generally ZeroMQ is a bit lower down in the stack -- it's something you'd build a distributed task queue or protocol on top of, but not something you'd usually reach for if you needed one for a web service or similar unless you had very special requirements and a specific, careful design in mind. This tool comes with more bells and whistles and presumably will be more constrained in what you can do with it, where ZeroMQ gives you the flexibility to build your own protocol. In principle they have many of the same use cases, like how you can buy ready made whipped cream or whip up your own with some heavy cream and sugar -- one approach is more constrained but works for most situations where you need some whipped cream, and the other is a lot more work and somewhat higher risk (you can over whip your cream and end up with butter), but you can do a lot more with it. reply jeremyjh 14 hours agoparentprevZeroMQ is a library that implements an application layer network protocol. Hatchet is a distributed job server with durability and transaction semantics. Two completely different things at very different levels of the stack. ZeroMQ supports fan-out messaging and other messaging patterns that could maybe be used as part of a job server, but it doesn't have anything to say about durability, retries, or other concerns that job servers take care of, much less a user interface. reply ctoth 12 hours agoprevMy only question is why did you call it Hatchet if it doesn't cut down on your logs? I'll show myself out. reply abelanger 5 hours agoparentReally have an axe to grind with this comment... reply sixhobbits 16 hours agoprevOne of my favourite spaces and presentation in readme is clear and immediately told me what it is and most of the key information that I usually complain is missing. However I am still missing a section on why this is different than any of the other existing and more mature solutions. What led you to develop this over existing options and what different tradeoffs did you make? Extra points if you can concisely tell me what you do badly that your 'competitors' do well because I don't believe there is a one best solution in this space, it is all tradeoffs reply sixhobbits 16 hours agoparentSorry I am dumb and commented after clicking on the link. I would just add your hn text to the readme as that is exactly what I was looking for reply abelanger 16 hours agorootparentDone [1]. We'll expand this section over time. There are also definite tradeoffs to our architecture - spoke to someone wanting the equivalent 1.5m PutRecord/s in Kinesis, which we're definitely not ready for because we're persist every event + task execution in Postgres. [1] https://github.com/hatchet-dev/hatchet/blob/main/README.md#h... reply neonsunset 3 hours agoprev [–] Yet another project written in this bad language :( reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Gabe and Alexander are creating Hatchet, an open-source distributed task queue, as a competitor to Celery and BullMQ, emphasizing reliability and observability, with Postgres as the queue's foundation.",
      "Hatchet aims for full transactional enqueueing and simplified task observability for developers, running workers on various VMs, clusters, and regions.",
      "The project includes optimizations for latency, along with enhancements such as retries, timeouts, and cron schedules, with feedback on the platform encouraged."
    ],
    "commentSummary": [
      "Discussion focuses on Hatchet, an open-source distributed task queue utilizing Postgres, known for its reliability and observability.",
      "Users appreciate Hatchet for its unique features and simple infrastructure compared to tools like RabbitMQ, Oban, and Graphile Worker.",
      "Talks cover improving Graphile Worker's management UI, exploring AI startups' claims, alternative backend setups for Celery, and challenges in job scheduling and distributed workflows."
    ],
    "points": 458,
    "commentCount": 137,
    "retryCount": 0,
    "time": 1709917655
  },
  {
    "id": 39643833,
    "title": "Teen Open-Sources Wallstreetlocal for Transparent SEC Filings",
    "originLink": "https://github.com/bruhbruhroblox/wallstreetlocal",
    "originBody": "Hello Hacker News! My name is Anonyo, and I am a seventeen-year-old from Southeast Michigan. This is wallstreetlocal, my passion project for the last year (and a half). I&#x27;ve posted this before, but I&#x27;ve finally open-sourced this entire project, so I thought I&#x27;d post it again.Heres the short pitch.The Securities and Exchange Commission (SEC) keeps record of every company in the United States. Companies whose holdings surpass $100 million though, are required to file a special type of form: the 13F form. This form, filed quarterly, discloses the filer&#x27;s holdings, providing transparency into their investment activities and allowing the public and other market participants to monitor them.The problem though, is that these holdings are often cumbersome to access, and valuable analysis is often hidden behind a paywall. Through wallstreetlocal, the SEC&#x27;s 13F filers become more accessible and open.By exploring the website (and the code), you can see the resources I used, check out some notable money managers I listed, and download any data that suits you. All for free. (Note, the mobile site likely needs work.)I made this project to better democratize SEC filings, and also to get some experience on my hands. I love computers, and one day hope to get involved with startups. In the comments, I&#x27;d appreciate any and all advice, as well as feedback on how to improve the site.",
    "commentLink": "https://news.ycombinator.com/item?id=39643833",
    "commentBody": "Wallstreetlocal – View investments from America's biggest companies (github.com/bruhbruhroblox)243 points by anonyonoor 16 hours agohidepastfavorite46 comments Hello Hacker News! My name is Anonyo, and I am a seventeen-year-old from Southeast Michigan. This is wallstreetlocal, my passion project for the last year (and a half). I've posted this before, but I've finally open-sourced this entire project, so I thought I'd post it again. Heres the short pitch. The Securities and Exchange Commission (SEC) keeps record of every company in the United States. Companies whose holdings surpass $100 million though, are required to file a special type of form: the 13F form. This form, filed quarterly, discloses the filer's holdings, providing transparency into their investment activities and allowing the public and other market participants to monitor them. The problem though, is that these holdings are often cumbersome to access, and valuable analysis is often hidden behind a paywall. Through wallstreetlocal, the SEC's 13F filers become more accessible and open. By exploring the website (and the code), you can see the resources I used, check out some notable money managers I listed, and download any data that suits you. All for free. (Note, the mobile site likely needs work.) I made this project to better democratize SEC filings, and also to get some experience on my hands. I love computers, and one day hope to get involved with startups. In the comments, I'd appreciate any and all advice, as well as feedback on how to improve the site. cobertos 15 hours agoDang, I saw the name and hoped it was a map-based app that showed you ownership of things around you (further intriguing me because I don't believe such data exists at a local scale anyway). Great project though! Opening up these sort of semi-encumbered datasets is what keeps humans well informed I'm from MI as well and always wondered about deeper datasets for watching money and influence change hands reply arrowleaf 13 hours agoparentThere are a lot of GIS / mapping software that shows property ownership and their boundaries, useful for hunting. onX, for example, and I think you can purchase 'ownership layers' on stuff like Avenza. I agree with what you're saying though, it would be cool to take something like those property owner layers and find the ultimate legal entity for stuff like LLCs owning land. E, only semi-related: In the 'urban design' part of the internet I've seen really cool mapping that puts bar charts on top of a city's grid, with the bar charts being how much tax revenue the city generates. It's really stark to see skyscraper-sized bars in downtown cores and mostly flat all around where cities have zoned residential separate from commercial, or even where suburbs tax less than the core city. reply throwup238 11 hours agorootparent> There are a lot of GIS / mapping software that shows property ownership and their boundaries, useful for hunting. onX, for example, and I think you can purchase 'ownership layers' on stuff like Avenza. Gaia GPS [1] is the one I use. It's got a lot of layers for free including land ownership which properly shows all of my neighbors plots. I use it often when collecting rock specimens and mushrooms to make sure I'm on public land that allows it or to figure out who to seek permission from. [1] https://www.gaiagps.com/ reply doctoboggan 7 hours agorootparentThanks for the pointer, I just downloaded the app. What is the name of the layer showing property boundaries for free? I can see the “Private Land (US)” but it’s prompting me to sign up for premium when I click on it. reply throwup238 7 hours agorootparentOooff that's my mistake. I might be grandfathered in on the premium layers and didn't realize. reply doctoboggan 6 hours agorootparentAh ok yeah I thought that was too good to be true. Currently using onX for this but am paying $30/year for the private land layer reply cman1444 3 hours agoparentprevThe app landglide does what you're saying pretty well. reply multicast 11 hours agoprevVery interesting project, I like the overview. I also really like that you took the finance industry as a theme for your project. >> Every company in the United States Sorry for being so fussy but I highly recommended changing the word 'company' / not using it in the future, as the title is quite misleading. No private company in the US has to register with the SEC or has to file with the SEC. 'Investment advisors', who also go by other aliases like 'asset manager', have to file a 13F filing only if they a) are registered with the SEC due to fund marketing purposes and b) if they have, as you already mentioned, over $100 million dollars under management (not 'in holdings'). This is also why large family offices (.e.g. Bayshore Global Management of Sergey Brin) won't show up in any SEC records as they meet the second but not the first criteria - same goes pretty much for any non asset-management company (e.g. McDonald's) as they do not raise money for fund vehicles. However, you have take this into account on your website, and further below you wrote \"money manager\" which is correct in finance jargon. I hope this gives you a better understanding, keep up the great work. reply anonyonoor 11 hours agoparentI geniunely did not know that there was a real difference between companies and asset managers. Sorry for the confusion. Thanks for informing me, and for the kind words, I'll make sure to avoid using the word company from now on. reply financypants 4 hours agorootparentYou spent a year on this project and didn’t know that? Sorry, but are you yourself interested in the content of what you’ve built? I don’t mean to be rude. This is valuable information and I’m surprised a tool beyond the SEC website does not provide this. reply anonyonoor 3 hours agorootparentGuess I just missed it. The SEC's search database provides info for companies and money mangers alike, so I wrongly assumed they were the same thing. Sorry. reply pmalynin 4 hours agoparentprevI think that’s not entirely correct either, for example Nvidia had to file a 13F for its holdings after their value of ARM shares exceeded 100M. And the requirement I believe is not for investment advisors (which is its own thing, see Register Investment Advisors from the SEC), but broadly extends to institutional investment managers which is a lot looser of a definition that can certainly include private companies like broker-dealers, in addition to hedge funds, RIC, etc as well as public corporation like Nvidia depending on the types of activities they are involved in. reply ashish10 13 hours agoprevImpressive work. Just a small comment - It seems to not able to track the prices after bonus or a stock split. If you can adjust the paid price for stock to account for that. Like for google i see the price: $1413.61 reply rvnx 12 hours agoparentsame with AMZN (Berkshire Hataway holdings for example), 1kUSD -> 100 USD because of stock split (eventually shows -90% performance) reply seanhunter 14 hours agoprevNice job! Working on things is really the only way to get better and this is a great project to sink your teeth into. My advice is: Keep working, keep learning. If you love computers and want to work for a startup you absolutely have what it takes to make that happen. And if there are no startups near you which are right for you, you can found your own. reply bklyn11201 15 hours agoprevLots of competition here as services like WhaleWisdom are quite powerful at the basics around 13F. Some ideas: * Cluster the 13Fs into buckets. Performance buckets and volatility buckets and aggressiveness buckets. * Build model portfolios that blend holdings from the best performers. * Do some basic regime analysis and find which 13F filers perform best during the various regimes. reply greatNespresso 14 hours agoprevI have learned something today, thank you for that! The pitch is clear and the fact that you put so much work in open sourcing it is really impressive. reply carlossouza 9 hours agoprevCongrats on the project! Here’s an idea to monetize it: implement collaborative filtering (example: funds in rows, assets in columns). Once you do it, you will be able to cluster similar funds. Let’s say X funds have the asset A, but Y funds do not have it, and X and Y belong to the same cluster. Thus, if you recommend asset A to the Y funds, there’s a high probability they’d add it to their portfolio (if property pitched). This is roughly how Netflix recommends movies, Spotify recommends songs, etc. A lot of players in the industry would pay high $$$ for a recommender system like that. And you already made 80%: it’s only missing the final machine learning part (which is the fun part :)) reply anonyonoor 8 hours agoparentThis is a great idea, especially since I am already learning machine learning with Python right now. I will try this, thanks. reply sroussey 12 hours agoprevHi! This looks great. I am playing with SEC data at Embarc.com and found that ingesting the filings is no fun. Well, if it is xml, it is fine. But 8k filings are all HTML. Thankfully they have standard section names. Anyhow, are you downloading the sec filings locally and processing them? It can be a lot of files! The EDGAR database has a lot of files in there. I download stuff daily, add to sqlite, and then process into various other things. I had to do some app side compression as the sqlite file gets big! reply anonyonoor 11 hours agoparentFor the search database, I did have to manually download each and every company, all 856,000 of them. To cut down on size though, I included all data points except for filings. This is because they're large, but also because they're updated often. By excluding them the database stays more up-to-date. Other than the search database which needs to be available on demand, the rest of the filers are queried and analyzed on demand. This is required because some filers get really, really big. Blackrock Inc alone is about a 30MB file. reply sroussey 10 hours agorootparentI get the list of filings, and for now only download the ones I want, which are related to S-1 or 8k (there are lot that are related, including withdrawals of a filing, updates to a filing, etc., and some companies use an S-4 etc). Right now I scan the list of daily filings, and for every cik I mark them as dirty. Another pass looks at ciks marked dirty, and downloads the xml or json of the filings. I then scan for anything new, create filing rows and mark them as dirty so to speak. Annual reports can be big, and for my purposes I don't need them so I skip them. The HTML of these things is garbage! But a lot of financial data is in the company facts which is nice and clean. reply thecosas 12 hours agoprevI think your site may be getting hugged to death right now. I have a feeling you'll learn a lot more about scalability after being on the front page of HN :-) reply anonyonoor 11 hours agoparentWow, you really nailed it on the head. I have been tearing my hair out for the last hour trying to get my Always Free Oracle instance running again. reply jzebedee 14 hours agoprevThis is a fantastic project. Thank you for releasing it! Do you have a plan to expand with new features and are you looking for contribution? Will the database be updated automatically? reply anonyonoor 14 hours agoparentI plan to add a lot of features, and all suggestions are appreciated. As for contribution, I am definitely looking for it. I have not been in the open-source field for long so I don't exactly know how to get it, but I would highly appreciate it if anyone could help. Contribution would be especially helpful since I was still learning a lot of the technologies I used while I built this project, and the code is prone to newbie mistakes. reply toomuchtodo 14 hours agoprevPlease consider contributing data from this to https://www.data-liberation-project.org/ https://www.data-liberation-project.org/datasets/ Awesome work!! reply anonyonoor 11 hours agoparentAwesome project, I have submitted a request. reply bdjsiqoocwk 15 hours agoprevFeedback is: by making this an npm thing, you reduce the pool of people that will use this. This should really be a lib that takes a folder of 13F forms and outputs a csv, or something like this. That's it. No need for webapps or whatever. reply anonyonoor 14 hours agoparentThere's actually a couple of libraries that already do this, and this project can probably function like a libary if edited here and there. The problem I found though is that you can't really just use the raw filings. The data is much more useful when the stocks are queried with third party APIs and organized along with things like recent price data. This project alone uses three APIs, and while you could include that in a library and force the developer to get three different API keys, it just works better as a service. If a \"library\" is all you want though, the API is available with documentation. There's also 13.info, an open-source project that predates this one. Although it is still a service, it is more like a library and could probably be used like one. https://13f.info/ reply ramathornn 14 hours agoprevThis is a cool concept, great job on the project so far! reply bigfatfrock 4 hours agoprevGreat work, keep going! reply ProjectArcturis 15 hours agoprevWhalewisdom and gurufocus have similar offerings. Not sure if they are 100% free. reply anonyonoor 11 hours agoparentBoth offer premium memberships, and can often be really restricting when viewing/downloading data. The main feature I added because of WhaleWisdom was the data download. For any filer you can download all data in CSV or JSON, as on WhaleWisdom that's unavailable. I also plan to add a bunch of features those sites have eventually. reply hbcondo714 14 hours agoparentprevYeah, I'll throw my SEC Filings site https://Last10K.com into the mix too that has a free feature on how a manager's portfolio changed by tagging which stocks are new / sold / increased / decreased from the previous to current quarter. Here's an example from the OP's \"Popular\" and \"Top\" filers: Tiger Global: https://last10k.com/sec-filings/1167483 Ruane, Cunniff Goldfarb: https://last10k.com/sec-filings/1720792 reply simpletone 7 hours agoprev> The Securities and Exchange Commission (SEC) keeps record of every company in the United States. No. SEC keeps tabs on publicly traded companies, large institutional investors and the like. Hence the 'securities' and 'exchange'. Most companies in the US are private and not publicly traded. And most companies are not asset managers. > and valuable analysis is often hidden behind a paywall. Sure, but that's because analysis is valuable. > I made this project to better democratize SEC filings The SEC already makes the information publicly available via their edgar system. > In the comments, I'd appreciate any and all advice, as well as feedback on how to improve the site. I'm not sure who your target audience is. Finance companies already have internal teams pulling this kind data for them or they buy it from finance data providers. As for the general public, what need do they have for top 'filers'? As for me, I'd rather dump the data into a database or excel and query it rather than looking at a static page. At the very least, don't make the site static. Try to add a spreadsheet. Or at the very least a sorting option. Also, vanguard, blackrock, etc aren't top filers. They are the largest or top asset managers. Maybe tie news stories to each asset manager? And more data? But as I said, not sure what the value here is for the end user. Why would I or anyone else ever use your site? Or better yet, what do you use the site for? reply neom 12 hours agoprevSo you're 17. It's a pretty unique niche for a young person, so I'd be interested to know where your curiosity in this comes from, and even more so, why you decided to share the output of your curiosity? What got you interested? Where do you see it going? Are other people around you at your stage in life also interested in this stuff? imo you're on a great path, stick with it! reply anonyonoor 11 hours agoparentMy cousin told me about 13F filers about a year and a half ago, and I was shocked to find out that all SEC data is public. After I saw that a lot of sites restrict data for SEC filings, I set out to find the SEC filings directly and make them more accessible. This project was a way to use my web development experience for a good cause, and to learn a lot along the way. I hope to improve the project thouroughly though user suggestion, then maybe hand it off to other people. Like I said in my post, my main goal is to one day get into start-ups, so I can create good in the world through them. This is hopefully the first step in that journey. Thanks for your kind words. reply Tijdreiziger 12 hours agoprev504: GATEWAY_TIMEOUT Code: FUNCTION_INVOCATION_TIMEOUT If the data changes irregularly, you’re probably better off making it a static site and having a script update it periodically, also to avoid excessive cloud charges (since you seem to be hosting this on Vercel). reply anonyonoor 11 hours agoparentVercel hosts the front-end, but the back-end is whats down right now. I thought I could rely on my Always Free Oracle cloud instance but I guess not. The problem with making a static site is that there are over 800,000 SEC filers, so it would be impossible to query all of them and store it. I hadn't expected so much traffic, so I really have no clue how to handle this without excessive cloud charges. The best I've done so far is to look into free hosting for open-source projeccts and add a donation link to the homepage. reply password4321 8 hours agorootparentYou can query a SQLite DB entirely client side over HTTP: https://github.com/proofrock/ws4sqlite The pre-processed data would have to be served from somewhere though. I'm not sure if GitHub could be used to host. If not, Scaleway Stardust includes a bit of disk, 75GB free S3-compatible storage, and most importantly: free 100mbps outbound data transfer for < $4/month. There are probably other cheap shared web hosts that claim unlimited data transfer but not sure they'll deliver. reply posix_monad 15 hours agoprev [6 more] [flagged] toomuchtodo 14 hours agoparentTo find value ahead of other market participants. https://www.cnbc.com/2024/02/15/nvidia-holdings-disclosure-p... reply Galanwe 13 hours agorootparentAs if that would be real... reply toomuchtodo 12 hours agorootparentGains are gains, regardless. You don’t have to believe for the number go up. reply Galanwe 12 minutes agorootparentThere's no gain to be made like that. The fact that you mention Nvidia is a selection bias: you won't mention the 1000s of other companies for which that didn't work, or even you would have lose money. I could tell you that stocks starting with a vowel have positive returns in Christmas of odd years. Even though \"gains are gains\", it's just a selection bias as well and isn't tradable. As a non professional investor, you are better off considering that any information you have access to is already priced in the market. If the price is lower than what you think, it's probably not a \"good deal\", but rather that you didn't understand the risk that other most sophisticated players priced. reply kristjansson 11 hours agoparentprev [–] Other retail traders buy on 13F, so number go up? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Anonyo, a seventeen-year-old from Southeast Michigan, has open-sourced his project, wallstreetlocal, focusing on making SEC filings, particularly the 13F form, more transparent and accessible for free.",
      "The project's aim is to democratize access to SEC filings and enhance transparency, showcasing Anonyo's passion for computer programming and ambition to be part of startups in the future.",
      "Anonyo encourages feedback and suggestions for site enhancement, highlighting his openness to constructive input."
    ],
    "commentSummary": [
      "Anonyo, a seventeen-year-old from Southeast Michigan, developed wallstreetlocal to offer free access to SEC 13F filings for companies with holdings exceeding $100 million.",
      "The project aims to democratize SEC filings, receiving positive feedback and suggestions for enhancement, such as GIS/mapping software for property ownership and addressing differences between companies and asset managers in SEC filings.",
      "The expansion of the project with new features is ongoing, encouraging user contributions to enhance accessibility and usefulness of SEC filings, involving feedback on additional data, features, hosting challenges, and investment strategies."
    ],
    "points": 243,
    "commentCount": 46,
    "retryCount": 0,
    "time": 1709920946
  },
  {
    "id": 39640647,
    "title": "Self-Control Key to Perceived Power and Success",
    "originLink": "https://today.ucsd.edu/story/having-self-control-leads-to-power",
    "originBody": "Having Self-Control Leads to Power New research from UC San Diego’s Rady School of Management reveals that people see those who have more self-control as powerful and want to give them power By: Jenn Riggle - jriggle@ucsd.edu Published Date February 27, 2024 By: Jenn Riggle - jriggle@ucsd.edu Share This: Article Content Out-of-control behavior by CEOs and other powerful people constantly makes headlines – so much so that some might consider impulsivity a pathway to power. New research from the UC San Diego Rady School of Management and Texas A&M University finds that having self-control is often what leads to power. In a paper published in the Journal of Personality and Social Psychology, researchers find that showing self-control influences how powerful an individual is perceived to be by their peers, as well as how much power they are granted by those peers. In a series of seven experiments with roughly 3,500 participants, both students and working adults read about or interacted with individuals with varying levels of self-control, which the researchers define as how much people tend to behave in ways aligned with their goals. Across all experiments, individuals with high self-control were seen as more powerful, and as better suited for powerful roles, than individuals with low self-control. In one experiment, working adults imagined a scenario where a colleague with the goal of being fit either ate a large dessert or abstained from dessert altogether. Researchers found that the colleague was seen as being better suited for high-power roles when they abstained from indulging, an indication of self-control. “It did not matter whether the colleague seemed to deliberate before acting, or just acted without thinking,“ said Pamela Smith, associate professor of management at the Rady School of Management and co-author of the study. “What mattered for participants’ judgments was whether the colleague acted in line with their goals. This pattern held across a variety of goals in our experiments, including saving money, being healthy and reading books.” The researchers also found that people are perceived as less powerful and less suited for powerful roles when they fail to meet ambitious goals, even if their performance is the same as their peers. In an experiment investigating how self-control often leads to power, a group of undergraduate students interacted with individuals who set various reading goals. Some set an ambitious goal of reading 200 pages each week, while others set a more moderate goal of reading 50 pages per week. All of these individuals read the same amount – 100 pages – but those who didn’t meet their goal were seen as less powerful by study participants. Furthermore, study participants were less interested in having those who didn’t meet their goal as the group leader in later tasks. “To motivate their employees, organizations often want employees to set stretch goals – goals that are challenging and hard-to-reach. However, we found that setting a stretch goal and not meeting it makes someone look less powerful than setting an easy goal and surpassing it,” said Rady School PhD student Shuang Wu, the first author of the paper. The paper, “Self-Control Signals and Affords Power,” was also co-authored with Texas A&M University associate professor Rachel Smallman. Share This: You May Also Like Healable Cathode Could Unlock Potential of Solid-state Lithium-sulfur Batteries Technology & Engineering Product Life Cycle Research Helps Define Economic Vitality, Influencing Researchers and Policymakers Business & Economics Helen V. Griffith Celebrated at UC Black Administrators’ Council Conference Awards & Accolades When Planning Sustainable Energy Systems—Don’t Forget About People Science & Environment Product Life Cycle Research Helps Define Economic Vitality, Influencing Researchers and Policymakers Business & Economics Helen V. Griffith Celebrated at UC Black Administrators’ Council Conference Awards & Accolades When Planning Sustainable Energy Systems—Don’t Forget About People Science & Environment Healable Cathode Could Unlock Potential of Solid-state Lithium-sulfur Batteries Technology & Engineering Stay in the Know Keep up with all the latest from UC San Diego. Subscribe to the newsletter today. Email Please provide a valid email address. Subscribe Subscription Notification Close Confirmation Thank you! You have been successfully subscribed to the UC San Diego Today Newsletter.",
    "commentLink": "https://news.ycombinator.com/item?id=39640647",
    "commentBody": "Self-control signals and affords power (ucsd.edu)192 points by namanyayg 21 hours agohidepastfavorite121 comments ranprieur 15 hours agoThis article mentions a study that found almost no correlation between people who self-report having high self-control, and people who actually do well on tests measuring self-control: https://www.vox.com/science-and-health/2018/1/15/16863374/wi... The most likely explanation is that people who report high self-control are really experiencing less temptation. reply nostrademons 12 hours agoparentIs this a distinction without a difference? In the original qualitative write-ups for the Marshmallow Test, they described the children using all sorts of distraction strategies to basically make themselves forget that there's a tasty marshmallow sitting right in front of them. Maybe this is all that self-control is - having enough self-awareness (and valuing your future self highly enough) to direct your attention elsewhere so that temptations disappear from your view. It fits with the neuroscience we know about consciousness as well (that it's effectively a brain network which taps into the other brain networks and can observe and direct their firing) and even into how attention mechanisms in GPTs work. It also is how most mature adults approach the world. If you're an alcoholic, don't go to the bar. If you're married, don't go to the strip club. If you want to lose weight, put less food on your plate. Most of what we know of as self-control is really having the skills to avoid temptation. reply Staple_Diet 1 hour agorootparentYou should know that the 'Marshmallow Test' is a poor study. The general conclusions after follow up were that children had an intrinsic self-control mechanism that influenced success rates in later life. However, the original experiment failed to control for several factors, least of which was socio-economic status. Subsequent controlled replications show that these factors have a significant influence on the outcome, suggesting that resisting temptation is less about self-control/will power and more about food security. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6050075/ reply noduerme 8 hours agorootparentprevThere's certainly an experiential difference between consciously stopping yourself from a behavior and not needing to think about avoiding it. If you can walk by the bar/stripclub/second helping of french fries without thinking, \"gee, I'd like that, but I'll abstain\", then you aren't experiencing what most people would describe as \"temptation\". You may be right that if abstaining becomes habitual amd automatic enough, that could be indiscernable from a state in which you have no temptation. But I'd argue it's only \"self-control\" for the purpose of measuring willpower as long as it requires some degree of conscious decision making. reply twoWhlsGud 8 hours agorootparentprevAnd I think that extends out further to a lot of little micro strategies. Like I know for myself that I have a lot more control in the supermarket to avoid something like corn chips than I have at home once said corn chips are in the pantry. So I have a strategy of limiting what I buy at the market rather than trying to struggle with eating carefully at home. There are a lot of little hacks like that, that can make life better. reply tpmoney 7 hours agorootparentprev>If you want to lose weight, put less food on your plate. Most of what we know of as self-control is really having the skills to avoid temptation. Funny you mention this, because I have the personal experience to answer your question, yes there is a distinction. I've told the story here before, but I'll tell it again. As a teen / young adult, could eat as much food as a wanted and burn it all away. A daily food intake might be a sandwich and chips for lunch, 2-3 full bowls of cereal and milk for an afternoon \"snack\". Dinner, with seconds. And then depending on the evening, cookies, chips, cracker and cheese or icecream for an evening snack. I left high school on the \"needs a few more pounds\" side of the weight scale. And that ability to eat that much food (and the appetite to back it up) never left. But the burning of the calories did. As I got older, weight kept climbing. Oh sure, I did the \"put less food on the plate\" thing. I did calorie counting. Even managed to lose some weight doing it. A whole 30 pounds at my best over an agonizing year. Every day was a constant battle with myself to look at the goals of where I wanted to be and fight every fiber of my being to not eat more food. I was hungry all the time. And it never stuck. No matter what I did I couldn't keep up with it. And I assumed it was a lack of self control. A lack of will power. I assumed I just needed to try harder. And try harder I did. Time and time and time again. Nothing. Then recently I started a medication, with a side effect of reducing your appetite. Suddenly, I'm losing weight with ease. And I learned something interesting. The first week, I was convinced I was having anxiety attacks. Like clockwork every day, around 6 hours after taking the meds, my stomach would start knotting up. I'd start feeling this weird feeling like something was wrong and I needed to do something about it. And then it hit me, ~6 hours after taking the medications was also lunch time, or dinner time. I was getting hungry. And what was novel was realizing I'd never NOT been hungry my entire life since I was a teenager. I thought I was having anxiety attacks because I had never once felt the feeling of not being hungry turning into hunger. I stopped eating when I physically couldn't eat any more food, or the food ran out, never because I was satiated by the food. \"Getting hungry\" again was just the feeling of being stuffed going away. But I was always hungry. Since starting the medication, I've lost close to 50 pounds. And it's SOOO much easier than it's ever been in the past. I can eat a normal meal, and just be satisfied with that. I can think to myself \"I'm hungry, I should get a snack\", and then follow that thought up with \"I'm about to have lunch/dinner in an hour, I can wait\" and actually follow through on that. I can stop thinking about that snack. I can count my calories for the day, see that I'm at my limit and just make the decision to stop eating any more food. Heck there are days where I have to make the conscious decision to make and eat more food because I'm too far under my limit for the day, but I've just not been hungry that day. So yes, I'm putting less food on my plate, just like I could and did before the medication. But let me assure you that doing the things to \"avoid temptation\" is a hell of a lot easier when your body and mind aren't actively working against you. reply anon291 12 hours agoparentprevHow in the world does a test of 'does this colored text match its name' test 'self-control'. 'Self-control' as used in the article means things like 'can you stick to a diet', or 'can you keep your word', not can you control your thought processes enough to quickly name a color. You can be highly self disciplined and bad at this task, or highly undisciplined and good at it. In fact, I'd imagine undisciplined people might be inspired by the entertainment value of this 'game', and spend useless time practicing it rather than doing what actually matters. This study is complete garbage. reply CharlesW 15 hours agoparentprevIt's the same \"illusory superiority\" cognitive bias as the Dunning-Kruger effect. The least competent will overestimate their self-control, and the most competent will underestimate it. reply fsckboy 15 hours agorootparentwhile it could be, that doesn't ring true to me. I'd go for more of a Freudian \"people who are somewhat obsessed with self control (therefore mindful of it) are people who struggle with it,\" coupled with salience bias. People who don't struggle with self control never think about it. reply advael 12 hours agoprevThis has a kind of silly sleight of hand I've unfortunately come to expect in psych research. This seems to try to study power while controlling for... power? It can definitely be interesting to see how the personality one projects can influence the social dynamics of a peer group (with the usual caveats that studying people in laboratory conditions for something iterated and evolving like social relationships is famously fraught in the first place), but this notion of power as something that's given to people by social perception on a moment-to-moment basis seems ill-suited to describe the real world, where in many contexts the relevant power is official and considerably less up for social negotiation reply JellyBeanThief 9 hours agoparentThe word \"official\" is doing an awful lot of heavy lifting there. reply advael 9 hours agorootparentIt's here used as shorthand for \"not merely a social illusion constructed by the person it is acting upon\" Some power derives from charisma, force of personality, or social fluency. But lots of power, from the perspective of the people it is acting on, derives from or relies on nothing of the sort, and merely requires that consequences can be enacted for failure to cooperate with the power dynamic at play This narrative is not insignificant. A lot of powerful people love to tell a story where power is just a personal competency, and consequently, that anyone's lack of power in any circumstance is, as the gamers say, a skill issue reply JellyBeanThief 8 hours agorootparentA single person, by definition, can't construct a social illusion. \"Social\" requires a society, which means at the very least two people. \"A social illusion constructed by the person it is acting upon\" is nonsense. We could call a language a social illusion. There's nothing red about the word red. And the distinction we draw between \"earth\" and \"sky\" is not one that the universe appears to use in its operation. Language does, however, require us to negotiate symbols, their meanings, and the consequences stemming from them. Like in how I rejected your definition of \"official\" with a rationale which you may attempt to overcome, accommodate... or just walk away from. And like in how we have structured our interactions so far as a contest of truths with bold and uncompromising assertions, when the option remains available to switch to a search for truth with tentative and attackable statements. It's exactly with the same with governments and other organizations, all based on language as they are. We make them up. We make up rules for making them up. Occasionally someone hits someone else with a big stick. But although the stick is real, the reasons why we do things, and the reasons why we believe we can do things with these consequences or those, are things we all work together or in opposition to establish. \"Official\" included. reply advael 7 hours agorootparentDang we sure are getting hung up on semantics rather than substance today, huh? reply janalsncm 6 hours agorootparentprevSeems pretty clear to me. Official power means the person has been designated as an authority. reply abeppu 17 hours agoprevAll of this is in a context where it's assumed that participants _know_ the goals of the person under consideration, can straight-forwardly evaluate whether actions are in line with those, and where the _participant's_ goals are framed to not be a part of the discussion. In real life, how often are these true for power that matters? - as an IC, I generally don't have the information an exec has, and cannot easily judge which choices are aligned with stated goals - as a voter, I'm regularly unsure which goals that a politician says publicly are their real goals - and for any situation where I'm close enough to judge the actual suitability of actions, and the intents of the participants, I probably have enough of a stake that my view of who should be in control is swayed by what outcome I want reply c4wrd 18 hours agoprevThis is something I've noticed in myself and I'm glad there's research to back this, although this is an open secret to those who do master self-control. I've spent the last three to four years working on self-control and discipline after I hit rock bottom in my life and realized I had no self-agency. For me, having greater self-control led me to ensure I can focus on providing value where it matters in my life, and not getting caught up by the shiny object syndrome I was distracted by a lot when I was younger. Not every thought needs to be acted on, especially if the thoughts come from external sources. In regards to why it leads to power, as you make your way up the managerial chain, when you have greater self-control you are less prone to get \"bullied\" by other managers into doing work for them and you can stand up more for your team and you will be able to provide more value. For perspective, I would personally trust others who have self-control more than those who don't for time-sensitive and critical tasks because I can rely on them to regulate their emotions and give honest answers, as well as hold themselves accountable. For someone, like me a few years ago, who is undisciplined and has not spent time cultivating self-control this is hard to hear. If you find yourself making excuses when you read this article for why the power hierarchy is against you, or that there is bias in the results of this study (as some of the comments here allude to), then you should consider reevaluating why you are making excuses. It's a sign that this post triggered you and your response was to make an excuse rather than accept a correlation that speaks to an underlying hard truth. Once you start digging into \"why did I make an excuse\" and chase that feeling over and over whenever you find yourself making excuses, you will start to realize that you can't think of a reason why you made an excuse, it's just what you've done and reinforced in the past. If you've read this comment this far and you have a spark of curiosity and relate to not knowing why you are making excuses, I suggest you take this moment to chase it down and gain agency over your own life. Some would say this is your red pill moment. 'The Daily Stoic' woke me up, I highly recommend it. Discipline equals freedom, my friend, and we sorely need you. reply smallmancontrov 17 hours agoparent> less prone to get \"bullied\" by other managers into doing work for them and you can stand up more for your team This is the real key. Management is about controlling others and not letting others control you without compensation. > I would personally trust others who have self-control more than those who don't for time-sensitive and critical tasks because I can rely on them to regulate their emotions and give honest answers, as well as hold themselves accountable. Of course you would, and if they wanted to be in your shoes they would do well to learn mastery over others as well as you have. Learning the language of self-control may be a path to that, especially if you have not heard it before. However, it can also be a path to being controlled, as in your example. I grew up in a conservative environment, so that was my problem: I was heavily indoctrinated in the language of self-control, responsibility, and accountability, and these made me easy to exploit. My own \"red pill\" moment involved understanding these as tools of power rather than facts of the world, thereby freeing myself to better represent my own interests. reply nebula8804 16 hours agorootparent>I was heavily indoctrinated in the language of self-control, responsibility, and accountability, and these made me easy to exploit. My own \"red pill\" moment involved understanding these as tools of power rather than facts of the world, thereby freeing myself to better represent my own interests. Can you give some clearer examples? I am curious how this is done in those circles. reply c4wrd 13 hours agorootparentHaving self-control can give a false sense of self-righteousness, and if you're not careful, this will lead to you eventually caring about how you look and you will do whatever it takes to maintain the image of self-control/responsibility. If you carefully analyze this, you are now basing your self-worth on external appearances, therefore you are giving your control away to anyone who can see this projection. Anyone experienced with controlling others can sniff out this projection and then use it against you in this way: \"We need to think about the importance of handling this (X) responsibly. What do you think we should do?\". Now they are telling you they think you can make responsible decisions on an important decision, stroking your projected ego, and they've activated your want to act responsibly to do something for them. Now, it doesn't always mean that this is a way someone is manipulating you. Self-control is realizing that this ego-stroke feeling of making you feel important does not mean to need to involve yourself or act. You need to separate the emotion from the decision-making process and realize that others can use your emotions to manipulate you. It's up to you to decide whether you can trust this person or not, you just need to be aware that the emotion could have been intentional or unintentional. reply detourdog 15 hours agorootparentprevI grew up in central Pennsylvania farm country. This is a conservative area that has voted republican in every presidential election since Lincoln. The area is also rich in both Amish and Mennonite families. When a whole area is so packed with conservative ideas and individual it has a strong effect on what is \"common sense\". Common sense is not generally universal it is culturally based. The best example I can think of is that 85% of the kids in my High School had the same haircut and rarely left the area. Suddenly mTV arrived on cable and the kids and hairstyles went wild. I know of parents calling the cable company to get mTV removed from the home(unsuccessfully). It is simply a cultural standard. reply faeriechangling 2 hours agoparentprevI have lots of strong and compelling evidence that my self-control is unusually low. Doctors have said that since I was very young, so if anybody was making an excuse, it was the adults around me more than myself and I simply internalised those excuses later. It's extremely humiliating, and I've gone a lot further than listening to the daily stoic, there's an entire body of scientific literature on how to improve self-control in general and for people like me who seem to have literal neurological abnormalities. I've spent the last few weeks miserable from medication side-effects for instance. reply andai 15 hours agoparentprev> Not every thought needs to be acted on, especially if the thoughts come from external sources. I've heard it said (I believe by Hormozi on Williamson's podcast) that at a certain level, success becomes mostly about saying no to increasingly great opportunities. I can confirm this is relevant near the bottom too, at least if you have a high level of openness (personality dimension) and are presented with inspiration on a regular basis. The 1% inspiration, 99% perspiration quote comes to mind. The principle of sacrifice comes to mind. It seems to be a choice between sacrificing many small things, or a few great things. reply c4wrd 15 hours agorootparentHere's an analogy that I think about from time to time. Imagine your life as a garden. You have a finite amount of days, with a finite amount of resources per day to plant new things or caretake existing things. Your soul represents the breadth and depth of your reach on any given day, but it is a fixed size and you must choose how much breadth and depth it has through your choices. Consider each day as an opportunity for change in this ratio of breadth vs depth, and over time this will play out as a spectrum between the two following situations: (1) You can spend each day traveling to a new area, planting a new seed wherever you go, but never watering the same area twice. You get to learn about many different seeds, but you never get to stick around somewhere long enough to watch them grow into fruit-bearing plants. Your soul will be full of different experiences, but you will not be able to relish in the details of any particular area (i.e. pluck the fruit from your garden when you are ready to relax and are thinking back on your life) (2) You can choose to focus on one or a few areas to add depth, learn the fundamentals of how things grow in those areas, and learn to care for and nourish them over time. In the end, you will be left with a beautiful garden that you have perfected and know every detail about that is full of fruit-bearing plants. You can wander this garden and eat the fruit from any of your plants. Your soul has a finite reach. By focusing on one thing, you are neglecting to focus on another, and there is nothing you can do to change that. It's up to you to choose how you want to live, and not making a choice is also a choice. If you don't make a choice (i.e. a sacrifice of not visiting some areas or not nourishing the area around you), you will be left with the worst of both worlds: a decaying garden and no knowledge of how to grow anything. reply andai 2 hours agorootparentThat's beautiful, thank you. reply coop_solution 17 hours agoparentprevIs there any way to disagree with this comment without being disregarded as resentful? Good work is the key to good fortune / Winners take that praise / Losers seldom take that blame reply Rediscover 13 hours agorootparentGood quote from Neil Peart. Roll the bones reply 15457345234 17 hours agorootparentprevDisagree? It's ad copy promoting a self-help book. reply c4wrd 17 hours agorootparentI have no affiliation with the author of the book and stand to gain nothing from helping others. I still stand by my choice of The Daily Stoic. I resented the thought of reading self help books because my pride led me to believe that if I read a self help book, I was admitting I was weak. That said, the reason for the book is simple: the book is intended to take a year to read, one page at a time. I wake up each day and the first thing I do is open the Books app on my iPhone, load up The Daily Stoic and read the days entry. It takes me 2-3 minutes and reminds me why I am chasing self-discipline. I have done this every day faithfully for three years. I hate to admit, but a page was as much as I was personally able to commit myself to, a full book was too much for my pride to handle at first. So if you’re like me and can’t make time for a full book, I ask you to make time for a single page per day. In fact I will double down on this book so much, that I will personally buy a copy for anyone who sends me an email cory@linux.com. No one will know you asked for a copy and I ask for nothing in return. reply zafka 12 hours agorootparentNice offer! I would also be very interested in how many people have taken you up on your generous offer. I myself would edit your offer to include the caveat: \"emails within a week\" to prevent all future orders for the book being funneled through you :). reply InSteady 17 hours agorootparentprevGood call. The author of this book is a marketer and \"media strategist,\" with an obvious focus on social media. He was great at plastering subtle ads all over relevant threads and subs over on reddit. Nice to know that he too has figure out reddit has gone to shit and is also looking for greener pastures.. reply richardgreeko27 17 hours agoparentprev> ...making excuses when you read this article for why the power hierarchy is against you, or that there is bias in the results > ...this post triggered you Definitely the language choices of an unbiased perspective from someone who doesn't have an axe to grind reply nebula8804 16 hours agorootparentAre these words really that verboten now? reply autoexec 16 hours agorootparentNo, but a person's choice of even our non-verboten words will still cause others to make certain assumptions about them. reply c4wrd 16 hours agorootparentprevYou're right. I never said it was an unbiased perspective, so let me make it clear. The post was intended to help those who were like me. When I wrote the post, I wrote it as if I was speaking to my past self in a style that would have motivated my younger self; nothing more, nothing less. reply mjburgess 16 hours agorootparentprevThese are the excuses culture makes available. I take the commenter just to be reporting them. reply atoav 17 hours agoparentprevI read somewhere: \"The first thought you have in reaction to something is a mirror of how you were brought up, the second thought is a mirror of who you are\". So the defining thing isn't reacting to a shiny thing, it is what you do after that initial thought and whether you can see yourself falling in the ever-same traps and do something against it. reply robocat 14 hours agoparentprevI suggest you read this: https://radicalcontributions.substack.com/p/escalation-theor... It is really insightful about how we train ourselves for compliance, but how that training struggles to cope when we interact with violence escalaters (who are common in some parts of society). I live near a port town and face and threat of violence is easily visible in men and many women. reply jnac 8 hours agoparentprevWhat steps did you take to \"cultivate self-control\"? reply astura 16 hours agoparentprev>when you have greater self-control you are less prone to get \"bullied\" by other managers into doing work for them and you can stand up more for your team Umm... You are confusing self control with confidence and self esteem. They aren't the same thing. Though I can see how becoming more disciplined can lead to more confidence and self esteem. reply hnthrowaway0328 15 hours agoparentprevI wonder what do you think about the usage of such training. I have had such trainings for diet control and distraction control but I felt unless one manages to gain a long term control the whole time is kinda worthless. For example I can lose 10 pounds in 2 months with diet control and a bit of discipline. But it comes back quickly once the self control goes away. What is the point? Practicing such self control does not give me better self control next time but only brings down self confidence a bit every time. I guess the ultimate reason is that I do not really enjoy the targets I set. How can I enjoy something I do not but inherently good for me? reply wholinator2 15 hours agorootparentPart if it is that enjoying it is not the goal, losing the weight or passing the test or whatever is the goal. You're not trying to enjoy yourself, so if the pursuit of the goal is very painful for you, you're not going to enjoy it. But you had a reason to start. Sometimes we encounter very difficult obstacles in our lives and relationships, we get so far out into trouble and unhappiness that it seems impossible to get back. General advice in this case is to try to connect back to why you started. What does passing the test do for you, why did you want to lose weight, why did you originally fall in love with that person. If you never had a grounding to start with then it's time to find one. reply hnthrowaway0328 14 hours agorootparentThanks. For weight lost I guess the initial grounding is to gain a healthier body. But then again, I probably have no idea what I am going to do with a healthier body. reply wholinator2 14 hours agorootparentYou do literally everything with your healthier body! Every moment of your life is lived inside a body which is more capable of physically influencing the world around you. A big physical change is like going from notepad to vim. It takes time and effort and attention but it's so damn _powerful_. Eating healthier and especially, _especially_ exercising is such a boon to generally feeling good throughout the day. I've never been more motivated, productive, and capable than when i was running every day. It was honestly astonishing. reply blueprint 16 hours agoparentprev> you will start to realize that you can't think of a reason why you made an excuse this may not always be true by the way. such constant observation of yourself, constantly asking yourself what such and such a thing means or came from - may eventually lead you to notice the cognitive jump you take - the experience you had all those years ago plus the new thought pattern you started to let yourself believe - which became compressed and hidden by the familiarity and comfort of having no problems due to the resulting dissociation. Before you let yourself believe there was no reason, consider deferring belief permanently until you remember what impression you had which caused 'what is'. Far along that path lies deep self-knowledge and therefore deep knowledge of the world. Most people aren't as interested in seeing the truth as they believe so they give up when they feel more comfortable. It's explained, then, that as a result of not being awakened to what is, their karma inside themselves can still conquer their destiny [1]. Whereas someone who actually respects the truth is going to think more seriously about controlling their karma, at the very least so as not to damage the truth more. 1. What is Destiny?- https://tathagata.netlify.app/page/destiny reply thecosas 12 hours agoprevEnjoyed this tidbit from the bottom of the article: “To motivate their employees, organizations often want employees to set stretch goals – goals that are challenging and hard-to-reach. However, we found that setting a stretch goal and not meeting it makes someone look less powerful than setting an easy goal and surpassing it,” said Rady School PhD student Shuang Wu, the first author of the paper. reply jyunwai 8 hours agoparentI hadn't considered the aspect of 'looking powerful/competent' before, but I have found setting limited goals useful for thinking strategically. The practical benefits of a limited versus highly ambitious short-term goal from the outset include an avoidance of 'feature creep' where you're tempted to add more nice-to-have features partway through, more self-confidence that you can achieve the goal with a little effort, and a clearly-identified decision point upon achieving the goal, where you can decide whether to continue on with the project for more ambitious goals, or satisfyingly call the project complete. In educational psychology, this is known as the \"zone of proximal development\" (https://en.wikipedia.org/wiki/Zone_of_proximal_development), which I've also heard as called \"i+1 planning.\" reply FredPret 11 hours agoparentprevLet me tell you - I did a five-star job of opening my laptop this morning. Just super - knocked it out of the park. reply m463 9 hours agorootparentdon't break the hinge achieving your stretch goal. :) reply danjoredd 18 hours agoprevSeek freedom and become captive of your desires, seek discipline and find your liberty. - Frank Herbert reply BenFranklin100 12 hours agoprevThe second half of the study (setting but not meeting ambitious goalposts) seems to conflate self-control with realism. Setting impossible goals is not a trait one wants in a leader. This result may have little to do with perceptions of self-control. reply silent_cal 16 hours agoprev“Thus, a good man, though a slave, is free; but a wicked man, though a king, is a slave. For he serves, not one man alone, but what is worse, as many masters as he has vices.” - St Augustine reply pilgrim0 16 hours agoprevIMO simply striving to gain power or money is easier than to achieve greatness and mastery, in general. Mainly because one can lie their way into power and money, countless examples of that both at a local level and global level. Now, mastery requires the opposite of lying and pursuing easy to achieve goals. It’s the hardest path because frustration is constant, and it requires one to keep stretching the limits of their own capabilities, and it never gets easier. Self control in that scenario is not necessarily about getting things “done”, but rather to not give up, and keep trying restlessly. The lack of nuance in the summary of that paper makes me think it’s not worth reading it. For me, I have greater admiration and respect for someone who has failed majestically at being great, than for someone who deals purely with median or below median expectations. The former will have amazing stories to tell, the latter usually don’t. The summary given by this blog, and likely the paper, too, is heavily tainted with the idea of “being for others”, rather than the idea of “being for self”. Again, being for others is super easy because it’s simply a matter of controlling the information you give, people do that all the time in social media. This doesn’t work for “being for self” because one can’t fool oneself indefinitely. Like the summary hints at, striving for greatness indeed involves fulfilling many achievements in the process of pursuing the greater achievement. This usually doesn’t count for observers, though, because the crowd has no taste or time for the story, but only for conclusions. Again illustrating how “being for self” is harder, as you’re set to be perceived as less than what you’re really trying to become. A functional remedy for that is simply to not be so public about your desires, which has the upside of protecting you from all sorts of exploitation. reply jack_pp 14 hours agoparent> IMO simply striving to gain power or money is easier than to achieve greatness and mastery While my ego would want to agree the truth is staying in power and making money exponentially isn't easy either, and you can't lie your way to lifelong power and fuck you money because eventually people figure you out, your reputation plummets and then you have less power and less opportunity to make money. That said you choose what game you want to play, if you care more about your craft focus on perfecting it even though that's not gonna make you as much money as pursuing money for its own sake. reply dasil003 12 hours agorootparentBoth great points. One way to think about it is that pursuit of wealth simply has more competition—the opportunities for the upper tiers are very few relative to the population that aspires to them. It is objectively hard to get there regardless of whether you lie or not. One thing is certainly true though, you can't get there without incredible social leverage of some sort, otherwise why would tens or hundreds of thousands multiples of per capita GDP be routed to you? The answer is: you kind of have to bring something uniquely valuable to the table. Exceptional skill with people and general intelligence are certainly valuable ingredients, but to really crack the upper tier some kind of domain mastery is what really pushes you over the top. Bonus points if it's a new domain, fast to monetize and scale, hence the rapid rise of tech in the Fortune 500 over the past couple decades through the web and smart phone revolutions. reply idiotsecant 13 hours agorootparentprevI think luck and lying is essentially the method to real wealth. Find me someone who got there without those two elements. Wealth is an expression of your ability to manipulate other people into devoting their labor to your enrichment. You don't get there without some deception. reply sunshinerag 13 hours agorootparenti.e like your worldview. good to know reply zelphirkalt 12 hours agorootparentprevWe just need to look at politicians and voters, to know, that it is not necessarily true, that ones power and opportunities vane, when people discover the truth about ones lies. reply GMoromisato 12 hours agorootparentprevAlso lying is not that easy. Humans have evolved various tools to spot liars. I would bet that the average person is better at detecting lies and liars than they are at lying. The exceptions people talk about, of people gaining power by lying, are all exceptionally good at lying. reply aiisjustanif 14 hours agorootparentprev> you can't lie your way to lifelong power and fuck you money because eventually people figure you out, your reputation plummets and then you have less power and less opportunity to make money. Surely we do have very notable exceptions to this. reply jack_pp 12 hours agorootparentI'm not saying top earners never lie, I'm saying overall they have to be somewhat trustworthy. There are exceptions to this rule of course but I do believe it's a rule. Take Elon, he does overpromise and you could say he lies about self-driving capabilities but overall he does deliver reply jyunwai 7 hours agoparentprev> \"For me, I have greater admiration and respect for someone who has failed majestically at being great, than for someone who deals purely with median or below median expectations. The former will have amazing stories to tell, the latter usually don’t.\" This helps give me words to understand my mixed feelings about an old post I found on a language learning forum recently, about a person who shared an ambitious goal that appeared unrealistic to achieve on their proposed timeline of six months. The person aimed to start studying as a complete beginner with a language and achieve a 'C1 level' certification by that time (this is an advanced exam that evaluates for fluency in a spoken debate on a scholarly topic, thorough comprehension of spoken debates on the radio, and the ability to write a timed persuasive essay with a limited number of flaws on a nuanced topic). The responses to the post at the time were largely discouraging. On the one hand, I quietly agreed from personal experience with the other users that the goal simply wasn't feasible for most people: it would have been ambitious (but doable) to employ the same stretch of time to go from the B2 level (the level just below, which is sufficient for immigration to major countries that used the language officially) to the C1 level. But on the other hand, I did feel sympathetic toward the idea of the person setting an ambitious personal goal. The person even tried to make this concrete by including a plan of intense hours of tutoring and television watching per week. The post was a year old when I read it, so I checked the person's user profile to see if they reported the outcome. But the person never gave a written update (neither a post, nor a comment, nor an edit), though the account was active with recent unrelated comments. My conclusion is that it's best to be supportive of people with ambitious goals to better themselves, but it's most practical to draw a distinction between goals in the short, medium, and long-term. Ambitious goals for self-betterment on any of these timelines ought to be celebrated. But there are risks with short-term goal that skirt too far outside of realism: this can cause someone to push themselves too much to the point of burnout and quitting (it's unclear, but possible, that this might have happened to that person). In my week-to-week life, I've personally seen beginner's burnout happen with overly-eager novices in martial arts. A set of attainable short-term goals in service of ambitious goals over a longer timeline would be more sustainable. But if one wishes to pursue an ambitious goal in the short-term, that ought to be okay too—as long as the individual understands the burnout risks upfront, to lower the odds of abandoning their goal if they fall short. reply verisimi 11 hours agoparentprevFantastic comment, thanks reply lchen_hn 14 hours agoparentprevWell said reply kingkawn 15 hours agoparentprevThe thing obstructing all progress is the wanton deployment of generalizations that hold no obligation other than to reflect the reality of their distributor reply seydor 17 hours agoprevThis doesn't pass my sniff test. The results are based on what people say about who \"looks powerful\" but didn't put people in a position where they actually give power to them . \"Look what people do rather than what they say\" is very relevant here. Reject and resubmit reply 082349872349872 14 hours agoparent> ...you gotta have the self-control first. Then when you get the self-control, you get the power. Then when you get the power, then you get the women. — not AM reply abalaji 15 hours agoprevI must not fear. Fear is the mind-killer. Fear is the little-death that brings total obliteration. I will face my fear. I will permit it to pass over me and through me. And when it has gone past I will turn the inner eye to see its path. Where the fear has gone there will be nothing. Only I will remain. reply barrystaes 12 hours agoprevNice article. It is good to bear in mind the perception of selfcontrol by observers does not actually correlate with the actual selfcontrol that a person exercises. reply disqard 18 hours agoprev\"Across all experiments, individuals with high self-control were seen as more powerful, and as better suited for powerful roles, than individuals with low self-control.\" Part of me sees this research as \"doing everything by the book\" and consequently failing to capture important insights. For instance, every economic model of humans assumes that we're \"rational agents\", yet we know that people with poor impulse control (like ElMo, Trump, etc.) are perceived as powerful by a wide swath of the public. Maybe their \"turns down dessert\" thought experiments are just that -- thought experiments -- and do not reflect what humans actually do, IRL. reply annoyingnoob 18 hours agoparentAmbition, persistence, and confidence play a role too. Some people do not take No for an answer and keep trying. reply glitchc 17 hours agorootparentNot to mention luck. reply ozim 15 hours agorootparentIf you try against all failures and are persistent and you finally get lucky. Versus giving up after first or second failure. Is it still luck that is most important? reply __turbobrew__ 13 hours agorootparentDepending on your background the luck variable will be different and you may have to try more or less times to get a successful outcome. reply glitchc 12 hours agorootparentprevLuck is a huge factor in your starting point. It's significantly harder to become a millionaire if your starting point is a group home in a poor country. An unlimited amount of persistence can't overcome those odds. reply ozim 27 minutes agorootparentThat is attacking a straw man. Discussion is about self control not things outside of ones control. I am also not talking about becoming a millionaire. I am talking about simple things like getting a decent job. If I sent 2 CVs out and gave up can I compare with someone who sent 100 CVs out and got the job? Can I say \"he just got lucky\"? reply KuriousCat 18 hours agoparentprevI think people like ElMo and Trump do have a lot of self control. In addition to getting what they want they are also masters of influence. It is also not poor impulse control if they are good enough to muster enough resources to afford their impulses in the first place. reply katmannthree 18 hours agorootparentHow is this different from “it’s not poor impulse control if you’re a _functional_ alcoholic who can muster enough resources to afford a handle every morning”? reply KuriousCat 17 hours agorootparentIt depends on the motive, if the person is highly motivated to indulge in hedonistic pleasure and alcohol gives him that, well he is achieving his objective. Where as person wants to experience some love/friendship and can't get it and he is using alcohol as a mask you know where to put him. reply sandspar 16 hours agoparentprevTrump and Elon Musk have incredible self control. They have selective outlets, yes. But they also both drive themselves constantly, invariably in the face of resistance. They're both self control addicts whose entire lives are built around the question \"How much resistance can I overcome?\" reply me_me_me 18 hours agoparentprevI think you are missing the point. Self control is a source of persistence to get to goal. If you are impulsive buyer of random crap, you will not save money for a house. If you give up easily or get distracted accomplishing difficult tasks will be impossible. We all know those people and at least to me they always feel like bit of looser who openly jeopardise their own future. Trumps of this world only impress naive people of this world, people who buy how to make $1m in a year books. Trump openly employs wrestling narrative building to win popularity. It doesn't get any basic than that (i am generalising avg trump voter of course, but a man who never worked a day of hard work in his life - is a hero of blue collar, cheated on every single of his 5 wives - christian hero and maybe eveb second coming, all his businesses went bust - a true successful businessmen) Would you trust trump to run your company? or oversee anything of value? reply FrustratedMonky 18 hours agoprevWonder if companies setting \"Stretch\" goals, and having people fail, is actually a form of control. Get results, but also keeps employees down. People may be less inclined to ask for raise if they miss goals, even if the results were 'good'. reply throwaway74432 18 hours agoparentThat becomes clear when management says that hitting all of your goals actually means you weren't ambitious enough. reply throwway120385 16 hours agoparentprevIt's also a tool for HR to say they have a process that they follow but then still allow all of the decisions about pay increases and performance to be entirely subjective. You miss your stretch goals, and so depending on how your manager feels at the time that's either totally acceptable and you're doing a great job, or if they need justification to shaft you then it's totally unacceptable and you didn't do the work well enough to deserve that. Same situation but totally different outcome. reply petsfed 16 hours agoparentprevThere's a coaching philosophy that says to never praise unconditionally; rather every piece of praise should be followed with a criticism. I suppose this is theoretically a good practice, but only if you're already operating in a high-trust environment. If you're not, it starts to feel like you're never good enough, because absolutely every victory, no matter how big or small, is followed up with \"but it could be better...\". Which is, again technically accurate, but part of the finesse of being a good people-manager is understanding that humans are not robots who can simply process and accept criticism without any emotional hangups. reply annoyingnoob 18 hours agoparentprevMy mother was a Manager at an insurance company for a very long time. She still firmly believes that one cannot get a 5 out 5 on a review because no one is perfect. Business likes to set unobtainable goals like that. In my experience, that kind of corporate behavior drives high performers away. High performers know what they have done and if you refuse to acknowledge it over time they leave. reply quickthrower2 17 hours agorootparentThe folly is the same as the rock star amplifier that “goes to 11” or how gig economy reviews are 5 star = acceptable or better, anything else means dreadful.’It is just a linear transform people have to do in their heads. reply xeckr 16 hours agoprev>In one experiment, working adults imagined a scenario where a colleague with the goal of being fit either ate a large dessert or abstained from dessert altogether. Researchers found that the colleague was seen as being better suited for high-power roles when they abstained from indulging, an indication of self-control. The result of the study seems obvious even if the design is a bit primitive. reply rgbrgb 16 hours agoprevCorrelates with the relatively high number of executives I know who do not drink alcohol. reply runamuck 16 hours agoparentUnexpected comment! Can you provide details? I have not noticed a correlation, and about a half of the successful executives I see drink heavily and frequently, and nearly all drink a little (except me). reply __turbobrew__ 13 hours agoparentprevYou will also see it correlates with people who exercise. reply fsckboy 15 hours agoprevhaving self-control could be a result of having higher IQ: being better able to figure out the future results of current behavior. And \"higher IQ leads to power\" (on average, population statistic) is not counterintuitive. reply faeriechangling 2 hours agoparentYou would think so, but IQ and conscientiousness have essentially zero correlation. Smart but lazy is an actual thing. reply davidhay 12 hours agoprevIt would be interesting to map how powerful a person is to their perceived power via this study. reply boringuser2 16 hours agoprevA controversial take from this is that, aside from the health benefits, you will also accrue personal and professional benefits from not being fat. I remember interviewing when I was chubbier post-COVID thinking that the only evidence the interviewer has of my self-control and drive is that I have none. Best case scenario, you are interviewing a temporarily embarrassed skinny person. Noble prize winner James Watson, famous bluntly tell it like it is-er, said he didn't hire fat people for this reason. (This has since been rectified by reinstating strength training and dietary controls, which I recommend.) reply CharlesW 15 hours agoparent> …you will also accrue personal and professional benefits from not being fat. As an overweight person, I don't think it's controversial. You'll also accrue personal and professional benefits from being a tall man and/or good-looking. However, \"fat people have no self-control\" is still fatphobia and ignores the interplay of genetic, metabolic, physiological, cultural, socio-economic, and environmental factors that contribute to obesity. There are countless fat people who are incredibly smart and hard-working, and have exquisite discipline in most areas of their life. reply tasuki 2 hours agorootparentI have never met a fat person with \"exquisite discipline\". They had other positive qualities, but discipline was not over of them. Also, I prefer to associate with people who are not fat. Is that a bad thing? reply boringuser2 15 hours agorootparentprev>fatphobia First of all, I really find this term to be gross. You should be afraid of being unhealthy. Second of all, are you posting that you have individual metabolic differences that substantially alter your caloric expenditure? FYI most studies show very little innate caloric expenditure differences between human beings outside of what can be predicted based on their body mass. Ironically, heavier people burn more calories, not less, innately. This makes sense because metabolic machinery is complex and so fundamental to life that it would obviously be very tightly tuned genetically. reply CharlesW 15 hours agorootparent> First of all, I really find this term to be gross. Lodge a complaint with dictionaries, I guess? https://dictionary.cambridge.org/us/dictionary/english/fatph... You can't agree with \"you will accrue personal and professional benefits from not being fat\" and then pretend fat-scrimination (is that better?) doesn't exist. reply boringuser2 15 hours agorootparentBeing negatively predisposed to bad things is both normal and healthy behavior. Also, using heuristics to make judgments about situations is how intelligence works. reply tekla 12 hours agorootparentprevAs a former obese person, the culture of America is excusing people for being fat is disgusting. I'm not saying you have to be supermodel thin, but being fat is literally bad in every single possible metric (Except for possibly surviving longer in a survival situation, which I presume most fat people will never be in) Being fat is bad. Being a normal weight is better in every metric reply boringuser2 9 hours agorootparentIt also makes you feel like something as trivial as taking out your garbage out to the curb is a monumental physical task. The quality of life improvement from not being obese is obscene. reply RajT88 15 hours agoprevI for one can attest to this. I saw great gains in power, and the power people perceived in me once I prioritized and stuck to a strict exercise regime. Once I started doing 100 pushups, 100 situps, 100 air squats, and a 10-km run every day, I found greater strength and focus in my life. I felt I could easily take on just about any task with ease and immediately conquer it! Eventually, I got into training which used higher-than-normal gravity to maximize resistance and further build strength. After a while I measured my power, and it was well over 9000! reply tboyd47 12 hours agoprevMakes you wonder why the stereotype of the \"Playboy\"-type billionaire who is slave to his desires has been pushed and accepted for long, when it's plainly false. reply 48864w6ui 11 hours agoparentIs it plainly false in your circles? In mine, only one acquired (economic) power; all the rest inherited, and they do tend more to the playboy stereotypes. (But this dichotomy may largely be due to the fact that only the former is a nerd?) reply bitwize 14 hours agoprevDarth Vader is just so much cooler and more menacing than Kylo Ren, and a lot of it is to do with that the latter loses his shit when he encounters an obstacle. reply hackerlight 7 hours agoprevOzempic side effects: may cause unwanted power reply pphysch 16 hours agoprevThis conclusion is a bit tautological: another way to frame \"self-control\" is \"power over yourself\" or \"ability to influence your own actions\". Of course people who have power over N=1 people are more likely to be perceived as \"powerful\" than people who have no self-control i.e. N=0, because they are. In other words, self-control doesn't just \"lead to\" power, it is power. reply zubairq 17 hours agoprevInteresting. Maybe I should try to get more done by having more self control reply dekhn 15 hours agoprev\"Researchers confirm their own biases using poor quality study\" reply omgJustTest 14 hours agoprevWelcome to 101 in people do not like failure. reply akomtu 15 hours agoprevIt's a tautology. Self-control is about controlling your self, which is made of thoughts and desires. Either you control them, or they control you. reply begueradj 14 hours agoprevWinston Churchill said: “A man is about as big as the things that make him angry”. > But Steve Jobs could also be a tyrant. He was obsessively controlling, and given to fits of rage, throwing tantrums and yelling at employees and board members. (quoted). >Read the internal Tesla employee survey from 2018, where employees called Elon Musk an 'unapproachable tyrant' who fires people 'because of his ego' (Quoted) ... The list is long ... reply gexla 18 hours agoprev [–] \"Some set an ambitious goal of reading 200 pages each week, while others set a more moderate goal of reading 50 pages per week. All of these individuals read the same amount – 100 pages – but those who didn’t meet their goal were seen as less powerful by study participants.\" Under-promise, over-deliver. reply tithe 15 hours agoparentA key part of that statement (from the article) is \"in comparison to your peers.\" By whom do you want to be considered powerful? And how can I maximize the real/perceived performance delta between myself and other participants in that target group? And how do I do so in a way that doesn't appear threatening to the decision-makers (group \"leaders\") in that target group? reply yoyohello13 15 hours agoparentprevThat's what I do at work, it works quite well. reply largbae 17 hours agoparentprev [–] Elon Musk appears to be a counter-example... reply notnaut 17 hours agorootparentOver promising is better when you’re interested in money/power and have plenty of your own to start with, rather than any underlying pointless nonsense like working toward an achievable goal or trying to be a decent human. reply nebula8804 16 hours agorootparentWe now know based on his numerous biographies and documentaries made about him that he overpromised because else his companies would not get the badly needed funds/support/customers. Is this fraud? Probably but he got away with it because he delivered enough tangible results to get people to back off. Otherwise he'd probably be in jail now. Why does he still overpromise when he does not need to? Well the latest biography sorta explained it: He can't help himself, he needs the thrill of an ever more difficult problem to solve. (ie. he's psycho) reply DinaCoder99 17 hours agorootparentprev [–] Musk has lost a massive amount of credibility because of failing to deliver on most of the self-proclaimed potential of his businesses—basically, all of them outside of the potential of SpaceX, and even there he should be grateful the perceived value of the company isn't tied to going to mars. reply time-less-ness 15 hours agorootparent [–] /me eyes Tesla reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Individuals with high self-control are viewed as powerful and are more likely to receive power from their peers, according to UC San Diego's Rady School of Management research.",
      "The study, including seven experiments with 3,500 participants and published in the Journal of Personality and Social Psychology, demonstrates that those with high self-control are considered more suitable for positions of power.",
      "Not achieving ambitious goals can lead to a perception of lower power, even if the performance matches peers, underlining the significance of self-control in achieving success and power in different objectives."
    ],
    "commentSummary": [
      "The discussion delves into the link between self-control and power, reevaluating the conventional wisdom on resisting temptations and considering factors like socio-economic status.",
      "Personal stories on weight loss and medication underscore the significance of habitual behaviors in self-control, emphasizing its role in achieving success and personal development.",
      "The conversation expands to societal influences, discrimination, and corporate goal-setting impacting self-control, and the intricate relationship between self-control, power, and executive behaviors like alcohol consumption is examined, citing figures like Elon Musk."
    ],
    "points": 192,
    "commentCount": 121,
    "retryCount": 0,
    "time": 1709902716
  },
  {
    "id": 39647331,
    "title": "Breakthrough: High-Temp Superconducting Magnets Transform Fusion",
    "originLink": "https://news.mit.edu/2024/tests-show-high-temperature-superconducting-magnets-fusion-ready-0304",
    "originBody": "Detailed study of magnets built by MIT and Commonwealth Fusion Systems confirms they meet requirements for an economic, compact fusion power plant. David L. ChandlerMIT News Publication Date: March 4, 2024 Press Inquiries Caption: In MIT’s Plasma Science and Fusion Center, the new magnets achieved a world-record magnetic field strength of 20 tesla for a large-scale magnet. Credits: Image: Gretchen Ertl Caption: A team lowers the magnet into the cryostat container. Credits: Image: Gretchen Ertl Caption: The test setup inside MIT’s Plasma Science and Fusion Center. Credits: Image: Gretchen Ertl Caption: The large team that worked on the magnets was from MIT’s Plasma Science Fusion Center and MIT spinout Commonwealth Fusion Systems. Credits: Image: Gretchen Ertl Previous image Next image In the predawn hours of Sept. 5, 2021, engineers achieved a major milestone in the labs of MIT’s Plasma Science and Fusion Center (PSFC), when a new type of magnet, made from high-temperature superconducting material, achieved a world-record magnetic field strength of 20 tesla for a large-scale magnet. That’s the intensity needed to build a fusion power plant that is expected to produce a net output of power and potentially usher in an era of virtually limitless power production. The test was immediately declared a success, having met all the criteria established for the design of the new fusion device, dubbed SPARC, for which the magnets are the key enabling technology. Champagne corks popped as the weary team of experimenters, who had labored long and hard to make the achievement possible, celebrated their accomplishment. But that was far from the end of the process. Over the ensuing months, the team tore apart and inspected the components of the magnet, pored over and analyzed the data from hundreds of instruments that recorded details of the tests, and performed two additional test runs on the same magnet, ultimately pushing it to its breaking point in order to learn the details of any possible failure modes. All of this work has now culminated in a detailed report by researchers at PSFC and MIT spinout company Commonwealth Fusion Systems (CFS), published in a collection of six peer-reviewed papers in a special edition of the March issue of IEEE Transactions on Applied Superconductivity. Together, the papers describe the design and fabrication of the magnet and the diagnostic equipment needed to evaluate its performance, as well as the lessons learned from the process. Overall, the team found, the predictions and computer modeling were spot-on, verifying that the magnet’s unique design elements could serve as the foundation for a fusion power plant. Enabling practical fusion power The successful test of the magnet, says Hitachi America Professor of Engineering Dennis Whyte, who recently stepped down as director of the PSFC, was “the most important thing, in my opinion, in the last 30 years of fusion research.” Before the Sept. 5 demonstration, the best-available superconducting magnets were powerful enough to potentially achieve fusion energy — but only at sizes and costs that could never be practical or economically viable. Then, when the tests showed the practicality of such a strong magnet at a greatly reduced size, “overnight, it basically changed the cost per watt of a fusion reactor by a factor of almost 40 in one day,” Whyte says. “Now fusion has a chance,” Whyte adds. Tokamaks, the most widely used design for experimental fusion devices, “have a chance, in my opinion, of being economical because you’ve got a quantum change in your ability, with the known confinement physics rules, about being able to greatly reduce the size and the cost of objects that would make fusion possible.” The comprehensive data and analysis from the PSFC’s magnet test, as detailed in the six new papers, has demonstrated that plans for a new generation of fusion devices — the one designed by MIT and CFS, as well as similar designs by other commercial fusion companies — are built on a solid foundation in science. The superconducting breakthrough Fusion, the process of combining light atoms to form heavier ones, powers the sun and stars, but harnessing that process on Earth has proved to be a daunting challenge, with decades of hard work and many billions of dollars spent on experimental devices. The long-sought, but never yet achieved, goal is to build a fusion power plant that produces more energy than it consumes. Such a power plant could produce electricity without emitting greenhouse gases during operation, and generating very little radioactive waste. Fusion’s fuel, a form of hydrogen that can be derived from seawater, is virtually limitless. But to make it work requires compressing the fuel at extraordinarily high temperatures and pressures, and since no known material could withstand such temperatures, the fuel must be held in place by extremely powerful magnetic fields. Producing such strong fields requires superconducting magnets, but all previous fusion magnets have been made with a superconducting material that requires frigid temperatures of about 4 degrees above absolute zero (4 kelvins, or -270 degrees Celsius). In the last few years, a newer material nicknamed REBCO, for rare-earth barium copper oxide, was added to fusion magnets, and allows them to operate at 20 kelvins, a temperature that despite being only 16 kelvins warmer, brings significant advantages in terms of material properties and practical engineering. Taking advantage of this new higher-temperature superconducting material was not just a matter of substituting it in existing magnet designs. Instead, “it was a rework from the ground up of almost all the principles that you use to build superconducting magnets,” Whyte says. The new REBCO material is “extraordinarily different than the previous generation of superconductors. You’re not just going to adapt and replace, you’re actually going to innovate from the ground up.” The new papers in Transactions on Applied Superconductivity describe the details of that redesign process, now that patent protection is in place. A key innovation: no insulation One of the dramatic innovations, which had many others in the field skeptical of its chances of success, was the elimination of insulation around the thin, flat ribbons of superconducting tape that formed the magnet. Like virtually all electrical wires, conventional superconducting magnets are fully protected by insulating material to prevent short-circuits between the wires. But in the new magnet, the tape was left completely bare; the engineers relied on REBCO’s much greater conductivity to keep the current flowing through the material. “When we started this project, in let’s say 2018, the technology of using high-temperature superconductors to build large-scale high-field magnets was in its infancy,” says Zach Hartwig, the Robert N. Noyce Career Development Professor in the Department of Nuclear Science and Engineering. Hartwig has a co-appointment at the PSFC and is the head of its engineering group, which led the magnet development project. “The state of the art was small benchtop experiments, not really representative of what it takes to build a full-size thing. Our magnet development project started at benchtop scale and ended up at full scale in a short amount of time,” he adds, noting that the team built a 20,000-pound magnet that produced a steady, even magnetic field of just over 20 tesla — far beyond any such field ever produced at large scale. “The standard way to build these magnets is you would wind the conductor and you have insulation between the windings, and you need insulation to deal with the high voltages that are generated during off-normal events such as a shutdown.” Eliminating the layers of insulation, he says, “has the advantage of being a low-voltage system. It greatly simplifies the fabrication processes and schedule.” It also leaves more room for other elements, such as more cooling or more structure for strength. The magnet assembly is a slightly smaller-scale version of the ones that will form the donut-shaped chamber of the SPARC fusion device now being built by CFS in Devens, Massachusetts. It consists of 16 plates, called pancakes, each bearing a spiral winding of the superconducting tape on one side and cooling channels for helium gas on the other. But the no-insulation design was considered risky, and a lot was riding on the test program. “This was the first magnet at any sufficient scale that really probed what is involved in designing and building and testing a magnet with this so-called no-insulation no-twist technology,” Hartwig says. “It was very much a surprise to the community when we announced that it was a no-insulation coil.” Pushing to the limit … and beyond The initial test, described in previous papers, proved that the design and manufacturing process not only worked but was highly stable — something that some researchers had doubted. The next two test runs, also performed in late 2021, then pushed the device to the limit by deliberately creating unstable conditions, including a complete shutoff of incoming power that can lead to a catastrophic overheating. Known as quenching, this is considered a worst-case scenario for the operation of such magnets, with the potential to destroy the equipment. Part of the mission of the test program, Hartwig says, was “to actually go off and intentionally quench a full-scale magnet, so that we can get the critical data at the right scale and the right conditions to advance the science, to validate the design codes, and then to take the magnet apart and see what went wrong, why did it go wrong, and how do we take the next iteration toward fixing that. … It was a very successful test.” That final test, which ended with the melting of one corner of one of the 16 pancakes, produced a wealth of new information, Hartwig says. For one thing, they had been using several different computational models to design and predict the performance of various aspects of the magnet’s performance, and for the most part, the models agreed in their overall predictions and were well-validated by the series of tests and real-world measurements. But in predicting the effect of the quench, the model predictions diverged, so it was necessary to get the experimental data to evaluate the models’ validity. “The highest-fidelity models that we had predicted almost exactly how the magnet would warm up, to what degree it would warm up as it started to quench, and where would the resulting damage to the magnet would be,” he says. As described in detail in one of the new reports, “That test actually told us exactly the physics that was going on, and it told us which models were useful going forward and which to leave by the wayside because they’re not right.” Whyte says, “Basically we did the worst thing possible to a coil, on purpose, after we had tested all other aspects of the coil performance. And we found that most of the coil survived with no damage,” while one isolated area sustained some melting. “It’s like a few percent of the volume of the coil that got damaged.” And that led to revisions in the design that are expected to prevent such damage in the actual fusion device magnets, even under the most extreme conditions. Hartwig emphasizes that a major reason the team was able to accomplish such a radical new record-setting magnet design, and get it right the very first time and on a breakneck schedule, was thanks to the deep level of knowledge, expertise, and equipment accumulated over decades of operation of the Alcator C-Mod tokamak, the Francis Bitter Magnet Laboratory, and other work carried out at PSFC. “This goes to the heart of the institutional capabilities of a place like this,” he says. “We had the capability, the infrastructure, and the space and the people to do these things under one roof.” The collaboration with CFS was also key, he says, with MIT and CFS combining the most powerful aspects of an academic institution and private company to do things together that neither could have done on their own. “For example, one of the major contributions from CFS was leveraging the power of a private company to establish and scale up a supply chain at an unprecedented level and timeline for the most critical material in the project: 300 kilometers (186 miles) of high-temperature superconductor, which was procured with rigorous quality control in under a year, and integrated on schedule into the magnet.” The integration of the two teams, those from MIT and those from CFS, also was crucial to the success, he says. “We thought of ourselves as one team, and that made it possible to do what we did.” Share this news article on: X Facebook LinkedIn Reddit Print Paper Papers: Special issue on the SPARC Toroidal Field Model Coil Program Related Links Dennis Whyte Zachary Hartwig Plasma Science and Fusion Center Department of Nuclear Science and Engineering School of Engineering Related Topics Fusion Nuclear science and engineering Energy Research Industry Collaboration Nuclear power and reactors Renewable energy Sustainability Magnets Plasma Science and Fusion Center School of Engineering Related Articles New study shows how universities are critical to emerging fusion industry Fast-tracking fusion energy’s arrival with AI and accessibility MIT expands research collaboration with Commonwealth Fusion Systems to build net energy fusion machine, SPARC MIT-designed project achieves major advance toward fusion energy Previous item Next item",
    "commentLink": "https://news.ycombinator.com/item?id=39647331",
    "commentBody": "Tests show high-temperature superconducting magnets are ready for fusion (news.mit.edu)185 points by paulsutter 11 hours agohidepastfavorite69 comments gpm 10 hours agoIs there a good reason we aren't seeing REBCO magnets in all new MRI machines? Seems like not needing liquid helium would be a pretty big win for hospitals? reply ororroro 50 minutes agoparentThere are already non-venting systems that use a reasonable amount of helium (7L) to manufacture and don't require refilling. It just was not a design consideration previously. https://www.philips.com.au/healthcare/resources/landing/the-... reply fabian2k 10 hours agoparentprevThey probably still can't carry the necessary current without helium cooling. There are some NMR spectrometers (which work essentially by the same principles as MRI machines) with high-temperature superconductors, but those are still cooled by helium. The lower the temperature is the more current you can pack into those. Could also be that it doesn't make economic sense yet, they're still pretty new compared to classical superconductors. reply caymanjim 8 hours agoparentprevMRIs don't consume much helium. Keeping them topped off is a trivial expense compared to the rest of the costs of the machine and operation. If there were a new design that didn't require helium, it would also have to cost less than current machines to make sense. reply refulgentis 10 hours agoparentprevYes: Commonwealth Fusion essentially is buying the world's supply of it. MRIs are a more mature market, i.e. > 1 units, and need to justify component switches not just for feasibility, but for profitability. Swapping out a known-good for something in such short supply isn't feasible, it's more expensive than liquid helium. Source: https://spectrum.ieee.org/fusion-2662267312 \"Over two years, the team managed to buy up most of the world’s supply of 4-millimeter-wide HTS tape\" reply actionfromafar 10 hours agorootparentCommonwealth Fusion is such a Fallout name reply deprecative 9 hours agorootparentTodd is in shambles that he can't use the name. reply fragmede 4 hours agorootparentI didn't want to get left out of the joke, so I asked ChatGPT to explain it to me (this kills the joke) and it gave me: > Todd Howard is a key figure in the Fallout series, but not as a character within the game world. He's the director and executive producer at Bethesda Game Studios, the company behind many of the Fallout games, including Fallout 3, Fallout 4, and Fallout 76. Howard has been instrumental in shaping the series and its direction since Bethesda took over the franchise. reply cyberax 9 hours agoparentprev> Is there a good reason we aren't seeing REBCO magnets in all new MRI machines? REBCO tapes are still very brittle, and MRI machines are notorious for the amount of banging they produce when active. reply andbberger 8 hours agorootparent??? the gradient coils make the noise, and due to field homogeneity requirements, it is safe to assume the superconducting coil is not moved by the gradient coils reply alacritas0 6 hours agorootparenthomogeneity is only optimized for a small volume within the bore. Regardless, the superconducting coils will still experience a change in Lorentz forces as sequences are run, and noise from other components will still be \"heard\" by the main magnet reply 7speter 7 hours agoparentprevWait, isn't the byproduct of fusion helium anyway? reply willis936 6 hours agorootparentYeah but you'd power a city for a year for 100 kg of helium. reply Tuna-Fish 38 minutes agorootparentFusion produces much, much less helium than that. D-T fusion that produces 100kg of He-4 would produce something like 25PJ of net primary energy. If you managed to turn 20% of that into electricity, you'd have 5PJ. This is within an order of magnitude of the yearly electricity consumption of the entire world. reply willis936 12 minutes agorootparentI do actually have receipts. For example numbers I picked Washington DC, which has 700k people that each use an average of 600 kWh per month. That's 5 TWh / year, which I rounded up to 10 TWh. This yields 475 m^3 of helium, which is 85 kg. I rounded up to 100. DC is a modest city and all of humanity uses 15000 TWh / year (of heat). This \"100 kg / city / year\" estimate is even on the lower end since I started with electric consumption and never put in the factor of 3 to convert to thermal. Your mass-energy conversion number is within error of mine, but your estimate of how much power we use is much lower. https://www.wolframalpha.com/input?i=%2810+TWh%29+%2F+%2817.... reply k8wk1 5 minutes agorootparentprev5PJ is not that large, only 1.4 TWh. reply ta988 5 hours agorootparentprevA single MRI needs around 1000 liters, 1L is roughly 130g. For those who wonder prices vary between $15-35 per liter. Recent MRIs have recycling systems that reduce refill needs by 90% when they work properly. reply willis936 1 minute agorootparentSo about $15k for 100 kg? p1mrx 10 hours agoprev> Like virtually all electrical wires, conventional superconducting magnets are fully protected by insulating material to prevent short-circuits between the wires. But in the new magnet, the tape was left completely bare; the engineers relied on REBCO’s much greater conductivity to keep the current flowing through the material. Much greater conductivity than what? I assume there are other non-REBCO layers in the tape, but the article neglects to mention them. reply Animats 10 hours agoparentThan stainless steel! If you have two resistors in parallel, the current in each is proportional to the ratio of the resistances. But what if one of the resistances is zero? Then all the current goes through that resistor. So if you have a superconductor in parallel with another resistor, all the current goes through the superconductor! Doesn't matter if the other resistance is very, very low, but still nonzero. Reportedly some undergrad thought of this. So the superconducting layer is bonded to stainless steel tape, which is strong, not brittle, and will go down to cyrogenic temperatures without problems. Most previous superconducting \"wires\" had brittle ceramic insulation, which was hard to wind into magnets. This is really clever. reply pfdietz 9 hours agorootparentThe other thing the normal conductor does is save the coil from exploding if the superconductor quenches and loses its superconductivity (1/2 L I^2 can be a lot of energy). Old low Tc superconductors used copper for this, I believe. reply raziel2701 4 hours agorootparentprevI think there's another level to this, when you put a superconductor in contact with a metal there's a proximity effect that lowers the Tc of the superconductor. If you're using a superconductor that has a 4K Tc then you cannot afford to have your superconductor be weakened by the proximity effect. So the insulation is to prevent the proximity effect from happening. Here, since you're using a much higher Tc material, if Tc is lowered by a few K you still have plenty of thermal budget, allowing you to take the hit from the proximity effect but be able to save on insulation. reply willis936 10 hours agoparentprevPresumably the hastelloy substrate. https://www.nature.com/articles/s41598-021-81559-z/figures/6 reply baking 3 hours agorootparentDuring a quench, almost all of the current is carried by the copper cap. https://ieeexplore.ieee.org/document/10316632 reply baking 3 hours agoparentprevThere is a layer of steel on the bottom that provides the structure for the tape and a layer of silver on top of the REBCO ceramic layer for electrical connection, but the whole tape is coated in copper which is the secondary conductor. When superconducting, the tapes, which are wound in a coil, carry the current and the copper acts as an insulator between tapes, but if it stops superconducting, the copper can become a resistive conductor which helps prevent quenching. reply Tuna-Fish 10 hours agoparentprevStainless steel. Which is a perfectly good insulator when the alternative is REBCO. reply glatisaint 10 hours agoparentprevMuch greater than previous superconductors I would assume. reply nielsbot 2 hours agoprevI saw this presentation by Dennis Whyte years ago and it left me really optimistic about Sparc/Arc's chances... He goes through a lot of the interesting design details, including why REBCO is such a huge difference and will make their commercial reactor design possible: https://www.youtube.com/watch?v=KkpqA8yG9T4 reply pinewurst 11 hours agoprevIt’s interesting that it took 3 years from the physical test until paper publication. reply probablypower 9 hours agoparentPublications often take a long time. Particularly novel topics that are difficult to review due to a lack of comparative material or willing expertise. The fact that it took 3 years is a positive, as it means multiple parties rigorously reviewed and approved the material. reply dang 11 hours agoprevUrl changed from https://futurism.com/the-byte/mit-magnets-ready-fusion, which points to this. (Normally we prefer the best third-party article to a press release but the balance of information here points the other way.) Edit: I should say that if there's a more informative third-party article, we can switch to that. reply elihu 10 hours agoprevIt's kind of funny that that image looks an awful lot like the housing of a Mazda rotary engine. I'd be curious if MIT / Commenwealth Fusion have made any progress on their \"remountable magnets\" approach. The problem is that if you have to replace parts of the reactor, the magnets get in the way. I think they found that you could just solder the ends of REBCO film to each other and even though the solder isn't superconducting, the layer can be thin enough that it doesn't matter. So, they were looking at technology to take a tokomak apart by desoldering the REBCO, then they can replace whatever parts are inside, then solder it back together when they're done. That seemed like a pretty hard problem, and not one they're trying to solve with SPARC (which is only meant to be a cheap prototype reactor, not something designed for a long service lifetime). reply tw04 10 hours agoparent> It's kind of funny that that image looks an awful lot like the housing of a Mazda rotary engine. You at least have to give the poor guy credit :). Wankel engines can be found all sorts of places including planes! https://en.m.wikipedia.org/wiki/Wankel_engine reply Certhas 10 hours agoprevIf someone is knowledgable on this: What does this mean for ITER? Any chance to retrofit new magnetic technology into ITERs design? reply elihu 10 hours agoparentI'm not super knowledgeable, but my understanding is that ITER was designed around the best superconductors known at the time, which meant making the whole thing really big. A benefit of stronger magnets is that you can make it a lot smaller, and presumably cheaper -- which is what MIT has been working on with SPARC. I don't know if it's practical to upgrade the magnets on ITER, but I'd expect it to be really expensive -- especially if they've already manufactured/installed the old magnets. reply pfdietz 9 hours agorootparentIt would be completely impractical, since ITER is not designed to withstand the much larger JxB forces (which scale as B^2). In the high Tc high field designs, the mass of the metal supports for the magnets dominates the reactor mass. reply johnbcoughlin 10 hours agoparentprevIt means nothing for ITER unfortunately. Changing the toroidal field strength implies a totally different plasma size, density, temperature, everything. ITER isn't designed for this strong of fields and the plasma they would confine. reply cyberax 9 hours agoparentprevNope. And this is fine. ITER is not a prototype of a commercial fusion reactor, it's a burning plasma laboratory. Its main goal is to study the plasma confinement technology. reply pauljurczak 2 hours agoprev20 K is high temperature? Well, I better take my mittens and ushanka off. I thought 77 K was the official threshold: https://en.wikipedia.org/wiki/High-temperature_superconducti.... reply fallingfrog 7 hours agoprevBSSCO is superconducting at much higher temperatures. Why are they not using that? reply GlibMonkeyDeath 7 hours agoparent\"In practice, HTS are limited by the irreversibility field H, above which magnetic vortices melt or decouple. Even though BSCCO has a higher upper critical field than YBCO it has a much lower H (typically smaller by a factor of 100)[11] thus limiting its use for making high-field magnets.\" https://en.wikipedia.org/wiki/Bismuth_strontium_calcium_copp... reply CrzyLngPwd 11 hours agoprev...in thirty years. reply refulgentis 10 hours agoparentPeople have been saying this for 5 years on every Commonwealth Fusion article as they hit every milestone exactly on time. It's happening this time, people rushing to make fusion jokes are getting downvoted because it's apparent you're just keyword-matching fusion then snarking. reply gerdesj 8 hours agorootparentI went to Abingdon School, which is quite close to Culham labs (JET) in Oxfordshire, in the mid 1980s. One of my A levels was Physics. We had some boffins from JET pop on over and do a presentation to us about what they were up to. I was one of the thickies. Some of my (then) compatriots are now dons and even proper scientists! I only managed a random walk into IT. I dimly recall a graph showing progress on how they were able to fire up the thing and the energy out compared to energy in. That graph had quite a decay on it initially but a really long tail. That long tail is where the always 25 years off meme comes from. Nuclear fusion as an energy source is a massively complicated beast. You might like to note, say, that building for Hinkley Point C (a fission plant, Somerset, UK - just up the road from here) was started late 2018 and was due to be commissioned in 2023 and now is due in 2030. You might also note that aircraft and the like tend to take 20+ years to go from concept to service. A nuclear fusion plant will take quite a while to commission once a concept has been thrashed out. Perhaps CF have got it sorted but there is a damn good reason for the always 25 years off \"joke\" - I was told it over 30 years ago by physicists working and already long time served at the coal face. reply lawrenceyan 5 hours agorootparentTrust me, this time is different. reply pfdietz 9 hours agorootparentprevThe problem I have can be see by going back to the original ARC paper. That reactor was much smaller than ITER, and had 10x power density, but was still the mass of several WW2 destroyers (for net 190 MW(e)). Its volumetric power density is still 40x worse than an existing commercial PWR. reply dotnet00 9 hours agorootparentprevI think they're getting downvoted because the joke is getting really worn out and endlessly dismissive cynicism is just annoying. reply bananapub 11 hours agoprevI really wish people involved in fusion research, and their university/corporate PR colleagues, had not spent the last forty years poisoning the fucking well. fusion power is still so far away that it can't even figure in to our plans for reducing emissions to zero, and yet people still publish articles like this to muddy the waters and make it sound like it matters in the short term at all. reply cubefox 11 hours agoparentThe real question is whether it can ever compete against fission in terms of cost. Currently it looks like fusion power plants will be substantially more expensive than fission power plants. reply rocqua 10 hours agorootparentHow does that change if you account for waste disposal in the costs? From what I gather the nuclear waste of fusion has a much shorter half-life. That should make storage and disposal much easier. reply ben_w 10 hours agorootparentVery little, in practice: the waste disposal of fission reactors is one of the ways that the energy density of nuclear fuel actually matters — there's hardly any by volume, even if you add up the entire world's nuclear waste. reply ericd 10 hours agorootparentIs there any reason for this to be less heavily regulated than fission plants/waste? Seems like that’s the major driver of cost? Are ridiculously massive containment vessels less necessary, since it’s very off by default/without power input, rather than potentially very on? reply compumike 6 hours agorootparentYes. See https://adamswebsearch2.nrc.gov/webSearch2/main.jsp?Accessio... \"Rulemaking: Regulatory Framework for Fusion Energy Systems, NRC Public Meeting, July 12, 2023\" and especially the graph on page 27, \"Relative radiotoxicity\" vs \"Time after shutdown (years)\" Also see https://www.nrc.gov/docs/ML2325/ML23258A145.pdf \"Fusion Systems Rulemaking: United States Nuclear Regulatory Commission Preliminary Proposed Rule Language\". The NRC are already proposing to regulate fusion reactors much more like particle accelerators (which can produce medical radioisotopes, for example), and less like fission reactors. reply Filligree 9 hours agorootparentprevNot much real reason for this to be less regulated. Still probably will be in practice, as fission is massively overregulated. reply pfdietz 9 hours agorootparentprevFor all the pearl clutching about nuclear waste, dealing with it contributes very little to the cost of fission energy. reply Certhas 10 hours agorootparentprev... which is considerably more expensive than solar + hydrogen. reply catchnear4321 11 hours agoprev> only needs hydrogen atoms for fuel rather than rare and dangerous elements like uranium and plutonium hydrogen isn’t exactly the safest element. not that any element is truly safe if used in the wrong way. reply u320 11 hours agoparentSo he's pushing an energy solution that requires Tritium. One of the rarest elements of earth. And then has the nerve to accuse fission of relying on rare elements. reply willis936 10 hours agorootparentWhy is it rare? Because it decays. Why would anyone make a plan to use it as a fuel without a plan to generate it? They don't. You breed it in the blanket that collects the energy. The energy balance is quite positive. reply refulgentis 11 hours agorootparentprevWho is he? Where did he push tritium? Is the word tritium in the article? (Chrome can't find it.) Is hydrogen more common than uranium? Is hydrogen more common than plutonium? What quote demonstrates \"accuse\", as in, \"they had the nerve to accuse\"? These are honest questions, I hope you don't take offense. Full disclosure: I live in Cambridge, MA, where MIT is located. reply EA-3167 10 hours agorootparentThe only fusion pathway being seriously studied for energy in the short (read: next century) term is D-T fusion, it doesn't need to be stated. Deuterium and Tritium are the fuels, they have to be. There are lots of other pathways, but they all require even higher temperatures/pressure than D-T, and we can't really sustain even D-T for any useful length of time. reply ben_w 10 hours agorootparentFortunately, T is a byproduct of DD fusion. Unfortunately, as we don't have any commercially viable fusion reactors yet, we can't even guess if we can afford to make T in the future. reply pfdietz 9 hours agorootparentprevThis is not true. Helion's very clever scheme appears to offer a near term route to using DD and D3He fusion. Helion could also burn DT, but they don't want to due to its practical drawbacks. If you didn't know this you need to pay much more attention to them. IMO, they are the least dubious of all the fusion efforts at reaching a practical reactor, and that includes the DT efforts. Helion is also going to produce prodigious amounts of excess tritium if their scheme works, which is good news for others trying the DT path (although how they compete against a workable advanced fuel reactor is not clear.) reply porkbeer 11 hours agoparentprevHow is it unsafe? I just electroplated on my desk and hydrogen was formed (right next to pure o2) with zero danger. Sure, an accumulated mix near stoich might be an issue, but that is easy to avoid. reply catchnear4321 11 hours agorootparentand you can split water and make neat bubbles of two elements. more likely to shock yourself than make a boom. most elements are safe if you handle them properly. that assumes nothing outside of your control. and no one. reply mcnamaratw 8 hours agoprevWhy? MIT could just rein these people in, and the reputational damage would stop right away. reply u320 11 hours agoprev [–] > \"Overnight, it basically changed the cost per watt of a fusion reactor by a factor of almost 40 in one day,\" So there is this myth that fusion enables unlimited cheap energy. This is 100% false. It's not unlimited at all. Most concepts relies on using lithium-6. That's not a super common element. But more importantly, it's not cheap. That should be obvious, if you could reduce the cost per watt 40x and still not be competitive. reply ianburrell 10 hours agoparent [–] Lithium is a super common element. They are mining tens of thousands of tons per year increasing rapidly as make electric car batteries. Lithium-6 is 5% of lithium. Fusion reactors only need tons for breeding blanket. reply pfdietz 9 hours agorootparent [–] The original ARC design had 950 tonnes of FLiBe, which contains 115 tonnes of Li-6. This is a substantial fraction of the Li-6 produced for the entire US hydrogen bomb program, all for a single 190 MW(e) (net) reactor. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "MIT and Commonwealth Fusion Systems have tested high-temperature superconducting magnets for a compact fusion power plant, setting a world record with a 20 tesla magnetic field strength.",
      "The use of the new superconducting material REBCO enabled innovative design elements, like no insulation around the superconducting tape, making fusion energy production more efficient.",
      "The successful testing demonstrates the potential of building a fusion power plant that generates more energy than it consumes, highlighting the critical role of collaboration and expertise from MIT's Plasma Science and Fusion Center."
    ],
    "commentSummary": [
      "High-temperature superconducting magnets are suitable for fusion, but they encounter issues in MRI machines due to helium cooling and brittleness.",
      "Challenges like economic feasibility, supply constraints, and noise from gradient coils limit their wider use.",
      "MIT's Sparc/Arc reactor design and the retrofitting challenges in ITER are discussed, along with regulatory concerns, waste management, safety, and potential excess tritium production in the fusion energy debate."
    ],
    "points": 185,
    "commentCount": 69,
    "retryCount": 0,
    "time": 1709937301
  },
  {
    "id": 39645991,
    "title": "New York Times Defends Wordle Clone Takedowns",
    "originLink": "https://www.theverge.com/2024/3/8/24094234/ny-times-wordle-clones-files-dmca-copyright-takedowns-knockoffs",
    "originBody": "Tech/ Gaming/ Entertainment The New York Times is targeting Wordle clones with legal takedowns The New York Times is targeting Wordle clones with legal takedowns / The publication has filed several DMCA copyright requests against Wordle knockoffs, citing ownership over the Wordle name and gameplay style. By Jess Weatherbed, a news writer focused on creative industries, computing, and internet culture. Jess started her career at TechRadar, covering news and hardware reviews. Mar 8, 2024, 1:59 PM UTC Share this story Some developers have already taken down their Wordle clones to avoid a legal battle with The New York Times. Photo by Mike Kemp / In Pictures via Getty Images Hundreds of games inspired by Wordle, the popular web-based word puzzle, are at risk of being deleted due to copyright takedowns issued by The New York Times. As reported by 404 Media, The New York Times — which purchased Wordle back in 2022 — has filed several DMCA notices over Wordle clones created by GitHub coders, citing its ownership over the Wordle name and copyrighted gameplay including 5x6 tile layout and gray, yellow, and green color scheme. Two takedown requests were issued in January against unofficial Korean and Bosnian-language versions of the game. Additional requests were filed this week against Wirdle — a variant created by dialect group I Hear Dee in 2022 to promote the Shaetlan language — and Reactle, an open-source Wordle clone built using React, TypeScript, and Tailwind. It was developed prior to the Times’ purchase of the game, according to its developer, Chase Wackerfuss. The Reactle code has been copied around 1,900 times, according to GitHub, allowing developers to build upon it to create a wide variety of Wordle-inspired games that use different languages, themes, and visual styles, some of which 404 Media says are “substantially different” from Wordle. The DMCA notice against Reactle also targets all of these games forked from the original Reactle code on GitHub, alleging that spinoffs containing the Wordle name have been made in “bad faith” and that “gameplay is copied exactly” in the Reactle repository. Numerous developers commenting on a Hacker News thread also claim to have been targeted with DMCA takedowns. Reactle coder Wackerfuss has removed the game, telling 404 Media he doesn’t want to get into a legal battle with the Times. In a statement to 404 Media, the Times said: The Times has no issue with individuals creating similar word games that do not infringe The Times’s “Wordle” trademarks or copyrighted gameplay. The Times took action against a GitHub user and others who shared his code to defend its intellectual property rights in Wordle. The user created a “Wordle clone” project that instructed others how to create a knock-off version of The Times’s Wordle game featuring many of the same copyrighted elements. As a result, hundreds of websites began popping up with knock-off “Wordle” games that used The Times’s “Wordle” trademark and copyrighted gameplay without authorization or permission. Amusingly, Wordle has itself been criticized over striking similarities it shares with Lingo, a 1980s game show that centered on players guessing five-letter words, with a grid that changes color based on accuracy. Most Popular Most Popular Fortnite was down all day Rivian surprises with R3 and R3X electric SUVs Rivian R2 revealed: a $45,000 electric off-roader for the masses Microsoft says Russian hackers stole source code after spying on its executives One fan spent three years saving a Final Fantasy game before it shut down Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox daily. Email (required)Sign up By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. From our sponsor Advertiser Content From",
    "commentLink": "https://news.ycombinator.com/item?id=39645991",
    "commentBody": "New York Times is targeting Wordle clones with DMCA takedowns (theverge.com)153 points by mryall 13 hours agohidepastfavorite95 comments mryall 13 hours ago> In a statement to 404 Media, the Times said: > > The Times has no issue with individuals creating similar word games that do not infringe The Times’s “Wordle” trademarks or copyrighted gameplay. Can you really copyright “gameplay”? This seems like pointless bullying by the Times, who is probably just upset they haven’t got a positive ROI on their acquisition of a free game. reply dougb5 12 hours agoparentI can't prove it, but I am quite sure NYT got a positive ROI on the acquisition of Wordle. They paid in the low single millions of USD, which isn't much for a brand as big as Wordle had become by then (2 million+ daily players) and it's been used to drive new digital subscriptions [1]. It's diminished somewhat since then but it has remarkable staying power. I can tell that from the Google Trends data [2] as well as the anecdata that I and so many of my friends and family still play Wordle every day. [1] https://www.afr.com/technology/why-on-earth-did-the-new-york... [2] https://trends.google.com/trends/explore?date=today%205-y&ge... [Edit: And, nearly two years on, they say they get \"millions\" of players per day, and they've assigned a dedicated Wordle editor and write articles about the game frequently. (https://www.nytimes.com/2023/12/17/upshot/wordle-bot-year-in...). They're definitely not having buyer's regret.] reply user_7832 10 hours agorootparentYou can look at the comments section to see how popular it is, oftentimes there are a few hundred comments. That is higher than most NYT news stories (back when I used to use their website). reply slyall 8 hours agorootparentThe Website has stats on the number who play each game (and their moves). March 7 - Wordle 992 - 1,748,583 players March 8 - Wordle 993 - 1,749,500 players To get these go to: https://www.nytimes.com/interactive/2022/upshot/wordle-bot.h... then \"Compare and review recent scores\" -> Next page -> on on the \"Your Recent scores\" select \"view analysis\" against the game number -> Next page Doesn't seem to work with anonymous browser unfortunately. reply david422 8 hours agorootparentThat is ... amazingly consistent. reply fbdab103 8 hours agorootparentI guess the question is how many people play around on the weekends. I could believe those still playing have a daily ritual when they play a match after logging into work. reply jokethrowaway 1 hour agorootparentprevWhy would someone playing wordle subscribe to the NYT? Free games on Epic Store don't make me buy games on Epic Store. I just claim the free goodies and buy on Steam. reply c3534l 13 hours agoparentprevEspecially considering wordle has no unique gameplay. Its a very, very old game with a million variations. There was even a TV show called lingo or something that did competetive wordle. The only thing NYT owns is a brand since the game is hundreds of years old. reply Fnoord 11 hours agorootparent> Amusingly, Wordle has itself been criticized over striking similarities it shares with Lingo, a 1980s game show that centered on players guessing five-letter words, with a grid that changes color based on accuracy. Indeed. I know this from TV, grew up with it. Was a fun educative program back in the days. reply lacoolj 11 hours agorootparentPretty sure there's a new reboot of this show right now too reply leereeves 11 hours agorootparentprevYeah, Wordle is clearly a copy of Lingo. https://en.wikipedia.org/wiki/Lingo_(American_game_show)#/me... reply JadeNB 11 hours agorootparentprevSiblings have mentioned the TV game show Lingo. I can't precisely date Bulls and cows (https://en.wikipedia.org/wiki/Bulls_and_cows), but it's hard to imagine that it (the gameplay, if not that particular pen-and-paper game) isn't much older than television, hence than Lingo. reply lupire 9 hours agorootparentAnd Jotto, 1955 reply archontes 12 hours agoparentprevYou cannot copyright gameplay. https://www.americanbar.org/groups/intellectual_property_law.... reply reactordev 11 hours agorootparentbut you can patent it. reply karaterobot 11 hours agorootparent> The publication has filed several DMCA copyright requests reply brewdad 5 hours agorootparentprevYou can’t expect a lawyer to know the difference between copyright and patents. reply lacoolj 11 hours agorootparentprevIf this is true, why wouldn't Microsoft patent the taskbar or directory structure tree layout pattern in Explorer? I don't think you would be granted a design (let alone utility) patent that is as broad as \"green and yellow blocks have significance, is a grid of 5x6, and has a keyboard below\". Not a lawyer though so what do I know reply helloworld 10 hours agorootparentMicrosoft did patent the taskbar. [1] [1] https://patents.google.com/patent/US5920316A/en reply AlbertCory 11 hours agorootparentprevNo you can't. reply lbotos 10 hours agorootparenthttps://patents.google.com/patent/US5662332A/en here is a patent for Magic the Gathering. reply reactordev 8 hours agorootparentprevhttps://patents.google.com/patent/US20070226648A1/en https://patents.google.com/patent/US10369470B2/en You most certainly can reply AlbertCory 7 hours agorootparenthttps://patents.google.com/patent/US11786805B1/en This one refers to Wordle in the spec (which implies it's well known) and cites 5 other patents. Not knowing how Wordle works, I wouldn't speculate as to their chances of getting a patent, but NYT seems to be pursuing a copyright approach for now. reply dabernathy89 12 hours agoparentprevYou can patent game mechanics - famous example would be Legend of Zelda's targeting system. Apparently Nintendo is extremely aggressive about patenting game mechanics. https://kotaku.com/nintendo-is-trying-to-patent-some-really-... reply torstenvl 11 hours agorootparentIrrelevant, since NYT does not and can never (due to prior art) hold a patent on Wordle's gameplay. This is about copyright. reply deprecative 9 hours agorootparentprevNintendo is a litigation company pretending to be a game developer. reply tomkarho 11 hours agorootparentprevAnother example that comes to mind is the Nemesis system from Mordor games reply autoexec 12 hours agoparentprevMaybe you can't copyright gameplay, but you can still sue someone for infringement anyway, or even just threaten people with lawsuits that you know they can't afford to defend themselves against so they'll do whatever you want no matter what their rights are. The Times might even win in the courts if it ever gets that far. You can't copyright a musical genre either but that hasn't stopped successful lawsuits against musicians for exactly that (https://abovethelaw.com/2018/03/blurred-lines-can-you-copy-a...). reply AlbertCory 11 hours agorootparentSuing when your case has no merit is a losing strategy. Eventually you not only lose to someone who stands up to your bullying, you face massive damage to your reputation; something media companies really don't like. reply standardly 13 hours agoparentprevNot sure why you're grayed out - no, you cannot copywrite gameplay. The name \"Wordle\" could be protected, though, which is probably the case here. In any case, Wordle is too damn easy. Don't think I've ever lost. I recommend sedecordle for word enjoyers reply CannisterFlux 12 hours agorootparentYeah I stopped playing it pretty quickly. There was that not-wordle game where you had to not get the word. It made me realise just how tricky failing wordle was. I still do \"where taken\", which is photos of countries, tradle for oec trade commodities and guess the game for video games. They're all far more interesting than guessing some random word. reply lastofthemojito 12 hours agorootparentPerhaps that was Don't Wordle? That was eye-opening to me too. Play it before NYT sues it out of existence I guess: https://dontwordle.com reply CannisterFlux 2 hours agorootparentYeah that's the one. reply Fnoord 11 hours agorootparentprevHere's a curated awesome-wordle list [1]. Back in the days (as if it is long ago) I used a different one and had quite some fun with some of the clones (I didn't check this specific one). [1] https://github.com/prakhar897/awesome-wordle reply extraduder_ire 11 hours agorootparentThis repo still being up shows they probably didn't just auto-takedown every repo that has the word \"wordle\" in its name at least. Often not the case with DMCA abuse. reply autoexec 11 hours agorootparentprevDoes it use the same wordlist as wordle or have they specifically chosen words that would be impossible to not get? reply CannisterFlux 2 hours agorootparentI don't know if it was the same list, but the words were possible to not get. reply gnicholas 9 hours agorootparentprevThey’re apparently also claiming that the offenders are using forked code, which would be subject to copyright. reply kelnos 10 hours agorootparentprevWordle is a trademark. That's not what the DMCA is for. reply lbourdages 12 hours agorootparentprevSemantle is nice but very difficult. You have unlimited tries, though. reply leereeves 11 hours agorootparentJust tried Semantle, which I hadn't heard of before. I don't want to spoil today's puzzle, but after getting a few hints and giving up to get the answer, I don't see any relation between the hints and the answer. (The similarity scores are word2vec cosine similarity, and that's fun to see in action, but the results make me think word2vec isn't that good.) Edit: I tried yesterday's word and that was much better. Today's word might just be word2vec's kryptonite. reply lbourdages 6 hours agorootparentI haven't played a whole lot but sometimes it definitely makes more sense than others. reply interestica 13 hours agoparentprevGames and Other Uncopyrightable Systems Bruce E. Boyden https://scholarship.law.marquette.edu/cgi/viewcontent.cgi?ar... reply hiccuphippo 10 hours agoparentprevSpecially since Wordle's gameplay is a straight copy of Bulls and Cows: https://en.wikipedia.org/wiki/Bulls_and_cows reply pests 6 hours agorootparentExcept wordle provides positional information. reply pgrote 13 hours agoparentprevLingo seems like Wordle. https://www.youtube.com/watch?v=sC0kie6dPjo Maybe the copyright is on the colors on the game field? reply Kluggy 12 hours agorootparent> The New York Times — which purchased Wordle back in 2022 — has filed several DMCA notices over Wordle clones created by GitHub coders, citing its ownership over the Wordle name and copyrighted gameplay including 5x6 tile layout and gray, yellow, and green color scheme. reply Verdex 12 hours agorootparentThis smells very similar to the issue with the tetris clone a number of years ago. The overall issue is that while games aren't copyrightable, the look and feel of a game can be copyrighted. https://en.wikipedia.org/wiki/Tetris_Holding,_LLC_v._Xio_Int... reply ben0x539 10 hours agorootparentThat ruling is really exasperating. The judge not accepting that the size of the tetris field is part of the rules of tetris is really weird to me. The tetronimoes don't even look similar in their comparison given the constraints of shapes made of four blocks, and, like, the texture obviously looks nothing like original tetris, it's just some basic-ass rendering of a shape with some color. reply phire 12 hours agoparentprevGameplay can't be copyrighted, but in the case of Worldle, the word lists are probably copyrightable. Any clone that derives their own word list is probably fine, but any clone that copy/pastes the exact word lists from wordle (especially the shorter list of 2,315 possible solutions) is probably infringing copyright. reply torstenvl 10 hours agorootparentI doubt it. The existence of the words themselves is a fact, which is not copyrightable. Only their arrangement can be. Feist Publications, Inc., v. Rural Telephone Service Co., 499 U.S. 340 (1991) However, to be copyrightable, the arrangement has to be expressive, i.e., it must \"possess the requisite originality because the author . . . chooses . . . in what order to place them.\" My strong suspicion is that Wordle's word list is randomized. Not a chosen expression of the author. reply lupire 9 hours agorootparentThe selection is expressive. https://www.copyright.gov/comp3/chap300/ch300-copyrightable-... The Originality Requirement for Compilations A compilation may contain several distinct forms of authorship: • Selection authorship involved in choosing the material or data that will be included in the compilation; • Coordination authorship involved in classifying, categorizing, ordering, or grouping the material or data; and/or • Arrangement authorship involved in organizing or moving the order, position, or placement of material or data within the compilation as a whole reply phire 6 hours agorootparentIndeed. The creator of wordle started with a reasonably exhaustive list of ~13000 five letter words. That list won't be copyrightable. But the first prototype wasn't so fun, because it would often pick a word the player didn't even know existed. So the creator (and his partner) manually classified the entire list based on if they knew the word or not; splitting the list into two groups, words which might appear as solutions and words that won't appear as solutions but will still be accepted. This manual classification step and splitting it into two groups makes a very good argument for the wordlists meeting the criteria for copyright. Source: https://slate.com/culture/2022/01/wordle-game-creator-wardle... reply torstenvl 6 hours agorootparentWhat's your basis for that? I'm very skeptical. Intuitively, whether a word is within the working vocabulary of a sample of the population is an objective fact, not creative expression. Do you know of any case law to the contrary? And, as it turns out, it was the author's girlfriend who categorized each of the words. Not the author. If there is copyright in the selection (which I doubt), NYT doesn't appear own it. reply gnicholas 9 hours agoparentprevThis is either confusingly written, or what the NYT said is confusing. The article refers to the source code being forked. That would be copyrightable. But I wouldn’t refer to the source code as “gameplay”. reply sidewndr46 12 hours agoparentprevWhen you have the kind of connections the NYT does and the lawyers to go along with it, sure! reply gnicholas 10 hours agoprev> citing its ownership over the Wordle name and copyrighted gameplay including 5x6 tile layout and gray, yellow, and green color scheme. So this is referring to trademark (the name Wordle) and copyright — but not patent. It makes sense to go after people who are using the same actual name, since this clearly infringes the trademark, and because if you do not enforce (\"police\") your mark against minor players, you can end up losing the ability to enforce it against major players. But the copyright bit is a bit novel from my perspective (lawyer, but not copyright lawyer). If you had asked me what a copyright claim about Wordle would be about, I would have said the precise code. I might have wondered about the specific word lists, even though these would probably fail the \"phone book\" test (don't remember the case, but these were deemed uncopyrightable). I never would have thought about the tile layout and color scheme. That seems more like what I think of as \"trade dress\" [1] or perhaps something related to patent (which wouldn't apply here, unless the original Wordle owner had filed for patents a long time ago. Are there any copyright lawyers who can elucidate how the tile layout and color scheme might be subject to copyright law? I assume the NYT has good lawyers, and has thought long and hard before going after folks on github... 1: https://www.inta.org/topics/trade-dress reply helsinkiandrew 12 hours agoprevIt’s interesting that they seem fine with blatently copying part of the BBC game show Only Connect for their Connections game: https://www.reddit.com/r/onlyconnect/comments/169j2p4/have_y... reply gnicholas 10 hours agoparentThey also get their knickers in a twist when employees leak internal documents to other news organizations...despite the fact that they make extensive use of others' leaks themselves: https://www.npr.org/2024/03/03/1235606433/an-investigation-i... reply a1o 13 hours agoprevMechanics can't be copyrighted (unless it's MTG tap mechanic, it seems), but what is weird is apparently the projects it went against were in different languages that Wordle doesn't cover (which I think matters considering Wordle is about guessing words) and using a different name. reply trothamel 11 hours agoparentThe MTG tap mechanic was patented, not copyrighted. https://patents.google.com/patent/US5662332A/en , claims 4, 5, and 6 - expired in 2014. The actual symbol is also a copyright and trademark, I believe, but right now other games can us the 90 degree rotation. (Disney's Lorcana, for example, uses 90 degree rotation and calls it 'exerting' a card.) reply jkaplowitz 12 hours agoparentprevLots of other games have the same mechanic as MTG’s tapping. But to avoid legal threats from WotC/Hasbro, most of them call it exhausting or refreshing or other similar words. I’m skeptical that there’s any valid legal claim there, but if it is more legitimate than well-funded big-corp lawyer bullying, it’s either a trademark claim or a claim that any game with a card-refresh mechanic called “tapping” must be a derivative work of MTG for copyright purposes. reply mynameisnoone 4 hours agorootparentFuck the \"owning\" of terminology laying claim to individual words. It's utter horseshit. reply eschaton 4 hours agoparentprevIf you work in a field that’s built around intellectual property law, as any creative field including software is, it behooves you to be sufficiently familiar with it to understand that MTG’s tap mechanic was patented not covered by copyright. reply racingmars 13 hours agoprevThis feels like a trademark question, not a copyright question. Seems like an abuse of DMCA; if only it were easier to ensure companies filing fraudulent DMCA claims end up facing real consequences. reply crtified 12 hours agoprevThe asset NYT bought when they bought Wordle, was to have their name indelibly associated with a trend that is more popular than they themselves are. It is simply \"our name in your face, on a regular basis, for years\" advertising, at a global scale. Case in point : here we all are, thinking and talking about the NYT. That is the ROI. reply autoexec 11 hours agoparentYeah, but we're all just thinking about what a dick they are. Does that really count? reply umeshunni 9 hours agorootparentAll publicity is good publicity reply willis936 9 hours agorootparentThat was dubious in the 20th century and is certainly wrong in the internet era 21st century. reply fieryscribe 12 hours agoprevIt's interesting to see them go after others for copying gameplay when they have done exactly the same: https://www.theguardian.com/media/2023/jun/15/connections-ne... reply dang 13 hours agoprevRecent and related: NY Times issues DMCA takedowns of Wordle clones - https://news.ycombinator.com/item?id=39618193 - March 2024 (44 comments) reply marssaxman 13 hours agoprevWell, that just makes me want to stop playing Wordle. reply brikym 8 hours agoprevHey NYT I hope you’ll enjoy my game called QWERTL https://qwertl.com Or my other game https://redactle.net which sounds ever so similar to wordle so you had better send your lawyers. Or what about https://memorycardsgame.com - I mean it has a grid layout so surely that must be copyrighted too. I will file any complaints into my computers /dev/null disk. reply kbf 12 hours agoprevIf you’re looking for a Wordle game they can’t take from you, Wordyl on Game Boy is one I’ve been enjoying :) https://bbbbbr.itch.io/gb-wordyl reply wackget 10 hours agoparentWhy wouldn't they allow keyboard input to type the words? Nobody has time to select each individual letter with the up/down arrows. I get it's an emulated game but there must be a way to hook keyboard input. reply brokensegue 8 hours agorootparent??? it's a gameboy game. gameboy had no keyboard reply mig39 9 hours agoprevNo problem with the New York Times ripping off Only Connect though. https://web.archive.org/web/20230709023951/https://www.teleg... reply mynameisnoone 4 hours agoparentThe rules apply to me but not to thee because some animals are more equal than others. reply onemoresoop 12 hours agoprevDoes anyone know if anything like https://www.threemagicwords.app/ is going to receive a DMCA takedown? It is somewhat similar to Wordle and yet quite different. reply ashton314 12 hours agoprevIf someone makes a FOSS version that can be easily dropped onto a public server somewhere, then we should all host a Wordle clone. (I'm sure the code has already been written a thousand times over…) reply extraduder_ire 10 hours agoparentAlmost all of these clones are FOSS, and can be dropped onto a server somewhere. Hell, the most common way to implement the game has everything on single html page with a javascript file that handles the entire game's logic and word list. reply ok123456 11 hours agoprevDoesn't the name \"Wordle\" and the rules for the game come from an article in Creative Computing in the 70s? reply ath0 11 hours agoparentThe name is a play on the author's last name (Josh Wardle): https://www.nytimes.com/2022/01/03/technology/wordle-word-ga... reply visarga 11 hours agoprevAre NYT still writing news, or just copyright trolling? reply fddrdplktrew 8 hours agoprevDMCA itself should be illegal reply mynameisnoone 4 hours agoprev\"The [self-congratulatory] paper of record\" behaves as an arrogant, evil megacorp. Is this news? (Pun intended) reply anthk 12 hours agoprevThis is just Mastermind with words. I remember word puzzles like that in several newspapers and magazines across the world for -decades-. In Spanish media it was usually called \"Deducción\". Link in Spanish as proof: http://pasatiemposmatematicosdelaprensa.blogspot.com/2013/10... The last puzzle it's from 1999. reply anthk 12 hours agoprevLingo predates Wordle and I remember that show in my country more than 30 years ago. reply wly_cdgr 10 hours agoprevGTFO NYT lol it is well established that gameplay mechanics are not subject to copyright. The level of presumption and disrespect and ignorance about the culture is pretty par for the course for the goons running the NYT. reply MisterBastahrd 10 hours agoprevWorldle is just hangman with hints. reply camdenlock 11 hours agoprevIn the end, everyone’s just becoming a game company. Er, sorry; an “experience” company. reply kepler1 13 hours agoprev [–] A company wants to enforce its copyright, etc., fine. That's their right to do so. Although I beg to differ with our copyright laws. No, what I find objectionable about the NYT games team, such as their spelling bee puzzle, is that they selectively deem certain words not valid responses. Not curse words or words with no redeeming value, but words that are perceived to be derogatory against disadvantaged groups or \"offensive\". It's like an extension of the hyper sensitive liberal newsroom. Fine, it's a private organization and their choice. But it reflects in my mind a hijacking of the language by people oversensitized to the point of ridiculousness. reply quink 12 hours agoparent> words that are perceived to be derogatory against disadvantaged groups The _game_ won't accept responses of such a nature? That doesn't sound too bad. reply jmull 12 hours agoparentprevOh, come on. These word games are for casual fun and enjoyment. It's really not a big deal whether a particular word is included or not in the dictionary for a particular game. For Spelling Bee, the levels appear to be calculated based on the word list, so while it may be a little frustrating that a particular real (albeit off-colored, so to speak) word isn't accepted, rest assured that that doesn't doesn't affect the puzzle's difficulty. So no harm done. IMO, if some particular word removes more fun than it adds, good riddance. Personally, I was most offended when \"ichor\" was not accepted, though I'm happy to say their reporting mechanism seemed to work, because it seems to be accepted now at least in the pangram game. reply add-sub-mul-div 11 hours agoparentprev [–] Good thing you're not sensitive. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The New York Times is sending legal takedown requests to developers on GitHub for Wordle clones, citing ownership of the name and gameplay.",
      "Some developers are removing their clones to avoid legal disputes with the Times, even if created before the Times acquired Wordle.",
      "This situation underscores the copyright infringement challenges present in the gaming sector and the importance of protecting intellectual property rights."
    ],
    "commentSummary": [
      "The New York Times is sending DMCA takedowns to Wordle clones, citing trademark and copyright infringement.",
      "Debate is ongoing about whether gameplay is copyrightable, with accusations of bullying by the NYT.",
      "Wordle's success, comparisons to other word games, and potential legal concerns about gameplay mechanics and word lists are under discussion, with criticism aimed at the NYT for their legal actions and content decisions."
    ],
    "points": 153,
    "commentCount": 95,
    "retryCount": 0,
    "time": 1709929731
  },
  {
    "id": 39643685,
    "title": "Apple Reinstates Epic's Account in EU",
    "originLink": "https://twitter.com/TimSweeneyEpic/status/1766158416093798866",
    "originBody": "The DMA went through its first major challenge with Apple banning Epic Games Sweden from competing with the App Store, and the DMA just had its first major victory. Following a swift inquiry by the European Commission, Apple notified the Commission and Epic that it would relent…— Tim Sweeney (@TimSweeneyEpic) March 8, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=39643685",
    "commentBody": "Apple restores Epic's account in the EU (twitter.com/timsweeneyepic)153 points by milen 16 hours agohidepastfavorite1 comment dang 14 hours ago [–] Comments moved to https://news.ycombinator.com/item?id=39643981, which has more background. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Digital Markets Act (DMA) faced a significant hurdle when Apple removed Epic Games from the App Store.",
      "Subsequently, the DMA achieved a milestone victory when the European Commission's investigation prompted Apple to retract its actions.",
      "This event highlights the DMA's role in regulating digital markets and tech giants' behavior."
    ],
    "commentSummary": [
      "Apple has reinstated Epic's account in the EU, indicating a potential resolution to their previous conflicts.",
      "More in-depth information and discussions regarding this development are available on a dedicated page for additional context."
    ],
    "points": 153,
    "commentCount": 1,
    "retryCount": 0,
    "time": 1709920391
  },
  {
    "id": 39643958,
    "title": "Piano on Paper: Play with Webcam and Paper Keys",
    "originLink": "https://github.com/Mayuresh1611/Paper-Piano",
    "originBody": "PAPER PIANO Now we don't need to buy Piano if we want to play music. We can play piano on paper, although it may not give feeling of pressing keys on piano but it gets work done! demo.mp4 Currently it only supports maximum 2 fingers (1 finger of both hands). Support for more fingers and highly susceptible training model is on the way. Setting up project How to Use Training and Adjusting Contributing to this project SETTING UP PROJECT Python version 3.11 and above Clone the repository git clone https://github.com/Mayuresh1611/Paper-Piano.git run command pip install -r requirements.txt in the command line. Execute run.py file HOW TO USE This is a little trickier part as the project requires you to set up a webcam in the specific angle at a specific height and distance. Also stronger the light, better the performance. STUFF YOU WILL REQUIRE webcam or you can use third-party tools for webcam. Two A4-sized white paper, horizontally joined together. 2 rectangles need to be drawn at both ends of the paper with a black marker, thicker lines yield better results. Recommended position for the webcam will be such that it can capture the finger and shadow beneath the finger and should have both boxes we drew on joined paper in the FOV of the camera. Just like shown in the demo video. Light source in front, ie. behind the camera would be preferred. Casting sharp shadows. Hand with all fingers. TRAINING AND ADJUSTING During training the model on your finger, the first window will appear where box will be drawn around the tip of the finger. If the box does not cover complete finger and little surrounding of the finger. Adjust the camera accordingly. In the training stage, do not move your finger furiously, move it slowly giving out every angle. While training, during the touched finger state, do press the finger but not too hard. In an untouched finger state, do not touch paper, you can get near paper but not too close. Lift the finger high like you would normally do. CNN has been used to train over the data, it distinguishes the touched and untouched finger, if the results are not satisfactory, retrain the model. CONTRIBUTING TO THIS PROJECT If we make this project into a complete working piano on paper. It would give access of piano and instruments for those who cannot afford it. like me :) There are no set rules or code of conduct as of now. Anyone wtih better ideas and improvement is welcomed. The only rules are The name of the function should match the functionality it is doing. Use comments only when required. Share with the ones who can improve this project even more.",
    "commentLink": "https://news.ycombinator.com/item?id=39643958",
    "commentBody": "Piano on Paper (github.com/mayuresh1611)140 points by thunderbong 11 hours agohidepastfavorite40 comments abrichr 4 hours agoCongratulations on shipping! You might find this interesting, a comment on https://news.ycombinator.com/item?id=35376138 (https://news.ycombinator.com/item?id=35379983): > I had been playing with the idea of creating a browser-based virtual piano for when I'm travelling and don't have access to a real piano but have my laptop with me. The idea would be to point the webcam down at the table between me and the laptop, and play on the table as if a piano were there. Then use the mediapipe framework [1] to capture finger positions, and use those to update a virtual environment like the one you have here. > I put it on hold due to the significant engineering required, but it seems you have already implemented (and open sourced!) the browser-based piano simulation component. > A quick scan through your repo indicates that this is all implemented in Python. I see that you are using mujoco_wasm [2]. [1] https://google.github.io/mediapipe/ [2] https://github.com/zalo/mujoco_wasm reply zerojames 1 hour agoprevThis is a delightful project. Congratulations on shipping it! I would be curious to see ways to do this where the piece of paper had lines so you could visually see on the paper which key was which. Doing this would be tricky, though. On first thought, an object detection model that detects each key from a perspective, then saves the coordinates, may work. Then you would calculate the Intersection over Union between the finger annd each region. This may only work with a top-down perspective, which might be less practical in real-world use. In any case, I always miss my keyboard when I am traveling. A project like this feels a lot more practical than the “portable” travel keyboards. reply mayureshsatam 0 minutes agoparentyess I am currently working on that too! reply pama 8 hours agoprevI’ve often wondered about the reverse. Get the finest grand piano action, like the ones used in the Fazioli, and create a minimal mechanical unit that has just that piece and the pedals (with the action mechanics that connect the left/right pedals to the keys) and then nothing else but a couple of sensors and a couple fast USB-C. The Kawai Novus series did an OK action and left pedal, but then they connect it to a whole furniture and their sound synthesis and vibrations are imperfect. With modern powerful GPU the sound should be possible to become near perfect and easy to tune to one’s preference. reply mastazi 2 hours agoparentVery high end digital pianos have an action that is quite similar to a real grand. This is a relatively cheap digital piano action https://www.youtube.com/watch?v=S_VT0IpM66k These are 2 versions of a mid-high range digital piano action https://www.youtube.com/watch?v=rJEAKI2jBLU This is a very high end digital piano action https://kawaius.com/wp-content/uploads/2021/06/Kawai-NV10S-M... That very same action is also used in acoustic pianos, and the acoustic version looks like this (you can see the main difference is that the hammers have felt) https://snadenspianos.com.au/wp-content/uploads/2018/05/MG_9... And this is another (more traditional) acoustic grand piano action https://www.youtube.com/watch?v=v5O1Kfd_4Fw reply throw5323446 4 hours agoparentprevKinda like a silent piano? There are aftermarket options like PianoDisc https://en.wikipedia.org/wiki/Silent_piano reply pama 4 hours agorootparentYes but without the need for the full furniture. One should be able to miniaturize everything else but the action. I think such compact designs will eventually appear but the difference in real action versus very good hybrid digital actions is of highest interest to musicians who typically have access to real pianos. Roland, Yamaha, Kawai, and others have been moving in that direction over the years and the top models by Yamaha and Kawai have passable actions though they still have a large piece of furniture around them. reply SamBam 6 hours agoparentprevI am no master pianist, but I imagine much of the feel of such an instrument comes from the physical weight of the keys, and the feel as they strike the strings. Although good quality keyboards have \"weighted\" keys, they don't feel exactly the same. I think it would be hard to reproduce this exactly. reply pclmulqdq 4 hours agorootparentThe top-end Yamaha electric pianos have a near-complete replica of the action inside of them. If you adjust the volume exactly right, you almost replicate the feel of playing a real piano. reply pama 5 hours agorootparentprevI agree. Perhaps I was unclear. I was hoping that someone can use the exact action of the grand piano, but skip the strings, soundboard and so on. reply ta988 5 hours agoparentprevGPUs may not be great for latency. reply pama 4 hours agorootparentYou may have a point. Perhaps naively I imagined that if competitive gamers can handle it, they may be useful enough for piano. Currently the CPU software is lacking in quality (though improving over time) but has acceptable latency compared to grands (which also have latency due to the mechanism and the location of the sound source). reply elwell 10 hours agoprevObviously a bit rough, but a start. This is the kind of content I like on HN. reply mayureshsatam 4 hours agoparentyeah! it was a hella challenging to detect touch. reply tmountain 1 hour agorootparentI had an idea for a similar project. Basically, a tabletop drum kit. In this case, you’d tap on a few objects on the table that make distinct sounds and then map that back to a virtual drum kit in your DAW using midi. I never got around to building it, but I am still keen on the idea. reply phlakaton 3 hours agoprevI'm glad you're finding ways of getting into music inexpensively. But my advice is: haptics are super important in music interfaces! Spend the extra effort to make something that you can satisfyingly press and which can give your fingers some mechanical feedback. Your playing will be sooooo much more rewarding. reply yakito 8 hours agoprevI've previously used PianoVision, and it worked quite well. The concept seems similar, now augmented with VR. I anticipate that VR pianos will evolve to such an extent that they'll replace traditional pianos in many scenarios (like teaching/learning) https://www.meta.com/experiences/5271074762922599/ reply josephg 6 hours agoparent> they'll replace traditional pianos in many scenarios I'm not convinced. Totally VR pianos are obviously a bad idea just like totally VR keyboards for typing are rubbish for anything more complex than a wifi password. And as for learning using VR - well, I'm not convinced by things like pianovision. How do you learn a piece with that? With sheet music I can play the music at my own speed, when and how I want. I can go back and play a measure a few times to experiment with timing. Speed up and slow down, and develop my own ear for how I want it to sound. If I'm feeling cheeky, I'll improvise extra notes in the song, or take some notes out if I'm not feeling it. At an instrument, sheet music is a polite suggestion. I can imagine playing something like pianovision and getting good at pushing the buttons at the right time like guitar hero. But being a human shaped CD player feels like a very small part of what makes a musician. reply m3at 5 hours agorootparentI agree for pianos, because the mechanical feedback is part of what makes playing enjoyable. I'm not as sure for keyboards in VR! There has been a lot of research on non-invasive brain computer interface (BCI), including predictive systems that guess what you want to type, instead of where you're actually typing (example from 8y ago [1]). Simpler gesture recognition is already on the market (like this one, from 2020, for the apple watch [2]). And now bigger VR players are investing in the tech [3]. I expect useful brain interface to be integrated in common VR devices in a couple of generations. [1] Air Keyboard: Mid-Air Text Input Using Wearable EMG Sensors and a Predictive Text Modeland a Predictive Text Model; https://digitalcommons.dartmouth.edu/cgi/viewcontent.cgi?art... [2] https://mudra-band.com/ [3] https://www.androidcentral.com/gaming/virtual-reality/zucker... reply superb_dev 6 hours agorootparentprevGetting good at pushing the buttons is exactly it. Just being able to hit the note that you want to hit on an instrument is a skill reply andoando 6 hours agoparentprevThe weight of the keys are an extremely important factor. Pianos and even good digital pianos have hammer action keys. You have no way of control the volume with this. I don’t think digital will ever replace a real piano. reply andoando 1 hour agorootparentWanted to add: This isn't to say that something like this can't make its own cool instrument, but as to learning the piano, no matter how good the tech is, its always going to be an imitation of the real thing. Which isn't to say that its useless for learning piano either. reply ludston 6 hours agoparentprevWhat? Why would they? Digital piano sounds still can't reproduce the richness of dynamics and texture of an acoustic piano. Practising on a digital keyboard and then performing on a real piano is jarring enough, let alone using an instrument with zero haptic feedback. This is pure gimmick. reply proee 8 hours agoprevWhen I was in 3rd grade we learned to type on a “keyboard” that was printed on a sheet of paper. I suppose it’s a novel way to teach kids to type with a budget of zero. reply devsda 6 hours agoparentI know that this is better than nothing, but as I understand, precise fingering is an important part of learning piano. Is there a risk of kids picking up wrong habits on a paper compared to a (cheap) real one ? reply xandrius 3 hours agorootparentI guess it's always better bad habits than no habits at all. The context in which this is done is rather important. Also, it might prevent having a ton of lost and worthless keyboards in everyone's attic :D reply dhosek 6 hours agoparentprevWhen I was in high school in the 80s, there was a piano class at the high school and the students had non-sounding, non-moving plastic piano keyboards to practice on at their desks. Nowadays, they get 60-key keyboards with headphones. reply a1o 9 hours agoprevThis is pretty cool work, love the concept. I am curious if there are fast, easy to use and efficient ways to also detect the full hand. Maybe by placing the camera from above would make it easier to process but at the cost of complicating the setup. reply latchkey 9 hours agoprevPreviously: The pianist who learnt to play on a paper piano https://news.ycombinator.com/item?id=21823936 (December 18, 2019 — 1 points, 0 comments) It is a video of a guy in 2019 who couldn't afford a piano, so he drew it out on paper and learned that way and became an award winning musician. reply macintux 7 hours agoparentObviously in no way comparable, but you reminded me that many, many years ago, when I was taking typing in high school, I'd sit in class \"typing\" in mid-air (hands at my side) transcribing what the teacher was saying. It was a little annoying to people around me, but ended up being very effective. reply dhosek 6 hours agoparentprevThere was a scene like this in the Talented Mr Ripley (1999). reply iefbr14 10 hours agoprevReminds me of virtual laser keyboards, eg: https://nerdknowbetter.com/best-virtual-laser-keyboards reply userbinator 3 hours agoparentI'm surprised they actually exist instead of being a rendered design concept that I saw many years ago, and are apparently common and numerous enough that someone would write a listicle about them. They don't seem to cost more than a good regular keyboard either, but I've found reviews saying that the typing experience is about as uncomfortable as you'd expect from tapping on a hard surface. reply robinsonb5 47 minutes agorootparentThe typing experience is Hhhhhhhideous[1] [1] https://www.youtube.com/watch?v=1Cy1dPlo8No (NSFW) reply p1mrx 8 hours agoprevI think this idea would work better using 3D printed keys. It's possible to print flexible springs, and filament can be used as a hinge pin. The computer just needs to recognize which keys have moved. reply mayureshsatam 4 hours agoparentyeah! but main motive was to make it as free as possible. Someone like me who does not have much of money to buy a piano but to play one see this as absolute win with trade offs ofc! reply p1mrx 54 minutes agorootparentI just made this piano key; the cost of filament is around $0.15: https://files.catbox.moe/kvop4m.webm So it's in the ballpark of a couple dollars per octave. I agree that's not very competitive with paper. reply two_handfuls 7 hours agoprevThis requirement is logical but also.. ouch > Hand with all fingers. reply ipeev 4 hours agoprev [–] Where is the file touch_detection_model.h5? reply mayureshsatam 4 hours agoparent [–] well actually you need to train the model then the file will be generated reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Paper Piano project enables users to play piano on paper via a webcam and two A4-sized white papers, supporting two fingers currently.",
      "Setting up the project entails Python 3.11, repository cloning, and installing requirements, while using it necessitates proper webcam positioning and lighting.",
      "Training the model includes adjusting the camera, moving fingers deliberately, and distinguishing between touched and untouched fingers, with contributions encouraged for enhancing musical instrument accessibility."
    ],
    "commentSummary": [
      "GitHub users are exploring a browser-based virtual piano simulation tracking finger positions with mediapipe, discussing high-end digital piano actions, similarities with acoustic piano actions, miniaturizing grand piano actions, and integrating VR technology.",
      "The conversation highlights the significance of weight and haptic feedback in piano keys, learning methods (on paper vs. real keyboard), and the potential of brain-computer interfaces in VR environments.",
      "Users are evaluating virtual laser keyboards' efficiency and cost, proposing enhancements through 3D printing technology."
    ],
    "points": 140,
    "commentCount": 40,
    "retryCount": 0,
    "time": 1709921435
  },
  {
    "id": 39642135,
    "title": "Deception Uncovered: Superconductivity Scandal Rocks Physics Lab",
    "originLink": "https://www.nature.com/articles/d41586-024-00716-2",
    "originBody": "NEWS FEATURE 08 March 2024 Superconductivity scandal: the inside story of deception in a rising star's physics lab Ranga Dias claimed to have discovered the first room-temperature superconductors, but the work was later retracted. An investigation by Nature’s news team reveals new details about what happened — and how institutions missed red flags. By Dan Garisto Twitter Facebook Email Ranga Dias and his team at the University of Rochester compressed materials in a device called a diamond anvil cell to explore superconductivity. Credit: Lauren Petracca/New York Times/Redux/eyevine In 2020, Ranga Dias was an up-and-coming star of the physics world. A researcher at the University of Rochester in New York, Dias achieved widespread recognition for his claim to have discovered the first room-temperature superconductor, a material that conducts electricity without resistance at ambient temperatures. Dias published that finding in a landmark Nature paper1. Nearly two years later, that paper was retracted. But not long after, Dias announced an even bigger result, also published in Nature: another room-temperature superconductor2. Unlike the previous material, the latest one supposedly worked at relatively modest pressures, raising the enticing possibility of applications such as superconducting magnets for medical imaging and powerful computer chips. Most superconductors operate at extremely low temperatures, below 77 kelvin (−196 °C). So achieving superconductivity at room temperature (about 293 K, or 20 °C) would be a “remarkable phenomenon”, says Peter Armitage, a condensed-matter researcher at Johns Hopkins University in Baltimore, Maryland. But Dias is now infamous for the scandal that surrounds his work. Nature has since retracted his second paper2 and many other research groups have tried and failed to replicate Dias’s superconductivity results. Some researchers say the debacle has caused serious harm. The scandal “has damaged careers of young scientists — either in the field, or thinking to go into the field”, says Paul Canfield, a physicist at Iowa State University in Ames. Why a blockbuster superconductivity claim met a wall of scepticism Previous reporting by The Wall Street Journal, Science and Nature’s news team has documented allegations that Dias manipulated data, plagiarized substantial portions of his thesis and attempted to obstruct the investigation of another paper by fabricating data. Three previous investigations into Dias’s superconductivity work by the University of Rochester did not find evidence of misconduct. But last summer, the university launched a fourth investigation, led by experts external to the university. In August 2023, Dias was stripped of his students and laboratories. That fourth investigation is now complete and, according to a university spokesperson, the external experts confirmed that there were “data reliability concerns” in Dias’s papers. Now, Nature’s news team reveals new details about how the scandal unfolded. The news team interviewed several of Dias’s former graduate students, who were co-authors of his superconductivity research. The individuals requested anonymity because they were concerned about the negative impact on their careers. Nature’s news team verified student claims with corroborating documents; where it could not do so, the news team relied on the fact that multiple, independent student accounts were in agreement. The news team also obtained documents relevant to the acceptance of the two Nature papers and their subsequent retractions. (Nature’s news and journal teams are editorially independent.) The investigation unearths fresh details about how Dias distorted the evidence for room-temperature superconductivity — and indicates that he concealed information from his students, manipulated them and shut them out of key steps in the research process. The investigation also reveals, for the first time, what happened during the peer-review process for Dias’s second Nature paper on superconductivity. Dias did not respond to multiple requests for comment. Together, the evidence raises questions about why the problems in Dias’s lab did not prompt stronger action, and sooner, by his collaborators, by Nature’s journal team and by his university. Zero resistance Dias came to the University of Rochester in 2017, fresh from a postdoctoral fellowship at Harvard University in Cambridge, Massachusetts, where he worked under physicist Isaac Silvera. “He’s not only a very talented scientist, but he’s an honest person,” Silvera told Nature’s news team. Once Dias settled at Rochester, he pursued high-temperature superconductivity. Three years earlier, the field had been electrified when researchers in Germany discovered superconductivity in a form of hydrogen sulfide with the formula H3S at 203 K (−70 °C) and at extremely high pressures3. This was a much higher temperature than any superconductor had achieved before, which gave researchers hope that room-temperature superconductivity could be around the corner. Dias proposed that adding carbon to H3S might lead to superconductivity at even higher temperatures. Ranga Dias at the University of Rochester, New York.Credit: Lauren Petracca/New York Times/Redux/eyevine His former graduate students say they synthesized samples of carbon, sulfur and hydrogen (CSH), but did not take measurements of electrical resistance or magnetic susceptibility that showed superconductivity. When a superconducting material is cooled past a critical temperature, its electrical resistance drops sharply to zero, and the material displays a similarly sharp change in its magnetic properties, called the Meissner effect. Students say they did not observe these key signs of superconductivity in CSH. Because of this, students say they were shocked when Dias sent them a manuscript on 21 July 2020 announcing the discovery of room-temperature superconductivity in CSH. E-mails seen by the news team show that the students had little time to review the manuscript: Dias sent out a draft at 5.13 p.m. and submitted the paper to Nature at 8.26 p.m. the same evening. When the students asked Dias about the stunning new data, they say, he told them he had taken all the resistance and magnetic-susceptibility data before coming to Rochester. The news team obtained e-mails that show Dias had been making similar claims since 2014. In the e-mails, Dias says he has observed a sulfur-based superconductor with a temperature above 120 K — which is relatively high, but far from room temperature. The students recall that they felt odd about Dias’s explanation but did not suspect misconduct at the time. As relatively inexperienced graduate students, they say, they trusted their adviser. During peer review, however, Dias’s claims about CSH met more resistance. Nature’s news team obtained the reports of all three referees who reviewed the manuscript. Two of the referees were concerned over a lack of information about the chemical structure of CSH. After three rounds of review, only one referee supported publication. The news team showed five superconductivity specialists these reports. They shared some of the referees’ concerns but say it was not unreasonable for the Nature editors to have accepted the paper, given the strongly positive report from one referee and what was known at the time. The paper was published on 14 October 2020 to fanfare. Dias and a co-author, Ashkan Salamat, a physicist at the University of Nevada, Las Vegas (UNLV), also announced their new venture: Unearthly Materials, a Rochester-based company established to develop superconductors that operate at ambient temperatures and pressures. At the time, students say, they trusted Dias’s explanations of where the resistance and magnetic-susceptibility data came from. Now, however, they no longer believe the result, or Dias’s explanation for the data. “I don’t think any of the other data was collected,” one student says. Matters arise Soon after the CSH paper was published, Jorge Hirsch, a condensed-matter theorist at the University of California, San Diego, began pressing Dias to release the raw magnetic-susceptibility data, which were not included in the paper. More than a year later, Dias and Salamat finally made the raw data public. In January 2022, Hirsch and Dirk van der Marel, a retired professor at the University of Geneva in Switzerland, posted an analysis of the raw data on the preprint server arXiv4. They reported that the data points were separated by suspiciously regular intervals — each exactly a multiple of 0.16555 nanovolts. Hirsch and van der Marel stated that this feature was evidence of data manipulation. Dias’s team used laser spectroscopy to measure the pressure of samples in diamond anvil cells.Credit: Lauren Petracca/New York Times/Redux/eyevine Dias and Salamat responded in an arXiv preprint, arguing that the voltage intervals were simply a result of a background subtraction5 (the preprint was subsequently withdrawn by arXiv administrators). In high-pressure experiments, the signal of a sample’s superconductivity — a drop in voltage — can be drowned out by background noise. Researchers sometimes subtract this background, but the CSH paper did not mention the technique. Questions about the data prompted Nature’s journal team to look further. In response to the concerns from Hirsch and van der Marel, editors at Nature asked four new referees to participate in a post-publication review of the CSH paper, which, like most peer review, was confidential. Now, Nature’s news team has obtained the reports, which show that two of the anonymous referees found no evidence of misconduct. But two other reviewers, whom the news team can identify as physicists Brad Ramshaw at Cornell University in Ithaca, New York, and James Hamlin at the University of Florida in Gainesville, found serious problems with the paper. In particular, Hamlin found evidence that led him to conclude the raw data had been altered. Nature applied an editor’s note to the CSH paper on 15 February 2022, alerting readers to concerns about the data. On 4 March 2022, Dias and Salamat sent a rebuttal to the referees, denying data manipulation. But the rebuttal, seen by the news team, does not provide an explanation for the issues that Hamlin and Ramshaw found in the raw magnetic-susceptibility data. “I don’t know of any reasonable way this could come about,” Ramshaw wrote in a 13 March e-mail to Nature’s manuscript team in response to the rebuttal. “The simplest conclusion would be that these data sets are all generated by hand and not actually measured.” On 27 March 2022, Hamlin sent Nature’s journal team his response to the rebuttal, which proposed an explanation for the odd data: rather than deriving the published data from raw data, Dias had added noise to the published data to generate a set of ‘raw’ data. To assess the evidence for data fabrication, Nature’s news team last month asked two superconductivity specialists to review the post-publication reports. They said that Hamlin’s analysis gives credence to claims of misconduct. In July 2022, using a different analysis, van der Marel and Hirsch independently came to the same conclusion and posted their findings on arXiv as an update to their original preprint. In it, they state that the raw data must have been constructed from the published data6. Why superconductor research is in a ‘golden age’ — despite controversy In light of these concerns, Nature started the process of retracting the CSH paper. On 11 August, Nature editors sent an e-mail to all the co-authors asking them whether they agreed to the retraction. Students who spoke to the news team say that they were surprised by this, because Dias had kept them out of the loop about the post-publication review process. They remained unaware of any of the referees’ findings, including that there was evidence for data fabrication. Nature retracted the CSH paper on 26 September 2022, with a notice that states “issues undermine confidence in the published magnetic susceptibility data as a whole, and we are accordingly retracting the paper”. Karl Ziemelis, Nature’s chief applied and physical sciences editor, says the journal’s investigation ceased as soon as the editors lost confidence in the paper, which “did leave other technical concerns unresolved”. The retraction does not state what Hamlin and Ramshaw found in the post-publication review process instigated by Nature: that the raw data were probably fabricated. Felicitas Heβelmann, a specialist in retractions at the Humboldt University of Berlin, says misconduct is difficult to prove, so journals often avoid laying blame on authors in retractions. “A lot of retractions use very vague language,” she says. Publicly, Dias continued to insist that CSH was legitimate and that the retraction was simply down to an obscure technical disagreement. As Nature journal editors were investigating the CSH paper, the University of Rochester conducted two investigations into Dias’s work; a separate one followed the retraction. One of the university’s inquiries was in response to an anonymous report, which included some of the evidence indicating possible data fabrication that surfaced during Nature’s post-publication review. The university told Nature’s news team that the three investigations regarding the CSH study did not find evidence of misconduct. A spokesperson for Nature says that the journal took the university’s conclusions into account during its deliberations, but still decided to retract the paper. The lack of industry-wide standards for investigating misconduct leaves it unclear whether the responsibility to investigate lands more on journals or on institutions. Ziemelis says: “Allegations of possible misconduct are outside the remit of peer review and more appropriately investigated by the host institution.” Heβelmann says the responsibility to investigate can “vary from case to case”, but that there is a trend of more journals investigating misconduct, regardless of institutional action. Funding agencies can also investigate alleged misconduct. In this case, Dias has received funding from both the US National Science Foundation (NSF) and the Department of Energy (DoE). The DoE did not respond to questions from Nature’s news team about Dias’s grant. The NSF declined to say whether it is investigating Dias, but it noted that awards can be terminated and suspended in response to an investigation. The students who spoke to Nature’s news team say that none of them were interviewed in the three investigations of the CSH work by the university, which they were not aware of at the time. “We were hoping someone would come talk to us,” one student says. “It never happened.” A new claim By the time the CSH paper came under scrutiny by Nature journal editors in early 2022, Dias’s graduate students were starting to grow concerned. In summer 2021, Dias had tasked them with investigating a compound of lutetium and hydrogen (LuH), which he thought might be a high-temperature superconductor. They began testing commercially purchased samples of LuH and, before long, a student measured the resistance dropping to zero at a temperature of around 300 K (27 °C). Dias concluded the material was a room-temperature superconductor, even though there was extremely little evidence, several students told Nature. “Ranga was convinced,” one student says. Physicist James Hamlin raised concerns about data reported by the Rochester group.Credit: Zach Stovall for Nature But the measurements were plagued by systematic errors, which students say they shared with Dias. “I was very, very concerned that one of the probes touching the sample was broken,” one student says. “We could be measuring something that looks like a superconducting drop, but be fooling ourselves.” Although students did see resistance drops in a few other samples, there was no consistency across samples, or even for repeated measurements of a single sample, they told Nature’s news team. Students were also worried about the accuracy of other measurements. During elemental analysis of a sample, they detected trace amounts of nitrogen. Dias concluded that the samples included the element — and the resulting paper refers to nitrogen-doped lutetium hydride. But further analysis, performed after the paper was submitted, indicated that nitrogen was not incorporated into the LuH. “Ranga ignored what I was saying,” one student says. Because they were not consulted on the CSH paper, the students say they wanted to make sure they were included in the process of writing the LuH paper. According to the students, Dias initially agreed to involve them. “Then, one day, he sends us an e-mail and says, ‘Here’s the paper. I’m gonna submit it,’” one student says. E-mails seen by Nature’s news team corroborate the timeline. Dias sent out the first draft of the LuH paper in an e-mail at 2.09 a.m. on 25 April 2022. “Please send me your comments by 10.30 AM,” Dias wrote. “I am submitting it today.” The manuscript they received did not contain any figures, making it difficult to assess. The students convinced Dias to hold off on submitting until the next day, when they could discuss it in person. One student was upset enough by the meeting that they wrote a memorandum of the events four days afterwards. The memo gives details of how students raised concerns and Dias dismissed them. Students worried that the draft was misleading, because it included a description of how to synthesize LuH; in reality, all the measurements were taken on commercially bought samples of LuH. “Ranga responded by pointing out that it was never explicitly mentioned that we synthesized the sample so technically he was not lying,” the student wrote. The students say they also raised concerns about the pressure data reported in the draft. “None of those pressure points correspond to anything that we actually measured,” one student says. According to the memo, Dias dismissed their concerns by saying: “Pressure is a joke.” Students say that Dias gave them an ultimatum: remove their names, or let him send the draft. Despite their worries, the students say they had no choice but to acquiesce. “I just remember being very intimidated,” one student says. The student says they regret not speaking up more to Dias. “But it’s scary at the time. What if I do and he makes the rest of my life miserable?” Dias made some changes that the students requested, but ignored others; the submitted manuscript contained a description of a synthesis procedure that had not been used. He sent the LuH manuscript to Nature that evening. Paper problems After Nature published the LuH paper in March 2023, many scientists were critical of the journal’s decision, given the rumours of misconduct surrounding the retracted CSH paper. They wanted to know on what basis Nature had decided to accept it. (In the case of both papers, neither the peer-review reports nor the referees’ identities were revealed.) Nature’s news team obtained those reviews and can, for the first time, reveal what happened during the review process for the LuH paper. Nature editors received the manuscript in April 2022 (about a month after Nature received the CSH post-publication review reports) and sent it out to four referees. Physicist Brad Ramshaw, together with James Hamlin, investigated data questions surrounding Dias’s superconductivity research.Credit: Kim Modic All four referees agreed that the findings, if true, were highly significant. But they emphasized caution in accepting the manuscript, because of the extraordinary nature of the claims. Referee 4 wrote that the journal should be careful with such extraordinary claims to avoid another “Schön affair”, referring to the extensive data fabrication by German physicist Jan Hendrik Schön, which has become a cautionary tale in physics and led to dozens of papers being retracted, seven of them in Nature. Referees 2 and 3 also expressed concern about the results because of the CSH paper, which at the time bore an editor’s note of concern but had not yet been retracted. Referees raised a plethora of issues, from a lack of details about the synthesis procedure to unexplainable features in the data. Although Dias and Salamat managed to assuage some of those concerns, referees said the authors’ responses were “not satisfactory” and the manuscript went through five stages of review. In the end, only one referee said there was solid proof of superconductivity, and another gave qualified support for publication. The other two referees did not voice support for publication, and one of them remained unsatisfied with the authors’ responses and wanted more measurements taken. The news team asked five superconductivity specialists to review key information available to Nature journal editors when they were considering the LuH manuscript: the referee reports for the LuH paper and the reports indicating data fabrication in the CSH paper. All five said the documents raised serious questions about the validity of the LuH results and the integrity of the data. “The second paper — from my understanding of timelines — was being considered after the Nature editors and a lot of the condensed-matter community were aware there were profound problems” with the CSH paper, Canfield says. The specialists also pointed to negative comments from some of the LuH referees, such as the observation by Referee 1 that “raw data does not look like a feature corresponding to superconducting transition”. When asked why Nature considered Dias’s LuH paper after being warned of potential misconduct on the previous paper, Magdalena Skipper, Nature’s editor-in-chief, said: “Our editorial policy considers every submission in its own right.” The rationale, Skipper explains, is that decisions should be made on the basis of the scientific quality, not who the authors are. Many other journals have similar policies, and guidelines from the Committee on Publication Ethics state that peer reviewers should “not allow their reviews to be influenced by the origins of a manuscript”. But not all journals say they treat submissions independently. Van der Marel, who is the editor-in-chief of Physica C, says that he would consider past allegations of misconduct if he were assessing a new paper by the same author. “If you have good reasons to doubt the credibility of authors, you are not obliged to publish,” he says. Under review Soon after the LuH paper was published in March 2023, it came under further scrutiny. Several teams of researchers independently attempted to replicate the results. One group, using samples from Dias’s lab, reported electrical resistance measurements that it said indicated high-temperature superconductivity7. But numerous other replication attempts found no evidence of room-temperature superconductivity in the compound. As previously reported in Science, Hamlin and Ramshaw sent Nature a formal letter of concern in May. Dias and Salamat responded to the issues later that month, but the students say they were not included in the response, and learnt about the concerns much later. A recording of a 6 July 2023 meeting between Dias and his students, obtained by Nature’s news team, shows that Dias continued to manipulate the students. Throughout the hour-long meeting, Dias said he wanted to involve the students in deciding how the team would respond to concerns about the LuH paper. But he didn’t tell them that he and Salamat had already responded to the technical issues raised by Hamlin and Ramshaw. One of Dias’s students adjusts a diamond anvil cell, which the team used in its experiments.Credit: Lauren Petracca/New York Times/Redux/eyevine The recording also reveals how Dias tried to manipulate the Nature review, because he believed the process would turn against him once more. “We can pretend we’re going to cooperate and buy time for a month or so, and then gather some senior scientists from the community,” Dias says in the recording. Dias explains how he wants to use the credibility of senior scientists — or the University of Rochester — to pressure Nature and avert a retraction. But Dias’s plans were thwarted. Later that month, the students received an e-mail from Nature’s editors that showed Dias and Salamat had, in fact, already responded to the concerns. The students realized that Dias had sent them a document with the dates removed, apparently to perpetuate the falsehood. On 25 July 2023, the journal initiated a post-publication review and asked four new referees to assess the dispute. All of the referees agreed that there were serious problems with the data, and that Dias and Salamat did not “convincingly address” the issues raised by Hamlin and Ramshaw. A spokesperson for Nature says the journal communicated with University of Rochester representatives during the post-publication review. Separately, Dias’s students were beginning to mobilize, re-examining the LuH data they were able to access. The students hadn’t done this before, because, they say, Dias produced almost all of the figures and plots in both of the Nature papers. Several other researchers told the news team that the principal investigator does not typically produce all the plots. “That’s weird,” Canfield says. The students say they were especially concerned about the magnetic susceptibility measurements — again, the raw data seemed to have been altered. Looking at the real raw data, one student says, the material does not look like a superconductor. But when Dias subtracted the background, the student says, that “basically flips that curve upside down and makes it look superconducting instead”. They continued finding problems. For the resistance measurements, too, the alleged raw data didn’t match data actually taken in the lab. Instead, it had been tweaked to look neater. “Science can be really messy … some of these plots just look too good,” a student says. Back to school By this point, some students were deeply concerned about their careers. “My thesis is going to be full of fabricated data. How am I supposed to graduate in this lab?” one student says. “At that point, I was thinking of either taking a leave of absence, or of dropping out.” During the summer, Dias began facing other issues. One of his papers in Physical Review Letters8 — unrelated to room-temperature superconductivity — was being retracted after the journal found convincing evidence of data fabrication. Around the same time, Dias was stripped of his students and the University of Rochester launched a fourth investigation — this time, the students say they were interviewed. ‘A very disturbing picture’: another retraction imminent for controversial physicist In late August, the students decided to request a retraction of the LuH paper and compiled their concerns about the data and Dias’s behaviour. Before they sent a letter to Nature, Dias apparently caught wind of it and sent the students a cease-and-desist notice, which the news team has seen. But, after consulting a university official who gave them the green light, the students sent their letter to Nature editors, precipitating the retraction process. Eight out of 11 authors, including Salamat, signed the letter and the LuH paper was retracted two months later, on 7 November. According to multiple sources familiar with the company, Salamat left Unearthly Materials in 2023 and is under investigation at UNLV. He did not respond to multiple requests for comment, and a spokesperson for UNLV declined to comment publicly on personnel issues. The scandal has also had an impact on Nature’s journal team. “This has been a deeply frustrating situation, and we understand the strength of feelings this has stirred within the community,” Ziemelis says. “We are looking at this case carefully to see what lessons can be learnt for the future.” With the university’s investigation now complete, Dias remains at Rochester while a separate process for addressing “personnel actions” proceeds. He has no students, is not teaching any classes and has lost access to his lab, according to multiple sources. Dias’s prestigious NSF grant — which has US$333,283 left to pay out until 2026 — could also be in jeopardy if the NSF finds reason to terminate it. Dias has not published any more papers about LuH, but on X (formerly Twitter), he occasionally posts updates about the material. In a 19 January tweet, Dias shared an image of data, which he said showed the Meissner effect — “definitive proof of superconductivity!” doi: https://doi.org/10.1038/d41586-024-00716-2 References Snider, E. et al. Nature 586, 373–377 (2020); retraction 610, 804 (2022). Article PubMed Google Scholar Dasenbrock-Gammon, N. et al. Nature 615, 244–250 (2023); retraction 624, 460 (2023). Article PubMed Google Scholar Drozdov, A. P., Eremets, M. I., Troyan, I. A., Ksenofontov, V. & Shylin, S. I. Nature 525, 73–76 (2015). Article PubMed Google Scholar van der Marel, D. & Hirsch, J. E. Preprint at https://arxiv.org/abs/2201.07686v1 (2022). Dias, R. P. & Salamat, A. Preprint at https://arxiv.org/abs/2201.11883 (2022). van der Marel, D. & Hirsch, J. E. Preprint at https://arxiv.org/abs/2201.07686v6 (2022). Salke, N. P., Mark, A. C., Ahart, M. & Hemley, R. J. Preprint at https://arxiv.org/abs/2306.06301 (2023). Durkee, D. et al. Phys. Rev. Lett. 127, 016401 (2021); erratum 130, 129901 (2023); retraction 131, 079902 (2023). Article PubMed Google Scholar Download references Reprints and permissions Latest on: Materials science Chemistry Scientific community Jobs Faculty Recruitment, Westlake University School of Medicine Faculty positions are open at four distinct ranks: Assistant Professor, Associate Professor, Full Professor, and Chair Professor. Hangzhou, Zhejiang, China Westlake University Post Doc Research Fellow Looking for Postdoctoral Scholars in cancer epigenomics, immunology, and metabolism Los Angeles, California University of Southern California CCMB Dechen Lin Lab Group leader We invite applications for a Group Leader in next-generation genome editing tools and technologies for innovative and translational applications. Vilnius (LT) Vilnius University Life Sciences Center Associate or Senior Editor (Social Impacts of Climate), Nature Communications The Associate/Senior Editor role is ideal for researchers who love science but feel that a career at the bench isn’t enough. London, New York City, Philadelphia, Jersey City or Washington DC - Hybrid Working Model Springer Nature Ltd Faculty Positions& Postdoctoral Research Fellow, School of Optical and Electronic Information, HUST Job Opportunities: Leading talents, young talents, overseas outstanding young scholars, postdoctoral researchers. Wuhan, Hubei, China School of Optical and Electronic Information, Huazhong University of Science and Technology",
    "commentLink": "https://news.ycombinator.com/item?id=39642135",
    "commentBody": "Superconductivity scandal: the story of deception in a physics lab (nature.com)137 points by rntn 18 hours agohidepastfavorite87 comments levocardia 16 hours ago>Students say that Dias gave them an ultimatum: remove their names, or let him send the draft. Despite their worries, the students say they had no choice but to acquiesce. “I just remember being very intimidated,” one student says. The student says they regret not speaking up more to Dias. “But it’s scary at the time. What if I do and he makes the rest of my life miserable?” Indeed it is scary, because as a student your PI can in fact ruin your life and ruin your career, without much recourse in most cases. This behind-the-scenes is a good insight into the bad incentives and lack of accountability for PIs in academic research. I suspect this kind of thing happens all the time in smaller fields, and never gets discovered because it's not a headline-grabbing topic like room-temp superconductivity. I had a bioengineering PhD friend who said she spotted obviously fraudulent papers all the time in her field--publications claiming they grew such-and-such bacteria in so-and-so medium, when she knew damn well that said bacteria cannot grow in said medium. reply araes 16 hours agoparentRather common question on Academia StackExchange for exactly the scary reasons mentioned. Almost every time, the rational is something close to \"my professor will ruin my life if I don't do what they say.\" My own research is creepy and I want to leave (except my professor will ruin me): https://academia.stackexchange.com/questions/67897/i-dont-wa... My professor is using us to rig the IEEE elections (except afraid of losing scholarship and grad courses): https://academia.stackexchange.com/questions/69486/what-shou... My professor is rigging nanotechnology data and plagiarizing (except I don't stand a chance against him): https://academia.stackexchange.com/questions/31265/my-profes... reply worik 11 hours agorootparentThose are terrifying What a mess. After the \"metrics\" revolution we started to measure and incentivise everything. It has not worked well for science. reply oldgradstudent 7 hours agorootparentMeasuring the wrong thing, even if it is highly correlated to what you really want to measure, often results not in a slightly suboptimal result, but in the worst possible result. reply Log_out_ 1 hour agorootparentIt's not a failed system ism.. But the sad truth is, we should go back to the 1800 were proof was travelling with others to the lab and watch the experiments happen life. Reproduction by the author under expert supervision and with a camera recording of the whole ordeal. That and reputations trees. The tree that spawned this rotten apple, root to roof, has spawned many more. reply userbinator 4 hours agorootparentprevThe first one is odd, but far less scary than the other two. reply paulddraper 14 hours agorootparentprevThat's unfortunate. There are bad professors in academia just like there are bad bosses in industry. Hopefully anyone with bad ones will find better ones. reply halfdan 14 hours agorootparentThe big problem in academia very often is that you cannot simply take your research and finish your PhD elsewhere. With jobs you mostly can switch without losing much. Obviously depends on the situation. reply paulddraper 13 hours agorootparent> With jobs you mostly can switch without losing much Seniority, bonuses, equity vesting, resume damage reply BeetleB 11 hours agorootparent> Seniority, bonuses, equity vesting, resume damage Relatively minor. Most senior people I know who leave with bad blood end up with equally senior roles, and often \"fail upwards\" to even more senior roles in another company. Bonuses/Vesting: Very common to get good vesting at a new company to make up for what you're losing. Of the people I know who've moved, it's the exception that didn't get decent replacement for vesting. Resume damage: What? Person spent 4 years at a company and left. What resume damage? Your experience still counts. Contrast with a PhD who leaves in the middle of a program: The years he spent are 0. Generally, the experience/knowledge accumulated is not factored into it when he applies elsewhere for a PhD - unless he has some fantastic publication. Still, this is the good outcome - I've often seen students leave an advisor or school and succeed elsewhere. The real danger is when you get your PhD and the advisor will not write letters of recommendation. Unless you've made some really special connections, it means your academic career is over. You won't get a post doc. You won't get an academic position. You won't get into a national lab. And depending on your field, you won't get a research position in industry either. In the industry, no boss has that power. reply JanisErdmanis 10 hours agorootparent> Contrast with a PhD who leaves in the middle of a program: The years he spent are 0. Generally, the experience/knowledge accumulated is not factored into it when he applies elsewhere for a PhD - unless he has some fantastic publication. When I relised this kind of power inbalance during my PhD resulting docile behaviour of my collegues and partly myself, I even did not even wanted to chage my supervisour nor make a protest as the power inbalances are normalized. This awareness made my PhD easier as I did not put false hopes and did not invest my leisure time into becoming more specialised or come up with my own ideas which could latter be turned down to not allign with funded project as my collegues did. I found the remedy in fight for a better tooling and investing my leisure time in completely different field in which papers were still not been written by robots. reply quickthrower2 9 hours agorootparentprevThose are a thing but probably not as bad. You may gain seniority. The second is unavoidable. The last can be dealt with and there are levels of ethics around that. reply analog31 4 hours agorootparentprevI think it's much worse for a grad student. It may have to do with the cultures of industry and the academe. In industry, competitive hiring and job-hopping are considered to be normal. Confidentiality is assumed, for the most part. There are some stock, face-saving reasons why you're leaving your previous job, that don't open cans of worms. There are lists of questions that you don't ask. And It's not weird if you're changing jobs after a couple of years. Your seniority and promotion clocks don't typically get reset to zero. reply bsder 12 hours agorootparentprevYeah, that doesn't work. Once you are in a PhD program you are locked in for 5+ years. It's almost as bad as the green card timeline. reply stared 15 hours agoparentprevPeople in the industry think that doctoral advisors are something like bosses in companies. They aren't. A Ph.D. is an all-or-nothing experience. When leaving on bad terms, you not only lose a few years of your life, but it is likely to ruin your career. With very competitive hires, an anti-recommendation might be a kiss of death. Sometimes, this power balance pushes people over the edge. There is a case of a PhD student who, after a struggle with his PI wanting him to publish fraudulent results, had a breakdown and took his own life. https://huixiangvoice.medium.com/the-hidden-story-behind-the... reply cma 14 hours agorootparentFor foreign PhD students their kids can be ripped out of school and they and their family deported if they can't get another advisor within 30-60 days depending on the visa. reply BeetleB 11 hours agorootparentNope. A student's ability to stay in the US (along with his dependents) is by remaining a student at the university (i.e. registering for a full time load each semester). They're not dependent on whether they have funding/assistantship, or whether they have an advisor. reply HarryHirsch 10 hours agorootparentIf you are a doctoral student you need a new advisor to remain in status. This is playing out at my institution right now, a female PhD student had had enough of the sexual advancements of her advisor, who is a favourite of the departmental chair, so she filed a Title IX complaint and needed a new advisor. There was exactly one faculty member who was in a position to take her on, an established figure close to retirement age. US immigration law is fucked up beyond reasonable. reply BeetleB 10 hours agorootparentWhat is going on is this: To remain in the US on a student visa, you need to be \"in status\". The government has rules on what that is, and mostly it is that you continue to register for a full load. I do not think the government has specified additional requirements. However, each university has an international programs office and they are required to specify for each international student whether the person is compliant with the university's guidelines. In your university's case, that office may have decided that \"Hey, if you're a PhD student, you can't go too long without an advisor, because that would mean you're not making progress towards a PhD.\" I've seen them say similar things if it's taking too long to get a degree (e.g. over 6 years for undergrad or 8 years for PhD). Or if the GPA is too low and you're on probation Typically, these rules apply to all students, and not just international ones. It's essentially the university saying \"You're about to be kicked out/expelled, so we can't tell the immigration folks that you are progressing towards your degree\" Still, I really doubt your university has a rule specifying only 30-60 days to find a new PhD advisor. To give you an idea, in many sciences, it's common that you don't have a thesis advisor until after your first year (you focus in the first year on passing your qualifying exams, and only once you pass it are professors interested in taking you on). If indeed your university is enforcing a 30-60 day rule, it's a really crappy university. TLDR: The law doesn't require it, but each university has its own policies. reply HarryHirsch 8 hours agorootparentIt's essentially the university saying \"You're about to be kicked out/expelled, so we can't tell the immigration folks that you are progressing towards your degree\" Yes, exactly that, it was something along the lines of \"you are not sleeping with me, consequently you are not progressing to your degree\". reply jcranmer 16 hours agoparentprevYep, when I was reading this article, as I kept hearing about what Dias was doing to his students, I was getting ready to throttle him. The students here didn't really do anything wrong, but Dias used his position of authority to essentially blackmail the students into becoming complicit in fraud, and attempted to justify it essentially with an \"everybody does it\" excuse. Kudos to them for taking the initiative to retract their paper over the head of their PI! That takes a lot of courage, given the immense powers an advisor holds over their students. reply strangattractor 16 hours agoparentprev> Soon after the CSH paper was published, Jorge Hirsch, a condensed-matter theorist at the University of California, San Diego, began pressing Dias to release the raw magnetic-susceptibility data, which were not included in the paper. More than a year later, Dias and Salamat finally made the raw data public. Explain to me why Nature would publish a paper without having access to/requiring the data. Could it be Nature wanted to be the first to publish such a ground breaking discovery? Dishonest people are simply a fact of life. The entire point - supposedly - of peer review is to insure that scientist do not make unsubstantiated claims. Job well done Nature. Job well done. reply dekhn 11 hours agorootparentNature very specifically wants to be the venue where the most important and impactful discoveries are first reported, and they tolerate a very high false positive rate to achieve it. Quick, light peer review is part of that process. We always joked that if you read it in Nature, it was certain to be wrong, but realistically, it meant that the topic was likely an important one with a small number of competitive groups who could all evaluate each other's papers and will loudly complain about every inconsistency or irreproducible detail. reply 0cf8612b2e1e 15 hours agorootparentprevPeer review is not designed to detect fraud. It is meant to improve the quality of work, by giving a publication a fresh set of eyes who might identify gaps in the original research. reply pdonis 10 hours agorootparent> Peer review is not designed to detect fraud. Maybe it should be. \"Peer reviewed\" is taken as a sign of reliability in many contexts, including public policy. If it isn't actually such a sign, then either it needs to change so it is, or we need to stop taking it as a sign of something it isn't. reply 0cf8612b2e1e 8 hours agorootparentI am not sure you appreciate the scope of distrusting submissions. Does this mean the reviewer has to clean-slate replicate everything? What if the author spent three years breeding a particular mouse model? Purified 8pg of an antibody extracted from flea tears? Spent six months of HPC time to simulate molecular dynamics? Some science is hard and incredibly difficult and expensive to do. Even just reevaluating a submitted dataset is no guarantee. Someone committing an elaborate fraud could have faked the raw datasets as well. reply pdonis 7 hours agorootparent> I am not sure you appreciate the scope of distrusting submissions. What you're saying is, the first option I gave, to fix the peer review process so it is a good indicator of the reliability of research, is not feasible. If that's the case, that just means we have to pick the second option: stop treating peer reviewed research as if it is reliable. reply kergonath 3 hours agorootparent> If that's the case, that just means we have to pick the second option: stop treating peer reviewed research as if it is reliable. It is the case, and that second option is exactly what we do. One article is nice, but replication is key before some new finding is accepted. That is exactly what happened with LK-99, for example, which is interesting because it was very public. reply beowulfey 4 hours agorootparentprevThat's actually exactly correct. Peer review is supposed to characterize the research in terms of quality, not validate it. Otherwise, those peers who validate it would practically be co-authors. It's a system built upon honesty. A dishonest person is obviously going to take advantage of it. Really blatant things are caught in peer review, but sneaky fraud will definitely slip through. reply strangattractor 15 hours agorootparentprevNot having data is quite a significant gap. BTW: I am not criticizing the actual reviewer of the paper. Data availability should be a policy of the journal and required for publication. How can you improve quality without access to the data? In this instance requiring data would have likely prevented the paper from even reaching peer review and wasting a busy individuals time. reply BeetleB 11 hours agorootparent> Not having data is quite a significant gap. Requiring (raw) data is relatively new, and is only in a few fields. For most of the history of peer review data was not required. In fact, none of the PIs I worked in during grad school would ever agree to releasing the raw data. I'm pretty sure they're still not in most of physics. IMO, it's reasonable to require it, but understand that this is a relatively new requirement and not part of peer review culture. reply worik 11 hours agorootparent> Requiring (raw) data is relatively new, and is only in a few fields. That on its own is scandalous I was reading of the \"reproducibility crisis\", in computing, in the nineties Science still wants to be the warm friendly club, whilst playing competitively for high personal stakes. Cannot be both. Fraud, deliberately cynical fraud and hopeful naive fraud, has been rampant for decades The journals benefit directly and seem to be utterly complicit. The system is dreadfully broken Nobody should wonder why people do not trust science reply ants_everywhere 7 hours agorootparent> Nobody should wonder why people do not trust science Taking an iterative approach where you make a prediction, test the prediction, and adjust course in response to the test (AKA science) is literally the only possible thing that can work to discover true things about the world. So if someone distrusts science they're basically giving up on a belief in objective reality. But I agree it's not surprising that people don't trust science, any more than it's surprising that people believe in astrology, lizard people, or ESP. Human minds don't have a special science sensor that lights up when they read true things. It's pretty much garbage in garbage out. Don't miss the story here, which is that the study failed to replicate and the fraud was detected. In many human endeavors there are frauds that have persisted for centuries. > That on its own is scandalous Yeah it is, I agree. Providing data should be mandatory to the reviewers. It should IMO also be provided publicly if feasible (at least upon request if it's financially burdensome to host) if the work is publicly funded. reply imchillyb 8 hours agorootparentprevScience is a method. Science isn't a religion, nor is it a paper, thesis, or journal. It's a method for understanding the world around us through rigorous application of testing and retesting. Science can be trusted. Measurements and the testing and retesting can be trusted. It's people that can't be trusted. We call people in the field of Science, Scientists. Perhaps we should stop labeling fallible and untrustworthy humans as such. Science is trustworthy. Humans are not. reply ants_everywhere 7 hours agorootparent> Science can be trusted. Measurements and the testing and retesting can be trusted. It's people that can't be trusted. This is an important point and I think people should make it more often. A human scientist who is committing fraud is misrepresenting facts about the world. And the way you would accurately describe those facts is with science. It's not that science is in the wrong here, it's that the human is lying about what the science says. You want to be able to say \"objectively, this guy lied about his research.\" But if you want to say that, then you need to be able to accurately describe both the world and the research so you can compute the diff between them. So if you throw out the science with the human scientist, then you've thrown out any objective means by which you were going to say the human scientist did anything wrong in the first place. reply 0cf8612b2e1e 15 hours agorootparentprevThat was one particular dataset being requested. There are near infinite ways to characterize a material, and even the best lab is only going to have the time, resources, and expertise to collect and present some of them. I am not in the field, so I cannot comment on the specific relevance of the request, but in my area, there are absolutely some mandatory data inclusions which should block a paper from being published. Presumably this was not on that list. reply strangattractor 14 hours agorootparentThere are numerous ways to embargo data and make it available to reviewers before publication and to the public after publication. [1] This isn't a new idea. They simply did not do it. Nature is a \"premier journal\" known for publishing cutting edge research. Any data - in this case a bunch of numbers - used to support the conclusion should be made available. I am sure there are disciplines where the evidence would be difficult to provide directly - fossils, physical items etc. Maybe this PI did the experiment himself - thought he won a Noble - only to find he could not in fact reproduce the results after publication - oops. IMO Nature did not do its job. Just pointing that out. [1] https://plos.org/resource/how-to-store-and-manage-your-data/ reply tornato7 15 hours agorootparentprevI would like it noted that I had Jorge Hirsch as a professor many years ago, and his grading was extremely harsh - you needed to be flawless at showing your work, not just getting the right answer. I remember getting a 14% on one of his tests, the worst score I ever got on any test in my life. It turned out to be a B+ after the curve was applied. Hence, it doesn't surprise me at all to see that he was the one to call out Dias. Some things never change! reply obviouslynotme 8 hours agoparentprevIt happens in all fields, at all levels. You cannot trust any papers without reproducing them. The intentional fraud is rampant. There is too much reward for publishing junk as quickly as possible and too little punishment for misbehavior. reply dotnet00 17 hours agoprevIn case others are coming into this thinking about LK-99, the article isn't about that. I had figured the LK-99 team might actually have something interesting considering that they were promising a proper presentation of their work at the American Physical Society. I had heard it had been withdrawn and wrote it off as another case of academic fraud, but looking it up right now, apparently it's weirder than that. Someone pretended to be the corresponding author and submitted a withdrawal, but this was fixed and the conference the presentation is at should be happening around these days. reply adastra22 11 hours agoparentLK-99 is interesting because it really is like another example of cold fusion. And I don't mean what you probably infer from that. If you look into the way that hydrogen adsorbs into a platinum lattice, and how phonon resonance modes can induce tremendous crushing forces on anything between two platinum atoms, cold fusion actually looks quite plausible. It's not like perpetual motion--there's a clear possible mechanism for it which is surprisingly fully compatible with the laws of physics as we understand them. That said, Pons and Fleischmann rushed publication on shoddy data, making a mockery of themselves and causing cold fusion to become an untouchable subject no one would go near a generation later due to perceived guilt by association. The theory behind LK-99 is pretty solid. The linear structure of these apatite compounds create chains of conductive electron bands that in theory might have sufficient band gaps to enable superconductivity. If you could get atomic precision in the copper/lead alternating pattern that creates those stresses. However I fear any research into room-temperature linear superconductors will now be as taboo as cold fusion. reply worik 11 hours agorootparent> That said, Pons and Fleischmann rushed publication on shoddy data, making a mockery of themselves and causing cold fusion to become an untouchable subject no one would go near a generation later Yes, they were shoddy But no, the field was swamped within weeks with people trying to replicate it. It has lost its luster, and become a parody, but that is because it was wrong It took a decade for the excitement to die down reply adastra22 8 hours agorootparentPons and Fleischmann's experimental setup did not produce cold fusion. But some other variation or different catalyst material might. Yet effectively no one has carried on this work. LK-99, or at least the manufactured samples of it, is not a superconductor. But some other variation or different manufacturing technique might. Yet I predict no one is going to look very hard in the years to come. reply kergonath 2 hours agorootparentprev> If you look into the way that hydrogen adsorbs into a platinum lattice, and how phonon resonance modes can induce tremendous crushing forces on anything between two platinum atoms, cold fusion actually looks quite plausible. No, it is not. These things are well understood. The behaviour of hydrogen in crystals is studied a lot. In metals because of issues with hydrogen embrittlement and fusion stuff, in some oxides for the storage of hydrogen for energy applications, and in material in general because proton irradiation is a common tool when studying them. All of this to say that this is not something that was never investigated. > It's not like perpetual motion--there's a clear possible mechanism for it which is surprisingly fully compatible with the laws of physics as we understand them. The orders of magnitude just do not match. The pressures involved is typically 1 GPa (usually less in metals, more in ceramics). That is far from enough to bring nuclei sufficiently close for fusion. There is a reason why nobody claims this, and it is not because we don’t look at the behaviour of hydrogen (or other light nuclei like deuterium, tritium, helium, or lithium for that matter). > The theory behind LK-99 is pretty solid. The linear structure of these apatite compounds create chains of conductive electron bands that in theory might have sufficient band gaps to enable superconductivity. Again, it is not. Some preliminary electronic structure calculations showed interesting features, but these features were no indication of superconducting behaviour, and indeed LK-99 is not a superconductor. There were plenty of people who tried to reproduce the results and it clearly showed that the initial paper was wrong. > If you could get atomic precision in the copper/lead alternating pattern that creates those stresses. Our understanding of what causes superconductivity and the creation of Cooper pairs is not complete. Otherwise, we’d design materials with the right properties from the beginning, rather than testing a lot of stuff just to see what works. > However I fear any research into room-temperature linear superconductors will now be as taboo as cold fusion. Contrary to cold fusion, superconduction is actually a thing. Current high-temperature superconductors work at the temperature of liquid nitrogen; finding one that works 100K higher is not that much of a stretch. The gap is not 6 orders of magnitude like with cold fusion. In fact, there is quite a lot of funding for research on high-temperature superconductors even this year so your fear is not really well founded. reply DarmokJalad1701 16 hours agoparentprevIt already happened a few days ago (March 4th). They showed more results, but nothing ground-breaking. There is another group that is also replicating the results and publishing results (SCT Lab or something). Their paper is currently pending. reply progrus 9 hours agorootparentThey’re waiting for Arxiv to approve because patents may be denied if they go around that system. reply lykahb 12 hours agoprevWhat is the point of fabricating data here? An extraordinary claim about room-temperature superconductivity is going to receive a lot of scrutiny that it wouldn't stand. It's not an average publication made to pad up the H index and meet the dept requirements. reply Vegenoid 10 hours agoparentI was wondering the same, and it seems like the only way it makes any sense is if you think that you really are on to something, and you want to make sure that nobody beats you to the punch. So you fudge the numbers and publish it, hoping that you, or maybe even someone else, makes it actually work, and your name gets associated with the discovery. I'd love to hear any other possible explanations for this behavior, because it seems to delusional to fabricate this and think you wouldn't get caught. reply rgmerk 8 hours agorootparentIt’s very common for researchers to be convinced of things they can’t currently prove. Why else are you going to spend years of your life bashing your head against a brick wall trying to prove it? Sometimes that belief pays off spectacularly. Often it results in a junior researcher’s career going down the toilet because they or their supervisor backed the wrong horse and they ended up with nothing exciting to publish. reply fabian2k 15 hours agoprevShowing your co-authors the manuscript for the first time a few hours before you submit it to Nature is not how it's done, not at all. That is really unusual and a giant red flag, though the PhD students were probably not experienced enough to judge just how weird this was. And I could imagine that in that field the PI could convince them e.g. by stating they wanted to avoid getting scooped or something like that. reply sega_sai 9 hours agoprevOk Dias is an a**hole (especially for manipulating students), but Nature is at fault here. They chose to publish those papers, ignoring referee/data concerns. reply bluenose69 14 hours agoprevAlso, an analysis shows that Dias' thesis was largely (21 percent) plagiarized [1, 2]. 1. https://www.science.org/content/article/plagiarism-allegatio... 2. https://www.science.org/do/10.1126/science.adi2603/full/dias... reply perihelions 17 hours agoprev- \"203 K (70 °C)\" 203 K is -70 °C, not +70 °C reply evilduck 17 hours agoparentMaybe the author was an accountant? reply eichin 2 hours agorootparentOh, is this another \"excel vs. scientific communication\" thing? ( https://support.microsoft.com/en-us/office/negative-numbers-... and https://support.microsoft.com/en-us/office/change-the-way-ne... ) reply eichin 2 hours agorootparent(looks like no, the linked article currently says \"at 203 K (−70 °C) and at extremely high pressures\" so it was more likely just a typo/transcription error) reply wyldfire 16 hours agorootparentprevWouldn't they put ((70 °C)) in that case? reply dekhn 11 hours agorootparentno, they write it in red reply WalterBright 11 hours agoprevHow did anyone think the deception wouldn't be promptly found out? Too many people would rush to replicate it. reply rwmj 15 hours agoprevSurely with something as concrete as a substance, you accept the paper to give it a priority date, and then you wait on publication until an independent lab has verified the claimed properties? It either is or it isn't a room-temperature superconductor. Of all the problems in science, this seems like one of the easier ones to fix. reply dotnet00 15 hours agoparentWho's going to pay for the independent replication, especially if the result hasn't been published? Peer review is done for free because it's relatively less effort compared to a replication, and even then it's only barely tolerable. reply rwmj 15 hours agorootparentThere's a big dispute about who pays the cost of publishing papers these days, so throw it in that bucket. reply dotnet00 15 hours agorootparentIt costs a lot less to publish a paper compared to replicating a study. reply rwmj 15 hours agorootparentI didn't say replicate the study, I said test if the material is a superconductor, which isn't nearly as difficult. reply dotnet00 14 hours agorootparentThat is replicating the study when the study is about the superconducting properties of a material. I think the LK-99 thing made it pretty clear that it is pretty difficult to do, there are lots of variables and lots of things that need to be ruled out to prove that something is superconducting. reply rwmj 14 hours agorootparentWhat are you talking about? Dias claimed to have measured the properties of the material he had made. The material exists, who needs to replicate anything? Send the material that was made to a lab where they'll probe it with a multimeter and wave a magnet at it. (I bet the reviewers of the paper could do this actually.) reply kergonath 2 hours agorootparentNobody is going to do this on their free time. We have our own work to do, and our own milestones to keep funding bodies happy in order to be able to keep doing research. So you’d need to set up a system from scratch to pay for (sometimes very expensive; a neutron source does not come cheap) replication studies. And then you get perverse incentives because reviewers can benefit financially or materially from this. The way it works now is already reasonable, actually. One paper is not a proof of anything, it is a claim. A ground-breaking article starts a gold rush of people who want to be where the action is, which leads to a lot of follow-up or exploratory studies, which leads to independent verification and replication. At this point, the initial claim starts being accepted. For all the noise in mainstream media, something does not become the state of knowledge after a single article. This is actually quite robust because that system works even if individual articles are unreliable. reply wnevets 17 hours agoprevIIRC people assumed it was a mistake when the claimed turned out to be false I didn't realize he was a career conman. reply fifnir 17 hours agoprevHow amazing is it to read all this on nature itself, the journal whose peer-review method failed again and again. reply dsign 16 hours agoparentExactly. It's bad motivations all the way: - Nature published a paper twice after the reviewers voted \"no\". Of course, it is not a voting process, the editor makes their decision. But evidently the editor didn't use the referee's input for that decision. That input, must I say, is free work that academics do for the journal. Click-baitiness simply weighted more to the editor and her supervisors. People should just stop paying attention to what Nature publishes. - The students... I think this is the worst part of the story. Sometimes, students are on a student visa that depends on them holding for dear life to their university position. Been there, done that. I remember when I did my PhD: four to five international students--very well prepared I must say--to one from the host country. The ones from the host country were the ones with the best soft skills but they rarely put the long hours. I'll never forget that Iranian student who was a political refugee passing as a PhD student, because she couldn't bear the indignity of an asylum process. She couldn't return to her home country either. - The replication...For the first retraction, I could find nowhere in the article that replication attempts were made. If those replication attempts were impossible to produce with the data published in the article itself, then its not a scientific paper, but a promotional brochure. In fact, some promotional materials are more extensive than Nature's articles; they really want to make the thing very short and academics are forced to jump through hoops to condense their publications, until they are practically impossible to understand, not to mention replicate. Again, the only solution to that problem is to ignore Nature. reply araes 16 hours agorootparentThe people writing the Nature article did not seem to notice the issue. The third part especially was what was occurring to me while reading. The LK-99 paper, lots of attempts at replicating the results. Crazy quick. Like front-running the published results. These results, like people never even had the thought to try, and it devolved into arguing sampling issues rather than just trying the material. There is some certain sub-segment of humanity that often feels that way. Like 8B humans will all bend over backward to let them get away with almost anything. Totally abandon all normal procedures, safety checks, laws, replication, validation, ... reply londons_explore 15 hours agorootparentprevNature should simply require the main article be short and snappy, but then put all the details needed for a replication in an online-only appendix. reply StressedDev 2 hours agoparentprevPeer review is not a magic thing which can detect all mistakes and prevent all fraud. The biggest problem with peer review is people assume it means published research is correct. I do not think that peer review can do that. I think peer review at best gives good feedback to honest researchers and catches some mistakes. Here are my reasons why I think peer review will not reach a higher standard. 1) Peer review is hard work and probably tedious. In order to do a really good job, someone would have to spend tens or hundreds of hours reviewing a paper. The best reviewers would actually replicate the research. This probably will not happen often. 2) Peer review is not rewarded. No one is going to get tenure for being an excellent reviewer. 3) Good reviewers may even be punished. Unfortunately, many humans do not like critical feedback and hate having their work invalidated. Some of these people take revenge, and \"shoot the messenger\". I think the solution is to stop thinking something is correct because it is published in a peer reviewed journal. Instead, people need to recognize that while the scientific process may eventually get the right result, it often makes mistakes, and is not perfect. reply dgellow 1 hour agoparentprevThe linked article is on Nature news. It’s a different team from Nature scientific articles. It’s mentioned in the linked article and they do some investigation in Nature’s own failures. reply semi-extrinsic 16 hours agoparentprevFun fact: Nature used to publish papers without any peer review whatsoever right up to the mid-60s. The selection was based on what the editors thought was interesting or provocative. Legend has it that the submissions tracking system consisted of a particularly wide windowsill, in which the submitted manuscripts were stacked up in piles for each month. reply autoexec 15 hours agoparentprevFor all the problems that Nature (and science in general) has, and while I'm sure this incident will be used by some as \"evidence\" that we shouldn't trust in science, ultimately this incident was another win for science. It should have happened much much sooner, but in the end, the truth was found and the record corrected. That's exactly the result we want. I appreciate that Nature's news team is willing to publish information about Nature's massively embarrassing failure to do their job reviewing the paper, and I hope they mean it when they say “We are looking at this case carefully to see what lessons can be learnt for the future.” reply dudeinjapan 14 hours agorootparentIt's a \"win\" for science in the sense that the truth was finally revealed. It's \"loss\" in the sense that our truth determining apparatus (e.g. peer review) appears to be highly unreliable, and a massive, unquantified amount of bullshit research has already passed it and is actively being used as a foundation for subsequent research. reply trogdor 4 hours agorootparentPeer review is supposed to review the quality of the research. It is not supposed to validate the findings, let alone detect fraud. reply dekhn 11 hours agorootparentprevRealistically, we can't reduce false positives to absolutely zero without giving up on some True Positives (sensitivity/specificity tradeoff). Nature intentionally plays loose with potentially world-changing discoveries. It probably makes sense to have some journals that publish with fairly high rates of (post-hoc determined) false positives, simply to move the fields forward more quickly. reply worik 11 hours agorootparentprev> and while I'm sure this incident will be used by some as \"evidence\" that we shouldn't trust in science How else can it be interpreted? Most fraud uncovered has been careless. So the fastidious fraudsters are flying high? That would be the logical conclusion Science needs to win its credibility back, not claim this is \"...[a] win for science.\" Science needs to be realistic about its decaying social license reply autoexec 8 hours agorootparent> How else can it be interpreted? That although scientists are people and so there will always be some who try to cheat and lie, those lies will eventually be found out and when that happens the record will be put right. It shows that science is not only non-dogmatic in the sense that new information which challenges or improves on our previous best understanding of a given topic will be used to update or replace the old, but that even when there is no new evidence, new research is still being openly challenged and tested by other scientists. Scientific journals have their problems, but finding and correcting mistakes, and being transparent about the correction and about how those mistakes were made, are all indications of credibility. Even better, in this case, the alarms were sounded very very early and yeah Nature really fucked up by ignoring them for as long as they did, but there were people who cared about the truth who were persistent and in the end Nature did the right thing. That's says something really good about science. The replication crisis is still a big problem. Lots of less dramatic results are probably false and will likely go unchallenged for a longer time, but that's just something we have to consider when deciding how confident we are in those results. Results which have been independently verified multiple times are those we can be more confident in, results which haven't been verified should be taken with a much larger grain of salt. It'd be silly to take examples like this as a sign that science as a whole shouldn't be trusted. reply lyu07282 4 hours agorootparentprev> truth was found and the record corrected. That's exactly the result we want. I hope science can one day overcome people like you and actually address the systemic issues it is facing. Denial that there is anything really wrong, that ultimately everything works fine so nothing has to fundamentally change, is exactly the problem that causes those systemic failures we are seeing in science and academia all the time. This is exactly why Nature can write about their own failure, because there are no expectations beyond \"hoping\" and trusting they change. But of course nothing will ever actually change. reply kergonath 2 hours agoparentprevIt’s not the same people. As they wrote in the article, journalists are not involved in peer-review. reply pfisherman 16 hours agoparentprevNature and Science are kind like the tabloids of scientific publishing. Legitimate news mixed with unsubstantiated gossip; all with a high “sexiness” factor that draws attention and moves copy. reply 1024core 17 hours agoprev [–] \"Publish or perish\" takes yet another victim. reply StressedDev 2 hours agoparentNot really. Publish or perish does not force academics to commit fraud or to abuse their subordinates. It does not force them to behave unethically. Professor Dias choose to lie, choose to abuse, and choose to submit papers with fake data. I agree publish or perish may not be the best system but we should not excuse people for being corrupt. People can and do chose to behave ethically. Those who don't should be punished. reply Simulacra 17 hours agoparentprev [–] It could be worse than that. How did he get into the program in the first place should be examined. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Physicist Ranga Dias' claim of discovering room-temperature superconductors was retracted due to data manipulation and misconduct allegations, impacting young scientists' careers and prompting questions about institutional oversight.",
      "Dias' research papers on superconductivity faced scrutiny for lack of transparency and data manipulation, leading to investigations by multiple journals and persistent doubts about research validity.",
      "The fallout includes personnel actions, loss of lab access, and potential NSF grant impact for Dias, with related research facing retractions or corrections, causing concern in the scientific community."
    ],
    "commentSummary": [
      "The article addresses deception and power dynamics in academia, focusing on a physics lab scandal and the lack of advisor accountability for PhD students.",
      "It highlights challenges like dealing with bad advisors, job security, and immigration laws, emphasizing transparency in scientific research and the importance of reproducibility and data availability.",
      "Specific cases like cold fusion and room-temperature superconductors are discussed, emphasizing the significance of replicating and validating research findings for maintaining integrity and credibility in scientific research."
    ],
    "points": 137,
    "commentCount": 87,
    "retryCount": 0,
    "time": 1709912794
  }
]
