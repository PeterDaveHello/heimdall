[
  {
    "id": 40817852,
    "title": "Software galaxies",
    "originLink": "https://anvaka.github.io/pm/",
    "originBody": "Welcome to the Code Galaxies, Commander Choose your destination: Bower Client side JavaScript package manager Composer Package manager for PHP RubyGems Package manager for Ruby npm JavaScript package manager Go Search Packages from Go language (based on go-search.org) Python (PyPI) Python Package Index NuGet Packages from .NET world R Language CRAN - network of R packages Debian Debian Package Manager Fedora Fedora Packages Arch Linux Arch Linux packages without AUR Arch Linux + AUR Arch Linux packages with AUR included Brew Homebrew is missing package manager for OSX Rust Rust package repository (crates.io) Elm A delightful language",
    "commentLink": "https://news.ycombinator.com/item?id=40817852",
    "commentBody": "Software galaxies (anvaka.github.io)511 points by matesz 14 hours agohidepastfavorite95 comments wiz21c 9 hours agoWhen you imagine that each dot is a program and that behind each of these dots, there is at least on person, it gives a very good appreciation of how complex each of these projects are. These are pretty big human architectures.. reply ErigmolCt 6 hours agoparentAnd it emphasizes the immense human effort involved in these projects. reply sva_ 4 hours agoparentprevWould be interesting to see one for the Linux Kernel. Each include an edge on the graph reply martin-adams 8 hours agoparentprevGoing to be wild when AI starts to contribute to the code base on it's own reply pmontra 13 hours agoprevNavigating the galaxies is frustratingly hard. One finger touch moves forward, but it makes very hard to touch a point and see what it is. I keep selecting something past it, especially for large dots, which I'm curious to see what they are. Rotating the device changes the direction but it's hard to point towards a specific star. On the good side it's very nice to look at. I wish there would be something as fast as this for navigating real galaxies, with of course better controls. reply sva_ 4 hours agoparentIt seemed hard at first, until I decided to get up and pan around (looking like a fool). Imagine you're in a spaceship and pushing down accelerates it. It was fascinating how quickly this perspective gave me a sense of orientation. reply Varriount 13 hours agoparentprevAlthough I agree that navigation via device orientation makes some navigation aspects difficult, I also find it oddly fascinating. It's like my phone has become a window into another world. reply ErigmolCt 5 hours agorootparentI think I had the same feelings reply soraminazuki 3 hours agoparentprevIt was easy with a PC and keyboard. reply omoikane 12 hours agoprevThis looks very nice, but a 2D visualization might have been more practical. For example, the fact that the dot size represents the total number of dependents is obscured by the fact that the dot sizes are also a function of camera distance. reply zbendefy 12 hours agoparentReminds me of the 3D file browser user interface in Jurassic Park, which was an actual application. Looks cool but its not good to use (I mean the 3d file browser, not this software galaxies, which i found quite good). 3D interfaces rarely plan out, wonder if something like a vision pro or quest could make a 3D user interface work better than a 2D counterpart. reply _kb 8 hours agorootparenthttps://fsv.sourceforge.net There's a great write up at https://scifiinterfaces.com/2023/11/27/jurassic-park-1993/ reply aphrax 12 hours agorootparentprevIIRC it was an SGI application - very cool but not terribly practical! reply surfingdino 12 hours agorootparentTo be fair, it was all new back then and people were playing with ideas, so a 3d file browser seemed like a cool idea. A bit like the metal roller on the Paris Metro ticket machines https://www.youtube.com/watch?app=desktop&v=9SjBfRA3YzA reply dcminter 6 hours agorootparentThe discoverability on those things is definitely lacking. I think it took us five or so broken touch-screens before my wife noticed that you could use that to select menu options instead! I guess once you know it's fine though? Feels a bit dated compared to the typical touch & go card payments elsewhere in Europe now though. reply surfingdino 5 hours agorootparentI couldn't work it out for a good while, because it's the most unintuitive UI I have found on reasonably recent ticket machines. Once you know how to use it, it's ok. ProTip: if you travel from London on a train, the buffet sells Paris Metro tickets. reply grimgrin 12 hours agorootparentprevLooks like it's \"File System Navigator\" or fsn (fusion) https://web.archive.org/web/20160416092919/https://en.wikipe... Since removed, but still mentioned here: https://en.wikipedia.org/wiki/Graphical_user_interface#In_sc... reply Fnoord 4 hours agorootparentprevYes, it was a SGI application. Probably used in the movie Hackers. There was also a Doom file manager where you'd use BFG to nuke a directory. I only found one for Doom 3 but this also existed with original Doom. Nowadays, BFG is only used to nuke git repos. reply giobox 1 hour agorootparentDoom process managers where a thing for a while too, 20 years ago. Using the BFG on a crowded room of processes usually resulted in a system crash. Hunting down a stuck program and shooting it in E1M1 was pretty neat though. Your comment reminded me of playing with this in MacOS X a long time ago. > https://www.cs.unm.edu/~dlchao/flake/doom/chi/chi.html reply Medox 10 hours agorootparentprevCan be seen here (@8.02): https://www.youtube.com/watch?v=1PP--lVTPCQ&t=482s (A $36,000 Graphical Workstation from 1993SGI Indigo 2) reply p_l 9 hours agorootparentprevThere was a bunch of \"demo\" applications bundled in Irix, some more some less useful, that were used to showcase the capabilities of the systems. File System Navigator was, afaik, one of them (similarly there was bundled \"dogfight\", a networked flight simulator game). reply HPsquared 9 hours agorootparentprevIn VR, there was a wave of that kind of thing (3D productivity apps, file browsers etc.) None really took off though as far as I can tell. reply ethbr1 4 hours agoparentprevWhat's used to compute distance? I couldn't find any legend or description (mobile). Edit Ah, noticed the bottom-right about: https://github.com/anvaka/pm/tree/master/about#software-gala... Distance is seemingly arbitrary, decided by clustering algorithm. reply digging 3 hours agorootparentIt's weird, because there are (at least in the Rust \"galaxy\") several tiny, extremely distant constellations. I thought they were background decoration until I zoomed way in on them. Hard to image why they would be so distant if they're relevant. reply mdtrooper 12 hours agoprevI love these kind of things: - https://github.com/acaudwell/Gource : generate a beautiful and organic videos from git repositorios. - https://code.google.com/archive/p/codeswarm/ : similar to Gource . - https://skyline.github.com : it is dead, like as Atom . reply FrostKiwi 12 hours agoparentHell yeah. In our department we setup Gource to render out a video every midnight and pimped it out with a bunch of overlays and profile pics to show project progress and to visualize who worked on what. Shown endlessly looping on an iPad in front of the department, so no contributions are forgotten, especially the ones by interns who participated only a short while. reply gavinhoward 3 hours agorootparent+1 for providing the setup if you can. I love Gource. reply mdtrooper 5 hours agorootparentprevCool. Do you have public (in a git repo or something) this setup for Gource? reply tantalor 4 hours agoprevMy God! It's full of leftpads reply egorfine 1 hour agoparentis-even was the first package I have tried to search. reply yayr 9 hours agoprevjust to be a bit astronomically nitpicky ... ;-) they are more like star clusters than galaxies. Galaxies usually have a lot of mostly circular momentum with arms forming etc. might be even the better marketing term \"Software star clusters\" not to mention the widely accepted hypothesis that galaxies require dark matter to be held together... we don't want to dive into the analogy here for software, or do we? ;-) reply bregma 8 hours agoparentBut really it's the dark web that binds us all? reply HPsquared 9 hours agoparentprevNot to be confused with Github stars and their social dynamics. reply ahmadnoid 3 hours agoprevAm I the only one who is getting some sort of gambling site (go-search.org) when clicking on golang galaxy? reply SushiHippie 1 hour agoparenthttps://github.com/anvaka/pm/issues/44 Yep, seems like someone already opened an issue reply linux2647 3 hours agoparentprevThat’s what I get too reply dim13 10 hours agoprevAs for Go, the dataset looks very-very old and outdated. At least 5 to 10 years old. reply kkoncevicius 8 hours agoparentSame for R reply drofmij 37 minutes agoprevcan we do one for the java + maven repository galaxy? reply atonalfreerider 30 minutes agoparentprimitive.io has a VR browser for Java Maven, .Net, Node, Pip and PhP Disclosure: founder posting here reply theoa 12 hours agoprevWonderful! Want more. Every blob displays its icon Mouseover over displays much more stuff Right-click: the world is your oyster Ctrl-click: make a group, etc, much much more Ultimately: create 3D bash/OS/ reply martypitt 11 hours agoparentThis is a UNIX System! I know this! reply dirkc 10 hours agoprevWow, I love this. A long time ago I did some dependency graphs for gentoo linux packages [1] and also for a django project [2]. I put all the packages on a circle with dependencies being drawn as lines. This is so much cooler! [1] https://www.thebacklog.net/2011/04/04/a-nice-picture-of-depe... [2] https://www.thebacklog.net/2012/10/13/visualizing-lernantas-... reply me_bx 10 hours agoprevFrom the same author: * Related subreddits graph - https://anvaka.github.io/sayit/?query=linux * Map of reddit - https://anvaka.github.io/map-of-reddit/?x=18239&y=12514&z=32433.55559794627&v=2 reply BoppreH 8 hours agoprevThe gyroscope aiming on mobile is fantastic! I've never seen a demo with such small latency and responsive to small movements. Even more impressive by being a web page and not a native web. reply smartmic 9 hours agoprevImpressive visualization, for sure. But a honest question: What are real use cases of such a representation? I mean, can (and will) this be used in a productive manner for solving what kind of problems? reply ordu 9 hours agoparentThe only use I can imagine is to use it to write a guide on the available software. You can pick from the image clusters and make them into chapters in your guide or something like. reply ErigmolCt 5 hours agorootparentIt could be very effective in some cases reply Fnoord 4 hours agoparentprevSeems like a very useful way to navigate on a large touchscreen. reply throwaway55533 9 hours agoparentprevno. reply Kuinox 10 hours agoprevLots of star in the nuget galaxy, but there is not several package I worked on :(. reply peteforde 12 hours agoprevI'm a bit confused by the Rubygems visualization. Many popular gems appear to be missing, and the role of Rails in the ecosystem is something you could miss if you weren't explicitly looking for it. Cool viz, just not 100% clear what I'm looking at. reply marapuru 13 hours agoprevInteresting and very cool! But since navigating around is not easy, would it be an idea to implement a game like controller that allows you to move around? Current controls are not working so well. reply artpar 12 hours agoparentseems to be done in the same way, but the parameters are off. aswd (camera angle) + arrow keys(panning) works nicely when zoomed out but very sensitive when zoomed in. reply etwigg 11 hours agoprevsuper cool, but no jvm maven central? reply ivolimmen 11 hours agoparentYeah that is what I came to ask here as well. Also no p2... Edit: I see someone open an issue for it https://github.com/anvaka/pm/issues/2 reply quectophoton 3 hours agoprevLinks in the Go galaxy point to a casino page. reply vavooom 13 hours agoprevMaybe provide some insights on the main clusters identified? I think of this youtube video on Wikipedia Graph: https://www.youtube.com/watch?v=JheGL6uSF-4&ab_channel=adumb reply xnorswap 4 hours agoprevNuget has a lovely SampleDependency constellation. reply adityaathalye 10 hours agoprevThis is art! I wonder... What if the depth at which a package first appears depends on its release date? And what if each universe evolves in terms of package releases? reply witx 10 hours agoprevThis is such a cool visualization. It's so interesting to see that Rust's embedded libraries are on a more separate, dense, group. reply pyeri 12 hours agoprevOff topic, I still couldn't find an easy or seamless way to search GitHub repos by keywords (repo name, coding language, etc) and have them order by most stars descending. reply SushiHippie 1 hour agoparentWeird you are right I just tried this: https://github.com/search?q=lang%3Arust&type=repositories&s=... Filter by language to rust and then select sort by most stars, and the top repository has 249 stars... Though if I add a filter for stars greater than 1000 the results look way better: https://github.com/search?q=lang%3Arust+stars%3A%3E1000&type... reply rpgwaiter 11 hours agoprevThis is so cool! I’d love to see this kind of thing for nixpkgs reply matesz 14 hours agoprevhttps://github.com/anvaka/pm/blob/master/about/README.md reply zheninghuang 8 hours agoprevDoing a Research paper Galaxies would also be interesting, especially in the domain of AI. reply bombela 6 hours agoprevThe UX is garbage. It tracks my phone's motion, making it incredibly jittery (I guess I don't have the rock steady hands required?). And one finger starts an automatic zoom, while two fingers unzoom. reply jzer0cool 10 hours agoprevHow does this manage to plot so many points yet running pretty smoothly here on a low end computer browser? reply sva_ 3 hours agoparentI'd presume a WebGL particle shader reply rmorey 4 hours agoprevI love anvaka's maps! See also reddit: https://anvaka.github.io/map-of-reddit/ and GitHub: https://anvaka.github.io/map-of-github/ reply wing-_-nuts 4 hours agoparentI use his reddit graph all the time to find related subs. That one is 2d and imho is much more useful than a 3d visualization. Sad that it's probably not getting updated any longer due to reddit's apis no longer being available. reply visarga 11 hours agoprevWhere is CPAN, I don't see it. reply gregorvand 11 hours agoprevThis is very hard to understand reply cloudwalk9 13 hours agoprevI imagine Gentoo would be extremely difficult to visualize because USE flags add a 4th spatial dimension... reply edweis 13 hours agoprevBeautiful work reply robertlagrant 10 hours agoprevIncredible. The amount of effort that goes into each of those dots. reply jackcviers3 5 hours agoprevNo maven central? I imagine it would be pretty large, too. reply sachahjkl 3 hours agoprevno nix pkgs, what's even the point reply Nischalj10 2 hours agoprevthis is crazyyyy reply flkenosad 1 hour agoprevShit that's cool. reply ggm 10 hours agoprevBrew but not ports or pkgsrc reply kreyenborgi 11 hours agoprevFun. Needs haskell hackage :-) reply brandly 4 hours agoparentI wrote up some details about adding Elm packages to this if you want to do the same for hackage! https://mattbrandly.com/every-elm-package/ reply klibertp 3 hours agoprevIs it just me, my extensions, or are the controls broken in Firefox? reply numbers 1 hour agoparentmight be you, I am on firefox and things work fine. Press ? key to show the controls if you don't see them. reply viveknathani_ 5 hours agoprevlove this! reply ramesh31 4 hours agoprevHoly crap, Bower still exists? reply brandly 4 hours agoparentI think this is a 9 year old snapshot of Bower reply classified 9 hours agoprevI'm a bit disappointed that it has Homebrew, but not MacPorts, which is superior in my opinion. reply KolmogorovComp 12 hours agoprev [–] It seems cool but is completely unusable on mobile. It still amaze me how today people do not think about designing mobile-first website. The gap between devs and users is far from closed yet. reply Arch-TK 11 hours agoparentAssuming this was done in free time, for fun and posted here because it looks cool, why would you hold these expectations? This is the kind of expectations you should have of a commercial product that you're paying for. Not of someone's random side project. reply KolmogorovComp 11 hours agorootparentAs you’ve said given it's a project done in their freetime I don’t have any expectations. At the same time when I design a project I want to share to others (in my free-time too), I always think about making it working for the majority of the users (mobile in that case). reply neontomo 11 hours agorootparentI do too, but usually not until I've first validated the idea is interesting to people. Not much sense in optimising the wrong thing. reply mgnienie 10 hours agorootparentprevIt's for us geeks, not the majority :) reply gluke77 8 hours agoparentprev [–] How users (who are non-devs) are planning to use this piece, I wonder. Also is there any well established web-native way to navigate in 3d space, that works on mobile? Personally, quake-style keyboard only navigation on my desktop works like a charm. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Software Galaxies visualizes software projects as dots, representing each program and its complexity, sparking discussions on its challenges and potential improvements.",
      "Users debate the practicality of the visualization, with some suggesting it could serve as an educational tool and others reminiscing about similar tools.",
      "There is interest in expanding the datasets included, such as the Linux Kernel or Java Maven repository, highlighting the project's artistic and technical achievement."
    ],
    "points": 511,
    "commentCount": 95,
    "retryCount": 0,
    "time": 1719550493
  },
  {
    "id": 40815139,
    "title": "200 people charged in $2.7B health care fraud crackdown",
    "originLink": "https://apnews.com/article/justice-department-health-care-fraud-garland-24948b951896d0f265c29ba3fcacf858",
    "originBody": "U.S. Attorney General Merrick Garland announced Thursday, that nearly 200 people have been charged in a sweeping nationwide crackdown on health care fraud schemes with false claims topping $2.7 billion. Read More Photos 5 By ALANNA DURKIN RICHER Updated 9:53 PM UTC, June 27, 2024 Share Share Copy Link copied Email Facebook X Reddit LinkedIn Pinterest Flipboard Print WASHINGTON (AP) — Nearly 200 people have been charged in a sweeping nationwide crackdown on health care fraud schemes with false claims topping $2.7 billion, the Justice Department said on Thursday. Attorney General Merrick Garland announced the charges against doctors, nurse practitioners and others across the U.S. accused of a variety of scams, including a $900 million scheme in Arizona targeting dying patients. “It does not matter if you are a trafficker in a drug cartel or a corporate executive or medical professional employed by a health care company,” Garland told reporters. “If you profit from the unlawful distribution of controlled substances, you will be held accountable.” In the Arizona case, prosecutors have accused two owners of wound care companies of accepting more than $330 million in kickbacks as part of a scheme to fraudulently bill Medicare for amniotic wound grafts, which are dressings to help heal wounds. Nurse practitioners were pressured to apply the wound grafts to elderly patients who didn’t need them, including people in hospice care, the Justice Department said. Some patients died the day they received the grafts or within days, court papers say. RELATED COVERAGE A father who lost 2 sons in a Boeing Max crash waits to hear if the US will prosecute the company Trump lawyers in classified files case challenge prosecutor’s appointment at start of 3-day hearing Boeing CEO defends his safety record, spars with senators and apologizes to crash victims’ relatives In less than two years, more than $900 million in bogus claims were submitted to Medicare for grafts that were used on fewer than 500 patients, prosecutors said. The owners of the wound care companies, Alexandra Gehrke and Jeffrey King, were arrested this month at the Phoenix airport as they were boarding a flight to London, according to court papers urging a judge to keep them behind bars while they await trial. An attorney for Gehrke declined to comment, and a lawyer for King didn’t immediately respond to an email from The Associated Press. Authorities allege Gehrke and King, who got married this year, knew charges were coming and had been preparing to flee. At their home, authorities found a book titled “How To Disappear: Erase Your Digital Footprint, Leave False Trails, and Vanish Without a Trace,” according to court papers. In one of their bags packed for their flight, there was a book titled “Criminal Law Handbook: Know Your Rights, Survive The System,” the papers say. Gehrke and King lived lavishly off the scheme, prosecutors allege, citing luxury cars, a nearly $6 million home and more than $520,000 in gold bars, coins and jewelry. Officials seized more than $52 million from Gehrke’s personal and business bank accounts after her arrest, prosecutors say. In total, 193 people — including 76 doctors, nurse practitioners, and other licensed medical professionals — were charged in a series of separate cases brought over about two weeks in the nationwide health care fraud sweep. Authorities seized more than $230 million in cash, luxury cars and other assets. The Justice Department carries out these sweeping health care fraud efforts periodically to help deter other potential wrongdoers. In another scheme targeting Native Americans, phony sober living homes were set up promising addiction treatment. Claims were then submitted for services that were never actually performed, officials said. Another case alleges a scheme in Florida to distribute misbranded HIV drugs. Prosecutors say drugs were bought on the black market and resold to unsuspecting pharmacies, which then provided the medications to patients. Some patients were given bottles that contained different drugs than the label showed. One patient ended up unconscious for 24 hours after taking what he was led to believe was his HIV medication but was actually an anti-psychotic drug, prosecutors say. ___ Follow the AP’s coverage of the U.S. Department of Justice at https://apnews.com/hub/us-department-of-justice. ALANNA DURKIN RICHER Richer is an Associated Press reporter covering the Justice Department and legal issues from Washington. twitter mailto",
    "commentLink": "https://news.ycombinator.com/item?id=40815139",
    "commentBody": "200 people charged in $2.7B health care fraud crackdown (apnews.com)398 points by apsec112 21 hours agohidepastfavorite215 comments rdtsc 21 hours ago> Authorities allege Gehrke and King, who got married this year, knew charges were coming and had been preparing to flee. At their home, authorities found a book titled “How To Disappear: Erase Your Digital Footprint, Leave False Trails, and Vanish Without a Trace,” according to court papers. In one of their bags packed for their flight, there was a book titled “Criminal Law Handbook: Know Your Rights, Survive The System,” I guess \"step 15\" from the book was \"marry your criminal associates, so you're not forced to testify against each other\": https://en.wikipedia.org/wiki/Spousal_privilege? Step 1 should be \"don't buy any incriminating books related to your planned disappearance\" followed by the next chapter, \"step 2: too late, already\". reply snibsnib 20 hours agoparentI have read 'How to disappear', i don't think it would be very useful here. It's written by two private detectives to help people evade stalkers or family violence. It specifically recommends against breaking the law. In any case it would be a bit out of date by now. reply pyuser583 20 hours agorootparentSo if you’re ever arrested for anything, that fact can be used to argue against bail. For the rest of your life. reply _DeadFred_ 19 hours agorootparentEveryone here making AI 'social profile' systems take note of this and add it as something to track. Will come in great for the marketing feature list. reply adolph 19 hours agorootparentYes, there's got to be people farmers who grow various fictional people over time for use by ___. Create and shut down employers who pay the fictional people, rent apartments left empty in their name, maybe ding up a credit report to look real, arrange for someone who matches a look to appear in a paper under the name being farmed, kind of like Sim City but in the \"real world\" like Pokemon Go. reply _DeadFred_ 16 hours agorootparentWe already have this for companies. They are called 'Shelf Companies'. It looks like 'shelfidentities.com' is available. reply robertlagrant 9 hours agorootparentI thought shelf companies were so you could quickly get a new company going (as it's all pre-registered), so you have an established history, which some organisations - such as banks - value, or so you can access a type of company no longer available[0]. They're not fake companies; to the extent companies are \"real\", they are real. They're just dormant. [0] https://www.sars.gov.za/businesses-and-employers/small-busin... reply adolph 2 hours agorootparentIt isn’t exactly the same but intriguingly similar and is something relatively normal. reply cess11 1 hour agorootparentprevYou're probably thinking of letterbox companies. Shelf companies are kept by lawyers, accountants and the like so they can provide them to clients without them having to go through an incorporation process. In some jurisdictions it's a way to not have to wait a year or so until you can pay out cheap dividends. reply kazinator 6 hours agorootparentprev> I have read 'How to disappear' I see what you didn't do there! reply makeitdouble 18 hours agorootparentprev> It specifically recommends against breaking the law. It's so kind of them. Otherwise yes, any trick that ends up in a popular book would probably not be that useful for people actually trying to flee from law enforcement. Or it would be some super vague advice in the tune of \"don't leave traces of you being there\". reply cess11 1 hour agorootparentThere's a rather rich literature about 'OPSEC', i.e. how to hide from powerful enemies. Breaking the law is commonly the right thing to do, could be to expose corruption for example. reply paulpauper 19 hours agorootparentprev\"recommends not breaking the law\" yeah after disappearing, obviously. reply m3kw9 19 hours agorootparentprevReading it for a friend? reply johndhi 15 hours agoparentprevI need to stock my library with books like... \"Wrongly accused\" and \"what it's like to get setup\" reply ErigmolCt 5 hours agorootparent\"Just Mercy: A Story of Justice and Redemption\" by Bryan Stevenson reply AceJohnny2 20 hours agoparentprevThis is bleak, but it reminds me how police found, in the home of convicted murderer and former Linux FS contributor Hans Reiser, a book about how to get away with murder. reply datadrivenangel 5 hours agorootparentThe author of that ended up being a murderer. [0] 0 - https://www.bbc.com/news/world-us-canada-61786575 reply scintill76 1 hour agorootparentIt would have been so interesting if two famous cases were connected like that, but I don’t think Reiser bought that book. > Reiser found David Simon’s, “Homicide: Life on the Killing Streets” and Masterpieces of Murder by Jonathan Goodman https://www.eastbaytimes.com/2008/03/06/update-hans-reiser-b... reply paulpauper 19 hours agorootparentprevhow to get away: step 1: do not leave this book lying around lol reply pyinstallwoes 19 hours agorootparentStep 2: publish a book what if I got away with murder? reply baud147258 11 hours agorootparentA book like If I Did It by O. J. Simpson? reply ErigmolCt 5 hours agorootparentThat's an amazing example! reply LanceH 17 hours agoparentprevThat the authorities think it is wrong for someone to know their rights is more of a condemnation of the \"authorities\". reply malfist 15 hours agorootparentIf they had other books about civil rights, sure, but having only the books related to their predicament and a packed bag is a little more than \"authorities scared of you knowing your rights\" reply cooljoseph 9 hours agorootparentHow do you know they didn't have other books about civil rights? Maybe they had an entire library filled with such books, but that wasn't brought up in the article. reply qingcharles 18 hours agoparentprevI've owned that book. I think that's the NOLO one? It's definitely not a bad intro to criminal defense, but it is no way likely to get you out from under this weight of charges. These days if you can get someone on the phone with access to an AI you can definitely get some pretty decent motions written, just watch for hallucinated citations. reply pridkett 20 hours agoparentprevSpousal privilege may not apply when both members of the couple are accused of committing the crime. reply defen 20 hours agorootparentI can't help but think of this classic Arrested Development scene https://www.youtube.com/watch?v=idqG3thKtpU reply metadat 20 hours agorootparentThat's too perfect. reply strangattractor 20 hours agorootparentprevGuess SBF marrying Changpeng Zhao wouldn't have helped any;) reply qingcharles 18 hours agorootparentprevI thought it only failed to apply when the crime was done by one partner to the other? (I'm only a lawyer on TV) reply macksd 16 hours agoparentprevWas it Hans Reiser who decided to randomly clean his car interior with bleach while there was a manhunt going on for his wife? Same vibes. reply etc-hosts 2 hours agorootparentHis dad testified in court that it was a Reiser family tradition to hose out the interior of your car to clean it (when Hans was arrested in the car there was an inch or two of water in it ) Hans also claimed to have misplaced the passenger seat (it was missing from the car) After trial he admitted he threw the passenger seat in a dumpster. reply duskwuff 14 hours agorootparentprevThere's an even closer parallel: he'd recently purchased the books Homicide: Life on the Killing Streets and Masterpieces of Murder. https://www.eastbaytimes.com/2008/03/06/update-hans-reiser-b... reply bryanrasmussen 12 hours agoparentprev>In one of their bags packed for their flight, there was a book titled “Criminal Law Handbook: Know Your Rights, Survive The System,” Don't know your rights, people, it's proof you're bad! reply marcod 21 hours agoparentprevWould you expect that kind of book to make any difference for final sentencing? reply armada651 21 hours agorootparentNo, but it will make a difference in determining whether they're a flight risk by the judge. Given the evidence these two will probably await trail in jail rather than at home. Or at the very least they just bought themselves a nice new ankle bracelet. reply sandworm101 20 hours agorootparentThe books evidence a desire to flee but equally evidence an ineptitude, the lack of an ability to flee. Buying a ticket for an international flight is akin to making an appointment with the FBI for you to be arrested. The FBI only dreams that every fugitive first pass through inspection/x-ray before being arrested in an airport terminal ringed with security. No need to worry about a fight, weapons or a dangerous police chase. The perps have already been frisked by the FAA. If these people ever run, they won't get far. reply mulmen 20 hours agorootparentThe FAA does not run airport security. The FAA administers aviation, an actual necessary and valuable endeavor. The theatrical display you are subjected to in airports is overseen by DHS as justification for their continued existence as a jobs program. It has nothing to do with aviation. But yes, the FBI would be happy to have fugitives be caught at an airport checkpoint, although I doubt they’d risk relying on DHS to do it. reply bredren 19 hours agorootparentAt what point does a person’s purchase of an international flight ticket trigger an alert to LE? Or does getting a notification like this require a warrant or similar court approval first? reply qingcharles 18 hours agorootparentI don't know that the system is all that great. I've known people with multiple warrants fly around and in/out of the country multiple times before finally getting stopped. reply aceofspades19 19 hours agorootparentprevIt's actually the TSA that does the frisking in the US at airports. reply rdtsc 21 hours agorootparentprevI can see it making a difference when presented to jury, if it's admitted as evidence. It may not be main evidence but combined altogether with other things they did, it might tilt the balance into the \"sentence them closer the maximum term limit\" vs the \"lower end\". reply wbl 19 hours agorootparentJudges not juries determine sentences in the US. In federal court the Guidelines are applied resembling nothing so much as a Pathfinder grapple roll. reply paulpauper 19 hours agorootparentprevconvince jury reply scrubs 11 hours agoparentprevIt's fun to satire the stupid and criminal ... as it happens I just watched the gang that can't shoot straight. Not as good as I hoped, but the repeated \"hey\" at film beginning and using a lion as motivation to pay mobsters for \"protection money\" was a good laugh. reply cs702 21 hours agoparentprev> The owners of the wound care companies, Alexandra Gehrke and Jeffrey King, were arrested this month at the Phoenix airport as they were boarding a flight to London The whole sequence of events reads like something straight out of a Vince Gilligan show, like Breaking Bad or Better Call Saul.[a] The only significant difference is that Gilligan's shows take place in New Mexico instead of Arizona. -- [a] https://en.wikipedia.org/wiki/Vince_Gilligan reply 7thpower 7 hours agoparentprevNot so fast, these are only the crimes we know about. reply adolph 19 hours agoparentprev> I guess \"step 15\" from the book was \"marry your criminal associates, so you're not forced to testify against each other\" That may constitute bigamy in some states. https://www.law.cornell.edu/wex/bigamy reply bushbaba 19 hours agoparentprevYou’d think these folks would takeout cash, go to Walmart and buy a laptop which they only use at public hotspot locations. To later dispose of. Still traceable but a lot harder reply jandrese 17 hours agorootparentThat's a lot of effort. No point getting into crime if you're going to have to work so much. The other way to think about it is these people were caught because they were being so blatant about it. There are probably lots more cases like this where the criminals are a tiny bit smarter about covering their tracks. reply wildzzz 14 hours agorootparentprevThey got DPR that way. Distracted him at a coffee shop while he was logged into his computer and snatched it. If he had just been at home on a VPN, he probably wouldn't have gotten snagged. They knew who he was already but needed that laptop to prove it. reply etc-hosts 2 hours agorootparentRoss had a long history of accessing internet resources on the internet without going through TOR, such as his email acct. Also he has an email acct ! He had fake IDs shipped to his home address through USPS. The federal agencies definitely knew who he was already. The scene in the library is noteworthy but they would have Got Him without it. reply more_corn 4 hours agorootparentprevHe was at the library not a coffee shop. I bet they still could have got him at home: Swat kicks he door in, shit! Lock the laptop! Slams the lid. Notorious Linux suspend bug kicks in and it fails to suspend. (You can tell because the fans keep going) Shit. Open, slam, open, slam. Hang on guys, I need to like modprobe or something… Shit those bracelets hurt. reply m3kw9 19 hours agoparentprevAlmost comical reply mahogany 20 hours agoparentprev“They can’t arrest a husband and wife for the same crime!” reply user3939382 4 hours agorootparentQuote of the century reply paulpauper 19 hours agoparentprevI don't get it. Why do these criminal masterminds always get arrested at the airport? The authorities know who they are, yet for some reason decide to wait for them to go to the airport instead of just arresting them at home or wherever? I guess this shows that if you are a fugitive from justice, or expect to be indicted, stay the fuck away from the airport. take the bus, uber, or train or something. reply creer 19 hours agorootparentAn investigation can keep gathering evidence more easily if the suspects are still generating evidence. About to board an international flight is logical as the spot where the investigators need to decide: arrest now or it's gonna be much much harder later. As long as the investigator knows they are at home, there is no rush. A bus to Mexico would be an arrest also. reply rblatz 13 hours agorootparentThey were in Phoenix why not drive yourself to Mexico? I’ve never been stopped by American border agents on the way into Mexico. Rocky point is a short ~3.5 hour drive, you can show up and get a nice condo for ~150 a night at one of the many resorts. Then you can start working on your next steps. reply more_corn 3 hours agorootparentIf they were smart they’d realize that the same amount of effort can be put into legal activities and you get to keep the proceeds at the end. For the most part smart people have already self-selected out of a life of crime. reply jimberlage 19 hours agorootparentprevAirports post-911 have a robust security setup with only a few clearly defined entrances and exits, with a person stationed at each one, in addition to whatever team you have brought with you. If you’re arrested at the terminal, they have likely already swept you for weapons without raising your suspicions already. reply jjtheblunt 19 hours agorootparentgood point about having been swept for weapons already! reply jjtheblunt 19 hours agorootparentprevBy arresting them at an airport, and presuming they're already under surveillance, law enforcement gets another piece of evidence (fleeing) to argue? reply Scoundreller 16 hours agorootparentprev> Why do these criminal masterminds always get arrested at the airport? International flight means entry/exit searches can happen more easily and/or less likely to be deemed unconstitutional. IANAL. https://www.northcountrypublicradio.org/news/story/48010/202... reply jlarocco 13 hours agorootparentprevBecause they're fugitives on the run, and the airport is the place to go to run away quickly. reply HappySweeney 15 hours agorootparentprevI am not a lawyer but I believe allowing them to attempt to flee goes to consciousness of guilt. reply spywaregorilla 17 hours agorootparentprevBuying airline tickets may be an excellent red herring strategy if that's how police units plan to catch people though! reply barbariangrunge 20 hours agoparentprevPeople shouldn’t be profiled based on what they watch or read. Are we going so say that couldn’t video games are evidence that somebody is going to go commit violence next? What if you’re into spy movies? Crime documentaries? Weird reality tv shows? I read all kinds of non fiction things I won’t even list here but I seem to be a perfectly boring member of society. It’s just interesting subject matter reply wildzzz 14 hours agorootparentWhen the authorities catch up to you, you are definitely going to be treated differently if you have bags packed, tickets booked, and a bunch of books on how to evade the police versus being on your volunteer shift at the local soup kitchen. Getting caught fleeing speaks poorly to your character if you don't have a good reason or have already been told to not leave town by the cops. reply jjk166 5 hours agorootparentIf they had contacted a defense attorney, does that also speak poorly to their character and should be held against them? Certainly that's something a criminal who knew they were guilty would do. reply crazydoggers 19 hours agorootparentprevIt’s not profiling as the book was collected as evidence after being charged with a crime and with a search warrant (or at least probably cause must be established with other evidence). Profiling would be using information about such a book before crimes were committed or charged. In addition, such evidence is important in establishing bail, as risk of fleeing is of primary concern. reply barbariangrunge 19 hours agorootparentApply that to other situations though. Those sorts of books are ordinary. If you used that sort of profiling after the fact on everyone, you could make anyone look suspicious or untrustworthy Witch hunters during the inquisition literally used to do this sort of thing to help condemn people reply Myllenum 12 hours agorootparentYou are absolutely correct! Law enforcement should not in any circumstance be allowed to go through someone's library of books that are publicly available and then try to tie in one of those books with the fact that they've committed a crime. That is the problem, like many people in this feed they are totally oblivious of how much illegal control and power they are giving the authorities. Interestingly though, my gut tells me that they didn't even find that book that that book may have been planted as evidence against them. We live in a world where police are always getting busted on social media for lying in deception. We can no longer sit back and assume that they are doing the right thing behind closed doors. I don't believe for one minute that they found the book I think that was their way of adding a layer so that they could later charge the people. If we don't fight this type of stuff now, our lives As Americans in the future lives of the next generations literally will not even be worth living!!! reply scrubs 11 hours agorootparentOh c'mon. That's just a bunch of vapid nonsense. reply bentley 19 hours agorootparentprev> In one of their bags packed for their flight, there was a book titled “Criminal Law Handbook: Know Your Rights, Survive The System,” the papers say. You sensibly mention that this was not brought up until after the warranted search. But why is this title being mentioned now? Is the suggestion that someone who’s been charged with a crime should not attempt to read up on his rights—that doing so is a black mark suggesting flight risk? If the other book, on disappearing, is derogatory in itself, then why bring up this book too? reply LodeOfCode 18 hours agorootparentI'd guess that it speaks to the \"knew charges were coming\" bit to support that they were specifically fleeing the law and not disappearing to escape a bookie or an annoying family member or something reply autoexec 21 hours agoprevThis is the sort of action we need more of to help combat corruption and restore some basic faith in the medical industry, but it's only a start. The convictions that follow have to come with severe enough consequences to more than offset the the millions they got while hurting patients and stealing from taxpayers. If the people involved get away with a slap on the wrist or fines that are only a fraction of what they made in profit it will only encourage others to do the same thing. Any doctors accepting kickbacks should, at the very least, lose their license and be prevented from practicing medicine again. reply rdtsc 21 hours agoparentI suspect this is just a the tip of the iceberg. These clowns are just the dumb ones who got caught. I mean they bought books on \"how to disappear\" and \"how to avoid criminal prosecution\". I guess the smart ones get away with it regularly. It's sad really. > The convictions that follow have to come with severe enough consequences to more than offset the the millions they got while hurting patients and stealing from taxpayers. I wish they land in prison for many years and then have to pay back what they stole and for all the court fees as well. reply Icathian 4 hours agorootparentI worked in this space as a statistician. You are completely correct. The environment is so incredibly target rich and resources are so limited that only the biggest and most obviously winnable cases are pursued. reply marcod 21 hours agoparentprevThe other good news of the day was that the Sackler family lost much of their future prosecution immunity https://news.ycombinator.com/item?id=40813369 reply toomuchtodo 18 hours agorootparentAlways nice when a little bit of hope shines through the darkness. reply ativzzz 4 hours agoparentprevYes totally, and then we need stricter regulations to keep this from happening again. Unfortunately every regulation makes healthcare even more expensive and adds more middlemen and makes healthcare companies hire more paper pushers to make sure they're meeting regulations. Every single fraudster costs future Americans a ton more money than they just stole reply resource_waste 9 hours agoparentprevThe legal thing to do is to over-treat and do sessions in 8 minute increments so you can round up. Cornwell health does it! reply paulpauper 19 hours agoparentprevWhen people talk about price distortion, this is it. Reimbursements always open up the possibility for abuse. reply dheera 20 hours agoparentprevRelated: https://hospitalogy.com/articles/2024-06-18/done-adhd-telehe... reply briffle 15 hours agoparentprevBut this fraud is huge, so you can be all but certain this will go on for years, and then a fine of 5% or less of what they took, and no admission of wrongdoing…, reply yelling_cat 17 hours agoprevMost of the article is about Gehrke and King, but the last fraud case mentioned is absolutely vile: > Another case alleges a scheme in Florida to distribute misbranded HIV drugs. Prosecutors say drugs were bought on the black market and resold to unsuspecting pharmacies, which then provided the medications to patients. > Some patients were given bottles that contained different drugs than the label showed. One patient ended up unconscious for 24 hours after taking what he was led to believe was his HIV medication but was actually an anti-psychotic drug, prosecutors say. reply tommica 12 hours agoparentThat is so messed up :( reply treeFall 21 hours agoprevHere's the scammer's wedding registry from earlier this year. They look happy https://www.theknot.com/us/jeff-king-and-lexie-gehrke-feb-20... reply SCUSKU 20 hours agoparentChecks out that they were in Phoenix/Scottsdale... When I visited last year it felt very much like LA 2.0. Lots of plastic surgery, people flaunting wealth, and generally materialistic and shallow culture. Not representative of the entire area of course, but I was surprised by how much of that there was. reply com2kid 20 hours agorootparentAgree on all accounts, but I'd also like to add: Surprisingly good Italian food. Not the standard \"high class\" American stuff slathered in cheese either. Just really good wholesome delicious high quality dishes I as not expecting that from Scottsdale. But yeah, otherwise kind of an odd place. reply data_ders 21 hours agoparentprevlol when you click \"our story\" [1] it says: > Our Story coming soon! also under \"Travel\": > Travel coming soon! haha no kidding! [1]: https://www.theknot.com/us/jeff-king-and-lexie-gehrke-feb-20... reply bustling-noose 6 hours agorootparentValet parking will be provided for no additional charge according to the FAQ. Probably cause no one is going to return your car back. reply montag 21 hours agoparentprevLouis Vuitton Medallion Blanket $1510.00 - Purchased Really can’t make this stuff up reply cs702 20 hours agorootparentDon't forget about the six beach towels for $4650: https://www.theknot.com/us/jeff-king-and-lexie-gehrke-feb-20... reply alluro2 17 hours agorootparentI will never not be in disbelief when learning about things like this existing. I think I understand art, items created by skilled craftsmen, special materials, uniqueness...This has none of those properties. reply petre 13 hours agorootparentGet rich fast type of people usually have bad taste. reply Scoundreller 16 hours agorootparentprevIf you're going to go interstellar hitchhiking, worth bringing a fancy towel with you reply Grimblewald 16 hours agorootparentprevthat whole registry is amazing. It's like satirical characters come to life. There's no way this is real people, but it is. reply janalsncm 20 hours agorootparentprevMaybe I’m cheap but $335 for an extra small pet leash seems a bit steep. reply cko 7 hours agorootparentThere's this theory that illegitimate or fast gains lead people to not value the gains as much, thus more frivolous purchases. reply BizarroLand 2 hours agorootparentThat's interesting. I suppose that if I won the lotto I would want to try expensive things to see if they were worth the extra money. I've never dried off with $700 towels before, would I feel $695 fancier for using them? reply arp242 19 hours agorootparentprevBetter hope that extra small pet isn't going to wee all over that $640 beach towel. Prices such as this give \"don't forget your towel\" a new meaning. reply squigz 20 hours agorootparentprevYou're probably just not disgustingly rich. reply chmod775 17 hours agorootparentI don't don't see why a sizable bank account would make one desire to put on a clown nose. If you were poor and threw your money at sucker-bait, I might not bully you for it, because the price of the item is punishment enough already. But if you're rich and buying that, I'm definitely bullying you. reply Shocka1 5 hours agorootparentI'm of the same opinion when I see the dime a dozen Mercedes or BMW in the middle/middle upper class ends of town. I can understand getting an AMG Merc or M series BMW if you have the money, but the average luxury brand 60 to 80k car that is parked every other parking spot with the owner maxxed out on leverage just seems foolhardy to me. To each their own I suppose? reply janalsncm 19 hours agorootparentprevEven if I was a billionaire I would still want to maximize the value I’m getting as much as possible. reply mistrial9 5 hours agorootparentmany wealthy people are penny-pinchers in daily life reply mensetmanusman 17 hours agorootparentprevInterestingly, this is the exact (typical) mindset that billionaires have which contributes so much to extreme wealth inequality. reply robertlagrant 2 hours agorootparentThey're billionaires because they don't waste their money? I don't think so. They're billionaires because they invest in companies that provide shed loads of value to their customers, so their stock is worth more. reply anjel 2 hours agoparentprev\"Our Story\" > Coming Soon! reply voganmother42 21 hours agoparentprevthe schedule not being chronological is baffling to me, is that a common thing? reply Jensson 11 hours agorootparentProbably wasn't a real wedding, they just wrote that as a cover. reply data_ders 21 hours agorootparentprevagreed! more baffling than their fraud! reply marcod 21 hours agoparentprevDolce&Gabbana Casa Leopard Silk & Wool Throw $3,245.00 - Purchased reply warunsl 13 hours agoparentprevDolce&Gabbana Casa Leopard Silk & Wool Throw - $3245.00 - Purchased Goddamn. reply bustling-noose 6 hours agoparentprevThe ‘our story’ section says ‘coming soon’. Can’t wait. reply lxm 21 hours agoparentprevBaccarat Lucky Butterfly $250.00 Purchased Maybe not as lucky as advertised? reply teractiveodular 17 hours agoparentprevNow I know who buys leopard print dessert plates for $1155: https://www.theknot.com/us/jeff-king-and-lexie-gehrke-feb-20... reply wildzzz 14 hours agoparentprevAll that money and they went with a free wedding website? reply b3lvedere 21 hours agoparentprevWedding Party coming soon! I bet $15 on 'nope' reply 1024core 17 hours agoprevHere's an idea: the government (HHS) makes available all public expenditure data (suitably anonymized) and if you can reliably identify fraud, you get 20% cut of the money recovered. Go forth and go crazy with the data, my ML homies! reply specialist 5 hours agoparentIIRC, there's a law practice that solely goes after pharmacy prescription management something something. They make good money. I've since been wondering if bounties for third parties is a way to mitigate regulatory capture. reply coltonv 16 hours agoparentprevUntil someone spams them with garbage, or uses it to deliberately screw with certain people, and the system starts requiring more people to sift through the garbage than it needed without it. reply EnigmaFlare 16 hours agorootparentIt wouldn't be randoms on the internet submitting data. Submitting false data could be fraud itself and the people doing it could trivially be identified. reply cs702 21 hours agoprev$2.7B in made-up claims, divided by 200 accused individuals, gives us an average of $13.5M per alleged criminal. That's not peanuts! reply kurthr 21 hours agoparent\"In the Arizona case, prosecutors have accused two owners of wound care companies of accepting more than $330 million in kickbacks as part of a scheme to fraudulently bill Medicare for amniotic wound grafts, which are dressings to help heal wounds.\" Total fraud in that case was $900 million, for a only 500 patients, or almost $2M per patient. reply guynamedloren 21 hours agorootparent> Total fraud in that case was $900 million, for a only 500 patients, or almost $2M per patient. That seems so outside the bounds of reality that I'm questioning if it's what happened here. I can't imagine how that would pass any kind of sniff test by Medicare. Quote from the article: \"In less than two years, more than $900 million in bogus claims were submitted to Medicare for grafts that were used on fewer than 500 patients, prosecutors said.\" The alternative interpretation of this is that the grafts were _applied_ to 500 patients, but potentially many many more were billed for grafts that they didn't need (and didn't receive). Maybe more feasible? reply Spooky23 21 hours agorootparentprevWound care is bullshit when it’s not fraud. My dad was getting billed $12,000 weekly for treatment of bedsores due to neglect when he was inpatient. The procedure took under 15 minutes. Compromised patient, injured due to basic incompetence, A Medicare probably paid a million dollars or more to treat it. reply proee 21 hours agorootparentThat is insane. Did his insurance cover this? What happens if you refuse to pay such outlandish fees? I'm sorry he had to deal with this. reply Aurornis 20 hours agorootparent> Did his insurance cover this? What happens if you refuse to pay such outlandish fees? The billed numbers are almost completely made up. The insurance company (Medicare in this case) will only reimburse up to a set amount. The reimbursement is calculated as min(billed_amount, allowed_amount). If the facility accidentally bills less than the allowed_amount, they get less than they could have. So to make sure they get 100% of the possible payout, they bill extremely high numbers to insurance. It's a dumb system, but it's something you have to keep in mind whenever someone talks about how much things cost in the US system. With expensive procedures, products, or drugs, virtually nobody ever pays the big number. It's just a placeholder to make sure insurance payouts are not left on the table. reply AlotOfReading 18 hours agorootparentThat's not quite it. Hospitals would make every code cost $1M or some other ridiculous number if that were the only thing going on. The provider has a list of prices called the chargemaster. The insurance (and Medicare) have lists of prices they'll pay. For private insurance, there will often be negotiations to set prices for patients with the insurer getting treatment at the provider, called the network agreement. These fall between the two lists, generally. If you're out of network, the provider bills their chargemaster, the insurer tries to pay their price depending on the terms of the insurance. In network, the provider bills the negotiated rate and the insurer (probably) pays it. Now what happens if the patient doesn't have insurance? The provider bills their chargemaster and the patient (rarely) pays it. If they do, confetti. If they don't, the provider graciously takes a percentage off and offers a repayment plan. They collect on a meaningful percentage of these. The patient pays more if the chargemaster is higher. reply bliteben 20 hours agorootparentprevI don't understand how it isn't fraud. I looked at an ambulance bill for my mom the other day: $1798 total -$388 Medicare paid -$1324 Service adjustment $86 Amount you owe. I don't think dumb is sufficient to explain this. It's pretty easy for me to see how someone leaps from this de facto legal system to outright fraud, because the line between them is pretty thin. reply Suppafly 37 minutes agorootparentMedical billing is dumb but I'm not sure why that would be or appear fraudulent. The normal cost of the ambulance service would be $1798, medicare pays a max of ~$500 for ambulances and your mom pays 20% of that. The reason the rest of us would pay $1798 is because we essentially subsidize the medicare rates. Many insurance companies negotiate rates that are some % of the medicare rate, so they might pay 2x or whatever, and pay $1000. Medicare often has agreements that they can't bill the rest of the charge to the patient so it's 'adjusted' essentially written off. reply Dylan16807 18 hours agorootparentprevIt's only fraud if the other party (Medicare in this case) is tricked. They expect this kind of pricing/discounting. reply mindslight 15 hours agorootparentprevThe way medical billing works is plainly fraud, but the people who are successfully defrauded don't end up knowing any better. For everyone else it's merely attempted fraud, and if you (forcefully) call out the fraudsters they'll bargain their bill down to something more defensible and/or find someone else to pay it. I'm nearly done dealing with an instance of this myself (for someone else). Had a $1k copay that was legit per their \"insurance\" plan. The hospital also sent a fraudulent bill for another ~$2k rather than doing the work of figuring out how to bill \"insurance\" for it. Told them we'd pay in full once they presented a complete set of non-fraudulent bills. Half a year later, with me holding the hands of both bureaucracies, they finally were able to get \"insurance\" to pay that second bill. I told them we were ready to pay the $1k legit copay, and they told me they had taken care of it months ago using some internal charity fund. The system is an utter joke. reply llamaimperative 21 hours agorootparentprevGP says Medicare paid for it, so the US taxpayer. reply Spooky23 20 hours agorootparentprevThat was from the Medicare statement. I think the wound care courts as a surgery. reply naijaboiler 14 hours agorootparentprevwhy do people have to be so damn greeedy. $900m on 500 patients is just asking to get caught. Even if all 500 patients had cancer and tripple bypass surgery, and required lengthy ICU stays, their costs will still barely be that. Why can't scammer just exercise some moderation and they get to keep scamming for a long time. No, they have to just get greedy. Greed always get you caught reply onlyrealcuzzo 21 hours agoparentprevNo - but $2.7B in fraud over $4.5T total healthcare spending per year is peanuts. This appears to happen over ~5 years or more - so that's $2.7B vs $22.5T or about 0.012% of total spending. The reason healthcare is broken in the US is not because of fraud. It's due to the design. By the way, fraud is inevitable. We might be able to save about ~1% per year on healthcare if fraud was reduced to more reasonable levels. That's not going to move the needle at all. reply anamax 16 hours agorootparentMedicare spends around $1T/year and the fraud estimates are $60B-90/year. If medicare is representative, fraud is 5-10% of total spend. https://www.aging.senate.gov/press-releases/lawmakers-join-a... reply Suppafly 35 minutes agorootparentHonestly I'd assume medicare fraud might not be representative. It's kinda big risk/ big reward because the govt has the power to imprison you, but other insurance companies would just stop dealing with you if you rip them off enough and still sue you to get their money back. reply dragonwriter 16 hours agorootparentprevThe other big national health program, Medicaid, is pretty similar scale and pretty similar proportion of fraud. reply onlyrealcuzzo 4 hours agorootparentprevEstimates for France, the UK, and Canada are around ~6%. Fraud is inevitable. reply Aloisius 20 hours agorootparentprevIt’s not like this is the only fraud we’ve found in years. It’s not even the first billion dollar fraud incident case this year. The FBI estimates fraud accounts for upwards of 10% of healthcare expenditures. reply WarOnPrivacy 16 hours agorootparent> The FBI estimates fraud accounts for upwards of 10% of healthcare expenditures. One of my clients had their business roped into a multi-million dollar medicaid fraud scheme. The FBI eventually handled it but it months and a lot of work to get the feds interested. reply rqtwteye 19 hours agorootparentprevI would argue the whole system is fraudulent and highly corrupt but profitable so it's ok. Insurances reject legitimate claims, hospitals charge exorbitant rates for procedures that never happened and much more. Look at what PBMs do. reply vkou 21 hours agorootparentprevThe design of the system makes normal billing practice indistinguishable from fraud. I will go as far as to say that charging $200 for a Tylenol, bundled with a million other different line items in a treatment is fraud. reply Suppafly 31 minutes agorootparent>I will go as far as to say that charging $200 for a Tylenol That's a common example but that's not actually what happens at all. reply samt 14 hours agorootparentprevYes. As usual the worst fraud is completely legal and right there for all to see. reply Terr_ 19 hours agorootparentprevIt's only fraud when the victim isn't an individual patient. /s reply jandrese 17 hours agorootparentprevThis was one case, and it's not the first time someone went and just checked for the most blatantly suspicious case and discovered fraud. It is safe to say there is a lot more going on that is being perpetrated by people with better opsec. reply nerdponx 15 hours agorootparentprevYeah but that tiny % of healthcare spending is enough to fund a lot of other stuff. Don't get thrown off by the orders of magnitude: a small fraction of a big number is still a big number. Imagine what $2.7B could do for transit or housing or relocating migrants or student loan debt relief or wildland firefighter pay raises or invasive species remediation. Hell, imagine what even 1% of $2.7B could do for independent journalism or the arts. reply loeg 19 hours agoparentprevMedicare fraud is really profitable (until you're caught)! There's a reason it keeps cropping up. reply Suppafly 29 minutes agorootparent>Medicare fraud is really profitable (until you're caught)! There's a reason it keeps cropping up. Seriously, if they'd just been content with a few million and jetted off to the Caribbean it would have never been noticed. All of these medicare fraud things are always in the multi-millions before they get noticed. reply sidewndr46 20 hours agoparentprevif they were smart they paid taxes on the fraudulent gains. So they netted US $5 million or more each reply rqtwteye 19 hours agoprevThey are ready to move on to become governor and senator: https://en.wikipedia.org/wiki/Rick_Scott reply anjel 2 hours agoprevThe healthcare system is rotten from top to bottom. https://www.usatoday.com/story/news/health/2024/06/25/baylor... Medicare expansion > provider-pigs at the trough. The scale of fraud is heartbreaking. Source: I'm fin. director for a large \"non profit\" community health clinic. reply hdivider 21 hours agoprevI wonder: would this level of fraud occur if the US had universal healthcare, or its effective equivalent? Asking out of genuine intellectual curiosity, not with political or other overtones (even with the highly political character of the news today). reply Aurornis 20 hours agoparent> I wonder: would this level of fraud occur if the US had universal healthcare, or its effective equivalent? The fraud was perpetuated against Medicare, which is the US-run insurance plan for people 65 and older and people with disabilities. Nearly 20% of the population is on Medicare. Universally letting everyone on to the plan would have actually allowed them to run the fraud on everyone, not just people on Medicare. reply ceejayoz 19 hours agorootparent> Universally letting everyone on to the plan would have actually allowed them to run the fraud on everyone, not just people on Medicare. They do. Private insurers get defrauded all the time just like Medicare does. reply erfgh 5 hours agorootparentThe difference is that private insurers have an actual incentive against this happening but when the scheme is state-funded you have to rely on the people's sense of duty which we know is not particularly strong in many cases. reply ceejayoz 5 hours agorootparentIn practice, the opposite is true. https://www.propublica.org/article/we-asked-prosecutors-if-h... Investigations are a cost-center. Government can staff investigators; private insurers just factor the fraud into next year's premium hike. > To put that record in context, take a look at the state’s Medicaid program, which covers about 13 million low-income people. During fiscal 2017 and 2018, the program’s fraud unit filed criminal charges against 321 fraudulent medical providers. It garnered 65 civil settlements and judgments and recovered more than $93 million, according to the state attorney general’s office. > A rigorous search for civil lawsuits filed by private health insurers over fraud in California turned up just one case in 2017 and 2018. Experts said insurers rarely sue over fraud because of the high cost of litigation. > I tracked down a dozen or so investigators who once worked for insurers, and they all said the same thing: Insurers don’t police fraud as much as they could because it hurts the bottom line. reply ajross 19 hours agorootparentprevThis is one of those moments where you realize how frustrating the discourse is that Everyone Just Knows the US doesn't have government-run health insurance. The US invented government-run health insurance. And Medicare has its warts, but works extremely well on the whole. reply silverquiet 19 hours agorootparentThe US has multiple government run single-payers. I remember being told six, but I remember Medicare, Medicaid, and the VA. What the US does not have is universal healthcare, though I think we spend something like as much of a percentage of GDP on our non-universal government systems as other countries do on their universal healthcare, and then again we spend more GDP on private. All for worse outcomes than those other countries (though we can do some really impressive stuff if you're rich). It's quite a racket for sure. reply vharuck 17 hours agorootparent1. Medicaid, 2. Medicare, 3. Military, 4. VA, 5. Indian Health Services Maybe 6 is Medicaid through a managed care plan. reply WarOnPrivacy 16 hours agorootparent> Maybe 6 is Medicaid through a managed care plan. House of Representatives and Senate offices will provide health coverage to Members of Congress and designated staff through the Small Business Health Options Program (SHOP). https://www.opm.gov/frequently-asked-questions/insure-faq/he... reply wiether 9 hours agorootparentprevI was always told that the first instance came in Germany in 1883, so I would love to hear about what happened earlier in the US? reply davidw 21 hours agoparentprevThe biggest scam in US health care is the insurance industry. Most of the 'fraud' that doctors complained about where I lived in Italy was old people coming in with mostly imagined complaints because they're lonely and don't have anyone and a doctor visit is free. Which is kind of sad, but... not the same level of problematic. reply Aurornis 20 hours agorootparent> Most of the 'fraud' that doctors complained about where I lived in Italy was old people coming in with mostly imagined complaints because they're lonely and don't have anyone and a doctor visit is free. Which is kind of sad, but... not the same level of problematic. I don't think that's a relevant analogy at all. In this case, the doctors were the ones perpetuating the fraud. Not the patients, not the insurance company. reply glzone1 21 hours agoparentprevThis level of fraud is almost non-existent when there is NO insurance and NO universal healthcare. Anytime the person paying is not the one receiving service, the appeal of fraud goes up. When the person paying is not paying with their own money the appeal goes up and a TON of incentives argue against finding fraud. At least in govt, there is sometimes a willful blind eye because finding fraud causes a huge headache and you don't benefit or get paid more. In fact, if you are taking an administrative fee to manage the program, reducing expenditures can hurt your own budget. I have traveled internationally where folks paid cash for services. It was a bit crazy - they would basically hold you hostage until you paid a bill / impound your car etc. BUT you could buy many drugs OTC (no DR visit even required) and the layers of things like approvals, pre-approvals, billing and billing codes etc simply didn't exist. reply svachalek 19 hours agorootparentYeah I've wondered about a system where health insurance is for major events like injury accidents and cancer, and typical office visits are handled as a consumer product. Insurance companies may want to pay for basic checkups and the like to reduce their risk of high ticket items -- or not, however the math works out. Seems like this has at least some of the benefits of a market system, for those who insist that nationalized health care is not something we can stomach in the US. reply LeonB 21 hours agoparentprevThere would still be some fraud, but since the prices of basic medical care would be considerably lower (as demonstrated everywhere else in the world) the amount of money made from fraud would be considerably smaller thus creating less incentives. reply canucker2016 8 hours agoparentprevfrom Dec 2016, https://www.thestar.com/politics/provincial/ontario-s-top-bi... ==== The province’s 12 top-billing doctors — who received payments of between $2 million and $7 million in one year — are overcharging the Ontario Health Insurance Plan, documents obtained by the Star suggest. [stuff deleted] Among additional “concerns” alleged in the report: - Three specialists “inappropriately delegated” duties — for which they billed OHIP and which were supposed to perform themselves — to unqualified individuals to undertake. - Six claimed to have worked between 356 and 364 days of the year. - Eight recorded notably high volumes of claims and/or patients. One radiologist, who worked 332 days, billed for 100,000 patients, indicating that more than 300 scans were interpreted per day, the report stated. - Eleven billed OHIP incorrectly. - An obstetrician/gynecologist billed for seeing male patients. ==== reply jcjkrigii 20 hours agoparentprevThis was Medicare fraud, which I imagine you’d see more of if universal healthcare was implemented as single payer “Medicare for all.” reply ceejayoz 19 hours agorootparentLikely a net overall drop. Private insurers get defrauded just like Medicare does. They just don’t care; it means they can raise premiums. https://www.propublica.org/article/we-asked-prosecutors-if-h... reply xtracto 14 hours agoprevInsurance is the Major healthcare fraud,and is widely accepted. I pay a monthly fee to a company for a \"service\" which actually is on the company's interest to avoid providing. Insurance companies would spend $500,000 in lawyers to prevent paying me $500,001 . Worse yet, in some cases I am required by law to contract this fraud. reply bozhark 21 hours agoprevJail for the execs. Loss of license for the nurses and doctors. People died but the money matters more? reply RecycledEle 20 hours agoparentIf people died that would not have died, is this murder? reply johndhi 15 hours agoprevMy personal opinion is that the extremely high levels of activity in healthcare fraud prosecutions really aren't good for our country. They sound perfectly bipartisan and like no brainers (I haven't read into the details of this one but assume there are plenty of compelling details) - but it's just another 50lbs on the pile of \"healthcare is highly complex and hard to do business in.\" These rules are BRUTALLY hard to comprehend for businesses. The anti kickback statute and false claims act and cures act are all riddled with ambiguity and cases where normal intuitive business practices become criminal. It just stinks to do business with Medicare. reply palijer 14 hours agoparentHealthcare business are fine, the Healthcare industry in America is larger than the retail industry and accounts for 17% of the GDP. Americans spend more per capita on Healthcare than any country, and get worse outcomes. No one is scared about doing business in the industry because the profits are enormous. The idea of just letting fraud go without prosecution in this industry is really hard for me to understand... Did you read the part where someone thought they were taking HIV medications, but because someone was profiting off fraud, they instead took anti-psychotic meds and were unconscious for 24 hours? We don't want to investigate that happening, because it might be bad for business? The business which is wildly thriving? reply 650REDHAIR 13 hours agoparentprevThese aren’t people failing to jump through hoops. I see medicare fraud nearly every shift… reply Suppafly 27 minutes agorootparent>I see medicare fraud nearly every shift… I hope you report it. I work a medical adjacent job and we get yearly training on how to report medicare fraud, waste, and abuse. reply which 13 hours agoparentprevMeh, most of these people are parasites. I agree the government going after things like patient assistance programs for helping Medicare or TRICARE patients due to the AKS is probably bad. But the typical prosecution is much more straightforward. A DME retailer whose “clients” are really just identify theft victims. A doctor who signed off on a bunch of things without ever meeting the patients. And some of it is going to get people killed. Especially when they do the scam where they buy HIV medicine from people who get it free and they use lighter fluid (!) to remove labels from the bottle and store the medicine in storage lockers and car trunks before some corrupt pharmacy buys it. Then the thief is back on the plane to whatever ex communist country they came from. There needs to be a deterrent because it is immensely profitable. reply guynamedloren 21 hours agoprevHorrible, disgusting, yet not at all surprising. I don't know what medical billing accountability looks like in this dizzyingly complex system, but as a US citizen and patient of the US healthcare system, it looks barely existent. My family has fought our fair share of bogus healthcare charges. One instance: after my daughter was born at a birthing center (independent of a hospital), my wife experienced postpartum complications and was transferred to a nearby hospital for care. As if that experience wasn't stressful and traumatic enough, many months later we were hit with a surprise charge from the hospital for treatment and care of our newborn baby - the baby that was delivered hours earlier, in a separate location, that never left my arms in the hospital! All that to say - it's alarmingly easy for a charge to get processed in a batch of other charges, and either insurance pays it without question, or the patient pays it unknowingly. During our experience, we learned that this kind of thing is exceedingly common. The power dynamic between patients and healthcare administration severely misaligned, the information imbalance is huge, and the patient is always in a compromised position. The article doesn't include details on how they tracked down these criminals (I'm curious to know!), but it wouldn't surprise me in the least if this is just the tip of the iceberg. reply linuxhansl 20 hours agoparentMy son got his first official letter when we was about two weeks old. It was a bill from the hospital for his birth: $8,000. He was directly charged on that bill, not us parents. And that was on top of the $24,000 bill that we had received from hospital. So $32,000 for a no-frills (from a medical viewpoint) birth and that was many years ago. The health insurance paid 100% of both bills, so we ended up not following up. reply supafastcoder 19 hours agorootparent$32K is cheap, we got charged ~$110K in Southern California (two kids - two different hospitals - both over $110K). Luckily insurance paid 95% of it. reply wbl 19 hours agoparentprevI use Kaiser. I have never seen a bill. reply DFHippie 20 hours agoparentprevWhen our first child was born (a long, traumatic story in itself), after the ordeal was over and we were resting in the hospital, a nice fellow came by and asked whether we wanted to test our baby's hearing. Sure? No mention was made of a price. He put a device in the baby's ears. Hearing was fine. Then we got the bill. Hundreds of dollars, maybe $500, for a moment's effort and no expenditure of resources. We had almost no money at the time and certainly no insurance. This was in Nashville. He was an independent contractor they let wander the halls of the laying in ward accosting unsuspecting new parents. He bought the machine. Now he's got a steady income from this grift. Another event in Nashville, my wife cut her hand making dinner. We walked to the emergency room. Eventually someone saw us. He asked whether we wanted to try some experimental tree sap glue to close the wound. Sure? It actually wasn't much of a cut and it had stopped bleeding. No mention of a price. While he was squeezing a droplet of liquid out of a tube the attending physician looked over his shoulder. Yep, checks out. It was basically a glance, less than a minute's interaction and no interaction with out. The whole experience, again, when we had essentially no means to pay, cost hundreds of dollars. They guy who glanced over glue man's shoulder tacked on a couple hundred for himself. Coming through the door and signing some papers then waiting hours to be attended to cost us maybe $400. I've got similar stories from Washington, DC. My son (not the baby with the hearing test) had to deal with the medical system in Vermont. He's dead now but we're still paying the bills. And now we have insurance. Regular healthcare in the US looks a hell of a lot like a scam. If you have insurance, the scam is less visible, but it's the same system. reply Aurornis 20 hours agorootparent> This was in Nashville. He was an independent contractor they let wander the halls of the laying in ward accosting unsuspecting new parents. He bought the machine. Now he's got a steady income from this grift. Newborn hearing screening is standard, or even mandated, in most states. This wasn't just a grifter they allowed to roam the halls. They were contracted with the person to administer the tests. It's likely it was even required by your state lawmakers: Here is the page from the TN department of health that says all newborns should be screened before leaving the hospital or before one month of age: https://www.tn.gov/health/information-for-individuals/i/fact... If your state is like mine, there might be a law that requires this to be offered at the hospital, along with several other newborn screens. Newborn hearing screening isn't a scam, and the hospital may not have even had a choice about offering it due to the laws. reply DFHippie 20 hours agorootparentWell, it wasn't presented to us like that. He asked, \"Do you want it?\" Implying we could say no. We looked at each other, shrugged, and said, \"Okay.\" It was very sweet until we got the bill. We had two kids more after the first one. No one offered to test their hearing or required that it be done. The second one is the one who died, but not from hearing-related complications. reply grugagag 19 hours agorootparentYeah, seems grifty to me as well and some medical practitioners are solely in for the money so you’ve got to always watch out. reply Suppafly 23 minutes agorootparent>and some medical practitioners are solely in for the money So is everyone else that works any kind of job. 'Solely in it for the money' is such a weird complaint, as if you'd do your job for free if you were independently wealthy. reply DFHippie 0 minutes agorootparentActually, most people have multiple motivations for working in their field. Nursery school teachers like kids. They also like getting paid. Most doctors like the status their job confers and helping people in need. They also like getting paid. People who have jobs that suck -- septic tank pumper, say -- may well be in it solely for the money. grugagag 17 hours agoprevAnd here’s their wedding invite [0]. Money made off dying old people had to sustain their vacuous lifestyle [0]: https://www.theknot.com/us/jeff-king-and-lexie-gehrke-feb-20... reply JKCalhoun 15 hours agoprev> The owners of the wound care companies, Alexandra Gehrke and Jeffrey King, were arrested this month at the Phoenix airport as they were boarding a flight to London... Man, I would love the job of catching these crooks. From a comment on HN recently I located and began watching the French spy series, \"The Bureau\". The series has in no way convinced me that I want to head to dangerous countries and spy on them. But catching white-collar criminals ripping off the system? How can I help! reply mavelikara 17 hours agoprev> a wound care company What is the name of the \"wound care company\" that Alexandra Gehrke and Jeffrey King owned? reply hiatus 4 hours agoparentApex Mobile Medical LLC, Apex Medical LLC, Viking Medical Consultants LLC, and APX Mobile Medical LLC. reply mavelikara 54 minutes agorootparentThank you! reply SoftTalker 16 hours agoprevI'm pretty convinced that this is the tip of the iceberg. I think most end of life care is a scam designed to separate the dying and elderly from their wealth and benefits, before they actually die. reply devmor 21 hours agoprevIf you're interested in how often people get away with this kind of scam, and how they get away with it, I strongly recommend the two episodes on the subject from the popular podcast \"Behind The Bastards\" titled \"Part One: The Fake Doctors Who Gave Everyone Alzheimer's\" and \"Behind the Bastards (2018) Part Two: The Real Bastard Was Health Insurance Companies All Along\" These episodes detail several people who pulled off these same scams, including how others tried to turn them in for it and were ignored. reply johndhi 15 hours agoparentNot to be pedantic, but you suggested general statistics up front (how often people get away with it) then delivered anecdotes later: examples from several people. reply RecycledEle 20 hours agoprevFrom what I see on the news, rich white-collar criminals get fined a small amount of what they stole and are sentenced to much less jail time than their poorer fellows. To remedy this, I suggest everyone adopt a system where anyone who stole money or did other financial damage through criminal means spends one day in jail for every $300 of damage they caused. This has several advantages: 1. The rich would no longer be above the law. 2. It would deter financial crimes and vandalism. 3. It would prevent \"victim restitution extortion\" where middle-class people are extorted to either give up their life savings or go to prison; They are going to jail regardless, so there is less incentive to set them up to get victim restitution. reply 1024core 17 hours agoprev.... and this is why we will never have universal healthcare. Too many people willing to cheat the system, and the government doesn't sweat the small stuff. reply ceejayoz 5 hours agoparent> Too many people willing to cheat the system, and the government doesn't sweat the small stuff. The government is far more likely to sweat the small stuff. They'll happily pay $20k in staffing to prosecute a $1k theft, because it doesn't hit their profits/bonuses at all to do so. https://www.propublica.org/article/we-asked-prosecutors-if-h... > Michael Crowley investigated fraud for more than a decade for three companies, including Humana and United. The flood of fraud was so great, he said, that investigators ignored suspect claims worth less than $300. Investigating those, the companies determined, would cost more than what they could recover, he said. > But those small claims add up. Williams, the personal trainer from Texas, billed insurers in increments of $300 and under for more than four years, and it added up to about $25 million. “If you’re a provider, you’re going to figure out that threshold real quick and stay under it,” Crowley said. reply loeg 19 hours agoprevWhy is Garland quoted talking about \"profit from the unlawful distribution of controlled substances\" when the vast majority of the fraud here had nothing to do with controlled substances? The quote is definitely missing context but it seems inappropriate inserted here. Edit: Ok, found the actual statement[0]. Indeed AP just pulled this quote completely out of context: > The fourth pillar of our strategy is ensuring that we keep pace with constantly evolving health care fraud schemes. That includes addressing the rise of schemes that exploit telemedicine technology — specifically as it relates to Adderall and other stimulants. > Utilizing proactive data analytics, we identified misuse of telemedicine as a possible source of an increase in prescriptions for stimulants. Thereafter, we worked with law enforcement officers to identify potential schemes. Our investigation led us to a digital health care company called Done. > Earlier this month, we charged and arrested the former CEO and the clinical president of Done for their respective roles in a $100 million scheme to defraud taxpayers and provide easy access to Adderall and other stimulants for no legitimate medical purpose. > Today, we are also announcing that we have charged an additional five defendants for their alleged involvement in that scheme to distribute more than [40 million] medically unnecessary pills. > One of the defendants, who was among the company's largest prescribers, was indicted for rubber stamping prescriptions without any medical review. As alleged, the defendant also signed prescriptions for patients who were deceased. > I want to be clear: it does not matter if you are a trafficker in a drug cartel or a corporate executive or medical professional employed by a health care company. If you profit from the unlawful distribution of controlled substances, you will be held accountable. So, yeah, Garland is talking about adderall prescriptions. Whereas AP sandwiched the quote between discussion of the (unrelated) Arizona woundcare fraud. Frankly this AP article is a really poor summary of the original DOJ statement and if I were dang I'd update the link to go directly to the DOJ. [0]: https://www.justice.gov/opa/speech/attorney-general-merrick-... reply twoodfin 21 hours agoprevnext [6 more] [flagged] ceejayoz 20 hours agoparentOh, bullshit. Private insurers get defrauded like this, too. https://www.propublica.org/article/we-asked-prosecutors-if-h... > I called up a former federal fraud prosecutor who’d worked with both Medicare and private insurers. He said my calls to the prosecutors exposed alarming differences in the way fraud is enforced in the private and government health plans. The Medicaid fraud units are “staffed and actively engaged,” said Michael Elliott, who ran about 100 fraud investigations when he worked for the Department of Justice in Texas from 2008 to 2015. > Private insurers, he said, simply don’t make fraud enforcement a big enough part of their mission. “At the end of the day, it shows their priorities are elsewhere,” he said. Plus they commit a good amount themselves: https://www.nytimes.com/2022/10/08/upshot/medicare-advantage... reply twoodfin 19 hours agorootparentA couple of dumb nobodies bilked Medicare for almost a billion dollars before they got caught. Nothing in that Propublica article gets within an order of magnitude. Also, just looking for fraud complaints is missing the way most insurers combat fraud: They have enough systemic protections to deny the claim in the first place. reply ceejayoz 18 hours agorootparentProPublica has done an extensive series on this; at least a skim is worth it. Even the industry doesn’t claim to be as effective at combating fraud as you do. https://www.propublica.org/article/health-insurers-make-it-e... > While Pratte, Lankford and some of Williams’ clients repeatedly flagged bogus bills, the mammoth health insurers reacted with sloth-like urgency to the warnings. Their correspondence shows an almost palpable disinterest in taking decisive action — even while acknowledging Williams was fraudulently billing them. > In September 2015, United wrote to Williams, noting his lack of a license and the resulting wrongful payments, totaling $636,637. But then the insurer added a baffling condition: If Williams didn’t respond, United would pay itself back out of his “future payments.” So while demanding repayment because Williams was not a doctor, the company warned it would dock future claims he would be making as a doctor. > In November 2016, United investigators caught Williams again — twice. They sent two letters accusing him of filing 820 claims between May 2016 and August 2016 and demanded repayment. Again, almost inconceivably, the company threatened to cover his debt with “future payments.” > Fraud in government programs, like Medicare and Medicaid, gets more publicity, he said, and has dedicated arms of agencies pursuing fraudsters. But the losses may be even greater in the commercial market because the dollar levels are higher, he said. > He called fraud investigators from Aetna, Cigna and United, who testified that their companies auto-pay millions of claims a year. It’s not cost effective to check them, they said. “Aetna relies on the honesty of the person submitting the claim verifying that it’s true,” testified Kathy Richer, a supervisor in Aetna’s Special Investigations Unit. Big catches make for splashy headlines, but for every $2B case that makes national headlines there are a whole bunch of these $25M cases they can’t be bothered to notice. Why would you care about fraud if you can just raise premiums to cover it? My insurer just sent me a letter saying they’re applying for a 21% increase in my $3k/month health insurance policy for next year. reply twoodfin 17 hours agorootparentStill looking for that 9-figure scam run by (apparently) morons. Are you or Propublica making the claim that private insurers pay out the majority of their claims on trust? Generally we hear the opposite, that they’re looking for any excuse to deny. reply ceejayoz 17 hours agorootparent> Still looking for that 9-figure scam run by (apparently) morons. Again, the health insurers themselves are doing much bigger scams than nine figures. Eleven figures: https://www.nytimes.com/2022/10/08/upshot/medicare-advantage... \"Another estimate, from a former top government health official, suggested the overpayments in 2020 were double that, more than $25 billion.\" > Are you or Propublica making the claim that private insurers pay out the majority of their claims on trust? That claim is made, under oath, by a \"supervisor in Aetna’s Special Investigations Unit\". In a case where the insurers were repeatedly willing to be reimbursed from future payments to a provider they knew was unlicensed, no less. Where were the magical \"systemic protections\" that private insurers have nearly perfected while Medicare supposedly lacks them entirely? reply Sparkyte 20 hours agoprev [–] If only we can be faster to detect health care fraud. reply m3kw9 19 hours agoparent [–] Like rats looking for holes, they always will find some reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "U.S. Attorney General Merrick Garland announced charges against nearly 200 individuals in a nationwide crackdown on health care fraud, with false claims totaling over $2.7 billion.",
      "The charges include a $900 million scheme in Arizona, where two wound care company owners allegedly accepted over $330 million in kickbacks for fraudulent Medicare billing, resulting in patient deaths.",
      "Authorities seized over $230 million in assets and charged 193 individuals, including 76 medical professionals, with other schemes involving fake addiction treatment for Native Americans and misbranded HIV drugs in Florida."
    ],
    "commentSummary": [
      "Authorities have charged 200 individuals in a $2.7 billion health care fraud crackdown, highlighting the scale and severity of the operation.",
      "Among those charged, Alexandra Gehrke and Jeffrey King were arrested at Phoenix airport with incriminating books on evading law enforcement, suggesting they were preparing to flee.",
      "The case underscores the importance of evidence like packed bags and specific literature in determining flight risk and influencing judicial decisions on bail and sentencing."
    ],
    "points": 398,
    "commentCount": 214,
    "retryCount": 0,
    "time": 1719522594
  },
  {
    "id": 40814011,
    "title": "A modern 8 bit design, built using 1950s thermionic valves",
    "originLink": "https://www.valve.computer/",
    "originBody": "Valve.Computer Valve Computer Technical Art The Valve.Computer A modern 8 bit design, built using 1950s thermionic valves that glow and heat the entire room. The prototype first ran on 28th May 2021 whist perched on our dining room table, chairs and surrounding floor space. The Valve.Computer is now firmly nailed to the study wall, and is almost safe to touch. Almost! Thermionic valves, (aka vacuum tubes), can switch several hundred million times a second, and in the 1950s were the basis for all computer designs. To work efficiently they require high voltages and are not for the faint hearted. The Valve.Computer is an 8 bit computer, with the usual 12 bit address and data buses plus the rather unusual current demand of over 200 Amps. It can play a decent game of PONG using its valve and relay RAM, or run a 32 bit Fibonacci sequence using modern NVRAM. After switch on you have to wait a while for the last thermionic valve to warm up. If you look from the side you see a few start to show a red glow. After about a minute the Valve.Computer has the pleasant homely aroma of 560 double valves, quietly burning off their dust. When all the valves are glowing, I check the fire extinguisher is full, and run the code. It has been a ridiculous amount of soldering and a fantastic amount of fun. The Why, The When, and The Where After visiting Bletchley Park, it occurred to me that several thermionic valve computers had been rebuilt, and now run in museums, but that no new design of a valve computer had been constructed in over 50 years. The thought of building one seemed ridiculous, but I wondered if a modern design could overcome the issues of size, power and the very real danger of high voltages. When I retired I looked at the problem again and realised it could be an interesting endeavour. I spent an enjoyable 18 months building both the prototypess and the final Valve.Computer. The Valve.Computer is mainly built from my two prototype machines, ena.computer and fred.computer. Modifying the prototype PCBs required changing over 1,000 components. The rebuilt main boards are now configured exactly as shown in the technical page schematics. The new system console incorporates the GUI from a prototype. There are also new auxiliary valve boards incorporating additional memory registers and modified oscillators. The Valve.Computer has valve memory, reed relay memory, (lethal) memory switches and NVRAM. I found it best not to use a separate room to build the computer, but to spread it all over the house, one bit in each room if I could, just so I knew where everything was. To my great surprise my lovely wife Judy was completely overjoyed when after only a year or so, I transferred all the components around the house onto the study wall. Please note that high voltages are very dangerous and shouting bang when a friend has their back to the computer is very childish, but great fun. I must stop doing it! A Warm Glow The Valve.Computer is designed using 1,120 thermionic triodes. Conveniently each 6N3P valve contains 2 triodes around a single heater, halving the physical size and power requirements. All the double valves are configured as identical NOR gates. Memory registers are built from groups of 5 NOR gates, and combined into D type flip flops. The NOR gates also form all the other functional components, including the 8 bit ALU, the two oscillators, and the relay drivers. The amount of heat is ridiculous, but the warm cosy aroma is divine. The Turner Prize The Valve.Computer integrates eight large printed circuit boards and four auxiliary PCBs, which combine the thermionic valves into a functional general purpose computer. It has now grown beyond its design, and for me, has become an art installation on the study wall. Every one agrees that the Turner Prize is much more than just a display of virtue signalling by the cultural elite, and I have decided to enter the Valve.Computer for the prize. I shall write about the blank canvas of machine code, the brush with danger, the sculpture of the valve, and a palette load of arty stuff like that. I am sure Tate Modern will be impressed! Polyonymous Triodes, a new Console, and no Safety Net Thermionic valves (or just valves) in the UK, are also named vacuum tubes (or just tubes) in the USA, and электронная лампа (or just electron tubes) in Eastern Europe. The 6N3P thermionic valve was produced in many communist factories in the 1950s and 1960s. The finest 6N3P thermionic valves were selected at manufacture for military use, with up to 5000 hours life expectancy and printed with an additional code (Cyrillic 6Н3П-ЕВ). The rest were for domestic use, and with the ability to operate at over 210Mhz, the 6N3P was used in many 1960s East European VHF radios, televisions and in several beautiful radiograms. The Valve.Computer has a new system console, which lifts open from the front. Five ASTEC MP6 PSUs are now behind the Console. These second hand, 60 Amp professional power supplies replace the prototype's cheap and cheerful PSUs. It turns out that cheap PSUs do not like a metal inspection torch across their unprotected outputs. The ASTECs enable the heaters to have a soft start, which hopefully will extend the life of the thermionic valves and I now use a plastic inspection torch. Valve.Computer programs are written using just 16 instructions. Machine code is real code, a totally different world to all the posh high level languages. It's gloves off with no safety net. You simply talk directly to the computer. It is incredibly fast compared to other programming languages, and gives you control over data storage, memory, and computer hardware. It can be a very useful tool in fast decryption and modern graphics. It's use may give a deeper understanding of high level coding, but really it's just great fun. Henry's Fibonacci Sequence Video On 15th August 2021, three months after the first table top run with manual ALU computation and storage, a prototype for the new Valve.Computer demonstrated a clocked, GUI displayed, 8 bit Fibonacci sequence. The Fibonacci value is displayed vertically, in binary. It uses both the GUI relay memory and NVRAM, and is demonstrated by Henry the cat. Judy's PONG Game Video On 11th January 2022, Judy starred as the Mysterious black gloved lady, in the epic video production of The PONG Game. Using the prototype for the new Valve.Computer, she demonstrated a simple version of the game. Less than 100 machine code instructions enable the ball to bounce around the court, and subroutines for gameplay and the GUI display complete the program. It's is a fast, action packed, no holds barred game [NOT]. Spoiler alert, the final score is ONE - NIL, but guess who wins! Build It Now, Fix It Later The Valve.Computer is a once in lifetime project for me, so I decided not to do a “build it now, fix it later” version. Which reminds me, the ducks on the pond needed a lighthouse, as it is sometimes difficult to see the edge of the pond. I thought I would help, and after discussing it with no one I just built a lighthouse. Just doing it is far more fun than planning and project managing, the only problem is the quality of the result. It was not even watertight, and a bit of a family joke, and so it went into the shed. I decided to build a second one. This time I had a plan. Plan It Now, Build It Later Project management is not just a tool to mislead the client into thinking you're actually doing something. It can also be used to create a high quality product, on time, and slightly over budget. For the second lighthouse I project managed, designed and then 3D printed all the parts, It looks great, and fits the brief completely. But guess which one I now prefer. So two lighthouses later, I realised that the act of planning sometimes loses the spirit of the idea. Even so, I decided to try to plan and manage the Valve.Computer project, and fortunately, as the build grew, it gained more spirit than I can shake a stick at. The Valve.Computer Project My stepdaughter is a senior Product Manager and my stepson is a Chartered Engineer and he frequently works as a Project Manager. They would know how to manage this project. Luckily they are too successful and busy to ever read this, thank goodness! I tried doing the project management on a PC but gave up, and instead used 3 coloured pens, A4 paper and self adhesive address labels to cover up my mistakes. I simply found it easier. The only problem I discovered is that the paper gets a bit thick after several layers of amendments, but the big advantage is that can always find your last version, it's simply the thickest. The main thing I discovered over the year was that the first time I tried almost anything I failed, but the annoying experience often enabled future success. I started by stabbing at various bits of the idea. Eventually I was able to define what I wanted to do. I now realise that this definition, was the most important part of the whole project management. The program seemed to consist of many complex components, all of which I needed to understand, to be able to build the Valve.Computer. How do thermionic valves work, how do you design a PCB, and how on earth do you get from a NOR gate, to a memory register, via microcodes from a clock and a ROM, to actually run software. And what exactly is a microcode anyway. At times, in fact most of the time, it all just seemed impossible. The main solutions I found were, firstly to keep the number of unique sub systems to an absolute minimum, secondly to use double triode valves to halve the physical size, which also reduced the construction time, and thirdly, to design multi layer printed circuit boards to reduce the inevitable build errors and produce a robust construction. So far it has only gone BANG twice. The first time I pulled out plugs like my life depended on it, and Judy cried, “Everything all right!”. The second time I only slightly panicked, and Judy continued to decorate the Christmas tree, but later bought me a surpise present, a fire extinguisher! But most important of all, is to have a lovely wife, who knows you're daft as a brush, and that life together is brilliant. © 2024 Valve.Computer",
    "commentLink": "https://news.ycombinator.com/item?id=40814011",
    "commentBody": "A modern 8 bit design, built using 1950s thermionic valves (valve.computer)388 points by cenazoic 23 hours agohidepastfavorite70 comments 082349872349872 23 hours ago> When all the valves are glowing, I check the fire extinguisher is full, and run the code. It wasn't until working with valve hardware that I finally grokked the original difference between a cold boot and a warm one. reply ahazred8ta 20 hours agoparent♫ \"And when they turn the power on, it's sure to dim the lamps / At plus and minus 16 volts and fourteen hundred amps.\" -- Frank Hayes reply JJMcJ 19 hours agorootparentTelephone central exchanges used to have enormous lead acid battery (like automobile batteries, the starter motor not the EV kind) arrays if they lost power. It was amazing to see the battery array discharge ammeter at 5000 amps when utility power went out. reply _joel 8 hours agorootparentsome ISP's still do (or did, last time I was in one), at least one I worked for a few years back. Had a room full of lead acid batteries on shelves and another with a big spinning wheel to smooth out the flow. reply Terr_ 19 hours agorootparentprevSong audio: https://m.youtube.com/watch?v=bMS6G83NqFQ reply kazinator 14 hours agoparentprevWhat? Silicone hardware generates heat. Neither silicone hardware nor tube hardware goes literally cold in a cold boot; just the power is cut for a moment, usually not long enough to lose a lot of heat. reply cjs_ac 10 hours agorootparentThe difference is that for solid-state logic, the heat is purely a waste byproduct, whereas for thermionic valves, the heat is the energy used by the electrons to move between the electrodes, hence the name. If the valves are cold, you have to wait for them to warm up before they can do anything. reply kazinator 8 hours agorootparentAll the same, \"cold boot\" doesn't refer to letting tubes cool down. If power is restored while the tubes are still hot, it's still a \"cold boot\". The electrons are not moved by heat; they are moved by the electric potential between the electrodes. The heat goes into maintaining the cathode at a certain temperature, at which the electrons \"boil off\" in sufficient numbers, but that requires only a smidgeon of the energy. Though depending on it, tubes are incredibly wasteful of heat. So you say silicon and other parts don't require heat? Check that data sheet again! If the part's operating range is from -40°C to 70°C, then that part requires heat. If the ambient temperature happens to be -100°C, you have to heat it, perhaps by installing a heating filament into the chassis. E.g. check the TI datasheet for the NE5532 op-amps: Free air operating temperature: NE5532, NE5532A: 0 to 70°C. SA5532, SA5532A: –40 to 85°C. (That standard part is quite delicate there, only down to zero!) Conversely, thermionic tubes for a hot environment could be built without heating filaments. If the ambient temperature is 500°C, and that happens to be quite enough for that tube's cathode, then it Just Works, like your NE5532 IC at 3°C. reply tlb 6 hours agorootparentIs this known to work? I would have thought that running a whole tube at high temperature would cause the grid to emit thermal electrons, creating a current between grid and plate. Even though the grid doesn't have the low-work-function coating that the cathode does, its surface area can be larger and it's usually more negative than the cathode so I'd expect substantial currents. If this does work, you could save a lot of power by building heated, insulated enclosures for entire tube appliances rather than heating each cathode and cooling every tube. reply tlb 1 hour agorootparentThough I guess you could save even more power by using transistors. reply Max-q 7 hours agorootparentprevThe valves need to be warmed up can still be the source of the names \"cold boot\" and \"warm boot\", even if we today have given them different meaning. reply kazinator 6 hours agorootparentThe meanings we give them today go back almost 50 years. Then the trail runs cold, pardon the pun. According to Google n-gram searches, \"cold reboot\" and \"warm reboot\" didn't exist at all until the late 1970s. Related terms like \"cold boot\" and \"warm boot\" come up, but only in references to footwear. Can't find any computer uses 1950-1970. The \"cold start\" and \"warm start\" mostly come up in automotive, aviation or marine contexts, confounding the search. Likewise, not finding computer uses. Not finding computer uses for \"cold restart\" and \"warm restart\", either. All these terms take off as computer terms in the late 1970's. By the way, an important feature of the warm reboot is (possibly) that data is still in memory (if you have the kind of machine whose memory is wiped when power is cut, not magnetic core). In machines with simple operating systems, you could recover your program or data from memory after a reset. (I think the timeline in the n-grams coincides with the explosion in microcomputing and reflects that: penetration of the jargon into mainstream writing. Nevertheless, the case doesn't seem to be good though for a vacuum tube origin of warm {start/restart/reboot}.) reply 01HNNWZ0MV43FF 13 hours agorootparentprevSilicon. Silicone is the plastic used to make crummy kitchen utensils reply Doxin 12 hours agorootparentSilicone is a rubber made to make good kitchen utensils. The crummy plastic ones tend to be nylon or the like. Biggest tell is that silicone does not melt. reply kazinator 12 hours agorootparentprevI'm amazed I made the typo twice in the same post. Kazinator comments containing \"silicone\": https://hn.algolia.com/?dateRange=all&page=0&prefix=false&qu... Kazinator comments using \"silicon\" https://hn.algolia.com/?dateRange=all&page=0&prefix=false&qu... reply klyrs 2 hours agorootparent> kazinator|7 years ago ... Love the irony of how silicon is used. Common confusion; silicon is a metalloid, irony is metallic reply tass 14 hours agorootparentprevI imagine these terms could have changed over time, but consider that a warm boot might have meant power off and on with these old computers. One difference is that the valves have more chance of burning out on a cold boot due to the temperature change. reply kazinator 10 hours agorootparentFilaments experience a high inrush current on startup because their resistance is lower when they are cold. That's why incandescent bulbs often burn out on powerup, rather than in the middle of a duty cycle. Tube filaments last a good long time though, particularly in small signal tubes. They do not shine like light bulb filaments; they glow red. The circuit designer also has a say in it; tube filaments can be operated over a range of currents: you can run tubes hotter or colder. In a digital application, you'd probably want to go as cold as you can get away with for longer life. Tubes have various modes of failure in addition to burned out filaments, like \"gassing out\", or the cathode emission decreasing, eroding the gain. Of course, abnormal conditions like parts melting from overcurrent. Speaking of incandescent bulbs: those are victims of planned obsolescence. A bulb can easily be made that will last 50 years; it's just not profitable because you can't charge 50 times more for it than one that burns out in a year. Once everyone has 50 year bulbs, you're out of business. reply ekidd 8 hours agorootparent> Once everyone has 50 year bulbs, you're out of business. This seems to have been the issue with the expensive Philips Hue LED bulbs (the ZigBee ones). We installed ours close to a decade ago, and we took them with us when we moved to a new house. I don't think I've replaced a single one yet. Philips seems to have de-emphasized that set of bulbs in favor of cheaper models, as far as I can tell? reply jaystraw 10 hours agorootparentprevin the music world, variacs are used to bring old tube amplifiers up to power slowly for that reason. not after being repaired or refurbished generally, but certainly when the amp's condition is unknown. the author of tfa said his new power supplies do that automatically, which is pretty neat. reply 082349872349872 13 hours agorootparentprevFirst boot in the morning was cold, and —the hardware having literally been cold— took minutes longer than subsequent, warm, boots. reply kazinator 10 hours agorootparentThat wouldn't be on account of the tubes; tube filaments do not take minutes to heat up the cathodes. Warm boots can save time by not repeating all the boostrapping steps. In a warm boot, you typically don't execute any power-on self-checks, for starters. reply 082349872349872 6 hours agorootparentMaybe I'm misremembering (I was very young) or maybe I was with people who were too superstitious, but these tubes themselves were actually from the 1950s, and they did indeed take a significant amount of time before they decided the hardware was \"warm enough\" that one could let the software start doing its thing. reply bentt 21 hours agoprev\"But most important of all, is to have a lovely wife, who knows you're daft as a brush, and that life together is brilliant.\" Hold out for the partner that cheers you on when you're doing what you love. reply naikrovek 21 hours agoparent> Hold out for the partner that cheers you on when you're doing what you love. If only it were that easy. Those kinds of people are few and far between. I’m almost 50 and I don’t think I’ve ever even met a woman who has actively encouraged me about a single thing. I’ve been married twice and have 4 daughters. reply kstenerud 13 hours agorootparentYou must advertise to potential mates who you are from the start, so that you naturally attract those who like your deal and chase away those who don't. I met my second wife this way, and our 6 years together were the best of my life. She was killed by a Russian tank in Ukraine in January, and I miss her more than anything. reply reneherse 13 hours agorootparentI'm very sorry for your loss. reply maccard 5 hours agorootparentprevIf you've had 6 women close to you in your life and not one of them has supported you, the problem might be you and not them. reply bentt 16 hours agorootparentprevIt's not easy. I missed once but struck gold the second time. I'm sorry you haven't felt supported. It's really hard when you don't feel like you can just be yourself. reply anonzzzies 20 hours agorootparentprevHence the hold out. I (50m) didn’t meet her until I was 30: until that time I dated and had few months relationships with a lot but it never felt quite right. Which became blatantly obvious when I met my wife. reply shiroiushi 17 hours agorootparentSame here, but it took me much longer than that. Partners like this are out there, but in my experience they're somewhat rare, plus you still have to deal with all the other compatibility/attraction issues present in dating (i.e., you might meet someone who's really lovely and supportive, but they're just not romantically interested in you, or the two of you just don't make a great couple for some reason). One piece of advice I do have, though: hold out for someone who's a great partner who you really enjoy spending time with. Don't settle for someone who obviously isn't just because other people are pressuring you to \"find someone\". reply salvagedcircuit 17 hours agoprevThis guy is the right kind of crazy. This is truly what the internet was made for: sharing our outlandish, over-the-top projects with others. Awesome writeup! reply FredPret 23 hours agoprevIf Lawrence Pritchard Waterhouse had a blog an a large stock of valves, it'd be close to this reply ssdsa 2 hours agoprevIt's really cool to design all those NOR gates using thermionic valves, as shown in the schematics in the middle of the article. I just wonder if using two regular diodes in the input paths of each NOR gate and another diode in the output path is a little bit of \"cheating\", since diodes are somewhat newer technology than thermionic valves. reply mikewarot 10 hours agoprevHaving become accustomed to helping a friend fix old ham radio gear, 80 volts doesn't strike me as terribly dangerous. In some of this stuff, over half of the power supplies output was dumped as heat in resistive divider networks just to bias things correctly and ensure operation. The filaments worked out to less than a fifth of the load in many cases. Coming from a background of transistors and chips, it was wild to see so many 5 watt or more resistors in use. reply cellularmitosis 22 hours agoprev> 200 amps Something I’ve been curious about: is the current actually required for the thermionic effect, or just the heat? Could you lower the current requirement by thermally insulating the tubes? reply doe_eyes 22 hours agoparentThey are insulated really well - by vacuum! I'm actually surprised by the figure, though. A small tube requires about 300 mA at 6 V, and the trick is that you can connect the heaters in series instead of doing it all in parallel and pumping out a ton of amps at a very low voltage. They could've done 10 tubes in series at a reasonably safe 60 VDC, and they'd only need 20 amps. Back in that era, because both valves and relays were expensive, it was also common to use them more creatively than just constructing standard logic gates. You'd try to make a full adder or a flip-flop cell as an analog circuit, breaking the abstractions we're now used to - but also saving components. reply hilbert42 17 hours agorootparent\"They could've done 10 tubes in series at a reasonably safe 60 VDC, and they'd only need 20 amps.\" Thermionic vacuum tubes of this type usually have a specified maximum heater/cathode voltage rating which varies considerably according to design. Exceeding that rating and one risks a short between the heater and cathode. For these types of tubes heaters can safely operate up to 200V negative with respect to the cathode and about 100V positive. In my post I suggested substituting a tube that's more common in the West—the 12AT7, it has the advantage of having a 'tapped' heater which means it can be wired in parallel mode to operate at 6.3V or in series mode at 12.6V. At 12.6V the current would be halved: https://en.wikipedia.org/wiki/12AT7 (pins 4 and 5, the tap on pin 9). reply bregma 8 hours agorootparentWhen I was in high school (many decades ago now) we had a vacuum tube tester with a switch that allowed you to select the heater voltage. We quickly discovered, devils that we were, that if you stuck a simple diode with a 1.5 V heater in the tester and cranked the voltage to max it would launch like a little rocket. Eye protection recommended. reply hilbert42 6 hours agorootparent\"We quickly discovered, devils that we were, that if you stuck a simple diode with a 1.5 V heater in the tester and cranked the voltage to max it would launch like a little rocket.\" Where I once worked we had several AVO Mk III valve testers† which we used in a nice little \"demo\" (for want of a better word) for both new employees and non-electronics types who'd occasionally wander into the engineering/electronics department. We'd take a 7 or 9-pin miniature valve (preferability 9-pin) and place it under water and break the evacuating seal on its top, being evacuated the valve would instantly fill with water. Now with suitable settings on the AVO we'd get the water to boil with steam bubbling out of its top. This all happened whilst we nonchalantly went about our business pretending that nothing unusual was happening. Sometimes the reaction from the newcomers/visitors was so funny that those of us who couldn't keep a straight face would quickly exit the lab and burst into hysterical laughter. That was party trick number one, there were more: half fill a CRT with water by the same process and put it back into the monitor for some poor unsuspecting tech to discover. Another was our famous CO2-powered valve gun which we'd use to shoot 7-pin and 9-pin valves at high speed across the carpark aimed at the door of the electricians' department with whom we were continually at war. The valves would embed themselves in the wooden door up to the full length of their pins and rarely would the glass break. Electricians would come in next morning to find our little gifts awaiting them. Yet another was the exploding electrolytic capacitor under one's seat. And there are many more to tell. Believe it or not, we were quite a professional outfit and our work output was excellent. But it was the funniest and most enjoyable place I've ever worked at. † https://www.radiomuseum.org/r/avo_valve_tester_mk3_mk_3.html reply userbinator 18 hours agorootparentprevand the trick is that you can connect the heaters in series instead of doing it all in parallel and pumping out a ton of amps at a very low voltage. Yes, this is what a lot of tube equipment did, as they are naturally high-voltage, low-current devices; here is one notable example: https://en.wikipedia.org/wiki/All_American_Five reply mannykannot 22 hours agoparentprevI did a quick search for the specs, and, if I am reading it right, Wikipedia [1] gives the filament current as being 350mA at 6.3V, a dissipation of 2.2W. As that current sums to 196A for the 560 tubes, I suspect this is what the 200A figure refers to. As the tubes are at high vacuum, they are already well-insulated, so I imagine that most of the heat loss is via infra-red radiation. I have a very vague recollection that, in thermionic tubes, the anode has to be kept reasonably cool so that it is not emitting electrons itself. I would be surprised if there are any low-hanging fruit to be plucked here, especially given that vacuum tubes were important technology for a half-century. [1] https://en.wikipedia.org/wiki/6N3P reply hilbert42 16 hours agoparentprev\"Could you lower the current requirement by thermally insulating the tubes?\" The thermionic effect is very interesting, if the right material is used to coat the cathode then very large emissions can be had. Combinations of oxides such as barium, strontium and others can have both low work functions and high emissions. Currents in the region of over 100A/sq cm can be achieved. Thus, valves/tubes could be designed to be much smaller and have much smaller currents. For a digital application such as this only a very small cathode current would be needed, this then would mean a much smaller heater could be used. In the past, miniaturizing vacuum tubes was desirable but wasn't a major priority and further development was stopped when the transistor became available. That said, in the 1950s portable tube radios were available that used much less heater power than their mains-operated counterparts, for example tubes like the 3V4. It has a directly-heated cathode and a filament/ heater voltage of 1.4V and current of only 100mA (in series mode it operates at 2.8V at only 50mA). reply jhallenworld 3 hours agorootparentThe directly-heated cathode (meaning that the heater is the cathode) tubes have another advantage: they are nearly instant turn-on, no warm-up needed. Same as vacuum fluorescent displays. Reminds me: there are new tubes based on VFD: https://www.korgnutube.com/en (only 12mW heater power) Very old 1920s tubes also used direct cathodes (but used a huge amount of heater power). If you look at the circuits, they had to jump through hoops to have the desired grid to cathode bias while at the same time providing the heater current. I think this would be easier for logic gates: set all of them to ground. reply Ductapemaster 22 hours agoparentprevCurrent into a low-resistance \"heater\" element is used to produce the heat required for Thermionic Emission [0] in a vacuum tube. You only need the heater/emitter to be hot, and insulating the tubes would just spread the heat around to everything inside of it — at some extreme, making everything into an emitter, instead of elements that control the emission. [0] https://en.wikipedia.org/wiki/Thermionic_emission reply doe_eyes 21 hours agorootparentThis is not really accurate. To get meaningful emissions from normal electrodes, you need to heat them up to about 2000 °C. Vacuum tubes operate at 700 °C or something like that. The trick is that one electrode is doped with special rare-earth additives that greatly increase electron emissions. The same treatment isn't applied to the rest of the device. So, even if all internal components have the same temperature, a vacuum tube can still work (to some extent). reply Ductapemaster 21 hours agorootparent100% correct and I appreciate the additional details! I couldn't come up with a good analogy to explain you want the emitter as a separate and unique element from everything else involved in a tube — oversimplified in the process. reply Prcmaker 21 hours agoparentprevReally it's both ways in terms of required current. The thermionic effect requires filament heat, which since being a filament, take some amount of current, often around half an amp, some more some less. A filament could, in theory, be run at any current though so long as the power through the filament stays the same and the voltage is kept low enough not to arc to adjacent parts. There is likely also a minimum current requirement (since you need a source for those free electrons), which often then implies some non-linearity at the low end. As others have mentioned, the 200amps of this case could be reduced substantially by running filaments in series (can be done with 6.3v heaters as the error from a common 5v or 9v supply is more than if you pair them up and use a 12V supply) though this introduces the failure mode of old Christmas tree lights. Source: I make vacuum tubes. reply creer 20 hours agoparentprevThere are tubes made which share one heater. For an exotic example with a working web page, see this tube where two triodes and one pentode share the same one heater filament. https://vinylsavor.blogspot.com/2021/11/tube-of-month-6bh11.... This one has two diodes and two triodes. https://vinylsavor.blogspot.com/search/label/6AY11 And in this very design, they chose \"6N3P valve contains 2 triodes around a single heater, halving the physical size and power requirements.\" There may have been tubes made where the triode function can be pretty rough (sufficient for a digital circuit) and several of them could share one enclosure. In the tubes shown above, apparently the limit was the number of pins on the socket - but also that all these active elements do not share any pin. Insulating the whole thing would run into issues like burning wire insulation. reply mrob 22 hours agoparentprevPerhaps the inside of the tube could be coated with a thin layer of gold using evaporative deposition before the grid/plate/filament/etc is added, like on spacesuit visors for IR reflection. reply an_aparallel 21 hours agoparentprevThe high voltage from my understanding is to be able to handle the high inrush current to the heaters on power up, I believe after that the requirements are lower (parroting what I've read written by Eric Barbour on his metasonix vacuum tube synthesizers) reply hilbert42 18 hours agoprevNow I'd truly love that computer as a talking point in say the rumpus room of my house, I could entertain my techie friends for hours. And it would take on extra importance in winter in keeping my house warm. I've not used 6N3P tubes before but looking at the circuit it seems to me a 12AT7 (an old favorite of mine) would substitute in that circuit almost without alteration. Then again with a tweak or two either the higher gain 12AX7 or its lower gain cousin the 12AU7 would do. reply Max-q 7 hours agoprevThis is so interesting! Quite often, I'm thinking about how a modern design would perform with tubes. Back in the 40s and 50s, digital computers were new. Much of the design was inherited from tabulating machines. When I study the early IBM machines, they seem very complex. When we learned more during the 60s, and ended up with microprocessors in the 70s, the design had been optimized a lot. So I have been wondering how a \"modern\" design would perform on tubes. This guy actually did it. What a fun project it must have been. Super interesting read. One detail in the introduction that made me a bit puzzled: \"The Valve.Computer is an 8 bit computer, with the usual 12 bit address and data buses\". In an 8 bit CPU, we had \"the usual 16 bit address bus and the 8 bit address bus\". reply nicetryguy 22 hours agoprevGreat for the winter! In all seriousness, amazing work. It's been tough to get tubes lately with the whole Ukraine situation sadly... reply lemonlime0x3C33 6 hours agoprevI was really hoping to see a picture of it in dim light with the valves glowing. Also I was not expecting to see such an adorable light house for ducks in the post. All around a great read :) reply nico 23 hours agoprevWhat a fascinating project. So cool Thank you for making it happen and sharing it here Super interesting read and very inspiring too reply cududa 20 hours agoprevVery cool. Couldn’t get the pictures to load in any higher res. Would love to see more of the detail reply fanf2 18 hours agoparentVery hot. reply cobbzilla 18 hours agoprevThat’s some serious power draw. Truly impressive work! reply poulpy123 9 hours agoprevI'm now wondering if someone ever tried to build a small computer/calculator with steam valves reply dang 23 hours agoprevnext [8 more] [stub for offtopicness] reply dr_dshiv 23 hours agoparentIs that a joke? Because this is the most hackernews article ever reply dang 23 hours agorootparentOh the article is thoroughly on topic! I mean the comments related to Valve the company. The thread was filling up with those before we changed the title. (Submitted title was \"Valve.computer\".) Sometimes when there are too many offtopic comments like this I make a stub comment, move all the offtopic subthreads underneath it, and then collapse it. https://hn.algolia.com/?dateRange=all&page=0&prefix=false&qu... reply GaylordTuring 23 hours agoparentprevGod dammit! I thought Valve had released a stationary computer. I was ready to open my wallet even before clicking the link ;( reply dang 23 hours agorootparentI suppose we'd better change the title before the entire thread fills up with variations of this reaction! reply Insanity 23 hours agorootparentprevHa, exact same reaction here! reply artyom 23 hours agorootparentOh man how many of us are out there? I wasn't disappointed about the actual article tho. reply artemonster 23 hours agoparentprevhl3 not confirmed :( reply orbat 21 hours agoprev [–] > Every one agrees that the Turner Prize is much more than just a display of virtue signalling by the cultural elite, and I have decided to enter the Valve.Computer for the prize. Uh, what? Where did this sudden \"virtue signaling\" by the \"cultural elite\" stuff come from? reply Grimblewald 15 hours agoparent [–] read the wiki page on the turner prize for more information, but the prize is rather controversial. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Valve.Computer is a modern 8-bit computer using 1950s thermionic valves, first operational on May 28, 2021, and now mounted as an art installation.",
      "It uses 1,120 thermionic triodes configured as NOR gates, forming memory registers, an 8-bit ALU, oscillators, and relay drivers, and can run programs like PONG and a 32-bit Fibonacci sequence.",
      "The project, inspired by Bletchley Park, took 18 months to build and is entered for the Turner Prize, highlighting its artistic and technical merits."
    ],
    "commentSummary": [
      "A modern 8-bit computer built using 1950s thermionic valves has generated significant interest and discussion among tech enthusiasts.",
      "Users shared stories about old hardware, the differences between cold and warm boots, and the complexities of thermionic valves, highlighting both the challenges and dedication involved.",
      "The project was praised for its creativity, evoking nostalgia and emphasizing the importance of supportive partners in pursuing one's passions."
    ],
    "points": 388,
    "commentCount": 70,
    "retryCount": 0,
    "time": 1719515890
  },
  {
    "id": 40820949,
    "title": "Supreme Court overturns 40-year-old \"Chevron deference\" doctrine",
    "originLink": "https://www.axios.com/2024/06/28/supreme-court-chevron-doctrine-ruling",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131}button,html{font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}@media (prefers-color-scheme:dark){body{background-color:#222;color:#d9d9d9}body a{color:#fff}body a:hover{color:#ee730a;text-decoration:underline}body .lds-ring div{border-color:#999 transparent transparent}body .font-red{color:#b20f03}body .pow-button{background-color:#4693ff;color:#1d1d1d}body #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}}body{display:flex;flex-direction:column;min-height:100vh}body.no-js .loading-spinner{visibility:hidden}body.no-js .challenge-running{display:none}body.dark{background-color:#222;color:#d9d9d9}body.dark a{color:#fff}body.dark a:hover{color:#ee730a;text-decoration:underline}body.dark .lds-ring div{border-color:#999 transparent transparent}body.dark .font-red{color:#b20f03}body.dark .pow-button{background-color:#4693ff;color:#1d1d1d}body.dark #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.dark #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}body.light{background-color:transparent;color:#313131}body.light a{color:#0051c3}body.light a:hover{color:#ee730a;text-decoration:underline}body.light .lds-ring div{border-color:#595959 transparent transparent}body.light .font-red{color:#fc574a}body.light .pow-button{background-color:#003681;border-color:#003681;color:#fff}body.light #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.light #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI2ZjNTc0YSIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjZmM1NzRhIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}a{background-color:transparent;color:#0051c3;text-decoration:none;transition:color .15s ease}a:hover{color:#ee730a;text-decoration:underline}.main-content{margin:8rem auto;max-width:60rem;width:100%}.heading-favicon{height:2rem;margin-right:.5rem;width:2rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"www.axios.com\",cType: 'non-interactive',cNounce: '63079',cRay: '89afee298cc197ff',cHash: 'fa9aec910db5528',cUPMDTk: \"\\/2024\\/06\\/28\\/supreme-court-chevron-doctrine-ruling?__cf_chl_tk=JEjrPC.UWRauh0Wop9QSBgY6xQ052wUyEyLBmXsrSEY-1719601337-0.0.1.1-3774\",cFPWv: 'g',cTTimeMs: '1000',cMTimeMs: '120000',cTplV: 5,cTplB: 'cf',cK: \"visitor-time\",fa: \"\\/2024\\/06\\/28\\/supreme-court-chevron-doctrine-ruling?__cf_chl_f_tk=JEjrPC.UWRauh0Wop9QSBgY6xQ052wUyEyLBmXsrSEY-1719601337-0.0.1.1-3774\",md: \"d6bQLTELjCMbDLwlPhsMUN_2hu7KEqAyJgibNy1oZPQ-1719601337-1.1.1.1-mCz_8.LIXMZw9KGsJQNkl93TfZtqENypO1gZQuJr42x6gCcowfK5ODQdRl9odpN5Wj5YwdfqQ3030z71hC4hfzCPaM42ZHtu8HeBw0NInIGzpzikYm7CWGJVoKmP.nMaa9ipGyf0UmLTankbnaRhEv4jCWVu3.sWzBwo_3at4btZ.qgyRmW033otje.kRwl.MeDj09.KABjRV4UJHImlhHVcVfU2D2j5082xiFzw3sZfGDoa6DRg6.ofFRHZiyBH_SNv7WqEcDL7ZzA1b7UuKqkXPv.2EGw6cz4D1yB3U5jzsk1Iw4VM.PxA2.ZKtGinqXHn7hxz889PM55JvWgsWamPTs3R0TtjfVJRR2ZEyXAYZOfSfDZSk8b9oRqPg4indp0eHFLd.p0hXEFB051kmG0GNerj9CNL4OLomcpW62fVga0gdYql0Ev6RGgHNJqWWqTWgniACVPDWZl2sGhCult3LKY2.ZDETxC6R.Jgx75pjI5G74sPdjeCSItrPpsV17b.mPPXF5SyQ06CHDpNti1swbKaD0ksohE6LIsitHFeWav9jwmufkQ7tr4ziPcVucilBZOGJQ4UEYc6pYpRXwXhty9VW0ktsGft5yncpUDJ2rbDf_7jBde46Noq3bzpVYXpCGkBhQbv2VYGsMepIOcuGlNOzPFIp_TTVyxSbENOqSjcpWU3Krrs02XIK78Woz4pvtujvRFZri0drn8gVC5hrGh2P25FZWFqcwGU.2D2XNvHUYFHbwoPrmq_5Dl34maKUe2_..oLvMd5RBd4vfGcRhDNlEdIXGnNquSs.r.Za7e6vBYr_FJBqG55ququgVfrch0QCMNtwvyWbudZEaOv9DJCZcMk0iwBUJ0ZcV4NK.vx_k_TbOxqkTbgZT7hmKQmVLJE4bvebRXUbORRYa58w9wD7mZ.3KlZY_HeBRJDtlkYvXB5AeL.jeNxH0.taJvX.po0fl2UlkB0weoopGiwy0ye.C5eqB1JuAIX8EMJDOFgeKKDnFEGjGY.RtYgxHJDkRocmCFSogj1p_CqeF7pyAYIMyCsak3xLfaKucjJAW3Apvmsf7FOAsUJZF_1o6Fs3MtK8dv1SSbhVZ6ZYovJpVBUtGY6K.7eh9dRSD02QRkznH0xSa5gyA7iL4zo5yuzEhcCGGb6uJU7xMFWUYcSk_yBfk5wMYLWQOo.Bk8rPI4J4jlTRTFtm2g6PJgUS6aAav.6Jag_uW2V_GzdNSkxm_cUO9VE7r2jQSmWCruuMoh_XVYi0UX3pGoDneT.ZJsVQY7DjIPfgY6F0QE4gBCGMW3td0.pOhxzNn8RSfvOAjSWxp5iDGglGEMKY1oL\",mdrd: \"MhWZjAhycdKRvJ2PPCc4GxHm6eSDoTDgO_CU_T3dOJo-1719601337-1.1.1.1-l_lGaMjdoIPhKOm9KkJ2Bt_FkGJhODvOA9bqIfynf3aEfMB.jzUmRZhLVUk7cPxcZjPTkhwr1gukqNptz1KHaFfu04PItp0RhPMb_AHO5rZpa_Efyp.C4PYjwmsZ.ilOPPvOJn.XtIus0AZWqFURfhG5GcYNAyL7ix_LgLMH1prdwKJCQyQ5rJIOKT0sKEI0Nk5HvLz5xuSag3aON1cRs0UF323rcmnf8O._DCWI0a8qtOYlB3iQPYhAVIC5DJfpJLdlImCvnlJFUP8bhJ7ZtTDv5aiUT5nI24yeb9dTwrPGl4vLgoQOFmtjEn2yHSyZYVa9BFVYUmAaxZjMDqTyjtDBRWy9S5DCTgOx8phcbAVehstHd8USB.sjQW5ZWWU1VyUOIx_JHuBWzqZh42FJEY1_xZKwpy.FWz32sL79DxYflwmJ_7xdd9qcXSscUmGlinFTR3eHLEqPYYsmNVEl1f9KQg97AQfTtAmK2KETqmtxigf60Yk.eAlenlFlxk21Xf79r7CGQOLh.U9.JE9SGJ9AvbSik2G4ntRqzAnO4F82Z88SyrEFkC2080uo51iTH8UgnlxaZZb5X9dHAz.m_LWv5.zjxvKW3sJiZy5kVXBw6qv3YhcR94WGukPHtwSfoze3WkEkSnIronibGobJRIOSLSMASZCp6OvkrncwlzJ2Fhf43Bqjb.nO1HCd3v7aRhyq5aL0rLGYlphujZ83jKYyX3twr6VF1dXg_ZaeNKeDUI1W2LBHBkOhs4uTQKCipKvHhyBpe9TDkQUS5XlQFZVEIEvORtVT5_a3s8_PJmdIJspD3uS0es49dguq4H_5KdFTd_.muv4C6DwDkjXJ0x_iQBPksVHKXF2AkqKz.S9Ae4f8GpUe0VC9ySNp5SpUobi4bOxLMZvvN_w6Pt6dqmt2slCaYYEcMyW8h.0DAGonBQe5ravMxImUHwQKzSnblNM8IBkp7UpXtRDjmjGffXYuTkOvVvevdexQXKA5vkvHTQ98Mpx3EwpXDImFKEXTYSDnup53njbHFNoQtVffdaeLH6jpmxy9T7RHZC1KSfbzvH27DHYRf6Xu45Lh12DFAV5ouWqxPPmkq0NpX9m2ppQZLsfTHTI1Ir4lI.bOUWD3y5xThtTPVrFaTAY1wdD51JYdbQ6GtKkNlXdygAASJWiVOGpYtr4nAw1XulSdVqqjm1sP2Pf28ymdDv7cdX4zoNrwSVfZ9c8rKcqKNj_o2nyNATbRCgVIpNKObwZwEsYzFTWKtI0SPVneomlsVS20Lqa3pcRkm81OQDLbN1JkT_HyUgU87n2PiyaToFwirAzuhR5SvYdqd4MghZrjzqERivV2.kJwqO_PNwdadIU9suryxu_A8SSuwQwk3LRrmmIKqSXiZNaGbtojfRxz330sJ__fSFF9zORKuaSoORhdN.XdmyBvCS7SHOoL7GCrGSv3CeiEcvcM4m.B.OgGd1t5sIeIj6rLsURLlso49x7aFo4EgVoTC4vWgAt3Atvcwk7ZKWl1X_GZavmZNVbYfdlUn3jdZ6nJC8XbaXunYZ.YSn1gESpjAnlzifMMFKf1pXd_Sv2_jXWpb0uBh2amdPZAkshTdy70jrJEb9CKQ09fmnv7_VPMY0.vDy4yvP9aOC3wiZ1RK3PGit.jdG3skKk9XbQWEcZ4X1XK9OE5g1r_fvmENo5k2mwxvF1.rfNdNv0mCrkXXmZ81NrmzM9o9P78tBSQd6Qutqo_DQixZtIn12YH0ZmRXv8EjtIq5le1gNvTNvmaz6771h.rNx8gGPu6eWKix7mfGZXqIT1eDt1ium2aEwiGdOKfRuQJGLK3tIC8NI4LaAoVmycS1rYw25Zhomq_2lR7BcZpCaQKM_pFzJk7ghcBDDz14REJ0dacvkHB.X0a2_M4vx2xv_7e7f3Gt7Fcw.Ag2iWz7fWNj3PcvCDjBmwhsMsqLgz4N5fVKSZxGU9XTJcN0JCYWfh12K8OADFkAXI3uThBL4jHD._9TXiDJoo1VlRwGrp8Y2NQLy7O9PyrBEYXval46k8krvsCMwJmfUPwO.T4GgqkdsswQmzdq4z4Z_r._lmf.zHUf0pAw7nTFZYLJ08NmkH_LDoddIiZbePQoXq.PDlvmoIemNkUwXo5t1C49_F3eIusUEPhZ3cs9kl65NydctykZlHo3dNi0gT8yRk0L96Tyude9kjpyOEwr9WSioHkLCobE4rwfMIEEty39IKNxbNgz7mrbm8KJ4BXvVeOCbo0Ca5IYjbctlF2dj05drdd6WYoC8IcCMehXJ_zDjZfoi9EdM8k\",cRq: {ru: 'aHR0cHM6Ly93d3cuYXhpb3MuY29tLzIwMjQvMDYvMjgvc3VwcmVtZS1jb3VydC1jaGV2cm9uLWRvY3RyaW5lLXJ1bGluZw==',ra: 'TW96aWxsYS81LjAgKGNvbXBhdGlibGU7IEdvb2dsZWJvdC8yLjE7ICtodHRwOi8vd3d3Lmdvb2dsZS5jb20vYm90Lmh0bWwp',rm: 'R0VU',d: 'WuBNHdlwSmEmf9UKx+lxqD98/LGQyx9BwjmSQWy5s/wFsY74fu8JPEKWIGoEm5KMpRAtPfVmRO+CVO4YdCBp8bkUox6PJH8tQVwE9IRSbmr1hPXHG+3IhXeE0qTbxW2Lvn7ARXQ9Cq8UdtJOc/l3rs+Wb7X3RlcsmJMumqZWQsCDWBKixGGXLYmOkbeNTSb0LTzKyRq08DAPomiXmwMtDO2TzN5Hh6w8WaaPR0wwGpGZYPWcK7l+fC9htLsi9G6Pepa8rtU+IYjmaC72vhFC7kyjcD75E1iDiR0RZlTKd3wYuo25yPs2ZB8hwHXdz7Xo+0C7krQs8HV6zlSxWgrWpCtfGrHJkdbe7p8caavjwI9NcwVgA5HrnqkIlB0ffu8pkZIjB7+6GM9NmEL3QgKdf6TVyaqtBLfOsSOjzve13AZsVkrgaSXIQU/bqoit72iGN7shPcyBm3V7hcUwsC3zdG6RlsYydSNp7i98efDjpGu4/rHtH0FMSXLBP4jOzcJmhsDj/MUcAcXUCVYyg42J9mF5seK21g/OikYc+Y2wZy2N8Kw2UzUqUA5Cwkrfa3wj',t: 'MTcxOTYwMTMzNy4wMDAwMDA=',cT: Math.floor(Date.now() / 1000),m: 'M19ezl2wzbGsA9U9+dFBOfTvjpJwXs1j7omn/dY7dX8=',i1: 'E/P/WY2r8Fj+ZaznW4p7fg==',i2: 'Fg2i6XuCwdTsdt7+gGvZdA==',zh: '4Jlx+EWobopqUAR6XBjkSB7IfrNn0Kxc35weu9M4x8s=',uh: 'idqvltDEaw6z1eUpAaUFY/6rIUCphTJo6GMHGHVnQbg=',hh: 'Z7lhynZnON3PPt+/h43vDZu7Ssbq8FNzJ8B2cbgZ8z0=',}};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/g/orchestrate/chl_page/v1?ray=89afee298cc197ff';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/2024\\/06\\/28\\/supreme-court-chevron-doctrine-ruling?__cf_chl_rt_tk=JEjrPC.UWRauh0Wop9QSBgY6xQ052wUyEyLBmXsrSEY-1719601337-0.0.1.1-3774\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=40820949",
    "commentBody": "Supreme Court overturns 40-year-old \"Chevron deference\" doctrine (axios.com)303 points by wumeow 4 hours agohidepastfavorite657 comments neonate 1 hour agohttps://www.supremecourt.gov/opinions/23pdf/22-451_7m58.pdf consumer451 4 hours agoprevFrom Justice Kagan's dissent on page 82: > This Court has long understood Chevron deference to reflect what Congress would want, and so to be rooted in a presumption of legislative intent. Congress knows that it does not—in fact cannot—write perfectly complete regulatory statutes... > It knows that those statutes will inevitably contain ambiguities that some other actor will have to resolve, and gaps that some other actor will have to fill. And it would usually prefer that actor to be the responsible agency, not a court... > Put all that together and deference to the agency is the almost obvious choice, based on an implicit congressional delegation of interpretive authority. We defer, the Court has explained, “because of a presumption that Congress” would have “desired the agency (rather than the courts)” to exercise “whatever degree of discretion” the statute allows. Smiley v. Citibank (South Dakota), N. A., 517 U. S. 735, 740–741 (1996). > Today, the Court flips the script: It is now “the courts (rather than the agency)” that will wield power when Congress has left an area of interpretive discretion. A rule of judicial humility gives way to a rule of judicial hubris. In recent years, this Court has too often taken for itself decision-making authority Congress assigned to agencies. The Court has substituted its own judgment on workplace health for that of the Occupational Safety and Health Administration; its own judgment on climate change for that of the Environmental Protection Agency; and its own judgment on student loans for that of the Department of Education. reply lolinder 52 minutes agoparentTo put it a bit differently: Congress has not been able to pass substantial laws in decades. The executive branch has filled in by interpreting these laws very loosely in order to adapt to the changing situation and—importantly—to adapt to changing presidencies. That last part is the single biggest problem with the administrative regime as it has stood hitherto: it means that almost everything that happens in the federal government can be completely undone based on the results of a single nationwide election that we have every four years. It means that every right, every process, every plan that interacts with the federal government in any way has a four-year shelf life. Government by administrative rule is why no one is getting too excited about non-competes getting banned or non-solicitation agreements curtailed. It's why I'm nervous about the future of the IRS's free tax filing software. It's why there are whole industries built up around trying to keep up with the latest about-face that the executive branch has made. The existing system of administrative rules absolutely sucks for stability. It sucks for anyone who gets used to a benefit only to have it stripped out with an administrative change. It sucks for anyone who's trying to plan anything out on a longer timetable than four years. If this forces Congress to get their shit together and pass lasting laws that can't just be upended with the next presidential election or if it forces states to start taking on the role that the federal government has hitherto failed to fill then in the long run this ruling will be better for everyone. It's just going to be very uncomfortable for the next few decades as we sort it all out. reply mywittyname 14 minutes agorootparent> The existing system of administrative rules absolutely sucks for stability. This new system is even worse. At some point, the Judiciary will make a poor ruling. Perhaps this ruling is impossible to hold to, but maybe the executive branch decides to usurp the court of its own volition. Then what? The checks and balances system of our government only works when everyone plays nice. But if push comes to shove, then the executive branch is the one that holds all the power. They don't have to obey the legislator nor the judiciary -- neither has any real capability to enforce their will. The country has been slow rolling into single pillar government structure for decades now. IMHO, this ruling is a huge step towards solidifying the executive branch as the de facto sole branch of government. Government agencies were provided a mechanism for all three branches of government to work together, legislators provided scope and leadership, the judiciary provided checks, and the executive provided the operations. Once the agencies are all gutted, a future administration is going get an opportunity to act on their own via executive authority and they will ignore any attempts by the court to stop them, because the court is literally powerless in all but word. And that's what opens the door to a president who begins seizing assets of political opponents. Lots of authoritarian countries masquerade as democracies because legislators and judiciaries are inherently powerless to stop executives. I know plenty of people will counter with the old way was supporting an authoritarian executive. But to them, I'll point out that the agency system has ~100 years of efficacy behind it. reply throwup238 8 minutes agorootparent> This new system is even worse. At some point, the Judiciary will make a poor ruling. Perhaps this ruling is impossible to hold to, but maybe the executive branch decides to usurp the court of its own volition. Then what? It's even worse than that: it's not just one judiciary, each of the District courts is independent until the Supreme Court makes a decision that binds them all. If they have conflicting rulings on Federal regulations that puts every national company in a terrible position of having to figure the two out. Plaintiffs that can jurisdiction shop will have a field day with the friendliest ones. Conflict is all but inevitable here. I suspect even the SCOTUS justices haven't thought through the full consequences of this decision. They're going to be inundated with every petty squabble between Districts soon enough and all the businesses pushing for this will get stuck in catch 22 situations. reply datavirtue 3 minutes agorootparentprevToo much importance and deferral has been granted to the courts. It was a mistake to imbue that institution as the \"third branch of government.\" I came to view that take as propaganda when I was younger and have soured increasingly more on the SCOTUS over these last few decades. I view that court as very dangerous and in need of being smacked down a peg or two. Congress has ultimate authority, period. So the SCOTUS running amok is a very bad look and smell. reply itsdavesanders 42 minutes agorootparentprevAnd imagine how our allies feel. If you can’t count on the U.S. for more than about 3 years at a time, then you quickly move away from them and insure you aren’t so tied to them that a foreign election suddenly makes you vulnerable. Which then makes everyone weaker as a whole and easier to pick off. Which is why U.S. foreign adversaries have been actively sowing chaos for a decade. reply dangoor 30 minutes agorootparentI don't think our allies felt quite so flung about until Trump came along. Sure, administrations might engage a little differently from one another, but fundamentally they could count on the US for a very long time. Presidents did not, before Trump, throw NATO under the bus, for example. reply spurgu 15 minutes agorootparentReminding NATO countries to adhere to the 2% of GDP spending stipulated in the terms of the alliance is \"throwing NATO under the bus\"? Or did he do something else I'm unaware of? reply llamaimperative 0 minutes agorootparentDid you forget the first impeachment for withholding critical aid from a buffer country between NATO and USSR-hopeful in exchange for them investigating his political rival? electrondood 6 minutes agorootparentprevA few: * Convicted Felon Trump has consistently made statements in alignment with Russian interests. * Convicted Felon Trump has threatened to pull the US out of NATO. * Convicted Felon Trump has criticized the mutual defense clause (Article 5) * Convicted Felon Trump's tendency to act unilaterally without consulting with NATO allies concerns them and they see him as unreliable. reply barrkel 11 minutes agorootparentprevAllies were so put out by Bush II that they gave Obama the Nobel peace prize before he did anything. reply datavirtue 0 minutes agorootparentThat was a jaw dropper. refulgentis 40 minutes agorootparentprevFWIW nothing changed re: presidential power over foreign relations. The judiciary held that the legislature can't empower an agency. reply dimitrios1 29 minutes agorootparentprevI am not buying this argument. America for better or worse (mostly worse) has a two party system that in practice functions as mostly a uniparty prioritizing defense spending, entitlements, and the economy, with some lip service paid to red meat/blue meat issues to ensure power is maintained. This means you can reliably predict what American policy will be in any given moment for any given president. Besides, EU member states have had much more iteration on their governments, policies, regulations, and parties. It's not uncommon for a European country to have 7 different parties. And unlike the US, EU's don't hold their constitutions in a such unchanging high regard. Ours is purposefully difficult to change. France, for example, on the other hand, has changed its constitution twenty-five times since circa 1958. edit: I took out He-Who-Must-Not-Be-Named because it seems even here on the board of Very Smart People ™ we can't help ourselves when we see that name and ignore the rest of the point someone tries to make. reply lolinder 26 minutes agorootparent> Arguably, the biggest wrinkle to this was Trump You say this as though he isn't favored to win the next election and take over the presidency and all its policies in about six months. reply beaeglebeachedd 37 minutes agorootparentprevnext [7 more] [flagged] consumer451 34 minutes agorootparentThe biggest beneficiary of Pax Americana is the United States, the second biggest is the entire world. reply phillipcarter 33 minutes agorootparentprevThis is a very uncharitable view of how foreign policy works. It is absolutely in our best interest to not only maintain strong diplomatic ties with peoples and countries who share our values, but also cooperate in defense efforts and ensuring safety and security from military actions that would undo things we benefit from in the long term. reply _DeadFred_ 5 minutes agorootparentSadly America's politicians stopped explaining this and took the American public's support for granted expected us to see things the way the Greatest Generation did. Our allies took our support for granted and spent their money elsewhere, then started making fun of us for subsidizing their affordable healthcare. Our political class getting lazy and stopping doing the hard half of their jobs (effectively communicating and building national consensus) and instead adopting Jon Stewart levels of discourse and you get where we are today. beaeglebeachedd 2 minutes agorootparentLove all the cope of people wanting to fund wars with other people's dime and necks while the guy who actually fought for our ally on his own fucking dime, nothing to fall back on if I was hurt, is flagged. Most of you are cowards. justinclift 23 minutes agorootparentprev> playing world police Don't you mean schoolyard bullies? Because that's how much of the world, including your allies, view the US. Then again, seeing the state of the US police force, maybe that's why US people think bullying and policing is the same thing? reply knowaveragejoe 33 minutes agorootparentprevThat is exactly what the US' adversaries would like you to believe is the case. By all means, carry water for the despots of the world as you turn inwards. reply malcolmgreaves 6 minutes agorootparentprevYou have an incorrect understanding here. What was occurring now was not \"government by administrative rule\" as you put it. It was government by rules passed by Congress. Congress explicitly defers on specifics of some laws to agencies because it is not qualified to provide specifics. Agencies, staffed with experts, are capable of making fine-grained decisions on how to implement laws that Congress passes. This SCOUTS ruling means there will be *significantly more* ambiguity and instability in the federal government. *More* things will be up for destabilization by Republican activist judges. The outcome will be a government that works less efficiently and effectively. This is the Republican play book: purposely make the government worse, distract with absurd claims, then come election time lie and say that the Democrats want to make the Republican's version of bad government even more expansive. Every four to eight years, the Democrats clean up the Republican's mess. Government gets better. Then Republicans lie again and the cycle repeats. reply skhunted 25 minutes agorootparentprevThere has always been judicial review in cases where some affected party felt that administrators of an agency went beyond the mandate. This has been the norm for many decades now. The Supreme court legalized foreign money to use in election campaigns with Citizens United. It has gutted voting rights by allowing thousands of polling stations to be closed. You can easily guess where those polling closures mostly have occurred. A few days ago it effectively legalized bribing government officials. It has taken away rights of women, convicts, and now the ability of government agencies to carry out their mandates. There are major power imbalances in the country and it is not sustainable. The path we are on leads to a breakup of the country or a rewriting of the Constitution. The status quo is not sustainable. reply pstuart 20 minutes agorootparentprev> Congress has not been able to pass substantial laws in decades For those in the sway of the Federalist Society, that's a feature, not a bug. reply PeterCorless 6 minutes agorootparentIt is ironic in the extreme that the \"Federalist\" Society want power to devolve to the states. Jefferson is snickering in the grave. reply idiotsecant 14 minutes agorootparentprevThis will not force Congress to suddenly become a responsible and capable legislative body. It will result in there being less government. Fewer rules to protect consumers, fewer rules to keep our air, water, and ecosystem healthy, fewer rules to prevent the powerful few from exploiting the many. Make no mistake about it, this is about giving more power to the powerful, and it's working. This is the swan song of America if we don't wake up. reply sshconnection 38 minutes agorootparentprevIf you think the administrative flip flop is bad now, wait until Trump implements schedule F. reply refulgentis 42 minutes agorootparentprevI used to hold this. I can't put a finger on what exactly changed. I strongly believe Congress is productive enough, and the ambiguity around this sort of thing is good, because it allows flexibility in the face of changing circumstances. With such a vibrant public democracy as America, things that drag out, like allowing non-competes for fast food workers, are best served with public participation, a long process, and sunlight. It's not as tightly coupled to a presidential administration as you may think, that's where \"Deep State\" grumbling comes in: if it's imposed by presidential fiat, its in violation of a 1000 mundane things that courts have historically consistently enforced: a long, public, process with a thorough cost/benefit analysis that indicates a net benefit. So, the \"Deep State\" (administrative processes that are required to occur for a rule change at an administrative agency, to whom authority was delegated to by the legislature) prevents a unilateral presidency. Parsing through the FTC press release on non-competes provides some indica of the sunlight/processes involved. [1] [1] https://www.ftc.gov/news-events/news/press-releases/2024/04/... reply immibis 43 minutes agorootparentprevIt won't force Congress to do shit. The same flip flop will still happen, but instead of 4-year executive terms, it will be driven by lifetime court appointments. The court is going to remain Republican for the foreseeable future, unlike the presidency. reply lolinder 37 minutes agorootparent> It won't force Congress to do shit. In which case the states will step in. We're already seeing this happen post-Dobbs, with blue states falling over themselves to create safe havens. If Congress can't get anything done and the courts won't let the executive branch do anything then that trend will continue with workers' rights and everything else. Maybe our problem is that the country has just gotten too big to run effectively as a single 350-million person democracy, and it's time for the state governments to step up and stand up for their people. Either way—federally or at the state level—we need written laws drafted by elected representatives in a body that has more inertia than the single position of chief executive. reply dangoor 27 minutes agorootparent> Maybe our problem is that the country has just gotten too big to run effectively as a single 350-million person democracy, and it's time for the state governments to step up and stand up for their people. Or, perhaps, we just need to alter some aspects of that government to run better. For example: changing the size of the House of Representatives to make it more representative. It wasn't supposed to be stuck at 435. Switching away from first past the post voting would also help enable more diverse voices within the government. reply lolinder 25 minutes agorootparent> Switching away from first past the post voting would also help enable more diverse voices within the government. You have my full agreement on that! Unfortunately that's not going to happen at the national level any time soon, both parties benefit from first-past-the-post too much. We'll see better luck experimenting with it at the local and state level. reply rayiner 30 minutes agorootparentprevThat would be great! reply sattoshi 40 minutes agorootparentprevWhats the problem? If congress disagrees with what the courts did to fill in the blanks in their laws, they just need to pass new laws. reply Powdering7082 27 minutes agorootparentWhy even have an executive branch? Surely everything can just be done through our efficient judicial system and wise legislative system reply xeromal 15 minutes agorootparentI have only grade school understanding of the branches but I was taught the executive branch is supposed to enforce what congress decides with laws which this reversal seems to support. reply kogus 6 minutes agorootparentprevI think you are being sarcastic, but actually a radically reduced executive branch would be a huge step in the right direction for the actual rights of citizens to determine their own laws. reply refulgentis 37 minutes agorootparentprevI used to feel this way, and I can't quite identify the totality of what shifted my position, but now I parse this as \"we just need to motivate congress\" and it feels wrong. The American system has always been full-throated adversarial -- and extremely successful. The historical system of legislature could delegate, and if the delegation went bad, the judiciary could intervene, rather than the legislature has to intervene in every bit of administrative minutae. Analogy would roughly be...idk, the CEO has HR handle pencil procurement. HR, over the years, used this to interpret they could swap in mechanical pencils, erasable pens. But the new CFO tells the board this has to stop, the CEO is responsible for signing off on expenses. And then the employees say this is a good thing, that'll get the CEO more involved. But the CEO is already involved, just busy with other things. reply megaman821 1 minute agorootparentWhat about this ruling stops Congress from explicitly granting interpretive power on some aspects of a law? The default now is implicit interpretive power, this ruling flips it. sattoshi 32 minutes agorootparentprevThe problem aren’t interpretative courts, its unclear laws. The pencil example is all fun and games, but swap « buying mechanical pencils » with « sending people to prison », and then it makes more sense why some people prefer the judiciary branch to constrain the power of HR when there’s ambiguity. reply throwway120385 14 minutes agorootparentprevAnd more importantly getting the CEO involved in pencil procurement prevents the CEO from handling business development tasks that would bring in tons of mechanical pencil money. So it's penny wise but pound foolish. reply refurb 35 minutes agorootparentprevIndeed. It seems like people don't understand how the system works. Congress cries about Roe vs. Wade, but the power is entirely in their hands to pass federal law to secure abortion rights. It's like the police crying that someone should do something about crime. reply xocnad 30 minutes agorootparentGerrymandering has removed much of the incentive on congresspersons to act affirmatively to pass legislation. reply kogus 8 minutes agoparentprev>Congress knows that it does not—in fact cannot—write perfectly complete regulatory statutes... Why not? Why can't Congress write complete regulatory statutes? Isn't that literally their job? Yes, it is. \"Chevron defense\" has been a way for Congress to shirk its duty for decades. If the law is ambiguous, courts must resolve the ambiguity. That is exactly what courts are for. To say that it would be better for an opaque, appeal-proof bureaucracy to have the final say was a ludicrous step on the path to our ever-growing executive tumor. The tone of your quotes from Kagan give the impression that federal agencies are \"responsible\" and able to use \"discretion\". But agencies are political animals, subject to the whims of the current president, who can potentially change every four years. Courts are much slower to change, and much less vulnerable to the political whims of the current administration. So many people are polarized and focused on winning presidential elections so they have their hands on the levers, that they never question whether the levers should be there in the first place. Perhaps politics would not be so polarized if the President did not have so much power, and the stakes were not so high. reply brettcvz 4 hours agoparentprevThis seems like the judicial branch just voted to give itself substantially more power. Are there any checks against this? Or can justices just keep granting themselves more powers and invalidating any restraints? reply mcguire 0 minutes agorootparentThis is the Supreme Court that claims, \"In the summer of 2023, Justice Samuel Alito told the Wall Street Journal that Congress has no authority to regulate the Supreme Court, despite the ethical regulations Congress already imposes on the justices\" (https://www.brennancenter.org/our-work/analysis-opinion/alit...) and that does not have any binding code of conduct. reply mike_hearn 1 hour agorootparentprevIt's worth reading the judgement itself. The court has indeed voted to give the courts more power, but not on the basis of nothing. It did so because it views it as taking back powers that were incorrectly/lazily given up without basis in what Congress wanted. From the judgement: Congress in 1946 enacted the APA [Administrative Procedures Act] “as a check upon administrators whose zeal might otherwise have carried them to excesses not contemplated in legislation creating their offices.” Morton Salt, 338 U. S., at 644. The APA prescribes procedures for agency action and delineates the basic contours of judicial review of such action. And it codifies for agency cases the unremarkable, yet elemental proposition reflected by judicial practice dating back to Marbury: that courts decide legal questions by applying their own judgment. As relevant here, the APA specifies that courts, not agencies, will decide “all relevant questions of law” arising on review of agency action, 5 U. S. C. §706 (emphasis added)—even those involving ambiguous laws. It prescribes no deferential standard for courts to employ in answering those legal questions, despite mandating deferential judicial review of agency policymaking and factfinding reply bumby 20 minutes agorootparentThis feels like one of those topics that may sound ok in theory, but breaks down in practice. The implication is that the judges must be well-versed enough in any domain brought before them to interpret the laws effectively. This seems like a tall order for nine people. We have already seen this trouble in expecting strict interpretations regarding tech. To be fair, Congress has the same problem. I believe that was in large part the impetus for giving the agencies discretion. They have a better chance of having the depth of expertise to craft effective regulations. reply mike_hearn 15 minutes agorootparentThe Supreme Court doesn't resolve cases directly, they resolve questions of law for lower courts to take into account. They are meant to be experts in law, so there's no problem there. The lower courts can't be experts in everything, but bear in mind two things: 1. Courts have expert witnesses and a whole system around how they are called, challenged and questioned. Judges are trained to learn what they need to know from witnesses. 2. Good court systems do have expert judges they can draw on. I recently took part in the Craig Wright case in the UK as a witness. Wright forged enormous quantities of evidence and proving the forgeries often required deep technical knowledge about file metadata, how computers worked etc. Fortunately the judge was deeply technical himself, being often a judge on complex patent cases, and had no difficulty with any of the complexities. reply bumby 5 minutes agorootparentIt reads like it creates a deliberate impasse. The opinion states that ambiguities in law no longer implicitly give agencies discretion. That means Congress has to write unambiguous laws. But my original post acknowledges they cannot. Based on this ruling, it seems like anything other than a perfect, airtight law means it's effectively non-enforceable. So where does that leave us? It seems like the SC has laid the table for constant rules-lawyering by corporations to get whatever they want. In other words, they've let the perfect be the enemy of the good. consumer451 58 minutes agorootparentprevAs a legal dilettante I have some questions: What does this decision mean for court caseload going forward? If it will increase, how much? Is there budget for that? reply mike_hearn 47 minutes agorootparentIt doesn't mean anything for court caseload. There seem to be a lot of posts in this thread that are misinterpreting what the judgement means. Here's what I understood from reading it: • This case does not affect Congress' ability to delegate defined lawmaking powers to the executive. Congress can continue to delegate whatever they want. • It will therefore not have any impact on the speed with which the US government can pass laws. • It does not award the courts any new powers. • What it does is go back to the pre-1984 system in which the meaning of ambiguous rules were decided by the courts. • It does so on the basis of a specific law called the APA, in which Congress spelled out that the courts should defer to agencies on matters of fact, but does not say courts should defer to agencies on how to interpret ambiguous law. Also that law was passed specifically to limit the powers of the executive. So, their ruling seems founded in the will of Congress. Because ambiguous rules would have to be decided on anyway, and they were already being decided in the context of a court case, this won't affect the number of cases being decided. I think the only way to attack this ruling would be to show that there was some law that superceded or replaced the APA, or that the relevant section of the APA itself was unconstitutional. But why would it be? As the court points out, the fact that ambiguous law is interpreted by the courts is a very old and unremarkable arrangement. The Chevron decision was the radical deviation from normal practice, reversing it just puts things back to how most people already think it works. reply Retric 18 minutes agorootparentCase load is simply the number of active cases and therefore not limited to the number of cases but also includes how long each case takes to complete. As this requires judges to consider a wider range of options it inherently means these cases will take longer thus increasing caseload. Further, it also means bringing these cases before the court will get more expensive as individual cases take longer. reply mike_hearn 10 minutes agorootparentYou're assuming that judges are slower to resolve ambiguities than regulators are. My experience with regulators has been that often they not only let the law be ambiguous for years despite repeated requests for them to make a decision, but are then fond of retroactively and suddenly \"clarifying\" things in response to shifting political/media winds. Nor do they feel any obligation to be consistent with past rulings. Courts are at least expected to make progress on cases as they are brought, to be roughly consistent with past case law, and they aren't allowed to just refuse to make a decision for a decade and return to it when it's suddenly in the newspapers. reply consumer451 7 minutes agorootparentprevAnother user has raised the other side of my question, while exaggerated, is this more accurate as to what will happen than the thrust of my original question? Do we need to increase the budget for Congressional aides? > The Roberts Court just decided to increase Congress' workload 100000x https://news.ycombinator.com/item?id=40823343 Meta: this has been one of the most interesting and educational threads in recent times. Three cheers for HN. reply skhunted 22 minutes agorootparentprevIt takes away power from the legislative and executive branches because it now requires an onerous level of specificity to regulate something. This decision will have lasting negative consequences. reply gnicholas 56 minutes agorootparentprevIt won't affect caseload so much as it will affect the balance of power in settlement negotiations. Source: I used to be a lawyer who worked in a heavily regulated field. reply ZoomerCretin 58 minutes agorootparentprevThe constitution very explicitly grants Congress the right to strip jurisdiction from the federal courts. https://constitution.congress.gov/browse/essay/artIII-S2-C2-... reply mike_hearn 54 minutes agorootparentBut they apparently haven't done so, unless you know of a law that supercedes the APA the court is citing? reply enragedcacti 19 minutes agorootparentprev\"If you accept the majority opinion at face value, then the majority opinion sure does make a lot of sense!\" reply treflop 1 hour agorootparentprevFrom my understanding of political science classes, this is how the founders wrote it to be. Actually, it's supposed to be like this… Congress writes laws. Executive interprets those laws and decides ambiguities on its own. Some of those ambiguities are contested so courts decide the outcome. If that court’s outcome is contested, then Congress makes a new ruling explicitly stating what they want. Then it repeats. It’s a cycle of checks and balances that is supposed to loop back into itself. Checks and balances is not a one time thing. reply andyjohnson0 52 minutes agorootparent> It’s a cycle of checks and balances that is supposed to loop back into itself. Except that the US doesn't have a functioning legislative branch, so the corrective feedback action never happens. The justices who are making these rulings, and their clients, are very well aware of this. reply consumer451 40 minutes agorootparentTo pseudo-quote an influential American Conservative via the All-in podcast: ~\"That's right, I want Congress dead-locked, I don't want any new laws passed!\" - David O. Sacks reply patmcc 8 minutes agorootparentprevThen the voters should kick the bastards out. That's the biggest check on the legislative branch, it has pretty fast turnover. Now, if you have a population that doesn't want to elect lawmakers who will actually pass laws...well, that sucks, but it's kind of working as designed. reply 23B1 23 minutes agorootparentprev> Except that the US doesn't have a functioning legislative branch, so the corrective feedback action never happens. That's neither the judiciary's problem nor purview. Its yours (and mine) as voters. reply Xeoncross 1 hour agorootparentprevIf you ignore the labels here, it's a small group of lawyers giving themselves more power because the large group of politicians can't get their act together and pass well-reasoned and descriptive laws. So the large body isn't functioning well and the small body doesn't trust it anymore. So if we make the small body (the supreme court) large like the large body (congress) will that actually fix the issue? Isn't the issue that politicians are corrupt and ignorant of actual expertise in the areas of the laws they pass? How will the Supreme Court overcome this same issue? (Personally, I would trust the nations top lawyers more than most of the congress members we have. However, it doesn't take much imagination to see the new issues that could arise) reply matthewdgreen 1 hour agorootparentCongress may be inefficient (by design, basically) but they have one advantage: they're elected. Everyone fantasizes about government by an unelected group of experts, until they wake up one day and find out those unelected experts don't share their values at all -- and there's nothing they can do about it. reply hypothesis 34 minutes agorootparent> Everyone fantasizes about government by an unelected group of experts, until they wake up one day and find out those unelected experts don't share their values at all -- and there's nothing they can do about it. Does SCOTUS fit into this hypothetical? reply digging 29 minutes agorootparentprevThis implies the common false dichotomy though that public officials can only be either: elected in toxic, wasteful campaign cycles every 4 years; or completely independent of public oversight. Those aren't the only two mechanisms that exist to develop an administrative apparatus. They are actually two points on a spectrum, and in fact closer to being at either end of the spectrum. One, quick example: You can have appointed experts who can be recalled by public input but never have to campaign for election. I'm writing this in short minutes with zero research so be assured there are countless possible systems that exist in the infinite space between the two binary options implied by your dilemma. In other words, being elected to office is not the advantage of congress. The advantage we seek is public accountability. Public elections are a pretty fucking poor proxy for accountability though because we end up with single-issue voters acting out of rage and electing people who are specifically inept at their job. reply ElevenLathe 1 hour agorootparentprevThe courts could reflect the values of elected officials. This is in fact a major strategy of the Right for about a generation, and is working great for them. It's only Liberals, who seem to be allergic to power, who have handcuffed themselves and decided that \"packing the courts\" (a loaded way to refer to the totally reasonable practice of appointing people from your own team to the bench so that they outnumber those from the other team) is somehow incompatible with democracy. reply psunavy03 1 hour agorootparentThe totally reasonable practice of \"I lost the game, so I'm going to flip over the table and pull a gun.\" reply Arainach 55 minutes agorootparentWhen your opponents are lying, cheating, and breaking their own made up rules (no supreme court nominees during the lame duck session unless nominated by a Republican) your characterization is uncalled for. reply immibis 39 minutes agorootparentprevYes, the right have been doing it for a long time and it works. Either make it stop working, or copy the thing that works. Don't just handicap yourself to a guaranteed loss. reply ElevenLathe 25 minutes agorootparentNothing in the rulebook says a {dog,Democrat} can't {play basketball,appoint liberal judges}. reply thyrsus 18 minutes agorootparentTell that to Merrick Garland and Mitch McConnell. reply wongarsu 47 minutes agorootparentprevAppointing people based on party loyalty is always cited as one of the major reason the Soviet Union became a slow-motion train wreck. It's not something America should emulate. Not to mention that packing the courts could well be interpreted as an open attack against the separation of powers reply mschuster91 1 hour agorootparentprevThe Democrats desperately want to keep the moral high ground. With a populace that cares about ethics and morals, that would be a sensible strategy, but unfortunately about half the voting US population doesn't give a flying fuck about either any more. Additionally, the danger for Democrats going for low blows and breaking tradition is that they might lose a significant part of their own voters as a result. reply immibis 37 minutes agorootparentIndeed they have already done so - many left-wing voters are swearing off voting for Biden, over his support for the Gaza genocide. This guarantees a Trump victory. reply mistrial9 1 hour agorootparentprevsuperficially this argument seems reasonable.. but my limited understanding of the history of the Supreme Court of the United States says that there have been substantially different eras, and substantially different rules in those eras, for this same Federal body. Needless to say, in a \"two party\" political system, the details of what each of those two parties represents has also changed dramatically.. i.e. what is called conservative has changed quite a lot, many times.. same with \"liberal\" reply btilly 1 hour agorootparentprevYou may trust the nation's top lawyers more than Congress. But in recent decades those lawyers have been picked for ideological purity in a process that distills what is bad about our political process. As a result I now trust Congress more than the Supreme Court. And not because I trust our broken Congress more than I used to! reply starkparker 1 hour agorootparentprev> I would trust the nations top lawyers more than most of the congress members we have If you're referring to the justices, who are approved by those Congress members you don't trust, it is a dramatic stretch to assume they are the nation's best lawyers. reply TylerE 1 hour agorootparentThere’s no requirement for them to be a lawyer at all, or have any legal training. reply jjmarr 44 minutes agorootparentThere's no federal constitutional requirement for anyone to have legal training or certification to practice law in the USA. The requirements to practice law in the federal system are set by the judiciary itself. This dates back to England where getting \"called to the bar\" meant the judge giving you permission to go to a physical bar separating the spectators from the court. It wouldn't make sense to mandate judges to be lawyers if they decide who is and isn't a lawyer. That would give the judicial branch control over their own appointments. reply TylerE 35 minutes agorootparentThe Supreme Court is explicitly subject to not even that. reply masklinn 50 minutes agorootparentprevAlso congress is full of lawyers. reply saveferris 42 minutes agorootparentprevBut, this decision didn't take those powers from Congress. It took those powers from federal agencies. Congress empowers the agencies, yes. But, Congress also deferred any technical decisioning to the agencies. Those agencies are filled with actual experts who are fully committed to their field. Now, the court just said that those experts aren't the right place to enforce anything but judges are. reply luddit3 1 hour agorootparentprevThis is taking power away from regulator bodies like EPA that enforce the laws and giving it to the courts... taking the enforcement out of the hands of the experts. reply psunavy03 1 hour agorootparentHow is it \"taking the enforcement out of the hands of the experts?\" Judges are supposed to be experts on law. That's literally their job. If the parties before them feel that they need expert knowledge to render the right ruling, then they need to take those experts and either depose them or have them testify. Expert witnesses are a thing; this is not some new idea. reply adriand 37 minutes agorootparent> How is it \"taking the enforcement out of the hands of the experts?\" Judges are supposed to be experts on law. Because the laws are about particular things in the real world that have nothing to do with the legal system. They are frequently about scientific matters, for example. What constitutes a threat to public health? What constitutes pollution of a waterway? When Congress authorizes an agency to maintain, say, clean drinking water, it entrusts scientific experts to determine, based on the most up-to-date evidence, what constitutes a pollutant that is harmful to human health. We do not need Congress to pass a new law every time we get new scientific evidence that a particular chemical (say, PFAS), is harmful. reply psunavy03 6 minutes agorootparent> Because the laws are about particular things in the real world that have nothing to do with the legal system. The laws have nothing to do with the legal system? That's a new one. masklinn 36 minutes agorootparentprevBecause this is not law in terms of billy having stolen a bushel of apples, and the expert is not called on to evaluate the value of the apples in order to determine whether billy is below or above the line for a class 3 misdemeanour. The statutes regulating agencies are generally broad signposts, giving the agency a mission statement and a direction but leaving it a large latitude to implement it and decide on the details. That latitude has a legal implication since the agency is generally responsible for setting and enforcing standards. The Chevron Deference is the legal doctrine that since congress delegated its power to the agency as matter and implementation experts, the agency's policy decisions should be deferred to so long as: - it's legally ambiguous aka congress has not answered the precise issue themselves - it is a permissible construction of the statute The entire point of the chevron statute is that it's not up to the judicial branch to set government policy, and if a problem is a legal void then they have no authority, and unless and until congress makes a specific decision the agency does. reply intended 18 minutes agorootparentprevThe courts are HIGHLY ideologically divided. Take a look at the recent Murthy verdict and Justice Alito’s dissenting opinion. The point is to avoid “experts”. reply no_wizard 46 minutes agorootparentprevLook at patent litigation for a good taste of how this can became a serious issue. All those calls for patent reform over the years and issues with the process is in large part because the USPTO decided as a general principle to liberally grant patents and let the courts decide what happens next if there is a dispute[0] That is the dangers of this ruling, that effectively all regulation can be tied up the same way, with companies filing lawsuits in the most sympathetic courts possible (like how most if not all patent disputes end up in a small Texas court due to its history of favoring the patent holder) and drags out the actual enforcement of regulations, rules etc. The potential for harm here is really high as a consequence [0]: this is a bit of an oversimplification but is the general “gist” of it reply refurb 23 minutes agorootparentprevThe US is a constitutional republic, not a dictatorship of experts. Go to Singapore if you want that. What I find funny is how the court is simply asking Congress to do their job - be clear in the intent of how laws should be executed. None of this \"well, I'll leave it up to unelected bureaucrats to decide\" and people think this is somehow a bad thing. reply dilawar 34 minutes agorootparentprev>Personally, I would trust the nations top lawyers more than most of the congress members we have. However, it doesn't take much imagination to see the new issues that could arise) Good luck with this. At least these corrupt politicians come to face the music every four years. reply brookst 1 hour agorootparentprev> because the large group of politicians can't get their act together and pass well-reasoned and descriptive laws How do you figure? This ruling says that Congress must be domain experts in every area, and agencies must merely implement the specific policies that Congress dictates. Is that even possible? For anyone? Sure, Congress is dysfunctional but so what? This new regime is unworkable, and it doesn't matter if it's dysfunctional politicians or \"top lawyers\". reply mike_hearn 56 minutes agorootparentPeople on this thread are talking as if this decision stops Congress delegating powers to the executive, or the executive drafting laws for Congress to pass. It clearly does neither. It's actually constitutionally entirely reasonable to demand that lawmakers are the people who make law, because there's no specific reason to assume that the volume of laws should naturally drown the people responsible for them. But even if you do assume that, nothing in this judgement would restrict the volume of laws passed in any way. It's just not about that at all. reply andyjohnson0 42 minutes agorootparentThe \"volume of laws\" required to regulate a complex modern society is far greater than that required for the US 200+ years ago. Thats why successful nations use rule-making agencies to regulate commerce, environmental protection, workplace safety, etc. Expecting the legislature to do it all is just not going to scale - which I suspect is the objective. The people behind these decisions want an overloaded, ineffectual legal system because that creates the best conditions for unrestricted accumulation of wealth and power. reply beaeglebeachedd 34 minutes agorootparentExcellent. Deregulate then. That's the desired effect of this anyway. reply zaphar 1 hour agorootparentprevCongress people are supposed to hire and listen to domain experts in the field they legislate on. reply dec0dedab0de 34 minutes agorootparentprevCongress could hire their own experts instead of having them work for the president. reply caseysoftware 1 hour agorootparentprev> \"This seems like the judicial branch just voted to give itself substantially more power. Are there any checks against this?\" Yes, absolutely. Congress can do their job and write the laws instead of delegating their authority to the Executive Branch. reply vundercind 1 hour agorootparentAs explained in the dissent, they literally have to delegate the kind of authority in question here. It’s the hostile-genie problem: you can’t close all the loopholes in some iron-clad unambiguous way in finite space. reply chasd00 40 minutes agorootparentthose loopholes and ambiguities should be left to the courts to decide with representation from both sides of the argument making their case and not some department head full of political bias and possibly an axe to grind favoring one side. reply intended 14 minutes agorootparentThis assumes a US Supreme Court that doesn’t exist in 2024. If you want it changed, you would have to either wait till the judges change, or expand the courts. reply another-dave 20 minutes agorootparentprevIsn't the whole point of the judiciary to interpret these ambiguities though? reply brookst 1 hour agorootparentprevIt seems crazy that Congress does not have the authority to delegate implementation details to experts. I just don't see anything in the Constitution that forbids that. reply bobthepanda 1 hour agorootparentIt does, the problem is the law as written doesn’t explicitly say that and this court is all about textualism when convenient. reply adriand 33 minutes agorootparentExactly, only when convenient. A glaring example of this is when they decided that section 3 of the Fourteenth Amendment did not disqualify Trump from the ballot. The plain language is not complicated: ---------- No person shall be a Senator or Representative in Congress, or elector of President and Vice President, or hold any office, civil or military, under the United States, or under any State, who, having previously taken an oath, as a member of Congress, or as an officer of the United States, or as a member of any State legislature, or as an executive or judicial officer of any State, to support the Constitution of the United States, shall have engaged in insurrection or rebellion against the same, or given aid or comfort to the enemies thereof. But Congress may, by a vote of two-thirds of each House, remove such disability. ---------- Note that this amendment provides a legislative remedy: Congress can remove the disability by a two-thirds vote. Textualism, but only when it serves their purposes. reply caseysoftware 1 hour agorootparentprevAre there provisions in the Constitution for one Branch to delegate its powers to another? reply freeone3000 48 minutes agorootparentThere’s no rule against it, and it’s what Congress has done, so it’s what’s happening. reply caseysoftware 35 minutes agorootparentSure there is: Article I, Section 1 All legislative Powers herein granted shall be vested in a Congress of the United States, which shall consist of a Senate and House of Representatives. Anything short of \"all\" contradicts that. Now you need to find somewhere that says \"Oh, btw.. we didn't mean 'All' but really 'some' because Congress might give some legislative powers to other branches.\" reply ZoomerCretin 47 minutes agorootparentprevDo you understand the implication of the answer to that question being \"No, and it cannot delegate those powers\"? Congress would have to vote on giving approval for each new drug, not the FDA's bureaucrats. Congress would have to vote on each individual edge case for welfare programs (SNAP, Social Security, Medicaid, etc), not their respective agencies. Congress would have to vote on which individual people get Pell grants, how much, and how much their parents are expected to contribute to their university schooling, not the Department of Education. Congress would have to vote to approve contracts for every federal agency. The federal government would not function without some degree of delegation. reply another-dave 16 minutes agorootparentLegislative powers, not all powers. You don't change the law every time a new drug gets approved, you grant it certification (the framework of which is based in existing legislation). You'd only need Congress to get involved if you wanted to change the approval process itself reply beaeglebeachedd 15 minutes agorootparentprevThe constitution arguably wasn't designed for a government that did much of what it's doing. Shouldn't be hard to pass an amendment to legalize it if it's that necessary right? We needed an amendment just to federally ban booze for God's sake. reply TylerE 1 hour agorootparentprevThe only checks involved were in the mail, and almost certainly addressed to Clarence Thomas, who has taken more in bribes than the last 30 other justices combined, and that’s only the ones he’s been caught on. reply psunavy03 1 hour agorootparentAnd yet there are eight other Justices, and nothing he has to say matters unless he can get four others to agree with him. If Thomas is known for anything on the Court, it's shouting into the void in concurrence or dissent. reply TylerE 1 hour agorootparentYeah, right. All the new conservative justices rammed through recently are cut from Thr exact same cloth. reply bell-cot 3 hours agorootparentprevObvious check #1: Congress gets its sh*t together, and stops writing endless vague blather into law. Obvious check #2: Congress enlarges the Supreme Count to 21 Justices. And lets the President know that his nominees for the 12 new positions will need to understand who's the real boss. reply dubcanada 2 hours agorootparentWhy stop at 21, why not get 1 supreme court from each state? You could get 2 if you wanted to be spicy and setup a sort of room for them all to debate in. Then after they heard the debates they could vote on the matter and if it passes it gets written into law. A sort of congress... reply tshaddox 55 minutes agorootparentIncreasing the size of the court isn’t a slippery slope to somehow making justices elected by states/districts. It’s not like as soon as you get too many justices it turns into a legislature. The interesting differences between the legislative and judicial branch is not the number of people (moreover, the Supreme Court is not exactly the entirety of the federal judicial branch). reply bell-cot 2 hours agorootparentprevI was thinking \"enough to routinely overrule the current 9 Justices\". Representing individual states, as such, is supposed to be the job of Congressmen. And - with how low-functioning Congress is looking, these days, patterning anything new after them is probably a bad idea. reply AnimalMuppet 2 hours agorootparentSo Congress is dysfunctional. The Supreme Court is semi-functional, but functioning in a way that you don't like. So you want Congress to vote in a bunch of new people to fix the Supreme Court. Why do you think that will work, instead of be ruined by the usual Congressional dysfunction? And, if the party in power adds enough Supreme Court justices to routinely overturn the current 9, what makes you think that when the other side is in power, they won't add enough to overturn your 12? The Supreme Court is not supposed to bend with the wind of every political election. It's by design. reply vlovich123 1 hour agorootparent> The Supreme Court is not supposed to bend with the wind of every political election. It’s by design Funny. Seems like it bent pretty hard in the last election. Why should we only honor the bends to the right? reply r2_pilot 1 hour agorootparentParticularly given McConnell's... Interpretations... Of how his obligated duties were fulfilled in regards to the timeliness of actions taken to ensure that such seats were filled. reply luxuryballs 1 hour agorootparentprevdid it though? reply intended 6 minutes agorootparentprev>The Supreme Court is not supposed to bend with the wind of every political election. It's by design. The design that can and has been undermined and bent on partisan lines, because of a dedicated campaign to achieve this very goal? ebiester 1 hour agorootparentprevYou act as if this is the first time that expanding the court has been discussed. Congress has yet to do this because it will never pass - at least unless one party gets a filibuster-proof majority in the senate or the filibuster is removed. reply bell-cot 1 hour agorootparentprev> The Supreme Court is not supposed to bend with the wind of every political election. It's by design. Looking at a few Supreme Court's rulings, say - https://en.wikipedia.org/wiki/Bush_v._Gore https://en.wikipedia.org/wiki/Trump_v._Anderson - I'd be inclined to say that the Supreme Court's design is to bend the results of every political election to suit their own wishes. reply vundercind 1 hour agorootparentprevOne-per-circuit, at least, would be a good change. Not as many as you’re proposing, but does make sense. reply AnimalMuppet 2 hours agorootparentprev> And lets the President know that his nominees for the 12 new positions will need to understand who's the real boss. And who, in your view, is supposed to be the real boss? Congress? Or the President? The Supreme Court is supposed to be independent. Changing that needs a much higher threshold than \"bell-cot doesn't like some recent Supreme Court decisions\". reply blacksqr 59 minutes agorootparentI have a fantasy solution that I know will never be implemented, but in my mind resolves all objections to expanding the court. Promote all eleven judges in the DC circuit court of appeals to the Supreme Court and leave the appeals court empty. For each vacancy that occurs on the Supreme Court, the president gets to pick one judge for the appeals court, until the Supreme Court justice count is back to 9 and the appeals court judge count is back to 11; at which time things go back to status quo ante. This would allow the Supreme Court to be rebalanced without the president packing the court with partisan choices. Rather, it respects the record of judicial confirmations for the appeals court going back almost 40 years and several presidential administrations. It would increase the number of perspectives on the court and make the Justices work harder to find consensus, rather than the majority being able to lazily fall back on pet legal theories that are out of the mainstream. It would counter and largely nullify the Republican strategy of targeting the Supreme Court with nomination of extremist and underqualified candidates with significant questions about their backgrounds, and confirming the nominees with dubious political maneuvering. It would be hard for Republicans to escalate; i.e., if a Democratic president added 12 slots to the Supreme court, what's to stop a Republican president and congress adding 20 more at first opportunity, and so on. Republicans could choose to elevate another court's judges to the Supreme Court, but that would tend to further balance the Court and make decisions more unpredictable, rather than produce a clear partisan advantage. It would take the Supreme Court nomination issue out of presidential politics for a generation. reply themaninthedark 52 minutes agorootparentprevExactly, the statement is very telling of the lack of understanding of how our government is supposed to operate, how and why the system is set up the way it is. reply refurb 15 minutes agorootparentprevIndeed. Despite FDR being quite popular with his New Deal laws, his own party was prepared to toss his ass out for trying to stack the Supreme Court in order to keep parts of his New Deal alive. It would be political suicide for either side to do that. reply rootusrootus 3 hours agorootparentprevCongress would have to agree that the power really belongs with them, and agree to limit the Court to only that which is covered in Article III. This is entirely plausible, but I think unlikely in the short term. reply remarkEon 3 hours agorootparentprev>granting themselves more powers and invalidating any restraints? You should read the actual opinion, because that's not what happened here. reply shrubble 1 hour agorootparentprevChevron has only been around since 1984. What was done previously? reply enragedcacti 1 hour agorootparentArguably the same thing, from wikipedia: > Chevron is probably the most frequently cited case in American administrative law,[16] but some scholars suggest that the decision has had little impact on the Supreme Court's jurisprudence and merely clarified the Court's existing approach. reply mike_hearn 1 hour agorootparentprevThe judgement discusses that. Previously in cases where a statute was ambiguous the courts interpreted it. Chevron changed that to allow the executive to interpret ambiguous laws, but the judgement argues that interpretation of the law is and always has been the role of the courts. reply riffic 1 hour agorootparentprevthe company, or the case? reply kube-system 59 minutes agorootparenthttps://en.wikipedia.org/wiki/Chevron_U.S.A.,_Inc._v._Natura.... reply intended 37 minutes agorootparentprevHey, that was the goal and plan of the various organizations that got these judges in place. I mean, what checks and balances apply to focused, dedicated, funded campaigns and teams, supported by backers willing to spend multiple decades and the millions necessary - to over turn laws, win minor elections, get judges into lower courts? People spent the time to understand the system so that it could be changed in a way they think is superior. The SC situation is the fruit of such labor. The shortest path solution to something like this is still decades long. reply harmmonica 34 minutes agorootparentprevOthers have said this using different words, but I'm going to chime in anyway. I don't think the courts will have more power. SCOTUS is saying that congress needs to actually make clearer (better?) use of its power by being more explicit when legislating (i.e. when writing laws) instead of relying on the executive branch agencies (for those unfamiliar with the US political structure, agencies like the FDA, EPA, etc. are executive branch agencies that, ultimately, report to whomever is the current US president) to interpret and in many cases read into the laws that congress has passed. The more practical reality of this ruling is, I think, this: there is no world where this is a win for anyone who believes in a bigger US federal government. This is a huge win for those people who believe the power of the federal government should be limited. It's likely the biggest challenge to the size of the federal government in my lifetime and I've been alive for a good bit. The dysfunctional congress that the US currently has makes it a certainty that in the short term countless regulations will be unenforceable and therefore this will be a picnic for anyone who is anti-regulation (note Trump in the debate last night where he talked about scrapping regulation. In comparison to this decision, Trump's regulation-slashing will look like he shot a rifle in comparison to the shotgun SCOTUS just fired). Last comment: this SCOTUS has made it clear that the federal government will be massively restrained. There are two avenues by which they've made this clear: first, they have ruled very aggressively in favor of state's rights (especially when it comes to social issues like abortion), and, second, with this Chevron ruling, federal agencies will not be able to make decisions unless there is explicit intent in the laws that congress passes. I'm having an extremely difficult time wrapping my head around just how epic of a change this SCOTUS has brought to the way the US population is governed, at both the state and federal level. Hard to really comprehend the gravity of the coming change, which will take decades and decades to fully understand. reply singleshot_ 1 hour agorootparentprevWe could solve all our problems via the ballot box in the legislature, and then these people would have more or less no cases to resolve. That has unfortunately proven unworkable. reply jmyeet 25 minutes agorootparentprev> This seems like the judicial branch just voted to give itself substantially more power. 100% this but it's not new. This court claim to be \"originalists\" or \"textualists\" (even though \"originalism\" was invented in the 1980s) but has made a massive power grab that we will feel for decades. The \"originalists\" invented two new doctrines to justify this: 1. History and tradition. Basically the court decides if how something was in 1780 as a legal basis for interpreting the constitution and law. Remember at this time some peoplw were property, women couldn't vote and there was no interracial marriage. This is the \"history and tradition\" the court seeks to return to; and 2. The major questions doctrine (\"MQD\"). This has gives sweeping powers to the court to say that even when Congress defined clear language if the consequences are \"large\" (as the court determines it) then the court can step in and say that Congress wasn't clear enough so the court gets to essentially write legislation and overrule both the legislative and executive branches. MQD was used to justify blocking student loan relief despite Congress giving the president and the education secretary expllicit powers in this regard. reply efitz 51 minutes agorootparentprevAlternative interpretation: The courts just remedied a situation where the executive branch of government had arrogated to itself powers reserved to the legislature by the Constitution. Notably another case ruled on this week did the same thing, by invalidating many agency-specific “administrative courts” and restored the rights of citizens to seek redress in actual courts. I and many others believe that executive branch agencies (“the federal bureaucracy“) has become an out-of-control unaccountable 4th branch of government, and I for one am delighted to see them reined in. Note that agencies will still be able to perform enforcement; they just have to stay within the bounds set by laws and they will no longer be the sole arbiters of those bounds. reply vkou 56 minutes agorootparentprev> Are there any checks against this? Yes, packing the court. reply blackeyeblitzar 4 hours agorootparentprevNo, that isn’t the case. It is saying that regulatory agencies cannot exceed their authority and act like the judicial branch. In other words, it was the executive branch that had taken more power previously. reply throw0101c 3 hours agorootparent> It is saying that regulatory agencies cannot exceed their authority and act like the judicial branch. On any given matter there are, at first, no laws on a given subject. Before airplanes were invented there were no rules or regulations for airplanes (FAA); similarly, pre-radio, nothing about how to use EM fields (FCC). Now, The (US) People gave The Congress authority to make laws on any subject (limited only by the Constitution). Congress said we will make laws limited actions on Topic X, and when non-prohibited actions are done they must be done in certain ways as prescribed by regulations. Congress further said that they cannot, ahead of time, know every situation that might arise on Topic X, but further rules may be needed. So Congress delegated further rule making, beyond the 'base' An Act to Regulate Topic X, to an agency that Congress itself created and funded via the above Act. An agency only exists because it was created by Congress; it only runs because it is funded by Congress. Congress says, in particular Acts, that some agency should look after the details of Topic X so Congress does not have it. Regulatory agencies have (limited) authority because it was given to them by The People (through their elected representatives). reply parhamn 1 hour agorootparent> So Congress delegated further rule making Couldnt they just do this formally? Afaict scotus didnt rule it's unconstitutional for congress to explicitly defer, but the derefence, which originated in court precedent, isn't good. Theres nothing stoping congress from explictly defering either via act or in the act. Right? reply bombcar 1 hour agorootparentRead the judgement, it's pretty simple. All this says is that if Congress defers something to a branch, and there is ambiguity, and it comes in front of a judge, the judge does NOT have to accept the branch's interpretation of the ambiguity, and can instead judge it as judges do. Chevron said that if the branch had a reasonable interpretation (e.g, not batshit insane like saying \"no arsenic in water\" means \"at least ten pounds per gallon of arsenic in water\") then the judge should defer to it. Now the judge can but does not have to defer to it - if he pushes back, Congress can clarify the law. This has been done many times in the IRS, where people find a \"loophole\", the IRS tries to patch it themselves, the courts say, yeah, nah, and then Congress amends the law to remove it. reply parhamn 10 minutes agorootparentIt isn't simple. The judgement states that broad implied deference to the agency of the act in question, per Chevron, is incorrect and the courts decide in the those case. There were a ton of arguments that interpretation, in general, is an Article 3 right of the courts. Though, I'd assume if congress explicitly granted interpretation to the specific agency of the act, we'd have a separate case on whether they're allowed to do that (explicitly defer). reply p_l 53 minutes agorootparentprevA textualist interpretation would be that indeed congress is now stopped from that, as anything that isn't described without doubt in an act of congress is now up to court to decide, not delegated agency. reply rswail 3 hours agorootparentprevNot \"taken\". It was inherently granted by Congress on the joint understanding that the intent was that agencies would engage in rule making to decide areas left undefined within the scope of the law as written. Regulatory agencies are responsible to Congress, the Legislative Branch that has the power to adjust the law to reflect its intent. Judges are not. The understanding is that it is the agencies that are intended to have the best understanding of what they regulate, not judges. Laws were written with this assumption in place, which the Court has just rug-pulled from the operation of the US government. reply tzs 3 hours agorootparentprevNo it isn't. What Chevron said was that when the legislative branch gives an agency power to do X and there is some disagreement between the agency and someone else over precisely what X means and the agency's interpretation is reasonable the courts should use the agency's interpretation. reply HelloMcFly 3 hours agorootparentprev> In other words, it was the executive branch that had taken more power previously. If I may disagree: it was the legislature that gave the executive branch power, and the judicial branch that essentially approved such an arrangement (unanimously) in the original Chevron ruling. reply tivert 3 hours agorootparent> If I may disagree: it was the legislature that gave the executive branch power, and the judicial branch that essentially approved such an arrangement (unanimously) in the original Chevron ruling. But the only way to properly do that is a constitutional amendment. To give an extreme though-experiment example: Lets say Congress 1) packed the Supreme Court with yes-men, 2) passed law giving themselves a huge pay raise and delegating all legislative powers to the President, while they go party. Didn't it just create a a king/dictator? Wouldn't that be unconstitutional? reply HelloMcFly 3 hours agorootparent> But the only way to properly do that is a constitutional amendment. A constitutional amendment make it permanent, but Congress never actually lost control. They always had the power - and still do - amend, restrain, clarify their own laws. > Didn't it just create a a king/dictator? Wouldn't that be unconstitutional? In a scenario with a packed Supreme Court of \"yes men\" there are no bounds to what could happen, so why bother with the thought experiment? In your example, the constitution is already worthless. reply luxuryballs 1 hour agorootparentprevIt will force Congress to act rather than allowing agencies to lurk in the shadows. reply dec0dedab0de 37 minutes agorootparentprevIt sounds to me like they just gave the legislative branch some of it's responsibility back. Delegating their job to the executive branch of government has created agencies that make and enforce rules themselves, and ultimately operate at the whim of whoever the president happens to be at the time. If congress wants to delegate details to experts they could explicitly state that in the law, and create their own organization of experts to do the job. Giving the president more power is not a requirement, and enforcement should remain separate. But even then, regulations shouldn't be ambiguous. The laws should state something like \"food purity should be within %x of yada yada, where x is updated yearly by the appropriate agency\" Then it's up to the courts to decide if the law was broken or not. In the short term this could be a nightmare as companies flaunt all sorts of regulation, but I think overall it is a good thing. reply curiousllama 26 minutes agorootparent> The laws should state something like \"food purity should be within %x of yada yada, where x is updated yearly by the appropriate agency\" Then it's up to the courts to decide if the law was broken or not. This is kind of true, but also belies the depth of the Chevron change. In this example, plaintiffs can now, for example, challenge how the \"X%\" calculation is done. What's an appropriate methodology? In the past, courts deferred to the agency: as long as it's scientifically valid + consistent, it's up to the regulator, not a judge. Now, it's up to a judge. So if I sue and say \"you should use a 0.01 alpha for calculations, not 0.05\" for your X% calculation, then a judge makes the methodological decision, not the statistician. IMO, it's not really reasonable for congress to design statistical methodologies as part of the text of a bill. reply remarkEon 3 hours agoparentprev> This Court has long understood Chevron deference to reflect what Congress would want, and so to be rooted in a presumption of legislative intent. Congress knows that it does not—in fact cannot—write perfectly complete regulatory statutes... I really hope she meant to convey a different point here, because it reads as if congress doesn't care and wants unelected bureaucrats figuring out what laws mean because they themselves know they suck at writing those laws. If that is the case, then why even have congress? reply Terr_ 1 hour agorootparent\"Perfectly complete\" is a pretty high bar. For example, consider a law directing the EPA to fine violators who dump \"fatal substances\". How complete is complete enough? That leaves us with some options, such as these ones which I'm ordering from \"most reasonable\" to \"most insane\": (1) In lawsuits, courts should generally assume that the lawmakers have given the EPA permission to create a formal list and judgement criteria for what counts. (2) In lawsuits, courts should assume the list is totally empty unless a federal lawsuit has happened where both sides have called in \"chemical experts\" to testify and then a federal judge decides which chemicals are deadly and which are not. (3) The law is totally meaningless until congress amends it with another bill that inserts a full list of every possible chemical composition and configuration required concentration-level, and anything not explicitly included on the list is exempt. reply sebzim4500 39 minutes agorootparentI think there is an option (4) which is probably the one the court would prefer. (4) In lawsuits, the regulator should have to prove that the substance in question is fatal. The EPA will have published a list ahead of time and and if challenged it will be up to a court whether the EPA has correctly determined the lethality of the substance. reply AuryGlenz 41 minutes agorootparentprevThe most reasonable option would be for congress to explicitly put in the bill that the EPA (or some other group of experts) makes the list. I’m no legal expert but in your example it sounds like that’s all that’s needed. reply consumer451 3 hours agorootparentprevWhile members of federal agencies are not elected, their heads are appointed by elected officials. I never understood the whole \"un-elected officials\" argument. How many people should we have on the ballot? 10s of thousands? reply tzs 2 hours agorootparentprevYou've misunderstood what she's saying. For many issues it is not possible for Congress to write laws that are simultaneously completely unambiguous and actually deal with whatever issue they are trying to deal with. Actually applying the law almost always involves numerous decisions to fill in some of the gaps. reply aport 1 hour agorootparentprevI would much rather unelected bureaucrat scientists decide how to implement the intent and application of laws than congress. reply bluGill 1 hour agorootparentYou hake no reason to assume that. The people who will try to write those regulations are those with have an angle. We call it regulatory capture. reply aport 1 hour agorootparentI don't see how the threat of regulatory capture would argue against letting experts decide what actions need to be taken to protect the publics health and the environment reply p_l 50 minutes agorootparentprevWhich is way easier when you only have to convince random court with a gish-galloping person as \"expert witness\". Regulatory capture at agency level is way harder to do. reply ceejayoz 1 hour agorootparentprevCongress is similarly captured. reply biomcgary 1 hour agorootparentprevRegulations that automatically expire after some time period but require Congress to given an up/down vote would give the regulations greater legitimacy in a democratic system. Congress doesn't need the expertise to write the regulations. The elected Congress could just vote to pass the regulations as laws. Congress just doesn't want to be on the hook for the regulations, which is part of the reason why they hand off law-making to the agencies in the first place. Theoretically, this approach would give people a greater voice in the rules that govern them. Sadly, in practice, we can't seen to rollback the proliferation of criminal laws that embolden prosecutors and lead to an unfathomable number of people in jail that have not been convicted by juries. reply Cody-99 58 minutes agorootparentCongress already has that power now though..? The congressional review act lets congress review and vote on new regulations issued by government agencies. Lots of people in this thread seem to be missing the fact that congress already approves of these agency rules. If they didn't they would have blocked them under the CRA. reply jkaplowitz 40 minutes agorootparentCongress failing to block a rule by passing a CRA resolution is very different from Congress approving of the rule. For example, if the majority in the House supports a rule but the majority in the Senate does not (or vice versa), neither an explicit approval action nor an CRA blocking resolution can be passed. reply jeffbee 1 hour agorootparentprevYou missed the point of the paragraph. Congress routinely writes laws directing, for example, the EPA to determine if and when an emissions-reducing technology is economically feasible. The majority opinion is saying that courts need not defer to such findings and they will decide for themselves. reply xbar 4 hours agoparentprevI would prefer if this conservative court would review the constitution and conclude that it must revoke its own right of judicial review. reply mmanfrin 3 hours agorootparentTurns out the notions of jurisprudence they claim to represent is all a farce for pushing specific political agendas after all. reply davidw 3 hours agorootparentIt's all \"calvinball\". reply paulmd 3 hours agorootparentprevI was thinking yesterday what a mockery of the concept of justiceability some of their past decisions have made. Like the court is forced into a Sophie’s choice on whether to agree to let a Captain Planet villain go free or let the lawyers drain the fund. And the court could also just flatly do a “in a one-time non-precedent-setting ruling, those assets are obviously still under your control and companies cannot indemnify individuals against actual knowing wrongdoing”… but that would never be used in that way for the benefit of mere plebs. But it does throw the whole idea of injusticeable claims right out the window. Bush had no claim at all, he literally still got thrown the election in a special one-time-ruling. What they did was a valid exercise of their power, just an extremely distasteful one. Right? As such, they’re literally, by the text of the constitution, an unjusticeable claim. The concept is facially incoherent, the court can justice anything it wants. The things they choose not to address, literally are because they’re things they don’t care about using their assumed powers to address. They literally invented the whole concept of a “one-off calvinball ruling” and formalized the concept already. Just your friendly \"textualist\" wing at work. reply kemayo 3 hours agorootparentprevIt would be the funniest \"textualist\" outcome, certainly. Every time I hear some disparaging comment about \"the penumbra of the constitution\" from the conservative justices, I can't help but roll my eyes because of that. reply diebeforei485 23 minutes agoparentprevI'd rather the Supreme Court exercise judicial hubris once every 50 years than new administrations exercise administrative hubris every 4 years. reply adamc 4 hours agoparentprevOur court is now activist in a very regressive way. The ride is going to be a very bumpy one until it generates sufficient pushback. reply alfalfasprout 56 minutes agoparentprevOn the flip side-- congress often will pass regulation with the very intent to allow ambiguity so that the regulatory agency will have extreme latitude often against the will of the people. Or a regulatory agency will grossly stretch their mandate to overstep what they are effectively allowed to regulate and interpret. Since there effectively minimal judicial checks and balances against that behavior that's also not desirable either. Just look at the farcical interpretations from the ATF in recent years that have sent innocent folks to prison for having a completely non-functional design of a gun part on a business card to prison. Or classifying a shoe string as a machine gun. These folks have had little to no recourse in court due to the ATF's broad unchecked discretion. reply vundercind 47 minutes agorootparentTo anyone wondering, to save you some googling: yes, both the mentioned instances aren’t obviously-unreasonable in context. One was someone formally asking the ATF whether it’d be illegal if they made a machine gun with a shoe string, and would that shoe string then be an illegal machine gun part and the ATF going “yeah, duh” and the other was someone seeing exactly how close they could get to selling machine gun conversion parts without crossing the line, and the karmic principle of FAFO kicked in. reply Avshalom 1 hour agoparentprevhttps://harvardlawreview.org/forum/vol-136/the-imperial-supr... reply gamblor956 48 minutes agoparentprevThe Roberts Court just decided to increase Congress' workload 100000x. This is one of those rulings that is going to get overturned in a few decades when it turns out to be completely unworkable to have Congress be subject-matter experts in thousands of areas. In 10 years when people wonder why their rivers are glowing green and everything in the ground is dying and there's a weird smell in the air, and corporations are just allowed to decide you pay them for no services and there's nothing you can do about it...this decision is going to be the reason. It looks like Thomas' and Alito's benefactors finally got what they spent most of the last decade trying to buy. reply slackfan 2 minutes agorootparent100000 * 0 is still 0 reply rswail 3 hours agoprevI'm impressed that they can so easily dispose of 40 years of law making by Congress that assumed that agencies would interpret the statutes and make rules for regulating their area of authority. Now Congress is going to have to specify every possible consequence of laws in the statutes, otherwise a judge will decide. So agencies will not have any power to actually regulate. Awesome logic work, but terrible legal thinking without considering the side effects of the decision. reply dilippkumar 1 hour agoparent> Now Congress is going to have to specify every possible consequence of laws in the statutes, otherwise a judge will decide. > So agencies will not have any power to actually regulate. This honestly sounds perfect. If this is the actual end result of this ruling, we’ll all be in a much much better place. reply unethical_ban 33 minutes agorootparentWe disagree. I prefer an effective administration of government. reply Mountain_Skies 11 minutes agorootparentWhat's your definition of effective and how are you evaluating if the administrative government is meeting your definition? reply lcnPylGDnU4H9OF 57 minutes agoparentprev> So agencies will not have any power to actually regulate. This isn't accurate. Agencies will just need to work with Congress to help them write laws which make sense according to how the agency would like something to regulated. reply waveBidder 3 minutes agorootparentexcept agencies need to be able to work on timelines faster than once a decade reply HDThoreaun 1 hour agoparentprevCongress can still delegate chevron style. They just have to explicitly do so reply Balgair 1 hour agorootparentThere's already been a big issue with 'regulatory capture' and lobbying in government. Congress is only going to delegate when some other entity, likely a business, isn't already writing the law/regulation. A concrete example: Boeing is going to up their lobbying game hard. They can now not only help write the laws, but help choose who says they've broken them. There is no way that it will be good for passengers before it is good for stockholders. reply deltarholamda 1 hour agorootparentThis is done with the regulatory agencies now, i.e. the revolving door from government agency to private sector. Worse, in some ways, because there's no real paper trail, such as donations to politicians or PACs. reply hindsightbias 36 minutes agoparentprev> easily dispose of 40 years As I tried to explain to people in 2016, your kids are going to be living with the consequences of your vote for generations. We are in a new era of judicial supremacy and they are out of bubble gum. reply catlikesshrimp 16 minutes agorootparent>\"We are in a new era of judicial supremacy\" Are you from Latin America?Martinelli (Panama) has been complaining of \"Civil Dictatorship\" since being on trial and hidden inside the Nicaraguan Embassy. Chavez (Costa Rica) is denouncing a Democratic \"Dictatorship then Tyranny\" because he finds independece of powers (Executive, Legislative and Judicial) cumbersome. Dangerous direction, people complaining about power balance checks reply maxwell 1 hour agoparentprevWhat side effects? reply rufus_foreman 20 minutes agoparentprev>> Now Congress is going to have to specify every possible consequence of laws in the statutes, otherwise a judge will decide. It has already been that way for a while. From the decision: \"Because Chevron’s justifying presumption is, as Members of the Court have often recognized, a fiction, the Court has spent the better part of four decades imposing one limitation on Chevron after another. Confronted with the byzantine set of preconditions and exceptions that has resulted, some courts have simply bypassed Chevron or failed to heed its various steps and nuances. The Court, for its part, has not deferred to an agency interpretation under Chevron since 2016.\" ... \"Given the Court’s constant tinkering with and eventual turn away from Chevron, it is hard to see how anyone could reasonably expect a court to rely on Chevron in any particular case or expect it to produce readily foreseeable outcomes.\" reply callroomlamp 3 hours agoprevDevastating that expertise will no longer influence the application of law and policy. The biggest question is who will interpret the application of law? Will it be challenged in court once again until a clear statement is made? Meanwhile, what will be the effects of this “deregulation” until a clear statement is made reply gnicholas 40 minutes agoparentOf course expertise will still influence the application of law and policy. The same people will still write the regulations, serve as expert witnesses in trials, and write amicus briefs. The thing that has changed is that the executive branch's preferred interpretation of laws passed by the legislative branch will no longer be granted deference by the judicial branch. They will be on a level playing field with other parties when it comes to putting forth proposed interpretations of laws. reply agensaequivocum 3 hours agoparentprevThe constitution mandates that the courts interpret the law. Thomas and Gorsuch are right in their concurrences, allowing the executive branch to both enforce and interpret law is abhorrent to our constitution's proscribed separation of powers. reply cthalupa 3 hours agorootparentExcept Chevron was just codification of the status quo that had existed since the founding of the country. Congress cannot be expected to craft every bit of law and regulation down to the finest detail, and the gridlock that has been congress over the past several decades should make it clear that it's practically impossible. The regulatory power of federal agencies has never been broad and without oversight from other branches - they operate on the authority given to them by Congress. The executive branch has not just been creating agencies wholesale and giving them sweeping regulatory powers, congress has passed laws creating them and delegating authority to them. As others have mentioned, you can look at the joke that is the patent system and the absurd games played around the law there to get an idea of what we're in for with this decision. I don't understand how anyone can think that's the place we want to get to for everything else. reply klyrs 2 hours agorootparentThe Federalist Society and its adherents see an ineffective Congress, and a general inability to enforce regulations, as a goal. reply rayiner 2 hours agorootparentNo, we believe we should follow what the Constitution says even if it’s convenient. There’s virtually nothing that unites Federalist Society members (many of whom are Biden voters) apart from an engineers’ commitment to technical accuracy over practical effects. reply cthalupa 48 minutes agorootparentConstitutional scholars since the founding of the nation have taken no issue with Congress delegating authority to federal agencies, and the initial Chevron decision followed along those lines. Why is it only now, with the hyper-politicization of the SC, with interested parties spending significant money providing luxury and lavish accommodations to at least one member of the SC, that this previously accepted interpretation of the constitution is suddenly in question? reply rayiner 8 minutes agorootparentChevron is about who interprets statutory law, agencies or courts. That question has been controversial for 100 years, ever since we have had administrative agencies. For example, the Supreme Court decided in 1944 that courts should give some respect to agency interpretations, but the court had the final say: https://en.wikipedia.org/wiki/Skidmore_v._Swift_%26_Co. Chevron didn’t create the deference concept until 40 years after that. Chevron was always contentious, and applied by courts in a rather haphazard way. But overturning it wasn’t “political.” It was originally decided by five republicans and a Democrat (with three justices not participating) and was overturned by six republicans. What happened was an ideological shift in the Republican Party to separation of powers that’s been going on since the 1980s. diffxx 1 hour agorootparentprevHow would you feel about someone who says \"we believe we should follow what the bible says even if it’s (in)convenient\"? reply 1123581321 59 minutes agorootparentYour quote reminded me of people with strong character: admitting to a mistake when it might cost them their job, urging a friend to return a stolen item, asking fellow church members to get to know a paralyzed man. Putting aside personal convenience is fundamental to loving God and neighbor at times. reply radley 39 minutes agorootparent> Putting aside personal convenience is fundamental to loving God and neighbor at times. No, it's fundamental to character. It's not exclusive to religion. That's just part of the myth. reply 1123581321 25 minutes agorootparentI'll just point out that loving God and neighbor is a paraphrase of a specific quote from the Bible. That's why I mentioned it. The quote is \"Love the Lord your God with all your heart and with all your soul and with all your mind and with all your strength. The second is this: love your neighbor as yourself. There is no commandment greater than these.” reply s1artibartfast 29 minutes agorootparentprevBeing fundamental to character, love of God, and love of your neighbor are not mutually exclusive claims. Someone could come up with a definition for each that excludes this need, but that is true of character and any subjective definition. reply tastyfreeze 1 hour agorootparentprevThat is a false equivalency. The Constitution is an explicit definition of what the federal government is and what they are allowed to do. The Bible is a collection of a bunch of ancient stories. reply AuryGlenz 35 minutes agorootparentprevI mean, I’ve never been able to wrap my head around people who are Christian (or whatever) and pick and choose which of the religions “rules” to follow. I realize to not do so in general makes society a pretty awful place, but most religions say you’ll go to hell if you don’t. It’s the Supreme Court’s job to explicitly follow the constitution. In your example I want them to be religious fundamentalists. If that turns out to be an issue we have a body that can change our society’s “bible.” reply s1artibartfast 1 hour agorootparentprevSeems fine. Not everything in life is convenient. In fact, many of the most important things are hard. reply rayiner 1 hour agorootparentprevI think the Catholic church for example should do that. The constitution is like America’s Bible. reply prisenco 1 hour agorootparentprevnext [–]an engineers’ commitment to technical accuracy over practical effects This reads like the design of nightmares. reply EricDeb 28 minutes agorootparentprevTextualism when convenient you mean. reply gamblor956 44 minutes agorootparentprev\"Federalist Society\" and \"commitment to technical accuracy\" are diametrically opposed concepts. You are right about them not caring about the practical effects of their actions. reply TylerE 1 hour agorootparentprevThe federalist society is far right, and any attempt to suggest otherwise is laughable. reply gnicholas 50 minutes agorootparentprev> many of whom are Biden voters I doubt there are many left after last night. reply beefok 1 hour agorootparentprev> There’s virtually nothing that unites Federalist Society members (many of whom are Biden voters) apart from an engineers’ commitment to technical accuracy over practical effects. In what fantasy universe do Federalist Society members vote for Biden? They have been backing conservative and libertarians for generations. Their members are part of the Supreme Court and clearly do not want a democracy anymore. They are the antithesis of liberal political positions. I call utter bullshit. How do their views even remotely line up with Biden voters? reply rayiner 1 hour agorootparentThere’s tons of Biden voters in federalist society because it’s not like Trump is much of a rule follower. And Biden himself for most of his career was a conservative Democrat. But I’m flummoxed by something. What does “democracy” mean to liberals? You’re the ones who want courts to decide issues that most other advanced democracies leave to voters, right? You believe in unelected bureaucrats and experts governing the country instead of elected officials. You seem to be using “democracy” in a very odd way to refer to rule by educated elites. reply relaxing 19 minutes agorootparentWe elect the executive to set the policy for and run the executive branch. An EPA employee doesn’t “rule” any more than a congressional staffer or court clerk. reply mcmcmc 35 minutes agorootparentprevAnd of course a Congress bought and paid for by lobbyists will actually represent voters, right? The US is a broken country in decline and actively working towards its own destruction reply rustcleaner 1 hour agorootparentprevMy voting pattern is to ensure minimum cohesion between parties in the State. When all parties agree, watch out they're a-comin'! reply bombcar 57 minutes agorootparentprevThe whole point of the judgement is that the status quo continues. Congress passes laws, the agencies implement them, if you disagree you go to court. All it's saying is that you don't have to go to the Supremes to get your disagreement to win. Imagine a patent system where the judge could never throw out a patent, because the experts at the agency (patent office) had granted it, so it must be valid. reply cthalupa 34 minutes agorootparentThis effectively neuters the ability for federal agencies to create regulations within the specific areas that Congress has tasked them with regulating. This is a significant blow to both legislative and executive branches, and further entrenches the power in the judiciary. In both reality and your supposed system, someone could always go to appointed officials and get their patent enforced - just look at how the Eastern District of Texas operated for decades. In reality, we see far more frequent turnover and shifting of opinion in federal agencies than we do in the judiciary. Just look at how often Net Neutrality has flopped back and forth at the FCC. (Constant reversal of regulatory decisions is also an issue, but it goes to show that the idea that a patent system that exists outside of judiciary control wouldn't have plenty of opportunity to make your case to sympathetic ears is silly) reply EricDeb 31 minutes agorootparentprevphilosophically I see what you're saying, but will congress be good at passing small laws to amend the ever changing regulatory landscape? I'm going to guess no. So practically this could be a nightmare reply throw0101c 3 hours agorootparentprev> The constitution mandates that the courts interpret the law. The idea of Congress delegating certain powers dates back to 1825: * https://constitution.findlaw.com/article1/annotation03.html Further precedents from the 1920s and 1930s (and more recent) are listed in the above link. It's not a new idea that some ambiguities are left to the Executive to figure out. reply ImJamal 2 hours agorootparentCongress can still delegate certain powers even after this. reply vkou 50 minutes agorootparentAnd every time they will, the courts will find that the delegation is not specific enough. Never mind that congress appoints the heads of the agencies, writes the laws directing them, and on an annual basis, renews funding for them. reply ImJamal 31 minutes agorootparentMaybe, but since this ruling just happened today I think we will have to wait and see what happens. reply stronglikedan 3 hours agoparentprevIt's not about the application of law. It's about the ambiguity of law. If anything, they'll need to rely on more expertise now, so they can craft laws that aren't open to interpretation. This is a fantastic decision on the part of the court. reply radley 3 hours agorootparent> It's about the ambiguity of law. If anything, they'll need to rely on more expertise now, so they can craft laws that aren't open to interpretation. I doubt that granting Congress more power will inspire them to be less political, more responsible, and more governed by facts. Particularly when the party that made this decision has veered completely in the opposite direction. If anything, it will be used to prioritize \"faith\" over fact, like what we've seen in Oklahoma and Mississippi. reply xboxnolifes 2 hours agorootparentCongress isn't being granted more power here. They're being granted more (their original) responsibility. reply rayiner 3 hours agorootparentprevIf that party can win elections by doing that, that’s what should happen. “Expertise” carries zero weight in a democracy other than its ability to persuade voters. reply notaustinpowers 1 hour agorootparent> Expertise carries zero weight in a democracy other than it's ability to persuade voters. Would you rather the \"holistic healer\" who says only drinking green juice for a week to \"detox\" your kidneys make laws? Or the person who actually went to me",
    "originSummary": [],
    "commentSummary": [
      "The Supreme Court has overturned the \"Chevron deference\" doctrine, ending a 40-year precedent that allowed federal agencies to interpret ambiguous laws.",
      "Critics argue this decision may lead to instability, as courts will now interpret ambiguous laws, potentially increasing Congress's legislative workload and affecting regulatory stability.",
      "The ruling is viewed as a significant shift in the balance of power among the branches of government, with Justice Kagan dissenting that agencies are better suited for interpreting laws due to their expertise."
    ],
    "points": 303,
    "commentCount": 656,
    "retryCount": 0,
    "time": 1719585072
  },
  {
    "id": 40816158,
    "title": "Infrastructure setup and open-source scripts to train 70B model from bare metal",
    "originLink": "https://imbue.com/research/70b-infrastructure/",
    "originBody": "We would like to thank Voltage Park, Dell, H5, and NVIDIA for their invaluable partnership and help with setting up our cluster. A special thanks to Ozan, Melissa, Drew, Michael, and David at Voltage Park for their dedicated support throughout the project. Setting up a cluster of this size is an enormous challenge and we couldn’t have done it without them. Introduction In the span of a few months, with a small team of researchers and engineers, we trained a 70B parameter model from scratch on our own infrastructure that outperformed zero-shot GPT-4o on reasoning-related tasks. Today, we’re sharing an end-to-end guide for setting up the required infrastructure: from bringing up the initial cluster and installing the OS, to automatically recovering from errors encountered during training. In each step, we detail the challenges we encountered and how we resolved them. Along with our learnings, we’re releasing many of the infrastructure scripts we developed to ensure healthy hosts, so that other teams can more easily create stable infrastructure for their own model training. Along with our detailed process, we are releasing: Host-level health checks: scripts to ensure that a given host is free from known errors An NVIDIA Collective Communication Library (NCCL) patch that improves logging around errors and stalls A stress test to confirm that GPUs are able to allocate large tensors and perform standard operations Networking tests to check that the GPUs on a given machine are able to communicate with each other (via NVLink) and with GPUs on other machines (via InfiniBand) A script which parses the Unified Fabric Manager (UFM) event log, checks for relevant events, and determines which network ports should be disabled A script which generates a comprehensive burn-in workload for InfiniBand fabrics, aiming to exercise every available link Throughout the process, members of our engineering team worked with our partners at Voltage Park to prepare the cluster for production use. The full process involved: Provisioning individual machines Provisioning InfiniBand Ensuring fully healthy machines Diagnosing common training issues Improving infrastructure tooling Each of these steps is described in greater detail below. Background: How this is supposed to work The purpose of our compute was to enable rapid experimentation with large-scale language models. To do this, we needed a large number of fast GPUs that could communicate with each other at high speeds. This post focuses on one cluster that had 4,092 H100 GPUs spread across 511 computers, with eight GPUs to a computer. There were 511 computers with GPUs because some connections needed to be reserved for the Unified Fabric Manager nodes, which managed the InfiniBand network. On the 511 hosts with GPUs, each GPU was directly connected to a ConnectX-7 card that could simultaneously transmit and receive at 400 Gbps to any other GPU on the InfiniBand network through its own ConnectX-7 card. Our InfiniBand network topology was called “fully non-blocking” because every GPU could, in theory, simultaneously talk to another GPU at the maximum rate. This was enabled by a three-tier InfiniBand network architecture: the three levels of InfiniBand switches, when properly connected, enabled this high level of throughput over the entire network. See below for an overview of the InfiniBand network: Click to enlarge. Note that the communication for training networks happened over InfiniBand, not over Ethernet. While the machines were also connected to an Ethernet network, that network was used to transfer datasets, checkpoints, and other data. It would have been far slower to send data over Ethernet because data would first travel from the GPU to the CPU, and then out one of the 100 Gbps Ethernet cards. While it would have been possible to train over Ethernet using a technique called RDMA over Converged Ethernet (RoCE), that would require a significant amount of additional work on both the hardware and software side, and would generally be less reliable than InfiniBand (see this paper that outlines the extensive process). There was also a secondary Ethernet network used purely for configuration and management, enabling access to the controller interface for the basic input/output system (BIOS), power supplies, and other low level machine interfaces. Without this management network, we would have had to manually set up our nodes with a USB drive, keyboard, and monitor, which would not have been not a sustainable approach for hundreds of machines. Using our cluster for high performance training meant that every component — InfiniBand, Ethernet, GPUs, and the nodes themselves — had to work near perfectly. If even a single one of the over 12,000 connections was a little flaky, it could slow down the entire training run. The rest of this post details the process of actually getting to a point where everything works perfectly, and ensuring that it stays that way. Process: How to go from bare metal to a fully operational cluster Provisioning individual machines After establishing an initial Ethernet connection to the cluster via the management network, we obtained access credentials to the baseboard management controller (BMC). The BMC is a specialized service processor that remotely monitors a host system and is typically wired to a separate network. It allowed us to interact with every machine as if we were physically present and provided additional APIs for hardware health, BIOS setting, and power management. With these components in place, we rolled up our sleeves and began setting up the cluster. Step 0: Getting one machine provisioned We first used iDRAC (Dell’s baseboard management controller) to install Ubuntu 22.04 on a single server, which would be used to set up everything else. Among other things, iDRAC allowed us to mount and boot off of an ISO image from a local computer, with a virtual console provided in the browser. This would ideally be the only manual installation in this process. Step 1: Installing an OS on every machine With patient zero taken care of, we proceeded to install Ubuntu’s Metal-as-a-Service (MAAS) software to help provision the remaining servers. With the Preboot eXecution Environment protocol (PXE) booting and automated iDRAC tools, we instructed every machine to boot off the network and configured MAAS to respond to the PXE boot requests. When performing the initial network boot, servers received an IP from MAAS via a dynamic IP allocation protocol (DHCP) and an initial kernel without needing to have anything installed on the local drives. This bare-bones environment was automatically used to perform a persistent OS installation. In theory, we would wait for the first boot, and everything would be taken care of. In practice, however, the MAAS integration with BMC was not reliable, so we used the iDRAC API to collect the MAC address (a unique physical hardware identifier) for every machine in advance. Throughout the training process, MAAS was a generally reliable component of the stack. However, we experienced some hiccups at the beginning that were fairly specific to our setup. For instance, during the first few provisions, the clocks were so far off that HTTPS certificate validation issues prevented anything from being installed via apt. Relatedly, because the MAAS server had to take care of so many responsibilities (DHCP server, DNS server for hostname to IP resolution, HTTP proxy between hosts and official Ubuntu package servers, NTP server, cloud-init configuration management, and ground truth database for connecting MAC addresses to IPs to hostnames to custom metadata), we had difficulty tracking issues to the root cause component. Adding to this was the learning curve around the MAAS provisioning lifecycle, as it was designed to handle the complexity of managing greenfield deployments as well as gradual migration of nodes and various debugging/unhealthy intermediate states. Step 2: Diagnosing broken machines As is typical in setting up large GPU clusters, we found that about 10% of the machines failed to boot, mostly due to physical issues with the servers. Some issues we encountered included: unconnected or miswired Ethernet cables, hardware issues in iDRAC, broken power supply units, bad NVME (nonvolatile memory express) drives, missing internal wires, and network cards or GPUs failing to show up. We automated checks for these issues, passed some machines back to Dell for re-testing, and filed appropriate tickets for the data center staff. An advantage of taking the cluster setup into our own hands was that we were immediately able to put the healthy machines to use while awaiting maintenance on the others. Step 3: Minimal viable observable metal We proceeded to set up the following on every server: Docker (for more easily running services and training jobs) Data center GPU drivers Prometheus node exporter (for exporting a steady stream of hardware / OS metric) DCGM exporter (additional metrics from NVIDIA for GPU state / clocks / utilization) RAIDZ ZFS pool on all the non-OS drives (this enables machines to survive one drive being down, and also gives transparent compression for free, which is particularly helpful for plain-text datasets and repetitive logs, routinely enabling us to use about 10 times more space than we would have otherwise been able to) We then ran basic GPU diagnostics to determine whether the GPUs were mostly functional — those that weren’t would typically experience hardware issues within a couple hours. During this time we ran into some bandwidth bottlenecks when we tried to install software packages across all 400 nodes in parallel. This was also the first time we started to receive heat alerts for high temperatures on various components in the data center deployment. This first batch of heat issues was mostly remediated via firmware updates. Step 4: Single-node GPU training The next step was ensuring that every machine could handle real GPU workloads in isolation. Many couldn’t, due to a number of issues: GPU-related errors, which were mostly fixed by reseating the cards in their slots: physically sliding out the 200-pound server from the rack, removing all the cables in between the cover and the GPUs, then taking the GPUs out and putting them in again before replacing all the cables and reracking the server. A number of the wires between the GPUs and the Peripheral Component Interconnect Express (PCIe) buses or network cards reported “limited width: x4 < x16” according to Ubuntu server logs. After updating the PCIe switch bus firmware, we found that the internal PCIe cables needed to be reseated for around a quarter of the hosts in the cluster — presumably because the rather fragile cables were situated between the casing and the GPUs, which meant that they would be jostled or unplugged any time anyone wanted to do maintenance on the GPUs. A number of miscellaneous failures affected single-digit hosts. Dell helped us fix these via firmware upgrades: NVMe drives didn’t show up as faulty, but locked up the entire machine when touched. Hard drives showing up in random order under Linux, which confused MAAS and caused the OS to get installed on the wrong drive. Wrong temperature readings, which caused fans to spin up to 100% all the time. This was caused in part by having a bad NVIDIA driver, which we resolved by downgrading to the previous driver version. Dynamic frequency scaling for CPUs going haywire, limiting to 2 GHz on the active cores. Direct GPU-GPU communication (GDR, or GPUDirect RDMA Peer Memory Client) was impossible to apply successfully. Provisioning InfiniBand Step 0: Installing the UFM One advantage of InfiniBand was its centralized design, as it has one brain for the entire network. Therefore, we only had to deal with one entity for the 320 network switches in the fabric. Our first task was to figure out which switch connected to which machines, then to correlate that with the wiring diagram and rename the switches by their physical location. Step 1: Time for rewiring Initially, the UFM couldn’t detect the 320 network switches, let alone all of the hosts expected to be present on the fabric. After conferring with our data center partners, we confirmed that the switches were powered on and wired in, yet remained undetectable. Upon examining the network wiring list, we noticed that the top level of the fabric was misdesigned: instead of a single unified fabric, we had eight disjointed networks with no common routing paths. After rewiring the connections, we added checks to verify that all the physical connections lined up with the new design. Step 2: Ten thousand temperature alerts After resolving the physical wiring issues, the UFM successfully established contact with all InfiniBand switches in the fabric. However, almost every switch port began reporting excessively high temperatures, sometimes exceeding 70 degrees Celsius, even though they weren’t transmitting data yet. We discovered that the issue stemmed from open spaces between switches in the same networking racks, which caused hot air to recirculate back to the front. Our data center partners helped us diagnose the issue quickly and develop a suitable workaround. Step 3: Eighteen hundred alerts Many ports also exhibited high error rates or fluctuated between working and broken states, known as “flapping.” These issues only surfaced when the ports were actively used, so preemptive detection proved challenging, as the entire fabric consisted of 10,000 links with a high degree of redundancy. Our data center partners helped clean and reseat alerting ports, and we disabled the remaining alerting transceivers while waiting for replacements. Although InfiniBand is highly resilient to hardware failures, once about 10% of the fabric began experiencing issues, features like adaptive routing couldn’t function reliably enough to work around haphazardly dropping links. During this period, we managed to conduct multi-node training runs with 100 to 200 machines. Our process was largely improvised: we sometimes launched on a random set of nodes, observed their performance, and tried to stay running on as many of those nodes as possible. This approach allowed us to find a reliable subset of the InfiniBand fabric, but proved tricky because every time we would change the set of nodes used for training, the default set of InfiniBand links would change. Step 4: Burn InfiniBand burn, disco inferno To diagnose InfiniBand issues more efficiently, we devised a specialized workload for the entire cluster that focused on simultaneously pushing as much data as possible through every port on the entire fabric. This was not the same as running one large all-reduce workload across the cluster, which would utilize NCCL to optimize communication within individual nodes by having GPUs communicate using NVLink via Server PCIe Module (SXM) sockets. Instead, we opted for a brute force approach, and succeeded handily. The UFM began sending alerts about data transmission exceeding 97% of the theoretical capacity through most ports, and some switches temporarily crashed. Every port left standing at the end of the day was deemed robust enough to proceed, and the rest were disabled or handed off for later repair. Step 5: GPUDirect RDMA For the GPUs to communicate without incurring CPU overhead, we enabled a feature called GPUDirect RDMA, which allowed direct communication with the InfiniBand network cards. This involved two key steps: Enabling an extra kernel module Ensuring PCIe Access Control Service (ACS) was disabled to prevent immediate hangs Step 6: Grow the golden set of servers A rule of thumb for GPU clusters using the newest hardware: expect about 3% of machines to break every week. There is a crucial nuance that often gets lost, however: it’s not that every machine has a uniform 3% chance of failing; rather, a small number of malcontent machines repeatedly break in different ways until they’re properly fixed. This highlighted the advantage of having a large number of machines on the same fabric. Instead of playing whack-a-mole on our large training run with random machines, we could instead focus on growing a set of known reliable, or “golden,” machines. Step 7: Maintenance InfiniBand maintenance mostly involved responding to UFM alerts, swapping failing cables and transceivers, and occasionally diagnosing more difficult errors, such as faulty switches. Large scale regressions were usually caused by two factors: Firmware upgrades, especially if applied to only half of the cluster, which could corrupt the UFM state and necessitate UFM restarts on all of the InfiniBand switches. Mass restarts of GPU boxes at the same time, which could flood the UFM state with updates and similarly necessitate a UFM service restart. Ensuring fully healthy machines Throughout this process, we discovered numerous ways individual machines could fail or slow down training runs. Many of these failure modes were not immediately obvious, so we wrote a number of health checks to determine which hosts were healthy enough to use for training. We have released the code for these here. Note that many of these health checks are specific to our runtime environments, are not necessarily related to the base hardware, or are extremely trivial to fix or automate. This is by design: for the overall goal of making our machines ready for training, we wanted a single entrypoint that would answer “yes” or “no,” and would abstract over any number of “trivial” details. GPU Health Check We checked that we had the correct number of GPUs, that ECC (error correction code) checking was enabled, and that there were no ECC errors. We also checked that the NVLink topology, which connects the GPUs to each other, was up and error-free. Disk Space Health Check We checked that hosts had no more than 95% disk space utilization. Docker Health Check We checked that Docker was able to run a container with GPUs attached (i.e. NVIDIA Container Runtime was working properly), and that all Docker containers relevant for monitoring/profiling were active and given the correct host permissions. Dmesg Health Check We checked that there were no hardware Xids or SXid errors (faults thrown by the NVIDIA GPUs or inter-GPU NVIDIA switches) in dmesg. We also read out all dmesg log lines to verify that they were all categorizable in a list of “usual/expected log lines”. iDRAC Health Check We checked for iDRAC errors on the machine, ignoring non-fatal error messages. This is specific to our Dell machines and is not part of the health checks that we are open sourcing. Disk Health Check We checked that zpool was mounted, Docker was connected to it properly, and that it could actually be touched without the CPU locking up. InfiniBand Health Check We checked for the presence of increased InfiniBand error rates and/or outdated driver firmware. Nvlink Health Check We checked for NVLink errors on the machine. Empirically, this didn’t seem to result in training failures, but it may result in slowdowns. GDR Health Check We checked that GDR was enabled on the machine. VBIOS Health Check We checked that the VBIOS version of the GPUs, as well as the H100 baseboard firmware were up to date. Flint Health Check We used flint and hca_self_test to check that we have the right version of the Mellanox OFED driver, card firmware, and transceiver firmware, and that they were compiled correctly against the NVIDIA drivers. PSB Health Check We queried PCIe devices to check that the speed and width of connections is what we expected between the GPUs, the PSB (PCIe Switch Bus), and the network cards. We also checked that the switch firmware was on the current version. This script was developed by Dell and not Imbue, so we are currently unable to share it. In addition to these quick health checks, we also had a couple of more involved health checks that included: Initializing a matrix computation via PyTorch and measuring the NVLink bandwidth and GPU computation speed and memory. We set the proper GDR flags to test both InfiniBand and NVLink. Using ib_write_bw with –use_cuda to send data over the IB cards and measure the PCIe and InfiniBand card bandwidth. We ran this for a longer period (~15 mins) to ensure we caught flappy InfiniBand links. Running a multinode diagnostic run to check NCCL initialization ability and whether it would randomly stall. If it stalled, our forked NCCL code added additional logging. This could take 12 to 24 hours to detect issues, so we typically only ran this for new nodes, or when we suspected an issue. Checking the DCGM exports for any GPU clock throttle events (excluding the expected gpu_idle and power_cap). Multinode training that exercised all GPUs, InfiniBand cards, and CPU and disk simultaneously was the best way to exercise these power events. Diagnosing common training issues Once the hardware began working properly, it was time to begin training. In this section, we share some concrete debugging steps and insights revealed through our experience running large language model training jobs on our cluster. Crashing on startup In some ways, this was the best error to encounter, because it would (theoretically) be easy to reproduce and iterate on. We first checked whether we were running our code on the correct version, configurations, and environment variables. While basic, we found that it was critical to ensure that launching training was reproducible and easily inspectable, especially since intermediate abstractions like Docker image caching or opaque secrets configurations could muddy the waters. Another basic check conducted was ensuring all our machines were online, and that the emitted stack traces or logs could be easily aggregated and inspected. We used a Loki, Prometheus, and Grafana stack, but any suitable log aggregation or tracing SaaS would be appropriate. Due to the synchronous, distributed nature of these runs, often the first error to trigger would cause a cascade of unrelated errors. Here, health checks also helped instantly detect obvious issues such as broken hard drives or missing or invalid GPUs. We built a system to automatically relaunch on failure, which made log and error aggregation even more important to avoid mixing up errors from different relaunches. Some common errors we encountered included: Errors like Forward order differs across ranks: rank 0 is all-gathering 43 parameters while rank 1228 is all-gathering 1 parameters. We found that this was a quirk of PyTorch Fully Sharded Data Parallel (FSDP) implementation that could be resolved by a relaunch. GPU Out of Memory (OOM) Errors, that looked like CUDA out of memory. Tried to allocate … We fixed these by by double-checking our configurations and code, and backing out any recent code changes that might have caused extra utilization of GPU#0 due to improper PyTorch device specification during startup. CPU/RAM OOM Errors, which were less easily spotted from error logs, and were typically best detected via dmesg logs from the host outside Docker containers. We saw them mostly as CalledProcessError or ConnectionError, when a forked process or network peer was reaped by OOM Killer invocation. We preferred to just fail health checks and restart the box when an OOM Killer invocation was detected from dmesg. We also checked our code path had a sufficient amount of manual garbage collection (see below sections on how to disable it), and wasn’t accidentally trying to do computations or move tensors on to the CPU. Crashing in the middle of training The first order of business was to automate systems that would rerun all diagnostic health checks (see previous sections), then auto-restart the run without unhealthy hosts. We encountered a few random hardware faults, including Xid and SXid errors which could crash the run without emitting meaningful Python stack traces. Some instances, like row remapping, were recoverable by a restart. Others, like uncorrectable ECC errors, often needed hardware maintenance or replacement parts. In addition, we observed crashes caused by particularly malformed training data. For instance, a very large single document in the corpus could cause OOM errors in either the GPU or CPU. To prevent these, we had a fully deterministic data loader, which made every crash easily reproducible via correlation with the epoch or step number. We found it helpful to disable dataloading or substitute fake data (such as all zeroes) to confirm whether the data was truly the root cause. Finally, it was also helpful to record network and general node health statistics via any preferred method of metrics aggregation. Issues like Ethernet briefly cutting out or running out of disk space may not show up as helpful error messages, but could be easily correlated with collected data. Hanging with no stacktrace information (possibly followed by a timeout) These types of errors were extremely frustrating to debug, due to the lack of helpful information and the fact that they were often difficult to reliably reproduce. The most memorable type was characterized by error messages like Watchdog caught collective operation timeout: WorkNCCL(SeqNum=408951, OpType=_ALLGATHER_BASE, … , Timeout(ms)=600000) ran for 600351 milliseconds before timing out simultaneously appearing across all GPU workers in the training run. What this meant was that one or more of the hosts failed to complete a NCCL operation, or even crashed out of the NCCL and InfiniBand connection, causing all other hosts to block synchronously on the particular tensor op until the NCCL_TIMEOUT was reached. Unfortunately, the nature of the NCCL library made it incredibly difficult to find which particular host(s) was the culprit. We made some logging changes to the NCCL library (see our fork here) to better surface which messages or operations were in-flight when the crash happened, and thereby identify which was the host or GPU that seemed to prevent the runs. Note that in order to identify misbehaving hosts, we often needed to figure out which hosts did not produce certain log messages. The lack of such messages indicated that the workers on that host were stragglers or had crashed. Other instances of unresponsiveness without helpful error messages could typically be associated with hardware-related issues, such as the aforementioned Xid/SXid/ECC errors causing the NVIDIA driver or NVIDIA docker communication driver to lock up. To distinguish the NCCL hangs from driver hangs and from a race condition or deadlock in Python code, we used tools including Py-Spy and GNU Project Debugger (GDB) to live-debug stalled processes wherever we encountered them. Using this method, we were able to catch one particular issue where, due to a misconfiguration in the Python threading settings, we were unable to launch the eight multi-threaded NCCL GPU processes properly on certain hosts which hit a race condition during pre-PyTorch initialization code. Training slowdowns (as measured by MFU) Lack of instrumentation could make these types of issues even more frustrating than the previous category. In addition to breaking out Py-Spy, stack trace inspection, and GDB, we also spun up NVIDIA Nsight and profiling tools to help, some of which were difficult to work with in a highly distributed setup. Sadly, generic slowdowns or lower-than-previously-demonstrated model flops utilization (MFU) could be caused by a variety of reasons. First, it proved helpful to double-check configurations, code, and environment variables. We experienced running the wrong model, the wrong batch size, the wrong UFM or NCCL settings, the wrong CUDA_DEVICE_MAX_CONNECTIONS, which all caused suboptimal performance. We also found it useful to measure instantaneous (i.e. per-batch) MFU rather than a smoothed or windowed average, as the pre-smoothed shape of the MFU curve often helped us diagnose the class of issue. Issues included: Training immediately started off at extremely low MFU (less than 1/10th of expected) and remained stable This was most often a hardware issue with the InfiniBand networking, such as a dead switch at the T2 or T3 layer. It could also be caused by hardware issues between the GPU and the NIC, showing up in dmesg as PCIe x16 lanes limited by … Training immediately started off at 30% of expected MFU and remained stable This could be caused by one host with improperly set GDR (NVIDIA Peer Memory), or incorrect GDR environment variables. Training immediately started off at ~60-80% of expected MFU and remained stable Most commonly, this was caused by degraded or faulty InfiniBand links, especially if a single particular GPU had a faulty associated InfiniBand NIC, causing NCCL to try to route the traffic over local NVLink and use the NIC on another GPU on the same host. It could also be caused by CPU throttling, which required tweaking some BIOS settings for particular hosts. Sudden drastic dips (by 10x) for single batches that occurred regularly This was almost certainly related to checkpointing or evaluations — verifiable by checking against epoch or step counts. Annoyingly, this causes many false positives if automated alerting is set just to trigger off of MFU anomalies. Sudden drastic dips (by 10x) for single batches that occurred randomly and rather rarely (on the order of every 15 minutes), and achieved full recovery to good MFU immediately afterward This seemed to be most commonly caused by other CPU-heavy workloads scheduled on one of the hosts in the run. Rather than build profiling tooling to identify the particular host, we found it easier to crudely monitor CPU usage by PID. This could also be attributable to sporadically poor networking, such as dataloader bottlenecks. We used metrics monitoring and added Python code timing logs for the dataloading, checkpoints, and any non-NCCL code, which proved quite reliable. MFU graph gradually sagged downward over the course of a run, but returned to 100% upon any restart Theoretically, this would be caused by heat accumulation on the switches, but we never saw that. Instead, we used Python and NVIDIA profilers to determine that the degradation seemed to be the result of automatic garbage collection. While debugging these slowdowns, we noticed a pattern of periodic dips in throughput that almost appeared deterministic. As the training run progressed, the dips impacted a progressively larger percentage of distributed operations. This led to a hypothesis that the dips could be related to automatic garbage collection, which we validated by profiling and testing. Once we disabled automatic garbage collection and scheduled garbage collection to occur at specific intervals across all hosts, these throughput “sags” disappeared. We used a synchronous distributed training algorithm, FSDP, which is based on ZeRO-3. During a blocking operation, a single worker process running garbage collection could slow down every other worker. With hundreds of worker processes, this could result in significant slowdowns. Good performance in the beginning, then sudden dips (to 70% of expected) that persisted at high frequency (every 15 seconds) We observed this was correlated with NVIDIA GPU “clock throttle reasons,” which we collected via applying the proper settings to NVIDIA DCGM. Heat issues (GPU temperatures or broken/degraded host cooling fans) or power supply failures caused this. Also, some of our hosts with specific power supply hardware had voltage problems when we maxed out all 8 GPU utilization and 8x NIC InfiniBand utilization and CPU/RAM/disk at the same time, but only when all were being used — typically only during an actual training run. Good performance but a bit “noisier” than usual (high-frequency white noise variance between 90% and 100% of expected MFU) This was also InfiniBand hardware related, but typically due to moderately degraded or flapping links higher up in the network rather than at the less redundant host to T2 layer. Unfortunately, many of these issues are not easily pinnable to a particular host, and the InfiniBand-related issues were especially hard to nail down because of the topology-aware nature of the InfiniBand switch technology. InfiniBand seemed to prefer adjacent hosts in the InfiniBand fat-tree design, and the UFM could route packets in ways that would result in asymmetric link speeds. Here’s a quick summary/flowchart/sanity checklist for debugging throughput regressions: Did it ever work? Did you change something recently (e.g. merged code, updated drivers)? Are you running on healthy hosts? Are all your dependent services running, including third party SaaS, e.g. Docker Hub, GitHub, or whatever else your stack depends on? Are you sure you ran with the exact same code, environment, configurations, versions, host list, rank order, random seed as the last time (if possible)? Is it reproducible? Is it correlated with anything else? Other processes? Daily crontab? Host or DCGM or UFM metrics? Are your tools to measure metrics correct? Does the issue still occur when running reduced code (smaller model, faked data, no checkpoint saving or loading)? Improving infrastructure tooling Upon completing the above steps, one can achieve good performance when training a model…at least until something inevitably breaks. In this section, we cover a few different tools and systems that we made to ensure that training continued running smoothly, ideally with a minimal amount of human intervention. Because we are a small team, we simply didn’t have enough people to constantly make manual repairs, so we attempted to automate as much of the process as possible. Almost all of our training run problems could be pinpointed to faulty machines or network components. These failures occur frequently in a large cluster, so it was essential to automate the process of disabling faulty machines and network components and requesting repairs. Faulty machines We developed a system for automatically relaunching crashed runs from the most recent checkpoint. The relaunch process would begin by running our health checks on every available machine and classifying each machine’s health based on which health checks it passes; it would then attempt to relaunch the training job on the healthiest machines. Faulty network components All the network component failures we observed were detected by the UFM and registered in the UFM event log, so responding to network component failures was just a matter of parsing the UFM log and taking the appropriate action for each event. The UFM events system is quite complicated, containing dozens of event types. In practice, however, we found that only a handful of events were problematic, mostly related to links going down or high symbol error counts. After identifying these events, we were able to write scripts to parse the UFM event log, disable links and ports implicated in recent events, file maintenance tickets on those network components, and re-enable those components once maintenance was finished. Local mirror file system It became obvious early on that one of the bottlenecks to large distributed training runs would be Ethernet speed into and out of the cluster. A shared Ethernet connection with a bandwidth of about 10Gbit/s would quickly become saturated if hundreds of workers tried to download datasets and model checkpoints simultaneously. As a result, we decided to build a local file system within our cluster to mirror cloud storage and essentially serve as a cache to reduce the number of files we needed to fetch from S3. To deal with cluster churn (machines would often be disabled or swapped out for maintenance reasons) we did three-fold replication of each file, using consistent hashing to distribute load evenly in a way that minimized file movements during churn. Limited disk space on the cluster meant that we also had to develop various tools for keeping track of file lifecycles and clearing out files that were no longer relevant. Local distributed Docker registry We also made use of Kraken, a fantastic open source software to enable peer-to-peer transfer of Docker images. We had almost no issues with it, which was a bit surprising given the complexity of both the task and the implementation. Various performance-monitoring tools We set up the default Torch profiler as well as NVIDIA’s Nsight Systems. The latter was helpful for understanding exactly how long forward/backward passes and NCCL communications take, and in determining if we were bottlenecked by communications or compute for a given model size and worker count. However, Nsight Systems was somewhat difficult to use, because it required running Docker in privileged mode, disabling security checks related to performance monitoring events, and because saving out the profiles often required stopping the entire training process. In addition, we found it helpful to write tools to detect slow training batches and understand potential causes of slowness. The most useful of these was a tool which monitored how long each batch took and dumped the stack trace of every worker when a batch was unusually slow - this made it easier to identify specific hosts with subtle hardware or software issues. Subdividing machine groups to pinpoint faulty hosts During our first few months using the cluster (when our health checks were not as thorough as they are now), we often ran into a situation where a training run on a specific set of machines was failing but it wasn’t clear which machine was at fault. To pinpoint faulty hosts, we developed tools to make it easy to partition the set of machines into subsets and launch a smaller job on each subset of machines. For instance, if a job on a group of 48 machines was failing, we would launch smaller runs on six groups of eight machines, and then launch smaller runs on eight groups of six machines. It would often be the case that only a single run would fail on each of these two stages, allowing us to conclude with high confidence that the machine which was part of faulty runs in both stages was problematic. Reflections and learnings Over the course of setting up and maintaining our infrastructure, we gleaned a few useful learnings on the overall process: Being able to swap out machines for each other is extremely useful. For any given training run we found it helpful to have 10-20% more machines than necessary for the run, so that we could easily relaunch in case of machine failures. Setting up the cluster networking in such a way that every machine is closely connected with every other machine meant that we could essentially use any working subset of the machines. It’s worth writing tests and automated solutions for every kind of hardware or software failure you experience, since every issue encountered during training will reoccur. Similarly, for every opaque error message, it’s worthwhile to write tools to make the error more interpretable. Reproducibility is the key to good science. One rule we quickly adopted is “change only one thing at a time,” even for the simplest things. Trust, but verify. Whenever we introduced an external tool to the process or onboarded a new person, either externally or internally, we made sure to double-check their claims, especially if subsequent steps depended on those results. Conclusion Training large language models requires complex infrastructure to even get started. We chose to be heavily involved in the details of the infrastructure set-up both because we believe it is important to fully understand the systems we work with, and because we suspected that it would ultimately prove more efficient. Now, having gone through the full process, we’re very glad we took this approach — it ended up being critical to have full control over our infrastructure and to be able to easily debug problems at every level of abstraction. While this process required extensive supervision and iteration, it allowed us to deeply understand the underlying procedures, build a series of tools to ensure healthy hosts, learn how to automate systems to ensure continual smooth training, and ultimately create infrastructure that has allowed us to rapidly iterate on the training of cutting edge language models. This infrastructure process exemplifies our approach to researching and building a robust foundation for AI agents: probing the nitty-gritty details, continually improving upon existing processes, and building useful tools and systems that enable our scrappy team to tackle larger challenges. If this full-stack approach resonates with you, we’re always hiring!",
    "commentLink": "https://news.ycombinator.com/item?id=40816158",
    "commentBody": "Infrastructure setup and open-source scripts to train 70B model from bare metal (imbue.com)247 points by thejash 19 hours agohidepastfavorite29 comments thejash 19 hours agoIn the span of a few months, with a small team of researchers and engineers, we trained a 70B parameter model from scratch on our own infrastructure that outperformed zero-shot GPT-4o on reasoning-related tasks. Using our cluster for high performance training meant that every component — InfiniBand, Ethernet, GPUs, and the nodes themselves — had to work perfectly. If even a single one of the over 12,000 connections was a little flaky, it could slow down the entire training run. We're sharing open-source scripts and an end-to-end guide for infrastructure set-up that details the process of making everything work perfectly, and ensuring that it stays that way. This is one of a three-part toolkit on training a 70b model from scratch. The other two sections focus on evaluations and CARBS, our hyperparameter optimizer; you can find them here: https://imbue.com/research/70b-intro/ Thoughts and questions welcome! :) reply vessenes 2 hours agoparentLoved this and the detail - thank you. It’s the best inside detail on the engineering work behind these models I’ve ever read. Two things I’m curious about- first, what, if any difference would you imagine in training a 400b parameter model? It seems that you have plenty of vram across the cluster, but I want to know what you think. Second, do you think this sort of architecture is the end game for model training? It seems sooo fragile. Are there better shared training mechanisms/architectures? Are there better cluster geometries? Thanks again - great read. reply ipsum2 11 hours agoparentprevWhat happened to the Minecraft-like 3d world your team built? Did you guys pivot? reply highfrequency 4 hours agoparentprev> outperformed zero-shot GPT-4o Cool stuff! Does this do RLHF or just pretraining? If the latter, how did you manage to beat GPT 4? reply Flumio 11 hours agoparentprevNice. Tx for the write up reply chx 8 hours agoparentprev> If even a single one of the over 12,000 connections was a little flaky, it could slow down the entire training run It's an unusual enough sentence to be remarkable and I was like \"I read this exact same sentence before\". Indeed, this and most of the writeup appeared on Twitter, LinkedIn, Reddit it seems word-by-word. Is this just spam ? https://x.com/imbue_ai/status/1805629547473518695 https://reddit.com/r/learnmachinelearning/comments/1dobgbs/t... https://www.linkedin.com/posts/mattboulos_training-a-70b-mod... reply lolinder 5 hours agorootparentThis is the kind of criticism that could only come from someone without much formal writing experience. This is a very normal workflow: You write a full-length text detailing the project you worked on. You then trim it down to a summary which you share with a group of people X. You then trim it down into a different summary which you share with a group of people Y. When you do this multiple times you unsurprisingly end up with some sentences that make it into multiple summaries because they're that important to the thesis! (Also, the summaries on Twitter and Reddit aren't anything close to \"most of the writeup\"—the full text is 6000+ words!) reply neilv 7 hours agorootparentprevI'd rather some company copy&paste the same text multiple places -- if the alternative was that those places would instead get obfuscation of the same information to appear novel each time (so I'd have to read all of them to realize they're all just the same info). reply fastasucan 1 hour agorootparentprevI dont inderstand your issue with this. Is it that they share their work several places, or that they don't describe their work in an unique way every time? reply ac29 3 hours agorootparentprevEh, seems like legit marketing to me. Yes, they are trying to sell you something, but they are doing that by releasing non-trivial research and open source code. reply leothetechguy 7 hours agorootparentprevThe same company reports multiple times on a finding they've made through multiple social media channels? Shocking. /s reply bottled_poe 8 hours agorootparentprevlmao, I was thinking this was bullshit and you’ve cemented that position. We’ve entered the grifting stage of this AI cycle. Salut. reply knowaveragejoe 2 hours agorootparentHaving listened to the person who wrote this speak at length about the subject, it is not BS or grifting. reply loudmax 6 hours agoprevThis was discussed on the Latent Space podcast a few days ago: https://www.latent.space/p/llm-training-2024 That was a good episode, worth listening to for hearing justifications behind some of these decisions. reply alias_neo 5 hours agoprev> This post focuses on one cluster that had 4,092 H100 GPUs spread across 511 computers, with eight GPUs to a computer Am I right in understanding, that's over $100 Million worth of GPUs? I wonder what/when/if any of this will be within the realms of an enthusiast with a gaming-pc budget. reply freeqaz 4 hours agoparentLooks correct. They raised $200m from NVIDIA which I presume is in pure GPUs. https://news.crunchbase.com/ai-robotics/new-ai-unicorn-imbue... reply mmastrac 3 hours agoprevHonest question: why is there so much PC hardware in the mix here? Why don't we have PCI + infiniband backends with GPUs and a little tiny orchestrating ARM controller and just let them all coordinate with each other? Is it just \"momentum\" from previous designs and/or lack of \"market\" for specialized GPU controllers? reply bick_nyers 1 hour agoparentAre you asking why pay extra for a CPU and RAM? Not everything can be done on a GPU, for example, .png decompression. If you really analyzed your training code and preprocessed your data substantially you could probably get away with very lightweight CPU/RAM resources but I think the reality is that it's such a minor contribution of cost to the overall system (GPU are expensive) that wasting development cycles on that degree of optimization isn't strictly necessary. When you're a hyperscaler you are likely chasing those fractions of a percent of cost efficiency though. To use my original example, you would likely want to preprocess your .png to either .webp (multi-threaded lossless) or .jpeg (lossy), but likely it wouldn't make sense to turn it into a GPU decompressible format as you would save on CPU cost during training but would pay more in storage (and maybe transfer) cost. Edit: To be more clear, if the CPU work is bottlenecking training, you want to optimize that as much as possible by preprocessing your data/tweaking training scripts. What I'm discussing here is the gap between \"fast enough\" and \"faster\": CPU is not fast enough for training < CPU is exactly fast enough for training < CPU is faster than needed for training reply lifeisstillgood 9 hours agoprevI am fascinated by the total electrical power drawn to build models - power and cooling I guess. Do you have any numbers on that (the point being Zuckerberg in a podcast suggested the next 1GW model was being planned - basically a data centre with a mid sized power plant attached) reply instagib 7 hours agoprev4,092 H100 GPUs. They’re working on “self-coding”. No-code or minimal code solutions or? Quite a few articles and such people may be interested in also on their website: https://imbue.com/our-work/ reply weinzierl 5 hours agoprevHow much did it cost? Overall, from nothing to the usable model files, in hardware cost, development hours and ultimately electricity and cooling? reply renewiltord 12 hours agoprevThis is hella cool. Cisco has a new nvidia collab with 800G per-port. I don’t recall if it was RoCE or not. The infiniband is accessible by the GPUs here? Beautiful. Thank you for sharing all this. One of the more directly useful posts. reply john2x 13 hours agoprev [–] once the model is trained, what happens to the hardware and infrastructure? reply trashtester 11 hours agoparentVoltage Park is a cloud provider. This is no different from renting barebone infra from AWS, GCP or Azure. Except Voltage Park, being smaller, is probably more willing to provide some customized setup. Indeed, they may even see it as a learning opportunity for when they rent similar setups to other customers. reply pvg 13 hours agoparentprevIt probably isn't the answer but should be - LAN party. reply rvnx 13 hours agorootparentGPUs will be reused for mining Monero and exfiltrate money to the founders at the expense of the investors. Oops, don't tell I told you. EDIT: Sorry Dogecoin, thanks to the tip! reply BetaDeltaAlpha 12 hours agorootparentMonero uses a CPU-optimized consensus algorithm. Dogecoin is a better bet. reply surfingdino 12 hours agorootparentprevAre you suggesting training models is a cover for mining crypto? The hardware is dual-purpose... reply gostsamo 12 hours agoparentprev [–] Either training the next model or inference for the already trained one. In some cases, you might even offer it as a service. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A small team successfully trained a 70 billion parameter model on their infrastructure, surpassing zero-shot GPT-4 in reasoning tasks.",
      "The setup involved 4,092 H100 GPUs across 511 computers, connected via a three-tier InfiniBand network, with detailed provisioning, diagnostics, and automated health checks.",
      "They are sharing a comprehensive guide and scripts for setting up similar infrastructure, emphasizing reproducibility, automation, and thorough testing, and are open to hiring."
    ],
    "commentSummary": [
      "A small team successfully trained a 70 billion parameter model from scratch, outperforming zero-shot GPT-4 on reasoning tasks.",
      "They utilized a high-performance cluster with over 12,000 connections, ensuring seamless operation of all components.",
      "The team is sharing open-source scripts and a guide for infrastructure setup as part of a three-part toolkit, which also includes sections on evaluations and hyperparameter optimization."
    ],
    "points": 247,
    "commentCount": 29,
    "retryCount": 0,
    "time": 1719529720
  },
  {
    "id": 40815130,
    "title": "Python grapples with Apple App Store rejections",
    "originLink": "https://lwn.net/SubscriberLink/979671/4fb7c1827536d1ae/",
    "originBody": "LWN .net News from the source Content Weekly Edition Archives Search Kernel Security Events calendar Unread comments LWN FAQ Write for us User: Password:| Subscribe / Log in / New account Python grapples with Apple App Store rejections [LWN subscriber-only content] By Joe Brockmeier June 27, 2024 An upgrade from Python 3.11 to 3.12 has led to the rejection of some Python apps by Apple's app stores. That led to Eric Froemling submitting a bug report against CPython. That, in turn, led to an interesting discussion among Python developers about how far the project was willing to go to accommodate app store review processes. Developers reached a quick consensus, and a solution that may arrive as soon as Python 3.13. The problem at hand is that Apple's macOS App Store is automatically rejecting apps that contain the string \"itms-services\". That is the URL scheme for apps that want to ask Apple's iTunes Store to install another app. Software distributed via Apple's macOS store is sandboxed, and sandboxed apps are prohibited from using URLs with the itms-services scheme. That string is in the urllib parser in Python's standard library, though an application may never actually use the itms-services handler. Of course, Apple did not do anything so straightforward as to explain this to Froemling. Once he filed an appeal with Apple about the rejection, Apple finally told him that parse.py and parse.pyc were the offending files. After that, he said, it was not hard to track down the problem: Now in retrospect I'm frustrated I didn't think to run a full text search for itms-services over Python itself earlier or stumble across one of the other cases of folks hitting this. Russell Keith-Magee started the discussion in the Python Core Development discussion forum on June 17. He wanted to know whether \"acceptable to app stores\" should be a design goal for CPython, or if that compliance should be a problem left to the tools that generate application bundles for app stores. Paranoid and inscrutable Keith-Magee noted in his initial question that Apple's review processes were the most \"paranoid and inscrutable\" of app-store-review processes, but that other app stores also had \"validation and acceptance processes that are entirely opaque\". One solution might be to obfuscate the offending string to pass review, but that might \"lead to an obfuscation arms race\" and there were no guarantees this would be the last time the project had to resolve app-validation problems. The other option, he said, was to consider this to be a distribution problem and leave it to tools like Briefcase, py2app, and buildozer to solve. Traditionally, they have had to patch CPython anyway, he said, because it did not support Android or iOS \"out of the box\". But that will change with Python 3.13 when no patching should be required for those platforms. Like what you are reading? Try LWN for free for 1 month, no credit card required. Alex Gaynor suggested that the project try an approach that Keith-Magee had not put forward inspired by Gaynor's experience with the cryptography library. The project often receives complaints that the library refuses to parse a certificate that is technically invalid, but was in wide use. He said that the policy was to accept pull requests that work around those issues \"provided they are small, localized, and generally aren't too awful\". But, he added, these patches should only be accepted on the condition that someone complains to the third party (in this case Apple), and extracts some kind of commitment that they would do something about it. He suggested that the workaround be time-limited, to give users a decent experience \"while also not letting large firms simply externalize their bizarre issues onto OSS projects\". Brandt Bucher wondered whether obfuscation was even allowed, or if it would be seen as circumventing the review process. That was a question no one seemed to have an answer to; and Keith-Magee responded with an 8-Ball emoji and the phrase \"ask again later.\" He added that Gaynor's approach sounded appealing, but it would be like screaming into the void. Apple, he said, barely has an appeals process and there is no channel available to the Python project \"to raise a complaint that we could reasonably believe would result in a change of policy\". Another approach, suggested by Alyssa Coghlan, would be to use a JSON configuration file that urllib would read to set up its module level attributes \"rather than hardcoding its knowledge of all the relevant schemes\". That could allow app generators to drop \"itms-services\" from the configuration file rather than patching urllib.py directly. Keith-Magee said that could work, but \"it strikes me as a bit of overkill for an edge case\" that could be handled by obfuscation or distribution-level patching. On June 20, Keith-Magee wrote that he had thought of another approach: adding a build-time option called \"--with-app-store-patch\" that removes code that is known to be problematic. He said it would be enabled by default for the iOS platform, and disabled elsewhere. It could be used when building an application for macOS, if the developer intended to distribute that application via the macOS App Store. He suggested that the option could also accept a path to a file with a patch, to allow distributors to provide an updated patch if an app store changes its rules after the maintenance window for a Python release has closed. Let's paint the bikeshed Coghlan asked if it was now time to \"paint a config option bikeshed\". She said that the proposed option name was both too broad and too narrow. The \"app-store\" component of the name was too broad, because it could encompass any app store, not only Apple app stores. The \"patch\" component was too narrow, because patch specifies the method of complying with policies rather than intent. There may be other methods required to comply with app-store-compliance checks. Keith-Magee liked the suggestion about dropping \"patch\" from the option name, and suggested painting the bikeshed a nice shade of \"--with-app-store-compliance\" that would interact with platform identification to sort out what is required. On June 25, Keith-Magee thanked participants in the discussion for their input, and pointed to a pull request that would implement the --with-app-store-compliance configuration option. In the request, he noted that it would be possible to use the option with platforms other than iOS or macOS, but there were no use cases for that at present. If all goes well, it should be available in Python 3.13. It is frustrating that free-software projects like Python have to waste time finding ways around opaque review processes just so developers can write software for non-free platforms. However, the approach taken by Keith-Magee and other CPython developers seems to be the least-bad option that offers the best experience for Python application developers. It will almost certainly not be the last time that a project runs into this problem. (Log in to post comments) Alternatively Posted Jun 27, 2024 14:06 UTC (Thu) by dskoll (subscriber, #1630) [Link] (7 responses) I understand the desire of free software advocates to have our software run everywhere, but at some point maybe we shouldn't continue enabling these kinds of shenanigans? Every piece of application software that runs on Apple's ecosystem makes it a bit more valuable. How about if we stop playing Apple's games and not support their platform until they become reasonable and transparent? Every software project that did that would make Apple's ecosystem a bit less valuable and eventually this would be noticed. Alternatively Posted Jun 27, 2024 14:45 UTC (Thu) by shironeko (subscriber, #159952) [Link] (1 responses) What ends up happens is that people start shipping slightly different workarounds in downstream. The amount of apple software would not decrease, it just gets more frustrating to use free software on it. Alternatively Posted Jun 28, 2024 13:15 UTC (Fri) by dskoll (subscriber, #1630) [Link] If you play Apple's app store game, you have lost. That's an incontrovertible fact. The only way not to lose is not to play the game. Free Software devs should not spend the energy required to tapdance as Apple fires bullets at their feet. Alternatively Posted Jun 27, 2024 14:55 UTC (Thu) by khim (subscriber, #9252) [Link] (2 responses) While pretty interesting, I think that's the question for another time. There are absolutely no need to have string itms-services in your app if you don't support said services. And both “we want to support Apple” and “we don't support Apple” stances lead to the same outcome: removal of these from the standard library. Now, if you want to both support non-developers who want to use these proprietary services and also developers who get rejection notices because of them then you are in bind, but I think simple and straightforward removal is the best outcome: remove the support, leave these for guys who actually want to use these services from Python to resolve, somehow. They decided to deal with unreasonable company, they can deal with the fallout. If would be time to decide when Apple would bad something because it supports some other free service. Alternatively Posted Jun 27, 2024 15:45 UTC (Thu) by flussence (subscriber, #85566) [Link] (1 responses) This is a general purpose URL parser. Saying it should not recognise an URL pattern used by a malevolent actor is like saying tzdata should delete timezones only used in \"bad\" countries. Anyway there's a really simple workaround for this. Simply ship the source file in EBCDIC. The type of whiteboard-interview weenies who come up with and enforce these restrictions will not be smart enough to figure that one out. Alternatively Posted Jun 27, 2024 16:28 UTC (Thu) by notriddle (subscriber, #130608) [Link] > This is a general purpose URL parser. Other general-purpose URL parsers don't special-case this scheme. Both URL specs have support for schemes that are unknown to the parser, and neither https://url.spec.whatwg.org/#special-scheme nor https://datatracker.ietf.org/doc/html/rfc1738#section-3 includes itms-services in their lists. Alternatively Posted Jun 27, 2024 16:40 UTC (Thu) by elw (subscriber, #86388) [Link] (1 responses) > How about if we stop playing Apple's games and not support their platform until they become reasonable and transparent? The problem I see with that position is that Python is not the only easy to use language for app development. The only thing that would be accomplished by standing firm and refusing to play their game would be a reduction in usage in the platform. Unless there is some Killer App ™, written in Python, that would cause tangible damage to the overall Apple ecosystem, Apple holds all the cards and they know it. Alternatively Posted Jun 27, 2024 23:43 UTC (Thu) by Heretic_Blacksheep (subscriber, #169992) [Link] I don't really see this as a problem. iOS is locked down and people know it. They also know that Apple's application of its policies are... arbitrary. Their policies are designed to protect their bottom line, not their customers. Appealing to the security of their customers is the PR smoke screen designed to mask their anti-competitive policies and practices. They use targeted advertising just like everyone else. People get up in arms about these practices with Microsoft, Google, etc, but hardly a peep when Apple does the same thing behind the mask of respectability. It's because Apple is really really good about smoke and mirrors while ignoring everyone that points out the Emperor has no clothes -- and their customers reward them for it. Refusing to put up with App Store policies won't reduce Python in the Apple space. Most people that use Python are going to be using it on Macs, not iDevices. It's trivial to install Python on Macs without the App Store. I'm a Mac and iDevice user. I'm not throwing stones without knowing something about the ecosystem from the end user's point of view. I know and understand the limitations of using Apple devices. Should such a point come where Macs are as locked down as iOS where I can't install and run code I want to run, it'll be the end of me using both. I'm saying that sometimes people need to say \"Enough is enough!\" Even to a trillion dollar company that may not care. Strip the functionality altogether. Posted Jun 27, 2024 14:45 UTC (Thu) by pavon (subscriber, #142617) [Link] (3 responses) I'd question whether something like itms-services scheme support really belongs in a standard library to begin with. That is a pretty niche functionality for a single operating system. Most python software running on macOS will never use it, just things like installers. There isn't equivalent support for the dozens of Windows-specific schemes that Microsoft uses. Obviously, you'd need to give time to deprecate it, so another work-around would be needed for the short-term, but in the long-run if Apple wants to make it hard to integrate with their platform, then stop supporting those integrations and let application developers use a third-party library. Costs vs Benefits Posted Jun 27, 2024 16:35 UTC (Thu) by nickodell (subscriber, #125165) [Link] (1 responses) I think that when asking whether to remove it, we should ask what the cost of the feature is. If you look at the PR which adds this support, excluding tests, it is added in one line of code. (https://github.com/python/cpython/pull/104312) This seems like pretty minimal maintenance burden. Costs vs Benefits Posted Jun 27, 2024 20:26 UTC (Thu) by pavon (subscriber, #142617) [Link] The cost was negligible, until Apple decided to make it more expensive by requiring different behavior and thus different builds depending on how the user installed the app. Strip the functionality altogether. Posted Jun 28, 2024 15:21 UTC (Fri) by smurf (subscriber, #17840) [Link] > I'd question whether something like itms-services scheme support really belongs in a standard library to begin with. Well, this one's easy. Some URL schemes contain a network location (\"netloc\"). Some do not. People tend to expect the URL parser to reply with a struct that breaks out the netloc for URLs that contain them. As urllib.parse contains a whitelist of URLs with netlocs (sensibly IMHO) it needs to contain this string. Alex's approach Posted Jun 27, 2024 16:06 UTC (Thu) by tialaramex (subscriber, #21167) [Link] (9 responses) The choice to time-limit workarounds allows you to properly frame the problem when it inevitably arises. For example TLS 1.3 as designed has Downgrade prevention, if you try to *pretend* by meddling with the TCP data that somebody else's server doesn't know TLS 1.3, then either you tip off the server that you're interfering or you tip off the client, or both. There is no way to get to a scenario where the client thought it asked \"Do you speak TLS 1.3?\" but the server thought the client asked only \"Do you speak TLS 1.2?\" without the session being torn down. But, of course idiot middleboxes broke this. Because they don't implement TLS properly, they'd hide the client's real question about TLS 1.3 from the server, but they'd pass through its answer to the question they did ask, thus exactly performing a downgrade, which sets off the downgrade prevention, your browser just closes the session. This was discovered after TLS 1.3 shipped, because Downgrade prevention obviously can't be enabled pre-standard, that's a DOS against yourself in the standards development process. So, Google shipped builds of Chrome with a hack, _if_ you turned on the hack and _if_ the server says \"I'm a TLS 1.3 server but you claimed not to know TLS 1.3 so that's fine, unless you were downgraded\" this Chrome ignores the problem. Temporarily. This damages your security, but you're using a middle box, you already don't have any security, that's what you paid money for, to destroy your security. Google told the affected vendors that their customers could enable this hack (in their installed Chrome builds) and *crucially* it told them that in a fixed period of time this hack would just be summarily deleted from Chrome. They were not going to reify this nonsense as part of the protocol design, it was a temporary hack to get some people working while these companies fixed their shit. And they did it. Years ago now Chrome just deleted the hack. If your middle box vendor didn't fix their bug in a reasonable timeframe, or you couldn't be bothered to install their fix - now Chrome doesn't work, too bad. Everybody gets reliable TLS 1.3 Without making it clear that this stops working in (say) 12 months, there is *no* incentive for Apple to fix it, for Apple's developers to care that it's a problem, for anybody to do anything except dump on the Python maintainers whatever they can't be bothered to fix. They're making their own lives worse. If they set a specific deadline and stick to it, it's clear what the problem is, that doesn't mean Apple will care, but too bad, you offload the problem onto exactly the people who wanted it, Apple developers, people who are used to and have accepted that they have an unhealthy masochistic relationship to an unfeeling corporation. There is no reason why anybody involved in Python who doesn't share that weird relationship should be inconvenienced. Alex's approach Posted Jun 27, 2024 16:20 UTC (Thu) by pbonzini (subscriber, #60935) [Link] (8 responses) > Without making it clear that this stops working in (say) 12 months, there is *no* incentive for Apple to fix it, for Apple's developers to care that it's a problem, for anybody to do anything except dump on the Python maintainers whatever they can't be bothered to fix. They're making their own lives worse. The question is, why would Apple have any incentive to fix it? In the middlebox case, the makers get money and have support contracts with whoever buys their products. In Apple case, app developers have no alternatives than Apple if they want their app to run on iPhone/iPad, and Apple gets money from users, not from app developers. So the odds are completely stacked against the app developers, and if Python tried to put any deadline on the workarounds, the only effect would be to place them between the hammer (Python) and the anvil (Apple). Alex's approach Posted Jun 27, 2024 18:14 UTC (Thu) by mb (subscriber, #50428) [Link] (7 responses) >The question is, why would Apple have any incentive to fix it? I don't know. Apple *clearly* caused the breakage. That's why they should care. If they now decide to *voluntarily* drop Python support from the App store, it is their decision and we should respect that. If they care about Python on their platform, they can add an exception to their obviously completely broken checker. >So the odds are completely stacked against the app developers That's the case anyway. The Apple platform is fully proprietary and Apple decides everything anyway. And developers and users already agree to that. That's the deal. Alex's approach Posted Jun 27, 2024 18:59 UTC (Thu) by pizza (subscriber, #46) [Link] (6 responses) > Apple *clearly* caused the breakage. That's why they should care. Why would Apple care about squishing an ant as they walk? Alex's approach Posted Jun 27, 2024 19:04 UTC (Thu) by mb (subscriber, #50428) [Link] (5 responses) It's their decision. No (apps with) Python for Apple users then. I don't see why this is Python's problem. Alex's approach Posted Jun 27, 2024 19:55 UTC (Thu) by pbonzini (subscriber, #60935) [Link] (4 responses) Reality is that it's Python's users' problem, and projects have a tendency to listen to their users' problems, and do something about them. Alex's approach Posted Jun 27, 2024 20:15 UTC (Thu) by mb (subscriber, #50428) [Link] (3 responses) Sure. Many Projects tend to put workarounds everywhere instead of really fixing things. Happy whack a mole. Users must complain to Apple. They broke Python. Alex's approach Posted Jun 28, 2024 9:04 UTC (Fri) by NYKevin (subscriber, #129325) [Link] (2 responses) That is not what will realistically happen. What will happen is that everyone who wants to run Python on iOS will get a rejection the first time, Google it, discover the problem, write and carry their own patch, and submit a modified application without this string. Or CPython can fix it once and we can all move on with our lives. IMHO the easiest solution is to just drop the string from urllib altogether - it's there to support Apple devices in the first place. Apple doesn't want it there? Fine, it's gone. Good riddance. Alex's approach Posted Jun 28, 2024 15:43 UTC (Fri) by mb (subscriber, #50428) [Link] (1 responses) >Google it, discover the problem, write and carry their own patch That's Ok. They decided to play by the rules of Apple. So they should carry the workaround for Apple's stupid decisions. Why should Python care? >Apple doesn't want it there? Fine Yes. It's fine. It's their decision and their problem. >Fine, it's gone. Good riddance. Do you realize that this is not a fix of the actual problem? Apple will append something else to the list of forbidden words tomorrow. Why should we let Apple indirectly make stupid decisions for an independent Open Source project? Alex's approach Posted Jun 28, 2024 18:53 UTC (Fri) by NYKevin (subscriber, #129325) [Link] The question is not why Python should care about iDevices. The question is why Python should care about the Apple ecosystem at all. itms-services is not a standard protocol. It appears in no RFC (that I've ever heard of). It is purely an Apple thing. And now it is causing problems, that were also created by Apple. Why should Python even waste time thinking about any of this? Just yeet the whole thing out of urllib, and Apple developers who want to use this protocol for whatever reason can take a PyPI dependency. But you don't even need a PyPI dependency, because urllib already DTRT on protocols it does not understand: >>> urllib.parse.urlsplit('nonsense://www.example.com:80/foo/bar') SplitResult(scheme='nonsense', netloc='www.example.com:80', path='/foo/bar', query='', fragment='') The string itms-services only appears in a list that tells the parser that it \"uses netloc\" (i.e. it should have a netloc part after the scheme, unlike say file://). As you can see above, it will happily parse nonsense schemes with netloc even if they are absent from said list. So at most, this might get you stricter checking in a few corners of the API, but it is probably not a showstopper. Apple broke it Posted Jun 27, 2024 17:38 UTC (Thu) by mb (subscriber, #50428) [Link] Apple broke it, Apple ought to fix it. It's that simple. Regulatory solution Posted Jun 27, 2024 23:59 UTC (Thu) by skissane (subscriber, #38675) [Link] (12 responses) I really hope that some regulator in some country mandates an independent third party appellate process for app store decisions. That way Apple can be forced to defend this kind of nonsense publicly, and get ordered by the appeal to stop it. If some minor county did it, Apple might just try to pull out of that country; if a major economy such as the EU did it, they’d have no real choice but to comply. For the EU, it could be a complementary move to their mandating support for third party app stores under the DMA, taking into account the reality that having to install a third party app store is likely too much friction for many users. Regulatory solution Posted Jun 28, 2024 2:20 UTC (Fri) by dskoll (subscriber, #1630) [Link] This is a great idea. I'd also like to see a mandate that the initial review process has to be completely transparent and any rejected app must come with a complete list of rules it broke as well as how the app can be remediated. Regulatory solution Posted Jun 28, 2024 5:43 UTC (Fri) by donald.buczek (subscriber, #112892) [Link] (10 responses) Looks like EU delivered. https://www.engadget.com/ios-174-is-here-enabling-third-p... \"Apple has rolled out its latest major iOS update, and there are some enormous changes for those in the European Union. With the arrival of iOS 17.4, the company is adhering to strict new rules in the bloc when it comes to the App Store. Apple now officially supports third-party app stores on iPhones in the EU, while developers can offer third-party payment options. Web browser makers no longer need to base their apps on Apple's WebKit, while Apple is opening up the NFC chip to wireless payments that have nothing to do with Apple Pay.\" Regulatory solution Posted Jun 28, 2024 9:22 UTC (Fri) by khim (subscriber, #9252) [Link] (9 responses) Are you even sure that something that lets you download apps from Apple and then without the ability to automatically upgrade them deserves the name third-party app store? I don't think anything but what China or Russia do would work. The goal should be not an attempt to make Apple adjust it's policy, this would just lead to endless cat-and-mouse games, the goal is to ensure it's not used by anyone who works for government, it's not allowed to be sold officially, etc. The fact that it was even allowed to exist in it's current form is clear failure of government regulators to catch the problem on time, but it's not too late to fix it. Regulatory solution Posted Jun 28, 2024 9:40 UTC (Fri) by donald.buczek (subscriber, #112892) [Link] (8 responses) > Are you even sure that something that lets you download apps from Apple and then without the ability to automatically upgrade them deserves the name third-party app store? No, I have no idea how this is implemented. I've just remembered the news that Apple should be forced to allow other app stores and then searched and found this announcement. I just assumed \"other app store\" would mean independent things like F-Droid. This is not the case? Regulatory solution Posted Jun 28, 2024 12:15 UTC (Fri) by khim (subscriber, #9252) [Link] (7 responses) > This is not the case? Judge for yourself. Just look on the picture and read the story. EU told Apple that everyone should be able to download app from third-party web site. And Apple delivered! App really comes from third-party web site. Specifically signed copy that Apple signed and gave you permission to install on 1 (one) device. And they charge 0.5$ for the privilege. Only if you are big and successful, of course. But if you are not then you have to provide Apple a stand-by letter of credit in the amount of €1,000,000 from a financial institution that’s at least A-rated or equivalent by S&P, Fitch, or Moody’s, and maintain that standby letter of credit as long as your alternative app marketplace is in operation. It wouldn't be used, don't worry, it's only there “just in case”, you see. And since Apple only gives you right to instal 1 (one) copy of app on 1 (one) device and you pay $0.5 for the privilege any update requires the end user to start the download process to save you money! Perfect, isn't? > I just assumed \"other app store\" would mean independent things like F-Droid. Well. There were two goal Apple wanted to achieve: Independent App Store should look and act like an independent app store Apple should control everything, earn money from each install and be able to reject request for installation for any reason. They achieved that. Whether to call that “an independent app store” is open question. Many say that this is not an independent app store, but this would require another court process, of course, and the end result would be equally silly. Ultimately the only fair treatment of Apple is two-fold: Decide how fast Apple needs to be outlawed in your country and there would be law that would forbid to buy and sell Apple devices (everyone accepts that such things take time not to cause immediate riots). Ensure that you are moving law toward that goal steadily and consistently. Anything else is absolute foolishness, none of independent countries should accept things like that. Well, maybe US is an exception since Apple can be controlled by various security agencies using internally planted people. But for everyone else it's either allowing Apple iOS devices or staying independent country, these things couldn't exist simultaneously. MacOS is moving in the same direction, BTW, it's a just a tiny bit behind. Regulatory solution Posted Jun 28, 2024 12:50 UTC (Fri) by donald.buczek (subscriber, #112892) [Link] (6 responses) You're right, that's disgusting. Regulatory solution Posted Jun 28, 2024 13:24 UTC (Fri) by Wol (subscriber, #4433) [Link] (5 responses) If the regulators agree with you, Apple are likely to find things get worse next time round the loop ... You play fair by the regulators, they play fair with you. You try to game the rules, the regulators will do the same. Cheers, Wol Regulatory solution Posted Jun 28, 2024 13:56 UTC (Fri) by khim (subscriber, #9252) [Link] (4 responses) > You play fair by the regulators, they play fair with you. You try to game the rules, the regulators will do the same. Sure, but why would that mean that Apple does the wrong thing? Apple Store brings around $100 billions per year to Apple, significant percent of that from EU, which means it would bring $1 trillion or so before collapse of EU in 10, after which EU would stop bringing billions to Apple, one way or another. And if instead of fighting for these $100 billion (and spending maybe $10 billion on courts and fines) Apple would “play fair” before collapse of EU the it would get $0.5 trillion which is much less than $1 trillion. Now, if you assume a different scenario, where EU doesn't collapse in 10 years and continue to bring billions for 100 years then sure, “playing fair” would be better, but how do you know EU would survive, let alone thrive, for 100 years? Regulatory solution Posted Jun 28, 2024 17:01 UTC (Fri) by Wol (subscriber, #4433) [Link] (1 responses) > Now, if you assume a different scenario, where EU doesn't collapse in 10 years and continue to bring billions for 100 years then sure, “playing fair” would be better, but how do you know EU would survive, let alone thrive, for 100 years? No. I'm assuming a different scenario where that doesn't matter. If Apple is perceived as gaming the rules, then the rules will change. In a lot less than 10 years. And in a way that is likely to be - shall we say - painful ... depending on how seriously the regulators are pissed off. Cheers, Wol Regulatory solution Posted Jun 28, 2024 17:26 UTC (Fri) by khim (subscriber, #9252) [Link] > If Apple is perceived as gaming the rules, then the rules will change. In a lot less than 10 years. How do you know? Apple is gaming the rules the whole existence of Apple. It's, essentially, what makes Apple Apple. Gaming the system is in Apple's DNA. Remember how Jobs swindled Wozniak before Apple even existed and paid $375 instead of $5000? That's how Apple operated it's whole life. > And in a way that is likely to be - shall we say - painful ... depending on how seriously the regulators are pissed off. I've heard these talks for decades, sorry. Nothing happens in a lot less than 10 years in European courts and Apple knows that. And now, almost half-century after it was founded, you say that it would be punished for something it successfully did for half-century? Count me unimpressed. Sure, it would be punished, but so what? That's part of the business-plan and it works. Just as one example: how long did it took for EU to make it use normal USB connector for it's phone? Regulatory solution Posted Jun 28, 2024 17:34 UTC (Fri) by zdzichu (subscriber, #17118) [Link] What about this fantasy of EU collapse? Someone has been feeding of russian propaganda? Time to stop Posted Jun 28, 2024 18:39 UTC (Fri) by jzb (editor, #7867) [Link] Speculating on the collapse of the EU is well beyond the topic and not useful. It's not going to lead anywhere good, so let's stop here. Good temporary solution Posted Jun 28, 2024 6:10 UTC (Fri) by sunshinerag (subscriber, #172199) [Link] (2 responses) I don't see anything nefarious in Apple's rejection here. The motive is simply to prevent apps launching other apps which can be abused by apps. Apple periodically adds constraints like these to App Store submissions based on app behaviour and user frustrations. The gates are there for a reason. It is a little crude to check for a string in the binary to enforce this but the alternative would be to test the app behaviour in all possible permutations to see if it does something like that. Also itms-services is a very apple specific scheme and as the discussion indicates it's a good question of why it is hardcoded in a generic library. The current solution looks temporary which is fine, the long term option would be to make the schemes available configurable which is also discussed. Good temporary solution Posted Jun 28, 2024 9:48 UTC (Fri) by taladar (subscriber, #68407) [Link] This is a typical security theater check. All the options such as obfuscation and configuration exist for the nefarious apps as well so checking for the string does nothing useful. Good temporary solution Posted Jun 28, 2024 15:17 UTC (Fri) by smurf (subscriber, #17840) [Link] Sorry but that's nonsense. If Apple wants to block apps from opening \"itms-services:\" URLs then they should teach the OS to not open them in the first place. Parsing an app for the string doesn't help. Malicious apps will just obscure the string while people who legitimately ship an URL parser (in Python, itms-services is in the \"uses_netloc\" list so that the parser returns the result the caller expects) need to add brain-dead patches or other workarounds. Copyright © 2024, Eklektix, Inc. Comments and public postings are copyrighted by their creators. Linux is a registered trademark of Linus Torvalds",
    "commentLink": "https://news.ycombinator.com/item?id=40815130",
    "commentBody": "Python grapples with Apple App Store rejections (lwn.net)238 points by leephillips 21 hours agohidepastfavorite96 comments heavyset_go 21 hours agoIt's not just Apple that pulls shenanigans like this. Try building a Python app with PyInstaller while you have Windows Defender live scanning on, which is the default setting. You won't even be able to compile a binary without Defender preventing you from doing so. Similarly, try running the binary produced by PyInstaller with Windows Defender on. Defender will say it's malicious and won't run it. It's a bit dystopian that both major OS platforms go out of their way to prevent you from distributing and running your Python apps. reply josephcsible 21 hours agoparentIt's not just Python apps. It's anything by small-time developers without expensive certificates. I once used MSVC to compile a C program that was little more than a \"Hello, World\", and Defender called it the Win32/Wacatac Trojan. reply yashg 17 hours agorootparentThe code signing certificates that MS requires are ridiculously expensive. It's a cartel of certificate isseuers. It's a downright robbery. We need something similar to Let's Encrypt for code signing. reply justinclift 15 hours agorootparentSignPath gives free certs to OSS projects: https://signpath.org/about/ We (sqlitebrowser.org) have recently started using them for signing our Windows builds. reply ronsor 20 hours agorootparentprevDefender calls anything Wacatac. Ironically I've seen tons of actual malware that doesn't even give the slightest warning. reply autoexec 19 hours agorootparentEven when legit malware gets flagged as \"Wacatac\" some percentage of users are sure to google the name, see that for years (if not decades) MS has wrongly flagged a ton of legitimate software as being that virus and then whitelist the actual malware on their machine assuming that Microsoft must have just screwed up again. I'm not surprised that MS hasn't fixed the problem after all this time, just disappointed. reply millzlane 6 hours agorootparentThis is how malware propagates. Most apps you get from questionable places have instructions that say disable your antivirus. I get it....but I don't want to play a game that bad. reply lagniappe 20 hours agorootparentprevI made a single for loop in Go at work to show a coworker, that binary got flagged as malware. reply vsuperpower2020 14 hours agorootparentprevThis is usually caused by their machine learning virus scanner. For some reason it determines basically anything that you compile yourself to be a virus. Can't miss a virus if you call everything a virus, I guess. reply 01HNNWZ0MV43FF 13 hours agorootparentTbf the gamedev community has seen people submit Trojans to game jams reply water-your-self 4 hours agorootparentThis is not relevant to the topic. reply 01HNNWZ0MV43FF 3 hours agorootparentNo, it is. Running unsigned exes with no sandbox actually is risky, and game jams are a specific case where it comes up. reply vsuperpower2020 12 hours agorootparentprevOkay? reply 01HNNWZ0MV43FF 3 hours agorootparentOkay :) reply heavyset_go 20 hours agorootparentprev> It's not just Python apps. It's anything by small-time developers without expensive certificates. This is definitely the case and has been my experience, as well. We live in some dark times when it comes to building and sharing anything as small developers, especially if the things you're building are free. I stopped updating my open-source Mac apps because I can't justify the cost of jumping over artificial hurdles Apple puts in place that ensure users can't run the apps they want to use. I have other hobbies where spending money actually gives me tangible goods and benefits versus paying an arbitrary yearly tax for the privilege to build stuff that ultimately benefits Apple. reply OsrsNeedsf2P 20 hours agorootparentThis was the case for our open source app as well. The only reason we're on Apple is one of our users likes the app so much, they handle the certificates. Which is deeply ironic, if you think about it. reply Brian_K_White 20 hours agorootparentIt is super interesting that someone who is not you can take care of proving to Apple that you are really you so that Apple can assert to all other users that they have verified that you are really you because they made you prove it. This world is just awesome. :) reply dlachausse 20 hours agorootparentprevI think Homebrew is the best solution for shipping open source Mac apps if you don't want to pay the developer fee or jump through any hurdles, assuming your users are technical enough to use it. The alternative is not signing your binaries and explaining to users that they can run them by right clicking and selecting \"Open\" from the menu. reply heavyset_go 19 hours agorootparentUnfortunately, getting users to install Homebrew is a hurdle that's hard to pass for what I'm dealing with. It's a non-starter if users have to open a terminal to install anything, even though Homebrew has a .pkg installer now. The users typically don't know what a terminal even is. > The alternative is not signing your binaries and explaining to users that they can run them by right clicking and selecting \"Open\" from the menu. That's what I'm doing now, and it's still an issue, unfortunately. Non-power users are not going to remember the right-click -> Open ritual when they just double-click on everything else. And the warnings Gatekeeper shows also caused users to think their apps, and even computers, were broken or hacked. reply philistine 18 hours agorootparentWhy aren't you just ... paying the 100$ a year to sign your app? reply heavyset_go 17 hours agorootparent> I stopped updating my open-source Mac apps because I can't justify the cost of jumping over artificial hurdles Apple puts in place that ensure users can't run the apps they want to use. I have other hobbies where spending money actually gives me tangible goods and benefits versus paying an arbitrary yearly tax for the privilege to build stuff that ultimately benefits Apple. reply exe34 20 hours agorootparentprev> stopped updating my open-source Mac apps because I can't justify the cost of jumping over artificial hurdles Apple puts in place that ensure users can't run the apps they want to use. hah, that's the exact reason I stopped using os x and went full Linux on my old Mac book air about 8 years ago. reply nox101 18 hours agorootparentprevThis is not my experience but maybe I'm doing something different. I ship an electron app. I build it into an installer with electron-builder. I'm not sure I set any configuration settings. It's set to install in the user's folders, not at the system level. My understanding is that's allowed and just works. reply helsinkiandrew 16 hours agoparentprevTo be fair to Windows Defender, a PyInstaller binary does look like malware. If it didn’t do this out of the box, it would become a standard way for malware to be distributed very quickly. Unfortunately for every person trying compile or run a valid python program from the net on a Windows PC, there’s a 1000 malware/Trojan instances trying to infect one. Ideally code signing / validation would be cheaper https://www.reddit.com/r/learnpython/comments/e99bhe/comment... reply FrostKiwi 11 hours agoparentprev> that both major OS platforms go out of their way to prevent you from distributing and running your Python apps This goes beyond just Python apps. The signing procedures to get past Windows SmartScreen require you to buy certificates and or deeply integrate with the golden path of Windows development. Stray from the Microsoft approved path that and all your users will get a nice scary warning. Part of it is justified by security concerns, but there is a bitter taste of major OSs pushing \"the\" way to develop. reply patrick451 15 hours agoparentprevThis is reason enough not to use either windows or macos. Calling these programs malicious is an outright lie. They have not proven this. At most, they can say the program is untrusted or unverified. But calling it malicious is a falsehood and therefore a breach of trust. reply modeless 16 hours agoparentprevI built PyInstaller binaries on Windows a few months ago and had no trouble with Windows Defender at all. reply _blk 12 hours agorootparentYup, it works mostly but not regularly. Did it for a few years for a cross platform proprietary PyQt app with Pyinstaller. Signing helped a lot but the release process was still to submit the binary to a website that checks with most known antivirus software just to be more certain we didn't ship a dud for most windows users. Interestingly, sometimes a rebuild fixed the issue facepalm.. reply shrimp_emoji 19 hours agoparentprevWindows isn't a platform for developers. It's a platform for normie consoomers. Isn't that obvious? If you want an engineering OS, use GNU/Linux. reply heavyset_go 19 hours agorootparentUntil you can run a Windows-free build system with WINE (there are a few reported blockers, several others and I have tried) and PyInstaller, cross-platform apps will require developers to compile their Windows ports on Windows itself. Windows is where the users are. Not targeting it is a bad financial decision. reply _niki_s_ 8 hours agorootparentPast 10 years we ship py2exe based Win32/Win64 software from WINE / Ubuntu There are no problems - we don't sign it (https://www.vintech.bg) reply heavyset_go 36 minutes agorootparentThanks for the info, I was hesitant to introduce more platform-specific tools hence why I stuck with PyInstaller. I'll see if py2exe suits my needs. reply patrick451 15 hours agorootparentprevTo hell with cross platform. Developing for windows perpetuates a harmful ecosystem. It's really no different than selling ammo to the Sineloa drug cartel. reply pfraze 15 hours agorootparentIt’s a little bit different. reply stemlord 19 hours agorootparentprevYes that's easy advice to follow when you don't have a job reply shrimp_emoji 2 hours agorootparentWhat do you mean? At my job, we build cross-platform software, but we build it on Linux. It runs on Windows, but building it on Windows is torture, so almost all our dev machines are Linux. reply 01HNNWZ0MV43FF 13 hours agorootparentprevI hope one day I won't have a job! reply esalman 13 hours agorootparentprevI've met uncomfortably high number of Mac users who cannot navigate the file system on their Mac. reply rdedev 19 hours agorootparentprevThe multi billion dollars company that I work for don't get this. I'm forced to do all me dev work on a virtual windows machine. They have their reasons, many of them valid, but it's still a pain reply r0ks0n 13 hours agorootparentprevdid you download your personality from /g/ reply edflsafoiewq 19 hours agoprevI thought this was interesting > Alex Gaynor suggested that the project try a an approach that Keith-Magee had not put forward inspired by Gaynor's experience with the cryptography library. The project often receives complaints that the library refuses to parse a certificate that is technically invalid, but was in wide use. He said that the policy was to accept pull requests that work around those issues \"\"provided they are small, localized, and generally aren't too awful\"\". But, he added, these patches should only be accepted on the condition that someone complains to the third party (in this case Apple), and extracts some kind of commitment that they would do something about it. He suggested that the workaround be time-limited, to give users a decent experience \"\"while also not letting large firms simply externalize their bizarre issues onto OSS projects\"\". as a solution to the familiar problem of users wanting OSS to work around bugs in commercial software because OSS maintainers are easier to bully and they know bug reports to Megacorp go straight to a black hole. reply dnawlp 11 hours agoparentSeems like the quote is a non-statement given that Apple's bug report system is a black hole. Big talk as usual from Pythonistas without any connection to reality. reply amelius 20 hours agoprevCan we have Separation of Powers on our digital platforms? It is pretty shitty that the one who sells phones also determines what goes on them. reply denkmoon 17 hours agoparentI choose Apple to be the custodian of apps that go on my phone. There is a platform where the user is the one that determines what goes on their phone, it's called Android. reply areoform 17 hours agorootparentWhile I am glad that you have the choice to be a digital serf - paying your lord for using land (devices), I would much rather prefer the choice to not be one. That way, you can opt into digital serfdom and people like me can opt out. - Serf here isn’t pejorative, but descriptive. Apple is behaving like a feudal lord. Down to the claims of protecting the land and arbitrating disputes. reply denkmoon 14 hours agorootparentWhy do you act like Android doesn't exist? You _can_ opt out of \"digital serfdom\". reply mirsadm 13 hours agorootparentHow is Android any different? reply pinusc 5 hours agorootparentIn some cases one can install a de-googlified version of android, and install whatever app (.apk) they wish through alternative app stores or directly. For 99.9% of people, Android and iOS will be virtually identical in terms of freedom, as they will just install apps from the play store and use google-services on the manufacturer-provided (and -bloated) android install that comes with their phone. That being said, for those who do care, the ability to take control of your phone and run AOSP, an actually FOSS distribution, and only run FOSS apps, or install whatever app you want, is unparalleled on Android vs iOS. reply beretguy 11 hours agorootparentprevYour line of thinking has so much wrong with it that it would be a waste of time trying to argue with you because you cannot be helped. reply bastawhiz 15 hours agorootparentprevSure, so why should everyone else with an iOS device have to make the same choice? reply denkmoon 15 hours agorootparentThose with an iOS device can make the exact same choice - to get an Android device. If I don't want Apple to be my custodian, that's the choice I have. I don't understand why people feel the need to force Apple into a specific strategy when they are not a monopoly. They aren't the only game in town, you don't _need_ an iPhone. Every single person who has an iPhone has chosen to have that device. Perhaps the reason people don't like the idea that if you want choice you choose the platform that gives you choice, is because Android is a steaming pile of garbage. reply lomase 12 hours agorootparentIt is sad when even here people dont care about the right to run whatever code you want in the hardware you own. reply Nab443 4 hours agorootparentYou're mistaken, you don't actually own it. reply ar_lan 2 hours agorootparentThis is not true - you do own your phone if you pay for it outright. Apple cannot take away your phone. As far as the software - Apple owns iOS and licenses it to you for your device. reply Slyfox33 9 hours agorootparentprevApple, the trillion dollar company, doesn't need you to defend their shit business practices for them. reply amelius 11 hours agorootparentprev> I don't understand why people feel the need to force Apple into a specific strategy when they are not a monopoly. According to your argument, they __are__ a monopoly. I.e. one on digital serfdom. Because apparently Android is in a different field. reply leptons 14 hours agorootparentprev>Every single person who has an iPhone has chosen to have that device. I didn't want an iphone. I was forced to get one, because Apple won't allow any web browser on their platform except their web browser. So I can't deliver a working website for iphone users without having a real iphone to develop on. This is an absolutely abusive, anti-competitive and shitty artificial limitation Apple has forced on the world. I'd really rather just tell my users to install Chrome or Firefox and use that instead of being forced to use Safari. This isn't good for developers, and it's not good for their customers whether they know it or not. reply Daishiman 14 hours agorootparentprevThey're not a monopoly; they're a duopoly. It is a marginally better position but I have no idea how you justify to yourself handing over a consumer's power to the largest tech company in the world. reply snapcaster 7 hours agorootparentBecause it's a set of tradeoffs and not a strict better/worse? reply Daishiman 2 hours agorootparentApple is perfectly capable of having a phone that runs your own software with minimal trade offs, they just don’t want to because they’re not about empowering users but about creating a class of dependent consumers. reply TillE 21 hours agoprevObfuscation seems like a great way to get your developer account suspended. I suspect Apple is doing a lot more than just basic static analysis of the binary on disk. Glad they went with a config option instead. reply kemayo 19 hours agoparentDepends. The actual rule being \"violated\" isn't that the app can't contain the string \"itms-services\". Rather it's: Guideline 2.5.2 - Performance - Software Requirements The app installed or launched executable code. Specifically, the app uses the itms-services URL scheme to install an app. i.e. the app can't try to trigger an install of another App Store app. The app in question isn't doing that, it's just that the basic check is incompetent for the rule it's supposed to be checking and the reviewer isn't doing any manual checking after that to see if it was a false-positive. So obfuscating the string, if you're genuinely not trying to install other apps, should leave your app just as non-violating as it was before... just not tripping the badly written check. Apple can of course be arbitrary and capricious after that point. reply veeti 20 hours agoparentprevThey are opaquely rejecting apps for just literally containing the string \"itms-services\" in the binary and you still give them credit for a more sophisticated analysis? Lol. reply doctor_eval 20 hours agorootparentYou’re assuming that’s all they are doing, and that it’s all they will ever do, but neither assumption is supported by any evidence. Apple is saying what test broke, not that other tests aren’t running. reply nomel 17 hours agorootparentNo. See the above article for more information, specifically the discussion linked [1]. > Some light obfuscation of the magic string appears to avoid the issue. It is a simple string match causing the failure. [1] https://discuss.python.org/t/handling-incompatibilities-with... reply doctor_eval 14 hours agorootparentFair enough, though I was really just saying that simply because they are doing something simple doesn't mean that don't (or, more importantly, won't) do something complex. In the LWN article discussion it's mentioned that Apple doesn't like obfuscation, presumably this means they can detect some forms of it. Put another way: if it was my app, and this string wasn't important to me, I wouldn't want it obfuscated, I'd want it removed. reply CivBase 19 hours agorootparentprevIf they did are doing more, why are the apps getting rejected? reply nozzlegear 20 hours agorootparentprevWe can assume that a simple string search is one of the basic checks they do before moving onto more advanced checks. reply coldtea 19 hours agorootparentEvery story I've seen over the years about MAS/iOS AS rejections point to their checks not being advanced at all. reply malfist 20 hours agorootparentprevWhy can we assume that? reply zarathustreal 20 hours agorootparentBecause it’s less effort to implement? reply gjsman-1000 21 hours agoprevWhy can’t Apple just add “itms-services” as a forbidden URL scheme on a sandbox level? I don’t see why the App Sandbox can’t block (and isn’t already blocking) certain protocols. Heck, what if I have a malicious web frame inside my app that tries to invoke “itms-services”, similar to this Polyfill.io debacle? reply praseodym 21 hours agoparentI’m not sure what the big deal with the url handler is, but I can’t imagine it causing remote code execution or other actual malicious behaviour. At this point Apple seems to be using simple substring matches, so if there is any exploit vector the malware authors can circumvent the check using \"itms\" + \"-services\" or something more sophisticated like ROT13. reply gjsman-1000 21 hours agorootparentWhich is also just why… the App Store review process claiming this is a problem doesn’t seem to make any sense. Imagine your app embeds a WebView at myapp.com/terms. It’s your Terms of Service, you show it to everyone when they sign up. Everyone clicks OK. After it’s on the App Store, you modify your WebView to include `itms-services` for some reason. You’ve just completely bypassed App Store review and gotten that URL handler into your app. The sandbox should stop you - but clearly the review processes don’t consider this possibility and are enforcing it before you publish. Why? My point is that the scanning for this handler, if Apple doesn’t want to allow it, seems misplaced if they wanted the ban to actually be effective. reply threeseed 20 hours agorootparentThe review process is about Apple giving you a clear direction about what is acceptable. There are many ways to get around their restrictions. But doing so will get you banned. reply thfuran 20 hours agorootparentI haven't heard anything to suggest that Apple intends the review process to give clear direction. reply RobotToaster 20 hours agorootparentprevThey refused to tell him why it was rejected, hardly \"clear direction\" reply threeseed 20 hours agorootparent> The app installed or launched executable code. Specifically, the app uses the itms-services URL scheme to install an app. Seems like pretty clear direction to me: https://github.com/python/cpython/issues/120522 reply josephcsible 19 hours agorootparentThe app in question doesn't actually do any such thing, though. reply threeseed 19 hours agorootparentThe wording could be better. But it has been known for over a decade now that Apple searches binaries for strings. They've never done runtime execution checks which would pick up you actually making such an HTTP call. reply Ukv 7 hours agorootparentMy baseline would be similar to that which you get from modern compilers and automated tests. Something like: Lib/urllib/parse.py contains disallowed string \"itms-services\" at line 62 column 25 Reason: Apps may not install or launch executable code, such as through the itms-services URI schema. To get credit for \"giving you a clear direction\", I'd want them to make available and suggest a fix. In general that could be \"if this is a false positive, click to request a human reviewer make an exemption\" but ideally monitoring for this kind of issue in the first place (sudden increase in identical rejections) and fixing the broken check. Instead, Apple seem to omit the information on the first line that they likely already get from their internal tool, and not only don't suggest a fix but make the proper fix so unavailable that it's easier to get the language itself changed than a check in their review framework. > But it has been known for over a decade now that Apple searches binaries for strings. They've never done runtime execution checks which would pick up you actually making such an HTTP call. It's true that someone with folk knowledge about the way Apple does checks, gleaned about the process by other frustrated users, could likely infer the way in which Apple's test is broken and so eventually deduce the first line from the second line. That's not Apple giving a clear direction - that's developers managing to work around an inscrutable system. reply ClassyJacket 20 hours agorootparentprevIt does the exact opposite of clear direction. It's an arbitrary black box reply lilyball 21 hours agoparentprevThe URL is likely used from within Apple frameworks for various purposes, and therefore it's possible for an app process to open the URL even without the app itself knowing about the URL. reply PaulHoule 21 hours agoparentprevPython just has to rot13 the scheme and hash the encrypted schemes. reply gjsman-1000 21 hours agorootparentThat came up in the original post with whether an obfuscation could be an acceptable workaround; but it came up that Apple really doesn’t like obfuscation techniques. The workaround for now is a compiler flag that excludes problematic code from iOS builds. I’m still asking though why, if Apple doesn’t like apps which use this protocol, the sandbox is not intervening; as surely an approved app could have a malicious web view. reply favorited 21 hours agorootparentSandboxed apps are allowed to use itms-services:// links, it's just not allowed in the App Store - iOS enterprise apps using in-house deployments can use it for installs and updates, and sandboxed Mac apps deployed outside of the App Store can use it as well. However, App Review Guidelines forbid App Store apps from installing other apps, so that scheme gets scanned during review. reply bmarswalker 19 hours agorootparentThis can be handled by granting privileges to open that scheme to enterprise Apps and not granting to regular App Store apps. Relying on string scanning is simply not secure. reply AshamedCaptain 19 hours agorootparentWelcome to Apple's much applauded security model. More seriously, I'm sure they also prevent the privilege to that URI scheme. This is likely part of some ill-thought defense-in-depth approach. Same way they search for the names of private symbols in the exec, even when the linker will outright refuse to give you those. I absolutely detest this pervasiveness of useless layers of security that add almost nothing. But since almost nothing is not nothing, no one can remove any of them. Like cockroach papers, I'm going to call them \"cockroach security\". Practically everything is infested with those these days. reply gjsman-1000 21 hours agorootparentprevThat makes some sense, but then I have a new question: Why doesn’t Apple have different certificate schemes for in-house versus App Store (if they don’t already)? In which case, the iOS Sandbox should be smart enough, and probably does already delineate, allowed functionality based on whether an app comes via App Store or via a private deployment. reply wodenokoto 9 hours agoprevWhy is the “ itms-services URL scheme” in base Python to begin with? Why does Python have code for interacting with iTunes out of the box? reply lifthrasiir 9 hours agoparentApparently it was another hack to prevent incorrect parsing of such URLs on the server side, without the actual interaction happening [1]. [1] https://github.com/python/cpython/issues/104139 reply polski-g 4 hours agorootparentThis is from a fucking test case to detect hyphens in the URL scheme? Just change it to asdf-zxcv. Or remove test cases from end-user python installs. What are we doing here? reply gorgoiler 15 hours agoprevThe offending string is only there because Python’s urllib has a hard-coded list of schemes which use a hostname component or “netloc”. It’s fine for that list to contain known schemes from RFCs. Anything else — including proprietary third party schemes — should just use a heuristic. The list is called uses_netloc and is used to help parse the user@host:port part of https, ftp, etc. domains. It’s this list of schemes that includes the forbidden string itms-services, used for Apple’s proprietary iTunes software. The only code that needs this is urlunsplit and urljoin. If your parsed URL has a netloc then the list isn’t even relevant — if you have a netloc then you are assumed to be in uses_netloc. This all seems like a much more sensible approach than trying to selectively include or exclude naughty strings from the source code, per some corporation’s passive aggressive demands. reply lilyball 21 hours agoprev [–] Why does urllib have this URL scheme anyway? If Python libraries are hard-coding knowledge about Apple proprietary stuff, then it should be no surprise that Apple may take issue with that. reply 7A2A-4AC 19 hours agoparentI believe it was added as the `itms-services://?` URL format is non-standard and broke the existing logic in `urllib.parse`: https://github.com/python/cpython/pull/104312 reply burnte 19 hours agoparentprev [–] The article is pretty clear that it has this string because it's the Apple recommended method for launching a new app, and Python on MacOS does that. It's part of a standard library. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "An upgrade from Python 3.11 to 3.12 caused some Python apps to be rejected by Apple's app stores due to the presence of the string \"itms-services\" in the urllib parser, which is prohibited in sandboxed apps on macOS.",
      "Eric Froemling's bug report led to a discussion among Python developers about how to accommodate app store review processes, considering solutions like obfuscation and distribution-level patching.",
      "A consensus was reached to add a build-time option called \"--with-app-store-compliance\" for Python 3.13, which would remove problematic code for app store submissions, highlighting the challenges free-software projects face with opaque review processes."
    ],
    "commentSummary": [
      "Python apps are being rejected from the Apple App Store due to the \"itms-services\" URL scheme in their binaries, and Windows Defender also flags these apps when built with PyInstaller.",
      "Developers argue that major OS platforms are hindering small developers by requiring costly code-signing certificates, making app distribution challenging.",
      "Alternatives suggested include using Homebrew for Mac apps or free certificates from SignPath for Windows builds, highlighting the broader issue of security-related rejections in app distribution."
    ],
    "points": 238,
    "commentCount": 96,
    "retryCount": 0,
    "time": 1719522551
  },
  {
    "id": 40818422,
    "title": "Frame.work laptop now available in Denmark, Finland, and Sweden",
    "originLink": "https://community.frame.work/t/now-available-in-denmark-finland-and-sweden/53690",
    "originBody": "DestroyaFramework Team 1d Our products are now available for shipment to Denmark, Finland, and Sweden! That includes the in-stock Framework Laptop 13 and Framework Laptop 16, pre-orders for the new Framework Laptop 13 with Intel Core Ultra Series 1 processors, as well as the modules in the Framework Marketplace. With this launch, we’re adding Danish and Swedish/Finnish keyboards that you can order now with a Framework Laptop 13 or as a standalone Keyboard or Input Cover Kit. For Framework Laptop 16, these keyboards will be available in late July this year. You can sign up for an email alert on the product page and we’ll notify you when they are in stock. 14 created 1d last reply 5h 25 replies 7.3k views 12 users 31 likes 7 links 8 3 3",
    "commentLink": "https://news.ycombinator.com/item?id=40818422",
    "commentBody": "Frame.work laptop now available in Denmark, Finland, and Sweden (frame.work)205 points by theshrike79 11 hours agohidepastfavorite177 comments Svip 10 hours agoFinally, someone offering a proper Danish keyboard. Denmark, Sweden and Norway have long been stymied with the low-effort that is the Nordic keyboard layout. The problem is if you are not familiar with keyboards, then the labels are supposed to help you, but they don't on the Nordic layouts, because it just show three different keymap's labels on some keys, but you don't know which belong to which. I know Apple also offers a proper Danish keyboard, and I've seen some modern Thinkpads with proper Danish keyboards. I normally use a US Intl keyboard, but whenever I see a Danish keyboard, I struggle to find thekey. reply tokai 8 hours agoparentNordic keyboards were a mistake. Should be redone. They are bad for both writing and coding. Changed to US International years ago too, and will never go back. reply 3836293648 38 minutes agorootparentåäö are fine. But I had to create a custom layout a few years back for the insanity that is {}[]`~$. And why do we have μ? (I know it's because it's a metric prefix but has anyone ever used it instead of u?) reply odiroot 6 hours agorootparentprevI'm really happy Poland went with the US layout (with right Alt \"abuse\"). Makes it very easy when buying computer equipment. reply KptMarchewa 6 hours agorootparentUnless you buy out of shelf Mac. The default is the weird \"international\" layout with tall Enter key. reply dcminter 8 hours agorootparentprevCoding in Swedish keyboard mode in a Mac with (){}[] all on the same two keys with various vulcan neck pinching modifiers is definitely not one of my favourite things! reply ErikBjare 7 hours agorootparentI've sticked to US Intl + AltGr as modifier key to access åäö in the usual location for ~a decade now. Managed to get the same config working on both Linux and macOS. reply Nullabillity 7 hours agorootparentprevThat's an Apple thing, though. reply dcminter 7 hours agorootparentOh, I'd assumed this was specific to Swedish Mac keyboards and not a more general Mac thing! How vile! reply rsynnott 7 hours agorootparentIt's def. not like that on their US or UK/IE keyboards. reply mtlmtlmtlmtl 7 hours agorootparentprevNot much better on standard Norwegian(non-apple) keyboards. ()[] are on the same two number keys and {} are on the keys surrounding those keys. []{} require alt gr while () require shift. Also $ is alt gr+5 which is revolting. Combined with / being at shift+7, typing in the shell is a terrible experience. reply CRConrad 4 hours agorootparentI think those are all the same on Swedish PC keyboards too, except $ -- yup, that was AltGr-4, not 5. And yeah, not lovely... Why have I put up with this for over 30 years?!? reply dcminter 7 hours agorootparentprevYeah, I have a Swedish layout Dell as well and it's better but not much. I'm confused now, though, as I took the parent as telling me that non-Swedish Macs also have the horrible symbol placement for the various braces. (rsynnott above clarifies that it's indeed Swedish-specific so it's just the Macs are a bit worse than the PCs). I'm fairly adept at switching between British PC keyboard, Swedish PC keyboard, and Swedish Mac keyboard nowadays, but the latter's significantly the worst (and don't get me started on how default IntelliJ key combos work with that) reply jltsiren 7 hours agorootparentprevI don't care much about keyboard layouts anymore. I use a mix of FI/UK/US normal/Mac layouts almost daily, and it rarely matters much. Maybe the layout used to be a bigger deal before smartphones, but touchscreen interfaces have taught me to expect that the layout changes randomly all the time. reply tokai 7 hours agorootparentYou definitely haven't use any of the nordic or the common nordic keyboards then. Its not the changing around that is the issue, its the absolute backwards positioning of key (heh) keys that is the problem. reply jltsiren 6 hours agorootparentI used the Finnish layout almost exclusively for 20+ years, and I didn't find it particularly inconvenient. I needed äö at least as often as ()[]{}/|\\$@, and no alternate layout was clearly better than the default. If there was a dedicated Finnish layout (instead of a shared Finnish/Swedish one), it could be a bit better by using the å key for something more common. But apart from that, I haven't seen any suggestions that would make the layout better. reply CRConrad 4 hours agorootparentI find that some keyboard shortcuts, in programs that use \"weird\" keys like, say, Alt-semicolon or Ctrl-backslash, don't work with a Nordic keyboard. I've hypothesised it's because we need to use modifiers to get to those characters in the first place; maybe the programs read keyboard scan codes directly in stead of OS-mediated character codes (or vice versa), Idunno... You never run into that? reply theshrike79 1 hour agorootparentYea, there are some programs that expect me to be able to press Ctrl-/ or something - but / is shift-7 so i'd actually have to press ctrl-shift-7, which in 99.9% of cases never works. It's easier to just remap the shitty hotkeys to something better. reply darthrupert 8 hours agorootparentprevYep, agreed. I'm using US Dvorak with modifications to get those äös. reply Nullabillity 8 hours agoparentprevYou try it once, and you see which colour applies to your language. It's not really a big deal. (And even that's only required to disambiguate between danish and norwegian.) As someone who grew up having to write both on a regular basis, it was neat having both on the same layout. Now that I use Linux it doesn't matter as much since altgr lets me type æø instead of äö, but back on Windows you'd have to switch layout for that. It's too bad they didn't at least ship nordic as a separate option. reply 3836293648 36 minutes agorootparentColour? They're all white reply Sammi 7 hours agorootparentprevI'm looking down at my nordic layout keyboard I've owned to ten years. I have no idea which color is which language. reply CRConrad 4 hours agorootparentMine are all the same colour, so no help there... But generally, ÄÖ is Swedish / Finnish, and ØÆ / ÆØ is Danish and Norwegian. But which of the latter two is which, and why on earth did they flip those two around between the languages??? So fricking weird... reply netsharc 7 hours agorootparentprevWhat are the colors? If I were tasked to design them, I'd take the red from the Danish flag, yellow from the Swedish one and the blue from the Finnish flag.. reply CRConrad 4 hours agorootparentYou don't need to differentiate between Swedish and Finnish, since the layouts are identical, but between Danish and Norwegian, because Ø and Æ swap places between those two. (So, blue for Norwegian?) reply Nullabillity 6 hours agorootparentprevThere isn't really a standard colour scheme AFAIK, but you can tell quickly which one you need to care about. And by the way, it's generally swedish/danish/norwegian. The finnish layout is basically the same as the swedish one. reply netsharc 6 hours agorootparentOh no, I forgot about Norway! I guess the blue would be for Norway then. reply Nullabillity 6 hours agorootparentprevThe one with Ö left is swedish, Ø left is norwegian, Æ left is danish. reply Hinrik 9 hours agoparentprevEven worse for Icelandic ones. Keychron used to show the Icelandic flag on their Nordic keyboard page, despite that layout not including Icelandic-specific characters (Ð / Þ missing, Ö in the wrong place). I asked them to start including those characters, but now they just removed the flag. For Framework I went with a blank ISO layout. reply ajnin 8 hours agoparentprevWhile we're talking about keyboards, why won't they put dedicated page up/down keys ? There are 2 empty spots right above the left/right keys that would be ideal for this and are basically just wasted, being left empty, as one can see on the photos. It's one of the reasons that makes me cling on my old Thinkpad for so long, I use them all the time. reply masklinn 8 hours agorootparent> There are 2 empty spots right above the left/right keys that would be ideal for this and are basically just wasted, being left empty, as one can see on the photos. In my opinion and experience it’s utter shite: the precision of fingers on half keys is not great, so it’s way too easy to hit the page keys when just trying to move two characters sideways, and now you need a few seconds to realise what the fuck just happened. It’s exactly what my current work laptop has, and if I had to use its internal keyboard for any length of time the laptop would long have gone out the window. If I’m paging, it’s easy enough and a lot more intentional to press the FN up/down key. I could see the argument for TKL laptops but this is garbage and I hate it. reply notpushkin 8 hours agorootparentprevMy T420's keyboard has some next/previous page keys there. I have zero idea what to use those for. The previous page key makes a nice additional modifier key though (e.g. for Compose) as it's next to the Ctrl key. reply s2l 7 hours agorootparentprevInspired from physical newspaper reading, how about page right/left keys too? Think of infinite canvas apps, games, web carousel, etc. reply numpad0 5 hours agoparentprevFrom reading comments around, is this \"Nordic\" layout just those four different languages overtyped on top of each others for users to ignore irrelevant parts? That sounds insane. reply CRConrad 4 hours agorootparentIt shouldn't really be all that bad; it's just two letters that differ -- both in actual font design and keyboard placement. Swedish and Finnish write the letters Ö and Ä; in Danish and Norwegian they're Æ and Ø. The really weird thing is that the latter swap places between the languages; in Danish the keys are Æ to the left, Ø to the right; in Norwegian it's the other way around. So we have these two keys to the right of L that are marked, respectively, ØÖÆ and ÆÄØ. Fortunately, the Å key seems to be the same for all four languages, and stay put in the row above the aforementioned ones, to the right of P. reply 3836293648 26 minutes agorootparentAnd a bunch of symbols reply wordofx 9 hours agoparentprevlol someone hasn’t seen some international keyboards. reply robxorb 9 hours agorootparentCan someone explain to my why some keyboard layouts hide some of the most important terminal punctuation in everyday use? For example, on an Apple Swedish keyboard you're not allowed to natively access your home directory ~, nor pipe anything |. What idiot came up with that, or is there legitimate reason? reply creshal 9 hours agorootparentThey're recycling typewriter layouts that optimized for what was at the time more important to the average user, making common non-latin characters more accessible. And arguably, for the average user, it's still the right choice. It's only IT nerds who really need all those weird extra characters that aren't even real punctuation, and they can just switch over to the US layout. reply exitb 8 hours agorootparentThat's true, but also not really an explanation. There is a Polish keyboard layout that's derived from a typewriter, but got abandoned very early on in PCs, even though Polish has 9 additional characters. reply philistine 4 hours agorootparentEvery single keyboard layout has its own story. Let me tell you the one about the keyboard layout I use: CAN/CSA Z243.200-92. When writing French on typewriters, people in Québec and the rest of Canada used a slightly modified US-keyboard layout that assigned the necessary accents and moved the symbols those keys used elsewhere. You could even use this layout with the US 104-keys keyboards on computers without losing much. Aside from having glyphs on your keys that didn't connect to what you were typing but people lived with it. But when we moved to computers there came an opportunity to improve the keyboard. We could now assign keys to common letters with their accents together without losing anything, since we didn't need to focus on the additive nature of accents. You know, moving from a mechanical system to pure software. So who jumped on the opportunity? The government of Québec led the charge in the early 90s for this new keyboard layout to replace the old French (Canada) layout. Feeling at some point that there was a risk of furthering divide in Canada, the federal government asked for some slight changes to the Québec proposal, in exchange for the promise of adopting it as the standard Canada-wide. Québec agreed, but Canada never did anything of value to help with adoption. So we're stuck with two standards, basically on the line of your OS: if you're on Windows, you're probably using the old French (Canada), and Mac users switched over to the new standard. Every. single. keyboard layout. has a story surrounding its creation and its usage. reply SSLy 6 hours agorootparentprevbecause Poles had to smuggle computers in, the supply was coming from USA, and then it was too late. reply kelnos 8 hours agorootparentprevNot sure I buy that explanation -- an image search suggests that a lot of US-made typewriters didn't have curly, square, or angle braces, and yet we have them on the standard US keyboard layout. reply TazeTSchnitzel 8 hours agorootparentprevApple specifically don't label these special characters on their keyboards for some reason, but they aren't inaccessible, you just need to press option/alt. If you use the keyboard layout viewer built-in to macOS, you'll discover there's actually extra letters and symbols on every key! reply robxorb 8 hours agorootparentThank you, that's most helpful! reply pavlov 8 hours agorootparentprevBecause the designers of Unix used whatever keys happened to be available on the teletype keyboards that they had in their lab, and didn’t give a second thought to international keyboard layouts. Programming languages used to be more cognizant of this. For example Pascal used mostly keywords rather than ASCII symbols, and this was entirely intentional to enable programmers in all countries to easily work in the language. Of course C inherited Unix’s approach, killed Pascal, and nobody cares anymore. Except all those millions of non-English users trying to learn programming, but nobody in Silicon Valley thinks about them. reply lokimedes 8 hours agorootparentNot to be snarky, but which popular programming languages have come out of Silicon Valley? I can think of Swift and perhaps Java (Sun)? reply pavlov 7 hours agorootparentJava, JavaScript. Those two adopted the C syntax and cemented it as the baseline for everything that followed. Still in the 1990s, Microsoft had a focus on BASIC which had a keyword-centric syntax. But Java and the web killed that. reply xxs 7 hours agorootparentprevJava did try to be C as much as possible, as language/syntax principles it's pretty much just C. reply Hikikomori 9 hours agorootparentprevNo {} either, anduses different keys. reply OJFord 9 hours agorootparentprevThey come from typewriters presumably, not designed for teletypes nevermind modern terminal emulators. reply afiori 9 hours agorootparentprevlocalized keyboards layouts are quite a bit old. reply Aissen 9 hours agoprevLearning to touch type is one of the best investments I made early in my career. It's never too late. The fact that you will no longer need to care about whatever is printed on your keyboard is only a minor bonus. reply Aachen 9 hours agoparentIt took until reading this comment that I understood why everyone's going on about the keyboard layout when it's always only ansi or iso anyway. I care that the left shift is long because then my hands can reach things like shift+4 comfortably (this php coder needs that $ a lot, my hand at an internship actually hurt from trying to use the bad layout), but beyond the physical keys, it wouldn't even occur to me that anyone who's into Framework hacker laptops cares about the print on them! Thanks for that hint reply hiAndrewQuinn 6 hours agoparentprevhttps://monkeytype.com/ is my go-to place to practice. It proved invaluable when I was first getting used to my fancy ortholinear split keyboard. reply matsemann 8 hours agoparentprevIt's not just what's printed on them, really. I touch type, but a US keyboard have different shapes. For instance a longer left shift, and then go straight to Z, while we have an extra button inbetween. After jkl we have three buttons and then a thin and tall enter, but you have two and a short enter. reply Aissen 7 hours agorootparentAgreed. You are talking about ANSI vs ISO layouts, and Framework already had both. reply spurgu 8 hours agoparentprevYeah I've had laptops with various hardware layouts and don't really care, other than I prefer a Nordic layout on Macs where the Enter key is tall rather than wide. But it's a trivial adjustment. reply phplovesong 10 hours agoprevHow good are these? They are pretty highly priced (1600-2000 USD), comparable to a mac laptop. I would expect the quality to be on mac level if i would pay this price for a laptop. reply BiteCode_dev 9 hours agoparentYou will not get mac level finish or integration, nor battery life. But you will get a lot of ram and disk for this price, a robust and fast laptop. And something you can repair. If you are a mac person, there is no substitute for the mac, by definition. If you don't care about mac, then the framework is a good machine. I rocked dell for a long time, I switched to framework, and so far, it's been great. The keyboard is comfy, the touchpad is good (for a non mac), the mate screen is productive and I enjoy its format. The specs have not let me down perf wise, the extension system is fantastic, and the value proposition of being able to be repaired is unmatched. I assembled it, and you can see they care a lot. Dual boot windows + linux on it is quick and easy. Support is perfect, including sleep, wifi, fingerprint reader, webcam, etc. The mechanical off switch for the mic and cam is a plus. The battery life sucks, and I can hear the fans more than I want. It's not as stylish as other notebooks. That's it. Mostly, I forgot about the machine in a week and it's transparent to me: it's plugged on a USB dongle that powers it, hook me to ethernet and extends the screen. I used it on the go twice, nothing special to report. It just works and I get things done. Which is probably what a lot of people want. reply dustincoates 9 hours agorootparentI'll co-sign everything you said, other than to say that if you care about the webcam or the sound, this isn't the laptop for you. I'd still recommend it highly despite those failings, though, as the configuration and the longevity is more than worth it. Plus, who knows, they might come out with a webcam or sound upgrade kit one day. reply davet91 7 hours agorootparentThe new screen option for the 13\" model has a much better webcam. reply BiteCode_dev 9 hours agorootparentprevTrue, I forgot to mention it because I almost never use them. Laptop cams and mics are never up to my standard, even on mac, and since I remote work 90% of the time, I always travel with a full mic stand and external cam. This is too important for good communication for me. But indeed, it's not on the higher end for sound and image quality. It's not the worse either. Just meh. reply mmusc 8 hours agorootparentI think I saw a video from their CEO showcasing some of the things in their pipeline, they are upgrading the webcams significantly. Best thing about a framework is you can just get the webcam and upgrade it. reply BiteCode_dev 8 hours agorootparentYep. Also, the new screen looks nice. Might upgrade in a year or two if I feel I need more brightness. The fact you can change it slowly, piece by piece, is fantastic. It spreads the cost of upgrading the machines over the years instead of having to pull the entire cash at once. No need to reinstall everything every time. You can upgrade the battery when you need to. Then maybe the RAM 3 years later once electron app with embedded AI model takes over the world and a hello world eats 4go. This is, to me, the killer feature. Young me would have hated it. I wanted a new poney regularly. reply sofixa 10 hours agoparentprev> I would expect the quality to be on mac level if i would pay this price for a laptop I'm not sure if the quality is \"on the mac level\" (I've heard conflicting things, like I have for Macs), but the repairability and upgradeability are a million times better. That means that you can buy the 8GB RAM version, and upgrade later if needed; you don't have to shell out 500euros more just to not have a useless device in a year or two. reply askonomm 10 hours agorootparentI heard that upgrading the framework laptop would end up costing about the same as just buying a new regular midrange laptop entirely, at which point I'm not really sure if I'd care about the upgradeability. Update: updating the main board (e.g CPU) would cost anywhere from 500 to 1000 EUR (https://frame.work/marketplace/mainboards). I'd much rather stay with a desktop PC where all the parts are also upgradeable, but much, much cheaper to the point that it actually makes sense. reply omnimus 9 hours agorootparentThere is also the thing about you know... things not ending up in landfill. Price/performance/quality wise nothing beats cheapest Macbook Air. But that machine is designed in a way that its 100% that in few years it will die and it cannot be repaired. By soldering SSD and RAM you are on ticking clock. Especially the 8gig ram version will swap depleting your SSD cycles. I own cheapest macbook air M1 - i think it's the peak laptop \"THE STANDARD\", its great. But all the ways it's designed to be unrepairable. It's so clear case of planned obsolescence... it should be criminalized. reply darthrupert 9 hours agorootparentApple seems to have a robust and well-defined recycling program that I haven't seen with any other manufacturer. It would be interesting if somebody made a study comparing their environmental efficiency vs repairable hardware. https://www.apple.com/recycling/docs/Recycler-Controls.pdf reply kalleboo 9 hours agorootparentprevOne nice thing about upgrading the framework is your old main board isn’t useless once removed. You can get an external case for it and use it separately as a mini PC (replacing a media PC, raspberry pi or NAS) reply michaljanocko 9 hours agorootparentprevI bought mine as part of one of the first two batches. When the 12th gen Intel came out I upgraded for about 40% of the price I'd pay for a similarly specced laptop in my country. reply vidarh 9 hours agorootparentprevI've opted for a mini PC for my desktop, and then we just bring a cheap Chromebook on the rare occasions I need/want a laptop when traveling. I like the idea of the Framework laptop, but just don't see how I can justify it. If I was working away from my desk more often, then maybe. reply sofixa 9 hours agorootparentprevWhy upgrade the mainboard? You only need this for CPU socket changes. You need to upgrade RAM/SSD/CPU much more frequently than the mainboard. reply delta_p_delta_x 9 hours agorootparentThe last Intel CPU micro-architecture to be offered with sockets for laptops was Haswell, where CPUs were offered in the Socket G3[1] package. I had one of these, Intel i7-4710MQ. Since then, every laptop Intel (and AMD, for that matter) CPU package has been BGA, and therefore soldered to motherboards. If you want to upgrade your CPU on a notebook, you must change your motherboard; no choice in the matter. [1]: https://en.wikipedia.org/wiki/Intel_Socket_G3?wprov=sfla1 reply xxs 7 hours agorootparentPretty much yeah -- all laptop CPUs since Haswell have been non-socketable/BGA. I still have a rather decent laptop with such a CPU - acer aspire v3-771g. It's still in daily use, after 8y back I had to replace a faulty mlcc (short to ground) that took a mosfet along with it. reply askonomm 9 hours agorootparentprevThe CPU is soldered to the mainboard and can't be swapped out on its own, unless I'm looking at it wrong on their website. reply BiteCode_dev 9 hours agorootparentTrue, but the prices you list are the price of the board + the CPU. On this main board ($340): https://frame.work/products/cooler-master-mainboard-case-and... The CPU is worth $250 already. You pay a $100 to get a swapable CPU on a laptop which is guaranteed to be compatible. You might value or not this feature, and that's fair. But I do value it a lot. Even on a tower, matching a board and a CPU is not a given. On a laptop, forget about it. reply N-Krause 9 hours agorootparentprevThe cpu, the heatsink and the mainboard are one part, that is correct. If you want to replace the cpu you have to place the other parts as well. Even though that is trivial. As for the costs: Yes the mainboards on frameworks website currently cost about 500-1000€, new laptops using those cpu's are tend to be a bit more expensive than just those parts, so you are saving a bit of money there. But in my opinion that is not the main selling point. The fact that you get a high quality, long lasting, well supported laptop from a company that respects it's customers and their problems coupled with the fact that it is just environmentally better is the main selling point. Yes currently you pay extra for those benefits. You have to choose if those things are worth more money than the next 3-year throwaway laptop. Btw: I am a very happy user of a Framework 16 as my gaming machine. So my opinion may be biased ;) reply BiteCode_dev 9 hours agorootparentprevYeah but you update the mainboard once every 10 years. Usually what you upgrade are: ram, disk and cpu. Comparing a tower with a laptop makes little sense. They don't have the same use cases and compromises at all. It's like saying \"I much rather stay on a framework, because all the part are upgradable, but I exchange the bigger price for portability and not having to by a battery to protect against power down\". It has little to do with the framework proposition, which is to be a laptop that happens to be easier and cheaper to repair than the competition. I tried to repair my previous laptop. I killed it because the parts are so hard to manipulate. Cost me $2000. I think anything is cheaper than that. reply brabel 8 hours agorootparentprev> you don't have to shell out 500euros more just to not have a useless device in a year or two. I have a 12yo iPad, a 8yo Macbook Air, and they're still going strong. I think after around 10 years you may not be able to update the OS anymore?? But tbh after 10 years I really want to change laptops, it's about time :) And hopefully one can install Linux on the Mac and keep using it for other things, I think... the hardware is just very good. reply sofixa 8 hours agorootparentBecause you paid enough in advance to get enough RAM and storage to last you years. If you get a modern low end 8GB / 256GB SSD MacBook, it will last you a long time if you only have a very limited use (basically light browsing). reply brabel 5 hours agorootparent> If you get a modern low end 8GB / 256GB SSD MacBook That's the one I currently use when not working. It's still very fine for browsing the web, watching videos and even coding as long as you don't mind some delays. With \"good\" light software (e.g. emacs, anything on the terminal), it's still very snappy and totally fine. reply nunobrito 10 hours agorootparentprevYes, that is really good here. You can buy new replacement covers, so basically renewing the part of the laptop that start to age due to heavy usage. The module expansion is looking good too. reply codeflo 9 hours agoparentprevI was in the market for a decent Linux laptop, but after having seen one in person, I'm less convinced. Note: I'm talking about the 13\" one. The battery life using Linux isn't good, the screen is isn't that good, the touch pad is okay, but not great, the layout of the arrow keys is ridiculous. I don't get how something can market itself as an enthusiast laptop and come with arrow keys like these. And I agree that for this price, you should expect best-in-class components. But the killer for me was seeing the modular port concept in action. It's the kind of thing that sounds great on paper, but in practice doesn't really work. It just wastes space that could be used for a larger battery, while limiting the number of ports that the laptop can fit at any one time. It's not a Linux laptop, but just for comparison, a Macbook Pro has way more ports, and all of them are usable at once. I'm sure things will improve, but basically my impression is that the Framework concept doesn't seem that well thought out yet. And the upgradability guarantee might lock them into their unfinished ideas for a lot longer than other manufacturers would be. I really hope they can solve these issues, because what they're trying to do is fantastic, but it’s not for me just yet. reply sirwitti 8 hours agorootparentIn your opinion, what's the problem with the port concept of the frame.work? reply codeflo 8 hours agorootparentTo quote myself in the post you responded to \"It just wastes space that could be used for a larger battery, while limiting the number of ports that the laptop can fit at any one time.\" The ports I personally need are HDMI, lots of USB-C ports, sometimes USB-A (but less and less), and sometimes an SD card. I know everybody has different requirements, but I'm also sure that's at least somewhat typical. And given that, I can't be the only one who thinks it's silly to reserve space for several 4x3x1 cm large USB-C-to-USB-C converters in a space-constrained device. [1] https://frame.work/de/en/products/usb-c-expansion-card?v=FRA... reply awkwardpotato 4 hours agorootparentThe replaceable ports are not interfering with the battery. The battery is under the touchpad, not up near the hinge like the ports. [1] I love my Framework 13. The battery life on it is fine, I'd estimate probably 6-8 hours and charges quickly. The touchpad is quite large which I really enjoy, I believe that's thanks to the 3:2 aspect ratio? It took a second to adjust back from a haptic touchpad to a physical one, but doesn't bother me at all anymore. The arrow keys are a none issue, the half sized up/down arrow is standard on most 13\" (and even some 16\") laptops I've seen. [1] https://d3t0tbmlie281e.cloudfront.net/igi/framework/5gFFHIX1... reply delfinom 8 hours agorootparentprevAs far as I know, those aren't wasting the space for a battery. Framework opted for their current battery sizes. Larger capacities are available for the same size of battery they do have. For example, my other favorite laptop, the Redmi Pro Books basically have the same physical size batteries but at 80wh/99wh ratings. I just think Framework can't find a contract manufacturer for higher density batteries at their volume. reply codeflo 6 hours agorootparent> As far as I know, those aren't wasting the space for a battery. Framework opted for their current battery sizes. I don’t get that logic. There’s extra space; more battery could fit in there. The fact that higher density batteries exist in no way invalidates that. (You might perhaps have a point if battery life was already excellent and the more important goal might be to save weight, but that’s not the case at all.) reply delfinom 6 hours agorootparentIn any laptop, if you have IO on both sides of the laptop, then the motherboard is at least going to extend to both sides. There is no room for batteries. Especially with high speed data links such as USB3 and USB4 these days, you need a nice impedance controlled PCB to the edge or some expensive cabling or in the case of framework, IO modules that are pcbs with a connector bridging the distance further. The opportunity is then taken to stuff the speakers in the framework in that same space. It's literally no different than a MacBook Pro or other mfgs. The only thing I could see is they need to figure out how to miniaturize their speakers to expand the battery size a bit. Or move the speakers to the main board and project out the top. Their speakers are currently larger than the competition and not really any better while stealing battery space. I would love to see them offer a larger battery with the trade-off of the user removing the modular speakers. Because personally I think using sound devices in public is just fucking rude and I'll use better quality earbuds/headphones both in public and privately anyway. reply codeflo 6 hours agorootparentIs your claim that redistributing that space would be physically impossible? reply delfinom 3 hours agorootparentEliminate or shrink the speakers and you can redistribute that space. IO? There is no space redistribution possible if you want the IO on the left and right like every other laptop in the world. The alternative would be to put the IO on the top edge of the laptop, in which case you can reclaim space. The downside is you limit the display opening angle (it can't go flat), but that's a weird obsession in laptop design that came from Apple and thinbooks. (There used to be IO on the top edge in older laptops). Unfortunately being the ulgy duckingly would be bad for Frame.works business most likely as I guarantee every reviewer would whine about it In particular Framework uses a 4 series cell battery pack configuration. Not unusual and they went for 4 equal sized cells put in series in a rectangular frame. Trying to fill awkward remaining space such as a \"U\" or \"L\" shape cavity requires a far more custom solution and Framework most likely does not have the volume to commission such an order from a custom battery pack manufacturer. reply haspok 8 hours agorootparentprevYeah, I have a HP laptop for work, with the same cursor setup - it is terrible. Thinkpad FTW! The only thing that saves it for HP is that you get an extra column of Delete/Home/PgUp/PgDn/End keys on the side. ps. Tuxedo has the same problem. No Linux-native laptop suits me :( Thinkpads it is. reply N-Krause 9 hours agorootparentprevYou're saying you're in the market for a Linux laptop and compare it to a macbook. I don't see the fair comparison to be honest. Sure there is Ashai and you could install Linux on an Intel MacBook, but for those setups you don't have company that has explicitly support for Linux. As for your other points, those are fair even though subjective. I really do like the modular ports, especially because they expose the possibility to build your own modules (or buy community made modules) for the laptop because everything is open source. reply codeflo 8 hours agorootparentI don't see why I shouldn't compare them. Besides economies of scale, there's no inherent reason why a company couldn't build a Macbook-quality laptop that runs Linux. reply lucideer 10 hours agoparentprevAs someone who went through two of the touchbar macbook pros up until recently getting a M2, \"on a mac level\" is a pretty tricky metric these days. Apple were charging crazy prices for garbage quality machines for years. A friend has a frame.work & I've trialed it a fair bit - combining his review with my hands on experiences I'd say it's not quite as nice as the new Ms nor up to the level of the old pre-2018 macbooks but it is very close, & certainly far better build quality than what Apple were churning out in those intervening years. reply jillesvangurp 10 hours agorootparentThe last few generations of intel macs weren't great indeed. The 2018 model I had had the flaky keyboard that ultimately destroyed the laptop when a loose key inserted itself in between the screen and the keyboard. Still angry about that. But I must say the M1 mac book pro (14\", 16GB) is one of the nicest laptops I've ever had. The second best would be a 2011 era mac book pr 15\" that I used for several years (pre-retina). I'd be interested in a decent linux laptop but so far everything I look at has compromises on quality, screen/resolution, trackpads, etc. These framework laptop come close. Especially now that they finally have decent screens that aren't the industry standard 1080p garbage. Not quite upto Apple's standards (colors, contrast, dynamic range, etc.). But it's nice enough. It does seems a bit bulky for my taste. But I could probably live with it. I'd use it for gaming and Steam mainly. I have a shitty old laptop I use for that currently but I hate the screen, keyboard, trackpad, and the anemic performance. reply apoptos1s 10 hours agoparentprevI bought the new framework 13 with a ryzen config (forgot which chip it was, i think i went for the 6 core one) with 16gb ram @5600Mtransfers/sec and 1tb wd black ssd with the cache thing, no OS. I also got 2 usb a, 2 usb c, and 1 hdmi ext. card. paid around 1300eur netto, and i found the build quality to be on par with my old macbook 13. Matte display is also nice, though the brightness settings are not as 'wide' as the macbook's. The fan rarely ever ramps up, the keyboard is very comfortable compared to a 1st gen butterfy keyboard, and you can set the battery charge limit in the bios settings (e.g. limit charging to 60 %) I am really happy with it. reply VadimPR 9 hours agorootparentWhat is the benefit of limiting battery charging? reply feb 9 hours agorootparentFully charging the battery can cause it to age more quickly and lose capacity. Limiting charge level helps to protect it, especially when it's always connected to a power supply and don't need the battery that much. reply xxs 5 hours agorootparentif the li-ion is keptso you don't have to pick it up from another eu country it's like saying any product is available worldwide as long as you are willing to travel (or arrange) to pick it up reply sabjut 6 hours agorootparentnot if you ask customs reply doikor 7 hours agoparentprevThe bigger thing is having Nordic keyboard layout available. Also we use ISO layout with more keys and shapes/sizes. > With this launch, we’re adding Danish and Swedish/Finnish keyboards that you can order now with a Framework Laptop 13 or as a standalone Keyboard or Input Cover Kit. For me the biggest issue is the different shape enter key causing me to miss click it all the time when using standard (ANSI?) qwerty keyboard. reply numpad0 6 hours agorootparentDoes it differ from standard ISO, or is it ISO with specific keycap prints? reply ajsnigrutin 7 hours agoparentprevBy that logic, they were available even before, because you could fly to US and buy it there. Being able to buy a laptop in finland is pretty much the same hassle for me as if i bought it in US... less custom fees, more shady remailers and a language I don't understand. Half of amazon.de won't deliver to my (not germany) EU country, half of amazon.it won't either, and sometimes there' stuff that one of them will send but not the other (stuff sold by amazon, not third party sellers), and some things are impossible to get (lipo batteries) while other companies somehow manage to ship them. EU is far from a \"unified market\" (in a way where buying stuff from other EU countries would be the same experience as buying stuff from your own). Even large retailers like amazon don't even care about the difference, since they have your shipping address, notify you that your shipping address is set towith a popup, let you search for an item, show you an item in the search results, and only when you click on the item, they tell you that they can't ship it to your address. reply edpichler 1 hour agoprevI wish they had a Dvorak keyboard. reply Muromec 10 hours agoprevI’m still waiting for Ukrainian keyboard. How hard can it be to get the mappings for all keyboards from gnome or somewhere and apply whatever unicode symbol during the engraving process ? reply dgan 10 hours agoparentYou can always buy a blank keyboard and add stickers yourself I am thinking about leaving it blank tho, looks way cooler hehe reply Svip 10 hours agorootparentI've tried that before, and I've not been able to find stickers that will last long (trust me, you'll have sweaty fingers from time to time). A better solution is to use a white permanent marker on the keys instead. It doesn't last, obviously, but it doesn't make it awkward to type, and you can just re-apply it as the keys lose their labels. reply simcop2387 3 hours agorootparentIt still won't be permanent, but water-slide decals will probably work better than stickers, and about as well as the marker while being more legible. It might also be possible to put a clear coat paint on it that'll help them last longer. The big advantage over stickers is that they aren't held on by glue, but instead by the plastic that they're made out of conforms to the surface and grips it instead. That said I'm not sure how to make white text for a black keyboard. reply nunobrito 10 hours agorootparentprevHave the same issue with portuguese keyboards while working as expat. Only other option was memorizing the keys. :-( reply vidarh 9 hours agorootparentprevOn my keyboard (not a Frame.work), the only letters that are still readable are Q, Y, U, J, Z anyway. The rest are worn down, some with nail striations in the plastic... I'm not sure how cool it looks, but clearly I manage without the labels. reply lawn 10 hours agorootparentprevI don't really understand the complaint about labels, it's not that hard to learn the keys. reply dgan 10 hours agorootparentI am with you: i am using a custom French layout, so the labels don't match anyway reply WesolyKubeczek 9 hours agorootparentprevAre there any stickers that don't fuck with backlight? reply yonatan8070 7 hours agoparentprevSpeaking of engraving, I have access to a 150W co2 laser cutter, is it possible to engrave with it on a blank keyboard? reply michaelmior 1 hour agorootparentThey do sell blank ANSI and ISO keyboards and it seems like others have been able to successfully engrave them. https://community.frame.work/t/blank-keyboard-laser-etching/... reply ThePowerOfFuet 1 hour agorootparentprevAt 150W you'd send the keycaps into the next dimension. reply yonatan8070 23 minutes agorootparentOf course I wouldn't be engraving plastic at full power. If I do end up getting a Framework I'd get an extra blank keyboard to dial in the settings reply WesolyKubeczek 8 hours agoparentprevIronic that Apple can serve you just such a keyboard. Proper Ukrainian, not \"well but actually it's Russian\". Note it's their Magic keyboards, not laptops. reply michaelmior 7 hours agoprevNote while the domain is frame.work, the brand name does not have a . and is just Framework. reply matsemann 10 hours agoprevA Swedish keyboard layout works for Norwegian, as long as the Ö maps to Ø and Ä maps to Æ. Or the Danish as well, except there for some reason Æ must map to Ø and Ø must map to Æ, otherwise they're identical. So most keyboards sold in Scandinavia just has one button with ØÖÆ on it and another with ÆÄØ on it, and then based on OS keyboard setting you get the correct one. Point is, I'm curious why they also just don't release in Norway when they have made the layout? But I guess it's because we're not in the EU that makes it harder? reply mtlmtlmtlmtl 10 hours agoparentPersonally I just use a US layout with alt gr for æøå. The Norwegian layout is very awkward for C style languages(and the unix shell), I find. The US one feels more natural, which makes sense considering the modern US layout is very similar to the layout of the teletypes used at bell labs in the 70s. Took a while to get used to it, but only a month or so to be just as fast as I was previously on the Norwegian one. reply moogly 1 hour agorootparentThis right here is the best solution. Been using that for maybe 15 years. reply kawsper 8 hours agorootparentprevI use US layout instead of danish as well, it's much better for development, and æøå is easily available on Mac: æ = option + ' ø = option + o å = option + a reply encom 8 hours agorootparentprevThe danish layout is brain damaged indeed. My greatest daily annoyance is shift+7 to get \"/\", when navigating around folders in the terminal, but on the other hand I have ~30 years of built up muscle memory at this point. reply matsemann 8 hours agorootparentprevMy reason for learning the US layout back in the days was because of Counter Strike. For some reason it didn't adhere to your OS keyboard settings when chatting, so trying to write ø always became ; etc, so we kinda after a while just learned how it worked (didn't even know it was a US layout thing to look up, just bruteforced and memorized). reply wingerlang 9 hours agorootparentprevAgreed, takes honestly a very short time to get used to it but the benefits are large, as a programmer at least. I don't think I would ever go back to the other layout again. reply vidarh 9 hours agorootparentprevYeah. Norwegian living in the UK, and I use a US layout as well. I do have a keybinding to swap to Norwegian keymap that I use occasionally, but it doesn't see much use. reply altacc 9 hours agoparentprevLack of keyboard as my reason for not buying a Framework laptop when upgrading last year. I wouldn't mind them releasing a Nordic keyboard, which isn't ideal but better than nothing (as a migrant I'm never quite sure what character I'll get when I hit those keys!). It seems Norway is often left out as being outside the EU and having it's own tax, customs and consumer rights requirements puts a large burden on companies to officially support a market here, especially when it's a relatively small market. reply Svip 10 hours agoparentprevTheand \\ keys are different places on Norwegian and Danish keyboards. reply lordnacho 10 hours agorootparentI always wondered why the special non-language characters are in different places in various language specific keyboards. Do Danish people not pipe their standard output as much? Do Swiss people not write emails as much? Are Europeans bad at finding the return key, thus requiring a larger one than Americans? These are all things I've actually come across. Why are the special characters in all sorts of weird places? One of the great mysteries of life. reply Ekaros 10 hours agorootparentOn other side I always found weird that ANSI layout has this weird monster key for\\ like are those so often used it needs bigger key? Why not just used standard sized key for normal thing... Also why miss such useful symbols as ¤ ½ § from layout? reply jimbobthrowawy 6 hours agorootparentprevI think most layout differences come from back then typewriters and word processors were the most that was available, and nothing got changed due to inertia. Going back and forth between \" and @ being capital-2 daily is probably the one that's caused me the most frustration. reply speedgoose 10 hours agoparentprevThe mandatory 5 years warranty is scary perhaps. reply Muromec 10 hours agorootparentOh… My framework laptop is two years old now and it starts showing. Had to replace the fan already (twice in fact) and now some random stuff with keyboard started to happen. 10 year old thinkpad in comparison can still be used both as a daily driver, a hammer and to barricade oneself during the zombie apocalypse reply Tade0 8 hours agorootparentThat's more or less the reliability of modern laptops for you. I had to replace the fans in my ASUS after less than two years, but the official manufacturer's parts store was out of stock so I had to rely on Ali Express. After that I had to go through the process of replacing them, which was anything but simple. You can't buy quality any more, but at least you can have repairability. reply chx 10 hours agorootparentprevThose ThinkPads still had a separate magnesium alloy structure frame of course they are indestructible. (Marketing term used to be \"roll cage\") reply Muromec 10 hours agoparentprevNeed to translate the manual to Norsk, it’s expensive or smth reply tordrt 8 hours agorootparentYou wouldnt have to make a lot of changes to the Danish manual for it to be Norwegian. Most words you could just keep as is reply matsemann 6 hours agorootparentYeah, I'm Danish but grew up in Norway, it's almost interchangeable. A few harder sounds in Norwegian, so change a few g's to k's etc. And Danish for some reason just litters commas randomly in a sentence, so just remove half of them randomly. And then remember the different meaning of \"må\" (NO=must, DK=may) when writing your manual, and you're good to go! reply idle76 6 hours agorootparentSoo, \"kan\"= \"can\" in norwegian and \"must\" in danish? Is that correct? reply hanche 4 hours agorootparentIn Norwegian, “kan” may mean either “can” or “may” depending on context. That’s a source of confusion sometimes. Not sure about Danish, but I don’t think it can mean “must”. reply flurdy 9 hours agoparentprevYeah, not in the EU makes it more expensive (customs), warranties, and more bureaucratic for a small market. I am sure they will eventually, but quicker and less friction to expand to EU countries and later maybe to other EEA countries. It will not be a hardware issues. All costs, red tape and priorities. reply FireInsight 9 hours agoprevCan't seem to find the finnish keyboard option for the 13, and it seems to be immediately out-of-stock for the 16. It's great, though. The keyboard layout was the main thing restricting me from getting a framework. I was just looking at slimbooks yesterday, in fact, since they offer more keyboard layouts. reply spindle 8 hours agoparentThis is an issue that should concern us all, since Finnish has a bright future: https://www.hagen-schmidt.de/suomi/worldlanguage.html reply FireInsight 5 hours agorootparentFun site, content seems to be around 20 years old. We've yet to see Finnish become the maailmankieli, though. reply kreyenborgi 9 hours agoprevaww, Norway left out again :-( reply cornedor 7 hours agoparentYou should be able to buy one, and pick it up in Sweden. https://www.eccnet.eu/consumer-rights/what-are-my-consumer-r... reply theshrike79 1 hour agorootparentNorway isn't in the EU, those rules don't apply reply darthrupert 9 hours agoparentprevCome to the dark side (EU), we'd love to have you. reply tordrt 8 hours agorootparentAs the current situation is I dont see why we shouldnt join the EU. We are better at adopting EU directives than most EU members. And we still end up having to adopt the horrible ones anyway, so might as well at least have an influence on the decisions that are made. reply jiripospisil 8 hours agorootparentprevDoes that actually help? I've been waiting for Czechia availability for years now. The best they can do is to tell me I should order to a supported country and handle shipping from there myself. reply poulpy123 9 hours agoprevThey are unfortunately too expensive for me personally, but framework is the only computer manufacturer that do interesting things reply nunobrito 10 hours agoprevIn case the manufacturer reads comments here: Can we have a LoRa module? reply mog_dev 8 hours agoparentThey will probably never make a LoRa module, but making a LoRa expansion card would be a pretty interesting project. You can get more info about the expansion cards on the dedicated github repo: https://github.com/FrameworkComputer/ExpansionCards reply StrLght 4 hours agoparentprevI wouldn't expect that such a small manufacturer to produce such a niche module reply kozinc 9 hours agoparentprevYou know, there's 3d models for the expansion cards, so you can always just design your own LoRa expansion cards, probably (or get someone else to do it) reply WesolyKubeczek 8 hours agoprev [–] I'd like to have one, but all laptops from my stack of laptops just stubbornly refuse to die. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "DestroyaFramework has announced that their products, including the Framework Laptop 13 and 16, are now available for shipment to Denmark, Finland, and Sweden.",
      "The new Framework Laptop 13 with Intel Core Ultra Series 1 processors is available for pre-order, along with Danish and Swedish/Finnish keyboards.",
      "Keyboards for the Framework Laptop 16 will be available in late July, with email alerts for stock updates available on the product page."
    ],
    "commentSummary": [
      "The Frame.work laptop is now available in Denmark, Finland, and Sweden, featuring proper Danish keyboards, addressing criticisms of Nordic keyboard layouts for coding.",
      "Framework's modular design allows for easy upgrades and repairs, emphasizing repairability and environmental benefits, though some users note issues with battery life and webcam quality.",
      "Expansion to Norway is complicated by its non-EU status, impacting costs and logistics."
    ],
    "points": 205,
    "commentCount": 177,
    "retryCount": 0,
    "time": 1719558557
  },
  {
    "id": 40814728,
    "title": "FCC rule would make carriers unlock all phones after 60 days",
    "originLink": "https://techcrunch.com/2024/06/27/fcc-rule-would-make-carriers-unlock-all-phones-after-60-days/",
    "originBody": "Login Search Search Startups Venture Apple Security AI Apps Events Startup Battlefield More Close Submenu Fintech Cloud Computing Layoffs Hardware Google Microsoft Transportation EVs Meta Instagram Amazon TikTok Newsletters Podcasts Partner Content Crunchboard Jobs Contact Us Hardware FCC rule would make carriers unlock all phones after 60 days Devin Coldewey 9:52 AM PDT • June 27, 2024 Comment Image Credits: Bryce Durbin / TechCrunch The FCC wants to make it significantly easier for consumers to unlock their phones from their carriers, proposing that all devices must be unlockable just 60 days after purchase. How this will mesh with current plans and phone-buying trends, however, is something the agency is hoping to learn before putting such a rule into effect. Mobile phones purchased from a carrier are generally locked to that carrier until either the contract is up or the phone is paid off. But despite improvements to the process over the years (unlocking was flat-out illegal not long ago), it still isn’t quite clear to all consumers when and how they can unlock their phone and take it to the carrier (or country) of their choice. To be clear, this is not about opening up your phone using a face, fingerprint or password, but changing settings in its software to allow it to work with different mobile networks. FCC Chairwoman Jessica Rosenworcel announced the Notice of Proposed Rulemaking, or NPRM, in a press release Thursday. “When you buy a phone, you should have the freedom to decide when to change service to the carrier you want and not have the device you own stuck by practices that prevent you from making that choice,” she wrote. “That is why we are proposing clear, nationwide mobile phone unlocking rules.” Specifically, the release says, carriers would simply have to provide unlocking services 60 days after activation. A welcome standard, but it may run afoul of today’s phone and wireless markets. For instance, although the dreaded two-year contract is no longer forced on most consumers, many still opt for them to lock in the price and get other benefits. And perhaps more to the point, the phones themselves are often paid for in what amount to installment plans: You get a phone for “free” and then pay it off over the next few years. The NPRM is the stage of FCC rulemaking where it has a draft rule but has not yet solicited public feedback. On July 18, the agency will publish the full document and open up commentary on the above issues. And you can be sure there will be some squawking from mobile providers! Not knowing the specifics of the proposed rule, we can’t be sure how it would mix with these common pay-over-time details. But unlocking a phone doesn’t free someone from needing to pay off the device — they can just use it on other networks if they want. And if a carrier lets you buy a phone outright from it but locks it to the bands for six months or a year out of sheer greed, this would offer an early exit. As Rosenworcel said, the point of the rule is to offer consistency and transparency: a simple, national rule from regulators setting a reasonable limit on how and whether carriers can lock down devices. We’ll know more in July when the full NPRM is published. More TechCrunch Get the industry’s biggest tech news Explore all newsletters TechCrunch Daily News Every weekday and Sunday, you can get the best of TechCrunch’s coverage. Add TechCrunch Daily News to your subscription choices Startups Weekly Startups are the core of TechCrunch, so get our best coverage delivered weekly. Add Startups Weekly to your subscription choices TechCrunch Fintech The latest Fintech news and analysis, delivered every Tuesday. Add TechCrunch Fintech to your subscription choices TechCrunch Mobility TechCrunch Mobility is your destination for transportation news and insight. Add TechCrunch Mobility to your subscription choices No newsletters selected No newsletters Email address (required) Subscribe By submitting your email, you agree to our Terms and Privacy Notice. Tags FCC, mobile phones Security Hubspot says it’s investigating customer account hacks Lorenzo Franceschi-Bicchierai 28 mins ago The company “identified a security incident that involved bad actors targeting a limited number of HubSpot customers and attempting to gain unauthorized access to their accounts” on June 22. Transportation Volkswagen’s Silicon Valley software hub is already stacked with Rivian talent Kirsten Korosec Sean O'Kane 35 mins ago VW Group’s struggling software arm Cariad has hired at least 23 of the startup’s top employees over the past several months. Featured Article All VCs say they are founder friendly; Detroit’s Ludlow Ventures takes that to another level VCs Jonathon Triest and Brett deMarrais see their ability to read people and create longstanding relationships with founders as the primary reason their Detroit-based venture firm, Ludlow Ventures, is celebrating its 15th year in business. It sounds silly, attributing their longevity to what’s sometimes called “Midwestern nice.” But is it… Connie Loizos 39 mins ago Social The White House will host a conference for social media creators Amanda Silberling 52 mins ago President Joe Biden’s administration is doubling down on its interest in the creator economy. In August, the White House will host the first-ever White House Creator Economy Conference, which will… Fundraising Pitch Deck Teardown: MegaMod’s $1.9M seed deck Haje Jan Kamps 52 mins ago In an industry where creators are often tossed aside like yesterday’s lootboxes, MegaMod swoops in with a heroic promise to put them front and center. AI Google Gemini: Everything you need to know about the new generative AI platform Kyle Wiggers 59 mins ago Google’s trying to make waves with Gemini, its flagship suite of generative AI models, apps and services. So what’s Google Gemini, exactly? How can you use it? And how does… Social Who won the presidential debate: X or Threads? Sarah Perez 2 hours ago There were definite differences between how the two platforms managed last night, with some saying X felt more alive, and others asserting that Threads proved that X is no longer… Commerce Following raft of consumer complaints, Shein and Temu face early EU scrutiny of DSA compliance Natasha Lomas 2 hours ago Ultra-low-cost e-commerce giants Shein and Temu have only recently been confirmed as subject to centralized enforcement of the strictest layer of the European Union’s digital services regulation, the Digital Services… Climate Cold shipping might be the next industry that batteries disrupt Tim De Chant 4 hours ago Artyc has raised $14 million to date and has a product on the market, Medstow Micro, that helps ship temperature-sensitive specimens. TechCrunch Disrupt 2024 Elevate your 2025 fundraising strategy at Disrupt 2024 TechCrunch Events 4 hours ago Get ready to unlock the secrets of successful fundraising in the upcoming year at Disrupt 2024. Our featured session, “How to Raise in 2025 if You’ve Taken a Flat, Down,… Security Remote access giant TeamViewer says Russian spies hacked its corporate network Zack Whittaker 6 hours ago The remote access giant linked the cyberattack to government-backed hackers working for Russian intelligence, known as APT29. Gadgets Here are the hottest product announcements from Apple, Google, Microsoft and others so far in 2024 Christine Hall 6 hours ago We’ve poked through the many product announcements made by the biggest tech companies and product trade shows of the year, so far, and compiled them into this list. Fintech Feather raises €6M to go Pan-European with its insurance platform for expats Anna Heim 10 hours ago As a foreigner, navigating health insurance systems can often be difficult. German startup Feather thinks it has a solution and raised €6 million to help some of the 40-plus million… Fundraising Rohlik rolls up $170M to expand in European grocery delivery and sell its tech to others Ingrid Lunden 12 hours ago The salad days of fresh grocery delivery startups are over, but those that have stayed the course, and built businesses that are seeing gains, are still here and hungry for… Robotics Robotics investments are gaining speed after post-pandemic slowdown Brian Heater 22 hours ago The first six months of the year have seen $4.2 billion invested in robotics, putting this year well on track to beat 2023’s 12-month total of $6.8 billion. Startups Hebbia raises nearly $100M Series B for AI-powered document search led by Andreessen Horowitz Marina Temkin 23 hours ago Hebbia, a startup using generative AI to search large documents and return answers, has raised a nearly $100 million Series B led by Andreessen Horowitz, according to three people with… Robotics Agility’s humanoid robots are going to handle your Spanx Brian Heater 23 hours ago Digit’s first job will be moving totes around a Connecticut Spanx factory — which is most definitely not a euphemism. Fundraising Will AI get an A+ in edtech? MagicSchool raises $15M to find out Ingrid Lunden 1 day ago These days, when you hear about students and generative AI, chances are that you’re getting a taste of the debate over the adoption of tools like ChatGPT. Are they a… AI Zuckerberg disses closed-source AI competitors as trying to ‘create God’ Sarah Perez 1 day ago In the conversation, Zuckerberg said there needs to be a lot of different AIs that get created to reflect people’s different interests. Fundraising Andrew Ng plans to raise $120M for next AI Fund Kyle Wiggers 1 day ago AI big shot Andrew Ng’s AI Fund, a startup incubator that backs small teams of experts looking to solve key problems using AI, plans to raise upward of $120 million… Transportation VW taps Rivian in $5B EV deal and the fight over Fisker’s assets Kirsten Korosec 1 day ago Welcome back to TechCrunch Mobility — your central hub for news and insights on the future of transportation. Sign up here for free — just click TechCrunch Mobility! Am I… Hardware FCC rule would make carriers unlock all phones after 60 days Devin Coldewey 1 day ago Specifically, according to the FCC, carriers would simply have to provide unlocking services 60 days after activation. Image Credits: Bryce Durbin / TechCrunch Transportation As battery startups fail, Sila snaps up $375M in new funding Rebecca Bellan 1 day ago Amid a fraught environment for battery startups, Sila has raised $375 million to finish construction of a U.S. factory that will scale its next-generation battery technology for customers like Mercedes-Benz… Security Startups scramble to assess fallout from Evolve Bank data breach Lorenzo Franceschi-Bicchierai Mary Ann Azevedo 1 day ago Fintech-friendly Evolve Bank disclosed a data breach, saying it may have impacted customers and partners. Apps Meta starts testing user-created AI chatbots on Instagram Ivan Mehta 1 day ago Meta CEO Mark Zuckerberg announced on Thursday that the company will begin to surface AI characters made by creators through Meta AI studio on Instagram. The tests will begin in… Climate Rondo Energy funding shows a new way across the climate startup ‘valley of death’ Tim De Chant 1 day ago Climate tech startups especially those building hardware, face a particular challenge when trying to move beyond the prototype or pilot phase and start selling finished products to customers. Biotech & Health Amazon consolidates Amazon Clinic into the One Medical brand Lauren Forristal 1 day ago Amazon is folding its Amazon Clinic telehealth service into its primary care business One Medical, the company announced on Thursday. The company explained in today’s blog post that, to simplify… Apps Just in time for the debates, Meta fixes bug impacting users’ political content settings on Instagram and Threads Sarah Perez 1 day ago Meta has fixed the bug that caused people to believe the company had adjusted their selections in a political content settings tool without their consent. The issue had impacted users… Apps YouTube Premium upgrade adds smart downloads and picture-in-picture mode for Shorts Ivan Mehta 1 day ago YouTube is adding several new features for Premium users, including smart downloads and support for picture-in-picture mode for Shorts, as well as a wider rollout of its “Jump Ahead” feature… TechCrunch Disrupt 2024 TechCrunch Disrupt joins forces with Google Cloud for Startup Battlefield 200 TechCrunch Events 1 day ago TechCrunch is joining forces with Google Cloud as its lead partner for Startup Battlefield 200. This event will highlight and support the most promising startups from around the globe at… About TechCrunch Staff Contact Us Advertise Crunchboard Jobs Site Map Legal Terms of Service Privacy Policy RSS Terms of Use Privacy Placeholder 1 Privacy Placeholder 2 Privacy Placeholder 3 Privacy Placeholder 4 Code of Conduct About Our Ads Trending Tech Topics Evolve Bank Data Breach Meta AI Fisker Character.AI Phone Unlocks Tech Layoffs ChatGPT Facebook X YouTube Instagram LinkedIn Mastodon Threads © 2024 Yahoo. All rights reserved. Powered by WordPress VIP",
    "commentLink": "https://news.ycombinator.com/item?id=40814728",
    "commentBody": "FCC rule would make carriers unlock all phones after 60 days (techcrunch.com)172 points by rntn 22 hours agohidepastfavorite83 comments paxys 21 hours agoI don't think people realize that the US is the only market in the world where phones are carrier locked, and in fact the only market where carriers have so much power over the features and overall experience of your phone. Mobile carriers dictate that a phone has to be sold with a locked bootloader. They decide if/when the phone should get OS updates. They are the ones who fill the phone with bloatware. Up until a few years ago the phone had a more prominent logo of the cell carrier than the company that actually made it. US carriers have used their government-granted monopolies to influence the market wayy beyond phone calls and data plans, and it's about time it should end. reply miki123211 20 hours agoparentKeep in mind that in Europe, some phones are still under carrier control, despite not technically being locked. You can resell the phone and use it with any carrier you wish, but as soon as the original owner stops paying the bill, the phone becomes remote-locked and turns into a brick. Samsung's Knox can do this, not sure which other brands also have that option. reply paxys 20 hours agorootparentWhich is still significantly better than the system in the US. Most people make all their payments in time like they are supposed to, but are still affected by carrier locks. For example if I want to travel abroad and buy a local SIM for cheaper data, I'm out of luck, because AT&T won't let me use my phone. So I have to use their ultra expensive roaming plan or buy a secondary phone just for those few days. reply SkyPuncher 17 hours agorootparentprevThis sounds like the prior owner is financing the phone, not purchasing it outright. reply entropyie 10 hours agorootparentprevI've never seen this in practice in the EU, so it can't be common at least. reply LocalH 2 hours agoparentprev> They decide if/when the phone should get OS updates. That's an Android issue. iPhones have no such issue. reply 0x5f3759df-i 20 hours agoparentprevPhones only started being unlocked in Canada starting in 2017, so the US isn’t so unique. Though Canada carriers are probably not the standard to hold yourself to. reply Brananarchy 14 hours agorootparent2017 is quite a long time ago, especially in terms of how long smartphones have been around (and arguably even in terms of how long cell phones have been mainstream). The US is way behind here. reply Mo3 21 hours agoparentprevThey used to be SIM and/or carrier locked here in west Europe, until if I remember correctly 10-15 years ago reply bogantech 21 hours agoparentprev> I don't think people realize that the US is the only market in the world where phones are carrier locked Probably because that's not true. https://en.m.wikipedia.org/wiki/SIM_lock reply ThrowawayTestr 21 hours agoparentprevDo other countries get subsidised phones with a phone plan? reply 0xTJ 21 hours agorootparentIn Canada, phone locking has been banned since 2017, with free unlocking being available to phones purchased before that. You can still get cheaper/free phones with your plan, but it won't be locked to that carrier, that would be silly. It's over a contract of some defined length, and you're bound by the contract to pay for whatever value's left of the phone if you cancel. If you paid for that phone by being subscribed to your carrier for that length of time (say, 2 years), why wouldn't you be able to take that phone elsewhere? You've already paid for it, it's yours. There's also been a trend (I don't know whether it's legislated or not) for phones to be explicitly $X/mo for 24 months (though that still ends up being far below retail, and is sometimes $0), instead of burying that in the plan costs. We still have plenty of problems with ridiculous plan pricing and data limitations, but those have gotten better, and were still bad before carrier locking went away. reply whycome 17 hours agorootparentHey now, there’s only so much data to go around. They have to charge really high prices for it. And of course it’s limited by month-use because that’s necessary. Canada needs some damn changes. reply paxys 21 hours agorootparentprevIf you run the numbers the subsidy offered by US carriers always ends up costing you more than just buying a new phone for full price with a cheaper plan. Your \"free\" iPhone isn't free if it involves (1) giving them your current one which is worth $500+ in a private sale and (2) signing up for a 3 year contract at $100+ a month. reply V__ 21 hours agorootparentprevYes reply Loughla 21 hours agorootparentprevYeah I'll take a big ass logo on the start screen for a free phone. reply neilv 22 hours agoprevThis is only for the normal sense of \"unlocked\" as not being restricted to use with a single carrier, correct? What about other carrier modifications to devices, like when a carrier prevents a Pixel phone's bootloader being \"OEM unlocked\" so that GrapheneOS or other alternative systems software can be installed? reply Molitor5901 22 hours agoprevThis seems like the fair and just thing to do. Being locked into a carrier is just wrong, but I would accept that if you purchased a discounted phone through the carrier that there would be divorce penalties. reply toomuchtodo 22 hours agoprevWithout provisions for equipment installment plans, this would likely end financing of phones by carriers (potentially pushing consumers to higher cost mechanisms, like credit cards or other traditional credit instruments). When locked, the phone is the collateral. Free and clear phones should be unlocked immediately. reply yyyfb 22 hours agoparentMarkets exist where SIM locking has been prohibited for a long time (Wikipedia says Canada, Chile, China, Israel, and Singapore at least). Financing still exists in these countries. The phone is absolutely not \"collateral\". The carrier does not take it back if you default (even if they could, they wouldn't be able to get anything of value for it). Unlocking is just an inconvenience that prevents enough people from churning that it lowers the risk of financing. reply dawnerd 22 hours agorootparentThey'd still make you pay off the full amount before you could close off your account. Pretty standard in the US. Locking is just an evil way to add more friction. reply toomuchtodo 22 hours agorootparentPeople with nothing don’t care if they get sent to collections, hence the need to have some control over the device while monies are owed for it. Just like vehicle interlocks for subprime auto notes. Until you pay off the collateral, it isn’t your ownership, simply permission to use while servicing the debt. reply dv_dt 21 hours agorootparentThen the companies should not extend that loan/credit reply toomuchtodo 21 hours agorootparentThen a lot of folks who finance phones on cheap or free installment plans currently won’t get phones, simple as that. If regulators are fine with that, that’s a reasonable position I suppose. Carriers aren’t a charity to take on aggressive credit risk in these financial consumer populations. Incentives->outcomes. reply 999900000999 21 hours agorootparentAlot of folks won't have iPhones. You can get a darn decent Android for 200$ unlocked, and if your really struggling a 50$ phone will get the job done. No one will suffer if they can't get an iPhone. A lot of folks also can't do math. I pay about $20 a month for my phone plan, and I paid $700 for my phone up front. AT&t isn't giving phones away, you end up on a more expensive plan that's around $80 or so and then they'll tack on 30 bucks for the phone. If you're lucky the bill credits will cover the entire cost of the phone, but over two years you've still spent an addition 700$. reply SR2Z 18 hours agorootparentprevMost Americans can afford a $300 phone, and that's enough to buy a pretty solid Android phone or a used iPhone. Carriers will just make people sign two-year contracts at a guaranteed rate, plus an early exit fee. This is how all kinds of predatory scams work; the carriers will do just fine and low-income people will continue to get the newest iPhone. reply dv_dt 3 hours agorootparentYup, arguably in the wider scale, more accessible unlocked phones will become available on the used market, serving low-income people better. Right now there are many locked phones on the used market, both wasteful and difficult to navigate. reply normaler 21 hours agorootparentprevWhat are free installment plans? Currently the most used phones in the US are iphones. If the actual cost is not hidden behind monthly payments anymore, but some users can not afford iphones, people might start to consider cheaper phone options. reply Detrytus 21 hours agorootparentprevHere in Europe they will actually ask you for a proof of employment/income before they sell you the cell phone plan. If you are unemployed then your only option is pre-paid sim, and those usually do not come with phones. reply Anduia 21 hours agorootparentIn which country? reply sgift 20 hours agorootparentRather typical in Germany (they usually do it via Schufa, but the end result is the same). No idea for other countries. reply yftsui 22 hours agorootparentprevI believe he meant financing will become more expensive, which may actually increase the consumer cost over a fixed period (if the carriers are not willing to decrease the plan prices which are jacked up to compensate the contracts). reply Molitor5901 22 hours agorootparentprevIt's purely anti-competitive and anti-consumer. reply yyyfb 14 hours agorootparentThere's been kind of a silver lining consumer benefit, historically. In early years, SIM locking allowed carriers in Europe to subsidize device cost based on expected revenue, effectively offering financing outside of the financial system, and benefiting a set of customers who wouldn't otherwise have been eligible for normal credit. reply icepat 22 hours agorootparentprevYes, because in Canada at least, if you terminate your contract, you have to pay out the phone. That's how it's done. Really, it's not a complicated solution. reply WD-42 21 hours agorootparentSounds like a reasonable thing consumers should be able to do. reply icepat 4 hours agorootparentConsumer protections in Canada are terrible, but in the very least they're not anti-consumer. reply Zak 21 hours agorootparentprevCollateral has two functions: 1. Incentivize the borrower to continue paying so as to not lose the collateral. 2. Allow the lender to recover some of the value of the loan. A locked phone serves the first purpose because the lender can disable its primary function. reply lbourdages 22 hours agorootparentprevYes. In Canada, the phones are not locked, but if you terminate your contract before the term (max 2 years) you have to pay the remaining cost. reply Amezarak 21 hours agorootparentWhat happens if you don’t pay? I ask this out of genuine curiosity - I’m not sure what happens in the US either, I don’t believe they brick carrier-locked phones that a customer stops paying for but I’m not sure. But I’ve enough experiences with enough people to know this is probably actually a fairly common scenario and I wonder what the consequences are. (A surprising amount of the time, there are no real consequences.) reply lbourdages 17 hours agorootparentA phone contract is on one's credit report. If they don't pay, the phone company will try to recover the debt like any other debt (via nagging, and then selling the debt to a collection agency). It's not fundamentally different from not paying the bill at the end of the month. reply Detrytus 22 hours agorootparentprevAnd in EU, it is technically not prohibited, but haven't really been a thing for a long time. One caveat: carriers do not really pay for your phones. Your phone bill would list two separate charges: service charge, for calls, internet use, etc., and then the monthly payment for your phone. If you add all those monthly payments over the whole contract period you get maybe 5-10% discount to the regular market price. reply pipodeclown 21 hours agorootparentYes but what is prohibited in Europe is to hide the cost of the phone in the payments of the cell plan. They must make clear exactly what part of your monthly payment is to pay off the phone and what part is the cell plan. That has basically blown up subsidised phones in Europe.. reply kirenida 21 hours agorootparentprevIt seems that Croatian operators didn't get the memo about the discount. Here the phones that you can get from your carrier and pay off on a monthly basis often end up costing more than in retail. reply jrockway 22 hours agoparentprevIs the phone really collateral in these financing agreements? They just want the guaranteed revenue stream from forcing you to stick with them for 2 years. I imagine it would be quite expensive to go retrieve the phone from someone's house after they stop paying, and they could do that even if it wasn't locked to a particular carrier. I think the threshold for repossession of collateral is somewhere around cars; stop paying your car lease, they'll take the car; stop paying for your house, they'll kick you out of your house. But I don't think it's worth it for phones. reply singleshot_ 22 hours agoparentprevFor me, never again encountering a locked phone would far outweigh the utility of being able to finance a phone. There should be no need to make a basically disposable item like a phone collateral: just buy it. reply ldoughty 22 hours agoparentprevUnlocking phones does not free the individual of contractual obligations... Yes, there's room for abuse in the system.. and perhaps prepaid phones won't be as well subsidized.. but people getting contracts typically take a credit hit or require a hefty security deposit to offset the risk. reply paxys 22 hours agoparentprevWhat does SIM locking have to do with financing? It's not like unlocking the phone means you suddenly don't owe the rest of the payments anymore. reply dangus 22 hours agoparentprevThis doesn’t affect equipment installment plans. The phone is still collateral whether it’s locked or not. In fact, AT&T is the only one of the big 3 carrier that locks their devices that are financed. The other ones don’t bother. If you terminate your cellular service with an installment plan it’s typical that you immediately owe the balance. Whether the phone is locked or not makes no difference on whether the company can collect on the debt. reply m463 19 hours agoparentprev> Free and clear phones should be unlocked immediately. ...and automatically Even if your phone is paid for, removing the lock is confusing and time-consuming friction that shouldn't happen. They make money by doing it. reply turtlebits 22 hours agoparentprevYep, no more carrier locked $20 iPhones. reply 2OEH8eoCRo0 22 hours agorootparentIt's only $20 because they add installment payments to the monthly bill which is recouped by early cancellation fee. reply turtlebits 21 hours agorootparentNo, these are prepaid phones. They're just betting on you staying with the prepaid carrier since they're locked. Recent deal: https://slickdeals.net/f/17036050-walmart-stores-64gb-apple-... I bought one for $20 several years ago as a glorified iPod touch and never activated it. reply mdasen 21 hours agoparentprevAll Verizon phones are automatically unlocked after 60 days and Verizon has still been financing phones. > When locked, the phone is the collateral Not really. If someone cancels service without paying off the phone, the carrier doesn't reclaim the phone. It simply prevents the phone from being used with a different carrier. The person could sell the phone to another customer on the same network. Houses are collateral because it's hard to hide a home from creditors and you can't sell the home without discharging the lien on the home. There's no lien on your financed phone. This change probably wouldn't change much for phone financing because phone companies are already running credit checks when handing out devices on payment plans, require higher-risk people to make down-payments on the phones, and once a person has done it once to you, it's easy to never offer it to them again. Once you've burned Verizon by canceling service and not paying off your phone, you've burned that bridge. Plus, Verizon would likely report it to the credit agencies where you'd have burned the bridge with the other carriers too. This is a rule that would help prevent a lot of e-waste and make it easier for folks to switch carriers. There is the chance that someone will finance a phone and leave without paying it off, but there's always been a risk that someone would run up a phone bill and not pay it. Someone could go abroad and run up a large roaming bill and not pay it. Back when data plans were limited, someone could run up a bill into the thousands and just cancel service without paying. Locked devices do serve a function for carriers. They make switching harder and they protect companies roaming revenues. If I have a locked phone, I have to pay Verizon $10/day to use my phone in Europe. If I have an unlocked phone, I can grab a European SIM for $30 and have cheap service for the month. Verizon has been financing phones and offering similar discounts that T-Mobile and AT&T have been offering even though their devices will automatically unlock after 60 days. A locked phone is worth marginally less than a carrier-locked phone, but a locked phone can still be sold to other people, even if it isn't paid off. A locked phone can still be used even if the original purchaser has defaulted on the debt. These aren't bricked phones, just carrier locked devices. If the issue were that they needed a form of collateral, carriers would want the ability to brick the devices rather than merely reduce the resale value of the device by 15%. Yea, if you finance an AT&T iPhone, default on the debt, and sell it to Gazelle, you'll get 86% of the price for your locked phone as you would for an unlocked one. If one could flip unlocked financed phones, it would be just as easy to flip locked financed phones - you'd just make 15% less per device. So what is the lock preventing? It's not preventing someone who has found a way to defraud carrier financing. They can still sell the phones (which are legally their phones which the carrier has no lien on). They merely get a marginally lower resale price. No, the locks aren't necessary for equipment financing. No, the phones aren't collateral. reply 2OEH8eoCRo0 22 hours agoparentprevIsn't that what the cancellation fee is for? reply jstummbillig 21 hours agoprevTIL locked phones are still a thing, somewhere. Not gonna lie: That feels fairly wild at this point. reply miohtama 21 hours agoparentThe US is also using paper cheques for payments reply dv_dt 20 hours agorootparentAnd for some forms of US electronic check payments, typing an account number wrong will take 4-5 days to return a \"bounced check\" result, often with a fee from the entity you're trying to pay. reply blitzar 21 hours agoparentprevIts always the US ... the comments are always full of \"the system will break down if they scrap it / wont somebody think of the children\" reply lotsofpulp 21 hours agorootparentYou can easily get an unlocked phone in the USA, so it is more like \"If you want an unlocked phone, buy an unlocked phone, and if want a mobile network to subsidize your phone, then you might have to deal with the terms of the agreement, including being locked to a mobile network\". reply the_snooze 21 hours agorootparentEnough people seem to love artificially low up-front prices, even if it costs them more (in money and headaches) in the long-run. See: Spirit Airlines, Ticketmaster, and restaurant fees and tipping. reply OptionOfT 21 hours agorootparentIn general in the USA a lot of things are pushed to you as monthly payments, and not the total cost of ownership. Yes, you can afford a $900 / month car note. But that doesn't mean that it actually makes sense to pay it for 72 months at some ridiculous interest rate. reply Spivak 19 hours agorootparentprevIf you're not planning on switching carriers the financing offered by carriers is a 0% interest loan. If you just need a flight somewhere for a weekend trip, carry on a backpack, and pack your own snacks those $100 Spirit flights are real. If you play the game you actually do come out ahead. It being possible at all to get flights that cheap is the lifeblood of broke bitches. Restaurant surcharges are a mixed bag because so long as they're advertised at the door I think they're great, they make take-out cheaper. But Ticketmaster is just garbage and their practices should be strongly illegal because it's just fuck you pay more. reply CursedUrn 21 hours agoprevThere are lots of subtle ways carriers can punish unlocked phones. I tried using an unlocked Samsung flagship with a Verizon MVNO and it never worked properly. They even told me that various features wouldn't work such as Wi-Fi calling. Had to go with the main carrier anyway, so I might as had a locked phone. If the FCC pursues this rule they need to cover all the loopholes and even then it will probably be years of malicious compliance like we're seeing from Apple in the EU app store ruling. reply RulerOf 21 hours agoprevMy device shouldn't be carrier locked at all. I took out a loan to pay for this thing. They got their money for it immediately. During the February AT&T outage[1], my wife's phone was affected, and she had to go somewhere. I should've been able to spend $20 on a throwaway e-sim and had it working before she left the house. Instead, I had to shrug my shoulders and suggest she find WiFi wherever she was headed. Carrier locks in today's age are leftover garbage from a dated, consumer-hostile business model that's no longer practiced. And if I default on my loan repayments, the creditor can garnish my wages. 1: https://www.cnn.com/2024/02/22/tech/att-cell-service-outage/... reply reboot81 21 hours agoprevHere in Sweden carrier locks ended a decade ago. And what are we offered now? 24/36 months plans of subscription and a okay payment plan for a +1000 euro device. iPhone 15 Pro Max is €1330 from the major resellers. With a 2-year with 40GB/ month plan it gives you a ~€200 discount on the phone. If you're a student or senior you get much cheaper plans, and if you got the cash, it might even be better to buy the phone with cash. reply Zak 21 hours agoprevIt seems to me that a carrier should be able to lock a subsidized/financed device until it's paid off. That makes it possible for people who would otherwise not qualify for financing to have relatively up-to-date devices. A carrier should not be able to lock a device that's paid off for any length of time. reply nritchie 22 hours agoprevI wonder what this would do to the prices of phones? In particular, I wonder if iPhones would be as popular (or as expensive) if people paid the full price up front? I suspect if people paid up front for phones, people wouldn't gravitate to the latest and greatest. reply 999900000999 21 hours agoprevI would love this. However I imagine what most people won't really think about is that if you buy a phone on say AT&t, and then you just decide to stop making payments they can just remotely brick it. It's going to be really interesting to see how prepaid carriers cope with this. For example you can buy a fully . For example you can buy a fully . For example you can buy a phone much cheaper if it's locked to MetroPCS or another prepaid provider. Since technically you've bought the phone, them bricking it becomes a lot harder to justify. reply enceladus06 21 hours agoprevFor a $200 discount I had to deal with the RedPocket 1yr lock. Sort of worth it, but the \"are you sure you really want to unlock\" and \"why do you want to unlock?\" questions from the customer service were really annoying. If you pay the $, the phone should be yours. reply reboot81 21 hours agoprevIn the end we the customers will foot the bill, no matter how the game is rigged. I see no reason for 60 days, the financial agreement between the carrier and subscriber persist no matter what. reply post_break 22 hours agoprevYes please. Some carrier unlocking rules are ridiculous, even if you pay off the phone. If you pay off the phone it should be unlocked immediately. Financing your phone raises some questions about unlocking, but I guess the carriers can decided to not allow you to finance your phone if you just stop paying on them. reply jobs_throwaway 21 hours agoprevCurious we don't see the same dickriding for carriers like AT&T that we do for Apple on HN. All of the same tired arguments of 'you can choose a different carrier if you don't like it' and 'I only feel safe giving grandma a phone that doesn't allow her to do anything outside of the warm teat of Apple' apply perfectly here. reply salvagedcircuit 15 hours agoprevThis would be a warm welcome to the frigid waters of the phone industry. Now force the OEMs to release all radio related firmware for each phone too, and we can finally own our phones again. I'm not too old to remember completely unlocked, feature complete CyanogenMod android phones. LineageOS is completely useless without VoLTE support. reply howmayiannoyyou 22 hours agoprevP L E A S E The carriers are terrible about this. I have a Samsung sitting on my desk I still cannot use because I can't get it unlocked. reply OutOfHere 15 hours agoparentIf you must use a Samsung, next time buy an unlocked phone directly from Samsung. reply orwin 21 hours agoprevSo, i'm obviously not from the US, and while i'm not a liberal anymore (For USians: here, liberal=pro-market/what you call capitalist), i do understand what free markets (in a frictionless vacuum...) bring for the consummers. It seems to me that in the last two years, the FCC and the FTC are kinda waking up and start annoying corporation into making the markets they compete in freer. I only get my US news from hackernews, so maybe i only have the good, and not the ugly, but even in cases those agencies lost, they made good points and seems to be pushing the US to be less corporatists and more liberalist (in the economic sense, free market and stuff you USian call capitalism) So how those agencies fall accross party lines? Are those independant? In my opinion a change of president/government shouldn't change the culture in those agencies, but you are a weird country (the fact that you _still_ have carrier lock proves it), is this a \"risk\" in your case? reply pessimizer 22 hours agoprevWhy 60 days? Why not zero days? If you're going to take away their extrajudicial means of contract enforcement, what's the point of this residue? The only purpose for leaving it would be to confuse the customer about whether they can unlock, and to allow companies to make the process burdensome and confusing. The reason carriers fight to preserve this isn't for enforcement of the contract at all, it's because of the other revenue streams from unwanted intrusion by the carrier into the use of your phone. Carriers can sue for the cost of the phone. And the FCC, by allowing this 60 day provision, would be making a conscious attempt to protect carrier data harvesting and customer capture. I get allowing carriers to lock for the length of a contract; I get not allowing carriers to lock at all. This, however, is an attempt to pander to people who think locking is wrong while still preserving the benefits that carriers get from locking. These benefits will now be delivered by jerking customers through a Kafkaesque dance of intentionally confusing bureaucracy. Unlocking becomes cancelling your gym membership or your subscription to the Economist. reply yftsui 22 hours agoparentFor premium devices, lock for 60 days reduce chances that the devices are stolen during shipment then appear in Shenzhen next day. As someone received a perfectly packaged empty box this year, I think this is not ideal but reasonable. reply scarface_74 22 hours agoprev [–] This only helps the credit card companies. There definitely should be an exception to force unlocking if you are on a payment plan. And it will make it harder for the poor and those with bad credit to get a phone. T-Mobile for instance will let you finance a phone regardless of credit once you have been a customer for a year. I believe that some of the MVNOs will even let you get up to a midrange phone like an older iPhone as long as you stay with them for a few months. They would only do that if it’s locked. reply dangus 22 hours agoparent [–] T-Mobile doesn’t even lock their device payment plan phones last I checked. reply scarface_74 14 hours agorootparent [–] They very much do. reply dangus 14 hours agorootparent [–] That’s correct, my mistake. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The FCC proposes a rule requiring carriers to unlock phones 60 days after purchase to simplify the process for consumers.",
      "FCC Chairwoman Jessica Rosenworcel announced the Notice of Proposed Rulemaking (NPRM) to establish clear, nationwide unlocking rules, with public feedback invited starting July 18.",
      "The rule aims to provide consistency and transparency, making it easier for consumers to switch carriers."
    ],
    "commentSummary": [
      "The FCC has proposed a rule mandating carriers to unlock phones after 60 days, aiming to reduce e-waste and facilitate easier switching between carriers.",
      "The US is distinct in having carrier-locked phones, which gives carriers significant control over phone features and updates, unlike Europe where phones can be remotely locked for non-payment.",
      "The proposal has sparked debate, with some viewing carrier locks as anti-consumer, while others argue they help finance phones for individuals with poor credit."
    ],
    "points": 172,
    "commentCount": 83,
    "retryCount": 0,
    "time": 1719520021
  },
  {
    "id": 40815097,
    "title": "Maker of RStudio launches new R and Python IDE",
    "originLink": "https://www.infoworld.com/article/3715702/maker-of-rstudio-launches-new-r-and-python-ide.html",
    "originBody": "Home Software Development Integrated Development Environments Maker of RStudio launches new R and Python IDE Posit, formerly RStudio, has released a beta of Positron, a ‘next generation’ data science development environment based on Visual Studio Code. By Sharon Machlis Executive Editor, Data & Analytics, InfoWorldJun 27, 2024 11:30 am PDT TippaPatt/Shutterstock The company best known for RStudio, the leading integrated development environment (IDE) for R programmers, has quietly launched a “next-generation” IDE designed specifically for both R and Python. The Positron IDE is available in public beta as of today for macOS, Windows, and Linux. Created by Boston-based Posit PBC, formerly RStudio, Positron is based on Microsoft’s Visual Studio Code. Users of VS Code will likely find Positron’s look and feel rather familiar, with panels for writing code and viewing code output, consoles and terminals, and an activity bar at the far left offering options for file navigation, version control, debugging, and extensions. However, Positron is packaged out of the box to be easier to set up, especially for R users but also for Python. There’s no need to install extensions in order to get R up and running in Positron, as is the case with VS Code. Likewise you don’t have to install an extension for Positron to run Python. In fact, you’re cautioned not to install the usual VS Code extensions for R and Python in Positron, since the IDE already comes with that functionality built-in. For both languages, you should have the basic language files installed on your system, as well as the IPykernel package to run Python. Positron easily found both my R and Python installations at first launch. IDG You can install other VS Code extensions in Positron if you want them, though. Because Microsoft does not allow third-party IDEs to access the official VS Code Marketplace, Positron extensions are installed via the OpenVSX registry. “Posit is a major sponsor of OpenVSX,” Posit noted in its Positron wiki. Not all VS Code extension authors also submit and regularly update their projects to OpenVSX, however. Posit calls the project “a next-generation data science IDE” and “an extensible polyglot tool for writing code and exploring data.” It has a built-in, easy-to-use data and variable explorer, which includes options like sorting and filtering data frames. It can be accessed by clicking an icon, for both R and Python data. “The Data Explorer is intended to complement code-first exploration of data, allowing you to display data in a spreadsheet-like grid, temporarily filter and sort data, and provide useful summary statistics directly inside of Positron,” according to the project wiki. “The goal of the Data Explorer is not to replace code-based workflows, but rather supplement with ephemeral views of the data or summary statistics as you further explore or modify the data via code.” There are other welcome little tweaks in Positron, such as cmd/ctrl + enter running one line of a Python script and then moving your cursor to the next line of code. This can be surprisingly helpful for quick code examination outside of the debug tool. If you are working on a project that combines both R and Python scripts, which I increasingly do as an R user working with generative AI, the IDE also easily pops up the correct console when you switch between scripts in both languages. The repo cautions that Positron is “an early stage project under active development.” Users should keep that in mind when weighing how and when to try it out. Next read this: Why companies are leaving the cloud 5 easy ways to run an LLM locally Coding with AI: Tips and best practices from developers Meet Zig: The modern alternative to C What is generative AI? Artificial intelligence that creates The best open source software of 2023 Related: Integrated Development Environments Development Tools Python R Language Data Science Analytics Software Development Sharon Machlis is Director of Editorial Data & Analytics at Foundry, where she works on data analysis and in-house editor tools in addition to writing. Her book Practical R for Mass Communication and Journalism was published by CRC Press. She was named Digital Analytics Association's 2021 Top (Data) Practitioner, winner of the 2023 Jesse H. Neal journalism award for best instructional content, 2014 Azbee national gold award for investigative reporting, and 2017 Azbee gold for how-to article, among other awards. You can find her on Mastodon at masto.machlis.com/@smach. Follow Copyright © 2024 IDG Communications, Inc. Sponsored Links Visibility, monitoring, analytics. See Cisco SD-WAN in a live demo.",
    "commentLink": "https://news.ycombinator.com/item?id=40815097",
    "commentBody": "Maker of RStudio launches new R and Python IDE (infoworld.com)171 points by javierluraschi 21 hours agohidepastfavorite103 comments SassyBird 9 hours agoAre they going to drop RStudio? I very much prefer its Qt interface over whatever VSCode invented. It’s fast, has nice keyboard shortcuts, none of that pointless padding and it just feels great to use. reply thomas_mock 5 hours agoparentHey! Product Manager for RStudio here (and Positron). We have no plans to stop development or maintenance on RStudio, and are committed to it for our users, both paid and community. While Positron and RStudio have some features in common, some R-focused features will remain exclusive to RStudio. If you're currently using RStudio and are happy with the experience, you can continue to enjoy RStudio. RStudio includes 10+ years of applied optimizations for R data analysis and package development. Cross-posting the FAQ: https://github.com/posit-dev/positron/wiki/Frequently-Asked-... reply dcreater 3 hours agorootparentI was a huge fan of RStudio and was pretty much the biggest reason I used R. But then I realized how bad R is, syntactically, and how much more useful Python and it's ecosystem are. Then I discovered VS code and Jupyter notebooks in VS code which completely sealed the deal. So unless you are in need of specific R data science packages, Python seems like the way to go. I'm quite excited to try Positron! reply extr 1 hour agorootparentI don't know. I bought into the Python hype and after a few years I've found myself missing R. If you're using the full python ecosystem more power to you...but for straight up data analysis and statistics, R is unbeatable. reply SassyBird 5 hours agorootparentprevIt’s great to hear. Thanks. reply wodenokoto 8 hours agoparentprevIt's not QT. The interface is HTML+css via node. reply SassyBird 8 hours agorootparentIt’s Qt: https://github.com/rstudio/rstudio/blob/main/src/cpp/desktop... It seems with some JavaScript generated from Java via Gwt. Regardless, I prefer it over VSCode UI. reply jmcphers 3 hours agorootparentRStudio was based on QtWebKit, then migrated to QtWebEngine, then finally migrated to Electron (which is what it uses today). You'll find some vestigial Qt code in the repository but it isn't used for the shipping releases any more. reply gepost 17 hours agoprevWhat is the strategy behind the dizzying pace of product changes in the R/Python space? I heard that RStudio was a great product, yet they have to redo everything again, of course also renaming the company as is also standard practice in the Python \"scientific\" market. Is it Jupyter envy? Why is it not possible to keep one good product and stay with it? I wish MatLab licenses weren't so expensive, at this point I'd just buy one and sit all this churn out. reply juujian 17 hours agoparentAs a regular R/bash/python user, I think the Posit philosophy is increasingly to break down barriers between the languages. It's no longer an either/or relationship, you can use both. And some packages or features that have been ported to python are just objectively worse in that language, so the want for R is definitely there. Also, a lot of the Posit team are fully \"bilingual\", it's not like the old guard of academic R contributors. My impression is they appreciate both languages for what they have to offer. For me, I'm apathetic to the languages, the only thing I care about is the output. reply ImaCake 15 hours agorootparent>ported to python are just objectively worse in that language This is absolutely the case. Dplyr syntax is much more intuitive for many use cases than Pandas or Polars equivalents. One thing I miss from RStudio is the Rmarkdown documents with inline outputs. Jupyter notebooks, even in VSCode, are so needlessly over-engineered and under-featured compared to the elegance of RMarkdown. So I am excited to see what Posit can do to bring that experience to python. My git repos will be thankfull anyway. reply asdff 14 hours agorootparentI don't even know why people need to use dpyler and the tidyverse, in my opinion R is very comfortable for data wrangling and making all kinds of plots out of the box. Its able to handle huge amounts of data as well especially if you adopt a functional programming approach vs object oriented (what I see with a lot of the classic \"academic\" brittle hardcoded slop that R gets a bad rap for). Very fast if you keep in mind it is a vectorized language and write your scripts with that perspective. Tidyverse seems like a new, unrelated syntax to learn on top of it all, whereas the base graphics packages very much work like the base statistical functions and the base data wrangling and everything else in the base R package. reply aftoprokrustes 9 hours agorootparentMy feeling is that people with a more mathematical background tend to like developping DSLs that look more like math than code, and is typically written once and then thrown away; whereas people with a more software engineering background tend to prefer code that is more explicit about what it does, and have a better understanding about long term implications for maintenanability/extensibility. Which for me is the summary of the R versus Python debate in general. One can see that in the JVM world with java vs scala: people attracted to scala tend to like \"cute\" DSL, java people tend to be more careful with shiny new features. (This is an oversimplification, of course) Specifically for dplyr: it looks cute and tends to be easier to use in a REPL setting (you can build your pipeline step by step by running your command, looking at the output, get the command from history, add a step, run again; and at the end you get a single line to copy paste in your script). But if you want to wrap it in a function, it tends to create issues. reply SassyBird 9 hours agorootparentprevThe base graphics packages make the plots as ugly as the ones generated by gnuplot though. ggplot2 on the other hand has very pretty output. And the concept of grammar of plots just makes so much sense to me. reply vegabook 10 hours agorootparentprevBase graphics are also _massively_ faster than ggplot when data sizes get larger. To the extent that ggplot essentially becomes unusable. reply ImaCake 8 hours agorootparentprevMaybe it is for you, but the success of Dplyr and ggplot suggests a lot of others disagree. reply asdff 1 minute agorootparentHardly. Hand holding tools are popular but that doesn't mean they aren't hand holding tools that don't give you any new function you didn't have otherwise. Jupyter notebooks are probably more popular to write than python scripts for new data scientists too, doesn't mean anything though or take away some of the advantages you get writing properly packaged scripts instead of a big old notebook you iterate a pipeline in line by line and figure by figure. ds_opseeker 5 hours agorootparentprevI wonder how much of this is just a feedback loop; were people taught both tools and then chose the one that works best, or was one more heavily promoted than the other, so people went with what was easiest to get started? reply asdff 0 minutes agorootparentIts definitely a feedback loop. Every time you look up an R question on stackoverflow people give you a ggplot or dpylr answer and usually not a base package implementation. Its almost as bad as Ole Tang spamming gnu parallel on every xargs thread. ImaCake 4 hours agorootparentprevIm sure that’s part of it. But you could say the same for using python or R over another language. Besides, someone who knows R well enough to write DplyR thought the situation was dire enough to write it. And there’s also data.table but that is inscrutable to most folks and I have only ever used it for fread - which is 10x faster than any other method of loading csvs into R. reply padthai 12 hours agorootparentprev> One thing I miss from RStudio is the Rmarkdown documents with inline outputs This is already in Quarto! https://quarto.org/docs/computations/inline-code.html#:~:tex.... reply cd4plus 15 hours agorootparentprevI can't stand jupyter notebooks for several reasons. I've been using https://quarto.org/ and writing .qmd docs and really enjoy it. reply setopt 11 hours agorootparentprev> One thing I miss from RStudio is the Rmarkdown documents with inline outputs RMarkdown (Rmd) was recently developed into “Quarto” (Qmd), precisely because they now support Python as well. I’ve used it a bit and it’s excellent. reply jhbadger 4 hours agorootparentBut Rmd already supported python chunks (as well as other languages such as ruby, which seem to be missing from Qmd) reply wjholden 11 hours agorootparentprevRMarkdown really is great. I used RStudio/RMarkdown for almost all my homeworks, projects, and even papers with no code during my MS program. I now realize that it was Pandoc's Markdown mixed with LaTeX that I appreciated so much for scientific writing, but with RMarkdown you can easily call R and Julia. I don't remember invoking Python from RMarkdowm (maybe you already could in RStudio but I never did), so this will be a welcome addition in this new Posit program. reply archiewood 15 hours agorootparentprevYou might be interested in what we’re building over at Evidence.dev It’s basically RMarkdown for SQL reply xmcqdpt2 6 hours agorootparentNeat! I have to deal with splunk at work and every time I do I'm annoyed that they decided to create their own query language. reply bbkane 13 hours agorootparentprevA friend and I have been having a blast with Observable Framework. It seems really well put together- markdown + code blocks. See https://m.youtube.com/watch?v=Urf_bPFyhIk for a short demo reply dr_kiszonka 12 hours agorootparentprevJust curious, what Rmarkdown features do you find lacking in VSCode notebooks? I have the opposite impression, but I am probably missing something because I don't use RStudio as much as VSCode. reply gullywhumper 6 hours agorootparentprevThe creator of pandas Wes McKinney is on the Posit team and working on this project too. Gives some additional credence to the idea of the convergence of tooling. reply quasarj 13 hours agorootparentprevOne of those is not like the others reply steve1977 12 hours agoparentprev> Is it Jupyter envy? The funny thing is that the R in Jupyter actually stands for R (the language). It's Julia, Python and R. No need for envy. Of course, RStudio/Posit != R (at least in theory) reply 0cf8612b2e1e 17 hours agoparentprevI think the R people see the writing on the wall. Python has sucked all the air out of the room, and it is becoming increasingly difficult for them to target the R space exclusively. If you had no legacy or compliance requirements, are you going to start a new data project in SAS, R, or Python? Where are you going to find the most talent? reply dongobread 16 hours agorootparentI'm not sure what would lead to you believe this. I've worked in the data science/ML space for over a decade now and I see the majority of pure analytics projects started in R, including at big tech companies I've worked at recently. Of course, ML projects and other things that need to result in production-grade models are almost always done in Python. This is currently the most visible form of \"data project\" due to all the ML/AI hype, but it is far from the only data work going on. reply boringg 8 hours agorootparentIm curious about the people who use R in big tech companies that you've worked at. Were the R users the people who had just come out of school and still working using their academic dev environment before weening off? I always found that was the group who used R - kind of a use what you are used to until it gets out of step with the remaining workflow. I also would say that the amount of R I see is far less than python. reply disgruntledphd2 8 hours agorootparentSo, (speaking as someone who started with R and now predominantly writes Python), I think there's a bunch of things going on here. 1. R is 100% better for analytics work and statistical modelling. There's just no contest. 2. Python is much, much better for data getting (APIs/scraping etc) and dealing with non table-like data. Again, there's basically no contest here. 3. Software engineers hate R (in most cases), which means that it's easier to hand over work for production in Python. This leads to a situation where it looks like most of the prod-level work is being done in Python, but if you look under the covers you'll discover that most prototyping/analysis/exploration is done in R and then ported to Python if it works. Like, Python is a great language for lots of things, but it's pretty terrible for exploratory DS work (pandas is like the worst features of base R and base Python mashed together in an unholy hybrid). There's also the fact that all the NN stuff is predominantly Python, so lots of companies believe that they need Python people, which reinforces the stereotype. And finally, while I love R, Python has more guardrails, and it's harder to make an unmaintainable mess with it (relative to R). Particularly when people use all the various lazy evaluation packages that the tidyverse has used over the past decade (I once maintained a codebase that used all of these in different places, it was not a fun experience). reply greentxt 2 hours agorootparentOne of the better comments in this thread, I would only qualify that different levels of ability mediate much of the \"how hard is it to make an unmaintainable mess\" dimension. Dplyr/tidy code can be pasta, as can pandas, and there is really a whole new level of that given llm generated nonesense edited/tweaked by novices masquerading as seniors. Apropos this idea of a vs code competitor, I wish they would spend more effort on existing products. I find quarto frustratingly buggy and meanwhile see no reason to move my workflow from vscode to this new thing. Ymmv reply aydyn 15 hours agorootparentprevYoure wrong. Python is outpacing R in usage. Every metric you can find proves it. R also has fundamental issues and lacks serious development. reply vixen99 6 hours agorootparentNot to dispute because I have no idea so I'll assume you're correct. But how many metrics did you find and how were they obtained? And how would you know they are representative of all R users? reply 0cf8612b2e1e 3 hours agorootparentFor whatever it is worth, the TIOBE index lists Python as #1, R at #21. Python is the first language many people are exposed to today. It has a library and tooling for every use case. https://www.tiobe.com/tiobe-index/ reply stoperaticless 14 hours agorootparentprevs/serious/hyped reply aydyn 13 hours agorootparentNo. R fundamentally has not really improved in the past ~10 years. Do you know much about how R works? Also try: gsub('serious', 'hyped', x) reply openrisk 13 hours agorootparentprevThe issue is that even if you peel the hype (which is a fact), python is still far larger. If you check e.g. the journal of open source software (which does not have much ML/AI bias), most of the papers are python, with an occasional R and julia submission. reply esalman 16 hours agorootparentprevI have an impression that most SotA algorithms in many fields that are not deep learning are made available first as R (or even Matlab) package. Obviously they do get ported to python once they receive enough traction. reply greenpresident 8 hours agorootparentThis has been my impression in social science academia: a lot of ready to run methods are released as R packages. reply kyawzazaw 16 hours agorootparentprevthe problem is life sciences and statisticans are not going to learn Python though. and that's the talent reply asdff 14 hours agorootparentOn the ground however, they learn both. Or I should say if you find someone who is exclusive to one or the other language (which happens), they aren't much of a programmer to begin with. There is a whole food chain essentially in life sciences in academia with computer knowledge. There are those who write the tooling, who are perhaps so abstracted they don't know the underlying biology and vet their tooling based on simulated data and a comparison with existing tooling, toiling in their own castles on tooling that might not ever see a real dataset. There are those who use the tooling to create novel pipelines to analyze data and draw conclusions based on their own or their collaborators literature research or life science perspectives, they might not care if the finding is truly novel or if its merely proving an existing gumption with a tool that hasn't yet been applied to an existing dataset. Then there are also those who run the pipelines created by others within their research group, sometimes others who have long left that given research group, with brittle hardcoded paths and other \"DO NOT TOUCH\" segments in a massive single 2500 line file that gurgitates some plots from a standard sort of csv file as input. reply _Wintermute 11 hours agorootparentprevThey are though. I work in computational biology, I would say the majority of work I see now is in python. reply dnawlp 11 hours agorootparentprevStudents I spoke to were forced to use SAS. They looked at me as if I was from Mars for suggesting alternatives. For a \"big data\" project, people will probably use Python (though Google apparently retreats from it except in machine learning). Why could you not use R and C++ or Java though? For example, Arrow has bindings for both, so the argument that Python is needed to shovel/scrape/steal data becomes less and less valid. reply dcreater 3 hours agoparentprevVS code has been a pillar all along? MATLAB is utter trash on macos, slow and a dated ui. Also why support proprietary languages with predatory marketing tactics reply manilbeat 17 hours agoparentprevRstudio came out a long time ago at this point. I actually think it feels quite dated personally. A data science focused version of VS Code with some kind of notebook sounds rather awesome to me. reply esalman 16 hours agorootparentI agree, Rstudio is really dated. Been using R and Python at work in parallel, I find Rstudio cannot do obvious things that R extension for VS Code can- like stopping at a breakpoint in the middle or knitting an Rmd file that has embedded code. reply asdff 14 hours agorootparentprevThere is already Rmarkdown and great tutorials to go along with that for the budding R data scientist reply lbeltrame 13 hours agorootparentprevDoes it still bundle an entire, old version of libclang? reply aydyn 15 hours agoparentprevThe issue is R is too small compared to Python and the gap is getting bigger. Theyre trying to grow the company, and realistically the only way is to support python. reply kkfx 9 hours agoparentprevModern software can't work, that's is and no, I'm serious and have no intention to start a flame. Original desktops was a single OS-environment-framework, witch was \"fragile\" to a certain extent, but anything have to evolve in a fully integrated environment, this means FAR LESS code FAR LESS deps for anything. This means that's easy for the user to bend the environment to their wishes and for devs to create something \"on the shoulder of giants\" being with them instead of having something like https://xkcd.com/2347/ modern projects are typically written in Silicon Valley mode, anchoring the project and some deps without any reasoning about they future, scalability, maintainability and so on. Something change, anything on top collapse. In practice we have nearly ZERO development for desktop apps, simply because modern desktops are still widget based stuff who was sold a \"what we need\", against the complexity of classic DocUIs, and then we migrate to modern web witch is a bad DocUI, so developing for the desktop is simply a nightmare. To add features we can't much \"use the environment\" so all apps tend to try doing anything inside evolving toward unmaintainable monsters no one can handle their codebase and at a certain point in time they became a kind of a framework where \"features\" became \"ideas of someone\" without a coherent vision or a target \"for the application\" like Eclipse from an IDE to a platform some use to code, some others to pay taxes (yes, Italian gov. have made Desktop Telematico witch is a custom Eclipse to fill taxes). As the classic Greenspun's tenth rule we witness the same: we damn need an OS as a single user-programmable application like Emacs or doing ANYTHING is a nightmare and there is no long lasting solution. reply teruakohatu 19 hours agoprevThis looks great, combining the best of VSCode and RStudio. I prefer coding in VSCode but prefer data exploration in RStudio. One issue with this is the lack of copilot. Copilot can be installed on VSCodium [1] but it breaks often. The other is MS’s proprietary Remove Development extension that enables a lot of functionality in VSCode. There is an open equivalent but I haven’t tried it [2] [1] https://github.com/VSCodium/vscodium/discussions/1487 [2] https://open-vsx.org/extension/jeanp413/open-remote-ssh reply adityamwagh 19 hours agoparentHow are you guys using Copilot? I feel it’s a PITA and interferes in between everytime I try to use it. How do you use it properly? reply NewJazz 18 hours agorootparentMy employer is asking me to try out Copilot to see if it offers a lot of value. IMO about 90% of the time it is an annoying interruption that offers to help me write a few characters (which I could probably write faster if I wasn't being interrupted) or maybe a few lines at most. 5% of the time it offers downright incorrect suggestions, and about half of those suggestions are only subtly incorrect, so I have to be on-guard about anything I use it for. 5% of the time it offers suggestions that I didn't consider, but are definitely a positive contribution. E.g. checking input parameter validity. I've heard that one way to use it effectively is writing a detailed comment about what you want to do, then let it suggest the code. I personally don't like those style of comments, so I'd have to: - enable copilot if I have it disabled (I have a keymap for this in vim) - write the comment - carefully review that the suggestion is correct and complete - accept the suggestion, then go back up to delete the comment kinda inconvenient, but if I was blanking on a bunch of stdlib functions maybe it would help? But accepting the copilot suggestions doesn't add imports, whereas accepting language server suggestions often does (e.g. with gopls). reply NewJazz 18 hours agorootparentSome examples of wrong suggestions... things like suggesting OpenIDConnectClientCredentialsFlow when the actual name of the class is just OpenIDConnectClientCredentials. Language servers will have the correct suggestion, and Copilot is just a bother and a hindrance in that case. reply setopt 8 hours agorootparentprev> I've heard that one way to use it effectively is writing a detailed comment about what you want to do, then let it suggest the code. I personally don't like those style of comments I do the same as you: write such comments, let Copilot draft the code, then I delete the comment. Since the comment is detailed, I think of such use of Copilot as a “pseudocode to code compiler”. reply mistrial9 18 hours agorootparentprev> My employer is asking me to try out Copilot this says more about your employer than CoPilot ? reply NewJazz 18 hours agorootparentWdym, the whole comment or just that statement? If you meant the statement, then yeah. I was just offering the context for my usage of copilot. Essentially a solution in search of an unstated problem. reply mattgreenrocks 17 hours agorootparentIt’s amusing to me to see companies asking their devs to play with AI to try to find some kernel of value there. Meanwhile, if said devs do that playing in other languages/frameworks, they’re chastised for not focusing on business value. reply mistrial9 2 hours agorootparentfamously in San Diego IIR, a large corporation of some kind announced outsourcing and layoffs.. then proceeded to require the employees to train the new outsourced people for more than a month! dimly recalled from business news after the 2008 meltdown reply esalman 16 hours agorootparentprevI work somewhere where they actively encourage copilot, and I actually have some good use-cases. Most of the time it is just autocomplete on steroids. Scanning the suggestion and hitting tab is almost always faster especially if you have to write a lot of repetitive code. Generating useful documentation. I work a lot with legacy codebases. Oftentimes I use copilot to explain what a chunk of code is doing, or give it a requirement and see what it generates based on context. At this point I think working with a legacy codebase as a new maintainer would be a lot harder if I didn't have copilot. reply remus 7 hours agorootparent> Most of the time it is just autocomplete on steroids. imo this is kinda underrated, I use it all the time and it's a real time saver for me. It seems to have a good recall for things you've been doing recently, so it'll make suggestions based on a method I just added or a variable I just declared. Another fun one I had recently was implementing elo ranking. I'd done some reading so knew what I was doing, but based on the function name and comment it generated the whole thing for me, including all the correct argument names and object properties. Watching it pop up line by line was very cool! reply setopt 11 hours agorootparentprevI find the best way to use it is the chat interface. In VSCode, I can e.g. have the cursor on a Matplotlib command, press Cmd-I to open Copilot Chat, and say “make this plot an inset in the top-right corner of the previous plot above” and it will do it. The other way that works OK is when drafting new code, I can write some comments about what I want to do and trigger Copilot via a keybinding to draft a first version of the code. It can be useful when working with new libraries since I often don’t know what functions to lookup in the documentation. reply jasonjmcghee 14 hours agorootparentprevManual trigger via shortcut key made it a useful tool I reach for in certain contexts. Automatic not being useful was the least of the issue, it used crazy CPU, slowed down the IDE, and drained my battery too. reply poulpy123 11 hours agorootparentprevI use it as auto completion, and ironically to discover/have an overview of librairies instead on trying to find the good source between all the AI generated slop websites that Google propose reply teruakohatu 18 hours agorootparentprevMostly for boilerplate things like assigning a bunch of thing to a dictionary, it usually knows what I am thinking. reply NewJazz 18 hours agorootparentHave you ever stopped to consider why you are writing code like that, and if it is necessary in the first place? I think if you are spamming a lot of code that looks like: thing = params.get('thing') thong = params.get('thong') ... thunk = params.get('thonk') then you probably aren't delivering a lot of value with your code. In fact that overly verbose code could be a liability because it has to be reviewed (possibly multiple times as people look over the code to see how values are getting assigned). reply klibertp 4 hours agorootparentTry not to write code like this in statically, nominally typed languages (Java, Kotlin, C++, etc.). Without a good compile-time metaprogramming, you're bound to need code like this sooner rather than later [EDIT: you still should try to minimize it; it shouldn't be the first solution you reach for, but it really is unavoidable in many cases]. It's a PITA, but that's just it is in those languages - and Copilot works pretty well as an ad hoc code-gen tool. reply teruakohatu 8 hours agorootparentprevYes I have considered that, and I consider it every time I write code. When you start using LLMs you realise how really is boilerplate. Error checking, unit tests, if statements, loops, so much code is boilerplate not just badly written code. reply jmcphers 19 hours agoparentprevopen-remote-ssh doesn't currently work with Positron because Positron does not currently build the bundles that need to be installed inside the remote host. We're definitely interested in making this work, though; stay tuned! reply teruakohatu 18 hours agorootparentThat is great to know. I work in an academic environment and would gladly give feedback on this from the perspective of teaching post grads. Feel free to contact, email in hn profile. reply mbreese 18 hours agorootparentprevThat’s been my problem with any non-MS vscode distribution. Missing Remote-SSH and devcontainers. Both of which I tend to use a lot — especially remote ssh with large datasets that exist on remote servers. Otherwise, I’d be all in for this! reply teruakohatu 18 hours agorootparentYes I agree, I forgot about dev containers. If I am doing ML most of the time I am using a server GPU with a large dataset. This is one of the use cases that made me use VSCode more than RStudio. When I have really wanted to use RStudio I use the web version, but its less pleasant than a simple ssh remote. reply ZunarJ5 16 hours agoparentprevhttps://tabby.tabbyml.com/ reply bluenose69 10 hours agoprevI would expect a bit more backwards compatibility for R packages. When I open a package.Rproj file with RStudio, it has GUI elements to build the package, to test it, etc. When I open it with positron, it is treated like a text file, at least as far as I can tell by looking at the many icons and pulldown menus. It is a weird choice, making a new application that cannot handle the key file type from its ancestor. reply jmcphers 3 hours agoparentPositron isn't supposed to be a replacement for RStudio for all use cases (especially not this early in its lifecycle!). If you open your package folder in Positron (as opposed to the Rproj file itself) I think you'll find it has most of the commands you need for package development in the Palette. We do hope to add better GUI tooling for project-level actions; more info here: https://github.com/posit-dev/positron/issues/1486 reply TomMasz 7 hours agoparentprevThat's unfortunate. I've got a personal project that I'm working on at the moment that mixes Python and R and I'm too far in now to make starting over from scratch viable. reply legobmw99 16 hours agoprevIt seems a little odd to me that this is not just… a vscode extension pack? reply jmcphers 12 hours agoparentA good question. The VS Code extension API is pretty powerful but extensions run separately from the main workbench process and they can't draw any meaningful UI on it. This was a great design decision IMHO as it is the core reason VS Code has a reputation for a minimalist UI and good performance despite being based on Electron. However it also made it impossible to build the kinds of experiences we wanted to with Positron. Positron has a bunch of top-level UI as well as some integrated/horizontal services that don't make sense as extensions. We built those into the core of the system which is why a fork was necessary. It's a goal for Positron to be extensible, so it has its own API alongside VS Code's API, and both the R and Python language systems are implemented as extensions. reply i-am-grout 9 hours agorootparentI'm curious if you would be willing to elaborate on what your plans are for longer term feature parity with vscode? As in I can imagine as vscode receives continued development and new features you will have the development burden of having to integrate these updates into your fork. Are you planning to keep up-to-date with vscode or will the products essentially drift a part over time? If the latter would this mean extension developers have to build separate extensions for your IDE? reply jmcphers 4 hours agorootparentWe merge upstream from VS Code every month and we plan to keep up to date with it, so extensions will continue to work and we'll continue to inherit new features as they become available. It's a development burden for sure -- but still an order of magnitude cheaper than trying to build a good workbench surface from scratch. reply josephcsible 17 hours agoprevWarning: This is released under the Elastic License, which for obvious reasons is neither free software nor open source: > You may not provide the software to third parties as a hosted or managed service, where the service provides users with access to any substantial set of the features or functionality of the software. > You may not move, change, disable, or circumvent the license key functionality in the software, and you may not remove or obscure any functionality in the software that is protected by the license key. https://github.com/posit-dev/positron/blob/main/LICENSE.txt reply kleiba 20 hours agoprevSpeaking of which: does anyone here on HN feel comfortable recommending a Python IDE that's half-way bearable on iPads? reply Terretta 14 hours agoparentPythonista is nicer but ships older Python: https://omz-software.com/pythonista/ Pyto is maybe less approachable but more up to date, with clang compiler and LLVM bitcode interpreter: https://pyto.app/ Juno is Python notebooks: https://juno.sh/https://juno.sh/ In general I prefer Blink Code: https://docs.blink.sh/advanced/code reply darkteflon 13 hours agorootparentYeah Blink with the built-in VS Code is so far ahead of other options. Being able to remote back to your machine and pick up right where you left off is unbeatable. reply Fwirt 20 hours agoparentprevHaven't tried it yet, but personally I'm very interested in Pythonista which also exposes an impressive number of iOS APIs (bluetooth, camera, etc.) that you can use in your scripts. No idea how it functions as an IDE though. reply shmoogy 20 hours agoparentprevHosted vs code server is what I used to use: https://github.com/coder/code-server They've added support in blink as well which is my favorite iOS purchase for productivity on my iPad https://blink.sh/ reply setopt 11 hours agoparentprevNot an IDE, but “Carnets” gives you a local (i.e. offline) Jupyter installation on iPadOS that includes NumPy, SciPy, Matplotlib, etc. reply spratzt 11 hours agorootparentI second this recommendation. I used Carnets for the Advent of Code last year. I ran out of steam long before it did. reply justinclift 13 hours agoprevJust noticed this: Because Microsoft does not allow third-party IDEs to access the official VS Code Marketplace ... Anyone know why? My wild guess is it means MS doesn't want third parties to build their own VS Code based IDEs (like this one)? reply jmcphers 13 hours agoparentThere's some thoughtful speculation on this here: \"Visual Studio Code is designed to fracture\" - https://ghuntley.com/fracture/ reply lembcva 10 hours agorootparentVery interesting, I hope the (formerly) RStudio people read this. So they are giving up an IDE (RStudio) that was famous for far better graphics and plotting than the Python alternatives for a half-open third party solution where Posit is the sharecropper. This Microsoft \"DevDiv\" (see the link) sounds like the classic EEE dressed up as \"open\", \"hip\" and with all the right buzzwords. reply nickstrayer 18 minutes agorootparentThe person that posted that link is the lead architect on Positron. So yeah it's certainly known about! reply pjmlp 10 hours agoprevVisual Studio Code is the new Eclipse it seems, now everyone and their dog are shipping IDE distributions that are just re-packing plugins. reply darkteflon 13 hours agoprevRelatedly, has anyone found a really good extension for interacting with .tsv, .csv, files in VS Code? PyCharm is much nicer on that front. reply setopt 11 hours agoparentDid you try the Excel Viewer? It shows CSV files as a table. Doesn’t handle huge files though. reply prometheon1 6 hours agorootparent+1 Excel Viewer user, I've heard good things about Data Wrangler as well reply conradolandia 2 hours agoprev [–] Spyder-IDE is a better option reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Posit, formerly known as RStudio, has released a beta version of Positron, a new Integrated Development Environment (IDE) for R and Python, based on Visual Studio Code.",
      "Positron is available for macOS, Windows, and Linux, and is designed to be user-friendly with built-in support for R and Python, eliminating the need for additional extensions.",
      "The IDE includes a data and variable explorer for easy data manipulation and supports other VS Code extensions via the OpenVSX registry, though it is still in early development."
    ],
    "commentSummary": [
      "The maker of RStudio has launched a new integrated development environment (IDE) called Positron, which supports both R and Python.",
      "Despite the new release, RStudio will continue to be developed and maintained, with some R-specific features remaining exclusive to it.",
      "Positron aims to bridge the gap between R and Python, offering a unified environment for data analysis and package development."
    ],
    "points": 171,
    "commentCount": 104,
    "retryCount": 0,
    "time": 1719522351
  },
  {
    "id": 40817079,
    "title": "The Death of NYC Congestion Pricing",
    "originLink": "https://www.apricitas.io/p/the-death-of-nyc-congestion-pricing",
    "originBody": "Share this post The Death of NYC Congestion Pricing www.apricitas.io Copy link Facebook Email Note Other Discover more from Apricitas Economics Data-driven Insights on Economics, Business, Finance, and Public Policy. Over 42,000 subscribers Subscribe Continue reading Sign in The Death of NYC Congestion Pricing And What is Says About the Institutional Problems in American Cities & Infrastructure Joseph Politano Jun 17, 2024 82 Share this post The Death of NYC Congestion Pricing www.apricitas.io Copy link Facebook Email Note Other 27 Share Thanks for reading! If you haven’t subscribed, please click the button below: Subscribe By subscribing you’ll join over 42,000 people who read Apricitas weekly! New York City and its surrounding suburbs make up the largest urban economic cluster on planet Earth. 2.4 million people work in the 23 square miles of land that make up the borough of Manhattan, and collectively they produce $886B in economic output, a nominal sum larger than Vietnam and Pakistan’s GDP combined. Most of those workers take the subway. Globally, that is nothing notable—in most urban cores a majority of workers take public transportation for work and daily activities. Increasing the density of jobs, hospitals, restaurants, homes, schools, and more is key to the agglomeration effects that make cities such economic powerhouses, and as density grows mass transit becomes essential since it can far surpass the maximum throughput capacity of even the largest roadways. Yet in the US, it’s extremely unusual for most workers to take public transportation—in 2022 only about 3.1% of Americans took trains or buses to their jobs and about 45% of those transit commuters lived in the Greater New York area. Chicago, Boston, DC, LA, San Francisco, and Philadelphia are the only other major American cities where a substantial number of workers take transit, and in none of them is the transit mode share above 25%. New Yorkers’ inflated sense of self-importance is one of the longest-running jokes about the city, but it’s true that in this way the Big Apple is fundamentally America’s only major city. Thus New York’s public transit policy is fundamentally America’s public transit policy—and NYC’s urban development is in many ways America’s urban development. That’s why it was so important when earlier this month, New York State Governor Kathy Hochul announced that America’s first congestion pricing scheme would be “indefinitely paused”. The policy would have charged vehicles for entering the perpetually gridlocked streets of Lower Manhattan, using the money raised to then fund many of the city’s transit projects—and it was slated to officially begin operating less than 30 days from the announcement. In the short term, the flip-flop will be extremely costly for the city and state, but more broadly the cancellation and the dynamics that led up to it are emblematic of many of the problems holding back American cities & infrastructure. Subscribe Why Tax Congestion? In one sense, the congestion pricing scheme is a radical new experiment in American transportation policy. Cars compete for scarce urban road space with more efficient modes of transportation like buses, emit large amounts of noxious chemicals and particulate matter from tailpipes and tires, are the leading cause of death for Americans under 55, and in large quantities become disruptively loud for workers and residents. Lower Manhattan is among the few places in America where substantial transit alternatives to driving exist, so a congestion tax should induce mode shifts away from cars and toward the subway—to the benefit of the local and global climate. Wealthier residents are more likely to drive and poorer residents more likely to take transit, so congestion pricing serves redistributive purposes as well. New York’s scheme promised to be a pioneering first for the nation, a pilot project that if successful could be copied in cities throughout the US. Yet in another sense, the plan should be entirely unremarkable. Congestion pricing has existed in some form throughout cities across the world for 50 years, and is fundamentally just a fancy version of the normal tolling systems that are common on bridges, tunnels, and other roadways. People who drove into Lower Manhattan were already “paying” for the privilege to do so, just with the time and gasoline lost to bumper-to-bumper traffic rather than in hard cash. As with any heavily overcrowded bridge or highway, mispricing was causing economic inefficiency—charging for entry would allow priority vehicles to get where they’re going faster, incentivize better use of existing roadways, and open up a funding stream that could be used to expand beyond current infrastructure constraints. Economists would recommend some form of congestion pricing just for the benefits to road users alone. Plus, once implemented, these schemes are usually popular among city residents—public opinion of London’s congestion charge rose nearly 20 percentage points after implementation and Stockholm’s congestion tax was approved by voters after a 7-month trial period. In a more practical sense, New York’s Metropolitan Transportation Authority (MTA) needed new post-COVID funding streams. After ridership (understandably) cratered during the early pandemic, the system had to be bailed out by federal cash infusions from the CARES Act and American Rescue Plan alongside nearly $3B borrowed from the Federal Reserve. Remote work enabled shifts out of New York’s expensive real estate market and into the city’s suburbs and other metro areas, making it even more difficult for city bus & rail ridership to recover. Yet while transit ridership continues to struggle, MTA bridge & tunnel toll revenue has more than completely recovered to pre-COVID levels. People were increasingly driving into the city and underutilizing transit infrastructure—thus congestion pricing, while initially envisioned for a pre-pandemic world, rapidly became a necessary part of plans for a post-COVID recovery. Taxing traffic, with all its negative externalities, was certainly much more politically palatable and economically beneficial than attempting to raise payroll taxes further, which would have strained New York’s already-slow post-COVID jobs recovery. Plus, the incidence of congestion taxes would also fall more on tourists and residents of New Jersey/Connecticut, helping to recapture some of the economic value lost to the suburbs as a result of remote work. The policy should have been an easy win for the city. Subscribe The Death of Congestion Pricing Yet just before implementation, congestion pricing was indefinitely paused at a press conference framed around inflation and cost of living concerns, highlighting how much the tax side of the scheme loomed large in voters’ minds. That makes sense for drivers from Bergen County in northern New Jersey, for whom the system was (financially) all cost and no benefit—they would pay to enter downtown, but the revenue collected would all go to transit investments outside their county, and their primary benefit would thus only be experiencing less traffic when driving through Manhattan. But Governor Hochul does not represent New Jerseyans, and for New Yorkers (especially the 55% of city households who don’t own a car), the tradeoff was supposed to be clearer—impose costs on a small subset of drivers to deliver benefits in the form of transit investments that would serve the majority of people. Those benefits, however, felt relatively small to many city and state residents. A lot of congestion-pricing criticism posits that the MTA already has “enough money” and just needs to spend it better. In many ways, they have a great point—the MTA has a total operating budget of $19B, an amount larger than many states, plus a capital investment budget of roughly $11B a year, and they build relatively little with it. High costs for new public transportation infrastructure is an America-wide problem but is at its absolute worst within the city—of the 5 most expensive projects tracked by the NYU-Marron Institute Transit Costs Project, 4 are in New York City. It would be great if New York could build transit infrastructure at something approaching the cost-per-mile of other dense urban areas in much of Asia, continental Europe, or South America, but even reaching the still-boondoggle levels of the Bay Area Rapid Transit extension to San Jose would be a massive improvement. Even as billions are spent, construction takes much longer to complete in NYC—in the 15 years it took to complete the East Side Access project, the Delhi Metro in India added 7 new lines and increased ridership by more than 1B trips per year. These MTA projects are still worthwhile—cost-per-rider projections for NYC transit actually tend to be relatively benign (by the very low standards of American transit) because the city’s expensive projects are still moving through some of the densest neighborhoods in the country and link to an extremely comprehensive existing network. But high costs force New Yorkers to settle for a much lower-quality, lower-capacity system. Take the Interborough Express, a planned orbital transit line between Brooklyn and Queens designed to improve access in transit deserts throughout the city and connect with 17 existing subway lines. The line will be lower capacity light rail, not the kind of heavy rail that the subway runs on, will mix with aboveground street traffic throughout some subsections of Queens, be built mostly by repurposing preexisting freight right-of-way, and is still projected to cost $5.5B. The fact that with billions of dollars in new revenue the MTA could only promise marginal expansions rather than transformative system-changing investments reduced the perceived benefits of the congestion pricing scheme—allowing narrow dedicated opposition among a portion of suburban drivers to topple the program. Yet I think congestion pricing opponents miss how their victory is emblematic of—and exacerbating—the city’s transit cost problems. The Governor is turning the MTA into a nightmare customer and a horrible coworker here, as the cancellation of congestion pricing jeopardizes key funding and matching federal dollars for a suite of construction projects at the 11th hour. The MTA’s CFO was forced to announce that the agency will now need to “reorganize the [2020-2024 Capital] Program to prioritize the most basic and urgent needs” because it “cannot award contracts that do not have a committed, identified funding source.” That means tens of billions of dollars in investments could be shelved, including Phase II of the Second Avenue Subway, the Metro-North Penn Station Access expansions, a host of station upgrades, and much more. Contractors who work with the MTA on these projects already charge a premium for having to manage the substantial risk that construction is delayed or canceled—introducing “the Governor might suddenly decide your job is over” as another risk to worry about will just increase those cost premiums, thin the pool of companies who are capable of working with the MTA, and reward the politically savvy. The fact that infrastructure plans can go through years of study, environmental review, public consultation, pilot projects, revisions, lawsuits, construction, delays, and more only to then be shelved days before final implementation is a large part of why public works are so expensive in the first place. The number of veto points for American infrastructure is extremely high—even one “no” across from any number of officials throughout construction can stop a project dead—and that veto power makes it easy to extract money and concessions from projects. Pick more costly construction methods to minimize surface disruption, reduce daily construction time to leave existing traffic unimpeded, move infrastructure to suboptimal locations to appease powerful voting constituencies, expand stations to meet interagency demands for back office space, and in the absolute worst cases just use discretionary powers to do quid-pro-quo corruption. Also, imagine if you were among the hundreds of state, local, and federal civil servants who worked on any part of the congestion pricing or MTA capital investment plans—would you want to remain at your job after having years of work tossed aside at the last minute? The hollowing out of institutional capacity within transportation agencies is a serious driver of cost problems, with planning becoming a bespoke project-by-project activity heavily dependent on expensive outside consultants rather than a regular in-house activity. As a more practical matter, hundreds of millions were already (over)spent on congestion pricing between years of studies, protracted legal battles, construction of physical infrastructure, contracts for implementation, and much more—all that money is wasted by pulling the plug at the last possible second. The cancellation of congestion pricing without an alternative funding stream will hurt the MTA’s credit rating, increase its debt-servicing expenses, and functionally impose even more costs on New Yorkers. Before the congestion pricing cancellation, the MTA had actually made some rare positive efforts to control infrastructure construction costs, including saving $1.3B in Phase II of the Second Avenue Subway project, yet instead of building on that the Governor has decided to throw the entire infrastructure program into jeopardy. Subscribe Talking Broadly About American Public Transit America invests little into public transportation infrastructure by the standards of similarly-wealthy European and Asian countries (or even relative to Canada and Mexico). Pre-pandemic, it was normal for urban mass transit to receive only 6-9% of all state and local land transportation spending, and for bus and train passenger terminals to receive roughly another 3-5%. However, relative spending on both items has cratered post-COVID, with the share of construction spending going to mass transit falling to the lowest levels in a decade this year. Even when these investments are made, American cities struggle to make the best of new infrastructure. Public transit is a network system that depends on scale to be successful—a two-line metro is more than twice as valuable as a 1-line metro, and a metro that goes between dense downtown neighborhoods is much more valuable than one that meanders through sprawly suburbs. A large part of why American transit infrastructure projects fail is because even when built they are placed in the middle of nowhere or are undercut by local governments that prevent the necessary complementary neighborhood housing and commercial developments. Take the new Silver Line Extension of Washington DC’s Metro—stations like Loudoun Gateway are currently transit to nowhere, situated near a field and highway interchange just outside Dulles Airport, and thus serve only a couple hundred riders per day. Meanwhile, existing stations that were coupled with substantial housing development, like Navy Yard-Ballpark, had some of the fastest-growing ridership of the entire system pre-COVID. The immediate vicinity of the North Berkeley BART stop in California is surrounded on all sides by parking lots, and thus the station currently sees only about 1,700 riders per weekday. A redevelopment is finally planned for the land surrounding the station, but this will still likely bring less than 750 new homes to the area. In New York, the high cost of the East Side Access project would be more palatable if more progress had been made on Sunnyside Yard—a plan to build housing and a new station on top of an existing train yard like in the Hudson Yards redevelopment—and if the proposed number of housing units hadn’t already been scaled back. More broadly, over the last 6 years more housing has been built across the Harlem River in the Bronx and across the Hudson River in Jersey City than in the extremely transit-accessible and high-demand neighborhoods of Manhattan. The city’s housing policies are, among a myriad of other issues, making the MTA less effective—had the city built more in the 2010s, COVID outmigration wouldn’t have been so severe and the MTA’s network could draw from a much stronger ridership base. The defeat of reform efforts like Governor Hochul’s housing compact, which would have allowed for more dense housing construction around transit stations, are entrenching this extremely flawed status quo. These cost and ineffectiveness problems are also mirrored in another infrastructure area where spending is actually increasing—intercity rail. State and local spending on rail construction has hit a new record high post-COVID, rising above $3B a year for the first time. The project most emblematic of this construction surge is California High Speed Rail, which aimed to be America’s first modern HSR system but has hit a long series of snags since it was first authorized by voters in 2008. Slow land acquisition, lengthy environmental review, and repeated legal battles continually delayed the project and increased its costs. Along with other issues, this has meant the line is only scheduled to start operation on a segment between Bakersfield and Merced in the early 2030s, and the full price to connect San Francisco to LA is likely to exceed $100B. At current cost estimates, the Bakersfield-Merced initial operating segment is already expensive by global standards and the SF-LA line would be among the most expensive high-speed rail projects in history. This is with generous assumptions for CAHSR—especially the assumption there won’t be further cost increases when construction starts on more difficult rail segments—and still the only lines with similar price profiles are those built largely underground or those in the UK where cost inflation is often even worse. Even accepting that the initial operating segment can open on schedule in the 2030s, there is still no timeline for the project to connect to either San Francisco or Los Angeles, and another source of funding will need to be found before construction to those cities can continue. While not complete transit-to-nowhere, the fact that the highest-speed train in the Americas will be only shuttling people between some of California’s smaller car-dependent cities for years is emblematic of the struggles of US transportation planning. Good infrastructure has large diffuse benefits and small narrow costs, but with enough twisting you can make the benefits smaller and the costs larger. Subscribe Conclusions The practical future of the congestion pricing scheme remains up in the air. There are inevitable legal battles that will have to be settled regarding Hochul’s attempt to pause the project, and the appetite for a payroll tax increase seems to be low, leading to talk of raiding the state’s general fund or issuing more debt as a stop-gap until another source of workable revenue can be found. Uncertainty is extremely high, but congestion pricing is not completely buried just yet and the lack of viable alternatives may push the Governor’s Mansion to eventually revive the scheme. For New York, a permanent killing of congestion pricing would help solidify the state’s position as a shrinking share of American jobs, residents, and GDP in the post-COVID world. These all still might just seem like local political problems, especially if you live outside NYC or are among the vast majority of Americans who aren’t reliant on public transit infrastructure. However, many of the issues facing urban public transportation plague all forms of US infrastructure spending. Real per-capita investment in all mass transit (including airports) has been stagnant for 20 years—but real per-capita investment in highways & streets is also stuck at pre-WWII levels. Cost-per-mile for US highway construction has skyrocketed over time, especially in wealthy areas with stronger local opposition, America just more readily pays those higher costs rather than the higher cost for transit infrastructure—which is why it is necessary to get cost problems under control rather than just abandon the idea of transit infrastructure entirely. American transportation agencies have a notable reluctance to adopt best practices from abroad, yet even within the US there are valuable success stories to learn from. Los Angeles is the city most synonymous with American highway urbanism, but LA Metro is currently engaged in one of the most transformative transit expansion projects in modern American history for a high-but-not-ludicrous price tag. Meanwhile, DC’s Metro has shuffled through a series of operational reforms under new leadership that have dramatically improved ridership for a system that was extremely at risk given how many city residents now work remotely. The death of congestion pricing was among the largest setbacks for American urban infrastructure since the start of the pandemic, but it’s still necessary and possible for US cities to expand and improve their transportation networks. Thanks for reading! To receive more new posts on economic data analysis, consider subscribing! Subscribe 82 Share this post The Death of NYC Congestion Pricing www.apricitas.io Copy link Facebook Email Note Other 27 Share",
    "commentLink": "https://news.ycombinator.com/item?id=40817079",
    "commentBody": "The Death of NYC Congestion Pricing (apricitas.io)159 points by gmays 17 hours agohidepastfavorite321 comments asoneth 6 hours agoWhen I lived in NYC many years ago I'd be walking, biking, driving, at a park, dining in an outdoor cafe, in my living room, etc and perpetually surrounded by the noise, smell, and inconvenience of gridlocked traffic and road construction. I eventually tuned it out, but every time I left and came back it drove home how ridiculous it was. I eventually moved to New England because it became clear that despite the city having good bones, the leadership (and many voters) were more interested in coming up with ways to make it convenient for drivers to drive and/or park in every neighborhood in the city than making it a pleasant place to live. The ghost of Robert Moses influences everything there. So I was floored when it seemed like such an incompetently managed city as NYC was actually going to implement the obviously correct solution of charging money to allocate a scarce resource. What was less surprising was the fact that it was derailed by a state politician but only at the last minute and after hundreds of millions had already been spent. reply sidvit 4 hours agoparentI moved to the financial district a couple of years ago and I was very happy to find how anti car parts of it can be. I pretty much never hear any street traffic where I live. The narrow streets mean lots of places it isn’t covered in parked cars, there’s low to seemingly no traffic on the weekends, the closed streets around the exchange area is basically a big dog park after working hours. I’ve looked at a bunch of different apartments this summer with my lease coming up, but I don’t think I want to deal with the noisy other parts of lower manhattan reply karanbhangui 4 hours agorootparentFidi is the most underrated neighborhood to live in NYC. Most MTA lines converge there, very secure due to post 9/11 security, not much traffic and quiet at night, and slightly cheaper than many other Lower Manhattan neighborhoods. reply halfmatthalfcat 5 hours agoparentprevI lived on 2nd Ave for years in UES and biked to FiDi for work - cannot relate to what you're claiming here. Yes there's noise, it's NYC, but \"gridlocked traffic and construction\" was not something I'd normally run into. reply xvedejas 5 hours agorootparentAt least visiting other parts of Manhattan -- I'm thinking for example LES -- there is definitely constant traffic and exhaust. Doesn't really matter if it's gridlock or not, moving cars emit more sound and fumes than idling cars do. reply sokoloff 4 hours agorootparent> moving cars emit more sound and fumes than idling cars do Per car, per minute? Sure. Per car, per trip? Not clear, but probably not. Per block, per day? Also not clear, but probably not. reply xdavidliu 2 hours agorootparentwhat does a trip mean for an idling car? reply sokoloff 2 hours agorootparentA trip where a car drives some distance without idling in gridlock vs that same distance trip where the car spends significant time idling in gridlock in addition to time spent moving is the obvious answer. reply asoneth 1 hour agorootparentprevMy bike commute took me past a bridge entrance and a tunnel entrance in midtown which were both perpetually gridlocked, so I'll admit that probably colored my recollection a bit and it wasn't literally every street in NYC that was gridlocked. Still, when living in NYC I felt constantly surrounded by car traffic, far more than other cities I've lived in. reply CSMastermind 51 minutes agorootparentprevYeah completely agree - I spent 10 years living in Manhattan and don't recognize anything in the parent comment. reply joenot443 4 hours agoparentprevIt's interesting you say that, coming from Toronto, I was floored at how _well_ NYC traffic moves in comparison. Obviously no city is perfect, but suggesting that NYC is \"incompetently managed\" is a little funny to me. Of the other cities I've lived in, Seattle, Vancouver, SF, and Toronto, I've found the NYC city services to be by far the most effective. What US city do you think does a better job? reply Spooky23 2 hours agoparentprevThe problem is other stakeholders live in the provinces. NYC transit advocates don’t represent them. NYC is a regional megalopolis. The commentary here is always focused on the strawman terrible people commuting to lower Manhattan, but there are a million reasons why convenient travel to or through Manhattan is needed. It’s also unfair to characterize the governor of New York as “some politican”. The MTA and most river crossings are owned by public authorities controlled by the state, and the entirety of the system is dependent on New Jersey Transit and the multi-state Port Authority. Congestion pricing was a state initiative all along. reply asoneth 1 hour agorootparentThe drivers commuting into Manhattan aren't terrible, they're responding totally rationally to the relative monetary and time costs of driving into the city vs park & ride vs living closer. The terrible thing is that the incentives that have been established make driving the right choice for so many people. > The problem is other stakeholders live in the provinces. Exactly. People who live in New York City often end up being beholden to the preferences of people who live outside the city. There are downsides to the culture of localism in New England, but one nice thing is that cities and towns are typically able to prioritize the lives of their own residents which makes them nice places to live. reply Spooky23 22 minutes agorootparentMaybe in some New England areas. Greater Boston is a suburban hellscape. If traffic is your Kryptonite, beantown is worse in some dimensions. NYC prospers today because it’s a regional hub. All of the business that required physical presence (shipping, industry, etc) is long gone. Also note “the provinces” include NYC itself. I grew up in Queens, and congestion pricing certainly didn’t align with the needs of my relatives who live in the old neighborhood. reply throwaway7ahgb 6 hours agoparentprev>> the leadership (and many voters) were more interested in coming up with ways to make it convenient for drivers to drive and/or park in every neighborhood in the city than making it a pleasant place to live. The ghost of Robert Moses influences everything there. Sorry but this statement is not true. Compare NYC today vs 1980s or earlier. There are more bike lanes, parks, walking paths than ever before. Roads in Manhattan are narrowed frequently to allow more pedestrians. The west side highway is amazing for biking, I love it. Just because it is not Copenhagen or Amsterdam doesn't mean progress isn't made. source: Lived in Manhattan for 20+ years. Own a car, bike regularly and walk to work. reply asoneth 4 hours agorootparent> Roads in Manhattan are narrowed frequently to allow more pedestrians. That's wonderful to hear! I lived in NYC for the better part of a decade and was a big supporter of orgs like Transportation Alternatives and their work with the DOT to advocate for road diets. At least when I lived there the painted bike lanes were a running joke and more dangerous than just taking a car lane because every block there was at least one taxi or truck blocking the bike lane. And in cases where there was physical separation you had to contend with moped drivers riding at unsafe speeds. The Queensboro Bridge bike path was dangerously overcrowded and every year they claimed they'd open another lane; it looks like that's still \"just around the corner\" https://nyc.streetsblog.org/2024/04/08/first-look-the-dot-fi... but I wouldn't hold my breath. The Queensbridge Greenway shared use path became a muddy parking lot soon after it opened and it seems like they're still struggling to keep cars off it. NYPD still seems more enthusiastic about ticketing cyclists than drivers who park illegally. I didn't mean to imply that the NYC bike/ped folks haven't notched some wins. And I used to tell myself that as bad as it was, other US cities were worse for cyclists and pedestrians. But that wasn't true and every time I go back to visit friends and family I'm still grateful when we leave. reply CalRobert 5 hours agorootparentprevIt’s so far behind Amsterdam, Paris, etc it doesn’t deserve to consider itself in the same class. reply enriquec 5 hours agorootparentBehind in what? Amsterdam is 800k people vs 8.3 million in NYC which is close to half of the population of the entire Netherlands reply CalRobert 3 hours agorootparentIn being a livable city reply immibis 3 hours agorootparentprevSo a bit more than twice Berlin? reply Bluecobra 5 hours agorootparentprevAlso I could be wrong here but I think Central Park was also closed off to normal traffic. I rode an e-bike there last year and it was so much fun. reply asoneth 1 hour agorootparentThat's awesome! It was well after I left but I heard they finally closed off the loops to cars. It always boggled my mind that they even let drivers take over the nicest park in the city. Now all they need to do is calm traffic on the transverses... reply giraffe_lady 6 hours agorootparentprevIt can improve a lot and still be godawful. reply grecy 4 hours agoparentprevOnce you've been out of a city for a while the smell of car exhaust can be extremely strong. Every time I fly into LAX from northern Canada I get a punch in the lungs - it's horrible. There must be some severe long-term health impacts. reply AtlasBarfed 2 hours agoparentprevElectrification really hasn't hit home in the US yet. EVs due to the simplicity of the drivetrain (which is already engineered for various electric motor sizes since we've been using them for 100+ years in 1000s of applications) can be adapted quickly and cost effectively to many different vehicle sizes and platforms: - electric hand scooter - maybe this has happened already, but the mini micro kids scooter design seems ideal to human application. more stable, turns well, better balanced. It folds up and is totally hand-portable when you get to your destination, so no locking up. Modern battery performance should be able to do dozens of miles at 10-15mph if necessary. These can be almost effortlessly carried into a subway car. - electric bike - as anyone who has been to the netherlands, this encompasses a wide range, from a simple bike to various cargo carrying versions and basic trailers. The downside of the electric bike is securing it at the destination, but theoretically secured parking should be a lot more compact for a bike. - electric vespa/moped - the big difference here is speed and danger. In a city, I argue this is right at the line of being too big and dangerous. It can't be adjacent to pedestrians or bicycles, needs to be in dedicated car lanes, but its kind of too small. These can go faster, carry more, and are more secure since they are harder to make off with. - electric motorcycles/trikes - like mopeds but bigger/faster still - electric K-cars/golf carts - this is where there is a big empty spot in US vehicles. You can see them starting to develop with side-by-sides, but an urban electric golf cart or similarly sized vehicle is tremendously useful and much more compact and cheap to make in theory. I would argue that outside of specifically permitted deliveries, nobody should have a vehicle larger than this in Manhattan. IMO these are the big three that cities need to start planning to adapt to, and big vehicles are strictly for special situations. A \"golf cart on steroids\" can really pull quite a lot given EV motors ability to produce torque. I lived in downtown Minneapolis for quite a while, it was amazing how much a bicycle shrunk the city once the snow melted. Anything under 4 miles of distance wasn't worth taking a car, because the parking delay and walk-to-destination removed any time advantage. e-bikes and e-hand scooters are like that, but expand the accessibility to a far bigger age range and fitness level. An e-handscooter that is portable takes a subway system and expands the radius/distance accessible from the base stations by a factor of 3 to 10. With a e-scooter you carry off the subway, you can easily go 1-2 miles from the station, weather permitting. Finally, e-scooters are QUIET and CLEAN. Which when people get used to them will really highlight how much ICEs stink and are loud, and will turn them into pariahs socially. When that happens tipping point wise who knows. reply 1024core 3 hours agoprev> The policy would have charged vehicles for entering the perpetually gridlocked streets of Lower Manhattan, If driving is so bad, and public transit is so convenient, why are people still driving? People like this author write from a position of \"people are stupid; they don't know what's good for them, so let me enlighten them\". It's this assumption that is stupid. People want the most convenience in their lives. If you want them to use public transit, make the public transit more convenient and cheaper than driving! I mean, take a look at a city like Tokyo. How many people drive there? Almost everyone takes public transit, putting NYC to shame. There is no need to drive in Tokyo! As a tourist, a 3-day all-you-can-travel subway pass costs less than $10, which isIf driving is so bad, and public transit is so convenient, why are people still driving? [this comes from an assumption that] people are stupid; they don't know what's good for them, so let me enlighten them\". It's this assumption that is stupid...make the public transit more convenient and cheaper than driving...look at a city like Tokyo But that's exactly what they did in Tokyo: owning a car (and finding a place to park it!) is so expensive that few resort to it. I've probably been in a private car in Tokyo only once. Whereas in NYC car costs are socialized (Manhattan street parking is quite cheap when you can get it; you don't need to prove you have parking in order to own one, etc) while public transit is not. Yes there's a phase transition issue (driving in suburbs is easier than driving in the city, but there's little incentive to transition from one to the other. Congestion charges do exactly what you suggest: make public transit cheaper than driving, and provide the funds to improve public transit. reply bombcar 1 hour agorootparentFrom what I understand Tokyo and Japan in general have a lot of car owners (something like \"one per house\") or so, but they're rarely used for commuting purposes. reply zip1234 3 hours agoparentprevIt's the tragedy of the commons. Driving is very convenient, however it is also VERY space inefficient and congested streets with drivers slows down busses and makes public transit worse. Furthermore, the price of driving is not born fully by drivers. The noise and particulate pollution, increased danger to the public, and space taken by cars is not paid for by drivers in any meaningful way. reply timeinput 2 hours agorootparentDriving is honestly pretty inconvenient. You have to find a huge space to put your car every night (difficult when trying to rent in an urban area) possibly with electrical service, you have to pay for the car which is likely tens of thousands of dollars, you have to maintain the car with regular oil changes, new tires, etc that's at least 4-8 hours a year. I think we feel the pain of waiting for a subway, and think boy it would be so much easier to drive, but few people think while they're working an extraa week to pay for their car+parking+insurance+maintenance+... boy would it be nice to not have to own a car. reply s1artibartfast 2 hours agorootparentIsnt this just reiterating the idea the all the car owners are too stupid to properly assess that tradeoff? That is what the top level post was objecting too. If the theory relies on everyone being consistently wrong, then perhaps it isnt properly accounting to the benefits of cars and inconvenience of public transportation as it exists today. I dont think anyone is challenging the idea that cars are expensive, or people would prefer something else, provided it is actually better. reply timeinput 2 hours agorootparentMy assertion isn't one of stupidity, it's a question of when people evaluate their decisions. A smart person may get frustrated when the subway is 5-10 minutes late every day and take a car instead. They have 5-10 minutes that they're dwelling almost exclusively on the lateness of the subway. That same person may get frustrated when they work n+1 hours per week because they own a car, but will not associate that frustration + time loss with their mode of transportation because they're focusing on work for that hour instead of sitting at a subway station waiting for the train. It's not that they're too stupid to address the trade off, they just aren't addressing the trade off because they're spending the time that they had been wasting waiting for the subway at work / driving, and don't have the time to reflect on the inconvenience compared to when waiting for the subway you're forced to dwell on the idea that you're waiting. reply s1artibartfast 1 hour agorootparent>It's not that they're too stupid to address the trade off, they just aren't addressing the trade off because they're spending the time that they had been wasting waiting for the subway at work / driving, and don't have the time to reflect on the inconvenience compared to when waiting for the subway you're forced to dwell on the idea that you're waiting. Seems pretty thin to me. I dont think people are so cognitively busy that they never have time to ponder things like the tradeoff. My experience is people think about the tradeoff very often. When they are in traffic, when they paying for their car, changing those tires, ect. reply steveBK123 2 hours agorootparentprevIt does not help that on time performance, outages, and safety on subways have all gotten worse versus 5/10/15 years ago. Got a lot better from 90s thru about 2010 or so and has been reverting since. Remember the \"Summer of Hell\" in 2017 which just sort of slowly faded into the COVID transit issues and now we are here with budget shortfalls. Hard to see it getting materially better before it gets worse. https://en.wikipedia.org/wiki/2017–2021_New_York_City_transi... reply ylhert 2 hours agoparentprevThe fundamental piece you are missing with your logic is that the negative externalities of driving a car are massive and not borne by those making the choice to drive. This leads to people choosing to drive a car at a rate that is much higher than the optimal balance for society. The best way to control for these negative externalities (risks to pedestrians, noise, pollution, congestion) is with a tax. In the US, we subsidize car usage in an eye watering and incredibly unfair way, which results in overuse of cars. If public policy were to better reflect the actual costs of driving a car, few would be able to afford it or choose it over other options reply caseyohara 2 hours agoparentprev> I mean, take a look at a city like Tokyo. How many people drive there? Almost everyone takes public transit, putting NYC to shame. I was curious so I looked this up. Tokyo transit ridership is 3.46B / year Tokyo population is ~39M (Urban figure on Wikipedia) NYC transit ridership is 1.81B / year NYC population is ~19.5M (Urban figure on Wikipedia) In terms of ridership / population, NYC is actually higher (93.17 vs. 88.47) reply maximilianthe1 2 hours agorootparentSorry, that's incorrect 40 million passengers (counted twice if transferring between operators) use the rail system daily (14.6 billion annually) with the subway representing 22% of that figure with 8.66 million using it daily. [1] 1. http://www.mlit.go.jp/kisha/kisha07/01/010330_3/01.pdf reply harmmonica 2 hours agorootparentprevGoing off on a tangent here, but thanks for posting a reality check for the comment the gp made (that's no offense to gp; I get their thought process and why they would think Tokyo was more mass-transit friendly (it still could be more non-auto friendly on some other dimension (e.g., more pedestrians and therefore less auto-reliance but I'm not making that claim)). Re the reality check, though, I wonder if this is going to be one of the no-questions-asked positives from AI. If, every time someone makes a claim that's not, but can be quantified, similar to what you just did with these ridership and population stats, an AI umpire would cite the stats behind the claim to better-educate the audience about the reality. Of course this assumes a non-biased \"AI,\" but it seems like something that could become a reality sooner than later. I experience this multiple times per day, on HN a lot, where people make claims and I'm like \"is that really true?\" And I'm not being an ass; I'm always genuinely asking the question to make sure I'm getting properly educated. P.s. I just typed \"is Tokyo more transit friendly than New York City?\" into 3.5 (yes, I'm still on 3.5. Some of us HN folks are actually tech laggards!). I won't paste the answer here since no one will read the comment given its absurd length, but your human answer is better, and more to the point, than 3.5 (+1 for humanity, I guess). reply FireBy2024 2 hours agoparentprevCase in point: I had to take my extended family visiting us to Manhattan for 2 days last week. Total 6 adults. The cost of taking the LIRR and Subway would have been upwards of $180 (LIRR $120 + Subway $60) per day! Driving was much more affordable (and convenient) even with the expensive parking charges of $50 for 12 hrs of parking in prime locations. Public transportation needs to become more affordable first. I would have happily taken the train if it was comparable in cost. But, I guess I am the minority case. reply ProfessorLayton 28 minutes agorootparentIf every car in a major city was filled to capacity, I don't think we'd be having this conversation. A major part of traffic congestion is that a significant portion of people are driving their vehicles with a single passenger, sometimes two. This is why we have HOV lanes that lower or make tolls free. reply jandrese 2 hours agorootparentprevNope, I had the same experience. Family of 5 trip to Manhattan and we looked at possibly taking the train into the city but it made no sense financially, even after accounting for the outrageous parking costs in lower Manhattan. For one or two people mass transit is a no-brainer, three was roughly break even, but once you have a bigger group you're pretty much stuck driving. reply bombcar 1 hour agorootparentIt's surprising how quickly small plane general aviation beats many forms of \"pay by the person\" travel. You can fit four adults in a small plane that costs $150/hr to run and goes roughly 150 miles an hour - or a dollar a mile. It doesn't take that much of a charge for four people on a train to equal that. And cars are way, way cheaper. reply borski 1 hour agorootparentprevThings change when you live in the boroughs. For one thing, no need for LIRR; which isn’t what most people refer to when they talk about “public transit in NYC.” But also, if you take the subway regularly, there are multi-use and unlimited passes, students get free metrocards for weekdays, and so on. reply stetrain 1 hour agoparentprev> If you want them to use public transit, make the public transit more convenient and cheaper than driving! So charge drivers for the actual value/cost of the land they park on, the lanes they drive on, the maintenance and infrastructure for the roads, the safety risk posed to those outside of cars, and the gaseous and noise pollution caused to others? Sounds good to me. We can call it a \"congestion charge\" or something. reply steveBK123 3 hours agoparentprevThis was always the original problem with NYC implementation of congestion pricing. Collecting revenue and making driving worse is easy (the stick). But the carrot of actually making timely, meaningful improvements in transit is hard (the carrot) and no one believes its really going to happen. reply stfp 1 hour agoparentprevIt's very easy to get informed on these topics, yet people keep posting these high level extremely simplistic takes. You can't summarize the world with \"People want the most convenience in their lives\" and use that to explain or justify the state of things. A couple examples: Cars compete with public transit and are highly subsidized. \"Roads\" are a public transportation method that doesn't have to be profitable in people's minds. But \"Public transit\" does. There's so much more and the information is readily available. reply jandrese 2 hours agoparentprev> public transit is so convenient Unless you live in New Jersey, a short hop across the river from Manhattan but might as well be on the moon as far as the subway is concerned. The NYC Subway is great where it has service, but has for years neglected to expand into areas where the people live. Queens is also terribly underserved. There are bus options, but they get stuck in the same traffic that makes driving so miserable. reply bombcar 1 hour agorootparentThis is a big issue for public transit - either it stays ahead of growth, or you end up with pockets that are underserved (and of course cheaper). This starts compounding, because those pockets still need to get to work, etc, making it harder to get new transit in later. Paradoxically you almost need to tax the locations that don't have access to public transit to pay for public transit, so the pricing inequality goes away. reply borski 1 hour agoparentprevYou have it backwards. The reason people still drive is inertia. The reason Tokyo has more people using public transport is that it’s expensive to own a car there and they encourage and advertise public transport heavily. reply jperras 1 hour agoparentprev> Instead of trying to tax the people into complying with your desires, make them want to do so of their own free will! … do you have any idea of the costs that are associated with owning and operating a vehicle in Tokyo? reply Spooky23 2 hours agoparentprevIt’s a chicken and egg problem. You need taxes to find multi billion dollar capital programs. I do agree this particular program was a dumb idea that advocates tried to push through with the COVID nadir in traffic and office occupancy. reply alexpetralia 2 hours agoparentprevDowntown areas in Tokyo have virtually zero public parking spaces. They made it very, very inconvenient to drive. reply MisterTea 50 minutes agoparentprevBorn and raised NYC so Ive been around since the Regan era. Growing up my parents drove everywhere - even to Manhattan. > If you want them to use public transit, make the public transit more convenient and cheaper than driving! Yup. I live in south Queens which is mostly suburban and a lot of people drive around. I'm two blocks from the A train so driving to Manhattan is pointless when I have a 30 min express subway ride. But if I wanted to go to the nearest Microcenter in Kew Gardens it's 20 minutes by car and an hour by bus, each way, with a transfer - guess what mode of transport I'm taking... reply tikhonj 2 hours agoparentprevI mean, sounds like the first step according to that logic would be making the subway free, right? reply 1024core 1 hour agorootparentWouldn't be a bad idea. E.g., here in SF, the fares bring in $200M/year, and Muni's budget is $1.2B/year (approx numbers), which means, for every $2 charged by fare, the City has to put in another $10. Making it free would just cost an extra $200M/year, which is within the realm of possibility for a City with a budget of $14B/year. reply mplewis 2 hours agoparentprevThe canceled traffic congestion fees were going to pay for public transit improvements that would be used by millions every day. reply asah 7 hours agoprev\"Contractors who work with the MTA on these projects already charge a premium for having to manage the substantial risk that construction is delayed or canceled—introducing “the Governor might suddenly decide your job is over” as another risk to worry about will just increase those cost premiums, thin the pool of companies who are capable of working with the MTA, and reward the politically savvy. ... The fact that infrastructure plans can go through years of study, environmental review, public consultation, pilot projects, revisions, lawsuits, construction, delays, and more only to then be shelved days before final implementation is a large part of why public works are so expensive in the first place.\" Savvy point that's often missed in these discussions. I'm on the boards of two buildings in Manhattan and see these invisible \"taxes\" all the time. reply yardie 6 hours agoparentI'm reviewing the budget for our condo now and some of the renovation projects have a contingency budget of 25%. You would think that was overkill but the last project was an elevator upgrade where the city inspector sat on the permit for almost a year. reply steveBK123 2 hours agorootparentAnyone who thinks NYC simply needs to pass more laws & taxes should sit on a condo/coop board for a few years. Thankless job, and dealing with the city bureaucracy is awful. Lot of mechanisms setup to punish & tax buildings but not to facilitate the timely curing of whatever they are fining you over. One favorite was after an elevator incident a few years ago, the city passed some new law that required mostly software updates to elevators. It was essentially an update to prevent elevator repairmen from overriding the safety mechanism preventing motion when the door is open. So putting a safety on the safety. However of course there are only like 2 companies in the city and they colluded on how much they'd charge. So in effect it was a $10k per elevator tax on every residential building in the city. Not huge in the grand scheme of things, but in my condo that was a random 4% of budget blown away at the stroke of a pen. And they do this all the time. We had to do facade inspections and the city institutes all sorts of rules & fines around it, but then we ran into a stubborn neighbor with axe to grind with sponsor disallowing access on adjacent side for our scaffolding. City of course does nothing to force compliance by neighbor, it's your problem. Eventually had to be paid off in some manner. Very much city of no. reply pc86 5 hours agorootparentprevThere's a simple (but not easy) fix to this. After 30 days of no movement, permits are issued automatically and any fees paid are returned out of that department's budget, regardless of where the permit fees actually go. The default response to government inaction is typically a) the citizen can't do whatever thing they're trying to do without violating the law, and b) nothing bad happens to the bureaucrats not doing their jobs. If those two things inverted you'd see government become more efficient overnight. reply thmsths 4 hours agorootparentI won't comment on the specific numbers or enforcement mechanisms. But I strongly agree that we need reasonable SLAs for the government. My only worry would be how to deal with abuse. reply mensetmanusman 4 hours agorootparentprevYes, and if you tie it to their paycheck, you might get more action. reply pc86 4 hours agorootparentI think this has been tried before and there are issues with Constitutionality, qualified immunity, etc. but I am not an attorney. Better to make sure the penalties stay at the official level, but also to make sure the people in these jobs are elected and not appointed so there is real accountability. reply immibis 3 hours agorootparentElections do such a good job of accountability... reply bombcar 1 hour agorootparentElections suffer from the bobcat problem - if something like a permit is only pulled five times in your life, you'll rarely, if ever, be affected. But those who are may go through hell, and yet the approval rating remains high. https://xkcd.com/325/ - \"You can do this one in every 30 times and still have 97% positive feedback.\" reply bluGill 5 hours agorootparentprevMake sure you ask all residents to contact their city representative on this. If politicians don't feel pressure they won't do anything. reply yardie 3 hours agorootparentIn fact that is what we did. Just 50-100 pissed off residents mail and phone bombed the city until we got a response. The inspector made the excuse that he issued the permit months ago. But the filing portal had it on the day after our campaign. We ended up burning through the contingency because this project started right at the beginning of COVID. Parts and labor suddenly became scarce. reply woodruffw 15 hours agoprevThis is a nice, data heavy summary of the situation! Something that stood out to me: there's been a marked decline in state and local funding for NYCT since before COVID. I wasn't fully aware of just how much the city and state had independently cut back: it looks like NYCT part of MTA alone is receiving $3B less per year than it was in 2019. Congestion pricing is sound policy regardless, but this hammers home the perception that NYS treats the MTA (and NYCT in particular) as a financial sponge that can be squeezed for spare change whenever a boondoggle needs funding somewhere else. reply onlyrealcuzzo 5 hours agoparentMTA operates at a ~$600B loss. How is the city using it as a sponge to pay for other things? I'm all for funding mass transit. But the city is not using MTA revenue to pay for other things. reply woodruffw 4 hours agorootparentMTA definitely doesn’t have 600B to lose, so I’m assuming you mean million. And it’s not the city; the MTA is run by the state. So the city can’t use it to pay for other things, but the state can both divert and reappropriate its budget. Take a look at the third image in the post: both state and local funding from the MTA have been reduced in recent years, dating back to the FY before COVID. Similarly, here’s an example of the state using MTA as a piggy bank for upstate ski resorts[1]. [1]: https://jalopnik.com/new-york-mta-gave-millions-to-bail-out-... reply joenot443 4 hours agorootparentIf you dig in a bit, it's a fairly minor scandal. There are much, much better summaries elsewhere. I've gotta point out though, the writing in that article is truly abhorrent. This is the weird Frankenstein of The Onion and Gawker that was picked up by private equity, right? The web is a better place without this style of publication, I think. I'd encourage you to get your news somewhere else my friend :) This reporting makes Buzzfeed look Pulitzer worthy. reply woodruffw 3 hours agorootparentI don't normally read Jalopnik; there's a Gothamist article on the payment as well, but it was paywalled. (I happen to agree it's a fairly minor scandal. It's intended to be demonstrative of the MTA's inability to protect its budget.) reply mechagodzilla 4 hours agorootparentprevI think you are off by a factor of 1000 - MTA lost $600M (not B!) last year. reply seoulmetro 15 hours agoparentprevCongestion pricing is cutting off the nose to spite the face. It's dumb, but it does \"stop the problem\", the same way charging 5x more for petrol would, or closing all the highways into the city.... whilst also hurting a bunch of people. reply woodruffw 15 hours agorootparentThis hasn't played out in other cities, so I don't know why you think this. Notably, NYC congestion pricing does not affect the highways into the city: if you take a highway into the city and stay on the highway, you won't be charged. The charge is only for entering the city grid, where the (reasonable) argument is that the transportation system is more than sufficient and the traffic, noise, and tailpipe emissions of your car do more harm to local residents than they fairly benefit you. reply evanelias 12 hours agorootparent> if you take a highway into the city and stay on the highway, you won't be charged. The charge is only for entering the city grid That's technically true, but it ignores the fact that both the Lincoln Tunnel (495) and Holland Tunnel (78) require you to enter the city grid. With the way the congestion pricing was designed, if you take either of those tunnels, there's no way to avoid the congestion pricing fee -- this was confirmed multiple times in FAQs and Q+A's. I'm mostly in favor of congestion pricing, but I really think they need to figure out a better solution for that tunnel issue. reply macNchz 5 hours agorootparentA lot of really terrible congestion comes from people who are only passing through Manhattan on their way to/from other places, because the toll structure incentivizes that vs going around. I worked in the general vicinity of the Holland Tunnel entrance for years and it was miserable for basically all users of the streets every afternoon. reply evanelias 4 hours agorootparent> the toll structure incentivizes that vs going around How so? The toll rate is identical between the Lincoln Tunnel, the Holland tunnel, the George Washington Bridge, the Bayonne Bridge, the Goethals Bridge, and the Outerbridge Crossing: https://www.panynj.gov/bridges-tunnels/en/tolls.html reply macNchz 1 hour agorootparentWell three of those involve a toll on the Verrazzano unless you're starting or ending your trip on Staten Island itself, whereas the East River bridges into Manhattan are free. The GWB can be done without a toll, but if you're shopping for a cheaper route to avoid the Verrazzano toll it's likely way out of your way. Ideally the toll structure should incentivize through traffic to stay on highways and out of the most gridlocked streets in the country, and I'm saying this as a longtime car owner in Brooklyn who has driven through the Holland Tunnel hundreds of times. Manhattan streets are a tragedy of the commons in action. reply spyspy 5 hours agorootparentprevToll optimization aside, there’s often a huge backup of the BQE or in Staten Island that makes the tunnel the next best option. I’ve faced that scenario probably 1/3 of the time trying to visit family in PA/NJ from Brooklyn. reply detourdog 9 hours agorootparentprevI think you should take either the Verrazano bridge exit off 95 or continue north to 87. The idea is to avoid driving through Manhattan planning one's trip accordingly should work. The only reason to take either tunnel is to reach Manhattan. reply spyspy 5 hours agorootparentI really wish there was a tunnel between jersey and Brooklyn. Every option sucks for that. You either end up stuck in traffic in Staten Island, manhattan, or queens. reply detourdog 4 hours agorootparentRemember when Chris Christie said a modern tunnel was not worth the cost to NJ? reply pc86 4 hours agorootparentprevI'm looking at a map and really confused about how from New Jersey you can end up in Queens while trying to get to Staten Island. reply detourdog 4 hours agorootparentCrossing the Verranzo Bridge and coninueing on the Brooklyn Queens expressway or 278. reply selimthegrim 4 hours agorootparentprevStarting in Ft. Lee? reply detourdog 3 hours agorootparentgoing where? reply evanelias 4 hours agorootparentprevThat's absolutely incorrect. Over a million NJ residents live closer to the tunnels, myself included. Every route I plan on google maps to various destinations in parts of Brooklyn and Queens puts the tunnels as being faster. reply datameta 4 hours agorootparentRight, that's part of the problem. During the work hours the temporary population of Manhattan grows about threefold. They contribute to immense gridlock and then disappear to their suburbs in another state. Why must the residents bear the brunt of the externalized traffic impact? It is only fair to treat available street space as a limited resource. reply evanelias 4 hours agorootparentThat's tangential to the issue being discussed in this subthread, which is that there's physically no way to stay on highways when coming from the tunnels, even when attempting to reach non-Manhattan destinations in outer boroughs. So the only relevant congestion for this specific issue is between the tunnels and the highways. Manhattan residents living near the tunnels know what they're signing up for. The tunnels opened in 1927 and 1937. The traffic isn't exactly a new problem. Also, the parts of NJ in question (where the tunnels are closer than bridges) are largely urban in character. You make it sound like people are coming from some far-away leafy suburb, that's not the case for the majority of this population. reply detourdog 2 hours agorootparentPeople in NJ know what they are signing up for. The outer boroughs have existed in the same locations for decades. Traveling by automobile through Manhattan isn't exactly a new problem. reply evanelias 5 minutes agorootparentBeing charged $15 to travel literally two blocks between the tunnel and the West Side Highway is a new problem. That's the point. There's no way to avoid the city grid, due to bad highway design that doesn't directly connect either 495 or 78 with 9A. Any congestion pricing plan which ignores that problem is going to be met with mass outrage, to an extent that will swing elections against incumbents. Especially when said incumbents promote this \"no charge if you stay on highways\" BS without explaining the fine print. Again, I'm saying this as someone who only ever takes public transit and is generally in favor of congestion pricing as a concept. detourdog 4 hours agorootparentprevI was assuming traffic from Florida to Boston or Albany. Local traffic is local traffic and I thought about the Holland tunnel->Kenmare->Williamsburg Bridge route. Which I found too exceptional to mention and likely only used by people that live in the region and should be using public transit for such trips. reply evanelias 4 hours agorootparentDue to the layout of the northern Brooklyn subway lines (no direct connection to PABT or Penn Station), some trips can easily take 2x-3x as long using public transit on a good day, let alone when there's some incident affecting the subways. Personally I always take public transit into NYC, but I can completely understand why it isn't a reasonable option for many people. Especially when both NJT rail and Amtrak have daily meltdowns whenever the temperature is above average. Edit to add: I'm completely puzzled by your comment about \"traffic from Florida to Boston or Albany\". Boston is on the I-95 corridor, which means going over the GWB -- there's already no reason for any sane driver to enter the congestion zone for that route. And routes to Albany don't need to cross the Hudson at all. I don't see how congestion pricing or these two tunnels have any connection to those routes. Anyway, my overall point here is that the upthread comment of \"if you take a highway into the city and stay on the highway, you won't be charged\" is true but worthless, because for one million people here there's no way to actually do that without driving massively out of your way to a bridge crossing. reply detourdog 2 hours agorootparentI agree with points more or less and have plenty experience moving around the region to understand what you are saying. The through traffic comment was a response to my original comment that a person wanted to use the Hudson River tunnels to pass through NYC onto their destination. I picked the Varanzono bridge route as the southern route around the city and 87 for the northern route. I usually go further north than the GWB to crossover. The massively out of your way I might prefer to NYC city traffic. reply evanelias 0 minutes agorootparent> The through traffic comment was a response to my original comment that a person wanted to use the Hudson River tunnels to pass through NYC onto their destination. Which parent comment are you referring to? I don't see any talking about non-NYC destinations besides yours. In any case, the only highways that are exempt are the West Side Highway and the FDR. These are the only highways in the zone. And generally you don't take either of those highways for anything other than \"local traffic\" as you said. So I'm just not understanding your point about routes outside of NYC, that doesn't make any sense in the context of congestion pricing and the exempted highways in the first place. seoulmetro 12 hours agorootparentprevI think this because that's what it is. It has played out in other cities, there is only one way for it to play out. It's okay if you disagree with the facts, but it's fairly easy to understand how pricing people out is a poor attempt at a solution, but solves the problem by brute force anyway. >where the (reasonable) argument is that the transportation system is more than sufficient and the traffic, noise, and tailpipe emissions of your car do more harm to local residents than they fairly benefit you. Yes. I never said that less cars is not beneficial. I understand this, that's why I said what I said. There's no such thing as \"fairly benefiting from other residents\"... You're still not solving the problem by slapping a $ figure to the entrance of cars. It's just a lazy and bad solution. reply CalRobert 8 hours agorootparent\"Poor people\" or rather \"people who actually need to drive in this particular moment\"? It's really nice to be able to pay for better facilities _when you need them_. The person who is late for a flight or a job interview, or who just found out their kid got hurt at school and is heading to the hospital, benefits massively from congestion pricing. They have the ability to access faster transportation when they need it, and to choose the slower alternative when they don't. reply immibis 7 hours agorootparentIt's nice to be able to pay for better facilities. It's even nicer if they are just better, without paying. Payment can create perverse incentives as well. reply Retric 5 hours agorootparentIt’s impossible for free city streets to both be fast and scale to the density of NYC. Congestion pricing allows them to be fast by removing traffic. Thus allowing busses or people with significant need to travel quickly. Where it fails is an Uber/taxi drivers add a lot more congestion than an average person and the fees don’t get adjusted for this issue. reply paulgb 5 hours agorootparentprevThe thing about congestion is that you can't “make it better” (in a densely populated place like Manhattan's CBD) without removing cars somehow. The only other real approach to this is rationing (https://en.wikipedia.org/wiki/Road_space_rationing), which leads to economic inefficiencies and doesn't raise money that will be used to make transit better for everyone who choses not to drive. reply CalRobert 5 hours agorootparentprevYes, magic definitely is nicer than dealing with actual constraints. reply rho138 4 hours agorootparentprevBecause that worked out so well with the internet and ad networks lol. reply intended 9 hours agorootparentprevClear accurate pricing is how people decide what to do with their resources. Pricing things correctly is basically the reason we have a modern economy. Externalities are one of the thornier issues and making those prices clear to people is how we get people to decide whether they actually want to pay the full price of a good or service. reply enriquec 5 hours agorootparentits artificial pricing by definition reply intended 4 hours agorootparentBy that criteria Any price that includes some tax would be artificial. I suppose libertarians would be happy. Artificial, organic, lab grown or otherwise - does the price of a good or service capture the cost of its production. reply alistairSH 4 hours agorootparentprevpricing people out is a poor attempt at a solution No, it's a good solution to negative externalities. In the status quo, drivers do not bear the full cost of driving (noise, pollution, opportunity cost of time lost in traffic). Congestion pricing address that by imposting a fee equal to the societal cost of those externalities. reply frereubu 11 hours agorootparentprevIt is a bit of a blunt instrument, but presuming you agree that it's a problem that needs to be solved, what other alternative would you suggest? reply seoulmetro 11 hours agorootparentNot doing that? I'm not here to give alternative solutions, I'm here to tell you the current solution is slightly worse than nothing. You would have to make the solution fairly complex for it to be beneficial. But I'd start with quantifying why people are in the city, rather than treating them all the same. If it were my job, I'd have a better solution than congestion charges, that's for damn sure. reply CalRobert 8 hours agorootparentGiven your username one might hope \"implement a world class public transport system\" or better yet \"convert an urban highway into a beautiful linear park and waterway\" (like Seoul) would come to mind. reply portaouflop 10 hours agorootparentprevIf it were your job you would think differently because you knew all the facts, constraints and trade offs. Contrary to popular belief most people make reasonable decisions and want to do a good job reply enriquec 5 hours agorootparentcorruption and incompetence exist. reply enriquec 5 hours agorootparentprevyou're right but judgmental statists get high off the feeling of control and \"fairness\" - effectiveness be damned. I thought we absolutely needed the government and taxes for the roads because paying for use would be so \"unfair\" and \"hard\" - oh wait, now they want to control what you do because they like bikes so its totally reasonable. Hard to find more statists than in New York reply timr 15 hours agorootparentprevI keep saying it and getting savaged, but as someone who lives here and doesn't own a car, it's been obvious to me that \"congestion pricing\" was a lot more about collecting an additional tax from residents, and a lot less about \"congestion\", than anyone in power was letting on. The entire thing was heavy handed and dumb, but branded just well enough to capture the passions of NYC city people who really really really hate cars (and New Jersey) and don't think too deeply about it beyond that. The plan was structured in a way that it would affect every resident of the city via higher prices on goods and services, surcharges on delivery, taxis, construction, etc. Setting aside the issue of local residents with cars (which I could care less about), the planners refused to make sensible carve-outs for things like cargo trucks, construction vehicles, and so on. They could have done it, they just didn't. It was about maximizing revenue for the MTA, not about making sensible policy. Maybe the fees would be small when amortized over a truck full of groceries or a tanker full of oil, but that didn't change the fact that it was a regressive tax on everyone who lives here, and taxes here are already amongst the highest in the country. This stuff adds up. reply afavour 14 hours agorootparent> it would affect every resident of the city via higher prices on goods and services I find the math on that claim highly dubious. The toll for a truck is set to be $36 at peak times. $9 non peak. And they get up to $20 credit towards that when travelling through one of the tunnels, as many do. When I think about the amount of goods that can be carried in a truck and divide the $36 minus $20 or $9 across all those items… how much are we really talking about here? I ask the question earnestly because I don’t know, but at a cursory glance it just doesn’t seem like that big of a deal. Some drivers might even be quite happy to pay if it means they don’t spend hours stuck in traffic! reply timr 14 hours agorootparentWhat part is \"dubious\", exactly? There's a tax. It applies to all vehicles. You think that doesn't get passed on to consumers? I have a bridge to sell you (don't worry, it's a toll bridge). You're just repeating the last part of what I wrote -- that the taxes would be low when amortized over a large number of items -- but ignoring the rest. The point is, New York does this all the time: yesterday a surcharge on food delivery, today a \"congestion tax\", tomorrow a \"global warming fee\" on electricity consumption, a \"save the whales\" fee on straws, or whatever else. The number of sneaky fees and surcharges and social-engineering taxes in NYC is enough to make even the most ardent liberal begin to resist. Eventually, you're paying $25 for a hamburger and wondering why you can't find staff for your store. Look, I have no problem with taxes. Just be honest. Don't sneak them in and lie about them and pretend that they're punishment for New Jersey car commuters when most of the revenue will come from the people who live here, directly or indirectly. If the people want to vote in another billion-dollar sales tax for the MTA, great. Go for it. reply pas 10 hours agorootparentIt is a consumption tax (on space taken up on roads, noise, and emissions basically combined), in effect it incentivizes consolidation of traffic, goods and people both, and steers some of it to other modalities. Of course there's a long way to go before visible changes happen. the US is extremely car-dependent after all, and public transportation does not organically expand with more demand for it. and without alternatives people will just pay it. but this doesn't necessarily make it a bad tax. other changes can build on it. reply Projectiboga 5 hours agorootparentIt's a regressive nearly flax tax amounting to a taking from the residents of lower Manhattan. If they wanted to reduce congestion all they would have to do is make the free street parking NYC residents only. Many cities do that. An issue with this was it was clearly not about congestion except in name it was MTA can't build anything without spending multiples of the next most expensive transit system. The singular goal was to hit an arbitrary number. And if traffic dropped leaving revenue below rhe goal the costs would have been hiked. This was a hammer when they needed a scapel. reply throwaway7ahgb 6 hours agorootparentprevBut it's not a consumption tax, it's a \"congestion\" tax. If there were no congestion there would be no reason for this to exist. reply immibis 7 hours agorootparentprevWhy did you completely ignore the question of how much money this tax amounts to per item? It's in the realm of one cent. reply afavour 14 hours agorootparentprevThe part I’m dubious about is there being any notable effect on prices. I don’t personally see the congestion charge as dishonest. You can see it as a quasi-tax, certainly, but provisions like off peaking pricing and credit for using the tunnels means that it has positive side effects a straightforward tax wouldn’t. reply timr 14 hours agorootparentI don't care if it moves prices one millionth of a penny, or ten dollars. That billion dollars of extra MTA revenue was coming from somewhere, and it was mostly going to come from locals, even if it meant that we're all paying a fraction of a cent more per sheet for toilet paper. The worst taxes are the ones that are regressive and secret. reply ilya_m 11 hours agorootparent> The worst taxes are the ones that are regressive and secret. In my book, negative externalities are worse than taxes. If we agree that a car driving in Lower Manhattan inflicts a non-trivial cost on everyone else, then not taxing it leads to socially inefficient outcomes. Effectively, locals are paying a price either way - either by having their bus moving slower, inhaling fumes, etc, or by buying goods and services that reflect congestion pricing. The difference is that congestion pricing aligns incentives - for instance, delivery drivers may choose to travel to Manhattan during the off-peak hours whereas in the status quo they do not care at all about inconveniencing others. reply throwaway7ahgb 6 hours agorootparentOP wasn't arguing about \"cars\", it was specifically about service vehicles that are required for Manhattan operations. Thought experiment: If we could somehow ban private residential use of all cars and only allow \"work\" vehicles, there would be no congestion and no tax. reply neilknowsbest 2 hours agorootparentThere are some good arguments against that. It creates a deadweight loss by banning high utility private uses of the car (driving kid to hospital) and instead there would be an increase in low utility \"work\" uses of the car (delivering a single banana to a bodega). There is a parent out there who would be willing to pay $X to drive their kid to hospital, and a work vehicle user who would forgo paying $X by staying off the road, but under a blanket ban that won't happen. It would also increase the incentive for people to play games like claiming their personal car as a \"work vehicle\", throwing a little advertising decal on the side, things like that. reply imtringued 9 hours agorootparentprevPeople don't grasp that you can get wealthier simply by avoiding harmful things that would otherwise be done by default. reply slt2021 14 hours agorootparentprevif prices don't move then you are basically stealing money from those who pay (delivery drivers, blue collar workers, contractors), who are not rich enough to live on Manhattan - to subsidize transit for affluent Manhattan residents reply afavour 14 hours agorootparentI’m sorry… you’re suggesting congestion charging would take money from poor car drivers and give it to affluent transit riders? By any data available that’s entirely backwards. Cars in the CBD are disproportionately driven by the affluent, transit is disproportionately taken by the poor. reply bluecalm 6 hours agorootparentThe same mindset that leads to calling bike commuters elitist apparently. reply UberFly 11 hours agorootparentprev\"By any data available\" Let's see that data then. Plenty of working class individuals are trying to commute through the CBD for their livelihoods. They aren't taking the bus or rail to fix someone's water heater. reply Karrot_Kream 11 hours agorootparentReally what's the point of asking this question? Did you do a search and not find anything and you're convinced this is false? Or are you just here to fight? [1] shows a pretty clear negative correlation between car ownership and poverty. Do you have a source that shows contradictory information? (I love social medi... sorry HN oops how can I get this wrong, the quality of discussion is so high! :) [1]: https://wellango.github.io/posts/2021/06/who-owns-cars-in-ny... reply slt2021 10 hours agorootparentprevManhattan residents are richer than outer boroughs. It is true, that some rich motorists will pay up congestion pricing, but they wont even notice these charges cause they are rich (so NO EFFECT whatsoever on actual congestion). In fact, that's exactly what MTA wants - they want congestion pricing Revenue, not reduction in congestion loss in revenue. But the most impacted would be workers who rely on car to make a living: uber, lyft, doordash, blue collar contractors, food delivery trucks, etc. Everyone who has to drive to make a living will be taxed, while rich Manhattan residents will enjoy subsidies from the working class. and they cannot take public transit, so no reduction in actual congestion either. it's all farce and show to steal money from honest working people and have over it to corrupted union bosses at MTA reply CalRobert 5 hours agorootparentprevYou could implement congestion pricing and throw the money the garbage and it would still be a net win for the city by reducing cars and traffic violence. reply pc86 4 hours agorootparentI'm not sure why you expect to be taken seriously when using terms like \"traffic violence.\" reply CalRobert 3 hours agorootparent“Having my kid’s skull popped open and their brains splattered on the road under the tire of your dodge ram” seemed too emotive. Stop sanitising killing people. reply tzs 5 hours agorootparentprevLess congested traffic tends to move faster. Won't that increase \"traffic violence\"? reply woodruffw 2 hours agorootparentThe bigger issue for pedestrian injuries on Manhattan's streets is box-blocking and hazardous turns, not really speed. Congestion pricing will probably cause a slight increase in average speeds, but we're talking about going from 5 MPH to maybe 10[1]. It's not ever going to be fast for cars, just faster. [1]: https://www.latimes.com/nation/la-na-new-york-traffic-manhat... reply AdamN 11 hours agorootparentprevThe ease of the truck moving through the city with less traffic and easier parking would have more than made up for the toll cost. Same for taxis where the cost compared to the increased demand (since the taxi might actually be faster than it would be now in midtown during the day) also probably would have benefited taxis and their customers. reply slt2021 7 hours agorootparentNot true, you cannot simultaneously have lower congestion and higher demand for taxi. This is contradictory statement. In the end not much will change, people who would have takes transit will end up takin transit, people who have to drive will drive, they just will pay additional tax. reply KptMarchewa 6 hours agorootparentOf course you can have, if it's the non-taxi traffic that's decreasing. reply slt2021 1 hour agorootparentSingle drivers will be replaced by single taxi passengers, whats the diff? Maybe less demand for daytime parking, but gridlock will be the same reply AdamN 7 hours agorootparentprev> you cannot simultaneously have lower congestion and higher demand for taxi How so? If a taxi used to take 20 minutes and $20 and now it takes 15 minutes and $18 the demand for that form of transport will go up. reply slt2021 55 minutes agorootparentIf a driver is replaced by a cab passenger it is 1-to-1 replacement of personal vehicle with taxi, no impact on congestion. If you see demand for cabs increasing + congestion pricing revenue of $1.5bln that means more traffic and more paying drivers reply paulgb 5 hours agorootparentprev> The plan was structured in a way that it would affect every resident of the city via higher prices on goods and services, surcharges on delivery, taxis, construction, etc. We already pay a congestion tax on all of these things, it just takes the form of paying for the labor of someone sitting in traffic. It's not obvious at all to me that the cost of the tax will be less than the gain on the labor cost. reply timr 1 hour agorootparentI mean...that's a precise argument. To have this argument effectively, you need to have more than a hand-wavy idea of how much congestion would be impacted by the fee, which we don't. The impact studies were ludicrously ambiguous, and as far as I can tell, the numbers were pulled from thin air. What I know, without doubt, is that the plan didn't make any reasonable exclusions for residents, so it was de facto an additional tax. Would such a tax reduce marginal wait time cost by more than the price of the tax? Golly, that would be convenient for proponents, wouldn't it? Regardless, even if you believe this, you still have to deal with the counterfactual of a world where we do the whole congestion fee thing, but exclude obvious categories of essential vehicles, like delivery trucks, construction, etc. That would be better, smarter, and more aligned with the stated goals of the system. reply slt2021 14 hours agorootparentprevCongestion pricing is about taxing blue collar workers (plumbers electricians and such), who cannot afford to live in Manhattan and have to use vehicle for work. reply SkipperCat 6 hours agorootparentI don't buy that argument. If the plumber/electrician is working for a company and driving a company car, then the company will pay the extra $15 for that corporate vehicle to be in Manhattan. If the person has their own business then they can easily afford the daily $15. Skilled trades and the owners of those companies make very good wages in NYC. I see plenty of blue collar workers on the MTA. Its one of the fastest ways to get around Manhattan and much cheaper than paying for parking. reply slt2021 31 minutes agorootparentI have a problem with this “can easily afford” logic. This is not true, and this is not how taxation should work. You are just justifying stealing money from blue collar by bogus “they can afford it” logic. A lot of NYC residents can easily afford extra $15 (their incomes are like 6 figures) why not just spread the tax to everyone to make it more equitable? And more revenue for MTA reply kcb 6 hours agorootparentprevI take the Lincoln Tunnel during rush hour(in a bus). I see seas of single occupant cars and personal vehicles entering the city. reply afavour 14 hours agorootparentprevWould they not just pass the charge directly onto their customers living in the central business district? reply slt2021 14 hours agorootparentall costs are passes through, but elasticity of demand and supply will lead to consumer/supplier splitting the tax in the ratio of their respectable elasticities. fundamentally though it is NOT about reducing congestion, it is about stealing money from poor blue collar workers from outer boroughs, and handing them out to finance MTA's bogus overtime and 400k salaries for doing nothing useful. while also enabling rich urban liberals from manhattan to signal their \"ecology conciousness\" reply afavour 14 hours agorootparent> it is about stealing money from poor blue collar workers from outer boroughs It isn’t though, is it? What percentage of poor blue collar workers in the outer boroughs drive into the CBD of Manhattan for work on a regular basis? Very few. The cost of parking already makes it prohibitive for most. If they work as e.g. an electrician they’re already passing the parking fee onto their customer. Congestion charge will be the same. But yeah, sure, there will be a small number of people affected that way. But orders of magnitude more poor blue collar workers would benefit from better public transit. You can’t run a city by vetoing anything that has a negative effect to someone. Nothing will ever get done. reply sqeaky 12 hours agorootparentBut you can run a city buy carefully picking and choosing who your policies impact and minimizing that impact where appropriate. reply Matticus_Rex 7 hours agorootparentSo... pick policies like this one that very disproportionately pull money from people who are wealthier to improve the situation for everyone? reply portaouflop 9 hours agorootparentprevYea running a city of millions of people is super easy. Just pick the good policies instead of the bad dummy! reply CalRobert 8 hours agorootparentprevWouldn't people like your example end up with more billable hours due to reduced congestion, likely coming out ahead? reply rjmunro 10 hours agorootparentprevIf it reduces congestion, delivery companies could make more deliveries with fewer vehicles. It would probably be a net saving in costs. reply intended 8 hours agorootparentprevI mean, even if its well designed, we should expect some increase in costs due to a congestion charge. Right now, you have Group A: people who are ok with some members of their city getting to deal with congestion, so that they can have cheaper goods. Group B: The people who deal with the effects of congestion, who have to find a ways to have their needs met. as mentioned: >Setting aside the issue of local residents with cars (which I could care less about), >someone who lives here and doesn't own a car The status quo should suit you. That said, the inefficiency caused due to congestion, ends up affecting both group A and group B. This will add to costs, in either wait times / Delays, wastage, wear and tear, pollution etc. Those costs are borne by you, but not associated with congestion. reply seoulmetro 12 hours agorootparentprevYou're getting attacked because a lot of people are brainwashed into a solution that is forced down peoples throats. It's much easier to solve the problem like a brute and attack people and say they're wrong when they point out the nature of the solve. It's not even about money, if it were about the usage of power, food, travel, etc. it'd also be terrible. If you were to convince people they can't eat meat because it's wrong and then offer a solution as bad as \"just become vegetarian\", it'd also be laughable... but of course people would attack anyone pointing it out. reply pjc50 6 hours agorootparentprevCharging 5x more for petrol would also be a good idea. NYC isn't Florida, but it's still exposed to the risks of climate change. reply baggy_trough 6 hours agorootparentIt would in fact be a very bad idea. reply tim333 8 hours agorootparentprev>Congestion pricing is cutting off the nose to spite the face. It's dumb, but it does \"stop the problem\"... I live in London they brought it in and it's been ok overall. It doesn't stop congestion but reduces it a bit. For commuters it's probably a bit better as they mostly use the train/bus etc and benefit from less traffic. For workmen needing to deliver and fix things it's a bit of a pain. For me living in the center I'd probably take the plus of less traffic over the minus of getting a plumber to visit being expensive. Re the tax grab - yeah but they have to get tax money from somewhere to pay for services and the like. reply tgeorge 5 hours agorootparentI'm curious, in London did they cut the fares for subways when they implemented the tolls? The one thing that has been bothering me about NYC congestion toll is I figured there was be a cut in the fares for subways and rail to entice more riders. But I don't see that mentioned; just for capital projects it seems. reply woodruffw 3 hours agorootparentMy understanding is that OMNY's weekly fare cap[1] was created for exactly this reason (along with incentivizing people to switch from MetroCard to OMNY). That being said, NYCT's fare ($2.90 without zones or times) is markedly cheaper than even the cheaper London Underground off-peak same-zone fare. I don't particularly want to pay more for the subway, but the fare also really hasn't kept up with inflation. [1]: https://omny.info/fares reply Symbiote 2 hours agorootparentprevIt's a long time ago, but I think the main benefit from the congestion charge money in London was improving the service. More buses, especially at night, which helped the poorest people (cleaners, security, etc). reply FooBarBizBazz 7 hours agorootparentprevCongestion pricing is good IFF the money is used to fund transit in some way that eases said congestion. (Personally I believe this should mean mass transit and pedestrian/bicycle infrastructure, but that's not the main \"theoretical issue\" I'm trying to address here.) Otherwise you're just replacing a more egalitarian form of backpressure (the traffic itself) with a less egalitarian one (prices). reply timbo1642 15 hours agoparentprevIts nothing but a scam and anyone that supports it is completely heartless. It makes people poorer and nothing else. NYC has the national guard at its subways because its so dangerous. Its beyond dangerous still during many times. It is also always fucked up and is late or it breaks or other issues. Its not that reliable. Its just a scam to grab money from already broke people that are suffering and only awful people support this because they can't think for themselves and are followers. reply katbyte 13 hours agorootparentDid the subway get drastically worse in the last few years or something? Last time I was in NYC I stayed in Harlem and either biked or took the subway everywhere I needed to go. It was.. fine? reply woodruffw 3 hours agorootparentThe subway was in a \"crisis\" operating mode from 2017 to 2021[1]. I've lived in the city for my entire life, and subjectively the \"crisis\" label was accurate for that period (in terms of timeliness, breakdowns, station decay, overcrowding pre-COVID, etc.). That ended, in part, because the MTA received a huge operating boost from federal COVID funds, which are now (4 years later) running dry. So we haven't seen the return to the 2017 subway yet, but we almost certainly will if essential maintenance and system improvements are once again deferred to keep the lights on. As with everything about public infrastructure: the best and cheapest time to fund infrastructure is before it breaks. If the subway is allowed to continue to fall into disrepair, it'll only cost us more down the road. [1]: https://en.wikipedia.org/wiki/2017%E2%80%932021_New_York_Cit... reply thebigman433 12 hours agorootparentprevNo, but the media coverage surrounding cities has decayed to an absolutely abysmal level, so every single issue on the subway gets 48 hours of coverage or more, and the current governor is awful, so she thinks things like sending in the national guard to stand around will help reply lancesells 5 hours agorootparentprevNot at all. Subway seems to have less people on it but it's still very safe, especially considering it's a 24/7 train. reply immibis 7 hours agorootparentprevIt's just propaganda - downvote and ignore it, but don't ignore the worrying amount of propaganda spread implied by seeing this on HN. reply yunwal 2 hours agorootparentprevThe NYC metro is a far, far safer way to travel than driving. 121 people die in traffic yearly in New York City. In the highest-crime year in recent memory (2022) there were 10 deaths in the metro. Generally it's 2 or 3 yearly. reply sleepydog 2 hours agorootparentprevCan I ask, do you live in NYC and take the subway? reply PaulHoule 5 hours agoprevWhat I haven't seen mentioned is the political context at exactly this point in time. People are freaked out about inflation and the prices of everything rising and I think they're very sensitive to anything that is going to add more expenses. Even in Ithaca, NY apartment dwellers have developed a consciousness that rising property taxes are going to get tacked on to their rent. In terms of national politics, Democrats feel vulnerable, and I think Democrats everywhere feel pressure to drop unpopular policies that could affect them nationally. For that matter the rest of New York is rich in competitive House seats that could go either way. reply kgermino 5 hours agoparentThe problem is that congestion pricing has always gotten _much_ more popular after implementation. Riding the wave of decisions made in the past would let current politicians decide whether they wanted to blame others or take credit closer to the election. Now we just see a probably illegal power grab by an executive which kills a bunch of popular and needed projects while very publicly demonstrating that Democrats can’t fucking deliver. reply PaulHoule 5 hours agorootparentIn 2019 it split down the middle in NY state voters: https://scri.siena.edu/2019/03/18/2-3-of-voters-say-amazon-c... More recently https://nymag.com/intelligencer/article/poll-congestion-pric... they found 23% of people opposed the decision to suspend, and about 45% of voters supported it and that support was not very different for D's, R's and I's. reply kgermino 5 hours agorootparentNotably all of those polls are before it's implemented (by definition, since has not and likely will not be implemented). But it's not a new concept, and it turns out that people like being able to drive easily, having quieter streets, and tax money for other services. Once you go from fear of the unknown to seeing the results a lot of policies get more popular and in every other example we have that absolutely applies to congestion pricing. reply PaulHoule 4 hours agorootparentMy RSS reader just showed me this: https://phys.org/news/2024-06-unpopular-political-affect-ele... reply kgermino 4 hours agorootparentWhich seems to indicate that Democrats will face the heat for congestion pricing as long as it remains unpopular, right? And again, my point is that it will remain unpopular until it’s implemented, but it will almost certainly become popular once it’s put in place. All you’re saying is that they turned what would probably be a non-issue by the election into something that people will remember and be upset about. That seems like a horrible political strategy. I know I for one am much more likely to vote for a democracy supporting Republican (if they exist at this point) because of shit like this, and you’re articles says they won’t even get benefit from cancelling it reply developerDan 3 hours agorootparentprevThis is what frustrates me so much about American politics. Fear is the primary driver and prevents so much from happening. If it causes problems we can, gasp, undo the changes. If the tolls actually cause more harm there is a dead simple solution of: stop charging them. Instead NYC wasted hundreds of millions of dollars for literally nothing because of fear. reply PaulHoule 3 hours agorootparentNot just the US. Look at how Germany took a generation to fulfill its promise to end nuclear power, waffled so many times, and came across as not taking its climate change and security commitments seriously in the end. It is not so much that I think they should have banned or not banned nuclear energy but that the process they went through to do it was damaging to legitimacy. reply dwallin 3 hours agorootparentprevI don’t put much value in polling claims that don’t disclose how their question was phrased. The difference between slight differences in wording can be transformational. What this also doesn’t capture is how strongly people feel about these policies. It’s very easy for people to casually support the status quo. reply HanShotFirst 2 hours agoparentprevThe Governor is pretty clearly doing this with the intent of saving vulnerable House seats. HOWEVER, it is misguided IMO because the political capital to get congestion pricing to the finish line has ~already~ been expended. The Governor, most local Democratic politicians, have already come out in favor of the project and advanced it against heavy opposition. The common refrain, that is a very reasonable interpretation, is that this will be re-implemented after the next election. But if all the voters know that, if we elect the Democratic candidate, this possibly unpopular plan will be passed, delaying it until after the election doesn't help; in fact, it makes it less likely for Democrats to get elected, or for the plan to pass. In summary, Governor Hochul has now pissed off both opponents to congestion pricing, and also supporters of it. It was a terrible move, politically. reply breaker-kind 4 hours agoparentprevNY Dems are the epitome of the \"We hate life and ourselves\" simpsons gag reply rob74 5 hours agoparentprevSo the Republicans are already winning (influencing politics in their direction) although they haven't actually won yet, great... reply PaulHoule 5 hours agorootparentIt's what happens when you have an election. It's why Biden did https://www.politico.com/news/2024/06/04/biden-border-close-... If you steal enough of the other guy's issues you win. reply asveikau 5 hours agorootparentActually Democrats have a long history of doing things like that and then losing. Look how they abandoned single payer and a public option during the Obamacare period. Then they lost the Congress immediately after. Democrats do centrist or rightward pivots seemingly blind to the fact that Republicans will not vote for them anyway and that these actions will alienate the base. If they drew stronger contrasts and gave people a reason to vote for them, I think they would do better electorally. Sometimes it feels like they live in a fantasy world where it's still the pre Gingrich era and bipartisanship is much more of a thing. reply gennarro 15 hours agoprevThe article vastly underplays how mismanaged the MTA is and how poorly the subway is run on a massive budget. It is one of the last organizations that should be receiving this level of additional revenue, regardless of any sunk costs that have been put towards congestion pricing. This whole thing has been a mess for years so why double down on it when it would punish a set of residents and commuters and have very little chance of improving things for riders? reply lmm 14 hours agoparentThe MTA might be badly run by subway standards but it still gives you a lot more bang for your buck than roads in NY, once you include the market-rate cost of the space that those roads take up. reply scrubs 12 hours agorootparentAgree - only today I complained about the nyc subway on hn. But a car? Parking it? Tolls? Insurance? To drive around in traffic going no where fast? You're daydreaming. Taxi/Uber when needed and subway the other 90%. I'm two blocks from the subway with coffee and a diner on the way there. Man, it's awesome. I'm annoyed def some days more than others at the mta, but this is a no-brainer. Gd it America! Not everything can be low cost now/today and have long term soundness ... there are too many cars polluting too much in too small of an area going nowhere fast. As a would be business operator of the city, I damn near have a moral imperative to make some cash on that imbalance. I've spent many a $25 going 2 miles or less (say when raining) in a yellow cab. I always feel half stupid for paying it given how slow it is to move two lousy miles. reply josephcsible 14 hours agorootparentprev> once you include the market-rate cost of the space that those roads take up That's like saying that since land closer to Central Park is worth more than land further away, that we should demolish Central Park and replace it with more apartments. Property in NYC would be worthless if there weren't roads you could use to reach it. reply lmm 14 hours agorootparent> Property in NYC would be worthless if there weren't roads you could use to reach it. Most of the people who go to Manhattan get there by subway, bus, bike or walking (especially the people who have to work for a living), at a fraction of the space cost of those using cars. Access to property is certainly an important part of making that property valuable (hence why property around stations is worth more), but Manhattan spends far too much (in space) on roads for the amount of access throughput it gets in return. reply katbyte 13 hours agorootparentprevIf there wasn’t the MTA and subway the roads would be so congested as to be useless and New York couldn’t only support a fraction of its density reply leetcrew 14 hours agoparentprevidk, I honestly think congestion pricing would still be a good idea even if they just lit all the toll proceeds on fire. I almost wonder if it was a political blunder to couple \"reduce cars in lower manhatten\" with \"more funding for MTA\" in the first place. reply walterbell 14 hours agorootparentCongestion pricing has little impact on the unlimited number of cars (unlike regulated taxis) that can enter Manhattan and drive around for hours for Uber/Lyft. reply paulgb 5 hours agorootparentUnlike in other cities, Uber/Lyft in NYC are regulated like taxis and limos. NYC has been paying a congestion surcharge on each ride since 2019, which was set to go up as part of the new congestion pricing program. reply leetcrew 14 hours agorootparentprevI haven't yet chosen a side in the great uber vs taxi debate. but in general, driving lots of different people around all day sounds like a pretty good use of a car. it would be great to have a more comprehensive approach for pricing the least productive vehicle uses out of the most congested areas, but setting up tolls in a few choke points is a good start. reply walterbell 14 hours agorootparent> driving lots of different people around all day Perhaps academics or NYC can analyze and publish data from Uber/Lyft on average rideshare occupancy by time of day and GPS location, e.g. what percentage of time is spent driving around empty (consuming energy, polluting), carrying a paid passenger, or parked in an area close to expected customers. reply woodruffw 14 hours agorootparentprevSure it does, if they enter and leave the CBD. Cars coming and going from the UES and UWS, for example, would be subject to the charge. (But at the same time: I don't think anybody has claimed that it's a panacea. Congestion pricing can be both good policy and still have gaps.) reply walterbell 14 hours agorootparent> still have gaps There's always the opportunity for street/sector level surveillance via automated license plate readers. Your favorite neighborhood or street has too many cars? Impose quotas, vary the quota by time/season that only a computer can decipher, then sell \"Fast Pass\" exceptions to generate more revenue. Win for neo-feudal middleman, without brand licensing fees for the \"environment\" that justified a new digital on-demand toll economy. reply woodruffw 14 hours agorootparentI don’t understand what this has to do with congestion pricing, which AFAICT had a very simple (arguably too simple, per complaints about charging blue collar workers) fare schedule. (I also don’t think anybody would describe Manhattan’s CBD as their favorite neighborhood.) reply walterbell 14 hours agorootparent> what this has to do with congestion pricing Try a web search for geo-fencing, which is closely coupled to modern technology for enforcement of geo boundaries that are not gated by a physical barrier. Manhattan Central Business District (CBD) is defined as \"Manhattan south of and inclusive of 60th St\" excluding some through-traffic highways. > don't think anybody would described Manhattan’s CBD as their favorite neighborhood It includes neighborhoods such as these, https://en.wikipedia.org/wiki/List_of_Manhattan_neighborhood... Chelsea Chinatown Flatiron Greenwich Village Koreatown Little Italy Lower East Side NoLita SoHo Tribeca reply kevin_thibedeau 13 hours agorootparentprevRideshares were capped in 2018. Only EVs are exempt from the limit. reply walterbell 13 hours agorootparentThanks. Do you know how the limit is enforced? reply paulgb 5 hours agorootparentUber/Lyft drivers in NYC need special plates (that start with a \"T\"). The commission that regulates them can simply restrict how many plates it issues/renews. reply Arainach 14 hours agorootparentprevSource? It absolutely has impact, as shown in other cities which have implemented congestion pricing. reply walterbell 14 hours agorootparentImpact on Uber/Lyft drivers? reply slt2021 14 hours agoparentprevAgree, MTA must go bankrupt and restructure its liabilities, contracts, and overall cost structure to start from clean slate. Only after thorough and deep cleanup it would gain taxpayer trust and be on a get-well plan to be more sustainable financially. It would still operate in maintenance mode in the interim, while going through restructuring, NYC has no lack of financial restructuring professionals in downtown reply asah 8 hours agorootparentYou misunderstand the power of organized labor, the oligopoly of skilled labor able to maintain and adapt the legacy infrastructure and the shocking cost of bespoke manufacturing. I'm on the boards of two Manhattan buildings and see the numbers and can't even imagine what MTA goes through. reply slt2021 7 hours agorootparentIt is mafia, originally created by some “Italian families” of NYC. Time to break these regulatory capture for the public good and benefit of taxpayer, not the organized mob At least I would love to see the discourse in oublic and honest communication about actual issue and its root causes. Not some bogus “congestion”. There are actual people with names that have caused the mess reply asah 1 hour agorootparentIf anybody is reading this, the mafia hasn't been a material force in NYC for decades, rendering this argument full-on racist ranting. sit2021 hasn't a clue. reply katbyte 13 hours agorootparentprevYou can’t start from a clean slate with so much legacy infrastructure reply teractiveodular 13 hours agorootparentYou actually can: not the infra of course, but the operating organization. In Melbourne, Australia, the state-run Public Transport Corporation was an MTA-style mess and the trains in particular were notorious for delays. In 2009, the operating contract was handed over to the company behind the Hong Kong Metro, and punctuality has ratcheted up from the low 80s to the mid-90s; not quite HK-style 99%, but still a marked improvement, all on the same crusty old infra. https://www.metrotrains.com.au/metro-performance/ reply slt2021 13 hours agorootparentprevIts not about dismantling infrastruture, its about dismantling legal and contractual obstacles that keep MTA a financial disaster and corruption pit reply dang 13 hours agoprevRelated. Others? I'm sure there were others: With congestion pricing stop, New York City enters new era of economic gridlock - https://news.ycombinator.com/item?id=40625307 - June 2024 (150 comments) NYC Congestion Pricing Delayed Indefinitely by Governor Kathy Hochul - https://news.ycombinator.com/item?id=40587485 - June 2024 (16 comments) MTA board votes to approve new $15 toll to drive into Manhattan - https://news.ycombinator.com/item?id=39841703 - March 2024 (831 comments) NYC's plan to ease gridlock? A $15 toll for Manhattan drivers - https://news.ycombinator.com/item?id=39549726 - Feb 2024 (24 comments) New York approves the first congestion toll in the US: $15 to enter Manhattan - https://news.ycombinator.com/item?id=38583532 - Dec 2023 (155 comments) NYC Congestion Toll Could Cost Drivers $23 per Trip Starting Next Spring - https://news.ycombinator.com/item?id=36512248 - June 2023 (41 comments) New York City will charge drivers going downtown - https://news.ycombinator.com/item?id=36270597 - June 2023 (671 comments) reply goodSteveramos 15 hours agoprev>Increasing the density of jobs, hospitals, restaurants, homes, schools, and more is key … as density grows mass transit becomes essential This guy has it all backwards. No developer in his right mind is going to plow billions of dollars into high density housing without existing rail access. The new subway and commuter lines have to be built first, then the high density development becomes viable around it. reply rallison 14 hours agoparentBoth what you say and what the article says is right. To the article's point, in urban areas, increased density often happens even when mass transit isn't expanded (which then often leads to ever worsening traffic). So, for urban areas that are organically becoming denser, mass transit becomes ever more important, which was partially the article's point in the section you quoted. Also, to generously read the article, it seems to be making two related points, but not actually imposing a \"one should follow the other\" ordering that you read into it. The full paragraph: > Globally, that is nothing notable—in most urban cores a majority of workers take public transportation for work and daily activities. Increasing the density of jobs, hospitals, restaurants, homes, schools, and more is key to the agglomeration effects that make cities such economic powerhouses, and as density grows mass transit becomes essential since it can far surpass the maximum throughput capacity of even the largest roadways. It is saying two things: 1) high density is critical to be an economic powerhouse and 2) mass transit becomes even more essential as density increases. That paragraph isn't inherently saying mass transit should follow densification (vs preceding densification). Ideally you build out mass transit in anticipation of densification vs playing catch up. Anyway, to your point, it's absolutely true that building out mass transit is critical in attracting more high-density development (especially very high-density development), and critical in enabling high density development in places that otherwise might not attract that sort of investment. reply tsimionescu 13 hours agoparentprevNot living in the USA, but I have never heard of the idea of a city drawing transportation lines to an empty area in anticipation of new buildings. Transportation is always reactive in my experience, especially if we're talking about costly things like metro lines or trams. You can't predict what will attract people to some area, so building a line to nowhere and expecting developers to move there because an unused line now exists is a waste of public money. reply teractiveodular 13 hours agorootparentThe reason places like Tokyo, Hong Kong and Singapore have such great public transport is precisely because the rail planners work hand in hand with urban planners. Railway construction is largely financed by property sales (both residential and commercial) and when done well is hugely profitable, as the former richest man in the world, Yoshiaki Tsutsumi of the Seibu Corporation, can attest. Retroactively building a subway line in an already densely populated area is a hugely expensive exercise, as most recently demonstrated in NYC itself with the 2nd Ave extension. reply tbihl 2 hours agorootparentI think this is pretty commonly known, but the railroads built the US American West. It was a hugely speculative endeavor where railroad companies would buy up worthless land for cheap, hire unfavored immigrants and work them harder than anyone else would work, and then recoup costs by selling the land of the railroad towns. reply stetrain 1 hour agorootparentprevAt a certain point in time, building new streetcar lines and \"streetcar suburbs\" along those lines was fairly common in US cities. Often the property developer and the streetcar operator were the same company or ownership: https://www.planetizen.com/definition/streetcar-suburbs Most of those have since had the streetcar lines removed or abandoned as cheap cars and gas replaced them, and the incentive for the original developer to maintain them went away. reply Symbiote 10 hours agorootparentprevSee this article for the original metro in London, which was built exactly like this, though with private money. https://en.wikipedia.org/wiki/Metro-land Nowadays it's common to build a new station in an area that needs redevelopment, perhaps a former industrial area. reply occz 12 hours agorootparentprevThe ongoing transit expansions in Stockholm were green-lit on condition that the municipalities getting increased transit access would invest in building housing in the vicinity of that transit. You can definitely predict what will attract people to an area - rapid access to everything that the downtowns of cities provide is one such thing. reply pif 11 hours agorootparentThe same happened where I live, in France, next to Geneva. Actually, here the municipalities were not even requested to invest, but just to allow higher density. reply PaulHoule 7 hours agorootparentprevMy wife and I were going to a graduation party in Maryland and figured we’d stay in a hotel near the end of the metro line and ride into DC to see the National Mall, Union Square, etc. The Shady Grove end of the red line has a large development of mixed commercial and residential of buildings that are uniformly about 5 stories (I think) tall that was built to go with the transit line, I understand it is like that in Virginia too. China is famous for building metros before building the rest of the city. reply nox101 12 hours agorootparentprevMany cities in and around Los Angeles cities built around train stations. https://www.amazon.com/This-Pacific-Electric-Stephanie-Edwar... reply xethos 6 hours agorootparentprevThe lower mainland (just outside Vancouver) has the Millenium Line extension. It was, when planned and built, called a train to nowhere. It doesn't go to nowhere anymore :) reply CalRobert 8 hours agorootparentprevThe Netherlands and China has built transit at the same time as (or before) new areas are developed. reply imtringued 9 hours agorootparentprevLand near public transport services is generally prime real estate. The only developers who wouldn't jump on this opportunity are the ones that hate money. reply skybrian 14 hours agoparentprevOne way to do it is for the transit authority itself to build a large amount of real estate next to or over the new station and lease it out. The costs are front-loaded, though, so a large amount of financing (like selling bonds) will be needed. reply TrainedMonkey 15 hours agoparentprevIt's a chicken egg problem... as usual there is no right solution. China is the largest example of building transit infrastructure first, my understanding is that there are a lot of issues there, but most of them are due to how the effort was structured. On the other hand when housing is built first there is a lot of demand for transit infrastructure, but it becomes extremely hard and expensive as seen in every metro area in USA. reply goodSteveramos 14 hours agorootparent>when housing is built first there is a lot of demand for transit infrastructure Not if it’s low density housing, which it will be if there is no transit. reply imtringued 8 hours agorootparentOnce the houses are built and people have moved in, they are going to resist new construction. It should be pretty obvious to anyone. reply seoulmetro 15 hours agorootparentprevIn East Asia the problem is that the developments are so good, and nearly always get done with support for infrastructure that there are so many cronies in the government or with insider knowledge that can make bank off the outcome. Leads to some serious issues. I highly, highly doubt it would be harder for those problems to happen here. In fact I think if it ever happened, it would be a huge corrupt mess. reply leetcrew 14 hours agoparentprev> This guy has it all backwards. No developer in his right mind is going to plow billions of dollars into high density housing without existing rail access. this is the default option? build a big complex with a courtyard, then surround it with a parking lot or dedicate 1/4 of the building to a garage. I'm interested to see what happens with the new potomac yards wmata stop. I appreciate they are trying to expand rail in anticipation of growth, but it looks kinda dumb right now. it's a good 15 minute walk through parking lots to get to the nearest store or apartment building. you would never use that stop if you could afford a car. to be fair, the station just opened. it could look very different in 5-10 years. if they waited for it to densify first, people might be complaining about how much more expensive the station was. reply CalRobert 8 hours agoparentprevIf this is true, then why has almost the entire US worked to make it illegal to build high density housing? Presumably they wouldn't have bothered if nobody was trying to build it? reply redandblack 1 hour agoprevI hope congestion pricing withers away and dies. It really promotes public infrastructure for the wealthy and disadvantages the people who cannot afford. MDs at my firm can easily afford any such surcharges, and for many it will be business write offs, and not even their own. Fund public infrastructure - put more buses on roads, maybe even limit some avenues and roads for buses only. Reduce the costs - yes, I know NYC tickets are really low, but make them simpler and easier for people to get a monthly pass and hop on buses, Really, buses everywhere even if they operate at net losses - get the business to subsidize bringing their workers in. reply stetrain 1 hour agoparentI agree that there is a limit to the \"just charge people what each option costs and let market forces sort it out\" capitalist utopian idea. Some are willing to pay large amounts for convenience that causes harm to many others. At some point we just have to make decisions about land use and infrastructure. Do we want this space dedicate to driving and parking for cars with one person who can afford to pay congestion charges, or instead used for busses, dedicated bike roads, etc. that can move more people in less space with less pollution and less risk of serious injury. reply jimby 15 hours agoprevInteresting that the major cities with sizable public transport user populations are all in the northeast hub. Boston, NYC, Philadelphia, DC. Growing up and living here, cars aren't really necessary day to day in any of these cities. I'm shocked when I travel to basically any other major-ish US city, especially \"newer\" big cities like Denver, Phoenix, Miami, Tampa. The best",
    "originSummary": [
      "Governor Kathy Hochul announced an indefinite pause on NYC's congestion pricing scheme, which aimed to charge vehicles entering Lower Manhattan to fund transit projects.",
      "The cancellation, driven by opposition from suburban drivers and concerns about the MTA's spending efficiency, jeopardizes key infrastructure projects and funding.",
      "This decision underscores broader issues in American cities and infrastructure, highlighting inefficiencies, high costs, and local opposition in public transit investments."
    ],
    "commentSummary": [
      "NYC's congestion pricing plan, intended to reduce traffic and fund public transit, has been stalled by state politics.",
      "Critics cite the MTA's mismanagement and high costs as reasons against additional funding, while supporters argue it could improve traffic flow and public transit.",
      "The debate underscores broader urban planning and public policy issues, balancing convenience with sustainability."
    ],
    "points": 159,
    "commentCount": 320,
    "retryCount": 0,
    "time": 1719540106
  },
  {
    "id": 40819784,
    "title": "Is Clear Air Turbulence becoming more common?",
    "originLink": "https://www.flightradar24.com/blog/is-cat-more-common/",
    "originBody": "SODPROPS: aviation’s best acronym?",
    "commentLink": "https://news.ycombinator.com/item?id=40819784",
    "commentBody": "Is Clear Air Turbulence becoming more common? (flightradar24.com)147 points by redtriumph 6 hours agohidepastfavorite88 comments w14 4 hours agoThis does not seem to be borne out by the accident statistics, which apparently show no trend in turbulence related accidents. (https://www.ntsb.gov/safety/safety-studies/Documents/SS2101....) I don't know if there are other factors which might be masking a rise in incidence of CAT from accident stats? reply _moof 1 hour agoparentAn increase in the frequency of clear air turbulence doesn't necessarily entail an increase in reportable accidents and incidents. The NTSB is only notified when a specific set of criteria are met. See 49 CFR Part 830 for details. If the increase in turbulence is all light to moderate turbulence with no serious injuries, there's nothing to report to the NTSB. reply alfalfasprout 1 hour agorootparentThis deserves to be the top comment. Turbulence accident statistics are only going to ever reflect clear air turbulence if the aircraft sustained detectable damage or passenger(s) sustain serious enough injuries. reply User23 7 minutes agorootparentIn that case what’s the explanation for clear air turbulence events having an exactly proportionally lower rate of causing damage and injury such that the rate has remained flat despite the increase in events? What’s making turbulence safer? reply cpncrunch 4 hours agoparentprevAlso, they just looked at 2 years, so there could be cherry picking. Jet stream is affected by el nino. 1979 was weak el nino, 2020 was moderate la nina. https://ggweather.com/enso/oni.htm reply dclowd9901 3 hours agorootparentRight, and those two years are generationally distant. Another question I have is if pilots have a stable mechanism (that is, an unchanged objective sensor or something) that records the CAT or if it’s recorded by pilots, whose sensitivity to CAT might differ over time. Didn’t mention in the article how it’s measured. reply throwup238 2 hours agorootparentCATs are recorded in pilot reports using terms like \"light\", \"moderate\", \"severe\", and \"extreme\" which each have a definition. I.e. severe is \"Occupants are forced violently against seat belts or shoulder straps. Unsecured objects are tossed about. Food service and walking are impossible.\" I think only severe and extreme turbulence need mandatory reports and the lower two levels are a bit more subjective (\"Food service and walking are difficult\") Newer planes have sensors to measure eddy dissipation rates which are an objective measure of turbulence but I don't know how widespread those systems are and whether they get reported anywhere. They're mostly used for long distance transoceanic flights. reply robxorb 3 minutes agorootparentSo this could be a trend in pilot reporting rather than turbulence? reply oceanplexian 1 hour agorootparentprevUnsecured objects being tossed around has no real meaning, it’s about controllability of the aircraft. I’m a pilot and it’s been a while since I went over PIREPS but generally severe is rarely used, severe means the turbulence is so bad you can no longer control the aircraft. What most passengers imagine as severe is probably light turbulence. Most of the time it’s not even reported. As a side note if you’re ever on an aircraft and not secured at all times, you’re making a huge mistake. reply makestuff 54 minutes agorootparentHave there been any reports of true severe clear air turbulence (where the pilot cannot control the plane) or are all of these cases not technically severe because the pilots were in control the entire time and it was just a bumpy ride? Another question I have is what do you do in that scenario if you can't control it? Just ride it out and hope for the best? reply orhmeh09 33 minutes agorootparentprevIs using the bathroom or stretching your legs advisable? reply throwup238 13 minutes agorootparentYou don't have to live in fear of turbulence when flying, just keep your seatbelt on when you're seated. Turbulence is fairly rare but it's still a numbers game. The probability that you experience it the 99% of the time you're seated is much higher than the probability of experiencing turbulence while standing, especially since pilots proactively turn on the seatbelt sign when turbulence is expected. reply withinboredom 1 hour agorootparentprevI watched Cast Away. I always wear my seat belt. reply ImaCake 3 hours agoparentprevCertainly no detectable trend in that data. But the accident frequency is so low that the random variation dominates and makes it impossible to distinguish any trend. What is demonstrably increasing is CAT, due to climate change. But considering how infrequent these incidents are we might not see a clear increase for several decades. reply vlovich123 3 hours agorootparentDo we really not record turbulence sensor data off the airplanes and download it when they’re on the ground? I’m also surprised that these airplanes have on demand satellite TV streaming to these airplanes but airlines claim that it costs 100k to add that to existing planes. There’s just no way it’s 100k per plane - there must be a cheap way to retrofit the data without having it be reliable since it’s opportunistic. And heck, France is doing it every 4 minutes for their planes so why can’t Americans figure out how to do it. reply MadnessASAP 3 hours agorootparent> Do we really not record turbulence sensor data off the airplanes and download it when they’re on the ground? No, accelerometer data is only recorded to the FDR. Which has a limited storage window (1-24 hours depending on the aircraft) and is slow to download requiring moderately specialized equipment and a technician to carry out the task. Aircraft downtime and technician hours are both expensive and in short supply. > I’m also surprised that these airplanes have on demand satellite TV streaming to these airplanes but airlines claim that it costs 100k to add that to existing planes. There’s just no way it’s 100k per plane - there must be a cheap way to retrofit the data without having it be reliable since it’s opportunistic. And heck, France is doing it every 4 minutes for their planes so why can’t Americans figure out how to do it. Everything on airplanes is expensive. Even cabin amenities. You have to prove it won't start a fire, was installed correctly, won't interfere with other equipment, won't interfere with the aircrafts structure, and again requires technician hours and aircraft downtime. reply vlovich123 1 hour agorootparentI find it hard to believe that the headsets they are using for software and hardware meet that level of criteria. They’re clearly using off-the-shelf parts. Some amount of care is called for sure, but 100k to apply a software patch or tweak the tech in FDRs which are swappable and upgradable? A flight recorder is 10k. You can’t tell me it costs 90k to install a new one capable of sending data over the satellite link in bursts. Clearly other countries and airlines with a similar safety record and cost of living and salaries are able to accomplish the feat. reply throwaway-blaze 2 hours agorootparentprevWell, you have to have paperwork claiming it was installed correctly. You don't _have_ to install it correctly. See e.g. doors installed so correctly that they blow out in flight etc. reply lucianbr 2 hours agorootparentWhat's the point of your comment? Should we just install a bunch of accelerometers on planes with faked paperwork because some people faked some paperwork somtime? Sounds like you're upset at Boeing and figured you would tell us you're upset on an unrelated thread. Note that it doesn't really matter if you are right to be upset at Boeing or not. It's still unrelated. reply renewiltord 1 hour agorootparentI think his point is that the proof is expensive, not the act itself. Reminds me of rivets in composites joined by adhesives. The benefit is inspectability. The cost is diminished strength. reply dheera 2 hours agorootparentprev> No, accelerometer data is only recorded to the FDR. Which has a limited storage window Apple and Google could fix this my streaming accelerometer data to the ground when people are connected to in-flight wifi. It is fairly easy to identify which phones out of a set are the stationary ones. reply supportengineer 2 hours agorootparentprevAlmost every single passenger is carrying an accelerometer with them. We just have to use that data. reply TylerE 3 hours agorootparentprevIf satelite TV is down, some passengers are mildly annoyed. If a regulatory required part of the aircraft is non-functional, you're not going flying today. reply vlovich123 1 hour agorootparentYou’re saying it’s impossible to have an optionally required feature? If the satellite TV is down some sensor data isn’t sent. Why would that be cause for grounding the plane? You could easily make the regulation an SLA like all routes flown must be sending data for 90% of the flights on that route for the month and failures to meet the SLA are investigated. Also I’ve flown a bunch and I’ve rarely seen the Internet link go out except where there’s technical limitations like crossing the ocean where they can’t maintain an internet and have to rely on preprogrammed content. Given how much money they make from cabin internet, the airlines are clearly incentivized to apply pressure to keep those things running. I doubt I’ve seen anyone be really annoyed when there’s technical difficulties. Most people who fall into that category would have made other arrangements for entertainment anyway. reply rdtsc 1 hour agoparentprevCould be an inverse relationship, too - the more frequently they occur the more experience, training and guidance the pilot, and the other crew members get to manage it: how to control the the airplane, urge passengers to wear seatbelts more, etc. reply eggy 4 hours agoparentprevYeah, you should have more concern over human error and Boeing than this. But, boy, the more I have flown and the older I am, the more I get anxious during turbulence when I fly. reply FabHK 3 hours agorootparentI relax when it's turbulent. The airframe can handle it [1], and at least the pilots are awake. [1] There's a speed limit for turbulence penetration, chosen such that the wings will stall, rather than over-stress the airframe. reply stouset 3 hours agorootparentprevWhen was the last time a commercial airliner crashed due to turbulence? reply rootusrootus 3 hours agorootparentBest I can tell, 1966. https://en.m.wikipedia.org/wiki/BOAC_Flight_911 reply stouset 1 hour agorootparentThen I think we can safely worry about other things than turbulence :) reply septic-liqueur 17 minutes agorootparentBut you never know... You might be the first one :-) reply vanderZwan 4 hours agoparentprevWell, assuming that there is indeed more turbulence, that could also mean more vigilance against accidents, so that could even itself out as safety regulations get stricter than in the past. Also, this is mentioned in the conclusions: > The report includes an important discussion of the risk to unrestrained occupants onboard aircraft, including flight attendants – who account for nearly 80% of those seriously injured in turbulence-related accidents. Key recommendations in the report are intended to help ensure better protections for flight attendants ... which makes me think of two more possibilities: 1 - I suspect any careless flight attendant involved in a turbulence-related accident would learn their lesson after the first time, and take better safety precautions. Perhaps that is a stabilizing factor on the number of accidents, since the number of flight attendants who need to learn that lesson the hard way is probably more a function of how many new flight attendants enter the field than it is a product of how much turbulence there is. 2 - Flight attendants under-report minor accidents so they don't get into trouble for not respecting safety rules Of course, this is pure speculation (and assuming that the premise of there being more CAT incidents holds up), I'm sure the actual document goes into this kind of thing in more detail but I don't have the time to dig through 115 papers. reply FabHK 3 hours agorootparent> I suspect any careless flight attendant involved in a turbulence-related accident would learn their lesson after the first time, and take better safety precautions. They are briefed, no need to learn their lessons after the first time. However, it's part of their job to walk around the plane (eg to serve food), and so they're less likely to be seated than pax. That is the (rather obvious) explanation for the fact that they constitute a very high proportion of victims, not \"careless\"ness. reply thisisauserid 1 hour agoprevI had never heard of Clear Air Turbulence until last month when I finally read the first Culture novel by Ian M. Banks: Consider Phlebas. Now I know that it's the perfect name for a space pirate ship. reply thiel 6 hours agoprev> The Prosser report outlines one of the primary reasons for the increase in CAT events as the intensification of the jet streams, driven by the warming of the planet. As global temperatures rise, the temperature gradients between the equator and the poles become more pronounced, strengthening the jet streams and increasing the likelihood of turbulence . I was under the impression that, as the poles are MORE affected by global warming, the jet stream is becoming weaker? is that incorrect? reply the_sleaze_ 6 hours agoparentYou might be thinking of the Gulf Stream, which is an oceanic current and definitively weakening, rather than the jet stream, which is an air current. reply wongarsu 4 hours agorootparentThe jet stream is getting stronger (and its path becomes more erratic), but the reasoning provided in the article is simplified to the point of being wrong. The projected warming at the North Pole is much stronger than the projected warming at the equator, decreasing the temperature gradient. However the moisture carrying capacity of air increases exponentially with temperature. Since the equator starts warmer, a given change in temperature has a bigger effect on moisture carrying capacity. It turns out that heating up the equator by one degree Celsius and the North Pole by 2 degrees Celsius increases the moisture capacity gradient, despite the temperature gradient dropping. And that increasing moisture capacity gradient strengthens the jet stream. (at least that's the intuitive reason they were probably going for. In reality there are many factors and a good bit of \"if we simulate it this keeps happening\") reply BlueTemplar 5 minutes agorootparentThanks for the detailed explanation. But I also thought that it was the jet stream getting weaker that caused it to meander more (which sounds like it could increase CAT events ??), which we seem to be observing ?? reply quakeguy 5 hours agorootparentprevExactly, air currents are becoming stronger with warmer conditions, but the gulfstream being a water current it is expected to become weaker. reply HPsquared 5 hours agorootparentprevThe jet stream is also driven by temperature differences. Same with most wind and weather, it's all various forms of heat engine. (Edit: though apparently the additional moisture in the Tropics more than counteracts any reduction in temperature difference: see link in Retric's comment) reply rob74 5 hours agorootparentprevTo muddle things even further, there is a (jet) aircraft manufacturer called Gulfstream (https://en.wikipedia.org/wiki/Gulfstream_Aerospace). ...but I fully agree with the rest of your comment. reply polar_low 5 hours agoparentprevThat is correct, as a longer term trend at least while paradoxically, we are also seeing periods of record strength in the Jet Streams. The truth is there are many oscillations and teleconnections(themselves being impacted by global warming) which influences this temperature gradient on a local/seasonal basis. QBO, El Nino/La Nina and mountain torque events to name a few can move and shift heat at the tropopause in a short period of time and is why we see this wider variance at both ends of the spectrum. reply photochemsyn 1 hour agoparentprevI think the seasonality of the polar environment is a critical factor - while warming is expect to decrease the average equator-to-pole temperature gradient (as all models predict faster polar warming than equatorial warming by a large margin), winter is still winter as the polar axis is tilted, so steep atmospheric gradients are expected over that seasonal period. reply badcppdev 6 hours agoparentprevGot some references? reply mcmcmc 5 hours agorootparentThey are cited in TFA https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1029/202... reply konschubert 5 hours agoparentprevThat’s also what I read: The poles warm FASTER than the equator. Thus, the global temperature gradients are getting smaller. And as a result, not only does the Jetstream weaken: as a result, weather patterns become more stable which leads to greater continuous periods of draught or flooding. reply Retric 5 hours agorootparentYour intuition is incorrect. Global warming increases the jet stream. “The new study, by University of Chicago Professor Tiffany Shaw and NSF NCAR scientist Osamu Miyawaki, uses climate models to show that climate change intensifies this density contrast because moisture levels for air above the tropics will increase more than above the poles.” https://news.ucar.edu/132935/jet-stream-winds-will-accelerat.... reply elevation 4 hours agorootparentprev> The poles warm FASTER than the equator The article cites the Prosser Report which contradicts this claim, but I find it hard to understand how this could be true for very long. Why wouldn't the atmosphere stabilize as gradients diminish? reply photochemsyn 1 hour agorootparentBecause the earth's rotational axis is not perpendicular to incident sunlight (hence dark polar winters). As winter sets in, gradients steepen relative to warming equator. It's all complicated by the general increase of atmospheric water vapor as warming proceeds, which can have different effects depending on whether the water vapor is gaseous or forms cloud droplets, which reflect sunlight. It's a hard physics problem. reply coldtea 5 hours agorootparentprevjetstream != gulf stream reply HPsquared 5 hours agorootparentJet stream is also driven by temperature differences, like basically all weather. Heat engines. reply Retric 5 hours agorootparentIt’s a heat engine but more than just temperature changes are occurring. “The new study, by University of Chicago Professor Tiffany Shaw and NSF NCAR scientist Osamu Miyawaki, uses climate models to show that climate change intensifies this density contrast because moisture levels for air above the tropics will increase more than above the poles.” https://news.ucar.edu/132935/jet-stream-winds-will-accelerat.... reply HPsquared 5 hours agorootparentFair enough, I guess it's a pretty non-linear system! reply siffin 4 hours agorootparentI also think some of the confusion is coming from the use of the term 'weakening'. It is true that the primary jetstream wind pattern is weakening relative to it's stabler state. That weakening means the jetstream meanders more, with more latitudinal movement in its form. The strength overall of the jetstream wind is weaker when it's meandering, but can also be much more intense in places. This says nothing about humidity or energy or pressure, just windspeed and direction. reply Octabrain 4 hours agoprevI hate flying with passion and get extremely scared when flying through turbulences but, there was a journalist in my country, that also had experience as a pilot and said once on TV that during turbulences, is one of the safest moments in a plane. I don't remember the reasons but is there anybody in here with knowledge in the field that could confirm/deny this? reply ibejoeb 4 hours agoparentThat's just not true. No turbulence is better than turbulence. That said, experiencing light chop on a modern large airplane presents no danger to the airframe or properly secured passengers. You really should be strapped in, though, especially if you're on a small plane. Wake turbulence, for example, actually does present a significant risk to smaller aircraft. reply mrWiz 4 hours agoparentprevDo you recall why they said it was the safest? My first guess is because the pilots are paying more attention while flying through turbulence. reply MichaelNolan 4 hours agorootparentMy first guess is that during turbulence everyone has their seatbelt on. No one is walking around the cabin. It’s only at the start of unexpected turbulence that anyone should get hurt. Once your seatbelt is on things have to get pretty bad to get hurt. reply sandworm101 4 hours agoparentprevIt is safe because you are flying. Airplanes almost never have issues at altitude. Problems occur when closer to the ground. Landing/takeoff are the most dangerous times, the transitions between flying and not flying. reply FabHK 2 hours agorootparentThe aviation industry has a perfect record, they've never left anyone up there. reply ThinkingGuy 3 hours agoparentprevAirline pilot/author Patrick Smith has a pretty informative article about turbulence on his website: https://askthepilot.com/questionanswers/turbulence/ reply aidenn0 3 hours agorootparent\"Fewer than forty feet of altitude change\" A 40 foot drop is pretty scary; by comparison, an NCAA dive platform is 33 feet. reply dghlsakjg 2 hours agorootparentThe standard that you must meet to get a pilots license is being able to hold your altitude within 100 feet in a 360* turn. A 40 foot He's not saying the drop was 40 feet instantaneously, he's saying the turbulence and the subsequent recovery only caused a 40 foot deviation from the assigned altitude. Just for reference, a descent rate on a standard flight is pretty normal at 40 feet per second. Some descent profiles can double that. The NCAA diver will hit the water at 46 feet per second. reply antoniojtorres 2 hours agorootparentprevI agree. It’s funny to think of the gap between safety and scary when talking about turbulence. Large planes can take an absolute beating and be completely fine, but it could feel like the end of the world inside. reply oceanplexian 1 hour agorootparentA modern airliner is rated to something like 3-4G's including the safety factor, probably much more if it's not at max takeoff weight. I'm sure you could do a Mythbusters-style test but I'd assume most passengers would pass out from the negative and positive G forces long before the aircraft structurally failed. reply bparsons 4 hours agoparentprevCommercial airliners are built for extraordinary stress on the airframe. You can get a taste of it here: https://www.youtube.com/watch?v=--LTYRTKV_A Other people will correct me if I am wrong, but I believe the last time a large commercial airliner was lost to turbulence was 1966. https://en.wikipedia.org/wiki/BOAC_Flight_911 reply loopdoend 2 hours agorootparent> Film footage shows Flight 911 taxiing past the still-smoldering wreckage of Flight 402 immediately before taking off for the last time. Wild how far we've come. reply MisterTea 1 hour agoparentprevI feel the same way about flying but a boating enthusiast friend bought up an interesting analogy. He asked me if I enjoyed boating and I said yes. Then he asked me if it was fun when you run over waves bouncing around and I said yes. Then he said that is exactly what turbulence is - wakes and waves in the air the plane is bouncing on so relax and enjoy the ride. Kinda made me feel a little better since I could now visualize what is going on but still - eh, Id rather be on terra firma. reply nashashmi 2 hours agoprevOne day we will research a boson particle that can be fired at the air and cause an abrupt polarization allowing for planes to travel through with very little air resistance. reply bparsons 4 hours agoprevThere is also just a huge increase in global air travel, which should increase the number of total incidents. The number of commercial flights doubled between 2004 and 2019, and is expected to continue on that trend for some time. reply osipov 5 hours agoprevCommercial pilot here. Instead of climate change, we should be talking about continuous descent profiles (CDPs) that have become more common in the past years 5-10 years. These profiles with idle engines allow for a smoother, more fuel-efficient descent by reducing the need for level-off segments. However, CDPs can increase the perception of turbulence during descent. This is because aircraft remain at higher altitudes for longer periods, where atmospheric instability and wind shear are more pronounced. This increased turbulence is not due to climate change but rather the result of these optimized descent procedures aimed at reducing fuel consumption and minimizing environmental impact. reply piombisallow 5 hours agoprevIs there anything global warming can't do? reply triceratops 5 hours agoparentConvince some people it exists. Whether it's responsible for more CAT - who knows? reply bromuro 3 hours agoparentprevI prefer to call it climate change, as it is a change of the conditions where everyone lives. As such, it is expected to affect everyone life. reply sandywaffles 4 hours agoprev> [Clear Air Turbulence] is particularly common around the tropopause, the boundary layer between the troposphere and the stratosphere, at altitudes between 7,000 and 12,000 meters (23,000 to 39,000 feet) . Oh, excellent the altitudes that 99% of aircraft fly at, unaffecting the ultra rich who fly private jets at 40,000k-50,000k+. reply hiatus 4 hours agoparent> Oh, excellent the altitudes that 99% of aircraft fly at, unaffecting the ultra rich who fly private jets at 40,000k-50,000k+. I didn't realize private jets fly so high. What's the reason for the difference in elevations? reply pc86 4 hours agorootparentPrivate jets don't really fly at 50k that often but I'm sure there are some that can. 40-42k is pretty common though. There are a handful of reasons. In no particular order: 1) additional separation between recreational private flights and scheduled commercial flights 2) higher performance in small private jets with typically less than 1k lbs of people and cargo 3) winds are generally faster the higher you go so you'll usually go about at high as you're able, at least in one direction. Most private jets aren't someone flying a billionaire around in a $60M Gulfstream, they're $3M toys being flown around by the owner to go to their ski trip. reply nickjj 4 hours agorootparent> 3) winds are generally faster the higher you go so you'll usually go about at high as you're able, at least in one direction. I recently flew from NYC to Lisbon, Portugal and it was 6.5 hours there (flying east) and 7.5 hours back (flying west) because you go with and against the jet stream. I wonder if there's a noticeable difference if you fly at a lower altitude against the wind. It didn't seem like the plane adjusted for that, it cruised at the same altitude both ways from what I remember. Both flights used the same exact plane type (A330neo). reply FabHK 3 hours agorootparentprevNote also that at higher altitudes the air is less dense, but the plane should fly at the same indicated/calibrated airspeed to generate the same lift, which means that it flies faster (at higher true airspeed). Basically, with aircraft mass and angle of attack unchanged, \\rho v^2 must be constant, so smaller density \\rho -> higher air speed. reply ddoolin 4 hours agorootparentprevTraffic separation. Since they can fly at those altitudes, it makes sense to put them there, away from other commercial airliners. reply zamadatix 4 hours agoparentprevYou can fly over the tropopause depending how high it is at the specific location (can be lower or higher than the numbers listed) but, by definition, it'll only be \"common\" to observe turbulence in the range 99% of planes actually fly. I wouldn't read too much into that. As some others pointed out the height difference probably more due tot he space being faster and unused since commercial flights stay to where is more efficient. reply FabHK 3 hours agoparentprevThe A320 has a ceiling of 39,100–41,000 ft, the 737 of 37,000 or 41,000 ft, the 747-8 and A350-900 and A380 of 43,100 ft. reply cinntaile 4 hours agoparentprevWhy do they fly higher? I expected them to fly lower. reply pc86 4 hours agoparentprevAh yes those pesky billionaires putting the tropopause right below where all their private jets fly. reply nytesky 5 hours agoprev [–] I wonder if this is like an immune system response by mother nature, it’s attacking the thing that’s warming it up i.e. air travel? Self correcting systems reply thfuran 5 hours agoparentNo, it just turns out that things get more energetic when you dump a lot of energy into a system. reply gosub100 5 hours agoparentprev [–] couldn't possibly be due to more planes flying more flights. reply piombisallow 5 hours agorootparent [–] They haven't actually analyzed reported turbulence, it's just a simulation study: \"Turbulence data from aircraft could also be analyzed, but the time period for which quantitative, automated measurements are available is far shorter than the 42 years covered here, making trend detection problematic.\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Clear Air Turbulence (CAT) incidents are not showing a trend in accident statistics, as the NTSB only reports significant incidents.",
      "Factors such as climate change and increased air travel may contribute to more frequent CAT, but newer planes with advanced sensors and pilot reporting make it difficult to measure accurately.",
      "The jet stream's behavior, influenced by climate change, also affects CAT frequency, but modern planes are designed to handle turbulence, and pilots are trained to manage it."
    ],
    "points": 147,
    "commentCount": 88,
    "retryCount": 0,
    "time": 1719576296
  },
  {
    "id": 40818809,
    "title": "What is the best code base you ever worked on?",
    "originLink": "https://news.ycombinator.com/item?id=40818809",
    "originBody": "And what made it so good?Was there someone enforcing good practices top down? Just being in a group of great engineers? Or something else?",
    "commentLink": "https://news.ycombinator.com/item?id=40818809",
    "commentBody": "What is the best code base you ever worked on?134 points by pcatach 10 hours agohidepastfavorite111 comments And what made it so good? Was there someone enforcing good practices top down? Just being in a group of great engineers? Or something else? pradn 1 hour agoGoogle's monorepo, and it's not even close - primarily for the tooling: * Creating a mutable snapshot of the entire codebase takes a second or two. * Builds are perfectly reproducible, and happen on build clusters. Entire C++ servers with hundreds of thousands of lines of code can be built from scratch in a minute or two tops. * The build config language is really simple and concise. * Code search across the entire codebase is instant. * File history loads in an instant. * Line-by-line blame loads in a few seconds. * Nearly all files in supported languages have instant symbol lookup. * There's a consistent style enforced by a shared culture, auto-linters, and presubmits. * Shortcuts for deep-linking to a file/version/line make sharing code easy-peasy. * A ton of presubmit checks ensure uniform code/test quality. * Code reviews are required, and so is pairing tests with code changes. reply tallowen 11 minutes agoparentI always find these comments about interesting, having worked at Facebook and Google, I never quite felt this way about Google's Monorepo. Facebook had many of the features you listed and quite performantly if not more so. Compared with working at Facebook where there are no owners owners files and no readability requirements, I found abstraction boundries to be much cleaner at FB. At google, I found there was a ton of cruft in Google's monorepos that were too challenging / too much work for any one person to address. reply robertsdionne 6 minutes agoparentprev* https://abseil.io/resources/swe-book/html/ch16.html * https://abseil.io/resources/swe-book/html/ch17.html reply dskloet 26 minutes agoparentprevCode search not just across the entire code base but across all of time. reply ok_dad 14 minutes agoparentprevI’m surprised they didn’t turn that into a product, it sounds great. reply twunde 5 minutes agorootparentParts have been. Sourcegraph is basically the code search post built by ex-Googlers originally. Bazel is the open source build tool. Sadly, most of these things require major work to set up yourself and manage, but there's an alternate present where Google built a true competitor to GitHub and integrated their tooling directly into it. reply pjungwir 47 minutes agoparentprev> Entire C++ servers with hundreds of lines of code can be built from scratch in a minute or two tops. Hundreds, huh? Is this a typo? It makes me wonder if the whole comment is facetious. Or do C++ programmers just have very low expectations for build time? reply jbyers 44 minutes agorootparentI suspect they meant \"hundreds of thousands\" reply pradn 26 minutes agorootparentYes, oops - fixed! reply ttfkam 2 hours agoprevPostgres. I don't code in C if I can avoid it, since it often feels like an awful lot of extra typing while still having to worry about memory safety. But the Postgres codebase is extraordinarily well organized and respects the humans that work with it with its intelligent handling of memory and judicious use of macros. I consider the core Postgres codebase to be the gold standard in development even though it's in a language I do not prefer to write in if given the choice. Shout out to the pgrx folks. You're awesome! https://github.com/pgcentralfoundation/pgrx reply rurp 18 minutes agoparentThat is nice to hear, albeit unsurprising. Their public documentation is some of the best that I have worked with. Postgres is such an impressive project overall. reply prinny_ 8 hours agoprevThe one in my previous job, which was an admin board for a market intelligence application. Ultimately, the reason it was good was because the engineers had zero ego on top of having excellent skills. The team that set the codebase were basically 4 seniors and 3 principals (the client actually did pay for top talent in this case) so not only everything was based on industry standards, written elegantly and organized perfectly, but every time some new requirement came up, these senior / principal engineers would discuss it in the most civilized matter I have ever seen. E.g, \"we need to come up with a way to implement X\". Person A gives their idea, person B gives another idea and so on until everybody shared their thoughts. Then someone would say \"I think what person C said makes the most sense\" and everybody would agree and that was it. 30 minutes to hear everybody out, 3 minutes to discuss who will do it and when and the meeting was over. I think the biggest testament to this code base was that when junior members joined the team, they were able to follow the existing code for adding new features. It was that easy to navigate and understand the big picture of. reply paxys 12 minutes agoprevOne that was written from scratch, by me. All others are terrible. reply tiffanyh 1 hour agoprevI've seen a few people say 'google3'. Q: is it actually the code that you loved, or simply the tooling that exists? (and if it's tooling, why can't that type of tooling be replicated for other codebases outside of google?) reply fragmede 18 minutes agoparentThat type of tooling can be replicated. That's why every Xoogler tries so hard to get Bazel adopted on everything they touch. Sometimes that's appropriate, sometimes it's not, but that's why. Bazel isn't the whole of everything though, the other piece being exported is kubernetes, which isn't Google's borg, but that's its roots. There's Apache Airflow if you need a workflow engine like Sisyphus, no shortage of databases to choose from, though now we're drifting into the operations side of things. But basically, Google has invested untold millions of dollars in the form of SWE and other engineering hours into making Google3 operate. If anyone else invested that kind of dough, with enough smarts, they'd also be able to make it a good experience. The problem is few people have that kind of budget, and even fewer invest in that thing, preferring to use free tools instead. What tool do you use to edit code, and how much did your employer spend on that for you? reply aleksiy123 33 minutes agoparentprevIts both. The tooling has a very direct impact on the quality of the code. I think the reason its not easy replicable is: 1. It takes a ton of initial investment and ongoing maintenance but its worth it when your code base is gigantic. 2. There is a consistent set of top down enforced rules. With the consistency it becomes much, much easier to build tight integrations between tools. (almost?) everything is buildable by a single build system (blaze). When anyone can consistently build/test/run anything in your codebase it becomes a lot easier to build a whole host of potential tools like code search. Probably someone can dive deeper than I can. But one thing I learned the most important property for a code base to be maintainable/scalable is consistency. reply aleksiy123 20 minutes agorootparentOne more thought, is also how much other systems can utilize the same tooling/workflows by just storing things in source code. Things that would probably traditionally stored as application state in a database are often stored instead in google3 as config. Things like on team rosters, on call rosters, code review permissions, service permissions, feature flags. All of it stored in google3 and can all utilize the same consistent set of tooling like presubmit test, integration tests, deployment automation, permissions, code search. Its sort of like Infrastructure as Code but more. reply okdood64 36 minutes agoparentprev> (and if it's tooling, why can't that type of tooling be replicated for other codebases outside of google?) The elegance of the tooling from what I hear is that there's tons of different tools maintained by different teams that work seamlessly (and fast) together to produce google3 and all of its supporting pieces. But to answer your question, sure it can. But good luck building your own. Google has been doing this since the 2000s. And if you're a big company already, you've already bought into your existing patterns & design choices; things like that are VERY hard to change. reply tlarkworthy 9 hours agoprevGoogle3 codebase. It's just so vast and it works. It's a miracle. I feel lucky to have seen it. Everytime you change it, it reruns the dependencies. Everyone has different view concurrently. Commits are efficient immutable snapshots. It's just incredible multiplayer. So massively beyond what can be done with GitHub. Really I feel it's peak codebase. I've not seen what the other big techs do but it blew my mind reply dilippkumar 1 hour agoparent+1 I can not understate how much I agree with parent comment. The opposite of move fast, build a shitty prototype and iterate is a deliberate problem solving approach undertaken by the highest caliber of engineers. The actual challenges to be addressed are effectively addressed right at the design stage. The result is a thing of immense beauty and elegance. I will forever be grateful for the opportunity I had to see this magnificent piece of engineering in action. reply semitones 6 minutes agorootparentI, like many others here, echo this sentiment. While I disliked working in ads, the experience of working with that repo+tooling is unmatched by anything else in my career. reply robertsdionne 11 minutes agorootparentprevCannot overstate. reply arcade79 8 hours agoparentprevIt was wonderful to work with. :-) It's one of the things I truly miss from Google. reply asymmetric 8 hours agoparentprevWhat is Google3? reply kqr 8 hours agorootparentI would assume Google's monorepo. reply Daub 8 hours agorootparentThis link might help... https://dl.acm.org/doi/10.1145/2854146 The linked pdf has lots of details. reply VirusNewbie 1 hour agoparentprevGoogle3 codebase very consistently has clean code, but some of the architecture there is very much not great. Some is great, some not so much. Some of Verizon's code was much more elegant (though much smaller scope) from an API perspective, and really leaned into advanced type systems in a way Google has not. reply inoffensivename 22 minutes agoparentprevI honestly can't tell if this comment is sarcastic... google3 is a quagmire and it's getting worse by the day reply pavlov 9 hours agoprevFor me, the most eye-opening codebase of my career was Cocotron, around 2007: https://github.com/cjwl/cocotron I was looking for a way to port my native Mac Cocoa apps to Windows. I had been already disappointed by the aimless sprawl of GNUstep. This one-person project implemented all the essential APIs for both Foundation and AppKit. Reading the code was a revelation: can it really be this simple and at the same time this effortlessly modular for cross-platform support? I contributed a few missing classes, and successfully used Cocotron for some complex custom GUI apps that needed the dual-platform support. Cocotron showed me that one person with a vision can build something that will rival or even outgun large teams at the big tech companies. But focus is essential. Architecture astronauts usually never get down from their high orbits to ship something. reply structural 9 hours agoprevWorked on a codebase for a large safety-critical system where everything was 100% documented, and the development guide for the project was followed so closely that you couldn't tell, across millions of lines of code, that the whole thing wasn't written by one person. Absolutely impressive levels of attention to detail everywhere, down to not even being able to find typographical errors in comments or documentation (a typo in a comment was treated just as seriously as any other bug). reply n_ary 8 hours agoparentLet me guess, it was very well funded and there were no fake deadlines and cross-team dependencies, am I correct or am I very correct? reply unboxedtype 1 hour agoparentprevI am almost sure this is because the system would have to pass a certification procedure somewhere, and for that they would need this level of clarity. Am I right? reply mikewarot 2 hours agoprevThe run time library for Turbo Pascal/Delphi for Windows was completely documented, sane, and very easy to work with. The working examples really helped. The free Pascal RTL seems opaque in comparison. Their reliance on and archaic help file build system keeps contributors away. Thus it's poorly documented at best. reply BiteCode_dev 8 hours agoprevLast year I worked for a client that gave me a lot of time, money and autonomy to lead dev on a critical software rewrite. We got a small team of competent people, with domain experts to peer code with the devs. It was wonderful. We could test, document and clean up. Having people who knew the trade and users at hand removed second guessing. The result was so good we found bugs even in competitors' implementations. We also got x5 in perfs compared to the system it was replacing and more features. reply thecupisblue 46 minutes agoparentSimilar thing. Had time and autonomy from a client, so took sweet time examining the domain, the existing systems et al. Spent a few months writing the basis and the framework around what will be done, based on years and years of experience I had with bad frameworks and codebases, combined with working on the same domain for their parent company years ago. And it worked. We delivered features insanely fast, hundreds of forms were migrated, feature generators would create 90% of the boilerplate code and the code was small, readable and neatly clustered. Maintaining it was a piece of cake, leading to us not having enough work after a while so we I negotiated our time to half a week for the same money. After a while, client deemed us too expensive to pay for only 2.5 days of work - after all, how does it make sense - if we are paying them that much, they should work 5 days! So they cut us out. Two things happened: 1. Devs that got moved to other projects in the company told me they didn't know development could be so smooth and tried to replicate it in future projects, even tho they say it failed a lot of lessons they picked up from the framework were highly relevant in their future careers. 2. The company found a cheaper developer, he said \"this is unusable and has to be rewritten\" and rewrote it into \"clean code\", taking longer than the original project took. At least he works 5 days a week now. reply leetrout 8 hours agoparentprevGood for you. Magical moments in careers are hard to find in my experience but they are so satisfying when you get there. Glad whomever was over this didnt just drop the \"dont rewrite\" joel spolsky article and fight making it happen. reply junon 2 hours agorootparentJoel has since said that that he doesn't really agree with that advice anymore, at least not in the same way. Super annoying that it gets parroted over and over again as though it's the word of the lord. reply BiteCode_dev 8 hours agorootparentprevI actually was the one telling them not to rewrite, lol. But the original code was a mess of matlab spaghetti, they couldn't find a way to hire for that. Not to mention turning it into a web service was already a big hack of java parsing a raw dump of matlab datastructures that nobody dared to touch. I had to read the matlab code, and it tooks hours to decypher a few lines. Plus the language doesn't have great debugging and error handling capabilities and the tooling is quite terrible. So rewriting to python won, and for once, I must say it was a good call. reply neilv 8 hours agoprevOne of them stands out, due to being super-productive, over years, and then decades. A large system that was originally written by only two super-productive engineers (I mean real engineers, both with PhDs in an area of Engineering). And a comparably capable and essential IT person. The reasons for the super-productivity include one of the developers choosing great technology and using it really well, to build a foundation with \"force multiplier\" effects, and the other developer able to build out bulk with that, while understanding the application domain. Another reason was understanding and being pretty fully in control of the code base, so that, as needs grew and changed, over years, someone could figure out how to do whatever was needed. One of the costs was that most things had to be built from scratch. Over time that also proved to be an advantage, because whenever they needed (put loosely) a \"framework\" to something it couldn't do, they effectively owned the framework, and could make dramatic changes. When I said \"costs\", I mean things like, many times they needed to make a component from scratch that would be an off-the-shelf component in some other ecosystem. So if someone looked closely at how time was sometimes spent, without really understanding it or knowing how that panned out, it would look like a cost that they could optimize away. But if they looked at the bigger picture, they'd see a few people consistently, again and again, accomplishing what you'd think would take a lot more people to do. It helped that the first programmer also became the director for that area of business, and made sure that smart engineering kept happening. Someone might look for a reason this couldn't work, and think of bus factor. What I think helped there was the fact that the work involved one of those niche languages that attract way more super programmers than there are jobs. \"Gosh, if only we had access to a secret hiring pool of super programmers who were capable of figuring out how to take up where the other person left off, and we had a way to get them to talk with us...\") It was easy to imagine a competitor with 100 developers, not able to keep up, and at many points getting stuck with a problem that none of them were able to solve. reply bruce343434 4 minutes agoparentWhat is this niche language attracting super programmers? reply klibertp 1 hour agoparentprevI assume you avoided identifying (or even hinting at) this \"great technology\" on purpose, but could you persuaded to divulge what it was? reply sibit 7 hours agoprevPretty much any internal tool/TUI/CLI/library I've created. If I had to guess I'd say at most 25% of the company projects I've worked on have launched AND have consistent usage. Working hard on something just for it to wither crushes my soul but internal projects are different. They're all skunk works projects. No tickets. No project/board. No PM pushing back on how many points (read: hours) something should be. I'm solving real problems that directly impact the quality of life for myself and my coworkers. The best part is getting real, genuine, feedback. If something sucks they'll tell you and they won't sugarcoat it. reply jftuga 7 hours agoparentI love this take. What language(s) do you typically use to write CLI programs? I'm also interested in learning about what types of internal TUI tools you have created. reply chrisvenum 9 hours agoprevMy favourite projects are small, with very focused goals and features. I have a Laravel project that I have maintained for a customer for seven years. The app is straightforward and allows users to create portals that list files and metadata, such as expiration dates and tags. Every other year, they ask me to add a new batch of features or update the UI to reflect the business's branding. As the app is so small, I have the opportunity to review every part of the app and refactor or completely rewrite parts I am not happy with. It is a joy to work on and I always welcome new requests. reply simfoo 8 hours agoprevMy past three employers code bases: mono-repos, Bazel, lots ot C++ and Python, thousands of libraries and tools, code generation and modeling tools that are fully integrated into the build, easy cross compilation, large integration tests just one bazel test invocation away, hermetic and uniform dependencies... reply fire_lake 7 minutes agoparentBazel Python does not cross compile IIRC. How was this achieved? reply ramses0 51 minutes agoprevIt's kindof a dumb answer, but `abcde` (A Better CD Encoder) continues to be my go-to reference for \"everything shell\". https://git.einval.com/cgi-bin/gitweb.cgi?p=abcde.git;a=summ... https://git.einval.com/cgi-bin/gitweb.cgi?p=abcde.git;a=blob... ...it's just so damned... like... it _is_ 5000 lines of bash, but given the constraints and the problem domain, it's an incredible (and well-organized) feat. If I ever question \"How would I do something like XXX in shell?\", I can usually find some clues, inspirations, or answers in the `abcde` codebase. reply donatj 9 hours agoprevThe latest Go micro-service I have built. About once a year roughly, for the last couple years, the opportunity has arisen to greenfield a Go micro-service with pretty loose deadlines. Each time I have come into it with more knowledge about what went well and what I wasn't particularly happy with the last time. Each one has been better than the last. I've been building software professionally for twenty years, and these micro-services have been one of the few projects in that time that have had clear unified vision and time to build with constant adjustments in the name of code quality. reply mulholio 9 hours agoprevAlthough I'm only on job three and have not had that much involvement with open source, I think my current employer (Attio) has one of the best codebases I've seen. Qualitatively, I experience this in a few ways: * Codebase quality improves over time, even as codebase and team size rapidly increase * Everything is easy to find. Sub-packages are well-organised. Files are easy to search for * Scaling is now essentially solved and engineers can put 90% of their time into feature-focused work instead of load concerns I think there are a few reasons for this: * We have standard patterns for our common use cases * Our hiring bar is high and everyone is expected to improve code quality over time * Critical engineering decisions have been consistently well-made. For example, we are very happy to have chosen our current DB architecture, avoided GraphQL and used Rust for some performance-critical areas * A TypeScript monorepo means code quality spreads across web/mobile/backend * Doing good migrations has become a core competency. Old systems get migrated out and replaced by better, newer ones * GCP makes infra easy * All the standard best practices: code review, appropriate unit testing, feature flagging, ... Of course, there are still some holes. We have one or two dark forest features that will eventually need refactoring/rebuilding; testing needs a little more work. But overall, I'm confident these things will get fixed and the trajectory is very good. reply fer 8 hours agoprevOne that had a sort of improvised facade/adapter pattern (it didn't really follow either) in a clearly cut multilayered and pipelined structure, with actor model bits where it made sense. The code wasn't simple, at all. It took active training of new arrivals for them to understand it. But it was very well thought out, with very few warts given the complexity, and extremely easy to extend (that was the main requirement, given constant changes in APIs and clients). We had an API, with multiple concurrent versions, that transformed requests into an intermediate model, on which our business logic operated, later targetted external APIs (dozens of them, some REST, some SOAP, some under NDAs, some also with multiple versions), whose responses turned again into the intermediate model, with more business logic on our end, and a final response through our API. Each transaction got its context serialized so we could effectively have what was an, again improvised, \"async/await\"-like syntax in what was (trigger warning) C++03 code. The person who engineered it didn't have formal CS background. reply kleton 9 hours agoprevgoogle3, all the devex tooling was taken care of by other teams. Tons of useful library functions available to import, accumulated over decades. reply karmakaze 4 hours agoprevProbably the microservices-based one for async video messaging (i.e. Slack for video) for workers in the field. Each service was small enough that we could do a blue-green deploy to prod in about 2 minutes running only the service's tests and a tiny (intentionally limited) set of about 8 system journey tests (can onboard a new user, user can create a contact/group, user can send a common content-type message, user can receive messages, user can react/respond to a message). Every commit to main/master automatically either deployed to prod or broke the CI/CD pipeline and needed to be fixed ASAP. Each service was also well-known by team members that it literally could be rewritten in a week or two if desired to change a key part of its design. reply karmakaze 4 hours agoparentThinking about what I wrote, I suppose my criteria for a good codebase is one that has the lowest friction to change: a fast edit/run/test/debug loop, a meaningful sense of security (without dogma), and fast automated deployment/revert (via blue/green). Given those a bad codebase can become a good one again (by uncoordinated action of individuals) without everyone having to buy into a large investment. reply 999900000999 4 hours agoprevRealistically any code base where the engineers had at least a basic understanding of programming. You do not know suffering until you've seen someone hard code basic variables, we're talking about strings all over the place, and then they just copy the function again to replace the strings . I've legitimately left jobs over bad code. We're talking about code that did nothing in reality. The best code bases have been ones where I've been able to lead the direction. I get to know exactly how things work. I'm privileged to have a job where I essentially created the initial framework right now . Plus I'm fully remote, life is pretty good. reply ysris 9 hours agoprevConsidering the biases, the one I wrote for the company I created. When we have the opportunity to be in this context, keeping in mind what bothered us in the codebases with which we were able to work in the past, we can force ourselves not to reproduce the same errors. Like the unmaintained unit and integration tests, the lack of refactoring, other developers that use fancy technologies instead of simpler concepts more for the opportunity to play with technologies than real need.. And also, I guess, because we are more aware that the code is a reflection of the company that we want to have, that the simpler the better is a key point when we need to debug. reply rthnbgrredf 8 hours agoprevThe best codebase is the one you fully understand. I prefer codebases that are small enough to understand within a week. This is why I like Microservices. Large codebases can be overwhelming and even senior developers working a decade in the company of might not fully understand them. Instead, I prefer maintaining a few Microservices that our team fully comprehends, where the entire codebase fits into a clear mental model. We then interact with other codebases, that have active mental models in other teams, via APIs. reply ethegwo 9 hours agoprevA Python version about core features of the Hindley-Milner type inferencer: https://github.com/ethe/typer reply 2Pacalypse- 9 hours agoprevAny codebase that I had complete control over. No, but more seriously, I've found that familiarity with the codebase is more important than having it be perfectly engineered. Once you're really familiar with the codebase, you know where dragons be, and you can make changes more easily. And God (PM) forbid, if you ever find yourself with some extra free time you might even reduce the size of dragons over time. This brings me to my final point. Any codebase that I really enjoyed working with was the one that was constantly evolving. I don't mean rewriting everything from scratch every few months, but as long as I have permission (and time) to refactor the things that have been bothering me for months as patterns emerge, I'm a happy bee. reply sam_lowry_ 9 hours agoparent> Any codebase that I had complete control over. No excuses. Code ownership is important. Sometimes it works for a team, sometimes only for individuals. But not having to submit to core teams, architects and self-proclaimed experts of all kinds is a blessing. I now work for an organization that discourages code ownership, and it struggles on many fronts: 1. core teams are dysfunctional 2. people find niches and stick to them 3. top talent is leaving, although pay is good and business creates real value for citizens 4. there is virtually no horizontal communication 5. mediocre ones rise to the level of their incompetence and infest the lives of others 6. and so on and so forth... And I think the root cause of all this is lack of individual (code) ownership. reply Aeolun 8 hours agorootparentI’ve had exactly the same issues but because I couldn’t change anything without getting approval from 16.5 code owners on every PR submitted. It’s a real pain if you start modifying your coding for ‘least code owners hit’ instead of ‘best architecture’. reply rsanek 6 hours agorootparentI like how my workplace does it -- there are rigorous codeowners and usually you only need approval from 1-2. if you do need approval from 5+, you can request a select 'super' codeowner review which will approve it for all. reply ownagefool 7 hours agorootparentprevI think core teams being helpful or harmful really comes down to the individuals. The problem is at this level, most orgs don't have anyone to really judge or contest competency, so they hire the salesmen rather than the doers and when they don't, they tend to cheap out and just get inexperienced people. Logically it makes a bunch of sense, though. Why rebuild yet another platform? Why is your central platform bad? Usually it's not self-service, sometimes it's because it's built in cumbersome ways, other times its because it actually enforces good standards on you rather than just giving app your apps admin. It's difficult for the person who hires the core team to differentiate between those complaints, unless they themselves both have the technical competency and the empathy to really understand the problem. They usually don't. Point being, done well, it's great, but most folks can't do it well. reply idrios 3 hours agoparentprev> And God (PM) forbid, if you ever find yourself with some extra free time you might even reduce the size of dragons over time. Honest question, what is the company like where you can do that? Everywhere I've worked (only been working in industry for 6 years) has had such rigid agile development that even when I do find myself with free time, there's no flexibility to work on things that haven't been assigned to you and the best I can do is work on profiling/debugging tools. reply nevi-me 8 hours agoparentprevI also echo the code ownership part. I have a part-time gig where I maintain accounting software for a former client of mine. It takes up a few months' weekends a year. I wrote about 60-70% of it when I was working for the owner of the software. It's something where as long as the client's happy, and they get new integrations and updates on time, they could keep using it for a decade longer. I had almost complete ownership of the architecting of the software. It's broken down into a few microservices (think database, core business logic, reporting, auth, logging etc). The best thing I did at the time was pushing to use gRPC even though management felt it was too new tech. The UI is in Angular, pain-free periodic upgrades. I've even rewritten some perf-sensitive code in Rust, and everyone's happy with snappier calculations. The code hygiene is relatively good. The only downside's that if someone else were to take over the code, they'd struggle (it's one of those things where I'm wearing many hats). I've been fortunate to be a professional accountant who moved into software engineering, so everything makes sense to me. reply sgbeal 8 hours agoparentprev> Once you're really familiar with the codebase, you know where dragons be, and you can make changes more easily. This is an interesting, and often overlooked, point. A month or two ago someone asked us, the Fossil SCM maintainers, if we'd be open to them refactoring the tree to something which better matches modern sensibilities (i'm paraphrasing here). Despite its many proverbial dragons, the long-time maintainers are comfortable with it and know where those dragons are and how to defeat them (or, in some cases, sneak around them), so, despite our collective geek tendencies to prefer \"perfect code,\" we're happier with the dragons we know than those we don't. (That's not to say that fossil's code is awful, but its structure varies significantly from what one might write today if one was to start such a project from the ground up.) reply null_glyph 8 hours agoparentprevThis is very much true. Initially when I joined the industry I used to work for a product from its inception. So I was aware of what section of the code affects what part of the product. Although I hadn't worked on all of those, I kept an eye for the all the changes that were coming in. I knew where I should look immediately a bug is reported even if it is not something related to my line of work. Recently I switched teams and now I find myself taking up bugs that are only related to my line of work. Not being familiar with the codebase decreases productivity and wants you to rely on other people in the team for most of the time. reply blitzar 8 hours agoparentprev> Any codebase that I had complete control over. Anyone other than myself would instantly observe it as the the worst codebase they have ever seen. reply sam_lowry_ 6 hours agorootparentFor that, there is a saying: \"code as if your kids will maintain it\". But I think it does not convey the right meaning. When I code something I will have to maintain for a long time, I try to make it as simple as possible for my future, older, less motivated and weary self. The worst codebases are written by people who landed the gig a few months before and do not expect to stay around longer than a year or two. reply threatofrain 9 hours agoparentprevAlso nice is when an entire community has agreed to architect a codebase more or less the same. You're basically psychic when that happens. reply KronisLV 8 hours agoparentprev> Any codebase that I had complete control over. This is probably one of the pillars of good codebases, or at least decoupling the bits that you don't control as well as you can (this includes external services). I remember needing to write a wrapper around another JWT library, but because it was quite important, I aimed for >95% test coverage, some of the tests acted as documentation for how to use the code, there was also a nice README, there was CI configuration for pushing the build artifacts to Maven and suddenly even managing dependency updates became easy because of the test suite. Years later, it's been integrated in a bunch of services and \"just works\". Come to think of it, things always get less pleasant once you add a bunch of complex dependencies and libraries/frameworks. Need to make a few RESTful Web APIs in Java? Something like Dropwizard will probably give you fewer headaches than Spring (or Spring Boot) and all of its inherent complexity, in the case of the former you might even need to do configuration in XML and that has honestly never been pleasant. If you need to integrate with a bunch of other stuff and want something opinionated, going for Spring Boot will make sense then, but for simple use cases it's overkill. Same for ASP.NET, Ruby on Rails, Laravel and many others, while they might be easier to use, updates (especially across major versions) and breaking changes will give you a headache and just add a bunch of churn to the project. Similarly, if you need a message queue, externalizing that into RabbitMQ might make a lot of sense, same with storing files in an S3 compatible store, using something like Redis or Valkey for key-value storage, as well as not trying to shove too much logic into your RDBMS. Just pick whatever tool feels best for the given task at hand, instead of shaping things into what they're not (using the database for blob storage, for example), unless you have a whole bunch of constraints to contend with. Otherwise, sometimes you just get the worst of both worlds, like needing to use Oracle for a project, not having easily launchable local environments (because Oracle XE doesn't support some features), having to share DB instances with others during development and also running into weird obscure issues like DATABASE LINK queries taking 100 times longer in some cases, even when executing the same SELECT query on the remote DB works without issues. To not go into a rant, I'd sum it up like this: be in control, isolate the things that you cannot control, pick the correct technologies, do the simplest thing that you can get away with without overengineering and think about the person who'll have to maintain, debug and grow the system in the following months and years (which might also be yourself, sans knowledge about what the code did, if you don't make it explain itself or don't comment the non-trivial stuff). reply wruza 7 hours agoprevThe ones that were straightforward and close to the business. It starts at the obvious, works in an obvious way and has comment blocks at hard parts. For this reason I despise most modern [web] projects, which have a weak start, immediately drop into “services” and “components”, do one action per source file per 30-50 lines of code, which are mostly imports and boilerplate, and have hundreds of these files. You can never tell what it does because it does almost nothing except formalities. I also noted a tendency to use wrong paradigms for a language. E.g. it may have no closures (imagine that in 202x) so they use events as continuations for asynchronicity, which results in a mess. Or it isn’t immutable/functional, but they pretend it is, which results in fragility. The best projects are both close to their business and written in a paradigm of the language used. Was there someone enforcing good practices top down? Natural time pressure is the best bs cleaner, imo. You write effing code, maybe have few hours a week to refactor strange parts. With no time pressure a project naturally becomes massaged by all members into the “likeable” form of their age. reply noelwelsh 9 hours agoprevMy open source projects. One in particular I've been working on for about 10 years. The code is consistent and always getting better, even though there is a lot more work that could be done. reply nijave 7 hours agoprevI was debugging some issues with Thanos and had pretty good success tweaking the codebase to add additional telemetry. The code was fairly well organized and more importantly worked out of the box with a Makefile and IDE integration (GoLand). All it took was `git clone` and opening GoLand to get started. For C (maybe it's C++), fluentbit seemed pretty straight forward (I don't have much experience in C though) reply Agingcoder 8 hours agoprevI haven’t contributed code to it, but I’ve read lots of code in it ( for work reasons ) - I really like the golang compiler and library codebase. About codebases I’ve written code for, the best one strived for simplicity, and was driven by very strong engineers who actively considered code hygiene ( in the broadest possible sense ) a first class citizen. reply 8474_s 5 hours agoprevI can't name a good one, but i have strong dislike for big project codebases where even finding one file out of thousands where the relevant code resides is a challenge: its never something isolated but acts like some \"component\". The best i can think of is one-person projects where organization is streamlined as its actually has to be used by the author, not like a cog in a giant project. reply _kb 8 hours agoprevI don't think the 'code' side of code base can be considered in isolation. What makes a project objectively good (from subjective experience) is a combination of code, design, documentation, and often the humans involved. reply bittermandel 9 hours agoprevAny code base that doesn't use the advanced features of it's language(s) are always better in my experience. Heavy usage of e.g. meta-programming in Python or perhaps uber's fx (dependency injection) in Go makes projects infinitely harder to get into. reply tommypalm 8 hours agoparentI worked at GOV.UK for a few years on what was effectively specialised CMSs all written in Rails. Mostly basic CRUD stuff. A contractor came along and built the most insane CMS I've ever seen. I found out later it was flavor of the Java Repository Pattern using dependency injection. It became so difficult to work with that the last thing I worked on there was to delete it and rebuild it using standard Rails patterns. The KISS philosophy exists for a reason and that includes over-using advanced language features just to show off. reply nijave 7 hours agorootparentBesides just KISS, a lot of messes I've seen have been implementing patterns outside the framework or implementing complex patterns that didn't add value. Besides KISS (or maybe as an extensive), try to keep framework-based codebases as close to the official documented setup as possible. You automatically get to s of free, high-quality documentation available on the Internet. reply andrei_says_ 2 hours agorootparentprevProps for gov.uk! I’ve looked at its documentation and design system and see both as peak user experience and clarity. reply rvdginste 1 hour agoparentprevI think one should not use advanced language features just because, but I also think one should not avoid using advanced language features where it is useful. Why would the code base be worse when advanced language features are used? reply BiteCode_dev 8 hours agoparentprevDepends who is providing it. Django and pydantic meta programming usually make the code easier to deal with. In shop written meta programming usually sucks. reply jamestimmins 8 hours agoparentprevSuch a great point. I audibly groan when I come across Python meta-programming. While not an advanced feature, I have a similar response when I see lots of decorators in Python. They quickly become a debugging nightmare. reply sebstefan 8 hours agoparentprevI can see a few cases where that depends... Really simple languages: Ruling out meta-programming is really going to limit you in Lua for example. Just being able to do `mySocket:close()` instead of `Socket.close(mySocket)` involves meta-programming. Older languages: For C++ the \"simple\" features are going to include raw pointers and macros. Maybe it's not so bad to allow smart pointers and templates to avoid those reply wruza 6 hours agorootparentBoth of these examples are examples of an under-programmed core though. Lua is notorious for lacking batteries, so everyone has to reinvent their own. There’s literally no serious Lua program without some sort of classes, but they still resist adding them into lauxlib. reply Mikhail_Edoshin 7 hours agoparentprevHaving just removed a metaclass from my Python code I totally agree. reply anonzzzies 9 hours agoprevGenerally code that hasn't been tested commercially. Unencumbered by pesky client driven features, just code for dreamt up features that are fun to code but perhaps will never be used. reply cqqxo4zV46cp 9 hours agoprevA blank slate, so I can make the first mistake :) reply azangru 8 hours agoparentSo true! I spend so much time obsessing over how what I am about to write ties in with what has already been written; or fuming over the stupidity of earlier decisions (usually made by myself). A blank slate is incredibly refreshing. reply escapecharacter 9 hours agoparentprevI agree, the best code base is always a zero-line code base. reply alex_suzuki 8 hours agorootparentAfter that first line, it’s all downhill from there… :-) reply Maledictus 7 hours agoprevRuby on Rails. It is the only framework I have read top to bottom. Also the FreeBSD kernel, if you want to see a C code base that's quite beautiful (for C). reply sgt 7 hours agoparentIn this category I would nominate Django as well. It's very well designed (opinionated, but usually for good reasons). In terms of large C code bases I enjoy reading the PostgreSQL source code. reply rc_mob 6 hours agorootparentwhenever I'm stuck on how to structure some code, I ask myself how would Laravel do it? and look up their code and structure mine similarly reply cpojer 8 hours agoprevhttps://github.com/nkzw-tech/athena-crisis reply nonameiguess 8 hours agoprevNo name because the project code name changes with each new contract, but the first-level ground processing system for the NRO that turns raw satellite downlinks into some kind of human-intelligible files. I think a lot was just the restrictions built into the way development happens required a high level of discipline, care, and planning. Also, requirements were pretty tightly coupled to sensor platform capabilities, which are known well in advance and don't change unexpectedly, so waterfall development actually works and you don't have to deal with the chaos of not really knowing what will and won't work and customers constantly changing their minds. Code base was overwhelmingly C++, some Fortran, a lot of it was very old. It was all developed on airgapped networks, and the difficulty of bringing in external dependencies meant there largely were not any. All of the library functionality we required was mostly developed in-house, so we had extremely well-documented and stable functions available to do damn near anything you could want, with a good chance that whoever first wrote it was still there. All development had always been tied to a ticketing system of some sort that included all of the original discussion, design documents, and that kind of thing might add process overhead upfront, but it means that forever new developers can simply read the history and learn exactly why everything works the way it works. The system itself was very Unixy. In production, it was meant to be run as a server with many instances striped across high-performance compute nodes, but it did not have to be run that way. Every individual product flow could also be built as its own transient executable, so that working on a single component could easily be done locally. You didn't have to rebuild the world or spin up your own server. Performance requirements were enough that we had our own customized networking stack and filesystem in production, but nothing depended on this for function, so you could still develop and test on plain Linux with ext4. The culture was terrific. We were part of one of the big five defense contractors, but an acquisition and this program was largely still staffed by employees of the original company that had been acquired. We were de facto exempted from most of the bullshit any other program had to deal with. I don't know if that was part of the original terms of being acquired or just a consequence of having so many long-time developers that couldn't afford to be lost if you subjected them to nonsense. This was the kind of project that people intentionally stayed on and retired from because the experience was so much better than any other project you could get assigned to. Ironically, it had none of the characteristics that high-performing companies often tout. You work in private. The rest of the company, including your own management chain, doesn't even know what you're working on. You'll never get any recognition or publicity. The pay is mediocre. We weren't attracting the best and brightest from all of the world. You had to be American, have a top-secret clearance, and be geographically close enough to the facility to get there every day, so this was a pretty constrained hiring pool. I still worked with some of the smartest people and best engineers I've ever known. The upside of this kind of environment is you have no mercenaries or publicity hounds. Everyone who sticks around is a person who really loves and cares about what they're working on, and a lot of people did stick around. The sanity and organization of the code was heavily facilitated by having a whole lot of people working on it who'd been working on it for 30+ years. reply Claudus 8 hours agoprevLaravel, once you get the hang of it everything just works, and using a debug bar to optimize database calls is very satisfying. reply llmblockchain 4 hours agoprevAny code I have ever touched. reply jmartin2683 9 hours agoprevHopefully the last one you were responsible for. reply Traubenfuchs 9 hours agoprevEvery code base I have ever worked on was a legacy nightmare. Every \"greenfield\" project I joined turned into a legacy nightmare within weeks. I have never encountered enjoyable code. I had the displeasure of wading through Spring, Hibernate and Apache HTTP client code before and they were all an incomprehensible mess. My conclusion: You know the claim \"any medication that really has an effect must also have side effects\". I would like to adapt that for code: Any code that does a lot of useful and complex things must be an arcane, barely maintainable mess that can only be understood by deep study. reply mrweasel 7 hours agoparentSomething in your answer triggered a flash back to when I worked for a phone company, 20 years ago. One team, under the leadership of our questionable chief architect, had produced our new, all encompassing backend system. This was to be the corner stone of all future development and integration, dog slow and complicated as it where. I worked as a .Net developer and had the misfortune to be among the first to integrate with this monster. Try as I might, I could not get .Net to interoperate with these services. Finally I figured out that the SOAP services was using some old deprecated versions. Going back to the architect, I ask \"did you build this on Apache Axis, and please say Axis2\", but no, it was just Axis, a deprecated version, that would generate webservices not supported by newer .Net version. That wasn't an problem, because: \"None of our project have upgraded to those .Net versions yet\"... DUDE, we've launched a brand new system based on that .Net version a year ago, and what was you f-ing plan for the future, to redo every single service using Axis2? This guy had based a brand new system on a framework/library that was no longer maintained, even before the system was launched. reply alex_suzuki 8 hours agoparentprevIMHO library code is especially challenging, as cruft has a tendency to accumulate, historical behavior needs to be preserved, and APIs are set in stone once they’re built. reply lukan 8 hours agoparentprev\"Any code that does a lot of useful and complex things must be an arcane, barely maintainable mess that can only be understood by deep study.\" \"Every code base I have ever worked on was a legacy nightmare. Every \"greenfield\" project I joined turned into a legacy nightmare within weeks\" I am sorry to say this, but it really sounds like you were either really, really unlucky, or part of the reason why it became a mess. Complicated things are complicated. Nothing can ever change that and it requires study to understand it. But it still does matter a lot, how one organizes the whole thing. How it is structured, documented(!), refactored. Are there competent people in charge who understand it all and kick peoples asses if they make even a temporary mess or forget to document, or do random people make changes wherever they see fit, because a deadline is ticking? Modularisation is usually the key. Small modules do one thing and are as seperated as much as possible with as little side effects as possible. And if one has to ship things, it is not always possible to keep it pure and if the code is not intended to live forever then this is often fine. But if the codebase is supposed to stay, then there needs to be the time to clean up the hacks. Or don't and then you end up scared touching anything. That being said, the technologies you mentioned, I would not like to touch either.. reply t43562 7 hours agorootparentI think that's a bit cruel. Usually mess happens because lots of people with different ideas about the future needs and best structure meet up in the code - and it's hard to develop a consistent culture. Also codebases get too large for any one person to refactor them into shape in the time they have each day. So you end up needing people who are responsible just for keeping things in shape. reply lukan 7 hours agorootparentWell, that is why I said with those words it sounds like this to me, not that he or she was in fact responsible. (at least this is what I meant) \"Also codebases get too large for any one person to refactor them into shape in the time they have each day.\" Which is why modules, or subprojects were invented. Or however you want to call it, if one person is only responsible for a small part and not everyone for everything. And yes, there is also the non trivial problem of time management. reply t43562 7 hours agorootparentyup, but the style of the modules starts to diverge as different people maintain them. IMO one does want people whose fulltime job is to looking at a codebase orthogonally to those who are just implementing a feature. People who make sure it builds fast, is secure tries to be consistent to some degree etc. Many companies call teams/indivuduals like this a \"cost centre\" and disparage it because they are dolts. reply lukan 45 minutes agorootparentIf those code checkers are experienced programmers, who do not annoy people with arbitary guidelines and standards, then yes, this might also work. But I do not think they are always necessary as a seperate full time role, if everything works normal otherwise. And with working normal, I mean there is enough time to refactor and document and clean up and is actually done. reply jmartin2683 9 hours agoprevHopefully the most recent one you were responsible for. reply t43562 7 hours agoprevI just cannot face the thought of calling any of them \"best\". Every one had good and bad features though. One or two were OS-sized and I think a codebase that compiles and links to 85GB of output for 20+ devices without being a total disaster inside is harder to do than a neat small python module or whatever. GOOD FEATURES: Maintenance of the build and test: I worked on tools that helped builds go faster so I saw a lot of codebases where people were not maintaining the build partly because nobody had that a s a responsibility. There was bad management of dependencies leading to build failures, poor performance, incorrect build output. Android would be a counter example to that - I don't know if people like developing in it but it was always hard to accelerate it as the maintainers fixed performance problems regularly leaving our tools with little to improve. Using appropriate languages. Writing everything in C++ was a fad at one time. All projects work better, port better, have faster build times, are easier to test etc if they use memory safe \"build once\" languages to a maximum (e.g. java) and unsafe ones (e.g. C/C++ which have to be rebuilt and tested for each device/os) to a minimum. IMO Android beat Symbian amongst other reasons because it wasn't all C/C++ and that meant a lot of code didn't have to be rebuilt for different devices. This made builds faster and fast builds lead to better quality because of a short dev-test cycle. Use of faster compilers over \"better\" compilers. Ultimate code performance and quality depends on a fast development cycle more (IMO) than on having the most optimizing compiler. GCC versus the older ARM compilers for example. Now the ARM compiler is based on LLVM and I know that happened indirectly from a suggestion I made to someone who then made it to ARM who then did it. The setup and build of one codebase I worked on was as easy as one could expect, the build errored out if you tried to use the wrong tools so you never ended up debugging weird failures because of an incorrect tool in your PATH somewhere. I made this feature happen :-D. With big codebases the tools could be included in the version control system so you knew you had the right compiler, right object dumper etc. This is another strength of Android and yet I was in a project for Symbian to do the opposite because of some utter bonehead who never touched a build in his life who was trying to make a name for himself with his slimy bosses as a \"doer\" and \"reformer.\" Codebases (especially big ones) benefit a lot from some kind of codesearch/index where you could find out where some function/class/variable was defined and what version of the source base it was introduced in. BAD FEATURES: Exclusively Owned code - we need to know who understands code best and who is best to review it but I don't think anyone should have totally exclusive control. It was a nightmare for me at one job - trying to get another team to make some needed change (like fixing their stupid makefiles to work properly in parallel). We (build team) should have been able to do it ourselves - maybe including them in the PR. Sometimes ownership is entirely theoretical - nobody who wrote it is still employed and nobody among the notional owners understands it and none of them want to approach it within 100 metres in case it blows up and becomes their problem. I simply had to approach such code - no choice - but I kept having to send diffs to people who didn't want to bother to look at them. It was a case of pushing wet spaghetti and took forever to do very simple changes. Insufficient tests that run infrequently. What else is there to say? Complicated code with no \"why\" or \"what this is for\" type comments. The kind of thing you trawl around in for weeks and cannot make head nor tail of what is going on overall. Code with so much dependency injection and general SOLID that you have to bounce all over the place to understand a very simple action. Code where writing tests is an enormous ballache. In one Go codebase the reason was because somone decided that the standard Go practise of an array of test data being run through a kind of \"test engine\" was the only way anyone should be allowed to write tests. Hence you had to do lots of weird things to make your test cases into data. Generally we use a kind of \"religious\" approach to try to get consistency out of a group of people but then take it much too far. codebases without automated reformatting - so everyone wastes time arguing about line spacing or camel-case names or whatever in their PRs. reply gwbas1c 5 hours agoparent> Code with so much dependency injection and general SOLID that you have to bounce all over the place to understand a very simple action. I find that happens when people get religious about patterns and methodology; without understanding the \"why\", the language, and how a computer works. Case in point: I once worked on a C# project that used a port of Spring for dependency injection: Ultimately, it was near impossible to know when something was constructed, and what was calling what. There were classes that couldn't call themselves through \"this\" because of certain weird dependency injection features used. Later, I decided to use dependency injection as a design pattern: Instead of a complicated DI framework, there was just a few files of code. It was very easy for newcomers to understand. It was also easy to swap in mock objects, and easy to swap dependencies based on the target platform. It was also easy to see when a dependency was constructed; because it wasn't hidden behind a giant framework. reply joeshachaf 8 hours agoprev [–] I worked with a very experienced engineer who created his own WAF (software-based) using Java. From a purely architectural perspective, it might not be the best (I might have used an ISAPI filter back then), but the code itself was very efficient, well-written and documented. I used it several times as a teaching example. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Google's monorepo is praised for its exceptional tooling, which includes instant creation of mutable snapshots and perfectly reproducible builds on clusters.",
      "Features like instant code search, quick blame and symbol lookup, and extensive presubmit checks ensure high productivity and uniform code/test quality.",
      "The environment enforces consistent style through culture, auto-linters, and mandatory code reviews, making it a standout experience for developers."
    ],
    "points": 134,
    "commentCount": 111,
    "retryCount": 0,
    "time": 1719564044
  },
  {
    "id": 40820063,
    "title": "New ways to catch gravitational waves",
    "originLink": "https://www.nature.com/articles/d41586-024-02003-6",
    "originBody": "NEWS FEATURE 27 June 2024 Five new ways to catch gravitational waves — and the secrets they’ll reveal Observatories, experiments and techniques are being developed to spot ripples in space-time at frequencies that currently can’t be detected. By Davide Castelvecchi Twitter Facebook Email Two black holes orbiting around each other generate gravitational waves of increasing frequency. New instruments and techniques could watch this merger for weeks or even years. Credit: NASA's Goddard Space Flight Center Conceptual Image Lab In September 2015, a vibration lasting just one-fifth of a second changed the history of physics. It was the first direct detection of gravitational waves — perturbations in the geometry of space-time that move across the Universe at the speed of light. Astronomers say it was like gaining a new sense — as if, until 2015, they had only been able to ‘see’ cosmic events, and now could ‘hear’ them, too. Since then, it has become almost a matter of daily routine to record the passage of gravitational waves at the two massive facilities of the Laser Interferometer Gravitational-wave Observatory (LIGO) in Louisiana and Washington state, along with their sibling Virgo observatory near Pisa, Italy. The detection of gravitational waves has provided new ways to explore the laws of nature and the history of the Universe, including clues about the life story of black holes and the large stars they originated from. For many physicists, the birth of gravitational-wave science was a rare bright spot in the past decade, says Chiara Caprini, a theoretical physicist at the University of Geneva in Switzerland. Other promising fields of exploration have disappointed: dark-matter searches have kept coming up empty handed; the Large Hadron Collider near Geneva has found nothing beyond the Higgs boson; and even some promising hints of new physics seem to be fading. “In this rather flat landscape, the arrival of gravitational waves was a breath of fresh air,” says Caprini. That rare bright spot looks set to become brighter. All of the more than 100 gravitational-wave events spotted so far have been just a tiny sample of what physicists think is out there. The window opened by LIGO and Virgo was rather narrow, limited mostly to frequencies in the range 100–1,000 hertz. As pairs of heavy stars or black holes slowly spiral towards each other, over millions of years, they produce gravitational waves of slowly increasing frequency, until, in the final moments before the objects collide, the waves ripple into this detectable range. But this is only one of many kinds of phenomenon that are expected to produce gravitational waves. LIGO and Virgo are laser interferometers: they work by detecting small differences in travel time for lasers fired along perpendicular arms, each a few kilometres long. The arms expand and contract by minuscule amounts as gravitational waves wash over them. Researchers are now working on several next-generation LIGO-type observatories, both on Earth and, in space, the Laser Interferometer Space Antenna; some have even proposed building one on the Moon1. Some of these could be sensitive to gravitational waves at frequencies as low as 1 Hz. But physicists are also exploring entirely different techniques to detect gravitational waves. These strategies, which range from watching pulsars to measuring quantum fluctuations, hope to catch a much wider variety of gravitational waves, with frequencies in the megahertz to nanohertz range (see ‘Opening the window on gravitational waves’). By broadening their observational window, astronomers should be able to watch black holes circling each other for days, weeks or even years, rather than just catching the last few seconds before collision. And they’ll be able to spot waves made by totally different cosmic phenomena — including mega black holes and even the start of the Universe itself. All this, they say, will crack open many remaining secrets of the cosmos. Pulsar timing array: catching waves that last a decade Last year, one viable alternative to interferometers entered the game. Since the early 2000s, radio astronomers have been attempting to use the entire Galaxy as a gravitational-wave detector. The trick is to monitor dozens of neutron stars called pulsars. These spin on their axis hundreds of times per second while emitting a radio-frequency beam, producing what looks like a pulse of light on each turn. Gravitational waves sweeping the Galaxy would change the distance between Earth and each pulsar, creating anomalies in detected pulsar frequencies from one year to the next. Observations of a collection or array of pulsars — called a pulsar timing array (PTA) — should be able to detect changes induced by gravitational waves with frequencies of just nanohertz, as might be produced by pairs of supermassive black holes, for example. It takes tens of years for successive crests of such waves to pass a given vantage point, meaning that tens of years of observations are needed to spot them. Giant gravitational waves: why scientists are so excited In 2023, the PTA technique began to bear fruit. Four separate collaborations, in North America, Europe, Australia and China, unveiled tantalizing hints of a pattern expected from a random ‘stochastic background’ of gravitational waves that make Earth slosh around, probably caused by a cacophony of supermassive black-hole binaries, says astrophysicist Chiara Mingarelli at Yale University in New Haven, Connecticut. The teams have not yet used the word ‘discovery’, because the evidence that each collaboration unveiled is not yet rock solid. But three of the groups — all but the Chinese one — are now pooling their data and conducting a joint analysis in the hope of getting to the ‘D’ word. This requires painstaking work, because each group processed its raw data in slightly different ways, and so it could take at least another year to get to publication, says Scott Ransom, an astrophysicist at the US National Radio Astronomy Observatory in Charlottesville, Virginia, and a senior member of the North American collaboration. “In our current data, we almost certainly have the hints of individual supermassive black-hole binaries out there,” says Ransom. With each extra year of observation, they should get closer to resolving single black-hole pairs out of the cacophony, he adds. “Things are just going to get better and better.” Microwave telescopes: spotting waves from the Big Bang A year before LIGO’s 2015 detection, a team of cosmologists using a South Pole telescope called BICEP2 claimed to have spotted gravitational waves — not directly, but in the pattern of light called the cosmic microwave background (CMB), sometimes described as the afterglow of the Big Bang. The BICEP2 claim turned out to be premature, but cosmologists are now doubling down on this idea. An array of telescopes much more powerful than BICEP2, called the Simons Observatory, is being set up on a mountaintop in northern Chile’s Atacama Desert. Some researchers are holding out hope for an even more powerful array called CMB-S4 (originally proposed to include 12 telescopes in Chile and at the South Pole) — although in May, plans for that project were put on hold because of the disrepair of the US South Pole base. Gravitational waves: 6 cosmic questions they can tackle What cosmologists are looking for in the CMB is a specific ‘B mode’ pattern in the swirls of its polarization — the preferential directions in which the microwaves wiggle — that would have been imprinted by the passage of gravitational waves. The theory is that such waves should have been produced by inflation, a quick burst of exponential cosmic expansion thought to have happened around the time of the Big Bang2. Inflation would explain many of the Universe’s most striking properties, such as its flatness and how mass is distributed. The gravitational waves that inflation produced would have started at high frequencies, but would by now be at incredibly low frequencies of around 10−14 Hz. Although inflation is a cornerstone of accepted cosmological theory, there’s no proof of it yet. The B-mode pattern would be the smoking gun and, moreover, would reveal the energy scales involved, which would be a first step towards understanding what powered inflation. The problem is, no one knows whether that energy scale was large enough to have left a noticeable mark. “Inflation predicts the B modes, but we don’t know if it’s big enough to be detected,” says Marc Kamionkowski, a theoretical astrophysicist at Johns Hopkins University in Baltimore, Maryland. But if the leading models are right, either the Simons Observatory or CMB-S4 should eventually find it, he says. Atom interferometry: closing the gap Although many of these projects push gravitational-wave science towards lower frequencies, they leave a crucial gap just below 1 Hz. Detecting such frequencies could reveal mergers of black holes much more massive than those seen by LIGO (which spots waves from collapsing stars that weigh at most a few tens of solar masses). “This is an unexplored region, but it could be populated with lots of black holes,” says Caprini. Jason Hogan (left) and Mark Kasevich work on an atom interferometer — a device that could reveal mergers of black holes much more massive than those seen by current laser interferometers.Credit: L.A. Cicero and Stanford University A nascent technique could come to the rescue, according to physicist Oliver Buchmüller at Imperial College London. “Atom interferometry sits in that gap which we currently cannot explore with any other technology,” he says. An atom interferometer is a vertical high-vacuum pipe in which atoms can be released and allowed to fall under gravity. As they do so, physicists tickle the atoms with laser light to toggle them between an excited and a relaxed state — the same principle used by atomic clocks. “We’re trying to push this atomic-clock technique to what’s ultimately possible,” says Jason Hogan, a physicist at Stanford University in California. To detect gravitational waves, physicists plan to drop two or more sets of atoms at different heights inside the same vertical pipe, and to measure the time it takes for a laser pulse to travel from one set of atoms to the next, says Hogan. The passage of gravitational waves would result in light spending either slightly less or slightly more time travelling between them — a variation smaller than one part in 100 billion billion. Pioneering experiments at Stanford University have developed atom interferometers with 10-metre drops, but detecting gravitational waves would require devices at least 1 kilometre in height, which could be installed in a mine shaft, say, or even in space. As a first step, several groups around the world are planning to build 100-m atom interferometers as test beds. One such facility, called MAGIS-100, is already under construction in an existing shaft at the Fermi National Accelerator Laboratory outside Chicago, Illinois, and is scheduled for completion in 2027. Desktop detectors: pushing the frequency up Other researchers are exploring ways of detecting gravitational waves with much, much smaller (and cheaper) detectors — including some that could fit on a desktop. These are designed to watch for extremely high-frequency gravity waves. Known phenomena probably don’t produce such waves, but some speculative theories do predict them. The Levitated Sensor Detector (LSD) at Northwestern University in Evanston, Illinois looks like a toy LIGO: it bounces lasers between pairs of mirrors just 1 metre apart. The LSD is a prototype for a new type of instrument designed to sense gravitational waves using resonance: the same principle by which even little pushes can make a child on a swing go higher and higher if they are timed just right3. Will the Einstein Telescope be split in two? In a vacuum inside each of the LSD’s arms, laser light suspends a particle just micrometres wide. As with an interferometer, the passage of gravitational waves will alternately elongate and compress the length of each arm. If the frequency of the gravitational waves resonates with that of the device, the lasers will then give many tiny kicks to the particle. The LSD can track the particle’s motion with a precision of femtometres, says Northwestern physicist Andrew Geraci, who is leading the project. The LSD is designed to be sensitive to gravitational waves with frequencies of around 100 kHz. This prototype might already have a shot at detecting some, if the team can keep experimental noise in check — and provided that such waves exist. “Depending how optimistic you are, we may be able to measure a real signal in that band even with a 1-m instrument,” Geraci says. Future versions could be scaled up to 100-m-long arms, he adds, which would increase their sensitivity. Theoretical physicist Ivette Fuentes at the University of Southampton, UK, has an idea for making an even smaller resonant detector. She aims to exploit sound waves in an exotic state of matter called a Bose–Einstein condensate (BEC) — a cloud of atoms kept at temperatures as low as a few millionths of a degree above absolute zero. If a gravitational wave passes through at a frequency that resonates with the sound wave, it can be detected. Because the act of looking for this signal destroys the BEC, a new flood of atoms needs to be released every second. The process might need to be repeated for months for a successful detection, Fuentes says. In principle, a BEC-based detector could expand the search for gravitational waves to extremely high frequencies of 1 MHz or more — again, provided they exist. Fuentes says her scheme would require pushing BEC techniques just a little beyond the current state of the art. “I think the idea is very bold,” she says. Physicists have posited that high-frequency gravitational waves could reveal exotic physics that went on in the first second or so after the Big Bang. “We could use it to study the state of the Universe at very high energies,” says Caprini. Quantum crystal: only takes a second A final, more radical proposal for detecting gravitational waves involves putting objects in two places at once. ‘Best view ever’: observatory will map Big Bang’s afterglow in new detail Sougato Bose, a physicist at University College London, has proposed a device in which a micrometre-sized diamond crystal is put in a superposition of two quantum states. In his scheme, the crystal’s two ‘personas’ would be pushed apart by as much as 1 metre and then brought together again — an extremely delicate procedure that has been compared to putting the nursery-rhyme character Humpty Dumpty back together after a fall. The passage of gravitational waves would make one persona travel further than the other when apart, putting them out of sync — in a measurable way — when reunited. The whole process would take around one second to complete, which would make the device sensitive to gravitational waves of around 1 Hz. This idea is extremely ambitious: such quantum tricks have so far been shown to work only for objects the size of molecules, and no one has ever tested whether quantum weirdness can be pushed to such extremes. “Putting Humpty Dumpty back together has never been demonstrated for crystals,” says Bose. But if the technique can be perfected, then table-top experiments such as this one could take gravitational-wave detection out of the hands of just a few large-scale labs. Together, these techniques could blow open the window on what can be seen. “The outlook is very positive,” says Caprini. doi: https://doi.org/10.1038/d41586-024-02003-6 References Branchesi, M. et al. Space Sci. Rev. 219, 67 (2023). Article Google Scholar Kamionkowski, M., Kosowsky, A. & Stebbins, A. Phys. Rev. Lett. 78, 2058 (1997). Article Google Scholar Arvanitaki, A. & Geraci, A. A. Phys. Rev. Lett. 110, 071105 (2013). Article PubMed Google Scholar Download references Reprints and permissions Latest on: Astronomy and astrophysics Cosmology Quantum physics First ever rocks from the Moon’s far side have landed on Earth NEWS 25 JUN 24 No massive black holes in the Milky Way halo ARTICLE 24 JUN 24 Bound star clusters observed in a lensed galaxy 460 Myr after the Big Bang ARTICLE 24 JUN 24 Jobs Postdoctoral Associate- Statistical Genetics Houston, Texas (US) Baylor College of Medicine (BCM) Senior Research Associate (Single Cell/Transcriptomics Senior Bioinformatician) Metabolic Research Laboratories at the Clinical School, University of Cambridge are recruiting 3 senior bioinformatician specialists to create a dynam Cambridge, Cambridgeshire (GB) University of Cambridge Cancer Biology Postdoctoral Fellow Tampa, Florida Moffitt Cancer Center Postdoctoral Scholar - Physiology Postdoctoral Scholar - Physiology-Epigenetic and Exosomal regulation of cardiovascular and renal function Memphis, Tennessee The University of Tennessee Health Science Center (UTHSC) Postdoctoral fellow – iPSC generation for modeling genetic disorders Postdoctoral fellow – iPSC generation for modeling genetic disorders – Mayo Clinic Rochester, Minnesota (US) Mayo Clinic",
    "commentLink": "https://news.ycombinator.com/item?id=40820063",
    "commentBody": "New ways to catch gravitational waves (nature.com)132 points by sohkamyung 6 hours agohidepastfavorite52 comments openrisk 1 hour agoSomething already lost in the twisted passages of history is that the first generation of gravitational wave detectors was of an entirely different design than the current interferometers [1]. It never worked and Weber's claim to have detected gravitational waves from SN1987A in 1987, was widely discredited... [1] https://en.wikipedia.org/wiki/Weber_bar reply umvi 1 hour agoprevProbably a dumb question, but... is it basically proven then that gravity doesn't exist (it's effects are just a result of spacetime's geometry?). Because it seems like these gravitational waves experiments show that spacetime exists and has measurable geometry. Yet every time quantum mechanics comes up everyone talks about how we haven't found the gravity force carrier yet which doesn't make sense to me if gravity doesn't exist and is a consequence of the geometry of spacetime. reply superposeur 49 minutes agoparentThese experiments confirm the classical theory of gravity, which is Einstein’s general relativity, just as Maxwell’s equations are the classical theory of the electromagnetic field. Just as Maxwell’s equations are perfectly adequate for waves of macroscopic intensity, GR is perfectly adequate for astrophysical gravity waves. A whole separate question is what is the quantum mechanical theory that has general relativity as its classical limit? For electromagnetism, quantum electrodynamics (understood in the 40’s) is the quantized version of Maxwell’s and predicts that electromagnetic energy measurement outcomes come in “chunks” (photons). But, although much is known about features of “quantum gravity” (like that gravitons will be massless, spin 2), there is famously no consensus yet about the precise theory. As to how to reconcile the force carrier picture with spacetime picture — even classically one can consider an “overall” spacetime background geometry such as that created by the whole earth. Then consider little ripples perturbing this background. Gravitons are these little ripples turned on a quantized amount (heuristically). How the overall background itself gets formed as an “enormous pile of gravitons” will depend on the precise theory of quantum gravity. String theory does have a partial answer to this so can model such things as black holes quantumly. reply simcop2387 1 hour agoparentprevSo I want to try to answer what I can, despite being a layman on this. Gravity exists, it manifests as the warping/geometry of space. This is in contrast to the other fundamental forces which get explained via Quantum Field Theory. That's the very high level difference of the two, our current understanding of gravity does not work the same way as the way everything else does, and so far we can't find a provable theory (yet) that makes the two work together at all scales. String theory purported as a way to create a quantum theory of gravity and explain everything else, but my understanding is that it's fallen out of favor because it mostly turned into a tunable mathematical framework that could just change to fit any observations that were made, so it doesn't have the same kind of predictive power that people want (i.e. too much freedom so it can be used to explain anything, not just everything). I believe this is where predictions about a possible gravitational force carrier generally come from, aka the graviton. Then there's theories like Loop Quantum Gravity, where the way it works is that space-time itself is quantized and that's how you get things to mesh because you can now use the same wave-function style of things that all other quantum theories use. Though I think this doesn't predict much about a quantum field for gravity on it's own. I believe one of the other things that runs into everything being difficult is that with relativity you end up with a lot of infinities in the equations and results and so there's a \"new\" kind of math for it that gets called \"renormalization\" that prevents them from coming out but it also has issues when translating between quantum theories and relativity. reply mati365 1 hour agoparentprevIsn't space gravity carrier itself? Something like the water that has waves? reply simcop2387 1 hour agorootparentI believe they're referring to the idea of a gravitational force carrying particle, like photons for electromagnetism, W/Z bosons for the weak force, and gluons for the strong force. Typically this gets called a graviton but they've never been observed and the theories predicting them as far as I know don't predict that we can detect them in any meaningful/practical way right now which is also one of the problems for issues with a quantum theory of gravity. I've always wondered (but not done the research/reading) on how that would mesh with black holes since you'd need gravitons to escape to mediate the curvature of space-time but that'd seemingly (to me) require them to be able to either ignore the curvature of space-time or travel faster than the speed of light in order to do so. And I believe that those two options there are actually mathematically equivalent as far as the consequences of things go. reply superposeur 3 minutes agorootparentThis question isn’t actually anything to do with quantum mechanics. One can already ask, for GR, can a gravity wave generated by a wiggling object that has already passed the event horizon propagate back out through the event horizon? The answer from gr is unambiguously no. Effectively, all complicated goings-on inside the black hole are washed out into just a couple parameters for an outside observer: total mass, total charge, total angular momentum. All the matter contained by the black hole does still make its gravitational effect felt on outside objects through these three parameters. This observation in fact is what inspired wheeler to ask his grad student bekenstein what then happens to the entropy of a cup of tea thrown into the black hole — how to reconcile with 2nd law of thermo. Which in turn was the start of the very long story of black hole thermodynamics. reply atombender 26 minutes agorootparentprevThat would suggest space is \"made of\" something, so what would that be? If it's made of something, how is that \"carrying\" the wave, causing it to distort in the presence of mass? reply gadilif 4 hours agoprevSo, going from a very narrow frequency band (up to 1000Hz) to a much wider range, which can theoretically encode information (e.g. frequency modulation)... Hmm, I'm wondering if comms over gravity is something a sufficiently advanced civilization might consider using, and should we be looking for that 'Hello, world' in some 'natural frequency' like we're doing for EM radiation? reply qual 4 hours agoparentI'm not sure I see what the advantages of communicating this way would be. The amount of energy required per bit transmitted would be astounding. I feel like, while theoretically possible, it's pretty much all downsides and no upsides. At least for communication purposes. However, your comment reminded me of an interesting PBS Space Time episode discussing the possibility of finding alien civilizations via the gravitational waves produced by their massive ships accelerating to near light speed. https://www.pbs.org/video/could-ligo-find-massive-alien-spac... reply supportengineer 3 minutes agorootparentWhat would a gravitational wave generator look like? A machine to \"wiggle\" an asteroid, or say a moon? What if you made a huge array of small machines that \"wiggle\", say, a bowling ball, in perfect sync. reply colmmacc 3 hours agorootparentprevThe medium is the message. If we detected a modulated signal in gravitational waves, it would be like an Iron Age tribe receiving a 100 foot tall perfectly polished stainless steel statue with ornate inscriptions and pictograms. It would be recognizable and within our conception, but it would also be a demonstration of development and access to resources beyond our imagination. That's the upside. In a galaxy of sparse and sparingly advanced civilizations, the message might be \"fear us and stay away\" in a way that EM would not convey. reply pitaj 2 hours agorootparentprevYou can detect gravitational waves based on the strain (amplitude) directly, rather than relying on intensity like electromagnetism. Because the amplitude is inversely proportional to the distance, but intensity is inversely proportional to the distance squared, this could allow for communication over longer distances. reply gadilif 3 hours agorootparentprevThanks for the PBS reference, interesting! As to the energy required, well, yes, I guess manipulating huge masses will be costly, but, if there is an efficient way to do this, then gravity waves are a parallel plane of communication. The analogy I heard once is about tribes communicating with smoke signals, while the air around them is filled with radio waves. Maybe we can't hear anyone out there because we're not listening to the right thing... reply Larrikin 2 hours agorootparentprevMaybe that is the filter. If your society hasn't figured out the tech to do it efficiently, the rest of the galaxy doesn't care about what you have to say. reply kimbernator 43 minutes agorootparentI've always been a bit confused about the idea that an advanced civilization would be uninterested in us because we've \"only\" reached the ability to communicate via radio waves. We're either a threat or an ally to these other civilizations, and if you were them and you detected a society that was clearly on course to eventually catch up, it would be in your best interest to treat them like one of those things ASAP so you can have a hand in their development. To me it feels most likely that our signals just have not had enough time to get to them. reply adrianN 2 hours agorootparentprevThe upside is that space is quite transparent for gravitational waves. reply imoverclocked 2 hours agoparentprevLots of naysayers but I can think of one very valid reason this might be the case. Our species, and many species on our planet, can see a very narrow band of the radiation spectrum. It’s possible an advanced civilization never developed that but was instead far more intimate with gravitational energy instead. We are talking about the universe after all. reply brianjlogan 3 hours agoparentprevSince gravitational waves would still be restricted to the speed of light what would the benefit be? Would \"obstacles\" be circumvented? I would think interference would still be possible but instead of line of sight it would be large gravitational distortions (black hole, stars). Humility clause: I don't know what I'm talking about. reply BurningFrog 4 hours agoparentprevI can't really imagine how such a transmitter works? Somehow moving planet sized objects around to create gravity waves? Of course, the cop-out \"using their advanced tech we don't have a clue about\" answer could actually be correct. reply vbezhenar 4 hours agorootparentCreate tiny black holes and throw them into each other. reply serial_dev 4 hours agoparentprevIf we are going all in on imaginary civilizations, even a very narrow (well, not that narrow) frequency band could be used to encode information, couldn't they? reply gadilif 4 hours agorootparentYes, but not efficiently (it will be very slow) and will be hard to detect because of noise (the article mentions the 'cacophony' of gravity noise). This is similar to EM radiation, so, you do frequency modulation (or amplitude, but that seems harder with gravity - you'll need to modify mass...), so you have a base high frequency on top of which you add lower frequency. The substraction later gives you a clean signal (or, Gravity Radio). reply vbezhenar 4 hours agorootparentWill you distinguish it from noise? Surely those will be compressed and encrypted. reply gadilif 3 hours agorootparentSome of it, definitely. But if the purpose is to seek others and announce yourself (like we did with Voyager 1 and 2), then you'll want it to be plain. Also - Public radio transmissions are not encrypted (as your purpose is to have as many listeners as possible... You monetize on ads). reply fsmv 3 hours agoparentprevThis idea was featured in The Three Body Problem trilogy. The problem is modulating the signal. The only way is to move large masses quickly. reply captainkrtek 3 hours agoprevJust a plug that you can tour the LIGO facilities for free! I visited a couple years ago and got a tour of the Hanford facility, included a lecture beforehand as well. Really awesome people and got to tour the entire facility, even going to the control room. https://www.ligo.caltech.edu/WA/page/lho-public-tours reply freeqaz 2 hours agoparentThe only dates available are during DEFCON. I would so love to go sometime though! Thanks for sharing -- this made my morning. reply captainkrtek 34 minutes agorootparentI’d highly recommend it if you get the time, was an excellent tour to geek out on. Enjoy DEFCON! reply nudgeee 4 hours agoprevI’m amazed there was no mention of LISA [0] — a space-based gravitational wave detector using 3 satellites flying in formation 2.5 million Km apart! Seriously cool engineering, planning to launch in 2035. [0] https://en.m.wikipedia.org/wiki/Laser_Interferometer_Space_A... reply shellac 4 hours agoparentIt is mentioned: > ...Researchers are now working on several next-generation LIGO-type observatories, both on Earth and, in space, the Laser Interferometer Space Antenna;... reply nudgeee 4 hours agorootparentOoof missed that, thanks! reply cubefox 2 hours agoparentprev> LISA was first proposed as a mission to ESA in the early 1990s. I remember reading about LISA when I was a little kid. Back then it was projected to launch in the far future of 2015. Now I would be surprised if it actually launches in 2035. reply jjk166 4 hours agoparentprevLISA is mentioned in the 6th paragraph. reply cypherpunks01 2 hours agoprevI was surprised that the article doesn't mention LISA at all. I had thought that's the next stage gravitational wave detector on the horizon? https://www.esa.int/Science_Exploration/Space_Science/LISA_f... reply qual 2 hours agoparentIt's mentioned both in the article, and in the comments here where someone else thought it isn't mentioned. They didn't use the acronym (\"LISA\"), but instead spelled out the entire thing. >Researchers are now working on several next-generation LIGO-type observatories, both on Earth and, in space, the Laser Interferometer Space Antenna; [...] reply ape4 4 hours agoprevThis made (very lay-person) me wonder if you could detect quantum wave function events. https://physics.stackexchange.com/questions/275556/can-you-d... reply tea-coffee 2 hours agoprevAny physicists in here that could layout a path of going from rudimentary first year level physics knowledge to being able to understand on a deeper level topics such as gravitational waves? These articles are interesting but are very abstract when you do not have knowledge from first principles. reply mrlonglong 3 hours agoprevIt's entirely possible if it's extremely sensitive to detect the waves given off by alcubierrie warp drives crisscrossing the galaxies. reply demondemidi 5 hours agoprevHow does one aim a gravitational wave detector? Or is that not how they work? reply serial_dev 4 hours agoparentYou can't aim our current grav wave detectors. In a very simplified form, it's two very long tubes with laser in it, you aren't pointing these buildings like you could do with even very large telescopes. The detectors have different sensitivity depending on the direction of the waves, so if a wave comes from a \"bad\" direction (perpendicular to both arms, e.g. directly from above), a particular detector might not detect anything, even if the wave is strong. This is why (amongst many other reasons) it's important to have multiple detectors around the world (e.g LIGO has two locations in the US, you also have Virgo in Italy, and they do collaborate), this way you can assure in theory that you can \"see\" every wave, no matter which direction it's coming from. (AFAIK). reply throwawaymaths 3 hours agorootparentHaving only two detectors was always a sketchy affair given the tremendous amount of filtering done by the teams. Now with three detectors, if the correlataed event count between pairs of detectors doesn't diminish from one detector events with the same ratio as from 2-3 detectors (high school statistics), then we will know that event detection is real. reply simcop2387 1 hour agorootparentprev> The detectors have different sensitivity depending on the direction of the waves, so if a wave comes from a \"bad\" direction (perpendicular to both arms, e.g. directly from above), a particular detector might not detect anything, even if the wave is strong. Unless there's such a thing as polarized gravitational waves, https://www.ligo.org/science/Publication-O1StochNonGR/ which might exist but are hard to discern with current detectors from what I understand. It'd be really cool to learn that there's such a thing as vector and scalar polarized gravitional waves. reply pwatsonwailes 2 hours agorootparentprevPretty spot on. reply jfengel 2 hours agoparentprevA gravitational wave moves at the speed of light. So if you compare arrival times from several widely-spaced detectors, you can get a pretty good idea of what direction the wave came from. Even a single detector has two arms at 90 degrees to each other, which can give you a rough idea of where in the sky it came from. But now that we have multiple detectors online we get better and better about spotting the origin. reply msarchet 4 hours agoparentprev> LIGO can't point to specific locations in space Since LIGO doesn’t need to collect light from stars or other objects or regions in space, it doesn't need to be round or dish-shaped like optical telescope mirrors or radio telescope dishes. Nor does it have to be steerable, i.e., able to move around to point in a specific direction. Instead, each LIGO detector consists of two 4 km (2.5 mi.) long, 1.2 m-wide steel vacuum tubes arranged in an \"L\" shape (LIGO's laser travels through these arms), and enclosed within a 10-foot wide, 12-foot tall concrete structure that protects the tubes from the environment. > A mirror at the vertex of the arms splits a single light beam into two, directing each beam down an arm of the instrument Mirrors at the ends of the arms reflect the beams back to their origin point where they are recombined to create an interference pattern called 'fringe https://www.ligo.caltech.edu/page/what-is-ligo https://www.ligo.caltech.edu/page/ligos-ifo reply sithadmin 4 hours agoparentprevIntuitively, it would seem that one wouldn't 'aim' a gravitational wave detector. The inputs are omnidirectional, and direction-finding would be accomplished by triangulation based on an array of detectors. reply 4ad 5 hours agoparentprevGravitational waves detectors, just like any kind of antennas have anisotropic sensitivity. By having multiple detectors with different orientations you can determine the source of the signal. But you can't aim them, no, at least not any current or planned ones. They are simply too big (many kilometers) and require extreme stiffness. Everybody knows of LIGO, but it's actually three detectors that work together, LIGO, Virgo, and KAGRA. reply _Microft 4 hours agorootparentThere is also a project called GEO600 in Germany that is collaborating with LIGO (and therefore also Virgo/KAGRA). This detector is much smaller though. https://en.m.wikipedia.org/wiki/GEO600 reply BurningFrog 4 hours agorootparentprevYou can also figure out the source direction by when the signal arrives at the different (currently three, on a good day) detectors. reply butterNaN 3 hours agoparentprevSimilar discussion elsewhere on the internet: https://einsteinathome.org/content/how-can-they-aim-ligo reply andsens 5 hours agoprev [–] Now that’s the kind of clickbait title I can get behind! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The first direct detection of gravitational waves in 2015 by LIGO and Virgo observatories revolutionized physics, enabling astronomers to 'hear' cosmic events.",
      "Current detectors are limited to frequencies between 100–1,000 Hz, but new instruments and techniques aim to detect a broader range, from megahertz to nanohertz frequencies.",
      "Five new methods include Pulsar Timing Arrays, Microwave Telescopes, Atom Interferometry, Desktop Detectors, and Quantum Crystals, each targeting different frequency ranges and promising to uncover more cosmic phenomena."
    ],
    "commentSummary": [
      "The first generation of gravitational wave detectors failed to detect waves, discrediting Weber's 1987 claim, but current interferometers like LIGO, Virgo, and KAGRA have successfully detected them.",
      "Gravitational waves confirm Einstein’s general relativity, yet a quantum theory of gravity remains unresolved, with string theory and Loop Quantum Gravity offering potential but challenging explanations.",
      "Future projects like LISA aim to enhance detection capabilities, and public tours of LIGO facilities are available for educational purposes."
    ],
    "points": 132,
    "commentCount": 52,
    "retryCount": 0,
    "time": 1719578482
  },
  {
    "id": 40818987,
    "title": "Parkinson's Link to Gut Bacteria Suggests Unexpected, Simple Treatment",
    "originLink": "https://www.sciencealert.com/parkinsons-link-to-gut-bacteria-suggests-unexpected-simple-treatment",
    "originBody": "Parkinson's Link to Gut Bacteria Suggests Unexpected, Simple Treatment HEALTH 26 June 2024 ByTESSA KOUMOUNDOUROS Illustration of bacteria on the colon epithelium. (Nanoclustering/Science Photo Library/Getty Images) Researchers have suspected for some time that the link between our gut and brain plays a role in the development of Parkinson's disease. A new study just identified gut microbes likely to be involved and linked them with decreased riboflavin ( vitamin B2) and biotin (vitamin B7), pointing the way to an unexpectedly simple treatment that may help: B vitamins. \"Supplementation of riboflavin and/or biotin is likely to be beneficial in a subset of Parkinson's disease patients, in which gut dysbiosis plays pivotal roles,\" Nagoya University medical researcher Hiroshi Nishiwaki and colleagues write in their published paper. The neurodegenerative disease impacts almost 10 million people globally, who at best can hope for therapies that slow and alleviate symptoms. Symptoms typically begin with constipation and sleep problems, up to 20 years before progressing into dementia and the debilitating loss of muscle control. (pixelshot/Canva Pro) Previous research found people with Parkinson's disease also experience changes in their microbiome long before other signs appear. Analyzing fecal samples from 94 patients with Parkinson's disease and 73 relatively healthy controls in Japan, Nishiwaki and team compared their results with data from China, Taiwan, Germany, and the US. While different groups of bacteria were involved in the different countries examined, they all influenced pathways that synthesize B vitamins in the body. The researchers found the changes in gut bacteria communities were associated with a decrease in riboflavin and biotin in people with Parkinson's disease. Nishiwaki and colleagues then showed the lack of B vitamins was linked to a decrease in short-chain fatty acids (SCFAs) and polyamines: molecules that help create a healthy mucus layer in the intestines. \"Deficiencies in polyamines and SCFAs could lead to thinning of the intestinal mucus layer, increasing intestinal permeability, both of which have been observed in Parkinson's disease,\" Nishiwaki explains. Summary of findings from the study and speculations from previous research. (Nishiwaki et al., npj Parkinson's Disease, 2024) They suspect the weakened protective layer exposes the intestinal nervous system to more of the toxins we now encounter more regularly. These include cleaning chemicals, pesticides, and herbicides. Such toxins lead to the overproduction of α-synuclein fibrils – molecules known to amass in dopamine-producing cells in the substantia nigra part of our brains, and increased nervous system inflammation, eventually leading to the more debilitating motor and dementia symptoms of Parkinson's. A 2003 study found high doses of riboflavin can assist in recovering some motor functions in patients who also eliminated red meat from their diets. So it's possible that high doses of vitamin B may prevent some of the damage, Nishiwaki and team propose. Illustration of a riboflavin (B2) molecule in the blood. (Nemes Laszlo/Science Photo Library/Getty Images) This all suggests ensuring patients have healthy gut microbiomes may also prove protective, as would reducing the toxic pollutants in our environment. Of course, with such a complicated chain of events involved in Parkinson's disease, not all patients likely experience the same causes, so each individual would need to be assessed. \"We could perform gut microbiota analysis on patients or conduct fecal metabolite analysis,\" explains Nishiwak. \"Using these findings, we could identify individuals with specific deficiencies and administer oral riboflavin and biotin supplements to those with decreased levels, potentially creating an effective treatment.\" This research was published in npj Parkinson's Disease.",
    "commentLink": "https://news.ycombinator.com/item?id=40818987",
    "commentBody": "Parkinson's Link to Gut Bacteria Suggests Unexpected, Simple Treatment (sciencealert.com)127 points by pelasaco 9 hours agohidepastfavorite48 comments sambeau 8 hours agoHere's the paper if you'd like to skip the horribly ad-laden science alert site. https://www.nature.com/articles/s41531-024-00724-z reply Larrikin 5 hours agoparentThere are no ads on my phone. But I have Ad Nauseam installed. reply narrator 8 hours agoprevThese guys say it's a certain strain of gut bacteria that causes Parkinson's: https://yle.fi/a/74-20030498 The immune system declines in old age and it migrates from the gut to the brain. Certain groups of gut bacteria like different nutrient environments, so the gut bacteria from this study might like that environment too. reply ThePhysicist 8 hours agoprevThis is very interesting when thinking about the known link to higher Parkinsons/Alzheimers risk for people that are exposed to fungicides / biocides. Gut bacteria synthesize plenty of vitamins for the body, so it's not far-fetched to think that killing off a fraction of your gut biome due to exposure to these substances will lead to vitamin deficiencies and neurological problems. There are studies that show transplanting gut bacteria from healthy people can alleviate some motor symptoms of Parkinsons [1]. So take good care of your gut bacteria! 1: https://www.parkinsons.org.uk/news/gut-bacteria-transplant-m... reply xtiansimon 7 hours agoparent> “So take good care of your gut bacteria!” Unless you get a bad bug and go on heavy antibiotics. I had a bad pneumonia and had 4-day stay at my local hospital. And they pumped so much antibiotics I was sh!tt’n like a goose for weeks reply bottom999mottob 6 hours agorootparentQuaternary ammonium compounds (quats) are commonly used as fungicides and disinfectants in hospitals, and I fear for the health-care workers that have to touch them all day. Of course, we should use disinfectants in hospitals, but the long-term neurological effects to healthcare workers and terminally ill patients is concerning. reply rom16384 6 hours agorootparentThat and Tetrachloroethylene (TCE) [1] which is used as brake cleaning fluid or in dry cleaning. [1] https://www.science.org/content/article/widely-used-chemical... reply voisin 6 hours agorootparentAlso, IIRC, the basis of the true story behind the movie and book A Civil Action - oft recommended for students entering 1L (the book) due to its coverage of tort law. reply viraptor 8 hours agoprevI really hope we dig into the gut processing of B group a bit more. Another candidate for improvements there is some types of ADHD. Some better defined links there would by awesome. reply mywacaday 7 hours agoparentI have recently been diagnosed with ADHD and haven't come across gut health as being a risk factor, do you have any references? reply viraptor 7 hours agorootparentGut relation doi.org/10.21203/rs.3.rs-28862/v1 B6, B2 deficiencies in ADHD doi:10.1192/bjpo.bp.116.003491 But that's a deep hole and a good way to accidentally poison yourself. If you've just been diagnosed, you'll have lots of other, approved options to try first. reply uncertainrhymes 7 hours agoprevGreat, they identified differences in gut bacteria. Their control group was usually the spouses, so they were geographically similar. Wonderful. People with PD are on very different medications, controlled diets (you need to avoid protein at different parts of the cycle), and.... what 'simple treatment'? The conclusions in that fluff piece are not in the original study. Sure, some bacteria are linked to differences in uptake of some vitamins, but why would adding more vitamins to the diet affect the PD? It isn't like that is the cause of PD, at best a symptom. Bad science journalism. reply froh 6 hours agoparentdid we read the same study?? https://www.nature.com/articles/s41531-024-00724-z from there: > Riboflavin (vitamin B2) in humans originates from food and gut microbiota. Therapeutically, riboflavin improves oxidative stress, mitochondrial dysfunction, neuroinflammation, and glutamate excitotoxicity, which are related to PD pathogenesis27. In a clinical study, high doses of riboflavin ameliorated motor deficits in PD patients28. In another disease, supplementation of riboflavin in patients with Crohn’s disease decreased systemic oxidative stress, inflammatory effects, and disease activity29. Supplementation of riboflavin to compensate for decreased riboflavin production by gut microbiota may be beneficial in PD patients. > Biotin (vitamin B7) produces anti-inflammatory substances and decreases inflammation, which leads to the relief of allergy, immunological symptoms, and inflammatory bowel disease30. The effects of biotin on PD have not been reported to the best of our knowledge. In contrast, in multiple sclerosis, open31 and double-blind32 studies showed that biotin ameliorated motor and optical defects. now these supplements are the tentative suggestion in the article. what you _may_ validly point out is that these supplements are only identified as of _potential_ helpt, because follow up studies are needed to validate if this really is the case. but neither the study nor the article claim otherwise, right? reply ionwake 6 hours agoparentprevI literally couldn’t understand your comment because I was unsure if you were being sarcastic reply datavirtue 6 hours agoprevVery interesting. I'm 45 and recently became chronically constipated. I determined that I had somehow killed off my gut bacteria and began a treatment of eating nothing but cruciferous salads each day, supplementing with probiotics (with as many bacterial strains as I could get). I was eating yogurts regularly, but none of those contained enough bacterial variety (you need many more strains to maintain health). This restored me to perfect function and taught me that my gut microbiome was fucked up for years, even decades. I cannot believe how good I feel and how perfectly my digestive system functions now. When the constipation hit I thought I was literally dying. Even a 100% raw vegetable diet was not digesting properly and would takes days to exit at first. Any carbs or bread at all would jam me up completely. I have been maintaining perfect gut function since then by avoiding alcohol and eating a wide range of raw vegetables and cruciferous salads to feed the bacteria. However, I'm able to digest anything now, effortlessly. Remarkable. reply Teslazar 1 hour agoparentHow long did it take to see significant improvement and how long did it take before you figured you had restored perfect function? Can you elaborate on what you ate? reply voisin 2 hours agoparentprevDo you intend to take probiotics forever? I understand (possibly incorrectly) that the microbiome shifts back quickly as soon as any of the interventions stop, and while the others seem like they should be sustainable (part of a good lifestyle) I always wonder about how realistic it is to keep taking probiotics for life. reply invalidname 8 hours agoprevThis seems odd, as someone who has no medical knowledge. If it's just adding B vitamins, then a lot of people take vitamins. Especially when diagnosed with something as terrible as Parkinson's. I would expect to hear some anecdotal evidence on that for a diseases that is so common. Edit: Very informative replies. Thanks! reply Attummm 8 hours agoparentIf we think about what vitamins are and consider the human body as an interconnected ecosystem with sub-ecosystems such as gut flora and skin flora, vitamins are compounds that your body cannot create from other compounds. Therefore, if these vitamins are not present, it could result in negative outcomes. Yet, the human body is remarkably good at making do, masking issues by priming itself for certain vitamins that are lacking in the diet. However, if this state is maintained for years, something has to give. Science is finding that many chronic diseases, even some that have been thought incurable, are actually tied to lifestyle and diet. So, expect more of these articles. reply atombender 7 hours agorootparentThe idea that vitamins are substances the body cannot make is somewhat outdated: * Most of the vitamin D we need is actually synthesized by the body. (You still need sunlight to release it.) * The body can also make vitamin B3 (niacin). * Vitamin K and B7 (biotin) are made by gut bacteria. reply Attummm 6 hours agorootparentThere is an ongoing debate in science regarding which compounds should or shouldn't be classified as vitamins. But that discussion is beside the point. Not only are vitamins important, but minerals are too. We cannot produce minerals ourselves. Magnesium is involved in at least 300 different metabolic processes. If these essential compounds are not available, metabolic processes become difficult or impossible, eventually leading to chronic diseases that had been thought of as inevitable. The study of the article found that a subset cases of parkinson are related to lifestyle and diet. reply r2_pilot 8 hours agoparentprevSo, it turns out many vitamins in supplements are poorly absorbed in the gut. B12 shots are a thing too. reply dghughes 8 hours agorootparentBioavailability is as important as the nutrient that you're trying to get. reply djtango 8 hours agorootparentprevYes - I was a Chemist in another life and the reality of physiology is that it is insanely complex and \"just take vitamin pills\" can amount to Dunning Kruger if things like the medium (a suspension, a solid, the solvents around it to think of a few...) can play such a huge role in the body's ready uptake of the active ingredient. I mean we kind of already know this anyway - \"person dies of alcohol poisoning after vodka up the butt\" was a legend as a student but also a story of how \"x units alcohol\" is an oversimplification. There are similar stories with class As and people varying up the modes of ingestion... Coming back to food I have anecdotally observed that my lactose intolerance reaction is more severe if the lactose is in something creamy. Even lactase pills have reduced effect on that. Although cream may be where the lactose is concentrated for all I know reply viraptor 8 hours agoparentprevThe dose makes a huge difference. The amount you get in the daily multivitamin pills is minimal really if you have issues with processing those. reply xanderlewis 8 hours agorootparentI wonder how that compares to the amount in Marmite, which seems to be often recommended by doctors in the UK for those deficient in vitamin B (and you’re in the lucky 50% of the population that actually likes it). reply invalidname 8 hours agorootparentprevBut wouldn't patients see deficiency in blood tests and get extra supplements for those two? reply bacro 8 hours agorootparentFrom the experience I have from doctors in Portugal, they will ignore that. My brother was not even warned by the doctor that he was pre-diabetic after checking his blood results. reply viraptor 8 hours agorootparentprevB vitamin levels beyond B12 are rarely tested. You're unlikely to ever get it checked unless you experience specific neurological issues. Overtesting itself is dangerous, so... doctors don't have the incentive. reply lurking15 4 hours agorootparentB12 can be tricky to pin down. It can be falsely elevated by vitamins. You need cofactors, e.g. a B2 deficiency could falsely give the impression of repletion. This is ON TOP of the really loose grounding of accepted ranges for various vitamins. Many of the levels are established on the basis of avoiding severe disease like vitamin D: \"no rickets above this level\" and B12: \"no anemia above this level.\" Which is completely different than assessing optimal levels. reply voisin 1 hour agorootparent> This is ON TOP of the really loose grounding of accepted ranges for various vitamins. Many of the levels are established on the basis of avoiding severe disease like vitamin D: \"no rickets above this level\" and B12: \"no anemia above this level.\" Which is completely different than assessing optimal levels. This is concerning. Is there anyone keeping a database of optimal rather than minimal levels? reply kvgr 7 hours agorootparentprevDangerous in a sense: costs money right now, we would have to deal with it. My friend once said: “no one is healthy, your doctor just does not have the testing resolution to find the issues” and in 3 years you get parkinsons… reply viraptor 6 hours agorootparentThat's too cynical. In reality, you're going to die with a number of completely benign issues. The more issues you find and try to interfere, the more likely you are to have a negative impact. And that's before running into issues with testing failures from a large number of tests. We not only can't find all issues, we can find more other issues than are ever going to be an actual problem. But it's not really a money issue (not in a lot of cases anyway) - where it leads to better outcomes, doctors do aim for overdiagnosing to a certain % to cover cases which would be otherwise missed. reply sph 8 hours agoprevFrom the article: > \"Supplementation of riboflavin and/or biotin is likely to be beneficial in a subset of Parkinson's disease patients, in which gut dysbiosis plays pivotal roles,\" From https://en.wikipedia.org/wiki/Riboflavin > Best sources of riboflavin: Beef liver, Chicken liver, Whey protein powder, Almonds > People at risk of having low riboflavin levels include alcoholics, vegetarian athletes, and practitioners of veganism. From https://en.wikipedia.org/wiki/Biotin > Best sources of biotin: Chicken liver, Beef liver, Eggs, Peanuts, Sunflower seeds, Pork chop Offered without comment. reply arghwhat 8 hours agoparent>Best sources of riboflavin: ... Whey protein powder ... Not the gains I was expecting. reply OJFord 7 hours agoparentprevCould really do with an easy way of viewing - for all important nutrients - all the best sources that you can eat where something you can't eat is a significant source. So for example I enter that I can't eat beef liver (presumably via a helper group that I'm vegetarian or Hindu or no beef or something) and it tells me to keep up my chicken liver, whey protein powder, ... intake for riboflavin & biotin. But doing that across all vitamins etc. at once having entered all the things you don't eat - because it's too much to be on top of otherwise, imo, and I think it's pretty easy to think you eat a balanced and varied diet but actually not have any good source of one or two specific things at all. reply zukzuk 6 hours agoparentprevThe big catch is that vegan diets seem to be protective against the progression of Parkinson’s disease: https://pubmed.ncbi.nlm.nih.gov/11516224/ As is often the case, the truth is probably a lot more nuanced than “x causes y”. reply arghwhat 4 hours agorootparentAnd again, more nuanced than \"vegan diets\", taking just the abstract of the linked hypothesis: > Three recent case-control studies conclude that diets high in animal fat or cholesterol are associated with a substantial increase in risk for Parkinson's disease (PD); in contrast, fat of plant origin does not appear to increase risk. > In aggregate, these findings suggest that vegan diets may be notably protective with respect to PD. However, they offer no insight into whether saturated fat, compounds associated with animal fat, animal protein, or the integrated impact of the components of animal products mediates the risk associated with animal fat consumption. That is, the underlying cause could very well just be high cholesterol, and that the carnivores in the study were more likely to have high cholesterol than the vegan groups, either due to lifestyle choices (despite veganism itself not implying any sort of healthy lifestyle, there might be a stronger overlap with those choosing veganism and those leading a healthy lifestyle than for carnivores), or due to difference in dietary cholesterol intake. reply DaoVeles 7 hours agoparentprevFor all the Aussies/Kiwis. Should this mean Vegemite and Mightymite are good against this? reply marci 7 hours agorootparentIs Parkinson's rate lower in Australia/New Zealand? reply gadflyinyoureye 6 hours agorootparentFollow up question: do Aussies actual eat that stuff or was just a lie in a song? reply lmpdev 6 hours agorootparentNot a lie and yes we do reply voisin 1 hour agorootparentAny data about how much is consumed on what regularity, and whether there is a difference in incidence of PD? reply JohnnyLarue 6 hours agoparentprev>Offered without comment. Pretty sure that's a comment. reply reedf1 8 hours agoparentprevI should eat more chicken liver pâté... reply carolinehey530 4 hours agoprev [4 more] [flagged] daneel951 4 hours agoparent [–] Caution! Curious about this comment (having a family member with Parkinson's) I searched for treatment from naturalherbscentre.com and I found that this type of comment was copy-pasted elsewhere Here https://www.crossfit.com/230111 there are 3 of them! It is really bad to play with people's health and suggest to stop taking real medications. reply hollerith 4 hours agorootparentI'm flagging GP for being spam. The personal story is probably made up. reply screye 4 hours agorootparentprev [–] New account too. Good catch. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Researchers have identified specific gut microbes linked to decreased levels of riboflavin (vitamin B2) and biotin (vitamin B7) in Parkinson's patients, suggesting B vitamin supplementation as a potential treatment.",
      "The study, led by Hiroshi Nishiwaki from Nagoya University, found that changes in gut bacteria affected the production of short-chain fatty acids (SCFAs) and polyamines, crucial for a healthy intestinal mucus layer.",
      "The findings, published in npj Parkinson's Disease, indicate that maintaining a healthy gut microbiome and reducing environmental toxins could help protect against Parkinson's symptoms."
    ],
    "commentSummary": [
      "A study suggests a connection between gut bacteria and Parkinson's disease, indicating certain bacterial strains might contribute to the condition.",
      "Discussions have emerged about the potential benefits of supplementing with vitamins like riboflavin (B2) and biotin (B7) to alleviate symptoms.",
      "The study underscores the importance of gut health on neurological conditions but advises that more research is needed and current treatments should not be ignored."
    ],
    "points": 127,
    "commentCount": 48,
    "retryCount": 0,
    "time": 1719566635
  },
  {
    "id": 40819617,
    "title": "The Rhisotope Project: Insertion of radioisotopes into live rhinoceros",
    "originLink": "https://www.wits.ac.za/news/latest-news/general-news/2024/2024-06/a-novel-way-to-save-rhinos-.html",
    "originBody": "A Novel way to save Rhinos 25 June 2024 - Wits University The Rhisotope Project at Wits is entering a new testing phase with the insertion of radioisotopes into 20 live rhinoceros. After three years of meticulous and dedicated hard work, the Rhisotope Project at Wits University has successfully inserted low doses of radioisotopes into 20 live rhinoceros. In this final phase of the research project, Professor James Larkin from the University of the Witwatersrand’s Radiation and Health Physics Unit (RHPU) in collaboration with a team of experts who are leaders in the world of rhino conservation and veterinary work, will closely monitor the health and vital statistics of the rhinos over a period of six months, in order to determine the viability of this approach. . The Rhisotope Project’s intention is to use nuclear technology in the form of small, measured quantities of radioisotopes and to insert these into the horns of rhinoceros, which can be picked up by radiation detection portal monitors at international borders, including at harbours, airports and land-crossings. These radioisotopes will provide an affordable, safe and easily applicable method to create long-lasting and detectable horn markers that cause no harm to the animals and environment. At a later stage, the work will expand to elephants, pangolins and other fauna and flora. Being pioneered in the UNESCO Waterberg Biosphere Reserve, the Project aims to benefit from existing, sophisticated multi-billion-dollar nuclear security infrastructure that already exists throughout the world. Over 11 000 radiation detection portal monitors are installed at airports, harbours and other ports of entry, including thousands of trained personnel equipped with radiation detectors, all of which can detect the smallest radioactive particles. In contrast to this, the infrastructure and number of trained officials to detect wildlife trafficking at ports of entry internationally is extremely limited. “Every 20 hours in South Africa a rhino dies for its horn. These poached horns are then trafficked across the world and used for traditional medicines, or as status symbols. This has led to their horns currently being the most valuable false commodity in the black-market trade, with a higher value even than gold, platinum, diamonds and cocaine. Sadly, rhino horns play a large role in funding a wide variety of criminal activities globally,” says Professor James Larkin. “Ultimately, the aim is to try to devalue rhinoceros horn in the eyes of the end users, while at the same time making the horns easier to detect as they are being smuggled across borders.” Starting on Monday, 24 June 2004, Professor Larkin and his team carefully sedated the 20 rhinos and drilled a small hole into each of their horns to insert the non-toxic radioisotopes. The rhinos were then released under the care of a highly qualified crew that will monitor the animals on a 24-hour basis for the next six months. “Each insertion was closely monitored by expert veterinarians and extreme care was taken to prevent any harm to the animals,” says Larkin. “Over months of research and testing we have also ensured that the inserted radioisotopes hold no health or any other risk for the animals or those who care for them.” The development and application of the Rhisotope Project nuclear technology has the capacity to help deter poaching, increase the detection capabilities of smuggled horns, increase prosecution success, reveal smuggling routes and deter end-user markets. Rhino poaching reached crisis levels since 2008 where close to 10 000 rhinos were lost to poaching in South Africa, with wildlife trafficking being the third biggest organised crime globally. Professor Lynn Morris, the Deputy Vice-Chancellor: Research and Innovation at Wits University says: “This is an example of how cross-disciplinary research and innovation makes a real difference. This novel approach pioneered by Prof Larkin and his colleagues has the potential to eradicate the threat of extinction our unique wild-life species , especially in South Africa and on the continent. This is one of many projects at Wits that demonstrates research with impact, and which helps to address some of the local and global challenges of the 21st Century.” The Rhisotope Project at Wits was set up by a small team of likeminded individuals as a South African-based conservation initiative in January 2021 with the intention of becoming a global leader in harnessing nuclear technology to protect threatened and endangered species of fauna and flora as well as communities of people. Aside from developing a solution to combat the illicit trade and trafficking of wildlife products, the Rhisotope Project seeks to provide education and social upliftment to empower people and local communities. A special focus is aimed at uplifting the girls and women of rural communities, who are often the backbone of these communities in the remote areas where endangered species are found and are the greatest components of success in changing the hearts and minds of local communities thereby creating rhino ambassadors and champions.",
    "commentLink": "https://news.ycombinator.com/item?id=40819617",
    "commentBody": "The Rhisotope Project: Insertion of radioisotopes into live rhinoceros (wits.ac.za)116 points by geox 7 hours agohidepastfavorite105 comments jjk166 6 hours agoUnfortunately by the time this detects a piece of ivory, the damage is already done. If it took 3 years to inject 20 rhinos, it's unlikely that a large enough portion of the population could be injected to act as an effective deterrent for those actually killing the animals. Even for those actually involved in moving the ivory, it's only a deterrent if they actually face consequences at these borders - many states have weak enforcement of anti-ivory laws, and many more have bribable customs agents. Further, ivory has a value to weight ratio that is extremely conducive to smuggling. By comparison, infusion which puts a dye and an anti-consumption toxin in the horns to render the ivory worthless and thus prevent the animals from being killed in the first place is a well developed and inexpensive process that has proven effective.[0] I don't see how radioisotope injection is an improvement. [0] https://rhinorescueproject.org/how-it-works/ reply wanderingstan 2 hours agoparentThis is a proof of concept project. The “3 years” is not because of some inherent difficulty, it’s just a typical research project. From the article: > …will closely monitor the health and vital statistics of the rhinos over a period of six months, in order to determine the viability of this approach. If this approach is shown to be healthy, I’m sure it could be done much faster. reply DistantCl3ric 4 hours agoparentprevRhino horn is not ivory. Its keratin. Higher value to weight ratio than ivory. reply __MatrixMan__ 3 hours agorootparentGotta wonder if it would be better to instead focus on making it easier to source keratin from a bioreactor / cell culture. Crash the market with \"counterfeit\" rhino keratin which is as good as the real stuff anyhow. reply williamdclt 3 hours agorootparentI'd guess that most buyers of objects made from rhino horns are more interested in its rhino horn nature than its keratin composition. If keratin became suddenly abundant, I doubt it'd impact the rhino horn market very much I know nothing of that market though, could be entirely wrong reply michael1999 1 hour agorootparentBut good counterfeits would crash the market. It’s already run by crooks. They’d be happy to cheat. reply __MatrixMan__ 3 hours agorootparentprevI also know nothing of it, it's just a fun thought. In a previous life I really enjoyed faking parking permits and IDs and transcripts and stuff... it would be pretty fun to return to that but this time be one of the good guys: faking rhino horns for the sake of protecting actual rhinos. reply gus_massa 38 minutes agorootparentprevWool is mostly keratin, also hooves and nails. And feathers, turtle shell, but they are made or another type of keratin. To be honest, each the keratin of each animal is slightly different, with small changes of the composition, something like plastic that can be made softer or harder tweaking the compostition. Anyway, I guess it's not about the exact composition, but the \"magic\" part of the rhino horn. reply IncreasePosts 3 hours agorootparentprevThen rich Chinese dudes who can't get a boner will demand a video of their rhino being killed and the horn being taken. reply StormChaser_5 3 hours agorootparentSurely that sory of video would be easy to fake if needed these days? reply michael1999 1 hour agorootparentprevI think being able to produce reliable supply chain traceability is indistinguishable from keeping detailed notes on a criminal conspiracy. None of the players want that besides the consumer. Never happen. reply IncreasePosts 38 minutes agorootparentSure, but it isn't clear to me that China actually cares about stopping rhino poaching, and I imagine the major players in the conspiracy never set foot in South Africa, and just have local low level agents (ie the ones we are told are so poor, they need to poach the rhinos to survive) exposed to consequences. reply joot330 1 hour agorootparentprevBuyers aren’t interested in “keratin” - they are interested in “rhino horn”. This is about mystical benefits from phallic animal parts. You aren’t going to tackle the market by fueling it. reply pvaldes 3 hours agorootparentprevIf we could only built a 3d printer that would work with keratin... reply joot330 1 hour agoparentprevOne reason it’s an improvement is that radiation is detectable by already-existing methods used at the borders. So it will help catch them in the act of smuggling. reply jjk166 5 minutes agorootparentUnfortunately by the time this detects a piece of ivory, the damage is already done. If it took 3 years to inject 20 rhinos, it's unlikely that a large enough portion of the population could be injected to act as an effective deterrent for those actually killing the animals. Even for those actually involved in moving the ivory, it's only a deterrent if they actually face consequences at these borders - many states have weak enforcement of anti-ivory laws, and many more have bribable customs agents. Further, ivory has a value to weight ratio that is extremely conducive to smuggling. reply philipwhiuk 5 hours agoparentprevAlso.. Geiger counters are not expensive and they can just ignore the tiny fraction of injected rhinos. reply dotancohen 4 hours agorootparentThey'll kill the rhino, then test it. If it tests positive, they'll just leave the carcass to rot. reply jjk166 3 minutes agorootparentNo one is talking about testing live rhinos. Even in the best case scenario this is to catch poachers after the rhino is dead. reply alan-hn 5 hours agorootparentprevI'm not sure that it would be detectable with a Geiger counter. Other chemical analysis methods are used to detect isotopes like this reply jjk166 4 hours agorootparentAccording to the article, the radioisotope is specifically meant to be detected by the handheld radiation detectors typically used by customs agents. If it can't be detected by passive radiation detectors, there is even less argument for its efficacy. reply Sanzig 2 hours agorootparentThe flux follows the inverse square law, however, which means to detect if a rhino is tagged you'd have to get really close. Admittedly I don't know anything about the mechanics of rhino poaching, but considering how aggressive rhinos are I imagine poachers would prefer to shoot them from a decent range for their own safety. reply jjk166 5 minutes agorootparentNo one is talking about scanning live rhinos. Even in the best case scenario, this is to catch poachers long after the rhinos are dead. 4gotunameagain 4 hours agorootparentprevThey say that the plan is to use existing radiation detectors at ports. Another article [1] mentions that the isotope emits Gamma rays, one of the types of radiation that can be detected by a Geiger - Muller tube. [1] https://www.extremetech.com/science/scientists-turn-to-radio... reply Muromec 5 hours agoparentprevWell maybe radioactivity should trip existing detectors in airports and customs? I’m not sure those exist, but sounds like something you want to scan for on the borderline reply jjk166 5 hours agorootparentWhen I say smuggling here, I mean crossing a border illegally, avoiding customs. reply BurningFrog 5 hours agoparentprev> Unfortunately by the time this detects a piece of ivory, the damage is already done Reminds me of one of the worst \"abolish the Police\" arguments: By the time you put a murderer in jail, the victim is already dead. reply wanderingstan 6 hours agoprev> Over 11 000 radiation detection portal monitors are installed at airports, harbours and other ports of entry, including thousands of trained personnel equipped with radiation detectors, all of which can detect the smallest radioactive particles. I didn’t realize this. Injecting small, safe radioactive material into rhino horns seems like an incredibly good hack: turn all that nuclear monitoring equipment into poached animal artifact detectors. reply omgJustTest 6 hours agoparentClassic case of a societal problem that technology tries to paper over, and does a poor job doing so. Rhino horns are used for their keratin and \"traditional\" medicine ingredients. Radiation portal monitors will not detect all quantities and there are simple techniques for masking these detections with sheilding, or via nuisance alarms if they are detected. [1] Shark fin extraction, for shark fin soup, has a similar cultural problem. Influential people in the communities that consume these products, ie Yao Ming, could make a lot more progress by simply having public campaigns against it. [2] [1] Source: me, I am a radation detecion PhD who works on similar kinds of problems, with similarly or more capable systems. [2] https://www.youtube.com/watch?v=mJG7RaLX-DM reply orev 6 hours agorootparentAny deterrence scheme relies on awareness and uncertainty just as much, if not more than, an actual technology. If poachers think it will raise the chances of detection, even if that chance isn’t 100%, that’s just as good to deter them. reply jjk166 6 hours agorootparentOnly if the percentage of the population injected is high, which is unlikely given it took 3 years to inject 20 rhinos. If the percentage is small, the poachers could just get a cheap radiation detector and screen out whatever portion of their haul is contaminated before passing it along. Further, many smugglers are unconcerned with detection, they already likely bribe a customs agent to ignore it. reply 0cf8612b2e1e 4 hours agorootparentEven if the poachers can detect the contamination, they are probably going to kill the rhino, flee the area, and then assess the goods. So you still have a dead rhino unless the doping is well communicated and obvious to all. reply omgJustTest 6 hours agorootparentprevSure, the multiagent problem needs communication of the credible threat to the bad actors. \"Credible threat\" implies that bad actors don't collect information about the system, technical and non-technical, that implies the balance of risk and reward isn't tilted in favor of continuing their actions. reply manarth 6 hours agorootparentprev> Rhino horns are used for their \"traditional\" medicine ingredients The level of radiation is non-toxic to rhinos in their horns, but extremely toxic if ingested as a \"traditional\" medicine. The \"product\" is ruined. reply alwa 5 hours agorootparentDo the black-market consumers know that? Fentanyl and xylazine come to mind: not many drug users intend to get those, they can get many batches of what they do want before they get an adulterated batch, and they often don’t find out they got the adulterated kind until it’s too late. I’m also wondering where we’re getting the idea that it’s extremely toxic if ingested. Maybe it’s in the video? The article seems to suggest that it’s “non-toxic” and that > “the inserted radioisotopes hold no health or any other risk for the animals or those who care for them.” reply manarth 5 hours agorootparentAn additional reference: > 'The radioactive material would \"render the horn useless... essentially poisonous for human consumption\" added Nithaya Chetty, professor and dean of science at [the University of the Witwatersrand]' https://phys.org/news/2024-06-radioactive-rhino-horns-curb-p... reply manarth 5 hours agorootparentprev> \"The radioactive dose makes the horn poisonous for humans\" https://www.bbc.co.uk/newsround/articles/cd16yjp0062o reply ta988 4 hours agorootparentprevFentanyl is more complex, now users are more and more looking for it... (source: a friend working in harm reduction) reply omgJustTest 6 hours agorootparentprevNot fully ruined... since the injections are typically not loose contamination, for fear that it migrates into the rhino via capillary action or irradiates the rhino more than people are comfortable with. Smugglers would likely be able to use simple, cheap detectors to remove these. Potentially they don't remove the pils, simply grind the horn + source into medicine... the amount of radiation will not cause acute effects to the person who ingests it and will likely just cause some \"unexplainable\" tumor or system failure later in life or after a prolonged consumption. reply shiandow 5 hours agorootparentHonestly I'm a bit confused, the descriptions seem contradictory or require an extraordinary level of fine tuning. If you just insert a capsule of radioactive material, it's easy to make it (extremely) poisonous, easily detectable and harmless to the rhino. But I don't see a way to do all three at once. If you just wanted to kill the consumers a small amount of an alpha emitter would work quite well, but would be hard to detect and carry a small amount of risk to the rhino if it breaks confinement. Conversely you could use a strong gamma emitter to make it easily detectable but I don't see a way to do so that would harm the consumers but not the rhino. Best you can do is some level or radioactivity that we're fine with in animals but not in humans because we're hypocrites. reply omgJustTest 3 hours agorootparentI think that no one believes killing consumers of these products is a viable solution or that it doesn't have disastrous side effects. What happens with rhinos that die? do we have to collect their tusks and these are rad-waste now? Making them radioactive with non-penetrating particles might be ok from the standpoint of trying to make them less desirable... but you aren't making them detectable & it is highly controversial to do this. In reality, and back to the solutions proposed by the article: I don't know if the source article's idea has merit or is just funded by a non-profit where this is a small pet project or if they really don't understand the tradespace. reply richardw 3 hours agorootparentprevIf this has a chance of getting to 100% of the population, I think if I were the rhino I’d want to take those odds given the alternative is being hunted. And if it’s eg 50%, I’d very much want to be in the injected 50% because all poaching effort would be in the non treated population. Obviously assuming all animals in an area were injected and that was known to poachers. reply GTP 4 hours agorootparentprev> Best you can do is some level or radioactivity that we're fine with in animals but not in humans because we're hypocrites. I don't know if this is the case, but if the life expectancy of rhinos is much less than humans, then there would be radiation levels that are safe for the rhino but harmful for humans. Tumors can take time to develop. reply krisoft 4 hours agorootparent> I don't know if this is the case, but if the life expectancy of rhinos is much less than humans, Luckily this is easily obtainable information on the internet. 'Rhinoceroses’ lifespans vary on species. A rhino’s lifespan is typically 40 to 45 years.\" https://www.pbs.org/wnet/nature/blog/rhinoceros-fact-sheet/ That is less than a human lifespan but not by orders of magnitude smaller. Sounds like a very narrow needle to thread. reply noisy_boy 5 hours agorootparentprev> Radiation portal monitors will not detect all quantities and there are simple techniques for masking these detections with sheilding, or via nuisance alarms if they are detected. I think more than actual quantities involved, the scary risk of radiation, however unlikely, may be a bigger deterrent for the consumers of these products. The more widely this news spreads, the better it is for the Rhinos. reply sbergot 6 hours agorootparentprevA solution doesn't have to be perfect to be useful. reply omgJustTest 6 hours agorootparentSure, but paying for an anti-shark fin commercial might be as much as a single portal monitor. Having portal monitors that serve many purposes is good, and generally the biggest impact they have is in the deterrence effect, ie bad actors might be constrained by their existence. However in order for deterrence to be effective it must be a credible capability. Since there are so many smugglers, and they could reasonably implement simple countermeasures, it is likely the deterrence effect is small to nil in this case. reply llamaimperative 6 hours agorootparentIsn’t the whole point that the portal monitors are already bought, paid for, and in fact operated by other people? reply jjoonathan 6 hours agorootparentprevNobody cares about preachy commercials, except in that doing the opposite of what they preach makes you edgy and cool. A big celeb getting caught doing something gauche and getting dogpiled in the press? That is what effective deterrent looks like. reply bluGill 6 hours agorootparentPeople say that, but the data proves good commercials work. Of course not all commercials work, but there is plenty of data showing they do work and what works. reply todd8 6 hours agorootparentprevRhino horns are not made of ivory. reply omgJustTest 6 hours agorootparentYes... sorry I am thinking of some related topics in this area, not only just about rhinos. Elephant trade has a similar issue, and this is why my mind was in that space. reply ceroxylon 5 hours agorootparentprevSorry, but if you're going to cite yourself you should do better than \"radation detecion\". reply omgJustTest 3 hours agorootparentAt least we know I'm not an LLM! (Cant change the spelling now!) reply BenjiWiebe 3 hours agorootparentprevWell, he didn't claim the PhD was in spelling... reply tomp 6 hours agoparentprevThe statement is false, unfortunately. There's definite a positive (non-zero) threshold below which it doesn't trigger. Bananas are radioactive, and while a single banana doesn't trigger the alarms, a lot of bananas might! from https://en.wikipedia.org/wiki/Banana_equivalent_dose > Although the amount in a single banana is small in environmental and medical terms, the radioactivity from a truckload of bananas is capable of causing a false alarm when passed through a Radiation Portal Monitor used to detect possible smuggling of nuclear material at U.S. ports. reply gus_massa 23 minutes agorootparentBananas are not so radioactive, my guess is that they are using a stronger radioactive source. Perhaps hide the horn inside a truck full of bananas? I guess the horn will be still more radioactive. Also, each radioactive source produce radioactivity with different energy, so you can use specialized equipment to identify the source. (We used one in the lab in an undergraduate couse of Physics. It's not very big, like the size of a shoe.) reply netsharc 6 hours agorootparentprevHN headline in 3 years: airport sniffer dogs being trained to smell for bananas. Because that's what the horn-smugglers will be carrying in attempts to deflect why the airport scanner is detecting radiation in their luggage. reply ben_w 6 hours agorootparentIt's the potassium in the bananas, so potassium rich salt will also do this. Presumably many other common items are also sufficiently potassium rich. reply fullspectrumdev 6 hours agorootparentIf you leave a scintillating detector such as a Radiacode or similar on a bag of potassium chloride water softener salt for a while you can actually detect a very slight amount of radioactivity and generate a spectrum :) reply giarc 4 hours agoparentprevProbably is by the time they are found, the rhino is dead. They might catch the last guy holding the bag, but I suspect it's passed through a few different groups by the time it reaches the airport. So the poachers just go on poaching as they already got paid. reply wanderingstan 2 hours agorootparentYes, for the first few. But the idea of the test is that this could be done in a wider scale. If a significant portion of horns are being confiscated (via radiation sensors) then there’s fewer horns being sold and less money available for all those middleman groups. Over time the market goes away. Perhaps similar to how the market for stolen iPhones dried up once people could remotely brick their stole phone. There’s just less money to be made so thieves move on to more lucrative targets. reply sneela 5 hours agoprev> Every 20 hours in South Africa a rhino dies for its horn. I didn't know this statistic before - this is disheartening. reply aziaziazi 4 hours agoparentHere’s another one : between 24 and 150 animal species go extinct probably every day ! > current extinctions were ‘up to 100 times higher than the background rate.’ https://e360.yale.edu/features/global_extinction_rates_why_d... reply Sharlin 6 hours agoprevI wonder what radioisotopes they’re using. I assume it’s a gamma emitter because alpha and beta would be readily absorbed by the horn and any packaging material? reply rustcleaner 1 hour agoprevI wonder if anyone is trying to lab-grow ivory like they're trying with organs, considering ivory's price there might be margin there. reply rustcleaner 1 hour agoparentYes I know rhino != ivory. reply jonathanlydall 5 hours agoprevSpeaking as a South African, I hope this can make a difference, every little bit helps. Unless it can completely stop poaching (which on its own I think is unlikely) I don't think it will solve the fundamental issue that drives poaching, that there is a market willing to pay exorbitant fees. Ivory has zero physiological medicinal effects, but their rareness convinces certain kinds of (shithead) people that they \"must\". The rarer the material, the more \"special\" it becomes, driving up the price further and the higher the price, the more emboldened the poachers become. reply gwbas1c 4 hours agoprev> Starting on Monday, 24 June 2004, Professor Larkin and his team carefully sedated the 20 rhinos Is that a typo? I'd think after 20 years we'd know if the plan worked. reply karaterobot 5 hours agoprevI see that it doesn't bother anyone else that \"novel\" is capitalized for no reason in the title of the article (the actual article, not just the HN title). So be it, I'll see myself out. reply poulpy123 4 hours agoprevIsn't it detected by a cheap geiger counter ? in this case the horn will be just discarded after the rhino jas been killed reply CorrectHorseBat 6 hours agoprevThis is genius, but what stops the poachers from removing the material? reply madamelic 6 hours agoparentI would figure cost and time. Poachers are looking for a quick payday and having to do this would cost additional time & money that will drive them elsewhere. Sort of like the adage of the best security is being less of a target / more of an annoyance to rob than your neighbor. reply tgsovlerkhgsel 6 hours agoparentprevEdit: Here was nonsense. I should read the article properly. reply CorrectHorseBat 5 hours agorootparentWhere did you read that? They bore holes in the horns and insert the radiotopes in the horn, not in the whole rhino. reply tgsovlerkhgsel 4 hours agorootparentApologies, I let the title (\"injecting\") mislead me. I assume it's a multitude of factors: - poachers/smugglers would need to know about this before they get caught (doesn't save that rhino, but adds friction to smuggling and removes some smugglers/buyers - both the ones caught and the ones who quit because they're worried about it) - testing for it likely requires non-trivial equipment (again adds friction to the trade) - depending on how visible the hole is, accurately locating it inside the horn may not be trivial either - once located, removing it still devalues the horn reply sharpshadow 3 hours agoprevWhich „non-toxic radioisotopes“ and how much is a small hole? reply antiquark 5 hours agoprevSo they have to tranquilize the rhinos first. That in itself can kill the animal. I can't find a reference, but that reminds me of an old project to dye rhino horns pink. Sadly, a few rhinos didn't survive the process. reply is_true 5 hours agoprevThis looks like a side project for one of the fake meat companies. reply red1reaper 7 hours agoprevSo they are going to put small ammounts of radioactive stuff in the horns of live Rhinos so that they are easier to detect at border patrols? Have I understood it right? Seems a little extreme to be honest. reply Aachen 6 hours agoparentArticle says > These radioisotopes will provide an affordable, safe and easily applicable method And > After three years of meticulous and dedicated hard work [...] in collaboration with a team of experts who are leaders in the world of rhino conservation and veterinary work, will closely monitor the health and vital statistics of the rhinos [...] Not sure why you want to just dismiss all that reply DiggyJohnson 5 hours agorootparentIt seems like the two things you quoted contradict each other. 3 years and a team of experts in their field doesn't really sound affordable or effective unless it's scaled up quickly and it's efficacy actually demonstrated somehow. reply ylk 41 minutes agorootparentYou are confused because you interpret the meticulous and time-intensive nature of the research as potentially conflicting with the claims of affordability and ease of application. To clear up this confusion, let's break down the key points: 1. Affordability and Ease of Application: The article states that the radioisotopes are intended to be an affordable and easily applicable method for marking rhino horns. This means that the technology, once developed and refined, can be applied to rhinos without significant cost or complexity. The goal is to make it practical for widespread use. 2. Three Years of Research and Expert Collaboration: The three years of research and the involvement of a team of experts highlight the rigorous development and testing phase needed to ensure the method's safety and effectiveness. This initial investment of time and expertise is necessary to create a robust, scientifically validated solution. 3. No Contradiction: The extensive research and expert collaboration do not contradict the claims of affordability and ease of application. Rather, they ensure that the final product is safe, effective, and ready for widespread, practical use. The upfront investment in research is typical for developing innovative technologies and does not imply that the method itself will be costly or complex to implement once it is ready. 4. Scaling Up: The project's next phase involves closely monitoring the rhinos to demonstrate the method's efficacy and safety. If successful, this proof of concept will pave the way for scaling up the application to more rhinos and potentially other endangered species. The affordability and ease of application refer to the potential widespread deployment after the initial research phase, not the development phase. In summary, your confusion arises from conflating the initial research and development effort with the final application's intended affordability and practicality. The article suggests that after this rigorous development phase, the method will be straightforward and cost-effective to implement on a larger scale. Was wondering what ChatGPT's answer would be to this comment chain. (Also mostly checked that what it claims about the article‘s content is correct) reply ylk 3 hours agorootparentprevTip: It only „seems“ and „sounds“ that way. Your conclusion is quite obviously based on incomplete information. reply Sharlin 6 hours agoparentprev> This has led to their horns currently being the most valuable false commodity in the black-market trade, with a higher value even than gold, platinum, diamonds and cocaine. I’d say unorthodox measures to curb poaching and trade are not uncalled for. reply jstanley 6 hours agorootparentWhat does it mean for rhino horns to be worth more than gold? There are lots of ways to measure it. All the world's rhino horns put together are worth more than all the world's gold put together? All of Africa's rhino horns are worth more than all of Africa's gold? A single rhino horn is worth more than a single bar of gold? 1kg of gold? The same mass of gold? The same volume of gold? reply JoshGG 6 hours agorootparentIf you’re unfamiliar with how commodity gold is priced, it’s by weight. So this comparison is likely comparing price per unit of weight. reply jstanley 6 hours agorootparentSure, but that's not a fundamental thing, it's just a unit that's convenient to trade. It's like wondering whether gold is worth more than Microsoft by comparing an ounce of gold to a share of Microsoft. Or to an ounce of Microsoft share certificates! The only sensible way to make the comparison is by total market cap. And I seriously doubt that rhino horns are worth more than 15 trillion USD. reply ben_w 6 hours agorootparent> The only sensible way to make the comparison is by total market cap From the perspective of smugglers and their stuff, that's about the worst possible comparison. So much so that yours is literally the first time ever that I've heard anyone even suggest market cap for a smuggled substance; looking at the definition of that term, I don't think that term is, or even could be by analogy, meaningful in this context. It's always money per mass, £$€ per imperial or metric, never anything else. reply ryanjshaw 4 hours agorootparentprevYou seem to be down voted by level 0 thinkers. No doubt they will down vote me now too. You are completely correct. While the statement is factually correct regarding the cost of rhino horn, it is completely meaningless information. We may as well throw in the cost per gram of a heart transplant while we're at it. I will say that annual demand is probably the more meaningful figure to work with, and it would be even more useful to express that in terms of \"how many more years of this level of demand will lead to the extinction of rhinos.\" reply yreg 5 hours agorootparentprev> Sure, but that's not a fundamental thing, it's just a unit that's convenient to trade. How is price per unit of mass not fundamental? I refuse to believe that you are actually confused that one commodity costs more per gram than another one. reply gus_massa 6 hours agorootparentprevMy guess is by weight. (It also may be by volume, that I guess is very important to transport illegal things, but it's too abstrtact. [1]) Comparison by weight is useful to give an idea of the price. It's just like measuring distances in football fields. Don't worry too much about that. Also, a cargo ship full of gold [2] is probably very valuable, but a magical cargo ship full of rhino horns will probably collapse the market and be worthless. Here is a list of most expensive materials per weight https://brightside.me/articles/the-17-most-expensive-materia... it includes saffron and antimater that have a very short shelf life, a few radioactive things that are expensive only beacuse they are difficult to produce [3]. If I had to stock huge quantities, I'd be very conservative and store gold and platinium. [1] Gold has a huge value per volume ratio. Toilete paper not. [2] Assuming it doesn't sink. [3] Aluminium used to be very expensive, until someone invented a method to make metalic aluminium easily. reply ryankrage77 6 hours agorootparentprevIt would be atypical to trade a physical commodity for a share. This is the purpose of money. reply usuehfjfi 5 hours agorootparentprevYeah, not fundamental. When you go to the store you compare bananas by the size of the company that produced them? Or by the price per kg? reply j16sdiz 6 hours agorootparentprevAll of above. ~$400k/kg vs ~$75k/kg reply jstanley 6 hours agorootparentWell then it's not all of the above unless there's more than 375 million kilograms of rhino horns in the world, because all the gold is worth 15 trillion dollars. Unless that's the case, which seems unlikely, all the gold is still worth more than all the rhino horns. Also, gold's density is about 19 g/cm^3. I can't easily find a figure for rhino horns, but let's guess about the same as fingernails, horse's hooves, etc., which is about 1.25 g/cm^3. Gold is 15x denser, so at the prices you've shown, gold is still worth more than rhino horns per unit volume. reply a_c_s 5 hours agorootparent\"x is worth more than gold\" means the price per unit of weight of x is greater than that of gold. What are you trying to achieve by arguing that a common turn of phrase doesn't mean what everyone else thinks it means? reply usuehfjfi 5 hours agorootparentprevPrice per volume is not used because volume changes with altitude. This is the reason planes measure fuel by weight. And scientists measure gases by weight. Because weight is the best way to compare things. reply yreg 7 hours agoparentprevIf it's of little harm to the rhinos then it's rather clever. reply gus_massa 6 hours agoparentprevThe dose makes the poison. If the dose is low enough (like a x-ray for a person per year) it's not a problem. If the dose is high it's dangerous. https://xkcd.com/radiation/ Also the horn is made of compressed hair. https://www.savetherhino.org/our-work/protecting-rhinos/what... So the local radioactivity is not a problem, you must calculate the distance to the head. reply surfingdino 6 hours agoparentprevThey are rhinos. It's not like your sister is brining one home on a Sunday to meet the rest of the family. /s I assume the doses are small enough to be harmless. reply adolph 5 hours agorootparentTypically she starts the salt soak the day before because the recipe calls for an hour per pound and rhinos are really big. https://www.thepioneerwoman.com/food-cooking/recipes/a11882/... reply carrotcarrot 5 hours agoprev [–] Scientists need to stop mucking with nature. First the \"edible vaccines\" genetic modification, and now this? I'd rather live in a world where agencies don't have absolute control over us. Perfect enforcement doesn't need to exist. reply breezeTrowel 5 hours agoparentAre you a rhinoceros? reply swayvil 5 hours agoparentprev [–] Ditto. I trust neither their abilities nor their intentions. Keep your big plans well away from me. Thanks. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Rhisotope Project at Wits University has begun inserting low doses of radioisotopes into rhino horns to aid in detecting smuggled horns at international borders.",
      "This innovative approach, monitored over six months, aims to deter poaching by making rhino horns detectable and less valuable on the black market.",
      "The project leverages global nuclear security infrastructure and focuses on education and social upliftment, particularly empowering women and girls in rural communities."
    ],
    "commentSummary": [
      "The Rhisotope Project aims to deter rhino poaching by inserting radioisotopes into their horns, making them detectable at borders.",
      "Critics argue the method is ineffective due to the slow injection process and weak enforcement of anti-ivory laws, suggesting alternatives like dyeing, toxin infusion, or lab-grown keratin.",
      "The project is currently in the proof-of-concept phase, with ongoing research to assess its viability and safety."
    ],
    "points": 116,
    "commentCount": 105,
    "retryCount": 0,
    "time": 1719574534
  }
]
