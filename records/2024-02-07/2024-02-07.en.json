[
  {
    "id": 39281178,
    "title": "Comprehensive Guide to SQL for Data Scientists: 100 Queries and Examples",
    "originLink": "https://gvwilson.github.io/sql-tutorial/",
    "originBody": "SQL for the Weary ☆ what this is notes and working examples that instructors can use to perform a lesson do not expect novices with no prior SQL experience to be able to learn from them musical analogy this is the chord changes and melody we expect instructors to create an arrangement and/or improvise while delivering see Teaching Tech Together for background ☆ scope intended audience Rachel has a master’s degree in cell biology and now works in a research hospital doing cell assays. She learned a bit of R in an undergrad biostatistics course and has been through the Carpentries lesson on the Unix shell. Rachel is thinking about becoming a data scientist and would like to understand how data is stored and managed. Her work schedule is unpredictable and highly variable, so she needs to be able to learn a bit at a time. prerequisites basic Unix command line: cd, ls, * wildcard basic tabular data analysis: filtering rows, aggregating within groups learning outcomes Explain the difference between a database and a database manager. Write SQL to select, filter, sort, group, and aggregate data. Define tables and insert, update, and delete records. Describe different types of join and write queries that use them to combine data. Use windowing functions to operate on adjacent rows. Explain what transactions are and write queries that roll back when constraints are violated. Explain what triggers are and write SQL to create them. Manipulate JSON data using SQL. Interact with a database using Python directly, from a Jupyter notebook, and via an ORM. ☆ setup Download the latest release Unzip the file in a temporary directory to create: ./db/*.db: the SQLite databases used in the examples ./src/*.*: SQL queries, Python scripts, and other source code ./out/*.*: expected output for examples ☆ background concepts A database is a collection of data that can be searched and retrieved A database management system (DBMS) is a program that manages a particular kind of database Each DBMS stores data in its own way SQLite stores each database in a single file PostgreSQL spreads information across many files for higher performance DBMS can be a library embedded in other programs (SQLite) or a server (PostgreSQL) A relational database management system (RDBMS) stores data in tables and uses SQL for queries Unfortunately, every RDBMS has its own dialect of SQL There are also NoSQL databases like MongoDB that don’t use tables ☆ connect to database src/connect_penguins.sh sqlite3 data/penguins.db Not actually a query But we have to do it before we can do anything else 1: select constant src/select_1.sql select 1; out/select_1.out 1 select is a keyword Normally used to select data from table… …but if all we want is a constant value, we don’t need to specify one Semi-colon terminator is required 2: select all values from table src/select_star.sql select * from little_penguins; out/select_star.out Adelie|Dream|37.2|18.1|178|3900|MALE Adelie|Dream|37.6|19.3|181|3300|FEMALE Gentoo|Biscoe|50|15.3|220|5550|MALE Adelie|Torgersen|37.3|20.5|199|3775|MALE Adelie|Biscoe|39.6|17.7|186|3500|FEMALE Gentoo|Biscoe|47.7|15|216|4750|FEMALE Adelie|Dream|36.5|18|182|3150|FEMALE Gentoo|Biscoe|42|13.5|210|4150|FEMALE Adelie|Torgersen|42.1|19.1|195|4000|MALE Gentoo|Biscoe|54.3|15.7|231|5650|MALE An actual query Use * to mean “all columns” Use from tablename to specify table Output format is not particularly readable ☆ administrative commands src/admin_commands.sql .headers on .mode markdown select * from little_penguins; out/admin_commands.outspeciesislandbill_length_mmbill_depth_mmflipper_length_mmbody_mass_gsex|---------|-----------|----------------|---------------|-------------------|-------------|--------|AdelieDream37.218.11783900MALE| AdelieDream37.619.31813300FEMALE| GentooBiscoe5015.32205550MALE| AdelieTorgersen37.320.51993775MALE| AdelieBiscoe39.617.71863500FEMALE| GentooBiscoe47.7152164750FEMALE| AdelieDream36.5181823150FEMALE| GentooBiscoe4213.52104150FEMALE| AdelieTorgersen42.119.11954000MALE| GentooBiscoe54.315.72315650MALESQLite administrative commands start with . and aren’t part of the SQL standard PostgreSQL’s special commands start with \\ Use .help for a complete list 3: specify columns src/specify_columns.sql select species, island, sex from little_penguins; out/specify_columns.outspeciesislandsex|---------|-----------|--------|AdelieDreamMALE| AdelieDreamFEMALE| GentooBiscoeMALE| AdelieTorgersenMALE| AdelieBiscoeFEMALE| GentooBiscoeFEMALE| AdelieDreamFEMALE| GentooBiscoeFEMALE| AdelieTorgersenMALE| GentooBiscoeMALESpecify column names separated by commas In any order Duplicates allowed Line breaks allowed encouraged for readability 4: sort src/sort.sql select species, sex, island from little_penguins order by island asc, sex desc; out/sort.outspeciessexisland|---------|--------|-----------|GentooMALEBiscoe| GentooMALEBiscoe| AdelieFEMALEBiscoe| GentooFEMALEBiscoe| GentooFEMALEBiscoe| AdelieMALEDream| AdelieFEMALEDream| AdelieFEMALEDream| AdelieMALETorgersen| AdelieMALETorgersenorder by must follow from (which must follow select) asc is ascending, desc is descending Default is ascending, but please specify 5: limit output Full dataset has 344 rows src/limit.sql select species, sex, island from penguins order by species, sex, island limit 10; out/limit.outspeciessexisland|---------|--------|-----------|Adelie| Dream| Adelie| Torgersen| Adelie| Torgersen| Adelie| Torgersen| Adelie| Torgersen| Adelie| Torgersen| AdelieFEMALEBiscoe| AdelieFEMALEBiscoe| AdelieFEMALEBiscoe| AdelieFEMALEBiscoeComments start with -- and run to the end of the line limit N specifies maximum number of rows returned by query 6: page output src/page.sql select species, sex, island from penguins order by species, sex, island limit 10 offset 3; out/page.outspeciessexisland|---------|--------|-----------|Adelie| Torgersen| Adelie| Torgersen| Adelie| Torgersen| AdelieFEMALEBiscoe| AdelieFEMALEBiscoe| AdelieFEMALEBiscoe| AdelieFEMALEBiscoe| AdelieFEMALEBiscoe| AdelieFEMALEBiscoe| AdelieFEMALEBiscoeoffset N must follow limit Specifies number of rows to skip from the start of the selection So this query skips the first 3 and shows the next 10 7: remove duplicates src/distinct.sql select distinct species, sex, island from penguins; out/distinct.outspeciessexisland|-----------|--------|-----------|AdelieMALETorgersen| AdelieFEMALETorgersen| Adelie| Torgersen| AdelieFEMALEBiscoe| AdelieMALEBiscoe| AdelieFEMALEDream| AdelieMALEDream| Adelie| Dream| ChinstrapFEMALEDream| ChinstrapMALEDream| GentooFEMALEBiscoe| GentooMALEBiscoe| Gentoo| Biscoedistinct keyword must appear right after select SQL was supposed to read like English Shows distinct combinations Blanks in sex column show missing data We’ll talk about this in a bit 8: filter results src/filter.sql select distinct species, sex, island from penguins where island = 'Biscoe'; out/filter.outspeciessexisland|---------|--------|--------|AdelieFEMALEBiscoe| AdelieMALEBiscoe| GentooFEMALEBiscoe| GentooMALEBiscoe| Gentoo| Biscoewhere condition filters the rows produced by selection Condition is evaluated independently for each row Only rows that pass the test appear in results Use single quotes for 'text data' and double quotes for \"weird column names\" SQLite will accept double-quoted text data 9: filter with more complex conditions src/filter_and.sql select distinct species, sex, island from penguins where island = 'Biscoe' and sex != 'MALE'; out/filter_and.outspeciessexisland|---------|--------|--------|AdelieFEMALEBiscoe| GentooFEMALEBiscoeand: both sub-conditions must be true or: either or both part must be true Notice that the row for Gentoo penguins on Biscoe island with unknown (empty) sex didn’t pass the test We’ll talk about this in a bit 10: do calculations src/calculations.sql select flipper_length_mm / 10.0, body_mass_g / 1000.0 from penguins limit 3; out/calculations.outflipper_length_mm / 10.0body_mass_g / 1000.0|--------------------------|----------------------|18.13.75| 18.63.8| 19.53.25Can do the usual kinds of arithmetic on individual values Calculation done for each row independently Column name shows the calculation done 11: rename columns src/rename_columns.sql select flipper_length_mm / 10.0 as flipper_cm, body_mass_g / 1000.0 as weight_kg, island as where_found from penguins limit 3; out/rename_columns.outflipper_cmweight_kgwhere_found|------------|-----------|-------------|18.13.75Torgersen| 18.63.8Torgersen| 19.53.25TorgersenUse expression as name to rename Give result of calculation a meaningful name Can also rename columns without modifying ☆ check your understanding 12: calculate with missing values src/show_missing_values.sql select flipper_length_mm / 10.0 as flipper_cm, body_mass_g / 1000.0 as weight_kg, island as where_found from penguins limit 5; out/show_missing_values.outflipper_cmweight_kgwhere_found|------------|-----------|-------------|18.13.75Torgersen| 18.63.8Torgersen| 19.53.25Torgersen|| Torgersen| 19.33.45TorgersenSQL uses a special value null to representing missing data Not 0 or empty string, but “I don’t know” Flipper length and body weight not known for one of the first five penguins “I don’t know” divided by 10 or 1000 is “I don’t know” 13: null equality Repeated from above so it doesn’t count against our query limit src/filter.sql select distinct species, sex, island from penguins where island = 'Biscoe'; out/filter.outspeciessexisland|---------|--------|--------|AdelieFEMALEBiscoe| AdelieMALEBiscoe| GentooFEMALEBiscoe| GentooMALEBiscoe| Gentoo| BiscoeIf we ask for female penguins the row with the missing sex drops out src/null_equality.sql select distinct species, sex, island from penguins where island = 'Biscoe' and sex = 'FEMALE'; out/null_equality.outspeciessexisland|---------|--------|--------|AdelieFEMALEBiscoe| GentooFEMALEBiscoe14: null inequality But if we ask for penguins that aren’t female it drops out as well src/null_inequality.sql select distinct species, sex, island from penguins where island = 'Biscoe' and sex != 'FEMALE'; out/null_inequality.outspeciessexisland|---------|------|--------|AdelieMALEBiscoe| GentooMALEBiscoe15: ternary logic src/ternary_logic.sql select null = null; out/ternary_logic.outnull = null|-------------|| If we don’t know the left and right values, we don’t know if they’re equal or not So the result is null Get the same answer for null != null Ternary logic equalityX Y null X true false null Y false true null null null null null 16: handle null safely src/safe_null_equality.sql select species, sex, island from penguins where sex is null; out/safe_null_equality.outspeciessexisland|---------|-----|-----------|Adelie| Torgersen| Adelie| Torgersen| Adelie| Torgersen| Adelie| Torgersen| Adelie| Torgersen| Adelie| Dream| Gentoo| Biscoe| Gentoo| Biscoe| Gentoo| Biscoe| Gentoo| Biscoe| Gentoo| BiscoeUse is null and is not null to handle null safely Other parts of SQL handle nulls specially ☆ check your understanding 17: aggregate src/simple_sum.sql select sum(body_mass_g) as total_mass from penguins; out/simple_sum.outtotal_mass|------------|1437000Aggregation combines many values to produce one sum is an aggregation function Combines corresponding values from multiple rows 18: common aggregation functions src/common_aggregations.sql select max(bill_length_mm) as longest_bill, min(flipper_length_mm) as shortest_flipper, avg(bill_length_mm) / avg(bill_depth_mm) as weird_ratio from penguins; out/common_aggregations.outlongest_billshortest_flipperweird_ratio|--------------|------------------|------------------|59.61722.56087082530644This actually shouldn’t work: can’t calculate maximum or average if any values are null SQL does the useful thing instead of the right one 19: counting src/count_behavior.sql select count(*) as count_star, count(sex) as count_specific, count(distinct sex) as count_distinct from penguins; out/count_behavior.outcount_starcount_specificcount_distinct|------------|----------------|----------------|3443332count(*) counts rows count(column) counts non-null entries in column count(distinct column) counts distinct non-null entries 20: group src/simple_group.sql select avg(body_mass_g) as average_mass_g from penguins group by sex; out/simple_group.outaverage_mass_g|------------------|4005.55555555556| 3862.27272727273| 4545.68452380952Put rows in groups based on distinct combinations of values in columns specified with group by Then perform aggregation separately for each group But which is which? 21: behavior of unaggregated columns src/unaggregated_columns.sql select sex, avg(body_mass_g) as average_mass_g from penguins group by sex; out/unaggregated_columns.outsexaverage_mass_g|--------|------------------|| 4005.55555555556| FEMALE3862.27272727273| MALE4545.68452380952All rows in each group have the same value for sex, so no need to aggregate 22: arbitrary choice in aggregation src/arbitrary_in_aggregation.sql select sex, body_mass_g from penguins group by sex; out/arbitrary_in_aggregation.outsexbody_mass_g|--------|-------------||| FEMALE3800| MALE3750If we don’t specify how to aggregate a column, SQL can choose any arbitrary value from the group All penguins in each group have the same sex because we grouped by that, so we get the right answer The body mass values are in the data but unpredictable A common mistake 23: filter aggregated values src/filter_aggregation.sql select sex, avg(body_mass_g) as average_mass_g from penguins group by sex having average_mass_g > 4000.0; out/filter_aggregation.outsexaverage_mass_g|------|------------------|| 4005.55555555556| MALE4545.68452380952Using having condition instead of where condition for aggregates 24: readable output src/readable_aggregation.sql select sex, round(avg(body_mass_g), 1) as average_mass_g from penguins group by sex having average_mass_g > 4000.0; out/readable_aggregation.outsexaverage_mass_g|------|----------------|| 4005.6| MALE4545.7Use round(value, decimals) to round off a number 25: filter aggregate inputs src/filter_aggregate_inputs.sql select sex, round( avg(body_mass_g) filter (where body_mass_g( select avg(body_mass_g) from penguins ) limit 5; out/compare_individual_aggregate.outbody_mass_g|-------------|4675| 4250| 4400| 4500| 4650Get average body mass in subquery Compare each row against that Requires two scans of the data, but there’s no way to avoid that Null values aren’t included in the average or in the final results 45: compare individual values to aggregates within groups src/compare_within_groups.sql select penguins.species, penguins.body_mass_g, round(averaged.avg_mass_g, 1) as avg_mass_g from penguins join ( select species, avg(body_mass_g) as avg_mass_g from penguins group by species ) as averaged on penguins.species = averaged.species where penguins.body_mass_g > averaged.avg_mass_g limit 5; out/compare_within_groups.outspeciesbody_mass_gavg_mass_g|---------|-------------|------------|Adelie37503700.7| Adelie38003700.7| Adelie46753700.7| Adelie42503700.7| Adelie38003700.7Subquery runs first to create temporary table averaged with average mass per species Join that with penguins Filter to find penguins heavier than average within their species 46: common table expressions src/common_table_expressions.sql with grouped as ( select species, avg(body_mass_g) as avg_mass_g from penguins group by species ) select penguins.species, penguins.body_mass_g, round(grouped.avg_mass_g, 1) as avg_mass_g from penguins join grouped where penguins.body_mass_g > grouped.avg_mass_g limit 5; out/common_table_expressions.outspeciesbody_mass_gavg_mass_g|---------|-------------|------------|Adelie37503700.7| Adelie38003700.7| Adelie46753700.7| Adelie42503700.7| Adelie38003700.7Use common table expression (CTE) to make queries clearer Nested subqueries quickly become difficult to understand Database decides how to optimize ☆ explain query plan src/explain_query_plan.sql explain query plan select species, avg(body_mass_g) from penguins group by species; out/explain_query_plan.out QUERY PLAN |--SCAN penguins `--USE TEMP B-TREE FOR GROUP BY SQLite plans to scan every row of the table It will build a temporary B-tree data structure to group rows 47: enumerate rows Every table has a special column called rowid src/rowid.sql select rowid, species, island from penguins limit 5; out/rowid.outrowidspeciesisland|-------|---------|-----------|1AdelieTorgersen| 2AdelieTorgersen| 3AdelieTorgersen| 4AdelieTorgersen| 5AdelieTorgersenrowid is persistent within a session I.e., if we delete the first 5 rows we now have row IDs 6…N Do not rely on row ID In particular, do not use it as a key 48: if-else function src/if_else.sql with sized_penguins as ( select species, iif( body_mass_g '$.acquired' as single_arrow, details->>'$.acquired' as double_arrow from machine; out/json_field.outsingle_arrowdouble_arrow|--------------|--------------|\"2023-05-01\"2023-05-01| \"2021-07-15\"2021-07-15|| Single arrow -> returns JSON representation result Double arrow ->> returns SQL text, integer, real, or null Left side is column Right side is path expression Start with $ (meaning “root”) Fields separated by . 72: JSON array access src/json_array.sql select ident, json_array_length(log->'$') as length, log->'$[0]' as first from usage; out/json_array.outidentlengthfirst|-------|--------|--------------------------------------------------------------|14{\"machine\":\"Inphormex\",\"person\":[\"Gabrielle\",\"Dub\\u00e9\"]}| 25{\"machine\":\"Inphormex\",\"person\":[\"Marianne\",\"Richer\"]}| 32{\"machine\":\"sterilizer\",\"person\":[\"Josette\",\"Villeneuve\"]}| 41{\"machine\":\"sterilizer\",\"person\":[\"Maude\",\"Goulet\"]}| 52{\"machine\":\"AutoPlate 9000\",\"person\":[\"Brigitte\",\"Michaud\"]}| 61{\"machine\":\"sterilizer\",\"person\":[\"Marianne\",\"Richer\"]}| 73{\"machine\":\"WY401\",\"person\":[\"Maude\",\"Goulet\"]}| 81{\"machine\":\"AutoPlate 9000\"}| SQLite (and other database managers) has lots of JSON manipulation functions json_array_length gives number of elements in selected array subscripts start with 0 Characters outside 7-bit ASCII represented as Unicode escapes 73: unpack JSON array src/json_unpack.sql select ident, json_each.key as key, json_each.value as value from usage, json_each(usage.log) limit 10; out/json_unpack.outidentkeyvalue|-------|-----|--------------------------------------------------------------|10{\"machine\":\"Inphormex\",\"person\":[\"Gabrielle\",\"Dub\\u00e9\"]}| 11{\"machine\":\"Inphormex\",\"person\":[\"Gabrielle\",\"Dub\\u00e9\"]}| 12{\"machine\":\"WY401\",\"person\":[\"Gabrielle\",\"Dub\\u00e9\"]}| 13{\"machine\":\"Inphormex\",\"person\":[\"Gabrielle\",\"Dub\\u00e9\"]}| 20{\"machine\":\"Inphormex\",\"person\":[\"Marianne\",\"Richer\"]}| 21{\"machine\":\"AutoPlate 9000\",\"person\":[\"Marianne\",\"Richer\"]}| 22{\"machine\":\"sterilizer\",\"person\":[\"Marianne\",\"Richer\"]}| 23{\"machine\":\"AutoPlate 9000\",\"person\":[\"Monique\",\"Marcotte\"]}| 24{\"machine\":\"sterilizer\",\"person\":[\"Marianne\",\"Richer\"]}| 30{\"machine\":\"sterilizer\",\"person\":[\"Josette\",\"Villeneuve\"]}json_each is another table-valued function Use json_each.name to get properties of unpacked array 74: last element of array src/json_array_last.sql select ident, log->'$[#-1].machine' as final from usage limit 5; out/json_array_last.outidentfinal|-------|--------------|1\"Inphormex\"| 2\"sterilizer\"| 3\"Inphormex\"| 4\"sterilizer\"| 5\"sterilizer\"75: modify JSON src/json_modify.sql select ident, name, json_set(details, '$.sold', json_quote('2024-01-25')) as updated from machine; out/json_modify.outidentnameupdated|-------|----------------|--------------------------------------------------------------|1WY401{\"acquired\":\"2023-05-01\",\"sold\":\"2024-01-25\"}| 2Inphormex{\"acquired\":\"2021-07-15\",\"refurbished\":\"2023-10-22\",\"sold\":\"|| 2024-01-25\"}|3AutoPlate 9000{\"note\":\"needs software update\",\"sold\":\"2024-01-25\"}Updates the in-memory copy of the JSON, not the database record Please use json_quote rather than trying to format JSON with string operations ☆ refresh penguins src/count_penguins.sql select species, count(*) as num from penguins group by species; out/count_penguins.outspeciesnum|-----------|-----|Adelie152| Chinstrap68| Gentoo124We will restore full database after each example 76: tombstones src/make_active.sql alter table penguins add active integer not null default 1; update penguins set active = iif(species = 'Adelie', 0, 1); src/active_penguins.sql select species, count(*) as num from penguins where active group by species; out/active_penguins.outspeciesnum|-----------|-----|Chinstrap68| Gentoo124Use a tombstone to mark (in)active records Every query must now include it 77: views src/views.sql create view if not exists active_penguins ( species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex ) as select species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex from penguins where active; select species, count(*) as num from active_penguins group by species; out/views.outspeciesnum|-----------|-----|Chinstrap68| Gentoo124A view is a saved query that other queries can invoke View is re-run each time it’s used Like a CTE, but: Can be shared between queries Views came first Some databases offer materialized views Update-on-demand temporary tables ☆ hours reminder src/all_jobs.sql create table job ( name text not null, billable real not null ); insert into job values ('calibrate', 1.5), ('clean', 0.5); select * from job; out/all_jobs.outnamebillable|-----------|----------|calibrate1.5| clean0.578: add check src/all_jobs_check.sql create table job ( name text not null, billable real not null, check (billable > 0.0) ); insert into job values ('calibrate', 1.5); insert into job values ('reset', -0.5); select * from job; out/all_jobs_check.out Runtime error near line 9: CHECK constraint failed: billable > 0.0 (19)namebillable|-----------|----------|calibrate1.5check adds constraint to table Must produce a Boolean result Run each time values added or modified But changes made before the error have taken effect ☆ ACID Atomic: change cannot be broken down into smaller ones (i.e., all or nothing) Consistent: database goes from one consistent state to another Isolated: looks like changes happened one after another Durable: if change takes place, it’s still there after a restart 79: transactions src/transaction.sql create table job ( name text not null, billable real not null, check (billable > 0.0) ); insert into job values ('calibrate', 1.5); begin transaction; insert into job values ('clean', 0.5); rollback; select * from job; out/transaction.outnamebillable|-----------|----------|calibrate1.5Statements outside transaction execute and are committed immediately Statement(s) inside transaction don’t take effect until: end transaction (success) rollback (undo) Can have any number of statements inside a transaction But cannot nest transactions in SQLite Other databases support this 80: rollback in constraint src/rollback_constraint.sql create table job ( name text not null, billable real not null, check (billable > 0.0) on conflict rollback ); insert into job values ('calibrate', 1.5); insert into job values ('clean', 0.5), ('reset', -0.5); select * from job; out/rollback_constraint.out Runtime error near line 11: CHECK constraint failed: billable > 0.0 (19)namebillable|-----------|----------|calibrate1.5All of second insert rolled back as soon as error occurred But first insert took effect 81: rollback in statement src/rollback_statement.sql create table job ( name text not null, billable real not null, check (billable > 0.0) ); insert or rollback into job values ('calibrate', 1.5); insert or rollback into job values ('clean', 0.5), ('reset', -0.5); select * from job; out/rollback_statement.out Runtime error near line 11: CHECK constraint failed: billable > 0.0 (19)namebillable|-----------|----------|calibrate1.5Constraint is in table definition Action is in statement 82: upsert src/upsert.sql create table jobs_done ( person text unique, num integer default 0 ); insert into jobs_done values(\"zia\", 1); .print \"after first\" select * from jobs_done; .print insert into jobs_done values(\"zia\", 1); .print \"after failed\" select * from jobs_done; insert into jobs_done values(\"zia\", 1) on conflict(person) do update set num = num + 1; .print \"after upsert\" select * from jobs_done; out/upsert.out after firstpersonnum|--------|-----|zia1Runtime error near line 14: UNIQUE constraint failed: jobs_done.person (19) after failedpersonnum|--------|-----|zia1after upsertpersonnum|--------|-----|zia2upsert stands for “update or insert” Create if record doesn’t exist Update if it does Not standard SQL but widely implemented Example also shows use of SQLite .print command ☆ normalization First normal form (1NF): every field of every record contains one indivisible value. Second normal form (2NF) and third normal form (3NF): every value in a record that isn’t a key depends solely on the key, not on other values. Denormalization: explicitly store values that could be calculated on the fly To simplify queries and/or make processing faster 83: create trigger A trigger automatically runs before or after a specified operation Can have side effects (e.g., update some other table) And/or implement checks (e.g., make sure other records exist) Add processing overhead… …but data is either cheap or correct, never both Inside trigger, refer to old and new versions of record as old.column and new.column src/trigger_setup.sql -- Track hours of lab work. create table job ( person text not null, reported real not null check (reported >= 0.0) ); -- Explicitly store per-person total rather than using sum(). create table total ( person text unique not null, hours real ); -- Initialize totals. insert into total values (\"gene\", 0.0), (\"august\", 0.0); -- Define a trigger. create trigger total_trigger before insert on job begin -- Check that the person exists. select case when not exists (select 1 from total where person = new.person) then raise(rollback, 'Unknown person ') end; -- Update their total hours (or fail if non-negative constraint violated). update total set hours = hours + new.reported where total.person = new.person; end; src/trigger_successful.sql insert into job values ('gene', 1.5), ('august', 0.5), ('gene', 1.0) ; out/trigger_successful.outpersonreported|--------|----------|gene1.5| august0.5| gene1.0| personhours|--------|-------|gene2.5| august0.5081: trigger firing src/trigger_firing.sql insert into job values ('gene', 1.0), ('august', -1.0) ; out/trigger_firing.out Runtime error near line 6: CHECK constraint failed: reported >= 0.0 (19)personhours|--------|-------|gene0.0| august0.0☆ represent graphs src/lineage_setup.sql create table lineage ( parent text not null, child text not null ); insert into lineage values ('Arturo', 'Clemente'), ('Darío', 'Clemente'), ('Clemente', 'Homero'), ('Clemente', 'Ivonne'), ('Ivonne', 'Lourdes'), ('Soledad', 'Lourdes'), ('Lourdes', 'Santiago'); src/represent_graph.sql select * from lineage; out/represent_graph.outparentchild|----------|----------|ArturoClemente| DaríoClemente| ClementeHomero| ClementeIvonne| IvonneLourdes| SoledadLourdes| LourdesSantiago84: recursive query src/recursive_lineage.sql with recursive descendent as ( select 'Clemente' as person, 0 as generations union all select lineage.child as person, descendent.generations + 1 as generations from descendent join lineage on descendent.person = lineage.parent ) select person, generations from descendent; out/recursive_lineage.outpersongenerations|----------|-------------|Clemente0| Homero1| Ivonne1| Lourdes2| Santiago3Use a recursive CTE to create a temporary table (descendent) Base case seeds this table Recursive case relies on value(s) already in that table and external table(s) union all to combine rows Can use union but that has lower performance (must check uniqueness each time) Stops when the recursive case yields an empty row set (nothing new to add) Then select the desired values from the CTE ☆ contact tracing database src/contact_person.sql select * from person; out/contact_person.outidentname|-------|-----------------------|1Juana Baeza| 2Agustín Rodríquez| 3Ariadna Caraballo| 4Micaela Laboy| 5Verónica Altamirano| 6Reina Rivero| 7Elias Merino| 8Minerva Guerrero| 9Mauro Balderas| 10Pilar Alarcón| 11Daniela Menéndez| 12Marco Antonio Barrera| 13Cristal Soliz| 14Bernardo Narváez| 15Óscar Barriossrc/contact_contacts.sql select * from contact; out/contact_contacts.outleftright|-------------------|-----------------------|Agustín RodríquezAriadna Caraballo| Agustín RodríquezVerónica Altamirano| Juana BaezaVerónica Altamirano| Juana BaezaMicaela Laboy| Pilar AlarcónReina Rivero| Cristal SolizMarco Antonio Barrera| Cristal SolizDaniela Menéndez| Daniela MenéndezMarco Antonio Barrera85: bidirectional contacts src/bidirectional.sql create temporary table bi_contact ( left text, right text ); insert into bi_contact select left, right from contact union all select right, left from contact ; out/bidirectional.outoriginal_count|----------------|8| num_contact|-------------|16Create a temporary table rather than using a long chain of CTEs Only lasts as long as the session (not saved to disk) Duplicate information rather than writing more complicated query 86: update group identifiers src/update_group_ids.sql select left.name as left_name, left.ident as left_ident, right.name as right_name, right.ident as right_ident, min(left.ident, right.ident) as new_ident from (person as left join bi_contact on left.name = bi_contact.left) join person as right on bi_contact.right = right.name; out/update_group_ids.outleft_nameleft_identright_nameright_ident_ident|-----------------------|------------|-----------------------|-------------|-----------|Juana Baeza1Micaela Laboy41| Juana Baeza1Verónica Altamirano51| Agustín Rodríquez2Ariadna Caraballo32| Agustín Rodríquez2Verónica Altamirano52| Ariadna Caraballo3Agustín Rodríquez22| Micaela Laboy4Juana Baeza11| Verónica Altamirano5Agustín Rodríquez22| Verónica Altamirano5Juana Baeza11| Reina Rivero6Pilar Alarcón106| Pilar Alarcón10Reina Rivero66| Daniela Menéndez11Cristal Soliz1311| Daniela Menéndez11Marco Antonio Barrera1211| Marco Antonio Barrera12Cristal Soliz1312| Marco Antonio Barrera12Daniela Menéndez1111| Cristal Soliz13Daniela Menéndez1111| Cristal Soliz13Marco Antonio Barrera1212_ident is minimum of own identifier and identifiers one step away Doesn’t keep people with no contacts 87: recursive labeling src/recursive_labeling.sql with recursive labeled as ( select person.name as name,person.ident as label from person union -- not 'union all' select person.name as name,labeled.label as label from (person join bi_contact on person.name = bi_contact.left)join labeled on bi_contact.right = labeled.name where labeled.label0)); insert into example values (10); insert into example values (-1); insert into example values (20); \"\"\" connection = sqlite3.connect(\":memory:\") cursor = connection.cursor() try: cursor.executescript(SETUP) except sqlite3.Error as exc: print(f\"SQLite exception: {exc}\") print(\"after execution\", cursor.execute(\"select * from example;\").fetchall()) out/exceptions.out SQLite exception: CHECK constraint failed: num > 0 after execution [(10,)] 94: Python in SQLite src/embedded_python.py import sqlite3 SETUP = \"\"\"\\ create table example(num integer); insert into example values (-10), (10), (20), (30); \"\"\" def clip(value): if value20: return 20 return value connection = sqlite3.connect(\":memory:\") connection.create_function(\"clip\", 1, clip) cursor = connection.cursor() cursor.executescript(SETUP) for row in cursor.execute(\"select num, clip(num) from example;\").fetchall(): print(row) out/embedded_python.out (-10, 0) (10, 10) (20, 20) (30, 20) SQLite calls back into Python to execute the function Other databases can run Python (and other languages) in the database server process Be careful 95: handle dates and times src/dates_times.py from datetime import date import sqlite3 # Convert date to ISO-formatted string when writing to database def _adapt_date_iso(val): return val.isoformat() sqlite3.register_adapter(date, _adapt_date_iso) # Convert ISO-formatted string to date when reading from database def _convert_date(val): return date.fromisoformat(val.decode()) sqlite3.register_converter(\"date\", _convert_date) SETUP = \"\"\"\\ create table events( happened date not null, description text not null ); \"\"\" connection = sqlite3.connect(\":memory:\", detect_types=sqlite3.PARSE_DECLTYPES) cursor = connection.cursor() cursor.execute(SETUP) cursor.executemany( \"insert into events values (?, ?);\", [(date(2024, 1, 10), \"started tutorial\"), (date(2024, 1, 29), \"finished tutorial\")], ) for row in cursor.execute(\"select * from events;\").fetchall(): print(row) out/dates_times.out (datetime.date(2024, 1, 10), 'started tutorial') (datetime.date(2024, 1, 29), 'finished tutorial') sqlite3.PARSE_DECLTYPES tells sqlite3 library to use converts based on declared column types Adapt on the way in, convert on the way out 96: SQL in Jupyter notebooks src/install_jupysql.sh pip install jupysql And then inside the notebook: src/load_ext.text %load_ext sql Loads extension src/jupyter_connect.text %sql sqlite:///data/penguins.db out/jupyter_connect.out Connecting to 'sqlite:///data/penguins.db' Connects to database sqlite:// with two slashes is the protocol /data/penguins.db (one leading slash) is a local path Single percent sign %sql introduces one-line command Use double percent sign %%sql to indicate that the rest of the cell is SQL src/jupyter_select.text %%sql select species, count(*) as num from penguins group by species; out/jupyter_select.out Running query in 'sqlite:///data/penguins.db' species num Adelie 152 Chinstrap 68 Gentoo 124 97: Pandas and SQL src/install_pandas.sh pip install pandas src/select_pandas.py import pandas as pd import sqlite3 connection = sqlite3.connect(\"db/penguins.db\") query = \"select species, count(*) as num from penguins group by species;\" df = pd.read_sql(query, connection) print(df) out/select_pandas.out species num 0 Adelie 152 1 Chinstrap 68 2 Gentoo 124 Be careful about datatype conversion 98: Polars and SQL src/install_polars.sh pip install polars pyarrow adbc-driver-sqlite src/select_polars.py import polars as pl query = \"select species, count(*) as num from penguins group by species;\" uri = \"sqlite:///db/penguins.db\" df = pl.read_database_uri(query, uri, engine=\"adbc\") print(df) out/select_polars.out shape: (3, 2) ┌───────────┬─────┐ │ species ┆ num │ │ --- ┆ --- │ │ str ┆ i64 │ ╞═══════════╪═════╡ │ Adelie ┆ 152 │ │ Chinstrap ┆ 68 │ │ Gentoo ┆ 124 │ └───────────┴─────┘ The Uniform Resource Identifier (URI) specifies the database The query is the query Use the ADBC engine instead of the default ConnectorX 99: object-relational mapper src/orm.py from sqlmodel import Field, Session, SQLModel, create_engine, select class Department(SQLModel, table=True): ident: str = Field(default=None, primary_key=True) name: str building: str engine = create_engine(\"sqlite:///db/assays.db\") with Session(engine) as session: statement = select(Department) for result in session.exec(statement).all(): print(result) out/orm.out building='Chesson' name='Genetics' ident='gen' building='Fashet Extension' name='Histology' ident='hist' building='Chesson' name='Molecular Biology' ident='mb' building='TGVH' name='Endocrinology' ident='end' An object-relational mapper (ORM) translates table columns to object properties and vice versa SQLModel relies on Python type hints 100: relations with ORM src/orm_relation.py class Staff(SQLModel, table=True): ident: str = Field(default=None, primary_key=True) personal: str family: str dept: Optional[str] = Field(default=None, foreign_key=\"department.ident\") age: int engine = create_engine(\"sqlite:///db/assays.db\") SQLModel.metadata.create_all(engine) with Session(engine) as session: statement = select(Department, Staff).where(Staff.dept == Department.ident) for dept, staff in session.exec(statement): print(f\"{dept.name}: {staff.personal} {staff.family}\") out/orm_relation.out Histology: Divit Dhaliwal Molecular Biology: Indrans Sridhar Molecular Biology: Pranay Khanna Histology: Vedika Rout Genetics: Abram Chokshi Histology: Romil Kapoor Molecular Biology: Ishaan Ramaswamy Genetics: Nitya Lal Make foreign keys explicit in class definitions SQLModel automatically does the join The two staff with no department aren’t included in the result ☆ Appendices ☆ Terms 1-to-1 relation A relationship between two tables in which each record from the first table matches exactly one record from the second and vice versa. 1-to-many relation A relationship between two tables in which each record from the first table matches zero or more records from the second, but each record from the second table matches exactly one record from the first. aggregation Combining several values to produce one. aggregation function A function used to produce one value from many, such as maximum or addition. alias An alternate name used temporarily for a table or column. atomic An operation that cannot be broken into smaller operations. autoincrement Automatically add one to a value. base case A starting point for recursion that does not depend on previous recursive calculations. Binary Large Object (blob) Bytes that are handled as-is rather than being interpreted as numbers, text, or other data types. cross join A join that creates the cross-product of rows from two tables. common table expression (CTE) A temporary table created at the start of a query, usually to simplify writing the query. consistent A state in which all constraints are satisfied, e.g., all columns contain allowed values and all foreign keys refer to primary keys. correlated subquery A subquery that depends on a value or values from the enclosing query, and which must therefore be executed once for each of those values. cursor A reference to the current location in the results of an ongoing query. data migration To move data from one form to another, e.g., from one set of tables to a new set or from one DBMS to another. database A collection of data that can be searched and retrieved. database management system (DBMS) A program that manages a particular kind of database. denormalization To deliberately introduce duplication or other violate normal forms, typically to improve query performance. durable Guaranteed to survive shutdown and restart. entity-relationship diagram A graphical depiction of the relationships between tables in a database. filter To select records based on whether they pass some Boolean test. foreign key A value in one table that identifies a primary key in another table. full outer join See cross join. group A set of records that share a common property, such as having the same value in a particular column. in-memory database A database that is stored in memory rather than on disk. index An auxiliary data structure that enables faster access to records. infinite recursion See “infinite recursion”. isolated The appearance of having executed in an otherwise-idle system. join To combine records from two tables. join condition The criteria used to decide which rows from each table in a join are combined. join table A table that exists solely to enable information from two tables to be connected. left outer join A join that is guaranteed to keep all rows from the first (left) table. Columns from the right table are filled with actual values if available or with null otherwise. many-to-many relation A relationship between two tables in which each record from the first table may match zero or more records from the second and vice versa. materialized view A view that is stored on disk and updated on demand. normal form One of several (loosely defined) rules for organizing data in tables. NoSQL database Any database that doesn’t use the relational model. null A special value representing “not known”. object-relational mapper (ORM) A library that translates objects in a program into database queries and the results of those queries back into objects. path expression An expression identifying an element or a set of elements in a JSON structure. primary key A value or values in a database table that uniquely identifies each record in that table. query A command to perform some operation in a database (typically data retrieval). recursive CTE A common table expression that refers to itself. Every recursive CTE must have a base case and a recursive case. recursive case The second or subsequent step in self-referential accumulation of data. relational database management system (RDBMS) A database management system that stores data in tables with columns and rows. subquery A query used within another query. table-valued function A function that returns multiple values rather than a single value. temporary table A table that is explicitly constructed in memory outside any particular query. ternary logic A logic based on three values: true, false, and “don’t know” (represented as null). tombstone A marker value added to a record to show that it is no longer active. Tombstones are used as an alternative to deleting data. trigger An action that runs automatically when something happens in a database, typically insertion or deletion. upsert To update a record if it exists or insert (create) a new record if it doesn’t. Uniform Resource Identifier (URI) A string that identifies a resource (such as a web page or database) and the protocol used to access it. view A rearrangement of data in a database that is regenerated on demand. window function A function that combines data from adjacent rows in a database query’s result. ☆ Acknowledgments This tutorial would not have been possible without: Andi Albrecht’s sqlparse module Dimitri Fontaine’s The Art of PostgreSQL David Rozenshtein’s The Essence of SQL (now sadly out of print) I would also like to thank the following for spotting issues, making suggestions, or submitting changes: Sam Hames Robert Kern Roy Pardee Manos Pitsidianakis Daniel Possenriede Adam Rosien Thomas Sandmann Simon Willison",
    "commentLink": "https://news.ycombinator.com/item?id=39281178",
    "commentBody": "SQL for data scientists in 100 queries (gvwilson.github.io)581 points by Anon84 12 hours agohidepastfavorite111 comments PheonixPharts 10 hours agoNot to detract from the article, but: Wow the meaning of the term \"data scientist\" has changed since the days of \"sexiest job\". From the article description: - Rachel has a master’s degree in cell biology and now works in a research hospital doing cell assays. - She learned a bit of R in an undergrad biostatistics course and has been through the Carpentries lesson on the Unix shell. - Rachel is thinking about becoming a data scientist and would like to understand how data is stored and managed. Data Scientists, back in the day, were largely people with both a fairly strong quantitative background and a strong software engineering background. The kind of people who could build a demo LSTM in an afternoon. Usually there was a bit of a trade-off between the quant/software aspects (really mathly people might be worse coders, really strong coders might need to freshen up on a few areas of mathematics), but generally they were fairly strong in each area. In many orgs it's been reduced to \"over paid data analysts\" but I wouldn't even hire \"Rachel\" for a role like that. reply vishnugupta 4 minutes agoparent> people with both a fairly strong quantitative background and a strong software engineering background. In my experience this intersection is a null set. And not just that it's an extremely rare feat to pull off IMO, the mental bandwidth and time needed to be good at one of those two alone would consume one person fully. This is why quant/stat specialists were paired with ETL/data-pipeline specialists to build end to end solution. One reason Data Science became such a hot role back in the day was that it was amorphously defined; because no one knew what exactly it entailed folks across a broad range of skill sets (stats, data engineers, NoSQL folks, visualisation and so on) jumped into the fray. But now companies have burnt their hands, they have learnt to call out exactly what's needed; even when they advertise for DS role they specify what's required of them. For example, this page on Coursera[1] is clear about emphasis on Quant, which is a welcome development IMO. [1] https://www.coursera.org/articles/what-is-a-data-scientist reply blagie 9 hours agoparentprevNo, it hasn't. The term has always gone in a half-dozen directions at once, and ranged anything from * an idiot making PPT decks for business presentations based on sales data; to * a statistician with very sophisticated mathematical background but minimal programming skills doing things in R or State; to * a person with a random degree making random dashboard in Tableau; to * a person with sophisticate background in software engineering, data engineering, and related fields who can kind of do math * an expert in machine learning (of various calibers) * a physicist using their quantitative skills to munge data ... and so on. That's been confusing people since the title came out. It depends on the industry, and there's a dozen overlapping titles too, some with well-defined meanings and some varying from company to company (business analyst, data engineering, etc.). reply 7thaccount 9 hours agorootparentThis is so true. Outside of maybe FAANG companies, a lot of places have wildly different expectations for that role. While one company may refer to the guy doing simple PPTs as a business analyst, others might call that a data analyst or a data scientist or something else. The pay probably mostly reflects the truth though outside of exceptions from office politics. reply poulpy123 5 minutes agoparentprevas the name implies a data scientist is a scientist that works on data. There is no reference to the need to be able to code a LSTM in one afternoon (and it would be absurd for most DS tasks) reply antman 3 hours agoparentprevData scientists with a strong software engineering background , where are they hiding? Jokes apart there used to be two categories of data scientists, those that came from a science/phd background where they duct taped their mathematical understanding to code which might work in production, and those those that come from a CS background that duct taped their mathematical/medium tutorial knowledge to an extravaganza of grid search and micro-services that made unscientific predictions in a scalable way. So now we have the ml engineer (engineer) and the data scientist (science) with clear roles and expectations. Both are full time jobs, most people cannot to both. reply screye 7 hours agoparentprevThe term sharded into multiple different terms Strong coder who can implement an LSTM = ML Engineer Decent coder who can implement a recent paper with scaffolding code = Applied Scientist Acceptable coder who is good enough at math to innovate and publish = Research Scientist Strong coder who cares about data = Data Engineer Acceptable coder who has lots of domain knowledge = Business analyst, Data Scientist. If you're just a Data scientist without any domain knowledge...... then you're in a precarious career position. reply CalRobert 4 hours agorootparentI've seen a disturbing rise in the number of people who think data engineering isn't software engineering. I don't plan to play up that part of my experience the next time I'm applying. reply chupy 1 hour agorootparentIt's because data engineering has been reduced to be able to login to a cloud provider and know which workflow to drag and drop. This is easily learned in a couple of weeks so that s why those skills might not be considered software engineering. reply HPsquared 35 minutes agorootparentprevI guess I'm a data scientist then, that sounds better than business analyst. reply krick 5 hours agorootparentprevWell, that's the GP's point, I guess: this thing was called \"Business analyst\", and, honestly, I don't know what being a domain-expert with somewhat above-average computer skills has to do with \"data science\". reply sampo 3 hours agoparentprev> the meaning of the term \"data scientist\" has changed http://i.stack.imgur.com/eLrhI.png vs http://image.slidesharecdn.com/daml-150908205332-lva1-app689... reply TrackerFF 54 minutes agoparentprevI use the following table (edit: table turned out ugly, sorry)domain knowledgequantitative knowledgetechnical knowledge-------------------------------------------------------------------------------------- data analysthighmidlowdata engineerlowmidhighdata scientistmidhighmidreply hyperman1 1 hour agoparentprevIn the Enterprise, the best qualified person for any specialisation you may want has always been whoever IBM/Oracle/Tata has sitting on their bench that week. They have the courses and certifications to prove it, too. It's magic! reply coldtea 2 hours agoparentprev>Data Scientists, back in the day, were largely people with both a fairly strong quantitative background and a strong software engineering background. The kind of people who could build a demo LSTM in an afternoon. As a field of \"science\" perhaps. In real life (when it became hot) data scientists mostly meant \"devs doing analytics\" and a lot of it involved R and Python, or the term \"big data\" thrown around for 10GB logs, and things like Cassandra, with or without some background in math or statistics. What it never has been, in practice, was a combination of strong math/statistics AND strong software engineering background. 99.9999% of the time it's one or the other. reply ProjectArcturis 10 hours agoparentprevYes, unfortunately when it was declared the Sexist Job, there was a tremendous influx of bootcamps promising you a six figure income after 3 months of part-time study. That has certainly lowered the overall quality of the Data Scientist title. reply minimaxir 7 hours agoparentprevOne funny aspect about the changing definition of \"data scientist\" is that I, currently a data scientist, spend most of my professional day working with the LLM/AI modeling areas nowadays and building custom models instead of building analyses and dashboards, since the former is more impactful. Job positions still want the latter, though. If I ever left my job I'm not confident I could get another job with the Data Scientist title, nor could I get a \"ML Engineer\" job since those focus more on deployment than development. My R is embarrassingly rusty nowadays and I miss making pretty charts with ggplot2. reply boredemployee 9 hours agoparentprevI really wonder whose fault is it. Unfortunately, what I see the most are many companies expecting you to be a jack of all trades (you should have GenAI/LLM skills, ML, Data Engineering, and what not) reply sukruh 1 hour agoparentprevI hate to be \"that guy\" but I find it a little bit sexist that the noob is called \"Rachel\". OK OK I'm gone. reply carabiner 10 hours agoparentprevNo that's MLE. A DS rarely gets asked leetcode algos questions, an MLE would. reply timdellinger 9 hours agorootparentI literally was asked two leetcode questions verbatim when interviewing for a data science position at TikTok a few months ago. Dynamic programming (I won't mention which question) and then one regarding binary trees. reply pests 9 hours agorootparent> (I won't mention which question) You must protect the corporate overlords. reply l33t7332273 6 hours agorootparentAlternatively, protect themselves since giving away an individualized question could identify them. reply fn-mote 5 hours agorootparentprevThe question does not matter at all. All of the information in the knowledge of Leetcode + category. (Does it really matter WHICH question?? They are different but all the same. That is the point.) reply KRAKRISMOTT 10 hours agorootparentprevDepends on the company, a research level MLE would be asked to derive loss functions and perform partial differentiation on pen and paper. You have to answer questions like what Kullbeck Leiber divergence is and how it can be utilized etc. reply defrost 9 hours agorootparentWould that be a lesser known cousin of the better known Kullback–Leibler separation measure for distributions? reply c0pium 9 hours agorootparentFull marks for snark, but points off for being incorrect. https://en.m.wikipedia.org/wiki/Kullback%E2%80%93Leibler_div... reply defrost 8 hours agorootparentFrom your link: a measure of how one probability distribution P is different from a second, ie. literally it's a separation measure for distributions .. just as I recalled from my first encounter with the notion ~ 1984 (ish). If you're sincere you should either add those points back or, preferably, expand upon your theory of how my snap take is incorrect. ( I'm aware it's not a metric due to triangle inequality, etc. ) reply elefanten 6 hours agorootparentThe snap take came across as an argument about which of two names for the measure is better-known. The wikipedia page implies the opposite of that argument. Perhaps that’s changed since 1984, but the proposition was about current practices. reply defrost 5 hours agorootparentIt's been Kullback since birth in 1907 to the best of my knowledge, never once Kullbeck. As a fully anglicized US citizen born in Brooklyn, New York I don't think there's ever been any vowel confusion over the spelling of the name: https://en.m.wikipedia.org/wiki/Solomon_Kullback Admittedly I did check as it's not uncommon for mathematicians to have alternate spellings for their names. Ditto Leibler, born Chicago, Illinois in 1914, no dropped L https://en.m.wikipedia.org/wiki/Richard_Leibler reply PheonixPharts 10 hours agorootparentprevYou must have worked at different places from me. Nearly every DS job I had (before wisely apparently) leaving that area had leetcode style algo questions during the interviews. Again, things have apparently changed. reply bllguo 10 hours agoparentprevi think you may be overestimating the avg. past data scientist's software engineering chops, but it's definitely true that the term has become more diluted than ever you still find these kinds of people and roles at smaller companies but at largecorps, what's the point? the interesting modelbuilding you shunt off to your army of phd-holding research scientists. deploying models and managing infra goes to MLE. what's left is the data analyst stuff, which you repackage as \"data science\" because cmon, \"analytics\"? are we dinosaurs? this is modern tech, we have an image to uphold! there's not really a need for, or supply of, people who can do everything (edit: _at largecorps_, obviously) reply itsoktocry 9 hours agorootparent>There's not really a need for Oh sure, if you have teams of research scientists and machine learning engineers to shunt the work to. That's, like, what? 5% of companies out there? Less? No need, indeed. reply bllguo 9 hours agorootparentso why exactly did you skip the first sentence of the paragraph so that you could make a self-evident point? anyway that 5% hires a disproportionately larger # of \"data scientists\" reply _diyar 1 hour agoprevAs mentioned by others, the author Dr. Greg Wilson has written/compiled many books/tutorials which I can recommend. I would especially laud Software Design by Example [1][2], The Architecture of Open Source Applications [3] and Teaching Tech Together [4]. [1] https://third-bit.com/sdxpy/ (python version) [2] https://third-bit.com/sdxjs/ (js version) [3] https://aosabook.org/ [4] http://teachtogether.tech/ reply throwaway99989 8 hours agoprevReally excellent concise SQL guide, and great teaching by example. At most universities in the US this content gets taught over an entire semester (and probably shouldn't be). This guide is complete enough for 99% of industry SWE jobs outside of database optimization. It's a great service when someone takes the time to document knowledge on a single page with quality examples, and trust the reader to follow along. Reminds me of the Rudin analysis book. reply shubhamjain 11 hours agoprevShameless Plug: If anyone here wants to practice their SQL, they are welcome to try my Mac app: TextQuery [1]. I built it because I wanted to quickly import CSV datasets and run SQL queries on them. I don't think there could be a more fun way to learn SQL than to jump in and start analyzing thousands of public datasets. Sure, you can use CLI/Code as well, but GUI is often faster and easier. Currently, the app is in the beta period and free-to-use. When launched, you'll get to keep the latest beta version. [1]: https://textquery.app/ reply _kush 4 hours agoparentThis is a very interesting idea! Great work reply Terretta 7 hours agoparentprevWhy do you need to harvest email addresses? reply friendlynokill 7 hours agoparentprevBeen meaning to learn SQL, so will check this out reply Atotalnoob 7 hours agoparentprevIsn’t this just a ui over SQLite? reply aeturnum 7 hours agorootparentThere's clearly some non-SQLite functionality that seems really neat! reply xp84 10 hours agoparentprevSuper cool! Thanks for sharing. reply saadatq 10 hours agoparentprevThis is brilliant. reply nomilk 11 hours agoprevBefore flights with patchy/no wifi, I often download a long, single-page tutorial. This is perfect. Curious if anyone knows of any for other languages/tech (e.g. beyond SQL). reply adamiscool8 10 hours agoparentIf not familiar: https://learnxinyminutes.com/ reply belter 9 hours agorootparentFunny on that one, there is so much but not SQLite :-) reply beeburrt 10 hours agoparentprevZig docs are all one page: https://ziglang.org/documentation/master/ reply nomilk 10 hours agorootparentWow - gave me a goofy smile. Also cool 'Zig Zen': https://ziglang.org/documentation/master/#toc-Zen reply nbbaier 1 hour agoparentprevGot any other favorite tutorials you've used for this kind of flight? reply Anon84 11 hours agoparentprevShameless Plug: - Not single page _per se_ but I have plenty of Jupyter Notebook based tutorials here: https://github.com/DataForScience/ Self contained with slide decks and notebooks. reply blackhaj7 5 hours agorootparentThese are great, thanks! reply rgovostes 5 hours agoprevThis is from Greg Wilson (https://third-bit.com), co-editor of \"The Architecture of Open Source Applications\" and many other superb references. Edit: He wrote about this project here: https://third-bit.com/2024/02/03/sql-tutorial/ reply AmazingTurtle 52 minutes agoprevWatch out, you're only referring to MALE and FEMALE sex, that might trigger a few people here reply zubairq 50 minutes agoprevSome useful SQL queries for me in this list of 100 queries, thanks! reply nomilk 11 hours agoprevThe tutorial can be downloaded here: https://github.com/gvwilson/sql-tutorial/raw/main/sql-tutori... (gives the penguins.db file necessary for the examples) reply dragonelite 55 minutes agoprevI would expect someone that has the title of data scientists to at least be a master in the universal data query language SQL. But im so used to ORM these days anything more complex then a sql join is already going over my head if i didn't do a sql refresher. As far as i have skimmed the article it seems like a very good refresher for even a SWE. I will definitely put this tutorial on my todo list. reply mvdtnz 8 hours agoprevA lot of these queries are only valid on SQLite (or maybe not \"only\" but certainly not generally available across DBMS's), including cases where the author doesn't point this out. For one example filtering on aggregates is not supported by MySQL, MS SQL, Oracle or most other DBMS's. reply petalmind 11 hours agoprev> left outer join > A join that is guaranteed to keep all rows from the first (left) table. Columns from the right table are filled with actual values if available or with null otherwise. This wording only works for identity equality join condition. It creates misleading mental model of left joins, and unfortunately is very common. reply nomilk 11 hours agoparentI'm not sure I understand, I think this definition still works for left outer joins on conditions other than identity equality, since joins on, say, inequalities or multiple conditions would still be \"guaranteed to keep all rows from the first (left) table. Columns from the right table are filled with....\". reply orlp 9 hours agorootparentIt kind of implies that the left join process works by first taking the left table and then filling in the right table with a match if one exists, and otherwise null. That model obviously doesn't work because if there's more than one match as the matching left row is duplicated for each match. However I don't understand their point of this being a problem when you don't have a \"identity equality join condition\", since this can also occur for equality joins as long as you're not joining on a unique key. reply erehweb 11 hours agoparentprevCan you elaborate with a gotcha example? reply hobs 11 hours agorootparentI assume they mean that row multiplication can occur but otherwise not sure. reply magicalhippo 8 hours agoparentprevI'd say it's misleading at best. It should specify that for multiple matches in the right table, the row from the left table is duplicated per matching row in the right table. reply simonw 10 hours agoparentprevCan you clarify? Is the problem here that the \"guaranteed to keep all rows from the left table\" piece is accurate, but the section about the right table might not be a good mental model if the join condition is more complex than a simple equality? reply reaperman 9 hours agorootparentThere was discussion about this exact same thing about 17 days ago. It comes up surprisingly frequently. https://news.ycombinator.com/item?id=39071550 reply aussieguy1234 7 hours agoprevAlso, remember ChatGPT can help you write queries like a data scientist, without actually being one. Just tell it what you want in plain english. I've used this to get all sorts of useful metrics like conversion rates, messaging click through rates, etc... reply blitzar 27 minutes agoparent> ChatGPT can help you write queries like a data scientist Badly then? Personally I want my SQL queries to be written like a database professional. reply jkrubin 3 hours agoparentprevConversely, it’s very helpful at dissecting a 3k line insanity query and explaining it. reply aussieguy1234 2 hours agorootparentYes. It'd be interesting to see what it makes of some of those ugly ORM generated queries. reply mdekkers 2 hours agorootparentprev> 3k line insanity query *laughs in PTSD* reply thehours 6 hours agoprev> full outer join (also called cross join) constructs their cross product Full outer joins and cross joins are different types of joins. A cross join returns the Cartesian product of both tables, while a full outer join is like a combination of a left and right join. Better explanation here: https://stackoverflow.com/questions/3228871/sql-server-what-... reply user3939382 10 hours agoprevI don't see how this is \"for data scientists\" it looks like a good summary of SQL in general. reply OJFord 7 hours agoparentAccording to the article itself it's explicitly not anyway - it's for instructors (of data scientists potentially, sure). reply swasheck 9 hours agoparentprevagreed. and sqlite specifically. i’d really like to see an authoritative resource that shows me how to use sql and relational algebra to do some basic and intermediate data analysis. i can find like 8 different ways to calculate skew and kurtosis, but is there a trusted resource that can show me how to do it? what other interesting data analysis can i do using sql? reply vavooom 11 hours agoprevLearning outcomes: * Explain the difference between a database and a database manager. * Write SQL to select, filter, sort, group, and aggregate data. * Define tables and insert, update, and delete records. * Describe different types of join and write queries that use them to combine data. * Use windowing functions to operate on adjacent rows. * Explain what transactions are and write queries that roll back when constraints are violated. * Explain what triggers are and write SQL to create them. * Manipulate JSON data using SQL. * Interact with a database using Python directly, from a Jupyter notebook, and via an ORM. reply pradeepchhetri 4 hours agoparentIf you are interested in reading about JOINS in detail, ClickHouse has good blogs around it: - https://clickhouse.com/blog/clickhouse-fully-supports-joins-... - https://clickhouse.com/blog/clickhouse-fully-supports-joins-... - https://clickhouse.com/blog/clickhouse-fully-supports-joins-... - https://clickhouse.com/blog/clickhouse-fully-supports-joins-... - https://clickhouse.com/blog/clickhouse-fully-supports-joins-... reply tucnak 2 hours agorootparentI'm sorry but Clickhouse isn't SQL-compliant so it has no business teaching us JOIN. reply bigger_cheese 9 hours agoparentprevOne thing that I have noticed confuses a lot of people is \"timeseries joins\" (I don't know the real term for this) I'm talking about where there is no \"one to one\" match between keys in the two tables. I'm a non software type of engineer in my world a lot of tables are structured as timeseries data (such as readings from a device or instrument) which uses timestamp as a key. Then we have other tables which log event or batch data (such as an alarm start and end time, or Machine start/machine stop etc). So a lot of queries end up being of the form Select A.AlarmId, B.Reading, B.Timestamp from Alarms A, Readings B where A.StartTime >= B.Timestamp and A.EndTime < B.Timestamp A lot of people seem to have problems grasping these kinds of joins. reply davery22 7 hours agorootparentCool use case. They're just called \"non equi-joins\" - because the join condition is an inequality. In general a join produces a row in the output table for each (left, right) pair of rows from the input tables that satisfies the join condition. It's just so common for joins to use a simple equality condition, where one or both sides is a unique id for its table, and people don't as often encounter joins where one input row can inform multiple output rows. reply Jgrubb 5 hours agorootparentprevDuck and Clickhouse call this an AsOf join - https://duckdb.org/docs/guides/sql_features/asof_join.html reply pradeepchhetri 4 hours agorootparentYup https://clickhouse.com/docs/en/sql-reference/statements/sele... reply mr_toad 7 hours agorootparentprevYou mean slowly changing dimensions? It’s not something there is much literature on, especially for outer joins. reply LegitShady 10 hours agoparentprevthese does seem to be a fairly good curriculum for an introduction to SQL reply hobs 11 hours agoprevDon't forget the venerable SQLZoo - I have referred a zillion people to it over the years. https://www.sqlzoo.net/wiki/SQL_Tutorial Edit: also an inaccuracy that's minor but can bite you if you're not careful - they mention temporary tables are in memory not on disk - that's not true in almost all sql databases, they are just connection specific eg they don't persist after you disconnect. Some databases are optimized for a temp table to be a throwaway, but that can be a good or bad thing depending on the use case. reply vavooom 11 hours agoparentI am also a fan of Mode's SQL Tutorial: https://mode.com/sql-tutorial reply wanderingmind 3 hours agoparentprevWhere is the datasources to practice them? reply StarlaAtNight 7 hours agoprevDon’t have time to do it myself, but might be good to redo this guide but with DuckDB (more likely used by DS’es nowadays than SQLite) reply Exuma 5 hours agoprevFantastic summary, saving for just general tips when trying to show people SQL reply dinkleberg 11 hours agoprevThanks for sharing this! Learn by example resources can be super helpful. reply bilsbie 9 hours agoprevDoes anyone remember some kind of adventure or mystery novel that you progress through by solving sql challenges? I remember seeing it once and I can never find it now. reply fbdab103 8 hours agoparentSelect Star SQL? https://selectstarsql.com/ It opens up with analyzing death row inmates, so significantly more real than classifying flowers. reply simonw 8 hours agoparentprevSounds like https://mystery.knightlab.com/ SQL Murder Mystery by Northwestern University Knight Lab. reply proamdev123 8 hours agoparentprevThere was one about space exploration based on Postgres, but I don’t remember the name. reply Tomte 5 hours agorootparentMastery with SQL. A paid course that‘s excellent. reply natrys 1 hour agorootparentYeah it's excellent, but I am pretty sure OP was talking about something else: https://sales.bigmachine.io/curious-moon reply Tomte 1 hour agorootparentDamn, I meant Curious Moon and said the other great one. People, do both! Worth every cent! reply pama 11 hours agoprevI’ve found that chatGPT is excellent in helping with generating and testing SQL queries. This tutorial is likely helpful for its intended audience; I’d like to see a bit more discussion of query optimization, stored procedures, table design, and available options both in db and in bindings, but it’s hard to fit everything in one place and keep it clean, which is why LLMs will eventually win. reply cyral 8 hours agoparentI've found the same. Especially dealing with sorta complex JSON column migrations where the JSON structure needs to be changed. Can describe in plain english what I want to do and get a working query for it. reply worik 9 hours agoprevWhat is a data scientist, if they are not a statistician? I am a grumpy old man, fed up with newspeak reply blitzar 23 minutes agoparentWhats this newspeak? Why cant people just talk properly like we did before the war? Not that silly little war, the actual war, WW1. I am a grumpy old man, fed up with all this newfangled nonsense. reply carabiner 11 hours agoprevAlso StrataScratch.com for leetcode for SQL. What really distinguishes an SQL master is working with queries hundreds of lines long, and query optimization. For example, you can often make queries faster by taking out joins and replacing them with window functions. It's hard to practice these techniques outside of a legit enterprise dataset with billions of rows (maybe a good startup idea). reply swasheck 9 hours agoparentone of the most performance-killing practices i’ve seen (outside of brutal orm-generated queries) is the sql hero who wants to do everything all at once in a single splat of sql. at a certain level of complexity, breaking things up into logical units and even forcing materialization (instead of hoping for some good intermediate materialization) can go a long way reply ddgflorida 9 hours agoprevNice! It could be better if the queries were interactive. reply blagie 9 hours agoparentThat's what would change this from a lousy resource to a brilliant one. Little SQLer (\"Little Squealer\") reply rqtwteye 8 hours agoprevItem 70 doesn't look right. reply atseajournal 11 hours agoprevGlad to have learned about iif() from this! reply webdoodle 11 hours agoparentIt's really handy in conjunction with GROUP BY. reply SilverBirch 11 hours agoprevThe queries are solid, but I really appreciate throwing in the worlds most confusing diagrams here and there. It keeps me alert trying to find where to even start with them. So to be clear, tables must have a name, and must have rows and the rows must have the same name as the table, yes? reply andrscyv 8 hours agoprevWhy bother, just use ChatGPT reply QAFred123 11 hours agoprev [2 more] [flagged] dang 11 hours agoparent [–] Could you please stop doing this? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The information provided is a comprehensive resource for teaching SQL to instructors, covering topics such as database management, SQL queries, data joining, windowing functions, transactions, triggers, JSON data manipulation, and Python's interaction with databases.",
      "The material includes setup instructions, background concepts, and examples of SQL queries with their outputs for different scenarios.",
      "It also covers concepts like aggregation functions, constraints, upsert, normalization, and provides code snippets demonstrating SQLite and Python usage, including exception handling, working with dates and times, using SQL in Jupyter notebooks, and utilizing Pandas with SQLite. A list of key terms related to databases and SQL is also included."
    ],
    "commentSummary": [
      "The summary covers discussions on data science, SQL, and related topics, exploring the definition and expectations of the data scientist role.",
      "It highlights confusion around different titles and expectations within the field.",
      "The usefulness of tools like ChatGPT for writing SQL queries in plain English is discussed, along with challenges in using Clickhouse for SQL joins and working with timeseries data.",
      "Recommendations are provided for SQL tutorials, resources for query optimization, and a suggestion to use DuckDB."
    ],
    "points": 581,
    "commentCount": 111,
    "retryCount": 0,
    "time": 1707256226
  },
  {
    "id": 39277767,
    "title": "HSBC Bank Leak Exposes Fraudulent Mortgages Fueled by Fake Chinese Income in Toronto Real Estate",
    "originLink": "https://www.thebureau.news/p/fake-chinese-income-mortgages-fuel",
    "originBody": "Share this post \"Fake Chinese income\" mortgages fuel Toronto Real Estate Bubble: HSBC Bank Leaks www.thebureau.news Copy link Facebook Email Note Other Discover more from The Bureau Investigative Journalism. Anti-Corruption. Counter-Disinformation. Whistleblowers. Sunlight. Over 7,000 subscribers Subscribe Continue reading Sign in \"Fake Chinese income\" mortgages fuel Toronto Real Estate Bubble: HSBC Bank Leaks “I found out a huge mortgage fraud showing borrowers with exaggerated income from one specific country, China\": The Bureau investigates whistleblower docs Sam Cooper Feb 6, 2024 56 Share this post \"Fake Chinese income\" mortgages fuel Toronto Real Estate Bubble: HSBC Bank Leaks www.thebureau.news Copy link Facebook Email Note Other 13 Share PHOTO ILLUSTRATION: THE BUREAU INVESTIGATES TORONTO METHOD MORTGAGE LEAKS. The whistleblower, a Canadian business school graduate, was staggered by the suspicious home loans he discovered in 2022 when he joined a mortgage approval team in a small HSBC branch on the outskirts of Toronto. He knew of suspicions surrounding Chinese capital in British Columbia real estate, but had never witnessed shady lending while working at an HSBC branch in Campbell River, a bucolic town on the coast of Vancouver Island. When he arrived at HSBC’s bank in Aurora, an affluent suburb north of Toronto, he discovered explosive growth in home loans to Chinese diaspora buyers during the Covid-19 pandemic. Chinese migrants living across Toronto were obtaining mortgages from HSBC while supposedly earning extravagant salaries from remote-work jobs in China. In one example, an Ontario casino worker that owned three homes also claimed to earn $345,000 in 2020 analyzing data remotely for a Beijing company. Before joining HSBC Canada, the whistleblower had studied fake-income mortgage frauds for his Business Masters degree at Vancouver Island University. After arriving at Aurora in February 2022, while digging into the branch’s loan books and interrogating his colleagues, he made mind-blowing assessments. Since 2015, the whistleblower concluded, more than 10 Toronto-area HSBC branches had issued at least $500-million in home loans to diaspora buyers claiming exaggerated incomes or non-existent jobs in China. These foreign-income scams spiked during the pandemic, the whistleblower believed, because borrowers could somewhat plausibly claim to be working remotely in other countries while riding out Covid-19 in Canada. While a small bank of Aurora’s size was expected to issue about $23-million in residential loans every year, this branch had shovelled out $88-million in mortgages in 2020, according to the whistleblower, and over $50-million in 2021. The whistleblower, whomThe Bureau is calling D.M., immigrated to Canada as an international student from India, making him a minority among mostly Chinese-Canadian co-workers at the Aurora branch. As D.M. probed his colleagues, his belief gained conviction, that HSBC Canada and other Canadian banks including CIBC had systemic problems with highly questionable mortgages issued to diaspora buyers with unverified sources of wealth in China. Losing sleep, in April 2022, D.M. sent an audacious email to senior bank executives: “I am going to reveal potential mortgage fraud at HSBC Bank Canada and possibly some employees benefited from the fraud, financially pocketing thousands of dollars, which I call the proceeds of crime.” D.M.’s explosive four-page complaint triggered an internal investigation that led to some reforms at HSBC Canada according to internal emails obtained by The Bureau. But more than a year later, D.M. was so dissatisfied with the bank’s response that he risked sharing his story and numerous internal documents for an unprecedented journalistic investigation into Canada’s housing affordability crisis. “I found out a huge mortgage fraud showing borrowers with exaggerated income from one specific country, China, pretending to be working remotely,” D.M. informed The Bureau in June 2023. “I believe the housing prices in Toronto are linked to this, because this is about income verification in banks, which is supposed to moderate demand.” The Bureau asked HSBC Canada to review emailed information for this story and provide an appropriate manager for an interview regarding D.M. 's records and allegations. “I won’t have anyone to speak with you directly,” Sharon Wilks, Head of Communications, responded. “But for context: As a global bank, HSBC is at the forefront of efforts to identify, prevent and deter financial crime … We will not do business with individuals or entities we believe are engaged in illicit conduct.” Wilks added that HSBC Canada “can and do regularly exit relationships with clients whose activities we deem too risky.” The Bureau’s seven-month investigation into D.M.’s allegations suggests HSBC Canada and other Canadian banks could have issued many billions of dollars in questionable mortgages to Chinese diaspora buyers, and a significant cause of Canada’s real estate bubble is hundreds of billions in illicit fund transfers from China into Canada, and bank lending that amplifies its impacts, especially in Toronto and Vancouver home prices. “There are thousands of these cases, large scale,” D.M. said in an interview. “Hardworking Canadians are denied mortgages and these Chinese residents forge documents and get mortgages approved, heating up the already hot Ontario real estate markets.” “These people don’t have steady jobs or income in Canada,” he alleged, “but what they are doing is scams to launder money, and get mortgages using fake documents.” The Bureau’s investigation included asking seven prominent Canadian experts to assess some of D.M.’s documents, allegations and conclusions. This investigation suggests D.M. 's calculation is plausible, that the Aurora branch and other Toronto-area HSBC branches have issued at least $500-million in questionable Chinese income loans since 2015. But D.M’s findings could also change the public’s understanding of housing affordability in Toronto and Vancouver, a politically explosive issue expected to frame Canada’s upcoming federal election. This is because, according to the academics and criminologists that reviewed D.M.’s documents with The Bureau, his evidence fits into FINTRAC’s much broader examinations of suspicious real estate and banking transactions. In 2023, the anti-money laundering watchdog published a ground-breaking study into 48,000 Chinese diaspora banking transactions. FINTRAC found that during the Covid-19 pandemic, because Canadian casinos were closed, Chinese underground banking schemes evolved, flooding electronic fund transfers from Hong Kong into Canadian bank accounts that served like corridors for murky real estate transactions. The Bureau’s analysis also finds that what D.M. discovered in Toronto banks, finally sheds light on mysterious capital flows discovered by a prominent Canadian academic in 2015, in a study of Vancouver land titles and mortgages. That examination of $525-million worth of real estate purchases in a six-month period found 66 percent of buyers in several affluent neighbourhoods were recent Chinese diaspora migrants, and most mortgages went to buyers with little or no income in Canada. Similarly, what D.M. found in his probe of pandemic-era loans could be called the evolving “Toronto Method” of an underground banking system discovered first in Vancouver, and found to be laundering a stunning $1.2-billion in cash from Mainland China through British Columbia government casinos in 2014. This system of shadowy transfers was dubbed the “Vancouver Model” by an Australian professor, and brings together transnational organized crime, affluent Chinese nationals seeking to export their wealth abroad, and Canadian casinos, banks and real estate, in transactions that evade policing because the pivotal cash exchanges are done off the books by professional money launderers serving the global Chinese diaspora. According to FINTRAC’s 2023 study of 48,000 pandemic-era transactions, this evolving Vancouver Model network “simultaneously facilitates money laundering and the circumvention of Chinese currency controls” “As a result of the temporary closures of Canadian casinos due to the COVID-19 pandemic, professional money launderers began to diversify their money laundering methods,” FINTRAC’s study says. “During this time, FINTRAC observed a rise in money laundering typologies involving transferring large sums of funds to Canada from foreign money services businesses, often located in China, notably Hong Kong, and the laundering of the funds primarily through the real estate, securities, automotive and legal professions.” These wire transfers from China were routed into bank accounts of “multiple, unrelated individuals in Canada,” that served as “money mules” in byzantine networks involving Canada-based real estate developers, real estate agents, mortgage brokers and banks. These Chinese diaspora bank account owners often claimed they were students, homemakers, office managers, or unemployed, FINTRAC reported. They sometimes used their accounts to send bank drafts to others in Canada for home purchases, or served as “straw buyers” for offshore investors. “Mortgage payments are sourced from incoming funds from China,” FINTRAC’s alert said. FINTRAC’s 2023 Project Athena alert on Chinese money laundering schemes using Canadian banks and real estate. FINTRAC’s study doesn’t say that Canadian banks knowingly issued fake-income mortgages to Chinese diaspora buyers in Toronto. But in an interview, D.M. said banking staff are trained to guard against fraud, and the loan application packages he reviewed in Aurora beggared belief. “The bank found out that one lady works in a casino part-time but got a $1.4 million mortgage showing over $300,000 annual income,” he said. “Plus she takes money as benefits from the government, for her two kids.” In other examples, an HSBC mortgage client claimed to earn $700,000 annually for remote work in China, while simultaneously living in Canada and paying off a $10,000 student loan. Another woman who owned homes in Aurora, Markham and Scarborough, worked part-time as a hairdresser while also claiming to earn $536,280 at a “Business Manager” job in Guangzhou. “Canadian workers have been put out of the real estate market by people working as a hairdresser that own a couple homes,” D.M. said in an interview. “How is that fair?” The most shocking case reviewed by The Bureau, shows that one woman that owns at least four Toronto properties opened her HSBC Aurora bank account in 2012, claiming to be a “Homemaker with no annual income.” But her Toronto account soon received incredible amounts of wire transfers from HSBC China accounts, and paid out “high value cheques” to third parties for real estate purchases. This case suggests “Toronto Method” shadow banking described in FINTRAC’s 2023 study has been seeping into Toronto real estate for about a decade. And yet in 2020, this same woman applied for another HSBC Canada mortgage, claiming to earn $763,000 remotely from her job in China. This evidence from the HSBC whistleblower complements the seminal investigations of Simon Fraser University academic Andy Yan, who examined sales from August 2014 to February 2015 in several communities on Vancouver’s westside. The average home price in Yan’s study was $3-million. Looking back at his Vancouver findings in comparison to D.M.’s Toronto banking documents, Yan told The Bureau “I think this helps affirm some of my early work that I did, almost nine years ago.” “This goes to the core of our banking system,” he said, “and how are we verifying identities and how are we verifying incomes.” In Yan’s controversial study the vast majority of mortgages went to buyers listing their occupation as home-maker, followed by students, and managers. HSBC and CIBC were the dominant lenders. Unlike the HSBC whistleblower, Yan had no access to internal banking data regarding the purported origin of funds behind these mortgages taken by Chinese diaspora buyers. But in an interview, Yan said what he found most interesting back in 2015, was suspicions that Chinese migrants were often buying homes with bulk cash, weren’t accurate. The truth was more complex and seems to be clarified by D.M.’s mortgage findings in Toronto. “It's about that global flow of capital, and how it's multiplied by Canada’s mortgage and lending system,” Yan said. “Because you have to remember, one of the biggest conclusions about my study was that it wasn't bags of cash that were being used to purchase Vancouver homes outright. They were loans being used. So now, I’m thinking, this is where my study connects up to what you have discovered in Toronto.” “The interesting story here,” Yan added, “is what happens in Toronto real estate may not repeat Vancouver, but it perhaps rhymes.” Probably the most famous Chinese property owner from Yan’s 2015 study areas is Huawei executive Meng Wanzhou. In 2009 her family bought a home in Vancouver’s Dunbar neighborhood for $2.73 million, land titles show. In 1998, ten years before Vancouver Model transactions started to surge in Vancouver real estate, the home was sold for $370,000. The home is now valued at almost $6-million. Ashleigh Gonzales, a former RCMP data scientist who recently published a criminology thesis finding Chinese diaspora underground banking causes significantly more money laundering into Canada’s real estate than previously estimated, said that D.M.’s findings resemble her own Vancouver Model research. “This whistleblower’s allegations of widespread mortgage fraud at HSBC Canada align with some of the first-hand accounts from staff of some Canadian financial institutions that I have come across in my research on money laundering in British Columbia,” Gonzales said. Gonzales, who worked for RCMP’s anti-gang unit in British Columbia until 2023, says she found reports of mortgage fraud accelerated “during the uptick in the Canadian housing bubble after the Vancouver 2010 Olympics,” and continued to surge from 2015 to 2018. With all this considered, and comparing data sources in this story with previous evidence confirmed in British Columbia’s Cullen Commission, The Bureau estimates that from 2014 to 2023, well over $200-Billion in Vancouver Model and Toronto Method funds could have poured through underground diaspora networks and Canadian financial institutions into Toronto and Vancouver’s real estate. A federal official not authorized to comment publicly also examined D.M.’s banking leaks for The Bureau, and called this information “explosive.” The official said money laundering is increasing in Canada, and D.M.’s belief that Chinese-income mortgage fraud has boosted home prices in Toronto is likely true, but also should apply for Vancouver and Montreal real estate prices. The official noted that other nations require tax agencies to verify incomes for mortgages, which isn’t the case in Canada. “It matters for our next generation because of the impact on the housing market,” the official said. Queen’s University professor Christian Leuprecht – editor of Dirty Money, a new academic text that probes how Ottawa’s weak regulation has “turned the Canadian federation into a destination of choice for global financial crime” – also reviewed some of D.M. 's leaks. “It’s not a new problem, but you’re taking it to the next level,” Leuprecht said. “Why does this matter? Because organized crime isn’t just laundering their ill-gotten gains, like any good business person, when they buy real estate, they generate a down payment, then get a mortgage for the rest. Why buy one property when you can buy four?” “Do you know how many mortgage frauds we have in our books?” The Bureau’s review of HSBC Canada emails and D.M.’s text messages, shows he came to believe numerous employees at the Aurora branch had direct knowledge of faked Chinese income mortgages, and a veteran manager with oversight of more than 10 Greater Toronto branches knew about broad and questionable mortgage lending for Chinese diaspora clients. Months after D.M. blew the whistle internally he exchanged texts with another employee, identifying colleagues that they believed had knowledge of diaspora mortgage scams. The texts suggest D.M. believed HSBC Canada and other Canadian banks continued to hold vast amounts of suspicious foreign income mortgages, which could cause systemic loan quality risks if Toronto’s real estate prices decline. “Do you know how many mortgage frauds we have in our books,” D.M. texted to his colleague. “It’s insane.” “She told me,” the colleague replied, referring to an HSBC branch manager. “She was like, if you do come, you gotta be prepared for the mortgage payout.” “These people showed fake income and got mortgage,” D.M. continued. “Now interest rate is high, they can’t cope.” “Other branches did the same thing too,” his co-worker replied. “I heard there’s a lot.” “Absolutely,” D.M. texted. “All branches engaged in it.” “This is like the unspoken secret,” his co-worker concluded. “I’m pretty sure other banks have it too. My Aunt have no income and got a mortgage for 700k. They just need a Covenanter from China.” Generally, in mortgage contracts a covenanter takes responsibility for the loan if the primary borrower defaults. Internal records reviewed by The Bureau confirm that on April 18, 2022, D.M. sent a lengthy complaint email to senior HSBC Canada executives, informing them of allegations he’d learned from his colleagues. In it, he alleges that an Aurora manager had informed him of a complaint letter posted to the branch, that accused mortgage brokers and branch employees of colluding in scam mortgages emanating from Mainland China fraud networks. Pointing to specific examples, D.M. claimed that another branch colleague had admitted processing numerous loan applications without meeting his clients, because a branch manager delivered her subordinates foreign income client applications so “they did not have to get sales themselves.” “Surprisingly all these clients he would get will have foreign income most of the time very inflated like 400k or 670k a year,” D.M. wrote. “To me that’s suspicious, but he never questioned the branch manager because in Asian culture it’s disrespectful to question elders.” D.M. also informs his bosses that one Aurora bank manager opened up to him, saying she believed allegations of mortgage fraud collusion involving some branch staff. “She said yes, she knows specially in Mainland China there is a team who would even answer emails and phone calls verifying [Chinese income] but it’s a sophisticated and well organised scam,” D.M. 's email to HSBC Canada managers says. His complaint explains that he continued to press an Aurora bank manager on her knowledge of fraud allegations. “When I asked for such a serious issue if she raised a HSBC confidential [complaint] or not she evaded my question,” D.M. wrote. “Now we all love numbers, but I don't think the bank will like these kinds of numbers achieved through this way.” Describing why he contacted HSBC Canada executives directly, the whistleblower’s complaint says he felt confused and isolated, but D.M. decided “local leadership if not participated, at least turned a blind eye,” to Chinese fake-income scams, forcing D.M. to “bring up a serious issue against people of superior positions.” “I could not have stayed silent, in fact I could not sleep well thinking about it,” his April 2022 complaint says. “It reminds me to some extent what happened with the Home Capital Group.” “The whole thing is wrong on so many grounds,” D.M. continued. “Now I know one more reason why Canadians and permanent residents are not getting into the housing market. It’s not only HSBC such things are happening across other Canadian banks as well.” In the Home Capital case, the Ontario Securities Commission fined the prominent Ontario-based subprime mortgage lender in 2017, alleging Home Capital failed to disclose several of its mortgage brokerages had major problems with faked-income mortgages. D.M. concluded his four-page complaint to senior executives, writing: “I recommend all mortgage deals of this branch in the last 3 years at least if not longer with Foreign income be probed.” “Bank statements can be verified directly with the foreign banks or use a reputable third party to verify,” he suggested. “When we find someone with Fake ID or trying to impersonate someone we call the cops. But these people, both staff nor clients who did fraud were reported.” Hours later on April 18, 2022, an HSBC Canada executive emailed back: “I am going to refer this to our Fraud and Risk teams and they will investigate your concerns.” “The Implications are Broader” The next day D.M. continued to hound HSBC Canada managers with emails to support his allegations, spotlighting the absurdity of massive Chinese remote incomes claimed by diaspora buyers. He pointed to one woman with a $1.6-million HSBC Canada mortgage. “The client claims to be in Canada but [is] a office supervisor in China. [In the] age of remote working in which country [does] a office supervisor makes 400k please tell me,” D.M. wrote. “[W]hen I asked the co-worker she said her job is not to use the brain or be a police, when I asked do you think she makes that kind of money and how is she doing her job being in Canada to be an office supervisor in China[?]” Pointing to another document, D.M. warned his managers about Ms. Chen, who claimed to make $721,000 annually as “project manager” for a Beijing telecommunications company, to secure a $1.89 million mortgage. Again on May 4, 2022, D.M. emailed executives, suggesting internal records for an Aurora client named Ms. Lin had been altered soon after D.M. blew the whistle on fake Chinese income loans. His email, which included Ms. Lin’s client profile, warned: “Something interesting happened yesterday, they added a China address to go with [the] story of working in China, please see below.” The Aurora branch banking records disclosed to The Bureau show that Ms. Lin owns three homes in the blocks surrounding Pacific Mall in Markham. “The client was onboarded on 24th March with Canada address only and Canadian tax residency,” D.M.’ s email continued. “She claims to be working in China and have foreign income, so the story she is stuck in Canada due to Covid is very interesting. Suddenly yesterday she decided her address in China. Someone saw the discrepancies and the branch team decided to change it.” “To me that's a red flag done to align with the story portrayed.” Next, D.M. exposed Ms. Lin’s foreign income claim. “She works for Food processing company, a logistics officer making 273k a year,” he wrote. “I don't know which logistics officer can work when physically in a different company and also who makes 273k working as a logistics officer.” Citing another internal banking record, D.M.’s email pointed to Ms. Lin’s $273,000 income and said “it’s interesting how they did the verification.” The email continues to explain that branch records showed Ms. Lin and her husband had a joint mortgage with a balance of $497,000 at CIBC. But suddenly during Covid-19, Ms. Lin applied for a new mortgage for $1.2 million with HSBC Canada. “When I see such things I can't stay quiet,” D.M.’s May 2022 email says. “[I] was assuming with the new rules things will stop, [but] declining the mortgage or retraining the staff is like treating the symptoms.” He added that many suspicious Chinese income loans had been “flagged by our Fraud Team already.” The whistleblower’s scathing assessment ends with the observation that D.M. didn’t believe “someone woke up and decided to scam the bank, but [worked with] a sophisticated network of agents who are training people what to say and answer.” “The implications are broader and as a responsible bank and citizen we have to,” request investigations from the Canadian Revenue Agency or Ontario Provincial Police, D.M. asserted. D.M., who asked not to be named because he could face reprisals, filed a four-page whistleblower complaint with HSBC Canada managers in April 2022. The Bureau asked Ashleigh Gonzales, the former RCMP data scientist, to review some of D.M.’s documents and conclusions. “From what I have reviewed, D.M.’s findings align with what appear to have been commonplace practices by some groups of staff complicit from the front line, middle office, and back office and sanctioned by management,” Gonzales wrote, adding “whether knowingly or not depends on the individual work cultures.” The Bureau also asked Stephen Punwasi to review D.M. 's leaked banking documentation. Punwasi is a financial expert who founded Better Dwelling, a real estate analysis website with a large following of young professionals trying to understand why they’re excluded from home ownership in Canadian cities. He also provided analysis for British Columbia’s 2018 report into Vancouver Model money laundering in casinos, real estate and luxury vehicles. What Punwasi explained to the report’s author, former RCMP executive Peter German, is that even though Vancouver Model money launderers don’t comprise a majority of buyers in Vancouver, their willingness to overbid on home sales causes ripples that sends prices skyrocketing, especially during times when political turmoil inside China triggers increased capital flight. “In 2015 and 2016 Ontario saw this flood of money from China, just like British Columbia, and it was not just to do with immigration, it was due to President Xi’s political crack down on corruption,” Punwasi said. “I think we’ve seen that capital flight in Ontario and B.C. in two big cycles, also including 2020 and 2021.” The Bureau asked Punwasi if the banking records disclosed by D.M. help to explain Toronto’s real estate price surges. “Absolutely,” he said, pointing to the case of Ms. Lin (who claimed a $273,000 remote-work income in China) and her three homes surrounding Markham’s Pacific Mall. Property buyers that aren’t shopping for shelter, but for capital flight or money laundering vehicles, are what Punwasi terms the “marginal buyer.” “The marginal buyer is like an exuberant buyer on crack, so if they are motivated to move as much money as possible,” he said, “the larger the mortgage they can get, it helps them to overpay for homes, and that can cause the price to launch.” “So if you see a townhome in Toronto going for $2-million, you don’t know if it is mortgage money laundering or someone buying a place to live. You just have to compete with the going price.” Punwasi says housing prices are a powerful political issue that will shape the next federal election. But at the same time, young generations are confused by competing explanations on the causes of Canada’s housing affordability crisis, Punwasi believes, whether its lack of housing supply due to restrictive zoning bylaws, or increased demand due to recent immigration surges, or other factors that make Canada’s housing bubble an outlier in the Western world. “There are so many conflicting narratives right now that people find it hard to believe the scale of impact that money laundering can have on Toronto real estate prices,” Punwasi said. “But no one has thought it through, that having criminals run our renting stock is a liability.” Punwasi also believes that Prime Minister Justin Trudeau’s government has decreased scrutiny of money laundering in recent years. He points to new data uncovered in a ministerial inquiry from Conservative MP Adam Chambers, who is a proponent of tougher money laundering laws, which found sharp declines in Canadian Revenue Agency audits of FINTRAC leads. “The systemic corruption in housing has been snowballing,” Punwasi said, “to where it's turned into, maybe the banks don’t need to check where the incomes are coming from, and now whole generations can’t find stable shelter.” Adam Chambers, a Toronto-area Conservative MP who unsuccessfully tabled a private members bill on stricter anti-money laundering laws, found CRA money laundering reviews are decreasing. “Delete, delete, delete” HSBC Canada emails reviewed by The Bureau show that while the bank appears to have responded to some of H.M. 's recommendations in 2022, troubling mortgage applications and problems with existing Chinese income loans continued. A January 2023 email to an Aurora branch manager from HSBC Canada’s office in Montreal pointed to a client named Ms. B., who worked at an Ontario government casino, and owned homes across Toronto, in Richmond Hill, Newmarket and East York. Documents show she obtained an HSBC Canada mortgage for $1.26 million in 2016, and that HSBC Canada staff “confirmed” in July 2021 that she was earning $345,000 with a remote work job in Beijing. Despite her incredible claimed income, documents show, Ms. B. was having trouble paying at least one of her three mortgages. An email from a “Senior Loss Mitigation” employee in Montreal to an Aurora branch employee says: “client is going through a tough time … her income is limited … I know she collect rent and she use it to pay her second mortgage. Please review the situation with the client to see if there is any special agreement available to her.” But Aurora’s branch wrote back to the Montreal branch: “What we have told her is … if she really can’t pay, then she just have to put her house for sale … but she doesn’t want to do that.” In an interview D.M. told The Bureau this case was typical. “What they are doing is AirBnBing these properties,” he said. “But they can’t manage with higher interest rates.” He said during mortgage application interviews at the Aurora branch he would often look across his desk and ask questions without letting clients know he was looking at their income claims from purported Chinese companies on his computer screen. “Most of these people don’t even know what type of company is in their job profile,” he said. And documents reviewed by The Bureau show that mortgage applications consistent with Fintrac’s 2023 Chinese money laundering report continued in Aurora. In May 2023, D.M. emailed a senior HSBC Financial Crime Compliance investigator, writing “Just came across two profiles of clients and I have strong evidence these mortgages were also obtained with fake docs and fraudulently.” When the investigator responded “I will take a look,” D.M. replied: “One had a CDA student loan of 10k and making 700k in China. Makes no sense, there are many other anomalies.” In interviews, D.M. told The Bureau he waited “patiently for a year” after reporting his Chinese-income mortgage concerns to HSBC Canada managers, before concluding the bank’s response was insufficient. “This has been going on for seven years and no one spoke up,” he said. “In my first meeting last year, they asked me a lot of questions, like why didn’t you use the normal channels? But I had no faith in the normal channels.” “Many bank staff were obviously involved,” D.M. alleged. “It was not one or two employees turning the blind eye but the entire system, someone verifying those fake offer letters and pay stubs, or their bank statements from China.” D.M. said his concerns also included HSBC Canada’s proposed sale to RBC, which was announced in 2022, about six months after D.M. 's April 2022 internal complaint. The sale was approved in December 2023 by Canada’s deputy Prime Minister Chrystia Freeland. Christian Leuprecht, among other experts interviewed for this story, agreed that D.M.’s allegations of widespread Chinese-income frauds at HSBC Canada could raise questions about whether Freeland, Canada’s finance minister, had knowledge of mortgage lending investigations inside HSBC when she approved the sale. Freeland directed RBC to “establish a new Global Banking Hub in Vancouver,” and “maintain Mandarin and Cantonese banking services at HSBC branch locations,” a Department of Finance statement says. Ultimately, D.M. says he chose to share his story with Canadian citizens partly because he felt pressured to erase evidence from his whistleblower complaint emails. A June 2023 email from the bank’s personnel department says “we hereby demand that you immediately and permanently delete any and all HSBC information on any personal email accounts.” “If you do not comply with these obligations,” the email warns, “HSBC also reserves the right to bring this matter to the attention of relevant law enforcement agencies.” Another July 2023 email from senior management said: “I will reiterate how much we appreciate that you spoke up,” and confirmed that D.M.’s complaints to HSBC Canada resulted in “several enhancements to income verification procedures.” The manager’s email continues, saying “as part of your escalation, you sent confidential information from the bank to your personal email account,” and asserts that D.M. had promised to delete the “confidential client information in your personal possession.” “I could not sleep, and they were telling me delete, delete, delete,” D.M. recalled in an interview. HSBC Canada did not respond to follow-up questions from The Bureau regarding D.M.’s specific allegations. Now he is looking for accountability. “There should be a thorough, retrospective audit of HSBC Bank mortgage deals and eventually all Canadian banks in Canada and stricter regulations when it comes to mortgage approval showing foreign income,” D.M. said, “because there are many skeletons in the cupboard.” Garry Clement, a former RCMP anti-money laundering expert who also contributed to the Canadian academic text Dirty Money, reviewed some of D.M.’s Toronto Method documents, and commented “the best description of the bank’s actions, is willful blindness.” “This is one well-documented example of how banks are catering to Chinese citizens without following strict know your customer guidelines as they would for any Canadian,” Clement said. “We must recognize that a lot of the loans occurred at a time when our political leaders catered to China with a naïve understanding.” Calvin Chrustie, a former RCMP transnational crime investigator whose recent report finds Canada’s weak regulations have made the nation a playground for underground banking linked to organized crime in China, Iran and Mexico, noted that in 2012, U.S. regulators hit HSBC with a $1.9 billion fine because of $881-million in suspicious transactions with Mexico’s Sinaloa cartel and Colombia’s Norte del Valle cartel. “The bigger question in Canada now is why aren’t we looking at what is happening in the banks?” Chrustie said. “We haven’t looked at the complicit actions of financial institutions, while we rely on well-intended entities like FINTRAC, who like police are often restricted in what they can do, or say to the public.” FINTRAC would not confirm or deny whether its recent fines on CIBC for failing to report suspicious wire transactions and a fine on RBC for failing to report suspicious transactions related to frauds, relate to FINTRAC’s 2023 report on Chinese shadow banking and pandemic-era bank account and real estate transactions. FINTRAC also would not confirm whether a Globe and Mail report that TD Bank faces a money laundering compliance fine of more than $10-million is accurate, or relates to the Covid-19 shadow banking schemes. Like the former RCMP experts interviewed for this story, Andy Yan says governments in Ottawa and British Columbia and Ontario are ultimately responsible for ushering mysterious wealth into Canada’s homes. “It’s like, this isn’t entirely new, what you have found in Toronto,” Yan summed up. “When you have programs that are directly meant to domesticate foreign capital into local real estate markets, you start seeing these patterns or these incongruities between incomes and house values. And we have institutions that really are supposed to safeguard us in terms of transparency and accountability.” Meanwhile, a second property owned by Meng Wanzhou in Vancouver attests to the murkiness surrounding Chinese wealth, but also broader concerns of former RCMP experts interviewed for this story, on how and why Chinese state and non-state actors move funds globally. In 2016 Meng, who held at least seven different visas or passports, purchased a $15-million mansion with an HSBC Canada mortgage in Shaughnessy, a luxurious community just outside of Yan’s study areas. Other mansions in Shaughnessy featured prominently in a confidential RCMP study of all homes sold for between $3-million and $30-million in 2016. That police intelligence study found Chinese transnational gangsters and Vancouver Model suspects linked to over $1-Billion in property purchases in 2016. Ironically — in the case that disrupted Canada’s warming relations with China under Justin Trudeau — Meng finally admitted to U.S. government financial fraud charges that she misled HSBC about international transactions, and “Meng and her fellow Huawei employees engaged in a concerted effort to deceive global financial institutions, the U.S. government and the public about Huawei’s activities in Iran.” sam@thebureau.news The Bureau is a reader-supported publication. To receive new posts and support my work, consider becoming a paid subscriber. Subscribe 56 Share this post \"Fake Chinese income\" mortgages fuel Toronto Real Estate Bubble: HSBC Bank Leaks www.thebureau.news Copy link Facebook Email Note Other 13 Share",
    "commentLink": "https://news.ycombinator.com/item?id=39277767",
    "commentBody": "\"Fake Chinese income\" mortgages fuel Toronto real estate bubble: HSBC bank leaks (thebureau.news)412 points by eswat 16 hours agohidepastfavorite347 comments ThisIsMyAltAcct 15 hours ago> The whistleblower, whom The Bureau is calling D.M., immigrated to Canada as an international student from India, making him a minority among mostly Chinese-Canadian co-workers at the Aurora branch. > “I am going to reveal potential mortgage fraud at HSBC Bank Canada and possibly some employees benefited from the fraud, financially pocketing thousands of dollars, which I call the proceeds of crime.” > FINTRAC’s study doesn’t say that Canadian banks knowingly issued fake-income mortgages to Chinese diaspora buyers in Toronto. But in an interview, D.M. said banking staff are trained to guard against fraud, and the loan application packages he reviewed in Aurora beggared belief. > The Bureau’s review of HSBC Canada emails and D.M.’s text messages, shows he came to believe numerous employees at the Aurora branch had direct knowledge of faked Chinese income mortgages, and a veteran manager with oversight of more than 10 Greater Toronto branches knew about broad and questionable mortgage lending for Chinese diaspora clients. > Pointing to specific examples, D.M. claimed that another branch colleague had admitted processing numerous loan applications without meeting his clients, because a branch manager delivered her subordinates foreign income client applications so “they did not have to get sales themselves.” > “She said yes, she knows specially in Mainland China there is a team who would even answer emails and phone calls verifying [Chinese income] but it’s a sophisticated and well organised scam,” D.M. 's email to HSBC Canada managers says. [...] “When I asked for such a serious issue if she raised a HSBC confidential [complaint] or not she evaded my question,” D.M. wrote. “Now we all love numbers, but I don't think the bank will like these kinds of numbers achieved through this way.” Sounds like that branch is compromised reply topspin 13 hours agoparent> Sounds like that branch is compromised \"Since 2015, the whistleblower concluded, more than 10 Toronto-area HSBC branches had issued at least $500-million\" It's not a branch. It's the whole bank. And you can safely infer it's not the only bank. reply throwaway907628 9 hours agorootparentWitnessed this first hand at RBC when my brother from Hong Kong was inquiring about getting a mortgage in Toronto since he wants to move back to Canada with his two young kids. We found a random mortgage specialist (a Chinese lady) at RBC. Without knowing the full picture of my brother's income situation, she immediately suggested that she could get the mortgage approved regardless, just needed to fake some documents. It was astonishing how she went straight to the point so quickly to someone she met the first time. She also said a lot of people has non-taxed income and needed a way to get a mortgage before the real estate price becomes out of reach. So definitely not just HSBC. reply bonestamp2 12 hours agorootparentprevDon't get me wrong, it's definitely a problem, and something needs to be done, but lets also try to keep it in perspective... $500 million could well be 500 mortgages in Toronto. That would be about 0.02% of the private dwellings in Toronto. reply simmerup 10 hours agorootparentRemember this is $500 million awarded to people out competing honest Canadians looking for housing who aren't lying about their incomes. If 10 people bid for a house, it's the guy in China lying about their income setting the house price when they win, by what HSBC is doing. This will have an outsized effect on the market. reply maxglute 8 hours agorootparentExcept HSBC knows 99/100 of the time, those bidders in PRC are actually wealthier bidders and that the irregular paperwork is perfunctory to the fact that the system has spend decades building loopholes for said wealthy PRC buyers with developed methods of capital flight, which has always made them better candidates than \"honest\" Canadians. Occasional fraud doesn't have outsized effect to the very real fact that there are simply more wealthy PRC buyers than Canadian ones, and sellers/bankers/ everyone who gains in the transactions wants it that way. Honest Canadians with 200k household income and 10% down can't compete against PRC buyer with millions of slightly illiquid assets. These PRC buyers are the same as rich Indian/Iranian/insert elite from other country buyers, there's just a shit load more of them and they have to jump through more hoops due to capital controls on PRC's end which Canada RE system is all too happy to ignore. reply PeterisP 8 hours agorootparentprevIt's $500m that went to \"honest Canadians\" selling their real estate. The house isn't gone, it's still there, and will probably get sold to Canadians when the bubble pops, possibly much cheaper. reply fennecbutt 7 hours agorootparentAhaha, the bubble popping. That'll be the day. All they're doing is increasing house prices for no reason. Houses are now just casino chips for rich people. reply PeterisP 7 hours agorootparentOn the other hand if it doesn't pop, then it literally doesn't matter if the income documents were fake, as the bank has good collateral and giving the loan turned out not to be a mistake. reply medvezhenok 4 hours agorootparentFraud is fraud, so it does matter. Same way that SBF is still going to jail even if the people with money in FTX get their money back (via crypto rising again / the AI investment). The real people harmed were the Canadians trying to start a family who got outbid on the house. reply xyzzy_plugh 6 hours agorootparentprevNow you're thinking like a bank. reply antisthenes 5 hours agorootparentprevOh, and think of the tax revenue! ;) reply fennecbutt 7 hours agorootparentprevNZ has this problem but with real buyers regardless. Chinese citizens evading laws as much as they can to buy houses and flip them, sometimes hours later. Laws like \"you must live in the house for 1 year before selling\" meant they just sent a relative to live there free before selling for higher price. Like investors and companies chasing ever higher profits, there's a bunch of people that think the house they own should be worth 100000% more by the time they sell it. Even had a gen x friend be like \"Well I pay rates (council services fees) while I own the property so it should be worth more!\" Bruh, getting your rubbish taken away while you live there doesn't add value to the fucking house. Otherwise I could claim paying my electricity bill while I live there does too. reply cyanydeez 9 hours agorootparentprevyes, remember it's capitalism doing this, not Chinese, Indian, Canadian or American. hail free market. reply techbro92 9 hours agorootparentThis is not an inherent feature of capitalism. This is a failure of the government to create a fair market. This is caused by people that are subverting the rules to out compete the people playing by the rules. reply porknubbins 5 hours agorootparentI have seen a lot that smaller, high trust places where people worry about long term reputation, status etc are not good at competing with large immigrant populations who do not have any stake in maintaining a reputation in the community. reply yukkuri 8 hours agorootparentprevTell that to the capitalists always protesting that there's no need for regulations reply AllegedAlec 56 minutes agorootparentprev> capitalism is whatever I feel is bad about it > bad actors define capitalism Sure thing bud. reply vineyardmike 8 hours agorootparentprevCapitalism doesn’t have to mean an unregulated free market. That’s one flavor of capitalism. reply Rehanzo 10 hours agorootparentprevIt says \"at least\". It's also looking at specific branches of one single bank. Who knows the true extent if all banks and branches are considered. reply bonestamp2 10 hours agorootparentYa, fair enough, and some Toronto mortgages are more than a million too. reply radicaldreamer 14 hours agoparentprevAre US banks just a lot more strict about source of income than Canadian banks? reply rconti 10 hours agorootparentTFA says: \"The official noted that other nations require tax agencies to verify incomes for mortgages, which isn’t the case in Canada.\" I don't know if it's a legal requirement, but I sure know I had to hand over all of _our_tax information to the bank to validate income when getting a mortgage in the US. You have to sign a form saying the bank is allowed to pul your data from the IRS; you're not just handing over paperwork and promising that it's legit. reply pxmpxm 10 hours agorootparentThis - not sure where the sentiment in this thread is coming from, but you have to disclose your tax filings for the last 2/3 years along with all your bank accounts and recent statements. reply dghlsakjg 13 hours agorootparentprevTotal conjecture: 1. US banks do not face nearly as diverse a set of applicants, and 2. are only required to hold the loan for 5 years. 1. It is very common in Canada for a person with wealth acquired outside the country to apply for a home loan. At the time I was approved as a guarantor for a home loan for over half a million CAD, I had only been in the country for 2 years, and had no credit history with any Canadian institution (out of laziness I just was added to my wife's accounts as a signer and cardholder when I moved). They accepted copies of my American credit history and bank statements, but had no real way to verify their truth. In the US, I don't think that (relatively) wealthy immigrants wanting a home loan are nearly as common. Richmond, BC is a great example of this: avg home price is 1.5mm and 60% of the residents are immigrants. 2. Canadian mortgages are refinanced every five years, traditionally (it is possible to get a longer term, but very uncommon). Combine this with the fact that Canadian real estate has ALWAYS gone up (until now), and financing a home really wasn't a risky thing. If a bank didn't like a customer, they could refuse to refinance after 5 years. If a bank foreclosed, they were basically guaranteed to be made whole. reply seanmcdirmid 11 hours agorootparentI got the feeling that the HSBC in Bellevue Washington catered primarily to overseas Chinese clients looking to buy homes in the Seattle area. Most of the employees spoke some dialect of Chinese. I have no idea about the loan officers, however. reply pchristensen 10 hours agorootparentBellevue's population is ~14% Chinese, over 21,000 people, and mostly high income tech salaries. It would be bad business not to have Chinese speakers on staff. reply FireBeyond 6 hours agorootparent> 14% Chinese, over 21,000 people, and mostly high income tech salaries. It would be bad business not to have Chinese speakers on staff. What language do these Chinese tech workers speak at their jobs? reply danielscrubs 3 hours agorootparentYou can have a tech vocabulary but not know what a secured property is for example. Maybe a non native would know the word secure and the word property and make an erroneous assumption. It’s probably the biggest purchase in your life so it makes sense you want to know all details. reply pchristensen 6 hours agorootparentprevIt depends on the composition of their teams, but probably something different than their most comfortable, native language. reply randomdata 3 hours agorootparentprev> Canadian real estate has ALWAYS gone up (until now) Have always gone up expect for when they haven't. Nationally, real estate prices dropped precipitously in the early 1980s. And the Toronto housing crash of 1989 was a complete meltdown. It took until the 2010s for prices to finally return to where they were in 1989! reply raydev 12 hours agorootparentprev> They accepted copies of my American credit history and bank statements, but had no real way to verify their truth I'm skeptical here, given how closely the US and Canada work together, both US and Canadian banks share an incredible amount of info with each other and not solely because of cross-border commerce. There is also a non-trivial number of US citizens living and working in Canada so there are services available to them given their special tax requirements. reply dghlsakjg 11 hours agorootparentIf you have information that contradicts what multiple banks have told me I would love to know it, since it would be very nice to have that information available to my Canadian bankers. In the 3 banks I've worked with in Canada, all were completely unable to access my American credit history. The governments do share tax data, but AFAIK the banks have no way to link \"John Smith SSN:123-45-6789\" to \"John Smith SIN:098-76-54321\". They even have my US SSN number since Canadian banks report to the IRS. Edit: here's experian explaining it: https://www.experian.com/blogs/ask-experian/u-s-credit-histo... reply joshuabaker2 4 hours agorootparentYes this is accurate. I work at a lender that operated in both Canada and the US and the credit bureau integrations were with totally different companies and had different APIs (even for Equifax on both sides of the border) reply JumpCrisscross 13 hours agorootparentprev> Are US banks just a lot more strict about source of income than Canadian banks? American financial regulators are much more comfortable letting banks fail than their Canadian or European counterparts. (In part this is because of the sheer diversity of banks we have.) Being fined out of existence is a real possibility for an American bank. That shapes behavior. reply joshuabaker2 5 hours agorootparentprevYes, but it’s partly the government’s fault. There’s literally no way for mortgage lenders to pull your tax records in order to verify income. There used to be some third party services to connect to the CRA but they got shut down and replaced with… nothing. reply bonestamp2 12 hours agorootparentprevNo. I've purchased homes in both countries and from what I've seen, US banks are far less strict. Policies, procedures, and operations are very undisciplined at many US banks (compared to Canadian banks). I have noticed some are starting to become more strict in the past 5 years though. reply cbsmith 13 hours agorootparentprevShort answer: no. reply mistrial9 13 hours agorootparentpreva non-trivial portion of residential real estate transactions across the USA never apply for a loan. In certain areas it is more so. When real estate retail value rises fast, money appears from everywhere -- hint, not from first time home buyers. The USA Federal system of mortgage loan gurantees has been gamed seriously, over and over since the 80s. It is a whack-a-mole for enforcement. All the parties close to the transactions have exactly the wrong incentives, most of the time. One of the defendants in a recent \"pay cash to get your kid into elite school via fake sports\" scandal was a mortgage broker in San Diego County. The Judge after reviewing evidence, reportedly told the man on the record \"you are a thief.\" etc reply hoseja 1 hour agoparentprev>Sounds like that branch is compromised By all accounts it seems the whole country is. reply ajkjk 5 hours agoparentprevYour first thought is to.. assume the bank is probably misunderstood? reply cyanydeez 9 hours agoparentprevupstream, the problem is _still_ the commoditization of housing markets. it's not like they're claiming these people are unable to pay. reply SunlightEdge 14 hours agoprevTo maybe offer a different perspective: I think the Canadian mortgages linked to Chinese accounts will likely all be paid. What may be happening is that there is a lot of underground chinese financial activity that is not recorded in Canada and part of this 'network' is utilized to get money out of china. reply avidiax 13 hours agoparent> I think the Canadian mortgages linked to Chinese accounts will likely all be paid. This is a \"heads, I win\", \"tails, you lose\" type of scam. The mortgage holders are all judgement proof. They have no income or assets to go after. So if the housing market crashes, the banks have no recourse. It's the same as taking out a mortgage and instead of buying a house, you go to the casino and bet double or nothing. Sure, the intention to pay back is there. But it is contingent on the investment performing, and the bank is taking on unknown risks. reply gscott 12 hours agorootparentThere is probably an unlimited number of people in China would would like to own Canada real estate. As long as you don't block those sales it can continue forever. reply joshuabaker2 4 hours agorootparentprevMortgages in Canada are different than mortgages in the US in that they are full recourse. If the sale price during foreclosure doesn’t cover the costs, the banks can go after you personally for the balance. So you’d have to either a) leave the country, or b) declare bankruptcy. So, not exactly a risk-free option. reply raydev 12 hours agorootparentprev> So if the housing market crashes, the banks have no recourse The housing market must first crash before the problem is tangible, and there's no sign of that happening. reply shutupnerd0000 10 hours agorootparentYes that's exactly what the person you're replying to said reply uLogMicheal 13 hours agoparentprevIt's not a matter of if they get paid, it's the unfair advantage this gives in an already competitive market. 2/3 of these properties are probably rented out at inflated prices and the two probably pay the mortgage of the third owners live in. This is a free money glitch, aka fraud. reply jeffbee 13 hours agorootparentIf the market will bear the rent, why does changing the nationality of the owner improve anything? reply rybosworld 12 hours agorootparentThe market's not bearing rent so much as existing property owners are colluding to prevent new construction. If new construction wasn't so aggressively blocked in some major cities (San Francisco, Boston, all of Canada it seems), then the rents would not be nearly as high as they are. Wealthy property owners are behaving a lot like a cartel in many places. reply jeffbee 11 hours agorootparentExactly. The discourse about foreign buyers of homes in Canada is centered on the morally bankrupt notion that it is only wrong if that race exploits the system. If a good old white guy exploits the same system it is not worth mentioning. What I am saying, and you seem to concur, is that the system itself is the problem. The identity of the person exploiting it is irrelevant unless you are a racist. reply shutupnerd0000 10 hours agorootparentYou're the only one fixated on race here. The distinction between foreign and local buyers is important to the discussion. reply jeffbee 10 hours agorootparentIn what way? reply NBJack 8 hours agorootparentIt is a legal distinction that has a huge number of implications in most countries. As is the focus of discussion on other threads, it necessarily changes approaches to income validation, the laws that apply, the presence or absence of credit history, etc. reply jeffbee 8 hours agorootparentYou're just arguing about whether the foreigner is qualified, bonafide enough to operate the grift. reply gruez 10 hours agorootparentprevracism might be canceled these days, but nativism is alive and well. reply uLogMicheal 12 hours agorootparentprevThis has nothing to do with nationality and everything to do with fraud. If other nations are doing this, it should stop too. Telling lies to acquire loans is illegal and inflates prices for everyone working legitimately. reply jeffbee 12 hours agorootparentYeah but you seem to be suggested that without this yellow peril, the tenants would be, for some reason, getting a better deal. As if the problem is actually that Chinese people are better at price finding. reply fennecbutt 7 hours agorootparentprevNationality doesn't matter, but a high % of profits gained leaving the local economy does. This is why trade with known tax havens should be banned, because no tax havens have large markets to capitalise. It's such a racket. reply cscurmudgeon 8 hours agorootparentprevWhy does nationality matter here? The core issue is fraud due to nationality not nationality itself. reply mtalantikite 14 hours agoparentprevTangentially related, there was an undercover Vice News report on the connections between Chinese Triads and the Mexico/US fentanyl trade a couple months ago [1]. I also wouldn't be surprised if there were underground networks of capital in Canada that were related to these mortgages. [1] https://www.youtube.com/watch?v=E8wEGVIPJ_4 reply JumpCrisscross 13 hours agoparentprev> think the Canadian mortgages linked to Chinese accounts will likely all be paid Out of curiosity, why? China's stock market is melting down in the midst of persistent deflation. A lot of people who thought they had liquidity may not anymore. Beijing could open the taps, but then that puts pressure on the currency. reply cbsmith 13 hours agorootparent> Out of curiosity, why? China's stock market is melting down in the midst of persistent deflation. A lot of people who thought they had liquidity may not anymore. Beijing could open the taps, but then that puts pressure on the currency. The whole point of the money laundering operation is to get the money out of the country. China going to pot only accelerates it. reply JumpCrisscross 11 hours agorootparent> China going to pot only accelerates it You have to have money to launder it. Also, if the currency keeps getting hammered, Beijing will crack down on the exit channels. reply alephnerd 8 hours agorootparentMost Chinese don't invest in their stock market. The accredited investor qualification is much harder to get than in the US. Most investment is in real estate, which has remained fairly stable in Tier 1 cities (which is where most of the post-2008 Chinese Canadians are from) To get more than the $50k limit out, people would use a hawala type system where you'd use assets in China as collateral and get guaranteed cash from a broker abroad. reply JumpCrisscross 8 hours agorootparent> Most Chinese don't invest in their stock market Most Chinese aren’t buying British Columbian property. > accredited investor qualification is much harder to get than in the US On a relative basis, right? In absolute terms, it’s still very low. Similar to the practical requirements for opening an American brokerage account. > Most investment is in real estate, which has remained fairly stable in Tier 1 cities Do you have data for this? My impression was sales are being discouraged. reply silent_cal 14 hours agoparentprevIt's still fraud reply jabbany 14 hours agorootparentYou can label it however you want, if both the lender and borrower are willing participants, it will be difficult to prevent this from happening. Like mentioned in the article, often times the material is very obviously suspicious and banks probably know this and still turn a blind eye to it because these borrowers are low risk and much less sensitive to the high/rising interest rates of today... reply ak217 13 hours agorootparentIt's not that difficult. You just appoint an auditor and make the bank pay progressively higher fines until they figure it out. American banks learned to be much better at it after 2008. And given 2008 and the MBS balance sheets at central banks and the municipal budgets propped up by property values and national mortgage programs intended to encourage homeownership, this is by no means just a matter between the bank and its clients, even if you put aside the money laundering angle. Mortgage fraud destabilizes economies. reply topspin 13 hours agorootparentprev> if both the lender and borrower are willing participants Eventually this behavior goes overboard and everything crashes. In the meantime, law-abiding people are screwed by the bubble. Then they are made to pay for the clean up. Fraud is costly, and rationalizing it contributes to the problem. reply maxglute 12 hours agorootparentBut the \"fraud\" is slowing the bubble. Without capita controls, PRC buyers would operate like any other international buyer, except there would be a shit load more of them who would do more cash purchases and outbit everyone else. This fraud to circumvent capital controls is why wealthy PRC buyers who liquidated their extra multi million dollar tier1 units only bought 1 house in Canada instead of 3. reply FireBeyond 6 hours agorootparentThe article discusses how they're not just buying one home outright. Far more profitable to put downpayments on 3 or 4 homes and just push that money to pay those mortgages while AirBnBing and renting a couple of them. reply maxglute 3 hours agorootparentThe article discusses one incident of person with Canadian tax residency with multiple mortgages, where the leaker allege is typical, when number of net worth assessments in low double digit per year suggest it's not typical. Not with 30k new Chinese Canadian immigrants per year, or 150k home sales in Ontario. From my experience (I know many people who facilitate this process in ON/BC) PRC buyers typically are not renters / airbnbers, they either live in their units part time / full time (or their kids) or leave it empty. Renting/slumlording/airbnbing is largely Chinese _Canadian_ affair, emphasis on Canadian, i.e. not wealthy, middle class who needs to make their capital work to cover costs. They speak mandarin and can insert themselves into the machine. Generally PRC buyers / big bidders aren't playing the RE game in Canada to play landlord. They just want to hide some of their assets abroad. The allegation kind of skips over that distinction, are these \"diaspora\" PRC individuals or Chinese-Canadians with access to PRC RE networks doing ponzi mortgages to try to get wealthy off Canadian RE. In my experience, it's the Chinese Canadians, because the PRC buyers were busy speculating off much more profitable (at the time) PRC RE, especially if whistleblower is talking about 2015-now time period. reply jabbany 13 hours agorootparentprev> this behavior goes overboard and everything crashes Maybe... but you need to keep in mind most of these people are not really building a bubble. Unlike the subprime mortgage crisis, where things were built on inflated valuations, many borrowers in this \"scheme\" do have more than enough funds to cover the entire mortgage. It's just that their capital is relatively illiquid. This is also why the high interest rates have not significantly affected this. The effects on housing cost is because of natural market merging where chinese properties are \"overvalued\" domestically. This is actually not new, and happened with Japan at some point as well. That being said, the main risk for this is actually geopolitical... Should capital controls tighten (or, like, if war were to occur etc.) then there is a much bigger risk, but many are banking on the fact that, at least given the signs today, that is still unlikely. reply topspin 4 hours agorootparent> Maybe... but you need to keep in mind most of these people are not really building a bubble. No, I'd don't need to keep anything of the sort in mind. I've lived through multiple real-estate and speculation bubbles and crashes now. The arguments you make are the same sort heard before each one, and I can easily anticipate the rationales and excuses that will be offered after the next one. You don't know how widespread this is. You don't know how many other banks are leaning on this latest house of cards, or how much of this is going on in the US and Europe as well. As far as the banks are concerned it's just one big world of suckers and they play these games everywhere, simultaneously. And there is no \"should.\" Capital controls will tighten. Wars will happen. Eventually, inevitably, the overhang destabilizes and this heinous crap will blow up. Again. reply JumpCrisscross 13 hours agorootparentprev> You can label it however you want, if both the lender and borrower are willing participants, it will be difficult to prevent this from happening These aren't purely private transactions. If HSBC Canada fails, Ottawa is on the hook. The defrauded party here is the public. (And possibly the bank's lenders and shareholders.) reply jabbany 13 hours agorootparentThis is somewhat counterintuitive but... the fraudulent mortgages are not more risky, they are often times more stable than other local borrowers. I think what many people are imagining is the subprime mortgage situation of yore. But in this case, a lot of the \"fraud\" is the result of knock on effects from capital controls in the PRC. Many (new and aspiring immigrants) have capital from sales of their property in China, but due to capital controls, cannot get it out quickly. They have to do it in $50k/year chunks. Usually a loan or mortgage is the solution for this, but those depend on _income_ rather than _wealth_, so normally these people can't take out as much as they need to, even though they could easily back actual value of the mortgage. So there's a little collusion between banks and mortgage brokers to get in on this market gap (probably more so now that interest rates are high, which these borrowers are much less sensitive to). Of course, there are risks, but those risks are tied to more geopolitical circumstances and less market-driven, and apparently banks are more willing to take their chances on that. reply kasey_junk 6 hours agorootparentThe issue here is that the geopolitical risks are hard to separate from the market risks. If BC property is being fraudulently leveraged against Chinese real estate, opaque decisions by the ccp can dramatically impact default rate for Canadian loans. No market actor would expect that in a non-fraud based market. Instead a transparent pricing of Chinese assets would show them as much less valuable on a risk adjusted basis than their book wealth value. Especially compared to western income or equivalent wealth. reply JumpCrisscross 11 hours agorootparentprev> the fraudulent mortgages are not more risky, they are often times more stable than other local borrowers You don't know. The paperwork's fraudulent. > Many (new and aspiring immigrants) have capital from sales of their property in China, but due to capital controls, cannot get it out quickly The Chinese property market is in freefall. And capital controls can get tightened. Either condition will result in default. reply jabbany 9 hours agorootparent> You don't know. The paperwork's fraudulent. They don't offer these services to anyone. Because the paperwork is fraudulent, a lot of people involved are/will be personally implicated (could easily lose their job and/or face legal challenges on top) in the scheme. It's not like banks are not monitoring delinquency/default rates already, and if the stats are start indicating problems they will certainly investigate... So while outside observers can't verify anything, those perpetrating the scheme do have to balance their own personal risk and many will in exchange request invasive details around the clients' assets in China to cover their own ass. Not admissible evidence to the bank, of course, but they're not handing these out like candy. > The Chinese property market is in freefall. Realistically, people involved have already sold so this doesn't affect them. At least in the Vancouver area, the brokers (who are the usual point-of-contact to the clients) won't even proceed unless you've already sold and have the cash. > And capital controls can get tightened. This is the main real risk that those in the scheme look out for, but it's a geopolitical risk rather than a market-based one. Which makes more sense when rates are high, like now. When rates were low, this didn't happen as much since there are plenty of clients to go around. --- Also, in the grand scheme of things, even if the bubble bursts, the broader economy is still not worse off. Each cent paid into these mortgages is real \"new money\" being introduced into the economy. This is not the subprime mortgage days where at the end it became just a transfer of wealth to the banking industry. For the most part \"the public\" is not the one being defrauded, it's China... reply JumpCrisscross 8 hours agorootparent> while outside observers can't verify anything, those perpetrating the scheme do have to balance their own personal risk Everyone in every corrupt scheme says this. The rule of law wins, in the long run, because these structures aren’t robust. They get perverted and subverted, and while it’s nice to imagine a bunch of competent crooks keeping up their shop, the reality is we have rules for a reason. > it's a geopolitical risk rather than a market-based one Capital controls aren’t geopolitical. Neither is an offshore property market bursting. The borrowers are borrowing against an doubly-illiquid asset. Buy long, borrow short—this has been a widowmaker since antiquity. > even if the bubble bursts, the broader economy is still not worse off Canadian banking would collapse. You’d see the equivalent of America’s 2008 crisis, except while the rest of the world has high rates. If allowed to fester, or if it already has, that’s a generation’s quality-of-life gains going down the tube. reply jabbany 7 hours agorootparentI agree with all the points on top. The proper instrument to do this would be banks setting up a system that lets people borrow against an illiquid asset, in this case would be CNY, but it's not anything new... (and is one proposal for how to work with cryptocurrencies). That would price in the political and market risks. --- The last one I don't agree with. This is different from the 2008 crisis in that the 2008 crisis was primarily \"internal\" and for the most part zero sum --- some people gained, some people lost (kind of loosely like a long-horizon pump-and-dump scheme), and at the end things revert to the original non-inflated value. This situation is more of an encouragement of external injection _into_ the economy. Rising prices are due to external capital flowing in (and the anticipation of more to come). Even if it were to pop, things would be no worse than a hypothetical alternative timeline where there was no bubble. And that's assuming no external capital actually flowed in, that not a single person wired money into the country. Clearly this is not true, and the money coming in is still net positive. So _in aggregate_, the economy is still improved due to the injection. Again, these gains are not spread evenly, and it may be the case again that some individuals will be hurt while others reap large returns. > a generation’s quality-of-life gains going down the tube If anything, that just means the previous quality-of-life gains were achieved by overdrawing against the future... nothing new here. reply fennecbutt 7 hours agorootparentprevWilling participants huh. Was the taxpayer a willing participant when we had to pay for the monumentous fuck up of 2008? Did the banks and investors pay for it, go to prison? Pretty sure it was just one scapegoat and that's it. reply PeterisP 6 hours agorootparentThe 2008 bailouts were all loans and taxpayers made a profit on them as they got repaid - see https://money.usnews.com/investing/articles/2017-01-19/finan... reply FireBeyond 6 hours agorootparentWhere do you think the banks got the money that enabled them to repay the loans? reply PeterisP 6 hours agorootparentFrom the mortgages, just as they should, and just as the borrowers (i.e. \"the willing participants\") deserved to pay - it's not a burden on some unrelated taxpayers. reply cm2187 12 hours agorootparentprevI won't throw a stone at anyone trying to circumvent chinese capital controls. Though Canada isn't the place I would go to escape financial repression. reply logicchains 14 hours agorootparentprevIt's a small wrong to right the much bigger wrong of the tyrannical Chinese government preventing people from taking their money out of the country. reply dddddaviddddd 11 hours agoparentprev> I think the Canadian mortgages linked to Chinese accounts will likely all be paid. If Canadian banks agree with this risk assessment, they have little incentive to actually verify income in these cases. reply neilv 15 hours agoprev> But more than a year later, D.M. was so dissatisfied with the bank’s response that I only recognize the HSBC name from scandals in the news: https://en.wikipedia.org/wiki/HSBC#Controversies reply saiya-jin 15 hours agoparentLook at any wiki article on any major global bank, the chapter about 'controversies and legal issues' is always a thick list, HSBC ain't worse or better than others. There are no good guys there, that's not why the business was set up and corresponding folks were/are hired. If you want more controls, enforce more regulations, they do work if well defined. reply jacquesm 15 hours agorootparentHSBC seems worse than many others. When I moved to Canada my immigration lawyer explicitly advised to stay away from HSBC, this was in 2000 or so and they already had a pretty bad rep. The various scandals since then haven't improved that reputation. TD, CT and RB have their own problems but none of them have received even close to the total fines that HSBC has (to the best of my knowledge). I agree there are no good guys here, but there are shades. reply radicaldreamer 14 hours agorootparentThey started off as an opium bank, that heritage and culture still pervades their business practices. If you want to read more about this: https://philebersole.com/2013/02/15/hsbcs-history-and-the-or... reply neom 14 hours agorootparentprevI've been using HSBC in Canada and the USA for 15 years, they're great for exactly the reasons they shouldn't be. Their tooling basically lets you do whatever you want with no oversight. It's kinda weird, but I liked it. Sad they sold their Canadian business to RBC (even though they sold their USA business to Citizen Bank, they allowed high net worths to stay but the Canadian arm did not, wondering if this news is the reason for their exit) reply hiatus 11 hours agorootparent> Their tooling basically lets you do whatever you want with no oversight What do you mean by this? Maybe some examples would help to clarify. reply jacquesm 1 hour agorootparentI can give you one: one guy I met while I lived in Canada had a scheme going where they got people to sign up for a service using a credit card and then they also signed them up for a bunch of unrelated very low monthly fee services that automatically renewed and/or unrelated one time charges marked as 'donation to some charity'. HSBC was happy to shield the company from VISA and MC for the longest time because they made money on it. This was pretty much a clear case of theft and without HSBC cooperating I really doubt it would have worked. reply JumpCrisscross 15 hours agorootparentprev> HSBC ain't worse or better than others Money laundering does seem to be their choice in poisons. reply emmanuel_1234 14 hours agorootparentThat's because they are one of the most global bank, therefore a prime choice when it comes to moving money from countries to countries. Most bank have a much smaller global footprint and can't be used as such. Banks don't benefit from their customer laundering money just like landlords don't benefit from drug trafficking in their building: it's a hindrance and it costs a lot to do anything about it. Source: I work on AML in a global bank. reply JumpCrisscross 14 hours agorootparent> they are one of the most global bank By what metric? I’d argue the driving factor is their proximity to dirty money. Same with Russian banks. Then other people notice you’re used to looking the other way and you get word-of-mouth network effects. With money launderers. reply xurbax 10 hours agorootparentprevDon't they? Surely they make some interest on the money if it is parked for any length of time? Plus transaction fees and such? And individuals probably get bonuses etc. based on volume in some way? (IANAB(anker), just wild guesses here.) reply Terr_ 15 hours agoprevIANAFinancialInvestigator, but skimming through it sounds like: 1. Fraudulent applicants come to the bank with crazy stories to ask for enormous loans/mortgages toward a Toronto house, allegedly to turn hyper-suspicious big piles of cash into a more reputable-looking asset. 2. HSBC goes along with that because they want to suckle on the sweet regular payments of suspicious cash, even though they ought to damn well know that these customers are just a front for an organized crime ring. 3. As a bonus, this locally-concentrated money-laundering/speculative-investment thing screws up the property market for Torontonians. The local multimillionaire babysitter is willing to buy at almost any price because their secret financial goals are very different than yours. While looking for other articles, I notice it's been ~16 months after the end of HSBC 10-year tangle with US regulators over their business with Mexican and Columbian drug cartels. [0] [0] https://www.reuters.com/business/finance/us-fed-terminates-e... reply seanmcdirmid 15 hours agoparentA lot of it doesn’t even sound like money laundering, just fraud. You say you have a job paying you lots of money from China, verification is loose, you get the loans, and then try to make mortgage payments via Airbnb. The risk is mostly with the bank, and if it doesn’t blow up you make all the money. reply Terr_ 15 hours agorootparentWhile that may happen too, the article alleges the mortgage payments are being made with funds wired from China. If the borrowers are making the mortgage via rent/Airbnb of the properties... then they are somehow keeping it secret within Canada and also sending it on an international round-trip, which seems like a strange stretch for any small-time \"lie on the loan application\" crook. reply seanmcdirmid 15 hours agorootparentThe article states more cases than that. One of the examples the mortgage payments aren’t being made via Chinese wires, they are being paid locally and the Chinese income doesn’t exist at all. They could be making round trips with Chinese banks but I don’t see why. You can transfer funds from China into your account before your monthly mortgage was due, the mortgage provider would never know. You can also put dollars into a Chinese bank account and do wires in demand, since it isn’t R!B there are no controls on it. reply opportune 15 hours agorootparentprevBanks these days often don’t hold on to those mortgages though. They can repackage them as securities like MBS and sell them to entities like pension funds. This type of thing is exactly how the GFC started reply cm2187 14 hours agorootparentExcept that the losses are rarely passed on to investors. The issuer (the bank) typically retains first loss (that changed after the 2008 crisis). So it's typically more a tool to get cheap funding than selling mortgages. reply koolba 15 hours agorootparentprevThere used to be a thing called a “down payment” that was supposed to cover things like this. It would force you to have a modicum of equity in the house and give the bank a 20% buffer on the price to break even after a short sale. reply kurthr 15 hours agorootparentprevHow could you lose money? RE prices only go up... as long as there's a greater fool... oh, wait?! reply DeRock 10 hours agoparentprevThis doesn't necessarily even need to have a laundering angle. I have a different take: 1. They come with some money, enough for whatever minimum down payment is required (but notably, the issuing bank will also lower down payment requirements for clients with high income or assets, which are faked anyway) 2. HSBC is incentivized to issue mortgages, yes, that's their business. But the actual fraud here sounds more like cash kickbacks from the fraud buyer to the issuing agent themselves. 3. Home prices are made on the marginal sale, so a small amount of this activity can have a large upward pressure on prices. This leads to typical bubble scenarios, and you can keep rolling over or refinancing mortgages as prices keep rising (or even just sell). To get a sense of this, over the past few years, the average Canadian house gained in price something like double the average Canadian income. So in summary, a lot of this could be explain by plain fraud, enabling foreign buyers to both perpetuate, and participate, in a giant housing bubble. If that's true, and it all comes crashing down, then god help us all. reply smashed 6 hours agorootparentPeople have been waiting for decades for the Canadian housing market to tumble. There are so many Canadians waiting on the sidelines with their down payment ready to be deployed and are out competed year after year. The pent up demand for housing is enormous. Even with the high interest rates. Hell, with the rumors of interest rates going down, there is a frenzy to BUY now, suck up the high interest and get a relief in a few years, because people are afraid that if the rates go down, more people will qualify for a mortgage and it will bring more pressure, so better to buy now if you can. I am convinced it will not go down in my life time ever. Too much demand, not enough supply. reply vondur 15 hours agoparentprevHow many fraud issues has HSBC had over the years? Why are they allowed to still do business in the US and Canada? reply raverbashing 15 hours agoparentprevThis is not really news, this is certainly not exclusive to one bank or another I remember some years ago I saw a comment somewhere that, for real estate purchases \"proceeds from a certain country were subject to AML regulations... but not from China\" Really. I can imagine a Canadian banker saying that with a straight face. reply beiller 15 hours agoprevAll sounds very plausible, but where are the effects of this? We should be seeing many people holding mortgages at HSBC not able to pay. Are there no public stats showing how many lack of payments being made to HSBC? Is HSBC going to hold on to these properties taking massive losses? For how long? It has definitely helped the run up of prices here. It will also help the collapse of prices as well, either that or the collapse of HSBC. Maybe the effects take a very long time to manifest. Lets hope it's not too long :) reply timr 14 hours agoparentThe article suggests this is money laundering. It would make sense to have fake borrowers with fake incomes as part of a layering operation. Someone wants to get $large_sum out of China. They can't do this without raising lots of flags in both countries. So they set up an army of fake borrowers, have them take out fraudulent mortgages on real properties in Canada, pay down the mortgages, and sell the property to obtain clean money on the other end. All the better if the property rises in value in the meantime due to enormous fraud. reply stevenwoo 14 hours agorootparentNot Canada, but before the pandemic caused rural housing prices to go up, there was a crime syndicate buying houses for marijuana grow operations and having the house pay for itself essentially until they got caught. https://www.cbsnews.com/losangeles/news/doj-raid-marijuana-g... reply avidiax 13 hours agorootparentprevThe article doesn't seem to suggest that the mortgage holders are actually straw purchasers, but the facts seem to suggest this is at least sometimes true. How can a hairdresser service several mortgages without \"income\" from China? reply timr 12 hours agorootparent> The article doesn't seem to suggest that the mortgage holders are actually straw purchasers, but the facts seem to suggest this is at least sometimes true. How can a hairdresser service several mortgages without \"income\" from China? The article explicitly says that the purpose is laundering via professional operators -- see the flowchart diagram toward the bottom of the piece. reply ABCLAW 14 hours agoparentprev> We should be seeing many people holding mortgages at HSBC not able to pay. Not really. Lying about the source of cashflow doesn't mean the cashflow isn't real. The end objective for a lot of these frauds isn't to sink the bank with fake loans. It's to launder money. reply beiller 14 hours agorootparentMakes sense I wasn't thinking about the full on laundering aspects. But even so, if the real estate is used in laundering, it will eventually have to be sold to get back clean money. This should still run up prices at the start, and run them down in the end. So I think the majority of the point still stands: there should be an uptick in sales (which there is not). They could be speculating on top of laundering, in which case they are taking some losses. We are -20% from peak. The time will come when they (the launderers) will need liquidity and sell which has not come. Will it ever come? reply avidiax 12 hours agorootparentThey can get clean money from the start if they structure things right. Have other mules or partners purchase crappy properties at a low price. \"Flip\" the properties, having another mule purchase at a greatly increased price and service the mortgage with more laundered money. So you get the capital gains immediately, and they are apparently completely clean. If the crappy house continues to appreciate naturally, that's also a bonus, but if not, you can eventually default the mortgage or short-sale. reply bostonsre 13 hours agorootparentprevI think they want to get their money out of china and parked into a safe place. If they pay off their mortgage, they don't want to find a new place to park their money, they can just keep the house as an asset. I think a lot of investing in china is real estate based and is part of the reason that market is struggling over there so much now. It would make sense for them to continue to follow that investing model when exporting their wealth to other countries. reply oldgradstudent 14 hours agoparentprev- \"A rolling loan gathers no loss.\" As long as real estate prices continue to rise, you won't see large scale missed payments because they will be able to sell the assets, refinance the loan, or even successfully rent it out. We've seen this dynamic in 2008 and in the S&L crisis before. Bad loans drive the bubble, the expanding bubble hides the bad loans, but when the bubble stops, there is a massive large scale loan failure. reply faluzure 14 hours agoparentprevAssuming these mortgages are insured through CMHC, would HSBC be on the hook or the insurance system when some of these mortgages fail? Canadians are certainly paying for social services used by folks who earn income abroad and pay little to no income tax in Canada, and folks who want to buy their first house are harmed by inflated housing prices. My currently overseas landlord for some reason needed to travel to Canada to give birth, and was very eager to get their health card / banking documents sent to our rental despite it being rented out for several years prior to us arriving... reply crustaceansoup 14 hours agorootparentCMHC doesn't insure mortgages where the property value is equal to greater than $1 million, which in the Greater Toronto Area essentially limits it to condo purchases. reply cm2187 15 hours agoparentprevI think that's what a lot of people here are not realising (or perhaps they haven't read the article). In this case the main victim is HSBC if these loans were made to individuals who are speculating on foreign real estate without the income to cover the loan. This doesn't look like originate and distribute, i.e. HSBC shareholders will bear the losses. reply empath-nirvana 14 hours agorootparentNo, they won't. It's money laundering. They'll pay the mortgages. reply flamedoge 14 hours agoparentprevHSBC Canada is sold to RBC reply cloudedcordial 13 hours agoprevThe issue is not new. I am of East Asian heritage and live in Canada. Several people asked me causally why I went to school and had a regular job. They explained that with the \"Chinese money\" I should not compete for jobs with folks who really needed the money. Smh... reply billy99k 8 hours agoparentI get that they shouldn't be stereotyping, but I frequently travel to Markham, and see lots of young Chinese kids with expensive cars. I think the last time I was there, I counted 40 Teslas. reply al_borland 9 hours agoparentprevCan you explain what they are implying? Is there a belief that China hands out blank checks to anyone who happens to be Asian? This sounds insane to me. reply noobermin 6 hours agorootparentRead the article. It's not china giving blank checks, it is the set of rich folks China's stellar rise has created who are property rich and have capital from China they can live off of. There's no blank checks. The stereotyping is lumping the parent post with this group on the basis of North American racial categories (\"Asian\" as a group, as if that's a thing.) reply darth_avocado 15 hours agoprev> making him a minority among mostly Chinese-Canadian co-workers at the Aurora branch. Not trying to point fingers on whether the branch workers were in on the whole thing, but maybe it was easier to perpetuate the fraud because of cultural familiarity? reply readthenotes1 15 hours agoparentYour fingers aren't required, only the whistleblower's: \"and possibly some employees benefited from the fraud, financially pocketing thousands of dollars, which I call the proceeds of crime.\" reply inSenCite 15 hours agoprevHSBC soon to be RBC. This is not very surprising, a lot of this also gets facilitated by the independent mortgage brokers who get ppl through the system with some 'creative coaching'. reply fakedang 15 hours agoparentLol no. HSBC has always had a much worse reputation than RBC, or any other bank for that matter. HSBC and Standard Chartered are amongst the worst international offenders when it comes to fraud and money laundering. reply wisemang 15 hours agorootparentProbably referring to the fact RBC is set to acquire HSBC’s Canadian business. reply fallat 14 hours agorootparentBrings to question: did RBC know? How did they not know? reply emmanuel_1234 14 hours agorootparentRBC probably indulges in this as well. HSBC just has way more ties to Hong Kong and China than RBC does. reply peterleiser 14 hours agorootparentprevExactly. HSBC was laundering money for Mexican and Colombian drug cartels and fined $1.9 billion by the US government, which is one of the largest penalties ever imposed on a bank for breaking U.S. law. Deutsche Bank fires pretty high on the fraud meter as well. reply heldrida 1 hour agoprevCould this happen in other parts of the world like London or Lisbon? What’s the point of working hard and playing by the rules… reply jacquesm 15 hours agoprevHSBC has been part of plenty of scandals. Here is one example: https://www.investopedia.com/stock-analysis/2013/investing-n... reply gnatman 14 hours agoprevSounds familiar! https://www.nytimes.com/2015/02/13/upshot/how-mortgage-fraud... https://www.justice.gov/opa/pr/wells-fargo-agrees-pay-209-bi... reply forinti 15 hours agoprevJust the other day I read that \"to shanghai\" is to kidnap or trick someone into working for you. I immediately thought of HSBC. reply denton-scratch 14 hours agoparent> kidnap or trick I thought it meant kidnap, specifically to kidnap a sailor in port and make him work aboard ship; as in \"press gang\", \"pressed man\". reply stainablesteel 13 hours agoprevreal estate in canada is so expensive that i think it will ironically cause the jobs to become worthless, and this is the only comeuppance that will bring the housing prices back down is when people give up on living there reply maxglute 15 hours agoprev>The whistleblower, whomThe Bureau is calling D.M., immigrated to Canada as an international student from India, making him a minority among mostly Chinese-Canadian co-workers at the Aurora branch. I mean it's real Chinese income backed by fake paperwork. There's PRC capital controls, rich PRC national who buy RE abroad are going to do it via laundering services and has been for decade+. Banks are fine with this and have dedicated branches in diasphora area to handle because the money is good and reliable. Sometimes rich Chinese immigrants also do odd jobs to fill time, bored aunties with multi million dollar mansions in Richmond working shifts at River Rock Casino. It's a bizarre world. reply seanmcdirmid 15 hours agoparentIt could be real Chinese income backed by fake paper work (money laundering) or fake income backed by fake paper work (fraud). You can take out $50k per year, a lot more via exemptions, so you don’t need money laundering to get money out of china into the USA (I had to move a few hundred K before, all the paper work was legit). reply jabbany 14 hours agorootparent> You can take out $50k per year Try buying a house with 50k a year... This is exactly what the \"scheme\" solves. You have new immigrants who have (in many cases) legitimate money (e.g. by selling property back in china) but cannot move it out of the country quickly due to capital controls on the chinese side. Since mortgages are meant to spread out costs over time, it's the perfect solution. However, banks (understandably) care about income rather than existing capital. So you have a lot of \"safe\" customers who are unlikely to default and less sensitive to interest rates (compared to the local borrower pool), and banks looking for customers amidst high interest rates... You can see how something like this can easily arise from these conditions... > you don’t need money laundering to get money out of china into the USA I cannot comment on what your situation was (maybe through a business?), but AFAIK it is very hard to move capital out for regular individuals. Your realistic options to wire out capital are just \"education\" and \"tourism\". While technically you can claim \"investment\", it will almost always not be approved and cause a watch to be put on all your accounts. That being said, usually documentation of the funds outside of china is completely legitimate and above board. There is no need to fake this. The only paperwork magic that needs to happen is towards the chines government... However, it still _looks_ like money laundering because you can only wire $50k a year, so many need to resort to wiring from accounts of different individuals (friends and family) to different individuals, despite the funds already being fully documented and reported outside china. reply seanmcdirmid 14 hours agorootparentForeigners on Z visas can wire out whatever they earn, so for me it wasn’t an issue. There are even foreigners who rent out some of their allocations since you always have PRC expenses, but this is of course illegal. I used my wife’s $50k allocation once because I was held up by some paper work (you need a lot of paper work, notarized, etc…). They didn’t ask any questions about what it would be used for, but this was 2016, and they changed the rules a few months later. reply jabbany 13 hours agorootparentForeigners are not fully exposed to the local market though, so there's no need to do capital controls. _Income_ is never an issue, as there are taxes to cover that. The main thing being cracked down on is people moving accumulated _wealth_ out. Also, things have changed _considerably_ since 2016 (as these things tend to do). Indeed, nobody --- foreign or domestic --- needed to document use as long as you stayed within the $50k (you would be asked to if you went above that). Later it became a mandatory question, but wasn't enforced. These days it is enforced rather strictly. If you claim education, you need to provide statements of tuition and housing etc. For tourism there's similar requirements, and usually they limit discretionary spending budgets to ~$10k. reply maxglute 15 hours agorootparentprevI presume you were foreign national with more options. Options for diasphora Chinese, many who illegally hold dual citizenship and relies on PRC nationality to do transactions/capital flight in PRC is different. I highlighted the original quote where branch is mostly Chinese-Canadian for context. Aurora is 20% Chinese, it's a big diasphora neighbourhood. There isn't some big \"fake chinese income\" conspiracy, it's the entire (proven) business plan (money laundering) with occasional fraud. I think pretty much everyone knew Chinese are buying million+ propertiers with laundered money, and it fuels the bubble as much as any other foreign buyer. reply seanmcdirmid 15 hours agorootparentYa, foreign nationals are allowed to export their earnings. But even Chinese citizens can also pool those $50k yearly allocations, it doesn’t take a lot of friends and family to do that. I’m not really sure what is happening among the rich, but among the middle class, it isn’t so much money laundering but having lots of savings with no good investment options, or just wanting to make money while someone else takes on the risk. They don’t have access to money that needs laundering. reply maxglute 14 hours agorootparentIt happens, it can be more annoying than one thinks since people with wealth to exfiltrate correlated with other family members who also have wealth to exfiltrate, graft is family business. Nephew/niece studying abroad? That's 100k gone per year for next 4-5 years. Meanwhile living well is expensive, annoying to be dependant on personal capital flight group. And post crackdown, it's more risky to drag family/friends into pooling. Especially if they're public sector/public sector adjacent. Frequently, it's easier to pay someone commission to move money for you. In my experience, middle class aren't buying million+ RE, they pool together savings to send kids abroad, and maybe put a down payment on a condo that the kid pays off once they get decent job in west. And by middle class we really mean upper flat out highincome top %5-10 relative to all PRC house holds who are middle class tier1/2 regions. But agreed, domestic non gov investment ecosystem pretty trash, hard to beat investing in your kid(s) and saving for retirement until a mature system develops. Which IMO hard goal since focus isn't to further wide wealth disparity by giving that 5-10% more consumption/investment abilities but to bring up the next quantiles of households - the actual middle class. This is where my assessment departs from most, I think \"common prosperity\" for PRC is getting more households richer, but at PRC development levels, that diminishes the households with enough savings to retire and surplus to invest. And this has all sorts of implications on inflation/FX rate. reply jabbany 14 hours agorootparentprev> But even Chinese citizens can also pool those $50k yearly allocations, it doesn’t take a lot of friends and family to do that. So as an observer of this, you will see money coming out of one place, spread to many accounts, wired overseas to different recipients, then re-aggregated... What do you think that looks like...? Surely not money laundering? reply maxglute 14 hours agorootparentNot on Canada's end, it's a bunch of people using legal capital controls to get money into Canada. On PRC end, it's evading capital controls, which depending on your geopolitical alignment, can be illegal activity. But if you're in the west, you want that Chinese money. Free money from competitotrs as good as brain drain from competitors. reply fennecbutt 7 hours agorootparentIt's not free money if house prices are pumped and sold, profits then returned to China. They used to try to overbuy milk powder in NZ, ignoring supermarket limits imposed bc NZ mothers couldn't find enough formula for their kids. Chinese buyers would sell it back to China to make a tidy profit, after their baby formula scandal drove demand for foreign baby formula (which still exists today). Worked at a supermarket at start of uni, got so fucking sick of being screamed at in mandarin bc I refuse to sell them 30 tins when the limit was 2. Over and over, every damn shift. Source that isn't me: https://www.nzherald.co.nz/nz/china-buys-up-big-in-nz-baby-m... reply maxglute 3 hours agorootparent>profits then returned to China Are those profits returning to China? If they wanted profit they would invest in Chine RE which (until recently) was much more profitable and speculative than western RE. Western RE investment was for capital flight to bring wealth abroad. Very few want to bring money INTO PRC. Baby formula actually great example, at least from what I know in AU market. Yeah you had the occasional tourists bringing back a few cans to savec money, but the sellers getting 30 tins and doing weekly shippments to regular customers in PRC were getting paid in AUD. $60 per tin into AU economy flipping milk powder. It's a good gig, it's no real estate money though. You can argue it's net bad for society because some gain at society loss, but that's how it be in capitalism. Some interests profit at the cost of others. And the interests who profit from PRC money, arguably the establishment, wants to keep profitting. reply seanmcdirmid 14 hours agorootparentprevCapital controls have bad intentions anyways, I have no qualm with PRC citizens trying to avoid them. Not everyone in the west wants or likes Chinese money. It does lead to them basically exporting their bubble abroad, but Japan did the same in the 80s before their big bust. reply maxglute 7 hours agorootparentYes, PRC citizens avoiding capital controls end up playing on the same field as other global wealth. I think PRC exporting their bubble is over stated - 10s of millions of tier1 buyers got into the speculation/RE game early in 90s that they'd be millionaires with wealth to enter western RE game regardless where PRC RE market stablizes. And there are just millions of millionaires in China, independant of wealth from RE bubble. Many buyers are simply independantly wealthy from other business ventures ontop of having multiple tier1 units to liquidate to buy western RE. The folks buying million+ dollar properties and 100k cars in the last few years didn't liquidate everything they have in PRC to start new life in Canada. Canada is a retirement plan they bought with change. Sure not everyone wants PRC money, but the sellers certainly do, as do the intermediates who benefits from PRC capital flight. reply jabbany 13 hours agorootparentprevYup. Hence the capital control (less money leaking out means an easier recovery after the bust). Of course, as is rational in capitalism, this just makes it even more urgent to skirt the capital control, lest you be caught with the burden of the bust. This was also why crypto was even an option for doing this, for a short while at least. reply huhtenberg 14 hours agoparentprev> has been for decade+ Try 25 years. Started right after the Hong Kong transition from under the British rule. reply Ericson2314 13 hours agoprevCanada needs more housing supply. Blaming foreigners' mortgage applications, sketchy or otherwise, is just cope. reply cbsmith 13 hours agoparentCanada does need more housing supply, and ironically an influx of fraudulent real estate money like this helps to fuel more growth in the supply. Still, fraud is fraud. Stopping it won't fix Canada's housing problems, but that's not a reason to stand idly by. reply Ericson2314 12 hours agorootparentI would say land value tax makes mortgage fraud a lot less enticing ;) Sapping demand for the crime is the most effective enforcement! (Like Semaglutide is probably the best silver bullet yet to \"win the drug war\"...) reply AllegedAlec 53 minutes agoparentprevIf you disallow foreigners to buy up whatever little housing there is on the market you solve part of the issue though. reply causi 15 hours agoprevIt's quite bizarre any jurisdiction would allow someone to buy housing there when they can't legally live in it. reply asah 14 hours agoparentThere's \"housing\" and then there's $5+MM manhattan apartments, which are so inflated above their value as functional housing as to effectively be NFTs. People whine about the pencil shaped buildings but they shutup fast when they see that it causes zero inflation to lower-end housing prices and a big help to city budgets. Also, strategically, having powerful people own expensive real estate influences them to visit and maybe not bomb it... at least, it's better than them never having stepped foot there. The famous example is Kyoto not being nuked because an American leader had seen it firsthand[1]. The Nazis spared Paris was apparently spared for similar reasons[2]. The counterexample is NYC which everybody loves to crap on (Gerald Ford \"drop dead\", 1993 bombing, 9/11, and lots of failed terrorist attacks since), presumably as a symbol of American greed and excess, but also as a symbol of urban chaos, rot and violence. [1] https://collider.com/oppenheimer-improvised-line-kyoto/ [2] https://www.google.com/search?q=Nazis+spared+Paris reply JumpCrisscross 15 hours agoparentprev> bizarre any jurisdiction would allow someone to buy housing there when they can't legally live in it Foreign-owned homes are a problem for asset acquirers. Vacant homes are a problem for anyone who needs housing. The former seems to get a lot of visibilty when concerns around the latter get raised. reply alchemist1e9 15 hours agorootparent> Vacant homes are a problem for anyone who needs housing. But they pay taxes without demanding any services and the seller assessed they had better use of the capital, they could buy or build a more suitable home. If I lived in a location with 50% vacant homes all paying property taxes then wouldn’t my schools and streets and all local government services be extremely well funded? reply JumpCrisscross 15 hours agorootparent> If I lived in a location with 50% vacant homes all paying property taxes then wouldn’t my schools and streets and all local government services be extremely well funded If the polity is smart, yes. It looks like British Columbia gets about 15% of its revenue from property taxes and transfers [1]. So you'd need adjustments to make up for the personal, corporate, sales, fuel, carbon, tobacco and insurance premium (?) revenues the vacant homeowner isn't paying. [1] https://www2.gov.bc.ca/assets/gov/british-columbians-our-gov... Table 2.3 reply wasimanitoba 5 hours agorootparentAssuming the drop in social program expenditure doesn't already compensate for the drop in tax revenue. reply causi 13 hours agorootparentprevKind of by definition you can't pay more in taxes than you contribute to the economy, therefore unoccupied housing is a drain on the local economy. reply alchemist1e9 13 hours agorootparentYour assertion flips economic principles on their head. Vacant homes paying taxes without drawing on local services represent a net gain, not a drain. Taxes paid on these properties directly fund public services, enhancing the community’s infrastructure without additional burden. Furthermore, the investment in property contributes to the economy through construction, maintenance, and property management industries. To claim that unoccupied housing harms the local economy is not logical. Austrian economics teaches us that restricting foreign investment misinterprets how markets work. Vacant homes signal opportunities for builders, not losses for workers. Investment flows where it’s valued, stimulating demand and construction, not stifling growth. Misallocating housing due to artificial constraints only distorts the market, harming those you aim to help. Let’s not forget, economic growth comes from creating value, not redistributing scarcity. reply FireBeyond 6 hours agorootparent> To claim that unoccupied housing harms the local economy is not logical. Sure, property taxes are paid on the house. But unoccupied homes don't buy groceries and clothes, don't go to restaurants in the local economy. So they do harm the economy, in the sense that they don't contribute as much to the economy as an occupied home. reply toast0 15 hours agoparentprevIt seems like a waste of limited government ability to act to try to deny this when it's quite simple for an interested foreign investor to form or invest in a domestic corporation that buys housing it doesn't live in, or find a domestic partner to make straw purchases of housing the partner doesn't live in. reply alchemist1e9 15 hours agoparentprevWhy? Should we restrict other investments with similar logic? For example should non-residents not be allowed to purchase vacation properties and lease them? The impulse to enlist the government to regulate private property and investments is not productive and results in endless encroachment of individual liberties and rights to governments. reply soggybread 15 hours agorootparent>For example should non-residents not be allowed to purchase vacation properties and lease them? AirB&B has been a terrible experience, so many people buying up houses just to lease them out for a weekend has definitely contributed to rising housing costs and over-all cost of living reply bdcravens 15 hours agorootparentThat's an issue absolutely, but it doesn't answer the question of whether non-residents should be allowed to do what residents can. reply JumpCrisscross 15 hours agorootparent> it doesn't answer the question of whether non-residents should be allowed to do what residents can Raise that fence too high and you turn landlords into the community's gatekeepers. (How else could a non-resident become a resident?) reply alchemist1e9 15 hours agorootparentDon’t worry the HN crowd seems to be so far gone from reality and understanding free markets that I’ve encountered numerous discussions where landlords are categorically evil, which is overwhelmingly empirically understood to be extremely beneficial to housing and communities to have landlords invest in them! Of course given the trend has been to remove all education of market fundamentals and inject eduction systems with endless collectivist propaganda we should not be surprised. reply stormfather 15 hours agorootparentprevSomething drastic has to be done about the cost of housing if we're to leave a functional society to the next generation. reply alchemist1e9 15 hours agorootparentBlaming foreign investors for a housing crisis is a smokescreen. It's not the demand from investors that's the issue; it's the government's stifling regulations that choke new construction. The mess is because of state failure, not market failure. When sellers freely sell their homes to foreign buyers, they're making choices that benefit them. Why should we deny them that right? The real absurdity is ignoring the elephant in the room: a bureaucratic quagmire that prevents building enough homes to meet demand. Instead of scapegoating investors, slash the red tape and let the market work. reply function_seven 15 hours agorootparentYup. The Power of Pricing is a thing that exists whether you want it to or not. Smart policy uses it to great advantage. Dumb policy redirects this to hurt those it intends to help. Rent control, restrictions on production, byzantine zoning and construction rules... all contribute to distorting the market in ways that push back on the original (or at least, stated) intentions. You want housing to be cheaper? Increase supply. That's it. You don't want foreign investment in your properties? Don't make them so damn attractive as pure investment vehicles. How? Increase supply. There's always a boogeyman to be blamed when markets are so broken like this. reply seanmcdirmid 11 hours agorootparentChina doesn’t allow non-residents to buy property at all. You have to be working in the city you want to buy in for a few years with documentation if you don’t have hukou there. Maybe the USA and Canada could do something similar? I find it ironic that it’s primarily Chinese investors who want us to keep our property markets open. reply denton-scratch 14 hours agorootparentprev> You want housing to be cheaper? Increase supply. The USA (and maybe Canada) has large stocks of government land. In my country, most land is privately-owned; interfering with landowners' property rights is seriously destabilising. Property law is the basis of most law. reply alchemist1e9 15 hours agorootparentprevAbsolutely correct. Unfortunately and surprisingly the supposedly educated users of HN are overwhelmingly anti-capitalists somehow, your opinion and mine are decidedly in the minority recently, and that is itself fascinating to me, as I can only conclude the education system is completely broken and failing to teach both basic economics which consists of facts and is scientific and also history. I liken it to teaching creationism over evolution, preferring fantasy over reality. reply ericmcer 14 hours agorootparentprevUSA population has increased ~10% in the last 20 years. Why has that small increase in population caused a humongous lack of housing? It feels like something else is going on other than \"we need to increase supply by 10% but can't\" reply vel0city 12 hours agorootparentAs Sean mentioned, not only have we increased in total population we have also changed where we are living. Lots of small towns have seen their populations decrease, with some completely disappearing. Large neighborhoods of cities like Detroit and elsewhere essentially emptied. We need more housing and we need to shift housing resources to where the demand is. There are loads of cheap houses in the USA. They're just in places where most people don't want to live. Here's a cheap house: https://www.zillow.com/homedetails/420-Tyler-St-Gary-IN-4640... The commute to Southern California is pretty killer though. reply seanmcdirmid 14 hours agorootparentprevHousing supply and demand isn’t uniformly distributed across the United States. When we say “housing shortage” we only mean in popular places to live. reply stormfather 12 hours agorootparentSo where have housing prices fallen? reply seanmcdirmid 12 hours agorootparentPlaces you don’t care about, like Jackson Ms, Detroit, Toledo, Gary. Recently, we see falling housing prices in Las Vegas and Phoenix, although they are probably ahead of where they were before the pandemic. reply mistrial9 8 hours agorootparentprevin the asset bubble leading to the 2008 credit collapse, the only area in the continental USA that had decreasing housing prices in large areas was .. the Ohio Valley. (likely plenty of niche areas too but that is what stuck out) reply gsk22 15 hours agorootparentprevSure, but the high cost of housing has little to do with non-resident investors, and everything to do with lack of supply. reply bdcravens 15 hours agorootparentResident investors are a big part of the problem, however. reply alchemist1e9 9 hours agorootparentOff with their heads! /s reply inglor_cz 14 hours agorootparentprev\"Something drastic has to be done about the cost of housing if we're to leave a functional society to the next generation.\" Drastic? Well, then: kill NIMBYism. Just off with its head. We know the 18th century in England as the \"Gin Craze\", future generations will look at our period as the \"NIMBY Craze\". Large-scale construction is absolutely possible. There were periods of massive construction booms all around the globe, especially after wars (when a lot of housing had to be rebuilt immediately). You can absolutely build a lot of comfortable middle-class housing in a fairly short time. Most German cities were rubble in 1945 and fine again in 1960. But you need density and straightforward approval processes. No artificial scarcity caused by one-family home zoning and endless environmental reviews that are abused to stall developments for decades. reply Fauntleroy 15 hours agorootparentprevOr if we don't want to watch it collapse during ours. reply tslocum 15 hours agorootparentprevYes. Restricting foreigners from owning domestic assets is the beginning. Next comes restricting / heavily taxing domestic owners that own more than one home. Sooner or later, housing gets closer to being what it's supposed to be (shelter and space for people who need it) rather than a financial vehicle. reply alchemist1e9 15 hours agorootparentThis proposal is a slippery slope to economic disaster. First, restricting ownership rights—foreign or domestic—distorts the market, disincentivizes investment, and ultimately harms those it claims to help by reducing the supply of housing. Second, treating homes purely as shelter ignores the reality that property is also an investment and a key component of individual wealth and economic freedom. Imposing heavy taxes on those owning more than one home would not only penalize success but also discourage rental market contributions, exacerbating the housing shortage. The real solution lies in encouraging development and reducing bureaucratic barriers to increase housing supply, not in draconian measures that trample on property rights and stifle economic growth. Let’s not replace a market-driven approach with a command economy that history has repeatedly shown to fail. reply denton-scratch 14 hours agorootparent> treating homes purely as shelter ignores the reality that property is also an investment But that's the problem, isn't it? The basic necessities of life shouldn't become a vehicle for speculation. FTR, all of my wealth is in two homes. reply alchemist1e9 13 hours agorootparentAsserting that housing—or any basic necessity—shouldn’t be an investment is a dangerously naive stance that flies in the face of economic reality and human history. Consider food, water, healthcare—all necessities, yet all benefit from private investment and innovation. The collectivist dream to strip away the investment aspect of housing is a recipe for disaster, leading to shortages, degradation, and inefficiency. Your stance isn’t just misguided; it’s empirically proven to fail, fostering misery under the guise of equality. Housing, like any resource, flourishes under conditions of freedom, not under the heavy hand of state control. To suggest otherwise is to ignore the lessons of history and to jeopardize the very foundations of prosperity and freedom. reply denton-scratch 12 hours agorootparent> Consider food, water, healthcare—all necessities, yet all benefit from private investment and innovation. I live beside the river Thames, which private \"investment\" has transformed into a sewer. My access to food has shrunk; I used to have access to butchers, greengrocers and so on. Now all my food comes from supermarkets. The health system I depend on has been gradually privatized, and it is now at breaking point. > the lessons of history History is squishy stuff; we mould it to support the conclusions we want to draw. [Edit] I'm interested that you didn't challenge my equating of investment with speculation, because I didn't mention investment. Obviously, without capital investment, you don't get capital assets like houses. But my neighbourhood is blighted by absentee landlords; one neighbour is an AirBnB, the other has been empty for 5 years. Both are owned by absentee landlords, one living 2,000Km away. That's not investment; that's speculation. reply alchemist1e9 11 hours agorootparentBlaming private investment for pollution and housing issues ignores the core role of government in regulating externalities and protecting property rights. The Thames’ state isn’t due to market failure but government inaction, a prime example of state failure. Moreover, the economic decline witnessed in the UK isn’t caused by capitalism but by anti-market policies stifling competitiveness. The real issue is the collectivist delusion that more state control is the solution, ignoring that such approaches have consistently led to further decline. Misplacing blame on capitalism and pining for socialism only exacerbates the problems, diverting us from the proven path to prosperity: free markets and effective, limited government intervention. Lamenting the rise of supermarkets as a death knell for local butchers and greengrocers is a misplaced nostalgia that ignores consumer choice and market efficiency. Supermarkets thrive because they offer what consumers demand: variety, convenience, and affordability. To decry this as a market failure is to advocate for a return to less efficient, more costly ways of living, under the guise of preserving tradition. It’s an affront to consumer sovereignty and a free market that naturally evolves to meet changing societal needs. Yearning for a past that restricts choice and elevates prices is a backward step, not progress. Criticizing absentee landlords as mere speculators ignores the benefits they bring: paying property taxes and injecting capital into the economy. This isn’t about speculation; it’s about fulfilling market demand and facilitating economic activity. The real issue lies in state-imposed barriers that prevent adequate housing supply, not in the actions of individual investors. Blaming investors for taking advantage of market opportunities is misguided and diverts attention from necessary reforms to increase housing availability and affordability. The collectivist dismissal of history as \"squishy\" is a deliberate evasion of undeniable truths. History is replete with the failures of socialism and the triumphs of capitalism. To mold it to fit a narrative that justifies state control and collectivism is intellectually dishonest and dangerously naive. The empirical evidence is clear: wherever socialism has been tried, it has led to economic stagnation, misery, and the erosion of freedoms. Capitalism has lifted billions out of poverty and spurred innovation and prosperity unmatched by any collectivist scheme. Ignoring these facts is not just an error in judgment; it's a willful blindness to the lessons that history has painstakingly taught us about the superiority of market freedom over state control. Whenever the cry of \"market failure\" echoes, a closer inspection often reveals the true culprit: state failure. \"If someone considers that there is a market failure, I would suggest that they check to see if there is state intervention involved. And if they find that that’s not the case, I would suggest that they check again, because obviously there’s a mistake.\" This wisdom holds true across the spectrum of economic grievances. Time and again, what is hastily labeled as a failure of capitalism turns out to be the unintended consequences of excessive regulation, misguided policies, or government overreach. The path to prosperity is not paved by increasing state control but by unleashing the creative and productive powers of the free market. reply denton-scratch 2 hours agorootparent> The Thames’ state isn’t due to market failure but government inaction > The real issue is the collectivist delusion that more state control is the solution My head hurts. > The collectivist dismissal of history as \"squishy\" Firstly, I am not a collectivist. Secondly, I don't dismiss history; I think it's very important and illuminating. Thirdly, Karl Marx, the arch-collectivist, hardly dismissed history; his entire theory was based on historical analysis. History is not a list of facts; what real historians do is largely interpretation. History is almost completely unlike maths. Expressions like \"history tells us that ...\" are rather stultifying; history tends to tell us what we want to hear. Your comment seems to be a catalogue of free-marketeer articles of faith, expressed as bald assertions, as if only a fool could fail to see their obvious truth. Well, we've had free-marketeers in charge here for 15 years now; everyone knows that things have got worse. reply lxgr 10 hours agorootparentprevAh yes, who would want to trade the flourishing real estate utopia of, say, San Francisco for the collectivist hellscapes of New York (some rent stabilization and public housing), Vienna (>50% public housing, consistently scores top in overall quality of life globally), or Singapore (>70% public housing)? Free markets are great, until they start incentivizing weird behaviors (NIMBYism, bubbles) instead of investments (construction, renovations) and efficient allocation. Successful cities have walked the balance successfully and stepped in (only) when necessary. reply alchemist1e9 9 hours agorootparentThis argument mistakenly credits collectivist policies for successes but ignoring the profound negative impacts of state overreach in places like San Francisco. This isn’t about choosing between so-called utopias and hellscapes but recognizing the failure of excessive regulation that stifles supply and inflates costs. Vienna and Singapore are outliers that succeed due to unique governance, cultural attitudes towards public housing, and centralized planning that meticulously balances supply and demand—conditions that are not easily replicated elsewhere. The real issue is state intervention distorting market incentives, leading to inefficiencies like NIMBYism and housing bubbles. A truly efficient housing market thrives under free market principles, minimally but effectively regulated to encourage development and affordability. Mentioning New York as a paragon of housing policy overlooks its glaring issues with affordability and efficiency—hardly a model of success. In comparison, cities like Chicago, with a different regulatory approach, demonstrate that a more balanced policy framework can indeed foster better housing outcomes. New York’s situation, far from an example to follow, actually underscores the pitfalls of overregulation and the necessity of rethinking housing strategies. reply lxgr 9 hours agorootparentI agree in that the right amount of regulation probably heavily varies between places and over time, but I'd just like to challenge the idea that anything other than complete liberalization of housing investments will inevitably lead to inefficiencies. > Vienna and Singapore are outliers [...] A model that has to explain away two historically, culturally, and geographically distinct cities as outliers is not very compelling to me. Again, I'm not proposing that more regulation is always good, but as soon as e.g. long-term residents are massively getting priced out by outside investors or homeowners start opposing new construction exclusively because of the impact on their property value due to an increase in supply (rather than for actual decreased quality of life), the incentives of the free market start drifting apart from those of the people actually living there. reply IncreasePosts 14 hours agorootparentprevHow about an ultimatum? Either people can only own one house without huge tax consequences, OR we open up restrictions on building so we can build a lot more housing stock than we currently do. reply Fauntleroy 15 hours agorootparentprevThere's a little too much \"housing is way too incredibly expensive for the citizens of the country\" going on here for us to really care about \"fairness of the open market\" reply ericmcer 15 hours agoparentprevIt feels like selling the next generations future for cash now. reply caseysoftware 15 hours agoparentprevIn the US, banks issue mortgages to illegal immigrants all the time. Last fall, the Biden Administration went as far as threatening banks who had refused. > “This guidance reminds lenders that denying someone access to credit based solely on their actual or perceived immigrant status may violate federal law.” Ref: https://www.justice.gov/opa/pr/justice-department-and-consum... reply michael1999 14 hours agoparentprevThe historical Canadian project, like all anglo settler projects, is structurally about early arrivals to the frontier making money selling to later arrivals. The idea that housing is primarily for locals is a newer idea. reply slavboj 15 hours agoparentprevApproximately anyone can buy a US-based REIT, even if they're ineligible to enter the country. reply bdcravens 15 hours agorootparentThis is less of an issue than owning the home, since in an REIT, you aren't controlling tenant access. reply mytailorisrich 15 hours agoparentprevMost governments would do anything for foreign direct investment. From many points of view, having foreign investors buy property without immigrating is a best case scenario for governments. reply ericmcer 14 hours agorootparentIn the long run (30+ years from now)? Or just for the few years they are in office? reply mytailorisrich 14 hours agorootparentI think probably in the long run because, all in all, net inbound capital into the economy is positive in the long run. What can cause problems is bubbles and 'over-heating', meaning too much over a too short period, which is when 'less' inbound capital may be sought. But overall best to keep the flow positive. reply bonestamp2 12 hours agoprev> during the Covid-19 pandemic, because Canadian casinos were closed, Chinese underground banking schemes evolved Hang on, what... I want to hear more about the Chinese underground casino stuff. reply dddddaviddddd 10 hours agoparenthttps://betterdwelling.com/vancouver-laundering-model-that-i... reply dade_ 15 hours agoprevHSBC Canada is in the process of being sold to RBC. As employees will no longer work for HSBC in March and possibly unemployed through the new found efficiencies by RBC, this creates an interesting situation for virtually no risk whistle-blowing. I hadn't thought of this possibility when the exit was announced. reply eswat 15 hours agoparent> this creates an interesting situation for virtually no risk whistle-blowing Not so risk-free unfortunately > A June 2023 email from the bank’s personnel department says “we hereby demand that you [the whistleblower] immediately and permanently delete any and all HSBC information on any personal email accounts.” > “If you do not comply with these obligations,” the email warns, “HSBC also reserves the right to bring this matter to the attention of relevant law enforcement agencies.” reply contingencies 15 hours agoprevNot only Canada. In Australia this became pervasive across all banks for the last 25 years. It got to the point where, around 5-10 years ago, new national rules were mandated that foreign income could only be assessed at some minor percentage of its evidenced volume under the epithet \"loan serviceability criteria\". At the face of it, these rules appear to have the public's best interests at heart. But in reality, they simply lock out anyone that isn't a locally registered card-carrying commuter wageslave (eg. cross-border entrepreneurs, immigrants, etc.). So the new scam - from multiple independent sources - is apparently people from Singapore taking out loans in Chinese Yuan Reminbi denominations for Australian property against Singapore or Hong Kong banks, then coming to the Australian banks and having them \"transferred\" (internationally, and across currencies!) which allegedly sidesteps the local restrictions. The fact that I know this simply from talking to bank staff as a stranger shows how extremely pervasive these sorts of things are. reply donavanm 2 hours agoparentA “fun fact” for another Australian. As of this month at least one if the big four is doing serviceability assessments based on the PAYG gross income. WRT your other foreign income/asset comments its much less nefarious. The local banks are focused on AU income and assets because it is directly tied to serviceability and recovery. You can do loans based on foreign income/assets but youll pay a few percentage points for the risk and conversion problems. The international loan outfits are usually smaller, though HSBC is a big one IIRC. reply msie 11 hours agoprevSam Cooper makes a living with China xenophobia. He was fired from his stint at Global for accusing a Canadian MP of colluding with China according to “sources”. That MP sued Global who fired Cooper. So I take his articles with a huge grain of salt. reply noobermin 6 hours agoparentAre there more details here? Accusing one of \"colluding with China\" which is a state does not imply xenophobia. reply canadiantim 11 hours agoparentprevWell atleast we know where you stand tho reply user3939382 15 hours agoprevHSBC funneled $1B (that we know about) for cartels, it came out as 100% knowingly committed, and no one went to jail. Basically the government asked for their cut. Why wouldn't do they this again and again? They are above the law, they are the law. reply mikeyouse 15 hours agoparent\"The government asked for their cut\" actually means that for laundering that $881 million in cartel money - they were fined $1.9 billion and then had to pay an additional $665 million in other civil penalties. They wouldn't do it again and again because they don't earn $880 million when accepting $880 million in dirty money - remember that bank deposits are liabilities to the bank - they only earn money from interest on that -- and it cost them billions in fines to do so... reply user3939382 14 hours agorootparentI'd be very surprised if the $880M was the extent of the crime. The government not jailing anyone involved in the scheme is completely inexcusable and in my view makes the government complicit in the crime, in which case my trust that the public information about this case is accurate is 0. reply creato 14 hours agorootparentHSBC probably got a few percent of that $880M in fees/interest/whatever. So unless they were laundering ~100x more than that, the fines absolutely did make all of that crime (even if it wasn't the full extent of it) net negative for the bank, and probably by a lot. reply mikeyouse 14 hours agorootparentprevThe \"scheme\" was bank tellers in Mexico, working for a recently acquired local bank, accepting boxes full of cash and lying about the origin. HSBC was fined for looking the other way and having shitty controls about suspect funds -- their AML teams were understaffed and they didn't do any real due diligence on the Mexican banking firm they had purchased. The entire executive team was forced out, they clawed back bonuses for everyone in the chain who profited off the shitty controls. So who would you jail in this case? The bank tellers interfacing with cartel? They're in Mexico anyway. Some overworked compliance manager in the US who ignored the suspicious transactions? Some C-Level exec person who didn't know about the suspicious origin of a billion dollars into a bank with something like 2.5 trillion in assets? What specific crime do you think they committed? Nobody likes these global banks, they're run by absolute psychopaths but remember, the optimal amount of fraud is non-zero. All of the mirror image complaints about banks not wanting to touch Crypto or proceeds from gambling/porn sites is downstream from settlements like these. https://www.bitsaboutmoney.com/archive/optimal-amount-of-fra... reply user3939382 12 hours agorootparentThis isn’t accurate. HSBC management trained their employees on how to encode transfers for the cartel organizations to evade the government’s detection mechanisms (by inserting punctuation). So there was more to this scheme than you’re describing. reply godelski 14 hours agorootparentprevThat comes out of the company's wallet, but does it come out of a person's? Persons responsible for committing crimes, not customers or other third party members. HSBC has trillions in holding, so those numbers might not be as big as they appear. reply mikeyouse 14 hours agorootparentThey literally clawed back bonuses from all of the executives involved in the compliance failures. reply user3939382 10 hours agorootparentOh no their bonuses. I’m sure it was tough not being able to spend the summer in Martha’s Vineyard that year. reply jszymborski 15 hours agorootparentprevAlso, HSBC is selling off all of its consumer banking in Canada largely as a result of the gov't fines and tattered reputation among Canadians. reply denton-scratch 14 hours agorootparent> tattered reputation among Canadians Hardly just Canadians; I first started hearing tales of huge HSBC corruption 20 years ago. FTR: I bank with First Direct, which is an online banking service of HSBC in the UK. reply Teever 15 hours agoro",
    "originSummary": [
      "A whistleblower at HSBC Bank in Canada has uncovered evidence of fraudulent mortgages in Toronto, involving fake Chinese income and estimated at over $500 million.",
      "These fraudulent home loans were issued by at least 10 HSBC branches in the Toronto area since 2015, with an increase during the Covid-19 pandemic.",
      "Chinese diaspora buyers were obtaining mortgages from HSBC while claiming extravagant salaries from remote-work jobs in China, using fake documents to launder money."
    ],
    "commentSummary": [
      "HSBC bank in Canada is allegedly implicated in fraudulent mortgage issuance to Chinese diaspora buyers in Toronto, possibly involving employees and a senior manager.",
      "The issue may go beyond one branch and be widespread throughout the bank, raising concerns about money laundering, fraud, inflated housing prices, and potential risks to the Canadian banking system.",
      "The discussion also focuses on the impact of non-taxed income on the Toronto real estate market, regulations, the role of foreign buyers, and potential consequences for the global economy, considering the decline of the Chinese property market and capital controls."
    ],
    "points": 412,
    "commentCount": 347,
    "retryCount": 0,
    "time": 1707241951
  },
  {
    "id": 39273954,
    "title": "A Comprehensive Guide on Using PostgreSQL in Various Applications and Scenarios",
    "originLink": "https://gist.github.com/cpursley/c8fb81fe8a7e5df038158bdfe0f06dbb",
    "originBody": "PostgreSQL is Enough Just Use Postgres for Everything Simplify: move code into database functions Background and Cron Jobs https://github.com/citusdata/pg_cron Message Queues https://adriano.fyi/posts/2023-09-24-choose-postgres-queue-technology https://github.com/tembo-io/pgmq GIS/Mapping https://github.com/postgis/postgis Audit Logs https://github.com/supabase/supa_audit https://github.com/pgMemento/pgMemento https://github.com/pgaudit/pgaudit Access Control https://github.com/arkhipov/acl Authorization https://www.postgresql.org/docs/current/pgcrypto.html https://github.com/michelp/pgjwt Search Postgres Full Text Search (bunch of helpful links) https://github.com/paradedb/paradedb https://github.com/neondatabase/pg_embedding https://github.com/pgvector/pgvector Time Series https://github.com/timescale/timescaledb Graph Data https://age.apache.org Foreign Data https://github.com/supabase/wrappers HTTP https://github.com/pramsey/pgsql-http https://github.com/supabase/pg_net APIs https://github.com/PostgREST/postgrest https://github.com/hasura/graphql-engine https://postgraphile.org https://supabase.github.io/pg_graphql Events, Replication, CDC, https://www.postgresql.org/docs/current/sql-notify.html https://github.com/cpursley/walex (Disclosure: I maintain this and think it's pretty awesome) https://github.com/PeerDB-io/peerdb https://github.com/debezium/debezium https://github.com/2ndQuadrant/pglogical Unit Tests https://github.com/theory/pgtap Migrations https://github.com/purcell/postgresql-migrations https://www.bytebase.com/ Dashboards / UIs Baserow NocoDB AppSmith Data Visualization Evidence Metabase HTML and Applications SQLpage Omnigres pg_render plmustache Language Servers https://github.com/supabase/postgres_lsp What's missing? Please share in the comments.",
    "commentLink": "https://news.ycombinator.com/item?id=39273954",
    "commentBody": "PostgreSQL is enough (gist.github.com)371 points by cpursley 20 hours agohidepastfavorite282 comments superb-owl 20 hours agoI often go down rabbit holes like this, trying to collapse and simplify the application stack. But inevitably, as an application grows in complexity, you start to realize _why_ there's a stack, rather than just a single technology to rule them all. Trying to cram everything into Postgres (or lambdas, or S3, or firebase, or whatever other tech you're trying to consolidate on) starts to get really uncomfortable. That said, sometimes stretching your existing tech is better than adding another layer to the stack. E.g. using postgres as a message queue has worked very well for me, and is much easier to maintain than having a totally separate message queue. I think the main takeaway here is that postgres is wildly extensible as databases go, which makes it a really fun technology to build on. reply deathanatos 19 hours agoparentI have certain experience with some technologies, e.g., SQS and Postgres. Say I'm on your team, and you're an application developer, and you need a queue. If you're taking the \"we're small, this queue is small, just do it in PG for now and see if we ever grow out of that\" — that's fine. \"Let's use SQS, it's a well-established thing for this and we're already in AWS\" — that's fine, I know SQS too. I've seen both of these decisions get made. (And both worked: the PG queue was never grown out of, and generally SQS was easy to work with & reliable.) But what I've also seen is \"Let's introduce bespoke tech that nobody on the team, including the person introducing it, has experience in, for a queue that isn't even the main focus of what we're building\" — this I'm less fine with. There needs to be a solid reason why we're doing that, and that we're going to get some real benefit, vs. something that the team does have experience in, like SQS or PG. Instead, this … thing … crashes on the regular, uses its own bespoke terminology, and you find out the documentation is … very empty. This does not make for a happy SRE. reply cnity 17 hours agorootparentThis desire can sometimes be so strong that people insist on truly wacky decisions. I have before demonstrated that Postgres performs perfectly well (and in fact exceeds) compared with a niche graph database, and heard some very strange reasons for why this approach should be avoided. A lot of the time you hear that it's engineers who chase shiny technology, but I've seen first hand what can happen when it's leadership. reply cbreezyyall 12 hours agorootparentOften referred to as resume driven development. reply smitty1e 9 hours agorootparentRDD leaves serious wreckage in its wake. reply tracker1 16 hours agorootparentprevI've been on both sides of this.. Rabbit MQ and Elastic Search for a public facing site. The dedicated queue for workers to denormalize and push updates. To elastic. Why, because the $10k/month RDBMS servers couldn't handle the search load and were overly normalized. Definitely a hard sell. I've also seen literally hundreds of lambda functions connecting to dozens of dynamo databases. I'm firmly in the camp of use an RDBMS (PostgreSQL my first choice) for most things in most apps. A lot of times you can simply apply the lessons from other databases at scale in pg rather than something completely different. I'm also more than okay leveraging a cloud's own MQ option, it's usually easy enough to swap out as/if needed. reply spothedog1 15 hours agorootparentprevCan you expand on Postgres vs Graph Databases? reply cnity 25 minutes agorootparentIt is easy to represent a graph in Postgres using edge and node tables. For the use case we have, it is more performant to query such a setup for many millions of relationships vs using the big names in graph databases. You just need a little bit of appropriate index selection and ability to read the output of EXPLAIN ANALYZE to do so. There are probably use cases where this doesn't hold, but I found in general that it is beneficial to stick to Postgres for this, especially if you want some ability to query using relations. reply kabes 15 hours agorootparentprevOk. I get that. But to play devil's advocate: with that mentality we'd never learn a new technology and still be stuck on punch cards. And I don't have the time anymore for hobby projects. I'd say it's ok to introduce something new as long as it's one thing at a time and not an entire new stack in the \"a rewrite will solve all problems\" projects reply beagle3 15 hours agorootparentTo me this argument sounds like “I don’t have time for hobby projects, so I’m going to treat this professional one as a hobby”. I always start a professional project with technologies I am intimately familiar with - have used myself, or have theoretical knowledge of and access to someone with real experience. There has never been a new shiny library/technology that would have saved more than 10% of the project time, in retrospect. But there have been many who would have cost 100% more. reply nyrikki 14 hours agorootparentThis isn't a dichotomy. That is the point of DDD,SoA,Clean, Hexagonal patterns. Make a point to put structures and processes in place that encourage persistence ignorance in your business logic as the default and only violate that ideal where you have to. That way if you outgrow SQL as a message bus you can change. This mindset also works for adding functionality to legacy systems or breaking apart monoliths. Choosing a default product to optimize for delivery is fine, claiming that one product fits all needs is not. Psql does have limits when being used as a message or event bus, but it can be low risk if you prepare the system to change if/when you hit those limits. Letting ACID concepts leak into the code is what tends to back organisations into a corner that is hard to get out of. Obviously that isn't the Kool aid this site is selling. With this advice being particularly destructive unless you are intentionally building a monolith. \"Simplify: move code into database functions\" At least for any system that needs to grow. reply beagle3 11 hours agorootparentI was not saying \"psql is all you'll ever need\". I was just replying to >>> \"Applied consistently, this logic would seem to preclude becoming familiar with anything.\" As a general principle. reply jamwil 14 hours agorootparentprevI take your point but you don’t explain how you came to be intimately familiar with those technologies in the first place. Applied consistently, this logic would seem to preclude becoming familiar with anything. reply beagle3 12 hours agorootparentFor projects where I have a paying customer, this rule is absolute; I do not experiment on my client's time (and dime) unless they specifically request it. But I do have projects which I finance myself (with myself as customer), and which do not have a real deadline. I can experiment on those. Call them \"hobby\" projects if you insist. > Applied consistently, this logic would seem to preclude becoming familiar with anything. Well, project requirements always rank higher, and many projects require some piece I am unfamiliar with (a new DB - e.g. MSSQL; a new programming language; etc). That means one does get familiar on a need basis , even applying this approach robotically. If a project requires building the whole thing around a new shiny technology with few users and no successful examples I can intimately learn from ... I usually decline taking it. reply deathanatos 14 hours agorootparentprevI'm okay with new technology, actually, but the person introducing it has to be able to champion it & do the work of debugging issues and answering questions about its interactions with the rest of the system. I.e., they have to be responsible for it. The last part in my parent comment is more of a \"it was chucked over the fence, and it is now crashing, and nobody, not even the devs that chose it, know why\". I do have examples of what you describe, too: a dev I worked with introduced a geospatial DB to solve issues with geospatial queries being hard & slow in our then-database (RDS did not, at the time, support such queries) — so we went with the new thing. It used Redis's protocol, and was thus easy to get working with¹. But the dev that introduced it to the system was capable of explaining it, dealing with issues with it — to the extent of \"upstream bugs that we encounter and produce workarounds\", and otherwise being a lead for it. That new tech, managed in that way by a senior eng., was successful in what it sought to do. The problematic parts/components/new introductions of new tech … never seem to have that. That's probably partly the problem: it's such an inherently non-technical issue at its heart. The exact thing almost doesn't matter. > as long as it's one thing at a time IME it's not. When there are problems, it's never just one new thing at a time. > a rewrite will solve all problems And the particular system I had in my mind while writing the parent post was, in fact, in the category of \"a rewrite will solve all problems\". Some parts of the rewrite are doing alright, but especially compared to the prior system, there are just so. many. new. components. 2 new queue systems, new databases, etc. etc. So it's then hard to learn one, particularly without someone championing its success. It's another to self-learn and self-bootstrap on 6 or 8 new services. ¹(Tile38) reply diggan 19 hours agoparentprevI think a lot of the industry struggles with the idea that maybe there is no \"one size fits all\", and what makes sense when you're a one person company with 100 customer probably doesn't make sense when you're a 1000 people company with millions of customers. If you use a stack meant for a huge userbase (with all the tradeoffs that comes with it) but you're still trying to find market fit, you're in for a disappointment Similarly, if you use a stack meant for smaller projects while having thousands of users relying on you, you're also in for a disappointment. It's OK to make a choice in the beginning based on the current context and environment, and then change when it no longer makes sense. Doesn't even have to be \"technical debt\", just \"the right choice at that moment\". reply davidw 19 hours agorootparent> It's OK to make a choice in the beginning based on the current context and environment, and then change when it no longer makes sense. Yep. And Postgres is a really good choice to start with. Plenty of people won't outgrow it. Those who do find it's not meeting some need will, by the time they need to replace it, have a really good understanding of what that replacement looks like in detail, rather than just some hand-wavy \"web scale\". reply tracker1 16 hours agorootparentTrue enough and with modern hardware that barrier is relatively high. IIRC Stack overflow was handling several million users in a single database server over a decade ago... We've got over 8x the compute power and memory now. Still need to understand the data model and effects on queues though. reply wvh 18 hours agoparentprevThat's a nicely balanced view. I've been working on the intersection between dev, sec and ops for many, many years and one of the most important lessons has been that every dependency is a liability. That liability is either complexity, availability, security, wasting resources or projects or key people disappearing. Do anything to avoid adding more service, library or technology dependencies; if necessary, let people have their side projects and technological playgrounds to distil future stacks out of. There are good reasons to go OLAP or graph for certain kinds of problems, but think carefully before adding more services and technologies because stuff has a tendency to go in easily but nothing ever leaves a project and you will inevitably end up with a bloated juggernaut that nobody can tame. And it's usually those people pushing the hardest for new technologies that are jumping into new projects when shit starts hitting the fan. If a company survives long enough (or cough government), a substantial and ever increasing amount of time, money and sec/ops effort will go into those dependencies and complexity cruft. reply brightball 19 hours agoparentprevPG works really well as a message queue and there's several excellent implementations on top of it. Most systems are still going to need Redis involved just as a coordinator for other pub/sub related work unless you're using a stack that can handle it some other way (looking at BEAM here). But there are always going to be scenarios as an application grows where you'll find a need to scale specific pieces. Otherwise though, PostgreSQL by itself can get you very, very far. reply stickfigure 19 hours agorootparent> PG works really well as a message queue It's also worth noting that by using PG as a message queue, you can do something that's nearly impossible with other queues - transactionally enqueue tasks with your database operations. This can dramatically simplify failure logic. On the other hand, it also means replacing your message queue with something more scalable is no longer a simple drop-in solution. But that's work you might never have to do. reply dfee 19 hours agorootparentprevWorth noting that Postgres has a pubsub implementation built in: listen/notify. https://www.postgresql.org/docs/current/sql-notify.html reply cpursley 17 hours agorootparentYep, I should add that. One of the libraries in my list (that I maintain) is WalEx: https://github.com/cpursley/walex/issues It subscribes to the Postgres WAL and let you do the same sort of thing you can do with listen/notify, but without the drawbacks like need for triggers or character limits. reply maxbond 14 hours agorootparentWhat's the drawback to a trigger? I would think that any overhead you recouped by avoiding a trigger would be offset by the overhead of sending the entire WAL to your listener, rather than the minimized subset of events that listener is interested in. (To be clear I do see other downsides to listen/notify and I think WalEx makes a lot of sense, I just don't understand this particular example.) reply cpursley 13 hours agorootparentYou don’t send the entire WAL, just what you subscribe to - and you can even filter via SQL: https://github.com/cpursley/walex?tab=readme-ov-file#publica... reply cpursley 10 hours agorootparentThis post describes some of the other issues with listen/notify trigger approach: https://news.ycombinator.com/item?id=36323698 reply brightball 16 hours agorootparentprevGoing to add this to my research list. reply cpursley 15 hours agorootparentPing me if you have any questions. Long time fan of your blog. reply brightball 15 hours agorootparentThat is really cool to hear, thank you. And I should have mentioned it before, but we have an open call for speakers for the Carolina Code Conference. This would make for an interesting talk I think. reply brightball 17 hours agorootparentprevOh yea, definitely aware of it. I believe many of the queuing solutions utilize it as well. I've ready a lot of reports (on here) that it comes with several unexpected footguns if you really lean on it though. reply agumonkey 16 hours agoparentprevThe more I do fullstack work the more I see an obesity crisis. I under the need to modularize (I dearly think I do) but god you have relational model, reimplemented in your framework, reencoded as a middleware to handle url parsing, the one more layer to help integrate things client side. I find that insane. And Postgrest was a refreshing idea. reply cpursley 12 hours agorootparentSeriously. There's like 7000 duplicates of the very same data layer in a single stack: database, back-end ORM/data mapper, front end and various caching things in between. Things like PostgREST and Hasura area great pared with fluent clients. reply agumonkey 7 hours agorootparentAnd then there's the failed microservice case.. what some people describe a distributed monolith where data has to be passed around through every layer, with domain logic replicated here and there. reply nextaccountic 19 hours agoparentprev> But inevitably, as an application grows in complexity, Some applications never grow that much reply ejb999 19 hours agorootparentI would go further and even say 'most' applications never grow that much. reply LaGrange 19 hours agorootparentprevBut it surely will! It will! See, now that we're profitable, we're gonna become a _scale up_, go international, hire 20 developers, turn everything into microservices, rewrite the UI our customers love, _not_ hire more customer service, get more investors, get pressured by investors, hire extra c-levels, lay-off 25 developers and the remaining customer service, write a wonderful journey post. The future is so bright! reply zwnow 19 hours agorootparentHonestly, that's one of the reasons I never want to monetize my work and stay miles away from the software industry. Modern world is all web apps that require you to subscribe to 20 different 3rd party services to even build your app. So you rack up bills before your product is even remotely lucrative... Building an app with no third party dependencies seems impossible nowadays. At least if you plan to compete. reply LaGrange 18 hours agorootparentI mean, you _can_ host your staging environment on a Minisforum PC hidden in your closet and then deploy to Hetzner, and probably save a _ton_ unless your service benefits from things like, say, autoscaling or global low-latency access. Niches where you can get away with that are limited, not just by technical challenges but because large parts of the social ecosystem of IT won't like that. But they do exist. There's also still things that aren't webapps _at all_, there's software that has to run without internet access. It's all far apart and often requires specialized knowledge, but it exists. reply zwnow 4 hours agorootparentYea I mostly learned web dev so far but wanted to get into IoT stuff so I might find something cool to do in there. reply legohead 19 hours agoparentprevMy saying has always been: be nice to the DB Don't use it anymore than you have to for your application. Other than network IO it's the slowest part of your stack. reply cpursley 17 hours agorootparentHandling business logic in the database is often going to be an order of magnitude faster than the application layer of some of the popular language stacks (looking at you, Rails, Node, etc). It also will outlive whatever webstack of the day (and acquisition which of en requires a re-write of the application layer but keeps general database structure - been there done that). reply jgalentine007 16 hours agorootparentMaybe faster... but I've met very few developers that are good DBAs (that understand procedures, cursors, permissions etc.) Database schema versioning / consistency is a whole other level of pain too. reply dventimi 3 hours agorootparentThat sounds like a social problem, not a technical problem. reply niels_bom 18 hours agorootparentprevWould you say it's slower than file IO too? reply Too 16 hours agorootparentIt’s not slow by itself. It’s a single point of bottleneck that will inevitably become slow as you cram everything into it. reply dagss 14 hours agorootparent...but by trying to avoid the bottleneck and moving things to backend, you make things 10x worse resource wise for the DB. So it is not a easy tradeoff. Take any computation you can do in SQL like \"select sum(..) ...\". Should you do that in the database, or move each item over the network and sum them in the backend? Summing in the database uses a lot less resources FOR THE DB than the additional load the DB would get from \"offloading\" this to backend. More complex operations would typically also use 10x-100x less resources if you operate on sets and amortize the B-tree lookups over 1000 items. The answer is \"it depends\" and \"understand what you are doing\"; nothing about it is \"inevitable\". Trying to avoid computing in the DB is a nice way of thinking you maxed out the DB ...on 10% of what it should be capable of. reply Too 14 hours agorootparentYes. Aggregations and search are often best done as close to the data as possible, in the DB. Rendering html, caching, parsing api responses, sending emails, background jobs: Nope. Basically, use the database for what it’s good at, no more. reply legohead 15 hours agorootparentprevWell, it is file IO, plus processing on top. But it's not that simple, since if your data is small enough it can all be loaded into memory, allowing you to sidestep any file IO. But you still have the processing part... reply alternatex 17 hours agorootparentprevKind of irrelevant since a DB provides some guarantees that a simple file does not by default. reply samtheprogram 16 hours agorootparentGP was responding to a comment comparing it to network IO in terms of bottlenecks in your application stack ...? reply brlewis 19 hours agoparentprev> you start to realize _why_ there's a stack, rather than just a single technology to rule them all Architecturally, there are other cases besides message queues where there's no reason for introducing another layer in the stack, once you have a database, other than just because SQL isn't anybody's favorite programming language. And that's the real reason there's a stack. reply wg0 19 hours agoparentprevI think SQS is cheap enough to build on as a messaging queue even if you're not hosting within AWS. Out of the widely underrated AWS services include SNS and SES and they are not a bad choice even if you're not using AWS for compute and storage. reply williamdclt 19 hours agorootparentThe problem is rarely cost, it's operational overhead. Using SQS for a queue rather than my already-existing Postgres means that I have to: - Write a whole bunch of IaC, figuring out the correct access policies - Set up monitoring: figure out how to monitor, write some more IaC - Worry about access control: I just increased the attack surface of my application - Wire it up in my application so that I can connect to SQS - Understand how SQS works, how to use its API It's often worth it, but adding an additional moving piece into your infra is always a lot of added cognitive load. reply fuy 17 hours agorootparent+. And then you have to figure everything one more time when you decide to move to (or to add support for) Azure/GCP. reply chuckhend 19 hours agorootparentprevYou get exactly once when you consume with pgmq and run your queue operations inside transactions in your postgres database. I can't think of an easy way to get some equivalent on SQS without building something like an outbox. reply rcaught 19 hours agorootparentSQS FIFO has exactly-once processing reply qaq 19 hours agorootparentprevSQS is at least once PG can give you exactly once reply rcaught 19 hours agorootparentSQS FIFO has exactly-once processing reply qaq 19 hours agorootparentwell that's a stretch it has \"5 minute window\" You can hold a lock on a row in PG queue for as long as you need reply chuckhend 18 hours agorootparentpgmq (which is linked on this gist) provides an api to this functionality. It can be 0 seconds, or 10 years if you want. It's not a row lock in, which can be expensive. In pgmq, its build into the design of the visibility timeout. FOR UPDATE SKIP LOCKED is there to ensure that only a single consumer gets any message, and then the visibility timeout lets consumer determine how long it should continue to remain unavailable to other consumers. reply qaq 19 hours agorootparentprevNot sure why this is making people upset. reply williamdclt 19 hours agorootparentBecause it's incorrect. If you have any non-postgres side-effect, you can't have exactly-once (unless you do 2PC or something like that). There isn't any technology that gives you \"exactly once\" in the general case. reply qaq 19 hours agorootparentThat's not how exactly once is defined for queue. We are talking about semantics of what queue systems is providing. reply silon42 19 hours agorootparentNobody will understand it like that. reply qaq 19 hours agorootparentAnyone who has ever selected queue service/product will understand it like that. Because thats one of the most prominent features that gets highlighted by those products: SQS Standard queues support at-least-once message delivery. NATS offers \"at-most-once\" delivery etc. reply tetha 16 hours agoparentprevThis is very much the way I'm pushing in our internal development platform: I want to offer as little middlewares as possible, but as many as necessary. And ideally these systems are boring, established tech covering a lot of use cases. From there, Postgres ended up being our relational storage for the platform. It is a wonderful combination of supporting teams by being somewhat strict (in a flexible way) as well as supporting a large variety of use cases. And after some grumbling (because some teams had to migrate off of SQL Server, or off of MariaDB, and data migrations were a bit spicy), agreement is growing that it's a good decision to commit on a DB like this. We as the DB-Operators are accumulating a lot of experience running this lady and supporting the more demanding teams. And a lot of other teams can benefit from this, because many of the smaller applications either don't cause enough load on the Postgres Clusters to be even noticeable or we and the trailblazer teams have seen many of their problems already and can offer internally proven and understood solutions. And like this, we offer a relational storage, file storage, object storage and queues and that seems to be enough for a lot of applications. We're only now adding in Opensearch after a few years as a service now for search, vector storage and similar use cases. reply TheCapeGreek 19 hours agoparentprevOn top of that, a lot of discourse seems to happen with an assumption that you only make the tech/stack choice once. For the majority of apps, just doing basic CRUD with a handful of data types, is it that hard to just move to another DB? Especially if you're in framework land with an ORM that abstracts some of the differences, since your app code will largely stay the same. reply twosdai 20 hours agoparentprevThe same argument of UNIX design patterns (Single responsibility, well defined interfaces and communication protocals) vs Monolithic design patterns comes up a lot. I think that its mainly because both are effective at producing products, its just that they both have downsides. reply macksd 19 hours agorootparentI read a meme yesterday about how you can just interject \"it's all about finding that balance\" into any meeting and people will just agree with you. I'm gonna say it here. Sometimes a flexible tool fits the bill well. Sometimes a specialized tool does. It's all about finding that balance. Thank you for coming to my TED talk. reply e12e 19 hours agorootparentprevJust noting that sometimes one can do both: seperate postgres DBs/clusters for different use-case, seperate instances of a web server for TLS termination, caching, routing/rewriting, Ed:static asset serving. Benefit is orderly architecture, and fewer different dependencies. reply philippemnoel 19 hours agoprevI'm one of the makers of ParadeDB, a modern alternative to Elasticsearch. We build Postgres extensions to do fast search (pg_bm25) and analytics (pg_analytics). I love Postgres. If you have a small workload, like a startup, it certainly makes sense to stay within Postgres as long as you can. The problem is, at scale, Postgres isn't the answer to everything. Each of the workloads one can put in Postgres start to grow into very specific requirements, you need to isolate systems to get independent scaling and resilience, etc. At this point, you need a stack of specialized solutions for each requirement, and that's where Postgres starts to no longer be enough. There is a movement to build a Postgres version of most components on the stack (we are a part of it), and that might be a world where you can use Postgres at scale for everything. But really, each solution becomes quite a bit more than Postgres, and I doubt there will be a Postgres-based solution for every component of the stack. reply chasd00 18 hours agoparentwhat is \"at scale\"? Is there a specific metric or range of metrics that raises a flag to begin considering something else? For example, in the olden days when it was my problem, page load times were the metric. Once it got high enough you looked for the bottleneck, solved it, and waited. When the threshold was broken again you re-ran the same process. Is there an equivalent for postgres? reply ndriscoll 17 hours agorootparentThis bugs me every time performance comes up. No one is ever concrete, so they can never be wrong. If Michael Jackson rose from the dead to host the Olympics opening ceremony and there were 2B tweets/second about it, then postgres on a single server isn't going to scale. A crud app with 5-digit requests/second? It can do that. I'm sure it can do a lot more, but I've only ever played with performance tuning on weak hardware. Visa is apparently capable of a 5-digit transaction throughput (\"more than 65,000\")[0] for a sense of what kind of system reaches even that scale. Their average throughput is more like 9k transctions/second[1]. [0] https://usa.visa.com/solutions/crypto/deep-dive-on-solana.ht... [1] PDF. 276.3B/year ~ 8.8k/s: https://usa.visa.com/dam/VCOM/global/about-visa/documents/ab... reply asah 14 hours agorootparentminor nit: 9K TPS for Visa are business transactions - TBD how many database transactions are generated... (still, modern postgresql can easily scale to 10,000s (plural) of TPS on a single big server, especially if you setup read replicas for reporting) reply ndriscoll 12 hours agorootparentYeah, I don't mean to say Visa can run global payment processing on a single postgres install; I'm sure they do a ton of stuff with each transaction (e.g. for fraud detection). But for system design, it gives an order of magnitude for how many human actions a global system might need to deal with, which you can use to estimate how much a wildly successful system might need to handle based on what processing is needed for each human action. For similar scale comparisons, reddit gets ~200 comments/second peak. Wikimedia gets ~20 edits/second and 1-200k pageviews/second (their grafana is public, but I won't link it since it's probably rude to drive traffic to it). reply gen220 15 hours agorootparentprevThe truth is that it really depends on your application work load. Is it read-heavy, or write-heavy? Are the reads more lookup-heavy (i.e. give me this one user's content), or OLAP heavy (i.e. `group by`'s aggregating millions of rows)? Is read-after-write an important problem in your application? Do you need to support real-time/\"online\" updates? Does your OLAP data need to be mutable, or can it be immutable (and therefore compressed, columnarized, etc.)? Is your schema static or dynamic, to what degree? I agree with others that a good simplification is \"how far can you get with the biggest single AWS instance\"? And the answer is really far, for many common values of the above variables. That being said, if your work load is more OLAP than OLTP, and especially if your workload needs to be real-time, Postgres will begin to give you suboptimal performance without maxing-out i/o and memory usage. Hence, \"it really depends on your workload\", and hence why you see it's common to \"pair\" Postgres with technologies like Clickhouse (OLAP, immutable, real-time), RabbitMQ/Kafka/Redis (real-time, write-heavy, persistence secondary to throughput). reply jimbokun 16 hours agorootparentprevFor me with any kind of data persistence backend, it's when you go from scaling vertically to horizontally. In other words, when it's no longer feasible to scale by just buying a bigger box. I don't know that there is a canonical solution for scaling Postgres data for a single database across an arbitrary number of servers. I know there is CockroachDB which scales almost limitlessly, and supports Postgres client protocol, so you can call it from any language that has a Postgres client library. reply jimbokun 16 hours agoparentprevFor scaling, has anyone here used hash based partitioning to scale horizontally? In principle, seems like it should work to allow large scale distribution across many servers. But the actual management of replicas and deciding which servers to place partitions, redistributing when new servers are added, etc. could lead to a massive amount of operational overhead. reply gen220 15 hours agorootparentAs other sibling comments noted, Citus does this pretty well. Recommend reading through their docs and use-cases. There's some migration/setup costs, but once you have a good configuration, it mostly just-works. Main downside is that you either have to either self-manage the deployment in AWS EC2 or use Azure's AWS-RDS-equivalent (CitusData was acquired by MS years ago). FWIW, I've heard that people using Azure's solution are pretty satisfied with it, but if you're 100% on AWS going outside that fold at all might be a con for you. reply philippemnoel 16 hours agorootparentprevCitus is indeed an example for \"distributed PostgreS\". There are also serverless Postgres (Neon, Nile, AWS Aurora) which do this. If you are interested in partitioning in an OLAP scenario, this will soon be coming to pg_analytics, and some other Postgres OLAP providers like Timescale offer it already reply findjashua 15 hours agorootparentprevhash based partitioning makes repartitioning very expensive. most distributed DB now use key-range based partitioning. Iirc, Dynamo which introduced this concept has also made the switch reply hot_gril 16 hours agorootparentprevI think that's what Citus does. reply cpursley 17 hours agoparentprevThis looks great, I'll add it to my list. I've gone far out of my way not to use Elasticsearch and push Postgres as far as as I can in my SaaS because I don't want the operational overhead. reply philippemnoel 16 hours agorootparentThis is exactly why we built ParadeDB :) reply prisenco 19 hours agoprevThis makes a strong case, but I've decided to start every new project with sqlite and not switch until absolutely necessary. If Postgres is the 90% case, then sqlite is the 80% case and is also dead simple to get going and genuinely performant. So when vertical scaling finally fails me, I know I'll be at a wonderful place with what I'm building. reply Ensorceled 19 hours agoparent> [...] sqlite is the 80% case and is also dead simple to get going and genuinely performant. I don't understand this. PostgreSQL is ALSO dead simple to get going, either locally or in production. Why not just start off at 90%? I mean, I get there are a lot of use cases where sqlite is the better choice (and I've used sqlite multiple times over the years, including in my most recent gig), but why in general? reply tvink 19 hours agorootparentI think \"dead simple\" is not doing anyone any favors when it is being used to try to equate the simplicity of things. It's obviously a lot simpler to just have a file, than to have a server that needs to be connected to, as long as we're still talking about running things on regular computers. reply Ensorceled 15 hours agorootparentI guess that's really my point here. They difference in setup time is negligible so I'm not sure why people keep bringing it up as a reason to choose sqlite over PostgreSQL. For instance, \"deployable inside a customer application\" is an actual requirement that would make me loath to pick PostgreSQL. \"Needs to be accessible, with redundancy, across multiple AWS zones\" would make me very reluctant to pick sqlite. Neither of these decisions involve how easy it is to set up. It's like choosing between a sportbike and dump truck and focusing on how easy the sportbike is to haul around in the back of a pickup truck. reply SJC_Hacker 15 hours agorootparentI'm not sure its neglibile, I suppose once you know what you're doing. But postgres setup, at least the package managers on Linux, will by default, create a user called postgres, and lock out anyone else who isn't this user from doing anything. Yeah you can sudo to get psql etc. easily, but that doesn't help your programs which are running as different users. You have to edit a config file to get to work, and I never figured out how to get to work with domain sockets and not TCP reply saltcured 14 hours agorootparentThat's interesting... my experience (almost all on RHEL/CentOS/Fedora) is that it is trivial to have unix domain socket with local Postgres clients and a pain to setup any remote clients. You just have to call the basic \"createuser\" CLI (or equivalent CREATE ROLE SQL) out of the postgres superuser account to create database users that match local Linux usernames. Then the ident-based authentication matches the client process username to the database role of the same name. reply prisenco 15 hours agorootparentprev> Needs to be accessible, with redundancy, across multiple AWS zones How many projects start with these requirements? reply rrr_oh_man 3 hours agorootparent> How many projects start with these requirements? In a world fueled by cheap money and expensive dreams, you'd be surprised. reply Ensorceled 14 hours agorootparentprevAnything with real customer data in the cloud? Certainly you need replication. reply prisenco 14 hours agorootparentLitestream can handle realtime replication. But most projects don’t even have customers when they start, let alone large quantities of their data and legal requirements for guaranteed availability. reply int_19h 7 hours agorootparentI think it's reasonable for a business, even a freshly starting one, to expect to grow to the point where it does have enough customers to outgrow SQLite fairly soon. Between that and PG offering more features in general (which often simplifies app code, because you can do more with queries), it's still not clear why not start with PG in the first place. PG, on the other hand, can scale enough to cover foreseeable future needs for most businesses, so aiming for something more complex than that is almost always premature. reply prisenco 7 hours agorootparent> outgrow SQLite fairly soon That would be the result of either vastly overestimating their business plan or vastly underestimating SQLite. reply klibertp 19 hours agorootparentprev> PostgreSQL is ALSO dead simple to get going I'm not saying it's hard to set up Postgres locally, but sqlite is a single binary with almost no dependencies and no config, easily buildable from source for every platform you can think of. You can grab a single file from sqlite.org, and you're all set. Setting up Postgres is much more complicated in comparison (while still pretty simple in absolute terms - but starting with a relatively simpler tool doesn't seem like a bad strategy.) reply scaryclam 16 hours agorootparentExcept for when your data is in it. Migrating data on a running app is one of the worst things to deal with. I can understand using something simple and cut down for other things, but the DB is not the place I'd want to do that. Postgres isn't exactly hard to get going with, and will grow with you easily, so why trade that off for saving an hour or two at the start of the project? reply benlivengood 19 hours agorootparentprevPractically, because sqlite is good enough for one machine and compatible-enough with postgresql that you can use either pretty easily. One thing I wrote was an exactly-once stream processor that fetched events from a lot of remote systems for processing. Transaction-based queue in the DB to achieve exactly-once with recovery (remote systems accepted time-stamp resyncing of the stream of events). It works fine at small scale on a single machine for design and testing (local integration tests with short startup time are very valuable) but trivially scales to hundreds of workers if pointed at a postgres instance. The work to allow sqlite vs postgres was a single factory that returned a DB connection in Go based on runtime configuration. It's also good practice for designing reasonably cross-database compatible schemas. reply runningamok 18 hours agorootparentprevOne use case where SQLite is a good option is for embedding as a local database in an app. Starting local-only with SQLite allows you to defer a lot of the backend effort while testing an MVP. reply cpursley 17 hours agorootparentYou might find https://github.com/electric-sql/electric pretty cool. reply mrbonner 15 hours agorootparentprevIf dead simple involves me babysitting a service process then not it is not. SQLite has embedded version that requires no service out of process. That's what my definition of dead simple. reply davidmurdoch 19 hours agorootparentprevBut PostgreSQL is not dead simple when compared to SQLite. reply randomdata 18 hours agorootparentprevPostgres complicates the application in several ways. In particular, Postgres suffers from the n+1 problem, while SQLite does not. That requires a significant amount of added complexity in the application to hack around. Why over engineer the application before it has proven itself as something anyone even wants to use? Let's face it, the large majority of software written gets thrown away soon after it is created. I already hear you saying that you know of a library that provides a perfect abstraction to hide all those details and complexities, making the choice between Postgres and SQLite just a flip of a switch away. Great! But then what does Postgres bring to the table for you to choose it over SQLite? If you truly prove a need for it in the future for whatever reason, all you need to do is update the configuration. reply sgarland 17 hours agorootparentThis is a misunderstanding of the n+1 problem, which is exacerbated by SQLite's deceptive phrasing of the issue: > In a client/server database, each SQL statement requires a message round-trip from the application to the database server and back to the application. Doing over 200 round-trip messages, sequentially, can be a serious performance drag. While the above is true on its own, this is _not_ the typical definition of n+1. The n+1 problem is caused by poor schema design, badly-written queries, ORM, or a combination of these. If you have two tables with N rows, and your queries consist of \"SELECT id FROM foo; SELECT * FROM bar WHERE id = foo.id_1...\", that is not the fault of the DB, that is the fault of you (or perhaps your ORM) for not writing a JOIN. reply randomdata 17 hours agorootparent> that is the fault of you (or perhaps your ORM) for not writing a JOIN. It's your fault for not writing a join if you need a join. But that's not where the n+1 problem comes into play. Often in the real world you need tree-like structures, which are fundamentally not able to be represented by a table/relation. No amount of joining can produce anything other than a table/relation. The n+1 problem is introduced when you try to build those types of structures from tables/relations. A join is part of one possible hack to workaround to the problem, but not the mathematically ideal solution. Given an idealized database, many queries is the proper solution to the problem. Of course, an idealized database doesn't exist, so we have to deal with the constraints of reality. This, in the case of Postgres, means moving database logic into the application. But that complicates the application significantly, having to take on the role that the database should be playing. But as far as SQLite goes, for all practical purposes you can think of it as an ideal database as it pertains to this particular issue. This means you don't have to move that database logic into your application, simplifying things greatly. Of course, SQLite certainly isn't ideal in every way. Tradeoffs, as always. But as far as picking the tradeoffs you are willing to accept for the typical \"MVP\", SQLite chooses some pretty good defaults. reply sgarland 11 hours agorootparent> Often in the real world you need tree-like structures, which are fundamentally not able to be represented by a table/relation. No amount of joining can produce anything other than a table/relation. The n+1 problem is introduced when you try to build those types of structures from tables/relations. I don't know how precisely strict you expect a tree to be in RDBMS, but this [0] is as close as I can get. It has a hierarchy of product --> entity --> category --> item, with leafs along the way. In this example, I added two bands (Dream Theater [with their additional early name of Majesty], and Tool), along with their members (correctly assigning artists to the eras), and selected three albums: Tool's Undertow, with both CD and Vinyl releases, and Dream Theater's Train of Thought, and A Dramatic Turn of Events. The included query in the gist returns all available information about the albums present in a single query. No n+1. The inserts could likely be improved (for example, if you were doing these from an application, you could save IDs and then immediately reuse them; technically you could do that in pl/pgsql, but ugh), but they do work. This is also set up to model books in much the same way, but I didn't add any. > A join is part of one possible hack to workaround to the problem, but not the mathematically ideal solution. Joins are not a \"hack,\" they are an integral part of the relational model. [0]: https://gist.github.com/stephanGarland/ec2d0f0bb54161898df66... reply randomdata 11 hours agorootparent> Joins are not a \"hack,\" they are an integral part of the relational model. Yes, joins are an essential part of the relational model, but we're clearly not talking about the relational model. The n+1 problem rears its ugly head when you don't have a relational model – when you have a tree-like model instead. > The included query in the gist returns all available information about the albums present in a single query. No n+1. No n+1, but then you're stuck with tables/relations, which are decidedly not in a tree-like shape. You can move database logic into your application to turn tables into trees, but then you have a whole lot of extra complexity to contend with. Needlessly so in the typical case since you can just use SQLite instead... Unless you have a really strong case otherwise, it's best to leave database work for databases. After all, if you want your application to do the database work, what do you need SQLite or Postgres for? Of course, as always, tradeoffs have to be made. Sometimes it is better to put database logic in your application to make gains elsewhere. But for the typical greenfield application that hasn't even proven that users want to use it yet, added complexity in the application layer is probably not a good trade. At least not in the typical case. reply sgarland 10 hours agorootparentn+1 can show up any time you have poorly modeled schema or queries. It’s quite possible to have a relational model that is sub-optimal; reference the fact that there are 5 levels of normalization (plus a couple extra) before you get into absurdity. I still would like to know how SQLite does not suffer from the same problems as any other RDBMS. Do you have an example schema? reply randomdata 5 hours agorootparent> I still would like to know how SQLite does not suffer from the same problems as any other RDBMS. That's simple: Not being an RDMBS, only an engine, is how it avoids the suffering. The n+1 problem is the result of slow execution. Of course, an idealize database has no time constraints, but the real world is not so kind. While SQLite has not figured out how to defy the laws of physics, it is able to reduce the time to run a query to imperceptible levels under typical usage by embedding itself in the application. Each query is just a function call, which are fast. Postgres' engine can be just as fast, but because it hides the engine behind the system layer, you don't interact with the engine directly. That means you need to resort to hacks to try and poke at the engine where the system tries to stand in the way. The hacks work... but at the cost of more complexity in the application. reply dventimi 3 hours agorootparentCompare and contrast the query execution stages of PostgreSQL and SQLite. How exactly do they work? Please be as precise as possible. Try to avoid imprecise terms like \"simple\", \"suffering\", \"system layer\", and \"hack.\" reply randomdata 3 hours agorootparentFor what purpose? I can find no source of value in your request. reply dventimi 2 hours agorootparentTo educate your adoring fans reply randomdata 2 hours agorootparentBut for what purpose? There is no value in educating adoring fans. reply dventimi 3 hours agorootparentprev> No amount of joining can produce anything other than a table/relation. The n+1 problem is introduced when you try to build those types of structures from tables/relations. A trivial amount of lateral joins plus JSON aggregates will give you a relation with on record, containing a nested JSON value with a perfectly adequate tree structure, with perfectly adequate performance, in databases that support these operations. There are solutions to these problems. One only needs to willingness to accept them. reply int_19h 7 hours agorootparentprevIf you need to query over a tree data structure, then that's what WITH RECURSIVE is for, and it's present in both DBMS. If you additionally need the result of that query to be hierarchical itself, then you can easily have PG generate JSON for you. reply mkleczek 14 hours agorootparentprev> Often in the real world you need tree-like structures, which are fundamentally not able to be represented by a table/relation. No amount of joining can produce anything other than a table/relation. The n+1 problem is introduced when you try to build those types of structures from tables/relations. You can easily get hierarchical output format from Postgres with its JSON or XML aggregate functions. You can have almost all benefits of an embedded database by embedding your application in the database. Just change perspective and stop treating Postgres (or any other advanced RDBMS) as a dumb data store — start using it as a computing platform instead. reply Ensorceled 15 hours agorootparentprevWe are still dealing with the fact that SQLite still HAS the n+1 \"problem\", it's just fast enough that it doesn't suffer from it. It's a very important distinction because, as you say, there are problem domains where you can't just \"join the problem away\". reply randomdata 15 hours agorootparentAre we? \"Suffer\" was the word used right from the beginning for good reason. Computers aren't magic. I find no importance in pointing out that fact. Presumably everyone here already knows that. And if it turns out that they don't, who cares? That's their problem. Makes no difference to me. reply Ensorceled 14 hours agorootparentI agree that suffer is the right word, but unclear. You are getting down voted because a lot of people are interpreting to mean you are saying applications using sqlite don't have n+1 queries. reply randomdata 14 hours agorootparent1. At time of writing, there has been one downvote in the first comment, followed by one upvote in the subsequent comment. Not only does that not translate to \"a lot of people\", it was quite likely just one person. And unless that person was you, it is impossible to know what their intent was. I'm not sure what are you trying to add here. 2. Who gives a shit? If the \"computers truly are magic\" camp don't understand what I wrote, great! It wasn't written for them in the first place. If that compels them to use their time pressing a button instead of learning about how computers work, great! Not my problem. I'm not sure what you are trying to add here. reply Ensorceled 11 hours agorootparent> Who gives a shit? [...] I'm not sure what you are trying to add here. I guess nothing. You must be fun at design reviews. reply randomdata 4 hours agorootparentWell, let's hope the \"computers are magic\" riff-raff never show up at the design reviews. Especially if they expect someone to explain to them the basics of computing without any reasonable offer of compensation in return. If those people show up here and put on a tantrum by pressing random buttons or whatever it was that you were trying to point out... Oh well? I wouldn't have even noticed if you didn't bring it up. What value you found in calling attention to their pointless tantrums is an interesting mystery, though! reply tangjurine 18 hours agorootparentprev> Postgres suffers from the n+1 problem, while SQLite does not. ? reply randomdata 18 hours agorootparentIndeed. reply prisenco 18 hours agorootparentIt’s worth elaborating: N+1 Queries Are Not A Problem With SQLite https://www.sqlite.org/np1queryprob.html#:~:text=N%2B1%20Que.... reply EasyMark 19 hours agorootparentprevit's much bigger and requires running a server, that's why I use sqlite3, and my needs are 99% modest most of the time. reply zilti 17 hours agorootparentIt requires running a separate process. reply neovim_btw 14 hours agoparentprevNot with concurrent writes, you're not! Even with a WAL or some kind of homegrown spooling, you're going to be limited by the rate at which one thread can ingest that data into the database. One could always shard across multiple SQLite databases, but are you going to scale the number of shards with the number of concurrent write requests? If not, SQLite won't work. And if you do plan on this, you're in for a world of headaches instead of using a database that does concurrency on its own. Don't get me wrong; SQLite is great for a lot of things. And I know it's nice to not have to deal with the \"state\" of an actual database application that needs to be running, especially if you're not an \"infrastructure\" team, but there's good reasons they're ubiquitous and so highly regarded. reply prisenco 14 hours agorootparentIt’s effortless to get 2-4K writes per second with SQLite on cheap commodity hardware. That will carry most early stage applications really far. reply turnsout 19 hours agoparentprevI’m with you in general, but what about vector search? It really feels like the DB industry has taken a huge step backward from the promise of SQL. Switching from Postgres to SQLite is easy because the underlying queries are at least similar. But as soon as you introduce embeddings, every system is totally different (and often changing rapidly). reply randomdata 18 hours agorootparentJust use SQLite? Specialized vector indexes become important when you have a large number of vectors, but the reality of software is that it is unlikely that your application will ever be used at all, let alone reach a scale where you start to hurt. Computers are really fast. You can go a long way with not-perfectly-optimized solutions. Once you have proven that users actually want to use your product and see growth on the horizon to where optimization becomes necessary, then you can swap in a dedicated vector solution as needed, which may include using a vector plugin for SQLite. The vector databases you want to use may or may not use SQL, but the APIs are never that much different. Instead of one line of SQL to support a different implementation you might have to update 5 lines of code to use their API, but we're not exactly climbing mountains here. Know your problem inside and out before making any technical choices, of course. reply turnsout 16 hours agorootparentYou can of course use a vanilla database, read every row and just roll your own vector distance function, but it's just frustrating that there isn't a standardized pattern for this. There are plenty of proprietary databases and APIs, but now you're taking on a dependency and assuming a certain amount of risk. reply randomdata 16 hours agorootparent> it's just frustrating that there isn't a standardized pattern for this. Be the change you want to see, I suppose. No doubt convergence will come, but it is still early days. Six months ago, most developers didn't even know what a vector database is, let alone consider it something to add to their stack. It took SQL well into the 1990s to fully solidify itself as \"the standard\" for relational querying. Even PostgreSQL itself was started under the name POSTGRES and was designed to use QUEL, only moving over to SQL much later in life when it was clear that was the way things were going. These things can take time. reply dventimi 3 hours agorootparent> It took SQL well into the 1990s to fully solidify itself as \"the standard\" for relational querying. IBM had SQL in their database product in 1981, Oracle had it by v4 in 1984, ANSI picked SQL as its standard that same year, and completed the first version by 1986. Some time scientists say that the 1980s occurred before \"well into the 1990s\" but I mean, who can really say, right? reply prisenco 19 hours agorootparentprevThere are vector search solutions for sqlite that basically work, so if my project doesn't rely on that 100% then I'm willing to use those as stop-gaps until it does. Of course, if there's a shortcoming of sqlite that I know I need right out of the gate, that would be a situation where I start with postgres. reply nycodez 16 hours agorootparentprevPostgres has a vector search extension! https://supabase.com/docs/guides/database/extensions/pgvecto... reply drittich 19 hours agorootparentprevVector similarity searches can be done with simple SQL statements, and while the performance will not match a vector db, it's often good enough. For a db like SQLite that can run in-memory, I suspect they would be reasonably fast. reply bun_terminator 18 hours agoprevAs a hardcore c++ guy, I recently switched to a company heavily into databases. I never had contact to databases before. And I'd like to go one step further: Why databases? I come from an industry that heavily uses custom binary file formats. And I'm still bewildered by the world of databases. They seem to solve many issues on the surface, but not really in pratice. The heavy limitations on data types, the update disasters, the incompatibility between different SQL engines etc all make it seem like an awful idea. I get the interop benefits, and maybe with extreme data volumes. But for anything else, what's the point? Genuinely asking reply dig1 18 hours agoparent> They seem to solve many issues on the surface, but not really in practice I believe you haven't had a chance to work on problems that require an actual database. Multi-user access, ACID support, unified API (odbc/jdbc), common query language... all of these would require many man-years to set properly with a custom solution. > the update disasters, the incompatibility between different SQL engines etc all make it seem like an awful idea What update disasters? If you meant by updating database versions, these aren't things you do frequently because the database is expected to be running 24/7. But Postgres and Mysql are already rock solid here. Wrt SQL engine incompatibilities, you usually set with a single database vendor in practice. If you suddenly start to switch databases in the middle of the project, something needs to be fixed with the process design, not database. reply bun_terminator 18 hours agorootparentAll these comments seem to fuel my suspicion that we in fact shouldn't use databases, because we don't use any of these features. We just use them as external data storage for a single application. And not even that much data, likebecause we don't use any of these features. We just use them as external data storage for a single application. You are using it :) Reboot the server where the database runs or suddenly cut off the connection. Unless you have ACID-compatible storage, you'll have malformed data. Plan for the future and use a database from the start. When your project/company expands and starts to use multiple applications/services (and that inevitably happens), you'll see (one of) the benefits of the database. > But the updating I would have expected to go more smoothly I'm not sure what you are talking about. Database updates are one of the smoothest (critical) software updates you'll find, assuming the database has a good track record. reply bun_terminator 14 hours agorootparenteh the oracle upgrades went awful. But I try to not touch the database at all if I can reply ndriscoll 16 hours agoparentprevIt gives you an easy, high-level way to use high performance data-structures and algorithms. You don't need to explicitly write or rewrite code to maintain hash maps or b-trees or whatever and to use the right structures for fast lookups from one set of data to another. You just say \"CREATE INDEX name ON table USING HASH(column)\", and from then on, your hash map will maintain itself, and any lookups that would benefit from that hash map will automatically use it. No need to rewrite any of your code that needs to work with that column. In some cases, it will also automatically do things like make temporary hashmaps or bitmaps for just the life of a query to speed it up. You mention further down a \"mystery box of performance\", but if you understand what data structures it's using and how it uses them, then it's generally pretty straightforward. Mostly you can reason about what indices are available and how trees work (and e.g. whether it can walk two trees side-by-side to join data) to know what query plan it should make, and you can ask it to tell you what plan it makes and which indices it used. Likewise, if you have a query plan you want it to run (loop over this table, then use this column to look up in this table, etc.), you'll know what indices are needed to support that plan. If people struggle with using a database correctly, they're really going to struggle with using something like a b-tree in a way where you don't corrupt your data in the event of a power loss or crash, or in a way where multiple threads don't clobber each other's updates or create weird in-between states (or you just use a global lock, but then you lose performance). reply bun_terminator 14 hours agorootparentIt's pretty opaque to me. If something in my cpp code is dodgy or runs slow, I can use a number of debugging and profiling tools. While I don't really even know how databases work on the insides, let alone profile or diagnose them. To this day my colleagues rewrite equivalent SQL statements because some run better than others. And we regularly run into unexpected latency spikes where most of the time a statement runs fine, but even nth time it's several magnitudes of times slower - and no one knows why. So we cobble code and caches around things. It all seems pretty mind boggling to me. reply ndriscoll 13 hours agorootparentIf you're going to find yourself working with databases, I'd suggest learning some about the internals (that's probably true of anything). In particular Markus Winand's information[0] is great for building an intuition about why different types of queries work with different types of indices. I don't know about Oracle, but Postgres and Mysql have pretty detailed documentation once you have that foundational knowledge. If you learn about the internals of a thing, especially when your background is in lower level dev like C++, then the use-cases are more obvious: you use the thing whenever you would've done what it does internally, but it gives you that functionality off-the-shelf and wrapped up in a way where you can write business logic without getting bogged down in details of tree-traversal and stuff. Once you get comfortable with it, you expand that to using it when you might not have done things exactly that way, but eh it's close enough and lower effort. Sometimes truly equivalent SQL statements will be faster just because the optimizer is not perfect. e.g. I've had cases where I had a templated query with a GROUP BY some id, and then other code added on a HAVING for that same id, and I know it should be algebraically valid to push the HAVING into a WHERE so it runs before the GROUP BY (and filtering before the aggregation would be much cheaper), but mysql just didn't have that optimization. Dealing with this kind of thing can be annoying. Other times you might have something like a compound index, and you might add a WHERE that you know for business reasons is redundant because the thing you're trying to filter on is not the first column in the index. Understanding why that works comes down to understanding what a compound index \"looks like\" as a tree. One thing that I imagine a database from the future could do is let you define logical implications like that (e.g. StateOrProvince = California implies Country = United States, or maybe deleted_at >= modified_at >= created_at, or a.id > b.id implies a.created_at > b.created_at) that it could use for query planning. But in general, if you learn how it works, and then think of it as a way to not have to write that functionality yourself (but understand that the trade-off is some rigidity in your ability to customize it), it will make more sense, and you'll be able to become one of those wizards that just knows how to rewrite a query to something that ought to be the same, but is for some reason much faster. [0] https://use-the-index-luke.com/ reply bun_terminator 12 hours agorootparentI don't plan on staying long enough so that a deep investment is useful. I'd rather continue my efforts into getting rid of some database stuff. I certainly won't work with databases ever again after this, so every hour I spend with it is wasted. I'll pass on this technology. But I appreciate the effort. reply bsdpufferfish 18 hours agoparentprevRead the original relational database paper by Codd. The problem is that trees of data are not flexible for querying, you really need a graph. If you draw it out on paper, you will find a relation is a very efficient way to store general graphs. I'm actually suprised to hear this from a C++ engineer, because sorting and searching columns of data is the name of the game in C++. Another key problem they solve is separating the logical representation from the on disk representation. You can evolve and and optimize storage without breaking anything. The other problem with files is that you have a disconnect between in memory and on disk. You have to constantly serialize and deserialize. Sqlite has quite a bit of info on this: https://www.sqlite.org/appfileformat.html reply bun_terminator 17 hours agorootparentBut that's the thing: All that is nice on paper. Of course the relational nature is nice. Of course the disk representation is nice. But I never feel like that's worth the trouble day in day out. The costs, the mystery box of performance, the insane statements, the extra hardware, the updates, the strange data types, the box of tricks everyone needs to make them behave. But I've been made aware that our usecases suck, so I'll attribute most to that. reply hobs 17 hours agorootparentThe problems are not specific to SQL though - if you want to store and manage huge amounts of data you're going to need some specialized data structures, some way to manage disks paging and memory, manage consistency, isolation, durability, and atomicity of your changes. The general idea is that its such a PITA to deal with all that so just let a database do it, and of course the consequence is an often leaky abstraction because computers suck. reply treflop 18 hours agoparentprevWell a database is just a binary file format with an API attached on. Because having hundreds or thousands of concurrent file handles across a data center is kind of hard. reply bun_terminator 18 hours agorootparentok I get that, but at least our applications usually only have one thing (one thread of one process on one machine) operating on the database. So maybe it's just our silly usecase reply marcosdumay 18 hours agorootparentThat's unusual. The rule is that you replicate the database consumers lots and lots of times. Still then, a database will improve your data access, validate your data, and provide interop in case you need to use the file with different software. It's also a very high quality serialization library, that replaces one of the most vulnerability-enabling layers of your software. (But then, I've just read you use Oracle, so maybe forget that one.) reply afandian 18 hours agorootparentprevIt's a narrow use-case that doesn't cover RDMS. SQLite fits it though. reply bun_terminator 18 hours agorootparentThe first thing I did was ask why we don't use sqlite or at least postgres. The answer was that they are free and therefore we don't trust them. So Oracle it is. Which is bananas because our customers have to buy an oracle license, which is money that we don't get. Pretty wild reply vhcr 17 hours agorootparentYour experience with databases was probably clouded by having to use Oracle. reply int_19h 7 hours agorootparentIt could be worse; there's always DB2... reply edwardsdl 5 hours agorootparentprev> The answer was that they are free and therefore we don't trust them. If you don’t mind a bit of unsolicited advice: run. reply bun_terminator 5 hours agorootparentI mean you do a job because of the pay, not because you like the people or they are particularly competent. So it's a good gig for two years or so reply edwardsdl 4 hours agorootparentI’ve found that surrounding myself with competent people - or better yet, people far better than me - has lead me to much better paying jobs. reply bun_terminator 38 minutes agorootparentIt also makes you look less like a genius haha. Yeah maybe you're right. It wasn't the best choice in hindsight. But switching jobs is a relatively rare thing in Europe. You only can do that ever so often, so I'll have a bit of waiting to do reply themerone 17 hours agorootparentprevSqlite is running on billions of devices and has a test suite with 100% branch coverage. Only a fool would put more trust in Oracle. reply tootie 17 hours agorootparentprevI think it's important context that gets lost in so many tech conversations. Software engineering is a vast field covering almost every domain of human endeavor and they're all going to have different constraints. Some are technical and some are social. If you're working with 30 other engineers trying to marshal and unmarshal the same set of data for different purposes, it's really easy to tell them all \"use SQL\" instead of \"here's the binary format invented by the guy who quit 7 years ago\" even if that binary format was technically superior. reply DanielHB 14 hours agoparentprevBesides what others are saying here, data integrity is a lot easier with a relational database if your data is heavily relational. I have managed systems withWouldn't it be nice to be able to write a trigger that called a web service? Rather you than me. reply cryptonector 15 hours agoparent> > Wouldn't it be nice to be able to write a trigger that called a web service? > Rather you than me. Yes, don't do that. Instead consume notification streams or logical replication streams and act on those -- sure, you'll now have an eventually consistent system, but you won't have a web service keeping your transactions from making progress. You don't want to engage in blocking I/O in your triggers. reply adlpz 20 hours agoprevLove postgres and use it extensively. However, there's always an issue when I start doing the more advanced stuff: how do I combine that with all my years of experience with version control, code reviews, types, tests, static analysis and all the niceties of coding in general? Migrations? reply agentultra 19 hours agoparentIt's not a great DX for sure. Once you start stuffing a ton of application logic in Postgres triggers (or worse split logic between your application layer and triggers) your system will explode in complexity really fast. Developers will insert a row in an inconspicuous table and things will break in ways that seem mysterious because it's not obvious, looking at the application code, why the system is misbehaving. There are commands in most postgres clients, even psql, to view the _currently_ defined functions... but when you go to debug those you will have to look through migrations to see how the function came into it's current state... bisecting through history here is not very useful since each change to the function is a new file. I think this can be fixed though and made much easier, it's just not there yet. In general I don't think the developer tooling is up to par to push very much of your application logic into postgres itself. I recommend using triggers for consistency and validation or table-local updates (ie: timestamps, audit logs) but keep process-oriented behaviour (iow: when this happens, then that, else this, wait for call and insert here, etc) in the application layer (chasing a cascade of triggers is not fun and quite annoying). ... all that being said, you can do unit testing in Postgres. And there is decent support for languages other than pgSQL (ie: javascript, ocaml, haskell, python, etc). It's possible to build dev tooling that would be suitable to make Postgres itself an application development platform. I'm not aware of anyone who has done it yet. reply vmfunction 19 hours agoparentprevIf you want to serious develop postgres as how you develop code, then you should also use a LSP. https://github.com/supabase/postgres_lsp reply cpursley 16 hours agorootparentThanks, I need to add this to my list as well. reply cpursley 16 hours agoparentprevMigrations. All my database logic lives in version control. Popular tooling like Phoenix, Hasura, etc have good built in migration stories. https://www.bytebase.com looks really promising. Hover, I do struggle with one big issue: changing database logic (views, functions, etc) that has other logic dependent on it. This seems like a solvable problem. reply gen220 14 hours agoparentprevYou might be curious to look up how Alembic and Django accomplish migrations. Even if Python's not your jam, the concepts/semantics are language/framework agnostic. Basically, you store each migration in a file, and you \"squish\" the migration history down to a table definition once you've decided you're happy with the change and it's been affected across all the different deployment environments. It's not perfect, but it works reasonably well. reply 0xbadcafebee 15 hours agoparentprevRDBMS's are still very much an 80s thing. Great at dynamic queries, horrible at managing changes in an immutable, atomic, versioned way. The structure and relationship of data is so brittle it's kind of crazy it's still used. Probably it's because so many people have grown dependent on its particular flaws; the incumbent just lumbers on. reply tmountain 14 hours agorootparentThere’s always a trade off though. Append-only storage models are great for use cases where they are truly necessary, but they also store substantially more data than mutable databases do, and designing for something like Datomic carries a heavier cognitive cost than a simple RDBMS. reply jackschultz 19 hours agoprevWas talking to coworker yesterday about a spectrum of where code lives, and the differences from where I started to where I am now in understanding. Start after college and backend web dev was fully in scripting language, Python or Ruby, and ORMs that completely fogged where any of the data was stored. Rails and ActiveRecord is so good at shrouding the database to the point where you type commands that create databases and you never see them. Classes are written to describe what we want the data to look like and poof! SQL commands are created to build the schema that we never need to see. On this end of the spectrum, the scripting language will stay the same, but we want to be agnostic to where the data is stored. On on the other end of the spectrum, Postgres is enough. More than enough. Like in the link, it can do all the tasks you ever care about. The code you're writing for the backend / data is about data, not about the script. We care where it's stored, that it's clear the structure, the reads and updates are efficient. We can write all statements in SQL to create tables, functions, trigger, queues, and efficient read queries with indexes to make the data come back to the scripting language in the exact form that's wanted. On this end, we know and optimize how the data is stored, agnostic to the scripting language that uses the data. I went from the first end of the spectrum to the second. Everything can be done in Postgres. Audibility, clarity, efficiency is much better there than in Python, is my position. The only thing holding it back is that people don't see development from the data side yet, and if you're deciding on tech, it's not easy to use a tech that people don't have as good of development ability yet. There are no Postgres bootcamps right now. But There's more and more adoption of this I'm seeing, and the money and development of Postgres leads me to trust that it'll be around a very long time, only getting better. Posts about the power of databases, Postgres and some SQLite for example are becoming more and more common. It's a cool change to follow and watch grow. reply cpursley 16 hours agoparentThis was similar to my journey as well. I'm a self-taught developer and was so green when I learned Rails I didn't even understand that there was such a thing as SQL behind the ORM. Took some grey beard .net folks to share the and power of the database. reply pjmlp 19 hours agoparentprev>....backend web dev was fully in scripting language, Python or Ruby... Not on my bubble, it has been fully in .NET and Java since 2001, with exception of a couple of services written in C++. reply throwaway918274 19 hours agoprevMy Trifecta is: Postgres, Redis, S3 Hasn't steered my wrong yet. Every once in a while I'm tempted to try to use Postgres for Pub/Sub but then I realize that I need Redis for caching and sidekiq anyways, and Redis is amazing too, so why bother. reply cpursley 16 hours agoparentIf you're open to Elixir (you'll like it coming from Ruby) then you don't even need Redis. Oban + Postgres for jobs, WalEx for database events, Nebulex for distributed caching. It simplifies things so much (and is cheaper to run). reply gen220 14 hours agoparentprevHave you had to deal with read-heavy/OLAP queries? Of the kind that can't be cached effectively (i.e. arbitrary filters). reply bearjaws 20 hours agoprevPostgres is enough as long as you have a good multi-tenant setup e.g. a separate database per customer. Ran a single postgres instance with multi-tenant SaaS product that crossed 4B records in a few tables, even with partitions and all the optimization in the world, it still hurts to have one massive database. We still got bought tho, so I will agree its enough reply Ensorceled 19 hours agoparent> Postgres is enough as long as you have a good multi-tenant setup e.g. a separate database per customer. Separate per customer has a lot of advantages (especially around customer security and things like deletion) but you can also shard by customer right from the beginning; customer #1 in the \"odd\" shard, customer #2 in the \"even\" shard etc. Switching to database per customer if that is working well is relatively easy so you're future proofed both ways. reply breckenedge 19 hours agoparentprevWorked at a company that suffered from this too. Compounded by the feature that let customers share data with other customers, so single DB was a decent architecture to begin with. And we were on postgis v1, which had no straightforward upgrade path to 2.0, nor was it supported by later versions of Postgres. Restoring a backup became a multi-day affair. reply cpursley 16 hours agoparentprevThat's easier now than ever with services like neon.tech and fly.io where you can quickly spin up new databases via api. reply refset 15 hours agorootparentBetter still, take a look at Nile's \"tenant virtualization\" concept: https://www.thenile.dev/ reply cpursley 12 hours agorootparentThat is cool, thanks! I feel like this is going to solve a lot of saas businesses problems. reply jimbokun 16 hours agoparentprevHow did you use partitions for scaling? Hash based partitioning on the primary keys of your tables? Or something else? reply cyrialize 19 hours agoprevI love postgres - although there are some things I dislike about it - mainly due to me using mysql for a years. All dislikes are mainly syntactical things, like having USING in deletes instead of JOINs. I also liked the output of EXPLAIN more in mysql. Using postgres made me realize that many of the people I work with had no idea what an ambiguous group by is, because unlike mysql, postgres will never, ever let you do an ambiguous group by. Running into many different things like that over the years has really made me realize how nice postgres is. reply sgarland 17 hours agoparentWhile I love MySQL for a variety of reasons (and think it's legitimately better for certain features – tangential to your point, being able to ORDER BY in a DELETE is great), its lax GROUP BY option isn't really a great idea in terms of correctness. You can emulate it in Postgres with DISTINCT ON if desired, and as long as you also ORDER BY something that makes sense for your query, it should work similarly. reply mihaic 14 hours agoprevAlmost every single time I've seen it was better to first prototype a new feature with the existing stack rather than introduce something new. And by careful curation, that initial prototype could be turned into production code with the same stack as well. At some point though the system does creek, and you're starting to wish you'd have a Redis or another specialized tool. The only important thing though for me is making sure I'd spend some time on writing a nice API wrapper on my end, and only call that. When I really need to use Redis, all that should be needed is changing the implementation inside your wrappers, and testing the migration very well. People are surprised how long you can delay to make a technical decision, until it becomes obvious. reply HermitX 14 hours agoprevI really like Postgres, but I still don't think Postgres can do everything. From my own experience, when the scale of data becomes very large, when you need to analyze massive amounts of data, your Postgres always seems somewhat inadequate. I think Postgres performs perfectly when dealing with OLTP-type workloads. However, if you need more OLAP support, I recommend you use StarRocks (www.starrocks.io). We now import data from Postgres into StarRocks for data analysis, and the experience is fantastic. Moreover, StarRocks also supports direct queries on data lakes, which is incredible. reply wesselbindt 20 hours agoprevSeems like it's less than a week ago that someone (rightly) posted that it isn't [1]. Postgres is a relational database, and it does this really well. It's not a convection oven. [1] https://news.ycombinator.com/item?id=39243655 reply karmakaze 19 hours agoparentThat is specifically about AWS/RDS and relates to EBS/IOPS moreso than PostgreSQL. In fact their solution was to run the db but not as an RDS instance. > The Solution: Roll your own: For us the solution was fairly simple: don’t use a managed database services and roll our own infrastructure on EC2. I've done the same running PostgreSQL on EC2 instances with NVMe local raid storage. It's very fast, but then you're responsible for its uptime, updates, backups, etc. That setup was used for performance sensitive but less critical data. reply nahnahno 19 hours agoparentprevDid you read the article? It’s about how expensive RDS is, not about how Postgres didn’t work. reply RockyMcNuts 15 hours agoprevMichael Stonebraker invented Postgres and has long said 'one size fits all' is an idea whose time has come and gone. https://cs.brown.edu/~ugur/fits_all.pdf the relational ACID model is overkill for mostly-read data warehouse and verticality helps; streaming is different; graph dbs are different. Postgres may not be 'all you need', it will take you pretty far though, maybe it's 'all you need most of the time'. 60%+ of the time it works every time. reply qaq 19 hours agoprevOne cool thing if you are running off Postgres - DDL is transactional so you get clean deploy of your new code version (stored procedures) with precise switchover for free. reply kevinmershon 19 hours agoprevThank you for this post. I've been looking high and low for weeks trying to find a way to connect Looker to a GraphQL API unsuccessfully and apparently \"PostgreSQL emulators\" are called FDWs as I learned from this link. I had searched every term I could think of for how to simulate or proxy a SQL database into an API but calling it a foreign data wrapper would have never occurred to me. reply pcthrowaway 19 hours agoparentDid you try describing the problem and the thing you're looking for to chatgpt to see if it knows of a name? I find that making connections like this is one of the things it actually does well reply kevinmershon 19 hours agorootparentI actually did, repeatedly. I threw away 3 or 4 chat sessions with it about this very subject. It kept hallucinating features into libraries and naming products that don't actually support this translation direction. And with Google searches the vast majority of results are going the opposite way: creating a GraphQL API from a real SQL schema. reply darkerside 19 hours agorootparentprevRemember when you could do this with Google? reply ThePhysicist 19 hours agoprevHow about metrics? Would it be possible to do this in Postgres as well? I'm talking e.g. about simple counters that are not directly vital to business logic. I'm currently using Redis for those things as I'm thinking that Postgres seems to be the wrong choice for this kind of data, would anyone disagree? reply tutfbhuf 19 hours agoparentYou can use the TimescaleDB PostgreSQL extension. It works really well and has a high compression rate for time series data, such as metrics. It can also downsample data. I have also tried InfluxDB, but you have to rewire your mental model to fit InfluxDB (it has its own query syntax and internal mechanism). They have versions v1, v2, and v3, and it's not clear which one to use, probably v2 at this point in time. It's kind of confusing and the state of development is very much in flux. I think TimescaleDB works well up to the single-digit terabyte range (according to various sources). If you need a solution in the multi-digit terabyte or petabyte range, then you probably need something like a distributed VictoriaMetrics setup. https://github.com/timescale/timescaledb reply __s 19 hours agoparentprevSo, having worked on a managed postgres service, we had an extension pushing metrics from the database, but we also used redis to keep service metrics Redis is great. This gist is more about how it's up to you how much you centralize on postgres. But overall being able to offload from the database has value, so \"PostgreSQL is Enough\" should not read as \"You shouldn't need more than PostgreSQL\" Metrics in an unlogged table could be great if you want to query those metrics against existing data. It depends reply tegdude 16 hours agoprevHas anyone made “I am Postgrenough” merch yet? reply jurschreuder 11 hours agoprevThe thing I'm missing in PG is jsonb compression the way mongo does it. We have both Mongo and PG. PG is much simpler and I would love to ditch Mongo for simplicity. Only thing I would need is a \"dumb\" compressed jsonb column. No updates, no queries, only insert, select, delete. And the same 80-90% compression without maintenance as Mongo on highly repetitive json keys. reply talkingtab 19 hours agoprevThere is a lot here that I find useful. Mastering multiple technologies: html, node, JavaScript, SQL, CSS - grids and flex, blah, blah, blah is simply impossible. Developing a working set of knowledge is not. The problem is that you don't know what you should know. A working set raises another set of issues: how do the pieces relate? What is the proper role of each technology? This gist is extremely useful to me both in helping me to understand more of what I need to know. The article https://sive.rs/pg was very helpful as one way to think about how postgresql could and should fit in. reply redskyluan 17 hours agoprevRegarding scale, PostgreSQL may not cover all bases. Though I'm a PostgreSQL fan, I prefer specialized services for specific tasks. Using PG-based plugins could help, but a dedicated SQL-compatible database is often a better fit. For vector retrieval, going with a database like Milvus, designed for vectors, is usually more efficient and cost-effective. Similar principles apply across domains. Is there any vectordb under PG format? What if we've got a deeply customized distributed vector search service on PostgreSQL, that's impressive! reply SparkyMcUnicorn 17 hours agoparentpgvector[0] is supported by AWS RDS, Azure, etc. I haven't gotten a chance to try out Latern[1] yet, but have heard some good things[2]. [0] https://github.com/pgvector/pgvector [1] https://github.com/lanterndata/lantern [2] https://tembo.io/blog/postgres-vector-search-pgvector-and-la... reply adamcharnock 19 hours agoprevIs anyone aware of a streams implementation for Postgresql? I see we can do simple queues, but how about a stream with multiple consumers / consumer groups (much like the Redis streams implementation)? reply benrutter 20 hours agoprevOn the [Simplify: move code into databases](https://sive.rs/pg), has anyone actually tried this? My instinct is that the output won't be simpler at all, but a big twisty web of varying parts and competing use cases, all extremely hard to discover. To me, having two things that do something distinct is simpler, and ironically, Rich Hickey says this exact thing in Simplicity Matters, which is quoted in the article. reply jjice 19 hours agoparent> To me, having two things that do something distinct is simpler I completely agree. Inside the DB, it's hard to argue about performance benefits in a lot of situations. That said, debugging, testing, and developing are so much harder. I love to have a well written bit of code that I can integration test as well as mock up and use for unit tests with other components of my application. It's much trickier to do that in the DB. Sometimes it is a lot more performant, and that's a situation you may want to consider moving towards, but I would try to stay away from it for as long as the performance gain isn't huge. reply wg0 19 hours agoparentprevOne caveat is when you have to upgrade across major database versions. The more your application utilises these features, the more you're likely to hit when next major version upgrade has to happen. Not that major database versions are any simpler even if you just stick to CRUD for databases. reply cpursley 10 hours agorootparentThis is also true of upgrading dependencies in the application layer. reply KronisLV 19 hours agorootparentprev> Not that major database versions are any simpler even if you just stick to CRUD for databases. I mean, even LTS versions have EOL dates, so even at the least frequent, you'll need to do upgrades around every 5 years or so. reply Kinrany 17 hours agoparentprevHaving two distinct things doesn't mean placing them in separate processes. Or we'd all be writing Erlang by now reply cpursley 16 hours agorootparentWe kinda all are writing Erlang now, just inferior and overly complex abstractions (\"serverless\", K8s, etc). reply jwoq9118 18 hours agoprevSupabase integrates a lot of the open source tools listed here. A Postgrs backend as a service offering. Just started using it. Very impressed. reply CraigJPerry 20 hours agoprevAspiring to \"use the right tool for the job\" leads you into conflict with some of the ideas posted on the gist. To give a concrete example on the first item in the gist: if i need periodic jobs - and all the operational headaches that go with (rerunning, ordering, dependencies, logging, yada yada...) - is postgres the right tool for the job? It CAN be, but for most people in most circumstances, it's probably not. reply jerrygenser 20 hours agoparentIf the purpose is to do database specific things then pg cron can be useful for e.g. periodic vacuums. However for running application logic, maybe less so. reply didip 18 hours agoprevSpecifically with PostgreSQL, number of connections is still the killer (if you don’t have smart proxies). So you can’t be cavalier about putting as many use cases as possible. For example, when using PG pub/sub, you will run out of connections quick. Generally, all DBMS needs a smart self adjusting query killer. Without it, one bad query will ruin it for everyone. reply cpursley 16 hours agoparentWalEx instead of pub/sub (listen/subscribe): https://github.com/cpursley/walex Supavisor connection pooler: https://github.com/supabase/supavisor reply 0xbadcafebee 15 hours agoprevSo, rather than use the right tool for the job, use one tool for everything. reply int_19h 4 hours agoparentUse one tool that's known to be \"good enough\" for most things. If you run into actual perf issues, then look at something else. reply rrr_oh_man 3 hours agoparentprev\"I'm stuck in the Taiga and a hammer is all I have\" reply hot_gril 16 hours agoprevI use Postgres for a lot, but I can't imagine using it to make HTTP requests. reply Chiron1991 16 hours agoparenthttps://github.com/pramsey/pgsql-http reply hot_gril 14 hours agorootparentThat's what I was referring to, wondering if anyone uses it and why. It even says \"Why This is a Bad Idea\" at the bottom. Maybe for databases queried directly by users who also need to mix in API responses, and you're wrapping this all up in triggers or some other stored procedure cause you really want to use Postgres without some controller written in Python or JS. reply cryptonector 15 hours agorootparentprevNo. (You don't want to cause blocking I/O in transactions.) reply cpursley 12 hours agorootparentNon-blocking: https://github.com/supabase/pg_net reply hot_gril 11 hours agorootparentImpressive! reply cpursley 12 minutes agorootparentYeah, still not sure I’d use it but a pretty interesting thing to do with the database. reply KingOfCoders 19 hours agoprevI think so too [0] glad more and more people come to this realization. It reduces cognitive load tremendously. [0] https://www.amazingcto.com/postgres-for-everything/ reply samtheprogram 16 hours agoprevI just wasted a lot of time debugging database triggers. I wish the ergonomics around some PostgreSQL features were better, but I do love the idea that it supports so much at the database level. reply jon_adler 15 hours agoprevSomewhat related is Dan McKinley’s Choose Boring Technology. https://boringtechnology.club/ reply code-faster 19 hours agoprevA response: https://codefaster.substack.com/p/postgres-is-enough-but-is-... reply geon 19 hours agoprevThere is https://github.com/pipelinedb/pipelinedb for streaming time series data, but it is abandoned since 6 years. reply philippemnoel 19 hours agoparentThere is now https://github.com/paradedb/paradedb/tree/dev/pg_analytics (disclaimer: I am one of the makers of pg_analytics) reply jbverschoor 18 hours agoprevSure, but I love SQLite’s single file simplicity reply chasers 18 hours agoprevbtw recently cleaned up my wal cache busting code quite a bit if you're interested. https://github.com/Logflare/logflare/blob/main/lib/logflare/... Need to make a lib out of this!! reply cpursley 16 hours agoparentThanks from one Elixir-using Chase to another ;) reply karol 19 hours agoprevCould you write an OS based on PostgreSQL?:) reply cpursley 16 hours agoparentNot quite an OS, but they are doing some neat things: https://omnigres.com/ reply petercooper 19 hours agoparentprevIt depends on your definitions, but if you took psql and Postgres and compared it to something like an OS on a 1980s microcomputer, you're kinda there. You can run arbitrary commands, you can program it, you can debug those programs, you can run programs, and you can access storage. If you're talking more like Linux or Windows, you have.. a bit more work to do ;-) reply hot_gril 16 hours agoparentprevI've thought about writing a general-purpose programming language that uses Postgres as its runtime, with the twist that it can save checkpoints to survive system reboots and also interop closely with SQL. Postgres already has a lot of nice built-in types and functions. reply papichulo2023 16 hours agoparentprevIsnt SQL turing complete? reply 30 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The summary provides a list of tools and resources for using PostgreSQL in various applications and scenarios.",
      "It includes links to GitHub repositories for code examples and further information on topics such as background and cron jobs, message queues, GIS/mapping, audit logs, access control, authorization, search functionality, time series data, graph data, foreign data, HTTP interactions, APIs, events/replication/CDC, unit tests, migrations, dashboards/UIs, data visualization, and language servers.",
      "Readers are invited to share any additional resources or tools they may be aware of."
    ],
    "commentSummary": [
      "PostgreSQL is a versatile but challenging database management system often used in software development.",
      "The passage highlights the benefits and limitations of using PostgreSQL and recommends leveraging existing technology whenever possible.",
      "It provides insights into various patterns and libraries for effective usage, scaling, handling complex application architectures, and understanding the trade-offs between different databases."
    ],
    "points": 371,
    "commentCount": 282,
    "retryCount": 0,
    "time": 1707226107
  },
  {
    "id": 39278631,
    "title": "Recognizing the World's Problems and Progress: A Path towards a Better Future",
    "originLink": "https://ourworldindata.org/much-better-awful-can-be-better",
    "originBody": "The world is awful. The world is much better. The world can be much better. It is wrong to think these three statements contradict each other. We need to see that they are all true to see that a better world is possible. By: Max Roser October 4, 2018 Cite this articleReuse our work freely The world is awful. The world is much better. The world can be much better. All three statements are true at the same time. Discussions about the state of the world too often focus on the first statement: The news highlights what is going wrong, rarely mentioning any positive development. A pushback on this narrative takes it to the other extreme, which is equally damaging. Solely communicating the progress that the world has achieved becomes unhelpful, or even repugnant, when it glosses over the problems that are real today. It’s hard to resist falling for only one of these perspectives. But to see that a better world is possible, we need to see that both are true at the same time: the world is awful, and the world is much better. To illustrate what I mean, I will use the example of one of humanity’s biggest tragedies: the death of its children. But the same is true for many of the world’s other problems. Humanity faces many problems where things have improved over time, which are still terrible, and for which we know that things can get better.1 The world is awful Globally, 4.4% of children die before they are 15 years old. This is the data for 2021, the latest available year. This means that 5.9 million children die every year – 16,000 children on any average day and 11 children every minute.2 Clearly, a world where thousands of tragedies happen every single day is awful. The world is much better History’s big lesson is that things change. But it is hard to imagine how dire living conditions once were, making it difficult to grasp just how much the world has changed. Data can help to bring the scale of change to mind. Historians estimate that in the past, around half of all children died. This was true no matter where in the world a child was born.3 It’s hard to imagine, but child mortality in the very worst-off places today is much better than anywhere in the past. Even in the world’s richest countries, the mortality of children was much higher very recently. In Niger, the country with the highest mortality today, about 14% of all children die.4 Just a few generations ago, the mortality rate was more than three times as high, even in the best-off places.5 What we learn from our history is that it is possible to change the world. Unfortunately, long-run data on how living conditions have changed is rarely studied in school and rarely reported in the media. As a result, many are entirely unaware of even the most fundamental positive developments in the world. But this fact — that it is possible to change the world and achieve extraordinary progress for entire societies — is something that everyone should know. The world can be much better Progress over time shows that it was possible to change the world in the past, but do we know that it’s possible to continue this progress into the future? Or were we perhaps born at that unlucky moment in history at which progress has to come to a halt? Studying the global data suggests that the answer is no. It is possible to make the world a better place. One way to see this is to look at the places in the world with the best living conditions today. The best-off places show that extremely low child mortality is not just a possibility but already a reality. The world region where children have the best chance of surviving childhood is the European Union. Of children born in the EU 99.53% survive childhood.6 To see how much better the world can be, we can ask what the world would look like if this became a reality everywhere. What if children around the world would be as well off as children in the EU? The answer is that five million fewer children would die every year.7 Of course, the child mortality rate in the EU is still too high, and there is no reason that progress should stop there. Cancers like leukemia and brain tumors kill hundreds of children, even in today’s richest countries. We should strive to find ways to prevent these tragic deaths. However, the largest opportunities to prevent the pain and suffering of children are in the poorer countries. There, we know not only that things can be better but how to make them better. You can use this research on how to make the world a better place to contribute to this progress yourself. I recommend relying on research published by the nonprofit organization GiveWell.org. GiveWell’s team spent years identifying the most cost-effective charities so that your donation can have the biggest positive impact on the lives of others. Several of the recommended charities focus on improving the health of children, giving you the opportunity to contribute to the progress against child mortality. Millions of child deaths are preventable. We know that it is possible to make the world a better place. The world is awful; this is why we need to know about progress The news often focuses on how awful the world is. It is easier to scare people than to encourage them to achieve positive change, and there is always a large audience for bad news. I agree that it is important that we know what is wrong with the world. But, given the scale of what we have achieved already and of what is possible in the future, I think it’s irresponsible to only report on what is wrong. To see that the world has become a better place does not mean to deny that we are facing very serious problems. On the contrary, if we had achieved the best of all possible worlds, I would not be spending my days writing and researching about how we got here. It is because the world is still terrible that it is so important to see how the world became a better place. With my work I hope to change our culture a little bit so that we take the possibility of progress more seriously. This is a solvable problem: we have the data and the research to see the problems we are facing and the progress that is possible. The problem is that we are not using the data and research we have. The data is often stored in inaccessible databases, the research is buried under jargon in academic papers and often locked away behind paywalls. With Our World in Data, we want to change this. If we want more people to dedicate their energy and money to making the world a better place, then we should make it much more widely known that it is possible to make the world a better place. For this we have to remember that all three statements are true at the same time: the world is awful, the world is much better, and the world can be much better. This is a revised and updated version published in February 2023. The last previous revision I had done in July 2022. The first version of this article was published in October 2018. Acknowledgments: I would like to thank Hannah Ritchie and Toby Ord for their feedback on this article. Continue reading on Our World in Data: Why do we need to know about progress if we are concerned about the world’s largest problems? Endnotes In a number of fundamental aspects – obviously not all – we achieved very substantial progress. These aspects include education, political freedom, violence, poverty, nutrition, and some aspects of environmental change. See also my short history of global living conditions. Except for the historical data, all data in this post is taken from IGME, the UN Inter-agency Group for Child Mortality Estimation. They publish their data here: childmortality.org/data/World Their point estimate for the global number of deaths for children under 15 in 2021 is 5,862,574. This means on average there are 5,862,574 / 365.25 = 16,051 child deaths per day, 5,862,574 / (365.25 * 24) = 669 child deaths every hour, and 5,862,574 / (365.25 * 24 * 60) = 11.15 child deaths every minute. If we still suffered the poor health of our ancestors, more than 60 million children would die every year. How many children died at the time? We don’t know because data on the number of global births at the time is not available. For the 1950s and 1960s, we have estimates of both the number of births and the mortality rate, and the records show that around 20 million children died every year. See the data shown here. See the world map on the mortality of under-15-year-olds. See the data reported in Mortality in the past – around half died as children. If we look at single countries, this difference becomes even more striking as in the countries with the best health, the child mortality rate is again almost twice as low as in the EU as a whole. 0.47% is calculated as the weighted average of the youth mortality rate of the following countries: Austria, Belgium, Bulgaria, Croatia, Cyprus, Czechia, Denmark, Estonia, Finland, France, Germany, Greece, Hungary, Ireland, Italy, Latvia, Lithuania, Luxembourg, Malta, Netherlands, Poland, Portugal, Romania, Slovakia, Slovenia, Spain, and Sweden. The data is also for 2021 and also from the same source (UN-IGME). The weights are assigned based on the number of children under the age of 15. The countries with the lowest mortality rates today include San Marino, Norway, Japan, Finland, Singapore, Iceland, and Slovenia, where 99.7% of all children survive. This chart shows the ranking. However, because several of these countries are small, I did not base this text on the data from any single country but on a large world region where millions of children are born every year. The global number of child deaths, as reported above, is 5,862,574. 5,862,574 – 5,862,574 / (4.4 / 0.47) = 5,236,345 fewer children would die if the global mortality rate were 0.47% rather than 4.4%. Cite this work Our articles and data visualizations rely on work from many different people and organizations. When citing this article, please also cite the underlying data sources. This article can be cited as: Max Roser (2022) - “The world is awful. The world is much better. The world can be much better.” Published online at OurWorldInData.org. Retrieved from: 'https://ourworldindata.org/much-better-awful-can-be-better' [Online Resource] BibTeX citation @article{owid-much-better-awful-can-be-better, author = {Max Roser}, title = {The world is awful. The world is much better. The world can be much better.}, journal = {Our World in Data}, year = {2022}, note = {https://ourworldindata.org/much-better-awful-can-be-better} } Reuse this work freely All visualizations, data, and code produced by Our World in Data are completely open access under the Creative Commons BY license. You have the permission to use, distribute, and reproduce these in any medium, provided the source and authors are credited. The data produced by third parties and made available by Our World in Data is subject to the license terms from the original third-party authors. We will always indicate the original source of the data in our documentation, so you should always check the license of any such third-party data before use and redistribution. All of our charts can be embedded in any site.",
    "commentLink": "https://news.ycombinator.com/item?id=39278631",
    "commentBody": "The world is awful. The world is much better. The world can be much better (ourworldindata.org)365 points by colonCapitalDee 15 hours agohidepastfavorite268 comments fodmap 2 minutes agoI do recommend David Byrne's Reasons to be Cheerful https://reasonstobecheerful.world/ a project that 'aims to inspire us all to be curious about how the world can be better, and to ask ourselves how we can be part of that change'. reply akkad33 14 hours agoprevThe book [factfulness](https://www.goodreads.com/book/show/34890015-factfulness) is precisely about this. They present how the world is improving in an irrefutable way but media and politicians would only focus on the negative. The authors are Swedish, so they talk first hand about Sweden, and their experiences how drastically Sweden changed in the last 2 generations is amazing reply s_dev 12 hours agoparent> They present how the world is improving in an irrefutable way but media and politicians would only focus on the negative. It's worth noting this 'world' that is constantly improving exists only for people. I routinely counter this with the analogy: that's like a dentist saying your teeth are better than they've ever been -- not only that; they're getting better but at the same time your gums are screwed. Are those teeth dependant on those gums you might ask. Yeah. Every serious envoirnmental science paper seems to conclude the same thing: biodiversity is collapsing. On the whole, the world we live in is worse than the past. Examples during the earths history that aren't really from external forces e.g. when the moon crashed in to earth, after asteroids, early earth volcanoes etc. are different and not under our control. reply dsign 12 hours agorootparentAs far as I can see, you are totally correct. As it is the post you are replying to. Which begs the question: how can the state of all of humanity improve without damaging even more the environment? Will we ever confront that question honestly? reply titzer 11 hours agorootparentThe answer is that birth rate needs to decline and the human population needs to level off and then also decline to sustainable levels. We're in overshoot right now and no amount of economic development (read: growth) is going to fix that. We need degrowth, and the only way that is achievable without harming standard of living is to have fewer humans. The only way to have fewer humans without murdering or starving a bunch is for population to level and drop off naturally. It's hard to even have this conversation without being branded an eco-fascist, but there it is. We are just too many. reply JeremyNT 11 hours agorootparent> The answer is that birth rate needs to decline and the human population needs to level off and then also decline to sustainable levels. I agree that's an answer, but is it the answer? It seems like reproduction rates in wealthy countries trend closer towards replacement levels, so perhaps you \"just\" need to get everybody's standard of living up to western levels to take population growth out of the equation. Then you're left with supporting roughly our current population level which seems... entirely feasible? Between existing but underutilized techniques like fission, renewables, smarter land use and management for agriculture (read: focus on producing more efficient crops and less on the intensive stuff like cows), retrofitting buildings with better insulation, and building out more efficient transit, it seems like we could get pretty close with current levels of technology. Nevermind the developments we continue to make. Fusion power would immediately solve a lot of problems. reply titzer 11 hours agorootparent> Then you're left with supporting roughly our current population level which seems... entirely feasible? That doesn't match data I've seen. For example, even the Wikipedia article on carrying capacity (https://en.wikipedia.org/wiki/Carrying_capacity), second paragraph, states there are a number of lines of scientific evidence that we are already over capacity of Earth today. One project (https://en.wikipedia.org/wiki/Earth_Overshoot_Day) does pretty intense data analysis to quantify how far we are over that capacity. The summary is, yes, we are definitely in overshoot, and all economic development to raise standard of living is making it worse. But it's even worse than that. We aren't just exceeding the carrying capacity, it is diminishing as we reduce biodiversity and biomass through land use (read: obliterating wildlife and forests), overfishing, and over-industrialization of agriculture, which kills healthy soils. > Between existing but underutilized techniques like fission, renewables, smarter land use and management for agriculture It's not just about producing more energy. We are consuming everything too much, from biomass to raw materials, even fresh water. > Fusion power would immediately solve a lot of problems. It might reduce CO2 consumption but ironically it will increase per capita energy consumption, which encourages even more environmental destruction by making everything cheaper to build and buy. So more land use and more consumption. reply williamcotton 8 hours agorootparentMalthus had plenty of scientific evidence as well! What he didn’t have was fertilizer and tractors. reply dbspin 24 minutes agorootparentFertilizer and tractors both contributed enormously to devastaing the biodiversity of the planet. The question is not whether we can sustain a larger population, but whether we can sustain a population as large as currently exists - while maintaining any semblance of a natural world. Over a long enough time line both questions almost certainly do in fact merge. But it's equally possible to conceive of a medium term future with nature utterly devastated and humanity clinging on in increasingly baroque ways to a dying rock. reply tuatoru 4 hours agorootparentprev> It seems like reproduction rates in wealthy countries trend closer towards replacement levels... That seems unlikely, given that total fertility in wealthy countries is well below \"replacement\" (2.1 children per woman). Reproduction rate, number of daughters per woman, could only be approaching replacement if the sex ratio is massively skewed in favour of girls. 1. UN chart of TFR for \"more developed regions\": https://population.un.org/wpp/Graphs/Probabilistic/FERT/TOT/... Observe the assumed but never yet seen floor to fertility rate. reply andrewmutz 11 hours agorootparentprev> We need degrowth, and the only way that is achievable without harming standard of living is to have fewer humans Why are you confident that having fewer humans would help with the standard of living? Doesn't most of the scarcity of what we consume come from the labor necessary to produce it, rather than the cost of physical stuff? With food for example, for every dollar you spend on food only about 15 cents is actually buying food items from the farmer and the rest of it goes it to the work of processing, packaging, transporting, selling, servicing, etc (https://www.ers.usda.gov/data-products/food-dollar-series/do...) With fewer people, those things become even more scarce. reply darth_avocado 9 hours agorootparent> only about 15 cents is actually buying food items from the farmer and the rest of it goes it to the work of processing, packaging, transporting, selling, servicing, etc Part of the reason we need to do all of that is because we have too many people to feed. If the population is 1/10 of the number of people we have today in the US, suddenly we only need 1/10 of the cows, which domestically we can raise sustainably. No longer to we need to grow alfalfa in Arizona, ship it to Australia or Brazil where they raise the cows and then ship the meat back to the US. We’ll need 1/10 of the avocados that can be sustainably grown in California instead of importing it from Mexico. reply PeterisP 7 hours agorootparentThis is generally true for primary production like agriculture, however, agriculture is something less than 5% of our effort, and for the majority of our economy we have the opposite effect where economies of scale enable much greater efficiency - i.e. more or better goods/services for the same amount of labor - than it could be possible with a much smaller population. reply matthewfelgate 1 hour agorootparentprevThis is a real stupid argument. reply titzer 10 hours agorootparentprevHousing, infrastructure, and energy would all be in huge surplus. reply ytx 11 hours agorootparentprevUnfortunately the assumption of real growth is very baked into things like social security, medicare, etc. There's no way to de-grow without an increased burden for those working to take care of a larger non-working aging cohort. A deeper problem is that the benefits / \"slack room\" provided by real growth are constantly eradicated because people's wants (and needs) keep growing along commensurately. Some changing expectations are fairly uncontroversial, e.g. better medical technology and doctor training costs more. Others are arguably less good, e.g. private jets (or taken to the extreme, passenger planes in general!) reply tomp 1 hour agorootparentprevYou're so wrong that you could hardly be any more wrong. Economic growth is possible without consuming more resources. That's literally what technology is - producing more output with less input. Rich countries have grown over the past 20 years without increasing CO2 emissions. Most developing countries have also grown faster than their CO2 emissions. https://www.iea.org/commentaries/the-relationship-between-gr... Degrowth is murder, anyone promoting it is evil. reply rdm_blackhole 31 minutes agorootparentDegrowth is murder is not an argument. Is that the best we can do in this day and age? Is this really the level of argumentation that we are at? OP posted a comment and even provided evidence for his argument in one of his/her other comment. You did not. You cherry picked 1 fact about CO2 emissions that is not even related to what OP was talking about which was resource scarcity and decrease in biodiversity. Finally, even if your argument is correct regarding the fact that CO2 emissions are declining in the developed countries, it is only valid if you don't take into account the fact that developed countries have outsourced most of their manufacturing to developing countries. That means that this CO2 was if fact emitted, just not counted in the stats of the developed countries anymore. We can hardly call that a victory. reply tomp 26 minutes agorootparentread the article I linked to > The fall in CO2 emissions in advanced economies is also seen while considering consumption-based metrics, meaning that the fall in emissions in these regions is not merely a result of offshoring of manufacturing. reply rdm_blackhole 1 minute agorootparentFrom you own quote: not merely. So it is in part responsible. And I can also point to many dozens of article that agree with my statement. But for the sake of argument, let's assume that you are completely correct, that doesn't change anything about deforestation, soil depletion, over-fishing and many other issues that have not been addressed. All of these issues have nothing to do with CO2. So my original point stands. You are focusing on the CO2 part when OP was talking about resource depletion in the first place. You can choose to bury your head in the sand and think that everything is fine, but that doesn't make it true. On that note, considering that you did not address the point in my original comment, I can honestly say that you are arguing in bad faith and I don't have time for that so feel free to not respond. MrVandemar 48 minutes agorootparentprev> Degrowth is murder, anyone promoting it is evil. That's some emotive rhetoric there. I'm thinking that you're pretty passionate about the subject, and therefore not remotely objective. reply tomp 45 minutes agorootparentI have used objective reasoning to conclude that degrowth is murder. Now I'm passionately opposing it. Same with Nazism, Communism, Decolonization, Hamas, etc. reply Miraltar 31 minutes agorootparentHow do you define murder ? reply tomp 25 minutes agorootparentchildren dying because you don't produce enough food to feed them. people dying because your economy cannot support high technology required to make advanced drugs. old people freezing to death because you don't produce enough energy to heat their homes this isn't a hypothetical. Look at any communist regime! reply 0x732202 9 hours agorootparentprev... or we could just build more nuclear plants and harness practically free energy, without getting weird about who's having babies reply Miraltar 25 minutes agorootparentIt's not just about energy, how do you store electricity ? How do you extract the materials ? reply swed420 10 hours agorootparentprevWhy not focus on reengineering our political and economic systems and their inefficient built in contradictions, like optimizing for consumption? Things used to be built to last. Planned obsolescence coupled with a human drive to consume needlessly are completely unnecessary to happiness, but decades of propaganda have convinced us otherwise. People don't need to keep driving to their fake jobs, etc. reply concordDance 10 hours agorootparentIt's not really extra physical goods that are the issue (e.g. that arm chair made from farmed wood going to landfill), it's the pollution required to create them or the improper disposal of them. Some cast iron statues made using solar power as an energy source aren't a relevant climate issue. As such, the proper response is a CO2 tax and proper landfill practices. reply sleazy_b 7 hours agorootparentprevDo you have children? How do you propose to convince or force others not to have children without devolving into something that could reasonably be called “eco-fascism”? reply defrost 7 hours agorootparentPretty easy really - promote social programs that lift the standard of living and promote easy access to quality education. You can look at, say, Hans Rosling presentations over the past decades and see just how highly correlated education and women's rights are with reducing population expansion rates. Free quality public TVstreaming doesn't hurt either. reply sleazy_b 6 hours agorootparentHow is that easy to accomplish nationally let alone globally? What if there’s places that buck the trend, or don’t want to incentivize low birth rates in this way? reply defrost 6 hours agorootparentIt's easier than devolving into literal facism which was the first bar you set. It's easy enough that the bulk of G20 countries got there sans the USofA which still struggles with public healthcare. Of course doing something isn't as easy as sitting around doing nothing save moving a bar along and just asking. Projection wise, for now, globally we're headed to peak and decline in population so much of this jawing is moot. reply sleazy_b 6 hours agorootparentThey got there by getting us to the point we’re at now, by burning fossil fuels and offloading many of their social ills to other regions. By all means, let’s improve education and get more money in people’s pockets. But degrowth is much more likely to be accomplished through fascism than by raising the standard of living globally in a way that preserves regional independence. reply whythre 2 hours agorootparentI agree that only an overwhelmingly powerful state apparatus could tear people away from their fast food and cat videos. And cars. And heat. ‘You will own nothing and you will be happy. For the good of the planet, of course.’ reply williamcotton 8 hours agorootparentprevThe birth rate is coming down rather rapidly to the point where this will be a large problem for you when you’re in your golden years. Won’t that be ironic! reply throwawayqqq11 11 hours agorootparentprevBirth rate is already declining everywhere. The declining death rate is what causes population growth. Stage 2-3: https://en.m.wikipedia.org/wiki/Demographic_transition The only thing, that appears hard to grasp is the already declining birth rate for africa since quite some time (go google it). It would be much more effective for fascists to argue for a higher mortality rate but that would be too cruel for the main stream. To end constructively: What we need is better wealth distribution for even better health care to accelerate this process. You need to understand that better living standards helps us platoe. Maybe advocate for that. > The only way to have fewer humans without murdering or starving a bunch is for population to level and drop off naturally. Our planet could easily support +10 billion people. Its also a matter of wealth distibution. You know that \"4 planets consumption rate\" stuff for US citizens? This turns the responsibitly from the fertile poor to the rich consumers without any social darwinism. I would say, only focusing on the poor and not the rich to hinder wealth distribution justifies the insult of an eco-fascist. The problem here is, that right-wingers stop their train of thought at \"too much people\" and \"look at birth rate\" (\"another thought process finished successfully\"), which your comment was all about, but ignore their own role in defining that upper bound and their consumption. It is comfortably to their way of life arguing against fertility rates, which wont effect them as much as wealth distribution would do to actually lower it. You could also cut the poor from resources which would revert the demograpgic transition into high death and ferrility rates. Also a solution... reply bombcar 10 hours agorootparentThat consumption would drop precipitously if we stopped upgrading/replacing working things. Wouldn't help support a growth economy as much, however. reply account42 2 hours agorootparentprevI hope you will lead by example and not have any children? reply melagonster 11 hours agorootparentprevthis will happen. when people are getting rich, they prefer to have less kids. UN predicted the largest number of population is 12 billion in future thirty years, then population start to diminish. reply WarOnPrivacy 10 hours agorootparent> when people are getting rich, they prefer to have less kids. People are also having less kids because they are too poor. It's hard to pair off in an economy where 4 typical incomes are needed to make basic bills. I know young people who aren't dating because there's presently no point. They aren't in the minority who earn enough to pay for half a household - nevertheless afford children. reply williamcotton 8 hours agorootparentUh, poor people have way more kids: https://www.statista.com/statistics/241530/birth-rate-by-fam... reply defrost 8 hours agorootparentWhich in itself is a good argument to raise the standard of living from the bottom upwards .. below some ceiling of per capita consumption that exceeds current planet capacity to endure. reply melagonster 6 hours agorootparentprevwhen society rich enough, the only way can let people want more kids is offer economic help. so this is not a pure linear relationship. reply matthewfelgate 1 hour agorootparentprevYou have degrowth. Because degrowth is frankly, a disgusting ideology. reply mym1990 11 hours agorootparentprevNot until our infatuation with constant growth is confronted. Currently all major economies focus on growth(GPD), and any sign of slowdown is essentially a doomsday scenario. We have gotten massive efficiency gains in almost all labor intensive work, and while this means more goods and services can be pumped out to increase global wealth, it also means a rapid degradation of the environment. reply matthewfelgate 1 hour agorootparentLol. Writing on the internet and ranting about growth. Simply idiotic. reply austhrow743 7 hours agorootparentprevBy moving manufacturing off world. reply adventured 11 hours agorootparentprev> how can the state of all of humanity improve without damaging even more the environment? It can't. There are three to four billion young, new consumers coming on in the third world over the next 30-50 years, and they want what the first world has. And there's no fair argument to telling them they can't have it or strive for it. That means enormous additional coal power plant production and emissions output. Just ask India and China, they can't build coal power fast enough. Next comes Africa's energy build-out, and it will only partially be green, the rest will be coal. Take Africa's next billion new consumers and give that block the coal output of China, that's the end of the world as we know it (if the climate change forecasts are correct). There's no stopping it now. Just enjoy your life, the future is set. Unless someone has an epic cheap energy breakthrough ready to go right now (so that we can have it deployed fully within 30-50 years). Just what China and India are adding alone is enough to end the world. You could immediately cut US emissions in half and it wouldn't matter at all to the expansion going on in Asia. And again that's before we even get to what's about to happen in Africa. No matter how many times you bring this up to the green crowd, they just ignore it aggressively, pretending that doing little token virtue signaling things in one market (we cut emissions 5% in NY state, it's amazing!) is going to actually contribute meaningfully to stopping the avalanche. Anything that doesn't involve massively slashing the coal output of China and India (and preventing its rise elsewhere), is meaningless. reply concordDance 10 hours agorootparent> that's the end of the world as we know it Notably, this does NOT mean a mass die off and the end of human civilization or even the end of lots of nicely habitable bits of Earth. reply Ekaros 53 minutes agorootparentMy take is that environment will become lot more variable and we will just have to deal with it. Living in some areas will be worse than now and in others better. And we might need to move some things a bit, which is likely expensive but not life ending. reply akkad33 2 hours agorootparentprevMaybe biodiversity is shrinking, but the book has examples of environmental conservation efforts that have improved biodiversity in the case of tigers in India, to give one example. I strongly believe that when humans are doing better, the life around them will too. We are now more humane than ever. S Korea is banning dog meat, we have banned ivory production, rhinos are being protected in Africa. reply MrVandemar 34 minutes agorootparentI live in the South West of Western Australia, in a bio-diversity hotspot. I look South, and I see bush-land, a relatively small postage stamp sized block. I've counted about 23 separate species of orchids, and a huge variety of plants, and birds. One of the species of orchids is basically a sex-lure for wasps, and is absolutely fascinating. I look North, East and West and I see farm-land as far as the eye can see (aside from another very small \"reserve\" to the NE). It's just paddocks for animals, and the diversity is \"cattle\" and \"Sheep\", and the various grasses which they eat. Very f absolutely fascinating.ew birds (not many trees), much less variety and diversity. And no orchids. That's where food comes from, and it starts with a bulldozer, it relies on fossil-fuels (including the fertiliser) to keep producing. And we're still growing. Banning ivory and protecting Rhino's is basically palliative care at this point. reply lukan 59 minutes agorootparentprev\"S Korea is banning dog meat\" Why is that a good thing, when the normal meat factories are bad as always? Because dogs have more rights to be treated nice, because they are considered cute, unlike pigs who are way more intelligent? reply rdm_blackhole 21 minutes agorootparentprevYou are cheery picking a few examples without looking at the whole picture. The Amazon forest is still being decimated, we are still over-fishing the oceans, and that's just for starters. Let's not even talk about healthy soil depletion and what comes with that problem. Insects are disappearing at an ever faster rate and they are at the base of many food chains. Are there some things that are better now? Sure but you are looking at the tree and you missed the entire forest. reply 0xfffafaCrash 1 hour agorootparentprevIt’s interesting how quickly an argument for “factfulness” morphs into cherry-picking data points as soon as the facts are unfavorable. Turns out that facts aren’t bestsellers or at least not memorable ones that everyone wants to read and share. Optimistic narratives are — they fit a human need in a time when on many fronts that matter realism and cynicism are predominantly one and the same. reply huijzer 4 hours agorootparentprev> biodiversity is collapsing This is not the conclusion of the book Not the End of the World by Hannah Richie. As with many things, things are not so simple/binary. reply ruszki 3 hours agorootparentThe consensus is clearly 100 to 1000 of the background extinction rate, and that biomass is decreasing in wide range of categories (like mammals, insects, fishes, plants, etc). How they refuted that? Are there more and more diverse bacterias, and the rate of how diverse they become increased 100 fold? reply skybrian 9 hours agorootparentprevI think it’s important to remember that the future isn’t known to us. People will fixate on one scenario or another, forgetting that there are others. Our ability to forecast is limited. Studying the recent past or the deep past gives us some perspective on how things have changed before. Making analogies might help us think of more scenarios, but it’s not so good for ruling things out. reply williamcotton 8 hours agorootparentprevBiodiversity will come roaring back once humanity is gone. To the rest of life on our planet there is no difference to the damage done by a meteor or by humans. It’s not like an endangered owl is sitting around thinking about what counts as an external, life-threatening force! reply Miraltar 16 minutes agorootparentThe condition of the planet affects our living, you can't really juste shurg it off reply ununoctium87 8 hours agorootparentprevYay? reply williamcotton 8 hours agorootparentIf you really care about the planet then you don’t really need to worry! reply paulddraper 8 hours agorootparentprev> On the whole, the world we live in is worse than the past. Some things are worse, some things are better. For example, in the U.S., clean air and water have improved dramatically over the past 50 years. Worldwide, deforestation is on the decline for the past 20 years. reply BHSPitMonkey 8 hours agorootparent> For example, in the U.S., clean air and water have improved dramatically over the past 50 years. In large part through exporting our staggering manufacturing demands to poorer parts of the globe such that the localized effects of pollution happen there instead. reply bryanlarsen 14 hours agoparentprevAn even better book recommendation (if you like the article) is \"Not the End of the World: How We Can Be the First Generation to Build a Sustainable Planet\" by Hannah Ritchie. Hannah Ritchie is head of research for ourworldindata.org; the book grew very directly out of the linked article. Ritchie credits a lecture by Hans Rosling (one of the authors of Factfulness) in the forward for being what turned her from a pessimistic biology student into the person that writes a book like this. A common criticism of the linked article is that \"sure child mortality is going down, but what about X\". Where X is climate change, species decline, quality of life, et cetera. The book addresses most \"X\"'s. reply ebcode 12 hours agorootparentNot sure if this is the same lecture that Ritchie cites, but for anyone who hasn't seen this one by Rosling, it's worth a watch: https://www.ted.com/talks/hans_rosling_the_best_stats_you_ve... reply account42 23 minutes agoparentprev> They present how the world is improving in an irrefutable For whom? Some things are improving, some things are getting worse. Saying that the world is getting better overall requires judgement of how much you value different things. reply handoflixue 4 minutes agorootparent> Some things are improving, some things are getting worse. Are there any global measures that are worse than they were a century ago? Or are you just saying that progress is uneven, with some areas seeing local setbacks despite the overall global gains? reply imjonse 12 hours agoparentprevThe problem with the world improving in an irrefutable way on average is that this can be and is used to defend the status quo in almost any field and can lead to complacency and a false sense of optimism. The world became better due to science, technology, various policies, cultural changes, etc. and will only improve if these are continued. It is not always clear what brought improvement and whether that sort of improvement will keep scaling. This is just my long-winded way of saying that I don't like it when people say the system is great, don't change anything, don't complain, and in a few more generations we'll all be fine. reply ordu 12 hours agorootparent> this can be and is used to defend the status quo in almost any field and can lead to complacency and a false sense of optimism So, should we stick to lies about world becoming worse to not become complacent? It is a highly politicized approach to the truth. The truth must be kept out of reach of political thinking. We need to keep our optimism at levels based on facts, not on our political goals. We need to stick to the truth based on empirical data even if it is not aligned with our political goals. At least if world wants to become much better. I hate the way politicians argue their points. They ignore anything good about their opponents and anything bad about their own ideas. And maybe this is the reason why our world is not great. > I don't like it when people say the system is great I don't think anyone saying that. But from the other hand, our system is much better then it might be, and we'd better remember that, because if it wasn't, if we were in a local minimum, then we could change it in any direction to make it better. We need to remember that we are not at a minimum or a maximum, we need to think carefully about gradients before deciding were to move. So maybe we should say that our world is great, to not break it, to not make it worse? reply Jensson 12 hours agorootparentprevThe problem with \"fight the status quo\" people is that they tend to demand more authoritarianism and centralization to fight something that isn't that bad. Authoritarianism tend to lead to worse outcomes and hurt progress, things progress faster with liberal ideals than authoritarian ideals. reply shredprez 12 hours agorootparentThere’s nothing about a “fight the status quo” mentality that inherently favors authoritarianism and it’s not helpful to claim otherwise. That said, it is helpful to remember both knowing- and unconscious authoritarians will twist any framework into an excuse to establish and flex authority — that’s their whole modus operandi, after all. reply srid 2 hours agorootparentprevIn particular, I find that \"fight the status quo\" people want to change some external \"system\" more than the individual people (ie., themselves). It is easier to blame and demand change of external entities, while being comfortably numb about oneself and one's way of living. For example, see this article by the political professor Eric Kaufmann: https://onlinelibrary.wiley.com/doi/10.1111/ssqu.13268 reply makeitdouble 11 hours agorootparentprevIf by \"centralization\" you mean giving more power to central entities that can intervene to fight local abuses and coordinate policies, I don't see a path were we can do without it. We're in this situation in no small part because big enough companies can just buy their way when facing local entities, and the only recourse that is working is to ask a higher up regulator to intervene. Weakening the regulation entities makes it basically impossible to have anything done. reply imjonse 12 hours agorootparentprevsome variants of 'fight the status quo' are explicitly opposing the centralization of power and the too big to fail entities in non-authoritarian societies. reply circuit10 12 hours agorootparentprevFocusing on the bad is natural and makes sense because that’s what can be improved, but I feel like your mental health will be better if you focus on the bad when it’s productive and you can do something about it, and at other times realise that things aren’t that bad reply PH95VuimJjqBqy 12 hours agorootparentprevthe improvement isn't even. It seems obvious, but even that little bit of nuance escapes far too many people, most especially those whose livelihood is to talk about it. reply tetha 13 hours agoparentprevYou make me wonder if this isn't the marketing/sales vs dev/ops misalignment. Marketing and Sales focus on the good parts of our software. Simple usability, high stability, great reactive customer support, ... Apparently, during corona, we were the only reliable vendor for a national support hotline by a massive margin, and chaotically on-boarded call-agents had the least number of issues with our system. But then, I as a team lead in ops have to wonder why a single priority ticket didn't meet SLA. Why a routine update caused 4 minutes of downtime. Why a usually stable provider ripped a system away for 3 minutes. Why strange cache timeout interactions caused some class of requests to take 30s+ to respond. My world is very much a world of shitty non-working janky software and it is my job to fix and improve it. It might be one of the better solutions in our market and other people may be able to sell it to happy customers, but in my world it's a janky and broken piece of shit with a million things to fix. And looking at the nation I live in... that's honestly not that far away. reply rco8786 11 hours agorootparentI run into this all the time w/ engineers I work with. I always tell them the story of the mechanic. The mechanic works tirelessly every day fixing Toyota Camrys. For years on end, he stares at broken Toyota Camrys. He knows every bolt, and every failure point across decades of Toyota Camrys. He is utterly and thoroughly convinced that Toyota Camrys are unreliable hunks of junk. After all, he sees them broken all the time. What he doesn't see are the 100s of thousands of Toyota Camrys loggings hundreds of thousands of miles on the road, perfectly intact with no mechanical issues. His vision is completely skewed by his day to day responsibilities. Same thing for us in software. It's our job to see the dirty edge cases. To notice when something goes wrong for 5 minutes. etc. Our job is to automate everything - so by definition, we don't see the thousands or millions of interactions between our customers and our software that go exactly as planned, because they're completely automated away from us. Completely invisible. We have a skewed vision because of our day to day responsibilities. reply bombcar 10 hours agorootparentThis is an important factoid - if you ask a mechanic for a vehicle recommendation you might not get the most reliable - but the easiest and cheapest to work on when it does break. reply rco8786 12 hours agoparentprevOne of my favorite books. Changed my whole worldview (and I already had a positive world view before reading). I've recommended this book to people probably 1,000 times. reply makeitdouble 10 hours agoparentprev> improving in an irrefutable way The book was great and I think the main shinning point was to base each arguments on actual measures, but that's far from creating an irrefutable narrative. At the end of the day those are numbers they chose and dressed, and other researchers will come up other equally true numbers that they'll filter and present in a different light. An instance of this is in this article is the number of infant mortality: sure it's improving, but birth rates are also plummeting. To me the overall picture on infants is greyish and I'm not sure it's overall better than a few decades ago (I have no idea, didn't crunch the numbers), but here we're only getting the bright side and shown a sheer positive trend. My point is probably that having numbers to back a claim is better but they still need to be looked at critically and in a context reply nl 10 hours agoparentprevAnd yet this narrative (\"They present how the world is improving in an irrefutable way\") falls exactly into the trap pointed out by the link in this story: \"Solely communicating the progress that the world has achieved becomes unhelpful, or even repugnant, when it glosses over the problems that are real today.\" (Even though I agree this narrative is mostly correct) reply akkad33 2 hours agorootparentWell, the world is improving, but it can be better. That's the message of the book. It's very optimistic and you should read it. reply FredPret 14 hours agoparentprevTwo more books in this vein are Fewer, Richer, Greener by Siegel and Abundance by Diamandis and Kotler reply pchangr 10 hours agoparentprevFYI, Max Roser has mentioned several times that the book inspired him to create our world in data. reply Havoc 9 hours agoparentprevtbh Sweden feels like an outlier to me. Recently visited and it struck me as the most functional society I’ve encountered thus far. Even the demographics are healthier than much of rest of Europe especially compared to other rich nations. That’s not to say the overall thesis is wrong but I suspect slight rose tinted glasses at play reply zubairq 12 hours agoparentprevI read \"factfulness\", really opened my eyes! reply zo1 19 minutes agoparentprevI honestly don't trust the stats, and this kind of analysis and \"blame on politicians for lying and focusing on negatives\" does not convince me. Especially with all the ensuing gaslighting that happens in response to valid criticisms. reply DiffEq 10 hours agoparentprevThere is still the specter of nuclear war…that side of the equation has gotten worse. reply elliotec 1 hour agorootparentSurely you've heard of the Cold War? reply maximinus_thrax 14 hours agoparentprevEveryone reading Factfulness should also take a look at https://www.researchgate.net/publication/328759928_Good_Thin... I've always regarded this book as being blatant neoliberal propaganda. reply throwaway29812 12 hours agorootparentWhat is a summary of the criticism? Is the data wrong or in some way misrepresented? It seems like the author doesn't like the attention Hans received. reply maximinus_thrax 12 hours agorootparentFrom the wiki page > Christian Berggren, a Swedish professor of industrial management, has questioned the authors' claims and suggested that Rosling's own thinking shows a bias towards Pollyannaism. Particularly, Berggren criticized the authors for understating the importance of the European migrant crisis, the environmental impacts of the Anthropocene, and continued global population growth. Furthermore, Berggren remarks that \"Factfulness includes many graphs of 'bad things in decline' and 'good things on the rise' but not a single graph of problematic phenomena that are on the rise.\" It \"employs a biased selection of variables, avoids analysis of negative trends, and does not discuss any of the serious challenges related to continual population growth.\" Berggren raises concerns that the simplistic worldview this book offers could have serious consequences. reply LastTrain 12 hours agorootparentprevCan you name a couple examples of blatant neoliberal propaganda in the book? Who is the propaganda being perpetrated on behalf of? Who is funding it? If it doesn’t have these latter elements, you might want to consider the possibility that it is simply a work that you disagree with. reply mistermann 12 hours agorootparentSymbols aside, dissemination of biased, misinformative, and arguably harmful information is not necessarily done with Ill intent. reply maximinus_thrax 12 hours agorootparentprevI don't need to defend a personal opinion if I don't want to. I believe the book is an 'everything is fine' meme in book format, pandering to the current oligarchy who's arguing that everything is going in the right direction and direct action is unnecessary. Other people replying to the same comment have articulated it better than I did. Whenever a book like this comes along and the real capitalists start reviewing and recommending it, it triggers my bullshit sensors. You might want to consider the possibility that it is simply an opinion that you disagree with and move on. reply LastTrain 9 hours agorootparentOstensibly, we participate in this forum to discuss things. I thought it was provocative to call this book propaganda so I thought I'd probe a little. When you tell someone they are reading propaganda, one of the unsaid things you are doing is implying that people are being duped, and that you somehow come from a higher place and have the real answers. So yeah - you are going to get questions. So maybe don't throw the term around in a public forum if you aren't willing to engage the people you're insulting. reply maximinus_thrax 9 hours agorootparentEverything is propaganda. This forum, for example, is full of techno-utopian/libertarian/accelerationist propaganda. Reading propaganda doesn't make you duped or stupid or whatever. > you aren't willing to engage the people you're insulting I'm sorry you feel insulted, that's mainly on you, this was not my intention. I suggest you reflect on why you feel insulted when an internet stranger calls a book (which you did not write) as propaganda. reply LastTrain 9 hours agorootparentI don't feel insulted, I pointed out you were being insulting. reply goatlover 4 hours agorootparentprevIf everything is propaganda, then the word is meaningless. reply goatlover 4 hours agorootparentprevWhat's wrong with being neoliberal, and what makes the book propaganda? Do you disagree with the factual claims? reply philshem 12 hours agoprevFun fact: Our World in Data is a YC funded non-profit https://www.ycombinator.com/companies/our-world-in-data reply philshem 2 hours agoparenttoo late to edit, but you can read about their 2019 experiences at YC here: https://ourworldindata.org/owid-at-ycombinator reply johngossman 14 hours agoprevGood article. I agree with the sentiment. It is easy to be pessimistic, and it is dangerous to be complacent. Recognizing that progress is occurring should motivate future actions, whereas believing the world is inevitably getting worse, or that it will get better on its own, can lead one to give up or withdraw from the world. I caveat this praise with my now instinctive skepticism of all of these EA projects coming out of Oxford. It feels like an overfunded set of charities. reply karaterobot 13 hours agoprev> Clearly, a world where thousands of tragedies happen every single day is awful. Here I will take the heartless position and say that it is not clear to me that this statement is true. I certainly agree that it's a tragedy for the people involved. But if 4.4% of children dying before age 15 means that the world is an awful place, at what point would that stop being true? 2.2%? 1E-10%? I don't think the world is an awful place, I think it's a good place, getting better in some ways, with a long way to go in many others. reply floodle 13 hours agoparentI agree, the statement is just poorly written. A qualifier like \"awful\" is only ever in relation to something else. Statement 3 (the world can be much better) is essentially a better formulation. 4.4% is awful in relation to 0.47%, which is possible. reply mistermann 12 hours agoparentprevConsider how the western world reacted when the harsh version of Mother Nature visited their lands temporarily during COVID...a few percentages sure seemed to be a big deal then. Luckily now that things are back to normal and people are thinking \"rationally\", we can once again see that it is not. reply kombookcha 19 minutes agorootparentThis doesn't just apply to covid - the reality is that a lot of global human suffering is being generated to prop up firstworld lifestyles, and the beneficiaries of that status quo have both material and psychological reasons for closing their eyes to it. People have a much easier time with telling themselves \"sometimes suffering just happens and can't be helped\" when the suffering is inflicted on strangers who are far away. The party can continue when you convince yourself there is no child in the cobalt mine, only cool new smartphone features. reply xboxnolifes 11 hours agorootparentprevThis was not a universally held position, so I don't see how it helps as a parallel in this case. reply zoogeny 7 hours agoprevAs an aside to this, I am reminded of paraconsistent logic systems [1] and three valued logic [2]. One quote I like from the three valued logic system, referencing Charles Sanders Peirce, states: > Peirce soundly rejected the idea all propositions must be either true or false; boundary-propositions, he writes, are \"at the limit between P and not P.\" I also recall reading some description of Buddhist or Advaita Vedanta where logical systems that rejected the law of non-contradiction were explored. I can't find the exact reference, but I recall it was related to the Two Truths doctrine which leads me to believe it was explored by Nagarjuna. At any rate, the idea that two contradictory things can be true simultaneously is a very interesting area to explore. Edit: To add to the link list: Dialetheism [4] is the view that there are statements that are both true and false 1. https://en.wikipedia.org/wiki/Paraconsistent_logic 2. https://en.wikipedia.org/wiki/Three-valued_logic 3. https://en.wikipedia.org/wiki/Two_truths_doctrine 4. https://en.wikipedia.org/wiki/Dialetheism reply dang 14 hours agoprevRelated: The world is awful, the world is much better, the world can be much better - https://news.ycombinator.com/item?id=32173146 - July 2022 (121 comments) reply opportune 14 hours agoprevOne day we’ll collectively realize how damaging constant access to “news” (in the modern day, engagement-optimized despair/outrage/fear porn) is on our mental health. Until then we’ll never be able to square the circle of how things can be so bad while our actual immediate lives are perfectly fine reply PaulKeeble 14 hours agoprevThe world on big measures is going better than it has. I am constantly annoyed there are too many in power slowing process to actually do better. If we all wanted it to be better to avoid preventable deaths and disability we could make progress a lot faster than we do. reply pizzafeelsright 13 hours agoprevSolving the Malaria and vitamin A deficiency is noble, I suppose, but only addresses the underlying condition of death. O wretched man that I am! who shall deliver me from the body of this death? reply seydor 14 hours agoprevA lot of awful things happen because some people think they can fix \"the world\". We can fix ourselves one by one, not all at once. reply chasing 14 hours agoprevBut what about that ragebait headline I read the other day? Clearly the entire system must be burnt to the ground. reply 1970-01-01 14 hours agoprev>It is wrong to think these three statements contradict each other. We need to see that they are all true to see that a better world is possible. I'll disagree with all of that. Awful, better, and improving are all measured differently, however these observations will push, pull, and merge with other observations. Quality of life is not so clean that we can dismiss any part of it as trivial. reply lapcat 13 hours agoprevFrom my perspective, global warming is the overriding problem, because time is running out to stop it. Maybe time has already run out, and it's too late to prevent the terrible consequences. In terms of other social problems, the world has gotten better in many respects, and it could continue to get better over time... given unlimited time. I'm just not sure that we have unlimited time. Wrecking our own ecosystem makes it very difficult to make progress on anything else, and the consequences of wrecking our ecosystem will only aggravate our other problems. That's why I'm overall pessimistic. reply bryanlarsen 13 hours agoparent> time has already run out, and it's too late to prevent the terrible consequences. Climate change is not a binary. We can choose how many terrible consequences it will have. It'll never be too late to prevent some terrible consequences. reply lapcat 13 hours agorootparentIt's not a continuum either. There are tipping points: think of the ice sheets all melting, the sea levels rising to the point where the coasts of continents are swamped. Moreover, continually adding energy to the system makes it more chaotic, engendering abrupt and unpredictable shifts. We can't necessarily \"choose\" the consequences in some kind of orderly manner, as if from a menu. reply Taylor_OD 13 hours agorootparentWe also have options that are unpopular/seem bad right now but may not in the future, like solar geoengineering via releasing specific compounds into the atmospheric. Something like that could also likely be achieved, or at least put into motion, by a single desperate nation. reply lapcat 13 hours agorootparentWas this supposed to make me feel better? The options \"seem bad\" in the same way that volcanic eruptions seem bad; i.e., they are actually bad. A single deperate nation could also put into motion a nuclear war. reply Ringz 12 hours agorootparentWhen one looks at the history of human interventions in ecosystems, one cannot be optimistic. Whether it’s about defending against invasive species or restoring ecosystems to their “original state,” our limited understanding of complex ecosystems inevitably leads to interventions with negative side effects. reply maxerickson 12 hours agorootparentprevGlobal warming is a result of the set point of the system going up more than cumulative addition of energy. The sun absolutely blasts the planet with energy and the planet radiates away most of it. Global warming is shifting the amount of energy in the system when those things are in balance. reply basil-rash 12 hours agorootparentprev> rising to the point where the coasts of continents are swamped Then it’s not a coast, it’s underwater. The coast has moved. That’s fine, they always do. People will need to move to accommodate that. That’s also fine, they always do. Also worth noting the sea level is actually falling relative to land level in many areas, especially towards the poles. What we’re most likely to see is mass migration into previously frozen/underwater areas towards the poles. reply lapcat 11 hours agorootparent> The coast has moved. That’s fine, they always do. Define \"always\", relative to, say, recorded human history. > People will need to move to accommodate that. That’s also fine, they always do. Oh yeah, no problem. Please tell the billion or so people who live in coastal areas that it's totally fine. > What we’re most likely to see is mass migration into previously frozen/underwater areas towards the poles. Because the land and infrastructure there is surely fantastic. I suppose those people don't need, you know, food or water, for example? But ignoring the geological and ecological problems for the moment, consider the political problems. We're already driving ourselves crazy over a relatively small amount of immigration, and you're saying it's fine that a billion or two people are going to move—or at least try to move—into different countries? I'm guessing they won't be welcomed with open arms. reply basil-rash 8 hours agorootparent> Define \"always\", relative to, say, recorded human history. Every second of every day, month, year, etc. they are constantly shifting. The political problems will sort themselves out (literally people problems), and the infrastructure will be built to accommodate the changing demographics. People move, people build. This is basic stuff. reply lapcat 7 hours agorootparent> Every second of every day, month, year, etc. they are constantly shifting. People who lives on the coasts don't have to move every second of every day, month, year, etc., due to the coasts moving. In fact, the people almost never have to move due to the coasts moving. > literally people problems Climate change is a people problem. As George Carlin joked, \"There's nothing wrong with the planet. The planet is fine. The people are fucked!\" > This is basic stuff. No, your comments are oversimplistic and perfunctory. reply basil-rash 4 hours agorootparent> In fact, the people almost never have to move due to the coasts moving. You're misinformed. It's very common in places with hurricanes, for instance. > your comments are oversimplistic and perfunctory. Your entire argument is based on the idea that humans can't move and build, or at least that doing so is \"hard\". When in fact putting in the hard work to move and build has been the cornerstone of human civilization for as long as human civilization has existed. It's a very conservative view that strangely seems to be held only by liberals: \"the way things were is the best way they can possibly be, all this change is no good, we have to go back to the good old days!\". With of course nothing real to justify it. You inverted Chronological Snobbery, but it's still just as much as fallacy. reply moffkalast 13 hours agorootparentprevWith that kind of tomorrowist mindset it'll be too late. We need to make changes and we need to make them yesterday. reply feoren 13 hours agorootparentSaying \"it's not too late to do something\" is more likely to effect change than saying \"it's too late, climate change already happened, we're all fucked, good bye.\" I think you agree with GP here. reply bryanlarsen 13 hours agorootparentprevI wrote \"we can choose how many terrible consequences we'll have\". Do you think that I would choose \"as many as possible\"? reply moffkalast 11 hours agorootparentLately it really seems like that's indeed the answer of those that we've entrusted to make our decisions for us. If it's never too late, then we never have to stop making profit from ecosystem destruction. reply tovej 13 hours agorootparentprevBy definition, there's a deadline for preventing something from happening. We're already in an extinction cycle, extinctions are happening at a rate of 100 to 1000 times the natural rate. And as for absolutes, total extinction of large mammals comes to mind as a consequence after which humanity can't really do anything. reply feoren 13 hours agorootparentI think the possible outcomes of climate change (especially if we continue on our current trend) are unthinkably catastrophic, at worst including the potential deaths of a significant percentage of humanity. But there's still an enormous gap between that and \"total extinction of large mammals\". I don't think that's anywhere near a realistic consequence of climate change. reply nradov 12 hours agorootparentprevThere is no conceivable future in which white-tailed deer become extinct in the next few centuries. We will probably lose some of the less resilient species, though. reply joe_the_user 12 hours agoparentprevThe inability to solve global warming is itself the result of a small number of wealthy and powerful groups having outsized influence and these groups operating in a fashion so short sighted that they cannot sacrifice their interests to solve the problem. reply mistermann 12 hours agorootparentIt's also the result of large numbers of people continuing to support an illusory political system that sustains this status quo. As I see it, the wealthy play a non trivial role in the marketing of the \"superiority\" of this system, and rare is the person who can seriously consider whether it may(!) objectively be one of the primary root causes, despite the role it plays in setting rules, distributing \"information\", etc. reply nradov 12 hours agorootparentprevThe inability to solve global warming is itself the result of billions of poor and middle-class people wanting to consume more energy (sometimes indirectly) in order to improve their quality of life. All else being equal, most regular people worldwide would prefer to live in a large private house filled with high-quality manufactured goods, eat a lot of meat, and drive a large comfortable private car. You can argue that this is irrational or unhealthy or that no one really needs so much stuff, but good luck convincing people that they should vote to voluntarily reduce their standard of living in furtherance of somewhat nebulous global goals. reply doublepg23 13 hours agoparentprevWhen humanity is still here in 2124 I wonder if there will be a Church of Climate Change that still prophesies the inevitable doomsday. reply acuozzo 12 hours agorootparentDefine \"still here\". If the population goes through a Toba-esque bottleneck due to climate-change-imposed breadbasket failures and there's no free energy (oil) left to bootstrap the world back to complexity similar to today, then is that \"still here\"? Does \"A Canticle For Leibowitz\" qualify as \"still here\"? reply doublepg23 11 hours agorootparentHave you considered reading the Christian eschatology Wikipedia page? It's easier to go to the source than reinventing it, you can recycle the Latin phrases too - win-win. reply southernplaces7 11 hours agorootparentprevArguments like these do a lot to make so many people roll their eyes at the real dangers of climate change that exist. There isn't a bit of concrete evidence nor serious predictions (not even by the IPCC) that claim climate change will be so bad in the next cnetury as to create a Toba type die-off (which by the way was, if it even affected humanity that severely since this is still heavily debated, caused by massive global cooling instead of warming). Get a grip on the real risks and work from there. Why rabidly lean towards an apocalyptic stance except out of a morbid fetishism that many humans have always had to end of the world predictions? reply lapcat 12 hours agorootparentprev> When humanity is still here in 2124 That's an extremely low bar. Especially when the submission title is talking about the world being \"much better\". reply mudlus 12 hours agoprevEvery point is a growth point, every point was a growth point. reply titzer 11 hours agoprevThe only optimists these days are humanists. Environmentalists generally think the planet is kind of screwed. reply zeroCalories 14 hours agoprevThese are contradictory statements because they are politically loaded. The \"things are alright, and are getting better, but we can do even better\" party doesn't energize people, and no one wants to be that party, even if it accurately describes things. reply 082349872349872 14 hours agoparentI'm looking forward to the TAAAAGBBWCDEB Party rallies: — What do we want? — Gradual change! — When do we want it? — In due course! I appreciate the local approach to referenda. A referendum can be brought \"solve X with Y!\", but the government then has a chance to say, \"we agree that X is a problem, but we think it'd be more effectively addressed via Z\". Then we all vote for either Y, or Z, or (the most popular response) \"X ain't so broke; don't fix it\" reply phrotoma 14 hours agoprevhow is this article dated 2018 using statistics dated 2021? reply frereubu 14 hours agoparentAt the bottom of the article: \"This is a revised and updated version published in February 2023.\" (I agree that this should be near the top, next to the publication date, to make it much clearer). reply phrotoma 14 hours agorootparentAh so it does, thank you! reply kamma 13 hours agoprev100% of children would die. The world was awful, is awful and continue to be awful. We bring sentient beings to life to only subject them to the inevitable death. reply epiccoleman 12 hours agoparentWhat a silly thought. I'll take my 40? 60? 80? 100? years of life over an eternity of nothingness any day of the week, thanks. The world sucks? Maybe so. It also contains food, love, music, children, laughter, grandparents, trees, cats, and just fuckin' pathos, man. Try not to waste it! reply archon1410 12 hours agorootparentYou'll have an eternity of nothingness regardless of how many years of life you get. Perhaps you don't want only an eternity of nothingness. As for the pathos, I've a hard time believing it compensates for even a billionth of say, the suffering of an abused child. There's no doubt that it would have better (impersonally, as Derek Parfit used that word) if the world didn't exist. But now that you're here, might as well enjoy it, but perpetuating it doesn't seem wise. reply epiccoleman 11 hours agorootparent> no doubt that it would have been better if the world didn't exist I strongly doubt this, for what little that's worth. Better by what metric? Better for whom? reply archon1410 10 hours agorootparent> Better by what metric? Better for whom? From Derek Parfit's On What Matters, chapter 126 (Has It All Been Worth It?): > If someone dies a slow and painful death, it would have been both better for this person, and impersonally better, if this person’s life had ended earlier. The last part of this person’s life was worse than nothing, or in itself bad. We can reach similar conclusions about the whole of someone’s life. If someone’s life contains much prolonged suffering, and nothing or little that is good, it would have been both better for this person, and impersonally better, if this person’s life had ended just after it started. Things may be in one way different if we suppose instead that this person’s life had never even started. Perhaps we could not claim that this alternative would have been better for this person. But when we ask which alternative would have been impersonally better, there is little difference between these two comparisons. Since it would have been better if this person’s life had stopped just after it started, it would also have been better if this person’s life had never started. In other words, it would have been impersonally better if this wretched person had never existed. And since such claims make sense when applied to one person, they also make sense when applied to all conscious beings, or to the whole of reality. Parfit thinks it was/is worth it, given the good things in life, and using future improvements in QoL as one of the arguments. I don't. What amount of pathos, pleasure, love, beauty, could justify the life and suffering of even one person who say, was raped and murdered in childhood? It would be very difficult to justify the creation of a world given that tradeoff, but people are much hesitant to accept that it is unjustified now that it already exists. reply epiccoleman 4 hours agorootparentI am wildly uneducated on this topic, but I don't know if the concept of \"impersonally better\" makes sense when extended as far as \"it would be impersonally better if none of this ever existed.\" I guess I can buy that \"impersonally better\" might be intelligible if there is some subjective experience to ... experience it, but in a universe with no consciousness how could we say any state of affairs is better than another? As for this question: > What amount of pathos, pleasure, love, beauty, could justify the life and suffering of even one person who say, was raped and murdered in childhood? I'm not really in the business of doing these sorts of utilitarian calculations, and I think this back-and-forth is a great example of why I don't think they lead anywhere useful. But it seems to me that you could make the same argument as Parfit above in reverse. If someone led an essentially perfect life for 100 years, filled with all the things we all agree make life good, but then was subjected to 30 seconds of torture and then murdered, that's still a good life on net, and I think you'd be hard pressed to make a serious argument that those 30 seconds of intense suffering at the end are terrible enough to make that life not having been worth living. So, by the same kind of induction that Parfit does above, it seems you could contrive a situation where the vast amount of universal conscious flourishing being experienced by, let's say, quadrillions or consciousnesses would \"outweigh\" the suffering of however many millions of apes it took to get there. And even in the particular case of a horribly murdered child, is it universally true that no life ending in that manner could have been worth the suffering at the end? If a child lives ten wonderful years with loving parents, doing all the best things a child can do, but then dies horribly, can we be sure that life wasn't worth living? That it would be better on net had it never happened? I'm not sure, and like I said, I don't typically go in for arguments that work on this basis. I'll admit that's probably partially because they feel distasteful, but I think there's also an intuition worth examining that those types of arguments don't really work, that they miss something critical about subjective experience when we try to sum up all the goods and bads of a life into some metric that can assign that life into a bucket of \"worth living\" or \"not worth living.\" tl;dr - I'm not convinced, and I'm happy I exist. reply Rapzid 10 hours agorootparentprev> no doubt that it would have been better if the world didn't exist That's incredibly boring though. reply pestaa 13 hours agoparentprevNo, not all children die while being a child. Antinatalism (the philosophy that having kids is immoral) has nothing to do with life expectancy. reply davidsawyer 12 hours agoparentprevSuper helpful, have a great day reply BadHumans 14 hours agoprevThe world is awful. Millions of children die each year. The world is much better. Even though children die, we don't have nearly as many as we used to. The world can be much better. Most of these child deaths are preventable. This is how I view the world and it is why it upsets me when people voice concern about crime in a city or something like that and people respond with \"ACKSHUALLY\" followed by some sort of statistic. It feels like people just want to stay stuck but as the author says \"to see that a better world is possible, we need to see that both are true at the same time: the world is awful, and the world is much better.\" Being too dismissive of certain concerns is just as bad as being too pessimistic. reply Ensorceled 14 hours agoparent> This is how I view the world and it is why it upsets me when people voice concern about crime in a city or something like that and people respond with \"ACKSHUALLY\" followed by some sort of statistic. Most people voicing \"concern\" are saying \"crime is increasing\" and usually blaming either current politicians and/or progressive policies for the \"increase\". It's almost always accompanied by either a \"vote for me\" or some screed on how we have \"gone soft on crime\". reply stuartjohnson12 13 hours agorootparentCrime rates are a prime battlefield for culture wars. The fact that talk about crime rates is usually an attack on progressive policies usually means progressives usually also respond in an ACKSHUALLY manner followed by a wall of anarchist crime theory which must account for all (not just some) of any disparity, because if it does not then you have stabbed your own side in the back. Whatever you think about Elizier Yudowsky, I think he hit this one right on the nail. https://www.lesswrong.com/posts/PeSzc9JTBxhaYRp9b/policy-deb... reply fasthands9 12 hours agorootparentprevCrime is obviously down over the decades, which is very important, but I think it would be silly to deny there was a major uptick in murders in 2020. https://www.axios.com/2023/12/28/us-murder-violent-crime-rat... There are obviously lots of reasons for this having nothing to do with crime policy, such as the economic shock of covid. That being said, I don't think its true at all it is straightforwardly settled how to reduce crime. Pretty much all people I have read on this suggest penalizing gun possession crimes more would decrease the violent crime rate. It's part of the reason murder rate in places like NY is relatively low. That said, prosecuting more people for probation violations or gun possessions is a a policy that cuts against both conservative ideology (because they are pro gun) and progressive ideology (because it will require a larger prison population that will be disproportionately minority) reply ethanbond 14 hours agoparentprevUsually when people are complaining about crime in a city they literally do not know the facts. That's very different from \"things are factually better than they've ever been, but we can and should do much better.\" reply colechristensen 14 hours agorootparentThe two groups that talk about crime are more or less equal in their self-delusion. The left-wing folks aggressively deny there is any sort of problem and the right-wing folks insist such and such a place is a warzone and anyone who goes there should be afraid. Mind you there are plenty of people who are fairly reasonable who you don't see talking about these things publicly much, usually because the people who respond are so very obnoxious. You have people denying reality vs. people exaggerating it, neither really have very good information. I have personally experienced both. I left a formerly nicer neighborhood that took a turn to avoid robberies and too frequent gun shots for downtown where I feel much safer... and manage to offend both sides when I mention this. reply whstl 14 hours agorootparentYep. It’s like this for lots of other subjects. From war to paper straws. Not saying whether it is wrong or right, but: it is often people trying to move the Overton window, or people getting manipulated by window movers. reply dfxm12 13 hours agorootparentprevThe left-wing folks aggressively deny there is any sort of problem I'm not familiar with anyone, at least in the mainstream of politics, doing this. I think it is important to note the mainstream, because we do have tons of elected officials and mainstream cable news talking heads making the \"warzone\" argument (and deliberately mischaracterizing any attempts at police or justice system reform). reply colechristensen 8 hours agorootparentI know many people personally who do it in conversations with me. Locally we have a county attorney who ran and won on, essentially, a platform to not prosecute crimes. She's done such a bad job that in a few egregious cases the state attorney general took cases over. It has gotten to the point where there are liberal-talking-point topics that I just don't address with good friends in the same way you avoid anything political with the slightly unhinged conservative uncle. I'm not familiar with anyone, at least in the mainstream of politics, who is talking about crime and criminals in a way that seems like they are actually interested in making the situation better. reply deanCommie 12 hours agorootparentprev> The left-wing folks aggressively deny there is any sort of problem and the right-wing folks insist such and such a place is a warzone and anyone who goes there should be afraid. This is contextual and is misleading without it. But it does accurately represent the ineptitude of the left and why it always loses in these situations. Here's how it goes: Problem: Society has people that are poor and mentally challenged. Both turn to drugs to cope. Drugs are illegal, increased demand increases criminality. Drug use exacerbates poverty, homelessness. Society becomes full of drug users committing petty crimes, and drug dealers perpetuating major crimes to contribute to supply. Not to mention addicts dying in the streets from overdoses. All of this in general make cities undesirable and less safe. RIGHT-WING Solution: LET'S GET TOUGH ON CRIME. Drug dealers get arrested. Drug users get arrested. For a little while the streets are cleaner and safer, and everyone is happy. But the root cause hasn't been addressed, so the same problems just return. LEFT-WING Solution: Drug use is a symptom not a root cause. And criminality is inherent to drug trade because drugs are illegal. If we legalize or decriminalize drugs we reduce the criminal element. If we give people safe injection sites they don't have to die from overdoses. And if we fund social programs we can get people out of poverty, off the streets, and into housing. PROBLEM 1: All of this is a lot harder than sending a dozen cops into a tent city and arresting a dozen homeless people. PROBLEM 2: Even if carried out PERFECTLY, there becomes an intermediate step where homeless people are seen being given funds or housing from the government which makes poor-but-not-homeless, and even middle-class people get mad saying \"I work so hard, how come this person who is clearly a loser is getting all these handouts? This makes people petty and the right wing seize on it. PROBLEM 3: Going soft on drug use means in the short term you have people using drugs more openly, but not being arrested for it. This makes people grossed out and the right wing seize on it. Before you know it and before any meaningful improvements have been made, you have right wing candidates screaming that all of the problems of society are because the left is too soft, and we need to get tougher. And they usually succeed and they usually win. Because the best the left wing can do is point to statistics that show fewer homeless people are dying of poverty or drug overdoses, and the truth is most humans in society just don't care. So they deny and minimize. The reality is that out of sight out of mind, most gentle moderate even somewhat progressive people would be just as happy if homeless people \"disappeared\". They don't want to think about what that means. So they vote left when they feel guilty, and right when they're annoyed. reply ajross 13 hours agorootparentprevFWIW: per SF's statistics, both violent and property crime rates are at 10+ year lows in every category: https://sfgov.org/scorecards/public-safety/violent-crime-rat... I'm not how you get a \"both sides\" argument from that data, but OK. We're in the same phase of this debate that we are with inflation: there was a burst of signal, driven largely by the pandemic and related causes, it receeded, but argumentation is still informed by feelings and not current state. (Edit: and the amount of argumentation below trying to refute this one link WITHOUT ALTERNATIVE SUPPORTING EVIDENCE is pretty much proof that this isn't a fight about facts.) reply Jensson 13 hours agorootparentStatistics from the police is the least accurate way to measure minor crimes, those stats mostly tracks how active the police is and not how much crime there is. Or do you believe that Denmark and Sweden has the most cases of thefts in the world? These stats has nothing to do with how much theft is actually happening. https://www.theglobaleconomy.com/rankings/theft/ reply ajross 13 hours agorootparentAre there better numbers? It seems like these track existing conventional wisdom about the recent crime burst, no? You're just trying to throw out the last 2 years showing a decline? Is that really a reasonable argument? Obviously yes, there's an apples/oranges problem with comparing data sets collected in different countries under different law enforcement regimes, etc... But between e.g. 2022 and 2017 in San Francisco specifically? I don't see the argument. (Also important to note that while \"Larceny\" might be plausibly related to police ignoring crime, other things like \"Murder\" are very much not if you aren't accusing the police of hiding bodies. And violent crime shows the same trend.) reply Jensson 13 hours agorootparentIt is much more likely that the police changed a bit on how they report things than that the population at large changed. The real changes gets lost in the noise of police reporting changes. Police reporting changes not just via bureaucratic decisions but also the feelings of the police force in general because it is the people at the bottom that decides what to report, which is very fickle and can change quickly with reasons like \"we catch thieves but they just get released, so we stopped caring\". Covid likely changed crime rates, yes, but it likely changed police reporting rates much more. That goes for all kinds of events. Saying crime is down since police reporting is down is like saying that kids learn more today since they get better grades today than 10 years ago. Edit: You get much better numbers by asking people if they have been robbed lately, or asking stores how much gets stolen. reply Fargren 13 hours agorootparentprev\"Are there better numbers?\" is not a question that justifies trusting bad numbers. If the best numbers you have are known to be unreliable, using them just because you don't have better ones is not justified. Let's say the real amount per year for the last 10 years is [100, 110, 120, 130, 140], and you have numbers that show [90, 89, 88, 87, 86]. Those numbers are much better than [1, 2, 3, 4, 5]. It would still be absolutely wrong to use them to figure out the trend. If you know your source of data is bad, you must throw it out, even if it's the best one you have. If our data is bad, we just don't know. reply cm11 12 hours agorootparentprevWe should be careful about the \"it's the best we got\" phase of the argument. It usually doesn't add information, but pushes the convo as though it does. Presenting numbers is additive, questioning those numbers relevancy can be additive. Of course the \"best we got\" might not be good enough to make a call. In this argument, the sides are something like crime is down, crime is up, and don't have enough info. Roughly speaking, you're arguing for the first over the second whereas the responder is arguing for not enough info. Even in situations where maybe you have to make a decision and don't have great data, if you don't feel great about the best info you have, then it might be better to use something else like the wisdom or gut instinct of the team or what's cheapest or what you're most able to walk back later. Data tends to make us lazy about digging deeper, it's okay when the data is good, but worse no data when it's not so relevant. reply Kalium 13 hours agorootparentprevThere are probably better numbers somewhere. Likely several sets worth. One of the things that makes SF's numbers especially thorny is that SFPD engages in a daily campaign to deter reporting crime. This makes the official numbers known unreliable, but also means there's lots of room to debate how much more reliable any alternative set of numbers might be. reply jstarfish 10 hours agorootparentprev> Statistics from the police is the least accurate way to measure minor crimes, those stats mostly tracks how active the police is and not how much crime there is. Bullshit. You're applying census logic to crime statistics to sow FUD. The census doesn't count people who hide from the government. That does not mean we cannot trust census data, just that it isn't flawless. It's a count of crimes reported by citizens. You can drill down further to see which ones stuck through to arrests and convictions. This is the closest to actual data we're ever going to come in measuring a concept like \"crime.\" The problem is that the stats tell a wildly different story than what progressives want to hear, so they move to discredit the police and their data with academic vagaries like \"overpolicing.\" Let's examine that. Police supposedly overpatrol black communities. Besides Wayne Williams and the DC snipers, can you name any black serial killers? Something like 35% of all serial killings in America are committed by black males (4-5 victims each, and this does not include gang shootings!). The implication is that if police \"overpoliced\" white areas in the same way, there would be a similar rise in body count from white perpetrators. White communities should be knee-deep in their dead by this logic, yet I don't see anybody complaining about all the corpses in the streets. The act of policing does not \"generate\" murder victims. Black men just kill a lot of people and it's an uncomfortable truth. Domestic violence is the other big lie we swallow. Men stopped beating their wives, so women invented new reasons to claim victimhood. Now just yelling at them is reframed as assault in the social sphere, and we're presented no end of excuses for why \"rape\" can't be reported to the police. It's not spousal rape anymore, it's more-vague \"consent violation.\" Adhering to agreements that are subject to arbitrary change is impossible and unenforceable, but if you run afoul of it, they run to social media telling \"their\" truth (which is notably distinct from \"the\" truth). Nobody asks your side or gives you a fair hearing. They immediately isolate you from friends and family and sever your means of financial support. This is literally vigilante domestic violence against men, committed by the \"victims,\" in plain sight, with public support. Believing women (or anybody else) without evidence is the flag of a fool. Police report or it didn't happen. Here's the truth: the argument of overpolicing was applicable only to property crime, but through sophistry the left reframes it to look like it applies to all crime, the stats are faulty and all cops are bastards. Every bit of this is exaggeration for political effect. It would not surprise me in the least if Denmark or Sweden did in fact have the highest rates of property theft. You have pickpockets, high density, and are welcoming of refugees and gypsies. I trust the stats. I don't trust you. reply Jensson 9 hours agorootparent> It would not surprise me in the least if Denmark or Sweden did in fact have the highest rates of property theft. You have pickpockets, high density, and are welcoming of refugees and gypsies. I trust the stats. I don't trust you. You really think they have 5 times more theft than Poland or 15 times more than Mexico? The total numbers doesn't make sense, countries are all over the place regardless of their situation or stability. Also that data is from 2003, it is before Sweden had taken in a significant amount of new immigrants. There were no security anywhere because it wasn't needed, and shops didn't close due to excessive theft. How can that be worse than a country where stores has to put products behind bars and put security guards to protect themselves, and still sometimes have to close due to the issues? reply colechristensen 8 hours agorootparentprevMany jobs and retailers have left SF. It's not surprising that statistics got \"better\". Also the police will have had a lot of pressure on them to make the statistics better, they're not to be trusted to publish good statistics. I could find lists of store closures and numbers of jobs gone, but instead one data point: whole foods closing an enormous store after only one year. https://abc7.com/whole-foods-san-francisco-store-closing-wor... That either means what they say: crime was a huge problem, or they're covering for bad sales: people have left. And the world isn't just SF. Locally I've been in line at a coffee shop twice this year while somebody stole something from the grab and go case and ran... there aren't any more grab and go cases or merchandise at several coffee shops. It wasn't long ago that I could go to the local target and buy things off shelves instead of waiting for someone to open the case with a key. reply inglor_cz 12 hours agorootparentprevHard science people tend to worship data and dismiss anecdotes, but a situation where the data is incomplete/incorrect (for whatever reason) and anecdotes which point in a different direction are, in fact, correct, is perfectly plausible. For example, the former Soviet Bloc was unmatched in its ability to produce impressive statistics of various achievements, but the real living standard of the people would strike you in the face the moment you would see it. Which is why the secret police often restricted free movement of Western visitors. SF is pretty bad. A few weeks ago, Czech TV reporters were robbed at a gunpoint in broad daylight [0]. Stuff like that simply doesn't happen in Prague, Warsaw or even war-torn, PTSD-heavy Kyiv. It rather corresponds to South African standards of safety. IDK if you can explain it away with positively sounding statistics, but as Feynman says, \"The first principle is that you must not fool yourself and you are the easiest person to fool.\" If a culture war coded topic like crime is being discussed and tribal loyalty kicks in, I can imagine people simply ignoring anything that goes contrary to their position and rallying to the flag. [0] https://www.nbcnews.com/news/us-news/czech-journalists-cover... reply dfxm12 12 hours agorootparentFWIW, with more guns than people, the USA is the most armed country in the world. It follows that being robbed at gunpoint isn't something that is frequent in a country where there's only 1 gun per 40 people. The same people arguing about violent crime in the USA could do themselves a huge favor by being open to more gun control, but by and large, they are against it. https://en.wikipedia.org/wiki/Estimated_number_of_civilian_g... reply inglor_cz 1 hour agorootparentDoes the pattern of armed robberies at daylight correlate with the pattern of gun ownership across the US, or no? If not, the causality may be a lot more complicated. People don't turn into gangsters just by owning more guns. As you mention, USA is the most armed country in the world, which means that Latin American countries have, theoretically, fewer guns than the US. But they have a lot more gang violence, so much more that LatAm cities fill the \"top 50 violent cities in the world\" category, with barely any representants from the Old World (AFAIK only Johannesburg is up to par). reply southernplaces7 11 hours agorootparentprevI could just as easily list off several countries that have strict gun control and at the same time also violent crime rates (including those with guns) that are far worse than general levels in the U.S. gun control by itself isn't the problem when it comes to violence. Other, largely social and political factors are much more important, but that's not a neat ideological talking point so it gets ignored more often. reply selimthegrim 12 hours agorootparentprevI wonder what Feynman and Robert Trivers (not a hard science person at all) would make of each other. reply ilikehurdles 13 hours agorootparentprevJust about every category in your chart shows a reversal from a downward trend before 2019 into an upward trend from 2019 onward. The precipitous and unusual drop at the end of the axis reflects a lack of data. Just about every kind of crime is up in Portland as well over 2018 numbers, with homicides almost quadrupling 3 years later. The “both sides” should be considered, because Portland isn’t a war-zone like the far right would portray it as, but it’s significantly worse than it used to be with no reversal in sight. There are worse places to live in the US today, but I’m not off-base for wanting safety and local quality of life to improve rather than decline when compared to previous years. And sure, the next thing to blame is the pandemic recovery. We all faced drastic changes over the last few years, but objectively some of our cities (like Portland where I live) are not recovering as well or at all compared to national trends. And as for Portland, most of our deteriorating trends started before the pandemic. 2020-2022 just accelerated their trajectories. reply ajross 12 hours agorootparent> Just about every category in your chart shows a reversal from a downward trend before 2019 into an upward trend from 2019 onward Literally every category in that chart shows a reduction in crime over the last year. You're inventing a \"trend\" by extrapolating a line straight across an outlier (the covid pandemic). No one would look at that data and say crime is getting worse. You'd say crime got worse and is now back at baseline. reply ilikehurdles 12 hours agorootparentWhat does the sentence after the one you quoted say? I forgot. A line graph showing a precipitous drop in the most recent time bucket available is a red flag that you shouldn’t trust that data point. reply ajross 12 hours agorootparentWhy? It's not a real time measurement, they're just adding up the crimes from the 2023 data. Do you expect the 2023 numbers to be revised? Was that true for earlier years? Seems unlikely. reply reducesuffering 12 hours agorootparentprevI like SF, and it's violent crime rate isn't too bad. But, c'mon, you can't point to property crime and say \"this isn't a fight about facts\" when it's the 4th highest property crime per capita city in the entire country in a country that already has high crime for the developed world. https://en.wikipedia.org/wiki/List_of_United_States_cities_b... reply JohnFen 13 hours agorootparentprev> The left-wing folks aggressively deny there is any sort of problem I have to admit, I don't think I've heard any left-wing folks saying that at all. reply klipt 13 hours agorootparentprev> The left-wing folks aggressively deny there is any sort of problem and the right-wing folks insist such and such a place is a warzone and anyone who goes there should be afraid. Some left wing folks will agree you should be afraid but only if you're female/black etc because men/white people might attack you. But they'll insist if you're a white man then you're protected from crime by \"privilege\". Which is interesting because statistically men are much more often victims of violent crime. reply Throw73747 13 hours agorootparentprevArguing about \"facts\" and statistics, when it is literally impossible to report basic crimes like robbery to police is pointless! reply X6S1x6Okd1st 13 hours agorootparentWhat do you mean it's literally impossible to report robberies to the police? reply threemux 13 hours agorootparentIf you report a minor robbery to the police in any large American city and you aren't a public figure, there is functionally a 0% chance of them following up on it much less solving it. The only benefit for reporting is if you plan to make an insurance claim. If not, there is no point to reporting a minor robbery to the police. Robberies and property crimes are hugely underreported in official statistics because the first time you try to report one you realize that it makes a bad situation worse by wasting your time after the event. That is what they're talking about. reply feoren 13 hours agorootparentOther benefits of reporting it, other than insurance, is that it makes these kinds of statistics more accurate, and that in the unlikely event that they \"accidentally\" solve the case, it'll be easier for you to get your stuff back. The latter can happen if they arrest someone for some other reason and find what appears to be a bunch of stolen property or something. It's not likely, but it seems like it'd still be worth reporting the crime. But you make a good point that a lackluster police response does lower the incidence of reporting crimes, effectively doctoring crime statistics. You'd have to evaluate whether this trend has increased or decreased relative to historical periods when factoring that into any comparison, though. reply sp0rk 12 hours agorootparentprev> If you report a minor robbery to the police in any large American city and you aren't a public figure, there is functionally a 0% chance of them following up on it much less solving it. I have known people that had stolen things returned because the police found the items while investigating/arresting the thief for other crimes. It seems foolish to not bother with filing a report just because they aren't actively investigating every report they receive. reply zhivota 13 hours agorootparentprevPolice stop taking reports of crimes they will not pursue or won't be prosecuted. There is no \"fact\" saying how much crime is occurring, only a snapshot of how many reports were accepted and filed by the police. reply Throw73747 13 hours agorootparentprevThey just refuse to take report, with some bullshit excuse. Like under $950 it is misdemeanor.. Everything in shop is locked up, but crime is down! reply banannaise 12 hours agoparentprevThe counter-argument to \"crime is too high\" isn't \"that can't be improved\", it's \"you're looking at the wrong thing, and to target 'crime' in certain ways actually makes the world worse\". reply p0wn 14 hours agoparentprevI wish the world was much butter. It would be more tasty. reply gumby 13 hours agorootparentYes but deaths due to coronary heart disease would increase. reply BadHumans 13 hours agorootparentprevHa! I didn't catch that. Thanks reply _a_a_a_ 14 hours agorootparentprevThe moon is much cheese. reply nabla9 14 hours agoparentprevWhen people say crime is getting out of hand, while it's actually decreasing, they are just wrong and ignorant. reply d4mi3n 14 hours agorootparentThat's a bit reductive. It can be true that globally (or in some superset) crime has decreased while in some local context it remains the same or has become concentrated (e.g. worse for people in that context). This whole article and commentary around it has also highlighted another issue for me: It's hard to have nuanced conversations about complex problems that can be simplified or generalized to something causes a difference in perception about what actual problems *are*. reply nabla9 13 hours agorootparentWhen people say crime is getting out of hand, while it's actually decreasing in the context they mean it, they are just wrong and ignorant. This is common bias. People consistently think that crime is getting worse. reply rcoveson 13 hours agorootparentSo there are people out there, not in this thread, who are wrong when they say things that are incorrect? reply kapp_in_life 13 hours agorootparentprevIn many metro areas crime is higher than it was before COVID. Sure the levels may be decreasing below the 2020 peak levels, but relative to 2019 or earlier its oftentimes still higher. reply PeterisP 7 hours agorootparentprevWhen people say crime is getting out of hand, while it's actually decreasing, all it means is that apparently the trends of average rate of crime doesn't match the trends of the particular types of crime or the location that actually impacts their group. reply acuozzo 13 hours agorootparentprevDoes \"it's actually decreasing\" mean \"it's decreasing on average in the US\" or \"it's decreasing on average in every county in the US\"? reply BadHumans 13 hours agorootparentprevMost crime goes unreported.I think blanket statements this are just wrong and ignorant. reply signatoremo 10 hours agorootparentAnd it was also unreported before. It isn’t a new phenomenon. Do you like it better with “The reported crime rate has been decreased over a long enough period”? reply amelius 14 hours agorootparentprevYou can blame the media. And social media included. reply cmrdporcupine 12 hours agorootparentprevMany many interesting points of information are lost when averages are taken. Averages can lie. If some working class people objectively feel their lives have worsened from crime, it's \"progressives\" in particular who need to be listening even if some of the facts appear to be wrong. It's quite possible critical theorists and sociologists and various experts are missing some important variance. reply thomastjeffery 13 hours agoparentprevThere is a selection bias driving the conversation itself. The people most likely to voice a criticism about a given subject are the people who are most engaged with that subject. Engagement itself is diverse. It can be driven by genuine interest, and it can be generated by political narrative. What is most important is the criticism itself. Is it valuable? To whom? The more people there are voicing criticisms, the more difficult it is to answer these questions. The usefulness of democracy is that we can coordinate our criticisms into coherent proposals, and vote on them. The tragedy of democracy is that we must coordinate our criticisms into coherent proposals, and vote on them. reply nonrandomstring 13 hours agorootparent> The tragedy of democracy is that we must coordinate our criticisms into coherent proposals, and vote on them. Despite classes in civics and debating at some good schools we are mostly given no training on how to do this as kids. Instead we are raised by \"Hollywood diplomacy\", which is deeply confrontational and revolves around vengeance and gun fights. Polarisation isn't just in the \"environment\" but in the lack of tools we are given to work with. There are actually long-form studies in things like Peace Studies (Columbia, George Washington Uni, Kroc Institute, Nottingham Uni in UK) I spent some time with graduate of a peace studies programme, which I initially mocked. But she introduced me to all kinds of ideas like those of Habermas and Discourse Theory. Most serious [fn] programmes on negotiation and diplomacy touch on this. How we get ordinary folk to take on board more of that is challenging but urgent. Sadly most of \"western\" life has conflict escalation built in as a value. EDIT: added some links for the curious [0,1] [fn] There's plenty of crappy MBA business type \"how to get what you want\" type programmes - I am absolutely not talking about those! [0] https://iep.utm.edu/habermas/ [1] https://en.wikipedia.org/wiki/Communicative_rationality reply thomastjeffery 8 hours agorootparentPolarization is also an emergent effect of first-past-the-post voting. We don't need every participant in democracy to be an expert in diplomacy. We just need a system that incentivizes genuine engagement and compromise. reply b450 14 hours agoparentprev> This is how I view the world and it is why it upsets me when people voice concern about crime in a city or something like that and people respond with \"ACKSHUALLY\" followed by some sort of statistic. It feels like people just want to stay stuck but as the author says \"to see that a better world is possible, we need to see that both are true at the same time: the world is awful, and the world is much better.\" I don't think people \"want to stay stuck\". I think it is simply the state of our polarized polity, and the mistrustful, zero-sum approach to discourse that it breeds. Even a plain factual statement feels like it's signaling some political allegiance or pushing some agenda, and that feels threatening. I'm not placing myself above this phenomenon, either. Your choice of example fact – something about 'crime in a city' – feels like a right-wing shibboleth (and after all, even if there are objective facts, the particular facts we recognize as salient, the narrative patterns of fact that we use to make political arguments, are of course driven by our values and political allegiances). Maybe I'm off-base about your example, but really my point is that it feels that way, so discussions of politics feel scary, even when they stick to matters of putatively objective fact. Apologies for shoehorning in the Israel/Palestine issue, but I thought this piece[1] was eloquent on (something like) this phenomenon: > [...]if, as many feel at this moment, the recognition of one “side” comes at the expense of the other, the expression of empathy for one is the refusal of empathy for another. Many seem to feel that no hand is bare—that all hands are holding knives, pointing in opposite directions. This moment is characterized by a widespread conviction that recognition can only go in one direction: that any show of empathy toward Israelis is tantamount to supporting the oppression of Palestinians, and that any show of empathy toward Palestinians is tantamount to supporting the massacre on October. > Those who subscribe to this dichotomy see attempts to recognize all suffering as disingenuous and manipulative. They sometimes complain that symmetrical empathy ent",
    "originSummary": [
      "The article highlights the dual nature of the world, acknowledging that it has both negative aspects and areas of progress.",
      "Using child mortality as an example, the author emphasizes the improvements made while acknowledging the existing issues.",
      "The article argues that recognizing both the problems and the progress is crucial for believing in the potential of a better world."
    ],
    "commentSummary": [
      "The discussion delves into various topics such as global state, population and economic growth, resource depletion, climate change, crime rates, and political polarization.",
      "Different perspectives, both optimistic and pessimistic, are presented, showcasing a balanced approach.",
      "The complexity and challenges surrounding these issues are emphasized, along with the importance of reliable data, being open-minded, and engaging in productive debates."
    ],
    "points": 365,
    "commentCount": 268,
    "retryCount": 0,
    "time": 1707245185
  },
  {
    "id": 39274918,
    "title": "GPT vs Lawyers: Language Models Outperform Humans in Legal Reviews",
    "originLink": "https://arxiv.org/abs/2401.16212",
    "originBody": "Computer Science > Computers and Society arXiv:2401.16212 (cs) [Submitted on 24 Jan 2024] Title:Better Call GPT, Comparing Large Language Models Against Lawyers Authors:Lauren Martin, Nick Whitehouse, Stephanie Yiu, Lizzie Catterson, Rivindu Perera (Onit AI Centre of Excellence) Download PDF HTML (experimental) Abstract:This paper presents a groundbreaking comparison between Large Language Models and traditional legal contract reviewers, Junior Lawyers and Legal Process Outsourcers. We dissect whether LLMs can outperform humans in accuracy, speed, and cost efficiency during contract review. Our empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers, uncovering that advanced models match or exceed human accuracy in determining legal issues. In speed, LLMs complete reviews in mere seconds, eclipsing the hours required by their human counterparts. Cost wise, LLMs operate at a fraction of the price, offering a staggering 99.97 percent reduction in cost over traditional methods. These results are not just statistics, they signal a seismic shift in legal practice. LLMs stand poised to disrupt the legal industry, enhancing accessibility and efficiency of legal services. Our research asserts that the era of LLM dominance in legal contract review is upon us, challenging the status quo and calling for a reimagined future of legal workflows. Comments: 16 pages Subjects: Computers and Society (cs.CY); Computation and Language (cs.CL) Cite as: arXiv:2401.16212 [cs.CY](or arXiv:2401.16212v1 [cs.CY] for this version)https://doi.org/10.48550/arXiv.2401.16212 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Rivindu Perera [view email] [v1] Wed, 24 Jan 2024 03:53:28 UTC (695 KB) Full-text links: Access Paper: Download PDF HTML (experimental) Other Formats view license Current browse context: cs.CYnewrecent2401 Change to browse by: cs cs.CL References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=39274918",
    "commentBody": "Better Call GPT: Comparing large language models against lawyers [pdf] (arxiv.org)357 points by vinnyglennon 18 hours agohidepastfavorite243 comments hansonkd 18 hours agoI run a startup that does legal contract generation (contracts written by lawyers turned into templates) and have done some work GPT analysis of the contract for laypersons to interact and ask questions about the contract they are getting. In terms of contract review, what I've found is that GPT is better at analysis of the document than generating the document, which is what this paper supports. However, I have used several startups options of AI document review and they all fall apart with any sort of prodding for specific answers. This paper looks like it just had to locate the section not necessarily have the back and forth conversation about the contract that a lawyer and client would have. There is also no legal liability for GPT for giving the wrong answer. So It works well for someone smart who is doing their own research. Just like if you are smart you could use google before to do your own research. My feelings on contract generation is that for the majority of cases, people are better served if there were simply better boilerplate contracts available. Laywers hoard their contracts and it was very difficult in our journey to find lawyers who would be willing to write contracts we would turn into templates because they are essentially putting themselves and their professional community out of income streams in the future. But people don't need a unique contract generated on the fly from GPT every time when a template of a well written and well reviewed contract does just fine. It cost hundreds of millions to train GPT4. If $10m was just spent building a repository of well reviewed contracts, it would be a more useful than spending the equivalent money training a GPT to generate them. People ask pretty wide range of questions about what they want to do with their documents and GPT didn't do a great job with it, so for the near future, it looks like lawyers still have a job. reply onetimeuse92304 15 hours agoparent> Laywers hoard their contracts and it was very difficult in our journey to find lawyers who would be willing to write contracts we would turn into templates because they are essentially putting themselves and their professional community out of income streams in the future. I notice same things in other professions, especially where it requires a huge upfront investment in education. For example (at least where I live), there was a time about 20 years ago where architects also didn't want to produce designs that would be then sold to multiple people for cheap. The thinking was that this reduces market for architecture output. But of course it is easy to see that most people do not really need a unique design. So the problem solved itself because the market does not really care and the moment somebody is able to compile a small library of usable designs and a usable business model, as an architect, you can either cooperate to salvage what you can or lose. I believe the same comes for lawyers. Lawyers will live through some harsh time while their easiest and most lucrative work gets automated and the market for their services is going to shrink and whatever work is left for them will be of the more complex kind that the automation can't handle. reply adra 15 hours agorootparentI think you greatly underestimate this group to retain their position as a monopoly. A huge chunk of politicians are lawyers, and most legal jurisdictions have hard requirements around what work you must have a lawyer to perform. These tools may make their practices more efficient internally, but it doesn't mean that value is being passed on to the consumer of the service in any way. They're a cartel and one with very close relationships with country leadership. I don't see this golden goose souring any time soon. reply onetimeuse92304 13 hours agorootparentI think what you are missing is businesses doing what businesses have always been doing: finding a niche for themselves to make a good profit. When you can hire less lawyers and get more work done and cheaper and at the same (or better) quality, you are going to upend the market for lawyering services. And this does not require to replace lawyers. It is just enough to equip a lawyer with a set of tools to help them quickly do the typical tasks they are doing. I work a lot with lawyers and a lot of what they are doing is going to be stupidly easily optimised with AI tools. reply d0odk 13 hours agorootparentPlease elaborate with some examples of what legal work you think will be optimized with AI tools. reply Spooky23 7 hours agorootparentLots of corporate and government law work. It’s just like programmers and artists. As the tools improve, you’ll need fewer, smarter humans. reply __loam 13 hours agorootparentprevSometimes it feels like people look at GPT and think \"This thing does words! Law is words! I should start a company!\" but they actually haven't worked in legal tech at all and don't know anything about the vertical. reply TeMPOraL 3 hours agorootparentPeople started companies and succeeded for dumber reasons. Generalized \"businessing\" skills and placing yourself somewhere in the space where money is made counts for much more than actually knowing anything about specific product beforehand. reply treprinum 11 hours agorootparentprevA friend of mine is a highly ranked lawyer, a past general consul of multiple large enterprises. I sent him this paper, he played with ChatGPT-3.5 (not even GPT-4) and contract creation, he said it was 99% fine and then told me he's glad he is slowly retiring from law and is not envious of any up-and-coming lawyers entering the profession. One voice from the vertical. reply vasilipupkin 9 hours agorootparentGeneral Consul? Is he a Roman general ? reply dkga 7 hours agorootparentThat’s why he exited the market - it’s tough out there for Roman consuls and other Latin-based professions generally. reply d0odk 12 hours agorootparentprevIt's a logical reaction, at least superficially, to the touted capabilities of Gen AI and LLMs. But once you start trying to use the tech for actual legal applications, it doesn't do anything useful. It would be great if some mundane legal tasks could be automated away--for example negotiation of confidentiality agreements. One would think that if LLMs are capable of replacing lawyers, they could do something along those lines. But I have not seen any evidence that they can do so effectively, and I have been actively looking into it. One of the top comments on this thread says that LLMs are going to better at summarizing contracts than generating them. I've heard this in legal tech product demos as well. I can see some utility to that--for example, automatically generating abstracts of key terms (like term, expiration, etc.) for high-level visibility. That said, I've been told by legal tech providers that LLMs don't do a great job with some basic things like total contract value. I question how the document summarizing capabilities of LLMs will impact the way lawyers serve business organizations. Smart businesspeople already know how to read contracts. They don't need lawyers to identify / highlight basic terms. They come to lawyers for advice on close calls--situations where the contract is unclear or contradictory, or where there is a need for guidance on applying the contract in a real-world scenario and assessing risk. Overall I'm less enthusiastic about the potential for LLMs in the legal space than I was six months ago. But I continue to keep an eye on developments and experiment with new tools. I'd love to get some feedback from others on this board who are knowledgeable. As a side note, I'm curious if anyone knows about the impact of context window on contract interpretation a lot of contracts are quite long and have sections that are separated by a lot of text that nonetheless interact with each other for purposes of a correct interpretation. reply __loam 12 hours agorootparentI think one of the biggest problems with LLMs is the accountibility problem. When a lawyer tells you something, their reputation and career are on the line. There's a large incentive to get things right. LLMs will happily spread believable bullshit. reply d0odk 12 hours agorootparentIn fairness some lawyers will too, haha. I take your point, though. Good lawyers care about their reputation and strive to protect it. reply freejazz 11 hours agorootparentprevLawyers are legally liable to their clients for their advice, it's a lot more than just reputation and career. reply psunavy03 10 hours agorootparentprevhttps://xkcd.com/1570/ reply onlyrealcuzzo 6 hours agorootparentprevYeah, lawyers will go away as soon as doctors do. AI has outperformed radiologists for a while now, and I don't care how much better AI performs, radiologists aren't going away. Radiologists get to decide the laws of the field essentially. Why would they vote to kick themselves out of the highest paying job in the world? Like the medical industry, the legal industry is designed around costing you as much as possible - not really anything related to your benefits. I just can't see disruption here. The industry will wail against it tooth and nail at every chance. reply epcoa 6 hours agorootparent> AI has outperformed radiologists for a while now Refer me to the evidence where AI is outperforming radiologists in the entire body of cross-sectional imaging. Are you seriously claiming that there is AI technology today that can take a brain MRI or CT A/P and produce a full findings and impression without human intervention? You have a reference for that? reply rscho 3 hours agorootparentprevI'm a doctor and I've been following this saga for a while. What you wrote does not match my experience. Firstly, you overestimate our political reach by a large margin. If you can have acceptable service for massively cheaper, it will happen regardless of any lobbying. What _is_ true, is that AI systems outperform docs for very select, often trivial situations (e.g. routine chest X-ray). I don't believe this would shrink the market for radiologists significantly. Secondly, the non-trivial work is exceedingly difficult to automate, because those cases currently have a prevalence to complexity ratio that make them impossible to train for. The US are making great strides towards that kind of thing, though. But not available yet. reply throwaway2037 4 hours agorootparentprevWhat is a realistic compromise? reply ramoz 6 hours agorootparentprevDo you work in the medical industry? (To be Frank it sounds like you’re spewing bs about a topic you have no intimate knowledge of - seems rather naive actually). reply singleshot_ 11 hours agorootparentprev“easiest and most lucrative work” I think this overlooks a big part of how the legal market works. Our easiest work is only lucrative because we use it to train new lawyers, who bill at a lower rate. To the extent the easy stuff gets automated, 1) it’s going to be impossible to find work as a junior associate and 2) senior attorneys will do the same stuff they did last year. If there’s a decrease in prices for a while, great, but a generation from now it’s going to be a lot harder to find someone knowledgeable because the training pathway will have been destroyed. reply faeriechangling 14 hours agorootparentprevLawyers are uniquely well equipped to legislate their continued employment into existence. reply ABCLAW 2 hours agorootparentKinda? Lawyers are myopic, vain, and we don't really do much to innovate. We wanted to make sure there would be no cross pollination between legal advisory services and other professional services in most jurisdictions, but the only thing that division did was significantly restrict our ability to widen our service offerings to provide more value. The end result is that we protected our little nest egg while our share of the professional services pie has been getting eaten by consulting and multi-service accounting firms for the past 20 years. reply tiahura 7 hours agorootparentprevLawyers, yes. Junior associates, not so much. reply jacquesm 15 hours agorootparentprevSo, you will get the template for free. And then a lawyer has to put it on their letterhead and they charge you the exact same as they do right now for that because that will be made a requirement. reply onetimeuse92304 13 hours agorootparentNo. As a business owner you will hire couple lawyers, give them a bunch of programs to automate searching through texts, answering questions and writing legalese based on human description of what is the text supposed to do. These three are from my experience great majority of the work. The 2 people you hire will now perform like 10 people without tools. Then you will use part of that saved money to reduce prices and if you are business savvy, you will use the rest to research the automation further. Then another business that wants to compete with you will no longer have an option, they will have to do this or more to be able to stay in the business at all. reply adrianN 15 hours agorootparentprevLawyers seem to be the prime group to prevent this outcome using some kind regulation. Many politicians are lawyers. reply freejazz 15 hours agorootparentThey were lawyers, they aren't still practicing attorneys representing clients. reply Tostino 7 hours agorootparentThey moved into the much more lucrative career of representing corporations. reply WanderPanda 15 hours agorootparentprevIIRC about 40% of us politicians are lawyers, unfortunately I’m sure they will find a way to gatekeep these revenue streams for their peers. reply pugworthy 13 hours agorootparentI’m assuming by the use of “us” and “they” you meant US - not that you are a politician. reply taneq 12 hours agorootparentprev> I notice same things in other professions, especially where it requires a huge upfront investment in education. Doctors, for instance. You hear no end of stories about how incredibly high pressure medicine is with insane hours and stress, but will they increase university placements so they can actually hire enough trained staff to meet the workload? Absolutely fkn not, that would impact salaries. reply singleshot_ 11 hours agorootparentUniversity placements aren’t the problem. Medical residencies are funded through Medicare and have been numerically capped. You could graduate a million MDs a year and if none of them have a training pipeline, we still lose. reply mikepurvis 17 hours agoparentprevI recently used Willful to create a will and was pretty disappointed with the result. The template was extremely rigid on matters that I thought should have been no-brainers to be able to express (if X has happened, do Y, otherwise Z) and didn't allow for any kind of property division other than percentages of the total. It was also very consumed with several matters that I don't really feel that strongly about, like the fate of my pets. I was still able to rewrite the result into something that more suited me, but for a service with a $150 price tag I kind of hoped it would do more. reply hansonkd 17 hours agorootparentOur philosophy at GetDynasty is that the contract (in our case estate planning documents) itself is a commodity which is why we give it away for free. Charging $150 for a template doesn't make sense. Our solution like you point out is more rigid than having a lawyer write it, but for the majority of people having something that is accessible and free is worth it and then having services layer on top makes the most sense. It is easier to have a well written contract that you can \"turn on and off\" features or sections of the contract than to try to have GPT write a custom contract for you. reply pclmulqdq 16 hours agorootparentI applaud the efforts to give away free documents like this. That is actually what happens when you have a lawyer do it: you pay pretty much nothing for the actual contract clauses to be written (they start with a basic form and form language for all of the custom clauses you may want), but you pay a lot for them to be customized to fit your exact desires and to ensure that your custom choices all work. The idea of \"legalzoom-style\" businesses has always seemed like a bamboozle to me. You pay hundreds of dollars for essentially the form documents to fill in, and you don't get any of the flexibility that an actual lawyer gives you. As another example, Northwest Registered Agent gives you your corporate form docs for free with their registered agent services. reply singleshot_ 10 hours agorootparentprevInterestingly, this is almost exactly how I draft a contract as an attorney. Westlaw has tons of what you might call “templates” but which have tons of information concerning when and why a client might need a certain part of the template. The difference is that when Westlaw presents me with a decision point and I choose the wrong option for my client, my client sues my insurer and is made whole. (My premiums increase accordingly). If you make the wrong choice in your choose-your-own-legal-adventure, you lose. (For some contracts, this is probably the right approach). reply JumpCrisscross 16 hours agorootparentprev> like the fate of my pets Pet trusts [1]! My lawyer literally used their existence, which I find adorable, to motivate me to read my paperwork. [1] https://www.aspca.org/pet-care/pet-planning/pet-trust-primer reply toss1 15 hours agorootparentprev>>didn't allow for any kind of property division other than percentages of the total. Knowing someone who works in Trusts & Estates, that is terrible. I've often heard complaints about drafting by percentages of anything but straight financial assets which have an easily determined value, because that requires an appraisal(s). Yes, there are mechanisms to work it out in the end, but it is definitely better to be able to say $X to Alice, $Y to Bob and the remainder to Claire. You have to think of not only what you want, but how the executors will need to handle it. We all love complex formulae, but we should use our ability to handle complexity to simplify things for the heirs - it's a real gift in a bad time. reply mikepurvis 14 hours agorootparentHeh, okay I guess what I wanted was going to end up as the worst of both— fixed amounts off the top to some particular people/causes, and then the remainder divided into shares for my kids. I guess there's an understanding the being an executor is a best-effort role, but maybe you could specifically codify that +/-5% on the individual shares is fine, just to take off some of the burden of needing it to be perfect, particularly if there are payouts occurring at different times and therefore some NPV stuff going on. reply OldOneEye 18 hours agoparentprevWhich is mostly what I feel also happens with LLMs producing code. Useful to start with, but not more than that. We've still got a job us programmers. For the moment. reply klabb3 15 hours agorootparentProducing code is like producing syntactically correct algebra. It has very little value on its own. I’ve been trying to pair system design with ChatGPT and it feels just like talking with a person who’s confident and regurgitates trivia, but doesn’t really understand. No sense of self-contradiction, doubt, curiosity. I’m very, very impressed with the language abilities and the regurgitation can be handy, but is there a single novel discovery by LLMs? Even a (semantic) simplification of a complicated theory would be valuable. reply jannw 16 hours agoparentprevYou said: \"However, I have used several startups options of AI document review and they all fall apart with any sort of prodding for specific answers. \" I think you will find that this is because they \"outsource\" the AI contract document review \"final check\" to real lawyers based in Utah ... so, it's actually a person, not really a wholy-AI based solution (which is what the company I am thinking of suggests in their marketing material) reply Aurornis 16 hours agorootparent> I think you will find that this is because they \"outsource\" the AI contract document review \"final check\" to real lawyers based in Utah ... so, it's actually a person, not really a wholy-AI based solution (which is what the company I am thinking of suggests in their marketing material) Which company is that? I don't see any point in obfuscating the name on a forum like this. reply jacquesm 15 hours agoparentprev> Just like if you are smart you could use google before to do your own research. Unfortunately people stop at step #1, they use Google and that is their research. I don't think ChatGPT is going to be treated any different. It will be used as an oracle, whether that's wise or not doesn't matter. That's the price of marketing something as artificial intelligence: the general public believes it. reply andrewla 16 hours agoparentprev> There is also no legal liability for GPT for giving the wrong answer It was my understanding that there is also no legal liability for a lawyer for giving the wrong answer. In extreme cases there might be ethical issues that result in sanctions by the bar, but in most cases the only consequences would be reputational. Are there cricumstances where you can hold an attorney legally liable for a badly written contract? reply Digory 15 hours agorootparentYes. If the drafted language falls below reasonable care and damages the client, absolutely you can be sued for malpractice. Wrong Word in Contract Leads to $2M Malpractice Suit[1]. [1]https://lawyersinsurer.com/legal-malpractice/legal-malpracti... reply gymbeaux 16 hours agorootparentprevI believe all practicing attorneys carry malpractice insurance as well as E&O (errors and omissions) insurance. I think one of those would \"cover\" the attorney in your example, but obviously insurance doesn't prevent poor Google reviews, nor would it protect the attorney from anything done in bad-faith (ethical violations), or anything else that could land an attorney before the state bar association for a disciplinary hearing. reply dctoedt 15 hours agorootparent> I believe all practicing attorneys carry malpractice insurance as well as E&O (errors and omissions) insurance. Nit: Malpractice insurance is (a species of) E&O insurance. reply kayfox 16 hours agorootparentprev> It was my understanding that there is also no legal liability for a lawyer for giving the wrong answer. There is plenty of legal, ethical and professional liability for a lawyer giving the wrong answer, we don't often see the outcome of these things because like everything in the courts they take a long time to get resolved and also some answers are not wrong just \"less right\" or \"not really that wrong.\" reply trogdor 14 hours agorootparentI think the reason you rarely see the outcomes is because those disputes are typically resolved through mediation and/or binding arbitration, not in the courts. Look at your most recent engagement letter with an attorney. I’d bet that you agreed to arbitrate all fee disputes, and depending on your state you might have also agreed to arbitrate malpractice claims. reply HillRat 12 hours agorootparentprevI mean, sure, if the attorney is operating below the usual standards of care -- it's exceptionally uncommon in the corporate world, but not unheard of. In the case of AI assistance, you run into situations where a company offering AI legal advice direct to end-users is either operating as an attorney without licensing, or, if an attorney is on the nameplate, they're violating basic legal professional responsibilities by not reviewing the output of the AI (if you do legal process outsourcing -- LPO -- there's a US-based attorney somewhere in the loop who's taking responsibility for the output). About the only case where this works in practice is someone going pro se and using their own toolset to gin up a legal AI model. There's arguably a case for acting as an accelerator for attorneys, but the problem is that if you've got an AI doing, say, doc review, you still need lawyers to review not just the output for correctness, but also go through the source docs to make sure nothing was missed, so you're not saving much in the way of bodies or time. reply freejazz 15 hours agorootparentprevIt's called malpractice reply d0odk 12 hours agoparentprevHow do you think organizations can best use the contractual interpretations provided by LLMs? To expand on that, good lawyers don't just provide contractual interpretations, they provide advice on actions to take, putting the legal interpretation into the context of their client's business objectives and risk profile. Do you see LLMs / tools based on LLMs evolving to \"contextualize\" and \"operationalize\" legal advice? Do you have any views on whether context window limits the ability of LLMs to provide sound contractual interpretations of longer contracts that have interdependent sections that are far apart in the document? Has your level of optimism for the capabilities of LLMs in the legal space changed at all over the past year? You mentioned that lawyers hoard templates. Most organizations you would have as clients (law firms or businesses) have a ton of contracts that could be used to fine tune LLMs. There are also a ton of freely available contracts on the SEC's website. There are also companies like PLC, Matthew Boender, etc., that create form contracts and license access to them as a business. Presumably some sort of commercial arrangement could be worked out with them. I assume you are aware of all of these potential training sources, and am curious why they were unsatisfactory. Thanks for any response you can offer. reply DanielSantos 11 hours agorootparentNot op but someone that currently runs an ai contract review tool. To answer some of your questions: - contract review works very well for high volume low risk contract types . Think slas, SaaS… these are contracts comercial legal teams need to review for compliance reasons but hate it. - it’s less good for custom contracts - what law firms would benefit from is just natural language search on their own contracts. - it also works well for due diligence. Normally lawyers can’t review all contracts a company has. With a contract review tool they can extract all the key data/risks - LLM doesn’t need to provide advice. LLM can just identify if x or y is in the contract. This improving the process of review. - context windows keep increasing but you don’t need to send the whole contract to the LLM . You can just identify the right paragraphs and send that. - things changes a lot in the past year. It would cost us $2 to review a contract now it’s $0.2 . Responses are more accurate and faster - I don’t do contract generation but have explored this. I think the biggest benefit isn’t generating the whole contract but to help the lawyer rewrite a clause for a specific need. The standard CLM already have contract templates that can be easily filled in. However after the template is filled the lawyer needs to add one or two clauses . Having a model trained on the companies documents would be enough. Hope this helps reply d0odk 11 hours agorootparentThanks. Appreciate your feedback. Do you think LLMs have meaningfully greater capabilities than existing tools (like Kira)? I take your point on low stakes contracts vs. sophisticated work. There has been automation at the \"low end\" of the legal totem pole for a while. I recall even ten years ago banks were able to replace attorneys with automations for standard form contracts. Perhaps this is the next step on that front. I agree that rewriting existing contracts is more useful than generating new ones--that is what most attorneys do. That said, I haven't been very impressed by the drafting capabilities of the LLM legal tools I have seen. They tend to replicate instructions almost word for word (plain English) rather than draw upon precedent to produce quality legal language. That might be enough if the provisions in question are term/termination, governing law, etc. But it's inadequate for more sophisticiated revisions. reply DanielSantos 3 minutes agorootparentDidn't try Kira but tried zuva.ai, which is a spin off from them. We found that the standard LLM performed at the same level for classification for what we needed. We didn't try everything though. They let you train their model on specific contracts and we didn't do that. For rewriting contracts keep in mind that you don't have to actually use the LLM to generate the text completely. It is helpful if you can feed all the contracts of that law firm into a vector db and help them find the right clause from previous contracts. Then you can add the LLM to rewrite the template based on what was found in the vector db. Many lawyers still just use folders to organize their files. reply jassyr 17 hours agoparentprevI'm in the energy sector and have been thinking of fine tuning a local llm on energy-specific legal documents, court cases, and other industry documents. Would this solve some of the problems you mention about producing specific answers? Have you tried something like that? reply hansonkd 17 hours agorootparentYour welcome to try, but we had mixed results. Law in general is interpretation. The most \"lawyerese\" answer you can expect is \"It depends\". Technically in the US everything is legal unless it is restricted and then there are interpretations about what those restrictions are. If you ask a lawyer if you can do something novel, chances are they will give a risk assessment as opposed to a yes or no answer. Their answer typically depends on how well they think they can defend it in the court of law. I have received answers from lawyers before that were essentially \"Well, its a gray area. However if you get sued we have high confidence that we will prevail in court\". So outside of the more obvious cases, the actual function of law is less binary but more a function of a gradient of defensibility and the confidence of the individual lawyer. reply danielmarkbruce 16 hours agorootparentI spent a lot of time with M&A lawyers and this is 100% true. The other answer is \"that's a business decision\". So much of contract law boils down to confidence in winning a case, or it's a business issue that just looks like a legal issue because of legalese. reply 3abiton 1 hour agoparentprevAre you using GPT-3 + RAG by any chance? reply wow_its_tru 15 hours agoparentprevWe're building exactly this for contract analysis: upload a contract, review the common \"levers to pull\", make sure there's nothing unique/exceptional, and escalate to a real lawyer if you have complex questions you don't trust with an LLM. In our research, we found out that most everyone has the same questions: (1) \"what does my contract say?\", (2) \"is that standard?\", and (3) \"is there anything I can/should negotiate here?\" Most people don't want an intense, detailed negotiation over a lease, or a SaaS agreement, or an employment contract... they just want a normal contract that says normal things, and maybe it would be nice if 1 or 2 of the common levers were pulled in their direction. Between the structure of the document and the overlap in language between iterations of the same document (i.e. literal copy/pasting for 99% of the document), contracts are almost an ideal use-case for LLMs! (The exception is directionality - LLMs are great at learning correlations like \"company, employee, paid biweekly,\" but bad at discerning that it's super weird if the _employee_ is paying the _company_) reply bkang97 14 hours agorootparentThat makes sense, how are you guys approaching breaking down what should be present and what is expected in contracts? I've seen a lot of chatbot-based apps that just don't cut it for my use case. reply gkk 12 hours agoparentprevHi hansonkd, I'm working on Hotseat - a legal Q&A service where we put regulations in a hot seat and let people ask sophisticated questions. My experience aligns with your comment that vanilla GPT often performs poorly when answering questions about documents. However, if you combine focused effort on squeezing GPT's performance with product design, you can go pretty far. I wonder if you have written about specific failure modes you've seen in answering qs from documents? I'd love to check whether Hotseat is handling them well. If you'r curious, I've written about some of the design choices we've made on our way to creating a compelling product experience: https://gkk.dev/posts/the-anatomy-of-hotseats-ai/ reply hansonkd 8 hours agorootparentThanks for the response. I will check it out. Specific failure modes can be something as simple as extraction of beneficiary information from a Trust document. Sometimes it works, but a lot of times it doesn't even with startups with AI products specific to extracting information from documents. For example it will have an incomplete list of beneficiaries, or if there are contingent beneficiaries, it won't know what to do. Not even a hard question about the contingency. Just making a simple list with percentages of if no-one dies what is the distribution. Further trying to get an AI to describe the contingency is a crap shoot. While I expect these options to get better and better, I have fun trying them out and seeing what basic thing will break. :) reply DanielSantos 11 hours agorootparentprevYour post is very interesting. Thanks for sharing. If your focus is narrow enough the vanilla gpt can still provide good enough results. We narrow down the scope for the gpt and ask it to answer binary questions. With that we get good results. Your approach is better for supporting broader questions. We support that as well and there the results aren’t as good. reply jerf 17 hours agoparentprevAs I've said before, one of my biggest concerns with LLMs is that they somehow manage to concentrate their errors in precisely the places we are least likely to notice: https://news.ycombinator.com/item?id=39178183 If this is dangerous with normal English, how much more so with legal text. At least if a lawyer drafts the text, there is at least one human with some sort of intentionality and some idea of what they're trying to say when they draft the text. With LLMs there isn't. (And as I say in the linked post, I don't think that is fundamental to AI. It is only fundamental to LLMs, which despite the frenzy, are not the sum totality of AI. I expect \"LLMs can generate legal documents on their own!\" to be one of those things the future looks back on our era and finds simply laughable.) reply wolverine876 3 hours agoparentprev> It works well for someone smart who is doing their own research. Just like if you are smart you could use google before to do your own research. That's a trap: If you don't have prior expertise then you can't distinguish plausible-sounding fact from fiction. If you think you are \"smart\", then afaik research shows you are easier to fool because you are more likely to think you know. Google finds lots of mis/disinformation. GPTs are automated traps: they generate the most statistically likely text from ... the Internet! Not exactly encouraging. (Of course, it really depends on the training data.) reply jonnycoder 15 hours agoparentprev\"So It works well for someone smart who is doing their own research.\" That's a concise explanation that also applies to GPTs and software engineering. GPT4 boosts my productivity as a software engineer because it helps me travel the path quicker. Most generated code snippets need a lot of work because I'm prodding it for specific use cases and it fails. It's perfect as an assistant though. reply DanielSantos 12 hours agoparentprevI launched a contract review tool about year ago[1]. The legal liability is an issue in several countries but contract generation can also be. If you are providing whatever is defined as legal services and are not a law firm, you will have issues. [1]legalreview.ai reply hansonkd 8 hours agorootparentThanks for the link. > If you are providing whatever is defined as legal services and are not a law firm, you will have issues. that is a big reason why we haven't integrated AI tools into our product yet. Currently our business essentially works as a free product that is the equivalent of a \"stationary store\" of you are filling out a blank template and it is your responsibility what happens. This has a long history of precedence since for decades people could buy these templates off the shelf and fill them out themselves. Giving a tool to our users to answer legal questions opens a can of works like you say. We decided that the stationary store templates are a commodity and should be free (even though our competitors charge hundreds for them) so we make money providing services on top of it. reply verelo 16 hours agoparentprev\"There is also no legal liability for GPT for giving the wrong answer.\" I mean, i get your point but lets be real: I cannot count the number of times I sat in a meeting and looked back at a contract and wished some element had a different structure to it. In law there are a lot of \"wrong answers\" someone could foolishly provide, but its way more often something more variable as to how \"wrong\" the answer is, than it is a binary bad/good piece of advice. I personally feel the ability to have more discussion about a clause is extremely helpful, v's getting the a hopefully \"right answer\" from a lawyer, and counting the clock / $ as you try wrap your head around the advice you're being given. If you have deep pockets, you invite your lawyer to a lot of meetings, they have context and away you go....but for a lot of people, you're just involving the lawyer briefly and trying to avoid billable hours. That's been me at the early stage of everything, and it's a very tricky balance. If you're a startup trying to use GPT, i say do it, but also use a lawyer. Augmenting the lawyer with GPT to save billable hours so you can turn up to a meeting with your lawyer and extract the most value in the shortest time period seems like the best play to me. reply hansonkd 16 hours agorootparentYou can read my other reply which agrees with you that law is a spectrum rather than a binary. > I cannot count the number of times I sat in a meeting and looked back at a contract and wished some element had a different structure to it. The only way to have something \"bullet proof\" is to have experience in ways in which something can go wrong. Its just like writing a program in which the \"happy path\" is rather obvious but then you have to come up with all the different attack vectors and use cases in which the program can fail. The same is with lawyers. Lawyers at big firms have the experience of the firm to guide them on what to do and what they should include in a contract. A small town family lawyer might have no experience in what you ask them to do. Which is why I advocate for more standardized agreements as opposed to one off generated agreements (with GPT or a lawyer). Think of the YCombinator SAFE, it made a huge impact on financing because it was standardized and there were really no terms to negotiate compared to the world before which the terms Notes were complex had to be scrutinized and negotiated. > Augmenting the lawyer with GPT to save billable hours so you can turn up to a meeting with your lawyer and extract the most value in the shortest time period seems like the best play to me. The issue is that a lot of lawyers have a conflict of interest and a \"Not invented here\" way of doing business. If you have a Trust for instance written by one lawyer and bring it to another lawyer, the majority of lawyers we talked to actually prefer to throw out the document and use their own. This method works well if you are a smart savvy person, but for the population at large, people have some crazy and weird ideas about how the law works and need to be talked out of what they want into something more sane. Another common lawyer response besides \"It depends\" is \"Well you can, but why would you want to?\" So many people of a skewed view on what they want and part of a lawyers job is interpreting what they really want and guiding them on the path of that. So the hybrid method really only works if you find a lawyer that accepts whatever crazy terms you came up with and are willing to work with what you generated. reply verelo 15 hours agorootparentThats all very reasonable, thanks for taking the time to reply! When i suggest going down a hybrid path, I mostly mean use GPT on your own (disclose this to your lawyer at your own risk) as a means to understand what they're proposing. I've spent so many hours asking questions clarifying why something is done a certain way, and most of that is about understanding the language and trying to rationalize the perspective the lawyer has taken. I feel I could probably have done a lot of that on my own time, just as fast, if GPT had been around during these moments. And then of course, confirmed my understanding aligns with the lawyer at the end. I need to be upfront, I really don't know I'm right here....its just a hunch and gut reaction to how I'd behave in the present moment, but I find myself using AI more and more to get myself up to speed on issues that are beyond my current skill level. This makes me think law is probably another good way to augment my own disadvantages in that I have a very limited understanding of the rules and exceptional scenarios that might come up. I also find myself often on the edge of new issues, trying to structure solutions that don't exist or are intentionally different to present solutions...so that means a lot of explaining to the lawyer and subsequently a fair bit of back and forward on the best way to progress. It's a fun time to be in tech, I'm hoping things like GPT turn out to be a long term efficiency driver, but I'm fearful about the future monetization path and how it'll change the way we live/work. reply freejazz 14 hours agorootparentIf you need a GPT to explain your lawyer's explanations to you, you need a new attorney. reply verelo 13 hours agorootparentEh, no...i mean, maybe...I honestly feel the issue is me. I always want a lot of detail, and that can become expensive. Sometimes the detail I want is more than I needed, but I don't know that until after I've asked the question. reply freejazz 11 hours agorootparentIf your attorney is not adequately explaining things to you or providing you with resources to understand things he does not need to spend his time explaining to you, then you need a new attorney. reply declan_roberts 15 hours agoparentprevIn other words, LLM’s are great examples of the 80/20 rule. They’re going to be great for a lot of stuff. But when it comes to things like the law the other 20% is not optional. reply asah 14 hours agorootparentSo the world needs 1/5 as many attorneys ? or 1/100 ? How will 6-figure attorneys replace that income? reply nwiswell 15 hours agoparentprev> If $10m was just spent building a repository of well reviewed contracts What's your objection to Nolo Press? They seem to have already done that. reply hansonkd 8 hours agorootparentThat was more directed towards people who are trying to train AIs to be a competitor to Nolo. Lots of document repositories exist, but they won't work with you if you want to sell legal contracts yourself. I have seen a lot of startups raise money to try to build an AI solution to this, but the results haven't been great so far. reply freejazz 15 hours agoparentprev>There is also no legal liability for GPT for giving the wrong answer. So It works well for someone smart who is doing their own research. Just like if you are smart you could use google before to do your own research. How is that good for the end user? Malpractice claims are often all that is left for a client after the attorney messes up their case. If you use a GPT, you wouldn't have that option. reply crakenzak 18 hours agoprevThis is one of the domains I'm very very excited about for LLMs to help me with. In 5-10 years (even though this research paper makes me feel its already here), I would feel very confident chatting for a few hours with a \"lawyer\" LLM that has access to all my relevant taxes/medical/insurance/marriage documents and would be able to give me specialized advice and information without billing me $500 an hour. A wave of (better) legally informed common-person is coming, and I couldn't be more excited! reply OldOneEye 18 hours agoparentI wouldn't blindly trust what the LLM says, but I take it that it would be mostly right, and that would give me at the very least explorable vocabulary that I can expand on my own, or keep grilling it about. I've already used some LLMs to ask questions about licenses and legal consequences for software related matters, and it gave me a base, without having to involve a very expensive professional into it for what are mostly questions for hobby things I'm doing. If there was a significant amount of money involved in the decision, though, I will of course use the services of a professional. These are the kinds of topics you can't be \"mostly right\". reply chaxor 17 hours agorootparentI don't understand how everyone keeps making this mistake over and over. They explicitly just said \"in 5-10 years\". So many people continually use arguments that revolve around 'I used it once and it wasn't the best and/or me things up', and imply that this will always be the case. There are many solutions already for knowledge editing, there are many solutions for improving performance, and there will very likely continue to be many improvements across the board for this. It took ~5 years from when people in the NLP literature noticed BERT and knew the powerful applications that were coming, until the public at large was aware of the developments via ChatGPT. It may take another 5 before the public sees the developments happening now in the literature hit something in a companies web UI. reply ARandumGuy 13 hours agorootparent> It took ~5 years from when people in the NLP literature noticed BERT and knew the powerful applications that were coming, until the public at large was aware of the developments via ChatGPT. It may take another 5 before the public sees the developments happening now in the literature hit something in a companies web UI. It also may take 10, 20, 50, or 100 years. Or it may never actually happen. Or it may happen next month. The issue with predicting technological advances is that no one knows how long it'll take to solve a problem until it's actually solved. The tech world is full of seemingly promising technologies that never actually materialized. Which isn't to say that generative AI won't improve. It probably will. But until those improvements actually arrive, we don't know what those improvements will be, or how long it'll take. Which ultimately means that we can only judge generative AI based on what's actually available. Anything else is just guesswork. reply ChatGTP 9 hours agorootparentI'm concerned that until they do improve, we're in a weird place. For example, if you were 16, would you go an invest a bunch of time and money to study law with the prospect of this hanging of your future? Same for radiology, would you go study that now Geoffrey Hinton has proclaimed the death of radiologists in 3 years or whatever? Photography and filmography ? My concern is we're going to get to a place where we think the machines can just take over all important professions, but they're not quite there yet, however people don't bother learning those professions because they're a career dead end and then we just end up with a skill shortage and mediocre services, when something goes wrong, you just have to trust \"the machine\" was correct. How do we avoid this? Almost like we need government funded \"career insurance\" or something like this. reply ufmace 15 hours agorootparentprevI'm not so sure that truth and trustability is something we can just hand-wave away as something they'll sort out in just a few more years. I don't think a complex concept like whether or not something is actually true can be just tacked onto models whose core function is to generate what they think the next word of a body of text is most likely to be. reply mjr00 17 hours agorootparentprevon the other hand the rate of change isn't constant and there isn't a guarantee that the incredible progress in the past ~2 years in the LLM/diffusion/\"AI\" space will continue. As an example, take computer gaming graphics; compare the evolution between Wolfenstein 3D (1992) and Quake 3 Arena (1999), which is an absolute quantum leap. Now compare Resident Evil 7 (2017) and Alan Wake 2 (2023) and it's an improvement but nowhere near the same scale. We've already seen a fair bit of stagnation in the past year as ChatGPT gets progressively worse as the company is more focusing on neutering results to limit its exposure to legal liability. reply chaxor 9 hours agorootparentYes again, it's very strange to see a simple focus on one particular instance from one particular company to represent the entire idea of technology in general. If windows 11 is far worse in many metrics than windows XP or Linux, does that mean that technology is useless? It's one instance of something with a very particular vision being imposed. Windows 11 being slow due to reporting several GB of user data in the first few minutes of interaction with the system does not mean that all new OS are slow. Similarly, some older tech in a web UI (ChatGPT) for genAI producing non-physical data does not mean that all multimodal models will produce data unsupported by physics. Many works have already shown a good portion of the problems in GPTs can be fixed with different methods stemming from rome, rl-sr, sheavNNs, etc. My point isn't even that certain capabilities may get better in the future, but rather that they already are better now, just not integrated into certain models. reply GaggiX 16 hours agorootparentprev>ChatGPT gets progressively worse https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboar..., In blinded human comparisons, newer models perform better than older ones. reply mvdtnz 16 hours agorootparentThat website doesn't load for me but anyone who uses ChatGPT semi regularly can see that it's getting steadily worse if you ever ask for anything that begins to border risque. It has even refused to provide me with things like bolt torque specs because of risk. reply Taylor_OD 14 hours agorootparentWorse is really subjective. More limited functionality with a specific set of topics? Sure. More difficult to trick to get around said topic bans? Sure. Worse overall? You can use chatgpt 4 and 3.5 side by side and see an obvious difference. Your specific example seems fairly reasonable. Is there liability in saying x bolt can handle y torque if that ended up not being true? I don't know. What is that bolt causes an accident and someone dies? I'm sure a lawyer could argue that case if ChatGPT gave a bad answer. reply GaggiX 14 hours agorootparentprevIt could be a bias, that's why we do blinded comparisons for a more accurate rating. If we have to consider my opinion, since I use it often, then no, it hasn't gotten worse over time. reply mvdtnz 14 hours agorootparentWell I can't load that website so I can't assess their methodology. But I am telling you it is objectively worse for me now. Many others report the same. Edit - the website finally loaded for me and while their methodology is listed, the actual prompts they use are not. The only example prompt is \"correct grammar: I are happy\". Which doesn't do anything at all to assess what we're talking about, which is ChatGPT's inability to deal with subjects which are \"risky\" (where \"risky\" is defined as \"Americans think it's icky to talk about\"). reply GaggiX 14 hours agorootparentThere is no selected prompt, humans ask the models (blindly) some questions in a chat and then select the best one for them. reply thallium205 18 hours agorootparentprevI wouldn't blindly trust what a lawyer says either so there's no difference there. reply brk 18 hours agorootparentSure, but you have a lot less personal risk following advice from a lawyer vs. advice from an LLM. reply toomuchtodo 18 hours agorootparentWhen your GPT is wrong, you will be laughed out of the room and sanctioned. When your attorney is wrong, you get to point at the attorney and show a good faith effort was made. Hacks are fun, just keep in mind the domain you're operating in. reply giantg2 18 hours agorootparent\"When your attorney is wrong, you get to point at the attorney and show a good faith effort was made.\" And possibly sue their insurance to correct their mistakes. reply Scoundreller 18 hours agorootparentBut you’ll have to find a lawyer that specializes in suing lawyers and their own malpractice plans. Maybe that’s where legal AI will find the most demand. reply kulikalov 15 hours agorootparentprevCan't a tech firm running a \"legal gpt\" have an insurance? reply giantg2 15 hours agorootparentNo. Malpractice insurance would be at the professional level. There could be lawyers using a legal chatGTP, but the professional liabilities would still be with the licensed professional. reply kulikalov 15 hours agorootparentWell, I guess since it's not \"practice\" we gonna call it \"mal-inference insurance\". reply toomuchtodo 15 hours agorootparentprevDo they have a license to practice law? reply freejazz 14 hours agorootparentprevMore legal malpractice? No, because they aren't attorneys and you cannot rely upon them for legal advice such that they'd be liable to you for providing subpar legal advice. reply kulikalov 13 hours agorootparentWhy? Because there's no word for \"insurance of AI advise accuracy\"? The whole point of progress is that we create something that is not a thing at the moment. reply freejazz 12 hours agorootparentNo, because, like I said, GPTs are not legally allowed to represent individuals, so they cannot obtain malpractice insurance. You can make up an entirely ancillary kind of insurance. It does not change the fact that GPTs are not legally allowed to represent clients, so they cannot be liable to clients for legal advice. Seeing as how you think GPTs are so useful here... why are you asking me these questions when a GPT should be perfectly capable of providing you with the policy considerations that underline attorney licensing procedures. reply giantg2 13 hours agorootparentprevThat was the point of my comment - no ability to collect the insurance. reply InsomniacL 18 hours agorootparentprevWhat about if your lawyer is using chatgpt? :D https://www.forbes.com/sites/mollybohannon/2023/06/08/lawyer... reply mannykannot 18 hours agorootparentprevI like the term \"explorable vocabulary.\" I can see using LLMs to get an idea of what the relevant issues are before I approach a professional, without assuming that any particular claim in the model's responses is correct. reply coffeebeqn 12 hours agorootparentprevI wonder could GPTs come up with legal loopholes. Like they are expected to come up with security vulnerabilities reply nprateem 18 hours agorootparentprevThe only problems are it could be convincingly wrong about anything it tells you and isn't liable for its mistakes. reply engineer_22 18 hours agorootparentThis is an area for further development and thought... If a LLM can pass the bar, and has a corpus of legal work instantly accessible, what prevents the deployment of the LLM (or other AI structure) to provide legitimate legal services? If the AI is providing legal services, how do we assign responsibility for the work (to the AI, or to its owner)? How to insure the work for Errors and Omissions? More practically, if willing to take on responsibility for yourself, is the use of AI going to save you money? reply p-e-w 6 hours agorootparent> If a LLM can pass the bar, and has a corpus of legal work instantly accessible, what prevents the deployment of the LLM (or other AI structure) to provide legitimate legal services? The law, which you can bet will be used with full force to prevent such systems from upsetting the (obscenely profitable) status quo. reply AnimalMuppet 18 hours agorootparentprevA human that screws up either too often or too spectacularly can be disbarred, even if they passed the bar. They can also be sued. If a GPT screws up, it could in theory be disbarred. But you can't sue it for damages, and you can't tell whether the same model under a different name is the next legal GPT you consult. reply engineer_22 17 hours agorootparentAgreed - which is why this is an area that needs more thought and development reply nprateem 16 hours agorootparentprevRe your first point: it's not conscious. It has no understanding. It's perfectly possible the model could successfully answer an exam question but fail to reach the same or similar conclusion when it has to reason it's own way there based on information provided. reply mvdtnz 15 hours agorootparentCareful, there are plenty of True Believers on this website who really think that these \"guess the next word\" machines really do have consciousness and understanding. reply lewhoo 15 hours agorootparentI incline towards you on the subject but if you call it guessing you open yourself up to all sorts of rebuttals. reply boplicity 18 hours agorootparentprevThe obvious intermediate step is that you add an actual expert into the workflow, in terms of using LLMs for this purpose. Basically, add a \"validate\" step. So, you'd first chat with the LLM, create conclusions, then vet those conclusions with an expert specially trained to be skeptical of LLM generated content. I would be shocked if there aren't law agencies that aren't already doing something exactly like this. reply freejazz 9 hours agorootparentAh, so have the lawyer do everything the GPT did so the lawyer can be sure the GPT didn't fuck up. reply sonofaragorn 18 hours agorootparentprevWhat if they were liable? Say the company that offers the LLM lawyer is liable. Would that make this feasible? In terms of being convincingly wrong, it's not like lawyers never make mistakes... reply naniwaduni 18 hours agorootparentYou'd require them to carry liability insurance (this is usually true for meat lawyers as well), which basically punts the problem up to \"how good do they have to be to convince an insurer to offer them an appropriate amount of insurance at a price that leaves the service economically viable?\" reply kulikalov 15 hours agorootparentGiven orders of magnitude better cost efficiency, they will have plenty of funds to lure in any insurance firm in existence. And then replace insurance firms too. reply giantg2 18 hours agorootparentprev\"What if they were liable?\" They'd be sued out of existence. \"In terms of being convincingly wrong, it's not like lawyers never make mistakes...\" They have malpractice insurance, they can potentially defend their position if later sued, and most importantly they have the benefit of appeal to authority image/perception. reply AnimalMuppet 18 hours agorootparentAll right, what if legal GPTs had to carry malpractice insurance? Either they give good advice, or the insurance rates will drive them out of business. I guess you'd have to have some way of knowing that the \"malpractice insurance ID\" that the GPT gave you at the start of the session was in fact valid, and with an insurance company that had the resources to actually cover if needed... reply kulikalov 15 hours agorootparentIt's funny how any conversation ends with this question unanswered. reply YetAnotherNick 15 hours agorootparentWeirdly HN is full of anti AI people who just refuses to discuss the point that is being discussed and goes into all the same argument of wrong answer that they got some time. And then they present anecdotal evidence as truth, while there is no clear evidence if AI lawyer has more or less chance to be wrong than human. Surely AI could remember more and has been shown to clear bar examination. reply giantg2 13 hours agorootparent\"while there is no clear evidence if AI lawyer has more or less chance to be wrong than human.\" In the tests they are shown to be pretty close. The point I made wasn't about more mistakes, but about other factors influencing liability and how it would be worse for AI than humans at this point. reply YetAnotherNick 1 hour agorootparent> at this point. This is the key point. Even if assume the AI won't get better, the liability and insurance premiums will likely become similar in very near future. There is a clear business opportunity that's there in insuring AI lawyer. reply baobabKoodaa 17 hours agoparentprevWe are literally building this today! Our core business is legal document generation (rule based logic, no AI). Since we already have the users' legal documents available to us as a result of our core business, we are perfectly positioned to build supplementary AI chat features related to legal documents. We recently deployed a product recommendation AI to prod (partially rule based, but personalized recommendation texts generated by GPT-4). We are currently building AI chat features to help users understand different legal documents and our services. We're intending to replace the first level of customer support with this AI chat (and before you get upset, know that the first level of customer support is currently a very bad rule-based AI). Main website in Finnish: https://aatos.app (also some services for SE and DK, plus we recently opened UK with just a e-sign service) reply nicce 16 hours agorootparentSo, let’s say that the chat will work as well as the real lawyer some day. If the current pricing would be $500 an hour for a real lawyer, and at some point your costs are just keeping services up and running, how big cut will you take? Because it is enough if you are only a little cheaper than the real lawyer to win customers. There is an upcoming monopoly problem, if the users get the best information from the service after they submit all their documents. And soon the normal lawyer might be competitive enough. I fear that the future is in the parent commenter’s open platfrom with open models and the businesses should extract money from some other use cases, while for a while, you get money momentarily based on the typical ”I am first, I have the user base” situation. It is interesting to see what will happen to lawyers. reply baobabKoodaa 15 hours agorootparent> If the current pricing would be $500 an hour for a real lawyer, and at some point your costs are just keeping services up and running, how big cut will you take? Zero. We're providing the AI chat for free (or free for customers who purchase something from us, or some mix of those 2 choices). Our core business is generating documents for people, and the AI chat is supplementary to the core business. It sounds like you're approaching the topic with the mindset that lawyers might be entirely replaced by automation. That's not what we're trying to do. We can roughly divide legal work into 3 categories: 1. Difficult legal work which requires a human lawyer to spend time on a case by case basis (at least for now). 2. Cookie cutter legal work that is often done by a human in practice, but can be automated by products like ours. 3. Low value legal issues that people have and would like to resolve, but are not worth paying a lawyer for. We're trying to supply markets 2 and 3. We're not trying to supply market 1. For example, you might want a lawyer to explain to you what is the difference between a joint will and an individual will in a particular circumstance. But it might not be worth it to pay a lawyer to talk it through. This is exactly the type of scenario where an AI chat can resolve your legal question which might otherwise go unanswered. reply nicce 13 hours agorootparent> It sounds like you're approaching the topic with the mindset that lawyers might be entirely replaced by automation. That is the cynical future, however, and based on the evolution speed of the last year, it is not too far away. We humans are just interfaces for information and logic. If the chatbot has the same capabilities (both information and logic, and natural language), then they will provide full automation. The natural language aspect of AI is the revolutionary point, less about the actual information they provide. Quoting Bill Gates here, like the GUI was revolutionary. When everyone can interact and use something, it will remove all the experts that you needed before as middle man. reply baobabKoodaa 16 hours agorootparentprevHere's an example of what our product recommendations look like: Given your ownership in a company and real estate, a lasting power of attorney is a prudent step. This allows you to appoint PARTNER_NAME or another trusted individual to manage your business and property affairs in the event of incapacitation. Additionally, it can also provide tax benefits by allowing tax-free gifts to your children, helping to avoid unnecessary inheritance taxes and secure the financial future of your large family. reply Closi 16 hours agorootparentprev> Since we already have the users' legal documents available to us as a result of our core business, we are perfectly positioned to build supplementary AI chat features related to legal documents. Uhh... What are the privacy implications here?! reply baobabKoodaa 15 hours agorootparentIf you look at the example I posted of our product recommendations, you will see that the GPT-4 generated text contains \"PARTNER_NAME\" instead of actual partner name. That's because we've created anonymized dataset from users in such a way that it's literally impossible for OpenAI to deanonymize users. Of course the same can not be done if we want to provide a service where users can, for example, chat with their legal documents. In that case we will have to send some private details to OpenAI. Not sure how that will pan out (what details we decide to send and what we decide not to send). In any case, all startups today are created on top of a mountain of cloud services. Any one of those services can leak private user data as a result of outsider hack or insider attack or accident. OpenAI is just one more cloud service on top of the mountain. reply XCSme 17 hours agoparentprevOr a LLM that helps you spend less. Imagine a LLM that goes over all your spending, knows all the current laws, benefits, organizations, promotional campaigns, and suggests (or even executes) things like changing electricity provider, insurance provider, buying stuff in bulk from a different shop that you get for 4x the price at your local store, etc. reply OldOneEye 17 hours agorootparentI love this idea. It would be incredibly useful! I feel LLMs are great at suggestions that you follow up yourself (if only for sanity checking, but nothing you wouldn't do with a human too). reply p-e-w 6 hours agorootparentprevThat would not be in the interest of anyone with any real power, so you're going to see tanks on the streets before it happens. reply nostromo 15 hours agoparentprevAnd not just legal either. I uploaded all of my bloodwork tests and my 23andme data to Chat GPT and it was better at analyzing it than my doctor was. reply p-e-w 6 hours agorootparentYup. Because the doctor doesn't have time and doesn't give a fuck. LLMs don't have to compete against the cutting edge of human professional knowledge. They only have to compete against the disinterested, arrogant, greedy, and overworked professionals that are actually available to people in practice. No wonder they're winning. reply slingnow 15 hours agorootparentprevThis is a really interesting use case for me. I've been envisioning a specially trained LLM that can give useful advice or insights that your average PCP might gloss over or not have the time to investigate. Did you do anything special to achieve this? What were the results like? reply frankfrank13 17 hours agoparentprevI think a lot of startups are working on exactly what you are describing, and honestly, I wouldn't hold my breath. Everyone is still bound by token limits and the two best approaches for getting around them are RAG and Knowledge-Graphs, both of which could get you close to what you describe but not close enough to be useful (IMO). reply Solvency 17 hours agoparentprevThis does not make sense to me. ChatGPT is completely nerfed to the point where it's either been conditioned or trained to provide absolutely zero concrete responses to anything. All it does is provide the most baseline, generic possible response followed by some throwaway recommendation to seek the advice of an actual expert. reply frankfrank13 17 hours agorootparentThe way to get around this is to have it \"quote\" or at least try to quote from input documents. Which is why RAG became so popular. Sure, it won't right you a contract, but it will read one back to you if you've provided one in your prompt. In my experience, this does not get you close to what the top-level comment is describing. But it gets around the \"nerfing\" you describe reply baobabKoodaa 16 hours agorootparentprevIt's very easy to get ChatGPT to provide legal advice based on information fed in the prompt. OpenAI is not censoring legal advice anywhere near as hard as they are censoring politics or naughty talk. reply freejazz 9 hours agorootparentThat's because its just advice, not legal advice. Legal advice is something you get from an attorney that represents you. It creates a liability relationship between you and the attorney for the legal advice they did provide. reply baobabKoodaa 9 hours agorootparentSure, we can call it \"advice\" instead of \"legal advice\" or we can even call it other names, like \"potato\", if that's what you want. My point is that potato not censored. reply freejazz 9 hours agorootparentFeel free to miss the point as much as you want. You can call it a baked potato then! reply throwaway17_17 18 hours agoprevI will reserve judgment of the possibilities of LLMs as applied to the legal field until they are tested on something other than Document/ contract review. Contract review is, in the large business law case, often done by outsourcing to hundreds of recent graduates and act more like proof reading with minimal application of actual lawyering skills to increase a corporation’s bottom line. The more common, for individual purchasers of legal services, lawyering is going to be family law matters, criminal law matters, and small claims court matters. I can not see a time in the near future where an LLM can handle the fact specific and circumstantial analysis required to handle felony criminal litigation, and I see nothing that would imply LLMs can even approach the individualized, case specific and convoluted family dynamics required for custody cases or contested divorces. I’m not unwilling to accept LLMs as a tool an attorney can use, but outside of more rote legal proof reading I don’t think the technology is at all ready for adoption in actual practice. reply giantg2 18 hours agoparent\"and I see nothing that would imply LLMs can even approach the individualized, case specific and convoluted family dynamics required for custody cases or contested divorces.\" Humans are pretty bad at this. Based on the results, it seems the judges' personal views and emotions are a large part of these cases. I'm not sure what they would look like without emotion, personal views, and the case law built off of those. reply staunton 18 hours agorootparent> judges' personal views and emotions are a large part of these cases That's a completely separate question. We're talking about automating lawyers, not judges. (to be a good lawyer in such a situation, you would need to model the judge's emotions and use them to your advantage. Probably AIs can do this eventually but it's not easy or likely to happen soon) reply giantg2 17 hours agorootparentWell, judges are a subset of lawyers. And interactions with judges are a large part of being a successful lawyer, as you point out. reply SkyBelow 18 hours agorootparentprevThe worse judges are at being perfectly removed arbiters of justice, the more room for lawyers to exploit things like emotions and humans connections with those judges, and thus the worse LLMs will be at doing that part of the job. A charismatic lawyer backed by an LLM will be much better than an LLM. At least until the LLMs surpass humans at being charismatic, but that would seem to be its own nightmare scenario. reply staunton 18 hours agorootparent> At least until the LLMs surpass humans at being charismatic Look into \"virtual influencers\". Sounds like you should find it interesting. reply pwmiller_ai 10 hours agoprevI lead data and AI at a tech-enabled commercial insurance brokerage. We have been leveraging GPT-4 to build a deep contract analysis tool specifically for insurance requirements. My teams at Google also built several LLM solutions to support Google's legal team, from patent classification to discovery support. Language models are great at digesting legalese and analyzing what's going on. However, many legal applications involve around pretty important decisions that you don't want to get wrong (\"am I contractually covered with my current insurance program?\"). Because of that, we've built LLM products in the legal space with the following principles in mind: - Human-in-the-loop tooling -- The product should be built around an expert using it whenever possible, so decision support as opposed to automation. You still see massive time savings with that in place - Transparency / citations -- With a human-in-the-loop tool, you need mechanisms to build trust. Whether that's highlighting clauses in the document that the LLM referred to or explaining why a part of the analysis wasn't provided, citing your work is important - Tuned for precision instead of recall -- False positives (and hallucinations) are especially bad in many of these legal use cases, so tuning the model or prompts for precision helps with mitigation. reply theptip 6 hours agoparentDo you have any pointers on how to get GPT-4 to do citations? Is a prompt like “quote back the passage you are citing” so you can locate and highlight the original? When you say “tuned for precision” is this your prompt engineering or are you actually fine-tuning GPT-4? Appreciate the insights. reply pwmiller_ai 6 hours agorootparentFor RAG applications, starting simply with a reference to the most relevant chunk(s) is helpful in building transparency. A lot of our contract review task is a data extraction one (e.g. extract this type of insurance language and compare against your policy). As such, it's much easier to pinpoint the exact text from the source doc for citation. In our applications, currently, we are doing citations as a postprocessing task as opposed to as part of the prompt itself. Finding that feeding too many instructions to the LLM results in worse responses. We're not fine-tuning today. Instead, \"tuning for precision\" is done through prompt chains. A simple example would be returning \"I don't know\" early on if the document isn't a contract or doesn't have clear insurance requirements in it. We've had success with various guardrail prompts. The citation work we did at Google used model internals to highlight text (path integrated gradients). It's also easier to finetune for precision when you have control over the model itself. reply District5524 16 hours agoprevWhile this paper is clearly not without merits, it intends to be more like an excuse to make a bombastic statements about a whole profession or \"industry\" (perhaps to raise their visibility and try to sell something later on?). The worst part is that they have actually referenced a single preprint document as \"previous art\" - and that document itself is not related to contract review, but to legal reasoning in general. (A part of LegalBench is of course \"interpretation\", and that is built on existing contract review benchmarks, but they could've found more relevant papers). Automating legal document review has been a very active field in NLP for twenty years or so (including in QA tasks) and became a lot more active since 2017. At least e.g. Kira (and Luminance etc., none of which is LLM-based) are already used quite widely in legal departments/firms around the world. So lawyers do have practical experience in their limitations... But Kira & co. are not measuring the performance of the latest and greatest models and they do not use transparent benchmarks etc. So the benchmark results in this paper are indeed a welcome addition in terms of using LLMs. But also considering its limited scope of reviewing 10 (!) documents based on a single review playbook, they should not have written about \"implications for the industry\". It is very much pretentious and shows more of the lack of knowledge of the authors of the very same industry than about the future of the legal services industry. If you're interested in the capabilities and limitations, I suggest these informative, but still light reads as well: https://kirasystems.com/science/ https://zuva.ai/blog/ https://www.atticusprojectai.org/cuad reply d0odk 9 hours agoparentAny particular papers you would recommend? The links are to blogs with lots of papers. reply Workaccount2 18 hours agoprevI wonder what the reach of a legal argument a bunch of lawyers are going to come up with in order to cripple the tech that threatens their industry? reply guluarte 17 hours agoparentI think the other way is going to happen, being a lawyer will now be a lot more expensive requiring some servers doing AI inference, developers, and 3 party services.. reply bongodongobob 14 hours agorootparentI wouldn't be so sure. I've worked in the MSP space and law is the most tech averse industry I've ever come into contact with. reply ProllyInfamous 8 hours agorootparentI have two attorney brothers. January 2023 my self-proclaimed smartypants lawyerbro tried to bully me into accepting that ChatGPT wasn't anything special. I still maintain that he is incorrect. reply phrz 11 hours agoparentprevNot a reach, it's called \"unlicensed practice of law\" and it's a crime. reply zugi 18 hours agoparentprevLawyers control government, at least in the U.S. Expect laws banning or severely restricting the use of AI in the legal field soon. I expect arguments will range from the dangers of ineffective counsel to \"but think of the children\" - whatever helps them protect their monopoly. reply lewhoo 17 hours agorootparent> whatever helps them protect their monopoly Ah yes, the story of bad people not wanting their livelihoods taken from them by good tech giants. Seriously, is there no room for empathy in all of this ? If you went through law school and likely got yourself in debt in the process then you're not protecting any monopoly but your means to exist. There are people like that out there you know. reply jjackson5324 13 hours agorootparent> Seriously, is there no room for empathy in all of this ? Are you joking? Do you not empathize with the far, far larger number of people who can't afford adequate legal representation and have no legal recourse? There are people like that out there you know!!!! reply lewhoo 1 hour agorootparentI gave a specific example with whom I empathize and no, I'm not joking. You on the other hand point to a different group and say \"look, look, there is this group, don't you like them as well ?\" which is close to \"but what of the children in Africa\" argument/deflection. reply kevingadd 6 hours agorootparentprevThere seems to be an assumption baked in here that somehow GPT will be \"adequate legal representation\" and I'm not sure how to get there. An \"adequate\" source of revenue for OpenAI, I guess. reply pb7 15 hours agorootparentprevIn general, we should not stall technological progress just to protect jobs. They will find other jobs. This is the way throughout human history. reply lewhoo 15 hours agorootparentI'm not advocating anything of this sort. I only reject the typical framing of \"bad guys\" on one side. reply axpy906 18 hours agorootparentprevThat’s some cartel level action. reply DenisM 17 hours agorootparentI think it’s more accurate to think of lawyers as a guild. Likewise doctors, accountants, plumbers, and electricians. reply robertlagrant 17 hours agorootparentA guild that has the inside track on changing the rules for itself. reply delichon 18 hours agoparentprevCopyright appears to be the primary attack vector. reply anotherhue 18 hours agorootparentAre the arguments submitted to a court (and made publicly accessible) subject to copyright? I kind of assumed they were in the same space as government documents. reply delichon 18 hours agorootparentA legal LLM would be significantly crippled without the knowledge stored in the universe of non-legal documents. reply anotherhue 18 hours agorootparentYou're probably right, but the law and reality often seem to be orthogonal. reply underlines 56 minutes agoprevConflict of interest in the paper, as this mainly is a PR piece from Onit: \"Onit Announces Generative AI-Powered Virtual Legal Operations Assistant for In-house Counsel\" reply zehaeva 17 hours agoprevGiven the recent legal cases where lawyers did use Chat GPT to do research and help write their brief did not go very well I'm not sold that on all the optimism that's here in the comments. reply minimaxir 17 hours agoparentThe technology is fine, the education and literacy about its flaws and limitations among typical nontechnical users is another story. reply bpodgursky 15 hours agoparentprevThey were all idiots too cheap to pay for GPT-4. Got caught by hallucinations. reply frankfrank13 17 hours agoparentprevThat was rookie level mistakes though. Not checking a *case* exists? Building a small pipeline of generation->validation isn't trivial, but it isn't impossible. The cases you describe seem to me like very lazy associates matched with a very poor understanding of what LLMs do. reply oldgregg 17 hours agoprevInteresting problem space-- I have a culture jamming legal theory this might work for: What if you had a $99/mo pro-se legal service that does two things, 1) teaches you how to move all of your assets into secure vehicles. 2) At the same time it lets you conduct your own defense pro-se-- but the point is not to win, it's just to jam the system. If you signal to the opposing party that you're legally bankrupt and then you just file motion after motion and make it as excruciating as possible for them they might just say nevermind when they realize it's gonna take them 5 years to get through appeals process. It's true lawyers don't want to give up their legal documents for a template service-- but honestly just going to the court house and ingesting tons of filings might do the trick. With that strategy in mind you don't really need GREAT documents or legal theory anyway. Just docs that comply with court filing requirements. Yeah we're def gonna need to deposition your housekeepers daughters at $400/h and if you have a problem with that I would be happy to have a hearing about it. If enough people did this is would basically bring the legal system to a standstill and give power back to the people. RIP Aaron Swartz who died fighting for these issues :'( reply trevithick 15 hours agoparentYou would be a vexatious litigant. https://www.courts.ca.gov/12272.htm reply oldgregg 15 hours agorootparentThat's different. I'm talking about using it as a defensive mechanism against wealthy individuals and corporations who bully (relatively) poor people knowing they can't afford years of litigation. In theory if you had an AI system like what I'm talking about it could be used the other way though. Honestly if every individual had the ability to go after corporations in the same manner it would even the playing field. Still wouldn't necessarily be vexatious. reply gee_totes 17 hours agoparentprevWhat you're describing in 2) sounds a lot like paper terrorism[0] [0]https://en.wikipedia.org/wiki/Paper_terrorism reply oldgregg 15 hours agorootparentSo what do you call it when the wealthy and corporations exploit their opponent's inability to afford legal representation? A normal Tuesday in Amerika. Yes, the banality of tuesday terrorism. reply akira2501 9 hours agorootparent> So what do you call it when the wealthy and corporations exploit their opponent's inability to afford legal representation? It's called a Strategic Lawsuit Against Public Participation. It's also illegal in most states. > A normal Tuesday in Amerika. We already understood your derogatory outlook without it needing to be literal. Anyways, that's why the founders gave us the First, Fourth, and Fifth amendments. It turns out, they recognized, the \"law\" is literally the worst mechanism for discovering truth and managing outcomes. > Yes, the banality of tuesday terrorism. It's terrorism because it has no logical conclusion nor any ability to positively benefit anyone's life, in particular, the person who would wield it. Your equivocation ignores this. reply adrianmonk 15 hours agoprevI wonder if this could help regular (non-lawyer) people understand legal documents they run into in everyday life. Things like software license agreements, terms of service, privacy policies, release of liability forms, deposit agreements, apartment leases, and rental car agreements. Many people don't even try to read these because they're too long and you wouldn't necessarily understand what it means even if you did read it. What if, before you signed something, you could have an LLM review it, summarize the key points, and flag anything unusual about it compared to similar kinds of documents? That seems better than not reading it at all. reply wow_its_tru 15 hours agoparentWe're building exactly this today, for common business contracts. We're not building for consumers today, because I think it's vanishingly unlikely that you'll, like, pick a different car rental company once you read their contract :) but leases, employment contracts, options agreements, SaaS agreements... all common, all boilerplate with 5-10 areas to focus on, all ready for LLMs! reply DanielSantos 12 hours agorootparentWe have also have been building this[1] but struggled to monetize even with 100s of users and 1000s of contracts review. We are live for about 1 year. If you want to share experience feel free to reach out [1] legalreview.ai reply mattmaroon 14 hours agorootparentprevHonestly, that type of consumer use case might actually be relevant once LLM’s can do this sort of thing. Certainly, nobody is going to contact their attorney before renting a car, but if this could be integrated into a travel site or something… You never know how consumer behavior may change when something that was either impossible or impractical becomes very easy b reply delichon 18 hours agoprevIn all criminal prosecutions, the accused shall enjoy the right ... to have the Assistance of Counsel for his defense -- 6th amendment to US Constitution When an LLM is more competent than an average human counsel, does this amendment still require assistance of a human counsel? reply District5524 17 hours agoparentAll the governments in the world would do everything in their power to get people accept this suggestion as a truth and use LLMs instead of human lawyers especially in criminal defense. Now, why is that? Maybe it's not the technical knowledge that is the most important feature of a lawyer? reply sudden_dystopia 17 hours agorootparentYea it’s their ability to manipulate language and people to bend the letter of the law to suit their specific cases regardless of any long term potential societal harm. reply AustinDev 15 hours agorootparentprevAll governments of the world at least in the west appear to mostly consist of attorney's I doubt they'd let it happen. It'd be bad for their guild. reply ilc 13 hours agoparentprevThey serve different uses. A lawyer can handle the trial for you and things like that. The LLM can help you with issues of fact, etc. And could even make stronger privacy guarantees than a lawyer if setup right. (But I doubt that will ever happen.) reply urbandw311er 18 hours agoparentprevIronically you might be better asking GPT this question. reply guluarte 17 hours agoparentprevyes because LLM are not free and still requires an expert to verify the output. reply Bud 17 hours agoparentprevThat's for a court to decide. It's certainly reasonable to guess that that court case is coming. The only question is how soon. reply notfed 4 hours agoprev> Cost wise, LLMs operate at a fraction of the price, offering a staggering 99.97 percent reduction in cost over traditional methods. This seems like a weirdly arbitrary and forced statistic, when \"100% reduction\" would be just as valid of a statement. reply TeMPOraL 4 hours agoparent100% reduction would mean \"literally free\", which it isn't. reply notfed 2 hours agorootparentBut LLMs are literally free. This paper just chose to focus on the \"prominent\" ones. reply vitiral 18 hours agoprevIt feels to me like the law is already a staggering heap of complexity. Isn't using technology going to just enable more of the same, making the situation worse? reply engineer_22 18 hours agoparenton the contrary, it may help to highlight incongruities in the legal domain and provide lawyers with compelling groundwork to make relevant claims reply urbandw311er 18 hours agoparentprevOr you could take the view that, in fact, this is one of the things LLMs are very good at, ie making sense of complexity. reply vitiral 16 hours agorootparentBut the lawyers reading the law won't be the only ones using LLMs. LLMs will also be used to write laws. Then lawmakers will use them to \"check\" that their 20,000 page law supposedly works. No human can understand the scope of today's laws: how much less when no LLMs can understand the laws created 20 years from now. I'd love to hear that LLMs can be used to trim and simplify complexity, but I don't believe it. They generate content, not reduce it. reply visarga 13 hours agoprevTwo reasons why it's a bad idea: 1. ChatGPT can't be held responsible, it has no body, like summoning the genie from the lamp, and about as sneaky with its hard to detect errors 2. ChatGPT is not autonomous, not even a little, these models can't recover from error on their own. And their input buffers are limited in size, and don't work all that well when stretched at maximum Especially the autonomy part is hard. Very hard in general. LLMs need to become agents to collect experience and learn, just training on human text is not good enough, it's our experience not theirs. They need to make their own mistakes to learn how to correct themselves. reply itissid 12 hours agoprevGPTs only generate specific answers if they are trained using RHLF to prefer certain answers to others. Won't this mean that coming up with a contract that meets a special individual's case will require that much more fine-tuning? Also how do you reconcile several logical arguments without a solver? Like \"If all tweedles must tweed\", \"If X is a tweedle, therefore it must tweed unless it can meet conditions in para 12\". How can it possibly learn to solve many such conjunctions that are staple in legal language? reply gumby 15 hours agoprevI would not want an transformer-generated contract but I would be delighted if a transformer-generated contract were used as input by an actual lawyer and it saved me money. In my experience current practice (unchanged for the decades I've been using lawyers) is that associates start with an existing contract that's pretty similar to what's needed and just update it as necessary. Also in my experience, a contract of any length ends up with overlooked bugs (changed sections II(a) and IV(c) for the new terms but forgot to update IX(h)) and I doubt this would be any better with a machine-generated first draft. reply advael 16 hours agoprevIt's bizarre how easily we got to the Goodhart's Law event horizon in our comparisons of complex fields to AI models But this is what happens when industries spend a decade brain-draining academia with offers researchers would be insane to refuse reply cwoolfe 17 hours agoprevWhere can someone upload a contract and ask the AI questions about it in a secure and private way? It's my understanding that most people and organizations aren't able to do this because it isn't private. reply DanielSantos 12 hours agoparentYou can try us [1] . During the upload process you can enable data anonymization. It’s not perfect though. We use open ai but they only get segments of a contract. Not the full one and can’t connect them. You get the review via email and after you can delete the document and keep the review. [1] legalreview.ai reply kveton 15 hours agoparentprevWe do this today (securely upload a file and ask questions or summarize) and part of our promise, and why we're having early success, is because we promise not to train with customer data and we don't run directly on top of OpenAI. https://casemark.ai reply kulikalov 15 hours agoparentprevChatGPT enterprise? Or over API. They state that those offerings data is not used for training. I'm not a lawyer but afaik illegally retrieved evidence cannot be used - \"exclusionary rule\". reply btbuildem 15 hours agoparentprevYou can host your own LLM - something like Mixtral for example - then you have full control over the information you submit to it. reply graphe 15 hours agoprevLaw is very specific. BERT was sufficient for law even before chatGPT. https://towardsdatascience.com/lawbert-towards-a-legal-domai... https://huggingface.co/nlpaueb/legal-bert-base-uncased reply adi4213 14 hours agoprevFor the auditory learners, here is this paper in a summarized Audiobook format : https://player.oration.app/1960399e-ccb0-44f6-81f0-870ef7600... reply gogogo_allday 15 hours agoprevI must not be reading this paper correctly because it appears that they only used 10 procurement contracts to do this study. If so, the abstract and title feels misleading. I’d be more interested in a study done on thousands of contracts of different types. I also have my doubts it would perform well on novel clauses or contracts. reply williamcotton 18 hours agoprevLexis has some AI feature built into it: https://www.lexisnexis.com/en-us/products/lexis-plus-ai.page I haven't had a chance to test it out as anyone should be a bit weary to add more paid features to an already insanely expensive software product! reply unyttigfjelltol 12 hours agoprevThis is apples and bowling balls. You also probably could replace the CEO and entire executive team with LLMs. And cheaper! Much cheaper! But, if the stochastic analysis was ... wrong ... who would be left to correct it? reply ok123456 15 hours agoprevDoes LPO (Legal Process Outsourcing) mean a paralegal? reply light_hue_1 17 hours agoprevTalk about a conflict of interest. A company that pushes llms for legal work says they work better. This isn't worth the pdf it wasn't printed on. reply carstenhag 16 hours agoparentI disagree. The company is mentioned multiple times, a conflict of interest is clearly visible. We also don't complain about Google et al publishing papers about some of their internal systems and why it helps, I hope? reply drewdevault 16 hours agorootparentGoogle isn't trying to sell you their internal systems. This is a bullshit AI hype bubble advert masquerading as an academic paper. Bet you 10 bucks it doesn't get through peer review (or isn't even submitted for peer review). reply og_kalu 15 hours agorootparent>Google isn't trying to sell you their internal systems. They sometimes are. If you have an issue with the methodology of paper then all well and good but \"conflict of interest\" is pretty weak. Yes, Google and Microsoft et al regularly publish papers describing Sota performance they sometimes use internally and even sell. I didn't have to think much before wavenet came to mind. Besides, the best performing models here are all Open AI. reply vibeproaaaac21 15 hours agoparentprevAgreed. That's most AI research though. They are a mechanism whose entire value proposition lies in laundering externalities. Not that this isn't exactly what all the big \"tech innovation\" of the last decade were either. It's depressing and everyone involved should be ashamed of themselves. reply very_good_man 16 hours agoprevWe may finally again get affordable access to the rule of law in the United States. reply ProllyInfamous 8 hours agoprevJust as reminder, Chief US Justice released his \"year in review\" on acceptance of LLM / GPT systems within legal practices, and that it will likely enable access to the law by more-and-more commoners. The entire six pages is worth the time reading [1]. [1] [PDF] https://www.supremecourt.gov/publicinfo/year-end/2023year-en... reply sandbx 12 hours agoprevI like that they incl the prompt in the paper reply FrustratedMonky 17 hours agoprevTheoretically. The language in laws should be structured similar to code. It has some logical structure. Thus should be more easily adopted to LLMs than other 'natural language'. So despite the early news about lawyers submitting 'fake' cases, it is only a matter of time before the legal profession is up-ended. There are a ton of paralegals, etc.. doing 'grunt' work in firms, that an LLM can do. These are considered white color, and will be gone. It will progress in a similar fashion to coding. It will be like having a junior partner that you have to double check, or that can do some boiler plate for you. You can't trust completely, but you don't trust your junior devs do you, but it gets you 80% there. reply NoboruWataya 17 hours agoparent> It will progress in a similar fashion to coding. I kind of agree with this, but this is why I am confused that I only ever see people (at least on HN) talk about AI up-ending the legal profession and putting droves of lawyers out of work--I never see the same talk about the coding industry being transformed in this way. reply FrustratedMonky 13 hours agorootparentI've heard it discussed. A lot more a few months ago when GPT first blew up. Maybe HN is full of coders that still think themselves 'special' and can't be replaced. Or maybe, the law profession has a lot more boilerplate than the coding profession? So legal profession has more that can be replaced? Coders will be replaced, but maybe not at same rate as paralegals. reply ulrischa 17 hours agoprevLawyers have been resisting technological advances for years. No industry rejects technological tools as vehemently as the legal industry, arguing that everything has to be judged by people. Even laws that are available online do not even link to the paragraphs that are referenced. All in all, progress is institutionally suppressed here in order to preserve jobs. reply NoboruWataya 17 hours agoparent> Even laws that are available online do not even link to the paragraphs that are referenced. It's not lawyers' job to publish the laws online. Lawyers are the ones who would benefit from more easily searchable online laws, as they are the ones whose job is actually to read the laws. That is why there are various commercial tools that provide this functionality, that lawyers pay for. You need to ask your government why public online legal databases are so poor, not your lawyer. reply ulrischa 15 hours agorootparentRight. 70% of governmental staff are people with a law education. So I mean lawyer in a broader sense reply DanielSantos 12 hours agoparentprevI also built an AI contract review ai tool and talked to > 100 lawyers. What I found is that lawyers want technological advances but only if they work 100% of the time. Also helped lawyers looking for a CLM, and they rejected something",
    "originSummary": [
      "Large language models (LLMs) have been found to be as accurate as or even surpass human legal contract reviewers in determining legal issues.",
      "LLMs are significantly faster than humans, capable of completing reviews in seconds compared to hours.",
      "The use of LLMs in the legal industry has the potential to revolutionize the field, increasing accessibility and efficiency while reducing costs."
    ],
    "commentSummary": [
      "AI and language models (LLMs) are being discussed for their impact on the legal profession.",
      "There are mixed opinions on their effectiveness and limitations, with integration alongside lawyers being suggested by some while concerns about accuracy and liability issues are raised by others.",
      "Job loss in the legal industry and the need for regulations to protect it are also subjects of debate. Privacy, data misuse, and the importance of human input in legal matters are additional concerns."
    ],
    "points": 357,
    "commentCount": 243,
    "retryCount": 0,
    "time": 1707231879
  },
  {
    "id": 39283733,
    "title": "jQuery 4.0.0 Beta Release: Bug Fixes, Performance Improvements, and Breaking Changes",
    "originLink": "https://blog.jquery.com/2024/02/06/jquery-4-0-0-beta/",
    "originBody": "jQuery jQuery UI jQuery Mobile Sizzle QUnit Plugins Contribute CLA Style Guides Bug Triage Code Documentation Web Sites Events Apr 16-17jQuery Virtual Training May 16jQuery UK Jun 17-19jQuery Virtual Training Sep 12-13jQuery Chicago Oct 13-15CSS Dev Conf 2014 Support Learning Center Try jQuery IRC/Chat Forums Stack Overflow Commercial Support jQuery Foundation Join Members Team Brand Guide Donate Official jQuery Blog Download API Documentation Blog Plugins Browser Support Navigate... Download API Documentation Blog Plugins Browser Support search Search Official jQuery Blog jQuery 4.0.0 BETA! Posted on February 6, 2024 by Timmy Willison jQuery 4.0.0 has been in the works for a long time, but it is now ready for a beta release! There’s a lot to cover, and the team is excited to see it released. We’ve got bug fixes, performance improvements, and some breaking changes. We removed support for IE`s with children in IE (ccbd6b93) Fix `contents()` on ``s with children (#4384, 4d865d96) Comments are closed. Categories Events Foundation jQuery jQuery UI Projects Weekly News Recent Posts jQuery 4.0.0 BETA! jQuery 3.7.1 Released: Reliable Table Row Dimensions jQuery 3.7.0 Released: Staying in Order jQuery 3.6.4 Released: Selector Forgiveness jQuery 3.6.3 Released: A Quick Selector Fix jQuery 3.6.2 Released! jQuery 3.6.1 Maintenance Release jQuery maintainers continue modernization initiative with deprecation of jQuery Mobile jQuery maintainers update and transition jQuery UI as part of overall modernization efforts jQuery project updates addressing temporary CDN issues Archives February 2024 August 2023 May 2023 March 2023 December 2022 August 2022 October 2021 June 2021 March 2021 May 2020 April 2020 May 2019 April 2019 August 2018 January 2018 March 2017 September 2016 July 2016 June 2016 May 2016 April 2016 March 2016 February 2016 January 2016 December 2015 November 2015 September 2015 July 2015 April 2015 March 2015 February 2015 January 2015 December 2014 October 2014 September 2014 August 2014 July 2014 May 2014 April 2014 March 2014 January 2014 December 2013 November 2013 October 2013 September 2013 August 2013 July 2013 June 2013 May 2013 April 2013 March 2013 February 2013 January 2013 December 2012 November 2012 October 2012 September 2012 August 2012 July 2012 June 2012 May 2012 April 2012 March 2012 January 2012 December 2011 November 2011 October 2011 September 2011 August 2011 June 2011 May 2011 April 2011 March 2011 February 2011 January 2011 December 2010 November 2010 October 2010 September 2010 August 2010 July 2010 June 2010 April 2010 March 2010 February 2010 January 2010 December 2009 October 2009 August 2009 July 2009 June 2009 May 2009 April 2009 March 2009 February 2009 January 2009 December 2008 November 2008 October 2008 September 2008 August 2008 July 2008 June 2008 May 2008 March 2008 February 2008 January 2008 December 2007 November 2007 October 2007 September 2007 August 2007 July 2007 June 2007 May 2007 April 2007 March 2007 February 2007 January 2007 December 2006 November 2006 October 2006 September 2006 August 2006 July 2006 June 2006 May 2006 April 2006 March 2006 February 2006 January 2006 August 1997 Books Learning jQuery Fourth Edition Karl Swedberg and Jonathan Chaffer jQuery in Action Bear Bibeault, Yehuda Katz, and Aurelio De Rosa jQuery Succinctly Cody Lindley Learning Center Forum API Twitter IRC GitHub Copyright 2024 The jQuery Foundation. jQuery License Web hosting by Digital OceanCDN by StackPath",
    "commentLink": "https://news.ycombinator.com/item?id=39283733",
    "commentBody": "jQuery v4.0 Beta (jquery.com)334 points by joshmanders 7 hours agohidepastfavorite223 comments technojunkie 7 hours agoFor those of you who are curious what drives jQuery in 2024 and beyond, you need to remember that WordPress is still more than 1/3 of the web, and the majority of installations and so many plugins critically rely on jQuery. Yes, seriously. Any advances to removing deprecated APIs or functions are great. jQuery will probably be around dominantly on the web for years to come. reply ecshafer 7 hours agoparentAlso, jQuery is awesome. People have been in love with the overly complex and fancy javascript frameworks for the last 15 years or so. But jQuery doing dynamic binding to dynamically generated forms for some error states and maybe an ajax calls is literally all the javascript you need in 99% of web pages and the rest is overkill. The industry is going to move away from the complexities to React and towards more of this simplicity with htmx, phoenix live view, ruby on rails turbo, and yes just jQuery. reply tipiirai 6 hours agorootparentSecond to that: jQuery is awesome. Or more specifically: the idea that websites can be built with vanilla HTML, CSS, and _optional_ JS. jQuery embraces progressive enhancement and separation of concerns pattern, which is quite the opposite of how websites are built these days. Web development starts with 10+ React import statements for components, CSS, images, and whatnot. JavaScript is a must, not optional. reply nextaccountic 6 hours agorootparentAbout optional JS: you can have that with server-side rendering plus a modern framework (like next.js or leptos). That's certainly not as simple as vanilla html and js, but totally doable. reply kkarpkkarp 2 hours agorootparent> you can have that with server-side rendering plus yes, we use PHP for this which is much simpler to grasp than JS-SSR reply geek_at 2 hours agorootparentplus you don't need to restart any services or rebuild servers. Just git pull and you have the new code on the live server without even a second of downtime reply wouldbecouldbe 2 hours agorootparentprevNah, not simpler, nextjs is simpler then php to start working with. Also running & deploying nowadays is easier. PHP used to be easiest, but fell behind. reply tipiirai 6 hours agorootparentprevYou sure can, but it then comes to the separation of concerns. CSS-in-JS (coupling of concerns) is the preferred option. reply teaearlgraycold 5 hours agorootparentHaving CSS in a separate file feels like defining the predicates for your if statements in a separate file. With tailwind we're getting pretty close to a great solution here. reply postepowanieadm 2 hours agorootparentprevEven better - you may use service workers as your 'server-side'. reply p-e-w 6 hours agorootparentprevOne website that almost always gets mentioned when people talk about jQuery today is \"You might not need jQuery\" (https://youmightnotneedjquery.com/). That site is the best ad for jQuery I've ever seen. For almost every task it describes, the jQuery code is shorter, cleaner, and more intuitive than the vanilla \"modern\" JS one. And that's after almost 20 years, and I don't know how many billions of dollars invested into advancing JavaScript. reply Finbarr 6 hours agorootparentThis is incredible and such a great ad for jQuery. It's almost as if the creator(s) built that site ironically. So many examples are way clearer and easier in jQuery, like this one: JQUERY: $(el).toggle(); IE8+: function toggle(el) { if (el.style.display == 'none') { el.style.display = ''; } else { el.style.display = 'none'; } } Gotta love that \"modern\" triple attribute repetition. From: https://youmightnotneedjquery.com/#toggle reply extra88 5 hours agorootparentA more modern solution would be to use classList.toggle() or toggleAttribute(hidden). reply 1123581321 5 hours agorootparentprevToggle was a bit of a pitfall because the developer had to keep track of all the possible states prior to that line. It's easy to drop in as a function when needed in simple scripting (expanding faq sections, etc.) reply frereubu 1 hour agorootparentprevIE8 is a bit of a straw man. We've dropped IE entirely when testing unless there's significant (1%+) usage for particular clients, but even the NHS in the UK has dropped IE11. Edit: https://caniuse.com/?search=classlist.toggle and click Usage Relative. reply mschuster91 2 hours agorootparentprev> Gotta love that \"modern\" triple attribute repetition. You can golf it down a bit: el.style.display=el.style.display == 'none' ? '' : 'none'; reply anakaine 1 hour agorootparentEven though I can appreciate the elegance of your solution, I'd prefer the former for clarity. reply admaiora 16 minutes agorootparentprevYesterday I was checking HN from 10years±2weeks ago and guess what the top posts were... \"Why you need jQuery\" and \"You might not need jQuery\". I'm too young to know about those days but I guess not much has changed in relation to people's attitude towards jQuery. reply tipiirai 6 hours agorootparentprevIt's the jQuery API design. I often find myself creating functions like this one: export function $(query, root=document) { ... } jQuery acted as a role model for standard web committees. The argument for querySelector / querySelectorAll calls is literally mimicked from John Resig's groundbreaking API design. reply cxr 6 hours agorootparent> The argument for querySelector / querySelectorAll calls is literally mimicked from John Resig's grounbreaking API design Absolutely not. reply tipiirai 5 hours agorootparentAbsolutely yes. jQuery selector engine was developed before the official Selector API, which ended up being almost exactly the same. Except for a few exceptions which John wasn't too sure about when they asked for feedback: https://johnresig.com/blog/thoughts-on-queryselectorall/ reply jampekka 18 minutes agorootparentprevMuch of the billions are used to make JavaScript worse. E.g. ESM is a byzantine mess compared to CJS, and the ceremony and bondage of TypeScript is dramatically increasing busywork. reply LamaOfRuin 6 hours agorootparentprev>For almost every task it describes, the jQuery code is shorter, cleaner, and more intuitive than the vanilla \"modern\" JS one. Just like leftpad. reply ipaddr 5 hours agorootparentInstead of needing to import leftpad you have the missing standard js lib reply midasuni 2 hours agorootparentprevYesterday I wrote a custom page which dumped out a table for some investigation. I threw jquery and a tablesorter plug-in. Without JS you get the table, but with you can easily sort and filter. Took about 2 minutes to add the optional filter. The backend is bash of course. reply sireat 1 hour agorootparentFor tables my favorite jQuery plug-in is: https://datatables.net/ Have a page with some tables? A few lines of jQuery and you have search sorting etc. It can be customized but defaults are fine. This is how abstractions and progressive enhancement should be done. reply midasuni 1 hour agorootparentI’m not even sure what I used, it’s the same 5 lines I’ve used for years. Boom bang and on to the next problem. For every unicorn saas webapp $300k a year developers are writing there’s a thousand small pages. If you’re building a house you aren’t going to use a Swiss Army knife, but if you’re camping out for a couple of days you aren’t going to take a van full of power tools. reply anakaine 1 hour agorootparentA couple.of well selected power tools are certainly helpful, however. A battery blower to start the fire, a small electric chainsaw for any wood cutting depending on the area and what you take in, and a portable fan if you're in a hot climate. Sometimes a couple of selected power tools are very helpful. reply GrumpyNl 51 minutes agorootparentprevBig plus is, it also gives yo the option to export the tables to pdf and other file formats. reply PrimeMcFly 4 hours agorootparentprev> JavaScript is a must, not optional. I build plenty of CSS only sites without any JS just fine. I avoid using it as much as I can and generally only need it for forms or galleries, occasionally some ajax stuff. But it's seldom required. reply krick 5 hours agorootparentprevI won't dare to predict where industry \"is going to move\", since I'm not really a frontend developer, so what do I know anyway (and especially since it seems to switch directions every couple of years, and mostly being pulled in all directions simultaneously). But personally I still prefer to use stand-alone JS-scripts and libs included directly into the page, if I can. Mostly because I hate having to deal with complicated (and, importantly, rather slow) build process just for the sake of a couple of web-pages, when the real stuff is some totally invisible backend processing or maybe some sort of HTTP API to be used by other services. Also, honestly, I really just don't know what's the \"correct\" way to do things in 2024 on frontend, and I don't know where to even look for a guide, if I don't really want to spend next 6 months entirely in the pursuit of attaining \"real\" frontend-dev proficiency, but ultimately just to make working stuff, even if it doesn't quite follow the v2024 web-etiquette. reply palmfacehn 3 hours agorootparent\"Etiquette\" implies there is some civilizing force. A system that allows us to coexist in peaceful, accessible sanity. Someone said of a profitable app/site I created with vanilla JS, \"But this is looks like it is just a Bootstrap site\" At this point, functionality be damned, customers expect a loading spinner. The norm is something that doesn't load on the first request. Sites don't paint the screen until 10 seconds later, because they are avoiding repaints. Maybe 10-20mb of JS is included in the typical app in this niche. Many totally fail on Firefox. Ambiguous user facing errors on a black screen, \"An application client side error occurred\" are par for the course. reply erhaetherth 5 hours agorootparentprevIf you don't want to use a big framework, then you needn't use any library at all. Nearly everything in jQuery is 1 line of modern JS, no? reply masklinn 35 minutes agorootparent> Nearly everything in jQuery is 1 line of modern JS, no? Since JavaScript never actually requires line breaks, yes. Otherwise, absolutely not. Modern web APIs remain highly statements-based, with little affordance for pipelining / cascading. reply pier25 5 hours agorootparentprevOne line of jQuery can replace many lines of vanilla js. reply troupo 2 hours agorootparentprevNearly everything in jQuery is more concise, consistent and composable compared to modern JS APIs. As this site clearly shows: https://youmightnotneedjquery.com/ reply colordrops 5 hours agorootparentprevYes, there isn't much reason to use jQuery these days other than keeping with the idioms of a legacy system. reply rdedev 6 hours agorootparentprevI'm not a web developer but sometimes I have to make a page with some JS functionality. jQuery saves me a lot of time in such cases reply webworker 4 hours agorootparentprevI keep saying this. At some point, some very smart people are going to figure out how expensive React is to build and maintain, and it’s going to change the conversation. reply patrickaljord 3 hours agorootparentprevjQuery is amazing, still remember the joy of using it 20 years ago. That being said, React, Vue and similar are just better IMO to build and maintain complex apps. Don't get me wrong, jQuery was the best thing at its time and I will always be grateful to its contributors, but today there are better alternatives. I think htmx is nice but not seeing it taking over other frameworks. reply linkjuice4all 3 hours agorootparentDefinitely agree that the modern frameworks are better for complex stuff. Ideally there’s not really a middle ground - you’re either doing complex interactive “app-like” stuff with a framework or adding modest dynamic features to otherwise static content (e.g. static export of WordPress with some jQuery for your forms). reply throwaway2990 1 hour agorootparentprevlol react. Never seen a good react project. They all end up a mess. reply holoduke 2 hours agorootparentprevYou still need some kind of application framework. Jquery is just a low level utility. But higher level you need to maintain architecture. Specially in spa's reply hnfong 2 hours agorootparentYou don't \"need\" an application framework. People have been building websites (even SPAs) before the 2010s and they didn't \"need\" these frameworks. Given what we know today, I'll grant you that in some cases people would want to use a framework, but it's hardly a necessity unless the context provides more specific requirements. reply yashg 2 hours agorootparentprevYup, jQuery is indeed awesome! reply esaym 6 hours agorootparentprev>But jQuery doing dynamic binding to dynamically generated forms Do you have an example of that? reply DonHopkins 3 hours agorootparentHold my beer! https://alive22-dev.turnaroundhealth.com/static/javascript/q... reply falsandtru 3 hours agorootparentprevjQuery remains the best JSONP library. It can get data even when other libraries are blocked. reply rolisz 3 hours agorootparentIf I understand this announcement correctly, they removed support for JSONP reply falsandtru 3 hours agorootparentJust removed \"Automatic\" JSONP \"promotion\". JSONP is still available. reply bufferoverflow 6 hours agorootparentprevjQuery is not awesome. It was awesome 15 years ago. Now it's completely outdated. It's very hard to reason about DOM that can be manually changed by any random piece of code. That's why declarative solutions like React/Vue/Svelte are so much better. reply rjbwork 6 hours agorootparent>It's very hard to reason about DOM I don't mean to be overly snarky here, but as someone who's just totally out of their depth in modern web UI - is that why people like these frameworks? Because they're very easy to reason about? I've generally found them to be mountains and mountains of boilerplate and spaghetti, but I really do not have a wide base of experience to pull from on this topic. reply Capricorn2481 6 hours agorootparentI started with Jquery, I learned React. I prefer React. React complexity is often conflated with things like Next JS or other SPA solutions. React was originally meant to be a library for building small components that you drop into an otherwise static websites. But people usually don't talk about React unless they're talking about whole sites being in React. For that reason, I think its complexity is exaggerated. More importantly, what the commenter is referring to is the fact that you are encouraged to write Jquery code that can break if you decide to move a div or change a class name. The workflow of CSS selectors is fine for small apps, but can lead to hard to track bugs down the road. It can be avoided if you make a lot of unique IDs. But otherwise you're screwed. There's no scoping. You technically have to read every piece of Jquery code before changing any other code, html, or css, because you could break half the site depending on what people were depending on. There's no IDE support telling you what is selecting what. Events are not colocated on the things they attach to, they could be anywhere. I can update a React component and be sure that the only places it affects are the places its rendered. And I can get type checking on every input going into a component. That's just way better to me. I still use JQuery at work, but I usually dread it. And people say JQuery is \"smaller\" but I would still prefer something like Svelte to Jquery if that's a concern. I can take or leave Vue, I don't really notice a huge difference between it and other component frameworks. reply treflop 6 hours agorootparentI’m a big fan of React and jQuery but it sounds like you’re not using jQuery right. All my jQuery components were self contained and you just initialized them with $(‘[data-date-picker]’).datePicker(). It is pretty obvious to anyone looking at the code that if you remove “data-date-picker”, it stopped being a date picker. reply Capricorn2481 6 hours agorootparentThat is true, but this is built-in to React and it's optional for JQuery. I am working on legacy projects a lot and nobody ever writes JQuery like this outside of famous libraries. reply erhaetherth 5 hours agorootparentprevThen you have dumb stuff like if you check a box and want some additional form fields, you have to inject a div or input into the DOM manually, and then make sure you initialize your datepicker, but maybe there will be a flash of unpickerified input if you don't do it right. reply spiderice 4 hours agorootparentI haven’t used jquery in 10 years or so, but I’m pretty sure this isn’t what you would do. You would just hide or unhide the inputs with jquery. reply rolisz 3 hours agorootparentprevReact was developed by Facebook. I don't think they did it for small components on otherwise static sites. reply Capricorn2481 2 hours agorootparentFacebook was a fairly static site back then, but small is a relative term here. I don't mean to say it was only meant to do that. Just that the origins of React are a far cry from the complexity of SPA frameworks like Next JS. And it can still be used as quite a simple drop in component system. Nothing is stopping you from doing that. Which is why people often say React is not a framework but a library, because it's true. https://legacy.reactjs.org/blog/2013/06/05/why-react.html reply christophilus 6 hours agorootparentprevThat's one of the reasons I like them, yes. I've done a lot of manual DOM manipulation, and a lot of jQuery. When you're building anything complex, it starts to become quite difficult to figure out why a particular piece of the DOM is behaving the way it is. It could be an overly aggressive selector that accidentally targeted it, or any number of things. Prior to React (and similar libraries), DOM manipulation was generally done in a mutating, effectful way. With React (and similar), if a thing is behaving strangely, I just need to go to the component that rendered said thing, and I can almost always find the problem. reply kccqzy 6 hours agorootparentprevYes React is easy to reason about. In fact old versions of React were even easier to reason about than modern versions using hooks (`useState`). I used to use React and just React: no random npm packages, no Babel or any transpilation, no JSX, and it was pure joy compared to jQuery in a complex multi-step form. The jQuery code was in fact spaghetti. reply jmondi 6 hours agorootparentprevFor your blog, jquery is totally fine and reasonable. Jquery for a web application? That is going to be way more of a headache. It all depends on what you’re trying to accomplish. reply genocidicbunny 6 hours agorootparentprev> It's very hard to reason about DOM that can be manually changed by any random piece of code. I think the parents point is that many websites don't need to change all that much DOM manually, automatically, or in any fashion. And I would agree with them, that for when the only 'dynamic' part of your site is one or two simple forms, and maybe a little carousel of images, jQuery may be perfectly awesome. reply williamcotton 6 hours agorootparentprevThat is not why React became popular. We have to go back to the end of the server-side ORM era. ERB templates creating an LI element in a certain manner and jQuery creating the same element. So have fun keeping those DOM elements in sync! This and the ever expanding DOM APIs led to single page applications where all of the markup was generated by the client. Cue, oh fuck this is slow and oh fuck, page loads and SEO suffers, so then we see the emergence of virtual DOMs along with server-side Javascript, and we are now generating the same DOM elements with the same code on the front and back. Then everyone realized we are making these complex applications with an untyped language with lots of warts… Flow and Typescript emerge. Guess what? Spooky action at a distance continues with reducers, custom hooks, custom injected contexts, etc, etc. reply hsbauauvhabzb 4 hours agorootparentprevI learnt jQuery and I would say ‘I could code in jquery, but not pure JavaScript’ - that’s since changed but I do still find foreach jQuery iteration by classname fairly elegant, in addition to $.get and $.post. I’m not saying that it’s good or bad, but maintenance may mean developers who don’t know or care for modern web don’t have to learn something new. reply Klonoar 2 hours agorootparentprevNo, if you actually make modules with jQuery where it’s creating it’s own DOM tree, it’s no different from what React & Co are doing. Reasoning about the DOM structure has almost never been an issue in my almost 20 years of professionally doing this junk. The complexity has always been elsewhere. reply drschwabe 6 hours agorootparentprevIt's good for quick things and prototyping cause you can always swap out those calls with native later. It's API is generally easier to remember/less typing that most native equivalents. You can also use its API serverside via Cheerio to do parsing & manipulation of html without a dom. edit: also its way more lightweight than React/Vue/Svelte i don't necessarily disagree you shouldn't reach for jQuery if you have a dynamic page (something like uhtml+preact signals would be good if you have fair bit of rendering logic going on) but I would say you should totally try seeing how far you can get with jQuery instead of Svelte/React/Vue on simple pages. reply zztop44 4 hours agorootparentIs it really way more lightweight than Svelte? Svelte has more tooling (of course) but it ships no runtime and only sends the user the JS they actually need to interact with your page. reply drschwabe 3 hours agorootparentWell, if you're talking SvelteKit then it requires a build step so yes, jQuery is way more lightweight. jQuery is also pure JS whereas Svelte is Typescript so it may be more difficult to debug/hack if your primarily JS coder. reply labster 6 hours agorootparentprevDifferent use cases. A lot of the web only has only one or two interactive elements in a scope. The DOM being changed by any random piece of code is not a problem when all of the JavaScript fits on two screens. In fact, it’s a benefit here. reply cxr 6 hours agorootparentprev> People have been in love with the overly complex and fancy javascript frameworks for the last 15 years or so. jQuery is an overly complex and fancy JavaScript framework. reply worthless-trash 5 hours agorootparentI think your definitive comment above 'absolutely not' has tainted how people see your authoritatively spoken statements. reply thr0waway001 5 hours agoparentprevI don't care what anyone says, jQuery still rocks and is so damn important and influential. You remove jQuery from every piece of software a good chunk of the all the websites stop working all around the Internet. reply TheMajor 5 hours agorootparentYup, hard truths: vanilla JS is far less readable and clunkier than just using jQuery to do the same thing. Also, not everyone needs or wants to move to a shadow DOM framework with a zillion components and high complexity. If you're building a SPA or PWA, yes, absolutely, but for the vast majority of us who use a traditional backend/CMS-driven site with server-side rendering where client-side interactivity is needed, jQuery still does the job really nicely. reply KTibow 4 hours agorootparentNow I might not know what I'm missing out on since I haven't worked in any projects w/ jQuery as I haven't been developing websites for a long time (and the type of sites I make probably also influences the stack), but there's probably something that does what you want as elegantly without jQuery. We have stuff like querySelector and toggle in vanilla JS that makes it possible to change state simply, async stuff is much easier to understand than callbacks, and there are ways to split your code into components without using shadow or virtual DOM (see: raw web components, shadowdomless Lit, Svelte, etc). I've never found myself longing for something like jQuery. reply skydhash 3 hours agorootparentIf you're creating a fully interactive webapp (google maps, docs, or apple music), go with one of the frontend frameworks because they will give you a much simpler way of managing states and binding it to the view layer. But the majority of websites are not apps or shouldn't be. You'd only have a couple of interactive elements if you strip the UX to its core. And that can be done easily with server rendered templates and a bit of jquery/vanilla js. reply prisenco 6 hours agoparentprevWordpress devs are the plumbers of the tech industry. It’s not glamorous work but there’s a ton of money in it because so many people need them. If I ever decide to retire from latest and greatest hype cycle big tech startup world, I’ll go build Wordpress sites for honest coin and wrap up every day by 3pm. reply TheMajor 5 hours agorootparentThe fact that Wordpress runs so much of the web sometimes wakes me in a cold sweat. If you're a dev worth their salt and knows proven engineering and design patterns, then Wordpress code is absolutely terrifying to look through. reply Zetobal 58 minutes agorootparentSee comments like yours do that for me. I will see you in the 30 meetings needed for the new form on the contact page. reply radium3d 4 hours agorootparentprevheh, try 11-12pm. o.O reply noduerme 4 hours agoparentprevI was going to post on HN not long ago and didn't, about how I still can't find a great event chain handling / bubbling model that lets me use both DOM members and abstract class instances to trigger interchangeable events. I've built my own event dispatchers here and there, but jQuery just does everything right. Although event handling is almost the only thing I still use jQuery for, it's so useful that I still include it in almost every client-side project. The reason I was going to post was to ask if anyone knew of a slim library that could both $(window).trigger('click') and $(myClassInstance).trigger('myCustomEvent',{data}) with the same API for listening to either one asynchronously. With the rise of fetch() and css selectors I could probably do without the rest of jQuery at this point. But why reinvent the wheel? And before anyone tells me that having class instances dispatch events is a code smell... it's absolutely necessary if you want to build responsive frameworks from scratch. [Just for example, my base component class listens for a particular custom resize event dispatched from the screen that contains it, which only dispatches to components on screens that don't scroll and need to reformat their contents. The screen class listens for window.resize but only dispatches if it's in trouble with the layout. Putting individual DOM resize listeners to window on each and every component would be insane.] reply tambourine_man 4 hours agoparentprev>…WordPress is still more than 1/3 of the web… Why still? WordPress, like jQuery, is awesome. It's an incredibly powerful and easy open source CMS. I hope it takes more of the Web. And if you're gonna defend the decentralization of the Web (and I do), it's hard to find a better argument than “just buy a domain and install WordPress”. reply PrimeMcFly 4 hours agorootparentIt's a buggy insecure mess. No one should be advocating for wordpress. reply dawnerd 1 hour agorootparentThe only buggy or insecure code is really from third party plugins and themes. Wordpress core has been rock solid. Ya you still need to setup caching and there’s some modifications to run it at scale but that’s all a solved problem thanks to the likes of Automattic and their VIP platform. reply tambourine_man 3 hours agorootparentprevI’d like to know the name of this alternative that’s as feature-rich and yet bullet-proof while still being open source reply rastographics 2 hours agorootparentit's called processwire and once I found it I never looked back reply seydor 1 hour agorootparentprevnature abhors a vacuum reply profmonocle 3 hours agoparentprevIt's not just WordPress. A company I used to work for has tens of thousands of lines of jQuery code powering their enterprise SAAS product. A rewrite in a modern JS framework is just not going to happen unless it becomes absolutely necessary. Much of this code is extremely client-specific stuff written over a decade ago. The argument that using a modern framework helps recruiting doesn't work - the company only pays $75-90k for frontend engineers and has very low turnover, most current engineers have been there 5+ years. And most importantly, their current stack works just fine; they have a bunch of long-time clients who are happy. A good chunk of software engineering happens in \"boring\" businesses like this in cities with a much lower cost-of-living than the big tech industry hubs like the Bay Area / Seattle / etc. reply thedangler 6 hours agoparentprevI remember switching from mootools to jQuery back in 2006ish. Thought it was fantastic. Awesome it’s still around. We still use it from Time to time. reply kentf 6 hours agorootparentDon't forget Prototype (http://prototypejs.org/) reply andrew_ 6 hours agorootparentAnd Dojo! https://dojotoolkit.org/ reply jkingsman 4 hours agorootparentOh man, back in the day I got my first exposure to big, enterprise web apps in the form of an undergrad internship on a control panel for a major database that ran on Dojo. Every time I could scurry back to Python to work on a backend feature, I breathed a sigh of relief. It put me off frontend for a long time until I arrived at a shop that taught better, and not being taught was definitely a big part of my fear of Dojo/frontend dev, but Dojo is also BIG and you can build some really unwieldy systems with it. You can do that with any framework, sure, but in the heyday of Dojo, it was a lot of opaque logic that wasn't as well doc'd out or understandable just by reading as today. Dojo put the fear (respectful) and fear (scared) of big, enterprise frontend development in me. reply slater 6 hours agorootparentprevand scriptaculous! http://script.aculo.us/ reply quickthrower2 6 hours agorootparentYUI https://clarle.github.io/yui3/ Do we need to get into underscore etc? reply irjustin 6 hours agoparentprevjQuery will always hold a special place in my heart. It made absolutely terrible APIs usable and help abstract away a lot of the different browser issues. While I do a lot of effort to stay off jQuery these days, every time I have a library that ends up using jQuery its always intuitive to use. reply seydor 1 hour agoparentprevA lot of us are using it actively. It's so lightweight now that it s free, and it s somewhat better than vanilla js. reply mardifoufs 1 hour agoparentprev1/3 of the web does not mean much. Are you referring to web usage or just the number of web pages? reply seydor 1 hour agorootparentweb pages and it does matter a lot. Among others, Woocommerce runs 23.43% of e-commerce. reply krick 6 hours agoparentprevHonestly, though, given more sophisticated browser-native selectors were implemented way after jQuery was, I wonder why didn't they just make at least somewhat jQuery-compatible API. It's just way simpler. reply quickthrower2 6 hours agoparentprevThe fact that it, well, works doesn’t help with the React world domination either. reply kamaal 49 minutes agoparentprevAnd many many company internal tools, that won't be rewritten and just get replaced with something better(ReactJS?) over time. It would be around for long is nice, but you can't make a career using it these days. jquery, perl just some of those insanely useful tools which get the work done, but they don't pay you well these days. reply mouzogu 1 hour agoparentprev> Any advances to removing deprecated APIs or functions are great. if something is used, it has value. it's not deprecated. reply todotask 4 hours agoparentprevNow we have React, React DOM and jQuery in WordPress. reply lostemptations5 4 hours agoparentprevjQuery is just way cooler than the comparatively build in (and sometimes obtuse) web standard replacements for it. reply Onavo 6 hours agoparentprevThey just need to add JSX support. Half the reason people use react is because of JSX. reply IgorPartola 6 hours agorootparentMaybe unpopular opinion but JSX is inferior to something like the Vue templating system. I much prefer to have HTML with some JS sprinkled in than JS that looks like HTML but isn’t. reply Klathmon 6 hours agorootparentAnd to wage this holy war a little, I personally much prefer JSX over a templating system that looks like html attributes but acts different, while trying to act like javascript but can't. Honestly at the end of the day they both get the job done, but for me personally working in JSX is just so easy to pickup and use, and while it's not perfect, I do prefer it over having to learn yet another library specific templating kinda-sorta language. reply TheMajor 5 hours agorootparentprevMost of us devs had \"separation of concerns is critical\" drilled into us for many, many years. For that reason alone, JSX just gives me the baddest of smells when I look at it. reply mardifoufs 1 hour agorootparentHow does jsx not have separation of concerns? I get what you are referring to but that's not SoC as I know it reply troupo 2 hours agorootparentprevThis way of looking at separation of concerns helps: https://x.com/simonswiss/status/1664736786671869952 reply troupo 2 hours agorootparentprev> I much prefer to have HTML with some JS sprinkled That's not what vue templates are. It's three or four different templating DSLs in one. v-for alone will show that it's not HTML with Javascript: https://news.ycombinator.com/item?id=28059397 And there's more: https://news.ycombinator.com/item?id=19199423 reply rpncreator 6 hours agorootparentprevA variant of this was tried over a decade ago, but never made it live - jQuery Templates. Not quite the same… https://github.com/BorisMoore/jquery-tmpl reply technojunkie 6 hours agorootparentprevIf \"they\" is WordPress, have you tried modern PHP? 8.x has come a long way to looking and feeling like a modern programming language. So many new things reminding me of Typescript, Javascript, Python, etc. reply quickthrower2 6 hours agorootparentprevShould be easy. It is just a case of presenting the right interface to the generated code. For example Mithril supports it (with some quirks admittedly). But in simple terms it is an adaptor. reply boredtofears 5 hours agorootparentprevFully agreed. I built a mostly vanilla JS app using a bare bones JSX lib[1] a few months ago and was surprised at how little I missed the rest of React. [1] https://github.com/alex-kinokon/jsx-dom reply kijin 6 hours agorootparentprevI think you can already do that by wiring together something like NakedJSX. You just need a fancy jQuery plugin to make it more ergonomic. reply wkirby 6 hours agoprevI'm consistently surprised by the commenters on HN who seem to think jQuery is just a DOM selection library, when in fact it is a widely supported, incredibly stable tool set for (yes) DOM selection, but also attribute manipulation, Ajax requests, event handling, animation, and general utility functions. What's more, where there _is_ native functionality that replaces jQuery, the API is never as fluent. For work that needs a little enhancement on top of server-side HTML, but not a full-blown JS UI framework, jQuery is a small price to pay for a stable, reliable, and cross-browser compatible dependency. reply DrSiemer 6 hours agoparentA few years ago I dropped jQuery for plain vanilla js and I've never looked back. Native js has everything jQuery has, except for the pile of often poorly maintained half baked \"plugins\", that usually only do what they want to do and not much else. Yes, the vanilla selectors are more verbose, but any half decent code editor will make them just as fast to type. I don't mean to be penantic, but I don't really understand why so many people still use jQuery at all. https://youmightnotneedjquery.com/ reply squeaky-clean 5 hours agorootparentSome of those examples aren't really equivalent. If you use that vanilla js way to show/hide, it won't restore the display property to the value it was before using hiding. It will just shift it between display: \"none\" and display: \"\". If you need something to be a display: \"inline-block\", the vanilla js example will lose that and break your styling. You can modify the code to set the display property as inline-block, but now you need a different show function for every possible display value you want to restore. Or you need to build a layer of state to remember what the original value was and hey you've rewritten the jquery version. And this is all ignoring that the jquery versions just look so much simpler to remember and are cleaner to read. reply aolo2 1 hour agorootparentExcept you can do this: ```css .hide { display: none important!; } ``` ```js function hide(el) { el.classList.add('hide'); } ``` reply QuadrupleA 5 hours agorootparentprevLook at your own link - the \"modern\" way of doing it is always about 2-3x as verbose as jQuery. Editors let you type that mess faster, but you'll still have to read through it, maintain it, scroll it into view in your editor, see less of it onscreen at a time, etc. I'll take a clean, succinct API over a noisy verbose cumbersome one any day. And what are we saving by eschewing it and going vanilla? One little 30kb-gzipped javascript file - smaller than most jpegs. I agree for super simple tasks it might be best to skip it. The DOM API is a sloppy and hastily-designed pile of crap from the OOP era that we're unfortunately stuck with for legacy reasons. jQuery shows what it could have been. reply austin-cheney 2 hours agorootparentThat is not correct at all. The DOM API is entirely functional based upon traversal of a lexically scoped model. It may not be as declarative as you like but lying about what it is because it makes you feel uncomfortable is rather ignorant. reply troupo 2 hours agorootparent> The DOM API is entirely functional based upon traversal of a lexically scoped mode wat DOM APIs have been, and still are, 90s-era OOP style with zero attempt at being functional, declarative, or composable. Even the newly developed API are usually stuck in the same mindset (see everything developed for web components) reply austin-cheney 2 hours agorootparentThe DOM is a big in memory object but the DOM APIs are entirely functional. Functional is not the same as declarative. reply troupo 1 hour agorootparent> DOM APIs are entirely functional. (Almost) none of the DOM APIs are functional. They are literally `object.methodCall()` reply skydhash 3 hours agorootparentprev> Native js has everything jQuery has, except for the pile of often poorly maintained half baked \"plugins\" The great things about these plugins is that you can strip them down to what you need and patch them yourself (remember lib or vendor directory). NPM and build tools make this a chore. reply gonational 6 hours agorootparentprevLMFAO every example of \"modern\" looks like a course in how not to design an API, etc. The jQuery examples are so clean and expressive. Imagine landing on a page like that and thinking, \"hey, that 'modern' option looks good.\" You'll end up with 28KB (jQuery gzipped size) of just extra syntax before you're done with your app, not to mention the additional 160 hours of work. You'll likely be adding 6-700KB of React and its ilk anyway (I see it in every project now). reply austin-cheney 5 hours agorootparentSigh. There are people who cannot program and fortunately for those people there is jQuery. Writing any kind of UI in vanilla JS is trivial and even more so with TypeScript. If that takes you a 160 hours to complete you are not the guy to be doing that work. I could pay my pest control guy to figure it out. Maybe it might take him 160 hours. JQuery used to make sense back in the days of IE, especially before IE9, and your team was green. Even then most cross browser issues were in CSS, not JS. Those days are long gone. reply cpill 5 hours agorootparentoh the arrogance! love it this is why I read the comments on hacker news before (or mostly in place of) the article :P reply genocidicbunny 6 hours agoparentprevI don't think it's that surprising given the demographics of HN posters that I've observed. Many will immediately reach for the big JS framework du jour when they need anything more than a little DOM selection stuff, because that's what they've always done. reply erhaetherth 5 hours agorootparentThere's 2 types of web pages. Websites and web apps. If you're building a web app, you're going to want a framework that isn't going to fall apart the second things start getting complicated. If you're making a little marketing website, sure, sprinkle some jQuery on there and call it a day. Probably better for performance and SEO unless you want to spend 10x as long micro-optimizing your SSR. Point is, there's different use-cases. reply austin-cheney 2 hours agorootparentIt’s all the same. The difference is not in the product but in the developer’s perception of the product. The code and the user don’t notice the difference, just the developer in the middle. There are those who can dynamically put text on screen and those who need just a little help to dynamically put text on screen. reply genocidicbunny 5 hours agorootparentprevOkay, what part of what I said are you trying to refute or educate me on? There's a large proportion of posters on HN who spend their days building web apps. They are very likely to stick to using what they know, even when building web sites. If you spend your days working as a welder, and suddenly need to build a box for some reason, you're probably not going to be breaking out the fine wood joinery tools. You're probably going to take some sheet steel and weld up a box and move on to whatever else you have to do. It's not a problem, it's not a fault, it just how things are. reply hnfong 1 hour agorootparent> Okay, what part of what I said are you trying to refute or educate me on? Not the GP, but that's why, when applicable, I try to prefix my replies with a variant of \"I agree, and btw...\" reply jazzcrab 4 hours agoparentprev> commenters on HN who seem to think jQuery is just a DOM selection library proceeds to name selection and native functions reply mouzogu 1 hour agoparentprevyes and far easier to drop jq into a project. no need for build tools and all headache. (i know you do that with react but its not comparable features). reply robertoandred 8 minutes agorootparentYou don’t run your JS through babel or minification? reply sazz 20 minutes agoprevOh I loved jQuery. Anybody remembers DataTables https://datatables.net or X-editable https://vitalets.github.io/x-editable/ ? reply voidwtf 7 hours agoprevIn a world where many of us are actively trying to remove the last vestiges of jQuery, who is actively developing using jQuery? Genuinely curious. I’ve found that most of what I went to jQuery before is baked in now. querySelectorAll being the most powerful. reply dylan604 7 hours agoparentBecause I'm old and new tricks are less interesting, I find $.ajax() much more simple than promise, await, async of native JS. I have used straight native JS on a couple of smaller personal projects just to get some familiarity with it. However, it's just muscle memory type of getting stuff done with $.ajax() for me. Also, maybe 20 people use any of the code I write for manipulating DOM. I'm not a UI person. I'm a back end person that has to write front end stuff because nobody else does it. reply lakpan 7 hours agorootparentIt’s only simpler because you know how to use it. For most use cases fetch and async/await is just easier. reply dylan604 6 hours agorootparentI mean, duh? Of course doing something that I've done for a long time is going to be more simple to me than me fumbling/stumbling through something I haven't done much. What new insight are you providing me here? reply umvi 6 hours agorootparentAsync await fetch lets you flatten your nested callback functions into simple procedural programming. It makes everything much easier to reason about, no more closures and such. reply peebeebee 1 hour agorootparentJQUERY: await $.post({ url: '/my/url', data: data }).then(() => {}); VANILLA: await fetch('/my/url', { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify(data) }).then(() => {}); I know which one I still prefer. reply hoten 27 minutes agorootparentYou don't need that then in the fetch example. I don't know about ajax. reply dylan604 5 hours agorootparentprevsee, this is where i'm not cool with this newness. i've spent years getting away from procedural, and switching to functions, classes/methods. now, we want to get away from that and go back to procedural? that's all fine and dandy, but you're trying to have a new trick conversation with an old dog that just doesn't care. you're bringing some sort of logic to a conversation where it's not needed. it works for me. i don't get paid to make UI apps or write heavy code nonsense with server side JS. i use a proper back end language. JS can stay in the browser and manipulate the DOM thank you very much. i get paid to make heavy processing code that sometimes is helpful to have UI dashboards. i can whip up a JQuery front end faster for my needs than most can even figure out what NPM librar[y|ies] they need to use reply boredtofears 5 hours agorootparentsurely you still use procedural code within your functions and classes. the point of await/async is you break down the callback nesting into a single set of procedural code. it doesn't mean all your code is procedural. just that what used to have to be a series of indented anonymous functions or spread all over the place named functions can now be an easy to read concise few lines of code all at the same indentation in a single spot. its an obvious net-win for readability and code maintainability. you are doing yourself no favors by not understanding/embracing it. reply duskwuff 6 hours agorootparentprev> I find $.ajax() much more simple than promise, await, async of native JS. $.ajax() returns a promise. If you're calling .done() / .error() on it to handle the results -- well, that's exactly how you work with promises. reply dylan604 6 hours agorootparentyou think it's the same, but it's not even close to me .ajax({ success:function(result), error:function(xhr), } vs fetch().then((e)=>function()) is even close to being the same, then we're just not even talking the same language reply erhaetherth 5 hours agorootparentfunction ajax({url,success,error}) { fetch(url).then(success, error) } Boom, same syntax if that's what you prefer. J/K, the error status handling is not the same and the auto-deserialization isn't present. Not hard to add but it's harder to argue not to use a lib instead of copy-pasting 114 lines of niceties into each project. reply duskwuff 3 hours agorootparent> the auto-deserialization isn't present You mean like response.json()? reply umvi 6 hours agorootparentprevYou would just do: const response = await fetch(...) Bam, now you have the response without needing callbacks. reply dvt 4 hours agorootparentThe async/await design is absolute garbage and no one will ever convince me otherwise. `fetch` is a particularly egregious example: it has all kinds of insane random quirks that you need to memorize. For example, how do I handle an error there? What's the obvious way to handle it? Is response null or something? Well, not exactly, you need to check for: response.ok; // false response.status; // 404 Um, okay, I get it. So to get the text content of the response, I just need to do response.text, right? No, you dummy, that's a function! You need to call response.text(), duh. It's so funny how an entire generation of front-end engineers just accepted this slop as acceptable API design. You can't fix the past, but thank God for (oh the irony) Microsoft and TypeScript. reply hoten 24 minutes agorootparentI don't see how the design of the fetch API relates to async/await being badly designed, can you explain? reply n_plus_1_acc 3 hours agorootparentprevThere are many popular wrappers like axios that fix some of these things. reply mschuster91 1 hour agorootparentThe fact that these kinds of wrappers exist instead of the most popular wrappers eventually ending up standardized in JS directly is mind-boggling. reply austin-cheney 2 hours agorootparentprevWebSockets are easier and faster. reply lawgimenez 7 hours agoparentprevBased on last year’s Stack survey, JQuery is the third popular. https://survey.stackoverflow.co/2023/#section-most-popular-t... reply sam0x17 6 hours agoparentprevIt's funny, I just spent a bunch of time removing the last vestiges of react for a company, replacing it with HTMX, which imo is the spiritual successor to jQuery in some ways reply kxrm 7 hours agoparentprev> who is actively developing using jQuery? Yea same, I am actively trying to get rid of jQuery, but my guess would be people get used to working with a tool and there just isn't enough desire or a business case to get rid of it as it would require a rewrite. I would say that I doubt new projects are starting with jQuery but I know there are devs out there that know the jQuery way better than the vanilla javascript way to get things done. reply MBCook 7 hours agorootparentBut if you don’t want to get rid of it for business reasons, which I get, a major upgrade that changes a bunch of APIs and drops features would still require a lot of work. And if you’re forced to leave 3.x over security or something (may happen in the future) then why not just move off then? reply dylan604 7 hours agorootparentI'd imagine upgrading from JQuery 3.x to 4.0 will be much less hassle than starting from essentially a blank page even if you were using some of the deprecated methods (even though you've previously been warned). reply kxrm 7 hours agorootparentprevYea, see the last half of my comment. There is a lot of legacy knowledge around how jQuery handles things and there are devs out there who just aren't interested in learning how to do it without jQuery. For their customers and users, it makes no difference. reply gexla 1 hour agoparentprevI'm guessing most active development which people are actually consuming. Already mentioned is Wordpress, which is most of the web. There's probably tons of front-ends using old Bootstrap which also came with Jquery. Greenfield projects might not be using Jquery, but is anyone actually using those? ;) reply mrieck 3 hours agoparentprevSolo or small team devs who hate modern tooling. Good example is levelsio guy: https://twitter.com/levelsio/status/1750175827197567165 I'm also in that camp and still use jQuery. My current SaaS is an extension that doesn't have much UI code https://www.snipcss.com, and my next SaaS will be a chatgpt powered web automation extension that has a good amount of UI. Both use jQuery. Edit: I didn't say why I don't just use vanilla with querySelectorAll - majn reasons are I like how I can attach events (attaching to parent while targeting dynamically added subelements), chaining functions, and it's just less code than vanilla reply arp242 6 hours agoparentprevThe DOM API now solves a lot of the issues jQuery solved 15 years ago, but often in very different ways. I don't think it's better; a lot of the APIs are awkwardly designed IMO. The big upshot is that it's built-in, but that's it, and with a very small jQuery library you get a nicer API. Sometimes the extra dependency is best avoided, but often it doesn't matter. So, whenever reasonable, I prefer to just add jQuery. Maybe there's something better, but the differences/improvements of what I've seen doesn't seem that great and jQuery works, so that's the point? I guess the younger kids never used it, but it's easy enough to pick up if you know standard JS. reply sublinear 7 hours agoparentprevThere are plenty of legacy web projects still using jQuery that nobody has been able to replace. Off the top of my head I'm thinking of complex boring stuff like WCAG-compliant carousels or whatever. The kind of libraries people tend to reach for because marketing wanted them, but are otherwise ignored. reply ahmedfromtunis 7 hours agoparentprevDjango uses jQuery for its admin interface. (That is, jQuery is used by the Django maintainers to write the built-in admin interface.) But I also know that a lot of the \"low-cost\" agencies are still using it. Probably because de the devs are used to using and don't want to bother to (re)learn things. At least in my country, a lot of website are still using it (and very old, outdated versions at that). reply miragecraft 6 hours agoparentprevjQuery is still excellent if you are creating websites (not web apps) with minimal JavaScript. reply mhitza 6 hours agoparentprevI wouldn't exclude the possibility of new developers that found some stackoverflow answer, or another, which suggests jQuery as the solution. reply didip 7 hours agoprevGreat job jQuery folks! I will forever love jQuery. It simply gets the job done. reply mouzogu 1 hour agoparentBootstrap, PHP, JQuery & WP. The stack for people who get shit done. reply OccamsMirror 7 hours agoprevWith browsers having much better support with DOM selection, why would anyone be using jQuery in 2024? Not trying to throw shade, I actually think it's cool that it's still being worked on. But I'm confused as to what the use case is. reply arp242 7 hours agoparentThere's a bunch of pretty convenient selectors that are not supported by document.querySelector(), such as :selected, :checked and a bunch of others (I forgot which exactly, but I've run in to it a few times). In general the DOM API offends pretty much every single of my sensibilities on how to design good APIs. I also have some gripes with the jQuery API, but it's a lot better. jQuery just reads and writes so much more fluently. reply mkl 6 hours agorootparent:checked is standard CSS, and old. reply arp242 6 hours agorootparentEh, do'h, of course. There's a few selectors anyway, but I'd have to check which ones exactly. :eq(n) and :visible from a quick check (:nth-child(n) can replace :eq(n) in some cases, but not all). reply mkl 6 hours agorootparentYes, I've used :nth-of-type(n) for that too. Visibility is surprisingly messy though. reply evan_ 6 hours agorootparentprev`:visible` and `:hidden` are really cool reply sibit 7 hours agoparentprevLegacy projects? I maintain an old .NET web app that's ~15 years old and runs on jQuery. While the dream of refactoring or rewriting the entire thing using something more modern (like Web Components) is nice we're never gonna do it. reply Semaphor 5 hours agorootparentSlightly older WebForms (I know) project. New stuff gets done in vanilla or Vue, but there’s a lot of jQuery there that certainly will not be replaced without great reason. reply anamexis 7 hours agoparentprevIf I had to guess, a convenient API and a massive ecosystem. reply rubymancer 6 hours agoparentprevI love it for admin pages. If only 20 internals are ever going to see it, jquery is as light as can be, can be loaded right from a CDN, needs no kind of transpilation and handles basically all you need. reply lcnPylGDnU4H9OF 6 hours agorootparent> admin pages > CDN There’s a sysadmin just got called in for a security incident. Best to include it as a vendor library. reply drakenot 6 hours agoprevI remember when jQuery was first released. To convince my coworkers to use it, I held an internal presentation called \"Don't Fear the $,\" where I walked through the advantages it provided. reply ent101 6 hours agoprevI wrote puter.com using jQuery :P reply Beefin 5 hours agoparentthis is a pretty amazing site reply ent101 5 hours agorootparentThank you! Glad you liked it :) reply shakabrah 6 hours agoprevI learned jQuery before i learned javascript proper over a decade ago. I didn’t even really understand what i was doing was considered programming. although i’ve not touched jquery in many years, i now have a great livelihood doing something i love. Long may they live. reply MangoCoffee 4 hours agoprevJquery made it to version 4 beta and there's a version 5 in the future. it looks like there's a big enough market for JQuery. reply hallman76 6 hours agoprevJust in case folks aren't familiar, this site breaks down jQuery features into web standard code: https://youmightnotneedjquery.com/ reply mm007emko 6 hours agoprevI remember jQuery from it's heyday when we used it as a replacement for Mootools. Since many people are going from React to HTMX, maybe we've made a full circle and we'll see web using just plain jQuery as well? reply evilduck 6 hours agoparentI came up in a similar era, I don’t think back fondly on those messes. Doing a modern app with accessibility, mobile support, real time updates, visualizations, and maybe a few PWA features with just jQuery sounds incredibly unpleasant. I’m waiting for the HTMX hype train to meet reality too. Programming your app in pseudo attributes with a DSL is going to wear thin quickly. reply holoduke 2 hours agoprevI still use it as my low level html operations lib in my framework. Its still super handy. And size wise small. reply nfriedly 6 hours agoprevI remember building web apps in the days before jQuery, it was a mess. The DOM APIs were full of quirks, and all the libraries that predated jQuery had their own issues. These days, we have better options for building complex web apps, including much-improved native APIs. I don't think I've started a new project using jQuery in 10 years. But for maintainin a lightweight website that just needs a little bit of interaction or pizzazz, I still think jQuery is perfectly fine. reply yread 2 hours agoparentEspecially the events! Those were browser-specific nightmares reply DonnyV 6 hours agoprevA lot of what jQuery was created to do is now baked into the browser. CSS selectors, proper ajax, animation, etc. I built a small convenience library around fetch to make it super easy to use ajax like in jQuery. Also built a small utility library around ES6 methods that recreate jQuery methods. It doesn't take much to get back the convenience of jQuery but still use all the new tools. I love jQuery and used it for years. But there are better tools now. reply andirk 6 hours agoprevI like Vue's reactive nature the best (specifically `v-if`, `v-for`), but jQuery was the one that got me over the hump before I actually knew Javascript. And I encourage most projects that use a lot of jQuery to consider using jQuery's CDN version as there is like a 50%+ chance the person already has it cached on their system. reply nsp 6 hours agoparentThis hasn't been the case for about four years. Since you can use cache status to track people, resources are now cached per hostname requesting as well. https://www.stefanjudis.com/notes/say-goodbye-to-resource-ca... reply lenkite 3 hours agorootparentThis feels like it is still pretty good when you are on a common host like github pages. reply notzane 6 hours agoparentprevDoesn’t work anymore :( https://news.ycombinator.com/item?id=24894135 reply tored 5 hours agoprevWorst part of jQuery is actually upgrading it to a new version, because the api design that made jQuery powerful is also what makes it quite difficult figure out if you broke anything. reply mkoryak 3 hours agoparentWhy worry when you can easily load both versions with jQuery.noConflict() !!! Or you can just load one on top of the other. It will be ok reply jszymborski 7 hours agoprevBring yayQuery back. The people demand it. reply mholt 6 hours agoprevI loved jQuery. Still great, but browser APIs have improved. Now I mainly use AJQuery: https://github.com/coolaj86/ajquery.js/blob/main/ajquery.js reply MythTrashcan 5 hours agoprevIf most of the web didn't have a huge jQuery dependancy, we would have mostly moved on years ago. reply seattle_spring 3 hours agoparentI thought so too before reading these comments. I'm surprised how much people think jQuery still provides everything you need for a modern webapp. I have to step back and realize that my world has been working on millions+ LOC apps with hundreds of simultaneously committing devs, vs a lot of jQuery devs committing to personal-sized projects or blog-like web pages. Totally different solutions for different problems. I'd still pick vanilla es6+ vs jQuery for smaller stuff, but I get why some might not. reply LorenzoGood 5 hours agoprevI feel like most of Jquery has been re implemented as browser api's anyway. reply synergy20 7 hours agoprevMost elegant API for Javascript, can it become part of Javascript API directly one day? reply etoxin 5 hours agoprevjQuery has mostly been replaced by new browser apis. It was a great library that brought the web forward. It will still be around the web for decades to come. If you're updating to jQuery 4. Why not try switching to native js. https://youmightnotneedjquery.com/ reply donohoe 6 hours agoprevIn job postings for FE developers, I usually put in this line: “A healthy disdain for jQuery” This changes nothing. reply hu3 6 hours agoparentThe filter works both ways. When I see too much emotion in engineering job posts I take it as a sign that the team is more likely to be cult driven and less likely to reasonably discuss tools in terms of trade-offs. reply donohoe 5 hours agorootparentFair. In reality it usually leads to some good questions on the nature of frameworks. reply tcper 3 hours agoparentprevA job pays you 10k a week for writing jQuery code, will you refuse it? reply joshka 5 hours agoprevIs jQuery the COBOL of the 2000s? reply voat 6 hours agoprevAnother day another JavaScript framework! I'm so tired of all this churn in frontend. /s reply ActualHacker 7 hours agoprev [–] I was evaluating a SaaS solution just yesterday, and in one of their examples on Github they used jQuery as the primary frontend for the project, with extensive usage. I passed on the company reply jraph 6 hours agoparentWhy? What's wrong, if it works well? reply paulddraper 6 hours agorootparentThere are things that work better. reply jraph 6 hours agorootparentSeems a weak reason for rejecting a solution and this is subjective. I prefer native JS but something can be well designed in jQuery. reply wiseowise 6 hours agorootparentprevDefine “better”. reply paulddraper 5 hours agorootparentMore succinct, fewer bugs. reply cynicalsecurity 3 hours agoparentprevToo bad for you. I'm glad for them they don't have to deal with you as a customer. reply kmoser 5 hours agoparentprev [–] With SaaS, the front end might be the least important piece of the application. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "jQuery 4.0.0 beta version has been released, bringing bug fixes, performance improvements, and some breaking changes.",
      "Support for IE with children has been removed in this update.",
      "The jQuery Foundation offers various resources, including training, events, documentation, support, and forums, to help users learn and contribute to the jQuery community."
    ],
    "commentSummary": [
      "Participants debate the relevance and significance of jQuery in contemporary web development, particularly its indispensability for WordPress-based websites.",
      "Advocates highlight its simplicity and versatility in handling diverse tasks.",
      "Conversely, proponents of modern JavaScript frameworks like React argue that jQuery's necessity is subjective when compared to newer technologies."
    ],
    "points": 334,
    "commentCount": 223,
    "retryCount": 0,
    "time": 1707272833
  },
  {
    "id": 39273932,
    "title": "Improving Command-Line Programs: Modern Updates for UNIX Principles (2021)",
    "originLink": "https://clig.dev/",
    "originBody": "Command Line Interface Guidelines An open-source guide to help you write better command-line programs, taking traditional UNIX principles and updating them for the modern day. Authors Aanand Prasad Engineer at Squarespace, co-creator of Docker Compose. @aanandprasad Ben Firshman Co-creator Replicate, co-creator of Docker Compose. @bfirsh Carl Tashian Offroad Engineer at Smallstep, first engineer at Zipcar, co-founder Trove. tashian.com @tashian Eva Parish Technical Writer at Squarespace, O’Reilly contributor. evaparish.com @evpari Design by Mark Hurrell. Thanks to Andreas Jansson for early contributions, and Andrew Reitz, Ashley Williams, Brendan Falk, Chester Ramey, Dj Walker-Morgan, Jacob Maine, James Coglan, Michael Dwan, and Steve Klabnik for reviewing drafts. Join us on Discord if you want to discuss the guide or CLI design. Foreword In the 1980s, if you wanted a personal computer to do something for you, you needed to know what to type when confronted with C:\\> or ~$. Help came in the form of thick, spiral-bound manuals. Error messages were opaque. There was no Stack Overflow to save you. But if you were lucky enough to have internet access, you could get help from Usenet—an early internet community filled with other people who were just as frustrated as you were. They could either help you solve your problem, or at least provide some moral support and camaraderie. Forty years later, computers have become so much more accessible to everyone, often at the expense of low-level end user control. On many devices, there is no command-line access at all, in part because it goes against the corporate interests of walled gardens and app stores. Most people today don’t know what the command line is, much less why they would want to bother with it. As computing pioneer Alan Kay said in a 2017 interview, “Because people don’t understand what computing is about, they think they have it in the iPhone, and that illusion is as bad as the illusion that ‘Guitar Hero’ is the same as a real guitar.” Kay’s “real guitar” isn’t the CLI—not exactly. He was talking about ways of programming computers that offer the power of the CLI and that transcend writing software in text files. There is a belief among Kay’s disciples that we need to break out of a text-based local maximum that we’ve been living in for decades. It’s exciting to imagine a future where we program computers very differently. Even today, spreadsheets are by far the most popular programming language, and the no-code movement is taking off quickly as it attempts to replace some of the intense demand for talented programmers. Yet with its creaky, decades-old constraints and inexplicable quirks, the command line is still the most versatile corner of the computer. It lets you pull back the curtain, see what’s really going on, and creatively interact with the machine at a level of sophistication and depth that GUIs cannot afford. It’s available on almost any laptop, for anyone who wants to learn it. It can be used interactively, or it can be automated. And, it doesn’t change as fast as other parts of the system. There is creative value in its stability. So, while we still have it, we should try to maximize its utility and accessibility. A lot has changed about how we program computers since those early days. The command line of the past was machine-first: little more than a REPL on top of a scripting platform. But as general-purpose interpreted languages have flourished, the role of the shell script has shrunk. Today’s command line is human-first: a text-based UI that affords access to all kinds of tools, systems and platforms. In the past, the editor was inside the terminal—today, the terminal is just as often a feature of the editor. And there’s been a proliferation of git-like multi-tool commands. Commands within commands, and high-level commands that perform entire workflows rather than atomic functions. Inspired by traditional UNIX philosophy, driven by an interest in encouraging a more delightful and accessible CLI environment, and guided by our experiences as programmers, we decided it was time to revisit the best practices and design principles for building command-line programs. Long live the command line! Introduction This document covers both high-level design philosophy, and concrete guidelines. It’s heavier on the guidelines because our philosophy as practitioners is not to philosophize too much. We believe in learning by example, so we’ve provided plenty of those. This guide doesn’t cover full-screen terminal programs like emacs and vim. Full-screen programs are niche projects—very few of us will ever be in the position to design one. This guide is also agnostic about programming languages and tooling in general. Who is this guide for? If you are creating a CLI program and you are looking for principles and concrete best practices for its UI design, this guide is for you. If you are a professional “CLI UI designer,” that’s amazing—we’d love to learn from you. If you’d like to avoid obvious missteps of the variety that go against 40 years of CLI design conventions, this guide is for you. If you want to delight people with your program’s good design and helpful help, this guide is definitely for you. If you are creating a GUI program, this guide is not for you—though you may learn some GUI anti-patterns if you decide to read it anyway. If you are designing an immersive, full-screen CLI port of Minecraft, this guide isn’t for you. (But we can’t wait to see it!) Philosophy These are what we consider to be the fundamental principles of good CLI design. Human-first design Traditionally, UNIX commands were written under the assumption they were going to be used primarily by other programs. They had more in common with functions in a programming language than with graphical applications. Today, even though many CLI programs are used primarily (or even exclusively) by humans, a lot of their interaction design still carries the baggage of the past. It’s time to shed some of this baggage: if a command is going to be used primarily by humans, it should be designed for humans first. Simple parts that work together A core tenet of the original UNIX philosophy is the idea that small, simple programs with clean interfaces can be combined to build larger systems. Rather than stuff more and more features into those programs, you make programs that are modular enough to be recombined as needed. In the old days, pipes and shell scripts played a crucial role in the process of composing programs together. Their role might have diminished with the rise of general-purpose interpreted languages, but they certainly haven’t gone away. What’s more, large-scale automation—in the form of CI/CD, orchestration and configuration management—has flourished. Making programs composable is just as important as ever. Fortunately, the long-established conventions of the UNIX environment, designed for this exact purpose, still help us today. Standard in/out/err, signals, exit codes and other mechanisms ensure that different programs click together nicely. Plain, line-based text is easy to pipe between commands. JSON, a much more recent invention, affords us more structure when we need it, and lets us more easily integrate command-line tools with the web. Whatever software you’re building, you can be absolutely certain that people will use it in ways you didn’t anticipate. Your software will become a part in a larger system—your only choice is over whether it will be a well-behaved part. Most importantly, designing for composability does not need to be at odds with designing for humans first. Much of the advice in this document is about how to achieve both. Consistency across programs The terminal’s conventions are hardwired into our fingers. We had to pay an upfront cost by learning about command line syntax, flags, environment variables and so on, but it pays off in long-term efficiency… as long as programs are consistent. Where possible, a CLI should follow patterns that already exist. That’s what makes CLIs intuitive and guessable; that’s what makes users efficient. That being said, sometimes consistency conflicts with ease of use. For example, many long-established UNIX commands don’t output much information by default, which can cause confusion or worry for people less familiar with the command line. When following convention would compromise a program’s usability, it might be time to break with it—but such a decision should be made with care. Saying (just) enough The terminal is a world of pure information. You could make an argument that information is the interface—and that, just like with any interface, there’s often too much or too little of it. A command is saying too little when it hangs for several minutes and the user starts to wonder if it’s broken. A command is saying too much when it dumps pages and pages of debugging output, drowning what’s truly important in an ocean of loose detritus. The end result is the same: a lack of clarity, leaving the user confused and irritated. It can be very difficult to get this balance right, but it’s absolutely crucial if software is to empower and serve its users. Ease of discovery When it comes to making functionality discoverable, GUIs have the upper hand. Everything you can do is laid out in front of you on the screen, so you can find what you need without having to learn anything, and perhaps even discover things you didn’t know were possible. It is assumed that command-line interfaces are the opposite of this—that you have to remember how to do everything. The original Macintosh Human Interface Guidelines, published in 1987, recommend “See-and-point (instead of remember-and-type),” as if you could only choose one or the other. These things needn’t be mutually exclusive. The efficiency of using the command-line comes from remembering commands, but there’s no reason the commands can’t help you learn and remember. Discoverable CLIs have comprehensive help texts, provide lots of examples, suggest what command to run next, suggest what to do when there is an error. There are lots of ideas that can be stolen from GUIs to make CLIs easier to learn and use, even for power users. Citation: The Design of Everyday Things (Don Norman), Macintosh Human Interface Guidelines Conversation as the norm GUI design, particularly in its early days, made heavy use of metaphor: desktops, files, folders, recycle bins. It made a lot of sense, because computers were still trying to bootstrap themselves into legitimacy. The ease of implementation of metaphors was one of the huge advantages GUIs wielded over CLIs. Ironically, though, the CLI has embodied an accidental metaphor all along: it’s a conversation. Beyond the most utterly simple commands, running a program usually involves more than one invocation. Usually, this is because it’s hard to get it right the first time: the user types a command, gets an error, changes the command, gets a different error, and so on, until it works. This mode of learning through repeated failure is like a conversation the user is having with the program. Trial-and-error isn’t the only type of conversational interaction, though. There are others: Running one command to set up a tool and then learning what commands to run to actually start using it. Running several commands to set up an operation, and then a final command to run it (e.g. multiple git adds, followed by a git commit). Exploring a system—for example, doing a lot of cd and ls to get a sense of a directory structure, or git log and git show to explore the history of a file. Doing a dry-run of a complex operation before running it for real. Acknowledging the conversational nature of command-line interaction means you can bring relevant techniques to bear on its design. You can suggest possible corrections when user input is invalid, you can make the intermediate state clear when the user is going through a multi-step process, you can confirm for them that everything looks good before they do something scary. The user is conversing with your software, whether you intended it or not. At worst, it’s a hostile conversation which makes them feel stupid and resentful. At best, it’s a pleasant exchange that speeds them on their way with newfound knowledge and a feeling of achievement. Further reading: The Anti-Mac User Interface (Don Gentner and Jakob Nielsen) Robustness Robustness is both an objective and a subjective property. Software should be robust, of course: unexpected input should be handled gracefully, operations should be idempotent where possible, and so on. But it should also feel robust. You want your software to feel like it isn’t going to fall apart. You want it to feel immediate and responsive, as if it were a big mechanical machine, not a flimsy plastic “soft switch.” Subjective robustness requires attention to detail and thinking hard about what can go wrong. It’s lots of little things: keeping the user informed about what’s happening, explaining what common errors mean, not printing scary-looking stack traces. As a general rule, robustness can also come from keeping it simple. Lots of special cases and complex code tend to make a program fragile. Empathy Command-line tools are a programmer’s creative toolkit, so they should be enjoyable to use. This doesn’t mean turning them into a video game, or using lots of emoji (though there’s nothing inherently wrong with emoji 😉). It means giving the user the feeling that you are on their side, that you want them to succeed, that you have thought carefully about their problems and how to solve them. There’s no list of actions you can take that will ensure they feel this way, although we hope that following our advice will take you some of the way there. Delighting the user means exceeding their expectations at every turn, and that starts with empathy. Chaos The world of the terminal is a mess. Inconsistencies are everywhere, slowing us down and making us second-guess ourselves. Yet it’s undeniable that this chaos has been a source of power. The terminal, like the UNIX-descended computing environment in general, places very few constraints on what you can build. In that space, all manner of invention has bloomed. It’s ironic that this document implores you to follow existing patterns, right alongside advice that contradicts decades of command-line tradition. We’re just as guilty of breaking the rules as anyone. The time might come when you, too, have to break the rules. Do so with intention and clarity of purpose. “Abandon a standard when it is demonstrably harmful to productivity or user satisfaction.” — Jef Raskin, The Humane Interface Guidelines This is a collection of specific things you can do to make your command-line program better. The first section contains the essential things you need to follow. Get these wrong, and your program will be either hard to use or a bad CLI citizen. The rest are nice-to-haves. If you have the time and energy to add these things, your program will be a lot better than the average program. The idea is that, if you don’t want to think too hard about the design of your program, you don’t have to: just follow these rules and your program will probably be good. On the other hand, if you’ve thought about it and determined that a rule is wrong for your program, that’s fine. (There’s no central authority that will reject your program for not following arbitrary rules.) Also—these rules aren’t written in stone. If you disagree with a general rule for good reason, we hope you’ll propose a change. The Basics There are a few basic rules you need to follow. Get these wrong, and your program will be either very hard to use, or flat-out broken. Use a command-line argument parsing library where you can. Either your language’s built-in one, or a good third-party one. They will normally handle arguments, flag parsing, help text, and even spelling suggestions in a sensible way. Here are some that we like: Multi-platform: docopt Bash: argbash Go: Cobra, cli Haskell: optparse-applicative Java: picocli Node: oclif Deno: flags Perl: Getopt::Long PHP: console, CLImate Python: Argparse, Click, Typer Ruby: TTY Rust: clap, structopt Swift: swift-argument-parser Return zero exit code on success, non-zero on failure. Exit codes are how scripts determine whether a program succeeded or failed, so you should report this correctly. Map the non-zero exit codes to the most important failure modes. Send output to stdout. The primary output for your command should go to stdout. Anything that is machine readable should also go to stdout—this is where piping sends things by default. Send messaging to stderr. Log messages, errors, and so on should all be sent to stderr. This means that when commands are piped together, these messages are displayed to the user and not fed into the next command. Help Display help text when passed no options, the -h flag, or the --help flag. Display a concise help text by default. If you can, display help by default when myapp or myapp subcommand is run. Unless your program is very simple and does something obvious by default (e.g. ls), or your program reads input interactively (e.g. cat). The concise help text should only include: A description of what your program does. One or two example invocations. Descriptions of flags, unless there are lots of them. An instruction to pass the --help flag for more information. jq does this well. When you type jq, it displays an introductory description and an example, then prompts you to pass jq --help for the full listing of flags: $ jq jq - commandline JSON processor [version 1.6] Usage: jq [options][file...] jq [options] --args[strings...] jq [options] --jsonargs[JSON_TEXTS...] jq is a tool for processing JSON inputs, applying the given filter to its JSON text inputs and producing the filter's results as JSON on standard output. The simplest filter is ., which copies jq's input to its output unmodified (except for formatting, but note that IEEE754 is used for number representation internally, with all that that implies). For more advanced filters see the jq(1) manpage (\"man jq\") and/or https://stedolan.github.io/jq Example: $ echo '{\"foo\": 0}'jq . { \"foo\": 0 } For a listing of options, use jq --help. Show full help when -h and --help is passed. All of these should show help: $ myapp $ myapp --help $ myapp -h Ignore any other flags and arguments that are passed—you should be able to add -h to the end of anything and it should show help. Don’t overload -h. If your program is git-like, the following should also offer help: $ myapp help $ myapp help subcommand $ myapp subcommand --help $ myapp subcommand -h Provide a support path for feedback and issues. A website or GitHub link in the top-level help text is common. In help text, link to the web version of the documentation. If you have a specific page or anchor for a subcommand, link directly to that. This is particularly useful if there is more detailed documentation on the web, or further reading that might explain the behavior of something. Lead with examples. Users tend to use examples over other forms of documentation, so show them first in the help page, particularly the common complex uses. If it helps explain what it’s doing and it isn’t too long, show the actual output too. You can tell a story with a series of examples, building your way toward complex uses. If you’ve got loads of examples, put them somewhere else, in a cheat sheet command or a web page. It’s useful to have exhaustive, advanced examples, but you don’t want to make your help text really long. For more complex use cases, e.g. when integrating with another tool, it might be appropriate to write a fully-fledged tutorial. Display the most common flags and commands at the start of the help text. It’s fine to have lots of flags, but if you’ve got some really common ones, display them first. For example, the Git command displays the commands for getting started and the most commonly used subcommands first: $ git usage: git [--version] [--help] [-C ] [-c =] [--exec-path[=]] [--html-path] [--man-path] [--info-path] [-p--paginate-P--no-pager] [--no-replace-objects] [--bare] [--git-dir=] [--work-tree=] [--namespace=][] These are common Git commands used in various situations: start a working area (see also: git help tutorial) clone Clone a repository into a new directory init Create an empty Git repository or reinitialize an existing one work on the current change (see also: git help everyday) add Add file contents to the index mv Move or rename a file, a directory, or a symlink reset Reset current HEAD to the specified state rm Remove files from the working tree and from the index examine the history and state (see also: git help revisions) bisect Use binary search to find the commit that introduced a bug grep Print lines matching a pattern log Show commit logs show Show various types of objects status Show the working tree status … Use formatting in your help text. Bold headings make it much easier to scan. But, try to do it in a terminal-independent way so that your users aren’t staring down a wall of escape characters. $ heroku apps --help list your apps USAGE $ heroku apps OPTIONS -A, --all include apps in all teams -p, --personal list apps in personal account when a default team is set -s, --space=space filter by space -t, --team=team team to use --json output in json format EXAMPLES $ heroku apps === My Apps example example2 === Collaborated Apps theirapp other@owner.name COMMANDS apps:create creates a new app apps:destroy permanently destroy an app apps:errors view app errors apps:favorites list favorited apps apps:info show detailed app information apps:join add yourself to a team app apps:leave remove yourself from a team app apps:lock prevent team members from joining an app apps:open open the app in a web browser apps:rename rename an app apps:stacks show the list of available stacks apps:transfer transfer applications to another user or team apps:unlock unlock an app so any team member can join Note: When heroku apps --help is piped through a pager, the command emits no escape characters. If the user did something wrong and you can guess what they meant, suggest it. For example, brew update jq tells you that you should run brew upgrade jq. You can ask if they want to run the suggested command, but don’t force it on them. For example: $ heroku pss › Warning: pss is not a heroku command. Did you mean ps? [y/n]: Rather than suggesting the corrected syntax, you might be tempted to just run it for them, as if they’d typed it right in the first place. Sometimes this is the right thing to do, but not always. Firstly, invalid input doesn’t necessarily imply a simple typo—it can often mean the user has made a logical mistake, or misused a shell variable. Assuming what they meant can be dangerous, especially if the resulting action modifies state. Secondly, be aware that if you change what the user typed, they won’t learn the correct syntax. In effect, you’re ruling that the way they typed it is valid and correct, and you’re committing to supporting that indefinitely. Be intentional in making that decision, and document both syntaxes. Further reading: “Do What I Mean” If your command is expecting to have something piped to it and stdin is an interactive terminal, display help immediately and quit. This means it doesn’t just hang, like cat. Alternatively, you could print a log message to stderr. Documentation The purpose of help text is to give a brief, immediate sense of what your tool is, what options are available, and how to perform the most common tasks. Documentation, on the other hand, is where you go into full detail. It’s where people go to understand what your tool is for, what it isn’t for, how it works and how to do everything they might need to do. Provide web-based documentation. People need to be able to search online for your tool’s documentation, and to link other people to specific parts. The web is the most inclusive documentation format available. Provide terminal-based documentation. Documentation in the terminal has several nice properties: it’s fast to access, it stays in sync with the specific installed version of the tool, and it works without an internet connection. Consider providing man pages. man pages, Unix’s original system of documentation, are still in use today, and many users will reflexively check man mycmd as a first step when trying to learn about your tool. To make them easier to generate, you can use a tool like ronn (which can also generate your web docs). However, not everyone knows about man, and it doesn’t run on all platforms, so you should also make sure your terminal docs are accessible via your tool itself. For example, git and npm make their man pages accessible via the help subcommand, so npm help ls is equivalent to man npm-ls. NPM-LS(1)NPM-LS(1) NAME npm-ls - List installed packages SYNOPSIS npm ls [[/] ...] aliases: list, la, ll DESCRIPTION This command will print to stdout all the versions of packages that are installed, as well as their dependencies, in a tree-structure. ... Output Human-readable output is paramount. Humans come first, machines second. The most simple and straightforward heuristic for whether a particular output stream (stdout or stderr) is being read by a human is whether or not it’s a TTY. Whatever language you’re using, it will have a utility or library for doing this (e.g. Python, Node, Go). Further reading on what a TTY is. Have machine-readable output where it does not impact usability. Streams of text is the universal interface in UNIX. Programs typically output lines of text, and programs typically expect lines of text as input, therefore you can compose multiple programs together. This is normally done to make it possible to write scripts, but it can also help the usability for humans using programs. For example, a user should be able to pipe output to grep and it should do what they expect. “Expect the output of every program to become the input to another, as yet unknown, program.” — Doug McIlroy If human-readable output breaks machine-readable output, use --plain to display output in plain, tabular text format for integration with tools like grep or awk. In some cases, you might need to output information in a different way to make it human-readable. For example, if you are displaying a line-based table, you might choose to split a cell into multiple lines, fitting in more information while keeping it within the width of the screen. This breaks the expected behavior of there being one piece of data per line, so you should provide a --plain flag for scripts, which disables all such manipulation and outputs one record per line. Display output as formatted JSON if --json is passed. JSON allows for more structure than plain text, so it makes it much easier to output and handle complex data structures. jq is a common tool for working with JSON on the command-line, and there is now a whole ecosystem of tools that output and manipulate JSON. It is also widely used on the web, so by using JSON as the input and output of programs, you can pipe directly to and from web services using curl. Display output on success, but keep it brief. Traditionally, when nothing is wrong, UNIX commands display no output to the user. This makes sense when they’re being used in scripts, but can make commands appear to be hanging or broken when used by humans. For example, cp will not print anything, even if it takes a long time. It’s rare that printing nothing at all is the best default behavior, but it’s usually best to err on the side of less. For instances where you do want no output (for example, when used in shell scripts), to avoid clumsy redirection of stderr to /dev/null, you can provide a -q option to suppress all non-essential output. If you change state, tell the user. When a command changes the state of a system, it’s especially valuable to explain what has just happened, so the user can model the state of the system in their head—particularly if the result doesn’t directly map to what the user requested. For example, git push tells you exactly what it is doing, and what the new state of the remote branch is: $ git push Enumerating objects: 18, done. Counting objects: 100% (18/18), done. Delta compression using up to 8 threads Compressing objects: 100% (10/10), done. Writing objects: 100% (10/10), 2.09 KiB2.09 MiB/s, done. Total 10 (delta 8), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (8/8), completed with 8 local objects. To github.com:replicate/replicate.git + 6c22c90...a2a5217 bfirsh/fix-delete -> bfirsh/fix-delete Make it easy to see the current state of the system. If your program does a lot of complex state changes and it is not immediately visible in the filesystem, make sure you make this easy to view. For example, git status tells you as much information as possible about the current state of your Git repository, and some hints at how to modify the state: $ git status On branch bfirsh/fix-delete Your branch is up to date with 'origin/bfirsh/fix-delete'. Changes not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git restore ...\" to discard changes in working directory)modified: cli/pkg/cli/rm.go no changes added to commit (use \"git add\" and/or \"git commit -a\") Suggest commands the user should run. When several commands form a workflow, suggesting to the user commands they can run next helps them learn how to use your program and discover new functionality. For example, in the git status output above, it suggests commands you can run to modify the state you are viewing. Actions crossing the boundary of the program’s internal world should usually be explicit. This includes things like: Reading or writing files that the user didn’t explicitly pass as arguments (unless those files are storing internal program state, such as a cache). Talking to a remote server, e.g. to download a file. Increase information density—with ASCII art! For example, ls shows permissions in a scannable way. When you first see it, you can ignore most of the information. Then, as you learn how it works, you pick out more patterns over time. -rw-r--r-- 1 root root 68 Aug 22 23:20 resolv.conf lrwxrwxrwx 1 root root 13 Mar 14 20:24 rmt -> /usr/sbin/rmt drwxr-xr-x 4 root root 4.0K Jul 20 14:51 security drwxr-xr-x 2 root root 4.0K Jul 20 14:53 selinux -rw-r----- 1 root shadow 501 Jul 20 14:44 shadow -rw-r--r-- 1 root root 116 Jul 20 14:43 shells drwxr-xr-x 2 root root 4.0K Jul 20 14:57 skel -rw-r--r-- 1 root root 0 Jul 20 14:43 subgid -rw-r--r-- 1 root root 0 Jul 20 14:43 subuid Use color with intention. For example, you might want to highlight some text so the user notices it, or use red to indicate an error. Don’t overuse it—if everything is a different color, then the color means nothing and only makes it harder to read. Disable color if your program is not in a terminal or the user requested it. These things should disable colors: stdout or stderr is not an interactive terminal (a TTY). It’s best to individually check—if you’re piping stdout to another program, it’s still useful to get colors on stderr. The NO_COLOR environment variable is set. The TERM environment variable has the value dumb. The user passes the option --no-color. You may also want to add a MYAPP_NO_COLOR environment variable in case users want to disable color specifically for your program. Further reading: no-color.org, 12 Factor CLI Apps If stdout is not an interactive terminal, don’t display any animations. This will stop progress bars turning into Christmas trees in CI log output. Use symbols and emoji where it makes things clearer. Pictures can be better than words if you need to make several things distinct, catch the user’s attention, or just add a bit of character. Be careful, though—it can be easy to overdo it and make your program look cluttered or feel like a toy. For example, yubikey-agent uses emoji to add structure to the output so it isn’t just a wall of text, and a ❌ to draw your attention to an important piece of information: $ yubikey-agent -setup 🔐 The PIN is up to 8 numbers, letters, or symbols. Not just numbers! ❌ The key will be lost if the PIN and PUK are locked after 3 incorrect tries. Choose a new PIN/PUK: Repeat the PIN/PUK: 🧪 Retriculating splines … ✅ Done! This YubiKey is secured and ready to go. 🤏 When the YubiKey blinks, touch it to authorize the login. 🔑 Here's your new shiny SSH public key: ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBCEJ/ UwlHnUFXgENO3ifPZd8zoSKMxESxxot4tMgvfXjmRp5G3BGrAnonncE7Aj11pn3SSYgEcrrn2sMyLGpVS0= 💭 Remember: everything breaks, have a backup plan for when this YubiKey does. By default, don’t output information that’s only understandable by the creators of the software. If a piece of output serves only to help you (the developer) understand what your software is doing, it almost certainly shouldn’t be displayed to normal users by default—only in verbose mode. Invite usability feedback from outsiders and people who are new to your project. They’ll help you see important issues that you are too close to the code to notice. Don’t treat stderr like a log file, at least not by default. Don’t print log level labels (ERR, WARN, etc.) or extraneous contextual information, unless in verbose mode. Use a pager (e.g. less) if you are outputting a lot of text. For example, git diff does this by default. Using a pager can be error-prone, so be careful with your implementation such that you don’t make the experience worse for the user. You shouldn’t use a pager if stdin or stdout is not an interactive terminal. A good sensible set of options to use for less is less -FIRX. This does not page if the content fills one screen, ignores case when you search, enables color and formatting, and leaves the contents on the screen when less quits. There might be libraries in your language that are more robust than piping to less. For example, pypager in Python. Errors One of the most common reasons to consult documentation is to fix errors. If you can make errors into documentation, then this will save the user loads of time. Catch errors and rewrite them for humans. If you’re expecting an error to happen, catch it and rewrite the error message to be useful. Think of it like a conversation, where the user has done something wrong and the program is guiding them in the right direction. Example: “Can’t write to file.txt. You might need to make it writable by running ‘chmod +w file.txt’.” Signal-to-noise ratio is crucial. The more irrelevant output you produce, the longer it’s going to take the user to figure out what they did wrong. If your program produces multiple errors of the same type, consider grouping them under a single explanatory header instead of printing many similar-looking lines. Consider where the user will look first. Put the most important information at the end of the output. The eye will be drawn to red text, so use it intentionally and sparingly. If there is an unexpected or unexplainable error, provide debug and traceback information, and instructions on how to submit a bug. That said, don’t forget about the signal-to-noise ratio: you don’t want to overwhelm the user with information they don’t understand. Consider writing the debug log to a file instead of printing it to the terminal. Make it effortless to submit bug reports. One nice thing you can do is provide a URL and have it pre-populate as much information as possible. Arguments and flags A note on terminology: Arguments, or args, are positional parameters to a command. For example, the file paths you provide to cp are args. The order of args is often important: cp foo bar means something different from cp bar foo. Flags are named parameters, denoted with either a hyphen and a single-letter name (-r) or a double hyphen and a multiple-letter name (--recursive). They may or may not also include a user-specified value (--file foo.txt, or --file=foo.txt). The order of flags, generally speaking, does not affect program semantics. Prefer flags to args. It’s a bit more typing, but it makes it much clearer what is going on. It also makes it easier to make changes to how you accept input in the future. Sometimes when using args, it’s impossible to add new input without breaking existing behavior or creating ambiguity. Citation: 12 Factor CLI Apps. Have full-length versions of all flags. For example, have both -h and --help. Having the full version is useful in scripts where you want to be verbose and descriptive, and you don’t have to look up the meaning of flags everywhere. Citation: GNU Coding Standards. Only use one-letter flags for commonly used flags, particularly at the top-level when using subcommands. That way you don’t “pollute” your namespace of short flags, forcing you to use convoluted letters and cases for flags you add in the future. Multiple arguments are fine for simple actions against multiple files. For example, rm file1.txt file2.txt file3.txt. This also makes it work with globbing: rm *.txt. If you’ve got two or more arguments for different things, you’re probably doing something wrong. The exception is a common, primary action, where the brevity is worth memorizing. For example, cp. Citation: 12 Factor CLI Apps. Use standard names for flags, if there is a standard. If another commonly used command uses a flag name, it’s best to follow that existing pattern. That way, a user doesn’t have to remember two different options (and which command it applies to), and users can even guess an option without having to look at the help text. Here’s a list of commonly used options: -a, --all: All. For example, ps, fetchmail. -d, --debug: Show debugging output. -f, --force: Force. For example, rm -f will force the removal of files, even if it thinks it does not have permission to do it. This is also useful for commands which are doing something destructive that usually require user confirmation, but you want to force it to do that destructive action in a script. --json: Display JSON output. See the output section. -h, --help: Help. This should only mean help. See the help section. --no-input: See the interactivity section. -o, --output: Output file. For example, sort, gcc. -p, --port: Port. For example, psql, ssh. -q, --quiet: Quiet. Display less output. This is particularly useful when displaying output for humans that you might want to hide when running in a script. -u, --user: User. For example, ps, ssh. --version: Version. -v: This can often mean either verbose or version. You might want to use -d for verbose and this for version, or for nothing to avoid confusion. Make the default the right thing for most users. Making things configurable is good, but most users are not going to find the right flag and remember to use it all the time (or alias it). If it’s not the default, you’re making the experience worse for most of your users. For example, ls has terse default output to optimize for scripts and other historical reasons, but if it were designed today, it would probably default to ls -lhF. Prompt for user input. If a user doesn’t pass an argument or flag, prompt for it. (See also: Interactivity) Never require a prompt. Always provide a way of passing input with flags or arguments. If stdin is not an interactive terminal, skip prompting and just require those flags/args. Confirm before doing anything dangerous. A common convention is to prompt for the user to type y or yes if running interactively, or requiring them to pass -f or --force otherwise. “Dangerous” is a subjective term, and there are differing levels of danger: Mild: A small, local change such as deleting a file. You might want to prompt for confirmation, you might not. For example, if the user is explicitly running a command called something like “delete,” you probably don’t need to ask. Moderate: A bigger local change like deleting a directory, a remote change like deleting a resource of some kind, or a complex bulk modification that can’t be easily undone. You usually want to prompt for confirmation here. Consider giving the user a way to “dry run” the operation so they can see what’ll happen before they commit to it. Severe: Deleting something complex, like an entire remote application or server. You don’t just want to prompt for confirmation here—you want to make it hard to confirm by accident. Consider asking them to type something non-trivial such as the name of the thing they’re deleting. Let them alternatively pass a flag such as --confirm=\"name-of-thing\", so it’s still scriptable. Consider whether there are non-obvious ways to accidentally destroy things. For example, imagine a situation where changing a number in a configuration file from 10 to 1 means that 9 things will be implicitly deleted—this should be considered a severe risk, and should be difficult to do by accident. If input or output is a file, support - to read from stdin or write to stdout. This lets the output of another command be the input of your command and vice versa, without using a temporary file. For example, tar can extract files from stdin: $ curl https://example.com/something.tar.gztar xvf - If a flag can accept an optional value, allow a special word like “none.” For example, ssh -F takes an optional filename of an alternative ssh_config file, and ssh -F none runs SSH with no config file. Don’t just use a blank value—this can make it ambiguous whether arguments are flag values or arguments. If possible, make arguments, flags and subcommands order-independent. A lot of CLIs, especially those with subcommands, have unspoken rules on where you can put various arguments. For example a command might have a --foo flag that only works if you put it before the subcommand: mycmd --foo=1 subcmd works $ mycmd subcmd --foo=1 unknown flag: --foo This can be very confusing for the user—especially given that one of the most common things users do when trying to get a command to work is to hit the up arrow to get the last invocation, stick another option on the end, and run it again. If possible, try to make both forms equivalent, although you might run up against the limitations of your argument parser. Do not read secrets directly from flags. When a command accepts a secret, eg. via a --password argument, the argument value will leak the secret into ps output and potentially shell history. And, this sort of flag encourages the use of insecure environment variables for secrets. Consider accepting sensitive data only via files, e.g. with a --password-file argument, or via stdin. A --password-file argument allows a secret to be passed in discreetly, in a wide variety of contexts. (It’s possible to pass a file’s contents into an argument in Bash by using --password $( ] 7.569MB/7.812MB c0afb8e68e0b: Download complete d599c07d28e6: Download complete f2ecc74db11a: Downloading [=======================> ] 89.11MB/192.3MB 3568445c8bf2: Download complete b0efebc74f25: Downloading [===========================================> ] 19.88MB/22.88MB 9cb1ba6838a0: Download complete One thing to be aware of: hiding logs behind progress bars when things go well makes it much easier for the user to understand what’s going on, but if there is an error, make sure you print out the logs. Otherwise, it will be very hard to debug. Make things time out. Allow network timeouts to be configured, and have a reasonable default so it doesn’t hang forever. Make it recoverable. If the program fails for some transient reason (e.g. the internet connection went down), you should be able to hitandand it should pick up from where it left off. Make it crash-only. This is the next step up from idempotence. If you can avoid needing to do any cleanup after operations, or you can defer that cleanup to the next run, your program can exit immediately on failure or interruption. This makes it both more robust and more responsive. Citation: Crash-only software: More than meets the eye. People are going to misuse your program. Be prepared for that. They will wrap it in scripts, use it on bad internet connections, run many instances of it at once, and use it in environments you haven’t tested in, with quirks you didn’t anticipate. (Did you know macOS filesystems are case-insensitive but also case-preserving?) Future-proofing In software of any kind, it’s crucial that interfaces don’t change without a lengthy and well-documented deprecation process. Subcommands, arguments, flags, configuration files, environment variables: these are all interfaces, and you’re committing to keeping them working. (Semantic versioning can only excuse so much change; if you’re putting out a major version bump every month, it’s meaningless.) Keep changes additive where you can. Rather than modify the behavior of a flag in a backwards-incompatible way, maybe you can add a new flag—as long as it doesn’t bloat the interface too much. (See also: Prefer flags to args.) Warn before you make a non-additive change. Eventually, you’ll find that you can’t avoid breaking an interface. Before you do, forewarn your users in the program itself: when they pass the flag you’re looking to deprecate, tell them it’s going to change soon. Make sure there’s a way they can modify their usage today to make it future-proof, and tell them how to do it. If possible, you should detect when they’ve changed their usage and not show the warning any more: now they won’t notice a thing when you finally roll out the change. Changing output for humans is usually OK. The only way to make an interface easy to use is to iterate on it, and if the output is considered an interface, then you can’t iterate on it. Encourage your users to use --plain or --json in scripts to keep output stable (see Output). Don’t have a catch-all subcommand. If you have a subcommand that’s likely to be the most-used one, you might be tempted to let people omit it entirely for brevity’s sake. For example, say you have a run command that wraps an arbitrary shell command: $ mycmd run echo \"hello world\" You could make it so that if the first argument to mycmd isn’t the name of an existing subcommand, you assume the user means run, so they can just type this: $ mycmd echo \"hello world\" This has a serious drawback, though: now you can never add a subcommand named echo—or anything at all—without risking breaking existing usages. If there’s a script out there that uses mycmd echo, it will do something entirely different after that user upgrades to the new version of your tool. Don’t allow arbitrary abbreviations of subcommands. For example, say your command has an install subcommand. When you added it, you wanted to save users some typing, so you allowed them to type any non-ambiguous prefix, like mycmd ins, or even just mycmd i, and have it be an alias for mycmd install. Now you’re stuck: you can’t add any more commands beginning with i, because there are scripts out there that assume i means install. There’s nothing wrong with aliases—saving on typing is good—but they should be explicit and remain stable. Don’t create a “time bomb.” Imagine it’s 20 years from now. Will your command still run the same as it does today, or will it stop working because some external dependency on the internet has changed or is no longer maintained? The server most likely to not exist in 20 years is the one that you are maintaining right now. (But don’t build in a blocking call to Google Analytics either.) Signals and control characters If a user hits Ctrl-C (the INT signal), exit as soon as possible. Say something immediately, before you start clean-up. Add a timeout to any clean-up code so it can’t hang forever. If a user hits Ctrl-C during clean-up operations that might take a long time, skip them. Tell the user what will happen when they hit Ctrl-C again, in case it is a destructive action. For example, when quitting Docker Compose, you can hit Ctrl-C a second time to force your containers to stop immediately instead of shutting them down gracefully. $ docker-compose up … ^CGracefully stopping... (press Ctrl+C again to force) Your program should expect to be started in a situation where clean-up has not been run. (See Crash-only software: More than meets the eye.) Configuration Command-line tools have lots of different types of configuration, and lots of different ways to supply it (flags, environment variables, project-level config files). The best way to supply each piece of configuration depends on a few factors, chief among them specificity, stability and complexity. Configuration generally falls into a few categories: Likely to vary from one invocation of the command to the next. Examples: Setting the level of debugging output Enabling a safe mode or dry run of a program Recommendation: Use flags. Environment variables may or may not be useful as well. Generally stable from one invocation to the next, but not always. Might vary between projects. Definitely varies between different users working on the same project. This type of configuration is often specific to an individual computer. Examples: Providing a non-default path to items needed for a program to start Specifying how or whether color should appear in output Specifying an HTTP proxy server to route all requests through Recommendation: Use flags and probably environment variables too. Users may want to set the variables in their shell profile so they apply globally, or in .env for a particular project. If this configuration is sufficiently complex, it may warrant a configuration file of its own, but environment variables are usually good enough. Stable within a project, for all users. This is the type of configuration that belongs in version control. Files like Makefile, package.json and docker-compose.yml are all examples of this. Recommendation: Use a command-specific, version-controlled file. Follow the XDG-spec. In 2010 the X Desktop Group, now freedesktop.org, developed a specification for the location of base directories where config files may be located. One goal was to limit the proliferation of dotfiles in a user’s home directory by supporting a general-purpose ~/.config folder. The XDG Base Directory Specification (full spec, summary) is supported by yarn, fish, wireshark, emacs, neovim, tmux, and many other projects you know and love. If you automatically modify configuration that is not your program’s, ask the user for consent and tell them exactly what you’re doing. Prefer creating a new config file (e.g. /etc/cron.d/myapp) rather than appending to an existing config file (e.g. /etc/crontab). If you have to append or modify to a system-wide config file, use a dated comment in that file to delineate your additions. Apply configuration parameters in order of precedence. Here is the precedence for config parameters, from highest to lowest: Flags The running shell’s environment variables Project-level configuration (eg. .env) User-level configuration System wide configuration Environment variables Environment variables are for behavior that varies with the context in which a command is run. The “environment” of an environment variable is the terminal session—the context in which the command is running. So, an env var might change each time a command runs, or between terminal sessions on one machine, or between instantiations of one project across several machines. Environment variables may duplicate the functionality of flags or configuration parameters, or they may be distinct from those things. See Configuration for a breakdown of common types of configuration and recommendations on when environment variables are most appropriate. For maximum portability, environment variable names must only contain uppercase letters, numbers, and underscores (and mustn’t start with a number). Which means O_O and OWO are the only emoticons that are also valid environment variable names. Aim for single-line environment variable values. While multi-line values are possible, they create usability issues with the env command. Avoid commandeering widely used names. Here’s a list of POSIX standard env vars. Check general-purpose environment variables for configuration values when possible: NO_COLOR, to disable color (see Output) or FORCE_COLOR to enable it and ignore the detection logic DEBUG, to enable more verbose output EDITOR, if you need to prompt the user to edit a file or input more than a single line HTTP_PROXY, HTTPS_PROXY, ALL_PROXY and NO_PROXY, if you’re going to perform network operations (The HTTP library you’re using might already check for these.) SHELL, if you need to open up an interactive session of the user’s preferred shell (If you need to execute a shell script, use a specific interpreter like /bin/sh) TERM, TERMINFO and TERMCAP, if you’re going to use terminal-specific escape sequences TMPDIR, if you’re going to create temporary files HOME, for locating configuration files PAGER, if you want to automatically page output LINES and COLUMNS, for output that’s dependent on screen size (e.g. tables) Read environment variables from .env where appropriate. If a command defines environment variables that are unlikely to change as long as the user is working in a particular directory, then it should also read them from a local .env file so users can configure it differently for different projects without having to specify them every time. Many languages have libraries for reading .env files (Rust, Node, Ruby). Don’t use .env as a substitute for a proper configuration file. .env files have a lot of limitations: A .env file is not commonly stored in source control (Therefore, any configuration stored in it has no history) It has only one data type: string It lends itself to being poorly organized It makes encoding issues easy to introduce It often contains sensitive credentials & key material that would be better stored more securely If it seems like these limitations will hamper usability or security, then a dedicated config file might be more appropriate. Do not read secrets from environment variables. While environment variables may be convenient for storing secrets, they have proven too prone to leakage: Exported environment variables are sent to every process, and from there can easily leak into logs or be exfiltrated Shell substitutions like curl -H \"Authorization: Bearer $BEARER_TOKEN\" will leak into globally-readable process state. (cURL offers the -H @filename alternative for reading sensitive headers from a file.) Docker container environment variables can be viewed by anyone with Docker daemon access via docker inspect Environment variables in systemd units are globally readable via systemctl show Secrets should only be accepted via credential files, pipes, AF_UNIX sockets, secret management services, or another IPC mechanism. Naming “Note the obsessive use of abbreviations and avoidance of capital letters; [Unix] is a system invented by people to whom repetitive stress disorder is what black lung is to miners. Long names get worn down to three-letter nubbins, like stones smoothed by a river.” — Neal Stephenson, In the Beginning was the Command Line The name of your program is particularly important on the CLI: your users will be typing it all the time, and it needs to be easy to remember and type. Make it a simple, memorable word. But not too generic, or you’ll step on the toes of other commands and confuse users. For example, both ImageMagick and Windows used the command convert. Use only lowercase letters, and dashes if you really need to. curl is a good name, DownloadURL is not. Keep it short. Users will be typing it all the time. Don’t make it too short: the very shortest commands are best reserved for the common utilities used all the time, such as cd, ls, ps. Make it easy to type. If you expect people to type your command name all day, make it easy on their hands. A real-world example: long before Docker Compose was docker compose, it was plum. This turned out to be such an awkward, one-handed hopscotch that it was immediately renamed to fig, which – as well as being shorter – flows much more easily. Further reading: The Poetics of CLI Command Names Distribution If possible, distribute as a single binary. If your language doesn’t compile to binary executables as standard, see if it has something like PyInstaller. If you really can’t distribute as a single binary, use the platform’s native package installer so you aren’t scattering things on disk that can’t easily be removed. Tread lightly on the user’s computer. If you’re making a language-specific tool, such as a code linter, then this rule doesn’t apply—it’s safe to assume the user has an interpreter for that language installed on their computer. Make it easy to uninstall. If it needs instructions, put them at the bottom of the install instructions—one of the most common times people want to uninstall software is right after installing it. Analytics Usage metrics can be helpful to understand how users are using your program, how to make it better, and where to focus effort. But, unlike websites, users of the command-line expect to be in control of their environment, and it is surprising when programs do things in the background without telling them. Do not phone home usage or crash data without consent. Users will find out, and they will be angry. Be very explicit about what you collect, why you collect it, how anonymous it is and how you go about anonymizing it, and how long you retain it for. Ideally, ask users whether they want to contribute data (“opt-in”). If you choose to do it by default (“opt-out”), then clearly tell users about it on your website or first run, and make it easy to disable. Examples of projects that collect usage statistics: Angular.js collects detailed analytics using Google Analytics, in the name of feature prioritization. You have to explicitly opt in. You can change the tracking ID to point to your own Google Analytics property if you want to track Angular usage inside your organization. Homebrew sends metrics to Google Analytics and has a nice FAQ detailing their practices. Next.js collects anonymized usage statistics and is enabled by default. Consider alternatives to collecting analytics. Instrument your web docs. If you want to know how people are using your CLI tool, make a set of docs around the use cases you’d like to understand best, and see how they perform over time. Look at what people search for within your docs. Instrument your downloads. This can be a rough metric to understand usage and what operating systems your users are running. Talk to your users. Reach out and ask people how they’re using your tool. Encourage feedback and feature requests in your docs and repos, and try to draw out more context from those who submit feedback. Further reading: Open Source Metrics Further reading The Unix Programming Environment, Brian W. Kernighan and Rob Pike POSIX Utility Conventions Program Behavior for All Programs, GNU Coding Standards 12 Factor CLI Apps, Jeff Dickey CLI Style Guide, Heroku",
    "commentLink": "https://news.ycombinator.com/item?id=39273932",
    "commentBody": "Command line interface guidelines (2021) (clig.dev)325 points by petercooper 20 hours agohidepastfavorite170 comments enriquto 19 hours ago> Most people today don’t know what the command line is, much less why they would want to bother with it. This is true today, and it was true as well \"in the 1980s\", to use the same time frame as TFA. The difference is that today there are more people than ever who know what the command line is, and who can use it. At least an order of magnitude more people; maybe two. We can certainly say that we live in the CLI golden age! reply hinkley 15 hours agoparentI’ve been writing command lines for parts of our app to help split up the monolith. Global state and dependencies thwart reasoning and thus debugging and performance optimization. Splitting out 500, 1000, 5,000, 10,000 or sometimes 50k lines of code to work separately can clarify a lot of things. If done right it can also encourage Functional Core, Imperative Shell, because a sensible Unix-philosophy command line needs lots of actions without side effects and a few with. You can write a little command that generates and dumps out the system state just before a (bad) decision is made, and do so against production systems with virtual impunity. And that means you can hand these tools to someone who you need to become part of a bus number, even if they are otherwise hesitant to do so. reply mvdtnz 14 hours agoparentprevIf you substitute \"people\" with \"computer users\" (which is obviously implied, despite your pedantry) then the author's jist is correct. What a villager in a remote undiscovered Amazonian civilisation thinks of the terminal is not relevant. reply atoav 16 hours agoparentprevAbsolute vs relative numbers. If you want to judge the shift in quality in a thing that grew in quantity, then you should look at percentages — otherwise you end up creating statements that are true, but don't say anything meaningful. E.g. there are probably more absolute listeners of Jazz music today than back at the height of the cultural bloom of the genre. But that isn't because Jazz is more popular today than it once was, but because there are more absolute listeners of any kind of music. Would you say that Jazz in the US is now more important, influential etc. than it was at its peak? reply hiAndrewQuinn 14 hours agorootparentNo, but there's so much great jazz these days that wouldn't have existed without those absolute numbers. Quantity (aggregate supply) is a quality (increases specialization of labor) all its own! reply atoav 9 hours agorootparentYeah, sure. But I hope you are aware your argument is dishonest? If you want to compare two times, looking at absolute numbers isn't useful unless those numbers contributed to the rise or fall of the phenomenon we are observing. Whether from that quantity new qualities emerged is a different question. E.g. the new quantity of diverse jazz listeners probably has lead to an explosion of new sub-genres, that might have never emerged otherwise. Still, just because there is more different Jazz now does not mean Jazz has become more relevant overall. This was the factor we discussed btw. reply nerdponx 17 hours agoparentprevWas that true as a % of computer users in the 80s? reply enriquto 16 hours agorootparentI guess not, but why does it matter? Let people enjoy other things. reply pimlottc 18 hours agoprevPlease also consider a --dry-run option that gives a preview of what actions would be taken without actually making any changes. This is really helpful for learning a tool and making sure you got complex options and fileglobs correct before committing to potentially irreversible changes. reply epage 16 hours agoparentOr if the command can't be undone and with significant side effects, be `--dry-run` by default and have an `--execute` flag reply BD103 15 hours agorootparentExactly. This is implemented by the Javascript code formatter Prettier [1] where you have to pass `--write` in order to overwrite an unformatted file. [1]: https://prettier.io reply bongodongobob 16 hours agoparentprevWhatif is one of my favorite things about powershell. Don't know what I'd do without it. Has saved me many times from completely destroying things. reply ojintoad 15 hours agorootparentAgree! I'll go further. PowerShell covers a lot of the concerns in the OP out of the box. It is extremely well thought out and is one of my favorite cli models to buy into. reply bigstrat2003 14 hours agorootparentPowershell is just plain a great shell. Blows all the *sh variants out of the water. I'd love for it to gain traction in Linux (so that, for example I could use it as my shell on my desktop) but I don't really see that happening. reply snuxoll 14 hours agorootparentprevWhatIf in Powershell also remains completely broken; not that it doesn't work, but it is not properly passed down the call stack like it's supposed to from cmdlets or functions written in PowerShell (as opposed to those loaded from .net assemblies). reply jcotton42 14 hours agorootparentWhere have you seen this? If the function does it properly (SupportsShouldProcess) then it should pass down automatically. reply danstewart_ 13 hours agoparentprevI tend to go the opposite way and have the default behaviour not actually make any changes and require passing `--commit` to actually do something. I feel it’s safer for scripts that have irreversible (or difficult to reverse) actions. reply enriquto 12 hours agorootparentEven safer: your program does never perform any actual deed. It just prints commands that you can run afterwards, using an external program, ideally a shell. This has the advantage of allowing the user to edit all the actions one by one. Instead of myprog # see what would happen myprog --commit # alright, do it You do myprog # see what would happen myprogsh # alright, do it But if you want to change something: myprog > x vi x cat xsh And if you just want to run everything in parallel: myprogparallel -j 16 reply azemetre 12 hours agoparentprevDo you have an example of a CLT that has a dry run flag? I am very confused on how to design one for some CLTs I'm making. If you call an API to retrieve data, how can that be a dry run? Are you suppose to give fake examples with fake output? reply scbrg 7 hours agorootparentapt-get. It simply tells you what it's going to remove/install. Calling an API to retrieve data is not really the type of program that requires a dry-run flag. It's mainly useful for commands that change the state of something in ways that could potentially be destructive, unwanted and/or hard to revert. reply amake 4 hours agorootparentprevThe AWS CLI has --[no]-dry-run in many of its subcommands. https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-h... reply samstave 18 hours agoparentprevdry-run should be a pipablefunction for any command; > do_thing.pydry-run -- Think of it as \"explain your work, step by step\" as one would prompt... also, its a food-for-thought you muppets. --- @jasonjmcghee ; ( $ ) . ( $ ) great justice. reply jasonjmcghee 18 hours agorootparentHow could that work? The script on the left executes first, piping its output to another command won’t prevent that reply saghm 17 hours agorootparentI think there are ways to detect if stdout is a pipe and operate differently (e.g. by using different defaults for coloring output), but I'm not sure if there are ways to detect what the other side of the pipe is, much less what `dry-run` would actually be expected to be in this case reply jasonjmcghee 17 hours agorootparentMaybe I’m misunderstanding but sounds like you’re proposing the modifying the command on the left to detect whether it’s piping it’s output (curious how you do that btw sounds pretty cool / useful) But at that point you could just handle —dry-run directly reply kergonath 17 hours agorootparent> Maybe I’m misunderstanding but sounds like you’re proposing the modifying the command on the left to detect whether it’s piping it’s output (curious how you do that btw sounds pretty cool / useful) You can do that. Possibly not from all languages but for anything that can call functions in the c standard library, that’s what isatty() is for (among other uses). It takes a file descriptor and returns whether it is a terminal or not. If you do this with stdout, this tells you whether it goes to a terminal or whether it is redirected in a way. As the parent suspects, though, this won’t tell you anything about what is on the other side of the redirection. reply jasonjmcghee 17 hours agorootparentVery cool function - thank you! reply sokoloff 16 hours agorootparentprevI often pipe output to tee and would be pretty annoyed if that changed the behavior of the original command to not do anything because stdout was a pipe. reply indymike 13 hours agorootparent> I often pipe output to tee and would be pretty annoyed if that changed the behavior of the original command The output sent to tee is usually not the same as the output from the command to the terminal, so you are getting something different than most human users expect from original command... the reason is that terminal escape codes and other formatting for humans may need to be omitted from output to a pipe. You do this by asking the OS, \"is this thing a terminal?\". Python example \"terminal\" if sys.stdout.isatty() else \"something else\" C is very similar: if (isatty (1)) fprintf (stdout, \"Terminal.\"); else fprintf (stdout, \"Not Terminal.\"); (printf works, too). reply saghm 9 hours agorootparentYou can see this pretty easily with `ls` as well by using different options for the color. If you run `ls --color=auto`, you'll get colored output when running directly but black and white output if you pipe to `less`. However, if you pass `--color=always`, you'll get colored output when running directly and a bunch of garbage around some of the entries when piping to `less` because it doesn't interpret ANSI escape codes for color by default (although depending on what the output you're piping into `less` is, there are some workarounds like https://www.gnu.org/software/src-highlite/) reply sokoloff 13 hours agorootparentprevSure; the output format changes, but the functionality doesn't. Even ls outputs a tabular format by default when it's on a terminal and a list one file/dir per line when it's not on a terminal (if it's piped to cat for example or why lswc -l correctly counts the entries). But the (essential) behavior of the command remains the same. ls still lists files/dirs... scp still copies files, etc. reply gpderetta 17 hours agorootparentprevon the other hand you could have 'dry-run ' that via .so interposition tricks could intercept and list all destructive changes done by an arbitrary , as a form of sandboxing. reply kjs3 17 hours agorootparentprevIf it operated differently based on what it was outputting to, that kinda defeats the point of 'dry run', which is to see exactly what's going to happen based on what's on the other side of the pipe when I run it for real. \"Did this blow up because it's a bad command, or because there's a bug in the 'only kinda real dry run' code?\". reply samstave 12 hours agorootparent+++ - What if there wasa 'deep-pipe' '||' which would be based on a set env/docker/blah - which would launch an env and execute your '||'d code in it, and output some log/metrics/whatever? reply int_19h 15 hours agorootparentprevHere's a possibly more interesting take on this: instead of do-thing.py, imagine if there was thing.py, which processed all that complex CLI options, and as output produced a linear shell script, with each line invoking some atomic operation, and a comment explaining which options contributed to that particular line. reply eichin 14 hours agorootparentYeah, that's a pattern I use for occasional sysadmin tools - the command itself generates the actual commands, I review them, then \"sh -xeu\". (Yes, there's no guarantee that the output is the same, so I don't use the pattern when that's a risk; it's also rarely if ever used for things I expect other people to run, just bulk operations that are saving me repetition.) reply jasonjmcghee 17 hours agorootparentprevWasn’t trying to be a prick- i was commenting “here’s what I’m seeing with your proposal and the presented obstacles, how would you overcome them?” One approach could be something like “set -x” after setting a confirmation with “trap” command. confirm_execution() { echo -n \"Execute $BASH_COMMAND? [y/N] \" read response if [[ $response != [yY] ]]; then echo \"Skipped.\" return 1 fi } trap 'confirm_execution' DEBUG set -x user_command set +x But that’s a wrapper script reply samstave 13 hours agorootparentconfirm_execution() { echo -n \"Execute $BASH_COMMAND? [y/N] \" read response if [[ $response != [yY] ]]; then echo \"Skipped.\" return 1 fi } draw_heart() { catEOF } trap 'confirm_execution' DEBUG set -x draw_heart set +x reply foobarqux 16 hours agorootparentprev\"try\" is not a prefix (like watch, nice, etc) uses an overlayfs in order to be able to see and accept or reject changes to your filesystem from a command https://github.com/binpash/try reply koolba 18 hours agorootparentprevHaving it be a shell function would work. It could create a copy-on-write file system or override the syscalls for file system access. reply agos 17 hours agorootparentnot all the destructive actions are on file system reply OJFord 15 hours agoprev> If stdout is not an interactive terminal, don’t display any animations. This will stop progress bars turning into Christmas trees in CI log output Never display animations in stdout! I quite liked TFA in general, but I was skimming around looking for where they were going to advise on the difference between stderr & stdout until I saw that and realised they weren't. stderr should be all (not just 'errors') of your logging, informational type stuff, the bits that maybe you might animate (and some people will hate) if tty, etc. stdout should be the useful output - which you may or may not have - regardless of whether tty or not, primarily because an inconsistency like that is just confusing. e.g. echo foomysed 's/oo/aa/'cat # mysed should: # stdout: faa # stderr: mysed version 1 here hellofound ooprinting aa (or whatever) I don't want to have to fight your tool with grep to get the 'actual' output lines. And I don't want to struggle to debug it because if I remove `| cat` above (as a silly example) it behaves differently than with it. reply eikenberry 15 hours agoparent> stdout should be the useful output To add a small tweak to this rule... \"stdout should be what you ask for\". If I ask for --help, that should go to stdout. If I ask for logs, they should go to stdout. If I don't ask for it, stderr. reply OJFord 14 hours agorootparentYup, absolutely, that is what I meant - the useful output from the command, the thing you were looking for, that it's actually doing/retrieving. reply samatman 15 hours agoparentprevThat's a good rule, pity about the name. If we could go back in time and make it stdin, stdout, and stdext (because UNIX so six letters, but standard_extra, or standard_extended), we might have a prayer of getting people to follow that convention. But it's called stderr, so devs think, quite reasonably, that it should be used for error reporting, and conversely, if it isn't an error, it goes in stdout. But I agree with you that this is the better way to structure a program. You might confuse more people with it, but they'll be able to do more useful things with its output, so that's a win. reply OJFord 14 hours agorootparentI think if I could go back in time, my fix would be to make stderr the default. i.e. everything written 'out' goes to stderr until you explicitly write it as an output to stdout (or another descriptor/file), the inverse of the current situation. (Ok, sure, I'd probably change the name too!) reply nerdponx 5 hours agorootparentprevPowershell has something like 5 or 6 different output streams. reply arrakeen 17 hours agoprev> Use symbols and emoji where it makes things clearer. for the love of god please don't. the yubikey-agent example provided exemplifies everything i dislike about github READMEs and whimsical user interfaces. on the technical side, symbols and emojis can render inconsistently among terminals, leading to potentially confusing messaging. on the artistic side, personal tolerances towards whimsy and playfulness vary wildly and should only be used very sparingly and ONLY if you know what you're doing (if you have to ask, you probably don't) reply samatman 14 hours agoparentI like it. I like colored terminal output, and emoji are colorful, which helps me rapidly form a gestalt of what's going on. I like syntax highlighting too, and find code quite a bit more difficult to read without it. Not everyone is like that, and that's ok. I don't expect my whims to be catered to, and you shouldn't either. reply nerdponx 17 hours agoparentprevI like it as an optional feature. Some people enjoy it, some people don't. Turn it off by default, but let users choose. reply kevindamm 14 hours agorootparentlike a --verbosity=cute level, perhaps reply Linux-Fan 15 hours agoparentprevStrongly resonates. The first time I saw the CLI Guidelines I even stopped reading them upon reaching that section. This time I read through the remainder and found it quite OK. reply candiddevmike 18 hours agoprevI get that some CLIs are absolutely huge and require nesting (like aws), but it really drives me nuts traversing nested CLIs. I'd rather most apps spit out all their options in the help and let me use less to find what I need instead of going into each level and running help. reply swozey 15 hours agoparentI've spoken about this previously, I write a huge number of TUI apps. I write every single app the have a nice TUI frontend for people who are less technical (like QA) that is purely the UI/UX IF you want to use it. What that does, in all of my apps, is pass the settings chosen in the TUI to some sort of separate generator file or function that can always be called by it's own on the terminal with all options/help that are used in the TUI. So it's scriptable and useful by multiple levels of skill. reply wjdp 17 hours agoparentprevI find it depends on the number of subcommands and if those subcommands are clear. It's rather useful to have the options whittled down to just the ones you need if you know which subcommand you need but otherwise can be painful. Grepping through a large list of options is also painful, seems ther needs to be a balance here. reply candiddevmike 17 hours agorootparentNo need to grep, just use less and vi commands like forward slash to search through things/move about a long list. reply hk__2 16 hours agorootparentIt’s the task of searching in a long list that’s painful, not the tool to do it. reply cellularmitosis 12 hours agorootparentBut we are comparing the pain of searching vs the pain of having to re-run the help command for each possible subcommand, and parsing through all of that. Much easier to just have one command invocation and search through it. There is a web analogy for this in how people organize FAQs. Some have a list of section links, and you have to click on a section to get the FAQs for that topic. Others just put everything on one giant page. Here's the problem scenario with splitting things up into section pages: You think you see the appropriate section, but then you don't see your concern answered. There are two possibilities: either the organization was counter-intuitive and your concern was answered in one of the other sections, or your concern wasn't answered anywhere. And what's the only way to be sure? Visit every single section page and search through all of them. Much, much less painful to just have it all on one page and search it. reply epage 16 hours agoparentprevSomething that can help is flattening subcommands' `--help` into their parent e.g. `git stash--help` just reports `git stash --help` which includes each `` reply frontalier 18 hours agoparentprevisn't that what man(ual) is for? reply otteromkram 15 hours agorootparentYou mean manpage? Yes. reply ceving 18 hours agoprevThe current Unix command line situation is on the one hand \"incredible useful\" and on the other hand \"broken by design\". Why is it incredible useful? Just imagine how long it would take to write the following in C or Rust: curl -sS https://go.dev/doc/devel/releasehtml2textgrep -o -P '\\bgo\\d+\\.\\d+\\.\\d+\\b'sort -Vuniqtail -1 Why is it broken by design? Read this: https://news.ycombinator.com/item?id=29747034 The problem: a command line interface must be human readable and machine readable at the same time. There is no canonical way to solve this problem. reply ta8645 18 hours agoparent> The problem: a command line interface must be human readable and machine readable at the same time. Since the computer is there to serve us, ultimately the solution must be for the machine to read as well as humans. reply oblio 18 hours agorootparentWhat if the ultimate solution is only achieved after our lifetimes? reply ta8645 18 hours agorootparentI don't really understand the question. Nobody got to fly until we figured out how to build airplanes, people before that lived on the ground. Likewise, we'll live with iterations on the current command line, until AI is fully integrated with it. reply ceving 18 hours agorootparentprevNever ever. Humans are idiots. If you build machines like humans you will get idiotic machines. Machines need to do things exactly right and not almost right or mostly right. Current AI trends will not create better machines, just more idiotic machines. Those idiotic machines will be sufficient to impress idiots, but they will not help to do things exactly right. Automated theorem proving is the only way to build better machines. reply samatman 15 hours agorootparentAutomated theorem proving will only ever be an arrow in the quiver, because even in theory, the space of useful computer programs is a vast superset of those which may have all their properties formally verified. In practice, it's a much larger superset than theory allows. Growing the space of formally verifiable subprograms is a worthy endeavor, sure, but so is good old-fashioned engineering. reply ta8645 17 hours agorootparentprev> Automated theorem proving is the only way to build better machines. Sure, but that will just be one aspect of an integrated and wholistic AI, which can configure the parameters input to the automated theorem proving component, and act on the results. reply PurpleRamen 17 hours agoparentprev> The problem: a command line interface must be human readable and machine readable at the same time. There is no canonical way to solve this problem. It's not really the \"same time\". Usually, human and machine use the same command, but at different times. And there are many ways to enable different outputs, even at the same time. But this all depends on having some standard which everyone follows. And that's where it becomes complicated. reply avgcorrection 17 hours agoparentprevThe article covers this. Special machine-readable output modes like `--json`. reply n_plus_1_acc 10 hours agoparentprevIf you assume a similar set of primitives, it could be as simple as html2text(get(\"https://go.dev/doc/devel/release\")).find_all(\"\\bgo\\d+\\.\\d+\\.\\d+\\b\").sorted().unique().last() It's just that Rust is designed to be more robust in exchange for stricter Compiler time checks. reply friendzis 18 hours agoparentprev> The problem: a command line interface must be human readable and machine readable at the same time. There is no canonical way to solve this problem. * Powershell has entered the chat reply riddley 18 hours agorootparentPowershell has some cool things, but it's a disaster when it comes to usability. reply adamrezich 15 hours agorootparenthow so? reply shadowgovt 18 hours agoparentprev> Why is it broken by design? I think your example self-explains why it's broken by design. It's a good example. > a command line interface must be human readable and machine readable at the same time. There is no canonical way to solve this problem. And you know, there could be one. Apple has Human Interface Guidelines to reify the meaning of the visual abstractions in its desktop UI. The problem is that the command line didn't come from people who think like Apple designers; it came from people who think \"How can I express what I want using the least code possible, because laziness, impatience, and hubris are virtues?\" And they weren't wrong for the time (especially because every byte matters), but the design decisions they made got baked into tooling that can't now be moved. I think at this point we'd have to punt the POSIX toolchain to get something better; it's hard for me to imagine how we'd build discoverable, conceptually-consistent UX atop what we currently have. reply rascul 16 hours agorootparent> I think at this point we'd have to punt the POSIX toolchain to get something better; it's hard for me to imagine how we'd build discoverable, conceptually-consistent UX atop what we currently have. Posix could potentially do this. It already has a bit about conventions and could be expanded. Problem is getting things to adhere to it, plus I doubt the posix authors could be convinced to add a lot more to it. https://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1... reply ceving 18 hours agorootparentprevI think graphical user interfaces are no high standard. They do not compose. How would you express a loop or a recursion in a graphical user interface? reply samatman 14 hours agorootparentI'm genuinely unsure how to translate your question into, hmm. Anything actually. The snarky answer is \"By writing it in source code, in a GUI text editor\", a thing I do frequently. But the problem is that I have no idea what you're getting at in the first place, so that's just an attempt to recover some meaning from what you wrote. reply shadowgovt 13 hours agorootparentShells allow one to pipe around the data flowing from called process to called process in a very smooth and transparent fashion. By and large, GUIs do not. There is no such thing as a universal shell for GUIs, and attempts to layer automation atop the GUI abstraction are generally spotty and unreliable (certainly when compared to CLI and shells). This is both for technical reasons (i.e. it's much easier to clearly delineate the two ends of a pipe than to clearly delineate \"I want to click on the red square inside the 'diagram' window inside the drawing app\") and for ecosystem reasons (since GUIs aren't thought of as automatable, GUI designers are free to move pieces around version-to-version of software, making it extremely challenging to describe a GUI structurally). I've seen some neat attempts at GUI automation (Sikuli is my favorite) but it's never been a core feature like it is in the CLIs-glued-together-by-shells world. reply samatman 13 hours agorootparentCLIs are in the modern world a subset of GUI programs which run inside a text-oriented window, and plenty of more sophisticated GUI programs can be directed textually, with macros or short scripts, and/or have a command-line component for doing that sort of pipe or batch-oriented work. Yeah, sometimes there's functionality stuck behind a button or menu select which I want exposed in a more textual way, in macOS that's when you break out Automator, Alfred, or Hammerspoon (the only one I've ever used fwiw), Linux and Windows have their own equivalents. I don't think the distinction you're pointing to is nearly so stark or clear-cut as it's often made out to be. Ecosystems converge towards the tasks which are amenable to batching and pipelining being equipped to do so. reply shadowgovt 11 hours agorootparentBut in general, batching and pipelining are done via a text-based interface. Extremely rare is the teachable GUI interaface where there's a point-and-click way to describe the concept \"See these five items I clicked? Do that twenty-eight more times with this radio button family swizzled via a linear sweep.\" reply samatman 11 hours agorootparentMy point being, I don't see why there ever should be. A text window is a valid and frequently-included part of GUI programs (I happen to be using one this instant, in fact), so there isn't a lot of advantage in replacing something like Lua scripting with a bunch of buttons and whizbangs. It's been tried for normal programming, and no one likes it. The specific kind of task you described is frequently exposed as macros in programs complex enough to deserve it. reply shadowgovt 17 hours agorootparentprevThey are a challenge for composition, often. Great for discoverability though, and that doesn't require graphics, just context. There should be a button I can push in my shell that lets me ask \"what does the token at cursor mean,\" and a button that lets me type a plain language search string that wires down to a contextual search (i.e. I'm in the middle of typing out \"grep\" I should be able to ask \"how do I search folders?\"). We didn't have the tools to build this when grep was invented; we have them now. reply BlueTemplar 17 hours agoparentprevYour example not only doesn't prove that it's broken by design (only broken most of the time ?), it even seems to give a (relatively ?) easy way to fix it ? > Since very few implementations of ls allow you to terminate filenames with NUL characters instead of newlines In fact the solution to this issue seems to be so obvious that I might be missing something ? (Rejection by shell interfaces for some reason ??) reply tester457 17 hours agoprev> Do not read secrets from environment variables > Secrets should only be accepted via credential files, pipes, AF_UNIX sockets, secret management services, or another IPC mechanism. Which one of these is the most convenient and portable to use? Do you use secret management services for work only, or do you use them in your personal projects too? reply tashian 15 hours agoparentHi, I'm one of the authors of CLI Guidelines. See my post https://smallstep.com/blog/command-line-secrets/ for a bit more of a deep dive about using secrets on the command line. Credential files are a good, simple, portable option. Files have permissions already. They don't depend on an external service or a proprietary API. And, if your program accepts a credential file, it will be compatible with systemd credentials. systemd credentials offer more security than an unencrypted credential file. They are encrypted and can be TPM-bound, but they don't require the software using the credential to have native TPM support. reply cbm-vic-20 13 hours agorootparentIt's probably a good idea to check the permissions of that file, too, and emit a warning or exit with an error if the they're too permissive. reply indymike 13 hours agorootparentA good example that all of us have seen is ssh. It does not run if permissions on certs are incorrect. reply n_plus_1_acc 10 hours agorootparentBut it definitely could improve its error messages in this case. reply flgstnd 16 hours agoparentprevI like it when programs have a way to specify a command to retrieve secrets. mbsync (https://isync.sourceforge.io/mbsync.html) e.g. has afaik 3 options to provide a password for IMAP authentication: If you don't configure a password, you'll be prompted on execution. You can also put the plain text password in the configuration (impractical if you want to share your configuration). But there is also a configuration option to provide a command to retrieve the password. That way you can delegate the password handling to another program, e.g. a password manager like pass(1) (https://www.passwordstore.org/) or some interactive graphical prompt. reply AlwaysNewb23 17 hours agoparentprevA secrets management service would be most convenient. Documentation makes them easy to set up without having to build anything extra yourself. A secrets manager like Doppler (https://Doppler.com) or AWS Secrets Manager (https://aws.amazon.com/secrets-manager/) has the advantage of protecting your secrets in a secure place and the advantage of minimizing exposure of those secrets - even to your own developers. That way, you don't end up with a data breach that could have easily been avoided. These types of leaks can cost companies everything and are becoming way more common. reply bluetomcat 19 hours agoprev> Traditionally, UNIX commands were written under the assumption they were going to be used primarily by other programs. They had more in common with functions in a programming language than with graphical applications. Not quite. They were primarily intended for interactive use within a login shell. There are the programs which generate output on stdout (ls, cat, find, tty, who, date), and there are the \"silent\" text filters (tr, grep, cut, uniq, sort, wc). A one-liner would enable you to do basic computing tasks in that era. Any complex program would be written in C. After the appearance of DSLs like sed and AWK, certain string-heavy programs were offloaded to the shell. The shell is not a sane programming environment and was never intended as such. reply rollcat 19 hours agoparent> The shell is not a sane programming environment and was never intended as such. There are 10kloc C programs that could be 10 lines of shell and there are 1kloc shell programs that could've been 100 lines of C. Both kinds are nowadays probably better done in Python or Lua, but the shell and C are what's most universally available. reply bluetomcat 18 hours agorootparent> There are 10kloc C programs that could be 10 lines of shell Only when the shell calls other external C programs. Ten lines of calling ffmpeg or curl is not shell programming. > there are 1kloc shell programs that could've been 100 lines of C The 1kloc shell programs are fragile spaghetti that breaks in weird ways. Any invocation of an external program can fail for a variety of reasons, and the shell doesn't provide adequate mechanisms for dealing with it, apart from exit codes and filtering error text output. reply fargle 17 hours agorootparent> Only when the shell calls other external C programs. Ten lines of calling ffmpeg or curl is not shell programming. 100% wrong. this is what the shell was designed to do and where it is at it's best. often shell \"scripts\" are used like \"macros\" or power-tools, shortcuts to save off a complex invocation or workflow. error handling isn't as important in a one-off and \"adequate\" is whatever gets the job done for the user, which it does. it's rare that 1kloc shell script is the best engineering choice vs. (in the ancient days) Perl or (today) Python. e.g. \"real\" programming languages. you mostly should not write large programs in shell. and you really should not glue together pipelines of external programs using, for example, Python or C, which is onerous. ah, the HN crowd: where everything is either black or white, great or terrible. how about \"each to his own\" and \"use the right tool for the job\" reply bongodongobob 15 hours agorootparentIt's not rare at all. In every corporate environment I've worked in, my choices were PowerShell or 3 months of red tape, meetings, and security audits. I get that in a dev shop that's not the case, but most businesses employ 0 devs. So people end up being forced into shell scripting because it's the only approved option. reply int_19h 15 hours agorootparentAt least PowerShell gives you access to the entirety of .NET, even if syntax is not ideal for non-interactive use in many cases. reply bongodongobob 14 hours agorootparentYup. My \"scripts\" end up with a lot of .NET stuff in there. I'd rather use Python, but being forced to use PS for years, I've come to like it. reply int_19h 7 hours agorootparentDepending on what you're doing, C# might also be an option - since the compiler is a part of the .NET Runtime (not just SDK), it's available on any Windows install with PowerShell these days. It's too bad that the system one is always an old version that doesn't do .csx, but still. reply shadowgovt 18 hours agorootparentprev> Ten lines of calling ffmpeg or curl is not shell programming Ten lines of calling ffmpeg or curl is shell programming in precisely the same sense that 100 lines of C that `#include ` are C programming. If your code isn't standing on the shoulders of giants, you're probably wasting everyone's time. reply samatman 14 hours agoparentprevSane or not, shells are programming languages, and in early Unix this was quite a bit more prominent and obvious, the fanout into sh, bash, ksh, and csh being exemplary. To me it makes total sense to think of the standard POSIX toolkit as the standard library of the various shell languages, that seems basically correct in fact. reply dang 14 hours agoprevRelated: Command Line Interface Guidelines - https://news.ycombinator.com/item?id=38053692 - Oct 2023 (1 comment) Command Line Interface Guidelines - https://news.ycombinator.com/item?id=31651161 - June 2022 (1 comment) Command Line Interface Guidelines - https://news.ycombinator.com/item?id=25492119 - Dec 2020 (5 comments) reply eterps 16 hours agoprevThe most comprehensive CLI guidelines in the past have been in this book by Eric Raymond: https://www.goodreads.com/book/show/104745.The_Art_of_UNIX_P... https://www.catb.org/esr/writings/taoup/ It has been a while since I read that book, but after skimming through clig.dev I gather opinions have changed over time quite a bit. reply hiAndrewQuinn 14 hours agoparentI thought much the same thing when I read CLIG. I'm a big fan of the 17 Unix rules he puts down, though, and I think a lot of really good equilibria exist on the spectrum between the two. reply hiAndrewQuinn 14 hours agoprevI know it goes without saying for most of us here, but actually being a heavy terminal user yourself is one of the most important things to understand how to design CLIs. It helps a ton to understand the ecosystem you live in, not just your own organism. Example: Something I did a few months back ago for a tiny personal project @ https://github.com/hiAndrewQuinn/finstem was implement `--format CSV`, `TSV` and `JSON` flags. I haven't had need for any of these myself, but they exist so any future people who want to use `csvkit`, `awk` and `jq` respectively to wrap around my program have easy ways to do so. That's not stuff I would have had the instincts to do if I wasn't myself a user of all 3 of those programs. reply hollerith 14 hours agoparent>being a heavy terminal user . . . is one of the most important things I wish people wouldn't conflate CLIs with terminals. I run (shell) command lines all day, but try hard to avoid terminals / apps that emulate terminals. (To run command lines, I use Emacs, which I never run inside a terminal.) reply EasyMark 19 hours agoprevThis document is far to large for \"guidelines\". reply shrikant 19 hours agoparentWhart's this based on? If anything, this is among the more concise set of interface guidelines I've come across. Design guidelines typically run into hundreds of pages long when PDF'ed -- this page is just about thirty on default \"Save as PDF\" settings... reply tester457 17 hours agoparentprevI thought so too until I ignored the philosophy section. reply peterisza 1 hour agoprevOne of my favorite CL use-case is pizza ordering. We had a script for it in college, something like this: # pizza --cheese Ordering... Success. # reply 3abiton 1 hour agoparentBefore the age of captcha. You'll have to tip your captcha solcer nowadays. reply restalis 8 hours agoprev\"Don’t allow arbitrary abbreviations of subcommands. [...] you allowed them to type any non-ambiguous prefix, like mycmd ins, or even just mycmd i, and have it be an alias for mycmd install. Now you’re stuck: you can’t add any more commands beginning with i, because there are scripts out there that assume i means install.\" Please avoid the use of short arguments in scripts. It makes the least sense there. The short arguments (along with aliases, abbreviations, and whatnot) are a convenience for human usage, to reduce the amount of manual typing. In scripts you can be explicit with minimal cost (and you also should, considering the ratio of writes vs. reads). reply pmig 18 hours agoprevWe recently chose cobra[1] to create a cli application. It comes with so many best practices already packaged like autocompletions, help texts etc. etc. [1]: https://github.com/spf13/cobra reply flykespice 18 hours agoprevPOSIX standard had its own command line interface guide already, didn't it? reply rascul 16 hours agoparentYes. https://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1... reply 0cf8612b2e1e 16 hours agoparentprevDoesn’t POSIX comment more on what programs did vs what they should do? More a manual of backwards compatibility. Like, why does tar get to be so special that it takes command line flags with or without leading dashes? reply Sprocklem 13 hours agorootparenttar isn't a POSIX command, and retains its argument format from before the POSIX CLI guidelines were standardized (as does the POSIX ar command). pax, the POSIX-specified (but rarely implemented/used) equivalent of tar does follow the POSIX CLI guidelines. reply MichaelMoser123 14 hours agoprevwhatever happened to \"Make each program do one thing well. To do a new job, build afresh rather than complicate old programs by adding new 'features'\"? now we got lots of mega-cli programs, each one with its own distinct option language. Examples: kubectl, docker, openssl, git - (git got two command line languages: plumbing and porcelain) https://danluu.com/cli-complexity/ ... ls had 11 command line options in 1979, in 2017 it got 58. reply avgcorrection 17 hours agoprevThe worst part about terminal programs is that they can’t be deprecated. Or I don’t know of a way to do it. Because once you release something someone might immediately put it into a script somewhere instead of running it interactively. Now what do you do? Display a “hint” about the deprecation? Well no one’s gonna read that because it’s a script which is run non-interactively. So you just have to design the UI perfectly on the first try. That’s possible for small tools but what about larger ones? Past a certain point it becomes a truism that do-it-once perfectly without iteration is impossible. reply fleischhauf 17 hours agoparenthow is this different from some library somewhere? reply Viliam1234 16 hours agorootparentOther programs can use a later version of the library. This would be analogical, if commands always included their version. For example: rm-2.44.287-SNAPSHOT -r / reply avgcorrection 17 hours agorootparentprevHow is it similar? I get deprecation warnings if I update the library. I can pin the version of the library. A library is something I work with, unlike a terminal program which might be written and forgotten. Then those terminal programs get upgraded on the next system update because hey, you’re supposed to the get latest version right? Terminal programs can do the same (`-v1`) in principle. Few do. reply samatman 14 hours agorootparentA terminal program is quite easy to pin, though, just do this: - $package-manager install $program - which $program - cp $program ~/my/own/directories - $package-manager uninstall $program done. It won't ever change by accident. reply 10000truths 13 hours agorootparentNot quite that simple. You can't just pin the program itself, you also have to pin its dependencies. Which means you have to run ldd to get the libraries it loads (whether directly or transitively), and copy those too, and then you patch the program and its dependencies to set the rpath to $ORIGIN/my/own/directories, then you have to examine all the other more subtle application-level gotchas (e.g. path search order for config files, hard-coded absolute paths, etc.) and port those too. Once you're done, congratulations! You've done 50% of the work of a package manager. reply samatman 11 hours agorootparentYeah, I wish this kind of thing[0] were a core POSIX tool which worked reliably on all systems and binaries. Bit of an oversight, that. [0]: https://github.com/greenpau/statifier reply avgcorrection 13 hours agorootparentprevNice. Then I will not hesitate to break the UI in my upcoming terminal program releases. (... they are quite well designed in my head.) reply gumby 15 hours agoprev> It is assumed that command-line interfaces are the opposite of this—that you have to remember how to do everything. The original Macintosh Human Interface Guidelines, published in 1987, recommend “See-and-point (instead of remember-and-type),” as if you could only choose one or the other. I find that guis have worse discoverability and cli better. It's pretty hard to search for a gui affordance. Plus they are essentially unscriptable. reply ho_schi 19 hours agoprevIf you wonder about command-line argument parsing with C and C++, getopt() is built-in: https://www.gnu.org/software/libc/manual/html_node/Getopt.html That said. I think CLI programs are user friendly for professionals. Because they support input-process-output. Where input is STDOUT read by human, process is thinking by human and output is keyboard input to STDIN by human. UIs for the general audience? TUIs! TUIs are easily to parse, succinct in organization and fast input - all for humans. Humans can parse TUIs. And they can make up a mental modal. GUIs fail often with a lack of organization, information overflow and distractions by weird metaphors. The Windows 95 desktop metaphor is an example. It doesn’t make sense. Same for Windows 11 and its file-browser which makes it hard to recognize the filesystem or even just the home-directory. Now open Nautilus on Linux, it opens by default your home-directory (in most cases the place to be). I like the CLI but TUIs are my love. GUIs are okay if are like a TUI. reply unwind 19 hours agoparentThat is only true in some environments, since it's not in fact part of the language specification. See the manual page's [1] \"STANDARDS\" section, which reads: getopt() POSIX.1-2008. getopt_long() getopt_long_only() GNU. The use of '+' and ' in optstring is a GNU extension. [1]: https://www.man7.org/linux/man-pages/man3/getopt.3.html reply ho_schi 19 hours agorootparentCorrect. It is probably a feature which isn’t necessary for the languages itself but Linux/POSIX. reply kergonath 17 hours agoparentprev> UIs for the general audience? TUIs! TUIs are easily to parse, succinct in organization and fast input - all for humans. Humans can parse TUIs. And they can make up a mental modal. TUIs have the same drawbacks as GUIs and then some. Their big advantage is to work seamlessly over SSH but that’s pretty much it. They are not more discoverable and they are not more efficient than GUIs. There is nothing preventing you from having a decent GUI along the same lines as Midnight Commander to have a file manager without the metaphors you dislike, for example (as a matter of fact, there are several). reply Linux-Fan 15 hours agorootparentWorking over SSH is not the only TUI advantage: IMHO one of the greatest benefits of TUIs is the usage of characters to display the UI. This way, the font size is always equal (no unreadably small fonts). When designing TUIs I found this to be limiting in a creative sense -- I have to really think about how I arrange the TUI elements and information because I cannot put as many elements as I can on a GUI in the same screen space. Also, TUIs seem to be mostely unaffected by the trend to make every GUI element “touch-friendly” large which is an advantage for me as a Desktop user. Full rant here:reply indymike 13 hours agoparentprev> UIs for the general audience? TUIs! TUIs are ideal (sometimes) where a command needs to be interactive. Many commands lend themselves well to batch processing or require no interactivity at all. In many cases, a script piped into a text editor (which is a TUI) is all that is needed, sparing apps from having to embed a text editor and deal with all of the design choices. Other times GUI will work a lot better. reply IshKebab 15 hours agoparentprevgetopt is not built in. reply egberts1 13 hours agoprevsystemd fails badly with the CLI guideline. My biggest beef is total lack of CLI exit code. Makes it useless for bash programming with systemd utils. reply jansan 19 hours agoprevWhat I find super irritating is that some terminals will automatically execute a command if you paste it from the clipboard and there is a newline char at the end. IMO command line interfaces should not do this. reply avgcorrection 17 hours agoparentBash bind 'set enable-bracketed-paste on' reply mixmastamyk 6 hours agorootparentPut in .inputrc which I believe is portable to readline shells: https://wiki.archlinux.org/title/Readline#Bracketed_paste reply bloopernova 19 hours agoparentprevOn macOS, iTerm asks if you try to paste a string with a newline. You can disable it if you find it annoying. reply aequitas 16 hours agoparentprevFish shell by default does the expected behaviour of inserting in and allowing you to edit the command (multiline if needed) before executing it with [enter]. I've found Fish shell to have a lot of sane defaults and have yet to find a thing I would like to customize except for the prompt. reply riddley 19 hours agoparentprevYou might try a clipboard manager. Most of these that I've used have an option to strip new lines from anything in the clipboard. reply eviks 18 hours agorootparentthat would break multi-line commands, so not a good option reply riddley 18 hours agorootparentIt just strips off the final one. reply marcosdumay 18 hours agoparentprevYou want to use C-x C-e. reply crlfcrlf 18 hours agoparentprevYou find it irritating that terminals execute commands when they receive a newline, which is indistinguishable from pressing the enter key? The program is doing exactly as it should. Consider not copying newline characters, and you will solve the problem. reply mixmastamyk 6 hours agorootparentBracketed paste has been a thing for twenty years now: https://en.wikipedia.org/wiki/Bracketed-paste Put in .inputrc: https://wiki.archlinux.org/title/Readline#Bracketed_paste reply ttyprintk 19 hours agoprevnot much has changed since the prior comments: https://news.ycombinator.com/item?id=25304257 reply scbrg 17 hours agoparentSeems they took a small step back from their previous \"don't bother with man pages\" stance. Now it's \"Consider providing man pages.\" I still find it a rather shocking order of priority, honestly. https://clig.dev/#documentation reply ttyprintk 7 hours agorootparentGood eye! reply nmz 15 hours agoprevThere's something I wish command lines would not adopt that is should not be as a CLI flag, and that's --json or --csv. Instead this should be offered as an environment variable that runs through all the commands like IOFORMAT=JSON { cmd1|cmd2|cmd3 } if they support the environment variable, they can all read input in a safe way and behave appropriately, if they don't, then they just print as normal reply gumby 15 hours agoparent(BTW sometimes ENV vars are bug removers and sometimes a terrible source of mysterious bugs as well as a possible injection vector.) A good option package should support long options being both command line flags and environment variables. The really good ones support them in init files as well (I'm partial to the JSON-ish HOCON format, though .INI works too). I always spec (later overrides previous settings): - system default (/etc/foo.conf) - user defaults (~/.foo.conf) - local defaults (${CWD}/foo.conf) - user-specified init file (overrides local default) - environment vars (FOO_OUTPUT=JSON) - command line var (--output=json) As well as --no-init and --no-env reply xixixao 18 hours agoprevThese are great. Most frustrating to me is that the basic commands: rm, mv, cp, touch, mkdir do not follow some of the basic guidelines, such as “ask before destructive action”. I have been reimplemting them in Rust for myself to fix this (although a wrapper would have been perhaps better, it’s hard to write it and have the program work in any shell). reply stephenr 18 hours agoparentIt's pretty easy to have shell aliases for commands like rm, cp, etc to use the \"safe\" mode (the `-i` option) by default. Writing a replacement utility seems a bit like overkill. reply wang_li 16 hours agoprevIf your program detects -? and tells the user to use -h or --help or --long-help or --full-help you should be kicked in the crotch. Just show me the help. reply SAI_Peregrinus 16 hours agoparentI'd say -h and --help should return 0 & print help to stdout. Any unknown option should return EINVAL and output an error message and help text to stderr. reply rascul 16 hours agoparentprevI often show help if an unknown option was given but in the cases I don't I am not going through a list of special cases that I don't even know of to try and determine that someone wants the help text. I've never even seen -? before that I can recall. reply shadowgovt 18 hours agoprevOf all the challenges with using CLIs, the one that bites me consistently is capitalization of flags. Really wish we'd standardized on being capital-agnostic (or even demanding lowercase only). ... but we didn't, so now you have to memorize whether recursion is capital or lowercase R, restart is capital or lowercase R, poweroff is capital or lowercase P, etc. reply thworp 18 hours agoparentMost utilities I use daily have --long-flags for all of these. Using completion (prferrably https://github.com/unixorn/fzf-zsh-plugin) you can complete them pretty quickly and without knowing the precise name. reply klodolph 15 hours agoprevIf you have -r / --recursive, I would rather just have -recursive work as the others. POSIX be damned, the double hyphen is just an opportunity to make more mistakes, and the combined -rptgo single-hyphen flags are more opportunities to make mistakes. Let some legacy programs like ls and sync keep their single-hyphen combinations. Most new programs should just accept separate flags, and allow either single or double hyphens. Edit: Yeah, this opinion collects downvotes, doesn’t it? Y’all love it when you accidentally type -recursive instead of --recursive, and it turns out that it means the same thing as -r -e -c -u -s -i -v? That never made any sense to me, for the vast majority of tools out there. It sucks, to be honest. reply OJFord 14 hours agoparentI don't like combined single-hyphens for a different reason: sometimes they accept arguments, and it seems strange to me that `-a0` can variously mean `-a 0` and `-a -0`. (I think some insist that argument-having options are long, and short options can only be flags, which sort of solves it, except for the existence of the others which makes it still confusing.) That said, in a script, which is the main time I'd have to read the way someone else has written it anyway, I favour everything being --long --anyway --to-make-it=really-clear. (Though there are a few that are so common I won't insist on, `cut -dX -fY` say.) You must like `find`. reply klodolph 8 hours agorootparent>I don't like combined single-hyphens for a different reason: sometimes they accept arguments, and it seems strange to me that `-a0` can variously mean `-a 0` and `-a -0`. I think the answer to that is just to stop designing argument parsers that way—if you want -a 0 then maybe -a 0 is acceptable, maybe -a=0 is acceptable, but -a0 should not be. I don’t see how this part is controversial. For the same reason, -all should not be parsed as -a -l -l. >I favour everything being --long --anyway --to-make-it=really-clear. Yeah—I think it would be equally clear to write it like -long -anyway -to-make-it=really-clear, if we decided to make more parsers that worked that way. Some parsers do work that way. >You must like `find`. I have to assume that this is just sarcasm. The `find` command gives you a DSL for writing queries, and for some reason, the tokens in that DSL are option flags starting with -. Bizarre. I don’t think anybody wants to design something like that, and I don’t think there’s really anything to learn from find except maybe “sometimes, for historical reasons, the command-line arguments for standardized tools just plain suck.” reply BobBagwill 19 hours agoprev [–] The command line is dead. Too late to clean up, standardize, organize. The non-wimpy future is chat. reply marcosdumay 18 hours agoparentWake me up when chat becomes deterministic. There is a lot of value on understanding the context and trying to parse what you meant in a mangled command. For the foreseeable future, that value will fall entirely on documentation and search procedures, and chat will keep a steady negative value for actual interaction. reply BobBagwill 16 hours agorootparentThe current shells aren't deterministic either. Everything depends on history, context, environment. Early AI shells will ask for confirmation like \"ansible --check\" or \"terraform plan\". Running commands in a disposable virtual environment will inspire confidence. When they are trusted enough, direct execution. Soon after you're able to say \"Set up a k8s cluster for me!\" you'll decide you don't need k8s anymore, you'll just introduce your AI model to your data-lake and be done. The days of thinking about how many nodes do I need, which immutable linux distro, t5g vs m4.xlarge are almost gone. Read the lyrics to Graham Nash's \"Teach Your Children\", substituting \"model\" for \"children\" as necessary. :-) reply int_19h 14 hours agorootparentI'm not saying that what you describe is impossible, but we're clearly far from the point where AI can be trusted to do something like that unsupervised (which is a necessity for any kind of scripting). So declaring that \"command line is dead\" is rather premature. reply BobBagwill 9 hours agorootparentImproving the existing command line for humans is a waste of effort. It's an heirloom activity already, unless you forbid the use of LLM AI. It's inefficient for even the simplest AI model to use a CLI, so API's are the future. But using API's to create a k8s cluster are pretty primitive too. The nearish future is things like unikernels running LLM's and rule-based logic engines and data-lakes. LLM's can help you create rule-based logic engines and data-lakes, so win-win. Window desktops are dead. MacOS is a zombie with lipstick. Linux desktops are like playing an electronic harpsichord. Linux servers and Windows servers are dead too. Who cares? CEO's ask \"How many widgets did we sell this quarter and will I get a bonus?\" Regular people ask \"When is my next dental appointment, how do I get wine stains out of my rug, what's good on TV? \" [There are more important questions but people probably won't ask their device and expect a useful answer] All of these things current LLM AI can answer with 90% accurancy, which is better than human accuracy. General AI is not necessary for pretty significant changes in the status quo. The IT sector's only future purpose is to help train models, IMHO. And future means now, not in 10/20/50/ years. reply int_19h 7 hours agorootparentLLMs are great until they start to hallucinate on subjects where you don't know enough to catch them. We're nowhere near solving that problem yet, and you're painting a hopelessly optimistic picture. Here, now, CLI is still in heavy use, and it's not going away wihin the next few years. reply Cockbrand 18 hours agoparentprevI was going to comment in protest - \"but the CLI has always been there, and it'll always be...\" But then I realized that you're probably right. I'm kinda looking forward to looking back with bewilderment on those decades of doing CLI stuff with remembering all those commands and shell intricacies and whatnot. reply kjs3 16 hours agorootparentIn my vision of the future I see one of my descendants 100 years from now cussing furiously that his AI avatar is borking up his request, and one of the local neckbeards walks up, does an obscure incantation that pops up a command window, types a few lines of text and fixes the problem right up. \"Newbs...geeze\" he says as he walks away. Maybe the proles will one day be permanently denied the CLI, but even if only as a lowest common denominator administration tool of last resort, I predict there will be a CLI somewhere. reply layer8 15 hours agoparentprev [–] Chats aren’t a formal language. You need a formal language to do anything reliably and reproducibly. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The text provides guidelines for improving command-line programs based on modern updates to traditional UNIX principles.",
      "It emphasizes the importance of designing CLI programs with the user in mind and adhering to good UI design and CLI conventions.",
      "The document explores the value and design principles of command line interfaces, including clarity, discoverability, and human-first design."
    ],
    "commentSummary": [
      "The article explores the current status and benefits of command line interfaces (CLIs).",
      "The comments section covers a range of subjects, such as the significance of a \"dry run\" option in commands, the behavior of commands when piped or redirected, different approaches to launching environments and executing code, the preference for nested CLIs versus displaying all options in one place, and the challenges of making CLIs both human and machine-readable.",
      "Opinions differ on the future of the command line and the role of AI, but there is acknowledgment of the ongoing utilization and importance of CLIs."
    ],
    "points": 325,
    "commentCount": 170,
    "retryCount": 0,
    "time": 1707225934
  },
  {
    "id": 39274882,
    "title": "Bluesky Opens Social Network to Public with New Features",
    "originLink": "https://bsky.social/about/blog/02-06-2024-join-bluesky",
    "originBody": "BlogJoin Bluesky Today (Bye, Invites!) Join Bluesky Today (Bye, Invites!) Feb 6, 2024 by The Bluesky Team Bluesky is building an open social network where anyone can contribute, while still providing an easy-to-use experience for users. For the past year, we used invite codes to help us manage growth while we built features like moderation tooling, custom feeds, and more. Now, we’re ready for anyone to join. Sign up for Bluesky Join more than three million people discussing news, sharing art, and just posting. What is Bluesky? To mark the occasion, we teamed up with Davis Bickford, an artist on the network, to share why we’re excited about Bluesky. To learn more about Bluesky and how to get started, read our user FAQ here. And if deep dives are more your style, we worked with Martin Kleppman, author of Designing Data-Intensive Applications and technical advisor to Bluesky, to write a paper that goes into more detail about the technical underpinnings of Bluesky. Looking Forward We’ve been working on more features that put you in control of your social media experience. Here’s what you can expect to see soon: Stackable Moderation Services Safety is core to social media. Bluesky moderates the app according to our community guidelines, and our vision for composable moderation allows users to stack more moderation services together, such as subscribable moderation lists. In the coming weeks, we’re excited to release the labeling services which will allow users to stack more options on top of their existing moderation preferences. This will allow other organizations and people to run their own moderation services that can account for industry-specific knowledge or specific cultural norms, among other preferences. One potential use case for labeling is fact-checking. For example, a fact-checking organization can run a labeling service and mark posts as “partially false,” “misleading,” or other categories. Then, users who trust this organization can subscribe to their labels. As the user scrolls through posts in the app, any labels that the fact-checking organization publishes will be visible on the post itself. This helps in the effective distribution of the fact-check and keeps users better informed. We’ll be sharing more in the coming weeks. In the meantime, if you’re interested in partnering with Bluesky and setting up a labeling service, contact us at partnerships@blueskyweb.xyz. An Open Social Network When you log in to Bluesky, it might look and feel familiar — the user experience should be straightforward. But under the hood, we’ve designed the app in a way that puts control back in your hands. Here, your experience online isn’t controlled by a single company. Whether it's your timeline or content filters, on Bluesky, you can easily customize your social experience. This month, we’ll be rolling out an experimental early version of “federation,” or the feature that makes the network so open and customizable. On Bluesky, you’ll have the freedom to choose (and the right to leave) instead of being held to the whims of private companies or black box algorithms. And wherever you go, your friends and relationships can go with you. For developers: We’ve already federated the network among multiple servers internally, and later this month, you’ll be able to self-host a server that connects to the main production network. You’ll be part of the first batch of servers that federate with the network, so expect to experiment alongside us! We’ll share more information on how to join the production network with your own server soon. In the meantime, you can test out your server set up via our developer sandbox. Find instructions here.",
    "commentLink": "https://news.ycombinator.com/item?id=39274882",
    "commentBody": "Bluesky signups are now open to the public (bsky.social)312 points by jakebsky 19 hours agohidepastfavorite255 comments jakebsky 18 hours agoThis is a big milestone for Bluesky! We've had the federation sandbox running for over six months but we're now able to commit to open federation on the production network this month as well. There's also stackable moderation coming shortly, which enables other individuals/orgs to operate moderation labeling services that users can choose to use. The technical challenges of setting up an (efficiently) scalable decentralized social network were quite interesting. The infrastructure itself is quite decentralized, with standalone PDS instances and two small shared-nothing datacenter PoPs. We're using SQLite with millions of individual databases for each user's repository and ScyllaDB for the global indexing service (AppView). https://bsky.social/about/blog/5-5-2023-federation-architect... If anyone has questions, technical or otherwise, some of the team should be around today to answer them. Edit: HN'ers might also appreciate this paper written primarily by Martin Kleppman about Bluesky and AT Protocol https://arxiv.org/pdf/2402.03239.pdf reply coldpie 18 hours agoparentThe obvious question is what your business model is. I know about your \"we plan to sell domains\" post from a bit ago[1], but that seems... optimistic, to me. Not sure I want to buy into yet another startup with no business model (e.g. Keybase). [1] https://bsky.social/about/blog/7-05-2023-business-plan reply CobrastanJorji 14 hours agorootparentOn the other hand, Jack Dorsey has proven himself as someone who can be very successful founding a large microblogging social network with no business model. reply ysavir 14 hours agorootparentI think Elon Musk proved Jack Dorsey to be that someone. reply jakebsky 17 hours agorootparentprevBluesky is in good financial shape for quite some time based on existing funding. And we're also working hard to be sustainable, which we believe is entirely feasible given our small team. But we're also ensuring that everything required to make the network sustainable over time is completely open. reply coldpie 17 hours agorootparentThat's not a very inspiring answer :( The consequences of taking VC money are going to come home to roost at some point. reply jakebsky 17 hours agorootparentWe'll see! The history of funded companies popularizing open protocols is not without precedent. I'm inspired by Netscape, which was the VC-backed company that made the web happen. reply vidarh 17 hours agorootparentSaying they \"made the web happen\" is just nonsense. They had one of several popular browsers, and one of several popular web servers. As much as I stayed up late to download new Netscape betas, had they never existed the web would still be just fine, and the customers of the ISP I ran at the time would have just used another browser. reply andybak 16 hours agorootparentHmmmm. I would argue the web would have been significanty different. There was a fairly big gap between Mosaic and Internet Explorer that Netscape filled and it was the period that largely defined the web as it came to be. Since IE was developed specifically to counter the threat of Netscape - it was also defined by Netscape. What other browsers of note were around in that period? reply vidarh 15 hours agorootparentNetscape 0.9 was released in October '94. IE was released in August '95 and the first version was just a licensed rebrand of Spyglass Mosaic (which despite the licensed Mosaic name was not a version of Mosaic). There was a number of browsers coming up at that time, and Mosaic was if anything what drove much of that early boom, as the most successful option that led to both Netscape, Spyglass, and by extension IE. Remember that Mosaic was readily licensed (and source available, though not under an open source license) - there were a number of other Mosaic offshoots (e.g. AMosaic for Amiga was released in December '93, with datatypes support) Other browsers than Netscape around that era, excluding the text based ones, included: * 1992: ViolaWWW (Unix; pioneered embedded objects, stylesheets, tables, client-side scripting); Erwise (Unix); MidasWWW (Unix) * 1993: Spyglass (licensed the Mosaic name, but written from scratch; also the origin of IE), AMosaic (Amiga), Cello (Windows), any number of Mosaic licensees, Arena (Unix, Linux, NeXT; pre-release in '93; full public release '94; Arena was co-written by the later Opera CTO Håkon Wium Lie, and pioneered layout extensions that turned into work on stylesheets and eventually CSS) * 1994: Argo (Bert Bos - co-creator of CSS; Unix; testbed for style sheets alongside Arena, and one of the first heavily plugin based browsers, with most functionality provided by plugins), IBM WebExplorer (Mosaic licensee); Slipknot (Windows; a really weird one which dealt with lack of SLIP/PPP connections by \"hijacking\" a Unix terminal connection, running lynx to retrieve the HTML, and then using zmodem to transfer both the HTML and images...) * 1995: IE (licensed version of Spyglass); Grail (Python; supported client side execution of Python...); OmniWeb (Mac) * 1996: Amaya (Unix, Windows, Linux, OS X), IBrowse (Amiga), Aweb (Amiga); Opera (Windows initially); Cyberdog; Arachne (DOS, Linux including framebuffer...; still updated as of two years ago...) Netscape took a lot of users from various Mosaic licensees, like Spyglass, and browsers like Cello; had it not existed, sure, things would have looked different, but timeline-wise the gap was narrow. Many of the browser - like Opera - that launched after Netscape had started development before Netscape launched, and others were abandoned in some cases directly because of Netscape. Some were probably no big loss, but Netscape's brief dominance contributed to the near monoculture we had for many years. There is no doubt it had improvements over Mosaic - I remember vividly the day the release with background image support spread across campus and every webpage looked garish for the next several years - but it was an advantage measured in months, and with competition heating up until Netscape stunted it for quite some time by becoming as dominant as they did until IE started catching up. A lot of the things Netscape is sometimes remembered for were not Netscape firsts either, or areas where they necessary had a lead. E.g. client-side scripting, style sheets, etc existed before Netscape; work on CSS was ongoing at CERN around the time Netscape launched etc. At most things would have looked different, and maybe some things might have taken a bit more time without Netscape scaring Microsoft. But I also remember a lot of ire at how Netscape pre-empted a lot of standards at the time by just throwing stuff at the wall, and untangling the mess they left took years. reply steego 13 hours agorootparentI remember that time and I too appreciate what the different browsers contributed feature-wise, but you’re missing the big picture. In late 1995, Netscape released a browser that provided investors a comprehensive proof-of-concept online platform that was billed as the operating system for the Internet and they were being offered an opportunity to get in on the ground floor. JavaScript and CSS didn’t matter. Investors were looking at SSL for eCommerce, Java applets, plugins, VRML, RealAudio, etc. Netscape stood out because nobody else was selling a comprehensive online platform with a compelling and plausible vision. The World Wide Web became something because a crap load of money was invested into developing browsers. It wouldn’t have happened on its own to this degree and none of those browsers were on their way to becoming a household name. reply vidarh 12 hours agorootparentIn late 1995 the market was even more crowded than when they launched in '94. IE was already out. Opera was around the corner. Netscape was already close to its peak market share. Plenty of people were selling alternatives, plenty of developers had funding. A lot of money had started flowing into browsers before Netscape. Had Netscape not soaked up the funding it did, more of that would just have flowed elsewhere. The argument is not that Netscape were irrelevant, but they were one - big, sure, - player among many racing to commercialise features that already existed before Netscape. reply throwing_away 10 hours agorootparentprev> Hmmmm. I would argue the web would have been significanty different. One can imagine a world without JavaScript... reply coldpie 17 hours agorootparentprevA notable difference is that Netscape had a business model, namely, selling Navigator. Anyway, enjoy the ride. reply pjlegato 14 hours agorootparentprevNetscape didn't invent the web, its open protocol, or even the first browsers. They did not make the web happen. Netscape had a business model (charge people for browser software.) Netscape also went bankrupt. It was a colossal failure as a business. reply londons_explore 16 hours agorootparentprevWow, all those VC's must have walked away very rich, considering how popular the web turned out to be!!! reply gumby 12 hours agorootparentprevNetscape did not start out an open source company -- quite the opposite. It was a saving throw once Explorer took away their dominance. And I wouldn't say it was a successful move. reply alexb_ 17 hours agorootparentprevMaybe I'm just being a hater, but the inspiration being a web browser that failed after being acquired by AOL at dot-com level stupid high prices, doesn't inspire confidence at all. Sure, it helped pave the way for Firefox, but Netscape itself never actually did anything. reply jakebsky 17 hours agorootparentNetscape created the first highly usable web browser, which introduced most people to the web. They also created SSL (TLS), JavaScript, the first high performance web server, and much more that that made the web go. reply vidarh 14 hours agorootparentThis is an exaggeration. Yes, people flocked to Netscape. Because, yes, it was marginally better than what was available, especially on Windows. But the main feature improvements that drove that initial rapid adoption was Netscape ignoring any attempt at agreement over standards and adding new \"trinkets\" like background images etc. in each release. And yes, they created Javascript, in a rush, but there already were other client-side scripting options. They were important, but their importance is inflated by looking back at a timeline where they won. We'd have lacked none of these things without them. They were one of many, and they were ahead in terms of features, but not by much, and the pressure they were under also left a wake of chaos. E.g., sure, they invented SSL, rushed it out with massive security flaws (that was a fun time... one of the gaping holes was that if someone ran Netscape on the same host they ran their e-mail on, which was not unusual, you could get a whole lot of the bits needed to cut down on the cost of bruteforcing the SSL key by triggering an e-mail bounce to help you narrow down current process ids), but there were prototypes of encrypted socket layers around for two years already by then e.g. see Simon S Lam's work on SNP [1]. \"Nobody\" used Netscape's web server - which wasn't developed by Netscape anyway (it was acquired from Kiva, unless Netscape had a pre-Kiva web-server I've forgotten) - it was way too expensive. It was a market leader, yes, but in a crowded tiny niche of commercial servers. I ran an ISP around that time. I sold packages to businesses, and we'd have loved to convince customers to pay for Netscape server software, but most people stuck with NSCA HTTPD, and quickly switched to Apache 1995 onwards. [1] https://www.internethalloffame.org/inductee/simon-s-lam/ reply PaulDavisThe1st 15 hours agorootparentprevThey may have done all those things and more. How did they actually make money? What's your equivalent? reply vidarh 14 hours agorootparentThey sold the browser until that market was yanked out from under them, and they leveraged control of the homepage into sales of their serverside packages, and then they sold out to AOL before their longevity was ever tested. reply PaulDavisThe1st 14 hours agorootparentIt was a rhetorical question. reply alexb_ 17 hours agorootparentprevOh yeah, I'm not denying that Netscape pioneered a lot of stuff. They also would have went out of business had they not been bought by AOL at a stupid, dot-com inflated price. You can do something that creates a lot of changes in the world, but if your business model involves giving people things for less than it takes to produce then I don't see how that's a business. What are VCs expecting to get a return on their capital? What's the plan to actually make a profit? Is the plan just to get bought out at a stupidly inflated price, similar to your inspiration of Netscape? reply vidarh 15 hours agorootparentThey pioneered very little. Viola pioneered client side scripting, stylesheets and more. Netscape popularized a number of things, thanks to heaps of cash that let them market heavily, and in the process overtaking a bunch of competitors, and snuffing out many of them. They did have a great browser that was best for a period of a few years, but it's not like there weren't plenty of alternatives either out or right around the corner when they launched. Fully agree with you they would not have survived long if the AOL sale hadn't happened. reply freeopinion 13 hours agorootparentprevPerhaps Jake is saying that it is more important to make the world better right now than to have 100 year business plan. It sounds like Jake is willing to lead the charge for now and risk death later if it means that the concept succeeds under any flag. Or maybe I'm just putting words in their mouth. What if the founders of MySpace are totally ok with its place in history and happy that social media under any name carries on their vision? Maybe they don't consider that a failure. reply PenguinCoder 16 hours agorootparentprevHighly *used. reply rglullis 17 hours agorootparentprevSorry, this is a non-answer. Is there a business model in mind or not? reply jakebsky 17 hours agorootparentWe've announced one business model and do intend to iterate and add others, but that's all we've announced for now. The plan is definitely to be sustainable over the long-term. reply taco_emoji 15 hours agorootparentHonestly this is fucking whacko to me. \"We've incorporated a legal entity whose entire purpose is to make money, but we have no idea how that's going to happen.\" How is this even allowed? Anyway the answer is ads. This just means it's going to be ads. It's always fucking ads. No one has ever gone into a capitalistic venture sans business plan and ended up doing anything besides selling fucking ads. reply coldpie 15 hours agorootparentAds are the most frequent answer, but it's not the only one. There's also the team getting acqui-hired, or getting bought by a competitor & shut down, or just plain old going out of business and sold for parts. None of those are good for users, obviously, but they are all viable paths for repaying VCs in absence of a business model. reply taco_emoji 14 hours agorootparentSure, I guess I meant for long-lived products. reply andruby 15 hours agorootparentprevI really hope they do freemium. If they can run it with a small enough team, then freemium could be feasible. Sell special tools and functions to the power users. reply clouddrover 4 hours agorootparentprevLook, those underpants aren't going to collect themselves, are they. And don't worry about Phase Two. Phase Three is when the profit will happen. reply throwing_away 10 hours agorootparentprevRelax bro, you're just looking at it upside-down. The ads let you know what services are not worth your time, or only worth consuming with sufficient adblocking. They're really doing you a great service by advertising that you're the product. Also, hope is not lost for ad-free capitalism. For the first time ever I'm actually paying for subscription services that don't have ads (yet). Mostly to do with search and AI. reply rvnx 17 hours ago [flagged]rootparentprevA bit macro and optimistic view about sustainability (in general, not specific to Bluesky): If everything goes according to the prediction of economists for 2024, a light crisis should decrease consumer confidence in the US. One of the solution to re-energize the economy might be to lower interest rates. Which means that if interest rates go down in 2024, companies are going to be able to borrow at extremely low cost. In such environment, does the question of business model even matter ? If your task is to raise debt, what you need is to sell a dream, not have a way to generate money. == Back to Bluesky: The bigger danger for the company now is most likely its own users. \"Open ecosystem\"/\"Freedom\"/\"Free-speech\" users tend to be greedy and consider everything should be free, and at the same time are very active when it's about criticizing. The \"normies\" of Twitter / Instagram, are likely higher spender because of the importance that vanity / self-promotion has in their life. One key could be for Bluesky to focus more on content, than on technology. Even on Telegram, people join groups and people, they don't really care if the source-code is here or not, or who controls what (because no matter how, this can change in the future). reply rglullis 16 hours agorootparentSpeaking as someone who has been stubbornly offering paid-for-access Mastodon/Lemmy/Matrix (and now Funkwhale) accounts at communick.com for 5+ years, I learned already that very few individuals are willing to put their money where their mouth is. Everyone loves to complain about the exploits of the tech companies, but no one really cares about paying for a service unless it gives some sense of exclusivity. What is going to make or break the alternative social media networks is the institutions. If/When newspapers (not journalists) start setting up their own instances, if companies put up support accounts on their own domain, if influencers start mirroring their social accounts on their own sites to try to their push their own brand... then I'll start believing that we have a chance. reply rvnx 16 hours agorootparentIt's difficult, we compare two different views, one from tech-perspective, and one from user-perspective. I understand your arguments about the technology, they are absolutely correct, but they attract a typology of niche users, which are extremely demanding and very difficult to convert to paying users. Twitter, the platform is very glitchy, the owners are who they are, the developer access is horrible, but still, I am using it, because there is exclusive and fresh content. Bluesky is an interesting project, but I can strongly suggest leaning toward content/user-focus than pure-tech, in order to secure a stronger business-model (and eventually, as a consequence, a sustainable + open ecosystem). Focus on onboarding great content first, and then walk back to the tech, not the other way around. For example, to support more extensively those newspapers or institutions to onboard the platform, and most of all, all these unofficial content creators. There are also some things which feel very strange, like the main description of Bluesky when you search for it on Google: \"Simple HTML interfaces are possible, but that is not what this is\". reply Ruthalas 15 hours agorootparentprevThis sounds interesting to me, but visiting communick.com I can't figure out how much the service costs, nor see any way to find out. I see a sign up page, but it also has no pricing info. Can you direct me to that in info? reply rglullis 13 hours agorootparentYeah, I am in the process of simplifying the offering and split down the site for managed hosting and the \"standard\" service. https://communick.com/packages/access should you give a link to the package: $29/year for Mastodon/Matrix/Lemmy/Funkwhale. reply JimDabell 16 hours agorootparentprev> we're also working hard to be sustainable When people ask you what your business model is, they are asking you how you are going to do this. reply Navarr 18 hours agoparentprevIs it completely infeasible for BlueSky to federate with ActivityPub while maintaining the pros of its architecture? If Threads, BlueSky, and ActivityPub all interconnected it really would be a great opportunity to compete on the software / UX front reply jakebsky 17 hours agorootparentThere has been some work by others already on this front: https://docs.bsky.app/blog/feature-bridgyfed reply johnmaguire 18 hours agoparentprev> There's also stackable moderation coming shortly, which enables other individuals/orgs to operate moderation labeling services that users can choose to use. Very excited for this - IMO, this is federated social media's biggest promise. reply cdchn 16 hours agorootparent\"We tag trolls so you don't have to.\" reply johnmaguire 15 hours agorootparentI'm not sure if you're being facetious or not, so I'll explain further. Currently, if you want to be part of a social network, your options are to opt-in to Zuckerberg's moderation or Musk's moderation. It would be great if these were _actually_ opt-in (as in, you can be part of a social network without opting in to their moderation polices) and if you can use anyone's \"moderation list.\" I think ultimately this would allow for freer expression than exists on current social networks. reply cdchn 2 hours agorootparentUltimately is what you end up with is being held under some instance operator's even more capricious, arbitrary and biases moderation \"policy.\" Although I think calling it a policy might be a bit generous. reply marxisttemp 18 hours agoparentprevWhy not use ActivityPub? Why should we trust Dorsey again? What is one good reason to use Bluesky over Mastodon? reply jakebsky 18 hours agorootparent1. \"Account portability is the major reason why we chose to build a separate protocol. We consider portability to be crucial because it protects users from sudden bans, server shutdowns, and policy disagreements.\" https://atproto.com/guides/faq 2. Jack Dorsey is on the board but has no day-to-day role in the company. Jay Graber is the CEO of Bluesky and is in control. The protocol is also designed not to require trust. The network is being \"locked open\" in a way that would allow it to survive Bluesky becoming evil. 3. Bluesky has a different approach in many ways. One of the biggest differences is that Bluesky is (IMHO) the first decentralized social network that is highly usable by regular non-technical users. reply AJ007 17 hours agorootparentThe account portability is probably the biggest problem with the fediverse right now. I finally signed up for Mastodon despite reading little to nothing positive about it on hn. It was easy to use, and the signal to noise ratio was vastly improved from my Twitter experience. However, that lack of account portability means users can, have, and will continue to get cut off. Servers cost thousands of USD per months with no revenue and domain name ownership can magically vanish for many reasons. With no business model for server operators, these are significant issues. That confusion for users may even be the primary force that drives them over to something like Facebook's Threads. There are analogies to e-mail here for the server operator. If I said any numbers I would be making them up, but I'm assuming 1 Mastodon user costs a lot more, both in compute/bandwidth and support, than 1 e-mail user. Free servers are not going to scale. Account portability doesn't solve this, but it means if something happens to one server operator, that user doesn't churn in to the ether and never return. I've been keeping an eye on https://fedidb.org/ (not my site.) While total users and servers keep going up over the past year, active users keep dropping. It could be something related to how they record usage, but it isn't a promising thing. I'm less skeptical about long term adaptation. Most of the negative sentiment I've read on hn about Mastodon just was wrong. Facebook, Twitter, Apple, Microsoft are all fully accelerating in to ad business models which will make much of their products less unappealing by the day. If history is any lesson, when a new competitor shows up without ads and a similar or better experience, the incumbent is in trouble. reply treyd 17 hours agorootparentYou can migrate your account between instances and take your followers and follows with you. Server shutdowns are rare, since administrators tend to proactively limit registrations when activity starts to be a financial burden. Avoiding the growth-at-all-costs mindset means that instances can stay sustainable. reply lapcat 17 hours agorootparent> You can migrate your account between instances and take tour followers and follows with you. You can if the server is operational. If the server not operational and cooperative, you can't. And you can't migrate your posts, only your followers. > Server shutdowns are rare Not rare enough though. reply kibwen 16 hours agorootparent> You can if the server is operational. And if Bluesky's servers stop being operational? Where is your data hosted? reply AgentME 8 hours agorootparentIf you or someone else has an archive of your data, then you can seamlessly port your account somewhere else. With Mastodon, I'm not sure there's an established flow for downloading your full account data, and you have to have your old server cooperate and redirect your user page to your page on your new server. reply pierat 15 hours agorootparentprevIts not even that. Look at Shitter and Reddit: they just turned off API access and introduced heavy rate limits to webpage loads. Good luck scraping your account details with that. Enshittification is a thing with ALL commercial services. And eventually BlueSky will have their \"The sky is falling! Crank the money extraction lever.\" And I'd move that timeframe up a LOT if they took VC money. Feces, err, uhmm VCs want their hockeystick growth to be a hockeystick. They want their 30x , 50x, or 100x. reply treyd 13 hours agorootparentprevThe handful of moderately large instances that have shut down rhat I'm aware of gave long notices, in a couple of cases over a year, before actually going offline. The only notable counterexample I remember was BitcoinHackers.org shutting down suddenly with a note saying \"haha look at how easy it is for mastodon instances to shut down go use nostr\", making it a self-fulfilling prophecy in that case. If you have other examples I would like to know about them. reply lapcat 13 hours agorootparentI've personally had to migrate instances twice. The first time because the instance suddenly became nonfunctional, and the administrator went AWOL. The second time, I discovered that the incompetent admin had silently enabled auto-deletion of data including posts and direct messages. Now I'm finally on mastodon.social, which wasn't open for new users the first two times that I needed an instance. reply vidarh 17 hours agorootparentprevThat, to me, is an argument for improving the existing account portability of ActivityPub, not for starting from scratch. To me, the Not Inventented Here feel to Bluesky makes me want to stay far away. People will bridge it to ActivityPub anyway. reply rglullis 17 hours agorootparentprevNostr solves the account portability, albeit poorly. (Your identity is your public key, so if the key gets compromised your identity is as well) I am more excited about Takahe, which decouples the servers running the federation from the domains holding the actor ids. This means that a hosting provider like mine won't need to allocate one whole instance for each user that wants to have their own domain. There is also a FEP from the developer of Mitra which aims to flip the ownership of the account keys, which would prevent cases of servers going under and stopping users from recovering their identity. reply shafyy 18 hours agorootparentprevI'm more worried about the financing of Bluesky. You have taken VC money, and we all know what that means - growth at all costs. > The protocol is also designed not to require trust. The network is being \"locked open\" in a way that would allow it to survive Bluesky becoming evil. I feel like we have seen this movie play out a few times. There are always way to close things down the road. For example, I can imagine that even with federation there will be a power law of distribution, and there's a high chance that most users will end up on official Bluesky servers. This means that you could one they stop federating, and most users would be backed in a walled garden. Sure, the protocol would be out there in the open, but it wouldn't matter because overnight it would lose most of its users. I trust that you and the initial team has genuine motivation not to do this. Forgive me for being cynical, but history does reapeat itself. I think the only antidote against this is regulation, as we're seing now with the DMA in EU that forces WhatsApp and other gate keepers to open their platform to other clients. reply AJ007 17 hours agorootparentThe big lesson to me has been for a platform to be open, there must be both third party clients and third party servers. A service that has only one backend server that no one else can run (looking at you Signal) isn't ok anymore. Even worse, Twitter or Reddit being \"open\" because they have an API: that's all bullshit, and you are setting yourself up to be rug pulled. We don't need to hear these lies anymore and it's time to move on from the services making either of those claims. I'm waiting for a little more progress and third party control to make a judgement on Bluesky. Users should think of this in terms of buy in cost. If you use a particular platform for 10 years, and build a community on it, you can take advantage of that and you get a mostly free service. But at some point the bill comes, and you move on. However, I keep thinking that the reason why some of those open third party protocols - even including email - \"suck\" is because so much of the time and focus has been on these proprietary, commercial communications platforms. I feel so old now I went from thinking email is a terrible way of communicating to, actually Facebook is far worse. Instead of seeing updates from my friends I'm looking at a firehose of noise of things I can't control and have zero interest in. Nearly 20 years later, I use e-mail every day and Facebook 0. Veering off-topic, but seeing conversations running for many years over the standards implementation and feature parity of the clients and servers both for XMPP and Matrix (meaning each separately, not inter-operating XMPP and Matrix, but rather each protocol has many servers and many clients, all trying to keep up with a moving protocol spec without breaking backwards compatibility), I have to laugh that a piece of legislation can just magically open the doors a some potentially very convoluted and continuously changing communications platform to third parties. It could be even more self defeating and monopoly re-enforcing if those platforms are relaying to users of third party apps the features they are missing along with warnings about non-existent encryption and everyone can read their messages. reply Kinrany 17 hours agorootparentprev> There are always way to close things down the road. If nothing else, Bitcoin is a successful existence proof. Maintaining control may be easier, but it's safe to say that Satoshi wouldn't be able to take control back now. reply wesleytodd 17 hours agorootparentprevPoint number three is critically important and no matter how many nerds complain it is not activity pub or some crypto thing, the company focus on delivering a product which is viable to use by normies is awesome. reply timeon 17 hours agorootparentprevCan you elaborate on point 3? What do you think are the differences or pain points of Mastodon that Bluesky fixes? reply TheCleric 18 hours agorootparentprevDorsey isn’t even involved. He kicked it off but hasn’t had a hand in it for a very long time. reply ngrilly 17 hours agorootparentprevQuestions 1 and 3 are answered in the paper. reply daveloyall 16 hours agoparentprevY'all let the public in before finishing your wait-list. I joined the list on 2023-03-02. Y'all didn't email me that signups were going to be public. Y'all didn't email me when signups actually went public. I found out about it here. ...And, let's see.. Yep, my handle is taken. reply jakebsky 15 hours agorootparentSorry about that. You definitely should have received a waitlist invite. We did invite everyone on the waitlist before launching. The deliverability of those emails was quite high but it's possible it went to the spam folder or something else went wrong. Something like 1 million of the users that joined came from the waitlist. Most users on HN are probably able to navigate using a domain handle, which is really the recommended and most decentralized option. https://bsky.social/about/blog/4-28-2023-domain-handle-tutor... reply omoikane 14 hours agorootparentprevSame here, I was on the waitlist for a few months, during that time I saw no evidence that anyone joined Bluesky via the waitlist. Eventually I got lucky and found someone who was giving out invites on Twitter, but by then all of my friends are no longer interested in joining yet another social network. I hope Bluesky prospers since it has some features that Mastodon and Twitter lacks, but it has a lot of catching up to do. reply pierat 15 hours agorootparentprevWell, join Mastodon! Find a community (server) that fits your liking and get your username! You can talk with anybody on other servers, but the one you choose is your homebase! https://joinmastodon.org/servers At least you're not succumbing to a commercial interest who will inevitably enshittify for eventual profit extraction. reply ipqk 15 hours agorootparentprevYeah, it's pretty ridiculous. On the other hand, the benefit of decentralized services is that your handle being taken shouldn't (eventually) matter, because you can just find another server. reply RobotToaster 17 hours agoparentprev>There's also stackable moderation coming shortly, which enables other individuals/orgs to operate moderation labeling services that users can choose to use. Will people be able to opt out of your moderation services? reply tracker1 17 hours agorootparentThis is my big question as well. As long as I'm able to block at an individual, or even org level, I'm generally okay seeing a feed of those I follow. Most social media moderation in my experience tends to be heavy handed. Most jokes could be offensive to someone and likely are. I'd prefer to preserve the collective works if George Carlin and Richard Pryor over heavily filtered systems. Edit: appears to be completely opt in and based on tagging... Wonder about positive filtering by tag now... reply jakebsky 17 hours agorootparentprevYes, every part of the Bluesky (atproto) network is composable, including moderation (labeling). reply lcnPylGDnU4H9OF 17 hours agorootparentprevIt sounds like you have to opt-in. reply pentagrama 11 hours agoparentprevI tried to sign up and it requires a phone number to verify trough SMS. Question. The phone will be attached to my account or is for one-time verification? If the latter, it is removed from blue sky database at some point? Thanks. reply zimpenfish 18 hours agoparentprev> we're now able to commit to open federation on the production network this month as well Aha, this is good to now. Looking forward to standing up my own PDS. reply muglug 18 hours agoparentprevThanks for all your hard work! Twitter is an obvious influence on Bluesky. Was the team able to benefit from the experience of working on Twitter, or were most of the big problems novel? reply pfraze 18 hours agorootparentNone of us worked at Twitter actually, but we chatted with a lot of folks who did. reply Kye 18 hours agoparentprevWhat's the thinking on BGSes? I haven't seen much talk of who's expected to run them or what they'll look like, but they seem to be the linchpin of reliable data portability. reply jakebsky 18 hours agorootparentA Relay (we used to call it the BGS) crawls all of the PDS hosts on the network and aggregates the data. This makes it possible for services to subscribe to all events on the network without putting load on PDS hosts directly. Anyone can run a Relay. They're somewhat comparable to Linux distribution FTP/HTTP mirrors. Bluesky will always run a Relay, but other organizations will hopefully as well. We expect these might be organizations doing other things in the ecosystem, universities, and possibly open consortiums. reply skybrian 17 hours agoparentprevAny plans to improve the search engine? reply jakebsky 15 hours agorootparentYes, it's always a bit of an after thought (unfortunately) but we have improved it already a couple of times. And like most things with atproto, there will likely be protocol support for pluggable search engines, so users can choose their search provider(s). It's already entirely possible for others to operate atproto search engines since all the data is public and available. reply kevinmchugh 18 hours agoprevI've been on for a few months now. It feels like 2014 Twitter in that it doesn't have gifs or video, so you will see some very good jokes. A number of people I used to follow on Twitter are over there but seem to have broken out of the posting habit and are quieter now. When I look at the \"discover\" tab I don't usually see much stuff that's interesting to me. It's a lot of men posting thirst-traps, furries, bog standard too online politics, and discussion about what's going on on Twitter Edit to add: people really like to advise blocking. It's to the point that new users are often advised to add a profile photo and an intro post before following people, because you might get blocked just for not having those. I don't know what this is about - possibly because there's no private/locked accounts? It seems really strange to me. reply delecti 17 hours agoparentRegarding your edit, much of bluesky's culture is developing in reaction to Twitter, or based on lessons learned from it. That block-first attitude is a response both to the kinds of bots that are endemic on Twitter, and to the kinds of engagement-bait that made it such a mess at times. New follower looks like it might be a bot? Block. See a skeet from someone who looks like an asshole? Block. Don't feed the trolls, just block and move on. Personally I tend to mute, rather than block, but I think lots of social media would be better if people didn't boost things they dislike for the purpose of dunking or disagreeing. And to be clear, not all of bluesky's culture is a response to twitter, a lot of it is just the kind of lighthearted playfulness you can only get on a small and new social network. reply kevinmchugh 17 hours agorootparentI mute annoying posters (the frequent users who already have >5k posts tend to be uninteresting to me and obviously show up more often in feeds), that makes sense to me. How goes my experience improve if I block a bot that follows me? Follower bots is a platform problem reply delecti 16 hours agorootparentI think it's mostly the same motivation as blocking assholes, partly \"I don't want to be part of your attempt to game the algorithm\" and partly \"I don't want to see this\". reply everybodyknows 14 hours agorootparentprevWhat is a \"skeet\"? reply delecti 13 hours agorootparentThe unofficial name for a post on bluesky. It comes from a combination of \"sky\" and \"tweet\". The people running the site don't like that name, which of course only made people embrace it even more. reply pests 11 hours agorootparentIt's also slang for ejaculation, so there's that. As Lil Jon might say, To the window, to the wall! Til the sweat drop down my balls Til all these bitches crawl Til all skeet skeet motherfuckers, all skeet skeet god damn! Til all skeet skeet motherfuckers, all skeet skeet god damn! reply delecti 11 hours agorootparentIncidentally that's also a big part of why the people running the site don't like the name, and why the users do. reply pests 11 hours agorootparentI don't know what's worse, this or toots. Can't we admit Twitter had the perfect name for it, and now that twitter is gone, we can all just call them tweets? Its almost absurd. Like if phone companies decided to brand text messages and someone got to \"Texts\" first so companies came up with \"Blurts\" and \"Yells\" or \"Phonomails\" and now we use a different verb based on which walled-garden we are in. edit: In thinking on this more, I do think this might impact the image of the communty. The hot lists and other feeds are dominated by the furry community, which is fine, but not everyone's cup of tea. Now naming posts / the verb be an euphemism of ejaculation... it just feels immature and is everyone going to feel welcome? reply kevinmchugh 10 hours agorootparentI really think \"tweeting\" worked to make it seem more normal and less serious than \"posting\", which is the brandless term X and Bluesky (the companies, not the communities) both prefer. Posting is for forums and forums are for nerds. I harbor some hope that Elon will firesale off the trademark on \"tweet\" and someone can put it to good use reply pests 9 hours agorootparentTweet is the only one that has made any sense IMO being the sound the bird mascot made, thematically with \"short chirps\", and birds \"tweeting\" at each other. reply panarky 17 hours agoparentprev> thirst-traps, furries, bog standard too online politics Maybe the biggest problem with Twitter never was its centralization. It shouldn't be surprising that cloning Twitter mechanics just to decentralize it would result in the same signal-to-noise ratio as Twitter. reply packetlost 17 hours agoparentprevYeah, I've tried to seek out and post technical stuff but it's mostly people virtue signalling, rage baiting, and complaining about other sites. reply j4yav 17 hours agorootparentI made an account to check it out and was surprised to see it was just recommending nothing but US politics outrage bait, just like Twitter. reply CM30 16 hours agorootparentprevThis is sadly a bit of a problem with most Twitter alternatives right now; many more niche communities haven't moved over (or have only chosen to go to one of them), so there's a good chance you won't find content you're interested in. Mastodon seems to have more of the tech crowd from what I can tell, but even then it's maybe 1% of the audience you might have had on Twitter. reply 15457345234 38 minutes agorootparentThey aren't going to move over; the reddit/twitter model of 'crosspollination' is fundamentally flawed - if trolls can just stroll over from their little trollzone at any time they will do so and every conversation will get derailed with politics/gender/whatever gets the most engagement; character assassination will be the dominant paradigm. Silos are the answer. Open to all but topic focussed and with no cross-referencable userhistory. Or 100% anonymity with no userhistory i.e. futaba type message boards. reply DoItToMe81 15 hours agorootparentprevMost \"Twitter alternatives\" don't deal with the fundamental, sensation-seeking oriented flaws in the medium and just turn out to be Twitter with more perpetually upset losers. It's disappointing, because a lot of work has gone into these. Especially Mastodon and Pleroma. reply packetlost 12 hours agorootparentI somewhat like Mastodon's model where you can have mostly focus on members within a community that maintains/owns the instance. Federation completely optional, though connecting communities more explicitly would be really cool! reply kevinmchugh 17 hours agorootparentprevI think a lot of that is on mastodon fwiw, and some hasn't left Twitter reply delecti 11 hours agorootparentIMO the biggest problem with Mastodon is that for a while decentralization was the biggest selling point. It led to the place being full of a lot of pedantic dorks. The situation did improve as Twitter exploded and various communities migrated in herds though. reply packetlost 16 hours agorootparentprevI've mostly been able to curate my Twitter feed to not include that stuff. Similarly for Mastodon. The problem is BlueSky tries to add algorithmic feeds while not doing it well enough to learn that I don't want to see it. reply AJ007 14 hours agorootparentI think the future has to be feed ingestion that is 100% controlled by the user. To some extent we were there with RSS readers. I'm still able to retrieve Twitter feeds with an RSS reader. Everything comes together in one place, in order. If this is done on device and everything is archived, it starts getting really powerful. For example, after the 737 MAX door incident, I searched \"Boeing\" in my RSS reader and instantly had a list of news stories going back over the past year. (The number of 737 incidents that has been happening around the world is a lot more than what sits in the top of the news, but that's another discussion.) A local LLM could even summarize large amounts of stored headlines and tweets going back over years, and that's tech that works today. I don't want third party ad platforms (which now consists of Google, Facebook, Apple, Microsoft, Amazon, and more) measuring, running tests, re-ordering, and deleting on my communications. The LLMs and machine learning advancements are going to just make this more invasive, ugly, and manipulative. Sure, hn is largely about VC backed startups going on to try to have a multi-billion dollar IPO. It's also about disrupting incumbents. I can't think of a better path to disruption than coming up with business models that choke off their user-bases and end ad monetized surveillance. I for one have no interest in a future where I wear an Apple/Facebook/Google controlled AR/VR headset that has cameras to make sure my eyes are looking at all of the ads which are targeted based on the other pixels it knows I looked at. (I feel like a paranoid schizophrenic writing that, but that is what they are making.) reply 008289x8820 13 hours agorootparentLook man, I'm a regular HN reader and even I don't buy into this. Are ads really the problem they are made out to be here? I'm asking because I don't mind and haven't mind about seeing an ad in years. In fact, I think they are perfectly good compromise when it comes to monetizing a website. Even HN has ads for gods sake. I even was okay with those old porn ads in The Pirate Bay or in 4chan, etc. Sure, if it's too much of that it can get annoying but whatever. On the other hand, the vast majority of people don't even know what RSS is and haven't even noticed a change in Twitter in the last few years. I'm sure most of them, believe it or not, aren't even aware of the transition to Elon Musk. I'm not saying that you're necessarily wrong, but perhaps you and the tech industry in general are vastly overestimating a extremely niche opinion and a \"nothingburger\". For example, cookies. Now we have extremely annoying cookie banners everywhere, when they didn't matter at all. Why? Because if I'm using an icognito browser, what are they going to track? Do you think I REALLY care? What are they going to do with that? Track me and offer me catered ads? I don't think \"they\" can, but even if they do, big fucking deal. I'm a lot more worried about Know Your Customer policies on everything, or social media platforms (or even hosting platforms) deplatforming you if they don't like your opinion, or the FBI planting child porn on your site if they want to take it down. Do you think I'll worry about an AD out of all things? reply flkiwi 16 hours agorootparentprevI tried to create a curated feed on Bluesky. It didn't go well. It interpreted my unambiguous keyword in the technology space as applying to content in the uh personal massage space. Much, much prefer Mastodon's direct hashtag following. reply RankingMember 18 hours agoparentprevIt will be interesting to see how the fragmentation in the post-Twitter landscape eventually consolidates. To your broader point, I think it's an interesting time where there's trepidation over just becoming Twitter 2.0 versus making some improvements that improve the experience of the platform. Even before Elon, Twitter definitely had some pain points, such as repetitive gif-replies and toxic political stuff, as you alluded to. reply psionides 16 hours agoparentprevYeah, I strongly dislike this blocking culture there, because blocking also affects the other person's experience on the site - they won't be able to follow threads where you're involved. There's no reason to not use muting instead if you just want to get someone out of your view, except if you do it out of spite. reply JacobThreeThree 18 hours agoparentprevThe network effect is very strong. reply Sol- 18 hours agoprevReading testimonials from people who seemingly enjoy Bluesky, it sounds like the main perk is its exclusiveness and feeling like the good old days when Twitter was just for hardcore social media nerds? Is that that a recipe for success and wouldn't opening up undermine it? I don't see how any Twitter clone would avoid the pitfalls that make social media like Twitter fundamentally annoying. It's not about the technology or being federated or not, but about such internet-scale communities just not working well. You'll always end up with online drama about the silliest things and terminally online power users. Everybody hates Discord, but I think more communities should strive towards isolating themselves from the broader net to keep conversations civil. reply flkiwi 16 hours agoparentMy experience on Bluesky was: 1. A direct port of the ragescrolling, today's-main-character culture of twitter 2. Complaining about mastodon and linux on a premise I haven't been able to tease out but appears to relate to open projects being inherently untrustworthy and private projects that receive funding being trustworthy. 3. Hyping Bluesky's ease of use (it is identical to Mastodon in every meaningful respect, except where Mastodon offers some additional functionality like private posts). I got out. I love a lot of the people that moved there, but the rage culture alone was what I originally left twitter to avoid. It's kind of a cultural AOL in the post-twitter-social space, with all that entails. reply jmull 16 hours agoparentprev> internet-scale communities just not working well I think you have to dig into that, and figure out why internet-scale communities don't work well, and then whether or not bluesky addresses it. First of all, what does internet-scale really mean? I think it has to be that, to some degree, everyone is talking at everyone else. For twitter and some other social networks this is because users don't fully control their feed -- they see what an algorithm decides they should see... and the algorithm is designed to increase engagement... because ads are how the social network makes its money. So you have content creators competing to make the most engaging posts and twitter doing its best to deliver those posts. So I think it's probably twitter's ad driven business model, combined with the sad fact that it's a lot easier to engage people with anger and outrage than with civil, thoughtful content, that leads to a social media wasteland more than whether you need an invite to join. I don't know if it will work, but if bluesky stays away from an ad-driven business model, they can let people control their own feeds and creators aren't incentivized only for engagement and it might stay a nice place to visit and hang out. reply WD-42 18 hours agoparentprevThis exactly. It’s not the platform, it’s the people. I think social networks work fine for small niche communities where conversion stays focused. The problem with all general social networks is that they eventually all devolve into political flame wars. Everyone is just over it. reply timeon 17 hours agorootparentNiche communities around some topic work well if people that are interested come and go. But if the niche is artificially created by fact that community is gated, then such community is not sustainable. Cabin fever and general fatigue of users. reply bombcar 17 hours agorootparentReddit almost solved it because they figured out a way to make a large collection of small communities, but then that slumped into the melt as it was likely to do. reply tracker1 17 hours agoparentprevI keep thinking something closer to BBS networks... There are local boards you can chat on, but also the network boards. Usually by topic or interest. It's not instant, like social media, but lends to longer communication. More like a self hosted, distributed Facebook group, less life Twitter. reply rurp 11 hours agoparentprevI agree that internet-scale communities are pretty bad for many (maybe most) topics. But Discord goes too far in the other direction with terrible discoverability. Most posters who would be good contributers to a given group never learn of its existence. The old school forum model does the best job of threading that needle that I have seen so far. Forums are easy enough to stumble across when searching a relevant topic, while the narrow focus makes them less of a magnet for stupid trolls and attention seekers. Reddit solved the hosting and setup problems of a forum, but that company is so far into the enshittification spiral that it hardly seems like a worthwhile place to invest much focus. I wish that all of the neo-Twitter resources where going towards making better forum-like platforms that actually encourage thoughtful discussion while also leaving space for more lighthearted posting. reply jeffbee 18 hours agoparentprevI don't understand how people believe they just \"end up\" like that. You don't ever need to see celebrities and related junk online. Just follow your friends and don't follow anyone else. reply vineyardmike 15 hours agorootparentThe obvious answer is that people want it. I follow a bunch of musicians. I love going to concerts, and it’s the easiest way to find out when they announce tours. The unfortunate reality is that scalpers make buying tickets a terrible experience, so unfortunately I have a strong interest in knowing exactly when tickets go on sale to improve my own chances of getting tickets. reply tcfhgj 15 hours agorootparentprevStuff like that is automatically shoved into your feed. reply jeffbee 15 hours agorootparentOn what platform? It's not true on bsky or twitter. reply tcfhgj 14 hours agorootparentIt's true on Twitter. reply jeffbee 14 hours agorootparentOnly if you read the algorithmic \"For You\" feed instead of the chronological \"Following\" feed. reply tootie 17 hours agoparentprevMy dream of social media utopia is that that we see a nonprofit organization who makes no bones about their moderation policy and enforcement or their revenue and expenses. I think centralization is just the only way a network can grow to useful size or behave predictably. It's not really a social network, but my model for this is Wikimedia. They've built something incredibly durable, centralized, aggressively moderated and financially viable. reply thinkingtoilet 17 hours agorootparent>we see a nonprofit organization who makes no bones about their moderation policy The problem is that the rules will be made by humans and will be enforced by humans. This will never be even close to perfect, especially with bad actors which are inevitable when a platform gets popular enough. reply tootie 15 hours agorootparentI don't think the goal should ever be \"perfect\". The problem I see with any commercial endeavor (this is probably as sideways critique of capitalism) is that if you have profit as the superseding interest, it will always be in conflict with being fair or being inclusive. At the same time what someone like Elon Musk clearly doesn't understand is the fundamental conflict between having an environment that fosters fruitful conversation vs absolutist free speech. Just having a platform that has a clear number goal of \"free sharing of information and idea\" at the top says clearly that it will come before platforming troublemakers or letting advertisers put their finger on the scales. reply the_duke 18 hours agoprevI feel like this would have been a lot more impactful a year ago, when the Twitter drama was in full swing. Feedback: the homepage looks more like a tech product pitch site, and the announcement post also doesn't look very polished. I guess the target for Bluesky isn't so much the \"regular Facebook/Twitter/IG user\", more the nerd. Side question: what's the interop story between Bluesky and Mastodon now? reply jakebsky 18 hours agoparentThere is some work by others to make this possible: https://docs.bsky.app/blog/feature-bridgyfed reply Retr0id 18 hours agoparentprev> I guess the target for Bluesky isn't so much the \"regular Facebook user\", more the nerd. I think it's more \"regular Twitter user\". It has implementation details that interest nerds, but my read is that the target audience really is regular users. reply angulardragon03 18 hours agoparentprevAfaik no interop - they use different protocols. reply lnxg33k1 18 hours agoparentprevI am not sure, during the Twitter drama was in full swing Meta tried to launch an alternative, and how long did it last? A couple of weeks? I think social sector is filled, with boomers on facebook, cool people on instagram, kids on tiktok, woke on twitter, meme people on reddit? reply zimpenfish 18 hours agorootparent> Meta tried to launch an alternative, and how long did it last? A couple of weeks? If you mean Threads, it's currently up to 130M monthly active users[0]. Estimates for Twitter late 2023-early 2024 are between 350-400M MAU. [0] https://techcrunch.com/2024/02/01/threads-now-reaches-more-1... reply lnxg33k1 17 hours agorootparentAh okay, didn't know that, I've read some time after the launch that it was going desert, somewhere on an article linked here, I am not a social network person, so wasn't following it reply piperswe 12 hours agorootparentprevWho are these users? I haven't heard a single person talking about actually using Threads. reply anhner 18 hours agoprevRequiring phone number is a straight no go from me, sorry. reply jillesvangurp 17 hours agoparentAnd it rejects my german phone number. So, double fail. What's this obsession with using phone numbers. It's 2024! Any scammer can buy a burner phone. Owning a phone proves nothing other than that you own a phone. reply cdchn 16 hours agorootparentI hate tying phone numbers to identity but it is a way to slow down the ability to create massive heaps of bot accounts. There really isn't a better way. (Insert \"better ways\" that aren't actually better ways below) reply meijer 17 hours agorootparentprevDid you possibly enter the prefix +49 as part of the number? It seems to expect a number without the country prefix, starting with a number like \"174\" or whatever your provider prefix is... reply jillesvangurp 17 hours agorootparentI tried all sorts of variations of my number. It just won't take it. I expect their validation is wonky/buggy. I used to work for Nokia and remember talking to people responsible for parsing phone numbers. This is not a trivial problem. Anyway, it hard rejects +49176..., 0049176..., 0176..., 176...; it correctly normalizes each of those to +49176... and then rejects the number. My number is fine. Their validation isn't. Anyway, for a new social network to repeat the security/privacy mistake of its predecessors (wrongly assuming phone operators are trustworthy and users never change their number) is just madness. Doesn't instill a lot of confidence that a lot of thought went into the whole thing. IMHO, phone numbers as a thing should just go away completely. Weird legacy identifiers from the last century. Absolutely no redeeming features. Hard to remember, easy to spoof. Etc. Why build your new network on the crumbling remains of an old one and give a lot of control to the typical abusers of the phone system (spammers, scammers, oppressive regimes, etc.)? reply jakebsky 16 hours agorootparentSorry about this. We've just made some changes to make this less likely of an issue going forward. reply kivle 15 hours agorootparentSame problem with my Norwegian phone number just now. reply lapcat 17 hours agoparentprevIt requires a phone number now? That's weird. The invites didn't require a phone number. reply cdchn 16 hours agorootparentBecause the numbers were limited before and now they've thrown open the doors to massive botnetting. reply jakebsky 17 hours agoparentprevUnderstandable! We hope to relax this but we felt it was necessary to maintain the quality of the network for now. reply anhner 17 hours agorootparentI thought that is why you might have required it, but on the other hand for malicious actors it's trivial to get a phone number unrelated to them, while legitimate users are giving up their only phone number, possibly exposing it to scammers and robocalls (in case of a breach), or having something linking to their real identity. Anyway, hoping you revisit this one day and that's also when I will try your app again. Wishing you luck! reply anigbrowl 11 hours agorootparentprevLong time user here, this is definitely not the way to go. reply omoikane 14 hours agorootparentprevCan people bypass the phone number requirement with invites? I still have a few invites left and I am wondering if they have value going forward. reply _Parfait_ 14 hours agorootparentprevI think we'd all rather not be flooded by bots so no reason to change reply smoothjazz 17 hours agorootparentprevDoesn't this go against the comic in the post that claims that it's easy to block what you don't want to see? Also that it's an open network? Why not just let everyone in if the moderation tools are good? I definitely would never give my phone number to a social network. reply jakebsky 17 hours agorootparentThe SMS verification requirement is only for the Bluesky operated PDS host. Soon (this month) it will be possible for others to self-host their own PDS hosts that do not have this requirement. reply jazzyjackson 16 hours agorootparenthow will the bluesky PDS deal with the flood of spammers on malicious PDSs, like, why is the phone number a useful gatekeeping tool if you're just going to fling open the gates at the end of the month? reply jakebsky 14 hours agorootparentWe do have an anti-abuse tool to help flag abuse. It is obviously important that PDS operators not allow themselves to become overrun with abusive users. Anyone technical can run their own single-user or low-user-count PDS and probably not have to worry about any problems. reply pc86 17 hours agorootparentprevHaving good tools to increase the quality of your network doesn't mean you should willingly let the whole network decline in quality. reply smoothjazz 17 hours agorootparentYou assume that requiring a phone number increases quality instead of decreases it. I see it as a filter to keep thoughtful people off the platform. reply tracker1 17 hours agoparentprevLikely a means of reducing bot accounts.. reply martin82 6 hours agoprevTried it out for a while, simultaneously with Nostr. It is noteworthy that Jack basically abandoned this and went all into Nostr. Bluesky is too much of a woke echochamber and it doesn't even have a real chance at achieving its mission, which was decentralized, permissionless, uncensorable sharing of information. Unfortunately, this project is dead on arrival. I don't think it will ever reach mainstream adoption. reply paxys 18 hours agoprevPretty ironic that when Twitter was imploding Meta launched a competitor that was fully compatible with ActivityPub/Mastodon while the team dedicated to building an open competitor (Bluesky) created a proprietary protocol that nobody uses. reply jakebsky 18 hours agoparentThreads has been promising to integrate with ActivityPub since it launched. To date they've done very little and their timeline extends until the end of 2024. I'd personally be very happy if Threads gives up control over their users but it remains to be seen. ActivityPub also lacks the very strong account portability feature that made AT Protocol necessary. AT Protocol is completely open source and the Bluesky network is completely open. The Bluesky network has had an open API for a year with full access to all public data (no auth required): websocat wss://bsky.network/xrpc/com.atproto.sync.subscribeRepos More info on: https://atproto.com reply threeseed 14 hours agorootparent> To date they've done very little Not true. You can now follow Adam Mosseri's Threads account from Mastodon. And like you said they are committed to fully rolling it out this year. reply jakebsky 14 hours agorootparentLimited functionality for a specified subset of users counts as \"very little\" to my mind given that they launched 6 months ago and have a team of hundreds working on it. But like I said, I sincerely do hope Threads follows through on their plan to federate. But it's just not correct to claim that they already have. reply threeseed 13 hours agorootparentMaybe you I and understand the software development process differently. Supporting one user end to end is a huge milestone and the first step in rolling it out to the other hundreds of millions of users. Especially when Threads isn't a standalone platform but is built on top of Instagram which means we could see it integrated with ActivityPub as well. The fact that Meta cares about ActivityPub at all is a huge win for open, interoperable standards. reply edavis 9 hours agorootparentI don't know... the whole Threads + ActivityPub thing just seems like Meta trying to keep the regulators at bay rather than them actually embracing the ethos of becoming a major federated social media player. So launch in July 2023, handful of accounts have AP support in November-ish 2023, full AP support end of 2024? I guess it's not nothing, they did have to bolt something entirely new on top of IG's infrastructure. But for a team that big, feels like we'd be seeing more if it was a true priority for them. Plus Threads leaning towards opt-in AP support which will only inhibit uptake (that was the last I heard, anyway). I'd be shocked if Instagram ever went with ActivityPub, though. reply ianopolous 17 hours agorootparentprevTo be clear, strong account portability predates Bluesky by ~5 years. Peergos[0], as reviewed by Jay before Bluesky Inc was created, has had this for years: https://book.peergos.org/features/migration.html [0] https://github.com/peergos/peergos reply mattl 14 hours agorootparentprevFWIW, you can now follow people on Threads from Mastodon and interact as if they're just another Mastodon user. reply Crosseye_Jack 18 hours agoparentprevDid threads launch with compatibility? Because I know a lot of “nsfw” creators that choose Bluesky over Threads because threads rules over NSFW content. Bluesky wasted a few opportunities they could have grabbed market share from Twitter simply because a) you needed and account to view posts (now fixed) b) you needed a code to sign up (now fixed). But has their ship sailed? L reply lionkor 18 hours agoprevWow that comic invokes very unpleasant feelings for me. Super weird, way too corporate for a ... comic...? Seems confused. Their big selling point is blocking stuff you don't wanna see? Thats the big selling point? And... exploration!? reply chihuahua 16 hours agoparentThe comic feels like self-parody. \"Come with me to the wonderful land of BlueSky, where everything is *open* and *magic* and all bad trolls are blocked!\" \"That's amazing! I think I'll like it here!\" \"YAY!\" reply dmix 17 hours agoparentprevI've seen the word \"block\" about 20 times since trying out Bluesky. Seems to be a core part of the culture there to aggressively block anyone posting stuff you don't want to see. On Twitter I just curate who I follow so I don't see annoying stuff and that's worked perfectly fine for me. reply raesene9 18 hours agoprevSlightly off-topic on this, but are there any apps that manage posting across BlueSky/Mastodon/Threads/Linkedin? With the fragmentation of social networks and different audiences being in different places, it's a bit annoying to have to manually post in different places (taking account of things like hashtag formats and post length limits), it'd be nice to have a site/app that helped with that. reply neogodless 17 hours agoparentThere have been a few. I know https://www.hootsuite.com is one of those, but does not currently seem updated to support any of the ones you asked about! https://fedica.com/ seems to maybe be a parent company (acquisition?) of Hootsuite. https://buffer.com/publish I first heard about this ages ago as a way to build a queue of things to put out to Twitter. They likely support Mastodon and LinkedIn now, but probably not yet the others. See https://support.buffer.com/article/567-supported-channels reply felixthehat 17 hours agoparentprevha same! I just last week started building an app for myself where I can cross post to mastodon and bluesky (& twitter is in progress, threads doesn't seem to have an api yet). https://grater.app (screenshot at https://imgur.com/lyefPGv ) reply mdorazio 18 hours agoparentprevDefinitely. Hootsuite is the one I’m familiar with but it’s targeted more at agencies or larger content creators and is priced for that. reply Hiko0 15 hours agoprevNo thanks. Mastodon has been there for ages and its background is not another VC funded startup company first burning through money and then finding no real way to sustain the platform, finally turning to ads and selling user data. reply patwolf 17 hours agoprevSign up. See the default feed is full of posts celebrating the death of Toby Keith. Delete Account. reply edavis 16 hours agoparentI think the \"default feed\" you're seeing is the \"Discover\" custom feed that shows up if you haven't followed anybody. Once you do that, your \"default feed\" is what your follows have posted in reverse chronological order. IMO, the \"Discover\" feed can be a bit much sometimes. It's very much the collective id of a certain type of social media poster. And not always my cup of tea. But the beauty of Bluesky is there are millions of accounts and thousands of custom feeds you can pick from to tailor your experience. For example, here's one where people share photos of mushrooms: https://bsky.app/profile/did:plc:hsqwcidfez66lwm3gxhfv5in/fe... reply thefz 13 hours agoparentprevMastodon: Sign up. Immediately receive a DM asking me to fund some person's sex change and weight loss therapy. Delete Account. Bluesky: Sign up. Feed is full of cartoons of anthropomorphic animals. Delete Account. reply CM30 16 hours agoprevIt's nice to see this is the case now, though I worry it may be a bit too late. Having BlueSky stuck behind an invite wall gave a huge boost in popularity to other alternatives like Mastodon and Threads, and even now I'm unlikely to find most of the accounts I want to follow on the former service as a result. It just feels super quiet to me at the moment, and I suspect the long time exclusivity played a big role in that. reply computer23 16 hours agoprevI've had some concerns about the namespacing issue. Bluesky accounts should be permitted to have both a bsky.social username and one with a custom domain. If I am a company and want to use @myname.com as my username, I would not want someone @myname.bsky.social to fall into the hands of anyone else. So it sort of necessitates signing up for 2 accounts, if only to reserve your name so nobody takes it. reply tristan957 15 hours agoparentMastodon solves this through verified accounts. reply renegade-otter 15 hours agoprevI am over social media just like I was over trying to become a mayor of something on FourSquare, wasting time on useless virtual accolades. There IS a way to use this to develop your online professional brand, but it is so hard not to get bogged down in the swamp full of below-average intelligence, bots, and now AI garbage. reply theryan 17 hours agoprevThe create account page seems to have incorrect form settings at least for Chrome. The email field prompts to create a new password (and generates it there) and the password field there is no prompt. reply ghaff 17 hours agoprevI sort of wonder if the whole short-form social media era just sort of faded away. After Musk essentially destroyed Twitter, it feels like--although various people casually migrated elsewhere--overall, it feels like a lot of people decided that a Twitter-like thing just wasn't part of their daily lives any more. Sometimes there's a forcing function that takes people off their automatic pilot and it feels like Musk caused a lot of people to do so with Twitter. reply WaffleIronMaker 17 hours agoparentI agree that, overall, short-form social media is leaving the mainstream with Twitter's decline. However, I still really enjoy short-form media. I can't speak for BlueSky, but I've been enjoying Mastodon a lot. Following people I like has given me a high enough signal to noise ratio that I often find myself saying \"Hey! Check out this thing I found on Mastodon!\". My friend just joined because of this, and they've been enjoying it too, specifically for programming, infosec, and queer memes (caveat N=2). Idk. I hope there's a future for it. reply ghaff 16 hours agorootparentI still have a Twitter/X account, got a Mastodon account with the Muskopalypse, and have had a Bluesky invite sitting in my mailbox for a while. It's just clearly moved on from something that was part of my routine to eh. reply mobiuscog 18 hours agoprevWhat does it offer over Mastodon, and following hashtags as topics ? reply PaulDavisThe1st 14 hours agoprevRiffing on a toot I saw this morning: Don't build your house on rented land. Especially if the rent is zero. reply yawebnw 16 hours agoprevThis is a response to Farcaster hitting great metrics lately. https://twitter.com/twobitidiot/status/1754905898743402558 reply srid 13 hours agoparentIf you are outside US, joining Farcaster via Warpcast will cost $5. I joined, and the UX does feel quite slick. Mostly it is just crypto users on this network. Note that Farcaster is not decentralized, but \"sufficiently decentralized\" https://www.varunsrinivasan.com/2022/01/11/sufficient-decent... reply davidw 15 hours agoprevI've been using Threads pretty exclusively lately. It's not perfect, but it's got what feels like critical mass and is being actively improved. It'll be interesting to see how this all shakes out. reply annexrichmond 16 hours agoprevI selected my interests as Cooking, Fitness, Nature, Video Games, and my feed has absolutely none of that. Just random politics, activism, and jabs at Twitter. Anyway, nope. reply BryantD 16 hours agoparentI don't actually know how the interests are intended to work, but I can say that the key feature of Bluesky is feeds. (I know this isn't at all obvious.) Feeds are third-party created, for the most part, and essentially function as filters. So, for example, I follow a movies feed. Any post which includes the movie camera emoji, the word \"filmsky\", or a few other keywords is included in the movies feed. It makes it very easy to swipe over and see discussions of cinema. That sounds (and is) a lot like tag-based feeds over on Twitter. However, there's additional potential. Behind the scenes, a feed is a service which takes the user info of the person viewing it and the firehose and decides which posts to include based on that input. So \"include all posts with these keywords\" is valid, but so is \"include the top 100 posts with these keywords, as measured by likes.\" Or \"show a feed including only the most recent post from every user the viewer follows.\" In other words: feeds are the way a third party can build their own algorithm for the firehose. Very powerful, very useful. reply ldoughty 15 hours agoparentprevI found this confusing too I think the interests section was to try get you to find people and feeds.. but after that, the feeds (or just the Discovery feed?) are influenced by who you follow.. so if you are offered a 'bad' (for you) starting group of people to follow, your feed will probably not be great until you find people you like (that are active) and follow them too. Also was unsure what \"Follow all +\" button was.. why not \"Follow Selected\".. and a 'deselect all'.. I had no idea who all but 2 of the suggestions were. Not going to follow a random \"Computer Scientist\" the system picked for me sight/posts unseen. reply bobajeff 18 hours agoprevWith the promises of AT Protocol I wonder what the reality of starting your own competing site to bluesky would be if it were to become as popular as Twitter. I've also been looking into LBRY too and have the same question in regards Odysee. I'm sure we won't know until someone tries. I have a feeling there might be some unforseen network effects or barriers that build up over time to prevent someone from coming along with a better site. I'm just curious to see what they are and how insurmountable they might be. reply suddenclarity 6 hours agoprevSigned up in an attempt to protect my username (already taken but couldn't verify without making an account). First impression? After selecting only tech subjects, my recommendations consisted of CNN journalists and newspapers. I decided to skip them and was forwarded to the Discovery feed which consists of American politics, Elon Musk hate, King Charles hate, and furries. So many furries. I don't see the attraction. It's just a different hate bubble. reply ChrisArchitect 18 hours agoprevThe comic on this post is a whole lot of mess. Talking about moderation? Talking about \"what if I want to leave\"? Those sound like (1) work for me, and (2) why would I want to leave if it's so great? Uggh. And the fragmentation into silos continues. reply edavis 16 hours agoparent> (2) why would I want to leave if it's so great? A lot of people felt burned by the changes at Twitter over the past 14 months or so. They made meaningful connections on there and with new ownership came changes that altered the character of the platform, in their eyes. But because Twitter is centralized, it's difficult to move your social graph to a new platform in a robust way. The promise of the AT protocol is being \"billionaire-proof.\" If Bluesky gets bought out and you don't like the new owner, you can move your entire social graph and all your posts to a new atproto service without needing permission from the old service. That would be the nuclear option. A smaller step you could take before that is use a different set of moderation services to curate the experience you want (more info: https://bsky.social/about/blog/4-13-2023-moderation) reply Hamuko 16 hours agoparentprev>why would I want to leave if it's so great? If you're on an instance in a federated network and the instance's admin decides that they don't want to keep hosting it, you don't really have an option to stay. reply Alifatisk 18 hours agoprevNote, it requires SMS verification reply hedora 17 hours agoparentI wonder if that's true if you set up your own federated server. reply jakebsky 17 hours agorootparentNope, it's not a requirement for other PDS operators. reply jayzalowitz 16 hours agoprevAnyone else seem to observe people who previous tended to move to bluesky and kill their twitter happened to be radical and potentially dangerous politicians (I can think of one in sf off the top of my head reply Finnucane 14 hours agoparent\"One\" is definitely a trend. reply smudgy 16 hours agoprevHey! Thanks for the heads up, signed up and now I'm going to do what I did when I signed up to Twitter in the deep past... lurk. reply eviks 18 hours agoprevDo they plan to remove the character limit to also be an alternative to other popular social networks? reply Aachen 17 hours agoprevAre there mobile apps for Blueky? When I type bluesky or bsky into f-droid, it comes up blank, and I'm not seeing a custom repository or even just an apk download mentioned on the website either. I thought it was like Mastodon except invite-only (till now), so was expecting a similar client ecosystem, but apparently it's more like Twitter was: open ecosystem closed software? Or is it like Tildes whose goal is to have a mobile website that is just as good as a native app? reply delecti 17 hours agoparentThere's an official app on the play store. I don't know if it has been mirrored to any other app stores/repositories, but perhaps knowing the app ID will help find it elsewhere. https://play.google.com/store/apps/details?id=xyz.blueskyweb... reply Aachen 17 hours agorootparentAurora lists a dependency on Google Play Services for that app, so probably notifications wouldn't work for me, if not more parts of that app But I appreciate the pointer! reply psionides 16 hours agoparentprevThere is an official mobile app for iOS and Android, on iOS there are also a few third party ones, not sure about Android. reply facorreia 18 hours agoprevWhy is it collecting dates of birth? reply aestetix 18 hours agoparentMost likely to enable protections for minors. Let's hope. reply psionides 16 hours agorootparentYup, exactly for this. There is a lot of NSFW stuff on Bluesky… reply speps 18 hours agoprevNice, hopefully it'll drown some of the more niche streams and surface more mainstream ones. reply add-sub-mul-div 18 hours agoprevI'm glad this happened as late as it did and that Threads absorbed the FB/IG people first. Let Twitter/Threads remain the home for the influencers, spammers, the eternal September. Hopefully Bluesky continues to attract a steady stream of early adopters but the followers never take over. reply lnxg33k1 18 hours agoprevHow many social networks recently, Lemmy? The other one with crypto guys? This one? It's 2005 all over again? I am wondering if it's either because it's a relatively easy thing to code or because investors are likely to throw money at it? reply kevinmchugh 18 hours agoparentMore, smaller networks seems like a big improvement over a handful of too-big, everyone's-there networks. reply RankingMember 18 hours agorootparentI think the \"everyone's there\" networks were nice due to the cross-pollination aspect. I would see random trending stuff that I didn't know I would find interesting pop up. reply Mistletoe 18 hours agoparentprevProbably because we need new ones badly. Reddit and Twitter are two sinking bot-infested enshittification ships and we need a new place to go. Network lock-in is proving to be a very difficult nut to crack in the dystopia we have created though. You need people to jump ship but no one wants to go where the millions of eyes aren’t. reply miroljub 18 hours agorootparentWhy do you think any halfway popular social network won't be infested with bots and commercalizers? reply RankingMember 18 hours agorootparentMy hope anyways is that there are more effective countermeasures. I remember Twitter not being nearly as bad before, but that was possibly just because verification actually meant something more than \"have $10\" and blue checks weren't turbo-boosted. reply MSFT_Edging 17 hours agorootparentThe fact that the verification system is paid, and that is abused by bots, means twitter no longer has any incentive to remove said bots. Its basically the entire ad industry, there's little proof ads work, the metrics for ads are all inflated, yet its one of the biggest money makers out there. Same deal with the bots grinding views and likes for marginal payouts. reply Mistletoe 17 hours agorootparentprevReddit wasn’t for a long time. Having leadership in place that actually cares about the users and doing something about bots would go a long way. Maybe when Reddit IPOs and it is a huge bust we can get some new sites going. reply who-shot-jr 18 hours agoprevAll the single word handles seem to be taken! :( reply paxys 18 hours agoparentYou don't need a bsky.social handle. Bring whatever domain you want. reply psionides 16 hours agoparentprevGet a short domain somewhere and use it as the handle, custom handles are cooler anyway :) reply Kye 18 hours agoprevDupe: https://news.ycombinator.com/item?id=39274438 reply wesleytodd 18 hours agoparentyeah but this one is more official right. It is from one of their team members. reply riffic 18 hours agoparentprevwhy do we say dupe on orange site? this is like the most trivial inconsequential thing to call out, let this be duped. This is going to be news to someone. to add some more substance to my comment this is how I see Bluesky's virality/stickiness playing out: everyone's going to move to it, it's going to be an unofficial but incompatible \"fediverse2\", folks will pretend the actual Fediverse doesn't exist, and then the actual Bluesky public benefit corporation (or some derivative, come on guys you wanna get filthy rich) will eventually turn to a data brokerage/ advertising play to enshittify the entire thing. reply nickthegreek 18 hours agorootparentWe say dupe so a moderator can combine the listings into one and merge the conversations. The original submission is 40mins older, don't worry the people will still see the news. A single article with more upvotes and comments will raise up further/stay there than 2 articles with moderate engagement. reply riffic 18 hours agorootparentokay that actually makes sense I appreciate your explanation. reply legohead 18 hours agoprevCan't choose your own handle (it's taken). reply chomp 18 hours agoparentYour handle can be your personal domain or any domain also. reply arcalinea 18 hours agorootparentYep — tutorial and some examples: https://bsky.social/about/blog/4-28-2023-domain-handle-tutor... reply legohead 15 hours agorootparentprevMy domains are not my handle, and even so, I can't buy a domain with the handle I'd prefer (it's taken). My point was subtle, that this shouldn't be an issue. It has been solved in various ways. reply rvz 16 hours agoprevUnsurprisingly, as predicted they eventually lifted the invite system. Unfortunately the interest in Bluesky is not the same as it was, but still a viable Twitter alternative when compared to the others. But my goodness this certainly did not age well at all. [0] [0] https://news.ycombinator.com/item?id=35876304 reply hw4m 16 hours agoprevYet another social media i don't want to sign up for. Sorry, I'm getting old and grumpy, but I'm not using any of the other social media sites, and I'm not planning on adding one to the list. reply itsthecourier 16 hours agoprevbluesky is really a bad name for a social network. sounds too entreprisy, compare it to: twitter, instagram, tiktok reply mattl 14 hours agoparentI figured it related to blue-sky thinking. https://en.wiktionary.org/wiki/blue-sky_thinking reply dev1ycan 18 hours agoprevMusk has done a lot of questionable things, but does anyone remember how actually unusable twitter was before the sale? Musk Twitter has its own set of problems but Jack has proved he has no idea what he's doing reply jiripospisil 17 hours agoparentI remember that before the whole fiasco of a sale Twitter was a place where I could follow interesting tech people. These days those people have left for Mastodon etc. (or stopped posting entirely) and were replaced with hoards of $8 spammers whose only interest is to push inflammatory content to generate the most views and get paid for it. reply TillE 15 hours agoparentprevDorsey isn't running Bluesky, he's just one member of the board. reply timeon 17 hours agoparentprev> how actually unusable twitter was before the sale? I do not know how usable it was but at least I was able to see the posts. Now I'm not. Well except for some lucky occasion when it shows the post just without replies. reply hedora 18 hours agoparentprevAt this point, whenever I go to twitter, it shows status updates that are years out of date. For example, compare this: https://ubuntu.social/@launchpadstatus to this: https://twitter.com/launchpadstatus?lang=en reply internetter 17 hours agorootparentThis is because you are logged out. Please note that this is not me advocating for this practice. It has broken the web in immensely frustrating ways. reply hedora 17 hours agorootparentGiven that it appears to be completely broken (last tweet for that account was 6 hours ago, and it's showing a 6 month old cached version), why would I bother creating an account? reply biggestfan 16 hours agorootparentWhen you're not logged in, the profile view shows you the most liked posts from that account, not the most recent. Not sure why. reply internetter 17 hours agorootparentprevI agree. It's ridiculous. I was simply explaining why this behaviour happened. reply psionides 16 hours agoparentprev… no? It was very usable before, it's much less usable now. reply HumblyTossed 18 hours agoprev [–] What's bluesky again? reply Mashimo 18 hours agoparentJust from reading the page it seems like a mix of twitter and reddit. You can follow certain channels instead of an algorithm. Not sure what they mean with \"Dock it every port, your network comes with your\" Is it federated? reply worldsayshi 18 hours agoparentprev [–] A Twitter alternative. reply pier25 18 hours agorootparent [–] by Jack Dorsey (he co-founded Twitter) reply pfraze 18 hours agorootparent [–] Jack directed Twitter to fund Bluesky when he was CEO of Twitter, but Jay Graber is the CEO of Bluesky and the technology & app was created by Jay and her team (which I'm a part of). reply pier25 18 hours agorootparent [–] Thanks for the clarification. Is he not involved in it anymore? reply pfraze 18 hours agorootparent [–] He's on the board but he's never been involved directly reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bluesky, an open social network, is now open to anyone without requiring an invite code.",
      "The platform has been developing features like moderation tooling and custom feeds.",
      "They are experimenting with \"federation,\" which aims to create a more open and customizable network where developers can self-host a server."
    ],
    "commentSummary": [
      "Bluesky is a decentralized social network that aims to promote open federation.",
      "Comparisons are being made between Bluesky's business model and Netscape's impact on web development, although opinions on its significance are mixed.",
      "Concerns are raised about the financial sustainability of Bluesky and the challenges of monetization, as well as technical hurdles, account portability, server shutdowns, and the need for regulation in the tech industry."
    ],
    "points": 312,
    "commentCount": 255,
    "retryCount": 0,
    "time": 1707231688
  },
  {
    "id": 39276687,
    "title": "AdGuard Home: Network-wide ad- and tracker-blocking DNS server",
    "originLink": "https://github.com/AdguardTeam/AdGuardHome",
    "originBody": "Privacy protection center for you and your devices Free and open source, powerful network-wide ads & trackers blocking DNS server. AdGuard.comWikiRedditTwitterTelegram AdGuard Home is a network-wide software for blocking ads and tracking. After you set it up, it'll cover ALL your home devices, and you don't need any client-side software for that. It operates as a DNS server that re-routes tracking domains to a “black hole”, thus preventing your devices from connecting to those servers. It's based on software we use for our public AdGuard DNS servers, and both share a lot of code. Getting Started Automated install (Linux/Unix/MacOS/FreeBSD/OpenBSD) Alternative methods Guides API Comparing AdGuard Home to other solutions How is this different from public AdGuard DNS servers? How does AdGuard Home compare to Pi-Hole How does AdGuard Home compare to traditional ad blockers Known limitations How to build from source Prerequisites Building Contributing Test unstable versions Reporting issues Help with translations Other Projects that use AdGuard Home Acknowledgments Privacy Getting Started Automated install (Linux/Unix/MacOS/FreeBSD/OpenBSD) To install with curl run the following command: curl -s -S -L https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.shsh -s -- -v To install with wget run the following command: wget --no-verbose -O - https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.shsh -s -- -v To install with fetch run the following command: fetch -o - https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.shsh -s -- -v The script also accepts some options: -cto use specified channel; -r to reinstall AdGuard Home; -u to uninstall AdGuard Home; -v for verbose output. Note that options -r and -u are mutually exclusive. Alternative methods Manual installation Please read the Getting Started article on our Wiki to learn how to install AdGuard Home manually, and how to configure your devices to use it. Docker You can use our official Docker image on Docker Hub. Snap Store If you're running Linux, there's a secure and easy way to install AdGuard Home: get it from the Snap Store. Guides See our Wiki. API If you want to integrate with AdGuard Home, you can use our REST API. Alternatively, you can use this python client, which is used to build the AdGuard Home Hass.io Add-on. Comparing AdGuard Home to other solutions How is this different from public AdGuard DNS servers? Running your own AdGuard Home server allows you to do much more than using a public DNS server. It's a completely different level. See for yourself: Choose what exactly the server blocks and permits. Monitor your network activity. Add your own custom filtering rules. Most importantly, it's your own server, and you are the only one who's in control. How does AdGuard Home compare to Pi-Hole At this point, AdGuard Home has a lot in common with Pi-Hole. Both block ads and trackers using the so-called “DNS sinkholing” method and both allow customizing what's blocked. We're not going to stop here. DNS sinkholing is not a bad starting point, but this is just the beginning. AdGuard Home provides a lot of features out-of-the-box with no need to install and configure additional software. We want it to be simple to the point when even casual users can set it up with minimal effort. Disclaimer: some of the listed features can be added to Pi-Hole by installing additional software or by manually using SSH terminal and reconfiguring one of the utilities Pi-Hole consists of. However, in our opinion, this cannot be legitimately counted as a Pi-Hole's feature. Feature AdGuard Home Pi-Hole Blocking ads and trackers ✅ ✅ Customizing blocklists ✅ ✅ Built-in DHCP server ✅ ✅ HTTPS for the Admin interface ✅ Kind of, but you'll need to manually configure lighttpd Encrypted DNS upstream servers (DNS-over-HTTPS, DNS-over-TLS, DNSCrypt) ✅ ❌ (requires additional software) Cross-platform ✅ ❌ (not natively, only via Docker) Running as a DNS-over-HTTPS or DNS-over-TLS server ✅ ❌ (requires additional software) Blocking phishing and malware domains ✅ ❌ (requires non-default blocklists) Parental control (blocking adult domains) ✅ ❌ (requires non-default blocklists) Force Safe search on search engines ✅ ❌ Per-client (device) configuration ✅ ✅ Access settings (choose who can use AGH DNS) ✅ ❌ Running without root privileges ✅ ❌ How does AdGuard Home compare to traditional ad blockers It depends. DNS sinkholing is capable of blocking a big percentage of ads, but it lacks the flexibility and the power of traditional ad blockers. You can get a good impression about the difference between these methods by reading this article, which compares AdGuard for Android (a traditional ad blocker) to hosts-level ad blockers (which are almost identical to DNS-based blockers in their capabilities). This level of protection is enough for some users. Additionally, using a DNS-based blocker can help to block ads, tracking and analytics requests on other types of devices, such as SmartTVs, smart speakers or other kinds of IoT devices (on which you can't install traditional ad blockers). Known limitations Here are some examples of what cannot be blocked by a DNS-level blocker: YouTube, Twitch ads; Facebook, Twitter, Instagram sponsored posts. Essentially, any advertising that shares a domain with content cannot be blocked by a DNS-level blocker. Is there a chance to handle this in the future? DNS will never be enough to do this. Our only option is to use a content blocking proxy like what we do in the standalone AdGuard applications. We're going to bring this feature support to AdGuard Home in the future. Unfortunately, even in this case, there still will be cases when this won't be enough or would require quite a complicated configuration. How to build from source Prerequisites Run make init to prepare the development environment. You will need this to build AdGuard Home: Go v1.20 or later; Node.js v16 or later; npm v8 or later; yarn v1.22.5 or later. Building Open your terminal and execute these commands: git clone https://github.com/AdguardTeam/AdGuardHome cd AdGuardHome make Building with Node.js 17 and later In order to build AdGuard Home with Node.js 17 and later, specify --openssl-legacy-provider option. export NODE_OPTIONS=--openssl-legacy-provider NOTE: The non-standard -j flag is currently not supported, so building with make -j 4 or setting your MAKEFLAGS to include, for example, -j 4 is likely to break the build. If you do have your MAKEFLAGS set to that, and you don't want to change it, you can override it by running make -j 1. Check the Makefile to learn about other commands. Building for a different platform You can build AdGuard Home for any OS/ARCH that Go supports. In order to do this, specify GOOS and GOARCH environment variables as macros when running make. For example: env GOOS='linux' GOARCH='arm64' make or: make GOOS='linux' GOARCH='arm64' Preparing releases You'll need snapcraft to prepare a release build. Once installed, run the following command: make build-release CHANNEL='...' VERSION='...' See the build-release target documentation. Docker image Run make build-docker to build the Docker image locally (the one that we publish to DockerHub). Please note, that we're using Docker Buildx to build our official image. You may need to prepare before using these builds: (Linux-only) Install Qemu: docker run --rm --privileged multiarch/qemu-user-static --reset -p yes --credential yes Prepare the builder: docker buildx create --name buildx-builder --driver docker-container --use See the build-docker target documentation. Debugging the frontend When you need to debug the frontend without recompiling the production version every time, for example to check how your labels would look on a form, you can run the frontend build a development environment. In a separate terminal, run: ( cd ./client/ && env NODE_ENV='development' npm run watch ) Run your AdGuardHome binary with the --local-frontend flag, which instructs AdGuard Home to ignore the built-in frontend files and use those from the ./build/ directory. Now any changes you make in the ./client/ directory should be recompiled and become available on the web UI. Make sure that you disable the browser cache to make sure that you actually get the recompiled version. Contributing You are welcome to fork this repository, make your changes and submit a pull request. Please make sure you follow our code guidelines though. Please note that we don't expect people to contribute to both UI and backend parts of the program simultaneously. Ideally, the backend part is implemented first, i.e. configuration, API, and the functionality itself. The UI part can be implemented later in a different pull request by a different person. Test unstable versions There are two update channels that you can use: beta: beta versions of AdGuard Home. More or less stable versions, usually released every two weeks or more often. edge: the newest version of AdGuard Home from the development branch. New updates are pushed to this channel daily. There are three options how you can install an unstable version: Snap Store: look for the beta and edge channels. Docker Hub: look for the beta and edge tags. Standalone builds. Use the automated installation script or look for the available builds on the Wiki. Script to install a beta version: curl -s -S -L https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.shsh -s -- -c beta Script to install an edge version: curl -s -S -L https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.shsh -s -- -c edge Report issues If you run into any problem or have a suggestion, head to this page and click on the “New issue” button. Please follow the instructions in the issue form carefully and don't forget to start by searching for duplicates. Help with translations If you want to help with AdGuard Home translations, please learn more about translating AdGuard products in our Knowledge Base. You can contribute to the AdGuardHome project on CrowdIn. Other Another way you can contribute is by looking for issues marked as help wanted, asking if the issue is up for grabs, and sending a PR fixing the bug or implementing the feature. Projects that use AdGuard Home AdGuard Home Remote: iOS app by Joost. Python library by @frenck. Home Assistant add-on by @frenck. OpenWrt LUCI app by @kongfl888 (originally by @rufengsuixing). Terminal-based, real-time traffic monitoring and statistics for your AdGuard Home instance by @Lissy93 AdGuard Home on GLInet routers by Gl-Inet. Cloudron app by @gramakri. Asuswrt-Merlin-AdGuardHome-Installer by @jumpsmm7 aka @SomeWhereOverTheRainBow. Node.js library by @Andrea055. Browser Extension by @satheshshiva. Acknowledgments This software wouldn't have been possible without: Go and its libraries: gcache miekg's dns go-yaml service dnsproxy urlfilter Node.js and its libraries: And many more Node.js packages. React.js Tabler whotracks.me data You might have seen that CoreDNS was mentioned here before, but we've stopped using it in AdGuard Home. For the full list of all Node.js packages in use, please take a look at client/package.json file. Privacy Our main idea is that you are the one, who should be in control of your data. So it is only natural, that AdGuard Home does not collect any usage statistics, and does not use any web services unless you configure it to do so. See also the full privacy policy with every bit that could in theory be sent by AdGuard Home is available.",
    "commentLink": "https://news.ycombinator.com/item?id=39276687",
    "commentBody": "AdGuard Home: Network-wide ad- and tracker-blocking DNS server (github.com/adguardteam)267 points by kls0e 17 hours agohidepastfavorite216 comments JadoJodo 16 hours agoI ran a competing project[0] on my home network for a few years before I discovered NextDNS[1]. What I lost in performance (requests don't leave my house) I gained in portability: ALL my devices can take advantage – at home and away – and time-saved. PiHole works 90% of the time, but when it did stop working, I'd have to spend a bit of time fixing it. At $20/year, I simply couldn't compete with NextDNS. Note: This isn't a shill for NextDNS; I love these kinds of projects and think they absolutely should exist, but NextDNS just happens to be one of those dead-simple SaaS tools that is an insanely good value. 0 - https://pi-hole.net/ 1 - https://nextdns.io reply sangnoir 4 hours agoparent> PiHole works 90% of the time, but when it did stop working, I'd have to spend a bit of time fixing it. I don't know what problems you had with your Pi that resulted in 10% downtime, but that sort of hyperbole sounds a lot like shilling. Cases of SD card corruption are 99.9% due to the use of underpowered power supplies - just buy the official Raspberry Pi power supply if you can be bothered to search for a proper 2.5-3A USB power supply. > At $20/year [...] At $20 a year, I could buy a RPi Zero 2W and an SD card to keep as a spare every single year and have enough left over for a celebratory Sheetz sandwich. PiHole + WireGuard + $15 RPi Zero (once off) are unbeatable. reply yumraj 3 hours agorootparentDon’t want to jinx it but I’ve been running a pihole on a RPi 3 for a really long time - at least 6-7 years and the only thing I’ve had to do is an occasional upgrade. I like the convenience and the fact that I’m blocking about 4M domains. My TV is also forced to use it so ads don’t update on Android TV. Not sure if NextDNS supports custom domain lists or not. reply ciceryadam 2 hours agorootparentYep, NextDNS supports custom domain lists. reply kelnos 3 hours agorootparentprevI think it's weird when people suggest that a self-hosted on-prem solution requires no maintenance and has so little downtime such that the time spent fixing issues doesn't matter. I run a bunch of local services on RPis and a decade-old Mac Mini. I love having the control over things, but I don't pretend I don't spend a decent amount of time maintaining it. I only run things that don't need to be highly available, so something like Pi-Hole is off the table. The last thing I want is for our DNS to go out while I'm sleeping, and my partner has to wake me up because she has work to do. You mention SD card corruption as the only reason why a RPi-based service might fail, but there are plenty of others: botched updates, random hardware failures, power supply issues, and likely other things I'm not thinking of. And even if a Pi-Hole can keep three nines of uptime (I'm skeptical of this claim), many people will find significant value in giving someone else money so they don't even have to think about digging into fix a problem for the rare occasion it happens. Suggesting that a particular home-hosted solution is \"unbeatable\" is meaningless; \"unbeatable\" in this case is a subjective measure, and other people will value different things than you do. reply pastorhudson 3 hours agorootparentprevWell you’re not wrong about Sheetz. Ha reply andreagrandi 2 hours agorootparentprevbecause your electricity bill is 0, right :D ? reply oivey 2 hours agorootparentEffectively, yes, for how much it costs to run. You know if you pay for a service that your subscription partially goes toward their power bill, right? reply evanreichard 14 hours agoparentprevI'm curious what issues you ran into with Pi-hole? I was running my instance for years without a single hiccup. I ended up moving to AdGuard Home about a year ago though because I wanted to run it on my OPNSense box. I have an automatic WireGuard VPN set up on my devices to VPN into my home network when I'm not connected to my SSID, so my local DNS still works remotely. reply RulerOf 9 hours agorootparent> I'm curious what issues you ran into with Pi-hole? My primary problem with Pi-hole or any other DNS-based blocker is that it silently breaks things. YouTube stopped saving my spot in videos. I couldn't click through on any link that involved a tracking service. These things accomplish their stated task well, but leave behind an insidious trail of browser errors, broken pages, and broken apps without ever indicating to the user what the cause of the problem really is. DNS just isn't the right tool for fixing shitty UX in the browser DOM or a mobile app. It's a happy coincidence that it works more often than not. reply Rastonbury 8 hours agorootparentIt must be the lists in pihole or something, I don't get any of those issues with NextDNS, if anything Ublock breaks sites before it does reply instagib 14 minutes agorootparentYeah nextdns regularly blocks things I don’t want to see and many email tracking links fail, some online stores don’t work (https://www.thermoworks.com/) and it’s really easy to turn off on my phone. I saw some people setup pihole 5min temporary off buttons one way or another to get by. I run lockdown also. reply foxylad 9 hours agorootparentprevOdd - I have a pi-hole on my home network and never hit the issue with YouTube. The only breakage I've found is the top \"results\" (actually sponsored ads) on Google search don't work, but I always scroll past those anyway to discourage bad behaviour. In fact pi-hole works so well that I'm always struck by how awful the internet has become when I venture away from my home network. Doctorow's enshitification in action. reply RulerOf 8 hours agorootparentThe YouTube thing was what turned me on to Pi-Hole's list of commonly-whitelisted domains[1], but even after adding it, the experience of things breaking was just ultimately too frustrating to keep using it. It's really an issue with feedback, though. When my ad blocker breaks a page, it says that it blocked something. When pi-hole breaks a page, it just appears to be broken. 1: https://discourse.pi-hole.net/t/commonly-whitelisted-domains... reply jethro_tell 9 hours agorootparentprevIs this an issue that next dns fixes for you? reply RulerOf 8 hours agorootparentNever used it, but I wouldn't expect it to, assuming it works the same way. reply theshrike79 10 hours agorootparentprevSD card corruption that just slowly started degrading the results, twice. For the price of a single Pi, I can get NextDNS ad protection for _all_ my devices for multiple years. No matter where they are. reply pdimitar 10 hours agorootparentRunning pihole on a Pi is severely overrated. I run it on my NAS Linux server (in a Docker container) where I have a bunch of other things. Zero problems, now using it for more than two years. reply throwaway742 10 hours agorootparentprevJust run it in a container. No need to use an actual Pi. reply stupidog 8 hours agorootparentprevSame here. After a few SD Card corruptions, I was done. NextDNS has been fantastic. And like you said, easily portable. reply Moru 2 hours agorootparentThe Pi needs a bit more power than most USB powerplugs deliver, did you get any warnings about underpower? The SD Card corruptions are often caused by this. reply zikduruqe 12 hours agorootparentprev> I have an automatic WireGuard VPN set up on my devices to VPN into my home network when I'm not connected to my SSID, so my local DNS still works remotely. Exact same setup for me also. I also run Tailscale since I have run into some remote networks that blocked wireguard's port. reply progbits 9 hours agorootparentHow's the latency? I like the idea and might set that up but my residential ISP doesn't have great peering and latency isn't great. I wonder if that extra roundtrip would be noticable or not. reply omnicognate 3 hours agorootparentI do this from my phone with crummy copper ADSL at home that getsI never felt comfortable putting my partner on the same vlan that it was serving DNS requests for fear that something would break for them when I was out of town One potential workaround, if your hardware supports it, is to broadcast two separate SSIDs for general users: one with a blocklist, and one without as a fallback. Users just need to know when to use each. reply qzx_pierri 12 hours agorootparentprevCouldn't you just monitor the query log and whitelist domains that were false positives? reply kelnos 3 hours agorootparent\"Just\" is doing a lot of work in that sentence. That sounds like a lot of work, and it isn't always obvious which weirdly-spelled domain is causing the issue. reply vin047 7 hours agorootparentprevThis is the way. Added Unbound as my upstream DNS server in recursive mode for extra privacy! reply lencastre 12 hours agorootparentprevIs there any config update to the wire guard profile needed to ensure that DNS request traffic is routed through pi-hole? reply evanreichard 12 hours agorootparentI use the bare WireGuard app on iOS. I just statically set the DNS server to the AdGuard Home IP (or Pi-hole IP) on my local network in the app. reply fdgadfagfgd 13 hours agorootparentprevI think op's saying local DNS was fine and preferred, just not usable outside the home network. reply drewg123 16 hours agoparentprevI love NextDNS. The one (fairly huge) issue that I have is that it cannot handle captive portals when its enabled on my iPhone. So if I'm joining the wifi on a plane, etc, I need to remember to turn it off. This means that I cannot recommend it to my non-technical friends. reply air7 13 hours agorootparentA general trick for bringing up the captive portal manually is to browse to a non ssl url such as http://example.com The portal would unapologeticly mitm the server response with a redirect to the portal login page. The domain needs to exist (to pass DNS) and not have HSTS, but otherwise any address will do. reply ssklash 13 hours agorootparenthttp://neverssl.com/ is my go-to for this. reply scosman 12 hours agorootparentNot http://nevertls.com ? reply hn_go_brrrrr 2 hours agorootparentOr http://gstatic.com/generate_204? reply maronato 16 hours agorootparentprevI’ve been using NextDNS for a little while and don’t remember having issues with captive portals on my iPhone. Maybe something changed? reply hipsterstal1n 16 hours agorootparentMost likely it's due to the different lists you can add or use on NextDNS. I also have issues with captive portals (I run a number of lists on NextDNS) and I just flip it off and on when I need to. reply mbesto 16 hours agorootparentInteresting. I've had the same issues. Is there a captive portal whitelist somewhere? reply drewg123 15 hours agorootparentprevI just checked, and I don't use any lists, except for an allow list I just started with captive portal domains. Eg .aainflight.com, .captive.apple.com, etc reply JulianWasTaken 16 hours agoparentprevInteresting -- for me pi-hole has worked for so long that I've forgotten my login even, but when I redo my home network in the near future I definitely intend to re-evaluate the options. Sounds like I've got 3 now... reply nickthegreek 15 hours agorootparentyou are gonna want to do a 'pihole -up' every few months. I would suggest finding that password! reply i2shar 15 hours agoparentprevHaven’t used NextDNS but have used PiHole and currently running AdGuard Home. But if you are paying $20/year just for DNS encryption/blocking, you may consider upgrading to Mullvad which gives you DNS Ad blocking but also IP anonymity, tunneling etc. reply schleck8 5 hours agorootparentThe issue being that it decreases your connection speed and increases your latency while good DNS naturally doesn't. reply oceanplexian 13 hours agorootparentprevExcept all of these third party VPN and DNS type services are literally NSA honeypots and privacy nightmares. I get that you have to do DNS lookups somewhere, but I'm not going to make it ridiculously trivial for a bad actor to scoop up all that data conveniently in a central location. reply screamingninja 12 hours agorootparent>> consider upgrading to Mullvad > all of these third party VPN and DNS type services are literally NSA honeypots https://mullvad.net/en/help/privacy-policy It is up to you to decide what you believe, but Mullvad is a swiss company that does not ask for your personal information for signup and even allows payment in cash. You hurt your own credibility each time you make an unqualified claim without looking into it. reply fetzu 10 hours agorootparentIt is in fact a Swedish (and not a Swiss) company. reply the-dude 11 hours agorootparentprevSwiss : https://en.m.wikipedia.org/wiki/Crypto_AG reply hackeman300 13 hours agorootparentprevMullvad is an NSA honeypot? Got any sources on that? reply lencastre 12 hours agorootparentYes, let me just get my tin foil roll, stand up in front of the mirror,… reply nprateem 12 hours agorootparentprevI agree there's a very high chance they and the majority of other VPNs are - or if not the US some other intel org. The US government has form (what was that early crypto machine they sold to allies and it was backdoored?), and they'd be foolish to miss such a strategically obvious play. reply ThePowerOfFuet 14 hours agorootparentprevThe two are not the same; with NextDNS I can choose to enable logging and see all requests from each device, as well as allowlist/denylist any domain/subdomain I want. reply Rastonbury 8 hours agorootparentNot familiar with pihole but are there not ways to do those things on it? reply screamingninja 12 hours agoparentprevI setup Pi Hole with tailscale on an inexpensive cloud server. It is configured to serve DNS requests over the tailscale interface. Also added tailscale IP address of the Pi Hole to tailscale DNS override to ensure that all devices on the tailnet use it without any additional reconfiguration. For redundancy, I have multiple DNS servers on my tailnet. Family and friends can use it without worrying about portability and be protected at all times, especially on cell networks. reply scosman 12 hours agorootparentTried this. Latency of DNS so critical, wasn't loving the self host option. Plus Tailscale wasn't quite reliable enough for all DNS traffic outside the house. I ended up with Pi-Hole on local network (manual DNS tied to Wifi SSID), NextDNS as default/fallback on other networks. reply snailmailman 5 hours agoparentprevI run a pihole server for myself- and access it over VPN when I’m traveling. But I’ve tried NextDNS and can confirm it works pretty well. Set my grandmother up on the free tier and within the first week it stopped her from getting phished, because the scam text she clicked went to a site that wouldn’t resolve. reply temp0826 13 hours agoparentprevHappy nextdns user here who used to have an overly-complicated setup with pihole and vpns etc. The only thing I have to complain about is the iOS app- I really wish it had a builtin way for viewing logs and white/blacklisting domains from the app, without having to go to the site. (Other settings would be nice too, sure, but as aggressive as I run it I find myself fiddling with the whitelist the most) reply JaggedJax 13 hours agoparentprevI've used ControlD [https://controld.com/] for this and liked it. Does anyone know how NextDNS compares to it? ControlD has worked well for me, outside a few UI complaints I have with their site. I do have some concerns with trust as I don't know much about ControlD, and I'd rather use the most trusted service for this. reply rnicholus 13 hours agorootparentI've been a NextDNS user for years now, and am trying out ControlD (last week) before I commit to switching. NextDNS development seems to have stalled and there are a number of conveniences missing, such as being able to label allowlist entries (ControlD supports this). Also, running the NextDNS app on a device that use a different profile then the one on my home router results in constant issues when the device wakes from sleep (not able to resolve domains for a noticeable amount of time on wake). NextDNS claims this is an Apple issue, but I don't think that's entirely true. Certainly not a problem for other similar services. I'm seeing ControlD as much more feature-rich and the service is evolving faster. I also personally like the UI a bit more vs NextDNS. Prices are comparable. reply SparkyMcUnicorn 13 hours agorootparentIt looks like cost is not comparable. ControlD pricing is per user and a router costs $5/month, but NextDNS is a flat $20/year. So ControlD would be significantly more than NextDNS for me personally. reply jostyee 3 hours agorootparentStackSocial has a five year deal for $39.99 https://www.stacksocial.com/sales/control-d-5-year-subscript... reply rnicholus 12 hours agorootparentprevIt's very much comparable...for personal use: https://controld.com/plans?step=plans reply SparkyMcUnicorn 11 hours agorootparentWith your link, I'm only seeing \"Free Trial\". While I'm not seeing any pricing for personal use (without signing up at least), I'll take you at your word. Maybe I'll give it a try sometime. reply rnicholus 11 hours agorootparentThat's odd. Even in incognito mode i see 2 plans and 2 prices for personal use. reply JaggedJax 12 hours agorootparentprevTheir \"personal\" pricing is $20 per year. It looks like they've moved that to a separate pricing page and are gearing the other for business use. reply muppetman 9 hours agoparentprevOn my Pixel I just set Private DNS. Yea I had to setup a SSL certificate but that's easy to do. So when I leave home, I still use my Adguard server for adblocking without having to touch settings etc (except, as mentioned, captival portals) I could do the same with \"vanilla\" DNS (udp port 53) as well, but I don't. Pihole can't, easily, do Dns vis TLS/QUIC etc without 3rd party stuff being bolted on etc. Adguard Home is a single binary, it's great. reply idatum 13 hours agoparentprevI ran Pi-hole along with my OpenBSD router running unbound for some period. Then I realized I can download the same entries used for Pi-hole, AdGuard, uBlock, etc. I created a simple script that generates an unbound configuration that I can include in my unbound.conf file. One advantage over Pi-hole I noticed is I can return NXDOMAIN which makes more sense to me. I didn't see how I had that option with Pi-hole. I just checked, and the generated unbound configuration comes in at 218000 lines, so takes a moment on my Celeron J3060 class router when loading unbound. reply anon9874 11 hours agorootparentCare to share your script? reply idatum 11 hours agorootparentIf I recall, I was inspired by this: https://www.tumfatig.net/2022/ads-blocking-with-openbsd-unbo... reply anon9874 11 hours agorootparentThanks! reply lnxg33k1 14 hours agoparentprevI also switched from pihole, because of the random disservice, I’d have it working, the suddently it would just stop, without changing anything, and even having it in their own docker container, unbelievable, I am quite happy with adguardhome, but now I kinda would try this nextdns reply afruitpie 15 hours agoparentprevAnother great (and free!) option is Mullvad’s ad-blocking DNS over TLS or HTTPS. https://mullvad.net/en/help/dns-over-https-and-dns-over-tls reply therealmarv 16 hours agoparentprev+1 for nextdns definitely, that would be also my preferred choice. Alternative and free for private usage is to set DNS to: dns.adguard-dns.com on your devices to block ads with DNS. UPDATE: it seems the old one was dns.adguard.com (which was blocked in some countries) reply bityard 15 hours agorootparentFor the home-gamers without a strong grip of DNS, note that you can't enter a domain name into your resolver fields, you have to use the IPs: 94.140.14.14 94.140.15.15 2a10:50c0::ad1:ff 2a10:50c0::ad2:ff Also, it looks like https://dns.adguard-dns.com/ redirects to https://adguard-dns.io/ which is a paid service for more advanced DNS filtering, a la NextDNS. reply vin047 7 hours agorootparentprev9.9.9.9 from Quad9 is another great, free, pro-privacy alternative. reply greenie_beans 16 hours agorootparentprevomg, thank youuuu reply stranded22 13 hours agoparentprevI love nextdns - pihole was fine but required admin, and I also had challenges vpn’ing in to use it out side of home. Whereas nextdns is simple to use, and effective. reply verelo 13 hours agorootparentNo idea how I have been living under a rock. I was using Google dns forever, but just switched my router over to next! This looks amazing, and great to see so many people using it with positive feedback. reply mrbonner 15 hours agoparentprevi paid for NextDNS back in 2020 but discontinue the following year due to services such as streaming from PBS app and websites not working properly. I knew this maybe related to aggressive blocking DNS but I wasn't having the time to investigate. I have no complain about NextDNS. Their service works and pricing is fine. I just use Adguard premium now and have no issue for a year. reply hsshah 14 hours agoparentprevHave you looked into their privacy/data collection policies? Generally prefer local solutions but gave up on Pi-hole some time back after recurring issues. Currently using client-specific adguard; however the centralized management with nextdns is enticing. reply sitzkrieg 8 hours agoparentprevi switched to nextdns all in a handful of months ago and mostly recommend it too reply boringuser2 10 hours agoparentprevOne of the major reasons why I don't use or recommend NextDNS is because they force you to use their DNS resolver when a DNS resolver like Quad9 has vastly superior threat intelligence. reply zukzuk 16 hours agoprevI looked at Pi-hole recently but went with AdGuard Home. Nicer UI and nicer everything by all appearances. There's also a surprising amount of customization for something this slick, like being able to defer to my internal DNS for local private domain queries, etc. I'm not entirely sure why AdGuard is giving this away, and maybe I should look into that, but seemed like a relatively low-risk decision to go with this for now. And I can't say enough about how much more pleasant using things like the NYTimes app has been without the obnoxious ads. reply andix 16 hours agoparentYes, it’s really awesome. The split-dns feature has all the options you would imagine. I thought i would need a second dns server behind it, but i could add all the rules I need right into adguard home. It even supports DoT and DoH upstreams, which is still not a thing with many home routers. Edit: here are the docs: https://github.com/AdguardTeam/AdGuardHome/wiki/Configuratio... reply andix 13 hours agoparentprevAbout the give-away-for-free aspect I was also wondering. Do they maybe configure their dns servers as default upstream and hope many people keep the defaults? DNS is one of the best technologies to do data mining and sell the data. I guess it's also why all those easy to remember dns servers like 8.8.8.8 and 1.1.1.1 exist. Google and Cloudflare for sure don't do it just to be nice. Disclaimer: adguard claims not to sell any customer data. reply madduci 16 hours agoparentprevThey can expand their user base and when they have acquired a certain amount of people, switch to a licensed model? reply andix 13 hours agorootparentThe main repo is GPLv3: https://github.com/AdguardTeam/AdGuardHome They already have many other commercials products and I guess also the default filter rules are very good because of their experience in the domain. But I think you can use it completely without the AdGuard servers and use other filter list sources. reply zymhan 3 hours agoparentprev> like being able to defer to my internal DNS for local private domain queries, etc. PiHole supports Conditional forwarding reply Brajeshwar 6 hours agoparentprev> I'm not entirely sure why AdGuard is giving this away Here is my reasoning. I can read up the documentation and set it up and get it working. I'm going to brag to my friends about how my home network has no pesky ads and stuff. They will ask me to “Set up for me, Set up for me.” I cannot help them maintain, even if I do set it up for them, so -- I'm going to say, “You know what, instead of that complexity, they have a simple app-based setup that just works for just $29 a year for your whole family.” See, I just got five of my friends to download and buy the service in that dinner party. I believe this is the same philosophy of todays' tech Startups -- have an Open Source Product but build a commercial business on top of that. reply throwaway742 10 hours agoparentprevDoes AdGuard support regex matching? reply seanieb 11 hours agoprevAdGuard is a Russian company, with Russian engineers, the majority of AdGuard developers and other employees working from Moscow, registered in Cyprus. Not a great recipe. Hard pass on security grounds. reply tills13 10 hours agoparentIt's open source you can verify it yourself. reply mrcarruthers 7 hours agorootparentTechnically, yes you can. But do you really have the time to sit down to understand a piece of software enough to know if it's doing anything nefarious? reply Sammi 44 minutes agorootparentIt only takes one obfuscated line of code buried somewhere deep where you wouldn't expect it. reply Sammi 9 hours agorootparentprevGood luck with that. reply modzu 3 hours agoparentprevand your macbook was built in china. uh oh reply time4tea 12 hours agoprevYou might be interested in py-hole. It's just a python script and some dnsmasq configuration, it runs on openwrt, is free and close to zero cpu usage. https://github.com/time4tea-net/py-hole reply hbcondo714 10 hours agoprevThere are a few mostly positive comments here about NextDNS but I'll start a new comment since I'm thinking about switching away from NextDNS. Why? I'm on a Mac / Safari now and would like to enable their \"Hide IP address from trackers\" feature but if I do, then I start seeing advertisements on websites that would normally be blocked by NextDNS. So I have to uncheck this option and can't use Apple's feature. Overall, I guess the two can't be used together, per an issue reported on the NextDNS Help site: https://help.nextdns.io/t/q6yq4xy/nextdns-stops-working-prop... Does anyone by chance know if this is a known issue with AdGuard or even Pi-hole? reply pseufaux 8 hours agoparentAre you referring to iCloud Private Relay? If so that's expected behavior for with any DNS based ad blocker. Turning on the relay proxies your connection and your local network's DNS server will not be used. Doesn't matter if it's PiHole, NextDNS, or AdGaurd. reply hbcondo714 6 hours agorootparentThanks, I did not think of that but iCloud Private Relay requires an iCloud+ subscription[1] which I do not have. I'm referring to the \"Limit IP Address Tracking\" option[2] in Safari/iOS and \"Hide IP address from trackers\" option[3] in MacOS/Safari [1] https://support.apple.com/guide/icloud/set-up-icloud-private... [2] https://support.apple.com/library/content/dam/edam/applecare... [3] https://appletoolbox.com/wp-content/uploads/2014/02/Hide-IP-... reply rahimnathwani 5 hours agoparentprevYou're using one product that blocks ads and trackers, but then bypassing that with another product that deliberately provides access to ads and trackers, but via a third party. What is the point of the latter? reply hbcondo714 3 hours agorootparentI subscribed + configured my router to use NextDNS years ago so ads + trackers are blocked on my IoT devices. More recently, I inherited a MacBook and now an iPhone and naturally enabled their built-in blocking capabilities. I think I assumed two blockers are better than one but now I just leave Apple's IP limiting features off and let NextDNS do its thing but it just feels weird to deliberately turn off a privacy feature. reply int_19h 15 hours agoprevOne other neat thing about AdGuard is that it is available as a Home Assistant addin - and it does integrate with the rest of HA, so you can e.g. have a switch to enable/disable blocking as part of your dashboard. reply fignews 1 hour agoparentNextDNS also, just set it up :) reply smarterhome 13 hours agoprevAdGuard Home is amazing! I used PiHole for a time but did run into small issues quite at lot. Mind you nothing serious but things like these are only really useful if they just work. Adguard Home works without any issues on my Pi setup via docker-compose [1] and it even runs on a second Pi as backup using a cool container called adguardhome-sync [2] to keep their configurations in sync. I am not seeing any ads in my network anymore and it is quite interesting to see how many tracking/ad requests are sent by some devices... 1 - https://thesmarthomejourney.com/2021/05/24/adguard-pihole-dn... 2 - https://thesmarthomejourney.com/2023/02/12/adguardhome-sync-... reply vin047 7 hours agoparentThe real eye-opener is when you start redirecting DNS 53 requests to your own DNS server and block DoT/DoQ/DoH – so many devices/apps just trying to reach out to their hardcoded DNS servers for tracking/ad targeting. reply briHass 4 hours agorootparentUnsurprisingly, Google and Facebook IoT junk is the worst. They both hardcode their own DNS, and I've caught Google devices ignoring the DNS IP from DHCP (not the gateway) and attempting to resolve from the gateway (with external blocked) reply ittan 15 hours agoprevUnsure if anyone here uses Technitium DNS(Opensource and free). It works on minimal hardware. I am running it on an Orange Pi 3 LTS. https://technitium.com/dns/ reply yumraj 10 hours agoparentThis looks great. Qs: this says “ Technitium DNS Server is an open source authoritative as well as recursive DNS server” Are pi-hole/Adgyard also recursive DNS server or just a blockers? Edit: I’ve been using pi-hole for ages, trying to figure out if this has any advantage. reply mianos 10 hours agoparentprevAnd you can load the ad blocking lists into anyway so you get solid DNS, ad blocking and none of those random youtube spinners from rando dns issues. For nothing but a little configuration. reply vin047 7 hours agoparentprevDecided against it due to being written in C#/NET and being relatively new. Went with Unbound reply neonsunset 2 hours agorootparentWhy? reply FuriouslyAdrift 10 hours agoparentprevI've been using it for years and love it. .Net based, so it is cross platform, too! There's a docker image if you want to go that route. reply az09mugen 15 hours agoparentprevYup, running it on a pi 4. Simple to set up and use, happy with it. I didn't know about Adguard but I don't want to try it even if it seems good. reply NoPicklez 9 hours agoprevI swear there is a set time that HN can't go without a Pi-Hole or Adguard Home post. reply Brajeshwar 6 hours agoparentI’ve a bi-annually repeating task on my calendar -- HN: Pi-Hole / AdGuard? ;-) reply Brajeshwar 6 hours agoprevI used Pi-Hole, then went to NextDNS, then to AdGuard DNS, tinkered with AdGuard Home, and currently testing Control-D. They are all actually pretty good, similar features, and it has become just a matter of personal choice. In all fairness, when I have some time and can invest in decent hardwares, I might go back to AdGuard Home with one of the paid services as backup for travel, and for the other family members. Pi-Hole works really well but once-a-while, when I'm traveling, it will decide to act up and it's a whole IT support with the family over phone for minutes if not hours. I'm not smart enough to setup a secure enough tunnel and the like, and haven't read up enough on the topic. This follows similar pattern with AdGuard Home. NextDNS, AdGuard DNS, Control-D are easy and just works, especially with the devices that the family uses. I think I bought one of those AdGuard Lifetime license, so I use that to block client-side rendered ads in conjunction with either AdGuard DNS or NextDNS or Control-D. Right now, Control-D is doing pretty good with my test-drive. Edit: The other reason is that many websites such as the Governments’, Banks (at-least in India) seldom works with Pi-Hole or AdGuard Home. With the other tools, I can turn off for a while, and go Internet-Naked and do the transactions, pay the insurance, etc. https://adguard-dns.io https://nextdns.io https://controld.com reply linuxandrew 9 hours agoprevI wonder how much DNS blocking would contribute to a unique browser fingerprint? Like a tracker could use a range of domains, some of which are known to be blocked by certain end-user software, to build a fingerprint. I currently use a vanilla LibreWolf which has uBlock Origin and reasonable defaults out of the box for this reason. My only other line of thinking is that a combination of DNS, IP and in-browser blocking could be more effective than just in-browser alone. reply Crosseye_Jack 13 hours agoprevAlso runs on home assistant. The only thing to remember is when your updating HA (or you forget that your HA pi is not on the UPS, and you trip your GFI when doing home maintenance on your ring main) that your DNS also goes down. Side note: it’s always DNS… reply Dries007 13 hours agoparentExactly why I run my DNS on an old pi just for that and some minor watchdog stuff. reply vladgur 12 hours agoprevWith a self-hosted DNS internally, how do you handle fallback? For example if the box with Adguard Home or pihole crashes, can you configure your router or your devices in a way that would instead go to say cloudflare or google DNS? reply briHass 4 hours agoparentMy router (Mikrotik Hex) redirects all DNS requests it receives to the Adguard server (with masquerade.) DHCP hands out the router for DNS. A recurring script attempts to resolve a domain from Adguard every 30s, and if that fails, the NAT rules are disabled and the router would handle the DNS directly. Downside to this approach is AG doesn't have client IPs, since they all come redirected by the router. I think DNS has a way to tag original IPs, but AG doesn't support it. I just use multiple DHCP configs to hand out AG directly to devices that are bad actors (and not critical), and critical stuff gets the method above. reply jerezzprime 12 hours agoparentprevI dealt with a less-than-ideally reliable pihole by configuring the pihole as the primary DNS, and an external DNS server as the secondary (most devices accept 2 or more IPs for DNS). reply 293984j29384 11 hours agorootparentOn Windows that means your requests are queried against all DNS servers listed. reply moontear 11 hours agoparentprevHonestly? Have two instances and point to both via your router dhcp dns. Very Client will use them and you are good to go. There are also solutions like adguardhome-sync to keep both installations in sync. reply lurking_swe 12 hours agoparentprevmost routers let you set a primary dns server and a secondary. just set the secondary to something like google or cloud flare dns. reply smarkov 12 hours agorootparentI believe this only works if your ad blocking DNS is configured to return 0.0.0.0 for all blocked domains rather than NXDOMAIN, since then services might try using the secondary DNS instead and that would result in nothing getting blocked. Ideally your secondary DNS should be a copy of the primary. reply vladgur 11 hours agorootparentdo you know if pihole or Adguard can configured to support confirming to the router or the client that resolution took place, rather than try the secondary DNS. If i understand you correctly, if you have a blocking internal DNS running pihole or Adguard and an external general DNS such as google or cloudflare, unless what you described can be configured, the requests that come back \"blocked\" from pihole would then simply be resolved by google/cloudflare, thus negating the point of pihole. reply smarkov 10 hours agorootparentAdGuard Home should by default be configured to return 0.0.0.0, you can check whether that's the case in Settings -> DNS Settings -> scroll down to Blocking Mode. I don't know about Pi Hole but it probably also has a similar setting. reply moontear 11 hours agorootparentprevThere is no primary and secondary dns on windows. Both dns servers are queried, if one goes down you are fine but you won’t hit your local dns all the time. reply dsheets 15 hours agoprevI contributed improved ipset support to this project. As far as I know, it’s one of the few off-the-shelf DNS servers that can insert result records into Linux ipsets to enable domain-based firewall policy. I run it on OpenWRT and use the ipset support to open the default drop firewall from my “smart” projector on my IoT subnet to NetFlix and YouTube. It sets the ipset entry expiry to the DNS TTL. Now, the only way for the machine to connect to the internet is to resolve a whitelisted domain and it can only access while the record is fresh. I haven’t encountered any issues so far. I take it that some Chinese users use this same functionality to selectively VPN domains to evade GFW. reply pandemic_region 15 hours agoprevHappy AdGuard user here. It's running directly on my EdgerouterX so no need for an extra device to maintain. I really love the high level service blocking as well, blocking the whole of Facebook is just ticking a checkbox! reply dang 12 hours agoprevRelated: AdGuard Home: Network-wide ads and trackers blocking DNS server - https://news.ycombinator.com/item?id=33387678 - Oct 2022 (113 comments) AdGuard Home – an open source network-wide ad blocker - https://news.ycombinator.com/item?id=18238503 - Oct 2018 (2 comments) reply triyambakam 16 hours agoprevCoincidentally I just set up OpenWRT [1] on a NanoPi from FriendlyElectric. How would this fit into using Wireguard? Or, how would I go about that? It seems like there might be something conflicting about running both, but I am very new to it all. [1] It is actually running their FriendyWRT variation which came with the precompiled drivers for getting a Realtek USB wifi adapter to work, otherwise stock OpenWRT would work as well reply 35mm 12 hours agoprevThose who are using DNS level ad blocking: how much do sites break? And how easy is it to unblock them? I currently use browser based blocking and find a lot of sites don’t work at all. Typically SPAs. But if I have to use them, I can disable the adblocker in two clicks. How does that compare? reply LeoPanthera 12 hours agoparentIt entirely depends on which blocklist(s) you use. I had to stop using the StevenBlack list because it started breaking a lot of things, apparently intentionally. I recommend using only one list, rather than a combination of several. I switched to the https://oisd.nl Big List, which has been great... although it did break GitHub yesterday. That was the first breakage since I switched, and it was fixed when I reported. But still, keeping an eye on it. reply vin047 7 hours agorootparentHagezi blocklists are the current standard now: https://github.com/hagezi/dns-blocklists You could go for one of the Lite blocklists for the network wide, family friendly (non-breaking) list. reply muppetman 9 hours agorootparentprevOISD is what I use as well. It's great, the family don't have any issues like we used to with the other lists I used. It doesn't block as much, but I'll take the odd thing slipping through vs not being able to load a page we need. reply ololobus 12 hours agoparentprevI use PiHole, it does break some stuff here and there, and sometimes useful things like Private Relay or iCloud in iOS; or once YouTube history stopped working for me (apparently they use a separate domain to track watched videos and progress!). It also depends on the block lists you upload. It’s pretty easy to unblock, especially web, as you just look on which domain cannot resolve in the browser dev tools and add it to the allow list. Yet, DNS-based blockers have a limited usefulness at this moment as some major ad-providers started using the same primary domain for serving ads. For example, YouTube, partially Google, Yandex. I guess they cover everything with top level load-balancer and then route internally to specific service ingresses reply kodt 11 hours agoparentprevAffiliate links break, which can be annoying for other members of the household who may want them to work. reply HumblyTossed 12 hours agoparentprevSites break often if they're shitty. Especially if you click Google's \"Sponsored\" link by accident after a search because I block Google's ad stuff. But, you get used to what sites break and decide if it is worth bothering to fix it or not. I can disable my pihole by opening a browser, navigating to pihole and disabling it. reply lock-the-spock 12 hours agoparentprevI use AdGuard home as part of my HomeAssistant setup and have had no problem at all. Only thing is to turn off the enforced safe search as that quite reduces results. reply downrightmike 11 hours agoparentprevrarely breaks. Also simple regex blocking goes a long way: .ads. will get rid of most ads domains. .tele. for telemetry etc reply nprateem 12 hours agoparentprevForget about streaming media from amazon prime and various terrestrial broadcast apps. But just create 2 networks, one protected, one not. reply amelius 13 hours agoprevHow can this possibly work? I don't know much about how adtech works, but if I were Google I'd provide ad blocking detection to all of my clients. And it should be pretty simple to detect if parts of the network that are essential to my ads are being blocked. reply s0ss 16 hours agoprevNeat! Similar: If you happen to run pfsense on your network, check out pfblockerng, I really like it!: https://docs.netgate.com/pfsense/en/latest/packages/pfblocke... reply readscore 11 hours agoprevI'm experienced in DNS but have never seen the point in DNS blocklists. It feels like the wrong layer. I do adblocking with a browser extension. The adblocking has more context, can modify the page, and has easy UI integration for debugging and turning it off. What else are DNS blocklists for? Clients except browsers? For the record, on my desktop I use systemd-resolved (for DNSSEC) and dnscrypt-proxy2 (for encryption). On my router I run unbound as recursive resolver for other devices. On my phone I use quad9, and adblocking via Firefox. reply Larrikin 11 hours agoparentI enjoy having ads blocked in apps and on my iPad, where ad blocking is extremely limited otherwise. If you look at the logs from your media box, (whether that is your TV, Roku, or whatever) there's a massive amount of tracking that gets sent up. Combined with Tail scale I can even block ads and tracking on my devices when I'm not home. reply readscore 10 hours agorootparentThanks I understand now. All my devices are plain Linux distro machines, or Android. reply muppetman 9 hours agoparentprevAdblocking via the browser is the best option if it's available. All the games the kids play on their iPad try to insert ads, track them, all that sort of stuff and DNS based Adblocking stops that. My wife's iPhone isn't subject to ads when she's reading the news in Safari. On my Google Pixel I don't see ads in browsers either, Firefox I use uBlock but even the Google Newsfeed uses Chrome for webview, so DNS adblocking stops me having to see the sponsered stuff in there. There's so many places other than \"the browser\" to see ads, to even question that seems like not really having knowledge of what the Internet is used for in 2024. Edit: Sorry that's a bit rude, I just meant maybe you don't use it the same way a lot of others do. Sorry for sounding obnoxious and rude. DNS blocking doesn't stop stuff like ads in Instagram, or Youtueb etc, but it certainly helps in a lot of other situations like Ads in the Imgur app etc etc. reply readscore 8 hours agorootparent> There's so many places other than \"the browser\" to see ads, to even question that seems like not really having knowledge of what the Internet is used for in 2024. I understand that many people use apps and smart TV sticks, but I'd forgotten that many have ads. I use some apps, but none that have ads. My family use apps but say that they appreciate targeted ads. reply muppetman 7 hours agorootparentYea sorry I've updated my comment to reflect the fact the way I phrased that was quite rude - my apologies. For the silly games my kids play on their iPad, blocking ads means they can \"skip\" ahead quite often instead of being forced to watch an ad before they're allowed to try again/progress to the new level. They're subject to enough advertising with Youtube anyway, just from all the content they watch that's subtle advertising. reply NamTaf 8 hours agoparentprevMy ISP-supplied router tries to ping back to some “AI driven wifi analytics” bullshit every 30 seconds. I put in a custom block for that. My TV would also probably love to phone home if I connected it to wifi to use the applications on it. The value is not just that I can block at the network level rather than the application/device level, it’s also that I can see what random connected devices that aren’t general computing devices are trying to do. If they have hard-programmed DNS servers, blocking 53 for any device besides my Adguard server quickly solves that. reply politelemon 16 hours agoprev> Runs on your OpenWrt box Where are you seeing that? The only reference to OpenWRT I see is in the \"Projects that use AdGuard Home\" section which links to a different project. Otherwise that's a misleading title - this is a PiHole alternative. reply dsissitka 16 hours agoparenthttps://openwrt.org/docs/guide-user/services/dns/adguard-hom... reply cricalix 16 hours agoparentprevIt absolutely runs on OpenWrt - simple as opkg install, then setting it up and sorting DNS redirection as needed. reply masfuerte 15 hours agorootparentYes, but the title suggests that OpenWrt is the only place it runs. Which is misleading. reply raajg 13 hours agoprevBeen 4 months and I'm pretty happy with the following setup: PiHole + RaspberryPi + Tailscale With Pihole running on a tailnet all my devices use it by default as long as they're on the same tailnet. That way I have seamless ad-blocking even when I'm on cellular data or my friends' wifi networks. reply NL807 8 hours agoprevHow effective is AdGuard against YouTube ads? Pi-Hole doesn't work as its filtering is at the DNS level, I suspect AdGuard has the same issues? reply vin047 7 hours agoparentDoesn’t work for YouTube ads – they no longer load ads via DNS and instead embed them directly into the video feed. Ublock origin via the browser is the best way to block them. If you wish to use a client app, best bet is to sideload a 3rd party app like like SmartTubeNext for Android TV or YTLitePlus for iOS. reply Brajeshwar 8 hours agoparentprevAdGuard blocks at the client level, so it works (so far) as far as I tested in the last couple of weeks (with a non-premium account). Disclaimer: YouTube is still very affordable in India, our family subscribe to the YouTube Premium. reply gotschi_ 13 hours agoprevUnfortunately it is a 11mb install, which makes it quite unfitting for your usual openwrt device reply winstonprivacy 16 hours agoprevSadly for the AdGuard team, there isn't much of an audience for this. It's one of those things everyone says they want but few people will actually install one, much less maintain one over time. Add to that the wife-forced uninstalls and the total long-term audience for this is (no kidding) in the thousands. reply breckenedge 16 hours agoparentMy spouse’s device is on a pihole exclusion list. Can you not do this with AdGuard? reply jraph 15 hours agorootparentWhat is the reason for someone in the network to not want the filtering? Does this break some websites? My own devices are covered, I definitely want full filtering even when not at home and my devices are completely hackable, but I'm wondering if such a tool would be a convenience for other people using the network in particular with less hackable devices, and people likely to use my network are likely totally uninterested in ads, but I don't want this to be a pain. reply breckenedge 15 hours agorootparentYes, it breaks some websites and apps that they use for work. My pihole also only runs on my “private” network, the “guest” network is not filtered. Apple’s Private Relay also does not work behind a pihole. reply syslog 14 hours agorootparentPrivate Relay does work, but it circumvents the Pihole (so no adblocking). reply jraph 14 hours agorootparentprevOkay thanks! I guess I'm not in the target of these things. reply muppetman 9 hours agorootparentprevI used to need my wife's devices on the whitelist too - she had a job working with tracking and needing to see trackers fire when she loaded webpages etc. I once made a mistake and she got unwhitelisted and waited 4 hours wondering why her tracking codes \"weren't working\" reply rockooooo 14 hours agorootparentprevIt breaks a lot of websites, I used NextDNS for about two years but got tired of the headaches. reply zukzuk 16 hours agorootparentprevYes, you can definitely use it selectively. reply dizhn 16 hours agoparentprevI don't get this comment. It is basically the same kind of tool as the Pihole only much easier to install and maintain. (It's a single go binary) Isn't this a popular class of software? reply nickthegreek 15 hours agorootparentIt is not a popular class of software to the masses, it is a popular class of software to a niche audience. I don't share as pessimistic attitude as OP though. I'm pretty sure the audience is in the tens of thousands! reply winstonprivacy 12 hours agorootparentWhat's funny is that I was once extremely optimistic about the potential for such a device, to the extent of having sold and delivered a few million in product. Hard experience taught us that churn is just crazy high, no matter how compatible it easy to use you make it. Getting tens of thousands of stars is not the hard part because it's such an easy concept to like. But I would be surprised there are more than let's say ten thousand piholes in active use. reply dizhn 14 hours agorootparentprevThey have that many stars on GitHub. They actually also have thousands of forks each. The api probably still has a way to count downloads but I didn't bother. I wasn't claiming users in the millions anyway. :) reply bityard 15 hours agoparentprevI guess I'm the exception to the rule, I spent a fair chunk of my previous weekend upgrading the hardware on my opnsense router/firewall so that I could virtualize opnsense and be able to glom on related services exactly like AdGuard Home easily. reply Naac 16 hours agoprevAnyone know of an Adguard home or pihole equivalent service I can run as part of OPNSense? I currently have a different machine dedicated to pihole, but it would be intriguing to have something built in. I would imagine split DNS and firewall rules would be simpler this way. reply cycomanic 15 hours agoparentAdguard runs directly on opnsense. https://0x2142.com/how-to-set-up-adguard-on-opnsense/ reply bityard 15 hours agoparentprevI'm in the process of migrating my OPNSense to a virtual machine so that I can run whatever network-related services I want right along side it in a container or VM. I used to scoff at those enterprising homelabbers who apparently stuck their firewall in a VM just because they could but I get it now. It's super nice to be able to just snapshot and back up the whole VM, and run whatever you want alongside it. (Although I will limit the box to specific network management things like AdGuard Home.) reply vin047 7 hours agorootparentDitto, just recently set mine up this way. Will never go back to ISP or proprietary routers. reply _micheee 16 hours agoparentprevThe built-in unbound dns server has support for blocklists, maybe you want to give it a try: https://docs.opnsense.org/manual/unbound.html reply moviuro 15 hours agoparentprevUnbound with tags? * https://unbound.docs.nlnetlabs.nl/en/latest/topics/filtering... * https://try.popho.be/securing-home3.html * https://git.sr.ht/~moviuro/moviuro.bin/tree/master/item/lie-... reply lawn 15 hours agoparentprevI run Adguard Home on my router with OPNSense. I don't remember how I set it up, but it wasn't that difficult. reply justaman 15 hours agoprevWill this work against ads on major streaming apps like prime, hulu, and netflix? reply Ninn 15 hours agoparentNo reply jklinger410 10 hours agoprevI love the AdGuard plugin as compared to UBlock because it allows me to make a blacklist instead of a whitelist. reply stzsch 13 hours agoprevI got my glinet gl-axt1800 mainly for the adguard support out of the box, as a way to keep my smart tv experience sane. Works pretty well. reply teruakohatu 8 hours agoparentAre there allow lists for services such as Apple TV. Do smart tvs not fall back to hardcoded ipv4 addresses? reply steeve 15 hours agoprevCurrently running this as a Home Assistant addon is reply karolist 15 hours agoprevWorks fine, beautiful and simple UI, I have it on my Dell R230 homelab server, running inside a container under Proxmox VM reply vosper 15 hours agoprevWhat does this break, if anything? Anyone run into sites or apps where Adguard Home needed to be disabled? How easy was that? reply mnt3 15 hours agoparentDepends on the blocklists you're using. I broke Google search sponsored links, some Slickdeals links, and the meta quest app store. You have the ability to whitelist as well if you want to unblock some things. I'm running it in a docker container and then pointing my router at it. reply fursund 15 hours agoparentprevPerhaps obvious, but if you’re using mixpanel or posthog for analytics on anything you build, you’ll have to put them on exclusion lists, in order to be able to use their analytics platform. reply JoshTriplett 13 hours agoprevStanding reminder that any device smart enough to run a real web browser shouldn't use one of these and doesn't need one. uBlock Origin works much better for any device capable of running it, both in terms of user experience (the browser understands a block rather than a mysteriously failing request) and because it can block first party ads and clean up page layout. The primary use case for these is for blocking ads on devices that don't allow running a real browser and yet still shows ads, such as \"smart home\" devices, TVs, etc. reply johntash 10 hours agoparent> Standing reminder that any device smart enough to run a real web browser shouldn't use one of these and doesn't need one. Why not? Or why not use both? > The primary use case for these is for blocking ads on devices that don't allow running a real browser and yet still shows ads, such as \"smart home\" devices, TVs, etc. What about non-browser apps on mobile devices or even desktops? Lots of apps have invasive ads and are unlikely to offer an extension api to block them with. reply shiroiuma 7 hours agorootparent>What about non-browser apps on mobile devices or even desktops? Lots of apps have invasive ads and are unlikely to offer an extension api to block them with. Simple answer: don't use those apps. Do you really need them? reply aantix 14 hours agoprevIs there something similar, say a proxy, that rewrites the responses to exclude certain ad patterns? reply miah_ 14 hours agoparentYes, Privoxy http://www.privoxy.org/ It comes with all the limitations of using a HTTP Proxy in today's world where SSL is everywhere. reply cyberax 13 hours agoprevI really hate that all these services break DNSSEC. I guess it can't be helped. reply grebly 16 hours agoprevHow does it compare to pfblockerng on pfsense? reply rekabis 16 hours agoprevWhat’s the difference between this and just using their DNS addresses with the force redirect option enabled? reply skottenborg 16 hours agoparentThe internal DNS records are very handy if you host local services. reply steviedotboston 14 hours agoprevcan this be used in conjunction with tailscale? reply dsheets 13 hours agoparentI use it with WireGuard. reply 2OEH8eoCRo0 14 hours agoprevI love AdGuard Home, been using it for years now after PiHole gave me issues. reply drcongo 16 hours agoprevI run AdGuard Home on a Pi and it's fantastic. I was running PiHole previously and found it endlessly problematic, I rarely have to even think about AdGuard Home. reply rpnx 16 hours agoprev [–] Don't do this. Network firewalls are harmful. Let people configure their own firewalls on device. Having to VPN around network blocks is annoying to say the least. Network firewalls are harmful and just a lazy excuse for bad client security. reply sn0wf1re 15 hours agoparentIt isn't a firewall, it's a DNS server that returns fake results for entries in its blocklist. reply derwiki 10 hours agoparentprev [–] Is it easier to configure a firewall on my iPhone than I think? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "AdGuard Home is network-wide software that blocks ads and tracking on all devices in your home by acting as a DNS server and rerouting tracking domains.",
      "It offers features such as customizable blocklists, network activity monitoring, and the ability to add custom filtering rules.",
      "AdGuard Home is an open-source project that can be installed using various methods and does not collect usage statistics unless configured to do so."
    ],
    "commentSummary": [
      "User discussions center around the effectiveness of various ad-blocking DNS servers, such as PiHole, NextDNS, and AdGuard Home.",
      "Users share their experiences regarding website compatibility, latency, and reliability when using these tools.",
      "Privacy concerns, pricing, and customization options are also discussed, with differing opinions on the usefulness and advantages of different ad-blocking solutions."
    ],
    "points": 267,
    "commentCount": 216,
    "retryCount": 0,
    "time": 1707238107
  },
  {
    "id": 39275272,
    "title": "Ocean Warming Surpasses Paris Agreement Goals, New Study Shows",
    "originLink": "https://eos.org/articles/oceans-may-have-already-seen-1-7c-of-warming",
    "originBody": "Sponges from the Caribbean retain a record of ocean temperatures stretching back hundreds of years. These newly revealed paleoclimate records suggest that sea surface temperatures (SSTs) began rising in response to industrial era fossil fuel burning around 1860. That’s 80 years earlier than SST measurements became common and predates the global warming start date used by the Intergovernmental Panel on Climate Change (IPCC). On the basis of these new sponge records, scientists think that temperatures are currently 1.7°C warmer than preindustrial levels. “We’re further advanced in the global warming scenario, and the amount of time we have to take action to prevent it is seriously diminished.” The study’s researchers argue that the world has already surpassed the goal of the 2015 Paris Agreement to limit atmospheric warming to less than 1.5°C above preindustrial temperatures and that we could reach 2°C of warming before 2030. “We’re further advanced in the global warming scenario, and the amount of time we have to take action to prevent it is seriously diminished,” said Malcolm McCulloch, a marine and coral geochemist at the University of Western Australia in Crawley and lead author on the new study. “We’ve got to start doing serious mitigation and serious reductions in emissions.” These results were published in Nature Climate Change. This newsletter rocks. Get the most fascinating science news stories of the week in your inbox every Friday. SIGN UP NOW Old Sponges Fill Gaps The sclerosponges studied (Ceratoporella nicholsoni) are a group of long-lived sponges that live exclusively in the Caribbean at depths with little variation in light or temperature. Like tree rings, a sponge’s skeleton retains a record of its environmental conditions throughout its lifetime. “These sponges are extremely slow growing,” explained Amos Winter, a study coauthor and paleoceanographer at Indiana State University in Terre Haute. “A 10-centimeter sponge, which isn’t very large, can go back 400 years.” That longevity is key to the new study’s analysis of modern-day global warming. The IPCC compares modern temperatures with the average temperature between 1850 and 1900 to define today’s warming relative to a preindustrial world. However, “it’s well recognized that human emissions began increasing significantly in the 1750s,” said Duo Chan, a climate scientist at the University of Southampton in the United Kingdom who was not involved with the new study. The global distribution of sea surface temperature measurements in the International Comprehensive Ocean-Atmosphere Data Set (ICOADS) since 1860. Warmer colors indicate more measurements at that location. Credit: McCulloch et al., 2024, https://doi.org/10.1038/s41558-023-01919-7, CC BY 4.0; animation by Kimberly M. S. Cartier Ship-based measurements of SST go back only to around 1850. The records contain many inconsistencies and remain sparse until the mid-1900s, when modern instrumentation took over. Even then, there are notable data gaps during such events as World War II. Paleoclimate proxies such as those stored in sclerosponges can extend the record of SSTs back to truly preindustrial times and help fill gaps in shipboard measurements. “The IPCC’s adoption of a later preindustrial reference period was a compromise, largely due to the lack of sufficient instrumental data for quantifying global temperatures before the 1850s,” Chan said. These sclerosponge records can unravel the history of past sea surface temperatures, said Kaustubh Thirumalai, a paleoceanographer at the University of Arizona in Tucson who was not involved with this study. (Thirumalai is a science advisor for Eos.) An 80-Year Head Start With the help of local divers, the researchers collected six sclerosponges from 2007 to 2012 near Puerto Rico and St. Croix in the U.S. Virgin Islands. They used uranium-thorium radioisotope dating to construct a growth timeline for each sponge that goes back about 300 years. Within each 2-year growth interval, they measured the ratio of strontium to calcium. Calcifying coral skeletons preferentially take in calcium over strontium as temperatures increase, so the ratio is a proxy for seawater temperature. They calibrated their sclerosponge temperature timeline against recent (1964–2012) instrumental measurements. Ceratoporella nicholsoni may be endemic to the Caribbean, but they can be used to understand globally averaged trends. Orange sclerosponges grow on a reef in the Cayman Islands. Credit: Liz Kintzing, CC BY-NC 4.0 Scientists have found that temperature trends in Caribbean waters closely follow global mean SST trends. Sclerosponges live at depths within the ocean mixed layer, where temperatures are mostly the same from the surface to about 100 meters down. So the ambient seawater temperatures recorded by sclerosponges can be used to understand sea surface temperatures too. The sclerosponges recorded some well-known global temperature anomalies, such as the cooling period after the Tambora eruption in 1815. Ocean temperatures were relatively steady from 1700 to 1790, followed by an era of volcanic cooling from 1790 to 1840 and then another steady but slightly warmer period from 1840 to the early 1860s. Researchers trace anthropogenic climate change to that period—about 80 years earlier than instrumental SST records show. The sponges’ preindustrial starting line of about 1700 implies that Earth warmed by 1.7°C between then and about 2020, assuming that the land and ocean have warmed by the same amount. That’s about 0.5°C higher than IPCC estimates and suggests that the planet is on track to surpass 2°C of warming before 2030. Thirumalai found the research to be “innovative and clever,” although he said the sclerosponge records might have too much uncertainty to sufficiently pinpoint events such as the 19th century volcanic cooling. Too, he wanted the researchers to have shown in more detail how temperature trends in the Caribbean were representative of global SST anomalies. Nevertheless, he said, “it is always useful to generate new and independent paleotemperature records to help minimize uncertainties in our understanding of anthropogenic warming and the baseline of preindustrial conditions.” Untangling the Cause of Warming “Integrating these sclerosponge findings with corrected instrumental data could offer a more comprehensive view of historical SST evolution,” Chan said. However, he cautioned against immediately adopting the updated warming values. The sclerosponge warming rates almost mirror modern instrumental records in a broad sense, but there are some differences, even when accounting for different starting lines, Chan said. Research is ongoing to correct some biases and errors in historic instrumental records, which might reconcile some of these differences. “This distinction not only enhances our understanding of climate change but also has significant political implications, informing policy and target setting in the context of global warming.” Too, when looking as far back as 1700, Earth’s climate might still have been rebounding from the Little Ice Age (roughly 1300–1850). That might account for some of the warming during the early 19th century, Chan said, but definitely not all. “It is essential to more accurately distinguish the anthropogenic component from other factors, particularly during the early industrial period,” Chan said. “This distinction not only enhances our understanding of climate change but also has significant political implications, informing policy and target setting in the context of global warming.” McCulloch, Winter, and their colleagues urged IPCC scientists and climate modelers to consider this new preindustrial starting line. Whether it will be adopted is uncertain, but if the world has already surpassed 1.5°C of warming, the researchers argue that continued climate action is more important than ever. —Kimberly M. S. Cartier (@AstroKimCartier), Staff Writer Citation: Cartier, K. M. S. (2024), Oceans may have already seen 1.7°C of warming, Eos, 105, https://doi.org/10.1029/2024EO240059. Published on 5 February 2024. Text © 2024. The authors. CC BY-NC-ND 3.0 Except where otherwise noted, images are subject to copyright. Any reuse without express permission from the copyright owner is prohibited. Related Tagged: Caribbean, climate, climate action, Climate Change, IPCC, Oceans, paleoclimatology & paleoceanography, proxies, sponges",
    "commentLink": "https://news.ycombinator.com/item?id=39275272",
    "commentBody": "Oceans May Have Already Seen 1.7°C of Warming (eos.org)261 points by Brajeshwar 18 hours agohidepastfavorite334 comments tschellenbach 16 hours agoThere is just no democratic support for fixing global warming since it's too expensive. See the riots in France when they tried to tax gas more heavily. I like Bill Gate's book on the topic. We need technological progress in several areas -> lower the premium on solving climate change -> enable political solutions. (Don't get me wrong, I wish a political solution was possible. Climate change is a major threat. But in a democracy leaders can't do deeply unpopular things. Lots of people want to fix climate change. The challenge is it currently requires increasing the price of construction, electricity, food, and gas) reply advael 16 hours agoparentIt'd be great to see how democratic support might be different were it not for the concerted, expensive effort to deny and downplay the problem by multiple enormous industries that have repeatedly captured governmental institutions in this effort, as well as changing public perception considerably Concentration of power underlies a staggering amount of the problems of the current age, and if our species is to survive, insane propositions like \"limited liability\" need to die reply whimsicalism 16 hours agorootparentI was initially partial to this explanation, but I no longer think that ‘in the absence of interference’ people would vote for the price increases needed. reply adrianN 15 hours agorootparentThe price increases would have been much more moderate if we had acted decisively forty years ago when the science was basically settled. reply callalex 14 hours agorootparentI don’t see how that follows. Solar panels and batteries have become massively cheaper in the past few decades and that has all relied on new materials science. I guess you could argue that windmills could have been made cheaper at scale with the right efforts 40 years ago. reply defrost 10 hours agorootparentSeventy years ago, when AGW climate change was recognised internationally at the UN in 1970s, during an oil crisis, the largest per capita users of fossil feuls and emmitters of CO2 were the North Americans. They were open to change in behaviour at the time, it's a matter of public record that Koch Industries started to heavily fund a number of think tanks devoted to changing American perceptions; individual freedom is expressed by wanton consumption, bigger gas guzzlers are better, public transport is bad. They, and their partners in the petro businesses, sank very nearly every grassroots movement for small, midle, and large city public transport launching, expansion, or improvement. They backed every move for more and more freeways. What could have been a time that saw greater emphasis on resource efficiency and less fossil fuel consumption was not. What could have been a signal to all the then developing economies about the direction a world leader was taking remained so - and the message was consume, and consume more. Developing better renewable energy sources was (and still is) only part of a bigger picture of changing behaviour. reply adrianN 2 hours agorootparentprevNuclear was around. But the real potential for savings was in reducing consumption by altering infrastructure investments. For example by favoring public transport and walkable cities over cars and sprawl, or requiring better insulation for new buildings. Today we don’t have the time to wait until simple tax incentives nudge people towards better behavior. reply s1artibartfast 13 hours agorootparentprevIndeed, it may be the case that now is far cheaper than any time in the past, even taking into account co2 release in the past 40 years. reply kccqzy 16 hours agorootparentprevPeople would vote for these price increases once they see that the increases are paltry compared to the opportunity cost, or to the externalities. reply dpe82 16 hours agorootparentI think you deeply overestimate people's ability to make painful near-term decisions to avoid long-term pain that they can't see, have never personally experienced and also nobody they know or have even heard of has personally experienced. It's human nature; we're extremely bad at making decisions related to seemingly abstract long-term problems. reply richrichie 4 hours agorootparentHow about you be the beacon of change and show us by taking the near term pain e.g. cutting down consumption (eat one meal a day of vegetables, stop traveling, stop taking vacations far away, live in a one room apt, no kids, etc. reply defrost 1 hour agorootparentWinyarn small minded thinking there. Makes more sense to recover energy from moving almost a billion tonnes per annum from plateau to sea level. https://reneweconomy.com.au/fortescue-says-regenerative-infi... .. but keep up those tiny thoughts, one day they might grow. reply r14c 14 hours agorootparentprevPeople can make long term plans and execute them, but the systems that control the levers related to climate change are operated by people who don't habitually operate based on long term planning. Calling it \"human nature\" is a cop-out. reply dpe82 12 hours agorootparentFor themsleves? Some people can, sometimes. For the commons? If humans were so naturally inclined we wouldn't be having this conversation. Blaming the problem on \"people who don't habitually operate based on long term planning\" is a likewise a cop-out while also kinda arguing my point. reply fnimick 16 hours agorootparentprevI disagree. Lots of people are perfectly happy to ignore externalities that happen to others. Just try to get a bike lane put in and you'll see it for yourself. Or as a lovely neighbor put it: \"if you die, why is that my problem?\" reply kccqzy 15 hours agorootparentI agree. However the key point in climate change messaging is informing people that this affects themselves, their kids, and their grandkids. reply s1artibartfast 13 hours agorootparentWhich is often a lie, or at least situational simplification . The costs of climat change and mitigation are not evenly distributed. reply whimsicalism 13 hours agorootparentprevIn my view, this is largely a lie in terms of the impact on themselves. reply somenameforme 15 hours agorootparentprevIf the costs are paltry, and you have a government who even remotely cares about this topic, then why wouldn't they simply pay for the costs? We're $34 trillion in debt, and rapidly growing. We spend hundreds of billions of dollars on ever more inane ventures which are frequently unpopular, and probably making the world a worse place. But the only solution, to pay a \"paltry\" sum to solve climate change, is to raise taxes? For instance they could easily print money, in the normal way, by issue special 'climate bonds' with a promise that all revenue from said bonds would be spent on whatever you see as this affordable solution. Those bonds would certainly sell well, people could get their virtue signaling on on social media showing them off, and - best of all - absolutely 0 divisiveness. Even somebody like me who thinks our debt's become catastrophic would just shrug, since we're probably already way past the point of no return. reply secondcoming 16 hours agorootparentprevThere are a lot of people who can't afford these price increases _today_. Being able to worry about the years to come is actually a privilege. reply fnimick 15 hours agorootparentHaving tried to advocate for better societal support for those people who \"can't afford those price increases\", I've honestly lost a lot of sympathy. I've had people explicitly tell me they want to see welfare benefits for people poorer than them cut so that they face less competition (and therefore less inflation and lower prices) for purchasing food. As if starving the poor to make your grocery bill better is a justifiable option. \"why should you be entitled to food and shelter just because you were born as a human and not some ape? if you are useless and die, it's not my problem\" If you have no empathy for anyone poorer than you, sorry, why should I be caring about your misfortune again? reply nuancebydefault 14 hours agorootparentIf I understood you correctly (maybe I didn't), you have decreased empathy for the poorer-than-you because of their lack of empathy for their poorer-than-them? The thing is, it is only human that when you are poor, your prority shifts more towards todays and one's own problems rather than tomorrow's and other people's problems. In other words, we should feel the privilege of being able to prioritize our future and that of our children. reply whimsicalism 14 hours agorootparentprevNobody in the US has to starve with the current social system. reply advael 9 hours agorootparentA bizarre claim given that so many are, in practice, starving. Maybe they're just holding it wrong reply polski-g 8 hours agorootparentPlease provide evidence that \"so many people are starving in the US.\" reply advael 7 hours agorootparentHere's what 10 seconds on a search engine provided from a fairly official source https://www.ers.usda.gov/webdocs/publications/107703/err-325... Roughly 10% of households being food insecure, defined as having inconsistent or inadequate access to food. That amounts to roughly 30 million people Social programs existing is great, but they are in their present form inadequate to make the claim that was made It's also worth pointing out that most likely \"households\" excludes all homeless people, which is not an insignificant portion of the population (as many people on HN - especially those living in SF - are quite aware) reply polski-g 6 hours agorootparentThat is not what I asked for. I asked for stats specifically on \"starvation\" -- the claim you made. reply advael 4 hours agorootparentStatisticians really like using dry descriptions that often border on euphemistic in their work, not unlike psychologists. I think this reads to them as more objective or serious. That said, figures I can find about how many starve to death in a year seem to hover around 10-15k, which would make up about half to a third of a percent of deaths per year (though with the COVID-19 pandemic recent numbers of total deaths are still somewhat higher than before it started) This is a pretty conservative estimator for what we might colloquially consider \"starvation\", because there are a lot of deaths in which severe malnutrition is a significant factor, but are not attributed to starvation as a cause of death. But even if we treat it as a perfect proxy, it's a lot more than zero. 10k people per year are just holding the perfectly adequate socialist machine we apparently already have fully operational and available to anyone who doesn't simply desire to starve, wrong reply whimsicalism 16 hours agorootparentprevHope so! From my lifetime of observing humans though, I don't agree. reply advael 16 hours agorootparentprevYour framing is designed to make that outcome inevitable. Of course if the proposal is keeping everything else the same but raising prices people would vote against it. Mitigating the catastrophic biome collapse we are already experiencing requires infrastructural changes, not just tweaking the economy a little reply whimsicalism 16 hours agorootparentIn the absence of interference in the media by rich people. Not the absence of interference in climate change. > requires infrastructural changes, not just tweaking the economy a little 'Infrastructural changes' -> Seems like it can be subsumed in 'tweaking the economy', did you mean 'structural changes'? And if so, do you think people who are unwilling to pay $10/mo to avert climate change are going to support structural changes to the economy to do so? reply WithinReason 16 hours agorootparentprevwhat do you mean by infrastructural changes? reply advael 15 hours agorootparentAn often-cited example is the considerable investment we've made into spread-out living spaces that require that people are doing long, daily car-based commutes. This kind of planning creates a situation where an increase in the price of gas is devastating to people's livelihoods and ability to move through the world, which is a lot of why gas prices going up can be a make-or-break political issue for so many people. The flight to suburbs is a relatively recent phenomenon that could be reversed with different infrastructure choices. We could invest into mass-transit and, probably more importantly, better colocation of the resources people need to live, especially in an era where a lot of job functions can be done remotely (and a considerable segment of the economy clearly want to work remotely if they're allowed to) Obviously this isn't going to just naturally occur, but we are already pretty artificially supporting the situation as it is: Part of the reason gas prices can stay low is that we already dump considerable subsidies into the already-profitable oil and gas industry reply whimsicalism 13 hours agorootparentYour solution would involve massively raising the cost of suburban and rural living. If it doesn't involve that, then people won't move. I'm generally in favor of that - but the people who won't even support $10/mo in electrical bills are not going to support tearing up suburbia. Restructuring our infrastructure/society is not an easier ask than higher gas prices. reply Log_out_ 14 hours agorootparentprevPeople just vote with there feet against this life of monks and hermits offered. And the usual rat catchers take advantage. Which make those insisting on those non working policies of reduction, instead of boosting science, accomplices feeding democracy to the rats. reply Dig1t 16 hours agorootparentprevIf you scroll down to the bottom of this thread you’ll see a ton of flagged and dead posts from people who question this stuff. There are plenty of normal people who doubt the scientific conclusions of climate scientists. It doesn’t help that the solutions to the problem almost always seem to be taking away freedoms and raising taxes. See: farmers in the Netherlands getting their farms taken away by the government, French folks rioting over increased taxes, ULEZ cameras in London fining people for driving in the wrong neighborhood, etc. It seems that the solutions are all sticks and there are no carrots. The majority of people want to fix the problem, myself included. But I also deeply care about the growing authoritarianism that we are seeing across the world. I want to decouple fixing the climate from authoritarian governance. reply lastiteration 16 hours agorootparentThis is exactly right. Let's engineer solutions that are technologically based and better than what we currently have, not increase government control over people. Freedom is at least as important as the climate. A solution that is mandated is not the same as a solution that emerges organically as better both for the user AND the environment. reply fnimick 15 hours agorootparent> Freedom is at least as important as the climate. This is a value judgment, not an absolute statement, and should be addressed as such. Should you have the absolute freedom to pollute as much as you want if it harms your neighbors? What if a solution exists that is better for you and the environment, should you have the freedom to not take it if you don't care about the harms to those around you? Do you view having to have emissions controls installed on your car as an unjustifiable infringement on your freedom? reply Levitz 16 hours agoparentprev>There is just no democratic support for fixing global warming since it's too expensive. See the riots in France when they tried to tax gas more heavily. It's not that simple. Gas is used primarily to work. People can't afford to live close to their jobs, so they pay money and take their time to get to their work, making them pay even more for gas with the absolute shitfest the housing market is means kicking someone while they are down. Not to mention, if anyone is actually serious about reducing carbon emissions from commute? Penalize every single work that can be done remotely if it's not done so. It's an evident solution and there are A LOT of office workers, but some people don't like that idea, so instead of that poor people get screwed. Again. The idea that it's about democratic support itself is something corporations have lobbied for, it's really not, the most evident example was covid, where consumption dropped like a rock, what was the first thing any government did to bounce back? Promote consumerism as much as possible, essentially throwing money at people. Our economic system is not equipped to deal with climate change, it's that simple. Not only are we on a train with no brakes, we are in that train, rocky terrain made us slow down and we shoved as much coal into the engine as possible to get back to speed because we can't have it any other way. For Christ's sake, democratic support is the only reason there is any consideration for the environment at all. No corporation would ever give one single solitary shit about climate if it wasn't for \"democratic support\". reply fransje26 15 hours agorootparent> For Christ's sake, democratic support is the only reason there is any consideration for the environment at all. No corporation would ever give one single solitary shit about climate if it wasn't for \"democratic support\". Let's give this little gem the highlight it deserves. reply matthewdgreen 16 hours agoparentprevThe good news: the renewables explosion is going to make a big difference. Thanks to the plummeting cost (and rapid deployment, particularly in China) of solar and wind, we are almost certainly going to see better emissions pathways than the bad ones we're currently projecting. None of this happened by accident: it was engineered. The bad news is that we seem to be much closer to some really bad climate outcomes than scientists realized. Sensitivity may be higher, warming seems to be further advanced than we thought, and there are many scary tipping points that we could easily trigger. My guess is that we're going to need some sort of (relatively near-term) geoengineering solution to keep things from spiraling, which means we really have no room left for climate denial. (And to be clear, this isn't going to be an alternative to decarbonizing, it's going to be in addition.) reply timeon 16 hours agorootparent> we are almost certainly going to see better emissions pathways than the bad ones we're currently projecting I hope you are right. But when I am looking at ourworldindata chart about energy sources I still do not see this picture. While renewables are increasing, fossil fuels are not decreasing. It just seams that renewables are starting to cover ever increasing demand - not replacing all demand. reply adgjlsfhk1 15 hours agorootparentthe good news is that 1. in most of the world energy usage is steady or decreasing 2. renewable energy production is accelerating 3. while renewable energy hasn't killed fossil fuel use, it has shifted it dramatically. coal is basically dead in most of the world because it has been priced out by renewables. reply lagt_t 16 hours agorootparentprevThe lithium bubble burst is an indicator that the renewables push is losing strength. The pandemic and later the yemen-gaza-ukraine combo shifted priorities. Meanwhile Europe is no longer inching but running towards an ecological disaster with the ever increasing drought-deluge cycles. reply matthewdgreen 16 hours agorootparentI don't think so. Right now the renewables push is 90% about China: they're the biggest global emitter and also (previously) had the biggest rate of emissions growth. They're now expected to plateau this year and enter a structural decline. And China is still deploying renewables at an unbelievable rate [1] as of last month. [1] https://www.reuters.com/business/energy/chinas-wind-solar-ca... reply lagt_t 16 hours agorootparentWe are lucky that the real estate bubble finally burst in China, that was one of the key drivers of pollution there. reply JohnMakin 16 hours agorootparentprev> The good news: the renewables explosion is going to make a big difference Sorry but I've been hearing this for 15 years now. It's only resulted in widespread greenwashing and misplaced climate optimism. Now, even if everyone went carbon neutral overnight with renewables the situation is so dire that we may still be in a very bad spot (as you mention in the rest of your post). And, while I'm at it, let me mention that part briefly: > The bad news is that we seem to be much closer to some really bad climate outcomes than scientists realized. They realized it for a long, long time. No one listened, and people/media latched on to the most absurdly optimistic models as a sign that everything was fine and we could continue to procrastinate on this problem. Any \"pessimistic\" models were thrown out as bunk science. stuff like this is even now still referred to as \"doomerism.\" reply antisthenes 16 hours agorootparent> Sorry but I've been hearing this for 15 years now. Really? Care to share the cost per KWh of batteries numbers from 15 years ago? What about $/Watt for PV modules? Or maybe just deployment numbers of solar/wind? I mean maybe, just maybe, we can have progress on the renewables front without people crying about \"greenwashing\"? > No one listened, and people/media latched on to the most absurdly optimistic models as a sign that everything was fine and we could continue to procrastinate on this problem. Many areas in the US will be fine. Rich people ignore it because they have the money to move away from affected areas, and poor people don't really have a media voice. But that's typical USA for you, so nothing surprising there. reply JohnMakin 15 hours agorootparentI don't really know how you can square this supposed \"tremendous success\" of renewables when statistics like this are out there: https://statista.com/statistics/282716/oil-consumption-in-th... Oil consumption is still increasing year over year. People like you have been saying the same things about renewables for a long, long time. I suppose it'll come any day now! reply ianburrell 14 hours agorootparentRenewables affect electricity production, not oil consumption. The renewable share is up to 41% in 2022. Oil consumption in 2022 is below 2005. It is increasing since the dip for the pandemic but it is below 2019. The population has grown since 2005 so the per capita usage has dropped significantly since 2005. reply s1artibartfast 15 hours agorootparentprevThere are more sources of carbon than oil. Now do coal. Consumption is down 50% from peak, and back to 1970's levels. https://www.eia.gov/todayinenergy/detail.php?id=44155 Why beat around the bush when we can just look at actual US CO2 emissions which are down 20% since 2005 (see figure 5). https://www.eia.gov/environment/emissions/carbon/ Edit: I always find it fascinating how negatively people respond to any mention of progress. reply tw04 16 hours agoparentprevIt isn’t a political problem, it’s a messaging problem. The fact we allow a large group of folks to outright lie about the problem on public airwaves results in the “division”. I’m all for freedom of speech, but when public figures are knowingly telling lies, whoever is broadcasting it should be FORCED to follow it up with the facts of the situation. In other words, at least in the US, the fairness doctrine needs to be re-instated. The number of things this country has killed off “because it worked so well we don’t need it any more” is mind numbing. reply whimsicalism 16 hours agorootparentMany of the countries we are discussing have much less significant influence of money on media and still - raising prices is very unpopular. The left needs to come to terms with the fact that sometimes the things they are proposing are unpopular independent of some “dark money influence scheme.” reply Timon3 16 hours agorootparent> Many of the countries we are discussing have much less significant influence of money on media In absolute terms you are correct, but in relative? Is there really a country where the media isn't strongly influenced by the rich and powerful of said country and the world? reply whimsicalism 16 hours agorootparentThis is a theory of the world that seems pretty non-falsifiable. \"The real, uninfluenced people agree with my policy priorities and the only reason they don't is due to money at play.\" Any example of people disagreeing just proves that there is monetary influence that we haven't yet identified. But I actually think it is somewhat falsifiable. Most people, when polled, are in favor of solving climate change - especially in nordic countries. But when you poll people with imposed costs, ie. $10/mo extra to completely solve climate change - the vast majority are opposed. [0] This seems like people organically responding negatively to costs, not being brainwashed by media - otherwise they wouldn't support solving it in the first place. [0]: https://www.cato.org/blog/68-americans-wouldnt-pay-10-month-... reply Timon3 16 hours agorootparentI didn't state a theory, I wrote a simple counter-argument to your position. If you're correct, you should be able to produce evidence that proves your position correct. I don't think you are correct, but I'm happy to be proven wrong. reply whimsicalism 15 hours agorootparentthe original commentator indicated that the reason people are against it is due to the influence of money in politics. i feel like i provided evidence against that theory, ball is not in my court reply JoshGG 16 hours agoparentprevJust watch how expensive not fixing it will be. reply cmdli 16 hours agorootparentI feel like this take is dismissive and unhelpful. Expecting people to sacrifice their current wellbeing for the sake of a nebulous, far off reckoning is simply unrealistic and a bit of a dead end in terms of climate progress. We would be better served finding lower impact solutions that are more easily adopted than high impact ones that are never going to be accepted. reply kccqzy 16 hours agorootparentWe aren't exactly asking people to sacrifice their wellbeing and convenience completely. That's why for example we aren't asking the population to ditch their cars and switch to trains and bikes, but we are offering them electric cars, which are exactly the kind of lower impact solutions you are speaking of. reply methodical 16 hours agorootparentAn electric car isn't really a lower impact solution for a large amount of the US population. From what I'm able to find, the lowest priced EV (Chevy Bolt) starts around $27k currently[0], whereas the cheapest gas car (Nissan Versa) starts at $17k[1]. This is without even accounting for the larger difference in the used car market, cost of installing a level 2 EV charger (which is another $1k-$2.5k[2], which is necessary for anybody who does any significant amount of traveling in a single day), or the 25% higher insurance premiums for EVs[3]. All said the difference between the two options is a pretty significant difference for most of the population who are already struggling because of the significant inflation rates experienced in recent years[4]. [0] https://www.chevrolet.com/electric/bolt-ev [1] https://www.nissanusa.com/vehicles/cars/versa-sedan.html [2] https://www.jdpower.com/cars/shopping-guides/how-much-does-i... [3] https://www.valuepenguin.com/how-having-electric-car-affects... [4] https://www.usinflationcalculator.com/inflation/current-infl... reply kagakuninja 16 hours agorootparentprevElectric cars help, but are not the solution. If everyone in Asia and Africa bought electric cars, it would be a disaster. reply kccqzy 15 hours agorootparentThat's my point. We aren't looking for \"the solution\" we are looking for something that helps. reply fhub 16 hours agorootparentprevAustralia is already having a bit of reckoning due to climate change in the way of mega fires and multiple \"hundred year\" floods per year in populated areas. At the last federal election, a newish political movement of independents took a large number of seats in parliament. This was possible due to preferential voting (like stacked voting in USA). The left party won power and has done very little on addressing climate change. I'm now convinced that political will just isn't there even if the people are there (Large emitters donate to politicians). So I see an inevitable path where geoengineering will be seriously on the agenda inside 15 years. reply Joeri 16 hours agorootparentprevWe’ve been talking about this problem since the 90’s. If there were lower impact solutions they are either long past the moment they needed to be implemented (back when denial was still the norm) or they just don’t exist and aren’t coming soon. Magical thinking isn’t helpful, we have to fix this problem with the solutions that exist, not the solutions we wish existed. Go to war with the army you have, no? Or not, and just decide that future generations are going to have to suck it up, which is the default choice anyway. reply mr_toad 10 hours agorootparentprev> We would be better served finding lower impact solutions There aren’t any lower impact solutions that will cut carbon emissions by 40% in the next 5 years. reply quonn 16 hours agorootparentprevWe know that, but it doesn't change a thing. Your or my opinion has little influence. Therefore baby steps should be taken which will reduce that cost. reply hayd 16 hours agorootparentWhy baby steps vs radical technological innovation? We waste our time with virtual signalling nonsense, that hurt the least affluent here, whilst China/India/Africa are going to be outputting vastly more CO2 to make any of our efforts net-irrelevant to climate change. We need to fix it for the world. That'll require big-impact innovation. reply erikpukinskis 16 hours agorootparentprevHow is that relevant? The voting public will typically delay expensive cost-saving measures. Because the majority of individuals will do the same. Also forced public expenditures are a huge money making opportunity for the capital class, so they will tend to encourage delay as well. reply whimsicalism 16 hours agorootparentprevthe reality is that the costs will mostly be borne by future generations. the cost we need to confront climate change is likely larger than the cost in this generation if we let it rip. reply bcrosby95 16 hours agorootparentHow do you define \"future generations\" anyways? People that aren't born yet? Because that sounds way more optimistic than I am. reply whimsicalism 16 hours agorootparentIf the notion of future generations sounds way too optimistic, you are likely far too pessimistic (if this is about climate change). There is no realistic scenario for lack of future gens due to climate change in the next 100 years even with absolutely terrible undiscovered positive feedbacks. reply bcrosby95 15 hours agorootparentI'm not saying there won't be future generations. This is what I was replying to: > the cost we need to confront climate change is likely larger than the cost in this generation if we let it rip. I'm saying the cost to confront it will be less than the cost to \"let it rip\" before some current generations die off from old age. reply whimsicalism 15 hours agorootparentI think that you are wrong, I am well versed in climate science. The costs to avert now are significant, even more significant than the changes needed for sea level adaptation in our lifetime. Most of the costs will be borne out by subsequent generations. reply irrational 16 hours agorootparentprevThose in power are counting on the cost having to be paid by future generations. I assume they either don't have children/grand-children, don't like their children/grand-children, or are so narcissistic that it never occurs to them to think about their children/grand-children. reply nullindividual 16 hours agorootparentTheir children will have enough money to carve out their own missile silo. The poors need not apply. reply echelon 16 hours agorootparentprevIf you tell people they have to stop all travel and electricity use and plastics purchases, they won't do a thing. There are no levers to pull besides technological ones. reply poulsbohemian 11 hours agorootparentThe plastics one needs to be done through legislation and collaboration with industry… there are so many products where consumers don’t have a choice - the manufacturers need to take the responsibility for eliminating single-use packaging that doesn’t biodegrade or recycle. Yes it may be painful, but there’s no other way to do it reply paganel 16 hours agorootparentprevAs expensive as “not staying in our homes for two weeks turned out to be”. Mind you, I was in the camp demanding just that at the beginning of the pandemic. Which is to say that the technocratic powers that be should tone it down a little when it comes to imposing their top-down decisions, after all they’re just a minority compared to us, the common people, we (the common people) can always bring to power some of our leaders in order to fight (ideologically and not only) said technocrats. reply 12345hn6789 16 hours agorootparentprevExpensive for folks in the future* That is what is running through people's heads. And apparently they are not a part of this future either. reply collyw 15 hours agorootparentprevProve that it actually a problem. In the 70's it was global cooling and a new ice age that we were to live on fear of. Do we really expect the climate to remain stable? Something that has never actually happened? That was all your fault as well. \"The Climate Change Saga unfolds as we visit 1978 and Leonard Nimoy explains how the human population is causing The Earth to Freeze\". https://www.youtube.com/watch?v=0-ZDnSbNIYs reply acuozzo 4 hours agorootparentWhat's your plan for when the boy cries \"Wolf!\" and he's actually there? reply collyw 25 minutes agorootparentCheck that it is actually there. I have heard doom monger predictions for over 3 decades and none of them have come true to date. I live by the sea and the level hasn't risen by any perceivable amount. 30 years ago I was told that major cities would be underwater by now. reply davis 16 hours agoparentprevYou haven't been following the recent developments in renewable energy and the deployment of clean tech then. We don't need new technological advancements. We just need to continue to produce more solar, batteries, wind and keeping existing nuclear plants afloat. We have all we need. It is just a matter of upgrading the grid and building enough to transition. reply ericd 16 hours agoparentprevWe don’t need to wait for technical solutions to make things cheaper. We need a revenue neutral carbon tax with dividend. For most people, especially the poor, they’d come out ahead, and it would allow the market price signals to gently steer everyone toward a lower carbon life. reply tschellenbach 16 hours agorootparentHow do you get people to vote for this given that it will increase prices/lower their standard of living today? reply ericd 16 hours agorootparentIt's revenue-neutral, so all money collected gets redistributed out evenly, per-capita (the \"dividend\" part). The way that then works out is that if you're responsible for less carbon emission than average in your country, then you end up ahead, potentially well ahead. Since carbon emission maps pretty well to material consumption, and things like flying a lot, that means that most people will end up ahead, and this mainly costs more for people on the richer end of the spectrum. Likely the breakeven point is well above the median, given how wealth and income are distributed in a bit of a power law distribution. An extremely rich person gets the same amount back as anyone else, but his yacht and private jet become a hell of a lot more expensive to run. This also has the benefit of making local manufacturing more competitive with overseas, since transportation becomes a larger part of the cost of goods. This would give blue collar workers back some of the leverage they've lost over the past decades. So most people voting in just their short-term self-interest would do well to vote for this. And everyone gets the long-term benefit of civilization being less likely to collapse with massive resource shortages. The challenge is explaining it well enough that people understand well that it's in their benefit. But I think \"all the money from the tax goes into your bank accounts every month\" would help a lot. reply lfuller 16 hours agorootparentprevThat was tried in Ontario (cap and trade), but there was enough misinformation going around that the general populace thought it was just a standard tax on individuals. reply nullindividual 15 hours agorootparentWashington State does cap and trade on industry. And yes, it has just become a 'standard tax' on individuals passed from industry to the pump to the tune of 40-50 cents/gal. While I agree that it is important to reduce consumption of fossil fuels, the infrastructure isn't there to enable people to commute from lower-cost areas of living to higher-cost areas of working. This will likely be repealed by voters via initiative[0] this fall. [0] https://www.seattletimes.com/seattle-news/environment/initia... More fun reading on our defacto gas tax: https://www.king5.com/article/news/investigations/former-wsd... reply ericd 16 hours agorootparentprevDid the people there start getting money every month into their bank account (the “dividend” part)? If not, I imagine that that would’ve cleared that misinformation up rather quickly. reply freetime2 16 hours agoparentprevI’ve always thought a carbon tax where the revenue is distributed directly to citizens in the form of a monthly check or bank deposit could be popular, or at least accepted by the public. Especially if it starts small and then ramps up over time. reply whimsicalism 15 hours agorootparentYes, I wish more things were paired like that. Another one: land/property tax increases paired with income tax decreases. reply hnthrowaway0328 16 hours agoparentprevIt is understandable if the riches and powerful people are flying around the world in private jets. Maybe they don't generate the most as a whole but a lot as individuals. It is very difficult to convince anyone to lower their standards of life (eating other food / shower less / drive a smaller car) which will make a big impact while the riches just do lip services. I don't see a future for that. Humans only turn around when they hit a wall multiple times -- we are not the exceptions. reply ejb999 16 hours agorootparentnot a popular opinion, but I agree completely - if you are trying to sell a message of sacrifice to the common person, being preached to, by people who own and travel in private jets , while they return to one of their many 20 bedroom mansions located just feet above the sea that is supposed to have a devastating rise any day now - it is very, very easy to see why people just shrug it all off as scare tactics when they are asked to give up their tiny little gas lawn mover or gas stove. May not be fair, and may not be scientific - but the message delivery matters - ignore it at your own peril. reply brightball 16 hours agoparentprevThere are a lot of factors at work here at the same time. It's far off (or perceived that way) and people naturally seek relief of immediate pain. It's human nature. You tell somebody that they'll save $X over the next 10 years by spending a fraction of it now, most won't bother. On the other hand, if you're currently spending $X on something and you need to invest in something to get that cost down, they'll do it almost immediately. Because it's far off, there's not a sense of urgency to it. Additionally, there are some fairly strong factors that call a lot of climate change activism into question, like wide spread opposition to nuclear power by many of the same activists. Examples like Germany shuttering nuclear facilities only to turn around and turn to coal power certainly doesn't help. Other factors such as the need for change across the entire world complicate matters when numerous businesses will just send their pollution to countries that don't impose the same requirements. Separating hyperbole from practical application in pursuit of pre-existing goals is another huge factor. We've heard about methane emissions from cattle ranches for years. It seems like we have a great solution for this by adding an ingredient to the feed: https://www.dcceew.gov.au/climate-change/publications/from-b... But rather than pursuing this, activists are more interested in continuing to attack the industry and pushing vegan diets when a potential solution seems readily available. Factor that in with alarmism over temperature increases as if every potential increase is man made while other factors are ignored by most outlets... https://climate.nasa.gov/news/3204/tonga-eruption-blasted-un... https://eos.org/articles/tonga-eruption-may-temporarily-push... There's just a great deal of inconsistency and alarmism that undermines credibility when ignored. reply aniftythrifrty 14 hours agoparentprevAll that is required to solve climate change is the elimination of the Military Industrial Complex, the largest polluter on earth by far and the most useless thing on earth by far. reply nojvek 14 hours agorootparentGot any sources on that? I'd like to learn more. reply mytailorisrich 16 hours agoparentprevWhat happened in France was not just \"because they tried to tax gas more heavily\". What the government did was textbook \"let them eat cake\". They suddenly decided to significantly increase taxes on diesel \"for the environment\" when people, especially trades people and people outside big cities, had no choice. Therefore, predictably, the people rebelled. As long as governments refuse to understand that, we won't get anywhere, indeed. reply mr_toad 10 hours agorootparentAs long as people refuse to understand that reducing carbon emissions will mean substantial reductions in their standard of living then we wont get anywhere either. reply mytailorisrich 7 hours agorootparentAre you volunteering to lose your livelihood? reply hotpotamus 16 hours agoparentprevI've heard the talking point regarding innovation for awhile now. To me it sounds a bit like an alcoholic with a failing liver hoping that medicine will come up with some miracle cure so they can keep drinking as much as they want. reply tschellenbach 16 hours agorootparentI think it's the opposite. The push for politics to do something is having an impact, we are making progress in lowering emissions. But it's not enough, and there are constraints on democratically elected leaders. Eventually, public sentiment will change, but not soon enough. reply marbartolome 14 hours agoparentprev> I like Bill Gate's book on the topic. So what's the book? reply Throw73747 16 hours agoparentprevPerhaps we should have a referendum, so people can show their support in a democratic way... Nobody is stopping people doing unpopular things to solve climate change. You just have no right to coerce others into that! reply atoav 16 hours agoparentprev> There is just no democratic support for fixing global warming since it's too expensive. There is no democratic support for fixing global warming in a capitalist system geared towards extracting value. If you ask people if they think global warming should be tackled in principle you will find many in support. The problem is that most people know that the way in which a fix for global warming would be implemented today would result in them bearing the costs while some shareholders somewhere get just a little richer. If we want to stop humanity from extinction we have to step off the gas pedal and disincentivize those who keep wanting to step on it. reply hedora 16 hours agoparentprevThere's strong popular support to fix climate change. We know this from polls. We also know that there are well-organized foreign propaganda teams attacking democracies around the world. The fact that some ill-informed right-wing radicals in France rioted doesn't mean there is no popular support for having a habitable planet in twenty years. reply whimsicalism 16 hours agorootparentYou cannot poll on things without tradeoffs if you want a real answer. Most Americans have been polled as unwilling to pay $10/mo more in electrical bills if it meant that climate change were completely averted. I think comments about 20 years uninhabitibility are unproductive as most people see it for the lie that it is. reply fnimick 16 hours agorootparentprevAnd yet every single proposed policy to fix climate change is deeply unpopular. People support \"fixing climate change\", but when they are asked to deal with a rising gas tax, or traveling less, etc, suddenly climate change isn't worth fixing. reply graemep 16 hours agorootparentprevThere is strong democratic support so long as someone else pays for it. > The fact that some ill-informed right-wing radicals in France rioted doesn't mean there is no popular support for having a habitable planet in twenty years. and alarmism does not help the cause. reply t0bia_s 16 hours agorootparentprevWhat car do you have? reply collyw 15 hours agoparentprevWhy does it need to be fixed? What's wrong with it getting a little warmer? reply Clubber 16 hours agoparentprev>There is just no democratic support for fixing global warming since it's too expensive. There's plenty of support, the US has lowered its carbon emissions quite a bit. There's one big problem though: what do we do about China and India? Are you prepared to get drafted and go to war to get China to stop burning coal? reply davis 16 hours agorootparent2023 made it very clear that China knows the stakes and also sees an opportunity with owning the clean tech future. They built more renewable energy just last year than has been installed in all of the United States in its history. BYD just passed Tesla as the biggest producer of EVs. The opportunity is there and with China's faltering economy that was propped up by housing, I expect Xi to do all he can to be the provider of the world with renewables. reply Clubber 16 hours agorootparent>2023 made it very clear that China knows the stakes What happened in 2023? https://www.macrotrends.net/countries/CHN/china/carbon-co2-e... reply davis 15 hours agorootparentYou have to burn carbon to build renewable energy components if you don't have enough energy from existing solar + wind; I don't know what you expect. But now they have ~500GW of energy to build next year's >>500GW [1]. It's an investment and every year it pays off. China's emissions peak wasn't supposed to happen this decade but that has been revised [2] 1: https://www.reuters.com/world/china/chinas-solar-capacity-ex... 2: https://www.reuters.com/world/china/chinas-carbon-emissions-... reply exitb 16 hours agorootparentprevWhich country should go convince China, the one that has twice the CO2 emissions per capita? reply Clubber 16 hours agorootparentIf 3 people are releasing chlorine gas in a large room and only one of them slows down, it doesn't make a huge difference what the per capita emissions rate is, does it? reply arrowsmith 16 hours agoparentprev> in a democracy leaders can't do deeply unpopular things. Oh, you sweet summer child. reply dog_boxer72 16 hours agoprevMy own father, who is in his 60s, has said blatantly to me that he will be dead before the real problems hit, so he would rather not give up his cruises, large cars and international flights - he just doesn’t even care that his grandchildren will suffer, since he personally won’t be around for it. Completely dispicable, but ultimately, asking people to do the right thing out of altruism is never going to work. Democracy will not work either because these people vote. At least he can admit he’s being selfish I guess. I think most climate deniers, especially the older ones, simply can’t face the idea they might be responsible for terrible things happening, and don’t want to change now, so the brain latches on to any explanation that spares them that psychological pain. Thus arguing facts is pointless with these people because it’s about feelings not actual truth (as many things are with our species). The solution is to hack peoples psyche to make them “feel good” about climate change, somehow… reply mkoubaa 14 hours agoparent100% Democracies are accountable to the interests of people as they understand them. Nobody wants to be told what's good for them. reply popularrecluse 16 hours agoparentprevThere's that 'me' generation for you. My parents say the same shit. reply Synaesthesia 16 hours agoparentprevDemocracy will work, it's the only thing that's ever worked. Mass movements achieved pretty much anything you can think of, like better working hours, better rights for women, civil rights, end of slavery ... reply hn_throwaway_99 16 hours agorootparentLiterally every (correct) example you gave is a case of people voting in their own short-term self interests, a dynamic which does not exist with global warming. In the case of slavery, democracy didn't end this. Southern states literally seceded, and we had the bloodiest war in American history, to end slavery. And the dynamics there were that the economic systems of Northern states didn't rely on slavery, so they had little problem opposing it on moral grounds. The entire economic system of the South was built around the plantation system and slavery and so it's not surprising democracy didn't end slavery there. reply sc68cal 16 hours agorootparentprevThe end of slavery in the United States was achieved by killing enough people who politically supported slavery in a war, that they surrendered and accepted the _de jure_ end of slavery. However, they then spent a century in a a low intensity conflict / rebellion (Reconstruction, Jim Crow, etc) against it, with significant success (unfortunately). Democracy is not the cure all that you make it out to be. reply thatguy0900 16 hours agorootparentprevWell, those things all directly benifitted the people who were currently alive campaigning for them. It doesn't go against op's point. reply ffsm8 15 hours agoparentprevI see literally no issues whatsoever with that outlook unless he's the CEO of a multinational company. Personal responsibility is pointless for climate change. The only thing that you can do - that actually impacts the climate - is to not have children. Everything else isn't even going to be a rounding error. Even if aggregated with thousands of other people abstaining as well. The whole thing is just purely virtue signaling so people can tell themselves that they're doing something and it's the other people's fault that it keeps getting worse. reply jetsetsushi 3 hours agorootparentAgreed, though for children there’s an argument that you could have a descendent that solves climate change with some technical achievement or discovery. Seems like a long-shot though, so I wouldn’t take that bet. To reiterate, there’s “no one simple trick” you can personally do to affect climate change. No mainstream action is worth a damn. That’s kind of the definition of a systemic issue. reply s1artibartfast 14 hours agorootparentprev>Everything else isn't even going to be a rounding error. Even if aggregated with thousands of other people abstaining as well. I truly dont understand this mindset. The total is always the sum of the components. You actions make the same difference if you do them alone or with 7 billion other people. Both actions are the same tiny marginal impact. The alternative just comes off as an excuse to do nothing. As if car manufactureres will just keep building cars that nobody buys. reply ffsm8 13 hours agorootparentEven if everyone in your city stopped doing $whatever you're taking issue with, the climate will not be impacted. Climate change is a global problem, nothing besides global regulation that's actually enforced will have a meaningful impact. It's just too profitable to ignore climate impact for this to change. What you're doing is just lying to yourself if you honestly think your actions (even if aggregated to hundreds of thousands of people doing the same), will be a meaningful contribution on the issue. And just to be clear, pollution is another story entirely. That's primarily a local issue and can be significantly improved (not solved!) through personal responsibility. reply s1artibartfast 13 hours agorootparentMost people don't apply your reasoning to any other moral question. Nobody thinks that killing a child is OK because it has little impact on the grand total of child deaths. They don't rationalize that some warlord in Africa will keep killing, even if they don't. How is this any different? You are either part of the solution or part of the problem, and accountable for your actions. My point is that the contribution to climate change is exactly proportional to your personal emissions devised by the the total. You talk about policy Solutions, but how is that any different than personal action? reply yboris 17 hours agoprevThe graphs of water temperature over the last 2 years have been off the charts - literally; required adjusting the y-axis: https://twitter.com/LeonSimons8/status/1754540413350732180 reply phkahler 16 hours agoparentBut why is that? Why a sudden jump this year? reply michael1999 14 hours agorootparentThere are lots of forces, and the feedback loops are starting to hit. - Warming has been going on for 200 years, so we are deep into it. - The polar ice sheet has been shrinking because it is warmer. So instead of the long summer days reflecting off the ice, they are absorbed by the ocean. - Shipping has been eliminating sulphur from bunker fuel to reduce acid rain and smog (2020 change), and the lack of smog allows more sun to reach the earth. The recent shock of cleaner shipping exhaust has combined with the historical warming and the feedback loops of falling ice albedo (and others) shows up in warmer water. https://www.nasa.gov/missions/aqua/nasa-study-finds-evidence... https://nsidc.org/arcticseaicenews/charctic-interactive-sea-... reply kouru225 16 hours agorootparentprevIf I remember correctly: Ships were using a particular type of gas that was outlawed recently. Turns out it was also acting as a coolant for the atmosphere. The bad news is that global warming is further along than we expected. The good news is that we now have evidence that a global campaign for cooling the atmosphere can actually work. Can’t find sources right now, but it was very interesting when it came out ~6 months ago. reply Levitating 15 hours agorootparent> it was very interesting when it came out ~6 months ago. It was also underreported. Hank Green made a great video about it.[1] It lists some sources in the description. [1]: https://youtu.be/dk8pwE3IByg?si=aiOv-7MGZRa6D9lS https://www.science.org/content/article/changing-clouds-unfo... reply lalalandland 15 hours agorootparentprevI heard that ships have cleaner fuel so their exhaust don't block as much solar radiation.[1] [1] https://pubs.rsc.org/en/content/articlelanding/2023/ea/d2ea0...! reply steve_adams_86 16 hours agorootparentprevAnother component besides el ninõ could be pollution from shipping emissions: https://www.ox.ac.uk/news/2022-10-06-new-analysis-shipping-e... reply hn_throwaway_99 16 hours agorootparentprevThe next self reply to that tweet (xeet?) hypothesizes it is reduction in aerosols (massive reduction in sulfur in global shipping): https://twitter.com/LeonSimons8/status/1754541230774419929 reply ancientworldnow 16 hours agorootparentprevOne component is that El Nino started. Until 2023 we were in La Nina that was suppressing ocean temperatures. reply downWidOutaFite 16 hours agorootparentprevThe reduction of sulfur dioxide pollution from shipping in 2020 has had a pro-warming effect. reply malfist 16 hours agorootparentprevChange in the fuels cargo ships use. Previous fuels where high in sulfur which is all kinds of nasty, but has the benefit if seeding clouds. More warming this year is due to fewer clouds, at least partially. reply sircastor 17 hours agoprevMy wife is taking a course in environmental architecture, and the information she's been sharing is nothing short for terrifying. Worse, the messaging about \"doing your part\" seems embarrassingly naive now. reply Y-bar 16 hours agoparent> \"doing your part\" seems embarrassingly naive now. Yes and no. I would even go as far as to say that nihilism of no value in doing the right thing has been one reason we have reach this point -- we have lost a lot of collective action because of the sentiment that nothing you will do will matter. Collective action requires individual action. The major industries don't just pollute for the sake of it, they respond to demand for which we individually share a part of the blame. I can choose to eat steak from pastures made from burned Amazon rainforest, or I can choose more local, less carbon-heavy food. I can choose bottled spring water, or use my tap. I can choose to take shorter showers. I can choose to buy a smaller car and rent when I only really need the horse-power to drag a trailer. I can choose to take public transport a bit more often. I can choose to buy fast-fashion and replace my clothes often, or I can repair or buy repair services from a local vendor. And so on. Not all options are open to everyone, but like no raindrop is responsible for a flood, no person is responsible for anthropogenic climate change. reply amelius 16 hours agorootparent> they respond to demand for which we individually share a part of the blame. I believe a large part of the blame should go to advertising. Advertising works or it wouldn't exist. And it stimulates consumerism by definition. Personal opinion, but kill advertising and you solve a lot of problems. reply cruano 16 hours agorootparentprev> The major industries don't just pollute for the sake of it, they respond to demand for which we individually share a part of the blame. It seems to me like one could use the same argument regarding slave labor, but somehow we manage to force companies to comply there. reply Y-bar 16 hours agorootparentIn many places across the world it took herculean effort to force that change. But it did not happen because some abstract group of something did cause it to happen. It required major concerted actions, sometimes violent, of many individuals willing to take personal responsibility to change the situation. reply s1artibartfast 8 hours agorootparentprevYou might be surprised that the entire slavery abolitionist movement was driven by people who refused consume goods from slavery, and partake in the institution. reply hn_throwaway_99 16 hours agorootparentprev> Collective action requires individual action. This is demonstrably false. I'd posit that basically no lasting societal changes have been implemented by large scale individual sacrifices. In a similar vein, as this has recently been in the news and on HN a lot lately, when it comes to the obesity epidemic, we've been talking about \"individual responsibility\" and \"diet and exercise\" for literally many decades and we've only seen global waistlines expand year after year. The technological advancement of semaglutide is literally the first advancement that many obesity researchers believe will actually make a dent in overall obesity. reply Y-bar 15 hours agorootparentI think you are misunderstanding me. When I say say \"collective action requires individual action\" I mean things like this: - A union is not a piece of paper, it is a group of people who individually joined, and then as individuals participated in the collective strikes for improved labour conditions. Early movements included significant violence against these individuals who often kept on fighting. - An abolitionist movement is not an abstract concept, it contained multitudes of individuals willing to fight for a cause and route slaves to safety and lasting change. Many died doing the right thing. - Women's suffrage was not just an idea, it required many individual women and men to stand up and take action. Narges Mohammadi chose individual responsibility for her cause and so do many others, and they are paying a heavy price. - Marriage equality for LGBTQ+ is currently happening around the world because individuals take collective action, often with significant sacrifices and death brought on these minorities. The power of collective action comes from the multiplication of individuals, not just the sum of them. reply DFHippie 16 hours agorootparentprevI agree with all of this. Much regulatory action amounts to forcing people to make different choices. If they just made those choices, we would need less regulation. There are cases where individual choice alone is inadequate, such as when we need to pool our resources to achieve something or we need a central organizer to make collective action possible, but a vast portion of the legislation proposed is aimed squarely at changing individual choices. On top of that, making a choice oneself inspires hope and commitment. If you make the hard choice, sacrifices near-term comfort for the common good, cognitive dissonance rewards you. Likewise, if you make a morally lazy choice, cognitive dissonance soothes your conscience and makes it harder to choose otherwise in the future. reply zooq_ai 16 hours agorootparentprevChoice is a privilege. reply Y-bar 16 hours agorootparentThat's obvious to everyone. And I am one of those who have lots of choices, and I have an ethical obligation to use it correctly against those who have less leeway to adjust their lives. reply hedora 16 hours agorootparentprevLocal food makes basically no difference unless you're talking about crops that have a shelf life of a day or so. For everything else, distribution is well under 10% of the carbon footprint. Shorter showers make basically no difference to carbon footprint unless your water is heated by an antique. Our electric water heater is less than 10% our power bill, and most of that goes into standby mode. Smaller cars matter more than the above, but are basically just pissing into the wind. Halving transportation CO2 per mile won't be anywhere close enough to keep us under 2.5C. Taking public transit a bit more often also doesn't matter. What percentage of your CO2 footprint is it reducing? Avoiding fast-fashion mostly matters for microplastics (the clothing industry's carbon footprint isn't that big), but I doubt clothes were ever a significant fraction of the plastic you throw away. Summarizing, if everyone did everything you said, we'd still be completely screwed. We need infrastructure level fixes instead: - ban power plants that emit CO2 - set up international protections for timberland, and sanction / bomb any country that ignores them (cutting down rainforests should be treated in the same way as terrorist attacks) - add a $2 per gallon atmospheric carbon capture tax to gasoline. This means that, for every gallon of gas sold, we'd sequester the emissions of two gallons worth of gas. It also means EVs and hybrids, hydrogen fuel cell, and other technologies will rapidly become more popular - fund public transit (busses and trains) at the same level as the road network - tax the crap out of plastic so that natural fibers (and paper bags) are economically viable. This'd less than double the price of stuff. No individual can pull any of those levers except via political means or via technological innovation. reply monkeynotes 16 hours agorootparentprevUltimately even the behaviour of big polluters is driven by consumer demands. Consumers make financial choices that are incongruent with their values, all the time. If consumers didn't demand so much cheap energy we would have less coal burning power stations, for example. People fund corporations, we have to pay for alternatives which are usually a significantly higher cost that we refuse to accept. Narratives that corporations that are responsible for a majority of pollution are BS. The corporations are at the mercy of our individual wallets. I do think it's a government's role to unify and lead people through this. Provide quality alternatives, support offramping, etc. Without this essential cultural contract it just won't work. Individual effort is only meaningful if it's coordinated and a majority of people buy into it. But we can't get that leadership behind the effort if we, the people, just keep voting it out. So while it's admirable to individually \"do your part\" we know for a fact the overwhelming majority of people don't actually give a flip enough to prioritize the planet and work together. Raindrops do contribute to floods, but you need coordination to get there. reply t0bia_s 16 hours agorootparentprev- I can choose to buy a smaller car and rent when I only really need the horse-power to drag a trailer. Rising average weight of cars around proofs that people do not care about saving environment. Safety first? But I wouldn't blame consumers for buying what manufacturers make. Individual never has significant impact on environment as corporations does. So first we need to stop with hypocrisy to make people believe in narration of helping our future. reply amelius 16 hours agoparentprev\"Doing your part\" always seemed like a nice way for the rich to put their heads in the sand. Meanwhile at least 95% of the world population lives from paycheck to paycheck (immediate existential threat versus long term threat) and doesn't have the financial means to buy solar panels, EVs, etc., but also more durable items instead of cheap stuff from China. reply kiba 16 hours agorootparentNo snowflake is single-handedly responsible for the avalanche, plus rich people pollute the most anyway, especially those who live in the west. reply kibwen 17 hours agoparentprevOne way or the other, people will be doing their part. Might as well get used to consuming less now before contraction forces the issue. reply petercooper 16 hours agorootparentIdeally, but I wonder if that's how the psychology will play out. People faced with an uncertain future may just as likely throw caution to the wind instead? https://www.independent.co.uk/life-style/tiktok-young-worker... reply toomuchtodo 16 hours agorootparentprevIt’s more about building your life to maximize self sufficiency with the understanding that you will still need to rely on society for some needs. Also living somewhere with low climate risk and high confidence of access to fresh water for the remainder of your life (and that of loved ones, if applicable). When contraction occurs, if you’ve planned properly, it’ll be news you read about, not impact on your life. TLDR Optimize for the expectation of persistent volatility. reply tenpies 16 hours agorootparentJurisdiction is crucial too. The chances that a Western government will leave you alone during all of this are virtually zero. reply toomuchtodo 16 hours agorootparentThe chances you'll do better elsewhere are worse, unless you're a warlord. I have looked at places off the map for the next 100 years for this purpose. Maybe I'm wrong, where would you go to be self sufficient and ungovernable outside the Western world? reply 36933 16 hours agorootparentprevWhere are these places? reply bmelton 16 hours agorootparentFEMA has a list somewhere, but IIRC, the top of it included places like Buffalo, a few cities in Minnesota, Wisconsin, Ann Arbor. Established cities along the great lakes are supposedly best positioned, but I've seen reports indicating that the age of the cities' infrastructure was a weakness for basically every one of the supposed climate havens reply ejb999 15 hours agorootparentprevThis site is pretty good for checking out the various risks by county or census tract for about 15 different climate risks. https://hazards.fema.gov/nri/map reply raisedbyninjas 16 hours agorootparentprevClimate havens, near the great lakes in the US. reply quonn 16 hours agorootparentprevWell, there are plenty of maps that show which areas will be impacted more than others. reply bevesce- 16 hours agorootparentprevWhy? Why it's not better to enjoy why it lasts? reply Diederich 16 hours agoparentprevhttps://en.wikipedia.org/wiki/Deep_Adaptation reply flir 16 hours agorootparent\"The agenda includes values of nonviolence, compassion, curiosity and respect, with a framework for constructive action.\" We're still talking about Humans, right? We are so screwed. reply lp4vn 15 hours agoparentprevWhat terrifying information is she sharing with you? reply GeoAtreides 17 hours agoprevA couple more threads: https://news.ycombinator.com/item?id=39268387 -- flagged (why?) https://news.ycombinator.com/item?id=39272301 -- no traction https://news.ycombinator.com/item?id=39272405 -- no traction reply tokai 17 hours agoparent>flagged (why?) Some user flag posts that have a good chance of ending up with subpar discussion. reply kibwen 17 hours agorootparentIn this case, it's more likely to be flagged by the recurring contingent of climate denialists that always show up in these threads. reply tokai 17 hours agorootparentI make no judgement of which users or if the discussion will objectively be bad. Just answering GeoA's question. reply throwaway5752 16 hours agorootparentprevBrigade, the post gets flagged, and discussion disappears. Victory. It so easy with the HN mod system that it is almost embarrassing. reply GeoAtreides 15 hours agorootparentI said it before, I'll say it again: where there are rules there's a game, where there's a game, there are players and the players will play the game. reply KingOfCoders 17 hours agoprevIn another thread today one mentioned Hypercanes https://en.wikipedia.org/wiki/Hypercane And I so, haha, 12°C higher ocean temperatures needed, I will not live long enough to see this. Not so sure anymore. reply notamy 17 hours agoprevOn the one hand, it’s incredibly cool that this method (measuring via sponges) even works. On the other hand, what a depressing result. reply hotpotamus 17 hours agoparentI had the same thought - really cool that they calibrated it against modern measurements and were then able to find known historic anomalies in the data. On the other hand, it would be nice to get one or two stories where things are going better than expected, wouldn't it? Of course, then the deniers will just use them as evidence that everything is going to be OK, so we don't need to do anything. reply Levitating 15 hours agorootparent> On the other hand, it would be nice to get one or two stories where things are going better than expected This is that story! The change in temperature is the result of outlawing a particular chemical used in ship fuel. That means that we were subconsciously geo-engineering the atmosphere with positive results. https://www.science.org/content/article/changing-clouds-unfo... https://youtu.be/dk8pwE3IByg?si=lqt2E9Q3GMExhLZ- reply TravisCooper 16 hours agoparentprevAlmost seems too-good-to-be-true right? Such a tight margin-of-error... reply sebmellen 17 hours agoprevWe need more rapid cooling and SO2 right now — https://makesunsets.com is leading the way but we need as many companies in the rapid cooling space as possible. We need to address the heatstroke in an emergency-room-medicine style before going into the proverbial ICU of carbon sequestration and nuclear power buildout. reply alexose 16 hours agoparentStratospheric Aerosol Injection is, unfortunately, a third rail in the climate community right now. The perception is that it's very dangerous due to the unknown knock-on effects. Plus, the way it's depicted in sci-fi (e.g., Snowpiercer) doesn't help. We actually have a good understanding of what SO2 does in the atmosphere thanks to both volcanic eruptions and (until recently) the large amounts of particulate sulfur emitted from oceangoing ships. I think the more interesting question is how much risk we're willing to tolerate versus doing nothing at all. The risk of letting sea level rise continue unabated seems much worse to me than a (temporary!) intervention while we figure out full decarbonization. reply drewg123 16 hours agoparentprevI agree that we need a \"quick fix\" before things get too far out of control. I'm really concerned about things like the Gulf Stream weakening and the resulting rapid climate change. It turns out that we've been accidentally doing geoengineering like this for years, and we've suddenly stopped due to regulations: RhodesianHunter posted this link in a different thread: (https://www.thedailybeast.com/weve-been-accidentally-geoengi...) reply asoneth 16 hours agoparentprevI suspect we will end up resorting to large-scale geoengineering, certainly once people in wealthy countries start dying in high enough numbers, and I don't oppose it. But once we go down that road I am highly skeptical that humanity will ever wean ourselves off it. As per the website: \"Just one gram of our clouds offsets the warming effect of one ton of CO₂ for a year.\" I would wager that politicians and business leaders who read such a statement would be relieved to hear that they no longer have to worry about a cure because the symptoms are treatable. reply sebmellen 16 hours agorootparentPoint taken and that is certainly a risk, but the other risk is a massive anthropogenic extinction. We need fast-acting cooling solutions so that we have time to scale up long-term sustainable ones. reply asoneth 15 hours agorootparentAgreed that it is a smaller risk. That's why I do not oppose geoengineering despite being reasonably confident that once we start that path we will be wholly incapable of quitting it. reply thehappypm 11 hours agorootparentprevWhat will kill people in wealthy countries? reply uoaei 16 hours agoparentprevSure there will be many moving parts to any successful recovery, but we should be careful. The cure for heat stroke isn't a freedive in arctic waters. Be wary of anyone who makes singular arguments. reply sebmellen 16 hours agorootparentThe immediate cure for heatstroke is LITERALLY an \"arctic dive\" in the form of immersion in an ice bath https://www.mayoclinic.org/diseases-conditions/heat-stroke/d.... reply uoaei 16 hours agorootparentI didn't say ice bath, I said freedive, precisely for the reason that one is much more easily reversible than the other once you reach a sustainable temperature. If it was as easy to remove SO2 from the atmosphere as it would be to step out of an ice bath, then I would have said ice bath. reply andyjsong 15 hours agorootparentAs a co-founder of Make Sunsets, I'd like to clarify the role of SO2 in climate control. SO2 is considered \"easily removable\" because, when combined with water, it forms sulfuric acid and precipitates out. This mechanism partially explains why 2023 was the hottest year on record; the EPA's stringent regulations on SOx emissions have significantly reduced the aerosols in our troposphere, removing critical reflective materials. The impact of SO2, including its effectiveness and atmospheric residence time, varies based on its deployment location (latitude, longitude, altitude), concentration, and particle size. For instance, the International Maritime Organization (IMO) 2020 regulations have reduced SO2 emissions from cargo ships in the troposphere, leading to decreased respiratory illnesses near ports and less acid rain. However, this reduction in SO2 has also warmed ocean shipping lanes, prompting discussions about reintroducing sulfur into the troposphere. [1] Deploying SO2 in the stratosphere, above most of the atmospheric water vapor and where winds reach speeds of 200kph, allows it to spread globally and remain airborne longer (1 to 3 years). This higher placement necessitates less frequent applications for the desired reflective effect. The 1991 eruption of Mt. Pinatubo demonstrated this, injecting 20 million tons of SO2 into the stratosphere [2], cooling the Earth by 0.5°C and, according to some models, temporarily reversing decades of warming. Stratospheric aerosol injection (SAI) is likened to Earth's sunscreen, [3] a temporary measure to reflect the Sun's energy and mitigate warming while we address the larger challenge of removing over a trillion tons of greenhouse gases emitted since the 1850s and transitioning away from fossil fuels. Climate change demands immediate action, and SAI offers us the critical time needed to live in a world with fewer catastrophic climate events. [1] https://docs.google.com/document/d/1vGEDuEn6uikvGZpR9oZplPPh... [2] For reference, estimates suggest that global SO2 emissions were around 131 million tons in 1970 and continued to rise, peaking at approximately 150 million tons by the late 1980s in the troposphere. [3] https://twitter.com/MakeSunsets/status/1741336520366444822 reply sebmellen 15 hours agorootparentIt is absolutely unbelievable that we now just dump SO2 in the ocean... reply sebmellen 16 hours agorootparentprevYes, SO2 falls out of the atmosphere after about one year. We already have proof of this in the form of volcanic eruptions: https://en.wikipedia.org/wiki/Stratospheric_aerosol_injectio.... SO2 is a short-term, fast-acting, and reversible cooling option. That is why we need to massively adopt it ASAP. reply uoaei 16 hours agorootparentConveniently in such conversations people never bring up where these compounds go when they \"fall out\". In fact they precipitate or are deposited on surfaces in the form of sulfuric acid. This harms plants and causes all manner of respiratory issues. There's a reason the EPA has fought so hard to reduce SO2 emissions. I get the impulse for unilateral techno-optimist solutions but continually releasing sulfur dioxide into the air is not a sustainable option in any respect. Sustainability and \"more, faster\" ideologies are not compatible. You will be imposing health risks on plants and animals that many simply will not be able to mitigate for themselves. It is always apparent very quickly in conversations like this that the people proposing such \"solutions\" spend most of their time indoors in filtered air. reply elil17 16 hours agorootparentprevI think doing a small amount of aerosol injection right now is an appropriate way to confirm that we understand how they work/what the effects will be. I hope that the work this company is doing is being adequately studied. reply collyw 14 hours agoparentprevNo we don't. It's nice it being a little warmer. Anything else is unjustified fearmongering.Historically good times coincided with warmer temperatures. reply greenie_beans 16 hours agoprevman, the climate apathy on this website discourages me. i thought HN users were smart? yall are blinded by your desire to be a contrarian and skeptic. reply p_j_w 16 hours agoparentHN users are just like most people: smart in their area of expertise and wildly optimistic about how smart they are outside of it. reply linuxftw 16 hours agorootparentMaybe. Or we know junk science and fake data when we see it. I guess the 'replication crisis' doesn't apply to the holy climate science. reply lostmsu 12 hours agorootparentWhat do you mean by \"replication crisis\" in relation to this data? reply linuxftw 6 hours agorootparent> Sponges from the Caribbean retain a record of ocean temperatures stretching back hundreds of years. That's not data. That's method. Probably cannot be replicated. reply copperx 16 hours agoparentprevI still remember the big group of climate change deniers on Slashdot in the early 00s. The consensus was that it was not real. reply lp4vn 17 hours agoprevI'm not optimistic honestly. We have politicians throughout the world whose political platform revolves around denying that the global warming(climate change) even exists. With this level of irrationality it's hard to see the world collaborating towards a solution for something that in the best scenario is going to change the world as we know it and in the worst scenario represents an existential threat. We thought that with the internet and the acessibility of scientific content we would have a global society of conscious people with a high degree of scientific literacy and instead of that what we got is the ressurgence of the flat earth theory, fakenews about vaccination and a growing general hostility to science. I think at some point in the future we will get \"used\" to progressively catastrophic climatic events, and the people who created this state of things will either be dead or will just adapt their discourse as \"it's sad but it was a necessary step to avoid a global green dictatorship\". reply JohnMakin 16 hours agoparentAgree - it's also extremely concerning that otherwise intelligent and educated people I know that have never been climate deniers have shifted into this absurd climate \"optimism\" which borders on delusional that's approximately like \"well, eventually things will get so bad that capitalism will save us and we'll plant a bunch of trees or capture carbon or pump coolants into the atmosphere\" and is generally unbothered by it otherwise. It's astounding to me, no matter how much you try to reason and give examples of why these approaches can't/haven't been working, it's like trying to get through a brick wall. I understand why - to have any kids right now would be terrifying. Hell, to be under 50 right now is terrifying. But just because the truth is scary does not mean we should retreat to delusion. reply orangecat 16 hours agorootparentIt's astounding to me, no matter how much you try to reason and give examples of why these approaches can't/haven't been working, it's like trying to get through a brick wall. Do you think trying to force everyone to radically alter their lifestyles is going to work any better? Once again I find myself confused when so-called environmentalists seamlessly shift between \"renewables will soon be so cheap and abundant that there's no need for nuclear power\" to \"we're all doomed unless we strictly ration energy and abolish capitalism\". reply JohnMakin 16 hours agorootparent> Do you think trying to force everyone to radically alter their lifestyles is going to work any better? I don't know where you think I said that. However, climate change is going to force radically altered lifestyles, and if you disagree with this, you simply aren't paying any attention. > Once again I find myself confused when so-called environmentalists seamlessly shift between \"renewables will soon be so cheap and abundant that there's no need for nuclear power\" to \"we're all doomed unless we strictly ration energy and abolish capitalism\". It seems like you're creating an argument that I did not present out of thin air. Have fun arguing with your strawman. reply andrewmutz 17 hours agoprevDoes this matter? If these results show that the ocean was cooler earlier than we thought, why is that important to our climate change goals? Aren't the absolute temperature level goals more important than the relative temperature goals? And the absolute temperature goals arent affected by this finding. I am not an ecologist or earth scientist, so please let me know if I am missing something. reply TSiege 16 hours agoparentI'm not sure what you mean by absolute temperature level vs relative temperature. The reason that these finding are important is because 1. The climate goals set in the Paris Agreement are looking realistically more impossible by the day. If this is finding is correct 1.5C is literally impossible and 2C looks implausible given current trajectories 2. This means we've been underestimating the amount of change that has already occurred which means we might be closer to climate tipping points 3. We've been finding that Climate Sensitivity, that is how likely a tipping point like glacier collapses or losing the Amazon Rainforest, is MORE sensitive than we thought. This means lower levels of warming will have bigger impacts. So coupled with already being passed. For example 1.5C is considered the cut off point for which we'll see a global extinction of coral reef ecosystems. Meaning it's likely to late to save 25% of marine ecosystems reply ajuc 17 hours agoparentprevHigh base effect. If you measure growth from erronously high starting point and then base your estimates on that mistakenly low growth rate - you'll significantly underestimate the future values. reply RGamma 15 hours agoparentprevNo it doesn't, just Don't Look Up. reply nullorempty 16 hours agoprevHow much do wars cost and what's their contribution to global warming? That could totally be fixed in a short order had there been a genuine concern of the politicians. reply Synaesthesia 16 hours agoparentA lot. They're extremely environmentally destructive of course, and the defense force operates 100% on fossil fuels. reply timeon 16 hours agoparentprevAre you saying that world peace could be fixed in short order? reply nullorempty 16 hours agorootparentYes, that's what I am saying. Are you fighting with your neighbour right this moment and can't stop the fighting and talk? reply wessorh 16 hours agoprevexplain flat soft drinks... Your beer, champaign, and all carbonated drinks contain CO2 that is dissolved in solution and as the temperature increases as it sits in your glass the CO2 comes out of solution. This isn't an IQ test: now think of the oceans, as temperature changes the solubility of CO2 changes inversely proportional to temperature just like your soda pop becomes flat when it warms up. Additionally this is why soft drinks are served cold. reply mr_toad 8 hours agoparent> Your beer, champaign, and all carbonated drinks contain CO2 that is dissolved in solution and as the temperature increases as it sits in your glass the CO2 comes out of solution. That’s mostly due to pressure, carbonated drinks are stored at 30-50 psi. Even a warm carbonated drink will produce bubbles. reply 1970-01-01 8 hours agoparentprevSoft drinks are served cold because people enjoy drinking cold drinks. reply RGamma 15 hours agoparentprevFlat earth theory confirmed? reply GlibMonkeyDeath 14 hours agoprevI simply don't understand this result - it takes ~4,000x as much energy to raise 1kg of water 1 degree compared to air. And how could this trend have started in 1850 (a clear trend from 1850-1900 is observed in the data in Figure 5b)? The sponges were collected at 30-100 m depth too, so this isn't just a surface effect. reply nvm0n2 7 hours agoparentYou are correct, it couldn't and in fact 1850 has traditionally been referred to in climatology as \"pre industrial\", but this site is really the wrong place to try and actually talk about science. The intellectual curiosity fans here don't like it at all. reply hnthrowaway0328 16 hours agoprevJust curious, is there any implementation of infrastructures that actually harvest greenhouse gases for whatever profit? I assume we would need a LOT of them, but is it possible to build something close to the poles, assuming a lot of gases are leaking from there due to ice melting? reply DFHippie 16 hours agoparentThe problem is that all the CO2 is in the air from extracting usable energy from chemical bonds. Due to thermodynamics we need to expend more than the energy that was extracted to recreate these bonds. And the useful work is just addressing prior harm. You aren't getting widgets or food from this process. Maybe someday we'll be producing graphene sheets or carbon nanotubes as a useful resource, but that's not currently on offer. reply ianburrell 14 hours agorootparentWe don't necessarily need to recreate the bonds to sequester CO2. My understanding is that CO2 can be pumped underground in locations that used to hold natural gas. It does take lots of energy to pull the 400 ppm of CO2 out of the air. It is possible there are better sequestration methods than direct capture from the air. If want to make green fuel from the CO2, that takes the same amount of energy as the fuel contains. Using green fuel in existing cars, trucks, and trains won't work. It only makes sense for things like shipping or airplanes that go long distances. And other fuels like hydrogen, methane, or methanol that are easier to produce might work for those. reply FrustratedMonky 17 hours agoprevI heard interesting tidbit on Sabine's YouTube channel. That climate scientist have actually 'toned' down the doom outlooks. Because it turns people off to much. Like there will be no action if people knew how bad it was. So they take 'averages', that kind of show warming, but 'not that bad'. When really, it is 'really bad'. So the 'right' is saying all the climate scientist are 'doomer's and causing sensational hype to promote their theories to sell books. When really, they are toning it down. To be as reasonable as possible. To be 'as sure' as possible. reply brabel 16 hours agoparent> Because it turns people off to much. No, that's not what she said at all! Her main point was that the \"hot models\" were not being considered as very reliable because they showed poor power predicting past climate when compared to other models, but when applied to predicting the weather within shorter periods (e.g. a few days) they were actually better, so that gives weight to the hypothesis that the \"hot models\" are actually more accurate than previously thought, so they may need to adjust the currently accepted rates of warming to be higher... scientists didn't downplay the \"hot models\" because they were trying to make people more comfortable - that would be doing terrible science! Science doesn't care about feelings. reply FrustratedMonky 14 hours agorootparentYou are correct. I was referring to start of the video when referring to the reporting bodies that aggregate the model data. Of course science doesn't have 'feelings' but the people that report it do. And, you can take the same words' and some people call it 'hype' and others call it 'downplayed'. Maybe I shouldn't have said 'scientist'. Scientist have 100's of studies, and create 100's of models. The results are a range. Then various agencies release 'reports' that are summaries of the models. But, like in a lot of fields, you can toss outliers like the top 5 extremes hot and cold. And of course, it's a report, so the language can be as 'toned down', or 'hyped' as you want to make it, or judge it. I'm sure if it was over-hyped, many would complain that way too. What this video was then explaining, was that some of the 'hot' models might actually have been more accurate. So the entire report is even more skewed 'cold', the emergency is actually greater. You are correct. The Video was all about the 'hot' models and how they might actually be the most correct. Thus the reports are skewed in wrong direction overall. reply graemep 16 hours agoparentprev> That climate scientist have actually 'toned' down the doom outlooks. Because it turns people off to much. Like there will be no action if people knew how bad it was. That sounds wrong. They should be giving people the most accurate estimates and the error range, not toning it down (or going the other way). > So the 'right' is saying all the climate scientist are 'doomer's and causing sensational hype to promote their theories to sell books. So now it is a partisan political issue? No chance of sensible discussion at all then. reply FrustratedMonky 14 hours agorootparentMaybe I shouldn't have said 'scientist'. Scientist have 100's of studies, and create 100's of models. The results are a range. Then various agencies release 'reports' that are summaries of the models. But, like in a lot of fields, you can toss outliers like the top 5 extremes hot and cold. And of course, it's a report, so the language can be as 'toned down', or 'hyped' as you want to make it, or judge it. I'm sure if it was over-hyped, many would complain that way too. What this video was then explaining, was that some of the 'hot' models might actually have been more accurate. So the entire report is even more skewed 'cold', the emergency is actually greater. reply mr_toad 8 hours agoparentprevIf you look at successive IPCC reports you can see that many of the worst case scenarios have been dropped or relegated to footnotes and appendices. reply astrodust 16 hours agoparentprevWhen they tone it down, they look like idiots because their most pessimistic predictions from one era become the most optimistic ones from the next. But of course, politics. reply dls2016 16 hours agoparentprevI've been saying this for years: climate science isn't science (in the falsifiable sense), climate is nonlinear and humans don't like to rock the boat. This is especially true with a big, cross-disciplinary project like the IPCC report. (Yes, I understand parts of climate science are falsifiable... I'm at least semi-educated as a former meteorologist and former PDE guy. But the conclusions of the IPCC report are not testable.) reply FrustratedMonky 14 hours agorootparentKind of like how biology isn't a science? Because observations doesn't count, since they are not testable? This is just playing with words. That 'science' must be 'testable' or it is not science, and all other fields that either have observations, or theory, is not 'science'. Theoretical Physics? Not science? What word should we use? reply alexb_ 17 hours agoprevThere's been some talk about ship emissions changing and created more global warming because some of the chemicals they pumped actually had a cooling effect. I can't remember the specifics, but was there ever a reason why we couldn't just pump out cooling agents into the atmosphere? reply RhodesianHunter 17 hours agoparentHad to look this up, very interesting: https://www.thedailybeast.com/weve-been-accidentally-geoengi... reply sci_prog 15 hours agoparentprevHere is a decent explanation as to why not: https://youtu.be/CHJKKsOHtAk?si=FYd1FawD-_GN_Cex reply dls2016 16 hours agoparentprevThere was measurable temperature change in the days after 9/11. reply thriftwy 17 hours agoparentprevSince it is actually a nasty, acid rain causing, Sulfur Oxide. reply wk_end 16 hours agorootparentGiven the choice - and I'm sincerely asking this, because I don't know enough about the subject - what'd be worse here, the disease or the cure? Could we live with acid rain if it staved off cataclysmic global warming? reply yojo 15 hours agorootparentIf the goal is to conduct intentional geoengineering to reduce global warming, there are better things to inject into the atmosphere than sulfur dioxide. Basically just reflective dust. reply sophacles 16 hours agorootparentprevAcid rain kills trees and other plant-life long term. This is bad for global warming... it's not an \"either or\". reply collyw 15 hours agoprevHow does that work if it's the atmosphere that is heating from man made global warming? reply throwaway5752 16 hours agoprevJust because a lot of people are too delicate for the conversation, we are experiencing a human apocalypse. It's really the end for us, but the oil execs have to pay down the yacht bill and we can't have people angry at the diner counter in Peoria over gas prices. This happens all the time in the natural world, it would be funny if it wasn't tragic. We developed tools that let us transition from a S to J population type, without time for cultural adaptation to that new reality. People here don't know enough about the topic, and are swayed by \"LOL look at Malthus\" type gotcha discourse due to the normalcy cognitive bias. The ideas here about 'democratic support' are absolutely laughable. Who cares what various locusts think or feel when they have swarmed and eaten all the available plant matter? They just starve and die because they overshot their environment's carrying capacity. Mars is entirely unviable and thinking otherwise is deluding yourself with science fiction. This will be the defining realization of the next 1-10 years. reply dadjoker 17 hours agoprevnext [2 more] [flagged] ceejayoz 16 hours agoparentWe can go quite a bit further back than 80 years for temperature records. https://www.bas.ac.uk/data/our-data/publication/ice-cores-an... > By measuring the ratios of different water isotopes in polar ice cores, we can determine how temperature in Antarctica and Greenland has changed in the past. The oldest continuous ice core we have was drilled by the European Project for Ice Coring in Antarctica (EPICA) from Dome C on the Antarctic plateau (Fig. 3). It extends back 800,000 years and shows a succession of long, cold ‘glacial’ periods, interspersed roughly every 100,000 years by warm ‘interglacial’ periods (of which the last 11,000 years is the most recent). reply TravisCooper 17 hours agoprevnext [4 more] [flagged] boxed 17 hours agoparentIt's ~420 PPM now. https://en.wikipedia.org/wiki/Carbon_dioxide_in_Earth%27s_at... reply stephen_g 17 hours agoparentprevDo you mean to imply something in such a small concentration can’t have any adverse effect - because in that case, would you breathe the same concentration of hydrogen cyanide in air? (Just a hint - don’t do that) reply TravisCooper 16 hours agoparentprevCO2 is 0.04% of the earth's atmosphere, amazing! 79% Nitrogen 21% Oxygen 0.9% Argon 0.1% Other (includes CO2) reply 84 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Sponges from the Caribbean have provided historical evidence that shows ocean temperatures started rising from fossil fuel burning in 1860, 80 years earlier than previously believed.",
      "Current temperatures are already 1.7°C warmer than preindustrial levels, surpassing the goals set by the Paris Agreement.",
      "The study emphasizes the importance of using paleoclimate data to supplement instrumental records and calls for reassessing the preindustrial reference period used by the IPCC."
    ],
    "commentSummary": [
      "This summary provides an overview of various topics related to climate change, such as ocean warming, lack of democratic support, and industry opposition.",
      "It highlights the need for behavior and infrastructure changes, as well as addressing the unequal impacts and costs of climate change.",
      "The role of renewable energy, China's emissions, and reducing carbon consumption are also discussed, along with the potential of electric cars as a solution."
    ],
    "points": 261,
    "commentCount": 334,
    "retryCount": 0,
    "time": 1707233349
  },
  {
    "id": 39274631,
    "title": "Mozilla Monitor Plus: Automatic Data Removal and Privacy Protection",
    "originLink": "https://blog.mozilla.org/en/mozilla/introducing-mozilla-monitor-plus-a-new-tool-to-automatically-remove-your-personal-information-from-data-broker-sites/",
    "originBody": "Mozilla Introducing Mozilla Monitor Plus, a new tool to automatically remove your personal information from data broker sites Jenifer Boscacci Today, Mozilla Monitor (previously called Firefox Monitor), a free service that notifies you when your email has been part of a breach, announced its new paid subscription service offering: automatic data removal and continuous monitoring of your exposed personal information. Introducing Mozilla Monitor Plus There’s a growing interest among 42% of young adults – aged 18-24 – who want to learn more about the types of information that companies have about them, according to a consumer privacy survey. Yet, taking the steps to request changes or delete personal data can be a bit overwhelming. At Mozilla, we’re always looking for ways to protect people’s privacy and give them greater control when they go online. Enter Monitor Plus. “When we launched Monitor, our goal was to help people discover where their personal info may have been exposed. Now, with Monitor Plus, we’ll help people take back their exposed data from data broker sites that are trying to sell it,” said Tony Amaral-Cinotto, Product Manager of Mozilla Monitor at Mozilla. “Our long-standing commitment to put people’s needs first and our easy step-by-step process makes Monitor Plus unique. Additionally, we combine breach alerts and data broker removal to offer an all-in-one protection tool and make it easier for people to feel and be safe online.” First step: Find out where your personal information has been exposed More than 10 million people have signed up with Mozilla Monitor so they can be notified when their personal data has been involved in a data breach. Today, we are rolling out a new feature with a free one-time scan, where people can take the next step to see where their personal information has been exposed on sites selling it for profit. This could include information like your name, current and previous home addresses, and phone numbers. It could also go another layer deeper with information like family member names, criminal history, your kids’ school district, and even your hobbies. To get your complimentary scan, you will need to provide your first and last name, the current city and state that you live in, your date of birth, and your email address. This information will be encrypted and follows Mozilla’s privacy policy, which always puts people first. This is the least amount of information we need to get the most accurate search results for you. From there, you can see where your personal info is exposed, either through a data breach or through broker sites. We also include high risk data breaches – exposures that may include social security numbers, credit card information, your bank account and pin numbers – that you’ve been exposed to and show how you can fix and resolve it. Take the step to see where your personal info has been exposed Second step: Take back your personal information with Monitor Plus If you’re the type who wants to set it and forget it, because you know the work is happening behind the scenes, then we can automatically and continuously request to remove your personal information with an annual paid subscription of $8.99 per month ($107.88 a year). On your behalf, Mozilla Monitor will start with data removal requests, then scan every month to make sure your personal information stays off data broker sites. Monitor Plus will let you know once your personal information has been removed from more than 190+ data broker sites, twice the number of other competitors. See the actual sites where your personal info has been exposed Mark as fixed in the dashboard At launch, the Monitor Plus free scan and paid subscription service will be offered to people based in the United States. Privacy starts with a Mozilla Account Mozilla has built a reputation of creating and delivering products – Firefox and Mozilla VPN – that put people’s privacy needs first so you can count on Mozilla Monitor as an ally in reclaiming your privacy. In order to get a free scan and sign up for the paid automated data removal, you’ll need to get a Mozilla Account (previously known as a Firefox Account). With a Mozilla Account, you’ll get security benefits such as two-factor authentication powered by Mozilla, as well as backed by Mozilla’s terms of service and privacy policy. To learn about the benefits of having a Mozilla Account, click here. Find out where your private info is exposed – and take it back Try a free scan today with Mozilla Monitor!",
    "commentLink": "https://news.ycombinator.com/item?id=39274631",
    "commentBody": "Mozilla Monitor Plus: automatically remove your personal info from data brokers (blog.mozilla.org)256 points by mikece 19 hours agohidepastfavorite186 comments dataflow 18 hours agoNot sure if dumb question: If they use the data you provide (such as your address) to search other data brokers, doesn't that potentially give the data broker MORE information than they already had on you? Do the companies in this space prevent this somehow? Edit: Lest people think this is somehow impossible otherwise - all it should take would be to search for just your name + location, get the query results, then filter on the client side. Which is exactly what a human would do for the brokers that have a \"remove this entry\" option when you see (presumably) yourself in the search results. However, this not only requires the data brokers to support such an API, but also requires the deletion services to actually put in the effort to do it this way for every broker they can, which seems nontrivial. Hence my question of whether these services make such an attempt at all. reply mattstir 17 hours agoparentNot a dumb question at all. Yeah, in the process of finding you within a data brokers system and sending a removal request, they need to send that broker your personal data... it's a bit awkward. Optery, another PII removal service has a whole section about this in their privacy policy (section 7 of https://www.optery.com/privacy-policy/): > Optery, Inc. must send your PII to the data brokers and information aggregators included in the Removal Lists... We cannot control, guarantee or warranty how these third-parties will treat your PII or what they will do with it. reply beyondd 17 hours agorootparentOptery also has a Help Desk article on this catch-22 where in order to opt out of data broker sites, you must first tell them who you are, otherwise, how else would they know who to opt out: https://help.optery.com/en/article/what-information-does-opt... reply hedora 11 hours agorootparentThey could use a bloom filter with some sort of a cryptographic hash. On a hit, the data broker could challenge them to compute a salted hash of the \"matched\" data. If the salted hash matched, the data broker would remove the data. I think the same algorithms that are used for password storage would work for this without modification (except the data broker would pick different salts during each session, and you'd send the hash over the network). reply politician 11 hours agorootparentNo company wants to implement this. I've been involved in efforts to use this approach with hospitals -- a perfect PII-preserving situation -- that went nowhere. We got it working with a startup once where we published the bloom filter to reduce the traffic load for the counterparty. Do you know what they did? They reverse engineered the filter by blasting it with every key and cached the result. reply Jommi 11 hours agorootparentprevits called a ZKP reply feedsmgmt 17 hours agorootparentprevAnd you need to enter all of the information that you're trying to protect into one central location that is probably heavily targeted. These types of services never made sense to me. reply m3047 16 hours agoparentprevNothing is impossible in tech. (Rhetorical hyperbole!) But seriously let me give you an analogous example, with its pros and cons. DNS now has something widely deployed called \"query name minimization\". For no particular reason other than it made server's lives easy (which it does, as we will explain) the recursion process historically sent the actual qname (what was asked for) to each nameserver contacted. Much was made of this in recent years, that this leaked potentially important information to servers which demonstrably couldn't have the actual answer for the qname (even if they could provide a useful referral). Two flavors of qname minimization exist in the field. One flavor asks qtype A questions of the form \"_.example.com\" until it triangulates on the server with the answer; the other asks qtype NS questions (regardless of the actual qtype). (In case you've noticed a change in the mix of your DNS traffic.) In a nutshell, qname minimization asks questions which enable it to triangulate on the server which can potentially answer the question, before sending the actual question to it. A good rule of thumb is that with a cold cache qname minimization will result in nearly twice as many queries being issued / answered during the resolution process, assuming nothing goes wrong. Both of these approaches are prone to mistakes when servers don't conform to assumptions about how proper DNS should operate. reply lancesells 12 hours agoparentprevCould there be some sort of Robin Hood action to all of this? What if you took all the leaked data about millions of people and used that to opt out them out of all the various services that buy and then sell the data? reply gzer0 12 hours agorootparentThat is a possibility. Another scenario is one in which you sign up to a service like Optery and submit a non-existent individual with fabricated information for PII removal; after about a month or so, this fabricated individual started showing up as a possible person that lived at my address when I was trying to get a quote from Progressive. So, seems like somewhere in the midst of this process, one of the 240 brokers that Optery sends your information to get it removed, someone aggregated it, sold it to Progressive and in the underground realm of data brokers and buying and selling data, someone unfortunately (or fortunately?) is now targeting 'Paige Notfound' and 'Meg A. Byte'. I got the last laugh! :) reply dataflow 12 hours agorootparentThanks so much for sharing this, I was wondering what would happen if I tried this. I guess this basically tells me to be weary of such services. Great info. P.S. Just a heads up that you may have basically revealed your address by sharing those fake names (though I haven't tried to search), unless you also made up those names just now for illustration... reply gzer0 9 hours agorootparentThanks for looking out; those were not the actual names I used! I added those in for comedic effect ;) reply andirk 12 hours agoparentprevWith an American SSN, one could dump 1,000 queries of numbers with only 1 of them being the client's actual SSN so the logs don't reveal as much. Still, though, it's a Catch 22 to find the thing you don't want found by using that thing. reply notyourwork 17 hours agoparentprevIt seems to me like this is a core problem with the scummy nature of this business. I’d like to believe you’re weong but have trouble given the business model. reply crawsome 11 hours agoparentprevIt feels weird, but this is how background checks work, and how the current removal process for data brokers works. I can't think of other ways to verify yourself other than to verify yourself. reply Flimm 18 hours agoprevI wanted to try this, but it seems to be restricted to only people in the USA. It is impossible to enter a location outside the USA in the sign-up form, and it's impossible to skip that form. Please, Mozilla, make it much clearer which countries are supported to avoid causing this frustration and to give people a reason to come back once other countries are supported. reply Vinnl 18 hours agoparentSorry about that. The form should only be shown for people in the USA, but detecting the country you're in can't be done perfectly. Which is a good reminder - we'll look into making the US-only part clearer. (I'm an engineer on Monitor.) reply cwales95 17 hours agorootparentI'm also not clear why this is US only. There's definitely a market in other areas of the world. I'd be interested to know why it should be US only. reply JohnTHaller 16 hours agorootparentThey're likely starting with the US because either their partner(s) for this is US only and/or it's easier to start with a single large market. The US is about a 50% larger market in terms of GDP than all EU countries combined. reply Vinnl 10 hours agorootparentThat is correct, and it's way more of a problem in the US (but also e.g. in Brazil). Also: are you the JohnTHaller of PortableApps fame? If so: thanks for making my high school computer usage bearable, way back when! reply JohnTHaller 7 hours agorootparentI am indeed! Still supporting and growing it. You're welcome! reply isodev 17 hours agorootparentprevOr perhaps, just thinking out loud, you could extend support for the service to other countries. The EU would love you for this at the very least. reply mynameisvlad 12 hours agorootparentI'm sure if it was as easy as snapping their fingers, they'd have done it. Time is a finite resource, and a lot of these data brokers seem to be very geographically-specific and have their own ways of requesting deletion. reply Vinnl 10 hours agorootparentprevI am from the EU, so tell me about it :) But yes, the sibling comment is right: this isn't something you can expand to other countries with the push of a button. Personally, for this specific functionality, I don't think the EU wouldn't be at the top of the list though: these types data brokers are way more of a problem in other countries. We have laws like GDPR :) reply callalex 15 hours agorootparentprevWhat’s wrong with simply displaying/linking to a list of supported countries? reply Vinnl 10 hours agorootparentThe \"list\" is: - USA It is listed in places, but clearly not explicitly enough in the right places. reply donkeyd 18 hours agoparentprevSame with Optery shared below. I wonder if there are any European/International counterparts to these services. reply westpfelia 18 hours agoparentprevCould just be in the short term they are limiting it to the USA and going global soon. reply pluc 18 hours agoparentprevhaveibeenpwned.com has been offering the same service for free for years. reply Liquid_Fire 17 hours agorootparentIt doesn't look the same to me. haveibeenpwned will notify you if your email address was in a breach. The Mozilla offering seems to include the same, but also cover other pieces of personal data, and the ability to request removal from data brokers. reply Vinnl 10 hours agorootparentThat is correct, with the additional note that Monitor has also existed for a while providing that functionality for free (in collaboration with HIBP), and will continue to do so for free, and worldwide. The new thing is scanning for your info at data brokers (for free, but USA-only), and automatically removing them and continuously checking that they stay removed (paid, US only). reply pluc 17 hours agorootparentprevThat ability is not the product they're offering though, that's something you can already once you identify where your data is. And obviously that's if they have a way for you to do request removal and if they feel like doing it at all which are the same constraints for Mozilla. I think this is all purely for convenience of having it all done for you (which is okay) reply Vinnl 10 hours agorootparentThat is correct, and we do help you identify where your data is for free. Many people unfortunately have their data exposed in lots of places,in which case manual removal is a PITA, and that's where the Plus plan comes in. reply Liquid_Fire 16 hours agorootparentprevWell, even if you consider the removal aspect of it useless since you could do it yourself, there is still value in knowing where your data is. Have I Been Pwned will tell you about breaches, but not about brokers reselling your data, and they only monitor email addresses. And yes, you could probably go and ask the brokers directly, but that is certainly a lot of time and effort, so paying for it might make sense, assuming you trust the service provider. reply beyondd 17 hours agorootparentprevhaveibeenpwned.com provides data breach monitoring, but does not remove personal info from data broker sites as Optery and Mozilla Monitor do. reply nickthegreek 18 hours agoprevMozilla Monitor Plus - $14/mo, or $108/yr. Too pricey for most. >Every month, we use the information you provided about yourself (name, location and birthdate) to search across ⁨190⁩ data broker sites that sell people’s private information. If we find your data on any of these sites, we initiate the request for removal. Data removal can take anywhere from a day to a month. This feature is available for ⁨Monitor Plus⁩ users only. Anyone know if there are any local/open source tools to do this? reply WirelessGigabit 17 hours agoparentI have used Permission Slip by CR with limited success. I use @., and you cannot enter a wildcard in Permission Slip. reply devrand 16 hours agorootparentI use this pattern but I'm starting to move away from it. Some things just don't work (ex. linking accounts between companies) and it also throws customer service agents into a panic when they see their own company name in the e-mail address. I'm also not sure it gets me that much. I do get to see how was compromised or sold my data, but most of that just goes to spam anyway. I also usually find out about the compromises from other sources anyway. reply MyNameIs_Hacker 13 hours agorootparentSure some of the CSA's panic a bit, but I've never had one not go along especially after explaining my purpose. I've not seen too many compromises, but some of them were not public. Especially with small businesses like a car dealership, they may never know themselves. reply rdgddffd 11 hours agorootparentprevTry just rot13 or hashing the website name. reply nickthegreek 13 hours agoparentprevClosest thing I can find to roll your own. https://github.com/yaelwrites/Big-Ass-Data-Broker-Opt-Out-Li... reply ary 18 hours agoprevI’m a happy, long term Optery user (not affiliated) and they take care of 100% of this for you. https://www.optery.com The Mozilla offering looks somewhat comparable, but I do wonder if they’re going to beat a company which has the sole focus of solving this problem. reply haswell 18 hours agoparentAlso an unaffiliated, long term, and happy user of Optery. If nothing else, I’m glad there are more offerings showing up on this space because of the competition this will hopefully generate. Consumer Reports also has a semi-related offering called “Permission Slip” that is focused on opting out of data sharing with individual companies, e.g. Netflix, Home Depot, etc. reply geor9e 15 hours agoparentprevHaha wow it's actually asking me to sign over LIMITED POWER OF ATTORNEY. It's optional but says it's recommended. That's a nope from me. reply beyondd 13 hours agorootparentMany data brokers will not permit third party services to remove the data without a signed limited power of attorney. Note that the power of attorney is limited to interactions for submitting removal requests and opt outs. reply GuB-42 14 hours agorootparentprevIsn't it to be expected? I guess that they have to make demands on your behalf to have your data removed. I guess that's optional because they can still work without it is some cases, and ask you on a case-by-case basis for others, but that's extra work for you and for them, so they may not do it, at least not on the lower tier pricing. reply khaki54 14 hours agorootparentprevWhy? You limit the power of attorney to the ability to remove your data from data brokers. reply darknavi 13 hours agorootparentprevBlame data brokers for making such asinine restrictions. You can also just use the free version to collect a list of brokers your self and manually contact all of them to find out how much of a pain in the ass it is. reply anjel 17 hours agoparentprevI cleared my name from the net using another service that charged by the month. I paid them for three months, when their work clearing my data from about 100+brokers was completed, then cancelled. 2 years later, my name and personal data still remain no longer to be found like it once was before the scrubbing. reply Vinnl 9 hours agorootparentThat's great to hear, often they do show up again later, which is why it's a longer-term subscription service. OneRep is the provider for the removal functionality of Monitor, incidentally. reply rayshan 16 hours agorootparentprevWhat is the service you used? reply anjel 10 hours agorootparentOneRep. I'm a once and former customer, otherwise unaffiliated. reply Workaccount2 18 hours agoparentprevI can't help but be a bit miffed that despite ostensibly being a privacy service, optery is still running a bunch of third party scripts on their site, including google... reply HaloZero 17 hours agoparentprevI'm curious, what's the point of paying for Optery per year? Isn't removing your data be a one time request. Except for supporting new brokers that might appear. reply beyondd 16 hours agorootparentYour point is spot on. Data removal services have an aspect where a ton of value is obtained in the first 1 - 4 months as the majority of profiles are wiped away, and then after that you're sort of in maintenance mode where the service catches profiles as they pop back up, or when new data brokers are added to the system for coverage. Optery generally has 2 types of customers: - The first type are those that care a lot about their privacy and the cost of an ongoing subscription is insignificant to them, so they keep the service running on an ongoing basis for the ongoing automated scans and removals and for getting new data brokers they get coverage for immediately as they are added into the system. - The second type of customer is more price conscious and is basically looking back and forth between their credit card statement and their Optery dashboard each month and then they either pause or cancel the subscription when they feel they're reached a good stopping point. Optery's pause subscription feature is very popular for this type of customer and you can use it to automatically re-start the service in 3, 6, 9 months, etc. - Another thing to point out is many other services only offer Yearly subscriptions, Optery offers Yearly or Monthly. If you're price conscious, the Monthly is nice because you can turn it on and off, or pause it as you wish. More detail on the topic of keeping Optery running on an ongoing basis is on the Optery Help Desk here: https://help.optery.com/en/article/why-should-i-keep-my-opte... reply mamidon 16 hours agorootparentHave you considered adding a 3-months-every-year option? I wonder if automating the second type of customer would provide you a lift in revenue. reply beyondd 11 hours agorootparentThis is a great suggestion and we would like to add this. Not because it would provide any revenue lift though, but because it is what some Optery customers have been asking for, e.g. can I have a lower cost subscription that runs every other month, or every three months, etc. Technically, you can do this today by cancelling and re-starting a Monthly subscription at your desired cadence, or pausing and re-starting your subscription periodically, but that requires manual effort. A configurable cadence is definitely on our backlog though. reply megasquid 18 hours agoparentprevAlso a satisfied Optery user. Been using their service for the past year, from what I can tell, they seem to have the most robust solution in the space. reply PascLeRasc 12 hours agoparentprevDiscover bank also offers something like this for free, but I can't tell if it's as capable as other services. https://www.discover.com/security/online-privacy-protection/ reply tholtken 18 hours agoparentprevEspecially with a backend service provider (onerep.com) that is questionable at best. reply eltondegeneres 17 hours agorootparentWhat are the issues with Mozilla's use of onerep? reply beyondd 17 hours agorootparentOne of the issues are OneRep's affiliate partnerships with the very data brokers you're paying them to remove you from: https://imgur.com/a/juSC66b reply sp0rk 16 hours agorootparentI think \"partnership\" seems like too strong a word for what appears to be the simple use of an affiliate program. Why would OneRep know or care about an individual affiliate and the content of their site, as long as their behavior with regards to the affiliate program is above-board? reply beyondd 16 hours agorootparentAffiliate programs have application processes intended to filter out bad actors and mis-alignment with a brand. To use an extreme example, a web site promoting terrorism would typically be rejected. Approving data brokers as affiliate partners for a data broker removal service is viewed by many as questionable. To use an another extreme example, how would you feel about an anti-virus software company that approved as affiliate partners creators and distributors of computer virus programs. reply anjel 17 hours agorootparentprevOneRep is the service I used, briefly. I have no Affilliation with them except as past customer. They delivered as promised and the effect has been persistent 2+ years since the time I discontinued the subscription. reply fdgadfagfgd 17 hours agorootparentprevAny other issues besides that possible conflict of interest? Also, you're the founder of a competing service, right? reply wolverine876 17 hours agorootparentThey are. There's a flagged dead comment where they say so (I don't know if this link will work for a flagged dead comment): https://news.ycombinator.com/item?id=39276106 beyonddd should really identify themselves as the founder of a competitor. Nothing wrong with posting, but pseudo-anonymously disparaging the competition seems very inappropriate. reply beyondd 16 hours agorootparentYes - I flagged myself as an Optery founder on my first comment, but as you mentioned the comment was subsequently flagged and hidden from view. It is also made clear here: https://news.ycombinator.com/user?id=beyondd reply wolverine876 15 hours agorootparentFrom my perspective, I'd put it in any comment mentioning Optery or criticizing competitors. People often read one comment; they don't read all your comments and your profile. It also adds some credibility: You actually know what you're talking about in regard to this kind of service. reply beyondd 16 hours agorootparentprevYes - I flagged myself as an Optery founder on my first comment, but the comment was subsequently flagged and hidden from view (https://news.ycombinator.com/item?id=39276106). It is also made clear here: https://news.ycombinator.com/user?id=beyondd reply tholtken 17 hours agorootparentprevnot affiliated with Optery but agree conflict of interest, also misleading by onerep and at best deceptive. take that potential lack of trust together with the several reports online that onerep's us operation is a sham and they are really operating out of eastern europe and sending user data there...seems shady. begs the question: what does a privacy-respecting org like Mozilla see in onerep and how is it better than what other companies offer? reply pininja 16 hours agoparentprevAnyone have experience comparing this to Incogni? I’ve been an unaffiliated user for over a year now. While many brokers have replied, many never seem to. reply beyondd 15 hours agorootparentOptery founder here. We did a deep dive comparison between Incogni and Optery (https://www.optery.com/incogni-review/). The biggest takeaway is Incogni, at this time, does not cover many of the most popular people search sites like Whitepages, TruePeopleSearch, Spokeo, RocketReach, ThatsThem, BeenVerified, TruthFinder, InstantCheckmate, and many others. Most Incogni reviews you'll find online are written by their affiliate partners. reply Urgo 7 hours agorootparentbeyondd, I've been reading through this thread and your comments about Optery and you got me to sign up for an account on your site vs Mozilla's service so good job. I was even going to pay for your Ultimate plan for a year. But.... you lost me when I got to the profile page. I have a handful of email addresses and a couple of phone numbers. I would want them all to be scanned for. I had previously been using experian's removal service and they allowed for 10 emails and 5 phone numbers. Your documentation says: \"You can only select one email and one phone number for scans at this time. However, Optery's engineering team is actively working on providing more configuration options such as the ability to run scans on demand for multiple email addresses and phone numbers.\" Any comments on when this will be an option? I would want automatic scans on all of my emails and phone numbers. Not very useful for me without this. reply beyondd 6 hours agorootparentThe core of Optery's search functionality is \"person\" centric. Meaning we start with searches by name, city, state, and age to find \"you\" regardless of which underlying email or phone number the data broker has on record for you. Because in many cases data brokers may have no email or phone on file for you at all (only home address), or they may have a really old phone or email you have forgotten about. When data removal service scans focus only on phone numbers and email addresses, a lot can get missed. Many people search sites are not even queryable by phone or email, and are only queryable by name, city, and state. Optery does search for phones and emails, but you are correct in that it currently limits them to just one each from the customer at this time. We plan to release the scan on demand feature you referenced in the next few months. That said, Optery recursively searches through data exposed by data brokers to alleviate the need to input numerous old phones and emails by the customer. In PCMag.com review they said this of Optery's recursive phone number search functionality: \"It uses data found in data broker profiles to recursively expand its reach. For example, in my latest testing, I only gave it my current phone number, but it found records associated with an old number that I used for some 25 years.\" source: https://www.pcmag.com/reviews/optery reply Urgo 6 hours agorootparentThank you for the reply! I suppose that does make sense, though it still doesn't give a warm fuzzy feel separating the functionality. While the average human might only have one email address they use, I'd venture to say people who would want a service such as this would skew more towards having many they use for privacy reasons. I get what you're saying about how emails aren't the primary means of finding people, but it is a way, and something people often do have more then one of. I'd humbly request you reconsider and try to better incorporate support for automated scans on multiple emails/phones into the main product. For what its worth it looks like Mozilla's product supports 5 based on their docs. That said, after submitting this comment I'm going to go ahead and sign up for the one year ultimate anyway in hopes that you will reconsider my request if I'm a paid user. :) reply beyondd 4 hours agorootparentThanks for the follow up! Scans for multiple phones and emails is something we're working on so stay tuned on that, and don't hesitate to contact customer support with any questions along the way! Also, you mentioned using Experian's data removal service previously. Do you mind me asking how many exposed profiles the Optery scan located that Experian missed? reply hangonhn 12 hours agoprevI really wish employers would pay for a service like this because a lot of spear phishing attacks start with data stole or scraped from brokers, LinkedIn, etc. If a company buys a service like this in bulk, it can get significant discounts. Personally I've resorted to hiding my information on LinkedIn and noticed that I've been passed over by attackers while my coworkers get spear phishing attacks all the time. reply doix 18 hours agoprev> Privacy starts with a Mozilla Account I like how the solution to the privacy issue is _yet another account_. I don't know why, but I find it highly amusing. I do get it, you need to share your details with them so they know which details to delete, but I still can't help but laugh. reply westpfelia 18 hours agoparentFor something like this to work you have to trust SOMEONE. And Mozilla is definitely more trustworthy then others in the space. reply mozempthrowaway 17 hours agorootparentEh kind of. One of the recent themes at our all hands was “data collection for user benefit” which I’m sure is what every company says. reply tholtken 17 hours agorootparentWhat does this even mean? How does Mozilla know what benefits me, the user? reply Vinnl 9 hours agorootparentFor example, Firefox can collect quite a bit of data regarding what hardware correlates with what type of crashes on what code paths. It doesn't have to know anything about you to know that fewer crashes benefit you. reply RDaneel0livaw 18 hours agoparentprevI attempted to use this, entered my email, was prompted with a \"create your account\" page, laughed out loud and closed the tab. This is a comical misunderstanding of what the product even IS or DOES. reply pietro72ohboy 12 hours agorootparentHow do they think they’re supposed to do their job if they don’t even have a way to identify you in the first place. What is comical is your blend of ignorance of the technical needs of the product and arrogance to suggest that it should be done in this “magical anonymous way” that nobody seems to grok. reply dvngnt_ 18 hours agorootparentprevcompetitors require an account too? reply riddley 18 hours agoparentprevCapitalism's whole thing is create the sickness and sell the cure, right? reply kmfrk 18 hours agoprevOne of the ironies of these things is that they tend to map to a specific e-mail address, whereas the more paranoid of us who'd want to pay for a service like that tend to have different addresses, either entirely or something like Gmail with +filters. HIBP supports domain searches[^1] at least, but part of the problem is also how we keep trying to reinvent the e-mail system to not fall prey to this, much how Fastmail have Masked Emails, and Apple have Hide My Email. In a sense, it sounds like the advice of the services is less subscribing to them than trying not to have a few e-mails that map to your personal identity. [^1]: https://haveibeenpwned.com/DomainSearch reply Vinnl 18 hours agoparent> In a sense, it sounds like the advice of the services is less subscribing to them than trying not to have a few e-mails that map to your personal identity. Firefox Relay is a great way to do that :) https://relay.firefox.com Integrating that with Monitor is pretty high on at least my personal wish list. reply kmfrk 18 hours agorootparentThe phone masking looks great, too. Like Privacy.com, it's awesome with virtual alternatives for PII, except they don't tend to be available here in Europe, but I'm definitely jealous. reply miki123211 18 hours agorootparentIf you need a privacy.com alternative for the EU, Revolut is a good option. They offer both one-time-use (disposable) cards, as well as normal virtual cards that are valid until revoked. They're not as advanced as privacy.com AFAIK, cards that only work for a single merchant but multiple transactions aren't offered for example, but they're good enough for most purposes. Eu regulations on card networks make such a service much harder to offer, privacy.com makes money on card fees, which you can't really do here. Such a service would either have to be paid or bundled with other services which you can make money on, which is what Revolut does. reply erinnh 17 hours agorootparentprevIt’s an ok way to do it. And I’ve been subscribed (but not using it) for 2 years. But until Firefox Relay supports custom domains, I am of the opinion that it’s not ideal. reply Vinnl 10 hours agorootparentAs someone who also runs their own custom email domains, I agree it would be nice if I could manage those through Relay. That said, I'd still be giving out Relay addresses way more often, since those email addresses can't be linked to each other, or to me. My personal domain is for things that are really important in the long term, but for things like concert tickets, I prefer having the added anonymity. reply ColonelBlimp 11 hours agorootparentprevWith providers like Addy and SimpleLogin it is possible to use your own domain. > https://addy.io/ > https://simplelogin.io/ reply DaSHacka 5 hours agorootparent+1 for Addy, been using it for ~2 years now with my personal domain and its been great. I want with Addy over others because plans are per badwidth used instead of per alias, so one-time email verifications for some signup doesn't count towards a total limit. reply johnkpaul 13 hours agoprevDo any of these offer family plans? I feel like At these price points, I would really like to sign up everyone in my household. The FAQ pages seem to all imply individual and I don't think I'm asking for a \"business\" or \"enterprise\" option. reply CharlesW 13 hours agoparentOnerep (another commenter believes this is Mozilla's U.S. partner) has a $15/mo family (paid annually, 6 people) plan. reply beyondd 11 hours agoparentprevOptery offers a family plan: https://www.optery.com/family/ reply sdn90 5 hours agorootparentDoes this support adding family members on a single account? I have some non technical family members who I'd like to manage it for them and giving them their own account is most likely going to be a major headache. reply beyondd 4 hours agorootparentYes - Optery for Family is very popular - here's how it works: https://help.optery.com/en/article/getting-started-with-opte... reply CharlesW 11 hours agorootparentprevAs feedback for the CEO, that \"family pricing\" landing page really does you a disservice by obfuscating your pricing (unless that was the goal). At a minimum, add a pricing calculator with a slider for \"family members\". For comparison, see Onerep's very clear pricing page here: https://onerep.com/pricing reply beyondd 11 hours agorootparentThat's great feedback! We'll add more pricing detail to the Family page. For comparison, here is the Optery pricing page: https://www.optery.com/pricing/ reply konart 18 hours agoprevI find it kind of amusing: The article mentions (obviously) Mozilla Monitor. When I follow the provided link (leads to https://monitor.mozilla.org) in the default Firefox container and enter my email a new tab (now https://accounts.firefox.com) is created in a Google container (despite the fact that nothing suggests me leaving https://accounts.firefox.com) Automatically remove your personal info from data brokers you say? reply sf_rob 18 hours agoparentI'm willing to bet that this is a inference due to \"Login with Google\" being an option. Probably worth sacrificing a click in their sign-in funnel to prevent it though. reply gnicholas 16 hours agoprev> we can automatically and continuously request to remove your personal information with an annual paid subscription of $8.99 per month ($107.88 a year). This is a lot of money for most people. What would the benefit be of doing this all the time versus just subscribing once a year? How quickly do details reappear in databases? reply beej71 3 hours agoparentI had this same question. What's the point of a removal request if the site can just add your info back in next month? And if they can't, what's the point of a monthly subscription? reply Klonoar 14 hours agoparentprevThis really isn’t a lot of money for anyone in the USA, which is where the product is offered. Hell with the current economic environment I unfortunately spend more than this on my morning coffee. reply gnicholas 13 hours agorootparentOver a hundred dollars a year? That's a lot of money to get people to pay for a product category that most people do not currently purchase. Most people would also wonder why this is a perpetual subscription as opposed to something they can pay for one-off once every year or two. reply spiffytech 15 hours agoparentprevI'm given to understand these data broker services make it as painful and time-consuming to opt out as they can. Supposing you can even find all the places you're listed (Optery supports 305+ sites), it sounds like a substantial time commitment to follow through on all of them. reply 7734128 13 hours agorootparentI'm confused how the internet is just ok with Mozilla engaging with these extortion websites. These sites are not legitimate and now that Mozilla and Google are engaging with them they just play into the protection racket. reply hellcow 14 hours agorootparentprevI signed up, and Mozilla warns it takes 7-14 days for data on most of these sites to be removed. They must need to do a lot of things by hand. This would also explain why you get 1 scan per month. reply KittenInABox 14 hours agorootparentThese sites deliberately are slow in the removal of requests. So there is both manual sending but also needing to re-check if the site actually removed your info because brokers just kind of suck. reply pkaye 16 hours agoprevWhich laws are Mozilla using to get the data brokers to remove personal info in the US. I know there is such a law in California but is there also a federal law? reply beyondd 11 hours agoparentNo federal law in the U.S. yet unfortunately, but more states are passing laws by the day (fortunately): https://iapp.org/resources/article/us-state-privacy-legislat... reply nurtbo 18 hours agoprevWhy would you use Mozilla Monitor Plus when onerep.com offers the same service for a lower cost? (And from other comments, I’d actually the same underlying service) reply diggan 18 hours agoparentBecause I've never heard of onerep.com before while I have a history of using Mozilla products for decades at this point. If the service is exactly the same, it's a no-brainer, even if it costs slightly more. reply bluish29 18 hours agoparentprevThe price on onerep for monthly payment is $14.95 vs Mozilla's $13.99. Both offer discount for yearly payment and they will be almost the same. Of course, this is the price for individual. onerep offer better, cheaper plans for family (6 for $28) but Mozilla doesn't offer that (yet at least). So I'm not sure if it is a lower cost. reply mozempthrowaway 17 hours agoparentprevCan confirm it is just one rep reply katrotz 17 hours agoprevFound the choice of words \"Get a free scan\" on their website button funny. My first involuntary thought was - it is a scam. reply Vinnl 9 hours agoparentIt's super annoying. We also have that when e.g. trying to emphasize what we're doing to protect your privacy, since \"caring about your privacy\" has become distorted to mean its opposite at least in my mind. reply niels_bom 18 hours agoprevPricing should be way more obvious and up front. I had to search the comments here to find pricing. Do I really need to login to get pricing information? reply Vinnl 9 hours agoparentIt should be listed in the pricing table on the front-page, but it's only available in the US, so we only try to show that to people there. reply ChrisArchitect 17 hours agoprevPut some dates on your blog posts Mozilla! reply DamnableNook 11 hours agoprevIronically, their page doesn’t seem to work on Safari. I get a 404 error after signing in, every time. Switching to Chrome on my desktop lets it work. reply pompino 16 hours agoprevFor people who are the target market for such products- Can you explain to me the appeal of such products for you? Have you previously been the victim of any escalation resulting from a data breach? reply causal 19 hours agoprevI like this in theory, I don't have time to chase down every data broker to opt-out on my own. I'm just wondering how I can measure whether it's really effective or not. Anyone have experience with this kind of thing? reply dylan604 18 hours agoparentThis is always my pessimistic view of the world we live in today. Why in the world would they delete that data vs just putting it on mute/ignore/etc? The only \"proof\" you have is if you send a request to see the data they hold on you. If they send you an empty report because the ignore flag was set, you would only see an empty report. You have no evidence that the data was actually deleted. reply myself248 13 hours agorootparentAnd I've never seen a single stalker-corp (\"data broker\") executive serve prison time for failing to delete data that they claimed was deleted. Either that has literally never happened, or there's inadequate auditing/enforcement, and I don't consider the former to be plausible. reply PascLeRasc 13 hours agorootparentprevThat's because this is actually a data validation service for brokers. Most of their data is junk or incomplete, but now they know which pieces belong to actual people who want to pay money for it to be deleted. reply StopTheTechies 16 hours agorootparentprev> Why in the world would they delete that data vs just putting it on mute/ignore/etc? If you're serious it's because having a fig leaf is useful to reduce risk in controversial business practices, especially if the vast majority of people don't take advantage of it. reply wolverine876 16 hours agorootparentprevI also wonder if it stops them from collecting it. Also, what are the legal requirements if a customer asks their data to be removed? Still, I'm not giving up a plausible solution because potentially it's only a partial solution. reply rdgddffd 10 hours agorootparentprevI get your point but also this is what whistleblower laws are for. A lot of times it’s in the company’s best interest to comply… until 50%+ of the population opts out reply beyondd 17 hours agoparentprevOptery customers get Removals Reports every 90 days. PCMag.com wrote this about the Optery Removals Report: \"With the Removals report, you see what was found along with a new screenshot demonstrating that the data was removed, and a link to verify the removal. No other personal data removal service I’ve seen gives you this level of verification.\" reply Vinnl 18 hours agoparentprevThe data brokers that show your info will be listed, so you can spot check them yourself to see if they still show you. Not perfect, but should give you some confidence that if it says your data has been removed, it actually has been removed. (You can scan for brokers before upgrading to Plus for automatic opt-out, so you can also check beforehand that you can see your data.) reply wolverine876 16 hours agorootparentThat doesn't account for them retaining your data and simply toggling the 'Publish' flag from 1 to 0. reply Vinnl 10 hours agorootparentTrue. reply flanbiscuit 18 hours agoprevThere is a service I've heard advertised on twit.tv podcasts called DeleteMe that I've been interested in that does a similar thing and seems to cover way more data brokers: https://joindeleteme.com/sites-we-remove-from/ OpenRep is another one I've seen mentioned. Covers 190+ sites: https://onerep.com/sites-we-remove-from One thing I can't find is a list of sites that Mozilla Monitor covers. Here's a comparison. I only listed the individual plans since Mozilla seems to only offer that. The other 2 offer plans for multiple persons DeleteMe: https://joindeleteme.com/ brokers: 750+ https://joindeleteme.com/sites-we-remove-from/ edit: I just realized looking through that list that they are a bit deceiving. They have qualifiers next to each website: * Included in Standard Plan and above (90 sites) ** Included in Business Gold, Diamond, Platinum and VIP Plans (27 sites) *** Included in Diamond, Platinum, and VIP Plans (1 site) ᵒ Exclusively in Platinum and VIP Plans (13 sites) ~ International requests (12 sites) ^ Custom Requests (665 sites) Seems like the majority need a \"custom request\" which defeats the purpose of signing up for something that is supposed to handle things automatically pricing: https://joindeleteme.com/privacy-protection-plans/ - individual plan: (they also have couples and family plans) - $10.75/month if you sign up for 1yr - $8.71/month if you sign up for 2yr ------------- OpenRep: https://onerep.com/ brokers: 190+ https://onerep.com/sites-we-remove-from pricing: https://onerep.com/pricing 1 person: $8.33/mo, they also offer family (up to 6 ppl) and teams (10+) ------------- Mozilla Monitor: https://monitor.mozilla.org/ brokers: 190 data brokers (could not find a list of data brokers they cover) pricing: https://monitor.mozilla.org/#:S1: - \"Monitor\" - their FREE tier where they scan the data brokers and just inform you which ones have your info and you have to manually go in and remove your information from each one through whatever process each site uses. - \"Monitor Plus\" - Automatic Data Removal - $13.99/month, or $8.99/month if you sign up for a year Both tiers come with \"Data Breach Alerts\" which I guess is similar to haveibeenpwned's notify me. -------------- edit: adding one more: https://www.optery.com/ brokers: 305+ https://www.optery.com/pricing/#data-brokers-we-cover pricing: https://www.optery.com/pricing/ & https://www.optery.com/business-pricing/ will only cover the personal pricing: free - self-service (similar to Mozilla's free tier) 3.99/month - removal from 110+ sites 14.99/month - removal from 200+ sites 24.99/month - removal from 305+ sites reply OnACoffeeBreak 18 hours agoparentIt doesn't look like DeleteMe's individual plan covers 750+ sites. There are only 77 sites with a single asterisk on https://joindeleteme.com/sites-we-remove-from/ reply flanbiscuit 17 hours agorootparentI noticed that as well after I posted, so I've edited it and added that in. reply beyondd 15 hours agoparentprevOptery founder here. We did a deep dive comparison between DeleteMe and Optery (https://www.optery.com/deleteme-review/). The biggest takeaway is you have to scroll to the bottom of the DeleteMe Sites We Remove From page and read the fine print on what is covered by the plan you are purchasing. The \"750+ Data Brokers\" written across the top of the page is misleading. The standard plan covers about ~90 sites. reply flanbiscuit 15 hours agorootparentI noticed that shortly after I posted and have included that info now (edited the comment). Classic dark pattern. That info should be more prominently displayed in their pricing information. So your service will handle (up to) 305+ data brokers automatically? depending on how much you are willing to pay of course reply beyondd 11 hours agorootparentAgreed on the dark pattern, and yes, Optery's Ultimate plan currently covers 300+ data brokers by default and offers unlimited Custom Removals. Optery has a team that's continually testing and adding more sites to the coverage defaults. There are several options, Free, Paid, Family, Business at different prices. For full disclosure, I'm one of the Optery founders, as mentioned previously. reply wolverine876 17 hours agoparentprevThank you for the comparison. Perhaps someone who uses it can add info on Consumer Reports' Permission Slip? reply flanbiscuit 15 hours agoparentprevSorry for the typo of calling \"OneRep\" \"OpenRep\" (I wrote it twice). I can't edit my post anymore but just wanted to clarify that it is OneRep. https://onerep.com/ reply bluish29 18 hours agoprevI wonder if they will bundle it with VPN, Relay, for a good and reasonable price. This would be an attractive bundle to subscribe. reply MiddleEndian 12 hours agoprevHow does Mozilla determine what 190 data brokers are relevant? reply nubinetwork 17 hours agoprevI'm not sure I want to give my information to Mozilla, should they get hacked, it's no different than my information being held by another entity. (I don't use pocket or Firefox sync, etc.) reply Vinnl 9 hours agoparentYour Firefox Sync data is end-to-end encrypted and thus won't be leaked in a hack. (Unless you also leak your password.) reply temp0826 15 hours agoprevDoesn't look like there is a place to enter past addresses. In the last 15 years I've moved ~10 times. Would be nice to have a way to check those as well. reply altairprime 13 hours agoparentAnecdote: I provided one zip code and it found a past address in another zip code — but I've only ever had two addresses total under this legal identity, so that doesn't speak to how far back it goes. reply ethagnawl 18 hours agoprevHow do they get your data removed from the brokers' databases? reply JohnMakin 18 hours agoparentThey submit an opt-out request on your behalf. Frequently, the data will not be removed entirely, or re-surfaces later on. You're entirely dependent on the good will of the data broker sites, who are likely trying very hard to stop automation like this. reply ethagnawl 15 hours agorootparent> Frequently, the data will not be removed entirely, or re-surfaces later on. You're entirely dependent on the good will of the data broker sites, who are likely trying very hard to stop automation like this. This was my instinctual, cynical assumption, too. Unless there's a GDPR-like law in place and some standard for differentiating identities, they're just going to find loopholes to recapture peoples' data (e.g. remove middle initial, modify address format, etc.). reply JohnMakin 15 hours agorootparentI've used several of these services now and they all have the same issue - the thing is, the data brokers don't even use loopholes. They'll (sometimes) cooperate with removing the data, and then it just reappears in identical form sometime later, often very quickly. They pretend like it isn't their problem and the problem is their data sources that contain the data. It's the complete wild west. reply tholtken 18 hours agoparentprevonerep.com \"If you are located in the United States and have a Monitor Plus subscription, OneRep receives your first and last name, email address, phone number, physical address and date of birth in order to scan data broker sites to find your personal data and request its removal. OneRep keeps your personal data until you end your Monitor subscription in order to check whether your information shows up on additional sites, or has reappeared on the sites you’ve already been removed from.\" reply riddley 18 hours agorootparentI wonder how it works for people who use business-name@personal-domain.tld as their emails with whatever businesses. reply Schnitz 17 hours agoprevDoes this cover spam-enablers like Zoominfo? reply 8f2ab37a-ed6c 13 hours agoprevHow does this compare to Kanary? reply asmor 13 hours agoprevThese services sure are the new sell it to everyone infinite margin after you built it once thing on YouTube sponsorships after everyone who was ever going to buy one has a VPN now. What actually creates this cost, though? I was hoping it'd be free or at cost for the infrastructure and maintenance. reply Mistletoe 17 hours agoprevWhat are the cons of data brokers having my info and does it outweigh losing $14.99 a month? reply beyondd 17 hours agoparentFor many its just getting their home address, phone number and email off the web, which can make you less of an easy target by attackers. For others its something really specific, like someone who is divorced and doesn't want their name showing up next to their ex's name as a spouse or relative. For others, they want their age off the web to prevent age discrimination in a job search. Others may be hiding from an abuser or stalker. reply AzzyHN 17 hours agoprevSnake oil at best reply hammyhavoc 17 hours agoprevNot sure what I think about charging people to remove this information—are they not also just as bad? This seems like the sort of thing that shouldn't require a victim to pay for, but for law to enforce this not happening. As with for-profit healthcare in the USA, just seems scumbag to profit off of misfortune and misery. reply wolverine876 17 hours agoparentIn fairness, Mozilla can't make a law. reply Vinnl 9 hours agorootparentWe do also lobby for better legislation. And from what I've heard (I'm not personally involved with that works), our being present in the market does help us wield influence with legislators. reply hammyhavoc 13 hours agorootparentprevSure, but they are also treating it as a business opportunity, just like the people compiling the data are. They should perhaps be pushing on the legal aspect of what's wrong with the situation rather than making money from it. reply wolverine876 13 hours agorootparentYou're assuming they are driven by a business opportunity; I have no evidence of their motives (do you?), but another way to see it: There is no law and no prospect of one soon. Mozilla can partially solve the problem by providing the service - I think that's great. Otherwise people would have less recourse. And also, Mozilla must have money to operate; charging for this service seems among the least-bad options. reply hammyhavoc 12 hours agorootparentWell, we arrive at the whole \"the optimal amount of fraud is non-zero\" train of thought, otherwise there is no money-making opportunity. They push on the legal aspects of other problems, but I don't see them pushing on the legal aspects of this. Mozilla receives half a billion dollars per year from Google, making up most of their revenue. Mozilla's CEO is also paid millions of dollars each year. If they can't survive as-is whilst paying out those kinds of salaries with such revenue, that's a management problem. reply wolverine876 8 hours agorootparent> Well, we arrive at the whole \"the optimal amount of fraud is non-zero\" train of thought, otherwise there is no money-making opportunity. So? Do doctors want you to have cancer? Do undertakers want you to die? Yet they still get paid. > They push on the legal aspects of other problems, but I don't see them pushing on the legal aspects of this. That's not persuasive, unless you are in that business. Where is a list of the things they do? > If they can't survive as-is whilst paying out those kinds of salaries with such revenue, that's a management problem. While I don't like the CEO's pay, competitors have far greater budgets - and pay CEOs far more - as do many businesses. The amount itself isn't evidence. Where is the evidence that Mozilla isn't allocating funds well? reply kiliancs 19 hours agoprevnext [13 more] [flagged] causal 19 hours agoparentNot sure I understand how this comment follows the announcement. What about it makes you lose trust in Mozilla? reply kiliancs 12 hours agorootparentThe consistent lack of focus. I should have elaborated in my comment. reply ethagnawl 18 hours agoparentprevThis is a much better path forward than some of their other gimmicks. Hopefully it works as billed and is successful. They could have been where Proton is -- offering a security/privacy-first suite of tools -- had they not spent time and resources chasing Pocket and invasive ad campaigns. reply drdaeman 16 hours agorootparentThe deeds speak more than the words, so I'm highly skeptical of their stance on privacy and attitude toward their users in general. I fail to see how it's any better than your average corporate \"we highly value your privacy here at $corpName\". Yes, of course, they aren't as obnoxious as Google or Facebook - spam and big data hoarding is not their business. But it's not that they're any particularly good, the world-wide enshittificaton just made us move the plank so much lower, that anyone not blatantly trying to make buck from anything they can touch is considered \"good\" those days. Heck, modern Mozilla seem to be very comparable to Apple - both use the same themes of \"oh look we're so pro-privacy (when it makes sense to us)\", while becoming increasingly user-hostile and \"knowing better\". Apple aren't my friends, so aren't Mozilla - yet I'm practically forced to use and trust them, because the competition is no better or, typically, even worse (and tech is too convenient to forgo entirely). The most obvious example - and my favorite pet peeve - is how they dealt with sync. They went the most typical corporate road - made their own unique system, extremely poorly designed (I've mentioned this a few times, it's exceptionally bad engineering), requiring an account with them, somewhat documented but with zero practical chances of being interoperable. And the ability to self-host and have a separate account wasn't anything more than an afterthought - I'm 99% sure it's there only because they needed to test against non-production servers, not because anyone cared about end users the slightest tiniest bit. If someone would've cared, and their voice would've been heard, the world would've been a somewhat better place - most likely, we would've had a simple (or, at least, sane) interoperable standard on storing and synchronizing browser data, and praised Mozilla for it, like in the old good days. That hadn't happened. They sure had some ideals, but they lost the sight of them, yet they pretend they didn't. And I'm sorry for those folks at Mozilla who have those ideals. I'm sure there are decent, good people there, who care and are passionate about what they do. But talking about the company as a whole, I can't say I'm happy to trust Mozilla - I do so begrudgingly. Their interests are certainly not aligned with mine. reply tsherr 18 hours agoparentprevI always enjoy the vague yet threatening online comment. I'm of the opinion that this is from one of the better bot farms who have learned if you make people nervous, you can control their choices. reply kiliancs 12 hours agorootparentI see how I was too vague. I decided to post without elaborating because I thought the concerns were obvious from previous HN discussions about Mozilla announcements of different sorts, and because I was running out of time, which probably was a mistake. The Pocket partnership and acquisition, the 250 layoffs with the 4x CEO pay increase, the Mullad partnership for a worse offering, are all concerning and they do destroy trust, but my primary concern is the apparent lack of focus. Can Mozilla sustain all these potentially positive initiatives (like this recent announcement or https://foundation.mozilla.org/en/privacynotincluded/)? I very much doubt it. And I hope this changes before Firefox usage drops even further. I'm not a Mozilla hater. I have contributed Firefox translations in the past, built extensions, and I even collaborate actively with some of the projects that I think are distracting Mozilla. reply HeatrayEnjoyer 18 hours agorootparentprevAgreed. I ceaselessly flag comments like this online. Scary passive voice messages would be worthless even if they weren't from an influence campaign. Say what you mean or hold your tongue! I have zero patience for cowards using vague passive voice. reply wolverine876 15 hours agorootparentI agree about vagueness, and it's great that there's more awareness about rhetorical technique. Still, it's written in active voice. reply tbitrust 18 hours agoparentprev> I lose trust in Mozilla Why did you lose trust in Mozilla? What other browser would you recommend instead? reply mozempthrowaway 16 hours agorootparentBrave reply temp0826 18 hours agorootparentprevMozilla != Firefox The ceo is a joke and the org sucks. reply mozempthrowaway 16 hours agorootparentNot sure why this is downvoted. Mitchell has been a disaster for Mozilla (her only accomplishment is squeezing more money out of Google for the search deal despite an ever dwindling user base for FF - we measure DAU and MAU and the hemorrhaging hasn’t stopped). Very few people in the org actually work on FF. It’s increasingly other bets that don’t pan out or weird acquisitions: Hubs, Pocket, FakeSpot, VPN, Relay, now Monitor. Eventually these lose enough money and they’ll cut them. Then inevitably Mitchell will run a surplus in the budget by cutting projects early and squeezing more money out of Google, funnel the money back to the Mozilla foundation which she also chairs, and then doll it out to her pet political projects. reply beyondd 17 hours agoprev [6 more] [flagged] some_random 17 hours agoparentWait what, why would a data broker partner with a company whose entire purpose is to reduce the completeness of their dataset? reply fdgadfagfgd 17 hours agorootparentIf you ever do this manually, the data brokers that have data removal options will first show you an ad for using a removal site. Because that way, they at least get a cut of the proceeds when you sign up. Data brokers don't get much benefit from people doxing $some_random, other than a few dollars for every thousand people who do that. But, they can get $10s of dollars for when $some_random signs up with their affiliate link. So, you have a clear conflict of interest with onerep not blocking data brokers from their affiliate. It probably doesn't go very deep, but with the subscription-based nature of these privacy services you start to wonder what happens when you churn... reply tholtken 17 hours agorootparentprevmoney reply lkdfjlkdfjlg 17 hours agoparentprev [–] > Optery (YC W22) Nothing against your company specifically, but at this stage anything associated with YN is a negative. reply fdgadfagfgd 17 hours agorootparent [–] I think if you express an opinion like that you ought to say why too. It could be you have a point. But you could also be (mis)interpreted as a critic who, instead of building things themself, finds imperfections in things the real builders make... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Mozilla Monitor, formerly known as Firefox Monitor, has introduced a new paid subscription service called Monitor Plus.",
      "Monitor Plus provides users with automatic data removal and ongoing monitoring of personal information that has been compromised in data breaches.",
      "The service enables users to take control of their online privacy by allowing them to request changes or deletion of their personal data from data broker sites, receive breach alerts, and have their information removed from over 190 data broker sites."
    ],
    "commentSummary": [
      "Mozilla has launched a new service called Mozilla Monitor Plus that automatically removes personal information from data brokers.",
      "Concerns have been raised about potentially providing data brokers with more information, but Mozilla addresses this issue in their privacy policy.",
      "The implementation of a solution using a bloom filter is considered unlikely.",
      "The comments discuss the limitations of centralized data protection services and introduce the concept of query name minimization in DNS.",
      "Other services like Optery, OneRep, and Incogni are mentioned and compared in terms of features and pricing.",
      "Some users express satisfaction with Optery, while others have concerns about affiliate partnerships and third-party scripts.",
      "The conversation also includes discussions about Firefox Relay and alternative providers for privacy protection.",
      "EU regulations present challenges for offering certain services.",
      "Pricing, the effectiveness of data removal, and concerns about privacy and data security are also discussed.",
      "Some users express distrust in Mozilla and criticize the company's management decisions.",
      "There are also criticisms about charging people to remove their personal information.",
      "The overall discussion covers a wide range of topics related to privacy protection and data removal from data brokers."
    ],
    "points": 256,
    "commentCount": 186,
    "retryCount": 0,
    "time": 1707230239
  },
  {
    "id": 39282225,
    "title": "Go 1.22: New Features, Optimizations, and Platform Updates",
    "originLink": "https://go.dev/doc/go1.22",
    "originBody": "Go 1.22 Release Notes Table of Contents Introduction to Go 1.22 Changes to the language Tools Go command Trace Vet Runtime Compiler Linker Bootstrap Core library New math/rand/v2 package New go/version package Enhanced routing patterns Minor changes to the library Ports Darwin Arm Loong64 OpenBSD Introduction to Go 1.22 The latest Go release, version 1.22, arrives six months after Go 1.21. Most of its changes are in the implementation of the toolchain, runtime, and libraries. As always, the release maintains the Go 1 promise of compatibility. We expect almost all Go programs to continue to compile and run as before. Changes to the language Go 1.22 makes two changes to \"for\" loops. Previously, the variables declared by a \"for\" loop were created once and updated by each iteration. In Go 1.22, each iteration of the loop creates new variables, to avoid accidental sharing bugs. The transition support tooling described in the proposal continues to work in the same way it did in Go 1.21. \"For\" loops may now range over integers. For example: package main import \"fmt\" func main() { for i := range 10 { fmt.Println(10 - i) } fmt.Println(\"go1.22 has lift-off!\") } See the spec for details. Go 1.22 includes a preview of a language change we are considering for a future version of Go: range-over-function iterators. Building with GOEXPERIMENT=rangefunc enables this feature. Tools Go command Commands in workspaces can now use a vendor directory containing the dependencies of the workspace. The directory is created by go work vendor, and used by build commands when the -mod flag is set to vendor, which is the default when a workspace vendor directory is present. Note that the vendor directory's contents for a workspace are different from those of a single module: if the directory at the root of a workspace also contains one of the modules in the workspace, its vendor directory can contain the dependencies of either the workspace or of the module, but not both. go get is no longer supported outside of a module in the legacy GOPATH mode (that is, with GO111MODULE=off). Other build commands, such as go build and go test, will continue to work indefinitely for legacy GOPATH programs. go mod init no longer attempts to import module requirements from configuration files for other vendoring tools (such as Gopkg.lock). go test -cover now prints coverage summaries for covered packages that do not have their own test files. Prior to Go 1.22 a go test -cover run for such a package would report ? mymod/mypack [no test files] and now with Go 1.22, functions in the package are treated as uncovered: mymod/mypack coverage: 0.0% of statements Note that if a package contains no executable code at all, we can't report a meaningful coverage percentage; for such packages the go tool will continue to report that there are no test files. Trace The trace tool's web UI has been gently refreshed as part of the work to support the new tracer, resolving several issues and improving the readability of various sub-pages. The web UI now supports exploring traces in a thread-oriented view. The trace viewer also now displays the full duration of all system calls. These improvements only apply for viewing traces produced by programs built with Go 1.22 or newer. A future release will bring some of these improvements to traces produced by older version of Go. Vet References to loop variables The behavior of the vet tool has changed to match the new semantics (see above) of loop variables in Go 1.22. When analyzing a file that requires Go 1.22 or newer (due to its go.mod file or a per-file build constraint), vetcode> no longer reports references to loop variables from within a function literal that might outlive the iteration of the loop. In Go 1.22, loop variables are created anew for each iteration, so such references are no longer at risk of using a variable after it has been updated by the loop. New warnings for missing values after append The vet tool now reports calls to append that pass no values to be appended to the slice, such as slice = append(slice). Such a statement has no effect, and experience has shown that is nearly always a mistake. New warnings for deferring time.Since The vet tool now reports a non-deferred call to time.Since(t) within a defer statement. This is equivalent to calling time.Now().Sub(t) before the defer statement, not when the deferred function is called. In nearly all cases, the correct code requires deferring the time.Since call. For example: t := time.Now() defer log.Println(time.Since(t)) // non-deferred call to time.Since tmp := time.Since(t); defer log.Println(tmp) // equivalent to the previous defer defer func() { log.Println(time.Since(t)) // a correctly deferred call to time.Since }() New warnings for mismatched key-value pairs in log/slog calls The vet tool now reports invalid arguments in calls to functions and methods in the structured logging package, log/slog, that accept alternating key/value pairs. It reports calls where an argument in a key position is neither a string nor a slog.Attr, and where a final key is missing its value. Runtime The runtime now keeps type-based garbage collection metadata nearer to each heap object, improving the CPU performance (latency or throughput) of Go programs by 1–3%. This change also reduces the memory overhead of the majority Go programs by approximately 1% by deduplicating redundant metadata. Some programs may see a smaller improvement because this change adjusts the size class boundaries of the memory allocator, so some objects may be moved up a size class. A consequence of this change is that some objects' addresses that were previously always aligned to a 16 byte (or higher) boundary will now only be aligned to an 8 byte boundary. Some programs that use assembly instructions that require memory addresses to be more than 8-byte aligned and rely on the memory allocator's previous alignment behavior may break, but we expect such programs to be rare. Such programs may be built with GOEXPERIMENT=noallocheaders to revert to the old metadata layout and restore the previous alignment behavior, but package owners should update their assembly code to avoid the alignment assumption, as this workaround will be removed in a future release. On the windows/amd64 port, programs linking or loading Go libraries built with -buildmode=c-archive or -buildmode=c-shared can now use the SetUnhandledExceptionFilter Win32 function to catch exceptions not handled by the Go runtime. Note that this was already supported on the windows/386 port. Compiler Profile-guided Optimization (PGO) builds can now devirtualize a higher proportion of calls than previously possible. Most programs from a representative set of Go programs now see between 2 and 14% improvement from enabling PGO. The compiler now interleaves devirtualization and inlining, so interface method calls are better optimized. Go 1.22 also includes a preview of an enhanced implementation of the compiler's inlining phase that uses heuristics to boost inlinability at call sites deemed \"important\" (for example, in loops) and discourage inlining at call sites deemed \"unimportant\" (for example, on panic paths). Building with GOEXPERIMENT=newinliner enables the new call-site heuristics; see issue #61502 for more info and to provide feedback. Linker The linker's -s and -w flags are now behave more consistently across all platforms. The -w flag suppresses DWARF debug information generation. The -s flag suppresses symbol table generation. The -s flag also implies the -w flag, which can be negated with -w=0. That is, -s -w=0 will generate a binary with DWARF debug information generation but without the symbol table. On ELF platforms, the -B linker flag now accepts a special form: with -B gobuildid, the linker will generate a GNU build ID (the ELF NT_GNU_BUILD_ID note) derived from the Go build ID. On Windows, when building with -linkmode=internal, the linker now preserves SEH information from C object files by copying the .pdata and .xdata sections into the final binary. This helps with debugging and profiling binaries using native tools, such as WinDbg. Note that until now, C functions' SEH exception handlers were not being honored, so this change may cause some programs to behave differently. -linkmode=external is not affected by this change, as external linkers already preserve SEH information. Bootstrap As mentioned in the Go 1.20 release notes, Go 1.22 now requires the final point release of Go 1.20 or later for bootstrap. We expect that Go 1.24 will require the final point release of Go 1.22 or later for bootstrap. Core library New math/rand/v2 package Go 1.22 includes the first “v2” package in the standard library, math/rand/v2. The changes compared to math/rand are detailed in proposal #61716. The most important changes are: The Read method, deprecated in math/rand, was not carried forward for math/rand/v2. (It remains available in math/rand.) The vast majority of calls to Read should use crypto/rand’s Read instead. Otherwise a custom Read can be constructed using the Uint64 method. The global generator accessed by top-level functions is unconditionally randomly seeded. Because the API guarantees no fixed sequence of results, optimizations like per-thread random generator states are now possible. The Source interface now has a single Uint64 method; there is no Source64 interface. Many methods now use faster algorithms that were not possible to adopt in math/rand because they changed the output streams. The Intn, Int31, Int31n, Int63, and Int64n top-level functions and methods from math/rand are spelled more idiomatically in math/rand/v2: IntN, Int32, Int32N, Int64, and Int64N. There are also new top-level functions and methods Uint32, Uint32N, Uint64, Uint64N, Uint, and UintN. The new generic function N is like Int64N or Uint64N but works for any integer type. For example a random duration from 0 up to 5 minutes is rand.N(5*time.Minute). The Mitchell & Reeds LFSR generator provided by math/rand’s Source has been replaced by two more modern pseudo-random generator sources: ChaCha8 PCG. ChaCha8 is a new, cryptographically strong random number generator roughly similar to PCG in efficiency. ChaCha8 is the algorithm used for the top-level functions in math/rand/v2. As of Go 1.22, math/rand's top-level functions (when not explicitly seeded) and the Go runtime also use ChaCha8 for randomness. We plan to include an API migration tool in a future release, likely Go 1.23. New go/version package The new go/version package implements functions for validating and comparing Go version strings. Enhanced routing patterns HTTP routing in the standard library is now more expressive. The patterns used by net/http.ServeMux have been enhanced to accept methods and wildcards. Registering a handler with a method, like \"POST /items/create\", restricts invocations of the handler to requests with the given method. A pattern with a method takes precedence over a matching pattern without one. As a special case, registering a handler with \"GET\" also registers it with \"HEAD\". Wildcards in patterns, like /items/{id}, match segments of the URL path. The actual segment value may be accessed by calling the Request.PathValue method. A wildcard ending in \"...\", like /files/{path...}, must occur at the end of a pattern and matches all the remaining segments. A pattern that ends in \"/\" matches all paths that have it as a prefix, as always. To match the exact pattern including the trailing slash, end it with {$}, as in /exact/match/{$}. If two patterns overlap in the requests that they match, then the more specific pattern takes precedence. If neither is more specific, the patterns conflict. This rule generalizes the original precedence rules and maintains the property that the order in which patterns are registered does not matter. This change breaks backwards compatibility in small ways, some obvious—patterns with \"{\" and \"}\" behave differently— and some less so—treatment of escaped paths has been improved. The change is controlled by a GODEBUG field named httpmuxgo121. Set httpmuxgo121=1 to restore the old behavior. Minor changes to the library As always, there are various minor changes and updates to the library, made with the Go 1 promise of compatibility in mind. There are also various performance improvements, not enumerated here. archive/tar The new method Writer.AddFS adds all of the files from an fs.FS to the archive. archive/zip The new method Writer.AddFS adds all of the files from an fs.FS to the archive. bufio When a SplitFunc returns ErrFinalToken with a nil token, Scanner will now stop immediately. Previously, it would report a final empty token before stopping, which was usually not desired. Callers that do want to report a final empty token can do so by returning []byte{} rather than nil. cmp The new function Or returns the first in a sequence of values that is not the zero value. crypto/tls ConnectionState.ExportKeyingMaterial will now return an error unless TLS 1.3 is in use, or the extended_master_secret extension is supported by both the server and client. crypto/tls has supported this extension since Go 1.20. This can be disabled with the tlsunsafeekm=1 GODEBUG setting. By default, the minimum version offered by crypto/tls servers is now TLS 1.2 if not specified with config.MinimumVersion, matching the behavior of crypto/tls clients. This change can be reverted with the tls10server=1 GODEBUG setting. By default, cipher suites without ECDHE support are no longer offered by either clients or servers during pre-TLS 1.3 handshakes. This change can be reverted with the tlsrsakex=1 GODEBUG setting. crypto/x509 The new CertPool.AddCertWithConstraint method can be used to add customized constraints to root certificates to be applied during chain building. On Android, root certificates will now be loaded from /data/misc/keychain/certs-added as well as /system/etc/security/cacerts. A new type, OID, supports ASN.1 Object Identifiers with individual components larger than 31 bits. A new field which uses this type, Policies, is added to the Certificate struct, and is now populated during parsing. Any OIDs which cannot be represented using a asn1.ObjectIdentifier will appear in Policies, but not in the old PolicyIdentifiers field. When calling CreateCertificate, the Policies field is ignored, and policies are taken from the PolicyIdentifiers field. Using the x509usepolicies=1 GODEBUG setting inverts this, populating certificate policies from the Policies field, and ignoring the PolicyIdentifiers field. We may change the default value of x509usepolicies in Go 1.23, making Policies the default field for marshaling. database/sql The new Null[T] type provide a way to scan nullable columns for any column types. debug/elf Constant R_MIPS_PC32 is defined for use with MIPS64 systems. Additional R_LARCH_* constants are defined for use with LoongArch systems. encoding The new methods AppendEncode and AppendDecode added to each of the Encoding types in the packages encoding/base32, encoding/base64, and encoding/hex simplify encoding and decoding from and to byte slices by taking care of byte slice buffer management. The methods base32.Encoding.WithPadding and base64.Encoding.WithPadding now panic if the padding argument is a negative value other than NoPadding. encoding/json Marshaling and encoding functionality now escapes '\\b' and '\\f' characters as \\b and \\f instead of \\u0008 and \\u000c. go/ast The following declarations related to syntactic identifier resolution are now deprecated: Ident.Obj, Object, Scope, File.Scope, File.Unresolved, Importer, Package, NewPackage. In general, identifiers cannot be accurately resolved without type information. Consider, for example, the identifier K in T{K: \"\"}: it could be the name of a local variable if T is a map type, or the name of a field if T is a struct type. New programs should use the go/types package to resolve identifiers; see Object, Info.Uses, and Info.Defs for details. The new ast.Unparen function removes any enclosing parentheses from an expression. go/types The new Alias type represents type aliases. Previously, type aliases were not represented explicitly, so a reference to a type alias was equivalent to spelling out the aliased type, and the name of the alias was lost. The new representation retains the intermediate Alias. This enables improved error reporting (the name of a type alias can be reported), and allows for better handling of cyclic type declarations involving type aliases. In a future release, Alias types will also carry type parameter information. The new function Unalias returns the actual type denoted by an Alias type (or any other Type for that matter). Because Alias types may break existing type switches that do not know to check for them, this functionality is controlled by a GODEBUG field named gotypesalias. With gotypesalias=0, everything behaves as before, and Alias types are never created. With gotypesalias=1, Alias types are created and clients must expect them. The default is gotypesalias=0. In a future release, the default will be changed to gotypesalias=1. Clients of go/types are urged to adjust their code as soon as possible to work with gotypesalias=1 to eliminate problems early. The Info struct now exports the FileVersions map which provides per-file Go version information. The new helper method PkgNameOf returns the local package name for the given import declaration. The implementation of SizesFor has been adjusted to compute the same type sizes as the compiler when the compiler argument for SizesFor is \"gc\". The default Sizes implementation used by the type checker is now types.SizesFor(\"gc\", \"amd64\"). The start position (Pos) of the lexical environment block (Scope) that represents a function body has changed: it used to start at the opening curly brace of the function body, but now starts at the function's func token. html/template Javascript template literals may now contain Go template actions, and parsing a template containing one will no longer return ErrJSTemplate. Similarly the GODEBUG setting jstmpllitinterp no longer has any effect. io The new SectionReader.Outer method returns the ReaderAt, offset, and size passed to NewSectionReader. log/slog The new SetLogLoggerLevel function controls the level for the bridge between the `slog` and `log` packages. It sets the minimum level for calls to the top-level `slog` logging functions, and it sets the level for calls to `log.Logger` that go through `slog`. math/big The new method Rat.FloatPrec computes the number of fractional decimal digits required to represent a rational number accurately as a floating-point number, and whether accurate decimal representation is possible in the first place. net When io.Copy copies from a TCPConn to a UnixConn, it will now use Linux's splice(2) system call if possible, using the new method TCPConn.WriteTo. The Go DNS Resolver, used when building with \"-tags=netgo\", now searches for a matching name in the Windows hosts file, located at %SystemRoot%\\System32\\drivers\\etc\\hosts, before making a DNS query. net/http The new functions ServeFileFS, FileServerFS, and NewFileTransportFS are versions of the existing ServeFile, FileServer, and NewFileTransport, operating on an fs.FS. The HTTP server and client now reject requests and responses containing an invalid empty Content-Length header. The previous behavior may be restored by setting GODEBUG field httplaxcontentlength=1. The new method Request.PathValue returns path wildcard values from a request and the new method Request.SetPathValue sets path wildcard values on a request. net/http/cgi When executing a CGI process, the PATH_INFO variable is now always set to the empty string or a value starting with a / character, as required by RFC 3875. It was previously possible for some combinations of Handler.Root and request URL to violate this requirement. net/netip The new AddrPort.Compare method compares two AddrPorts. os On Windows, the Stat function now follows all reparse points that link to another named entity in the system. It was previously only following IO_REPARSE_TAG_SYMLINK and IO_REPARSE_TAG_MOUNT_POINT reparse points. On Windows, passing O_SYNC to OpenFile now causes write operations to go directly to disk, equivalent to O_SYNC on Unix platforms. On Windows, the ReadDir, File.ReadDir, File.Readdir, and File.Readdirnames functions now read directory entries in batches to reduce the number of system calls, improving performance up to 30%. When io.Copy copies from a File to a net.UnixConn, it will now use Linux's sendfile(2) system call if possible, using the new method File.WriteTo. os/exec On Windows, LookPath now ignores empty entries in %PATH%, and returns ErrNotFound (instead of ErrNotExist) if no executable file extension is found to resolve an otherwise-unambiguous name. On Windows, Command and Cmd.Start no longer call LookPath if the path to the executable is already absolute and has an executable file extension. In addition, Cmd.Start no longer writes the resolved extension back to the Path field, so it is now safe to call the String method concurrently with a call to Start. reflect The Value.IsZero method will now return true for a floating-point or complex negative zero, and will return true for a struct value if a blank field (a field named _) somehow has a non-zero value. These changes make IsZero consistent with comparing a value to zero using the language == operator. The PtrTo function is deprecated, in favor of PointerTo. The new function TypeFor returns the Type that represents the type argument T. Previously, to get the reflect.Type value for a type, one had to use reflect.TypeOf((*T)(nil)).Elem(). This may now be written as reflect.TypeFor[T](). runtime/metrics Four new histogram metrics /sched/pauses/stopping/gc:seconds, /sched/pauses/stopping/other:seconds, /sched/pauses/total/gc:seconds, and /sched/pauses/total/other:seconds provide additional details about stop-the-world pauses. The \"stopping\" metrics report the time taken from deciding to stop the world until all goroutines are stopped. The \"total\" metrics report the time taken from deciding to stop the world until it is started again. The /gc/pauses:seconds metric is deprecated, as it is equivalent to the new /sched/pauses/total/gc:seconds metric. /sync/mutex/wait/total:seconds now includes contention on runtime-internal locks in addition to sync.Mutex and sync.RWMutex. runtime/pprof Mutex profiles now scale contention by the number of goroutines blocked on the mutex. This provides a more accurate representation of the degree to which a mutex is a bottleneck in a Go program. For instance, if 100 goroutines are blocked on a mutex for 10 milliseconds, a mutex profile will now record 1 second of delay instead of 10 milliseconds of delay. Mutex profiles also now include contention on runtime-internal locks in addition to sync.Mutex and sync.RWMutex. Contention on runtime-internal locks is always reported at runtime._LostContendedRuntimeLock. A future release will add complete stack traces in these cases. CPU profiles on Darwin platforms now contain the process's memory map, enabling the disassembly view in the pprof tool. runtime/trace The execution tracer has been completely overhauled in this release, resolving several long-standing issues and paving the way for new use-cases for execution traces. Execution traces now use the operating system's clock on most platforms (Windows excluded) so it is possible to correlate them with traces produced by lower-level components. Execution traces no longer depend on the reliability of the platform's clock to produce a correct trace. Execution traces are now partitioned regularly on-the-fly and as a result may be processed in a streamable way. Execution traces now contain complete durations for all system calls. Execution traces now contain information about the operating system threads that goroutines executed on. The latency impact of starting and stopping execution traces has been dramatically reduced. Execution traces may now begin or end during the garbage collection mark phase. To allow Go developers to take advantage of these improvements, an experimental trace reading package is available at golang.org/x/exp/trace. Note that this package only works on traces produced by programs built with Go 1.22 at the moment. Please try out the package and provide feedback on the corresponding proposal issue. If you experience any issues with the new execution tracer implementation, you may switch back to the old implementation by building your Go program with GOEXPERIMENT=noexectracer2. If you do, please file an issue, otherwise this option will be removed in a future release. slices The new function Concat concatenates multiple slices. Functions that shrink the size of a slice (Delete, DeleteFunc, Compact, CompactFunc, and Replace) now zero the elements between the new length and the old length. Insert now always panics if the argument i is out of range. Previously it did not panic in this situation if there were no elements to be inserted. syscall The syscall package has been frozen since Go 1.4 and was marked as deprecated in Go 1.11, causing many editors to warn about any use of the package. However, some non-deprecated functionality requires use of the syscall package, such as the os/exec.Cmd.SysProcAttr field. To avoid unnecessary complaints on such code, the syscall package is no longer marked as deprecated. The package remains frozen to most new functionality, and new code remains encouraged to use golang.org/x/sys/unix or golang.org/x/sys/windows where possible. On Linux, the new SysProcAttr.PidFD field allows obtaining a PID FD when starting a child process via StartProcess or os/exec. On Windows, passing O_SYNC to Open now causes write operations to go directly to disk, equivalent to O_SYNC on Unix platforms. testing/slogtest The new Run function uses sub-tests to run test cases, providing finer-grained control. Ports Darwin On macOS on 64-bit x86 architecture (the darwin/amd64 port), the Go toolchain now generates position-independent executables (PIE) by default. Non-PIE binaries can be generated by specifying the -buildmode=exe build flag. On 64-bit ARM-based macOS (the darwin/arm64 port), the Go toolchain already generates PIE by default. Go 1.22 is the last release that will run on macOS 10.15 Catalina. Go 1.23 will require macOS 11 Big Sur or later. Arm The GOARM environment variable now allows you to select whether to use software or hardware floating point. Previously, valid GOARM values were 5, 6, or 7. Now those same values can be optionally followed by ,softfloat or ,hardfloat to select the floating-point implementation. This new option defaults to softfloat for version 5 and hardfloat for versions 6 and 7. Loong64 The loong64 port now supports passing function arguments and results using registers. The linux/loong64 port now supports the address sanitizer, memory sanitizer, new-style linker relocations, and the plugin build mode. OpenBSD Go 1.22 adds an experimental port to OpenBSD on big-endian 64-bit PowerPC (openbsd/ppc64).",
    "commentLink": "https://news.ycombinator.com/item?id=39282225",
    "commentBody": "Go 1.22 (go.dev)254 points by bestinterest 10 hours agohidepastfavorite78 comments BillyTheKing 5 hours agoI've been mostly writing Typescript the past 3 years - and recently started writing code in Go. Initially I was a little apprehensive, lack of array functions, slightly less flexible type-system, etc. But after spending some time writing Go I now had to re-initialise a typescript project for a small-ish team (4-5 devs). The amount of time spent on things such as linting, selecting the correct library for server routing, the correct server, coding standards, basic error-handling and enforcing it with a custom error or Result type to get out of this nested try/catch hell which still loses the majority of errors. Setting up testing and mocking. Setting up Prisma and what not - and finally the PRs are still a hit and miss, some ok, some make use of weird JS functions.. Don't get me wrong, I really do like Typescript. But I gotta say after all of that it's just great using a language with a fantastic standard library, proper type-safety, with some coding standards built-in. It's obviously not without quirks, but it's pretty decent - and great to see that routing has now also moved into the standard library, another bit that you don't have to worry about - can't wait for some map/filter/find slice functions though! reply hambos22 1 hour agoparent> can't wait for some map/filter/find slice functions though Till that day comes, you could use the \"lo\" library (inspired from lodash). It's my goto Swiss army knife for golang projects. https://github.com/samber/lo reply KingOfCoders 4 hours agoparentprevIn my CTO newsletter I recently wrote about TS vs Go releases, with Go 1.22 as an example. TS gets more and more complicated with each release, catering to the power users. Go adds things that makes it simpler to use, like the (missing) range over integers. It's like game sequels, they add more and more canon and game mechanics, then game sequels (or comics) need to reset to make them more accessible to newcomers again. Programming languages can't do this, so I'm happy that Go keeps this in mind. reply pjmlp 2 hours agorootparentThe thing with Typescript is that it is only a fancy JavaScript linter, so the only way to justify newer releases is to keep adding up the type system, there is nothing else when language features that aren't type system related are supposed to come from JavaScript evolution. So they either say they are done, or keep adding type theory stuff until it implodes, I fear. Actually I am looking forward to type annotations in JavaScript now in the roadmap, being good enough for general use cases. reply norman784 1 hour agorootparentDo you know if the Javascript type annotations is progressing? I didn't hear anything after the initial proposal. reply miningape 0 minutes agorootparentI don't have the source at hand but I remember seeing that they wouldn't support it until it had progressed as a JavaScript proposal. Their reasoning was that the decorations API is really weak, and it will likely be changed meaning a complete rewrite of the TypeScript decorator implementations. falsandtru 48 minutes agorootparentprevThey just keep adding new features for fear of losing their position because they can't decorate the release notes. MS doesn't give high marks for bug fixes. Thus the bugs keep growing. reply ken47 56 minutes agorootparentprevYes, but TS users can stay on the \"type-newbie\" path, which is still a huge improvement over vanilla JS and doesn't take much effort. What I've had issues with is devs who came from the vanilla JS world and love it, so they go out of their way to avoid utilizing more complex types when they would add no-cost safety (aside from the initial minutes or hour spent learning the feature). reply cookiengineer 3 hours agoparentprevI feel very similar to your experience. What made me stay in go is its amazingly unified build toolchain. The things you can do with go:embed and go:generate blow my mind in every other project. The golang.org/x package is also another thing, where there is pretty much every internet RFC related implementation available, ready to use. reply avtolik 1 hour agorootparentCan you give some examples how are you using go:embed and go:generate? reply maccard 10 minutes agorootparentWe use go:generate to generate services and types from protobufs. //go:generate protoc --go_out=src/generated/ protos/service.proto Our CI pipeline is a dockerfile that looks vaguely like this: FROM golang:1.21 as build go generate go build go test FROM scratch COPY --from=build ... The CI command is `docker&& docker push `, and we have a goland project template that has options for generate, build, test that matches what we do in CI, rather than having that _one_ edge case that is the difference between `make build` and `go build` - which has caused more than one outage in my career. reply bheadmaster 1 hour agorootparentprevOne example that comes to mind is building a single-binary full-stack application. You can use whatever frontend framework you want, and just embed the html/css/js/asset files inside the binary with go:embed. In case of dynamic set of files, you can also write a Go utility to generate the embeddings with go:generate. In addition to the ease of distribution (no more assets/ directory - just a single binary executable!), it also increases speed of the application, as it no longer has to perform file system reads in order to serve a webpage. reply gotbeans 57 minutes agorootparentprevNot OP, used embedded to add ebpf code compiled for a project, helps to only ship the binary. Same thing for shipping swagger static html stuff to host an OpenAPI server. reply ryloric 4 hours agoparentprevYeah, it's quite productive in a counter intuitive way, not having a ton of features just removes a lot of tiny decisions you have to make in a richer language. reply lnxg33k1 2 hours agoparentprevI’ve worked for a while at a client using typescript, after a while I started calling it “Tricks Driven Development”, every time I had to do something I’d read the docs, but then someone would communicate some trick not on the docs that was possible to use reply endtime 44 minutes agoparentprevI got to spend a couple years writing Dart (not Flutter) and found it to be the best of both worlds. Such an underappreciated language. reply tonis2 16 minutes agorootparentYeah, Dart is way better / easier to debug, than Typescript in my opinion reply baby 2 hours agoparentprevThe problem of typescript is the lack of convention and the emphasis on configuration, the reverse made Golang a great language. reply nalgeon 9 hours agoprevIf you find the official release notes a bit dry, I've made an interactive version: https://antonz.org/go-1-22 reply justinclift 5 hours agoparentCool, that does help. It wasn't immediately obvious what the problem with the for loop sharing thing meant, but seeing it run and give unexpected results helped. :) reply thatswrong0 8 hours agoparentprevThank you this is awesome. I find it so much quicker to read and understand things like this with examples.. and these are runnable / editable! Sick. reply yeonsh 3 hours agoparentprevOh, it's great. reply drewlesueur 4 hours agoparentprevreally cool reply tommiegannert 2 hours agoprev> Enhanced routing patterns > This change breaks backwards compatibility in small ways, some obvious—patterns with \"{\" and \"}\" behave differently— and some less so—treatment of escaped paths has been improved. The change is controlled by a GODEBUG field named httpmuxgo121. Set httpmuxgo121=1 to restore the old behavior. That's a great enhancement now that the future of Gorilla Mux is shaky. But why doesn't that go against the Go 1 compatibility promise? > It is intended that programs written to the Go 1 specification will continue to compile and run correctly, unchanged, over the lifetime of that specification. reply EdiX 39 minutes agoparentThe way they are using to route around the Go 1 compatibility promise is to gate these backwards incompatible changes on the value of the go directive in go.mod. If it says 1.22 or later you get the new library behavior, otherwise you get the old one. We'll see how well this ends up working in practice. reply maccard 8 minutes agoparentprevI agree this is a (rare) mistake with Go. If they're going to break versioning using go.mod, they should at least break it in a way that makes it better. I'd much rather have my code fail to compile when we change go.mod to 1.22 than have to detect subtle runtime issues like this. reply zaphodias 1 hour agoparentprevUsually the Go team scrapes from GitHub and open source programs how people use something before breaking them; I suspect they found little usage of { and } in HTTP handler paths. They also provide a way of opting out the new behaviour, so they don't force you to change anything in your code (but yes, it does require you to set a new env var). The change to the for loop semantic is another example in this release; it effectively is a breaking change. All Go programs continue to compile and run, though with minor behavioural changes. I think Go took a pragmatic approach, and that was one of the reasons for its success. reply carstenhag 1 hour agorootparentI don't see the point of using semver (or at least telling us about the same guarantees) and then not making use of it. If there were 10 breaking chances we should be at 11.x now, not at 1.x with 20 environment variables. reply bheadmaster 56 minutes agorootparentFrom a purist perspective, you're right - the contract has been broken, and a major version should've been incremented However, Go has always been more of a pragmatic than a purist language. For example, they've analyzed tons of code and found that most of them had bugs caused by having `for` loop with a single variable being mutated. So they changed the `for` loop (in a technically backwards incompatible way) in order to make all those bugs disappear. In a way, they modified the formal contract to make it more aligned with the de-facto contract that users were expecting. I personally think that kind of pragmatism beats purism any day. Maybe the fact that I've never personally been affected by Go backwards incompatibilities also plays a role... But I've yet to find a single person who has :) reply fbdab103 1 hour agoparentprevBecause then they would lose their pithy advertising slogan. Minor or not, it is a non-compatible change. reply saclark11 6 hours agoprev> When io.Copy copies from a TCPConn to a UnixConn, it will now use Linux's splice(2) system call if possible, using the new method TCPConn.WriteTo. Interface upgrades, yet again, transparently giving us more zero-copy IO. Love how much mileage they’re able to get out of this pattern in the io package. reply lifthrasiir 5 hours agoparentGo standard library is absolutely littered with these kinds of tweaks. Not necessarily bad, but it does make an integration with non-standard library less satisfactory because such special casing is not scalable. Still within the expectation though. reply omoikane 8 hours agoprev> In Go 1.22, each iteration of the loop creates new variables This was previously discussed here: https://news.ycombinator.com/item?id=33160236 - Go: Redefining For Loop Variable Semantics (2022) reply peter_l_downs 7 hours agoprevThe addition of `sql.Null[T]` is great. I'll probably start using that in new projects. In current stuff, I'm relying on sqlboiler's null [0] whose API is very similar — it works the same way as `sql.Null` will, but has an additional `IsSet() bool` method that tells you whether or not the value was ever explicitly set, to help you distinguish \"intentionally null\" from \"null due to uninitialization\". (Sounds nice, but I've never once used it.) reply eweise 6 hours agoparentCurious when you need to know when null is intentional or not. There's no corresponding type like that in the db. reply buzer 4 hours agorootparentFor example when you are using partially populated record to update the database. If field is null intentionally it means it should be updated to it. If it's not set the update statement should not touch it. reply divan 1 hour agoprevI've been writing Go for 9+ years, but for the last 4 years, I had to write a lot of Dart (for Flutter). I consider these two languages to be on the opposite sides of the complexity stance. Dart tries to add and implement every feature possible, but Go is the opposite. Two observations: 1) I'm spending a lot of time fighting multiple ways to init stuff in a class (i.e., declare the variable and set the value). Depending on the final/const/static/late prefix, there are multiple ways to init it in the constructor or factory or initState() method of Flutter's StatefulWidget, and god forbid you to refactor any of that – you'll be forced to rehaul all the initialization. Dart's getter feature (which makes functions look like variables) also adds confusion, especially with a new codebase. I constantly find it embarrassing how much time I need to spend on such a straightforward thing as variable/field initialization. I often find myself missing the simplicity of Go. 2) Compared to Go, Dart has all the juicy stuff like maps/streams/whatever for packing all in a single line. It's very tempting to use those for quickly creating singleliners with hard-to-understand complexity. Sometimes I even start feeling that I'm missing those in Go. But when I get to debugging those or, especially, junior developers looking at these write-only singleliners, with an array of ugly hacks like .firstWhereOrNull or optional '(orElse: () => null)' parameters, they are very confused, especially when cryptic null safety or type errors stops them. Rewriting those singleliners as a simple Go-style for loop is such a relief. reply Jean-Papoulos 1 hour agoparentTip about Dart : Don't use initState for stuff that doesn't directly concern UI (setting the hint of a text field, for example). Most of my Flutter pages rely either on having very few things to do, or having a MyPageController object such that the parent creates a controller, initializes it however it likes and the child page's behavior is dependent on that controller. A typical example of this would be the parent being a page containing a list, and the user wanting to edit a list element. Create a controller, give it the element, and send the controller to a child page where the user does the editing. On return, the parent can look at the element or other variables/callbacks in the controller to decide what should change in the UI. This also allows finely-controlled interactions between widgets without having to delete with InheritedWidget or the likes. Of course you should use a state management library with this, even though a lot of the time I don't. reply divan 16 minutes agorootparentSome people joke that state management in Flutter is like http mixers/routes in Go. But I think it's much worse :) Granted, UI state handling is not an easy task, and it's not directly related to the topic of complexity of the languages. I had an article written a few years ago about a thought experiment of Flutter being implemented with Go. It's a bit naive, but still fun to think about. [1] [1] https://divan.dev/posts/flutter_go/ reply doctor_eval 1 hour agoparentprevI agree, Go’s simplicity is the best thing about it. I think the same thing about ranges, I think I end up typing the same number of characters - but spreading it over three lines makes it feel “longer” than a comprehension like “forEach(f)”. But then I write the range longhand, and it’s no big deal. Speaking of initialisation though, I do wish Go had an idiomatic way to initialise struct fields to specific values. I don’t care about the lack of constructors; mostly, I just wish I could have bools initialise to true sometimes. reply divan 23 minutes agorootparentSure, it's always a tradeoff. Yet my pet peeve is that people rarely talk about the social aspects of the programming language. It's called \"language\" for a reason. We express our thoughts using this language (\"I intend this code to do such and such\"), and we expect other people to be able to understand what we intended, and we want to make sure that they understand exactly as we want them to. I judge languages on their ability to collectively construct mental maps in the brains of the developers who work on the same project. If they all read the same code, will they be able to understand the task and intent of that code without additional explanations? How cognitively hard would it be? Gottfried Liebnitz was obsessed with finding a Universal Language, which was exactly about this – making communication clear and lacking misunderstanding. I feel like Dart (and most other languages) approach is the opposite of that. Creating multiple ways to express the same intent is a sure go way to introduce misunderstanding and fracture the speakers of that language into dialects and groups. Go's, on the other hand, is really good at making this \"reconstructing mental map\" part a joy. reply candiddevmike 9 hours agoprevLooking forward to revisting the stdlib mux and possibly removing chi. I really appreciate seeing this stuff move into the stdlib. reply catherinecodes 6 minutes agoparentCame here to say the exact same thing. chi has been reallly great for a couple projects--in fact, it's easy to forget it's even here. Moving them into the stdlib means they'll always be maintained and ensure that approach is used in many Go programs. reply emmanueloga_ 9 hours agoprevFor those using Go for production, do you get to switch to the latest versions quickly, or do you get stuck in older releases? In the open a lot of projects seem to avoid newish features. I like to use `any` (from 1.18 ~ 2 years ago) where before we had to use `interface{}`, even if I'm not using generics (although I've been told the latter is more \"idiomatic\" :-/). reply maccard 6 minutes agoparentWe try to keep up to date. We'll be on 1.22 by the end of the month, give or take (all going well) reply peter_l_downs 7 hours agoparentprevWe usually wait to experiment with new features until we find a good problem that would fit, but we upgrade compiler version pretty much right after they release. For instance we waited on actually using generics for about 6 months until we were able to experiment a bit and make sure they were really useful, the devx was good, and build/test speeds weren't significantly impacted. reply adtac 6 hours agorootparent> build/test speeds weren't significantly impacted it must be getting a little repetitive to test every version and find that Go continues to build crazy fast every time :) reply AdamJacobMuller 5 hours agorootparentI remember early in the go release cycle (talking around go1.4->later era) when I would rebuild our application with a new version and marvel at how much faster they made it. GC was another thing which was crazy, they halved GC pause time for like 5 or 6 releases in a row. like 1s -> 500ms -> 250ms -> 125ms etc etc etc. reply peter_l_downs 6 hours agorootparentprevThe test doesn't take much time :) reply mappu 9 hours agoparentprevAt my $DAYJOB devs are free to jump on latest builds for containers we operate (SaaS / API services). But we also deliver end-user apps with Go, and those have to stick to 1.20 for compatibility with older client OSes. 1.21 made significant cuts and so we will likely be on 1.20 for some years to come. reply drchase 8 hours agorootparentWhich older OSes? The last 1.20 dot release was earlier today, some years from now it will be stale and probably insecure, though that matters less for internal apps than for internet-exposed ones. reply mappu 5 hours agorootparentWindows 7, Server 2012, macOS 10.13 + 10.14, and so on. Together it makes up nearly 10% of our user base. We have a very slow-moving customer base and their choice of OS is out of our control. At this time last year there were still more Windows 7 users than Windows 11 in telemetry. Those old OSes are leaving security support, but actually Windows Server 2012 still gets updates from Microsoft (ESU) until 2026 so i'm surprised you dropped it already. Yes, Go1.20 will get insecure, but until that usage drops, not much can be done. Well, one thing: in the past, in order to maintain support for Server 2008, for a long while we built the app with both Go1.10 and Go1.(newer) and switched at install time. I don't recommend it! Every year was more difficult as the open source packages drifted away from compatibility and our build system had to still support GOPATH. Eventually we finally abandoned Go1.10! reply signal11 5 hours agorootparentprevI thought the 1.21 cuts were for out of support Windows versions only? Eg Windows 7. You’ll have to also factor that 1.20 won’t receive security updates any more. reply mappu 5 hours agorootparentServer 2012 was dropped in https://github.com/golang/go/issues/57004 and it's still in security support. My numbers are somewhat similar to @olivielpeau in that thread. reply zellyn 6 hours agoparentprevAt Square, we typically wait until the first point release before updating the default version in our Go monorepo. Individual application owners are free to upgrade sooner. This time, we had some wiggles, so the Go team beat us to 1.22 before we could upgrade our default to 1.21 :-) Waiting is more or less just customary at this point; Go releases seldom break things. reply mortallywounded 9 hours agoparentprevI switch right away. If you're worried about supporting older Go compilers, you could always have build conditions for older versions that define missing things like: // +build !go1.7 type any = interface{} reply miki123211 3 hours agoparentprevI don't think this is necessarily true, most of the standard library docs have switched over to using any, even when generics are not involved. reply didip 6 hours agoparentprevI switched right away. All of our images are tagged anyway so rolling back is hella easy. reply voidfunc 5 hours agoparentprevUsually we switch after the first point release. reply dharmab 6 hours agoparentprevUpgrade our binaries immediately, then bump go.mod `go` directive as we touch things. reply kubb 53 minutes agoprevRange over function is actually good. Now we can implement proper iterators for collections. In particular, it should be possible to have abstractions like clojure-style lazy sequences. reply KingOfCoders 4 hours agoprevI love how easy upgrading Go has become. Change 1.21.6 to 1.22.0 in your go.mod, done. reply erik_seaberg 7 hours agoprevIt’s good to see them starting to make for-range loops work on user-defined ADTs. reply moondev 2 hours agoprevStill holding out hope for a \"go run\" flag to easily run module programs with replacements in go.mod go run k8s.io/kubernetes/cmd/kubectl@v1.28.2 reply doctor_eval 1 hour agoprevI’m super excited to see range over functions - iterators. It’s one of the only things I miss from Java. It took me a few reads to understand how the work, but in typical Go style (once I got it) it was so much simpler than the equivalent in most other languages I’ve used. Looking forward to it being promoted out of experimental. reply shakow 9 hours agoprev [–] > \"For\" loops may now range over integers The future is now, old man! reply kristianp 8 hours agoparentfor i := range 10 { What was the syntax for this before? reply tptacek 8 hours agorootparentfor i := 0; ianything ambiguous is probably going to haunt me at 3am some day Man, I hope you never wondered how ERRNO behaves with CGo or that you didn't miss to update any initialization of that struct you recently added a new field to, that haunts me at 3AM. reply cwbriscoe 6 hours agorootparentprevI always just assume it is consistent to whether arrays start at [0] or [1] on whichever language I am working on. reply everybodyknows 5 hours agorootparentprevInterestingly, this does not raise any error, rather has no effect. for i := range -10 { panic(i) } reply kristianp 7 hours agorootparentprevThanks. The new syntax emulates the python `for x in range(10):`. reply gocsjess 2 hours agorootparentprevfor i := range [10]struct{}{} { reply metaltyphoon 6 hours agorootparentprevWish you could do for i := range [3:10] { reply dgunay 4 hours agorootparentI think you could write something using the experimental Rangefunc feature that does that. reply icholy 3 hours agorootparentYou can, which makes me wonder why they added the range-over-int functionality when it could have been a function in the proposed iter package: for x := range iter.N(10) { ... } reply maximus-decimus 8 hours agoparentprev [–] While the phrasing is funny, it's really more \"we now have a direct equivalent to 10.times in ruby\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Go 1.22 is the latest release of the Go programming language, bringing improvements and changes across various aspects such as the toolchain, runtime, and libraries.",
      "Updates include enhancements to the trace tool's web UI, improved warnings in the vet tool, optimization in garbage collection, and reduced memory overhead.",
      "The release introduces new packages, updates to existing packages, and changes to packages like encoding/json, go/ast, and database/sql. Platform-specific updates are also included, such as position-independent executables on macOS and support for loong64 port and OpenBSD on big-endian 64-bit PowerPC."
    ],
    "commentSummary": [
      "The conversation revolves around programming languages like Typescript, Go, and Dart, discussing their advantages, challenges, and coding standards.",
      "Updates and changes in Go, such as the addition of \"sql.Null[T]\" feature and improvements in the standard library, are discussed and appreciated by the community.",
      "Participants share their experiences and opinions on language design and upgrading to newer versions, adding valuable insights to the conversation."
    ],
    "points": 256,
    "commentCount": 79,
    "retryCount": 0,
    "time": 1707261318
  },
  {
    "id": 39277990,
    "title": "Millions in Damages as 3M Infected Smart Toothbrushes Carry Out Swiss DDoS Attack",
    "originLink": "https://www.tomshardware.com/networking/three-million-malware-infected-smart-toothbrushes-used-in-swiss-ddos-attacks-botnet-causes-millions-of-euros-in-damages",
    "originBody": "Networking Three million malware-infected smart toothbrushes used in Swiss DDoS attacks — botnet causes millions of euros in damages News By Mark Tyson published 6 February 2024 Dental IoT devices caused millions of Euros in damages for Swiss company, says report. Comments (22) (Image credit: Philips) According to a recent report published by the Aargauer Zeitung (h/t Golem.de), around three million smart toothbrushes have been infected by hackers and enslaved into botnets. The source report says this sizable army of connected dental cleansing tools was used in a DDoS attack on a Swiss company’s website. The firm’s site collapsed under the strain of the attack, reportedly resulting in the loss of millions of Euros of business. In this particular case, the toothbrush botnet was thought to have been vulnerable due to its Java-based OS. No particular toothbrush brand was mentioned in the source report. Normally, the toothbrushes would have used their connectivity for tracking and improving user oral hygiene habits, but after a malware infection, these toothbrushes were press-ganged into a botnet. Stefan Züger from the Swiss branch of the global cybersecurity firm Fortinet provided the publication with a few tips on what people could do to protect their own toothbrushes – or other connected gadgetry like routers, set-top boxes, surveillance cameras, doorbells, baby monitors, washing machines, and so on. “Every device that is connected to the Internet is a potential target – or can be misused for an attack,” Züger told the Swiss newspaper. The security expert also explained that every connected device was being continually probed for vulnerabilities by hackers, so there is a real arms race between device software/firmware makers and cyber criminals. Fortinet recently connected an ‘unprotected’ PC to the internet and found it took only 20 minutes before it became malware-ridden. We don’t have the finer-grained details of the specific Swiss company targeted and suffered from the extremely costly DDoS attack. However, it is common for malicious actors to issue threats with monetary demands attached before weaponizing their DDoS zombie army. Perhaps the Swiss firm refused to pay up, or perhaps the malicious actors instigated this attack to show their muscle (teeth?) ahead of making any demands. Though we don’t have the finer details of the DDoS story, it serves as yet another warning for device owners to do their best to keep their devices, firmware, and software updated; monitor their networks for suspicious activity; install and use security software; and follow network security best practices. Stay on the Cutting Edge Join the experts who read Tom's Hardware for the inside track on enthusiast PC tech news — and have for over 25 years. We'll send breaking news and in-depth reviews of CPUs, GPUs, AI, maker hardware and more straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors By submitting your information you agree to the Terms & Conditions and Privacy Policy and are aged 16 or over. Mark Tyson Freelance News Writer Mark Tyson is a Freelance News Writer at Tom's Hardware US. He enjoys covering the full breadth of PC tech; from business and semiconductor design to products approaching the edge of reason. MORE ABOUT NETWORKING Amazon’s new AWS charge for using IPv4 is expected to rake in up to $1B per year — change should speed IPv6 adoption Super long-range Wi-Fi works at a range of 1.8 miles — HaLow standard aces a real-world test despite high interference LATEST BitLocker encryption broken in 43 seconds with sub-$10 Raspberry Pi Pico — key can be sniffed when using an external TPM SEE MORE LATEST ► TOPICS INTERNET MALWARE SEE ALL COMMENTS (22) 22 Comments Comment from the forums PEnns Somebody remind please, again: Why does something like toothbrush need to be connected?? People are really asking for trouble with this kind of \"let's connect everything...because it's so cool\"! Reply peachpuff PEnns said: Somebody remind please, again: Why does something like toothbrush need to be connected?? People are really asking for trouble with this kind of \"let's connect everything...because it's so cool\"! To get on the toothbrush leader board... duh. Reply Phaaze88 'Smart' toothbrushes... holy crap, humanity. Insert that saying about, 'because we can, doesn't mean we should'. Replace the word smart with dumb. Reply chaz_music In general if the product name has \"smart\" as part of its description, you should be very wary. The amount of engineering effort needed to make IoT devices truly secure on the Internet is substantial, and many times the engineering team is rather green and not knowing what they don't know. Add to this that many companies will outsource their product development to design groups only based upon cost of the project, you end up with catastrophes like this story. Even larger companies like HP have had problems with IoT printers and they had to go through growing pains to get the security right, with lots and lots of reuse of code, checks, etc. And most design teams are only cost focused, and don't want to add the cost of using more mature RF/networking products with the included code stacks such as by TI, Laird, Qualcomm, NXP, etc. So far, I have read about or myself found compromised devices in nearly all market areas: garage door openers, refrigerator, printers (why have Internet printing??!), smart doorbell cameras with off site recording, inexpensive network switches, smart LED lights (often color changing types), cars, RVs, phones (my goodness, that just makes you want to say damnit!), and now toothbrushes. And the hacked system vector is not always WiFi, as there are many other RF systems with another popular one to goof up being Bluetooth. The first automotive Bluetooth systems could be easily compromised, with one car type being used in a proof of concept in which the car was controlled by a passing car and the brakes were locked up while the car was traveling at highway speeds, triggering the anti lock brakes. And think about the Hyundai and Kia vehicles that can easily be stolen with a USB device. Same stuff. One of my biggest scare was not even with an RF based device but instead an Ethernet connected SCADA device from many years ago. It had a huge installed base, and it was sending data back on forth through the network using ... ASCII. Yep. And it was SCADA. Used in power plants, substations, transformers, generators, ... So the culprits are: 1. Businesses only counting R&D and BOM costs, with virtually no consequence for poor security quality. 2. Complacent and less knowledgeable engineers who are completely in charge of making serious decisions about cost vs. security. 3. Designing IoT tech into devices and leaving the update complexity up to the user. In my opinion, the user should never be required to be in the technology loop to make their devices safe. This is not the same as when it is used based upon common knowledge (driving a car, drinking hot coffee). The expected long term fix for industrialized nations is going to be more safety agency regulations, So think of UL in the US and CE/IEC in Europe. These protect the consumer from poorly designed products, but these always add cost (no free lunch). I hate going in that direction because it will cause many clever products to go away, and others to never come to market. Reply Murissokah Not trying to pick on Java, but why do you need Java on a toothbrush? Reply Giroro Murissokah said: Not trying to pick on Java, but why do you need Java on a toothbrush? That ones easy: Because it's cheaper to have first-year computer scientists ridiculously overbuild the system with off the shelf demo code than to hire electronics engineers who know how to write efficient firmware. The toothbrush probably has (and maybe needs) a multi-core ARM CPU as well, because you can just pass that extra $1 in hardware costs off to the customer in the $300+ asking price I know Philips/Oral-B charges for the smart version of a toothbrush with near identical brushing performance to the $30 non-smart version. Reply newtechldtech PEnns said: Somebody remind please, again: Why does something like toothbrush need to be connected?? People are really asking for trouble with this kind of \"let's connect everything...because it's so cool\"! to sell them expensive 10 times the cost. it is all abut the $ and fooling the masses Reply Giroro I sort-of understand how a marketing executive could want the company to sell a Bluetooth toothbrush. App tracking enabling access to a customer's sellable information, a branded billboard app icon on the users phone, etc etc. All the usual reasons to have an app. You can sell it to customers as having a fancy timer or whatever. I kinda get it. But why in the world would they pay engineers to enable wifi in the thing? It's probably built into their SoC, but like this has to be enabled by accident, right? This is some kind of backdoor thing? What's the selling point, revenue stream, or perceived value to the customer? You already have all you can get from Bluetooth, so why spend money on dev time to add in menus and get the wifi working? Reply voodoochicken Watch out for those IoT Swiss Army Knives Reply evdjj3j Smart toothbrushes for dumb people. Reply VIEW ALL 22 COMMENTS Show more comments MOST POPULAR China chipmaker SMIC on track to produce sanctions-busting 5nm processors for Huawei this year: Report By Anton ShilovFebruary 06, 2024 Orange Pi enters handheld PC space with Orange Pi Neo, a Ryzen 7840U-powered handheld gaming device By Christopher HarperFebruary 06, 2024 Researchers design a processor from DNA — microfluidic chip completes math calculations and also stores data in DNA By Christopher HarperFebruary 06, 2024 TSMC is now the world's largest semiconductor maker by revenue, beating Intel and Samsung: Analyst By Anton ShilovFebruary 06, 2024 $10,000 for a $3,500 Apple Vision Pro? Scalpers mark up Apple's headset, despite the fact that it's still in stock By Andrew E. FreedmanFebruary 05, 2024 Nvidia's flagship RTX 4090 gets a green and gold makeover By Zhiye LiuFebruary 05, 2024 Sharkoon unveils high-airflow ATX mid-tower wood case with 11 fan mounts By Aaron KlotzFebruary 05, 2024 PCIe 6.0 over optical cables demonstrated in custom data center solution By Anton ShilovFebruary 05, 2024 International Space Station gets Kioxia SSD upgrade for edge computing and AI workloads -- HPE Spaceborne Computer-2 now packs 310TB By Roshan Ashraf ShaikhFebruary 05, 2024 Raspberry Pi project lets you generate AI art for your TV using voice commands By Ash HillFebruary 05, 2024 Nvidia feature that converts SDR games to HDR uncovered by modder -- RTX TrueHDR settings found in latest Game Ready driver By Aaron KlotzFebruary 05, 2024 TOPICS INTERNET MALWARE",
    "commentLink": "https://news.ycombinator.com/item?id=39277990",
    "commentBody": "Three million malware-infected smart toothbrushes used in Swiss DDoS attacks (tomshardware.com)254 points by dist-epoch 15 hours agohidepastfavorite162 comments tjasko 12 hours agoThis article is strange & many details are lacking. All the big smart toothbrushes use BLE and are not WiFi-connected. Tried to fact-check the article, but nothing. A bunch of BLE chips are also WiFi capable, so not ruling out that someone compromised the firmware to enable WiFi functionality, but I wonder how they were able to connect to WiFi to trigger a botnet in the first place. Quite skeptical of this article, while the premise of the danger of IoT devices still remains, nonetheless. reply smashed 10 hours agoparentI tried to fact check it also. They talk about a \"java-based\" os that could have been the cause. I know java me was a thing and there are micro jvm that can run on microcontrollers but still, it does not add up. I think a DDoS attack happened (happens all the time) and security \"experts\" mentioned that these things could come from anywhere, even toothbrush, and the details got lost in translation / used for click bait. reply a321neo 12 hours agoparentprev>A bunch of BLE chips are also WiFi capable, so not ruling out that someone compromised the firmware to enable WiFi functionality The ESP32 is now used as a general-purposed chip even in applications where an 8-bit MCU would have been enough. A remotely exploitable vulnerability in the ESP32/SDK could have large-scale consequences. reply greggsy 10 hours agorootparentThe only way to load firmware to consumer esp platforms is usually via mobile apps… so, someone with privileged access to consumer’s apps, or the supply chain, used that access to load bespoke firmware to toothbrushes.. highly doubtful. reply exe34 11 hours agorootparentprevLeaves open the question of how they joined the network - WiFi passwords and such. Maybe stolen from the phones/laptops and then sent to the device as part of the exploit? reply graypegg 11 hours agorootparentI could imagine there’s a lot of toothbrushes near unsecured wifi hotspots. (Hotels, in backpacks of travellers in a cafe, a demo unit in a store) Could be as simple as polling continuously till one allows the device to phone home. This does seem to be a debunked story though. reply depereo 11 hours agoparentprevIt's not something that actually happened. It's just some bullshit that's gone viral. https://cyberplace.social/@GossiTheDog/111886558855943676 reply tgsovlerkhgsel 10 hours agorootparentThat toot references https://archive.is/2024.01.30-203406/https://www.luzernerzei... which attributes the story to Stefan Züger from the Swiss branch of Fortinet and claims it to be an actual event. I don't see a mention of \"NoName Ddosia\". reply rakslice_ 10 hours agorootparentRe \"Noname Ddosia\": It's from the context, if you know your recent infosec history: \"Jüngst wurden damit auch Server von Schweizer Regierungsstellen während des Weltwirtschaftsforums angegriffen – als Retourkutsche für die Teilnahme des ukrainischen Präsidenten Wolodimir Selenski. Eine russlandnahe Gruppierung bekannte sich zum Angriff.\" (translation: \"Servers of Swiss government offices were recently attacked during the World Economic Forum - as a retaliation for the participation of Ukrainian President Volodymyr Zelensky. A group close to Russia claimed responsibility for the attack.\") Background: https://www.ncsc.admin.ch/ncsc/en/home/aktuell/im-fokus/2023... But it's not clear to me that's right, isn't WEF in the summer? Ah, they've been ongoing, here's the earlier one: https://www.ncsc.admin.ch/ncsc/en/home/aktuell/im-fokus/2023... reply tschoesi 49 minutes agorootparentWEF is in winter reply fiprisoner 39 minutes agorootparentUsually in January, 2021 was in August and 2022 in May. reply croon 10 hours agorootparentprevI can't read dutch, but machine translated to english the \"original source\" as referenced in your link says this: > The example that comes like a Hollywood scenario has really happened that way. reply greenyoda 6 hours agorootparent> I can't read dutch, but... It's not Dutch, it's German, so trying to translate it as if it were Dutch would give strange results (although Google Translate tried to do it anyway, and came fairly close, due to the languages being somewhat related). The original sentence: \"Das Beispiel, das wie ein Hollywood-Szenario daherkommt, hat sich wirklich so zugetragen.\" Translating from German, Google Translate gave me: \"This example, which seems like a Hollywood scenario, actually happened.\" Translating, as if it were Dutch, Google Translate gave me: \"The play, the one who comes to a Hollywood theater, is such a work in itself.\" reply tauchunfall 52 minutes agorootparent>It's not Dutch, it's German, so trying to translate it as if it were Dutch would give strange results Anecdote: One of my coworkers once thought that Dutch means German. I guess it was because the words \"dutch\" and \"deutsch\" look so similar. reply n2d4 9 hours agorootparentprevHere's a better translation: > This example, which seems like a Hollywood scenario, actually happened. reply Cpoll 11 hours agoparentprev> but I wonder how they were able to connect to WiFi to trigger a botnet in the first place. Wardriving for oral health? reply Twirrim 14 hours agoprevThat's a really flimsy article. Someone is claiming 3 million smart toothbrushes were used in a DDoS, but no one is talking what/who/how. That seems like the kind of extraordinary claim that requires at least some kind of evidence. There is surely at least some technical details that enabled them to identify the toothbrushes, right? reply blacksmith_tb 14 hours agoparentIt also seems odd that even if you (maybe unknowingly) connected your 'smart' toothbrush to wifi, it would be exposed to the public internet. Aren't most people using some kind of clunky cable modem etc. from their ISP, which would have a basic inbound firewall? reply Retr0id 13 hours agorootparentThere's lots of ways for this expectation to be broken. The most obvious is UPnP, where the device can ask the gateway router to forward ports. The second is the fact that devices on the LAN are accessible to other devices on the LAN. Malicious JS in a webpage can scan for and compromise other local devices. And the third is the fact that whatever serves code to the toothbrush (whether it's firmware updates, or an HTML5 dashboard) can be compromised. In the latter case, it could be something as simple as persistent XSS. reply gtirloni 13 hours agorootparent> The second is the fact that devices on the LAN are accessible to other devices on the LAN. Malicious JS in a webpage can scan for and compromise other local devices. Which browser API enables that? reply metadat 13 hours agorootparentI was skeptical at first, but did some superficial scouting.. it's trivial for a malicious website to do nasty things to any internal resource which doesn't have a strict CORS policy. https://security.stackexchange.com/questions/177486/can-webs... As the adage goes, the \"S\" in IOT stands for \"Security\". reply mewpmewp2 37 minutes agorootparentBut CORS is strict by default. They must specially add headers to allow such requests. reply Retr0id 13 hours agorootparentprevYes, I have (non-public) variations of the https://rootmy.tv/ exploit that can fully compromise an LG smart TV from the browser session of any other LAN-adjacent device. reply Retr0id 13 hours agorootparentprevHTTP, it's all the rage these days. (via , fetch, XMLHttpRequest, et al) reply gtirloni 13 hours agorootparentAh ok, so we are talking about dumb old methods. I thought it was something like the fancy APIs that are all the rage these days. reply Retr0id 13 hours agorootparentThere was a brief window when people knew that if they used non-HTTP protocols, then malicious webpages couldn't talk to it. But now even \"native\" apps are web apps, and IoT devices all use web APIs too. They can be locked down through CORS etc., but it's easier for devs to set `Access-Control-Allow-Origin: *` and worry about it \"later\". reply kevincox 10 hours agorootparentIn most cases `Access-Control-Allow-Origin: ` is actually a decent policy. It importantly blocks cross-site credentials. So as long as your API has any* authentication it should prevent it. The real mistake is mirroring the Origin header from the request in the `Access-Control-Allow-Origin` response header which allows credentials (unless you add other headers) Of course this all relies on you not accepting form posts without auth. reply blacksmith_tb 13 hours agorootparentprevI suppose you could just loop through all the IPs for some common ranges like 10.0.0.0/16 and 192.168.0.0/16 looking for a given port, if you knew the toothbrushes exposed it and there was something exploitable there, that makes sense. reply metadat 13 hours agorootparentEven 192.168.1.0/8 will probably get you ~95% coverage for residential networks. reply Retr0id 13 hours agorootparentIt's even easier if the device has assigned itself a \"toothbrush.local\" hostname via mDNS etc. reply gnabgib 9 hours agorootparentprevDid you mean /24? As low as /16 is valid, but /8 includes plenty of public addresses. reply metadat 9 hours agorootparentThanks, you're right! reply d1sxeyes 14 hours agorootparentprevHypothetically, let’s say these toothbrushes connect periodically to an API from which they fetch firmware updates. If you’re able to MitM that connection, you could deliver whatever you like as a firmware payload to the toothbrush. Or maybe someone designed the toothbrush to open ports using UPNP to enable a remote connection to tell the toothbrush that the update server has moved to a new URL? reply zoeysmithe 13 hours agorootparentprevA lot of home, small business, or neglected enterprise routers and firewalls are broken into permanently. Many of these will not auto-update their firmware or the attackers got in before the patch was available. Then the initial actor sells access to them to other actors. I believe the Ubiquity Edge router, a small/medium/AV industry favorite, was paired with other exploits by a state actor to perform attacks on high value orgs. reply Retr0id 13 hours agoparentprevI'd like to see more details too, but it's not that extraordinary in my opinion - par for the course for low-cost wifi-enabled appliances. reply rkagerer 12 hours agoprevI have an older Phillips toothbrush without Bluetooth, Internet or vendor-locked heads, and it charges wirelessly in a glass cup. I love it. I recently tried to buy a second one and could only find newer models with all these garbage features I don't want. Who the hell wants their toothbrush to connect to the internet? Wound up turning to eBay to find stock of the old one. It might sound cruel, but I hope the moron who decided to add these features into their product, and the lackey who implemented it, are having a bad day and reflecting on the wisdom of what they did. reply jhbadger 12 hours agoparentWifi is silly, but there really is a benefit to the Bluetooth/app connection -- it is used to see where you are brushing and spots you are missing. My dentist definitely has seen an improvement in the plaque in my back teeth since I started using a smart toothbrush that uses an app on my phone. reply kwhitefoot 11 hours agorootparent> spots you are missing Just brush each tooth systematically. My dentist tells me \"Just keep doing what you are doing.\" I have the cheapest Braun Oral-B with a two minute timer. I've worked out by trial and error that that is about the time to stroke each face of each tooth about twelve times. Now I do that even if it takes a bit longer than two minutes because I occasionally brush slower. reply progbits 11 hours agorootparentprevHow does it know the location you are brushing? reply quenix 10 hours agorootparentI would presume a bunch of IMUs and fusing together orientation/accel data? I can see how knowing the orientation and relative motion of the head might allow one to map this out. reply progbits 9 hours agorootparentI assume you mean the head of the toothbrush but what about my head? The IMU data is relative to earth (kinda). But toothbrush position over teeth is relative to my head. Moving around and turning your head gets added to the IMU measurements with no way to tell it apart. Did I turn my head/body 90 degrees left, or did I move the toothbrush 90 degrees from my front teeth to the rear left ones? Impossible to tell these apart. reply zelphirkalt 11 hours agorootparentprevBluetooth from the teeth! It is in the name ; ) reply jhbadger 9 hours agorootparentHa! But presumably the bluetooth is there to tell the app what it measured and to visualize it. But presumably through a combination of sensors and accelerometers reply zzyzxd 14 hours agoprev> Though we don’t have the finer details of the DDoS story, it serves as yet another warning for device owners to do their best to keep their devices, firmware, and software updated; monitor their networks for suspicious activity; install and use security software; and follow network security best practices. Maybe they should only allow qualified consumers with required certification to purchase such a smart toothbrush. reply abdullahkhalids 14 hours agoparentThe call if of course on device owners, and not device manufacturers whose responsibility it truly is to manufacture secure devices. reply d4mi3n 14 hours agorootparent100% agree, but I have to wonder how much of the problem is that the cost of security is: A. Not mandated B. Increases cost of the product At what point would people just prefer a regular toothbrush if a smart one doesn't provide enough utility to justify the cost? This isn't specific to toothbrushes, but I wonder what products or services wouldn't exist if they were made to be secure (or safe/ethical/sustainable/etc). Makes me wonder how many existing externalities are causing hard to measure problems that could be prevented by making a higher quality product. reply abdullahkhalids 10 hours agorootparentOf course it's because security is not mandated. In the past, and in the present, there are many companies who are willing to kill people for the sake of profits, if they can get away with it legally. IMHO, any commercial violations of human rights, like privacy, should have criminal penalties. reply accrual 14 hours agoparentprev> monitor their networks for suspicious activity Indeed. It's asking the same people who only know their router as a box where the internet comes from to run a packet capture and interpret the results. reply 082349872349872 15 hours agoprevI had misremembered these as a single comic, but you can't have everything: https://i0.wp.com/www.litterboxcomics.com/wp-content/uploads... https://i0.wp.com/www.litterboxcomics.com/wp-content/uploads... reply mckn1ght 13 hours agoparentSo do I drink my verification can before or after brushing? reply shrimp_emoji 12 hours agorootparentBefore. Otherwise, you're washing down all the fluoride instead of giving it time to bind to your enamel via chemical API calls. reply girvo 11 hours agorootparentExplains why I went into programming after doing a BSc in Chemistry. Just a different kind of API! reply dist-epoch 14 hours agoparentprevThis also good: https://i.imgur.com/YnBnsKA.jpeg reply kps 14 hours agorootparent“The door refused to open. It said, “Five cents, please.” He searched his pockets. No more coins; nothing. “I’ll pay you tomorrow,” he told the door. Again he tried the knob. Again it remained locked tight. “What I pay you,” he informed it, “is in the nature of a gratuity; I don’t have to pay you.” “I think otherwise,” the door said. “Look in the purchase contract you signed when you bought this conapt.” In his desk drawer he found the contract; since signing it he had found it necessary to refer to the document many times. Sure enough; payment to his door for opening and shutting constituted a mandatory fee. Not a tip. “You discover I’m right,” the door said. It sounded smug. From the drawer beside the sink Joe Chip got a stainless steel knife; with it he began systematically to unscrew the bolt assembly of his apt’s money-gulping door. “I’ll sue you,” the door said as the first screw fell out. Joe Chip said, “I’ve never been sued by a door. But I guess I can live through it.” — Philip K Dick, Ubik, 1969 reply throw0101b 14 hours agorootparentMore recently see Cory Doctorow's \"Unauthorized Bread\": > The toaster wasn’t the first appliance to go (that honor went to the dishwasher, which stopped being able to validate third-party dishes the week before when Disher went under), but it was the last straw. She could wash dishes in the sink but how the hell was she supposed to make toast—over a candle? * https://arstechnica.com/gaming/2020/01/unauthorized-bread-a-... * From: https://en.wikipedia.org/wiki/Radicalized_(Doctorow_book) reply dylan604 13 hours agorootparentThis would funny but since it's pretty much exactly how printers behave it's more just a slap in the face reply znpy 11 hours agorootparentI guess it's time to echo the meme: \"The band 'Rage against the machine' does not explicitly says what kind of machine they are enraged to, but I'm pretty sure it's a printer\". EDIT: screen of the original tweet: https://old.reddit.com/r/printers/comments/vqmbu4/rage_again... reply whoisstan 14 hours agorootparentprevWarren Ellis at Thingscon 2017 \"1. It’s hard. Don’t get me wrong. I know it’s hard. And Samsung and Apple and several other large corporations want in on it. On the bright side, that will give you lots of exit opportunities, and soon you could be drinking cocktails in Bali while Amazon deals with the backlash from the smart doorlock you sold them that still doesn’t work properly. And they’ll spend the money on iteration until the device either goes away or starts working properly, and the users will have to buy Amazon Prime membership for their houses. And then someone will hack your house through the buggy wifi thermostat you bought, and your house will start ordering DOWNTON ABBEY downloads and you’ll come home to find it’s 40 Celsius indoors and the sink is flooded and your fridge has been turned into a porn spambot and you’ll realise that your house is masturbating to DOWNTON ABBEY. If you can get in the front door.\" reply renewiltord 12 hours agorootparentprevWell, now we're straying from the original, but there's the libertarian copypasta on Reddit: https://www.reddit.com/r/copypasta/comments/7iqxko/libertari... > I was shooting heroin and reading “The Fountainhead” in the front seat of my privately owned police cruiser when a call came in. I put a quarter in the radio to activate it. It was the chief. > “Bad news, detective. We got a situation.” > “What? Is the mayor trying to ban trans fats again?” > “Worse. Somebody just stole four hundred and forty-seven million dollars’ worth of bitcoins.” > ... and so on. reply renewiltord 8 hours agorootparentI've later found that this is a New Yorker humor piece https://www.newyorker.com/humor/daily-shouts/l-p-d-libertari... That makes sense. Their fiction was always top notch when I subscribed. reply greenyoda 6 hours agorootparentWhen Michael Bloomberg was the mayor of NYC, he did actually ban trans fats from NYC restaurants: https://en.wikipedia.org/wiki/Michael_Bloomberg#Political_po... reply dessant 14 hours agoprevA warning about Philips electric toothbrushes: you cannot turn off Bluetooth on them, even if you are not using the smart features. Also be careful with all Philips air purifiers that support Wi-Fi, because the remote control feature cannot be disabled. They create a Wi-Fi hotspot that you need to connect to with a smartphone to finish setting up the device, but if you don't use these features, the air purifier will create a permanent Wi-Fi hotspot, waiting to be exploited. reply UberFly 13 hours agoparentI'm reminded of this that I read a few days ago: Home assistant picked up my neighbours Bluetooth toothbrush and now I can see when they brush their teeth. https://old.reddit.com/r/homeassistant/comments/1306pcw/home... reply radicality 3 hours agorootparentThe exact same thing happened to me! Randomly one day a new toothbrush entity appeared in HA, even though I’m still using a “dumb” electric toothbrush. reply Animats 12 hours agorootparentprevSend them a message if they miss a brushing. reply somedude895 10 hours agorootparent\"You shouldn't stay up that late you know\" reply burningChrome 13 hours agoparentprevI finally got rid of one of my fitness watches that had dreadful battery life and I couldn't figure out why. After a few months of this, I finally realized the same thing, you can't turn off the bluetooth on it. The app on your phone and the watch are constantly searching for each other to always sync and the alternative is to unpair the watch, use it, re-pair, sync and go which became a total headache, but did in fact give me better battery life. The weird thing is I complained to the company's CSR people online and they had no idea why the battery was so bad and just told me to try and factory hard reset the phone as there must be something I changed in the settings. I switched over to Polar and now the watch I have lasts 5 days on a single charge - quit the change from about a day or less. reply II2II 9 hours agorootparent> I switched over to Polar and now the watch I have lasts 5 days on a single charge - quit the change from about a day or less. I uncovered a cheap digital watch in the cupboard the other day. It hasn't been in use since it's strap broke at least four years ago. It is still keeping time. Poorly, granted. It is off by half an hour, Then again, it is the type of watch that needs updating twice or thrice a year to account for DST and leap years. I realize that modern watches are much more than timepieces, but the difference is battery life is astounding. reply LeifCarrotson 12 hours agorootparentprevI've been using Garmin GPS watches for more than a decade, they get two weeks on a single charge (double or triple that if you don't use 24/7 heart rate, or GPS, or Bluetooth/Wifi, but even on long trips I don't need months without a charge). And they have Bluetooth that syncs with my phone for weather data and optionally shows notifications, but it doesn't need a phone connection to be a great watch. Sure, my top-end Fenix 6 Pro cost $750 new in 2019, and very little of that is hardware BOM (there's a lot of price segmentation), but it's still just as good as it was then. It's honestly extremely refreshing to deal with a company and an app that tries to build and sell good hardware rather than tricking you into a subscription. reply throwway120385 12 hours agorootparentI've gotten 5-7 days out of a charge with my entry-level Vivoactive 3 even 4 years later. They're very good. reply inglor_cz 12 hours agorootparentprevMy Garmin stays connected to my Samsung smartphone via Bluetooth constantly and will last about 6-8 days on a single charge. I can't imagine charging my watch every night. reply manicorganic 4 hours agoparentprevSince we're on the subject, also be careful of Philips CPAP machines, they will slowly spray disintegrating cancer-causing foam into your lungs as you sleep. Great company though, it's not like they had the choice to not buy out one of the best CPAP manufacturers and then skimp out on materials until they hit the cancer recall margin of diminishing returns (and then hide it for as long as possible). reply dmix 13 hours agoparentprevWhat risks could a WiFi hotspot on an air purifier expose if it's not connected to the network or a computer? reply dessant 13 hours agorootparentAnyone in Wi-Fi range can exploit the device. The sensors of the air purifier can be used for spying, and the device could also serve as a hopping point for exploiting other devices in your home. reply mynameisvlad 13 hours agorootparent> The sensors of the air purifier can be used for spying To be able to... know if your target's house has a lot of pollutants? Is particularly warm? There is practically no useful information that can't be gleamed by just looking through their windows, blinds and all. > and the device could also be used as a hopping point for exploiting other devices in your home. It's not connected to your home network, that's the whole reason for the hotspot existing. How, exactly, could it be used as a hopping off point, except to other devices with hotspots that... can just be exploited in the first place. reply snapcaster 12 hours agorootparentYou're lacking in imagination, and maybe the conceptual idea of \"sensor fusion\". Multiple seemingly innocuous data streams in isolation can be combined to create sensors you wouldn't have imagined reply mynameisvlad 11 hours agorootparentDo you understand what data is available in a smart air purifier? Please, explain exactly what sensor fusion would get you actionable data out of the PM2.5 sensor and \"gas sensor\" in a Philips smart air purifier. reply jamesrr39 10 hours agorootparentAt a guess; if able to monitor over a period of time (e.g. pick up data from a parked car), a potential burglar can see when there is activity and figure what times of the day house occupants are normally at home. reply mynameisvlad 10 hours agorootparent> that can't be gleamed by just looking through their windows, blinds and all. I mean, sure, but who is going to do that when they can... look to see when people are home. reply jamesrr39 9 hours agorootparentMore subtle; the burglar could just park up and go off for a few hours and gather the data they need - no need for a suspicious camera pointing out of the car to monitor patterns. If the burglar only takes a 30 second look before breaking in, residents could be home but away from a window, with this the burglar can more confidently know when is a good time to break in, without exposing themselves to the same risk that looking around the house brings. reply dotancohen 10 hours agorootparentprevMaybe increased CO2 on Tuesday afternoons will tip off that the wife is cheating? It's not even far fetched, smart watches reporting physical actively at unexpected hours have revealed infidelity in the past. reply cromka 10 hours agorootparentprevJust a mere few years ago you wouldn’t believe WiFi access point can be used as a sonar to literally scan the area like a low-def camera in real time. Stuxnet also sounded like a completely made up scenario. As someone said, you lack imagination. And that’s OK, but you’re also being quite arrogant, too. reply yread 11 hours agorootparentprevIf the sensors don't detect your farts for a while you're probably not at home so the burglars can come in reply rightbyte 13 hours agorootparentprevWorst case would be a fire hazard. Maybe produce too much poisonous ozone. If the hardware is fail safe I guess it can waste electricity. reply LesZedCB 13 hours agorootparentprevyou could believe you're inhaling purified air but, lo! you are breathing impure air, muahahaha! reply kps 13 hours agorootparentYou may think you're joking, but 4 days ago: https://news.ycombinator.com/item?id=39223982 reply whyenot 13 hours agoparentprevYou might not be able to turn bluetooth off, but you can choose not to pair them with anything (or remove the pairing after setting up the device). reply dessant 13 hours agorootparentThe issue is what happens to these toothbrushes in a couple of years when their vulnerabilities will be discovered. Their inevitable exploitation could be prevented by simply allowing to turn off bluetooth. Or even better, only enable bluetooth if the user wants to set up and use these smart features, at least in that case the vulnerable firmware can be updated using the smartphone app. reply ethbr1 13 hours agorootparent\"Shipped dumb by default\" is enticing as a legal requirement. Have a colorful switch to enable it, whatever. But poor security posture out of the box, for a questionably-supported, poorly-developed, long-lived physical device seems important enough to mandate slight one-time inconvenience. In the future, this bullshit is going to be looked back at like default passwords on ISP WAPs. reply HnUser12 11 hours agoparentprevSame with my samsung tv and my neighbour keeps trying to pair her watch to it for reasons I don’t know. reply SoftTalker 11 hours agorootparentShe most likely doesn't know either. reply true_blue 15 hours agoprevWhy do toothbrushes need to be able to make web connections in the first place? I get that it's for tracking brushing habits, but can't that be done with local connectivity only, like LAN or something? reply soco 15 hours agoparentNot every toothbrush user has a server at home and the skills to attach to it. I would even say that most of those users had no idea what they enabled when they activated their toothbrushes. And let's not forget about vacuum cleaners, refrigerators, washing machines, coffee makers and the other zillions of \"smart\" personal data channeling smart appliances. I'd dare a survey, how many HN people actually work on exactly these technologies, how many read these words, and how many actually care? reply samatman 14 hours agorootparentI have several gizmos which use Bluetooth. They're a little bit slower to connect to than the WiFi ones, but they work fine, and \"a bit slower to connect\" seems fine for a toothbrush. I also have several gizmos, including lightbulbs, which use WiFi. To my chagrin, I've had internet outages which meant that I can't turn on a given light until the Internet comes back. I put up with it, because telling my computer to change the lights is too much fun, but when the internet goes out, I'm embarrassed both personally and professionally. Somehow we've failed as a profession to provide people with a home network which continues to function as long as the router has power, and that sucks. reply dunham 13 hours agorootparent> Somehow we've failed as a profession to provide people with a home network which continues to function as long as the router has power, and that sucks. This already existed for lightbulbs in the 70's: https://en.wikipedia.org/wiki/X10_(industry_standard) Wikipedia says the computer interface was 80's, but if you managed to have a computer in the seventies, you probably knew enough electronics to homebrew something. reply samatman 13 hours agorootparentYeah, we've invented it several times over, and yet, what people buy and use is IoS crapware which craps out when the network does. That's worse. You see how that's worse, right? reply dunham 12 hours agorootparentyeah, everything keeps getting reinvented worse or made worse by adding unwanted, poorly implemented features. My unstated point was that a version existed decades ago which was more robust than the new, reinvented version. I'm not sure that people (in general) want these things. It seems like product managers adding stuff to justify their existence and people buying what they find on the shelf. You get an internet connected oven because you have no choice anymore. (Hyperbole, but the non-internet choices are narrowing.) Maybe people want to change the color of their lightbulb (I'm guessing it gets old quick), but I suspect they're not asking for it to be on the internet. reply samatman 11 hours agorootparentI find it a genuine quality-of-life improvement to adjust the color of light. The temperature matters more, but being able to do strong hues is really nice. Not everyone is into mood lighting, but I like it. And I don't care as much about whether or not the bulb uses IP to reach my phone, but why should my outside connection going down ever matter? As long as the router has power, the internal network should continue to function. It's a shame is what it is. I figure I could put in the sweat to make it \"work on my machine\" but that doesn't solve Joe Normal's problem, and it doesn't sound like a fun hobby to me either. reply vel0city 10 hours agorootparentprevSeparate access points from the router are a thing, and if the command and control for the lights are local they'd continue to work. People just mostly choose to go with a single integrated unit instead of a router, a switch, and one or more access points. reply kwhitefoot 11 hours agorootparentprevJust have the toothbrush run a web server and then the user can point a web browser at it. It can also come with a mobile app that would scan the local network looking for the device in order to discover the IP. reply nonrandomstring 14 hours agorootparentprev> I'd dare a survey, how many HN people actually work on exactly these technologies, how many read these words, and how many actually care? This is an excellent question. We'd likely find that there is an enormous disconnect between high IQ, well educated engineers and high emotional and social intelligence. The perennial excuses; \"it's just a job\" , \"everybody's doing it\", \"if I didn't buildthen someone else would\" ... these have grown tiresome and weak. Everybody now knows these are stupid and dangerous things we are doing. Is there a kind of fatalistic malice at work? How do people who work on this kind of thing manage the dissonance? reply jprete 15 hours agoparentprevBecause the actual business model is selling the aggregated data? reply Gigachad 14 hours agorootparentWhat data though? How would it be valuable? From what I saw they are getting money from the device sale itself. These iot toothbrushes are like $400 and basically just track brushing time and pressure. Those don't seem like super valuable ad tracking metrics. reply thaumaturgy 13 hours agorootparentThis might be a fun exercise. Let's assume we have the following data: the user's email address, some sort of smartphone identifying value, their ip address, and their brushing habits. That's not very much; who would want that? Well, we know this is a person who will drop $400 on a toothbrush. They like shiny things, they have at least a middle-class disposable income, and they don't mind the headaches of internet-connected devices. Let's sell this information to big-box electronics retailers and other smart appliance manufacturers. Maybe this person would like to buy a $500 toaster too, or espresso machine, or soda machine, or bread machine, or microwave. They care a little bit about oral hygiene. Have they seen a dentist lately? If they have $400 for a toothbrush, then they probably have better than average dental insurance. Let's also sell their information to the larger dental offices in their area (as determined by IP). Do they need mouthwash? Let's pop up an ad for a subscription mouthwash service. How about floss? Would they perhaps also appreciate a razor made out of aerospace titanium? Oh, but wait ... their IP address just changed, and they are brushing their teeth 3 hours later than typical. They're traveling! They're traveling and they took their expensive toothbrush with them. This opens up an entirely new set of possibilities. Travel insurance? A credit card with travel incentives? New luggage? How about offers for travel upgrades? There are hundreds of companies paying for the opportunity to contact pre-qualified customers that travel with disposable income. Oh, wait ... they just bought a set of lightbulbs that we also make... reply Gigachad 14 hours agoparentprevI was at the store looking at them recently and all the toothbrushes advertise having \"AI\", an app, wifi/bluetooth etc. I guess it's hard to come up with reasonable upsells on this stuff. reply pmontra 15 hours agoparentprevI'm with you, but unless the brush stores the data on itself, which appliance should receive those data in a typical home? reply IncreasePosts 15 hours agorootparentThe users phone, via a Bluetooth connection? reply Gigachad 14 hours agorootparentIt's possible but its really unreliable. A device trying to reach out to an app on your phone to proxy the data while your phone is sleeping/app not running just doesn't work that well. You don't want to have to open the app while using the device, you just want all the data to be there when you look in a week. These devices almost always have wifi since the chips usually have both anyway. And reaching out to a fixed wifi is so much more reliable. reply thomastjeffery 12 hours agorootparentIf you have enough room to store WiFi credentials, then you probably have enough room to store toothbrush use statistics. There is no need to copy that data to a phone immediately. It can be put off until it's convenient. reply Gigachad 12 hours agorootparentAnd then the user goes out for the day, opens up the app, and wonders why the last 3 days of data is missing. Meanwhile the chip that does Bluetooth also just has wifi bundled in. Aside from the security risk, directly connecting to wifi is a vastly superior experience. reply kwhitefoot 11 hours agorootparentHow much data can a toothbrush collect? Surely just a few hundred bytes per brushing session. The ESP32 has 160 kB of usable RAM out of the 520 kB total capacity. Surely enough for weeks of data even if the data structures are badly designed. reply Gigachad 10 hours agorootparentThe problem is you can't reliably collect it over bluetooth. I doubt iOS allows apps to just run always active background tasks to fetch data over bluetooth while the app is not open. The user expects the data on the app to always be in sync. If they check it outside of bluetooth range and see days of data is missing, it'll look broken. reply thomastjeffery 10 hours agorootparentIt'll look however the UI designer made it look. The right answer here is to show them a \"last synced\" timestamp and a sync button. reply jawns 15 hours agorootparentprevWhat's wrong with \"the brush stores the data on itself\"? Plenty of consumer products do that. reply MichaelMoser123 12 hours agoprevStanislav Lem wrote the \"Washer Tragedy\" where washing machines got smarter and were taking over. I think he would have been proud of these toothbrushes... reply ano-ther 15 hours agoprevMy theory is that every technology goes through a period of experimentation before it’s clear how it should be employed. That’s why we had project plowshare for the bomb and now internet connectivity for every imaginable device, even ones that only need an on-off switch. I am a bit mystified why we need connected toothbrushes, but I very much applaud the spirit of experimentation, even if it sometimes gives us toothbrush-powered botnets. reply nonrandomstring 14 hours agoparent> every technology goes through a period of experimentation before it's clear how it should be employed. Sure, but as I said here yesterday [0] experimentation is something that has far reaching consequences, and that's why professional scientists have codes of ethics that seem quite absent in the tech world. Also, as far as the Internet goes, we've had maybe 40 years of time to \"experiment\". There comes a time for results, conclusions and some sort of maturity in outcomes. [0] https://news.ycombinator.com/context?id=39253045 reply Levitating 15 hours agoparentprevIs it really experimentation or just slapping a higher price tag onto a device for added wifi connectivity. How do we sell our product X for more? Just add wifi and AI. You can do it with almost anything. reply 082349872349872 15 hours agoparentprevFirst they came for *The Onion* And I did not speak out For I was not an Onion writer Then they came for *Black Mirror* And I did not speak out For I was not a Mirror writer Then they came for Horselover Fat... The sanity is already here – it's just not evenly distributed. reply axus 14 hours agoparentprevNo need to teach your children honesty, when you can just spy on their toothbrushes. reply thomastjeffery 12 hours agoparentprevWe're not taking about experimentation, though. We're talking about a fully hashed-out business model with its own casual acronym (SaaS). The \"why\" is obvious: there is a market for any aggregate data on human behavior. reply Freebytes 14 hours agoprevI dread the inevitable \"Internet dildo wars of 2037\" where millions of networked dildos and refrigerators wreaked havoc on the entire Internet causing billions in damage. \"Suspects remain at large.\" reply kramerger 2 hours agoparentDidn't we already have a smart dildo incident in 2022? Something about the sound recordings made by the accompanying app leaking? reply RajT88 14 hours agoparentprev\"ChatGPT-enabled Internet-Connected Dildo\" is a devastating insult for internet commenters. reply maxglute 1 hour agoprevHow many bitcoins can three million toothbrushes mine? reply anonymouskimmer 15 hours agoprev> Normally, the toothbrushes would have used their connectivity for tracking and improving user oral hygiene habits Along with that thread on the Folk computer the other day (https://news.ycombinator.com/item?id=39241472 ), and a discussion on signal interference in long-range wifi and the like (https://news.ycombinator.com/item?id=39246399 ) this makes me wonder if broad household surveillance centralized to a single computer per home for analysis might have benefits over decentralized IoT computation. reply hef19898 15 hours agoparentHow about no household surveillance at all? Crazy idea, I know. reply anonymouskimmer 14 hours agorootparentThat's what I do. But I presume others like it, so here's an alternative for them. I have an uncle who was really into household automation back in the 80s/90s. reply a_shoeboy 12 hours agoprevI wish my lone internet-of-shit device worked well enough to participate in a botnet. My house came with an internet connected sprinkler system--if the power blips, the sprinkler system boots up before the WIFI router, can't connect and then refuses to work until rebooted. I realized this when my lawn started dying. reply the_wolo 14 hours agoprevDental Denial of Service reply whyenot 13 hours agoprevAssuming that the article accurately reports the facts (I have my doubts) and these unnamed toothbrushes were used in DDoS attacks, it seems like the obvious deterrent would be for the harmed party to sue for damages. That seems like it work to deter companies from making internet connected when they aren't really needed. reply beeandapenguin 11 hours agoprevWhich toothbrush company/product are they referring to? The stock image implies Phillips, but I don't see any mention of that in the article. Never thought I'd be judging a toothbrush based on cybersecurity, but here we are... reply vivzkestrel 5 hours agoprevI challenge the hackers to hack my toothbrush , i ll even send it you with Fedex reply WalterBright 14 hours agoprevWouldn't one have to give the toothbrush your wifi password so it can connect? reply tmiku 14 hours agoparentOne probably has to enter the wifi password in an app and then the connection info gets sent to the brush via Bluetooth. That's how my smart watch behaves. reply hsuduebc2 14 hours agoprevIn 2024 there will be flying cars! Meanwhile. reply imglorp 14 hours agoprevIs nobody going to mention Java running on the toothbrush? One might guess the firmware included a battery controller, bluetooth or wifi stacks, a little storage, and business logic for buttons and brushing. reply p91paul 1 hour agoparentwell, you need to have something to run that business logic (which includes phoning home to the manufacturer), don't you? Java is as good as any other runtime. reply tjasko 12 hours agoparentprevWhy does it matter? Embedded Java is quite popular. https://en.wikipedia.org/wiki/Embedded_Java reply duohedron 12 hours agoparentprev\"3 Billion Devices Run Java\" Maybe not all of them should. reply cfeduke 14 hours agoparentprev3 billion devices run Java! ;) reply beardyw 15 hours agoprevIf only there was a kind of toothbrush which doesn't use the internet. Seems like an opportunity. reply pixxel 13 hours agoparentCalm down grandad, the benevolent cloud demands tooth data and it will have its data. reply usefulcat 13 hours agoprevCame here expecting to find something from The Onion reply Thaxll 14 hours agoprevThat's the reason why you need some serious router at home, one with vlan capabilities so all those iot devices get sandboxed network wise. reply mikkohypponen 13 hours agoprevIf It's Smart, It's Vulnerable. reply nottorp 11 hours agoprevWaiting for the refrigerator... reply snow_mac 15 hours agoprevDude. This is too much. My fridge is capable of being connected to the internet, so is my oven, garage door opener and my dishwasher. WHY? These things have worked so well without this crap. I wish manufacturers would stop this insanity. reply soco 15 hours agoparentTheory: you just don't connect them, right. Reality: connect or it won't start. Next step: integrated sim card. reply notaustinpowers 14 hours agorootparentThis literally happened to me on Friday. I was setting up a smart TV for my uncle and he just uses it for his Chromecast so I thought \"whatever, I'm not going to connect this TV to his wifi.\" Come to find out, the TV locks you out of EVERYTHING if you do not connect it to the internet. You see the homescreen but you aren't allowed to switch the input unless you connect to the wifi. Even after connecting to wifi, you only get access to FAST channels, and still have to register with a Samsung account before you get permission to change the input. I don't think I had ever been more upset at a piece of tech in my life. reply nonrandomstring 13 hours agorootparentYou're sure you read the instructions correctly? What's the make and model of that, please? I think people would like to know. That would certainly be illegal over here in Europe. reply Vrondi 14 hours agorootparentprevI had one like this (Toshiba), and I did the initial setup, then blocked it at my router from ever accessing the Internet again. Next TV purchase was a different brand (TCL) that didn't require such stupidity. reply Symbiote 11 hours agorootparentI would have returned it. \"It's for the basement room, no WiFi there\" if the shop argues. reply jimjimjim 14 hours agorootparentprevTVs are the worst. Everything except OLED sets have been getting cheaper and cheaper and I'm certain these manufacturers aren't achieving this via production line optimizations. It starts with the connection to vacuum up the data, next comes overlay ads, in a few years it'll be subscription plans instead of a sticker price. and the general public will love it. reply NoboruWataya 14 hours agoparentprevGenuine question, are these things really the norm where you live? I don't have a garage but none of those other appliances are capable of being connected to the internet for me. I am well aware that there are \"smart\" models out there and their prevalence is probably on the rise but it surprises me that someone so opposed to everything being internet-connected has so many such appliances. I'm in the UK, are these smart appliances way more common in the US or something? reply klabb3 14 hours agoparentprevIf you’ve ever tried to use these extra “connected features”, you’ll notice that they are completely useless. Aside from the small detail that these things don’t really solve any problem, these companies are not… let’s say software savvy. reply nonrandomstring 14 hours agoparentprev> My fridge is capable of being connected to the internet, so is my oven, garage door opener and my dishwasher. WHY? Because you bought them dude! :) reply carleton 12 hours agorootparentThese are all things that renters would not purchase. How many people actually try and use appliances at a rental property before signing? reply GrumpyNl 12 hours agoprevWhy you would buy a toothbrush that needs a app and wifi is beyond me. reply stcredzero 14 hours agoprevThis headline reads like a story element from the Silicon Valley TV series. reply jakub_g 13 hours agoprevEvery internet-of-shit device should be legally required to go through a security audit, and the vendor should commit to mandatory 5 years of API being up + 5 years of security updates, with N days to fix CVEs with severity over a certain threshold. Would make the shitty vendors think twice before creating piles of e-waste due to zero cost of entry. reply BlueTemplar 11 hours agoprev [–] Still better than three million plain infected toothbrushes, which is what this looked at first glance ! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Hackers have infected approximately three million smart toothbrushes in Switzerland and used them to launch a DDoS attack on a company's website.",
      "The company suffered millions of Euros in damages as a result of the attack.",
      "The toothbrushes were vulnerable to the breach due to their Java-based operating system.",
      "Cybersecurity experts recommend device owners update their devices, monitor for any suspicious activity, and utilize security software to safeguard against similar attacks."
    ],
    "commentSummary": [
      "The discussion explores the security risks and concerns surrounding internet-connected toothbrushes and smart devices.",
      "Participants question the validity of a news article suggesting smart toothbrushes were utilized in DDoS attacks.",
      "Various concerns are raised, including device security, data privacy, potential surveillance, and the importance of better security measures for smart devices."
    ],
    "points": 254,
    "commentCount": 162,
    "retryCount": 0,
    "time": 1707242852
  },
  {
    "id": 39272952,
    "title": "Prioritizing Server Importance: The Need for Regular Tracking",
    "originLink": "https://utcc.utoronto.ca/~cks/space/blog/sysadmin/TrackingMachineImportance",
    "originBody": "Chris Siebenmann :: CSpace » blog » sysadmin » TrackingMachineImportance Welcome, guest. We might want to regularly keep track of how important each server is February 5, 2024 Today we had a significant machine room air conditioning failure in our main machine room, one that certainly couldn't be fixed on the spot ('glycol all over the roof' is not a phrase you really want to hear about your AC's chiller). To keep the machine room's temperature down, we had to power off as many machines as possible without too badly affecting the services we offer to people here, which are rather varied. Some choices were obvious; all of our SLURM nodes that were in the main machine room got turned off right away. But others weren't things we necessarily remembered right away or we weren't clear if they were safe to turn off and what effects it would have. In the end we took several rounds of turning servers off, looking at what was left, spotting remaining machines, and turning more things off, and we're probably not done yet. (We have secondary machine room space and we're probably going to have to evacuate servers into it, too.) One thing we could do to avoid this flailing in the future is to explicitly (try to) keep track of which machines are important and which ones aren't, to pre-plan which machines we could shut down if we had a limited amount of cooling or power. If we documented this, we could avoid having to wrack our brains at the last minute and worry about dependencies or uses that we'd forgotten. Of course documentation isn't free; there's an ongoing amount of work to write it and keep it up to date. But possibly we could do this work as part of deploying machines or changing their configurations. (This would also help identify machines that we didn't need any more but hadn't gotten around to taking out of service, which we found a couple of in this iteration.) Writing all of this just in case of further AC failures is probably not all that great a choice of where to spend our time. But writing down this sort of thing can often help to clarify how your environment is connected together in general, including things like what will probably break or have problems if a specific machine (or service) is out, and perhaps which people depend on what service. This can be valuable information in general. The machine room archaeology of 'what is this machine, why is it on, and who is using it' can be fun occasionally, but you probably don't want to do it regularly. (Will we actually do this? I suspect not. When we deploy and start using a machine its purpose and so on feel obvious, because we have all of the context.) (5 comments.) Written on 05 February 2024. « I switched to explicit imports of things in our Django application What the max_connect Linux NFS v4 mount parameter seems to do » These are my WanderingThoughts (About the blog) Full index of entries Recent comments This is part of CSpace, and is written by ChrisSiebenmann. Mastodon: @cks Twitter @thatcks * * * Categories: links, linux, programming, python, snark, solaris, spam, sysadmin, tech, unix, web Also: (Sub)topics This is a DWiki. GettingAround (Help) Search: Page tools: View Source, Add Comment. Search: Login: Password: Atom Syndication: Recent Comments. Last modified: Mon Feb 5 23:14:53 2024 This dinky wiki is brought to you by the Insane Hackers Guild, Python sub-branch.",
    "commentLink": "https://news.ycombinator.com/item?id=39272952",
    "commentBody": "We might want to regularly keep track of how important each server is (utoronto.ca)218 points by pabs3 22 hours agohidepastfavorite133 comments _dan 17 hours agoSimilar thing (catastrophic aircon failure due to a flood in a crap colocated DC) happened to us too before we shifted to AWS. Photos from the colo were pretty bizarre - fans balanced on random boxes, makeshift aircon ducting made of cardboard and tape, and some dude flailing an open fire door back and forth all day to get a little bit of fresh air in. Bizarre to see in 2010-ish with multi million dollar customers. We ended up having to strategically shut servers down as well, but the question of what's critical, where is it in the racks, and what's next to it was incredibly difficult to answer. And kinda mind-bending - we'd been thinking of these things as completely virtualised resources for years, suddenly having to consider their physical characteristics as well was a bit of a shock. Just shutting down everything non-critical wasn't enough - there were still now critical non-redundant servers next to each other overheating. All we had to go on was an outdated racktables install, a readout of the case temperature for each, and a map of which machine was connected to which switch port which loosely related to position in the rack - none completely accurate. In the end we got the colo guys to send a photo of the rack front and back and (though not everything was well labelled) we were able to make some decisions and get things stable again. In the end one server that was critical but we couldn't get to run cooler we got lucky with - we were able to pull out the server below and (without shutting it down) have the on site engineer drop it down enough to crack the lid open and get some cool air into it to keep it running (albeit with no redundancy and on the edge of thermal shutdown). We came really close to a major outage that day that would have cost us dearly. I know it sounds like total shambles (and it kinda was) but I miss those days. reply SonOfLilit 16 hours agoparent> have the on site engineer drop it down enough to crack the lid open Took me four reads to find an alternative way to read it other than \"we asked some guy that doesn't even work for us to throw it on the ground repeatedly until the cover cracks open\", like that Zoolander scene. reply _dan 12 hours agorootparentHonestly that was pretty much the situation. In our defence, he offered. It had hit hour 6 of both the primary and the backup aircon being down, on a very hot day - everyone was way beyond blame and the NOC staff were basically up for any creative solution they could find. reply SonOfLilit 10 hours agorootparentWait, you didn't mean \"he repositioned it a couple levels down on the rack to make some room above so he could unscrew the cover and crack it a bit open it like a grand piano\"? reply _dan 1 hour agorootparentSorry misunderstood. Yes I did mean exactly that. Though he did damage the lid in the process, so who knows how he got it open. reply rezonant 10 hours agorootparentprevI suppose the case could've been locked-- if so you get a bonus penetration test out of it. reply tetha 10 hours agoparentprev> Similar thing (catastrophic aircon failure due to a flood in a crap colocated DC) happened to us too before we shifted to AWS. Photos from the colo were pretty bizarre - fans balanced on random boxes, makeshift aircon ducting made of cardboard and tape, and some dude flailing an open fire door back and forth all day to get a little bit of fresh air in. Bizarre to see in 2010-ish with multi million dollar customers. I'd have considered calling a few friends from the fire brigade or the catastrophe protection there. It's not an emergency, yes. However, if you want a situation for your trainees to figure out how to ventilate a building with the force of a thousand gasoline driven fans without anyone complaining and no danger to any person... well be my guest because I can't hear you anymore. Those really big fans are loud AF, seriously. And, on a more serious note, you could show those blokes how a DC works. Where power goes, what components do, how to handle uncontrolled fire in areas. Would be a major benefit to the local fire fighters. reply macintux 17 hours agoparentprevI find it’s much less stressful to rescue situations where it wasn’t your fault to begin with. Absent the ability to point fingers at a vendor, crises like that are a miserable experience for me. reply organsnyder 17 hours agorootparentBeing the hero always feels better than cleaning up your own messes. reply RexM 10 hours agorootparentWhy not both? reply gottorf 14 hours agoparentprev> some dude flailing an open fire door back and forth all day to get a little bit of fresh air in That's hilarious (probably for you as well, in hindsight). Do you feel comfortable naming and shaming this DC, so we know to avoid it? reply RHSman2 4 hours agoparentprevI wanna watch this movie reply oasisbob 18 hours agoprevGood plan! I think this is a relatively common practice within some corners of the telecom world. At university (Western WA in B'ham), I worked for our campus resnet, which had extensive involvement with other networking groups on campus. They ran layers 3 and below on the resnets, we took DNS+DHCP, plus egress, and everything through to layers 8 and 9. The core network gear was co-located in a few musty basements along with the telephone switches. DC and backup power was available, but severely limited under certain failure scenarios. All of the racked networking gear in the primary space was labeled with red and green dots. Green was first to go in any load-shedding scenario. Think: redundant LAN switches, switches carrying local ports, network monitoring servers, other +1 redundant components, etc. I'm not sure if the scheme was ever required in real life, but do know it was based on hard-earned experiences like the author here. reply red-iron-pine 18 hours agoparentUsed to run data centers for ISPs and such around NoVA. This was built into the building plan by room, with most rooms going down first and Meet-Me-Rooms + the rooms immediately adjacent where the big iron routers were, being the last to fail. It's been a while but IIRC there weren't any specific by-rack or by system protocols. reply spydum 21 hours agoprevAsset management is definitely a thing. Tag your environments, tag your apps, and provide your apps criticality ratings based on how important they are to running the business. Then it's a matter of a query to know which servers can be shut, and which absolutely must remain. reply ethbr1 20 hours agoparent> provide your apps criticality ratings based on how important they are to running the business In a decentralized, self-service model, you can add \"deal with convincing a stakeholder their app is anything less than most-critical.\" Although it usually works itself out if higher-criticality imposes ongoing time commitments on them as well (aka stick). reply chronid 3 hours agorootparentSome kind of cost shedding to the application owner (in many enterprises this is not the infra owner) is definitely needed otherwise everything becomes critical. \"Everything is critical\" should sound a million alarm bells in the minds of \"enterprise architects\" but most I've discussed this with are blissfully unaware. reply j33zusjuice 19 hours agorootparentprevThat seems like a poorly run company. Idk. Maybe we’ve worked in very different environments, but devs have almost always been aware of the criticality of the app, so convincing people wasn’t hard. In most places, the answer hinges on “is it customer facing?” and/or “does it break a core part of our business?” If the answer is no to both, it’s not critical, and everyone understands that. There’s always some weird outlier, “well, this runs process B to report on process A, and sends a custom report to the CEO …”, but hopefully those exceptions are rare. reply NovemberWhiskey 17 hours agorootparent>devs have almost always been aware of the criticality of the app I'm sure that developers are aware of the how important their stuff is to their immediate customer, but they're almost never aware of the relative criticality vis-a-vis stuff they don't own or have any idea about. reply hinkley 18 hours agorootparentprevI maintain a couple of apps that are pretty much free to break or to revert back to an older build without much consequence, except for one day a week, when half the team uses them instead of just me. Any other day I can use them to test new base images, new coding or deployment techniques, etc. I just have to put things back by the end of our cycle. reply letsdothisagain 15 hours agorootparentprevWelcome to University IT, where organizational structures are basically feudal (by law!). Imagine an organization where your president can't order a VP to do something, and you have academia :) reply spydum 19 hours agorootparentprevagree. ethbr1 is 100% right about this being a problem; if politics is driving your criticality rating, it's probably being done wrong. it should be as simple as your statement, being mindful of some of those downstream systems that aren't always obviously critical (until they are unavailable for $time) edit: whoops, maybe I read the meaning backward, but both issues exist! reply outofpaper 20 hours agoparentprevIn moments of crisis, immediate measures like physical tagging can be crucial. Yet, a broader challenge looms: our dependency on air conditioning. In Toronto's winter, the missed opportunity to design buildings that work with the climate, rather than defaulting to a universal AC solution, underscores the need for thoughtful asset management tailored to specific environments. reply monkeywork 19 hours agorootparentToronto's climate and winters is dramatically changing, the universal AC solution is almost mandatory due to the climate not being as cold in this area as it once was. reply gosub100 14 hours agorootparentdo you have a source for that? my source[1] appears the average temp hasn't changed much in the past quarter century: https://toronto.weatherstats.ca/metrics/temperature.html reply lazyasciiart 14 hours agorootparentAverage temp probably isn’t what you need here - peak temperature and length of high temperature conditions would be more important when figuring out if you need to have artificial cooling available. reply bcrl 8 hours agorootparentprevSeveral data centres in Toronto (including the massive facilities at 151 Front Street West where most of the internet for the province passes through) make use of the deep lake cooling loop that takes water pumped in from Lake Ontario to cool equipment before moving on to other uses. Water is pumped in from a sufficient depth such that the temperature is fairly constant year round. reply Scoundreller 2 hours agorootparentI think the system just has an isolated loop that heat exchanges with the incoming municipal water supply. Unsure if the whole system cools the loop glycol further or not, but ultimately there’s still a compressor-based aircon system sitting somewhere, probably at each building, that they’re depending on. They’re just not rejecting heat to the air (as much?). Would love to know if a data centre could get paid for rejecting it’s heat to the system during what is heating time for other users. reply letsdothisagain 14 hours agorootparentprevI know someone who did that in the Yukon during the winter, just monitor temperatures and crack a window when it got too hot. Seems like a great solution except that they were in a different building so they had to trudge through the snow to close the window if it got too cold. reply ykman 1 hour agorootparentwas that “PC” on the corner of “S” Street by any chance? didn’t think I’d see Yukon here :) reply j33zusjuice 19 hours agorootparentprevI upvoted, but I agree so much, I had to comment, too. I wonder how long it’d take to recoup the loss of retrofitting such a system. Despite this story today, this type of problem must be rare. I imagine most of the savings would be found in the electric bill, and it’d probably take a lot of years to recoup the cost. reply spydum 14 hours agorootparentIt's pretty common for hyperscalers actually: https://betterbuildingssolutioncenter.energy.gov/showcase-pr... https://greenmountain.no/data-centers/cooling/ I vaguely remember some other whole building DC designs that used a central vent which opened externally based on external climate for some additional free cooling. Can't find the reference now though. But geothermal is pretty common for sure. reply thakoppno 13 hours agorootparentYou may be thinking about Yahoo’s approach from 2010? > The Yahoo! approach is to avoid the capital cost and power consumption of chillers entirely by allowing the cold aisle temperatures to rise to 85F to 90F when they are unable to hold the temperature lower. They calculate they will only do this 34 hours a year which is less than 0.4% of the year. https://perspectives.mvdirona.com/2011/03/yahoo-compute-coop... reply spydum 11 hours agorootparentNo, what I was remembering was a building design for datacenters, but I can't find a reference. Maybe it was only conceptual. The design was to pull in cold exterior air, pass thru the dehumidifiers to bring some of the moisture levels down, and vent heat from a high rise shaft out the top. All controlled to ensure humidity didn't get wrecked. reply Scubabear68 20 hours agoparentprevHaving an application, process and hardware inventory is a must if you are going to have any hope of disaster recovery. Along with regular failovers to make sure you haven’t missed anything. reply datadrivenangel 19 hours agoparentprevGood documentation and metadata like this is necessary for corporations to truly be organized. reply cowsandmilk 22 hours agoprevIt is interesting to contrast with where the wider industry has gone. Industry: don’t treat your systems like pets. Author: proudly declares himself Unix herder, wants to keep track of which systems are important. reply tonyarkles 21 hours agoparentI’m absolutely loving the term Unix herder and will probably adopt it :) I’m generally with you and the wider industry on the cattle-not-pets thing but there are a few things to keep in mind in the context of a university IT department that are different than what we regularly talk about here: - budgets often work differently. You have a capex budget and your institution will exist long enough to fully depreciate the hardware they’ve bought you. They won’t be as happy to dramatically increase your opex. - storage is the ultimate pet. In a university IT department you’re going to have people who need access to tons and tons of speedy storage both short-term and long-term. I’m smiling a little bit thinking about a job 10 years ago who adopted the cattle-not-pets mentality. The IT department decided they were done with their pets, moved everything to a big vSphere cluster, and backed it by a giant RAID-5 array. There was a disk failure, but that’s ok, RAID-5 can handle that. And then the next day there was a second disk failure. Boom. Every single VM in the engineering department is gone including all of the data. It was all backed up to tape and slowly got restored but the blast radius was enormous. reply jodrellblank 21 hours agorootparentAt the risk of no true Scotsman, that doesn’t sound like “cattle not pets“; when the cattle are sent to the slaughterhouse there isn’t any blast radius, there’s just more cattle taking over. You explicitly don’t have to replace them with exact clones of the original cattle from tape very slowly, you spin up a herd of more cattle in moments. reply tonyarkles 18 hours agorootparent> you spin up a herd of more cattle in moments Where does the decade of data they've been collecting come from when you \"spin up a new herd\"? reply jodrellblank 15 hours agorootparentIf you are using clustered storage (Ceph, for example) instead of a single RAID5 array, ideally the loss of one node or one rack or one site doesn't lose your data it only loses some of the replicas. When you spin up new storage nodes, the data replicates from the other nodes in the cluster. If you need 'the university storage server' that's a pet. Google aren't keeping pet webservers and pet mailbox servers for GMail - whichever loadbalanced webserver you get connected to will work with any storage cluster node it talks to. Microsoft aren't keeping pet webservers and mailbox servers for Office365, either. If they lose one storage array, or rack, or one DC, your data isn't gone. 'Cattle' is the idea that if you need more storage, you spin up more identikit storage servers and they merge in seamlessly and provide more replicated redundant storage space. If some break, you replace them with identikit ones which seamlessly take over. If you need data, any of them will provide it. 'Pets' is the idea that you need the email storage server, which is that HP box in the corner with the big RAID5 array. If you need more storage, it needs to be expansion shelves compatible with that RAID controller and its specific firmware versions which needs space and power in the same rack, and that's different from your newer Engineering storage server, and different to your Backup storage server. If the HP fails, the service is down until you get parts for that specific HP server, or restore that specific server's data to one new pet. And yes, it's a model not a reality. It's easier to think about scaling your services if you have \"two large storage clusters\" than if you have a dozen different specialist storage servers each with individual quirks and individual support contracts which can only be worked on by individual engineers who know what's weird and unique about them. And if you can reorganise from pets to cattle, it can free up time, attention, make things more scalable, more flexible, make trade offs of maintenance time and effort. reply ElevenLathe 16 hours agorootparentprevIt's stored in another system that is probably treated as a pet, hopefully by somebody way better at it with a huge staff (like AWS). Even if it's a local NetApp cluster or something, you can leave the state to the storage admins rather than random engineers that may or may not even be with the company any more. reply logifail 20 hours agorootparentprev> when the cattle are sent to the slaughterhouse Data isn't \"sent to the slaughterhouse\". Ever. Data can be annoying that way. reply datadrivenangel 20 hours agorootparentThe true problem with kubernetes and modern cloud. Not insurmountable, but painful when your data is large compared to your processing needs. reply j33zusjuice 20 hours agorootparentprevI think the point is that systems that aren’t replaced easily shouldn’t be managed like they are, not … whatever it is you’re getting at here. reply carbotaniuman 14 hours agorootparentSadly you can't just spool down data either - data isn't fungible! reply chris_wot 20 hours agorootparentprevAnalogies always break down under scrutiny. Any cattle farmer would find “spinning up a herd of cattle” to be hilarious. reply jrumbut 17 hours agorootparentprev> budgets often work differently. Very differently. Instead of a system you continually iterate on for its entire lifetime, if you're in a more regulated research area you might build it once, get it approved, and then it's only critical updates for the next five (or more!) years while data is collected. Not many of the IT principles developed for web app startups apply in the research domain. They're less like cattle or pets and more like satellites which have very limited ability to be changed after launch. reply throw0101b 20 hours agorootparentprev> You have a capex budget […] As someone who has worked IT in academia: no, you do not. :) reply tonyarkles 18 hours agorootparentI'm laughing (and crying) with you, not at you. From my past life in academia, you're totally right. But that kind of reinforces the point... you do occasionally get some budget for servers and then have to make them last as long as possible :). Those one-time expenses are generally more palatable than recurring cloud storage costs though. reply jsjohnst 20 hours agorootparentprev> moved everything to a big vSphere cluster, and backed it by a giant RAID-5 array I’m with sibling commenter, if said IT department genuinely thought that the core point in “cattle-not-pets” was met by their single SuperMegaCow, then they missed the point entirely. reply fishtacos 19 hours agorootparentprev>>The IT department decided they were done with their pets, moved everything to a big vSphere cluster, and backed it by a giant RAID-5 array. There was a disk failure, but that’s ok, RAID-5 can handle that. Precisely why, when I was charged with setting up a 100 TB array for a law firm client at previous job, I went for RAID-6, even though it came with a tremendous write speed hit. It was mostly archived data that needed retention for a long period of time, so it wasn't bad for daily usage, and read speeds were great. Had the budget been greater, RAID 10 would've been my choice. (requisite reminder: RAID is not backup) Not related, but they were hit with a million dollar ransomware attack (as in: the hacker group requested a million dollar payment), so that write speed limitation was not the bottleneck considering internet speed when restoring. Ahhh.... what a shitshow, the FBI got involved, and never worked for them again. I did warn them though: zero updates (disabled) and also disabled firewall on the host data server (windows) was a recipe for disaster. Within 3 days they got hit, and the boss had the temerity to imply I had something to do with it. Glad I'm not there anymore, but what a screwy opsec situation I thankfully no longer have to support. reply rjbwork 13 hours agorootparent> the boss had the temerity to imply I had something to do with it. What was your response? I feel like mine would be \"you are now accusing me of a severe crime, all further correspondence will be through my lawyer, good luck\". reply justsomehnguy 19 hours agorootparentprev> even though it came with a tremendous write speed hit Only on a writesAuthor: proudly declares himself Unix herder, wants to keep track of which systems are important. Because not all environments are webapps which dozens or hundreds of systems configured in a cookie cutter matter. Plenty of IT environments have pets because plenty of IT environments are not Web Scale™. And plenty of IT environments have staff churn and legacy systems where knowledge can become murky (see reference about \"archaeology\"). An IMAP server is different than a web server is different than an NFS server, and there may also be inter-dependencies between them. reply LeonB 20 hours agorootparentI work with one system where the main entity is a “sale” which is processed beginning to end in some fraction of a second. A different system I work with, the main “entity” is, conceptually, more like a murder investigation. Less than 200 of the main entity are created in a year. Many different pieces of information are painstakingly gathered and tracked over a long period of time, with input from many people and oversight from legal experts and strong auditing requirements. Trying to apply the best lessons and principles from one system to the other is rarely a good idea. These kind of characteristics of different systems make a lot of difference to their care and feeding. reply cookiemonster9 20 hours agorootparentprevSo much this. Not everything fits cattle/chicken/etc models. Even in cases where those models could fit, they are not necessarily the right choice, given staffing, expertise, budgets, and other factors. reply viraptor 21 hours agoparentprevI think you're missing some aspects of cattle. You're still supposed to keep track of what happens and where. You still want to understand why and how each of the servers in the autoscaling group (or similar) behaves. The cattle part just means they're unified and quickly replaceable. Buy they still need to be well tagged, accounted for in planning, removed when they don't fulfil the purpose anymore, identified for billing, etc. And also importantly: you want to make sure you have a good enough description for them that you can say \"terraform/cloudformation/ansible: make sure those are running\" - without having to find them on the list and do it manually. reply luma 21 hours agoparentprevWhen you are responsible for the full infrastructure, sequencing power down and power on in coordination with your UPS is a common solution. Network gear needs a few minutes to light up ports, core services like DNS and identity services might need to light up next, then storage, then hypervisors and container hosts, then you can actually start working on app dependencies. This sort of sequencing leads itself naturally to having a plan for limited capacity “keep the lights on” workload shedding when facing a situation like the OP. Not everyone has elected to pay Bezos double the price for things they can handle themselves, and this is part of handling it. reply bbarnett 21 hours agorootparentDouble? Try 100x!! reply philsnow 12 hours agorootparentIf you’re running a couple ec2 instances in one AZ then yeah it’s closer to 100x, but if you wanted to replicate the durability of S3, it would cost you a lot in terms of redundancy (usually “invisible” to the customer) and ongoing R&D and support headcount. Yes, even when you add it all up, Amazon still charges a premium even over that all-in cost. That’s sweat equity. reply marcosdumay 20 hours agorootparentprevExactly. If it was double, it would be a non-brainier. reply p_l 21 hours agoparentprevEven if you're running \"cattle\", you still need to keep track of which systems are important, because to surprise of many, the full infrastructure is more like the ranch, and cattle is just part of it. (and here I remind myself again to write the screed against \"cattle\" metaphor...) reply pch00 21 hours agoparentprev> Industry: don’t treat your systems like pets. The industry has this narrative because it suits their desire to sell higher-margined cloud services. However in the real world, especially in academia as cks is, the reality is that many workloads are still not suitable for the cloud. reply mrighele 19 hours agoparentprevThe issue here is not much the hardware, but the services that run on top of them. I guess that many companies that use \"current practices\" have plenty of services that they don't even know about running on their clusters. The main difference is that instead of the kind of issues that the link talks about, you have those services running year after year, using resources, for the joy of the cloud companies. This happens even at Google [1]: \"There are several remarkable aspects to this story. One is that running a Bigtable was so inconsequential to Google’s scale that it took 2 years before anyone even noticed it, and even then, only because the version was old. \" [1] https://steve-yegge.medium.com/dear-google-cloud-your-deprec... reply readscore 21 hours agoparentprevAt a FAANG, our services are cattle, but we still plan which services to keep running when we need to drain 50% of a DC. Latency is important. Money makers > Latency sensitive > Optional requests > Background requests > Batch traffic. Bootstrapping is important. If A depends on B, you might to drain A first, or A and B together. reply _heimdall 21 hours agorootparentThe cattle metaphors really is a bad one. Anyone raising cattle should do the same thing, knowing which animals are the priority in case of draught, disease, etc. Hopefully one never has to face that scenario, but its much easier to pick up the pieces when you know where the priorities are whether you're having to power down servers or thin a herd. reply bluGill 20 hours agorootparentCattle are often interchangeable. You cull any that catch a disease (in some cases the USDA will cull the entire herd if just one catches something - bio security is a big deal) In the case of drought you pick a bunch to get rid of - based on market prices (If everyone else is you will try to keep yours because the market is collapsing - but this means managing feed and thus may mean culling more of the herd latter.) Some cattle we can measure. Milk cows are carefully managed as to output - the farmer knows how much the milk each one is worth and so they can cull the low producers. However milk is so much more valuable than meat that they never cull based on drought - milk can always outbid meat for feed. If milk demand goes down the farmer might cull come - but often the farmer is under contract for X amount of milk and so they cannot manage prices. reply p_l 18 hours agorootparentHonestly, big issue with the cattle metaphor is that the individual services you run on servers are very much often not interchangeable. A DNS service is not NTP is not mail gateway is not application load balancer is not database etc etc etc At best, multiple replicas of those are cattle. And while you can treat the servers underlying them as interchangeable, that doesn't change the fact the services you run on them are not. reply jodrellblank 8 hours agorootparentI think it's a way of thinking about things, rather than a true/false description. e.g. VMware virtual hosts make good cattle - in some setups I have worked on the hosts are interchangeable, move virtual machines between them without downtime. In others the hosts have different storage access, different connectivity and it matters which combination of hosts are online/offline together, and which VMs need the special connectivity. The regular setups are easier to understand, nicer to work on. The irregular ones are a trip hazard, they need careful setup, more careful maintenance, more detailed documentation, more aware monitoring. But there's probably ways they could be made regular, if the unique connectivity was moved out to a separate 'module' e.g. at the switch layer, or if the storage had been planned differently, sometimes with more cost, sometimes just with different design up-front. Along these lines, yes DNS is not NTP but you could have a 'cattle' template Linux server which can run your DNS or NTP or SMTP relay which can be script deployed, and then standard DNS/NTP/SMTP containers deployed on top. Or you could build a new Linux server by hand and deploy a new service layer by hand, every time, each one slightly different depending how rushed you are and what verison of installers are conveniently available and whether the same person does the work following the latest runbook or an outdated one or going from memory. You could deploy a template OpnSense VM which can front DNS or NTP or SMTP instead of having to manually login to a GUI firewall interface and add rules for the new service by hand. 'Cattle not pets' is a call to standardise, regularise, modularize, template, script, automate; to move towards those ways of doing things. Servers are now software which can be copypasted in a way they weren't 10-30 years ago, at least in my non-FAANG world. To me it doesn't mean every server has to mean nothing to you, or every server is interchangeable, it means consider if thinking that wasy can help. reply p_l 1 hour agorootparentIt might have been the original idea (though taken into account the time period and context, I suspect we're missing possible overfocus on deployment by AWS ASG and smallish set of services). What grinds my gears is that over years I found it a thought limiting meme - it effectively swings a metaphor too hard into one direction, and some early responses under original article IMO present quite well the issue. It's not like people are stupid - but metaphors like this exist to make shortcuts for thinking and discussion, and for last few years I've seen that it short-circuits the discussion too hard, making people either stop thinking about certain interdependencies, or stopping noticing that there are still systems they treat like \"pets\", just named differently and in different scope, but now mentally pushing out how fragile they can be. reply naniwaduni 18 hours agorootparentprevCattle often aren't interchangeable too. Not gonna have a great time milking the bulls. reply bluGill 16 hours agorootparentBut if you are milking you don't have bulls. Maybe you have one (though almost everyone uses artificial insemination these days). Worrying about milking bulls is like worrying about the NetWare server - once common but has been obsolete since before many reading this were even born. Of course the pigs, cows, and chickens are not interchangeable. Nor are corn, hay, soybeans. reply cortesoft 17 hours agorootparentprevYeah, really the only difference is whether you are tracking individual servers or TYPES of servers reply karmarepellent 22 hours agoparentprevI would argue even the \"wider industry\" still administrates systems that must be treated as pets because they were not designed to be treated as cattle. reply _heimdall 21 hours agoparentprevI'm pretty sure anyone in the industry that draws this distinction between cattle and pets has never worked with cattle and only knows of general ideas about the industrial cattle business. reply jodrellblank 8 hours agorootparenthttps://largest.org/geography/largest-cattle-ranches-in-the-... says Deseret Ranches has 44,000 cattle. Google tells me beef cattle are slaughtered after 2 years. Split 2 years among 44,000 cattle and you get to spend at most 24 minutes with each one, if you dedicate 2 years of your life to nothing else but that, not even sleep, travel, eating. If you let them live their natural life expectancy of 20 years, you get 240 minutes with each cow - two hours in its 20 year life. \"I care about my cattle\", yes, I don't think \"cattle\" is supposed to mean \"stop caring about the things you work on\". \"I know each and every one as well as the family dog I've had for ten years\", no. That's not possible. You raise them industrially and kill them for profit/food, that's a different dynamic than with Spot. reply _heimdall 4 hours agorootparentI believe this is just a great example of why cattle shouldn't be raised in such high volume industrial processes. Have you ever been around cattle? Or helped them calve? Or slaughtered one for meat? I know every one of my animals and understand the herd dynamics, from who the lead cow is to who is the asshole that is the one often starting fights and annoying the others. We shouldn't be throwing so many animals into such a controlled and confined system that they are reduced to numbers on a spreadsheet. We shouldn't raise an animal for slaughter after dedicating at most 24 minutes to them. reply antod 11 hours agorootparentprevLikewise, anyone talking about civil engineering and bridges. At least back in the day Slashdot was fully aware of how broken their ubiquitous car analogies were and played it up. reply bluGill 20 hours agorootparentprevRanchers do eat their pets. They generally do love the cattle, but they also know at the end of a few years they get replaced - it is the cycle of life. reply bayindirh 21 hours agoparentprevHPC admin here (and possibly managing a similar system topology with their room). In heterogeneous system rooms, you can't stuff everything into a virtualization cluster with a shared storage and migrate things on the fly, thinking that every server (in hardware) is a cattle and you can just herd your VMs from host to host. A SLURM cluster is easy. Shutdown all the nodes, controller will say \"welp, no servers to run the workloads, will wait until servers come back\", but storage systems are not that easy (ordering, controller dependencies, volume dependencies, service dependencies, etc.). Also there are servers which can't be virtualized because they're hardware dependent, latency dependent, or just filling the server they are in, resource wise. We also have some pet servers, and some cattle. We \"pfft\" to some servers and scramble for others due to various reasons. We know what server runs which service by the hostname, and never install pet servers without team's knowledge. So if something important goes down everyone at least can attend the OS or the hardware it's running on. Even in a cloud environment, you can't move a VSwitch VM as you want, because you can't have the root of a fat SDN tree on every node. Even the most flexible infrastructure has firm parts to support that flexibility. It's impossible otherwise. Lastly, not knowing which servers are important is a big no-no. We had \"glycol everywhere\" incidents and serious heatwaves, and all we say is, \"we can't cool room down, scale down\". Everybody shuts the servers they know they can, even if somebody from the team is on vacation. Being a sysadmin is a team game. reply johann8384 20 hours agoparentprevPets make sense sometimes. I also think there are still plenty of companies, large ones, with important services and data, that just don't operate in a way that allows the data center teams to do this either. I have some experience with both health insurance and life insurance companies for example where \"this critical thing #8 that we would go out of business without\" stillnloves solely on \"this server right here\". In university settings you have systems that are owned by a wide array of teams. These organizations aren't ready or even looking to implement a platform model where the underlying hardware can be generic. reply krisoft 20 hours agoparentprevI don't see the contrast here. > proudly declares himself Unix herder You know what herder's herd? Cattle. Not pets. > wants to keep track of which systems are important. I mean obviously? Industry does the same. Probably with automation, tools and tagging during provisioning. The pet mentality is when you create a beautiful handcrafted spreadsheet showing which services run on the server named \"Mjolnir\" and which services run on the server named \"Valinor\". The cattle mentality is when you have the same in a distributed key-value database with UUIDs instead of fancyful server names. Or the pet mentality is when you prefer to not shut down \"Mjolnir\" because it has more than 2 years of uptime or some other silly reason like that. (as opposed to not shutting it down because you know that you would loose more money that way than by risking it overheating and having to buy a new one.) reply falcor84 21 hours agoparentprevWhere's the contrast? Herding is something you do with cattle rather than pets. reply NovemberWhiskey 17 hours agoparentprevNot sure if the goal was just to make an amusing comparison, but these are actually two completely different concerns. Building your systems so that they don't depend on permanent infrastructure and snowflake configurations is an orthogonal concern from understanding how to shed load in a business-continuity crisis. reply karmarepellent 22 hours agoprevIt's generally a good idea to have some documentation that states what a machine is used for, the \"service\", and how important said service is relative to others. At my company we kind of enforce this by not operating machines that are not explicitly assigned to any service. However you have to anticipate that the quality of documentation still varies immensely which might result in you shutting down a service that is actually more important than stated. Fortunately documentation improves after every outage because service owners reiterate on their part of the documentation when their service was shut down as \"it appeared unimportant\". It's a process. reply PaulKeeble 13 hours agoprevOne place I worked did a backup power test and when they came back from the diesels to the grid the entire datacentre lost power due to a software bug for about 10 seconds. It caused a massive outage. The problem was a lot of the machines pulled their OS image from central storage servers and there was no where near enough IO to load everything and they had to prioritise what to bring up first to lighten the load and stop everything thrashing. It was a complete nightmare even though the front end to take sales were well isolated from the backend. Working out what was most important across an entire corporation took as long as the problem resolving slowly by just bringing things up randomly. Nowadays you would just run multiple datacentres or cloud HA and we have SSDs but I just can't see such an architecture understanding being possible for any reasonably large company. The cost of keeping it and the dependencies up to date would be huge and it would always be out of date. More documentation isn't the solution, its to have multiple sites. reply jiggawatts 13 hours agoparentThat brings back memories of a similar setup with hundreds of Windows servers booting over the network. We had regular “brownouts” even during the day just because the image streaming servers couldn’t handle the IOPS. Basic maintenance would slow down the servers for ten thousand users and generate support tickets. I jumped up and down and convinced management to buy one of the first enterprise SSDs on the market. It was a PCIe card form factor and cost five digits for a tiny amount of storage. We squeezed in the images using block-level deduplication and clever copy scripts that would run the compaction routine after each file was copied. The difference was staggering. Just two of those cards made hundreds of other servers run like greased lightning. Boot times dropped to single digit seconds instead of minutes. Maintenance changes could be done at any time with zero impact on users. The whole cluster could be rebooted all at once with only a slight slowdown. Fun times. reply marcus0x62 21 hours agoprevYears ago during a week-long power outage, a telephone central office where we had some equipment suffered a generator failure. The telephone company had a backup plan (they kept one generator on a trailer in the city for such a contingency,) and they had battery capacity[0] for their critical equipment to last until the generator was hooked up. They did have to load shed, though: they just turned off the AC inverters. They figured anything critical in a central office was on DC power, and if you had something on AC, you were just going to have to wait until the backup-backup generator was installed. 0 - at the time, at least, CO battery backup was usually sized for 24 hours of runtime. reply Scoundreller 2 hours agoparent> had some equipment suffered a generator failure. The telephone company had a backup plan (they kept one generator on a trailer in the city for such a contingency Well that’s more forward thinking than AT&T’s Nashville CO when it got bombed. They just depended on natgas grid for their generators if grid electric went out. They underestimated the correlation between energy grids. When natgas got cutoff and the UPSs died, they had no ability to hook in roll-up generators and had to hastily install connection points (or hardline them). And no standby contracts for them. reply radiowave 20 hours agoprevTracking servers is one thing, but tracking the dependency relationships among them is likely at least as important. reply h2odragon 17 hours agoparentWriting down and graphing out these relationships is a good way to identify and normalize them. I once had a system with layers of functionality; lvl 0 services were the most critical; lvl 3+ was \"user shit\" that could be sloughed off at need. Had some stub servers at lvl 0 and 1 that did things like providing a file share of the same name as the lower level services, but not populated; so that inadvertent domain crossing dependencies weren't severe problems. There was a \"DB server\" stub that only returned \"no results.\" The actual DB server for those queries was on the monster big rack SPARC with the 3dz disks that took 10min to spin up fully. When it came up it took over. reply j33zusjuice 19 hours agoparentprevI’m really glad we realized that before disaster struck. We have a project in-progress to do exactly this. It’d even better if SWE wrote ADRs (or whatever) that document all this stuff up front, but … well, there are only so many battles anyone can fight, right? reply tlb 22 hours agoprevOr you might want to have redundant cooling. Cooling system prices seem to scale fairly linearly with the cooling power above a few kW, so instead of one 100 kW system you could buy four 25 kW systems so a single failure won't be a disaster. reply throw0101a 21 hours agoparent> Or you might want to have redundant cooling. Can you provide a cost centre or credit card for which they can bill this to? In case you didn't notice the domain, it is UToronto: academic departments aren't generally flush with cash. Further, you have to have physical space to fit the extra cooling equipment and pipes: not always easy or possible to do in old university buildings. reply gregmac 19 hours agorootparentThis kind of thing is like insurance. Maybe IT failed to state the consequences of not having redundancy, maybe people in control of the money failed to understand.. or maybe the risks were understood and accepted. Either way, by not paying for the insurance (redundant systems) up front the organization is explicitly taking on the risk. Whether the cost now is higher is impossible to say as an outsider, but there's a lot of expenses: paying a premium for emergency repairs/replacement; paying salaries to a bunch of staff who are unable to work at full capacity (or maybe at all); a bunch of IT projects delayed because staff is dealing with an outage; and maybe downstream ripple effects, like classes cancelled or research projects in jeopardy. I've never worked in academics, but I know people that do and understand the budget nonsense they go through. It doesn't change the reality, though, which is systems fail and if you don't plan for that you'll pay dearly. reply wongarsu 21 hours agorootparentprevIf you designed the system like this from the start or when replacing it anyways, N+1 redundancy might not me much more expensive than one big cooling unit. The systems can mostly share their ductwork and just have redundancy in the active components, so mostly the chillers. Of course these systems only get replaced every couple decades, if ever, so they are pretty much stuck with the setup they have. reply throw0101b 20 hours agorootparent> If you designed […] University department IT is not designed, it grows over decades. At some point some benefactor may pay for a new building and the department will move, so that could be a chance to actually design. But modern architecture and architects don't really go well with hosting lots of servers in what is ostensibly office space. I've been involved in the build-out of buildings/office space on three occasions in my career, and trying to get a decent IT space pencilled has always been like pulling teeth. reply bluGill 20 hours agorootparentprev> Of course these systems only get replaced every couple decades, if ever This is despite the massive energy savings they could get if they replaced those older systems. Universities often are full of old buildings with terrible insulation heated/cooled by very old/inefficient systems. In 20 years they would be money ahead by tearing down most buildings on campus and rebuilding to modern standards - assuming energy costs don't go up which seems unlikely) But they consider all those old buildings historic and so won't. reply light_hue_1 19 hours agorootparent> In 20 years they would be money ahead by tearing down most buildings on campus and rebuilding to modern standards - assuming energy costs don't go up which seems unlikely) But they consider all those old buildings historic and so won't. It has nothing to do with considering those building historic. The problem is unless someone wants to donate a $50-100M, new buildings don't happen. And big donors want to donate to massive causes \"Build a new building to cure cancer!\" not \"This building is kind of crappy, let's replace it with a better one\". It doesn't matter that over 50 years something could be cheaper if there's no money to fix it now. reply marcus0x62 21 hours agoparentprevWith that model, you’d probably want 5 instead of 4 (N+1), but the other thing to consider is if you can duct the cold air to where it needs to go when one or more of the units has failed. reply nine_k 22 hours agoparentprevWon't 4 x 25 kW systems mean also 4x the installation cost? reply bluGill 19 hours agorootparentMaybe, but costs are not linear and the nonlinear goes different ways for different parts of the install. The costs of the smaller systems installed could be cheaper than just the large system not installed if the smaller systems are standard parts. reply dist-epoch 22 hours agorootparentprevProbably less if you install them all at the same time. reply mdekkers 22 hours agorootparentprevWhat the % of install cost over the projected lifetime of the system? reply dotancohen 21 hours agoparentprevThis will mean just about 4 times the number of failures, too. And can 75% cooling still cool the server room anyway? reply bluGill 19 hours agorootparentIt means 5 times the number of failures as you intentionally put in an extra unit so that one can be taken offline at any time for maintenance (which itself will keep the whole system more reliable), and if one fails the whole keeps up. The cost is only slightly more to do this when there are 5 smaller units. Those smaller units could be standard off the shelf units as well, so it could be cheaper than a large unit that isn't mad in as large a quantity - this is a consideration that needs to be made case by case) Even if you cheap out and only install 4 units, odds are your failure doesn't happen on the hottest day of the year and so 3 can keep up just fine. It is only when you are unlikely that you need to shut anything down. reply donkeyd 21 hours agorootparentprevMaybe not, but some cooling means less servers to shut down. reply evilduck 18 hours agorootparentprevFour service degradations vs. one huge outage event. Pick your poison. reply giaour 19 hours agoprevCloud environments have an elegant solution in the form of \"spot\" or \"pre-emptible\" instances: if your workload can tolerate interruptions because it's not time sensitive or not terribly important, you can get a pretty steep discount. reply jabart 10 hours agoprevAlso important servers should also go lower in the rack, but not too low to flood. Learned that from an aircon failure and the top server hit 180F+ and shutdown. Thankfully had temp logs in Grafana to figure that out. reply cogman10 10 hours agoprevPeople like to rag on Kubernetes for it's complexity, but this is the exact sort of scenario where k8s really does shine. The answer to \"which machines are important\" is \"only enough to provide the resources to run everything\". You can kill whatever nodes you like just so long as there is enough of them to keep the cluster health and k8s will simply migrate workloads where they need to be. That being said, storage is still an issue. Perhaps NAS is the one place where you might mark \"these are the important machines\". reply rjbwork 9 hours agoparentAnd what if you only have 33% of your nominal cluster capacity available because the AC goes out in your server room? Now which containers should your cluster stop running? You haven't actually solved anything with this, you've just changed the abstraction layer at which you need to make decisions. Probably an improvement, but does not obviate some kind of load shedding heuristic. reply cogman10 7 hours agorootparentBut that's not what this was about. It was about \"hey, which of these boxes had the ldap running on it? We need to make sure that gets shifted somewhere else!\" K8S let's you say \"ok, we don't have enough capacity to run everything so let's shut down the Bitcoin deployment to free up capacity\". There's no leg work or bookkeeping to figure out what was running where, instead it's \"what do I need to run and what can I shut down or tune down\". All from the comfort of the room with AC. And if you're really cleaver, you went ahead and gave system critical pods elevated property. [1] [1] https://kubernetes.io/docs/concepts/scheduling-eviction/pod-... reply zetsurin 9 hours agorootparentprevYou have: you've got clearly labelled workloads you can decide which ones are important and not, and easily turn down the ones that aren't important. reply iwontberude 12 hours agoprevIsn't this the point of decoupling your compute and datastores using CSI with disaggregated storage Kubernetes? So long as you keep your datastores available, whatever compute you can manage to attach it from Kubernetes can run whatever you truly need at capacities that you can handle with that level of hardware. Similarly, you could scale down the workloads on all the machines so they generated less heat without turning anything off at the expense of performance. reply VikingCoder 17 hours agoprevI often think of the SNL sketch with the lines, \"We should keep a list of our clients, and how much money they have given us.\" https://www.youtube.com/watch?v=reUjEFI0Slc reply quickthrower2 12 hours agoprevThis is where the cloud kicks ass. Run multiple nodes with geo redundancy (where based on various concerns: cost, contracts, legal). But nodes should cross data centres. Maybe if one city gets nuked (literally or a fire/power outage) you still have uptime. Use Kubernetes maybe. reply jll29 19 hours agoprevI suspect making a list of \"I think these ones are not critical.\" is not sufficient. You may overlook some subtle forms of interdependence. To be sure, you need to test your documentation by actually switching off the \"uncritical\" assets. reply syslog 14 hours agoprevAny server that must not fail is just not important enough. (Build your services with HA in mind, so you don‘t have to worry about a situation like this one.) reply mgaunard 15 hours agoprevIf machines aren't important, why do you have them at all? Because it's an university and we don't care about justifying costs? reply barrucadu 13 hours agoparentIt's possible to have two important things and yet for one to be more important than the other. reply throwawaaarrgh 18 hours agoprevIf your machines are all hypervisors you could migrate important VMs to a couple hosts and turn off the rest. You could also possibly throttle the vcpus, which would slow down the VMs but allow you to run the machines cooler, or more VMs per machine. Finally the ones with long running jobs could just be snapshotted and powered down and restored later, resuming their computation. There's a reason us old fogies were so excited when virtual machines got increasingly robust. We could use them to solve problems quickly that used to be nearly impossible. reply charcircuit 20 hours agoprevTurning off servers seems like the wrong call instead of transitioning servers into a lower powered state which can be exited once the power budget is available again. reply bluGill 19 hours agoparentThe right answer is turn them all off - anything important is in a redundant data center. But odds are they don't have that. If a redundant data center isn't an option, then you should put more into ensuring the system is resilient - fireproof room (if a server catches on fire it can't spread to the next - there are a lot of considerations here that I don't know about that you need to figure out), plenty of backup power, redundant HVAC, redundant connections to the internet - and you should brainstorm other things that I didn't think of. reply znlfl2 20 hours agoprevnext [3 more] [flagged] Scubabear68 20 hours agoparentI just did a due diligence with a company that only had servers in one data center. They were supremely confident that there was no way a whole DC could be impacted by an event. reply Retr0id 20 hours agorootparentfwiw you're likely replying to someone's GPT bot reply yevpats 20 hours agoprev [–] Check out CloudQuery - https://github.com/cloudquery/cloudquery for an easy cloud asset inventory. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author's main machine room experienced a major air conditioning failure, forcing them to power off machines.",
      "The incident highlighted the need to keep track of which machines are critical and which ones are not, in order to better plan for future cooling or power limitations.",
      "While the author acknowledged the importance of documenting this information, they mentioned that it may not be prioritized due to ongoing maintenance work."
    ],
    "commentSummary": [
      "The passage and comment thread cover topics such as server management, data centers, and IT infrastructure.",
      "Key themes include the significance of asset management and criticality ratings and treating servers as cattle, not pets.",
      "The discussion delves into challenges in implementing this approach, the use of cloud services, the need for server system redundancy and resilience, as well as limitations, costs, budget constraints in academia, and the importance of documentation and organization."
    ],
    "points": 218,
    "commentCount": 133,
    "retryCount": 0,
    "time": 1707216697
  }
]
