[
  {
    "id": 36061407,
    "timestamp": 1684949292,
    "title": "PyPI Was Subpoenaed",
    "url": "https://blog.pypi.org/posts/2023-05-24-pypi-was-subpoenaed/",
    "hn_url": "http://news.ycombinator.com/item?id=36061407",
    "content": "PyPI was subpoenaedby: Ee Durbin \u00b7 2023-05-24#transparency#complianceIn March and April 2023, the Python Software Foundation (PSF) received three (3) subpoenas for PyPI user data. All three subpoenas were issued by the United States Department of Justice. The PSF was not provided with context on the legal circumstances surrounding these subpoenas. In total, user data related to five (5) PyPI usernames were requested.The data request was:\"Names (including subscriber names, user names, and screen names);\"\"Addresses (including mailing, residential addresses, business addresses, and email addresses);\"\"Connection records;\"\"Records of session times and durations, and the temporarily assigned network address (such as Internet Protocol addresses) associated with those sessions;\"\"Length of service (including start date) and type of services utilized;\"\"Telephone or instrument numbers (including the registration Internet Protocol address);\"\"Means and source of payment of any such services (including any credit card or bank account number) and billing records;\"\"Records of all Python Package Index (PyPI) packages uploaded by...\" given usernames\"IP download logs of any Python Package Index (PyPI) packages uploaded by...\" given usernamesThe privacy of PyPI users is of utmost concern to PSF and the PyPI Administrators, and we are committed to protecting user data from disclosure whenever possible. In this case, however, PSF determined with the advice of counsel that our only course of action was to provide the requested data. I, as Director of Infrastructure of the Python Software Foundation, fulfilled the requests in consultation with PSF's counsel.We have waited for the string of subpoenas to subside, though we were committed from the beginning to write and publish this post as a matter of transparency, and as allowed by the lack of a non-disclosure order associated with the subpoenas received in March and April 2023.Next StepsPyPI and the PSF are committed to the freedom, security, and privacy of our users.This process has offered time to revisit our current data and privacy standards, which are minimal, to ensure they take into account the varied interests of the Python community. Though we collect very little personal data from PyPI users, any unnecessarily held data are still subject to these kinds of requests in addition to the baseline risk of data compromise via malice or operator error.As a result we are currently developing new data retention and disclosure policies. These policies will relate to our procedures for future government data requests, how and for what duration we store personally identifiable information such as user access records, and policies that make these explicit for our users and community.Please continue to watch this blog for related announcements as policies are finalized, published, and implemented.DetailsIn order to provide as much transparency as possible, the following will detail the shape of and extent of the data that was contained in the responses to these subpoenas.We will not be releasing the usernames involved publicly or to the users themselves.1) Names (including subscriber names, user names, and screen names);These were confirmed via our database recordsselect id, username, name from users where username = '{USERNAME}';Returning:id    | UUID for USERNAMEusername | PyPI usernamename   | Display name for userAnd are also publicly available at https://pypi.org/user/{USERNAME}/.PyPI allows users to delete their accounts.PyPI does not allow for the username field to be changed without admin intervention, and no such intervention has occurred for the users in question. If they had, we would have provided records of those changes.PyPI does allow for user to update their display names and keeps no record of the history of these changes.2) Addresses (including mailing, residential addresses, business addresses, and email addresses);PyPI only stores email addresses for individual users. No physical addresses are stored. Organization accounts who have signed up for billing (not yet live) will be required to provide a billing address that validates to their payment method.These were sourced from our database records and is private to PyPI.select email, user_id from user_emails where user_id ='{UUID for USERNAME}';Returning:email  | An email addressuser_id | UUID for USERNAMEPyPI allows for users to add and remove email addresses without admin intervention. Records of these changes are kept, and no such changes were observed in our records for the usernames in question. If they had, we would have provided records of those changes.3. Connection records3a. Project eventsPyPI retains records of all changes to projects on the index, and has since 2002-11-01 17:11:36 UTC.These were confirmed via our database recordsselect * from journals where submitted_by='{USERNAME}' order by submitted_date;Returning:id       | An auto incrementing integer representing the \"Serial\"name      | Name of a PyPI Projectversion    | Version of a PyPI Release if relevantaction     | Description of the action taken against the Project/Releasesubmitted_date | ISO-8601 datetime in UTCsubmitted_by  | PyPI Usernamesubmitted_from | IP Addressand with the exception of the submitted_by (PyPI username) and submitted_from (IP Address) columns are publicly available via our XMLRPC API.3b. User eventsPyPI retains records of critical user events including account creation, emails sent, email address changes, logins, and login failures. See this list for the comprehensive set of events recorded.These were sourced from our database recordsselect * from user_events where source_id = '{UUID for USERNAME}' order by time desc;Returning:id        | UUID of the eventsource_id     | UUID for USERNAMEtag        | EventTagtime       | ISO-8601 datetime in UTCip_address_string | IP Addressadditional    | JSONB metadata about the eventip_address_id   | UUID of associated IPAddress objectand are private to PyPI.4. Records of session times and durations, and the temporarily assigned network address (such as Internet Protocol addresses) associated with those sessions;PyPI does not record session durations.Session creation (Login) times were provided as a synopsis of the data in 3b.Sessions are not created for uploads, but the associated login events for project uploads were provided as a synopsis of the data in 3a.5. Length of service (including start date) and type of services utilized;PyPI retains records of the date that a user account was created, as well as the last time it was successfully logged in by any method (web UI or command line tool for upload).select date_joined, last_login from users where username = {USERNAME}Returning:date_joined | ISO-8601 datetime in UTClast_login | ISO-8601 datetime in UTCThese were sourced from our database records and are private to PyPI.Types of service utilized are \"standard\" to PyPI and include the ability to create projects, releases, and distribution files for downloads.6. Telephone or instrument numbers (including the registration Internet Protocol address);A synopsis of all IP Addresses for each username from previous records were shared.These were sourced from our database records and are private to PyPI.7. Means and source of payment of any such services (including any credit card or bank account number) and billing records;PyPI has no cost to use for individual users and no such payment records or billing records exist.8. Records of all Python Package Index (PyPI) packages uploaded by the given usernamesA list of all past and any current projects associated with each username was provided.These were sourced from our database records and for past projects, are private to PyPI.9. IP download logs of any Python Package Index (PyPI) packages uploaded by the given usernamesPyPI does not retain download logs for packages which include IP addresses. Download logs are processed by a pipeline which includes GeoIP information reported by our CDN only.These records were sourced from the Google BigQuery Public dataset with the following queries:SELECT * FROM `bigquery-public-data.pypi.file_downloads`WHERE project IN ({LIST OF PROJECT NAMES FROM 8})AND timestamp > '{START OF PERIOD IN QUESTION}';Ee Durbin is the Director of Infrastructure at the Python Software Foundation. They have been contributing to keeping PyPI online, available, and secure since 2013.",
    "summary": "- The Python Software Foundation received three subpoenas for PyPI user data from the United States Department of Justice in March and April 2023.\n- The PSF found it necessary to provide the requested data concerning five PyPI usernames.\n- PyPI is committed to developing new data retention and disclosure policies to ensure user privacy and security.",
    "hn_title": "PyPI Was Subpoenaed",
    "original_title": "PyPI Was Subpoenaed",
    "score": 1047,
    "hn_content": "PyPI, the repository of Python packages, revealed that they received subpoenas in March and April 2023. Although they were committed to transparency, they waited for the string of subpoenas to subside before publishing details of the incident. This has led to a discussion about the problem of subpoenas and surveillance of citizens by the state and the lack of accountability. The article also includes a conversation on free speech and the U.S. First Amendment, the legality of gag orders, and the differences between Western countries in terms of free speech and restrictions.The post discusses the legal limits of free speech and hate speech in different countries, including the US, the Netherlands, Canada, Germany, and Europe. It includes examples of court cases and laws in each country and how they relate to freedom of expression. The post highlights the different levels of protection for free speech in different countries and how the interpretation of laws can differ. It also notes that the US Supreme Court is more active in interpreting laws related to free speech than in other countries. Overall, the post provides an informative look at free speech in different countries and the challenges of balancing individual rights and social harm.The commentary discusses the limits of free speech in the United States, particularly around civil rights and restrictions on speech in private contexts. Examples include issues such as harassment, defamation, and incitement to violence. Private entities like employers can also restrict speech in various ways. The thread includes discussion about the differences in how other countries handle free speech protections, including the example of employee protections in Europe. There is also debate about the effectiveness of the First Amendment and how well it protects free speech, given the actions of the government and private entities. The commentary overall raises questions about the nature of free speech and its limitations in practice.The conversation is about the First Amendment and its interpretation by the US Supreme Court. The discussion includes the contradiction of the First Amendment laws, examples of limitations to free speech such as libel, slander, and pornography, the Supreme Court's evolving interpretation of the law, and the limitations on rights and freedoms granted under the US Constitution for\u00a0sufficiently\u00a0grave reasons. There is also\u00a0an argument that gag orders are necessary to avoid potential jurors seeing information about the case. Lastly, the post discusses how laws are written and how changing opinions can change our understanding of long-standing laws or Constitutional Amendments from the 1700s.A discussion on the use and justification of gag orders in investigations. Gag orders are used to avoid tipping off the subject of an active investigation, avoid general knowledge or disclosure of key sources of information and investigative methods used by law enforcement, and conceal the general scale, nature, and purpose of surveillance activities from the general public. There is debate on the ethics and democratic oversight of gag orders. While courts oversee the issuance of subpoenas and gag orders, critics argue that there are instances of abuse, lack of transparency, and limited accountability. However, proponents argue that eliminating gag orders entirely could hamper law enforcement's ability to build cases against criminals.PyPI, the package repository for Python, receives government subpoenas and national security letters (NSLs) with non-disclosure orders attached, and must comply with them. Three such subpoenas were received and the company was permitted to disclose them, but others may be secret. PyPI has struggled with end-to-end package signing security issues in the past, and while PGP package signatures have been removed, there is no replacement security mechanism for package upload verification.\n",
    "hn_summary": "- The PyPI repository for Python packages received subpoenas in March and April 2023, raising concerns about surveillance and lack of accountability.\n- The post discusses free speech and its legal limits in different countries, including court cases and laws in the US, Netherlands, Canada, Germany, and Europe.\n- The conversation also includes discussions on the First Amendment and its evolving interpretation by the US Supreme Court, as well as the use and ethics of gag orders in investigations."
  },
  {
    "id": 36065175,
    "timestamp": 1684970717,
    "title": "The tiny corp raised $5.1M",
    "url": "https://geohot.github.io//blog/jekyll/update/2023/05/24/the-tiny-corp-raised-5M.html",
    "hn_url": "http://news.ycombinator.com/item?id=36065175",
    "content": "the tiny corp raised $5.1MMay 24, 2023Here we go again. I started another company. The money is in the bank.What is the tiny corp?The tiny corp is a computer company. We sell computers for more than they cost to make; I\u2019ve been thinking about this one for a while. In the limit, it\u2019s a chip company, but there\u2019s a lot of intermediates along the way.The human brain has about 20 PFLOPS of compute. I\u2019ve written various blog posts about this. Sadly, 20 PFLOPS of compute is not accessible to most people, costing about $1M to buy or $100/hr to rent.With the way AI is going, we risk large entities controlling the majority of the compute in the world. I do not want \u201cI think there\u2019s a world market for maybe five computers.\u201d to ever be the world we live in.The goal of the tiny corp is: \u201cto commoditize the petaflop\u201dWhat is tinygrad?I started tinygrad in Oct 2020. It started as a toy project to teach me about neural networks, it\u2019s now carved out a good niche in the inference space running the model in openpilot, and soon will be a serious competitor to PyTorch in many places.The main advantage is in the tinygrad IR. It has 12 operations, all of which are ADD/MUL only. x[3] is supported, x[y] is not. Matrix multiplies and convolutions are just multiplies and sums, surrounded by a bunch of zero cost movement operations (like reshape, permute, expand).# a fast matmul in tinygrad (a@b works also of course)from tinygrad.tensor import TensorN = 2048; a, b = Tensor.randn(N,N), Tensor.randn(N,N)c = (a.reshape(N,1,N) * b.permute(1,0).reshape(1,N,N)).sum(axis=2)tinygrad is lazy, like Haskell, to allow op fusion without the user ever having to think about it.Ok, so?The current crop of AI chip companies failed. Many of them managed to tape out chips, some of those chips even worked. But not a single one wrote a decent framework to use those chips. They had similar performance/$ to NVIDIA, and way worse software. Of course they failed. Everyone just bought stuff from NVIDIA.I think the only way to start an AI chip company is to start with the software. The computing in ML is not general purpose computing. 95% of models in use today (including LLMs and image generation) have all their compute and memory accesses statically computable.Unfortunately, this advantage is thrown away the minute you have something like CUDA in your stack. Once you are calling in to Turing complete kernels, you can no longer reason about their behavior. You fall back to caching, warp scheduling, and branch prediction.tinygrad is a simple framework with a PyTorch like frontend that will take you all the way to the hardware, without allowing terrible Turing completeness to creep in.The Red Team (AMD)10 or so companies thought it was a good idea to tape out chips. Ironically, taping out the chip is the easy part. It requires a lot of capital, but that just involves convincing foolish investors that the space they are targeting is SO HUGE that even if they have a 3% chance of success it\u2019s worth investing. Investors fall for this, they invest, and the world tapes out useless AI chips.There\u2019s a great chip already on the market. For $999, you get a 123 TFLOP card with 24 GB of 960 GB/s RAM. This is the best FLOPS per dollar today, and yet\u2026nobody in ML uses it.I promise it\u2019s better than the chip you taped out! It has 58B transistors on TSMC N5, and it\u2019s like the 20th generation chip made by the company, 3rd in this series. Why are you so arrogant that you think you can make a better chip? And then, if no one uses this one, why would they use yours?So why does no one use it?The software is terrible! There\u2019s kernel panics in the driver. You have to run a newer kernel than the Ubuntu default to make it remotely stable. I\u2019m still not sure if the driver supports putting two cards in one machine, or if there\u2019s some poorly written global state. When I put the second card in and run an OpenCL program, half the time it kernel panics and you have to reboot.That\u2019s the kernel space, the user space isn\u2019t better. The compiler is so bad that clpeak only gets half the max possible FLOPS. And clpeak is a completely contrived workload attempting to maximize FLOPS, never mind how many FLOPS you get on a real program (usually like 25%).The software is called ROCm, it\u2019s open source, and supposedly it works with PyTorch. Though I\u2019ve tried 3 times in the last couple years to build it, and every time it didn\u2019t build out of the box, I struggled to fix it, got it built, and it either segfaulted or returned the wrong answer. In comparison, I have probably built CUDA PyTorch 10 times and never had a single issue.Where does the tiny corp come in?Forget all that software. The RDNA3 Instruction Set is well documented. The hardware is great. We are going to write our own software.If you were to tape out your own chip, you\u2019d be struggling with both hardware bugs and software bugs, and you wouldn\u2019t be sure which one it is. Here, you have a good idea, and have the AMD provided driver as an open source reference.This is life on easy mode, and I still doubt any of those AI startups could have done it. This is what the tiny corp is going to do to start. Build a framework, runtime, and driver for AMD chips.AMD on MLPerfEvery couple months, MLCommons hosts MLPerf, a competition to train a common set of models fast. AMD has never been on the list, not because the hardware can\u2019t do it, but because the software can\u2019t. The list is dominated by NVIDIA, but Google TPUs, Intel CPUs, Intel Habana, Huawei Ascend, and Graphcore IPU have made appearances. Never seen AMD.Our short term goal is to get AMD on MLPerf using the tinygrad framework.But how do you make money doing that?We don\u2019t. We raised $5M from an amazing set of investors who are aligned with real value creation.We make money selling computers for more than they cost to make. (preorder a tinybox today)If we succeed at this project, we will be on the cutting edge of non NVIDIA AI compute. We have the ability to make the software, and that\u2019s the hard part. Comparatively, taping out chips is easy. If we even have a 3% chance of dethroning NVIDIA and eating in to their 80% margins, we will be very very rich.How can I help?We\u2019re hiring software engineers to work on tinygrad. In person in San Diego, looking for people who want to work hard and build something incredible. Come work on an open source project that, if done right, will play a role in the joint destiny of humanity and its machines.I don\u2019t want to live in a world of closed AI running in a cloud you\u2019ve never seen, I want everyone to have an AI that they own, both training and inference. I want compute to be available from 50 different companies all competing to drive the price to zero. And I want an open source framework to run cutting edge AI on any one of those 50 chips as seamlessly as Linux supports 50 network cards.If Elon has the FSD Chip and Dojo for Autopilot and Tesla\u2019s robots, we have the tiny corp for openpilot and comma\u2019s robots. comma, along with 100s of other companies, will need computers both big and small for training and inference. We will sell them those computers. If NVIDIA is the Apple, we are the Android.E-mail george@tinygrad.org to apply and I\u2019ll send you the programming challenge. If you feel like you missed the beginning of comma, get in on this. The fun of the beginning is how much you get to shape.",
    "summary": "- The tiny corp is a new computer company that aims to commoditize the petaflop and democratize access to AI compute.\n- The company's flagship product, tinygrad, is a simple yet powerful neural network framework that enables efficient inference on a variety of hardware.\n- The tiny corp plans to leverage the RDNA3 Instruction Set and build its own software stack for AMD chips, with the goal of getting AMD on the MLPerf competition and disrupting the dominant position of NVIDIA in the AI compute landscape.",
    "hn_title": "The tiny corp raised $5.1M",
    "original_title": "The tiny corp raised $5.1M",
    "score": 559,
    "hn_content": "George Hotz's tiny corp has raised $5.1 million for a new project that aims\u00a0to provide an alternative to Nvidia graphics cards. Hotz\u00a0is well-known in the tech industry for creating buzzy but unfulfilled projects.\u00a0Some sceptics say Hotz's ideas are often pie in the sky and his attempts lack resources and attention span to fulfil adequately. Others have criticised his beliefs about the ease of fixing social media platforms such as Twitter. A broader discussion within the post involves why technical people mistakenly assume tasks such as software development are relatively easy when in reality the opposite is often true.Technology enthusiasts on social media platform HN discussed the challenges that big tech companies face in creating self-driving cars, with some discussing the merits of a nimble and fast approach to development, while others questioned the business viability of such products. Comma, an AI self-driving car company founded by hacker George Hotz, was mentioned several times, with some praising the product and others questioning the company's business model. The conversation also touched on the importance of software in designing AI chips. The blog post went viral on social media and HN, with users offering opinions on everything from current developments in technology to the efficacy of self-driving cars.A post on HN discusses the difficulty of hiring good software engineers for AI chip companies and suggests that this may be due to a sociological problem, where HW people don't value SW people. Many prominent AI chip companies were founded by ASIC guys who see SW as making more money and don't attract the experienced SW people currently working in the field of AI systems, resulting in a downward spiral. The author suggests that this could be solved by flipping the model on its head and starting with SW first. In a separate thread, another user highlights a successful AI chip company where the founders hired a good software team and doubled the chip's performance purely through enhancing the compiler with previous experience, while a post discussing George Hotz's startup mentions that their TinyBox will run LLaMA on commodity hardware. Finally, a commenter responds to a complaint about Hotz's new startup having a no-remote-work policy, stating that in-person work increases communication bandwidth and team cohesion but decreases the talent pool.Tech enthusiasts debate the benefits and drawbacks of on-site vs. remote work for specialized hardware development, following a tweet from George Hotz offering remote work for contributors to his new AI chip project. Hotz believes that current AI accelerators are underutilized due to issues with software. Some argue that fixing these issues in open-source software would make these accelerators more widely available, while others point to the difficulty in optimizing software for specific hardware and the benefits of in-person collaboration for hardware development. Additionally, some discuss the state of AMD's GPU accelerators for machine learning and their competition with NVIDIA.George Hotz, an entrepreneur, programmer, and hacker, is creating an open-source AI chip designed for machine learning (ML) training: tinygrad. Tinygrad is a simple framework with a PyTorch-like frontend that enables ML models to operate at maximum efficiency. The most significant advantage of the tinygrad IR is that it has only 12 operations, all of which are ADD/MUL only. The computing in ML is not general purpose computing; 95% of models in use today have all their compute and memory accesses statically computable. The tinybox is built with AMD\u2019s Radeon VII graphics cards, which have a higher Double Precision FLOPS performance than TitanRTX or Tesla V100 GPUs, or AMD\u2019s MI60. It costs $15K for 738 TFLOPS using six of them.- George Hotz, the founder of Comma AI, is developing a server that can run 6 AMD Radeon RX 7900 XTX GPUs.\n- The server is called \"TinyBox\" and is available for pre-order at $15,000.\n- Hotz plans to use his machine learning library, \"tinygrad,\" to power the TinyBox.\n- The TinyBox can run large machine learning models such as OpenAI's GPT-3 and can compete with NVIDIA's DGX A100.\n- AMD's lack of investment in software and drivers have left them unable to compete in AI, creating a market opportunity for Hotz's TinyBox.",
    "hn_summary": "- George Hotz's tiny corp raised $5.1M for a project that aims to provide an alternative to Nvidia graphics cards.\n- A broader discussion involves the difficulty of software development and the importance of software in designing AI chips.\n- The founder of Comma AI is developing a server called \"TinyBox\" that can run 6 AMD Radeon RX 7900 XTX GPUs, which can compete with NVIDIA's DGX A100."
  },
  {
    "id": 36063943,
    "timestamp": 1684962839,
    "title": "The brand new Thunderbird logo",
    "url": "https://blog.thunderbird.net/2023/05/introducing-the-brand-new-thunderbird-logo/",
    "hn_url": "http://news.ycombinator.com/item?id=36063943",
    "content": "AnnouncementsAdd-onsReleasesDOWNLOADTHUNDERBIRDDONATESEARCHANNOUNCEMENTSTHUNDERBIRDIntroducing The Brand New Thunderbird Logo!May 24, 2023Ryan Sipes12 responsesHello Thunderbird Family! After nearly 20 years, we are thrilled to share a completely redesigned Thunderbird logo that honors our history and vital connection to Mozilla, while carrying us forward into the next 20 years.It\u2019s no secret that after many years of being viewed as stagnant, Thunderbird is enjoying a resurgence. Our project is thriving with a renewed sense of purpose, and we see an invigorating energy bubbling up from our users, our community of contributors, and our core team. Just like the software, the current Thunderbird logo has seen small, iterative improvements throughout the last 20 years. But now the software is evolving into something more modern (while retaining its powerful customization) and we believe it deserves a fresh logo that properly represents this revitalization. But you should never forget your roots, which is why we asked Jon Hicks, the creator of the original Firefox and Thunderbird logos, to re-imagine his iconic design in light of Thunderbird\u2019s exciting future. Here\u2019s a look at our new logo across Linux, Windows, macOS, Android, and iOS.Yes, we have officially added an iOS version of Thunderbird to our future development roadmap. Expect more concrete news about this toward the end of 2023.And here\u2019s a glimpse of what Thunderbird for Android will look like on an Android device, sitting next to our best friend Firefox:When can you see it integrated with Thunderbird itself? Our plan is to incorporate it into Thunderbird 115 (code-named \u201cSupernova\u201c) this summer. During the next few months, we\u2019ll also gradually redesign our website and update the branding on various social channels and communication platforms.We understand that change can be uncomfortable, but we hope you agree this is a positive new look for the project. I encourage everyone to do what we did throughout this process: to live with the new design for a while. Let it breathe, let it sink in, and let us know what you think after a few days.We all have a soft spot for the old Thunderbird logo (which I affectionately call the \u201cwig on an envelope\u201d), but our project is changing in big, positive ways, and we want to clearly show that to the world with a beautiful, revitalized logo and icon.So here\u2019s to a bright future! On behalf of the entire team: thank you for taking this journey with us. We wouldn\u2019t be here without you.Ryan SipesThunderbird Product Manager Tags:ANDROIDSUPERNOVATHUNDERBIRDTHUNDERBIRD 11512 responsesJason Sneerwrote onMay 25, 2023 at 01:38Using a white background and slapping the icon on top of that is just lazy\u2026 The adaptive icon exists for Android, which means that you can use a glyph on top of a colored background. Why go with the white background?ReplyJason Evangelhowrote onMay 25, 2023 at 06:39Read the entire post? There is an Android Adaptive version.ReplyWolfgangwrote onMay 25, 2023 at 02:34I think the new logo looks great! Already excited for the launch of Supernova and really looking forward to using it both on Windows and Android!ReplyWojtekwrote onMay 25, 2023 at 04:02This looks awesome!ReplyDavid Thomaswrote onMay 25, 2023 at 04:36I love it.ReplyTimowrote onMay 25, 2023 at 05:50Looks great. Thank you!Replydcwd13wrote onMay 25, 2023 at 07:08rooster eats a letterReplyVadimwrote onMay 25, 2023 at 07:11Great icon refresh. Thank you!ReplyGianlucawrote onMay 25, 2023 at 07:26I really appreciate everything you are doing for TB, changing the logo is a brave and deserved moveReplyZushwrote onMay 25, 2023 at 07:40Why bird is so angry? Because of round-shaped envelope?..ReplyNiklas Bwrote onMay 25, 2023 at 08:15Love it! Can we have the option on Mac OS to _not_ have the rounded square background? That trend need to go away.ReplyNidowrote onMay 25, 2023 at 09:21I really like the new look. Gets in line with the new Firefox Browser Logo. But the original Idea still gets trough. Thanks a lot, I just get more and more excited for supernovaReplyLeave a ReplyYour email address will not be published. Required fields are marked *Comment *Name *Email *WebsiteSave my name, email, and website in this browser for the next time I comment.POST COMMENT",
    "summary": "- Thunderbird has a brand new logo designed by Jon Hicks, the creator of the original Firefox and Thunderbird logos.\n- Thunderbird is evolving into something more modern and adding an iOS version to its future development roadmap.\n- The new logo will be integrated with Thunderbird 115 this summer, and the branding on various social channels and communication platforms will be updated.",
    "hn_title": "The brand new Thunderbird logo",
    "original_title": "The brand new Thunderbird logo",
    "score": 519,
    "hn_content": "Mozilla Thunderbird has updated its logo for the first time in 19 years, with the new design created by the original Thunderbird designer, John Hicks. The new logo features a protective stance from the bird, conveying the idea that the message is personal and important. The logo also ties in with the design of Firefox, as both Mozilla products share the same shape. The updated logo has been well-received, with many users praising it for preserving the identity of the original logo while incorporating modern touches. However, some felt that the new logo looks like an angry baby bird, and criticized its lack of confidence and the roundness of its shape.Thunderbird, the open-source email client from Mozilla, has launched a new logo that received mostly positive feedback from users. The new design was influenced by the look of the Firefox browser's logo. Thunderbird's logo had not been updated for two decades. The change aims to keep the brand fresh and attract new users. Critics, however, argued that the new bird design looks aggressive and unfriendly compared to the friendly-looking old logo. Other users welcomed the change, saying that it's necessary to keep up with current design trends. Thunderbird is also reportedly working on an iOS version of its email client.The Tech Times editor discusses a thread about Thunderbird and recommends Thunderbird as a way to have a unified inbox across multiple email providers, searching all emails at once, using a slicker UI, and having a backup in case of service failure. Users also discuss their favorite email clients and reminisce about old email clients they've used, while some criticize Thunderbird's high CPU usage. Thunderbird is said to be decreasingly taking logo design cues from Betterbird. The editor predicts Thunderbird could be the natural client to the emerging fediverse, bundling related functions for users. However, users report difficulties removing threading as the default for new accounts or having an easy button to remove threading.Mozilla's email client, Thunderbird, has unveiled a new logo. The design features a blue, stylized bird with a sleeker appearance than the previous logo. Some have criticized the similarity in appearance to the Firefox browser's logo. Thunderbird is a desktop email client that integrates well with operating systems and supports multiple email accounts. It also offers in-built privacy features like the ability to avoid loading images or certain trackable files. Thunderbird was an independent project for many years, but is now operating under Mozilla as a subsidiary. The new logo is set to release with version 115 of Thunderbird over the summer. The redesign marks a shift in improving the brand relationship between Mozilla and Thunderbird.",
    "hn_summary": "- Mozilla Thunderbird updates its logo for the first time in 19 years, with positive and negative feedback from users.\n- The updated logo ties in with the design of Firefox and aims to keep the brand fresh and attract new users.\n- Thunderbird is also reportedly working on an iOS version of its email client and is predicted to become a natural client to the emerging fediverse."
  },
  {
    "id": 36059247,
    "timestamp": 1684941137,
    "title": "Simple exercise to eliminate gastroesophageal reflux (2022)",
    "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9106553/",
    "hn_url": "http://news.ycombinator.com/item?id=36059247",
    "content": "Journal List Cureus v.14(4); 2022 Apr PMC9106553As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with, the contents by NLM or the National Institutes of Health. Learn more about our disclaimer.Cureus. 2022 Apr; 14(4): e24122.Published online 2022 Apr 13. doi: 10.7759/cureus.24122PMCID: PMC9106553PMID: 35573490A Simple Exercise to Strengthen the Lower Esophageal Sphincter and Eliminate Gastroesophageal Reflux: An Autobiographical Case ReportMonitoring Editor: Alexander Muacevic and John R AdlerEric Karrfalt1Author information Article notes Copyright and License information DisclaimerGo to:AbstractA novel exercise is described for resistance training of the lower esophageal sphincter. Resistance is provided by gravity as food is swallowed and pushed up an incline into the stomach. The incline is established by kneeling with the head bowed lower than the stomach. After several months of daily repetitions, symptoms of gastroesophageal reflux ceased and the exercise was discontinued without relapse. Keywords: esophageal resistance training, lower esophageal sphincter, autobiographical case report, gastroesophageal reflux, eliminate gastroesophageal refluxGo to:IntroductionGastroesophageal reflux results from weakness or relaxation of the lower esophageal sphincter (LES) [1]. Personal experience with this problem lead me to think about it, repeatedly. I came to entertain the hope that strengthening the LES might alleviate my reflux problem. Voluntary muscle can be strengthened by resistance training, but involuntary muscle like the LES characteristically cannot be strengthened in this manner. The esophagus, however, offers a special case. The swallowing process begins as a voluntary act which ultimately initiates a peristaltic wave of involuntary contractions through the smooth muscle in the lower two-thirds of the esophagus [2]. It occurred to me that that the LES might be strengthened if it were made to do a little extra work. The novel resistance training described here accomplishes this by requiring the LES to push food upward against gravity.Go to:Case presentationI had been experiencing gastroesophageal reflux for a number of years. The symptoms became dramatically worse immediately after an endoscopy in 2016 in which an inexplicably large biopsy (estimated to be 0.3 x 0.5 x 0.2 cm, but not actually measured in three dimensions) was taken from my esophagus to rule out Barrett\u2019s Esophagus. Nonetheless, all of my symptoms were eventually substantially controlled with ranitidine (150 mg three times a day) and a bed wedge. The ranitidine was a minor nuisance, but even after several refinements, the bed wedge remained intolerable.Eventually, I devised the following regimen with the intent of providing the LES with some resistance training. The resistance was provided by positioning my head below my stomach in a kneeling posture. This required food being swallowed to be pushed up an incline. I began eating part of each breakfast (oatmeal) and sometimes lunch (a sandwich) in the exercise position. I would kneel on a platform (which happened to be 6 \u00bd\u201d high), take a normal mouthful, chew it as needed, and prepare to swallow. I would then lay my forearms and the backs of my hands on the floor, rest my head on my hands, and complete the swallowing process. With a little practice, I was soon able to initiate and complete the swallowing process with my head resting on my hands on the floor. I did not attempt to determine what the optimal height of the platform might be or if, indeed, any was necessary. Sixty-eight days after beginning daily LES exercises, I noticed that I could bend over at the hip and pull weeds in my garden without acid running into the back of my throat. This was not possible the previous year. I then tried sleeping without the bed wedge but found that it was still needed. I interpreted these observations to indicate that the LES was getting stronger, but still not strong enough. For about five more months I remained ambivalent as to whether the exercise would fully correct my problem and eventually sought help at the Cleveland Clinic. A 24-hour pH and manometry test was done, which yielded completely normal results. I then discontinued the use of the bed wedge and now have no symptoms that I can attribute to gastroesophageal reflux. I considered the possibility that a continuing training regimen might be necessary to maintain full LES function, so rather than risk a relapse, I continued to do the exercise a few times each week for a few months and then less frequently. I have not done the exercise at all for the past two years with no relapse.My elimination of gastroesophageal reflux includes benefits beyond the ability to sleep comfortably on a horizontal surface. I can again do vehicle maintenance lying on my back with no esophageal discomfort. On one occasion I needed to do a minor repair to the gutter on my house in a location where the placement of a ladder was somewhat inconvenient. I was able to do the repair in about 10 minutes while lying prone looking downward near the edge of the sloping roof with no esophageal discomfort. Estimating from the pitch of the roof and my orientation with respect to the edge of the roof, my stomach was about two to three inches higher than my throat during this repair. These observations further attest to the restored competence of my LES.Go to:DiscussionIn retrospect, I probably should have tested sleeping without the bed wedge at intervals after I first noticed an improvement in LES function. But once I had determined that I no longer needed the bed wedge, I would probably never have gotten the pH and manometry test, and this report, were it to be made, would have been based entirely on my interpretation of symptoms.These observations clearly constitute proof of the concept of the LES exercise. Hopefully, others will benefit from this exercise, and their experiences may become the basis of a standardized protocol for its use. Many details, which I simply guessed at, could be optimized by systematic study. The height of or need for a kneeling platform, the frequency and duration that constitute effective training sessions, and a time frame in which results may be expected might all be determined. The texture and amount of food being swallowed may have some significance. It also remains to be seen if any contraindications exist for the LES exercise. This exercise is probably quite safe for any otherwise healthy person, but anyone beginning this exercise should use reasonable caution while developing his or her technique so as to avoid any discomfort.Go to:ConclusionsThe resistance training exercise to strengthen the LES has many desirable attributes. It may eliminate the cause of gastroesophageal reflux rather than treat its symptoms and may well be a permanent solution to the problem. The exercise involves little or no risk or cost, and its use may be beneficial to many people.Go to:NotesThe content published in Cureus is the result of clinical experience and/or research by independent individuals or organizations. Cureus is not responsible for the scientific accuracy or reliability of data or conclusions published herein. All content published within Cureus is intended only for educational, research and reference purposes. Additionally, articles published within Cureus should not be deemed a suitable substitute for the advice of a qualified health care professional. Do not disregard or avoid professional medical advice due to content published within Cureus.The authors have declared that no competing interests exist.Go to:Human EthicsConsent was obtained or waived by all participants in this studyGo to:References1. GERD (Chronic Acid Reflux) [ Feb; 2022 ];https://my.clevelandclinic.org/health/diseases/17019-gerd-or-acid-reflux-or-heartburn-overview 20222. Anatomy and physiology of feeding and swallowing: normal and abnormal. Matsuo K, Palmer JB. Phys Med Rehabil Clin N Am. 2008;19:691\u2013707. [PMC free article] [PubMed] [Google Scholar]Articles from Cureus are provided here courtesy of Cureus Inc.",
    "summary": "- A case report outlines a novel exercise for resistance training of the lower esophageal sphincter (LES) to eliminate gastroesophageal reflux.\n- The exercise requires swallowing food in a kneeling position with the head bowed lower than the stomach, providing resistance against gravity for the LES.\n- After several months of daily repetitions, symptoms of gastroesophageal reflux ceased and the exercise was discontinued without relapse, indicating potential permanent solutions for the problem.",
    "hn_title": "Simple exercise to eliminate gastroesophageal reflux (2022)",
    "original_title": "Simple exercise to eliminate gastroesophageal reflux (2022)",
    "score": 489,
    "hn_content": "A user on Hacker News shared their experience with gastroesophageal reflux and how they eliminated it through exercises. They positioned their head below their stomach and kneeled on a platform while eating, pushing the food up an incline. They completed the swallowing process with their head resting on their hands on the floor. This helped to train the muscles around the entrance of the stomach to keep the acid in the stomach. Other users in the comments shared their own experiences with acid reflux and suggested other methods like drinking alkaline water, taking medication, eating mint, or doing handstands. Overall, the post provides an interesting and alternative method to deal with gastroesophageal reflux.Discussion on the causes and potential solutions for gastroesophageal reflux disease (GERD), including personal experiences and medical advice. Factors such as diet, stress, obesity, and posture are considered potential causes. Suggestions for managing GERD include medication, dietary changes, exercise, and breathing and relaxation techniques. Anecdotes regarding the effectiveness of various treatments are shared. The article itself is not particularly informative or newsworthy, but the discussion in the comments section may provide useful insights for individuals suffering from GERD.A report shows that performing a \"squat and gulp\" exercise, where food is eaten while kneeling and leaning forward, may help reduce acid reflux symptoms. While the method lacks scientific backing, the author tested the technique on themselves and others, with positive results. However, caution should be practiced when trying this technique as there may be potential aspiration or choking risks. Other commenters suggest alternative exercises or strategies to treat acid reflux, such as modifying diets, trying \"inspiratory muscle training,\" or testing for an H. Pylori bacterial infection. It is essential to seek medical advice and not to rely solely on anecdotal evidence or unproven strategies.Readers discuss the causes and alternative remedies for gastroesophageal reflux disease (GERD). One reader states that the lack of acid in the stomach may cause GERD and suggests an LES exercise to help alleviate symptoms. Other readers share personal anecdotes and recommend various remedies such as dietary changes, sleeping positions, supplements, and medication. However, there is no mention of any new or exciting developments in the field.Individuals with GERD or acid reflux have found success with a yoga pose that involves eating upside down. Other practices such as diaphragm/breathing exercises and sodium alginate have also been helpful for some individuals. On the other hand, some individuals have found that PPIs, such as Nexium, have negative side effects, including causing bone fractures and inhibiting vitamin D uptake. Eating a high-fiber diet or adding Indian gooseberry to one's diet may also help alleviate GERD symptoms. Other commenters have suggested exercises to help with issues such as eustachian tube dysfunction.",
    "hn_summary": "- Users share personal experiences and suggest various methods, including dietary changes, medication, and exercises, for managing gastroesophageal reflux disease (GERD).\n- The article discusses a \"squat and gulp\" exercise that may help alleviate GERD symptoms, though caution is advised due to potential risks.\n- Alternative remedies such as yoga poses, breathing exercises, and sodium alginate have also been successful for some individuals, while medications like PPIs can have negative side effects."
  },
  {
    "id": 36061574,
    "timestamp": 1684950099,
    "title": "The long road to recover Frogger 2 source from tape drives",
    "url": "https://github.com/Kneesnap/onstream-data-recovery/blob/main/info/INTRO.MD",
    "hn_url": "http://news.ycombinator.com/item?id=36061574",
    "content": "IntroductionBefore getting into how to recover data, I want to introduce this guide with my story explaining what happened.This should serve as a warning, because learning from my mistakes will prevent you from making your recovery process harder.This took place over the span of months, and many details and side-tangents have been omitted for clarity.My StoryI received a tape containing the \"end of project\" development for Frogger 2: Swampy's Revenge, part of my favorite childhood franchise.This tape is believed to be the only backup of the final game source code, game assets, and other development data.As one could imagine, this is priceless to recover. But how does one even read/write data from a tape? Why did they even use tapes?The average hard drive size in 1999/2000 seems to have been around 10GB, and hard drives are not known for their longevity.OnStream tapes were a very appealing proposition because they could offer 50GB cartridges (25GB when uncompressed), which were cheaper than most hard drives!Tapes are great for backups and have very high longevity when stored properly. And they could buy a tape drive that they could put into their computer just like a CD or floppy disk drive.Unfortunately, while tapes have good longevity, these tape drives did not. OnStream, the company who made the tapes and tape drives, ceased operations in 2003, after only just releasing their first drive in 1999.These drives are uncommon, especially the model which can use 50GB tapes. So, the first big hurdle was finding a compatible tape drive.Tape Picture:1) Finding a compatible/working tape driveI was lucky to know that I needed an \"OnStream SC-50\" tape drive, because that was the model written on the label of the tape.And, I found one single business selling this drive online, so I pounced. Unfortunately, no matter what I did, when I put the tape into the drive, it would appear to read, then eject automatically.I tried different operating systems, different software versions, different sofware, different drivers, etc. Nothing worked.Eventually, I concluded the drive was broken, and looking inside proved this to be true, as the rubber pinch roller had melted.My efforts to fix the drive were let's say unsuccessful, so I looked for another drive. Because there were no other options, I got an ADR-50e. That drive was advertised as compatible with these tapes. When that drive worked with my \"test tape\" (a blank tape I had purchased exclusively for testing), but still refused to read the Frogger tape.At this point, I incorrectly concluded that there must have been a problem with the tape itself and it were damaged.What had actually happened was that the ADR-50e drive was advertised as compatible, but there was a caveat. It was only compatible with tapes which were written with an ADR-50e drive, or tapes written with certain software.At this point, I didn't even know what software had been used to write the data, let alone this obscure quirk of the tape drive, it was advertised as compatible after all.So assuming the tape was defective, I sent it in to a professional data recovery company, believing it to be damaged. This was a HUGE mistake...Melted Pinch Roller:2) Professional Data Recovery SucksThey were the professionals, right? They made it seem like they had a special method of recovering data for this kind of tape!Their website said specifically that they could recover data from OnStream tapes, and they had good non-botted reviews as far as I could tell.Perhaps they were fine for common data storage types like hard drives or SSDs. Those should be easier since there are advanced recovery tools available for them.However, I do not think I will ever consider sending tapes into a professional data recovery company again, even for more common digital tape formats.What this company should have done:This company should have not advertised the capability of recovering data from these tapes, a capability they clearly did not possess.The company should have made the risks clear they they didn't know if whatever machine they used was even capable of recovering data from the tape instead of pretending they knew it was.The company should not have told me they could recover data from this tape.What this company actually did:This company specifically advertised that they could recover data from OnStream tapes. Bullshit.They clearly didn't even have an OnStream SC-50 tape drive, because if they did they could have just used the official Linux kernel from 10-20 years ago which came with a driver that could have dumped this data.Over the span of about a month, I received very infrequent and vague communications from the company despite me providing extremely detailed technical information and questions.Eventually after a month and I had realized what the real problem was, and because things didn't sound good in data recovery, I asked for it to be sent back.I was told that \"it will perform the way it did when we received it\". What a fucking lie. The tape I got back had been ripped in several spots, and spliced back together.If this were like an audio tape or something archaic maybe it would have worked, but the tape I got back had problems which it did NOT have when I sent it in.They were never capable of recovering the data:OnStream used technology that no other tape drives did. They had a dedicated data processing chip (ASIC), which was designed specifically for their machines, as opposed to the common chips available on other drives such as Travan.The entire data processing pipeline from the tape to digital data uses custom hardware designed from scratch for OnStream tape drives, because it let OnStream get way more capacity than any other tape company could at the time.None of it was shared in any other tape drive. This effectively means that the only way to read data off of one of these tapes is to use one of the original tape drives, or the hardware inside of one.In other words, whatever machine they put the the tape I was recovering on was NOT compatible with OnStream tapes.I don't know why they claimed to be able to recover data from OnStream tapes, but it's downright false advertising for their website to claim they can recover OnStream data.Their actions show they clearly didn't know how to recover data from OnStream tapes.3) The RealizationUnfortunately, this is only clear in hindsight.While the tape was in data recovery, I had received a few tapes for another game, and I was able to easily dump two with the 'OnStream ADR-50e' drive. It was also determined the software which had been used to write the data was ARCServe 2000. Because the labels on the tape had the tape drive model written, I realized the reason I couldn't read the Frogger 2 tape. It was not because the tape had problems, but because I did actually need an SC-50 drive instead of an ADR-50 drive.It was too late. The damage had been done by the data recovery company, and upon finally locating another SC-50 tape drive (this time a working one), my worst fears were confirmed.The splices didn't just prevent reading from the spliced area, but the splices impacted the drive so severely that upon just putting the tape into the drive, it would enter a state of infinitely trying to re-read the spliced area.By this point I had found documentation on the commands which could be sent by a computer to a drive, and what the drive would do.But because the tape would enter into an infinite retry loop when just inserted into the drive (a process henceforth referred to as \"initialization\"), the drive wasn't ever even reaching a state where the computer could tell the drive anything about what to do.This is where the story should probably have stopped. Given up and called it a day, right? Maybe, but I care about this data, and I happen to know a thing or two about computers.Splice Picture:At least they made good quality splices.4) It's hacking time, baby.Time and time again, I've come to really learn that sometimes if you want something done right, sometimes you really do need to do it yourself.This really annoys me, because professional data recovery is expensive. If I hadn't chose a company which had a \"no data recovered, no charge\" policy, I would have paid thousands of dollars for them to screw up the tape.That's insane, and I'm still upset that they told me they could even recover the data on the tape. It feels like they didn't take their job seriously.But if you asked me at the time, you wouldn't have known it upset me. I used it as further motivation instead.Months of effort was spent understanding the drive, studying its SCSI command interface, reverse engineering firmware, reading documentation, digging up patents, etc.While much was learned about the drive, the major breakthrough came just after midnight on April 5th 2023 and ended up being significantly simpler than the other attempted methods.If the initialization process is the problem, then what if we were to run the initialization process with a working tape? Then, by tricking the drive sensors we could switch the tape without the drive knowing a swap occurred.Using this trick, all undamaged portions of the tape were dumped successfully.Modified Drive:Tape Dumping:5a) ARCServe Sucks TooNow that I had the data off the tape, there was still one problem, it wasn't in a usable format.The data which comes off the tape is formatted in whatever way the software which wrote it chose.Unfortunately, that means every single compatible backup software product made their own proprietary format, including ARCserve.This is the challenge of doing a raw dump, if I were able to use the original software, it would automatically give back the data in a usable form. Or at least, it would if ARCserve wasn't garbage! It turns out ARCserve is broken. It isn't even capable of reading the OnStream tapes it writes, even when they are not damaged.This issue doesn't affect tapes written with the ADR-50 drive, but all the tapes I have tested written with the OnStream SC-50 do NOT restore from tape unless the PC which wrote the tape is the PC which restores the tape. This is because the PC which writes the tape stores a catalog of tape information such as tape file listing locally, which the ARCserve is supposed to be able to restore without the catalog because it's something which only the PC which wrote the backup has, defeating the purpose of a backup.Yet, despite ARCserve showing a popup which says \"Restoration Successful\", it restores up to the first 32KB of every file on the tape, but NO MORE.For an undamaged several gigabyte tape which takes two hours for ARCserve to read, it will restore about 1MB of data total.5b) Making it usableIn order to convert the files into a usable format (.zip), I had to write a program to convert the tape dumps from the ARCserve format into a .zip.Thankfully, the ARCserve format wasn't very complex and I figured it out pretty quickly.However, there was something strange. An abnormally high amount of files were exporting improperly.After spending some time analyzing data and thinking, it became apparent that ARCserve was using an undocumented mode for reading the tape. This is explained in more depth in the guide, but ARCServe was using a feature not documented in the official OnStream driver development document.This undocumented feature allowed ARCServe to read and write data in a completely different pattern from what the documentation described.Even after changing my program to read data in the undocumented way ARCserve did it, I still saw errors which I didn't expect to see.This time it didn't take long to see that the missing data was in a position of the tape which the documentation explicitly states \"no user data can be recorded\".So, I modified my tools and the dumping program again to read the area \"where no user data can be recorded\".Finally, after months of reverse engineering of ARCServe, the drive firmware, and other garbage, the program worked, and the data was saved.Source code for the extraction program is included here.Recovered Files:6) The ResolutionIn the end, the recovery was an unquestionable success. Thank you everyone who helped with this project, without your help who knows how long it would have taken or if the data would have even been recovered.All the important data such as the VSS repository backup (source code history), final game assets, tools, and more were saved.The tape was the only backup for those things, and it completes Frogger 2's development archives, which will be released publicly.It might sound bad that approximately 12GB of the 15GB written data was recovered, but this is misleading.A couple thousand files were damaged, but they make up less than 5% of the total files on the tape.Nearly all of the damaged files were either found in another CD backup or were duplicated on another part of the tape.There were only 15 files which were not perfectly recovered, and only one was noteworthy, a CD image of a PC game build from 1 month after release.Having recovered 58149 out of the 58164 files on the tape, this adventure can only be considered a success.Here's what the damage looks like to the Frogger 2 tape, showing the significant damage and how lucky the recovery was.7) Further Technical DetailsThe hope is to share all the information we learned so that it might help someone else, and let those who are less experienced recover data too.Further technical details are scattered throughout the repository, and this guide, and I am willing to answer questions.",
    "summary": "- The author shares their story of recovering the source code for the game Frogger 2 from an OnStream tape drive. \n- Finding a compatible tape drive was the first challenge, and a professional data recovery company worsened the situation. \n- With months of effort and reverse engineering, the author was able to extract the data from the tape, but had to overcome challenges with proprietary file formats and undocumented tape drive features.",
    "hn_title": "The long road to recover Frogger 2 source from tape drives",
    "original_title": "The long road to recover Frogger 2 source from tape drives",
    "score": 464,
    "hn_content": "The article discusses the difficulty of recovering data from tape backups due to the lack of proper cataloging and indexing. The issue specifically affects tapes written with the OnStream SC-50 drive. Many commenters suggest standard solutions for this problem, such as regularly writing the backup catalog to tape and being able to rebuild the catalog from scratch. Some commenters express frustration and disbelief at the lack of foresight in the design of the backup solution. Additionally, the article highlights the larger issue of software formats constantly changing and the difficulties in maintaining backwards compatibility. By maintaining standard formats, data can be accessed more easily across different computer systems.Old technologies, like HTML1.0, the PE/WIN32 subset, gzip, and zip, are still used today and likely to be still valid in the future. While new competitors (like webp, avif, and zstd) may arise, it's hard to predict whether they will supplant the old standards. Future-proofing technology is challenging due to the need for independent bootstrapping, separating signal from noise, and creating stable representations of volatile formats. While physical mediums like punched cards or archival paper may last a long time, preserving software and its data is difficult and may rely on emulation or special tooling. However, with cloud storage, the outdated hardware problem is no longer a significant issue. The primary way to ensure longevity is to use well-documented, ubiquitous, and well-maintained software, like the JVM or Java.A thread on HN discusses the challenges of long-term data retention and archival storage. The discussion highlights concerns about the loss of information, backwards compatibility, proprietary formats, and the challenges of preserving data on various mediums, including magnetic tapes, optical disks, and flash memory. Some users share their stories of successfully retrieving old data despite the difficulties, while others express concerns about the lack of open-source backup systems and the prospect of losing access to digital archives due to resource depletion or societal collapse. The discussion recommends storing data in multiple formats and locations, employing a mix of storage technologies, and staying vigilant about compatibility and preservation issues.- LTO is a good choice for compatibility for tape backups\n- Test restores are necessary for success\n- LTO drives have backward compatibility\n- The data recovery company that irreparably damaged a customer's tape is allegedly https://www.datarecovery.net/tape-data-recovery.aspx\n- CDs have a limited lifespan for data retention\n- A tape backup was the only backup for Frogger 2's development archives, which will be released publicly\n- Copyright infringement is a possibility for those who don't possess a backup themselves.",
    "hn_summary": "- Difficulty in recovering data from tape backups due to lack of cataloging and indexing\n- Older technologies still valid in the future, but preserving software and data is difficult\n- Thread on HN discusses challenges of long-term data retention and archival storage, recommends using multiple formats and technologies"
  },
  {
    "id": 36056376,
    "timestamp": 1684926760,
    "title": "Don't open the details in a modal window, have it be a separate page",
    "url": "https://youdontneedamodalwindow.dev/",
    "hn_url": "http://news.ycombinator.com/item?id=36056376",
    "content": "YouDontNeedAModalWindow.devYou don't need a modal windowJust make a separate page.Maybe you have a master/detail setup on your web app. There's a list of products/documents/whatever, and clicking on one brings up the details.ID NameXYZ-173 Example 1 DetailsXYZ-489 Example 2 DetailsDon't open the details in a modal window. Have it be a separate page.Modal windows can't be bookmarked or shared as links. Deep linking can be added to modals, but it's complex. What will show up in the background when the link is followed? How much application state needs to be restored? How will the user be confident that it will work?Modal windows can't be opened in a new tab. Even if you implement this, you'll be forcing users to duplicate the page underneath.Modal windows make the Back button confusing. Will it close the modal and return to the page in the background, or return to the page you were before that?Modal windows are hard to get right. The \"final boss of accessibility\". If you have access to <dialog>, it's easier \u2014 if you need to support older browsers, good luck. There's a very good chance the modal window you have in production has dealbreaking bugs.Consider if the modal window is really the best choice. There are many reasons modal dialogs are used when they're not appropriate:To compensate for slow page loadWeb apps should pass the \"refresh test\": If I sneak up behind your user and refresh their tab, their frustration should be negligible and no data should be lost.Because it seems easyMany UI toolkits offer modals out-of-the-box. A developer might be tempted to reach for it to zhoozh up the app. Really make it pop, you know.Because it looked good in the mockupYou're designing a website, not a Figma artboard.Modal windows can be used for views that don't constitute a \"resource\" or correspond to a domain entity:AlertsConfirmation dialogsForms for creating/updating entitiesBonus: RESTful UIYouDontNeedAModalWindow.dev by Deniz Ak\u015fim\u015fek (@DenizAksimsek@twitter.com, @dz4k@indieweb.social). Contact deniz@dz4k.com for suggestions.",
    "summary": "- Using a modal window for detailed content is not necessary. Instead, use a separate page.\n- Modal windows can't be bookmarked or shared as links, and opening them in a new tab can be confusing.\n- Modal windows can be difficult to implement accurately and may not always be the best choice. Consider alternatives and ensure your web app passes the \"refresh test.\"",
    "hn_title": "Don\u2019t open the details in a modal window, have it be a separate page",
    "original_title": "Don\u2019t open the details in a modal window, have it be a separate page",
    "score": 396,
    "hn_content": "The article discusses whether or not to use modal windows for user interfaces and includes opinions from commenters. Some argue that modals allow for discrete tasks to be completed within the greater context of the app, while others contend that modals are overused and often cause problems. The article suggests that modals should be used for alerts, confirmation dialogs, and forms for creating/updating entities, while resources should be presented in a separate page that can be bookmarked. Good user experience requires careful consideration, and modals should be mentally cheap to open/close/reopen, with modal state preservation often being a part of that. Poorly done modals can cause issues with losing state, interacting with other parts of a page, and getting lost in detours. Ultimately, the decision to use a modal should depend on the specific use case.The article discusses the role of modal windows in user experience (UX) and argues that modals should be viewed as a tool in a UX toolbox rather than a default solution. The best option for an application is dependent on a multivariate consideration. Modals provide advantages such as polishing the user experience and enhancing user engagement with certain types of applications, such as round-based casino games. However, poorly-constructed modals can negatively impact the user experience. The article points out the importance of addressing accessibility issues for screen readers when designing modal windows, given the increasing use of mobile devices and touchscreens. It advocates for considering a balanced approach that provides the option to both open a new tab and view in a modal window.Accessibility support is important for all users, not just those with disabilities. Screen readers are just one use case, and other accessibility features include zooming, focus/reader modes, light/dark modes, performance, print views, copy/pasting content, SEO, searchability, and bookmarkability. Improving accessibility doesn't necessarily worsen the experience for non-disabled users, and there are many legitimate reasons not to use modal dialogs. The ADA in the US requires website accessibility compliance for businesses with over 15 employees. Accessibility requirements increase web development costs by an estimated 10%. However, supporting accessibility features in web development will only help users have a better experience. Improving accessibility doesn't affect only users with disabilities, as it can also assist users with small or touch-based devices or those with poor internet connectivity.The article mainly discusses the use cases of modal windows and their impact on the user experience, and proposes selectively avoiding them in certain situations. The conversation shifts to the legal requirements of accessibility, with regulations mandating compliance in the US and Europe by a certain date. The exemption of small businesses, the financial implications of compliance, and the difference between a stand-alone page and a modal are also discussed. Some users support the use of modals for certain functionalities, while others find them annoying and dismiss them entirely. The conversation emphasizes the need to consider user context and usability before implementing modal windows.The post discusses the use of modals in web design. Some argue that modals are useful for performing actions within a page and providing context, while others argue they can be unintuitive and inaccessible. The article suggests avoiding confirmation prompts as much as possible and using modals only for actions that don't require a resource URL. Some readers argue that modals can be optimized for accessibility by using the HTML <dialog> element and that modals are useful for preventing users from getting lost in pages. Others argue that modal pop-ups can be confusing and hard to get right. Overall, the post presents different perspectives on the use of modals in modern web design.Users prefer that UI shouldn't scramble their context and mental model of what's on the screen. The UI should support cmd+click to open a dedicated full page for that ticket, but that's the 10% use-case. A user's trackpad could be used in place of a middle mouse button. Modals are often fine for dialogs and fleeting interactions within the context of the page.  Users expressed frustration with pages opening suddenly in new tabs without taking the tab focus away from the user's current work. There should be a built-in open link as a popup option for browsers for the rare times you want that. Bulk edits of Jira issues should be converted into pop-ups since having the entire page reload 3 times is frustrating and takes too long.Users and developers discuss the use of modal windows in web design. The main argument is that modal windows can be confusing, do not persist after a page refresh, and should not be shared. However, some users argue that modal windows are useful for displaying small amounts of information and can be implemented well with proper knowledge and prioritization. The discussion highlights the importance of considering context and user needs in web design. Some suggest using history.pushState() and treating the modal window like navigation to a subpage/route in an SPA-style app. The article clarifies that modal windows can be useful for views that don't constitute a \"resource\" or correspond to a domain entity, such as alerts and confirmation dialogs.Web developers are reportedly \"overusing\" modal windows, which present information on top of an open window within a user interface. Critics of the technique argue that it is a poor emulation of native dialog boxes and can confuse users. Modals are argued to be spatially coupled to the parent window, so moving the modal can also move the parent window and make referring to information beneath it impossible. It is said modal windows do not always prove effective in deep-linking. However, proponents of the technique have argued that it can speed up page loading times and be used in certain situations to improve UX \u2013 for instance, creating forms for creating/updating entities.",
    "hn_summary": "- The article discusses the use cases and potential issues with modal windows in UI design, suggesting that they should be used only for specific tasks like alerts and forms.\n- Accessibility is an important consideration when designing modal windows, not just for screen readers but for other features like zooming and focus/reader modes.\n- There is a debate among developers about the usefulness and potential downsides of modal windows, with some arguing that they can be confusing and others advocating for their use in certain situations."
  },
  {
    "id": 36064971,
    "timestamp": 1684969376,
    "title": "SectorC: A C Compiler in 512 bytes",
    "url": "https://xorvoid.com/sectorc.html",
    "hn_url": "http://news.ycombinator.com/item?id=36064971",
    "content": "xorvoidhome githubSectorC: A C Compiler in 512 bytesSectorC (github) is a C compiler written in x86-16 assembly that fits within the 512 byte boot sector of an x86 machine. It supports a subset of C that is large enough to write real and interesting programs. It is quite likely the smallest C compiler ever written.In a base64 encoding, it looks like this:6gUAwAdoADAfaAAgBzH/6DABPfQYdQXoJQHr8+gjAVOJP+gSALDDqluB+9lQdeAG/zdoAEAfy+gIAegFAYnYg/hNdFuE9nQNsOiqiwcp+IPoAqvr4j3/FXUG6OUAquvXPVgYdQXoJgDrGj0C2nUGV+gbAOsF6CgA68Ow6apYKfiD6AKrifgp8CaJRP7rrOg4ALiFwKu4D4Srq1fonP9ewz2N/HUV6JoA6BkAieu4iQRQuIs26IAAWKvD6AcAieu4iQbrc4nd6HkA6HYA6DgAHg4fvq8Bra052HQGhcB19h/DrVCwUKroWQDoGwC4WZGrW4D/wHUMuDnIq7i4AKu4AA+ridirH8M9jfx1COgzALiLBOucg/j4dQXorf/rJIP49nUI6BwAuI0G6wyE0nQFsLiq6wa4iwarAduJ2KvrA+gAAOhLADwgfvkx2zHJPDkPnsI8IH4SweEIiMFr2wqD6DABw+gqAOvqicg9Ly90Dj0qL3QSPSkoD5TGidjD6BAAPAp1+eu86Ln/g/jDdfjrslIx9osEMQQ8O3QUuAACMdLNFIDkgHX0PDt1BIkEMcBaw/v/A8H9/yvB+v/34fb/I8FMAAvBLgAzwYQA0+CaANP4jwCUwHf/lcAMAJzADgCfwIUAnsCZAJ3AAAAAAAAAAAAAAAAAAAAAAAAAAAAAVao=Supported languageA fairly large subset is supported: global variables, functions, if statements, while statements, lots of operators, pointer dereference, inline machine-code, comments, etc. All of these features make it quite capable.For example, the following program animates a moving sine-wave:int y;int x;int x_0;void sin_positive_approx(){ y = ( x_0 * ( 157 - x_0 ) ) >> 7;}void sin(){ x_0 = x; while( x_0 > 314 ){  x_0 = x_0 - 314; } if( x_0 <= 157 ){  sin_positive_approx(); } if( x_0 > 157 ){  x_0 = x_0 - 157;  sin_positive_approx();  y = 0 - y; } y = 100 + y;}int offset;int x_end;void draw_sine_wave(){ x = offset; x_end = x + 314; while( x <= x_end ){  sin();  pixel_x = x - offset;  pixel_y = y;  vga_set_pixel();  x = x + 1; }}int v_1;int v_2;void delay(){ v_1 = 0; while( v_1 < 50 ){  v_2 = 0;  while( v_2 < 10000 ){   v_2 = v_2 + 1;  }  v_1 = v_1 + 1; }}void main(){ vga_init(); offset = 0; while( 1 ){  vga_clear();  draw_sine_wave();  delay();  offset = offset + 1;  if( offset >= 314 ){ // mod the value to avoid 2^16 integer overflow   offset = offset - 314;  } }}ScreenshotBut, how?When I started thinking about SectorC, I had just finished Deobfuscating OTCC with a lot of its ideas freshly loaded into my head. I also just had some healthy doses of justine.lol and Tom7 to inspire the absurdity of it all.Did I think I would succeed? I suspected NO. Fit an entire C compiler in 510 bytes of instruction memory? Good luck (sarcasm).TokenizingThe first problem came quickly. In C, the tokenizer/lexer alone seems larger than one 512 byte sector! We need to consume an arbitrary stream of bytes and produce \u201ctokens\u201d.For example:int main(){ if( a < 5 ){  func(); }}Would be consumed and converted into:'int' TOKEN_KEYWORD_INT'main' TOKEN_IDENTIFIER'('  TOKEN_LPAREN')'  TOKEN_RPAREN'{'  TOKEN_LBRACE'if'  TOKEN_KEYWORD_IF'('  TOKEN_LPAREN'a'  TOKEN_IDENTIFIER'<'  TOKEN_OPERATOR'5'  TOKEN_NUMBER')'  TOKEN_RPAREN'{'  TOKEN_LBRACE'func' TOKEN_IDENTIFIER'('  TOKEN_LPAREN')'  TOKEN_RPAREN';'  TOKEN_SEMI'}'  TOKEN_RBRACE'}'  TOKEN_RBRACEWe need to specifically recognize keywords, identifiers, operators, and numbers. And then we need to convert numbers from string to integer with something like atoi():int atoi(const char *s){ int n = 0; while (1) {  char c = *s++;  if (!c) break;  n = 10 * n + (c - '0'); } return n;}I wrote a fairly straight-forward and minimalist lexer and it took >150 lines of C code. A crude estimate of the same code in x86-16 would require 300-450 bytes minimum (e.g. a simple add ax,bx instruction encodes as 2 bytes). And this doesn\u2019t include any symbol table, recursive-descent parser, code-generator, branch-patching, etc.No Chance.So, naturally \u2026 I continued. Always pick the losers. The lolz are more fun that way.Big Insight #1Big Insight #1 came while thinking about other languages such as Forth. The tokenizer in Forth is nearly trivial. Every token is simply space-delimited. Every token is just called a WORD and nothing is special (slight lie). Hmm, how about a C that does that? So dreamed up a C that is technically still a C, is probably turing-complete, and will definitely make every code maintainer terrified. \ud83d\ude0fI will call it the Barely C Programming Language:int done , a , b , c , p , cond ;int(main)(){while(!done){   a = b - c ;   *(int*) p = b - c ;   a = *(int*) p ;   if(cond) a = b - 45 ;}}Here we have spacing strategically placed to create \u201cmega-tokens\u201dFor example: int(main)(){while(!done){ is one such \"mega-token\".In a sense, we actually have a language more like:VAR_BEGIN done AND a AND b AND c AND p AND cond ENDMAIN_BEGIN a = b - c END DEREF p = b - c END a = DEREF p END COND a = b - 45 ENDMAIN_ENDBut, a normal C compiler will also recognize it as C!Even after using space-delimiters, we still have a lot of tokens and need to find more ways to minimize the tokenizer. What is essential? Well it\u2019s quite hard to avoid the atoi() if we want to actually have integer literals. What else do we need? How about nothing.Big Insight #2Big Insight #2 is that atoi() behaves as a (bad) hash function on ordinary text. It consumes characters and updates a 16-bit integer. Hashes are perhaps the holy-grail of computer-science. With a good hash, we can just side-step all the hard problems by trading them for an even harder problem (hash collisions), and then we just ignore that harder problem. Brilliant. (sticks fingers in ears) \ud83e\udd2aSo we have this:Token Type Meaning of atoi()Integer Literal uint16 numberKeyword token \u201denum\u201d valueIdentifier hash value into a 64K arrayImplementing Barely CThe first implementation of Barely C fit in 468 bytes. It was a simple recursive-descent parser over the atoi tokens. There was no symbol table of any kind. Variables simply access a 64K segment using the hash value. Codegen is emitted somewhat similar to OTCC, using ax as the result register and shuffling values to the stack and then to cx for binary operators.Minimizing with Byte-Threaded CodeIn an attempt to steal every good idea Forth ever had, I then dreamed up what I will call \u201cbyte-threaded-code\u201d. Since a sector is 512 bytes, if we simply align address on a 2-byte boundary, we can do addressing with a single byte! We can have a series of \u201cgadgets\u201d and do forth-style threading:bits 16 cpu 386 jmp 0x07c0:entryentry: push cs pop ds lea si,operationsnext: xor ax,ax lodsb add ax,ax push next jmp axputch: mov ah,0x01 mov al,bl mov dx,0 int 0x14 ret align 2hang: jmp hang align 2print_F: mov bx,'F' jmp putch align 2print_G: mov bx,'G' jmp putchoperations: db 17 ; print_F db 20 ; print_G db 17 ; print_F db 17 ; print_F db 20 ; print_G db 17 ; print_F db 17 ; print_F db 17 ; print_F db 20 ; print_G db 16 ; hangAnnoyingly, nasm won\u2019t let you do something like db print_F/2 so I had to write a custom little assembler to do it.Alas, this idea didn\u2019t work out. In 512 bytes, the overhead of this Forth-style computation model doesn\u2019t pay for itself. There are a lot of little overheads: 2 byte alignment, extra ret instructions, calling other \u201cthreads\u201d, the next function, etc. The byte-threaded version of Barely C ended up at the same size as the straight-forward versionHowever, the idea is fun and I decided to document it anyways in the event that someone else finds utility.Minimizing the Straight-Forward versionInstead, I returned to the straight-forward version and minimized it as much as possible. From 468 bytes \u21d2 303 bytes (165 bytes saving): 510 - 303 \u21d2 207 spare bytes to use for new features!Some tricks:Reorganize code to allow \u201cfall-through\u201d instead of jmp or callUse tail-calls via jmp wherever possiblePerform call-fusion (e.g. call tok_next2 instead of call tok_next; call tok_next)Utilize stosw and lodsw extensivelyEliminate machine code tables for cheaper inline stosw versionsPrefer cmp ax,imm over cmp bx,immKeep jump offsets within 8-bits to encode more efficientlyLook Ma, A Real C!As it turns out, a lot can be accomplished in 200 bytes if you already have a tokenizer, parser, and code-generator in 300 bytes. With these 200 bytes, Barely C became a proper C:Arbitrarily nested if statement block with an arbitrary expression conditionArbitrarily nested while statement block with an arbitrary expression conditionLots of operators: +, -, *, &, |, ^, <<, >>, ==, !=, <, >, <=, >=Grouping expressions: ( expression )Function definitions and recursive function calls (using func() as a hash value into a symbol table at segment 0x3000)A special asm statement for inline machine-codeSingle-line // commentsMulti-line /* commentsA trick to do \u201cspace-injection\u201d before semicolons to make code look more normalThe biggest enabler here is the binary_oper_tbl which allows for a very cheap way to add lots of operations. Each operator is simply a <16-bit token-value> <16-bit-machine-code> pair, costing just 4 bytes.The above 14 operators cost just 56 bytes plus a little overhead to scan the table.GrammarHere's the full grammer specification:program   = (var_decl | func_decl)+var_decl  = \"int\" identifier \";\"func_decl  = \"void\" func_name \"{\" statement* \"}\"func_name  = <identifier that ends in \"()\" with no space>statement  = \"if(\" expr \"){\" statement* \"}\"      | \"while(\" expr \"){\" statement* \"}\"      | \"asm\" integer \";\"      | func_name \";\"      | assign_expr \";\"assign_expr = deref? identifier \"=\" exprderef    = \"*(int*)\"expr    = unary (op unary)?unary    = deref identifier      | \"&\" identifier      | \"(\" expr \")\"      | indentifier      | integerop     = \"+\" | \"-\" | \"&\" | \"|\" | \"^\" | \"<<\" | \">>\"      | \"==\" | \"!=\" | \"<\" | \">\" | \"<=\" | \">=\"In addition, both // comment and /* multi-line comment */ styles are supported.(NOTE: This grammar is 704 bytes in ascii, 38% larger than it's implementation!)Inline Machine-CodeA programming language without I/O is useless. And, as the C language is defined in an I/O agnostic way, we need some way out. Thus, an asm extension is supported. This allows programs to generate raw x86-16 machine code literals inline. Using asm, programs cans access any low-level detail of the machine. This is used extensively in the example code.Error-HandlingWhat is \u201cerror-handling\u201d? \ud83e\udd23In traditional C style, we trust the programmer to write correct and well-formed programs. We are certain they are all minor gods and goddesses with the ability of perfection. Obviously, spending bytes on error-checking would be foolish. Surely all will agree that this is a very reasonable standard.For the less divine among us, a lint was also written (that doesn\u2019t fit in a sector) to detect errors. The author certainly didn\u2019t require this tool for development.RuntimeIf C compiler writers were a secret shadow organization like the Free Masons, Illuminati, Lizard Peoples, or Pizzagaters our inner-secret would be \u201cC actually has a runtime\u201d.SectorC has a runtime under rt/ consisting of two files implemented in C itself:rt/lib.c: A collection of library routines, often coded in inline asmrt/_start.c: The actual entry-point _start()The runtime code is concatenated with program source to construct the full source to compile and run.ExamplesA few examples are provided that leverage the unique hardware aspects of the x86-16 IBM PC:examples/hello.c: Print a text greeting on the screen writing to memory at 0xB8000examples/sinwave.c: Draw a moving sine wave animation with VGA Mode 0x13 using an appropriately bad approximation of sin(x)examples/twinkle.c: Play \u201cTwinkle Twinkle Little Star\u201d through the PC Speaker (Warning: LOUD)ConclusionIt seems fitting to end an article with \u201ctakeaways\u201d or \u201cwhat did we learn\u201d. So.. umm.. what did we learn? Honestly, I\u2019m not sure. But in the interest of fun, here\u2019s a Choice Your Own Adventure version of What Did We Learn:What Did We Learn Your Chosen AdventureThings that seem impossible often aren\u2019t and we should Just Do It anyway Move to the South Pole with absolutely no gear on a Homesteading MissionSoftware is too bloated these days, we only need a few KBs Go check yourself into the technology hippie-commune of sucklessError checking is overrated Take Elon up on his pitch to be a mars astronaut because Earth really doesn\u2019t need more software that ignores errors.Anything X can do, C can do better Something like this? link. Monzy, we need a new rap! (call me)That was all gibberish nonsense and thank you for wasting my life (passive-aggression) Feel regret that you wasted the time because there is a lot better content in the world you\u2019d rather consume and decide to get a therapist to work through your issues with reading nonsense internet gibberishThis xorvoid person/robot/AI is ridiculous/absurd/dumb and does arguably pointless things for fun Follow, like, subscribe, ring the bell.. \ud83d\ude01home",
    "summary": "- SectorC is a C compiler written in x86-16 assembly that fits within the 512 byte boot sector of an x86 machine, likely the smallest C compiler ever written.\n- Despite its small size, SectorC supports a subset of C that is large enough to write real and interesting programs, including global variables, functions, if statements, while statements, lots of operators, pointer dereference, inline machine-code, comments, and more.\n- SectorC implements a minimalist lexer and parser and utilizes the hash function of atoi to minimize code size, allowing it to become a proper C",
    "hn_title": "SectorC: A C Compiler in 512 bytes",
    "original_title": "SectorC: A C Compiler in 512 bytes",
    "score": 387,
    "hn_content": "SectorC is a C-subset compiler written using only 512 bytes of x86 assembly. It is a JIT interpreter that does not include operator precedence, but is a technical achievement that demonstrates the value of questioning common assumptions. JIT (Just-In-Time) compiling to machine instructions and executing the compiled output is explained, which is able to do more optimizations than an AOT (ahead of time) compiler. The ability of interpreters to have a JIT compiler allows for some languages like Javascript to be faster than C, and is a dynamic analogue for profile-guided optimization. The C Star (C*) language from the selfie project is also highlighted. Lessons for how to reduce the size of interpreter binaries are sought, and META II metacompiler is mentioned.A discussion thread on Hacker News revolves around the Bootstrap Project, a tool to deploy a C compiler in a virtual machine using a bare metal environment with minimum memory. The discussion goes on to explore the potential uses of the Bootstrap Project, including in deep-space applications or for smaller microcontrollers. One commenter wonders if the tool can be used as the lowest level for creating \"Linux from scratch.\" A debate also arises around the use of structs in this minimal environment, with one person estimating an additional 200-400 lines of C needed and another pointing out historical nuances in struct member naming conventions in C.",
    "hn_summary": "- SectorC is a C-subset compiler using only 512 bytes of x86 assembly, demonstrating the value of questioning common assumptions.\n- JIT compiling to machine instructions and executing the compiled output allows for more optimizations than AOT compilers, and is used in some languages like Javascript and as a dynamic analogue for profile-guided optimization.\n- A discussion on Hacker News explores the potential uses of the Bootstrap Project, a tool to deploy a C compiler in a virtual machine, including in deep-space applications and for smaller microcontrollers, with debate over the use of structs in this minimal environment."
  },
  {
    "id": 36056299,
    "timestamp": 1684926023,
    "title": "Ask HN: How do you not take criticism of your work personally?",
    "url": "",
    "hn_url": "http://news.ycombinator.com/item?id=36056299",
    "content": "",
    "summary": "- A member of the Hacker News (HN) community asked how to avoid taking criticism of their work personally.\n- Many HN users shared their own experiences and techniques for dealing with criticism.\n- The discussion focused on separating personal value from the value of the work and learning from feedback without letting it affect one's self-worth.",
    "hn_title": "Ask HN: How do you not take criticism of your work personally?",
    "original_title": "Ask HN: How do you not take criticism of your work personally?",
    "score": 339,
    "hn_content": "Ask HN: How do you not take criticism of your work personally?\n\n- It's normal for people to take constructive criticism personally, but it's a necessary step in becoming a better developer. \n- Develop an anti-fragile attitude towards criticism to handle it better and become a better human being from it. \n- Being aware of your emotional state can help you increase your tolerance of criticism and prevent it from taking control. \n- Learn to detach your self-worth from your output or delivery and focus on self-acceptance and self-compassion. \n- Giving up on the notion of retirement and learning to process criticism by experiencing it over and over again can help us control our emotions in the moment. \n- Take criticism constructively and see it as an opportunity to learn and improve.No meaningful content to summarize.- A book recommended for emotional-based therapy called \"Focusing in Clinical Practice: The Essence of Change\"\n- Discussion on introducing meditation to children around 11-12 years old\n- Suggestions for developing an anti-fragile attitude towards criticism through detachment and shedding close convictions\n- Importance of going through critiques in school or art as it helps in one's professional life\n- Tips for handling criticism such as assuming positive intentions, not responding immediately, and focusing on feeling better\n- Dealing with online communities where bad intentions are the norm and the importance of moderationA discussion thread between developers about dealing with negative feedback on their work. The comments suggest strategies such as: separating oneself from their work, examining the quality of feedback and responses, considering the motivation of the critic, and practicing self-reflection to control negative emotions. A related TED talk is shared as a resource for further exploration. The tone is generally positive, relatable, and empathetic.The post discusses how to receive and give criticism in a professional setting, specifically in software engineering. The main points made by commenters include:\n- Not apologizing for honest mistakes, but apologizing for mistakes due to laziness or shortcuts\n- Understanding that critiques are about the work, not the individual, and using them as an opportunity to learn and improve\n- Recognizing that code belongs to the company, not the individual, and being open to constructive feedback\n- Learning how to give and receive critiques through frequent practice and self-awareness\n- Separating one's professional work from one's personal identity to avoid taking criticism too personally\n- Reframing criticism as a learning opportunity, rather than a negative judgement.The post discusses how to handle criticism in the software industry. Key points from the comments include detaching personal identity from work, talking to coworkers about struggles, and using art or meditation to prepare oneself for criticism. Specific strategies for receiving criticism include noticing feelings, separating identity from work, and practicing humility. The importance of separating work-related feedback from personal attacks is emphasized, along with the value of taking constructive criticism as an opportunity to learn and grow. Overall, the post highlights the importance of developing resilience and a growth mindset in the face of criticism.Constructive feedback is essential for personal and professional growth, but not all feedback is created equal. Recognizing poor feedback is the first step towards accepting constructive criticism. Constructive feedback suggests a logical change to the code that will have a positive impact on its fitness for use, rather than interrogating the author\u2019s choices. Team leads should encourage an environment where constructive feedback is welcome and create code review guidelines to enshrine some simple rules that nudge people towards giving the kinds of constructive feedback you're looking for. Accepting feedback may be difficult, but owning up to mistakes, being polite and assertive, and anticipating criticism with optimism is the first step towards growth. It is essential to differentiate personal work from the company's and strive towards the company's goals.A collection of comments on dealing with receiving criticism in a professional setting. Some suggest getting therapy to work on personal issues tied to taking things personally, while others recommend framing criticism as an opportunity to learn and improve. The importance of being humble and leaving ego at the door is also emphasized. Others suggest focusing on the process rather than the individual when things go wrong. It is highlighted that mistakes are inevitable and not to blame oneself too much. The notion of being a \"hired gun\" and not getting too emotionally invested in one's job is also brought up. The overall message is to not take criticism personally and to use it as a chance to learn and grow professionally.The article discusses how to handle constructive criticism graciously. It emphasizes the importance of embracing criticism in a healthy work environment. The author suggests taking a step back and separating oneself from the criticism, determining if it's a personal attack or beneficial feedback. They also suggest looking at criticism as an opportunity to learn and grow. Some strategies include being open-minded, avoiding perfectionism, welcoming feedback, and treating it as a gift. Additionally, constructive criticism is a shared responsibility, and anyone who dishes it out should be able to take it seriously and make changes to their own code. Lastly, readers are advised to focus on the work being right, rather than being right themselves.The post is a discussion thread with various comments about how to handle criticism at work. Key takeaways include: criticism is valuable for learning, embracing criticism can lead to personal and professional growth, people often project their own issues onto others, and it is important to remain detached from your work. Suggestions for coping with criticism included reading self-help books, adapting a growth mindset, separating your sense of self from your work, and practicing various philosophies like stoicism or non-violent communication. Responding to criticism with a conversation instead of a debate was also suggested.This post contains various advice on dealing with criticism at work as a programmer. The advice includes: distinguishing between criticism of your work and criticism of you as a person; not identifying too closely with your work and remembering that mistakes are normal; considering feedback from an objective perspective and asking for specific suggestions for how to improve; reframing criticism as an opportunity for growth and learning; focusing on the project as a collective effort rather than on one's own personal success; and seeking outside help or advice if feeling personally attacked.No meaningful content for a concise summary.",
    "hn_summary": "- It is normal to take constructive criticism personally, but developing an anti-fragile attitude can help handle it better and become a better human being from it.\n- Detaching self-worth from output or delivery, assuming positive intentions, practicing self-reflection, and focusing on self-compassion can help handle negative feedback in a professional setting.\n- Recognizing poor feedback, striving towards company goals, and owning up to mistakes while anticipating criticism with optimism lead to personal and professional growth in the software industry."
  },
  {
    "id": 36059429,
    "timestamp": 1684941960,
    "title": "Walking naturally after spinal cord injury using a brain\u2013spine interface",
    "url": "https://www.nature.com/articles/s41586-023-06094-5",
    "hn_url": "http://news.ycombinator.com/item?id=36059429",
    "content": "",
    "summary": "- Scientists have developed a brain-spine interface that allowed three spinal-cord-injured patients to walk again with natural and balanced gaits.\n- The interface involves recording signals from the brain using implantable sensors and decoding them to electrical stimulation of the muscles in the legs via an electrode patch on the skin worn on the lower back.\n- The three individuals were able to walk with training and some assistance, with the effort being similar to walking with leg braces after years of paralysis.",
    "hn_title": "Walking naturally after spinal cord injury using a brain\u2013spine interface",
    "original_title": "Walking naturally after spinal cord injury using a brain\u2013spine interface",
    "score": 316,
    "hn_content": "A digital brain-spine interface has allowed a patient with a complete spinal cord injury to walk unassisted, according to a study published in Nature. The technology connects brain implants to spine electrodes via a headset. After regaining natural motion ability, the patient was able to walk even without the device turned on. The study indicates that the method can restore movement to people who have incurable, complete spinal cord injuries. The authors note that it is unclear whether the approach will be applicable to other injury locations and severities but expressed confidence that it can be applied to a broad population of individuals with paralysis. While some commentators criticized the development for not offering enough potential for regenerative research, others praised the benefits of the device for spinal cord injury patients.There is no meaningful content in this text to provide a summary.",
    "hn_summary": "- A digital brain-spine interface allowed a patient with spinal cord injury to walk unassisted\n- The technology restores movement to people with complete spinal cord injuries\n- The authors expressed confidence that it can be applied to a broad population of individuals with paralysis"
  },
  {
    "id": 36057675,
    "timestamp": 1684934643,
    "title": "Hypersonic missiles are misunderstood",
    "url": "https://medium.com/@ToryBrunoULA/hypersonic-missiles-are-just-misunderstood-1a35c8ae3dd0",
    "hn_url": "http://news.ycombinator.com/item?id=36057675",
    "content": "Hypersonic Missiles are Just MisunderstoodTory Bruno\u00b7Follow11 min read\u00b7May 161935By Tory BrunoIt\u2019s hard to go anywhere without hearing about the new threat of hypersonic missiles. China and Russia have them, the United States does not! Even the news has been breathlessly announcing that \u201cRussia used hypersonic missiles against Ukraine\u201d \u2014 alarming! The average member of the public, as well as many policymakers, now understand that these things are dangerous because they are just too fast to shoot down. Clearly something needs to be done\u2026There\u2019s just one problem; about half of that is just plain wrong. Like an angsty teenager, \u201chypersonic missiles\u201d aren\u2019t bad, they\u2019re just misunderstood.You see, hypersonic missiles are not new. We have had them since the 1950s. In fact, pretty much every ballistic missile in the medium (>1000 Km) range class is hypersonic. All intercontinental ballistic missiles (>5,500 Km) are hypersonic. The longer the range of a missile, the greater its velocity and the velocity of its warhead. That\u2019s the intuitively obvious physics of ballistic missiles.Figure 1: Ballistic Missile Ranges / Credit: United Launch AllianceBy the way, the U.S. has excellent defenses against all classes of ballistic missiles: short, medium, intermediate, and long-range. I have personally been a designer on several of these systems and developed the THAAD (Terminal High Altitude Area Defense) missile defense interceptor first as a designer, and then as its Program Manager. THAAD has become the gold standard for protecting the U.S. and our allies from hypersonic missiles.So, what the heck is everybody carrying on about?The misunderstanding arises from the short and lazy phrase \u201chypersonic missile.\u201d The challenge of this new threat is NOT that it\u2019s hypersonic (faster than Mach 5). The problem is that it\u2019s not ballistic. Its correct and full name is the \u201cHypersonic Maneuvering Threat.\u201d\u201cManeuvering\u201d is the problem. That is the characteristic that makes this \u201cnew\u201d threat a challenge for our Missile Defense Systems. These are specifically designed for ballistic threats, which are common, and their extreme effectiveness is precisely why Russia and China have invested in something else.Figure 2: Maneuvering vs. Ballistic Trajectories / Credit: United Launch AllianceWhat is a ballistic missile anyway?Well, I\u2019m glad you asked! By definition, a ballistic missile is a rocket that follows a ballistic trajectory. If you\u2019re not a rocket scientist, that probably wasn\u2019t very helpful, so let me explain.A ballistic missile is like a major league pitcher. Swinging a powerful arm through a short distance, the pitcher imparts a massive amount of energy to the ball and lets it go. The ball then travels most of the distance between the mound and the plate after it\u2019s released. The pitcher committed the path of the ball when he opened his fingers. (Yes, I know about curve balls, but even that path was also determined when the ball left the hand. It\u2019s an analogy. Work with me\u2026)A ballistic missile works in a similar way. For example, the powered and steerable portion of a long-range ballistic missile flight is just four to five minutes long. Whereupon, the warhead is released to travel the remaining 40 to 50 minutes on its own. Just like the baseball above, the warhead\u2019s path is fixed from the moment of release.Figure 3: Typical Ballistic Trajectory / Credit: United Launch AllianceIn fact, with thousands of miles of unguided flight between release and impact, ballistic missile designers work very hard to make that ballistic trajectory as predictable as possible to achieve target accuracy.While the numbers are obviously classified, as a designer and the former Chief Engineer of the world\u2019s most accurate ballistic system, I can give you another baseball analogy to help put this into context. The Trident II system\u2019s accuracy is roughly like a Rockies pitcher throwing a strike across the plate at Denver\u2019s Coors Field from a pitcher\u2019s mound in Kansas\u2026 We worked very hard to make its trajectory smooth and predictable to pull this off.So, why does all that matter?The predictable and unalterable trajectory of a ballistic missile\u2019s warhead matters because it\u2019s also really, really fast. Ok, the hypersonic part does actually come into all of this, after all.You see, we defend against ballistic missiles with other missiles. We do so because they are the only things we have that can fly at these crazy speeds. Missiles that defend against other missiles are called \u201cinterceptors.\u201d They work by intercepting the warhead thrown at us by an attacker\u2019s ballistic missile.Because we would like to do that as far away from the people or thing(s) being protected as possible, the interceptor will travel for a significant amount of time before reaching the warhead.During that travel time, the warhead will have traversed a large distance by virtue of its hypersonic velocity. So, we aim the interceptor at a point in front of the warhead, along its ballistic trajectory. Once we launch the interceptor to its not-so-romantic rendezvous with the incoming warhead, we have very little opportunity to change our aim point. We are committed.Figure 4: Missile Defense Interceptor Battle Space / Credit: United Launch AllianceThis is like putting another pitcher a few feet to the right of the batter\u2019s box and asking him to wait until he can see the baseball on its way from the pitcher\u2019s mound and then throw another baseball to hit it in midair.And, no, I\u2019m not exaggerating, quite the opposite. The preferred method for ballistic missile defense is a \u201cKill Vehicle\u201d (KV) to warhead collision. This is literally hitting a bullet out of the sky with another bullet, only harder. The kinetic energy imparted by this \u201cHit to Kill\u201d technology imparts far more energy, more effectively, than any practical explosive. The velocity of the incoming warhead will be several times faster than a rifle bullet. It will be met nearly head on by a kill vehicle also traveling at multiples of a bullet\u2019s velocity. The resulting kinetic energy of the collision is impressive and lethal.But what happens if the warhead doesn\u2019t do what it\u2019s supposed to?There\u2019s the problem! If the warhead maneuvers after we\u2019ve launched the interceptor, we\u2019re in trouble.The interceptor does have some ability to cope with this in flight. It will typically receive an update during its boosted flight from its ground radar that is tracking the incoming warhead. That In-Flight Target Update (IFTU) will give the interceptor the latest predicted trajectory, an indication of what its on-board sensor (typically an infrared camera) will see when it gets close, and which of the visible objects is the actual warhead. This IFTU comes in during the interceptor\u2019s boost, so it can redirect to a new aimpoint within the \u201cbattle space\u201d box that the energy remaining on the booster can fly to.Sometimes, there is another update after the KV separates, but that battle space is obviously a lot smaller. The KV will then use its own sensor to steer itself into the warhead during its final (terminal) flight just before collision.Unless\u2026 the warhead isn\u2019t where it\u2019s supposed to be. If it maneuvers to the outside of either of these battle spaces, after the interceptor has committed to its own flight path, it won\u2019t be able to catch up to the warhead, which will fly right by.Figure 5: Maneuvering Glider to Avoid an Interceptor / Credit: United Launch AllianceConventional missile defense systems are highly dependent upon the threat following a predictable ballistic trajectory.That\u2019s cheating! How can it do that?There\u2019s two basic ways to maneuver a hypersonic vehicle.A long-range threat could theoretically stay attached to an upper stage or kick stage and wait for the defender to launch for an intercept in space (the \u201cmidcourse\u201d of the flight path that traverses above the atmosphere), and then fire off the stage to carry the warhead outside the battle space.Figure 6: In Space Maneuvering to Avoid an Interceptor / Credit: United Launch AllianceAnother, and more versatile approach, is to maneuver inside the atmosphere as a \u201cglider\u201d with aerodynamic control surfaces (sort of like fins). I have personally developed several prototype systems of this nature. By using the warhead\u2019s extreme velocity, in conjunction with the atmosphere, very high G maneuvers are possible. You can think of this like \u201cbobbing and weaving\u201d to avoid the interceptor.Because of the aerodynamic lift attainable by this, you can also avoid transiting through space entirely. This allows the warhead to avoid systems designed to intercept threats in space, by flying under them.Additionally, a gliding warhead can achieve very high accuracy because it\u2019s steerable all the way to the target and can extend its range by gliding, using the atmosphere to continue flight far beyond a simple ballistic trajectory. Sadly, the several hypersonic maneuvering systems I worked on were set down and left unfinished, as we pivoted to the Global War on Terror (GWOT).Does Space play a role in all of this?Yes!The missile defense mission begins in Space, especially for longer-range threats. The Space-Based Infrared System (SBIRS) missile warning satellite constellation detects an adversary\u2019s rocket launch, figures out if it might be hostile, and generally where it\u2019s heading. SBIRS then gives a heads up to the terrestrial missile defense radars, allowing them to focus their search energy in a specific direction so they can detect the incoming warhead further out. This lets the missile defender extend the battle space, perhaps even allowing for multiple shots.Figure 7: Regional Coverage of a Missile Warning Satellite / Credit: United Launch AllianceSpace based assets could also track an exoatmospheric maneuvering threat through its mid-course burn, as discussed above. Crucially, only a space-based system can maintain 100% custody of a hypersonic maneuvering glider. This is because the lower altitude of the glider means that it will disappear over the horizon of terrestrial radars as it travels to its target. That creates gaps, during which, it can maneuver outside the battle space, divert to a completely different target, or even maneuver to approach its intended victim from behind, ambushing it where it has no defenses.But is all that enough?Sadly, no!The most capable maneuvering threats will simply delay their crazy Ivan dodge until there is nothing the interceptor can do about it.Our space-based systems will simply watch helplessly as the high G hypersonic glider dodges and weaves its way around our interceptors. Hypersonic they may be, but once committed to a battle space, the interceptors remain vulnerable to a maneuvering threat leaving the battle space once the intercept flight path is committed.If only we had something faster than hypersonic\u2026As a matter of fact, I once worked on just such a technology: Directed Energy (DE).In other words, Lasers (the most common form of DE). If you think hypersonic is fast, that\u2019s nothing compared to the speed of light. Once again, this is a technology we set down to pursue the GWOT.Once a line of sight is established from the DE platform to the warhead, it is impossible to outrun this \u201cspeed of light interceptor.\u201d No high G maneuver will work. The battle space is the entire field of regard of the laser. That battle space is where the target actually is at that moment. We do not aim in front of the warhead as we do with a missile interceptor, so there is no opportunity to maneuver away before the interceptor arrives.Figure 8: Ground Based Defense Laser / Credit: United Launch AllianceWhen we were developing this technology in earnest for missile defense 15 years ago, there were many theories about how it could be defeated. People thought that mirrored surfaces might just reflect the beam. It turned out that reflective surfaces are actually more vulnerable. Would the range be far enough? Maybe the atmosphere would scatter the beam too much? Could platforms defend themselves? What if the warhead or missile was spinning, etc.? All those questions were answered. The only real barrier at the time was generating very high laser power levels in a way that was logistically practical in the field.Our marque platform was the Airborne Laser. This was a megawatt class system on a 747 aircraft. Its mission was to remain aloft near an adversary\u2019s border to kill ballistic missiles while in the boost portion of flight over the attacker\u2019s own territory. It used a chemical oxygen iodine laser (COIL) as the light source. It was, for all intents and purposes, a plane with a giant liquid rocket inside. Admittedly, not super practical in the field. This technological challenge has since been solved with multiple solid state and electric laser approaches now available.Directed energy also has several unique and very attractive characteristics. The power level of the laser is inherently variable; lethal at full power or merely strong enough to blind a sensor at low power. The energy can also be applied very precisely. We used a tactical power level laser in a government sponsored exercise to demonstrate some pretty amazing versatility.One day, we destroyed some small tactical missiles in flight by detonating their rocket motors. The next day, we disabled drones by specifically targeting their avionics, causing them to harmlessly lose altitude and crash, much to the confusion of the remote-control pilots. Later that same day, we sank zodiacs by puncturing their inflatable hulls, only to switch to simply immobilizing them by targeting just the outboard motor. You get the idea. We could apply our laser energy surgically across a wide variety of targets.Another really important feature is that our laser was electric and powered by a simple, commercial generator sitting on a trailer. As long as we had gasoline, we could shoot all day. And each shot only consumed about a dollar\u2019s worth of fuel! With interceptors, you must constantly be concerned about magazine depth. Will I run out of interceptors before the enemy runs out of missiles? That\u2019s not really an issue with directed energy.Let\u2019s take a quick inventory:Speed of light round, dialable affects, surgical targeting, bottomless magazine, and a dirt-cheap cost per kill\u2026 what\u2019s not to love!The time has come.Where should we put these lasers?Some should be placed as point defenses in a city, airfield, or at critical infrastructure sites.However, the only practical way to defend against long-range hypersonic gliders, which can threaten entire regions along a single flight corridor, is from Space. Orbiting DE platforms, looking down on entire regions from the ultimate high ground can leverage \u201cbirth to death\u201d tracking of any given glider, combined with its speed of light \u201cinterceptor,\u201d to completely nullify this threat.(Another very interesting capability of a DE platform is that it need not create a debris field should we ever be forced to employ them to defend ourselves in Space.)And, in doing so, we can deter an attack from ever being seriously contemplated, stopping it before it begins.Figure 9: Space Based Defense Laser / Credit: United Launch Alliance",
    "summary": "- Hypersonic missiles are not new, and we have had them for decades.\n- The real problem with hypersonic missiles is that they are maneuverable, making them challenging to intercept using traditional missile defense systems designed for ballistic threats.\n- Directed Energy (DE), like lasers, could be a potential solution for intercepting maneuverable hypersonic missiles due to their high speed and a battle space that covers the entire field of regard of the laser.",
    "hn_title": "Hypersonic missiles are misunderstood",
    "original_title": "Hypersonic missiles are misunderstood",
    "score": 283,
    "hn_content": "The article discusses the potential use of lasers from satellites to destroy hypersonic missiles; the CEO of United Launch Alliance, Tory Bruno, advocates for such a system. Some skepticism is expressed about the cost and complexity of deploying a large constellation of such satellites, and there is debate about the effectiveness of intercepting hypersonic missiles. There are also concerns about the potential for escalation of conflict into space and the deployment of weapons systems. International treaties currently prohibit the weaponization of space. Overall, the article addresses the ongoing technological arms race and the potential for new developments in weapons systems.The article discusses the limitations of missile defense systems, including cost and logistics, and argues that a massive increase in interceptors to achieve a perfect defense is impractical. Additionally, the article argues that defense systems don't guarantee protection and that the focus should remain on maintaining a strong deterrent, \"mutually assured destruction\" (MAD). The costs of interceptors and ICBMs are compared, with the former being much more expensive per unit. The article also briefly mentions directed energy as a potential solution but notes that it is impractical in a hot war scenario. Overall, the article critiques the idea of a perfect missile defense system as a \"money grab\" for the military-industrial complex.The cost of THAAD is expensive, but it is the only interceptor that has a chance against ICBMs. North Korea's small arsenal means it needs to make every warhead count against THAAD. It's unnecessary to mass-produce nukes as credible-looking dummy payloads will suffice. Intercepting ICBMs is difficult due to short engagement times, off-trajectory, and initial deviations in aiming, which creates larger errors at the target. There is no consensus view on defenses against MAD salvos, but few defences are adequate for defending against the likes of Pyongyang. Patriots have a high failure rate, raising questions on its effectiveness, and they can miss incoming drones.  Saudi Arabian defense forces have expensive gear but fail to maintain and train on it.The post is a discussion about the effectiveness of the Patriot missile system and its ability to intercept incoming missiles, including hypersonic ones. There are conflicting reports about the recent attack on a Patriot battery during the Ukraine-Russia conflict, but it is unclear what exactly was hit and how effective the interception was. The discussion touches upon the different types of hypersonic missiles and their capabilities, as well as the use of decoys to hinder interception. The modularity of the Patriot system is highlighted, and the importance of spreading out the different components is noted.Experts debate the effectiveness of missile defense systems like Patriot in intercepting incoming targets, with some arguing that defense may be stronger than offense again, while others suggest that the saturation of systems is a problem. Recent technological advances have led to the development of more effective military solid-state lasers, opening up the possibility of space-based laser interception and planetary asteroid defense. However, the deployment of weapons in space, including lasers, is prohibited by several agreements, leading to concerns about enforceability and the potential for offensive uses. Lasers may have limited effectiveness against hypersonic weapons that fly through plasma shields, and plasma can also disperse beams, reducing their effective range. Cautious optimism prevails about the potential uses of powerful military lasers, but there are also many caveats and limitations to consider.The cost of genome sequencing has dropped substantially. Mirrored surfaces are more vulnerable than non-reflective surfaces. There are concerns about the militarization of space and the risk of making Earth's orbit unusable. Lasers can be cheap, with energy consumption being a large problem. It takes time for a satellite to establish a line of sight with any missiles targeted at it. US arms manufacturers have fallen into an imagined capabilities trap that saw them design systems based on Soviet propaganda about new invincible weapons systems. Ukraine downed a Russian hypersonic missile with a Patriot, with two explosions happening on the ground level and two launched Patriot missiles failing, causing minor damage.Summary:\n- Space-based directed energy weapons are needed to intercept hypersonic glide vehicles and the US does not have them currently.\n- The conversation debates Russia's potential to deploy hypersonic missiles and the US's response to it, including past failed efforts such as the Strategic Defense Initiative.\n- There is discussion of the vulnerabilities of critical infrastructure, with some suggesting that precision hypersonic strikes against ships, refineries, and production facilities could limit US expeditionary posture and create an escalation rung that the US has not had to deal with before.\n- The nuclear component of this discussion is acknowledged but not the subject of the main argument.",
    "hn_summary": "- The article discusses the potential use of lasers from satellites to destroy hypersonic missiles, but there is skepticism about the cost and complexity of deployment, as well as concerns about the weaponization of space.\n- The effectiveness of missile defense systems like Patriot in intercepting incoming targets is debated, with some arguing for a strong defense and others pointing out limitations.\n- Experts discuss recent technological advances in military lasers and their potential uses, but there are also caveats and limitations to consider, such as the effectiveness against plasma shields and the prohibition of weapon deployment in space."
  }
]
