[
  {
    "id": 40239164,
    "title": "Creating \"ShapeUp\": A 3D Modeler in C",
    "originLink": "https://danielchasehooper.com/posts/shapeup/",
    "originBody": "Daniel Hooper Home ・ Posts ・ Projects ・ Twitter ・ RSS Making a 3D Modeler, in C, in a Week May 2, 2024・8 minute read Last fall I participated in a week long programming event called the Wheel Reinvention Jam. The point of the Jam was to revisit existing software systems with fresh eyes. I ended up making a 3D modeler called “ShapeUp”. This post will make more sense if you watch the video demo of ShapeUp before reading more. You can try ShapeUp in your browser. This is what it looks like: Mike Wazowski modeled in ShapeUp A 3D Modeler I hate how slow the typescript compiler is (this connects, trust me). The jam seemed like a good opportunity to implement a faster subset of Typescript to beat tsc. Starting with the esbuild or Bun typescript parser made the project seem plausible. It dawned on me that success would look like one terminal command finishing faster than another. As far as demos go, not super compelling. I wanted a cool demo. So I pivoted to 3D. The only reason a 3D project from scratch in a week seemed doable was because of a technique called ray marched signed distance fields (SDFs). A ray marched SDF scene with colors, soft shadows, and ambient occlusion can be implemented much faster than an equivalent triangle based renderer. The amazing Inigo Quilez uses SDFs to create pixar-like characters in one sitting. I had written SDF shaders before but they were rudimentary. Modeling by editing code felt unnatural to me. I wanted to edit the shapes with a mouse. This jam seemed like my chance to make that a reality. ShapeUp’s Signed Distance Field visualized In C I wrote ShapeUp in C, and used raylib to create the OpenGL window. Raylib turned out to be one of those libraries that gets you going quickly, but slows you down in the long run. More about that later. Some view C as a language so simple and raw that you’ll spend all your time working around the language’s lack of built in data structures, and fixing pointer bugs. The truth is that C’s simplicity is a strength. It compiles quickly. Its syntax doesn’t hide complex operations. It’s simple enough that I don’t have to constantly look things up. And I can easily compile it to both native and web assembly. While C has its share of quirks, I avoid them by habits developed over 22 years of use. My “day job” project is 177,000 lines of C and Objective-C. By comparison, ShapeUp’s small single C file is trivial. Even so, I think it’s interesting to look at how it uses data. Models are made up of Shapes: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 typedef struct { Vector3 pos; Vector3 size; Vector3 angle; float corner_radius; float blob_amount; struct { uint8_t r,g,b; } color; struct { bool x,y,z; } mirror; bool subtract; } Shape; The Shapes are kept in a statically allocated array: 1 2 3 4 #define MAX_SHAPE_COUNT 100 Shape shapes[MAX_SHAPE_COUNT]; int shape_count; int selected_shape = -1; Can’t fail to allocate, can’t be leaked, no fluff. Lovely. The 100 shape limit wasn’t limiting in practice. With very little time to optimize the renderer, the framerate would drop before you even got to 100 shapes. If there had been time, I would have broken the model into little bricks and then raymarched within each brick. For dynamic memory, ShapeUp calls malloc in only 3 places: Saving (allocates a buffer big enough to hold the whole document) .OBJ export (again, allocates a buffer large enough to hold all vertices) GLSL shader generation (buffer for shader source) In all cases there is a single free at the end of the function. Again, this is all trivial - I mention it mostly as an existence proof that memory in C can be trivial. You could certainly make it harder on yourself by malloc-ing each Shape individually and storing those pointers in a dynamic array. Using a language like Java, Javascript, or Python would force that allocation structure. I appreciate that C gives me control over memory layout. The UI is implemented as an immediate mode user interface (IMGUI). I love this approach to UI. It’s very easy to debug and you use a real programming language to position elements (unlike CSS, constraints, or SwiftUI). Like most IMGUIs, I used an enum to keep track of what element had focus, or what action the mouse was making: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 typedef enum { CONTROL_NONE, CONTROL_POS_X, CONTROL_POS_Y, CONTROL_POS_Z, CONTROL_SCALE_X, CONTROL_SCALE_Y, CONTROL_SCALE_Z, CONTROL_ANGLE_X, CONTROL_ANGLE_Y, CONTROL_ANGLE_Z, CONTROL_COLOR_R, CONTROL_COLOR_G, CONTROL_COLOR_B, CONTROL_TRANSLATE, CONTROL_ROTATE, CONTROL_SCALE, CONTROL_CORNER_RADIUS, CONTROL_ROTATE_CAMERA, CONTROL_BLOB_AMOUNT, } Control; Control focused_control; Control mouse_action; This project didn’t need dynamic arrays or hashmaps, but if it had, I would’ve used something like stb_ds.h. Aside: Wrestling Raylib So while I feel good about deciding to use C, raylib turned out to be trouble. First off, it has strange design choices that harm the developer experience: Raylib uses int everywhere that you would expect an enum type. This prevents the compiler from type checking and the functions don’t self document. Take this line in raylib’s header for example: 1 2 // Check if a gesture have been detected RLAPI bool IsGestureDetected(unsigned int gesture); It looks like gesture might be an ID for a gesture you’ve registered for. Reading the raylib source reveals that gesture parameter is actually a Gesture enum! This happens everywhere. Raylib’s only documentation is the header file, so you have to go to the implementation to see if any int parameter is really an enum, and if it is, which enum. Raylib doesn’t do basic parameter validation, by design. This function segfaults when dataSize is null: 1 unsigned char *LoadFileData(const char *fileName, int *dataSize); The raylib header doesn’t indicate that dataSize is an out parameter, or that it must not be null. This no-validation choice affects many functions and made trivial problems hard to track down. If you’re lucky it segfaults somewhere useful (but it doesn’t log an error). If you’re unlucky it just silently does something weird. Raylib doesn’t take responsibility for its dependencies. There are issues in GLFW that raylib won’t work around or submit a patch for. As an end user of raylib, the method they chose to create a window is an invisible implementation detail. I care about raylib’s features working for me, regardless of what that means internally. The raygui UI library is just a toy: can’t display floating point numbers. I had to make a float text field. doesn’t handle mouse event routing for overlapping or clipped elements can’t do rounded corners, which are everywhere in UIs. can’t be styled to look good And finally just plain bugs: raygui tooling had a bug that prevented changing the font from the hyper-stylized default (please pick a reasonable default!) Drawing functions like DrawCircle(...) don’t share vertices between triangles. That causes pixel gaps due to floating point error when the current matrix has scaling or rotation. For a while I reported issues as I found them, but almost all of them them were closed as “wont fix”. This was frustrating and discouraging, and it was time consuming to write the bug reports, so I just stopped. So yeah, while it was great that raylib made me an OpenGL window, I paid dearly for that convenience. Luckily I usually found an escape hatch: either by using OpenGL functions directly, or implementing a feature from scratch. In the future I’ll go with sokol. In a Week At a high level, ShapeUp came down to 4 main parts that needed to be completed in 6 days: The user interface (3D gizmos, keyboard shortcuts, sidebar, game controller) GLSL shader generator + Ray marching renderer (explained in video) GPU-based mouse selection (explained in video) Marching cubes for export (explained in video) Each one individually was not hard. The hard thing was prioritizing correctly and not getting sidetracked. It helped to solve finicky or time consuming problems by designing around them, or by using a dumb solution that works in 90% of cases. Sometimes punting a feature by a day gave my subconscious time to find a solution. I tried to work in such a way that I always had a working 3D modeler, and progressively improved it as time allowed. I think about it like building a pyramid. If you build layer by layer, you don’t have a pyramid until the very end. On the other hand you can build it so that stopping at any step is a complete pyramid. Closing By the end of the week I had a 3D program that could make meaningful 3D models and export them to an .obj file. It also runs on multiple platforms and has file open/save. A Wrench Modeled in ShapeUp The project is 2024 lines of C and 250 lines GLSL. Kind of surprising that a somewhat useful 3D modeler can be expressed in ~2300 lines. Other jam participants seemed impressed by ShapeUp but I don’t feel like I achieved much. It’s a relatively simple project. If there is anything special about what I did, it is that I had the taste to choose what to make, the knowledge to make it, and the discipline to do it in a week. You can try ShapeUp in your browser, just keep in mind it was made in a week :) The source code is avalible on github Discuss on Twitter Discuss on Lobste.rs Discuss on Hacker News ❖ See more by Daniel",
    "commentLink": "https://news.ycombinator.com/item?id=40239164",
    "commentBody": "Making a 3D modeler in C in a week (danielchasehooper.com)494 points by jasim 16 hours agohidepastfavorite112 comments netule 15 hours agoI agree entirely with the author on the limitations of Raylib. I'm currently working on a tower-defense style game that I started in Raylib, but I'm running into many of the same limitations (and more). Things such as toggling fullscreen not working consistently across platforms, not being able to enumerate screen modes, toggling rendering features at runtime, saving compiled shaders etc., etc. Having said that, I appreciate Ray's work on this library and will continue to sponsor him. Raylib is great for quickly banging out a prototype, but not much beyond that unless you're okay with living with severe limitations. Lesson learned, for sure, but I'm too far into the development to swap all of the Raylib stuff out for SDL (or something else) now. reply oersted 14 hours agoparentQuick appreciation for the detail that Raylib is named after the creator's name Ray and not ray-tracing, fun. Things Unexpectedly Named After People: https://notes.rolandcrosby.com/posts/unexpectedly-eponymous/ reply TrainedMonkey 12 hours agorootparentHow do you know Ray was not named after ray-tracing? reply dymk 11 hours agorootparentThe author's name is the first hint, and the lack of ray tracing the second reply oersted 11 hours agorootparentI choose to interpret it as: How do you know that *the author* was not named after ray-tracing? Which is amusing :) reply linkdd 11 hours agorootparentprevYou missed the joke, so let me ruin it by explaining it: What if Ray the person was named after ray-tracing by his parents? reply MenhirMike 9 hours agorootparentPlot twist: Ray Tracing was the name of a person that was very important to both of them and unfortunately passed away, so they named their son Ray as a tribute. reply dymk 4 hours agorootparentprevAh shit, I was That Guy on the internet, sorry. I guess it happens to everyone eventually. reply airstrike 13 hours agorootparentprevSuch a good list.. worth a submission of its own IMHO reply oersted 11 hours agorootparentIt got good traction a couple times before, many more fun examples in the comments. https://news.ycombinator.com/item?id=39462516 https://news.ycombinator.com/item?id=23888725 reply WJW 12 hours agorootparentprevI wish they'd add French drains. reply diath 9 hours agoparentprevKind of share the same feeling, started a project about 2 months ago and chose to use Raylib, and while the basic stuff is really simple to get going, the more you use it, the more random minor inconveniences you run into, but at this point I've invested too much into this project to back out of using Raylib. My biggest issue with it right now is the font handling and text rendering, I think I'll have to switch from TTF fonts to pre-baked bitmap fonts for it (which will suck for localization later). The biggest two features I'm missing after switching from Love2D is being able to render multi-color text (with Raylib, you have to manually split up the text into chunks based on color markup, then apply width offset, then call the draw function for each chunk, while also taking into account things like linebreaks and such), not to mention that it seems to tank the FPS a lot when you try to draw a lot of text on the screen (perhaps the draw call batching is broken for text?) and being able to easily chop up textures and make them wrap or tile, Raylib used to have tiled texture draw function in the past but for some reason they removed it. reply abnercoimbre 9 hours agorootparentI'm the organizer for the conference [0] mentioned in TFA. We had a professional UI/UX designer react to ShapeUp [1], and one of the things she commented on was the font being hard to visually parse. I laughed a little when the author yelled \"raylib!\" to make sure blame was assigned appropriately XD. I'm currently the top GitHub sponsor for raylib, so there's no hate, but I wish he changed some of his defaults. [0] https://handmadecities.com/seattle [1] https://vimeo.com/887532756/2972a82e55#t=49m58s (timestamped) reply edflsafoiewq 6 hours agorootparentprevWhy did you switch from Love2D? reply diath 3 hours agorootparentI need multithreading, async TCP, and have to implement systems that benefit from type safety. Though, I do still use Lua (using sol2), for interface modules, but I miss the features that Love2D provided on the framework level. reply rwbt 15 hours agoparentprevRaylib is easy to get started but once the project gets a little complex it bites back. SDL on the other hand takes more time to setup everything but scales extremely well as the project gets bigger and bigger. Also, SDL is exceptionally well written code. reply visil 10 hours agorootparentAnd an exceptionally well written documentation, too! One of the first big-ish projects of mine was a raytracer I wrote in C with SDL. reply throwaway2046 34 minutes agorootparentGot a link to that raytracer? reply vsuperpower2020 10 hours agoparentprevRaylib has a lot of issues that are never going to be fixed, but I wouldn't blame fullscreen on it. Fullscreen is just absolutely unusably FUBARed on windows and has been for decades. It's probably the same for other platforms. The modern strategy is to just do borderless windowed and pretend true fullscreen doesn't exist. reply duxup 7 hours agorootparentI have no knowledge about such things but… > The modern strategy is to just do borderless windowed and pretend true fullscreen doesn't exist. This explains a lot as to why I experienced some interesting inconsistency going full screen in different applications in Windows ;) reply sph 1 hour agorootparentIn Linux/Wayland, Windows-style fullscreen does not exist and has no reason to exist either. In a composited environment, fullscreen windows are just maximized windows without a border, which triggers some heuristic to unredirect windows (i.e. does not have to pay the price of compositing) when they are the sole thing that is drawn on the screen. So, on Linux, fullscreen and maximized borderless are the same. Given that Windows is a fully compositing desktop as well, I wonder why the difference still exists. reply keyle 9 hours agorootparentprevI do the same, I do wonder however, if there are performance issue at the OS level, from running (on a 4K screen) 4K borderless vs. 4K fullscreen. Like, the whole giving the application maximum priority would work the same? reply Just_Harry 6 hours agorootparentWhether there's a performance detriment from borderless-fullscreen vs exclusive-fullscreen depends on how a program presents its frames (and on the OS, of course). On Windows, under ideal circumstances borderless-fullscreen performs identically to exclusive-fullscreen as Windows will let the program skip the compositor and present its frames more-or-less directly to the display. (Under really ideal circumstances the same applies to bordered non-fullscreen windows.) If the compositor can't be skipped, borderless-fullscreen can be a bit brutal on performance: on a 4K 160Hz screen I've experienced an additional 40-milliseconds+ of frame-latency purely from borderless-fullscreen being used. The Special K wiki has some pages that go into more detail about the situation on Windows: https://wiki.special-k.info/SwapChain, https://wiki.special-k.info/Presentation_Model reply ImHereToVote 2 hours agorootparentThis is absolutely bonkers. reply spookie 8 hours agorootparentprevAt least on Xorg it isn't an issue, but to ensure cross-compatibility your solution trumps all. I cannot comment on other platforms. reply Maken 1 hour agorootparentIn Xorg it is definitely an issue. Virtually all games use a borderless window because actual X11 fullscreen is awful: it captures all input events and changes the system's screen resolution to whatever the application is running at. reply netule 9 hours agorootparentprevTrue, it’s just one of the many issues that came to mind while writing this. My solution to the issue is also full screen borderless combined with resolution scaling. reply spacechild1 9 hours agorootparentprev> just do borderless windowed and pretend true fullscreen doesn't exist. Hah, good to know I am not the only one. reply sgt 15 hours agoparentprevThis made me want to look at raylib. It comes with some cute examples that run using WebAssembly: https://www.raylib.com/examples.html One thing that's always bothered me about Wasm and browser 3d/2d graphics is that I often find minor issues such as scrolling. Look at the example called \"Background scrolling & parallax\" here: https://www.raylib.com/examples.html I've tested on several devices and it's definitely not smooth scrolling, unless there's something wrong with my eyes. How can 2D smooth scrolling not be a solved problem in 2024? reply flohofwoe 10 minutes agorootparentBecause it's a surprisingly tricky topic on modern operating systems [1], and even trickier in web browsers (it was actually much easier to achieve on hard-realtime systems like 8- and 16-bit home computers like the C64 or Amiga). One problem is that web browsers don't let you measure the exact frame duration, or let you know when exactly a frame was presented, or even query the display refresh frequency. Even in the native APIs that provide a presentation timestamp (like DXGI on Windows or CVDisplayLink on macOS) that timestamp has considerable jitter. And as soon as you base your animation timings on such a jittery timestamp you'll get micro-stutter (the easiest way to get smooth animation is actually to assume a fixed frame duration, but then your animation speed will be tied to the display refresh rate, and on some platforms - like web browsers - there's now way to query the actual display refresh rate). It's often possible to eliminate the timing jitter with 'noise removal' filters or just running an average over a couple dozen frames, but those then may behave funny in situations where the frame duration changes drastically (such as moving a window between displays with different refresh rate, or when rendering stops because the window is fully obscured). [1] https://medium.com/@alen.ladavac/the-elusive-frame-timing-16... reply modeless 13 hours agorootparentprevIn that sample the foreground scrolls perfectly smoothly for me, but the background looks jittery. This indicates to me that it's not a platform issue at all. That sample is just doing something weird with the background. reply joeld42 10 hours agorootparentThis jitteryness is because the sample doesn't have antialiasing enabled (since it's pixel art) and the background scrolling is 0.1pixels per frame, which means every 10 frames it snaps 1 pixel. The scrolling is also updating on fixed amount per frame instead of looking at deltaTime, so if there are lags or small differences in frame time this might look choppy. But I think it's more meant to demonstrate drawing parallax layers rather than subpixel scrolling. reply sgt 3 hours agorootparentSo by modifying it to look at delta time it would be smooth? reply sgt 13 hours agorootparentprevYes, the background is odd but the foreground is definitely not smooth. I see small little jitters occasionally. At one point I had to wait 15 seconds for it to jitter, though. reply jay_kyburz 11 hours agorootparentYes the back and foreground is quite jittery for me on Firefox, and I'm almost certain its the browsers own requestAnimationFrame that's the problem. Update: Although, having a closer look at the scene, I see its pixel art, so I bet the author is snapping floating point positions to a pixel point to prevent sub pixel blurring. Another small update: I was sure requestAnimationFrame was locked to 60fps, but I noticed on Chrome the other day it was 144hz, the full speed of my monitor. reply dekhn 14 hours agorootparentprevthe answer to your last question is \"inner platform effect\" and \"second system effect\". reply nextaccountic 9 hours agoparentprev> I agree entirely with the author on the limitations of Raylib. Wow this is kind of insane. About this > Raylib doesn’t do basic parameter validation, by design. This function segfaults when dataSize is null: (...) The developer answered this > For most of the raylib functions is up to the user to validate the inputs, if raylib should consider all possible bad-use scenarios it would require reviewing most of the library functions and it will increase source-code complexity. reply lelanthran 4 hours agorootparentI think the word 'insane' is going to far to describe the behaviour of the specified function. It returns an array of bytes. If you, the programmer, wrote a line that called that function, on the very next line you are going to try to use the array, realise that you don't know the length, and realise that the `NULL` that passed in on the line above is probably the output for the length! In order to actually write a call with `NULL` for the dataSize argument, the programmer needs to be clueless about how to write a for loop. So, no, I can't easily see a situation where a programmer accidentally uses a `dataSize` parameter of `NULL`, because that would mean they don't know that arrays in C have no length information, which is C 101. reply kragen 6 hours agorootparentprevin c and c++ this is not just normal but almost unavoidable. if your function takes a t* and someone casts a random int to a t* and pass it in, it is gonna segfault. no possible way to validate it, though in theory you could open /proc/self/maps and iterate through it to catch the segfault cases, or install a segfault handler reply lelanthran 5 hours agorootparent> if your function takes a t* and someone casts a random int to a t* and pass it in, it is gonna segfault. True, that is not something a function ought to defend against. However, the complaint was not about that, though, it was about not checking if the dataSize parameter is NULL. I don't really have a problem with functions that segfault when given NULL pointer parameters as long as this is clearly documented! Sometime, however, you just need to be familiar with enough projects in C that common-sense gets built. In the specified example: unsigned char *LoadFileData(const char *fileName, int *dataSize); I expected that a function that returns an array needs to tell the caller how large that array is. This specific function is not one I would complain about. These things are usually documented in the headers. In this case it says: // Load file data as byte array (read) So, yeah, it's pretty clear to me that the number of bytes has to be returned somewhere, and there's only one parameter called `dataSize`, so this isn't something I consider to be a valid complaint. [EDIT: Escaped pointers, and added last paragraph] reply defrost 5 hours agorootparent> unsigned char WARNING unpairedunescaped *'s !! :-) grrr, markup reply lelanthran 4 hours agorootparentThanks, fixed. reply pests 13 hours agoparentprevIIRC it defines some common words too like all the color names and uses a lot of names that should be prefixed. Good otherwise. reply diath 9 hours agorootparentAt least they're all-caps, but as somebody that writes C++ and uses Raylib, I just wrapped it in a namespace in my project that I include, like so (note that cstdio must be included before raylib if you're using it from C++): #pragma once #includenamespace raylib { #include} reply SoKamil 14 hours agoprev> The Shapes are kept in a statically allocated array [...] Can’t fail to allocate, can’t be leaked, no fluff. Lovely. The 100 shape limit wasn’t limiting in practice. With very little time to optimize the renderer, the framerate would drop before you even got to 100 shapes. That's the best example of avoiding premature optimization I've seen in a while. reply Narishma 5 hours agoparentI think this is the opposite. It's avoiding premature abstraction/generalization. reply bruce343434 3 hours agorootparentBut finding abstractions and generalizations optimizes scratching that itch inside our heads reply kuon 2 hours agoprevI really like this kind of projects. I still like the low level of C. Now I work with rust a lot and elixir/erlang but I often miss the simplicity and explicitness of C. For this, I use zig a lot too. It is a very nice improvement over C while keeping a lot of its philosophy. reply runevault 14 hours agoprevSuper interesting post, and appreciate him talking about the various decisions like his handling of memory (and the issues he ran into with raylib). As someone who's finally diving into part 2 of crafting interpreters (and using it to refresh myself on C) being reminded of what C does well is great. reply fallingsquirrel 14 hours agoprevI really love the live demonstrations in the video. Forget building the app, I couldn't even produce that video in a week if I tried. reply dhooper 14 hours agoparentThe video took me longer to make than the app! I don't know how youtubers do it so regularly. reply an_aparallel 13 hours agorootparentThey have a team :) reply movedx 12 hours agorootparentNot all of us do. If you can get to an average of 300k+ view per video then you can hire an editor and maybe someone else on a contract basis to assist. Must of don't have a team though. reply djmips 10 hours agorootparentYeah sometimes they eventually hire an editor but I think channels like Cutting Edge Engineering it's a fulltime job for one of the two people to do the filming and editing. reply drtgh 7 hours agoprevOff-Topic: Glad to see for the first time a WebAssembly interface where the text does not look blurry. I repeat, it is the first time. Extending this to programs and some operating systems (such as Windows), in the past few years, there has been a pervasive issue with the text rasterization methods that have become a common trend and default setting. Unfortunately, users often do not have the option of turning off anti-aliasing to get sharp text, and in the rare cases where this option is available, the interface (menus, etc.) still uses anti-aliasing. reply themerone 7 hours agoparentThe crisp text is unimpressive. The typeface has no curves and there is no smoothing. That text will be crisp and blocky at any resolution. reply flohofwoe 1 hour agorootparent> The crisp text is unimpressive. Not if you actually tried to achieve that in practice across all browsers and display configurations ;) For instance Safari had for the longest time a hardwired (e.g. not fixable via CSS) linear filter applied when a WebGL context had to be upscaled. It's only been fixed last year or so. Also unfiltered upscaling done by the window system outside the browser clashes with fractional scaling, the result will generally look horrible and there is no way around (as long as upscaling is involved at least). Browsers running on top of Wayland most likely still have all those issues. E.g. for fractional scaling you only have a choice between \"blurry\" or \"terrible scaling artifacts\" where \"blurry\" is usually the less bad option. And when this scaling happens outside the browser in the window system, there's not much you can do. reply drtgh 7 hours agorootparentprevThe crisp text maybe it is unimpressive in the sense that is being used an graphic library targeted to games, so is using an homologous to pixel fonts. But it is not the norm. I'm simply increasing awareness among the development community about the problems with text rasterization in applications and OS interfaces. I'm glad not to see blurry text. reply ederamen 8 hours agoprevJust started using Raylib, bummed to hear about the limitations! As a novice C programmer, the simplicity and immediacy of results opened my eyes to how C can feel as productive as higher level languages with robust standard libs. reply lelanthran 4 hours agoparent> As a novice C programmer, the simplicity and immediacy of results opened my eyes to how C can feel as productive as higher level languages with robust standard libs. TBH, once you have halfway-good libraries for dealing with `char *` strings as-is, dynamic arrays and hashmaps, you are not going to be much slowed-down using C than using a higher-level language. You even get much stronger isolation guarantees than most other high-level languages, while getting much more compatibility[1] with any other language you may wish to interface to: https://www.lelanthran.com/chap9/content.html [1] I did a little Go project, and it annoyed me slightly when I wanted to do performant FFI. For Go, I think the situation has improved since I last checked, though. reply neonsunset 3 hours agorootparentYou can trivially replace Go with C# in order to get almost zero-cost FFI (you can make it fully zero-cost with additional effort but even the baseline is better than the alternatives, hell you can statically link other .lib/.a's into AOT compiled .NET binaries). reply antirez 13 hours agoprevI hope somebody will continue this project. It's a few months away to be a serious alternative to Blender / FreeCAD for certain use cases, with a much softer learning curve. reply OskarS 2 hours agoparentYou should check out MagicaCSG, which is a more sophisticated version (though still free!) of this: https://ephtracy.github.io/index.html?page=magicacsg#ss-caro... reply captainhorst 41 minutes agorootparentThere's also https://womp.com as an SDF modeler reply turtledragonfly 12 hours agoparentprevEDIT aha, the program already supports exporting to a mesh via marching cubes; see the youtube video on the site. I hadn't realized that (: ---- Be aware that since it fundamentally works with SDFs, it is a somewhat different modeling experience (and stores different data) than traditional meshes with triangles, verts, etc. Transforming it from SDFs into meshes could be done with marching cubes or similar, but you'd likely need to \"clean up\" such data afterwards in a Blender-style app anyway. SDFs are great though, if your renderer is SDF-based, too (most are not). [sorry if you knew this already, wasn't sure] reply antirez 12 hours agorootparentYep the export is already done! I used TinkerCAD a ton for things ways more complex than it should be used to, so even when I use more advanced CADs at this point I tend to think in SDF terms. For many things it's faster and more natural than extruding, rotating, ... But the fact is, the engine behind TinkerCAD is quite good, but there is little interest for AutoDesk to compete with its own Fusion360, so TinkerCAD is left forever as a children / beginners tool, without the more advanced stuff that could implement. reply moarinfoszszz 10 hours agoparentprevCheck out Dune3D and Salome-Platform reply adastra22 10 hours agoparentprevBlender maybe, but not CAD work unfortunately. reply nineteen999 9 hours agorootparentNot even Blender. Not even close. No disrespect to the author, it's impressive, but Blender gives you extreme control over mesh topology and this doesn't. reply adastra22 9 hours agorootparentSometimes that’s what you want though. This is good for “digital clay modeling.” reply ffitch 14 hours agoprev> The project is 2024 lines of C got to appreciate the effort to make the irony possible : ) reply poopicus 11 hours agoparentAs someone who has difficulty in detecting irony, could you explain the irony in this statement? reply booleandilemma 11 hours agorootparent2024 is the current year and it's the same as the number of lines of code. I don't think describing it as ironic is correct though. reply roland35 9 hours agorootparentIt's about ironic as rain on your wedding day. reply lelanthran 4 hours agorootparent> It's about ironic as rain on your wedding day. Ah!!! It's the Alanis Morrisette meaning of irony, not the dictionary one! reply vsuperpower2020 10 hours agorootparentprevThat certainly is gregarious! reply ngcc_hk 9 hours agorootparentprevNever get that. Bad in literature. Thanks. reply koushik 7 hours agoprevThis looks cool, after 3 years in financial technology industry working on c/c++ projects I’m in process of revisiting textbooks and relearning computer science fundamentals! Added raylib to my ‘explore’ list I love this idea of reinventing wheel with such explicit goal (even if it sounds counterintuitive to some), we can rethink initial assumptions, best case scenario we come out with better implementations and paradigms than existing standards, worst case scenario you learn internals of tools and techniques you use daily! reply parasti 15 hours agoprevThere's something really powerful about taking the tools that you know very well and just making something cool with them. Really enjoyed this writeup, thanks. reply rkagerer 6 hours agoprevYou could certainly make it harder on yourself by malloc-ing each Shape individually and storing those pointers in a dynamic array. Using a language like C# ... would force that allocation structure. What's stopping you from using a fixed array of structs in C#, just as the author has done in C? reply kronovecta 3 hours agoparentNothing. In C# it's not unusual to use struct arrays in this way. reply OskarS 2 hours agorootparentProbably confusing it with Java, where the only value types are the basic ones (unless you store each individual coord in a different array). reply icoder 11 minutes agorootparentStill you could in Java avoid the (under the hood) memory allocation and garbage collection by reusing the same objects. If you wanted to, of course, it probably takes a bit more effort / caution at the developer's side but may provide some improvements in performance in very specific cases/situations. reply jasonjmcghee 13 hours agoprevThis looks like such a fun jam - wish I'd have known about it! When's the next one? reply bvisness 10 hours agoparentThe next Wheel Reinvention Jam will be in September! We're just finalizing our plans for that jam and our smaller Visibility Jam in July. If you're interested in participating, then join the Handmade Network Discord server (link on our home page at https://handmade.network/). reply gorkermann 7 hours agoprevTo get a look at SDF rendering in a game, check out the blue clouds on the ground in Solar Ash: https://youtu.be/HqQpYSQDIZQ?si=vMKplmGJIGvUn_LT reply freecodyx 10 hours agoprevI sometimes think, that c is all we need reply yazzku 9 hours agoparentI write C all day and enjoy every second of it. I assume there are many like myself. There's just nothing new to make noise about, and that's a good thing, other than small quality-of-life improvements in the standard. reply kewp 6 hours agoprevI wish he had tried instead to do the faster subset of typescript, that is a pet peeve of mine and I've love to see how it would be done! reply cnity 30 minutes agoparentAssemblyScript? reply anthk 1 hour agoprevCan I compile it with GNUStep? reply swiftcoder 13 hours agoprevThat's some impressive development speed. Really enjoyed the explainer video too! reply dhooper 15 hours agoprevThanks for the share! reply Jach 8 hours agoprevReally cool and the video was great too. Indie devs should probably consider doing this sort of thing more often, building these simple tools in service of a game rather than just using the industry standard tool. It can be a great way to add artistic character as well as rigorously enforce some limitations if that's what you're after. reply sandwichukulele 14 hours agoprevis the source code available? I looked through the blog post and linked videos but could not find a github repo or anything similar reply dhooper 14 hours agoparentI just made it public: https://github.com/danielchasehooper/ShapeUp-public reply jbritton 13 hours agorootparentHow did you create / obtain the example shapes? Is there a standard format your code parses? reply dhooper 13 hours agorootparentthe example files in the repo were made using the macos build of shapeUp and saved (the web build doesn't have saving) reply TruthWillHurt 14 hours agoparentprevOr just being able to save/load creations would be nice :) reply dhooper 14 hours agorootparentyeah I never implemented saving/loading for the web. Thats one example of how raylib doesn't totally abstract the underlying platform for you. reply RamiAwar 13 hours agoprevAmazing write up, thanks! Really enjoyed it, miss working on C/C++ apps from scratch and having full control reply xixixao 15 hours agoprevSuper impresssive for getting this done in a week. Being able to make pretty demo models definitely helps too! :) reply emmanueloga_ 13 hours agoprevWhat is a stablished 3d modeler that uses the same kind of modeling as this one? reply dantondwa 11 hours agoparentThe two best SDF modelers in existence are MagicaCSG[1] and Adobe Substance Modeler[2]. There are also a few others, like Womp, but those two are the most feature-complete. Blender is also adding them as part of geometry nodes, and there is also an add-on that is working on adding SDF for hard-surface. [1] https://ephtracy.github.io/index.html?page=magicacsg & https://www.patreon.com/magicavoxel [2] https://www.adobe.com/products/substance3d/apps/modeler.html reply Lichtso 13 hours agoparentprevMany 3D content creation tools such as Blender [0] have SDF [1] (e.g. metaballs in Blender) and CSG [2] (e.g. boolean modifier in Blender) features. But these are rarely used as they can only define volumes, not surfaces. And we are usually interested in surfaces for assigning textures and materials. Thus, polygons / meshes and curves / splines dominate the industry. [0]: https://www.blender.org/ [1]: https://en.wikipedia.org/wiki/Signed_distance_function [2]: https://en.wikipedia.org/wiki/Constructive_solid_geometry reply joeld42 10 hours agoparentprevBlender Geometry nodes supports SDF modelling. It's just not a widely known or used technique. But it's super powerful. reply starmole 12 hours agoparentprevAdobe/Substance3D Modeler reply antirez 13 hours agoparentprevTinkerCAD. reply ngcc_hk 9 hours agoprevReally agreed with his assertion of c. More to his “ Its syntax doesn’t hide complex operations. It’s simple enough that I don’t have to constantly look things up”, and further if you need to look up something about c, it is very easy and very informative. Simple and old lang has its benefits. reply syphiant 9 hours agoprevImpossible! Wow! This is absolutely mind blowing. reply rationalfaith 12 hours agoprevGood stuff! As a c/c++ coder maintaining his game engine (for commercial and hobby purposes), this is always good to see! reply neonsunset 4 hours agoprev [–] \"Using a language like C#, Javascript, or Python would force that allocation structure.\" No. C# structs are C structs. Shape[], Span or Shape* would not be an array of pointers. Any of the following would work: var fromHeap = new Shape[100]; var fromStack = (stackalloc Shape[100]); var fromPool = ArrayPool.Shared.Rent(100); var fromMalloc = (Shape*)NativeMemory.Alloc(sizeof(Shape) * 100); reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Daniel Hooper developed a 3D modeler named \"ShapeUp\" during a week-long programming event using C and ray marched signed distance fields for feasibility.",
      "He managed shapes with static memory allocation and incorporated a user interface, GLSL shader generator, mouse selection, and marching cubes export.",
      "Despite challenges with the raylib library, Hooper completed the project, producing a functional 3D modeler with 2300 lines of C and 250 lines of GLSL code, available online for viewing and testing."
    ],
    "commentSummary": [
      "The post delves into the limitations, challenges, and positives of utilizing Raylib for game development, touching on fullscreen issues, rendering capabilities, and saving shaders.",
      "Users express their preferences for borderless fullscreen mode compared to true fullscreen, while discussing smooth scrolling in 2D graphics on browsers with WebAssembly.",
      "Various topics such as parameter validation in C/C++, clear documentation importance in C programming, benefits of C in modeling/rendering, and utilization of Signed Distance Functions (SDFs) are also covered, along with mentions of advanced CAD tools, memory management in Java, upcoming game dev events, SDF rendering in Solar Ash, and the advantages of employing C/C++ in game development."
    ],
    "points": 494,
    "commentCount": 112,
    "retryCount": 0,
    "time": 1714672094
  },
  {
    "id": 40237745,
    "title": "Mastering Cryptography: Dive into Dan Boneh's In-Depth Course",
    "originLink": "https://crypto.stanford.edu/~dabo/courses/OnlineCrypto/",
    "originBody": "Online Cryptography Course Instructor: Dan Boneh, Stanford University Online cryptography course preview: This page contains all the lectures in the free cryptography course. To officially take the course, including homeworks, projects, and final exam, please visit the course page at Coursera. Go to course Textbook: The following is a free textbook for the course. The book goes into more depth, including security proofs, and many exercises. A Graduate Course in Applied Cryptography by D. Boneh and V. Shoup (free) Course syllabus, videos, and slides Week 1: Course overview and stream ciphers (chapters 2-3 in the textbook) Slides for week 1: Introduction: pdf pptx Stream ciphers: pdf pptx What is cryptography? Course overview (10 min.) What is cryptography (15 min.) History of cryptography (18 min.) Crash course in discrete probability Discrete probability (crash course) (18 min.) Discrete probability (crash course, continued) (13 min.) Stream Ciphers 1: the one-time pad and stream ciphers Information theoretic security and the one-time pad (18 min.) Stream ciphers and pseudorandom generators (19 min.) Stream Ciphers 2: attacks and common mistakes Attacks on stream ciphers and the one-time pad (23 min.) Stream Ciphers 3: real-world examples Real-world stream ciphers (19 min.) Stream Ciphers 4: what is a secure cipher? PRG security definition (24 min.) Semantic security (15 min.) Stream ciphers are semantically secure (10 min.) Week 2: Block ciphers (chapters 4-5 in the textbook) Slides for week 2: Block ciphers: pdf pptx Using block ciphers: pdf pptx Block Ciphers 1: overview What are block ciphers (16 min.) Block Ciphers 2: The Data Encryption Standard The Data Encryption Standard (DES) (21 min.) Exhaustive search attacks (19 min.) More attacks on block ciphers (16 min.) Block Ciphers 3: AES and other constructions The AES block cipher (13 min.) Block ciphers from PRGs (11 min.) How to Use Block Ciphers 1: one-time key Review: PRPs and PRFs (11 min.) Modes of operation: one-time key (7 min.) How to Use Block Ciphers 2: many-time key Security for many-time key (CPA security) (22 min.) Modes of operation: many-time key (CBC) (16 min.) Modes of operation: many-time key (CTR) (9 min.) Week 3: Message integrity (chapters 6-8 in the textbook) Slides for week 3: Message integrity: pdf pptx Collision resistant hashing: pdf pptx Message Integrity 1: definitions Message authentication codes (15 min.) MACs based on PRFs (9 min.) Message Integrity 2: constructions CBC-MAC and NMAC (19 min.) MAC padding (8 min.) PMAC and Carter-Wegman MAC (15 min.) Collision Resistance 1: what is a collision resistant function? Introduction (10 min.) Generic birthday attack (14 min.) Collision Resistance 2: constructions The Merkle-Damgard paradigm (11 min.) Constructing compression functions (8 min.) HMAC: a MAC from a hash function HMAC (7 min.) Timing attacks on MAC verification (8 min.) Week 4: Authenticated encryption (chapter 9 in the textbook) Slides for week 4: Authenticated encryption: pdf pptx Odds and ends: pdf pptx Authenticated Encryption 1: why is it so important? Active attacks on CPA-secure encryption (12 min.) Definitions (5 min.) Chosen ciphertext attacks (12 min.) Authenticated Encryption 2: standard constructions Constructions from ciphers and MACs (20 min.) Authenticated Encryption 3: pitfalls Case study: TLS 1.2 (17 min.) CBC padding attacks (14 min.) Attacking non-atomic decryption (9 min.) Odds and Ends 1: how to derive keys Key derivation (13 min.) Odds and Ends 2: searching on encrypted data Deterministic encryption (14 min.) Deterministic encryption: SIV and wide PRP (20 min.) Odds and Ends 3: disk encryption and creditcard encryption Tweakable encryption (14 min.) Format preserving encryption (12 min.) Week 5: Basic key exchange (chapter 10 in the textbook) Slides for week 5: Basic key exchange: pdf pptx Crash course in number theory: pdf pptx Basic Key Exchange 1: problem statement Trusted 3rd parties (11 min.) Merkle puzzles (11 min.) Basic Key Exchange 2: two solutions The Diffie-Hellman protocol (19 min.) Public-key encryption (10 min.) Number Theory 1: modular arithmetic Notation (14 min.) Fermat and Euler (18 min.) Modular e'th roots (17 min.) Number Theory 2: easy and hard problems Arithmetic algorithms (12 min.) Intractable problems (18 min.) Week 6: Public-key encryption (chapters 11-12 in the textbook) Slides for week 6: Trapdoor permutation: pdf pptx Diffie-Hellman: pdf pptx Public Key Encryption from Trapdoor Permutations Definitions and security (15 min.) Constructions (10 min.) Public Key Encryption from Trapdoor Permutations: RSA The RSA trapdoor permutation (17 min.) PKCS1 (21 min.) Public Key Encryption from Trapdoor Permutations: attacks Is RSA a one-way function? (16 min.) RSA in practice (13 min.) Public Key Encryption From Diffie-Hellman: ElGamal The ElGamal public-key system (19 min.) ElGamal security (13 min.) ElGamal variants with better security (10 min.) Public Key Encryption: summary A unifying theme (11 min.) Farwell for now (5 min.) Week 7: Digital signatures (chapters 13-14 in the textbook) Slides for week 7: Digital signatures: pdf pptx Hash-based signatures: pdf pptx (c) Dan Boneh, Stanford University",
    "commentLink": "https://news.ycombinator.com/item?id=40237745",
    "commentBody": "Online Cryptography Course (2017) (stanford.edu)337 points by Tomte 18 hours agohidepastfavorite56 comments feross 16 hours agoCS255 Intro to Cryptography was one of my favorite courses as a Stanford student. Dan is an incredible instructor. If you want more Stanford security course material, I also recommend CS253 Web Security (https://web.stanford.edu/class/cs253/) (disclosure: I created this course) and CS356 Topics in Computer and Network Security (https://cs356.stanford.edu/). Videos for CS253 are online here: https://www.youtube.com/playlist?list=PL1y1iaEtjSYiiSGVlL1cH... reply amyamyamy2 15 hours agoparentSeconding this - I loved 155, 255, and 251 from Professor Boneh. He's very talented and was one of my favorite instructors for multiple years; great at explaining concepts. reply Aaronstotle 15 hours agoparentprevThank you for listing these! reply johnwatson11218 17 hours agoprevI took an online cryptography course from this professor a few years back. It was very good. Even though I have a background in math that class made me realize I don't want to be a professional cryptographer, which in itself is pretty nice. But all joking aside I really enjoyed the way that crypto systems were analyzed using demons and games to try and discern the random bit stream from the encrypted bytes. reply foma-roje 17 hours agoparentSame here. I also took the course from Coursera a few years ago and I really enjoyed it. The conclusion? It’s hard to get it right so don’t do crypto yourself! Quite cynical, but it kinda killed my interest in pursuing it further. reply kryptonomist 16 hours agorootparentYes, the Coursera title was \"Cryptography I\", so I was expecting another part, but ten years after, I still do not see any. A great course anyway. reply Ar-Curunir 15 hours agorootparentThere's a running joke among cryptographers that Dan will release Crypto II next semester =) reply pdevr 17 hours agoprevExcellent course. The only caveat I want to add is, the estimated hours to complete (23 hours) can vary extremely, depending on your 'pre-mastery' of the subject (or lack of it). Prepare and pace yourself considering that. reply lordgrenville 15 hours agoprevA lot of people in this thread seem to be interested in a hands-on, no theory, practical way of learning crypto. If this is you check out (HN MVP tptacek's) cryptopals.com reply failbuffer 17 hours agoprevLooks good, but I wish there was a practicioner-oriented resource for how to use cryptographic libraries that didn't start by focusing on the math. I don't need to know the intricacies of RSA, I need to know how to securely compose it with other primitives to engineer a system with the desired properties. reply Vegenoid 16 hours agoparentI wanted to have a better understanding of crypto, simply to feel more confident in writing programs that use existing protocols, and started 'Real-World Cryptography' by David Wong. I'm about 3/4 through, and I've been happy with it. It is light on math, but does go into it a little bit - it seems designed for the kind of person who isn't comfortable using something until they understand how it works under-the-hood, but doesn't actually need to do any under-the-hood work. It has taught me enough that I think I could compose a protocol out of primitives that on the surface appears to do what I've intended it to do. It has also taught me that there are many subtleties that can completely break a protocol, combining primitives can lead to unexpected weaknesses, and many people who understand crypto far better than I ever will have created broken protocols out of secure primitives. I'm not sure it's the book you're looking for, but I think it's a good book if you want to understand crypto, but not design your own. reply tptacek 13 hours agoparentprevI want to put a word in here for being cautious about the capabilities you can achieve in novel systems --- software developers are often working with multiple whole sieverts of novelty without realizing it --- without having a lot of the boring theory stuff nailed down. If you're using (say) libsodium to do exactly the kind of thing 100 other developers have successfully used libsodium to do in the past, you're fine. But it takes a deceptively small and subtle set of steps to end up synthesizing a new cryptosystem (see: attempts to build secure messaging systems out of libsodium primitives) without realizing that's what you're doing. Learn a bunch of the theory! It's important. reply schoen 12 hours agorootparent> sieverts of novelty Yikes! Is this clever metaphor original with you? reply tptacek 12 hours agorootparentI'm a little proud of it. reply splix 16 hours agoparentprevGoogle \"Cryptographic Right Answers\". There are a couple of different posts, but they agree on the most of the things you would look for. Ex.: https://gist.github.com/tqbf/be58d2d39690c3b366ad or https://www.latacora.com/blog/2018/04/03/cryptographic-right... reply miketery 15 hours agorootparentThis is great, finding NaCl (libsodium) has been a godsend, specifically the JS lib. 1 - https://nacl.cr.yp.to/ 2 - https://github.com/dchest/tweetnacl-js reply foma-roje 16 hours agoparentprevPerhaps what you need is something like „Cryptography Engineering: Design Principles and Practical Applications“ Book by Bruce Schneier, Niels Ferguson, and Tadayoshi Kohno. reply tptacek 13 hours agorootparentPretty outdated. For awhile, it was the best book available, but in 2024 it's probably harmful. Today, I'd read Serious Cryptography or Real World Cryptography. reply Ar-Curunir 15 hours agoparentprev\"Secure composition\" is definitely covered in the course. It doesn't talk only about the details of RSA (though there are some lectures about that), but also about what security properties different primitives satisfy, how to compose them safely, etc. A large part of modern cryptography is figuring out secure composition. reply galleywest200 16 hours agoprevIf anyone would like to practice some of these lessons in a \"capture the flag\" format, I would recommend https://cryptohack.org/ -- great site! reply lazzlazzlazz 15 hours agoprevAn excellent course and one that has been critical in my professional development. Worth noting that Dan Boneh is also an advisor for a16z crypto's research team[1], and he produces a significant amount of blockchain-related content with them.[2] [1]: https://a16zcrypto.com/research/ [2]: https://a16zcrypto.com/team/dan-boneh/ reply blacklion 14 hours agoprevWho is still waiting for Cryptography II course on Coursera? :))) reply meling 14 hours agoparentCount me in! reply funcimp 6 hours agoprevThis is a fantastic course. I took it in 2018, and that started a snowball of online learning that lead to me doing Georgia Tech’s Online Masters in Computer Science program. I just finished that this semester. These sorts of programs are fantastic structure for life-long learners. reply AlexCoventry 15 hours agoprevThe book he co-authored, A Graduate Course in Applied Cryptography, has been very helpful. https://toc.cryptobook.us/ reply amingilani 16 hours agoprevI wish there was a similar part 2 course. All the courses I’ve seen seem to end at roughly the same point. I want to learn about elliptic curve cryptography and post quantum crypto systems. reply alternativity 16 hours agoparentIn case it helps, I found this course useful as intro to elliptical curve crypto - https://youtube.com/@introductiontocryptography4223?si=O-5_a... lectures 16 and 17 in particular. reply dvas 16 hours agoparentprevI would like to add the thought of looking at where these elliptic curves are deployed, things like embedded devices and implementations bitcoin-core libraries for say secp256k1 [0]. Ref: [0] Optimized C library for EC operations on curve secp256k1 https://github.com/bitcoin-core/secp256k1 reply ShaneCurran 17 hours agoprevIf anybody's interested in any of the algorithms and papers that underpin most modern cryptography, we created a dedicated page on our site[0] as an homage to the great cryptographers of the last century(!) (and their works). [0]: https://evervault.com/papers reply dvas 16 hours agoparentThanks for sharing Shane, and nice to see companies engaged with the community on a technical level! reply the_svd_doctor 15 hours agoprevI took both his in-person and coursera classes, and Dan is a great teacher. Highly recommend. reply paladin314159 15 hours agoprevDan Boneh is amazing. I took his Cryptography course at Stanford and loved it so much that I ended up having him advise me on my senior thesis. Would highly recommend stuff that he puts out. reply Bnjoroge 14 hours agoprevDidn't take the class, but always felt like his book was too theoretical. I enjoyed \"real world cryptography\" and supplemented it with Dan's book. reply davepeck 15 hours agoprevI took this course ages ago, along with the follow up Crypto II. Dan is a great instructor, and his courses helped fill in a number of gaps in my knowledge. Highly recommended! reply lifeinthevoid 15 hours agoprevI took it for the second time recently, this time with the goal to shift my career to cryptography. Still one of the best resources out there imo. reply nailer 16 hours agoprev> Public-key encryption And it’s all RSA. Can crypto tutorials please add ECC already? reply kwantam 2 hours agoparentMaybe we're looking at different things, but the link appears to discuss ElGamal encryption, which is discrete log based (which means modern implementations use elliptic curves; historically it would have been discrete log in a subgroup of a large prime field). It also talks about BLS signatures, which are exclusively elliptic curve based. By and large, anything whose security relies on discrete log can be implemented using an elliptic curve, but beginning cryptography classes treat that as an implementation detail because mostly all you need is a prime-order group, and elliptic curves can mostly be treated as a black-box prime order group. (BLS signatures are an exception; they require a bilinear pairing, which in turn requires a special kind of elliptic curve that's not just a black-box prime order group.) There are all sorts of great algebraic geometry tricks to be played with elliptic curves, but those almost certainly aren't going to be found in an intro crypto class, or maybe any CS class... reply zer0tonin 16 hours agoprevWill they ever release Crypto II? reply withzombies 16 hours agoparentI've been registered for Crypto II on Coursera for over a decade now! reply phantom--88 17 hours agoprevCould seeve me as i'am a noob on this field. Thanks for the share reply brcmthrowaway 12 hours agoprevVery outdated. reply malviyamukul 15 hours agoprevThanks for sharing reply shihanwan1 15 hours agoprevfrom site alone, you can tell it's legit reply xhkkffbf 15 hours agoprevA stellar course! reply begueradj 16 hours agoprev [–] It looks more suitable for those who are into mathematics. \"Applied Cryptography\", by Bruce Schneier, is also good for those who, like myself, do not need all the mathematical details behind cryptography. reply H8crilA 16 hours agoparentIt is impossible to study cryptography without \"all the mathematical details\". You can at best implement someone's scheme, but even that is not the best idea, as you're likely to make some mistake somewhere. reply ilya_m 16 hours agoparentprev [–] (Writing as a professional cryptographer.) Schneier's \"Applied Cryptography\" is about as useful for learning about cryptography as \"The Da Vinci Code\" for learning about Renaissance. It is a lively book that name-checks relevant concepts, and may even lead someone to develop interest in the actual stuff. (That was my gateway to cryptography!) Mention Schneier at a gathering of cryptographers, and you'll elicit groans and eye-rolls. The main reason for that is that his book creates an illusion of understanding without instilling tthat it covers literally 1% of what one needs to seriously work in the field. It is also ~30 years old, and was dated even when it appeared. This is not to diminish the fact that Schneier is an excellent communicator and has done a great service to the security field by being a consistent and effective critic of the domestic security apparatus. reply helpfulclippy 16 hours agorootparentWhen you say \"Mention Schneier,\" do you mean Schneier himself or Applied Cryptography specifically? I was unaware of any particular generalized disdain for the man, though I'm certainly aware of plenty for the book, which you've summarized quite well. I remember in the intro to one of his later books (Cryptography Engineering, I think), Schneier actually apologized for making a book that was in many ways quite dangerous, and said his newer work was in an effort to make something a bit more focused on providing people with the firm foundations they'd need to do responsible work in cryptography. That said, Applied Cryptography is a very inspiring book in many ways (which is both the best thing and worst thing about it, because it's not obvious upon reading it just how unprepared the reader is to act on that inspiration). I really wish someone would go write a new Applied Cryptography that dreams and inspires as much, but balanced with perspective and caution, and based on more recent developments. reply tptacek 13 hours agorootparentThe authors had some weird blind spots, even for the time, when Practical Cryptography (now called Cryptography Engineering) was published --- curves and authenticated encryption seem like the two obvious examples. reply mttpgn 12 hours agorootparentprevThe cryptographer Dan J. Bernstein once told me a story that Bruce Schneier kept some cryptographic protocol secure for an additional 24 hours. The researcher demonstrating this protocol's weakness based their proof-of-concept on a proof in Schneier's book. However, Schneier's description contained a mathematical error. When the error in the proof-of-concept was pointed out to the researcher at the conference, this researcher went back to their hotel room, discovered the origin of the error in Schneier's text, and fixed the proof-of-concept for the conference-goers by the following day. Thus, Bruce Schneier kept a cryptographic protocol secure for an additional 24 hours. reply MattSteelblade 16 hours agorootparentprevI'm surprised to hear that. I have never read Applied Cryptography, but I find that an incredibly damning simile (though maybe it wasn't intended to be?). Didn't Schneier develop Blowfish? reply ilya_m 11 hours agorootparent> an incredibly damning simile I stand by my comment, however harsh it may seem. Some of the disdain held by cryptographers, especially of a certain generation, is in no doubt a reaction to Schneier's prominence in the public eye as Mr. Crypto. The fact that he is highly quotable and media-savvy makes him a go-to person whenever a comment is needed on something (anything!) happening in security. reply egl2021 16 hours agorootparentprev [–] What is a better book? I don't want anyone eye-rolling when I'm LARPing. reply helpfulclippy 16 hours agorootparent\"Serious Cryptography\" is good. There's an updated edition dropping later this year. \"Real World Cryptography\" is also good. reply ilya_m 11 hours agorootparentprevA better book for what audience? The scientifically minded can do much worse than \"A Graduate Course in Applied Cryptography\" by Dan Boneh and Victor Shoup (on which the online cryptography course is based). For a more practical angle, I agree with other commenters on this thread: \"Cryptography Engineering\" (Ferguson, Schneier, Kohno), \"Serious Cryptography\" (Aumasson) and \"Real-World Cryptography\" (Wong) are pretty solid. reply seabass-labrax 11 hours agorootparentprev [–] What LARP involves academic comparisons of cryptographic algorithms? Whatever it is, it sounds like my sort of thing ;) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Stanford University offers an online cryptography course by Dan Boneh, covering stream ciphers, block ciphers, key exchange, and more.",
      "The course provides video lectures, slides, and a free textbook for a comprehensive understanding of cryptography topics.",
      "Students can explore topics like message integrity, digital signatures, and public-key encryption with in-depth explanations and real-world applications each week."
    ],
    "commentSummary": [
      "Stanford University offers a cryptography course by Dan Boneh, receiving positive reviews and suggested additional resources.",
      "Books like \"Real-World Cryptography\" and \"Applied Cryptography\" by Bruce Schneier are recommended for practical and theoretical insights into cryptography.",
      "There's a debate on the difficulty of Boneh's course and critiques on the relevance of Schneier's book due to outdated content; however, Boneh's course is highly recommended for cryptography enthusiasts."
    ],
    "points": 337,
    "commentCount": 56,
    "retryCount": 0,
    "time": 1714665643
  },
  {
    "id": 40243238,
    "title": "Cardio Fitness: Key Predictor of Health Risks",
    "originLink": "https://bjsm.bmj.com/content/58/10/556",
    "originBody": "Skip to main content Subscribe Log In More Log in via Institution Log in via OpenAthens Log in via your Society Select your society ACMSE ACPSEM ACSEP AMSSM BASEM BASRAT CASEM CLINICAL EDGE FSEM GISSPORT IOC OSCA SASMA SGMS SMA SPC SPNZ SSMS SSPA SST SUFT VSG Log in using your username and password For personal accounts OR managers of institutional accounts Username * Password * Forgot your log in details?Register a new account? Forgot your user name or password? Basket Search More Search for this keyword Advanced search Latest content Current issue Archive For authors Resources About New editors Search for this keyword Advanced search Close More Main menu Latest content Current issue Archive For authors Resources About New editors Subscribe Log in More Log in via Institution Log in via OpenAthens Log in via your Society Select your society ACMSE ACPSEM ACSEP AMSSM BASEM BASRAT CASEM CLINICAL EDGE FSEM GISSPORT IOC OSCA SASMA SGMS SMA SPC SPNZ SSMS SSPA SST SUFT VSG Log in using your username and password For personal accounts OR managers of institutional accounts Username * Password * Forgot your log in details?Register a new account? Forgot your user name or password? BMJ Journals You are here Home Archive Volume 58, Issue 10 Cardiorespiratory fitness is a strong and consistent predictor of morbidity and mortality among adults: an overview of meta-analyses representing over 20.9 million observations from 199 unique cohort studies Email alerts Article Text Article menu Article Text Article info Citation Tools Share Rapid Responses Article metrics Alerts Article Text Article info Citation Tools Share Rapid Responses Article metrics Alerts PDF PDF + Supplementary Material Systematic review Cardiorespiratory fitness is a strong and consistent predictor of morbidity and mortality among adults: an overview of meta-analyses representing over 20.9 million observations from 199 unique cohort studies http://orcid.org/0000-0002-1768-319XJustin J Lang1,2,3, http://orcid.org/0000-0001-6729-5649Stephanie A Prince1,2, Katherine Merucci4, http://orcid.org/0000-0002-4513-9108Cristina Cadenas-Sanchez5,6, http://orcid.org/0000-0002-5607-5736Jean-Philippe Chaput2,7,8, http://orcid.org/0000-0002-1752-5431Brooklyn J Fraser3,9, http://orcid.org/0000-0001-5461-5981Taru Manyanga10, Ryan McGrath3,11,12,13, http://orcid.org/0000-0003-2001-1121Francisco B Ortega5,14, http://orcid.org/0000-0002-7227-2406Ben Singh3, http://orcid.org/0000-0001-7601-9670Grant R Tomkinson3 1 Centre for Surveillance and Applied Research, Public Health Agency of Canada, Ottawa, Ontario, Canada 2 School of Epidemiology and Public Health, Faculty of Medicine, University of Ottawa, Ottawa, Ontario, Canada 3 Alliance for Research in Exercise, Nutrition and Activity (ARENA), Allied Health and Human Performance, University of South Australia, Adelaide, South Australia, Australia 4 Health Library, Health Canada, Ottawa, Ontario, Canada 5 Department of Physical Education and Sports, Faculty of Sport Sciences, Sport and Health University Research Institute (iMUDS), University of Granada; CIBEROBN, ISCIII, Granada, Andalucía, Spain 6 Stanford University, Department of Cardiology; and Veterans Affair Palo Alto Health Care System, Palo Alto, California, USA 7 Children’s Hospital of Eastern Ontario Research Institute, Ottawa, Ontario, Canada 8 Department of Pediatrics, Faculty of Medicine, University of Ottawa, Ottawa, Ontario, Canada 9 Menzies Institute for Medical Research, University of Tasmania, Hobart, Tasmania, Australia 10 Division of Medical Sciences, University of Northern British Columbia, Prince George, British Columbia, Canada 11 Fargo VA Healthcare System, Fargo, North Dakota, USA 12 Department of Health, Nutrition, and Exercise Sciences, North Dakota State University, Fargo, North Dakota, USA 13 Department of Geriatrics, University of North Dakota, Grand Forks, North Dakota, USA 14 Faculty of Sport and Health Sciences, University of Jyväskylä, Jyväskylä, Finland Correspondence to Dr Justin J Lang, Public Health Agency of Canada, Ottawa, Canada; justin.lang@phac-aspc.gc.ca Abstract Objective To examine and summarise evidence from meta-analyses of cohort studies that evaluated the predictive associations between baseline cardiorespiratory fitness (CRF) and health outcomes among adults. Design Overview of systematic reviews. Data source Five bibliographic databases were searched from January 2002 to March 2024. Results From the 9062 papers identified, we included 26 systematic reviews. We found eight meta-analyses that described five unique mortality outcomes among general populations. CRF had the largest risk reduction for all-cause mortality when comparing high versus low CRF (HR=0.47; 95% CI 0.39 to 0.56). A dose–response relationship for every 1-metabolic equivalent of task (MET) higher level of CRF was associated with a 11%–17% reduction in all-cause mortality (HR=0.89; 95% CI 0.86 to 0.92, and HR=0.83; 95% CI 0.78 to 0.88). For incident outcomes, nine meta-analyses described 12 unique outcomes. CRF was associated with the largest risk reduction in incident heart failure when comparing high versus low CRF (HR=0.31; 95% CI 0.19 to 0.49). A dose–response relationship for every 1-MET higher level of CRF was associated with a 18% reduction in heart failure (HR=0.82; 95% CI 0.79 to 0.84). Among those living with chronic conditions, nine meta-analyses described four unique outcomes in nine patient groups. CRF was associated with the largest risk reduction for cardiovascular mortality among those living with cardiovascular disease when comparing high versus low CRF (HR=0.27; 95% CI 0.16 to 0.48). The certainty of the evidence across all studies ranged from very low-to-moderate according to Grading of Recommendations, Assessment, Development and Evaluations. Conclusion We found consistent evidence that high CRF is strongly associated with lower risk for a variety of mortality and incident chronic conditions in general and clinical populations. Cardiovascular Diseases Review Cohort Studies Physical fitness Data availability statement Data are available on reasonable request. http://creativecommons.org/licenses/by-nc/4.0/ This is an open access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited, appropriate credit is given, any changes made indicated, and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/. https://doi.org/10.1136/bjsports-2023-107849 Statistics from Altmetric.com Supplementary materials Request Permissions If you wish to reuse any or all of this article please use the link below which will take you to the Copyright Clearance Center’s RightsLink service. You will be able to get a quick price and instant permission to reuse the content in many different ways. Cardiovascular Diseases Review Cohort Studies Physical fitness WHAT IS ALREADY KNOWN ON THIS TOPIC Many systematic reviews have examined the prospective link between baseline cardiorespiratory fitness and health outcomes, but no study has compiled all the evidence to help identify important gaps in the literature. WHAT THIS STUDY ADDS This study identified 26 systematic reviews with meta-analysis representing over 20.9 million observations from 199 unique cohort studies. Cardiorespiratory fitness was strongly and consistently protective of a variety of incident chronic conditions and mortality-related outcomes. Gaps in the literature continue to exist, with limited evidence available among women, and certain clinical populations. Several health outcomes could benefit from future meta-analyses, including specific cancer types, especially among women (eg, breast cancer) and mental health conditions beyond depression. HOW THIS STUDY MIGHT AFFECT RESEARCH, PRACTICE OR POLICY Given the strength of the predictive utility of cardiorespiratory fitness across many health outcomes, cardiorespiratory fitness would be a valuable risk stratification tool in clinical practice. Introduction Cardiorespiratory fitness (CRF) is a physical trait that reflects the integrated function of numerous bodily systems to deliver and use oxygen to support muscle activity during sustained, rhythmic, whole-body, large muscle physical activity.1 CRF can be objectively measured using direct (usually by maximal exercise testing with concomitant gas exchange analysis)2 or indirect (exercise predicted equations)3 4 methods with a variety of maximal or submaximal protocols using different modalities (eg, stationary cycling, treadmill running/walking, bench stepping, field-based running/walking). Non-exercise prediction equations with reasonable validity are also available when direct CRF measurement is not feasible.5 6 CRF is commonly expressed as the maximum or peak rate of oxygen consumption per kilogram of body mass (common units: mL/kg/min) or metabolic equivalents of task (METs). Nearly half of the variance in CRF is attributable to genetics, with the remainder modified primarily through habitual physical activity.7 For example, brisk walking for approximately 150 min per week can result in large relative improvements in CRF among sedentary and unfit individuals.8 9 Even those with severe chronic disease can improve CRF through well-planned aerobic physical activity programmes.10 Low CRF is considered a strong chronic disease risk factor that is not routinely assessed in clinical practice.11 Evidence suggests that the inclusion of CRF as a clinical vital sign would enhance patient management by improving the classification of those at high risk of adverse outcomes.11 The evidence supporting CRF as an important risk factor has accumulated since the 1980s through large cohort studies that investigated the prospective risk of all-cause mortality and cardiovascular events associated with CRF.12–14 Research has linked CRF to the incidence of some cancers (eg, colon/rectum, lung),15 type 2 diabetes,16 metabolic syndrome,17 stroke18 and depression.19 Higher CRF may even improve the prognosis in those with chronic conditions such as cancer,20 peripheral artery disease,21 heart failure22 and chronic kidney disease.23 Given the mounting evidence supporting CRF as an important risk factor, numerous systematic reviews with meta-analyses summarising results of primary studies for various health outcomes have been published. Kodama et al 24 published the first meta-analysis on the health-related predictive validity of CRF and found that a 1-MET (3.5 mL/kg/min) higher level of CRF was associated with a 13% and 15% reduction in the risk of all-cause mortality and cardiovascular disease (CVD) events, respectively. This study helped to establish the meaningful clinically important difference (MCID) of 1-MET for exercise trials. Since Kodama’s study, there have been several systematic reviews with meta-analyses, with several published in recent years (ie, 2020+). Most systematic reviews have focused on a single health outcome. To date, there has not been a systematic synthesis of the relationships between CRF and a broad range of health outcomes. To help summarise the breadth of evidence, an overview of reviews provides a systematic method to examine evidence across a range of outcomes for a specific exposure.25 Thus, the objective of this study was to conduct an overview of systematic reviews with meta-analyses from cohort studies that investigated relationships between CRF and prospective health-related outcomes among adults. We also aimed to assess the certainty of the evidence for each identified health outcome. Methods This overview followed the methods outlined in the Cochrane handbook,25 and additional methods that were published elsewhere.26 We adhered to both the Preferred Reporting Items for Overviews of Reviews statement27 and the Meta-analyses of Observational Studies in Epidemiology reporting standards.28 The overview was prospectively registered with the PROSPERO international prospective register of systematic reviews (#CRD42022370149). Here, we present a condensed methods section with the full methods available in online supplemental methods. Supplemental material [bjsports-2023-107849supp001.pdf] Eligibility criteria Population Adult populations (≥18 years) including apparently healthy and clinical populations with diagnosed chronic conditions. Studies that focused on certain special populations were excluded (ie, those recovering from surgery, athletes, disease at birth, pregnant individuals). Exposure The primary exposure was CRF measured using the following approaches: (1) maximal exercise testing with gas analysis (ie, directly measured V̇O2max/peak), (2) maximal or submaximal exercise testing without gas analysis, which used either exercise prediction equations to estimate CRF or the measured exercise performance (ie, indirect measures) or (3) non-exercise prediction equations for estimating CRF. Outcome Any health-related outcome such as all-cause or cause-specific mortality, incident conditions related to physical risk factors, chronic conditions or mental health issues were included. Among populations with diagnosed chronic conditions, we included evidence on outcomes such as mortality or disease severity. Study design Only systematic reviews with meta-analyses that searched a minimum of two bibliographic databases and provided a sample search strategy were included. We also included meta-analyses that pooled data from primary prospective/retrospective cohort or case-control studies. These studies were the focus because of their ability to assess causality for observational research. Publication status and language restriction Only systematic reviews published in peer-reviewed journals in English, French or Spanish (based on authors’ language capacity) were eligible. Conference abstracts or papers, commentaries, editorials, dissertations or grey literature were ineligible. Time frame Systematic reviews published during the past 20 years for the initial search. Information sources Five bibliographic databases, including OVID Medline, OVID Embase, Scopus, CINAHL and EBSCOhost SPORTDiscus, were searched from 1 January 2002 to 21 November 2022. The search was later updated from 1 November 2022 to 8 March 2024. Search strategy A research librarian (KM) created the search strategy in collaboration with the authorship team, and the final search was peer-reviewed by an independent research librarian using the Peer Review of Electronic Search Strategies guidelines.29 The search strategies for each database are available in online supplemental appendix 1. The reference lists of included papers were also searched for additional relevant systematic reviews. Selection process All records were imported into RefWorks where duplicates were removed using automated and manual methods. Records were imported into Covidence for further deduplication and record screening. Reviewers were not blinded to the study metadata when screening. The title and abstract from each record were screened by two of the following independent reviewers (JJL, SAP, CC-S, J-PC, BJF, TM, BS and GRT) against the inclusion criteria. Full-text papers were obtained for each record that met the inclusion criteria or provided insufficient evidence to make a conclusive decision at the title and abstract stage. Conflicts during title and abstract screening automatically advanced to full-text screening. Each full-text record was screened by two of the following independent reviewers (JJL, SAP, CC-S, J-PC, BJF, TM, BS and GRT) against the inclusion criteria. Conflicts at the full-text stage were resolved through discussion by two reviewers (JJL and SAP), with a third reviewer resolving disagreements (GRT). Data collection process Data extraction was completed in Covidence using a form that was piloted by the authorship group for accuracy. Data from the included studies were extracted by two of the following independent reviewers (JJL, SAP, CC-S, J-PC, BJF, TM, FBO, BS and GRT). Conflicts were resolved by one reviewer (JJL), who contacted the reviewers who extracted the data when necessary to resolve conflicts. Data items The data extraction form included several items related to the demographic characteristics of the primary studies, the meta-analyses effect estimates and related statistics, and details for risk of bias and subgroup analyses. Review quality We extracted the original risk of bias assessment for each primary study, as reported by the study authors. Most of the included studies used the Newcastle-Ottawa Scale (NOS) to assess risk of bias for cohort studies.30 In the event that risk of bias was not assessed, a new assessment was conducted and verified by two reviewers using the NOS. We also assessed quality of the systematic reviews using the second edition of A MeaSurement Tool to Assess systematic Reviews 2 (AMSTAR2) checklist.31 Two of the following independent reviewers (JJL, SAP, CC-S, J-PC, BJF, TM, FBO, BS and GRT) assessed review quality. Conflicts were resolved by one reviewer (JJL), with the reviewers who extracted the data contacted to resolve outstanding conflicts. Effect measures We presented pooled hazard ratios (HRs) or relative risks (RRs) for an incident event (ie, mortality or morbidity) across the included systematic reviews. We extracted data from models that compared high versus low CRF and those that examined the impact of a 1-MET higher level of CRF. Synthesis of data We followed an outcome-centric approach, as outlined by Kho et al.26 Our goal was to identify systematic reviews with non-overlapping primary studies for each outcome to avoid double counting evidence. When more than one eligible systematic review was identified for a single outcome, we calculated the corrected covered area (CCA) to assess the degree of overlap in the primary studies.32 Where, N is the total number of times a primary study appeared across reviews (inclusive of double counting), r is the number of unique primary studies and c is the number of systematic reviews included for the outcome. The CCA was interpreted as slight (0%–5%), moderate (6%–10%), high (11%–15%) or very high (>15%). If the CCA was slight or moderate, we included multiple systematic reviews per outcome. If the CCA was high or very high, we selected the highest quality systematic review according to the AMSTAR2 assessment. We included the most recent systematic review when reviews of the same outcome were rated as equal in quality. Synthesis of results For each health outcome, we reported evidence for apparently healthy and clinical populations separately. We summarised results using a narrative synthesis approach using summary of findings tables. Results were reported as described by the systematic review authors. Meta-analytical results, including the effect, confidence limits, number of studies and number of participants, were presented by outcome using a forest plot to allow for easy comparison between studies. RR values were taken to approximate the HR. When comparing high versus low CRF, we inverted the scale when studies compared low versus high by taking the reciprocal (ie, HR=2.00 was changed to HR=0.50). Dose−response values were rescaled to a 1-MET higher level of CRF when more than 1-MET was used or if the unit increase was in VO2. We rescaled by taking the natural log of the HR, dividing or multiplying it to correspond with 1-MET, and exponentiating the result. Subgroup analyses for sex were described when available. Certainty of the evidence assessment For each outcome, the certainty of the evidence was assessed using a modified Grading of Recommendations, Assessment, Development and Evaluations (GRADE) approach.33 Observational cohort evidence began at ‘high’ certainty because randomised controlled trials were deemed not feasible for our research question.34 The certainty of the evidence could be rated down based on five domains (ie, risk of bias, imprecision, inconsistency, indirectness and publication bias). See online supplemental table 1 for a GRADE decision rules table. Equity, diversity and inclusion statement Our research team included diversity across genders with representation from researchers at all career stages. We stratified our results by sex which allowed use to identify the potential need for more diversity in this area of the literature. This stratification allowed us to discuss the overall generalisability of our results. The GRADE evaluation carried out in this study assessed the indirectness of the results. We downgraded evidence that did not demonstrate good global representation or did not provide a gender-balanced sample. Reducing indirectness is important for ensuring the results are representative of the target population. Results We identified 9062 records after removing duplicates, assessed 199 full-text papers, and excluded 165 papers during full-text screening, and 8 papers because of high or very high overlap based on the CCA calculation (see figure 1 and online supplemental appendix 2 for full texts with reasons for exclusion). The proportion of agreement between reviewers for title and abstract screening ranged from 95% to 100% while the agreement for full-text screening ranged from 75% to 100%. We included 26 systematic reviews with meta-analyses representing over 20.9 million observations from 199 unique cohort studies, including 21 mortality or incident chronic disease outcomes. We identified CCA values in the high or very high range for sudden cardiac mortality (CCA=33%; n=2), incident heart failure (33%; n=2), incident depression (50%; n=2), incident type 2 diabetes (25%; n=4) and all-cause mortality among those living with heart failure (14%; n=3; see online supplemental appendix 2 for more details). We included multiple systematic reviews for all-cause mortality because the CCA was moderate (10%; n=3). Download figure Open in new tab Download powerpoint Figure 1 PRISMA flow chart depicting the number of papers identified, screened and included in the overview. *A list of excluded studies with reasons are provided in online supplemental appendix 2. Tables 1–3 describe the study characteristics. We identified 8 systematic reviews that investigated mortality outcomes, with pooled data from 95 unique primary cohort studies. Nine systematic reviews investigated incident outcomes, pooling data from 63 unique primary cohort studies. The remaining 9 systematic reviews investigated health-related outcomes among populations living with chronic conditions, which represented data from 51 unique primary cohort studies. 11 reviews were of critically low quality, 4 were low, 8 were moderate and 3 were of high quality as assessed using the AMSTAR2 (see online supplemental table 2). See online supplemental table 3 for a detailed summary of findings with the certainty of the evidence for each outcome. View this table: View inline View popup Table 1 Study characteristics for general populations without known disease at baseline and mortality outcomes View this table: View inline View popup Table 2 Study characteristics for general populations without known disease at baseline and incident outcomes View this table: View inline View popup Table 3 Study characteristics for clinical populations with diagnosed chronic disease at baseline and mortality outcomes Figure 2 illustrates results for CRF as a predictor of mortality outcomes, which included all-cause, CVD, sudden cardiac, all cancer and lung cancer mortality. When comparing high versus low CRF across all outcomes, there was a 41% (HR for all-cause mortality24=0.59; 95% CI 0.52 to 0.66) to 53% (HR for all-cause mortality35=0.47; 95% CI 0.39 to 0.56) reduction in the risk of premature mortality. The certainty of the evidence was assessed as very low-to-moderate, mainly due to serious indirectness (ie, most studies only included male participants). In assessing the dose–response relationship, a 1-MET higher level of CRF was associated with a 7% (HR for all cancer mortality35=0.93; 95% CI 0.91 to 0.96) to 51% (HR for sudden cardiac mortality36=0.49; 95% CI 0.33 to 0.73) reduction in the risk of premature mortality. The certainty of the evidence ranged from very low-to-moderate, largely due to serious indirectness from a large proportion of male-only studies. Sex differences were similar between outcomes with larger CIs for females because of smaller samples (see online supplemental figure 1). For example, there were 1 858 274 male participants compared with 180 202 female participants for all-cause mortality. Download figure Open in new tab Download powerpoint Figure 2 HRs for each mortality outcome in apparently healthy populations at baseline for high versus low CRF and per 1-MET increase in CRF. Estimates from Laukkanen (2022), Han (2022), Kodama (2009) and Aune (2020) were reported as RR, the remaining studies were reported as HR. Qui (2021) reported estimates from self-reported CRF. Kodama (2009) reported low versus high CRF which were inverted for this study. CRF, cardiorespiratory fitness; CVD, cardiovascular disease; eCRF, estimated non-exercise cardiorespiratory fitness; GRADE, Grading of Recommendations, Assessment, Development and Evaluations; MET, metabolic equivalent of task; NA, not applicable; NR, not reported; RR, relative risk. Figure 3 describes results for CRF as a predictor of newly diagnosed chronic conditions, including: hypertension, heart failure, stroke, atrial fibrillation, dementia, chronic kidney disease, depression and type 2 diabetes. Online supplemental figure 2 describes results for all cancer (male only), lung cancer (male only), colon/rectum cancer (male only) and prostate cancer. When comparing high versus low CRF, there was a 37% (HR for incident hypertension37=0.63; 95% CI 0.56 to 0.70) to 69% (HR for incident heart failure38=0.31; 95% CI 0.19 to 0.49) reduction in the risk of incident conditions. The certainty of this evidence was rated as very low-to-low largely due to inconsistency and indirectness (ie, high heterogeneity that could not be described by subgroup analysis and largely male populations). The dose–response relationship per 1-MET higher level of CRF was associated with a 3% (HR for incident stroke39=0.97; 95% CI 0.96 to 0.98) to 18% (HR for incident heart failure38=0.82; 95% CI 0.79 to 0.84) reduction in the risk of incident conditions. The certainty of the evidence ranged from very low-to-low due to inconsistency and indirectness. Only two studies reported results for females separately. High versus low CRF was more protective for incident stroke and type 2 diabetes among females compared with males (online supplemental figure 2). Among men, there was a null association between high versus low CRF for prostate cancer (HR=1.15; 95% CI 1.00 to 1.30).40 Download figure Open in new tab Download powerpoint Figure 3 HRs for each incident outcome in apparently healthy populations at baseline for high versus low CRF and per 1-MET increase in CRF. Note: Estimates from Cheng (2022), Aune (2021), Wang (2020), Xue (2020), Tarp (2019) and Kunutsor (2023) were reported as RR, the remaining studies were reported as HR. Kandola (2019) reported estimates for low versus high which were inverted for this study. The estimates from Tarp (2019) are fully adjusted for adiposity. Aune (2021) was reported per 5-MET increase which we converted to 1-MET increase for this study. CRF, cardiorespiratory fitness; CVD, cardiovascular disease; GRADE, Grading of Recommendations, Assessment, Development and Evaluations; MET, metabolic equivalent of task; NA, not applicable; NR, not reported; RR, relative risk. Figure 4 highlights results comparing high versus low CRF among individuals living with chronic conditions. There was a 19% (HR for adverse events among those living with pulmonary hypertension41=0.81; 95% CI 0.78 to 0.85) to 73% (HR for cardiovascular mortality among those living with CVD42=0.27; 95% CI 0.16 to 0.48) reduction in the risk of all-cause and type-specific mortality. Comparing delayed versus not delayed heart rate recovery was associated with an 83% reduced risk of adverse events among those living with coronary artery disease. The certainty of the evidence for mortality in those living with a chronic condition was rated as very low-to-low largely due to risk of bias, indirectness and imprecision (ie, low-quality studies, mainly male participants and small sample sizes). No evidence examining sex differences were available. See online supplemental table 3 for a detailed summary of findings. Download figure Open in new tab Download powerpoint Figure 4 HRs for health outcomes in patients living with chronic conditions at baseline for high versus low CRF and delayed versus not delayed HRR. Estimates from Morris (2014) were reported as RR, the remaining estimates were reported as HR. Yang (2023), Fuentes-Abolafio (2020), Morris (2014), Rocha (2022) and Lachman (2018) reported estimates as low versus high which were inverted for this study. Cantone (2023) was reported per 1-unit VO2 increase which we converted to 1-MET increase for this study. Adverse events for Lachman (2018) were all-cause mortality, cardiovascular mortality and hospitalisations for congestive heart failure. CRF, cardiorespiratory fitness; CVD, cardiovascular disease; GRADE, Grading of Recommendations, Assessment, Development and Evaluations; HRR, heart rate recovery; MET, metabolic equivalent of task; NA, not applicable; NR, not reported; RR, relative risk. Discussion This overview of systematic reviews demonstrated that CRF is a strong and consistent predictor of risk across many mortality outcomes in the adult general population. Among populations living with chronic conditions such as cancer, heart failure and CVD, this study showed better prognosis for those with higher CRF. We also demonstrated that low CRF is an important risk factor for developing future chronic conditions such as hypertension, heart failure, stoke, atrial fibrillation, dementia and depression. Given that we summarised evidence from cohort studies, and randomised controlled trials cannot be used in our investigation, the results of this study may signal a causal relationship between CRF and future health outcomes. We also found a significant dose–response effect showing protection for every 1-MET higher level of CRF. This evidence further supports 1-MET as an MCID for CRF and could be considered as a target for interventions. The strength and consistency of the evidence across a wide range of outcomes supports the importance of CRF for clinical assessment and public health surveillance. Several studies have identified the need for the routine measurement of CRF in clinical and public health practice.11 43 For instance, a scientific statement from the American Heart Association concluded that healthcare providers should assess CRF during annual routine clinical visits using submaximal tests (eg, treadmill, cycling or bench stepping tests) or self-report estimates and that patients living with chronic conditions should have CRF measured regularly using a symptom-limited direct measure.11 There are several benefits to regular measurement of CRF in clinical practice. First, CRF is an important risk factor that provides additional information beyond traditional risk factors such as blood pressure, total cholesterol and smoking status.44 Second, given the strong link with habitual physical activity, CRF could be a valuable tool to help guide exercise prescription. In those with low CRF (defined based on age, sex and health status), large relative improvements can be attained through additional moderate physical activity (ie, brisk walking at a heart rate of 50% of peakO2).45 The largest health benefits have been observed when individuals move from being unfit to fit.46 Lastly, CRF measured using field-based tests are easy to implement with a variety of tests that could be adapted to suit space and time limitations. Areas of future work Applying the GRADE approach to evaluate the certainty of the evidence helped identify several important gaps in the literature. Nearly all the outcomes identified in this study were downgraded due to the evidence being generated largely from samples comprising males. Although an increase in female samples would help improve the certainty of the evidence, it likely would not impact the magnitude of the observed effects because the benefits of CRF were similar for males and females in our study (see online supplemental figures 1,2) and other large cohort studies.47 There is also a need for higher-quality studies with larger samples sizes in clinical populations, as many of the outcomes were downgraded due to primary studies with high risk of bias, low sample sizes (<4000 participants), and inconsistencies in the measurement of CRF across studies. Improving the evidence for CRF in clinical populations remains an important research gap. For instance, outcomes in clinical populations with a serious or very serious risk of bias were often rated this way due to a lack of adequate control for confounding, including a lack of adjustment for age, sex, and body mass. In addition to the need for higher-quality studies with greater samples in more diverse populations including females, we did not identify any systematic reviews that explored the association between CRF and breast cancer48 or mental health outcomes beyond incident depression and dementia, as an example. These outcomes present important areas for future work. Finally, future studies would benefit from repeated longitudinal measures of CRF to further establish causality. Implications for clinical practice This study further demonstrates the importance of including CRF measurement in regular clinical practice. For every 1-MET (3.5 mL/kg/min) higher level of CRF, we identified substantial reductions in the risk of all-cause, CVD and cancer mortality. We also identified significant reductions in the risk of incident hypertension, heart failure, stroke, atrial fibrillation and type 2 diabetes per higher MET. For most, a 1-MET higher level of CRF is attainable through a regular aerobic exercise programme. For example, in a large population-based observational study of over 90 000 participants, nearly 30% were able to increase their CRF by 1-MET (median follow-up was 6.3 years) without intervention.49 However, for some, improvements as small as 0.5-METs may substantially benefit health.50 51 Given the strength of the predictive utility of CRF across many health outcomes, CRF would be a valuable risk stratification tool in clinical practice. Furthermore, the predictive strength of CRF is maintained regardless of age, sex and race.47 Through regular CRF measurement, clinicians could better identify patients at greater risk of premature mortality, initiating the need for targeted exercise prescription. Improvements in CRF through regular physical activity results in a proportional reduction in mortality risk, regardless of the presence of other major risk factors such as higher body mass index, hypertension, type 2 diabetes, dyslipidaemia, or smoking.49 There is an important need for clinical and public health guidelines around the assessment, interpretation of results and MCID of CRF across age, sex and clinical populations. Strengths and limitations Our paper has several strengths, including a focus on pooled meta-analyses from cohort studies, assessment of the certainty of the evidence using a modified GRADE, and an evaluation of the systematic review quality using AMSTAR2. Our study identifies gaps where new evidence is needed across a broad range of health outcomes. However, this study is not without limitations. As in any overview, the quality of the data is restricted to the included papers. In our case, heterogeneity was high for many of the included meta-analyses and was often not explained by subgroup analyses. We also identified low-to-very low certainty of the evidence for most outcomes, suggesting the need for higher-quality studies in this research area including adequate adjustment for confounding and greater representation of females. The evidence was also limited to studies examining associations between a single measure of CRF and prospective health outcomes. Conclusion Our findings showed that high CRF is strongly associated with lower risk of premature mortality, incident chronic conditions (ie, hypertension, heart failure, stroke, atrial fibrillation, dementia and depression), and poor prognosis in those with existing chronic conditions. The consistency of the evidence across a variety of health outcomes demonstrates the importance of CRF and the need to incorporate this measure in routine clinical and public health practice. Future studies should focus on outcomes with limited evidence and where the certainty of the evidence was rated as very low by improving study quality. Data availability statement Data are available on reasonable request. Ethics statements Patient consent for publication Not applicable. Acknowledgments We would like to acknowledge the support of Valentine Ly, MLIS, Research Librarian at the University of Ottawa for her help with translating and conducting the search strategy in CINAHL and SPORTDiscus. We would also like to acknowledge the Health Library at Health Canada and the Public Health Agency of Canada for their support in constructing and carrying out the search strategy for MEDLINE, Embase and Scopus. The PRESS peer-review of the search strategy was carried out by Shannon Hayes, MLIS, research librarian, from the Health Library at Health Canada and the Public Health Agency of Canada. We would also like to thank Joses Robinson and Iryna Demchenko for their help with the paper. References ↵ Caspersen CJ , Powell KE , Christenson GM . Physical activity, exercise, and physical fitness: definitions and distinctions for health-related research. Public Health Rep 1985;100:126–31. OpenUrlCrossRefPubMedGoogle Scholar ↵ Balady GJ , Arena R , Sietsema K , et al . American heart Association exercise, cardiac rehabilitation and prevention committee of the Council on clinical cardiology; Council on epidemiology and prevention; Council on peripheral vascular disease; Interdisciplinary Council on quality of care and outcomes research. clinician’s guide to cardiopulmonary exercise testing in adults: a scientific statement from the American heart Association. Circulation 2010;22:191–225. doi:10.1161/CIR.0b013e3181e52e69 OpenUrlGoogle Scholar ↵ Pollock ML , Foster C , Schmidt D , et al . Comparative analysis of physiologic responses to three different maximal graded exercise test protocols in healthy women. Am Heart J 1982;103:363–73. doi:10.1016/0002-8703(82)90275-7 OpenUrlCrossRefPubMedWeb of ScienceGoogle Scholar ↵ Kaminsky LA , Whaley MH . Evaluation of a new standardized ramp protocol: the BSU/Bruce ramp protocol. J Cardiopulm Rehabil 1998;18:438–44. doi:10.1097/00008483-199811000-00006 OpenUrlCrossRefPubMedGoogle Scholar ↵ Nes BM , Vatten LJ , Nauman J , et al . A simple Nonexercise model of cardiorespiratory fitness predicts longterm mortality. Med Sci Sports Exerc 2014;46:1159–65. doi:10.1249/MSS.0000000000000219 OpenUrlCrossRefPubMedGoogle Scholar ↵ Qiu S , Cai X , Sun Z , et al . Is estimated cardiorespiratory fitness an effective Predictor for cardiovascular and all-cause mortality? A meta-analysis. Atherosclerosis 2021;330:22–8. doi:10.1016/j.atherosclerosis.2021.06.904 OpenUrlGoogle Scholar ↵ Bouchard C , Daw EW , Rice T , et al . Familial resemblance for Vo2Max in the sedentary state: the HERITAGE family study. Medicine &Amp Science in Sports &Amp Exercise 1998;30:252–8. doi:10.1097/00005768-199802000-00013 OpenUrlGoogle Scholar ↵ Stofan JR , DiPietro L , Davis D , et al . Physical activity patterns associated with cardiorespiratory fitness and reduced mortality: the aerobics center longitudinal study. Am J Public Health 1998;88:1807–13. doi:10.2105/AJPH.88.12.1807 OpenUrlCrossRefPubMedWeb of ScienceGoogle Scholar ↵ Garber CE , Blissmer B , Deschenes MR , et al . American college of sports medicine position stand. quantity and quality of exercise for developing and maintaining cardiorespiratory, musculoskeletal, and Neuromotor fitness in apparently healthy adults: guidance for prescribing exercise. Med Sci Sports Exerc 2011;43:1334–59. doi:10.1249/MSS.0b013e318213fefb OpenUrlCrossRefPubMedWeb of ScienceGoogle Scholar ↵ Sandercock G , Hurtado V , Cardoso F . Changes in cardiorespiratory fitness in cardiac rehabilitation patients: a meta-analysis. International Journal of Cardiology 2013;167:894–902. doi:10.1016/j.ijcard.2011.11.068 OpenUrlCrossRefPubMedGoogle Scholar ↵ Ross R , Blair SN , Arena R , et al . Importance of assessing cardiorespiratory fitness in clinical practice: a case for fitness as a clinical vital sign: a scientific statement from the American heart Association. Circulation 2016;134:e653–99. doi:10.1161/CIR.0000000000000461 OpenUrlAbstract/FREE Full TextGoogle Scholar ↵ Bruce RA , DeRouen TA , Hossack KF . Value of maximal exercise tests in risk assessment of primary coronary heart disease events in healthy men: five years’ experience of the Seattle heart watch study. Am J Cardiol 1980;46:371–8. doi:10.1016/0002-9149(80)90003-x OpenUrlCrossRefPubMedWeb of ScienceGoogle Scholar ↵ Cumming GR , Samm J , Borysyk L , et al . Electrocardiographic changes during exercise in asymptomatic men: 3-year follow-up. Can Med Assoc J 1975;112:578–81. OpenUrlAbstractGoogle Scholar ↵ Blair SN , Kohl HW , Paffenbarger RS , et al . Physical fitness and all-cause mortality: a prospective study of healthy men and women. JAMA 1989;262:2395–401. doi:10.1001/jama.262.17.2395 OpenUrlCrossRefPubMedWeb of ScienceGoogle Scholar ↵ Robsahm TE , Falk RS , Heir T , et al . Cardiorespiratory fitness and risk of site-specific cancers: a long-term prospective cohort study. Cancer Med 2017;6:865–73. doi:10.1002/cam4.1043 OpenUrlGoogle Scholar ↵ Sieverdes JC , Sui X , Lee D , et al . Physical activity, cardiorespiratory fitness and the incidence of type 2 diabetes in a prospective study of men. Br J Sports Med 2010;44:238–44. doi:10.1136/bjsm.2009.062117 OpenUrlAbstract/FREE Full TextGoogle Scholar ↵ Kim S , Kim JY , Lee DC , et al . Combined impact of cardiorespiratory fitness and visceral Adiposity on metabolic syndrome in overweight and obese adults in Korea. PLoS ONE 2014;9:e85742. doi:10.1371/journal.pone.0085742 Google Scholar ↵ Hooker SP , Sui X , Colabianchi N , et al . Cardiorespiratory fitness as a Predictor of fatal and nonfatal stroke in asymptomatic women and men. Stroke 2008;39:2950–7. doi:10.1161/STROKEAHA.107.495275 OpenUrlAbstract/FREE Full TextGoogle Scholar ↵ Åberg MAI , Waern M , Nyberg J , et al . Cardiovascular fitness in males at age 18 and risk of serious depression in adulthood: Swedish prospective population-based study. Br J Psychiatry 2012;201:352–9. doi:10.1192/bjp.bp.111.103416 OpenUrlAbstract/FREE Full TextGoogle Scholar ↵ Groarke JD , Payne DL , Claggett B , et al . Association of post-diagnosis cardiorespiratory fitness with cause-specific mortality in cancer. Eur Heart J Qual Care Clin Outcomes 2020;6:315–22. doi:10.1093/ehjqcco/qcaa015 OpenUrlGoogle Scholar ↵ Leeper NJ , Myers J , Zhou M , et al . Exercise capacity is the strongest Predictor of mortality in patients with peripheral arterial disease. J Vasc Surg 2013;57:728–33. doi:10.1016/j.jvs.2012.07.051 OpenUrlCrossRefPubMedGoogle Scholar ↵ Ferreira JP , Metra M , Anker SD , et al . Clinical correlates and outcome associated with changes in 6-minute walking distance in patients with heart failure: findings from the BIOSTAT-CHF study. Eur J Heart Fail 2019;21:218–26. doi:10.1002/ejhf.1380 OpenUrlGoogle Scholar ↵ Roshanravan B , Robinson-Cohen C , Patel KV , et al . Association between physical performance and all-cause mortality in CKD. J Am Soc Nephrol 2013;24:822–30. doi:10.1681/ASN.2012070702 OpenUrlAbstract/FREE Full TextGoogle Scholar ↵ Kodama S , Saito K , Tanaka S , et al . Cardiorespiratory fitness as a quantitative Predictor of all-cause mortality and cardiovascular events in healthy men and women: a meta-analysis. JAMA 2009;301:2024–35. doi:10.1001/jama.2009.681 OpenUrlCrossRefPubMedWeb of ScienceGoogle Scholar ↵ Pollock M , Fernandes RM , Becker LA , et al . Chapter V: Overviews of reviews. In: Higgins JPT , Thomas J , Chandler J , et al ., eds. Cochrane Handbook for Systematic Reviews of Interventions version 6.3 (updated February 2022). Cochrane, 2022. Available: www.training.cochrane.org/handbook Google Scholar ↵ Kho ME , Poitras VJ , Janssen I , et al . Development and application of an outcome-centric approach for conducting Overviews of reviews. Appl Physiol Nutr Metab 2020;45:S151–64. doi:10.1139/apnm-2020-0564 OpenUrlCrossRefGoogle Scholar ↵ Gates M , Gates A , Pieper D , et al . Reporting guideline for Overviews of reviews of Healthcare interventions: development of the PRIOR statement. BMJ 2022;378:e070849. doi:10.1136/bmj-2022-070849 Google Scholar ↵ Brooke BS , Schwartz TA , Pawlik TM . MOOSE reporting guidelines for meta-analyses of observational studies. JAMA Surg 2021;156:787. doi:10.1001/jamasurg.2021.0522 OpenUrlGoogle Scholar ↵ McGowan J , Sampson M , Salzwedel DM , et al . PRESS peer review of electronic search strategies: 2015 guideline statement. Journal of Clinical Epidemiology 2016;75:40–6. doi:10.1016/j.jclinepi.2016.01.021 OpenUrlCrossRefPubMedGoogle Scholar ↵ Wells G , Shea B , O’Connell D , et al . The Newcastle-Ottawa Scale (NOS) for assessing the quality of nonrandomised studies in meta-analyses, 2013. Available: https://www.ohri.ca/programs/clinical_epidemiology/oxford.asp Google Scholar ↵ Shea BJ , Reeves BC , Wells G , et al . AMSTAR 2: a critical appraisal tool for systematic reviews that include randomised or non-randomised studies of Healthcare interventions, or both. BMJ 2017;358:j4008. doi:10.1136/bmj.j4008 Google Scholar ↵ Pieper D , Antoine S-L , Mathes T , et al . Systematic review finds overlapping reviews were not mentioned in every other overview. J Clin Epidemiol 2014;67:368–75. doi:10.1016/j.jclinepi.2013.11.007 OpenUrlCrossRefPubMedGoogle Scholar ↵ Guyatt GH , Oxman AD , Vist GE , et al . GRADE: an emerging consensus on rating quality of evidence and strength of recommendations. BMJ 2008;336:924–6. doi:10.1136/bmj.39489.470347.AD OpenUrlFREE Full TextGoogle Scholar ↵ Iorio A , Spencer FA , Falavigna M , et al . Use of GRADE for assessment of evidence about prognosis: rating confidence in estimates of event rates in broad categories of patients. BMJ 2015;350:h870. doi:10.1136/bmj.h870 Google Scholar ↵ Han M , Qie R , Shi X , et al . Cardiorespiratory fitness and mortality from all causes, cardiovascular disease and cancer: dose-response meta-analysis of cohort studies. Br J Sports Med 2022;56:733–9. doi:10.1136/bjsports-2021-104876 OpenUrlAbstract/FREE Full TextGoogle Scholar ↵ Aune D , Schlesinger S , Hamer M , et al . Physical activity and the risk of sudden cardiac death: a systematic review and meta-analysis of prospective studies. BMC Cardiovasc Disord 2020;20:318. doi:10.1186/s12872-020-01531-z Google Scholar ↵ Cheng C , Zhang D , Chen S , et al . The Association of cardiorespiratory fitness and the risk of hypertension: a systematic review and dose-response meta-analysis. J Hum Hypertens 2022;36:744–52. doi:10.1038/s41371-021-00567-8 OpenUrlGoogle Scholar ↵ Aune D , Schlesinger S , Leitzmann MF , et al . Physical activity and the risk of heart failure: a systematic review and dose-response meta-analysis of prospective studies. Eur J Epidemiol 2021;36:367–81. doi:10.1007/s10654-020-00693-6 OpenUrlCrossRefGoogle Scholar ↵ Wang Y , Li F , Cheng Y , et al . Cardiorespiratory fitness as a quantitative Predictor of the risk of stroke: a dose-response meta-analysis. J Neurol 2020;267:491–501. doi:10.1007/s00415-019-09612-6 OpenUrlGoogle Scholar ↵ Pozuelo-Carrascosa DP , Alvarez-Bueno C , Cavero-Redondo I , et al . Cardiorespiratory fitness and site-specific risk of cancer in men: A systematic review and meta-analysis. Eur J Cancer 2019;113:58–68. doi:10.1016/j.ejca.2019.03.008 OpenUrlCrossRefGoogle Scholar ↵ Barbagelata L , Masson W , Bluro I , et al . Prognostic role of cardiopulmonary exercise testing in pulmonary hypertension: a systematic review and meta-analysis. Adv Respir Med 2022;90:109–17. doi:10.5603/ARM.a2022.0030 OpenUrlGoogle Scholar ↵ Ezzatvar Y , Izquierdo M , Núñez J , et al . Cardiorespiratory fitness measured with cardiopulmonary exercise testing and mortality in patients with cardiovascular disease: A systematic review and meta-analysis. Journal of Sport and Health Science 2021;10:609–19. doi:10.1016/j.jshs.2021.06.004 OpenUrlGoogle Scholar ↵ Myers J , Kokkinos P , Arena R , et al . The impact of moving more, physical activity, and cardiorespiratory fitness: why we should strive to measure and improve fitness. Prog Cardiovasc Dis 2021;64:77–82. doi:10.1016/j.pcad.2020.11.003 OpenUrlGoogle Scholar ↵ Gander JC , Sui X , Hébert JR , et al . Association of cardiorespiratory fitness with coronary heart disease in asymptomatic men. Mayo Clinic Proceedings 2015;90:1372–9. doi:10.1016/j.mayocp.2015.07.017 OpenUrlCrossRefPubMedGoogle Scholar ↵ Church TS , Earnest CP , Skinner JS , et al . Effects of different doses of physical activity on cardiorespiratory fitness among sedentary, overweight or obese postmenopausal women with elevated blood pressure: a randomized controlled trial. JAMA 2007;297:2081–91. doi:10.1001/jama.297.19.2081 OpenUrlCrossRefPubMedWeb of ScienceGoogle Scholar ↵ Blair SN , Kohl HW , Barlow CE , et al . Changes in physical fitness and all-cause mortality. A prospective study of healthy and unhealthy men. JAMA 1995;273:1093–8. OpenUrlCrossRefPubMedWeb of ScienceGoogle Scholar ↵ Kokkinos P , Faselis C , Samuel IBH , et al . Cardiorespiratory fitness and mortality risk across the spectra of age. J Am Coll Cardiol 2022;80:598–609. doi:10.1016/j.jacc.2022.05.031 OpenUrlCrossRefPubMedGoogle Scholar ↵ Katsaroli I , Sidossis L , Katsagoni C , et al . The association between cardiorespiratory fitness and the risk of breast cancer in women. Med Sci Sports Exerc 2024. doi:10.1249/MSS.0000000000003385 Google Scholar ↵ Kokkinos P , Faselis C , Samuel IBH , et al . Changes in cardiorespiratory fitness and survival in patients with or without cardiovascular disease. J Am Coll Cardiol 2023;81:1137–47. doi:10.1016/j.jacc.2023.01.027 OpenUrlGoogle Scholar ↵ Bonafiglia JT , Preobrazenski N , Islam H , et al . Exploring differences in cardiorespiratory fitness response rates across varying doses of exercise training: a retrospective analysis of eight randomized controlled trials. Sports Med 2021;51:1785–97. doi:10.1007/s40279-021-01442-9 OpenUrlGoogle Scholar ↵ Grace SL , Poirier P , Norris CM , et al . Pan-Canadian development of cardiac rehabilitation and secondary prevention quality indicators. Can J Cardiol 2014;30:945–8. doi:10.1016/j.cjca.2014.04.003 OpenUrlCrossRefPubMedGoogle Scholar Barry VW , Baruth M , Beets MW , et al . Fitness vs. fatness on all-cause mortality: a meta-analysis. Prog Cardiovasc Dis 2014;56:382–90. doi:10.1016/j.pcad.2013.09.002 OpenUrlCrossRefPubMedGoogle Scholar Barry VW , Caputo JL , Kang M . The joint Association of fitness and fatness on cardiovascular disease mortality: A meta-analysis. Prog Cardiovasc Dis 2018;61:136–41. doi:10.1016/j.pcad.2018.07.004 OpenUrlGoogle Scholar Laukkanen JA , Isiozor NM , Kunutsor SK . Objectively assessed cardiorespiratory fitness and all-cause mortality risk: an updated meta-analysis of 37 cohort studies involving 2,258,029 participants. Mayo Clin Proc 2022;97:1054–73. doi:10.1016/j.mayocp.2022.02.029 OpenUrlCrossRefPubMedGoogle Scholar Lee J . Cardiorespiratory fitness physical activity, walking speed, lack of participation in leisure activities, and lung cancer mortality: A systematic review and meta-analysis of prospective cohort studies. Cancer Nurs 2021;44:453–64. doi:10.1097/NCC.0000000000000847 OpenUrlCrossRefGoogle Scholar Kandola A , Ashdown-Franks G , Stubbs B , et al . The association between cardiorespiratory fitness and the incidence of common mental health disorders: A systematic review and meta-analysis. J Affect Disord 2019;257:748–57. doi:10.1016/j.jad.2019.07.088 OpenUrlPubMedGoogle Scholar Kunutsor SK , Isiozor NM , Myers J , et al . Baseline and usual cardiorespiratory fitness and the risk of chronic kidney disease: A prospective study and meta-analysis of published observational cohort studies. GeroScience 2023;45:1761–74. doi:10.1007/s11357-023-00727-3 OpenUrlGoogle Scholar Lee J . Influence of cardiorespiratory fitness on risk of dementia and dementia mortality: A systematic review and meta-analysis of prospective cohort studies. J Aging Phys Act 2021;29:878–85. doi:10.1123/japa.2019-0493 OpenUrlGoogle Scholar Tarp J , Støle AP , Blond K , et al . Cardiorespiratory fitness, muscular strength and risk of type 2 diabetes: a systematic review and meta-analysis. Diabetologia 2019;62:1129–42. doi:10.1007/s00125-019-4867-4 OpenUrlCrossRefPubMedGoogle Scholar Xue Z , Zhou Y , Wu C , et al . Dose-response relationship of cardiorespiratory fitness with incident atrial fibrillation. Heart Fail Rev 2020;25:419–25. doi:10.1007/s10741-019-09871-5 OpenUrlGoogle Scholar Cantone A , Serenelli M , Sanguettoli F , et al . Cardiopulmonary exercise testing predicts prognosis in Amyloid cardiomyopathy: a systematic review and meta-analysis. ESC Heart Fail 2023;10:2740–4. doi:10.1002/ehf2.14406 OpenUrlGoogle Scholar Ezzatvar Y , Ramírez-Vélez R , Sáez de Asteasu ML , et al . Cardiorespiratory fitness and all-cause mortality in adults diagnosed with cancer systematic review and meta-analysis. Scand J Med Sci Sports 2021;31:1745–52. doi:10.1111/sms.13980 OpenUrlGoogle Scholar Fuentes-Abolafio IJ , Stubbs B , Pérez-Belmonte LM , et al . Physical functional performance and prognosis in patients with heart failure: a systematic review and meta-analysis. BMC Cardiovasc Disord 2020;20:512. doi:10.1186/s12872-020-01725-5 Google Scholar Lachman S , Terbraak MS , Limpens J , et al . The Prognostic value of heart rate recovery in patients with coronary artery disease: A systematic review and meta-analysis. Am Heart J 2018;199:163–9. doi:10.1016/j.ahj.2018.02.008 OpenUrlGoogle Scholar Morris DR , Rodriguez AJ , Moxon JV , et al . Association of lower extremity performance with cardiovascular and all-cause mortality in patients with peripheral artery disease: a systematic review and meta-analysis. J Am Heart Assoc 2014;3:e001105. doi:10.1161/JAHA.114.001105 Google Scholar Rocha V , Paixão C , Marques A . Physical activity, exercise capacity and mortality risk in people with interstitial lung disease: A systematic review and meta-analysis. J Sci Med Sport 2022;25:903–10. doi:10.1016/j.jsams.2022.10.002 OpenUrlGoogle Scholar Yang L , He Y , Li X . Physical function and all-cause mortality in patients with chronic kidney disease and end-stage renal disease: a systematic review and meta-analysis. Int Urol Nephrol 2023;55:1219–28. doi:10.1007/s11255-022-03397-w OpenUrlGoogle Scholar Supplementary materials Supplementary Data This web only file has been produced by the BMJ Publishing Group from an electronic file supplied by the author(s) and has not been edited for content. Data supplement 1 Footnotes JJL and SAP are joint first authors. X @JustinJLang, @SPrinceWare, @bensinghphd Contributors JJL, GRT and SAP conceptualised and planned the study design. JJL and SAP led the study. JJL accepts full responsibility for the work and/or the conduct of the study, had access to the data, and controlled the decision to publish. All coauthors contributed to article screening. JJL and SAP wrote the first draft of the article. All coauthors reviewed, revised and approved the final manuscript. Funding The authors have not declared a specific grant for this research from any funding agency in the public, commercial or not-for-profit sectors. Dr. Ortega is supported by the Grant PID2020-120249RB-I00 funded by MCIEI/10.13039/501100011033 and by the Andalusian Government (Junta de Andalucía, Plan Andaluz de Investigación, ref. P20_00124). Dr. Cadenas-Sanchez is supported by a grant from the European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska Curie grant agreement No 101028929. Dr. Fraser is supported by a National Heart Foundation of Australia Postdoctoral Fellowship (106588). Disclaimer The content and views expressed in this articles are those of the authors and do not necessarily reflect those of the Government of Canada. Competing interests None declared. Provenance and peer review Not commissioned; externally peer reviewed. Supplemental material This content has been supplied by the author(s). It has not been vetted by BMJ Publishing Group Limited (BMJ) and may not have been peer-reviewed. Any opinions or recommendations discussed are solely those of the author(s) and are not endorsed by BMJ. BMJ disclaims all liability and responsibility arising from any reliance placed on the content. Where the content includes any translated material, BMJ does not warrant the accuracy and reliability of the translations (including but not limited to local regulations, clinical guidelines, terminology, drug names and drug dosages), and is not responsible for any error and/or omissions arising from translation and adaptation or otherwise. Read the full text or download the PDF: Subscribe Log in Log in via Institution Log in via OpenAthens Log in via your Society Select your society ACMSE ACPSEM ACSEP AMSSM BASEM BASRAT CASEM CLINICAL EDGE FSEM GISSPORT IOC OSCA SASMA SGMS SMA SPC SPNZ SSMS SSPA SST SUFT VSG Log in using your username and password For personal accounts OR managers of institutional accounts Username * Password * Forgot your log in details?Register a new account? Forgot your user name or password? Content Latest content Current issue Archive Browse by collection BJSM e-edition: Female athlete health Top Cited Articles Most read articles BJSM education Responses Blog Podcasts Journal Meet the new editors About Editorial board Sign up for email alerts Subscribe Thank you to our reviewers Authors Instructions for authors Submit an article Editorial policies Resources Open Access at BMJ BMJ Author Hub Help Contact us Reprints Permissions Advertising Feedback form RSS Twitter Facebook Blog SoundCloud YouTube Website Terms & Conditions Privacy & Cookies Contact BMJ Cookie settings Online ISSN: 1473-0480Print ISSN: 0306-3674 Copyright © 2024 BMJ Publishing Group Ltd & British Association of Sport and Exercise Medicine. All rights reserved. Cookies and privacy We and our 60 partners store and/or access information on a device, such as unique IDs in cookies to process personal data. You may accept or manage your choices by clicking below, including your right to object where legitimate interest is used, or at any time in the privacy policy page. These choices will be signaled to our partners and will not affect browsing data.Cookie policy We and our partners process data to provide: Use precise geolocation data. Actively scan device characteristics for identification. Store and/or access information on a device. Personalised advertising and content, advertising and content measurement, audience research and services development. List of Partners (vendors) I Accept Reject All Manage preferences",
    "commentLink": "https://news.ycombinator.com/item?id=40243238",
    "commentBody": "Cardio fitness is a strong, consistent predictor of morbidity and mortality (bmj.com)328 points by wjb3 8 hours agohidepastfavorite357 comments keybored 3 hours agoSome of my peers are deep into running. I don’t get it. Running is sometimes fun for me but most often painful. Then I overheard one of them (the fittest) say to a budding runner that he [should] do mostly easy sessions. Okay what’s easy to him? He said that so slow that it can feel awkward and unnatural. What? Then I searched around and found out about Zone 2 and how you should do most of your work in that zone when building aerobic fitness. And that it is characterized by being able to hold a conversation, although strained. I searched around and found atheletes like amateur ultrarunners say the same thing. Then it hit me. I’ve probably been jogging a lot in Zone 3. Or higher? Because the harder you go the more benefit, right? That seems to be the basic logic for everything.[1] Relatively short, painful sessions. Have I been conditioning myself to associate cardio with more pain than is necessary for the average session? So maybe I should just go on the stationary bike today, do a “conversatitional” (talk to myself) pace and listen to my audiobook for an hour? And try to not let my groin fall asleep. [1] With nuances like go-to-failure for hypertrophy in weightlifting and more back-off-a-little for strength training. reply danielbln 1 hour agoparentI've been streak running (so running at least a mile per day with no exceptions, mostly 5ks during the week, 10-20k on weekends) for almost 8 years (2817 days). The single best tip I give to everyone is \"slow down! run slow!\" Of course, almost no-one adheres to it unless they are already well practiced. It's just deeply ingrained in peoples heads that \"only if it's hard or painful it must work\". Then people check out of running because it feels like crap, which it does if you always push too hard. In my opinion, three of the most important rules are: - slow down! does it feel \"too slow\"? Great, that's the right speed for most runs - small steps! feels awkward at first, but is sooo much more efficient and soo much better for your joints - land on mid- or forefoot (that happens mostly automatically with small steps) reply bpizzi 1 hour agoparentprevThere's also value in spending some time in zone 5 [1]: this is where the heart is really trained as a muscle, and where the cardiovascular system is pushed to its limit (the famous vo2max: increasing vo2max is done in zone 5, for ex. with HIIT [2]). Zone 2 is all about giving the mitochondries a chance to get better at providing a steady energy flow over a long time, mainly by optimizing for burning fat as fuel instead of glucose, avoiding lactate accumulation during the process [3]. In between, in zones 3 & 4, you get a little of both those ends of the spectrum, it's still helpful to a degree, but it's not really optimized: that why it's deemed preferable to spend the bulk of your training time in either your zone 2 or zone 5. The ideal composition of a training period seems like 90% zone 2 and 10% zone 5, and going for more than 1h of zone 5 per week seems not that interesting. Also, mixing zone 2 and zone 5 in the same training session is not ideal, it's better to stay focused on one thing at at time. [1] https://peterattiamd.com/category/exercise/high-intensity-zo... [2] https://peterattiamd.com/category/exercise/vo2-max/ [3] https://peterattiamd.com/category/exercise/aerobic-zone-2-tr... reply apatheticonion 1 hour agorootparentI got a indoor bicycle trainer (with power readings) and do intervals in Zone 5. I created a custom \"track\" on TrainerDay and spend about 20 minutes 2-3 times a week doing this. It feels like dying - but I like being able to extract the most value out of the lowest time investment reply lostlogin 54 minutes agorootparent> I like being able to extract the most value out of the lowest time investment Biking to work using Strava is like this for me. It gamifies exercise in a very addictive manner. Getting a PR, being quickest on a segment or becoming a local legend. There is always someone or something to beat. reply bpizzi 1 hour agorootparentprevYeah, I'm in Team Rowing (indoor) myself, and 30min of HIIT every week is HELL. My next session is in 1h... reply wjnc 2 hours agoparentprevA part of become a good runner is letting go of expectations. There is a lot of quasi meditative written on the topic, but even if that’s not your thing: even if you walk/jog/walk to retain a constant, middle to low heart rate you could easily have the same effort as a trained runner doing some great mileage. The humbling part is part of the experience. Stay away from anything with speed for the first thousand miles. It will only lead to injuries. The good thing is, both condition and speed will improve very fast and the happy run feeling isn’t based on speed or distance. It’s being outside, alone, in silence with sounds and weather while doing something actively (but not overly). Did I mention I miss running? (Alas I am now missing some necessary cartilage.) reply jajko 44 minutes agorootparentI never understood why people are chasing some stupid numbers, constantly comparing to each other, and then are unhappy or push themselves into injury. Such an unhealthy mindset for life overall. That's really the opposite of what sports are supposed to be for 99.9% of the people. Who cares how life and training of some fastest guy in XYZ category looks like? Go at your pace, progress slowly also at your pace, also good strategy for life overall. You also don't take tips for driving in traffic from top formula 1 driver, do you, sorta worse than useless. I do naturally push myself a bit when I run alone, to the point when even say 5-6km run makes me tired and drenched in sweat, but certainly not first 10-20 runs after some lull (like now broken foot, starting gently next week). And running is really just training for other more adrenaline sports for me, not chasing kms and preparing for races. But never pushing beyond whats OK for my body, and with experience one knows oneself very well in this regard. reply aqme28 1 hour agoparentprevIf you don’t like running, find an exercise you do enjoy. Running isn’t the only cardio out there. If you’re jumping through hoops to force yourself to enjoy it, try a few other activities. Me, I can only get myself to run if I’m engaged in sports. reply bo1024 2 hours agoparentprevYes, most of experienced runners' running is at a conversational pace, often literally (this is why running groups are popular). To explain why harder isn't better: improvement in running comes only a little from muscular strength, so that mental model is bad. Improvement is primarily based on aerobic development of the cardiovascular system. This is your body's ability to burn mostly-fat with oxygen, deliver the energy (ATP molecules) to muscles, and remove waste products. So you develop more capillaries, your heart gets literally stronger, and this fat-burning pathway gets more efficient. Going too hard pushes into the anaerobic zone (burning sugar without oxygen). This is good training in some ways, but it can be counterproductive to aerobic development and, most important, it's too hard to do very much volume so the amount of stimulus is limited. By keeping it easy, you can do a higher volume of purely aerobic stimulus. reply djtango 1 hour agorootparentYes.... I have been exclusively an anaerobic athlete for the past two years and my aerobic fitness has dropped to a shockingly low level. But I am oddly comfortable at 180-190bpm I am slowly building in more aerobic cardio into my training but man cardio is just such a chore for me. It's always the first to go so it takes constant maintenance and it's time consuming... reply Dove 3 hours agoparentprevI always hated running, but I had always been doing it for exercise. Perversely, this meant if I moved inefficiently and made the movement harder, I was doing better. I characterized running as \"competitive suffering\". No pain no gain, right? And like you, I always did more than felt good, and figured that meant I was doing it right. But I don't think that way now. I want to enjoy moving. I want to move playfully and powerfully and efficiently. Exercise until it feels good, not until it feels bad. Love the body you have and take care of it and build it up, rather than hate-exercising to change it. reply terhechte 3 hours agoparentprevI watch all streaming shows on a rowing machine with not-so-high speed and not-so-high resistance. The show keeps me entertained and sometimes I even forget that I'm rowing. Previously I did that with an iPad 12.9\" next to the machine, but I've just started watching shows on the Quest 3 and that's way better. reply oezi 2 hours agorootparentUff that must get hot under the Quest. reply gadilif 1 hour agorootparentI didn't try exercising with it, but, Quest 2 was getting too hot for me even after a short while playing Beat Saber for example. Quest 3 is much better in that aspect. I can actually keep it on for long durations, and heat isn't a problem at all. reply Aperocky 2 hours agoparentprevThis is absolutely true, when I started, I didn’t have a zone2 while running and running is just suffering. But gradually I could hold a lower heart rate while running and now my zone 2 runs are actually faster than my fastest pace I could hold a year ago. Now I enjoy running more than cycling (though my fitness are still probably mostly derived by cycling). reply echamussy 2 hours agoparentprevThis has been the biggest game changer for me. People often think they need to push it hard and they quickly fatigue, burn out and hate going running. The trick is to do a 80% of your running sessions in zone 2 and 20% in high intensity to increase VO2 max. If you are at the start that means sometimes walking on hills and that is ok. There is a great book about this: https://www.amazon.com/80-20-Running-Stronger-Training/dp/04... reply samuell 2 hours agoparentprevI love running and doing it consistently over ~10 years, always longing to get out again. I'm the only one among my friends that do this, and I find this to be highly correlated with my running style: Really comfortable pace 7-10km through local forests, only (I tend to do more 10km more than 7km as I like it so much). I learned this one time when I ran on a threadmill and realized the enormous difference between a slight variation in speed (from 11 to 9 km/h in my case, but the exact numbers don't matter), where I realized with this slight change I could go on indefinitely instead of being quickly exhausted. reply fransje26 36 minutes agoparentprev> Then it hit me. I’ve probably been jogging a lot in Zone 3. Or higher? Because the harder you go the more benefit, right? That's unfortunately not fully how it works. For maximum benefits, you would want to train your aerobic energy system, and to do that without risking injury, zone 2 is the recommended zone. The claim is that pain is gain, but more often than not, pain is your body telling you you are over the limit. By the description of your running experience, you were possibly closer to zone 4. Of course, the body gets lazy, so you will want to vary your training. Repeat the same type of load, and your body will get efficient operating at that load, meaning your fitness gains will eventually stagnate. That's where the other zones come in. At the zone 3/4 threshold you start using your anaerobic energy system more, with a stronger build-up of lactic acid as a by-product. Training in that range means your body will get more efficient at faster-paced work. Of course, as you build-up lactic acid, it also means that it is not a pace you can sustain. But if you do short bursts of exercise at the lower-end of the anaerobic zone, you are actually contributing to making you aerobic energy system more efficient, without over-straining your body too much. And it's exactly the type of exercise variation your body needs to not get into the \"lazy\" mode. The traditional way of approaching that is to have a base of zone 2 days (say 80% of your runs), and a few days of zone 3/low 4 days (say 20% of your runs), so that you alternate your loads. Another approach with similar benefits, especially if you train less often, is the Fartlek approach, developed in Sweden in the 1930s. That's a type of training where you mix-up the load intensity during your runs, in about the same proportions. Fartlek means \"speed-play\", and it describes the method well. It's generally about approaching your training as you feel it that day. At some point during your run you might decided to increase the pace for a short time. Or you see a small hill that you can decide to attack at a faster pace, thereby changing your training zone. It's all about having fun and making your trainings less boring. There was this belief in some circles until not that long ago that if you would enter a zone 3 or zone 4 training load during a zone 2 training day, you would mostly destroy the benefits of a \"pure\" zone 2 training. (Or the other way around, having some zone 2 stretches on zone 3/4 days). That turned out to be nonsense. All in all, for good progress, and against boredom, Fartlek is the way to go, all the while keeping the bulk of your training in zone 2. reply TomK32 2 hours agoparentprevI'm a cyclist, 100km without much effort, but a year ago I started running and it was super hard. The muscles are used differently, the body get impacted with every step and it takes some learning to find the right speed to reach whatever your target is. Pain kills the fun, the only way to succeed is to start slow and on short distances. My goal was strengthening hip (one side has a three plates and a few screws in it) and knee mobility, with losing weight being an extra that I only reach this spring due to diet changes. I did find my Zone 2, where I can chat with a running buddy, and slowly extended the distances I run: Now 5km at 5:40m/km are the norm and 10km is just as possible. Injury-free so far and no ambition for a Marathon. The thing I now have to work at is the cadence which I (at 193cm) had trouble with on the bike as well, but shorter strides at a higher cadence are important to prevent knee injuries. Whatever you do, I think sticking to it at least three times a week so you look forward for the next run, feel well during it and keep healthy. This is more important than beating records or running insane distances. Those goals are for those with the right genetics. reply ktosobcy 3 hours agoparentprevI do love running and indeed it's better to jog slowly for longer time... though, to avoid injuries - please do exercises before and after because you can easily injure your (lower-)back and knees (talking from experience, chronic lower-back pain after ~20y of running…) reply ulnarkressty 17 minutes agorootparentThere are no exercises that restore joints which are degenerating. Most likely you would have gotten 5-10 more pain-free years without the running, but it would have come anyway at some point. The increased cardio health and bone density does help sweeten the terrible deal though. reply cameldrv 3 hours agoparentprevI had a very similar experience years ago, I tried to start running and I hated it. I went and got a heart rate monitor and found that after I was going for a while where I started to hate it, my heart was up at 180 or so. I started doing alternating run/walks so that I was averaging more like 145, and it was actually enjoyable and I'd get in the zone sometimes. After a while I managed to get up to half marathon distance. I'm a little skeptical about the zone 2 craze that's hit recently, but definitely, if you're just starting out, it's very easy to try to run like the other people you see on the trail and exhaust yourself. A heart rate monitor is very useful to help keep yourself in a reasonable range that's actually enjoyable and sustainable. reply SwiftyBug 3 hours agoparentprevI too hate to run. And I do it everyday. To me it's the time I get to be alone with my podcasts for an hour. Now I look forward to it every morning. I'm not sure in what zone I usually run, but my pace is usually around 6min/Km. reply TomK32 2 hours agorootparent6min/km is an average pace, so I do wonder why you still hate it? Have you looked into your cadence? Are you steps too long or the shoes too hard? Or do you have to run through post-apocalyptic ruins? reply 01100011 3 hours agoparentprevAre you carrying excess weight? I've been overweight most of my life. For a while in my 40s I hiked a lot and my BMI dropped into the normal range. When that happened, I became a runner. It felt amazing. I would run 5+ miles and felt like I could keep going forever. Once I put the weight back on(new relationship with someone who liked to eat), I couldn't run. It hurt and didn't feel good. I don't know what the current research says, but I believe jogging is pretty hard on your joints vs running. I'd recommend hiking and fast walking as an alternative if you're looking to transition into running. reply beezlebroxxxxxx 1 hour agorootparentUnless you have actual mechanical problems with your joints or you're, as you said, heavy or out of normal BMI, then running is actually healthy for your joints. One caveat is the surface you run on. Running on softer surfaces than asphalt can be easier on your legs and feet. Most injuries from running come from repetitive motion overuse and are muscular. People go out way too hard then discover that their muscles are weak for running (especially hips and glutes). Easing into running and doing some weightlifting or bodyweight exercises to supplement can usually keep you injury free and running well into old age. Some research shows it actually reducing the onset of osteoarthritis. reply Refusing23 1 hour agoparentprevI've heard about this zone 2 as well, it makes sense. its a decent stable pace. when i've been running i do similar. though i also do accellerate a bit and slow down, because its... more fun. reply gjjydfhgd 3 hours agoparentprevNassim Taleb wrote in recent months about Zone 2, 3 and relevant statistical analysis. reply Ylpertnodi 1 hour agoparentprevZone 2 works on a bicycle, too. reply portaouflop 3 hours agoparentprevAll things need balance- don’t go too slow, don’t go too fast. If it feels right you know reply anal_reactor 2 hours agoparentprevWhen I was a kid I loved long-distance walking because it was \"me-time\". I have some introvert genes and I really needed to be on my own, away from people, and going for a walk was the only way I could achieve that. Then I moved to a bike-friendly country and I started biking all the time. 30 minutes commute one way, 30 minutes back. I live right next to subway but god I hate public transport, and owning a car in this city is a massive headache. Often I go for a small one-hour ride in the evening. I put on some music, start cycling, and thinking about things. I feel like I get into some kind of trance. When sitting at home I constantly feel \"now what now what now what now what\", but while cycling, I get into a rhythm and often; not always, but often; I get into a state where my thoughts kinda flow more easily, I can just think about something happy and focus on it, or maybe listen to the music. I live in a low-density area with biking lanes separated from most car traffic, so sometimes I get drunk or high and go cycling, which feels amazing - I have to focus and make it through one intersection, and after that the worst thing that can possibly happen is falling off the bike and getting a few bruises. Sometimes I go for longer whole-day trips. When I reach the point of complete exhaustion the part of my brain that constantly worries about getting fired or possible WW3 shuts down and it's like being high but without drugs. I plan these trips so that I fight against the wind on my way \"there\" and then enjoy the help of the wind on my way back which becomes almost efforless. When I come home exhausted, the body experiences relaxation in ways impossible to achieve in other ways. If you want advice, from my experience: 1. Stationary machines are a scam, 90% of pleasure comes from being outside. 2. Do exercise at a pace \"I could keep on going like this forever\" and then after an hour \"oh not anymore\" 3. Get something that keeps you entertained but doesn't require your attention. I don't recommend audiobooks because you need to keep actively listening. I recommend music or radio because if you zone out and stop listening, nothing bad happens. And sometimes try not having anything at all, just watch all the things around you. reply cycomanic 5 hours agoprevWeird that people seem to assert that causality is not established? There are hundreds to thousands of studies that show that pretty much all markers for mortality and morbidity go down with excercise, i.e. put someone regulalrly on a treadmill for a couple of month and pretty much all health markers improve. People implying that the study shows only correlation really don't seem to understand how we establish causality in science. reply prerok 4 hours agoparentWell, I don't doubt that cardio fitness decreases the chance for morbidity and mortality, but in this study they only established correlation. I think that in science we have to be extra careful just what we are saying. And saying that this study proves causality would be wrong. I am sure there are other studies that could (do?) prove it but this statistical analysis can only prove correlation. reply bigfudge 3 hours agorootparentWe only have correlation for lots of important results. Smoking cigarettes is one example. Experiments are almost always better, and a lot of correlational research is terrible. But knowing something for sure doesn’t always require an experiment. reply tea-coffee 2 hours agorootparentprevFrom the Methods section of the paper (under \"study design\"): \"We also included meta-analyses that pooled data from primary prospective/retrospective cohort or case-control studies. These studies were the focus because of their ability to assess causality for observational research\". While \"causality\" is a strong word in the sentence above, the data is much stronger than simple correlation. Of course, outright causality has not been established, but the evidence to determine a predictive association is strong. reply bjornsing 2 hours agoparentprev> People implying that the study shows only correlation really don't seem to understand how we establish causality in science. Or perhaps you don’t fully understand the challenges of establishing causality? Just because an intervention causes an improvement in some bio markers that are associated with lower mortality (unfortunately) does not mean that the intervention will cause lower mortality. The classic example is vitamin D supplements: Higher vitamin D levels are associated with lower mortality in many medical conditions. Vitamin D supplements increase vitamin D levels. But vitamin D supplements seldom lower mortality. Why? Probably because vitamin D is produced in the skin when we are in the sun. The more healthy subpopulation of any study will typically spend more time outside, so they will have higher vitamin D levels. But it’s (relative) health that causes higher vitamin D, not the other way around. The only way to reliably establish causality is really an end-to-end randomized controlled trial (RCT). Stitching together two RCTs is not sufficient. (Not saying that exercise does not lower mortality BTW, just that it’s complicated, and a study such as this is probably picking up two signals: one causal and one purely correlational.) reply cycomanic 1 hour agorootparent> > People implying that the study shows only correlation really don't seem to understand how we establish causality in science. > Or perhaps you don’t fully understand the challenges of establishing causality? Just because an intervention causes an improvement in some bio markers that are associated with lower mortality (unfortunately) does not mean that the intervention will cause lower mortality. > The classic example is vitamin D supplements: Higher vitamin D levels are associated with lower mortality in many medical conditions. Vitamin D supplements increase vitamin D levels. But vitamin D supplements seldom lower mortality. That is quite a simplification. The research as far as I know is that too low Vitamin D levels are associated with higher mortality, but that supplementation above a certain level is meaningless (and most caucasians can get those levels through normal sun exposure). So yes vitamin D supplements in general don't improve mortality. Note also the situation here is completely different. The link between mortality and morbidity markers and exercise have been established in other studies. The study here establishes that this directly translates to a correlation between fitness and mortality. So in a way it's the opposite of the vitamin D case. > Why? Probably because vitamin D is produced in the skin when we are in the sun. The more healthy subpopulation of any study will typically spend more time outside, so they will have higher vitamin D levels. But it’s (relative) health that causes higher vitamin D, not the other way around. You realise that you are proclaiming causality here? > The only way to reliably establish causality is really an end-to-end randomized controlled trial (RCT). Stitching together two RCTs is not sufficient. A RCT does not establish causality. That's essentially my point. A single study/experiment never proofs causality. You need a theory to explain the causality and multiple studies that falsify other possible causality mechanisms. That has been done extensively for excercise and morbity/mortality, the current study just establishes that this also correlates in the bigger picture. So yes the study itself does not \"proof\" causality, it's just a piece in the bigger puzzle of causality. > (Not saying that exercise does not lower mortality BTW, just that it’s complicated, and a study such as this is probably picking up two signals: one causal and one purely correlational.) reply bjornsing 1 hour agorootparent> > Why? Probably because vitamin D is produced in the skin when we are in the sun. The more healthy subpopulation of any study will typically spend more time outside, so they will have higher vitamin D levels. But it’s (relative) health that causes higher vitamin D, not the other way around. > You realise that you are proclaiming causality here? Yes, with emphasis on the word probably. Just like I will happily proclaim that exercise probably lowers mortality. > A RCT does not establish causality. That's essentially my point. A single study/experiment never proofs causality. You need a theory to explain the causality and multiple studies that falsify other possible causality mechanisms. That has been done extensively for excercise and morbity/mortality, the current study just establishes that this also correlates in the bigger picture. So yes the study itself does not \"proof\" causality, it's just a piece in the bigger puzzle of causality. The beauty of an end-to-end RCT is that it effectively neutralizes other possible causality mechanisms. You do not seem to appreciate this. My impression is that your reasoning is more in line with evidence based medicine (EBM), rather than the hypothetico-deductive method that I personally subscribe to. In my way of thinking there will never be definitive “proof” of causality, but I will happily take a drug that has gone through sufficiently powerful RCTs that failed to prove its ineffectiveness (and harm). reply Fricken 4 hours agoparentprevExercise is a four letter word for many on HN. Content posted here touting the various benefits of exercise is always met with mental gymnastics from commenters who would rather do anything but go out and break a sweat. reply portaouflop 3 hours agorootparentI used to be like that- for some reason I started exercising this year and it completely changed my life. I feel better than ever and it’s not like I’m training a lot, a couple of times per week. If you’re on the fence and reading this definitely give it another shot reply huijzer 4 hours agoparentprevNo sorry but this is not how causality works. When event B happens a thousand times after event A, you cannot conclude that A causes B. For example, there could be a confounder, that is, a third event that triggers A and then B (so not A triggers B, but this third event triggers B). See The Book of Why by Pearl for more information on causality. reply knallfrosch 2 hours agorootparentCan you give an example where causality is established then? It sounds like I can hit you a 1,000 times, you feel pain a 1,000 times and you still don't believe there's a causality. reply gwd 1 hour agorootparentConsider this quote: \"Every time we send 5 fire trucks to a fire, the damage is 10x than when we only send one fire truck. We've observed this 1000 times. And still you don't believe that the fire trucks are causing the damage.\" In this case it should be absolutely clear that A (lots of trucks) aren't causing B (lots of damage), but rather a third aspect, C (size of fire) is causing both A and B. Insisting that A causes B will result in completely counterproductive interventions, like \"send only one truck to all fires\". The same thing could be true for cardiovascular fitness. If people are sick, they're much less likely to running or hiking up a mountain. So rather than poor cardio fitness (A) causing high mortality (B), it could be that a third thing, sickness (C) is causing both A and B. If that is the case, then shaming people who are sick into trying to exercise, instead of making them healthy enough so that they feel like running, is likely to make things worse rather than better. How do you tell the difference? Well the \"gold standard\" is randomized controlled trials. Pick 3,000 random people. Tell 1000 of them to exercise more, and 1000 of them to exercise less, and 1000 leave alone, and compare. If the \"exercise more\" group is healthier at the end of 10 years, that's decent evidence that \"exercise more\" is a useful intervention. Failing that, you can think of other possible confounding factors and control for them. Don't just ask how much they exercise; ask how old they are, and how well they are, how stressed they are, and loads of other factors which might both cause both A and B, and use statistical methods to detect whether one of those factors is actually a better explanation than \"A -> B\". reply gjjydfhgd 3 hours agorootparentprevYou cannot conclude, but at the same time you should increase your probability of causality. Keeping it unchanged is another kind of logical error. reply bigfudge 3 hours agorootparentprevBut Pearls book explicitly argues for using correlational evidence to infer causation. The trick is to actually have a causal model and design observational studies that rule out as many alternative explanations as possible. reply cycomanic 1 hour agorootparentThank you! That seems to be the issue with many commenter here, they believe you can have a single experiment that somehow proofs causality. The same scepticism would not be brought against many \"hard science\" experiments, even though they do the exact same thing, falsify alternative explanations until they have high certainty that they have causality. reply lovecg 7 hours agoprevThere’s amusing statistics that show that if you’re out of shape and a smoker, you get a bigger bang for the buck from getting in shape first than quitting smoking. Disclaimer: not an endorsement of smoking. reply m463 6 hours agoparentreminds me of the bicycle helmet stuff. There was a ted talk that said bicycle helmet laws would kill more people than save. The reasoning was preventing people from riding would also prevent increased fitness, and more lives were lost from that than saved from accidents. EDIT: I think this one: https://www.youtube.com/watch?v=07o-TASvIxY reply tim333 9 minutes agorootparentI think that's pretty true in my demographic - kind of old fat, at risk of heart attacks and a casual cyclist. For us lot heart attack deaths are like 1000x+ more than cycle head fatalities. On the other hand for young cycle racers I'd go with compulsory helmets. reply AbbeSomething 6 hours agorootparentprevThis sounds ridiculous until you go to Amsterdam see that few wear helmets there. And everyone is cycling. They might be more inclined to cycle because they don’t need to wear a helmet. After years of cycling with helmet you get so used to it and it doesn’t bother you. But how many skip the bike because they never got used to wearing a helmet? reply ideamotor 5 hours agorootparentI biked through Amsterdam as a commuter along with everyone else for a week, and it just blew me away. Everyone was absolutely predictable and part of the “school of fish”. No hesitation or ill-conceived politeness. It was only a week but it was so refreshing. I think about this experience daily when driving because I think of how much time would be saved if people just knew absolutely when to take their turn and took it; instead of processing each decision and deciding based on their current mood. People knew the damn rules and norms. So, I think it’s a function of having a critical mass, being necessary, and being embedded already as a norm. I don’t believe a city could make riding without a helmet legal and expect any sort of increase in safety … reply bluejekyll 5 hours agorootparentThis doesn’t work when driving primarily because cars are generally moving a lot fast and are less maneuverable that bikes in avoiding conflict. reply jeffbee 4 hours agorootparentAnd because drivers can't see or hear anything. reply bigfudge 3 hours agorootparentAnd because they are physically more removed from the consequences of being a dick. reply late2part 5 hours agorootparentprevnext [2 more] [flagged] bazeblackwood 4 hours agorootparentOh brother, talk about histrionics… reply Broken_Hippo 2 hours agorootparentprevBiking in Amsterdam is leagues safer then biking in most places in the US, though. The risks are different. Helmets aren't as important if most of your crashes are going to be with another low-speed, mostly-soft bodied cyclist. Traffic and pedestrians are pretty separated and you have your own lanes to cycle in. I'm also pretty sure that a lot of biking is convenient: The local grocery store is just a short bike ride but it'd take 15 minutes by car. Most biking in the US is biking shared with cars. You probably won't have a bike lane. Most likely, you aren't commuting or going to the grocery store - the grocery store might take 15 minutes of driving but 30 minutes of biking - if you can even go the most direct route legally. Longer biking sessions generally means more risk. I'll take the helmet in places that I must defend myself against automobiles on an unsafe path. reply jajko 39 minutes agorootparentYes this is annoying in all these same debates which point to Netherland / Denmark - the infrastructure elsewhere is just not there. Lanes shared with cars (or just taking away from car lane so no normal cars fits in anymore) isn't a solution, just adding friction danger zone. Or dedicated bicycle lanes wide enough for a single row of cyclists, if even that. Also look at those 'old' basic bikes they use there, you don't need more on those flatlands and everybody is fine with 20-25kmh. Add tech bros or generally young folks with fast ebikes and escooters going 50kmh and things change. reply gjjydfhgd 3 hours agorootparentprevHelmets are also overrated. I've fallen many times and not once I've hit my head. They are probably useful in high speed impacts, but then you are fucked anyways. Gloves are much more useful on a daily basis. Saved my hand many times. reply snemvalts 5 hours agorootparentprevIn Amsterdam they are commuting, and in a fantastic infrastructure where cars get red lights when bicycles approach on an intersecting cycleway. That's probably the main reason for safety and why they ride so much. reply vasco 3 hours agorootparentThere's very few places where the light changes automatically for bikes in Amsterdam - all that I can remember now, don't. The large majority of lights do respond to input from pressing the cross button (also pressable by bycicles), but it's not automated. They do use \"change on approach\" lights outside of the cities way more, but in cities it's usually only for trams and buses. reply SwiftyBug 3 hours agorootparentprevI know I'll never be a motorcyclist because I don't want to ruin my hair with the helmet. People with curly hair will know what I'm talking about. reply EasyMark 5 hours agorootparentprevSince Amsterdam is a \"peak biking\" city, I wouldn't trust the cited study to even apply there and would think that an independent study would be needed since it would be likely an outlier. reply kanbara 5 hours agorootparentprevmore like because it’s safe and there are penalties for injuring pedestrians and cyclists, and because the infrastructure is great. not riding a bike because a helmet is ??uncomfortable?? is ridiculous and probably self-selecting reply Gigachad 5 hours agorootparentIs it that ridiculous? Big tech tracking shows that basically any inconvenience at all causes people to drop off. The page taking half a second longer to load and now you’ve lost a few sales. There isn’t going to be anyone consciously thinking “I’m not gonna ride because I have to wear a helmet” but instead “eh I can’t be bothered riding” without digging too much in to why. reply imtringued 3 hours agorootparentprevBicycle helmets are big. You can't just put them away in your bag when you're done with them. So you will have to carry the helmet alongside your bags etc. reply lostlogin 37 minutes agorootparentLock it up with the bike. reply eimrine 5 hours agorootparentprevnext [10 more] [flagged] pjerem 3 hours agorootparentHelmets aren’t uncomfortable if you try them before buying them. There are different shapes, materials, sizes and settings. About finding my helmet, I always attach it together with the bike with the U lock. Unless I’m parked in a secure place where I let it hang on the handlebars. I’m not especially advocating for mandatory helmets and I’m the first to say absence of helmet shouldn’t prevent you to ride, but if you are a regular cyclist, having one at hand is not a ridiculous idea. reply eimrine 1 hour agorootparent> Helmets aren’t uncomfortable if you try them before buying them. There are different shapes, materials, sizes and settings. Come on man, I have never rode a bicycle more expensive than $100 despite riding at least 5000km annually. I bet that your helmet costs more than all of my 3 bicycles. reply com2kid 5 hours agorootparentprev> Me and my bicyclist friends have never injured a head while falling down from bike Back when I did bike, I'd fall off and hit my (helmeted) head once a year or so. I was always thankful for having a helmet. Granted it was typically doing something stupid (jumping a curb or such), but still, I was thankful for having a helmet! reply eimrine 2 hours agorootparentCan you describe the most common scenario of how your helmet got broken? reply dangmumwhore 3 hours agorootparentprevnext [3 more] [flagged] imtringued 3 hours agorootparentYour username isn't? I once jumped a curb and scraped off half the skin on my palms. reply eimrine 1 hour agorootparent> the skin on my palms. The down-voted comment of mine tells that gloves are far more useful than helmet for a bicycle rider. But I know I have messed with the holy cow so c'est la vie. reply lambdaphagy 5 hours agorootparentprevI'm a pretty proficient cyclist (lifetime mileage in the tens of thousands) but there have still been a handful of incidents where I've been very grateful to have had a helmet. It might be the case that helmets are a net negative for casual riders. But whenever I've done a spontaneous unplanned dismount at 20mph I didn't find that knowing how to fall helped me much. reply eimrine 1 hour agorootparent> But whenever I've done a spontaneous unplanned dismount at 20mph I didn't find that knowing how to fall helped me much. What was the circumstances (type of road and type of bike)? Have you touched the ground with any other parts of body except of palms, elbows and knees? If yes then consider to keep learning how to fall because your falling skill is not that proficient. If not then the helmet was not that useful. reply dangmumwhore 3 hours agorootparentprevive ridden as much and there have not been those incidents, ymmv reply abandonliberty 5 hours agorootparentprevMore cyclists on the road makes it safer for cyclists. Combined with risk compensation, this seems enough to make helmet laws a net negative. Well studied. Wear a helmet though, they work! reply goda90 5 hours agorootparentA better alternative law would be to provide free helmets. People can choose not to use them out of preference but at least they'll have one to make that choice with. reply vasco 3 hours agorootparentNothing is free, you're just making other people pay for helmets that likely wouldn't be used. reply Broken_Hippo 2 hours agorootparentEveryone that comments that \"nothing is free\" is just being a pedant in a way that means the conversation can't usually go forward as easily. People do understand that with government programs, \"free\" means taxpayer funded. As in, almost everyone understands that. The comment isn't needed. Those comments are the reason I put things like \"fare free public transport\" - not because it is more realistic, but because arguing with these comments is exhausting. Society is full of things other people helped pay for - and you most definitely use them. Your health insurance company pools money together to cover everyone's ills, for example. You don't pay individually for your infrastructure use - other people help you pay so that you can get electricity. And so on. You can't have modern society without this. reply pjerem 3 hours agorootparentprevNeither is public healthcare system but if a free helmet for everyone reduces healthcare costs globally, ultimately it’s other people paying less. (I guess it doesn’t work if you don’t have public healthcare) reply vasco 3 hours agorootparentSaying other things that aren't free doesn't make the first thing become free. People who don't use helmets don't do it because they are too expensive. They do it because it's not convenient to carry, because it's not cool, because it messes your hair, because you need somewhere to store it. (Not) Free helmets solve zero of these problems, it's just a bad idea. reply RamRodification 2 hours agorootparentYou are just nitpicking on semantics. If the total cost of publicly funded healthcare is reduced from people using helmets then that could result in not increasing what you are \"making other people pay\" even though you are also offering helmets at no cost. That is what most people would consider free. People who don't use helmets for whatever reason would be more inclined to do so if they could just go pick one up, and didn't have to pay for them in a store. Even if those reasons are not that they are expensive. It's a great idea. reply vasco 2 hours agorootparentWell we disagree. I think there's way more effective things you can do, and this is demonstrated by the netherlands where I live. For an idea to be good it doesn't just have to in theory be net positive, many things can be net positive if you use tax money for them. The problem is we don't have an infinite government or infinite resources or time, so we should pick good measures. All the cities maintaining a bunch of locations full of helmets for free pickup would just create more waste. I bet people would pick them up and just discard them when it wouldn't be convenient to use them. And nobody wants to pickup and wear a discarded helmet that is dirty and was in the elements so there would be huge waste. You can have a similar effect without any waste by just having a class that teaches children to ride bycicles at school and tells them the benefits of helmets and keeps helmets there for that one class. This memory would be with you for life, and you'd make your own decision. If helmets were cost prohibitive I'd be with you, I believe in using tax money for that kind of stuff, but price is not the reason people don't use helmets. reply imtringued 3 hours agorootparentprevExcept roads for car drivers and then people wonder about this mysterious infinite latent demand for free roads that they call \"induced demand\". The demand for things that cost nothing is infinite. reply vasco 3 hours agorootparentRoads have huge utility to society. Unless you want the ambulance to go get you on a unpaved mess and take you back to the hospital banging all over the back. Or that they fetch you by bycicle. reply Refusing23 1 hour agorootparentprevfree just means tax paid its a nice idea, but i think adults should pay for their own helmets however, children... sure. give them one. reply arpinum 13 minutes agorootparentprevBicycle helmets are not designed for impact with a car. They are designed to handle falls to the ground. The forces are quite different. reply smileysteve 5 hours agorootparentprevThere's real data out there that while a helmet has better survival rates than not; but cars give more room to bicycles without helmets, so the incident rate is lower. reply sonofhans 5 hours agorootparentThe only data I’ve seen on that is one guy who self-sampled and self-reported. I’d be happy to see more. reply Scoundreller 5 hours agorootparentprevAppearing like a woman buys you even more space than riding without a helmet https://www.eta.co.uk/2011/04/01/safest-bicycle-helmet-has-b... https://pubmed.ncbi.nlm.nih.gov/17064655/ reply jjtheblunt 4 hours agorootparentprevWhere’s that data? reply bdcravens 5 hours agorootparentprevKind of tacky, but I had a strange \"what if\" kind of conversation with someone about alcohol. The premise was that more people are born due to drunken sexual encounters than are killed by drunk drivers. reply lorenzk 4 hours agorootparentBut then alcohol kills or shortens life span in many more ways than drunk driving. reply Mawr 4 hours agorootparentprevTalk to someone whose father was an alcoholic. reply rTX5CMRXIfFG 6 hours agoparentprevThat sounds like the sort of stuff you’d remember because it validates what you already want to believe reply darth_avocado 6 hours agorootparentSmoking does reduce appetite and therefore can help in weight loss/getting in shape. (I don’t smoke) reply EasyMark 5 hours agorootparentI have definitely seen more than a few smokers who quit gain weight, no idea if it's picking up a new addiction or if the smoking subdued their hunger pangs, or some combination thereof. reply TylerE 5 hours agorootparentNicotine is absolutely scientifically known to be an appetite suppressant. reply nanidin 6 hours agorootparentprevI remember when going through scuba diving certification, the instructor mentioned that smokers generally get more time out of a tank because their lungs absorb oxygen more efficiently or something like that. And she smoked a lot. reply lovecg 4 hours agorootparentprevAbsolutely true, I found it amusing because I came to strongly believe over the years that lack of physical activity is one of the worst things one can do for one’s overall wellbeing. We evolved to be physically active, sitting around all day can be compared to being malnourished or not getting enough sunlight or social contact, etc. reply nradov 6 hours agorootparentprevThat sounds like the sort of stuff that is completely correct. Low fitness is significantly more risky than smoking. (I am not suggesting that anyone start or continue smoking.) https://peterattiamd.com/all-things-vo2-max/ reply llm_trw 6 hours agorootparentIt's pretty funny seeing people today seethe about the obvious (being unfit is terrible for you) the same way people in the 1970s were seething about the obvious (smoking is bad for you). There is a reason why life expectancy was dropping in the US years before covid hit. reply throwaway2037 4 hours agorootparentI asked my parents about the \"when was smoking viewed as bad for you\" bit. They said late 1980s to early 1990s. They said no one thought it was bad for you in the 1970s -- maybe a stinky habit, that is it. reply mikestew 4 hours agorootparentI grew up in the 70s, and that’s hilariously wrong. Cigarettes had warning labels in the 60s, and cigarettes were called “coffin nails” long before that. reply yakshaving_jgt 3 hours agorootparentprevThere’s a line in a film from 1983 called The Outsiders which is like “give me one of those cancer sticks”, so I think smoking was understood to be harmful earlier than that. reply shermantanktop 5 hours agorootparentprevWhat obvious-to-others things are you surprised by? reply unmole 5 hours agorootparentprevNicotine suppresses appetite and is also a potent nootropic. Consuming nicotine in losenge or gum form doesn't have any of the ill effects of smoking or vaping. It's also virtually impossible for a nicotine naive person to become addicted through losenges. reply wisty 5 hours agorootparentIt's still addictive and likely to be horrible for your heart (smoke causes cancer, but nicotine even in low doses causes what looks like heart issues, though unlike cancer these might be reversible). reply Scoundreller 5 hours agorootparentprevalways wondered what the effects on the gums would be since the saliva concentration would be higher with gums/lozenges and it’s primarily surface absorption vs. airborne nicotine. reply arendtio 6 hours agoparentprevHow much time do you have to get into shape? When it takes 50 years, the statistic probably doesn't hold because you will be dead by then due to the negative influence of smoking on your health. reply s1artibartfast 6 hours agorootparentIts not cardiovasuclar, but the top 2 causes of death in the US are tied to obesity, which takes zero time to reduce. You just have to not eat, which is actually time positive. Most humans are just incredibly bad at impulse control. reply imiric 6 hours agorootparentTo be fair, our diets and lifestyles changed drastically in the 20th century. When we're bombarded with advertisements of highly caloric, processed, sugary foods engineered to flood our taste buds and trigger dopamine, we can't be surprised that people get addicted to it. Combine that with forms of entertainment, transportation, and a work culture that keep us sedentary, and it's no wonder many people struggle with being physically healthy. reply s1artibartfast 6 hours agorootparentindeed, a lot has changed from when we lived on rural farms doing manual labor. People aren't going back to the farms, so we need to adopt new behaviors and norms around self control. There is simply no viable alternative. reply EasyMark 5 hours agorootparentI would suggest that in the next 5 years or so if ozempic and others don't turn out to be cancer causing or have more than awful side effects that we'll see a large drop in obesity, as obesity isn't an easy thing to change. Likely other competitors will come along and use a similar mechanism and Novo won't be able to charge $1000 a month for it any longer allowing 80-85% of those who are overweight to give it a shot. reply s1artibartfast 4 hours agorootparentI think it will be very interesting to see how it plays out and has a lot of positive potential. I don't think it is very sustainable to have 50% of the population on a biologic medication to help self control, but hopefully it will help individuals and societies change norms. reply vocram 4 hours agorootparentprevParadoxically, lowering your weight artificially may be causing more damage than good because of the muscle loss. Being fit is not just about low fat. It requires a similar discipline required to control what you eat. reply throwaway2037 4 hours agorootparentprevAs I understand, ozempic only helps you lose up to 15% of your body weight. Most overweight people are much more than 15% overweight! reply safety1st 5 hours agorootparentprevTo frame the discussion as one of either/or alternatives is self-defeating, a more productive framing is what are the various strategies we can employ to reduce obesity? There are plenty which are unrelated to self-control or personal discipline. One is a sugar tax (not a huge fan personally but it exists). One I do like is to regulate the advertising and labeling of food. Frankly I'd like to know how many calories I'm eating or being persuaded to eat, pretty much all the time. This is a work in progress with the FDA gradually expanding the types of restaurants that are required to disclose caloric and nutrition info about their food. Frankly I'd like to see it required in advertisements too, if you're advertising a pizza, the advertisement should disclose that there are 2,000 calories in that pizza, many people actually are not aware. Not that I ever thought they were serving health food at the Cheesecake Factory, but I recently learned that their peanut butter cheesecake is 1650 calories per slice! Almost a full day's calories in one slice of cake! Nearly everyone I've talked to about it knew it was a gut buster but no one guessed that high. Those cheesecakes are only for special occasions now and it's because C.F. is required to disclose calories, I'd be fatter if not for this simple government intervention. Nothing to do with self control. Apologies to C.F. but with the nation's obesity rate cresting 50% I consider this a wholly reasonable imposition on their free speech rights or whatever. reply vasco 3 hours agorootparent> Those cheesecakes are only for special occasions now and it's because C.F. is required to disclose calories, I'd be fatter if not for this simple government intervention. Nothing to do with self control How is eating a cake from a store \"nothing to do with self control\". It's not like you were buying vegetables and turns out big sugar made the vegetables be worse for you. You not knowing if the cake was 800 calories or 1.6k calories makes it no less about self control. Even to avoid eating it at 1.6k, it's still all just self control. reply s1artibartfast 4 hours agorootparentprevI'm for calorie labeling, but view it simply as a tool and aid for those who choose and try to self regulate. There is no amount of labeling that will make a different if there isn't a human expressing agency to read the label and take action. I think there are tons of helpful strategies, but the critical infrastructure t Needed is more self actualized agency. This isn't an insult or dismissal, but a hope we can all become better humans. reply Aerroon 5 hours agorootparentprevThere was a relatively recent study that found that BMR has actually been dropping over the last 30 years. The difference in men was large enough (7% iirc) that it would explain most of the obesity epidemic. Why it's dropping is still a n mystery though. Nevertheless, the solution is still the same: eat less. The study: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10445668/ reply kelseydh 3 hours agorootparentI would like to see replications, as it could just be bad data: \"It is also possible that the long-term reduction in BMR represents methodological artefacts. In the early years, measurements of BMR were often made using mouthpieces to collect respiratory gases, and recently such devices have been shown to elevate BMR by around 6%. A second possibility is that early measurements paid less attention to controlling ambient temperature to ensure individuals were at thermoneutral temperatures.\" reply s1artibartfast 5 hours agorootparentprevI'm not an expert on the literature, but I do know Bmr isn't a fixed number. It is a function of activity, muscle mass, and other factors. It seems that a largely sedentary lifestyle free from exertion matches both. It would be insightful to see the sensitivity analysis for BMR with respect to strength, activity, muscle mass, ect. I know there is a body of data about environmental hormones and BMR, ect, but my understanding is that the impact is barely measurable with large sample sizes. I expect that it can't hold a candle to activity. In my personal experience, my BMR can easily modulate by 1000 calories based on body composition and behavior. That is a huge impact. reply 123yawaworht456 1 hour agorootparentprevlack of physical activity (sedentary job + sedentary entertainment) + shitty diet = lower testosterone lower testosterone = lower lean body mass (you gain tens of pounds of muscle on TRT even without exercising) lower lean body mass = lower BMR. an average doughy 200 pound guy and a 200 pound athlete have vastly different caloric requirements. reply Scoundreller 5 hours agorootparentprev> Nevertheless, the solution is still the same: eat less. Or make life harder! Quit the elevator. Quit the automatic transmission (and car). Quit the online shopping. Making some progress with self-checkouts and pointless lineups at airports. Less jokingly (hard to outrun a bad diet) but more pedanticly: drinking less (alcohol and calorific beverages) might be a better start than eating less. reply the_third_wave 4 hours agorootparentprevIt would be interesting to correlate these results with data about reduced testosteron levels and sperm counts/fertility, especially whether one of these leads the other or whether both go down at the same time. reply EasyMark 5 hours agorootparentprevclearly there is no \"just\" to it as a level of effort on the part of those who are overweight. It is not an easy road and something like 95% of the people fail at it, so I guess it depends on what your meaning of \"just\" is. reply s1artibartfast 5 hours agorootparent“Just“ means the options and consequences are clear. The actions are the hard part. People are faced with the choice between eating more or living longer, and nearly everyone knows it. reply cycomanic 5 hours agorootparentprevPeople here complain a lot about social media and the tuning for engagement which makes people addicted to their phones etc.. Well compared to the food industry industry that's nothing. Incidentally they learned their trade from the tobacco industry who invested heavily in the junk food industry in the 60s. [1] All to say, much of modern food (and the advertising around it) is designed to get people hooked on junk food from a very early age. So saying just eat less is a simplistic solution that requires people to act against a conditioning that has been aggressively imprinted on them from a very early age. [1] https://lsa.umich.edu/psych/news-events/all-news/faculty-new... reply s1artibartfast 5 hours agorootparentMost of life is acting against one conditioned impulse in favor of a better one. Every solution to every problem involves overcoming ones own detrimental impulses in favor of positive ones. reply WalterBright 4 hours agorootparentprevFortunately, people have minds which can override their programming. reply bagels 6 hours agorootparentprevIt can take months to go from obese to not obese through calorie restriction reply s1artibartfast 6 hours agorootparentOf course. I was responding to the time spent (out of ones day), opposed to calendar days of not eating. That said, people can make pretty rapid weight loss without eating. skip eating 1 day a week and that is 30 lbs a year for an average obese man. skip 2 days/week and that is 60 lbs/year. reply throwaway2037 4 hours agorootparent> people can make pretty rapid weight loss without eating Your suggestion is not sustainable. A staggering 90+% of people regain all weight loss a year after their diet ends. Seriously, who is not going to eat for two days per week? Your mental health would be a wreck. Can you do it? I cannot. reply s1artibartfast 3 hours agorootparent90% of people regain weight because they go back to eating like they have a death wish. I think conflating between mental health and eating is at the heart of the obesity epidemic. Nobody will die if they skip a few meals. They will suffer some moderate discomfort depending on how much they fixate on it. I can go a week without eating and a few days is trivial. The longest a human has gone without food is 381 days and they lost 275 lbs reply wasmitnetzen 2 hours agorootparentPlease do not assume your experience is universal. You simply cannot know how it feels for other people to be hungry, it might be a completely different feeling than what you get. reply jagraff 5 hours agorootparentprevThose numbers are assuming someone who skips a day of eating doesn’t increase their calorie consumption on other days to compensate reply EasyMark 5 hours agorootparentA lot of people assume humans are mechanistic, unfeeling CICO convertors and exclude psychological and physiological realities of losing weight. I've grown used to it. It took several tries for me to drop 50lbs and most of it was psychological and good habit formation. it took about 3 years to complete that journey. reply s1artibartfast 4 hours agorootparentI think the point is rather to communicate that there is no trick to around cico. Instead, they are reinforcing that the path is clear, and the the barriers Are entirely psychological. Effort to build habits, effort to exert self control, effort to find solutions when you stumble. If people are looking and waiting for a strategy that makes weight loss easier than inaction, they are bound for disappointment. Some rare people find a passion that makes the process of getting healthy and fit more fun and easy than getting fatter, but that is exceedingly rare. reply aidenn0 5 hours agorootparentprevExactly; when I fast for 24 hours, I feel a compulsion to consume every single calorie within a 3 block radius afterwards. reply WalterBright 4 hours agorootparentPeople who do this regularly tell me it gets a lot easier after a while. reply s1artibartfast 5 hours agorootparentprevYep. You can't eat your way to weight loss. There there isn't any easy trick to beat the laws of physics. Just eat less and burn more calories. reply agos 1 hour agorootparent\"just\" is doing a lot of work here. reply davorak 5 hours agorootparentprevAnd keep track of the calories in the meals they do eat to make sure they are not compensating is going to take nonzero time. reply wrycoder 6 hours agorootparentprevIt only takes a few months to make substantial gains in c-v fitness. reply refurb 5 hours agorootparentThis is very true. Long ago I started biking to work, and biking really hard - basically making the best time I could. The first few weeks, at the end of a 10 minute sprint, my heart would be racing and I would be out of breath for 5 minutes after arrival. I was amazed after 2 months of doing the same thing I wasn't out of breath, it took maybe 1 minute for my heart rate to slow back down. reply kccqzy 6 hours agorootparentprevDefine substantial. I find that incredible. EDIT: Now that I've seen the replies, I realize that this is from a very low baseline. I'm personally going to the gym four to five days a week already and my own cardiovascular fitness has not seen much of an improvement in many months. reply s1artibartfast 6 hours agorootparentNot a source, but it tracks for me. Walk, jog, or run daily and you will have massive CV improvement from baseline in 3 months. people can and do go from being exhausted walking to their car to being capable running many miles in an incredibly short time. The typical human body is very responsive when pushed. reply WalterBright 4 hours agorootparentThe heart, lungs, and muscles get strong fast. But the bones and tendons take more time. It's best to give them time so you don't wind up with shin splints, etc. reply SamPatt 5 hours agorootparentprevIt's the same with lifting and adding muscle mass. If you've never lifted before and you hit the gym consistently and have the proper diet, you will get some serious \"newb gains.\" The first year can be amazing. But if you've lifted seriously for a decade, it's totally different. Muscle mass is very hard to add now. Cardio gains from a coach potato introducing a serious routine will be incredible. I went from not being able to run more than 30 seconds to running a 5k in 4-5 months, and that's without pushing myself extremely hard. reply hamandcheese 6 hours agorootparentprevI went from having not run in years to running a 5k in about 2 months. I was obese when I hit that goal, though I recently graduated to being merely overweight. My VO2 max as reported by my Apple Watch has been making steady gains, though I'm still far below average for my age. I didn't train hard, just consistently. reply elijaht 6 hours agorootparentprevIt’s true. Between myself and some close friends I have seen the following in ~6 months: - unable to run -> run a 10k - increase of ~50% in a all out 20 minute cycling test - max mile time 9min->6:30min The human body can make incredible cardiovascular leaps when starting from a relatively low point reply Mawr 4 hours agorootparentprevhttps://en.wikipedia.org/wiki/Diminishing_returns reply hnick 6 hours agorootparentprevOne example: https://en.wikipedia.org/wiki/Couch_to_5K 9 weeks. I have basic fitness but have never been close to completing a 5km run (I think I finally realised I have breathing issues - seeing an ENT soon). reply EasyMark 5 hours agorootparentprevthe only way I was able to improve mine was to use interval training to get my heart rate up. It took a few months but going from a walk to sprint levels on my elliptical did wonders after a while. Observing my heart rate carefully of course. reply Jimmc414 5 hours agorootparentprevn=1, but I can personally attest to going from average sedentary 29 year old to marathon running shape in roughly 3 weeks of continuous hiking 8 to 10 hours a day in the Appalachian mountains with a pack that was about 35% of my body weight. reply TylerE 5 hours agorootparentI have a hard time believing that a truly sedentary person could do that. reply Jimmc414 4 hours agorootparentI wasn't overweight, I just sat 8+ hours per day. If you are interested, here is the photo journal https://www.facebook.com/media/set/?set=a.1420283804872269&t... reply dopidopHN 6 hours agorootparentprevA bit less than a year ? And also I made friends along the way. reply m3kw9 6 hours agoparentprevGetting back to the baseline isn’t a bonus reply Broken_Hippo 2 hours agorootparentIt most definitely is a bonus if you aren't there now. reply hyperthesis 18 minutes agoprevCan anyone give a sense of how much fitter 3.5 mL/kg/min is, in practical terms? e.g. distance cycled/how out of breath? > For every 1-MET (3.5 mL/kg/min) higher level of CRF, we identified substantial reductions in the risk of all-cause, CVD and cancer mortality. We also identified significant reductions in the risk of incident hypertension, heart failure, stroke, atrial fibrillation and type 2 diabetes per higher MET. > For most, a 1-MET higher level of CRF is attainable through a regular aerobic exercise programme. For example, in a large population-based observational study of over 90 000 participants, nearly 30% were able to increase their CRF by 1-MET (median follow-up was 6.3 years) without intervention. reply Aperocky 6 hours agoprevFortunately, this seems relatively simple to change (compared to other things with this magnitude of impact). It was a transformative experience to go from 40 VO2max to 57 (as reported by Apple Watch, and losing 30lbs on the way) over the past 12 months. The best part about it is that I didn't even have to give up anything - the extra energy I gained from 90-120 minutes of aerobics per day also had the effect of making everything else more efficient. reply mvkel 5 hours agoparentYou should know that vo2 measurements that aren't performed via gas exchange or blood measurement are a complete guess. The other thing is the denominator of vo2 is weight. So losing weight without working out at all will increase vo2 reply rhplus 5 hours agorootparentIt really doesn’t matter for most amateurs trying to improve their health whether the metric is accurate or not. The huge benefit of fitness trackers is that a metric is a metric is a metric. Eat less, exercise more, sleep soundly - you can’t go wrong, and if the metric is steps, or HR, or pace, or calories burned, or (in this case) estimated VO2max and seeing improvements in those metrics relative to a baseline is a motivating factor then that’s all that matters. Well done to OP. Keep it up. reply milch 4 hours agorootparentThe problem with the VO2max metric is that there is no measurement of it outside of a few lab methods, certainly not in a fitness tracker, so they're estimating. It makes more sense to look at the other metrics that it CAN easily measure, like heart rate recovery to get a quantifiable and repeatable measurement. I agree for amateurs accuracy is not necessary, but repeatability is. If I weigh myself today and the scale says 10 lbs below my actual weight, and tomorrow it's 10lbs above my actual weight that's a problem. If it's always 10lbs below that's fine, the delta will be correct. You can't say that for VO2max you get out of a fitness tracker. My watch says my VO2max is 36.8 which would put me very strongly in the long term negative health effects bucket and well below average. Meanwhile Garmin claims it's 51 which would put me in the top 15% of my age group. It can't be both. They're just made up estimations, and following them blindly could be dangerous. What if someone's is going up even though they're not actually doing a healthy amount of exercise? Or someone's is going down or maintaining even though they are, causing them to fall into negative thought or behavioral patterns? Better to stick with some other data points reply dextro42 3 hours agorootparentI myself did a lot of exercising during the last 1 1/2 years using an apple watch for measures and I would also go with the previous comment: Even thought the VO2max is not accurate (especially not compared to actually lab data) it is a nice graph with a tendency which REALLY helps. High differences might come from different estimation techniques and thus this value is not comparable among different brands but it seems to be for your own value and progress. During a lazier time it depleted a bit, now its going up again. Most other graphs don't show it that nice. E.g. your avg hearth rate will not make huge changes (and drop which, even if its good, doesn't feel that good). Edit: I actually did a Lab test (not VO2max) but an exercise ecg with lactate measurements. I couldn't choose between running and cycling. The problem is: if you choose the bike they can measure the values while you continue riding. If you choose running you have to stop now and then while they take a blood sample. Even there it isn't 100% accurate and might yield different results based on the used method. reply Aperocky 3 hours agorootparentprevI think you just want to track both over the long term - that is compare your watch with your watch and your garmin with your garmin. But that is a huge gap, I think in general running creates more realistic VO2max data than cycling since there are too many factors at play (e.g. if you're in a group or not or using the drop vs. not). reply mvkel 5 hours agorootparentprevI disagree. Weight loss doesn't automatically mean \"healthier.\" Using vo2 as a metric of \"health\" would not be correct if weight loss is the sole attribution of vo2 gain. One can lose weight without doing a minute of exercise and vo2 would increase. If one is interested in getting healthier, I think it's well worth the time to learn about how the metrics work. reply rhplus 5 hours agorootparentMy point is that for most adults in the world right now (western, industrialized, developing, modern, whatever) there is an obesity epidemic and any amount of focused self-care through exercise and diet will improve health outcomes. Eat less, exercise more. If an arbitrary metric on fitness tracker helps you reach that goal, then just do it. reply Aperocky 3 hours agorootparentprevYes. But at least the watch is comparing against a previous measurement, making the baseline consistent. It could have been named Vo5max and for me it'd have worked. In the real world, this number actually tracks pretty linearly with how much faster I'm able to climb a hill on a bike. reply hamandcheese 3 hours agorootparent> In the real world, this number actually tracks pretty linearly with how much faster I'm able to climb a hill on a bike. That's because both of those things are directly related to weight. (VO2 max is usually given in a weight-normalized unit. So losing weight will make it go up as long as you at least maintain your fitness). reply jebarker 6 hours agoparentprev40 to 57 is a huge increase in a year - kudos! reply Aperocky 2 hours agorootparentThank you! This post is a little braggy but it's also one of the best things I've did and I'm genuinely proud of it. The result and the process wildly exceeded my expectation at the time when I started last summer and it made me happier, more efficient, and even gave me time to reflect (since there's not much else you can do while going on a long ride/run) mentally. If anyone is thinking about it - give it a try! The starting months are the hardest, but once it gets going a positive feedback loop takes over. I've been able to convince some people in my life to start exercising when they saw me change so substantially in a single year. reply johnfn 5 hours agoparentprevWhat sort of workouts do you do? I've been trying to do the same thing, but I'm finding it difficult to get VO2max to go up consistently. reply Aperocky 3 hours agorootparentFull aerobics with no strength training. So far I only do cycling and running, averaging to about 110~ minutes per day (with weekends having longer times and weekdays shorter) 80% or more of it is spent at Zone 2/Endurance where I try to keep my heart rate below 145. reply throwaway2037 4 hours agoparentprevI love these kinds of posts. They are ridiculous. First: > Fortunately, this seems relatively simple to change Second: > the extra energy I gained from 90-120 minutes of aerobics per day Are you joking? You are asking average people to commit to 90-120 of c-v exercise per day? So, 7 days per week? This stuff is nonsense. Most people would quit after a week. I guess 99% of people could not sustain this for more than 3 months. reply Aperocky 3 hours agorootparentI still average more screen time on the phone.. The hard part is getting started. Once you start it's self-perpetuating. Zone 2 endurance aerobic exercise are in some way similar to an addiction but you'll need a certain volume to trigger it. It is a little braggy but I also consider this one of the best things I've did in my whole life - and I myself didn't remotely dreamed of this result when I started last year. reply bo1024 2 hours agorootparentprevThey did say \"simple\", not \"easy\"... reply timeon 1 hour agorootparentprevYou could already be running instead of spending time in this thread. With 30km bike commute you can scroll the web as much as before. reply purplerabbit 6 hours agoparentprevthat’s an amazing change. Could you elaborate on your training regimen? reply Aperocky 3 hours agorootparentIt's a mixture of running and cycling and really less planning than what I should have. Generally, my runs are between 5-10K and rides are 30km/20mi ish. More days than not I'll do both, but on other days I only get one done due to time issues. Weekends are usually for longer rides. I average around 200km/130mi on the bike and 40-50km of running per week. I've had to build up to this level gradually so it started from a much lower volume but I've been able to keep it pretty consistent for this year. It's gotten to a point where if I don't do any exercise I would feel really bad physiologically so it drives me to go out everyday. But I essentially choose what to do at the hour and since most of the training is zone 2/endurance it doesn't matter too much. reply 38 5 hours agoparentprev2 hours a day is a ridiculous amount reply adrianN 4 hours agorootparentJust my commute is about an hour of cycling per day and I do some other sports two to three times a week for about two hours. It's not that hard to average out to about two hours of activity per day. reply SamPatt 5 hours agorootparentprevWhy is two hours ridiculous? I find most people who devote themselves like this enjoy the process. If they enjoy it, it objectively improves their body, and they have the time - why not? reply andoando 4 hours agorootparentI definitely waste more time during the day than that lol. Happiest and most productive Ive been was when I was skating, riding a bike, dancing etc for hours every day. reply throwaway2037 4 hours agorootparentprev> Why is two hours ridiculous? Because it isn't sustainable, except for 0.1% of people. reply timeon 1 hour agorootparentprevNot as ridiculous amount as average (and mine) screen time. reply keybored 3 hours agoparentprevCongrats. You’re an inspiration to me. :) reply joduplessis 23 minutes agoprevI've been running a consistent 10-11ish kilometers 5 times a week for the last 13 years. Recent (old) re-injury has forced me to switch to weight training about 6 times a week - and absolutely loving it. I'll probably switch to something like Crossfit next year to take up a more cardio-routine, but it's showed how you don't need to necessarily \"run\" to stay fit & healthy (although I do miss it a lot). reply weatherlite 5 hours agoprevIt's also absolutely amazing for mental health. It's a shame it isn't considered a first line of treatment for mental health because it's super effective (and free). reply rincebrain 1 hour agoparentThe problem is that, if you're in a position where you are having serious depression, for example, it doesn't really matter how objectively or subjectively helpful it is, you can't even bother getting motivated to do things you are certain you enjoy, let alone something unknown. This is the same problem with CPAP machines - they nominally have a huge efficacy rate, and a rock-bottom rate of people sticking to them, because for a lot of people, they're deeply invasive and uncomfortable, and insurance (in the US) in many cases has stopped paying for them if their obsessive babysitting doesn't swear you've used it 95% of the time, so people just say \"hell with it\" and stop stressing about, say, not being able to take a hike for a few days of camping because their insurance will stop paying for it. It all goes back to the fundamental premise of \"if your solution requires changing people, you will not succeed.\" reply pjc50 2 hours agoparentprevExercise is the perfect treatment for mental health: it doesn't cost the \"provider\" anything, and when the patient doesn't do it because they're depressed or disordered, you can blame them for not cooperating. It's not a solution, it's something you can say so that it's not your problem any more. (Would be great if there were actually prescribed work-compatible supervised exercise sessions on offer, but AFAIK that's not a thing offered anywhere by any health service) reply steve_adams_86 3 hours agoparentprevI deal with pretty severe depression and running is far more effective than antidepressants. It varies for everyone, but I know I’m not alone. It’s tough because I’ve known this for 15 years or so, but when I’m in a hole it somehow seems like it’ll only make things worse. I understand why someone who hasn’t had the experience of feeling better from exercise would see the entire premise as unlikely or absurd. reply makeitdouble 4 hours agoparentprevI think it's partly because as it also works as prevention of those same mental illnesses, many patients will have a higher than average barrier to clear to exercise (including just getting out of the house for a walk). I've had friends able to start exercising a few months in their treatments, when the medication gave them more mental leeway. Before that they were fighting to wake up and properly eat, and do anything really. reply fortyguy40 4 hours agoparentprevI struggle with anxiety and have below average HRV and VO2max. I started jogging a month or so ago and have definitely seen an improvement in my mental health. reply weatherlite 4 hours agorootparentI love hearing this. If you'll keep at it you'll see your fitness and VO2max explode up so don't worry about it. Do some easy miles a few times a week during the first few months, just keep at it and be consistent. reply agos 1 hour agoparentprevis it not? I thought the suggestion \"have you tried exercising\" to depressed people had reached meme status based on its frequency reply weatherlite 29 minutes agorootparentI'm not familiar with people suggesting this. It should come from school counselors , college counselors, family doctors and therapists. The thing is all these people, statistically, are not exercising enough themselves so I find it hard to believe they will offer exercise as a first line. reply SwiftyBug 3 hours agoparentprevIt wasn't before I had a good place to run (away from cars and the city) that I started running. Yes, it's free, but it's very hard to pick the habit if your environment doesn't help. reply weatherlite 31 minutes agorootparentNo doubt. Picking any new habit is quite hard. If your immediate surroundings don't support running a treadmill could also be a good solution. reply snemvalts 5 hours agoparentprevInstead, people try to fix it with antidepressants, benzodiazepines or something else. It feels a bit like the US approach to pain management – instead of massages or physical therapy, some opioid pill is cheaper to prescribe. reply dailykoder 5 hours agorootparentI dislike this black n white rhetoric from both sides. \"Just do some workout\" - \"no this doesn't work for me\". Yes, workout does help, but mental illness is still real. Both sides should try to be more sensitive and more understanding in my opinion. I can't fix my social anxiety through workout. But I sure can feel better about myself when doing it and then approach those anxieties with more confidence, but the anxieties are still there. reply snemvalts 1 hour agorootparentI'm speaking from experience regarding mental illnesses and exercise. And I never discounted medication. Just that exercise is critically underprescribed, I'm fairly sure it would work better for milder cases compared to meds. Not to mention the other health benefits listed in the thread. Same way an opioid pill is still prescribed in cases of cancer or severe pain. Just that there are probably better, milder alternatives that don't have as many side effects that could fit a lot of these people with milder problems. reply weatherlite 4 hours agorootparentprevI'm not saying that workouts will necessarily fix your social anxiety or any other mental disorder, but I don't know of anything else that necessarily will - meds and psychotherapy are also quite limited in their effectiveness. All I'm saying is people should at least consider that exercise (and more specifically - mild to rigorous cardio workouts) can be just as effective as psychotherapy / meds are. The evidence is there. I don't expect this understanding to come from therapists, this needs to come from society at large. Also, it doesn't have to be mutually exclusive, you can do both. reply throwaway2037 4 hours agorootparentprev> my social anxiety I am considering trying some beta blockers. Did you try them? They seem like an interesting experiement. reply dailykoder 3 hours agorootparentHave you tried pineapple on your pizza? Seems like an interesting experiment reply TillE 4 hours agorootparentprevExercise (and sunlight!) are proven treatments for mild to moderate depression. It's probably not all you need, but it's a very good start. Exercise does nothing for severe depression. reply weatherlite 4 hours agorootparentAgreed, but mild to moderate depression is the majority of people with mental health issues - that's where we should start. Also , I suspect it will also help a lot in severe depression but its hard to get someone with severe depression to exercise - in that case meds should be the way to go. reply steve_adams_86 3 hours agorootparentprevMy depression is severe and exercise does make a difference. I’m not sure why you’d think that. It doesn’t make the bad thoughts go away. It doesn’t turn off the bad feelings. I’d still be diagnosed with severe depression if I went in with a fresh slate. What it does is give me the energy to endure it, though. The physiological symptoms subside quite a bit, and it makes a meaningful difference. It also helps more than medication since I seem to be a non-responder. It’s a big help in my life. reply noonanibus 4 hours agorootparentprevIt's not that hard a leap is it? For some reason it seems oblivious to non-sufferers that the idea of a physical treatment for a mental ailment is a given? For most folks, that connection doesn't exist. Hell, I work out 3x a week and even I don't notice the obvious side-effects even though I'm certain they exist. When we're dealing with ordinary people living their daily lives, the idea that something so \"non-mental\" - in the most literal sense \"physical\" can have an effect on the mental, is a really tough thing to swallow, understand and hell, even percept when things are going well? Sorry. But I'm an avid gym-goer and even I have to remind myself of the positive it's doing. We're not all the same. reply weatherlite 4 hours agorootparent> Sorry. But I'm an avid gym-goer and even I have to remind myself of the positive it's doing. We're not all the same. Maybe you're one of the people that for whatever reason exercise does nothing to - though I highly doubt it. I'm not sure what training you do exactly but to reap most of the benefits the workout should include moderate cardio work. I don't think going to lift weights for a 40-60 minutes with plenty of rest between sets will cut it. Running for 45+ minutes is what most people should aim for, of course beginners will do less. Anyway I agree with you - for most folks the connection doesn't exist, perhaps its time this changed. reply weatherlite 4 hours agorootparentprevYep exactly this. The thing is we are now so removed from exercise (and healthy living in general) as a society - take the car, take the elevator, sit at your desk the whole day and then fall asleep in the couch at home. And paradoxically this lifestyle makes us so tired and energy depleted, that even the thought of starting to exercise seems ridiculous to many. This is making it super hard for many people to start exercising and persist, it seems like everything in modern society is geared to make us couch zombies - so no surprise we have high levels of obesity, anxiety, depression and what not. reply Mathnerd314 5 hours agoprevThe relationship isn't linear, you can see in https://bjsm.bmj.com/content/54/20/1195#sec-16 that above 2200 MET-minutes/week there is increased cancer risk which outweighs the cardiovascular benefits. Now admittedly it is a very weak correlation, and MET measurement is imprecise, but I think it is safe to say that there are risks associated with over-exercising and while being in shape is good, being a ultra-marathon runner or other extreme fitness enthusiast is unhealthy long-term, and the risks start sooner than you might expect. (2200 MET-minutes/week is about 3 hours of 8mph running per week) reply julianeon 5 hours agoparentI remember reading somewhere that, to maximize your health, run as much as you can each week, you won't hit the upper bound when it gets bad. In the example you cited, that's 36 hours (!) of runnning in a week: meaning I'd have to run 6 hours a day and only take Sunday off each week before I was officially running too much. And I've noticed I can't run more than 4 days a week anyway, so even that's off limits. 6 hours a day, 6 days a week, even under optimal conditions, wouldn't be something I'd approach even if I was retired. So practically speaking, \"run as much as you can before your body aches stops you\" seems like a good rule of thumb. reply Mathnerd314 5 hours agorootparentI'm not sure what math you're using. 12.9 MET for running * 3 hours * 60 minutes/hour is 2322 MET-minutes, so just 3 hours of running is sufficient to exceed the 2200 threshold. 36 hours of running would be 27864 MET-minutes, of course it is too much. But 3 hours of running in a week is quite easy to exceed, e.g. if you run half an hour every morning. There are some studies which suggest that the artery buildup from frequent running is not bad, but these don't take cancer or other factors into account - in contrast all-cause mortality is one of the most comprehensive statistics available. The drawback is that although it gives you a number, that number might be affected by unknown variables. reply ywnico 5 hours agorootparentprevMET-minutes seem to be \"Metabolic equivalent of task\" minutes [1]. In the paper, they give running an MET of 8 (paragraph 3 of Data Analysis section), so 2200 MET-minutes of running would be 2200/8 = 275 minutes, if I understand correctly. [1] https://en.wikipedia.org/wiki/Metabolic_equivalent_of_task reply voisin 5 hours agorootparentprev> In the example you cited, that's 36 hours (!) of runnning in a week Isn’t OP saying explicitly that 2200 MET minutes per week is simply 3 hours of running per week at 8mph? Where does your 36 hours come from? reply ParacelsusOfEgg 4 hours agoparentprevGreat share! That meta-analysis was a good read. I'd disagree slightly with your characterization of the author's findings on over-exercising; quoting from the discussion section: \"Our findings do not provide evidence for increased mortality risk with physical activity amounts as high as seven times above the current recommended target range. All cause, CVD and CHD mortality risks were lower at physical activity levels up to approximately 5–7 times the recommended level but the additional reduction in risk of mortality with engagement in activity at levels beyond the recommendations was modest and with increasing uncertainty, as reflected by the wide confidence intervals. Thus based on all of the available studies, we did not identify a higher mortality risk at any level of physical activity above the recommended level, although the lowest point estimate for all cause mortality was approximately 2000 MET min. Our analyses suggest that 10–12 hours of weekly vigorous physical activity cannot be considered harmful to longevity.\" My reading is that the authors did not draw any conclusions about increased risks at higher MET levels due to the broad confidence intervals caused by the cancer risk data. reply Mathnerd314 4 hours agorootparentRight. I think that statement is based on Supplement eTable 5, where they analyzed numbers up to 6,000 MET-minutes/week. Using confidence intervals in this case is a bit misleading though. What would be most useful IMO would be the prediction interval for the lowest point estimate, as that could be used to determine if individual activity amounts are appropriate. reply bo1024 2 hours agoparentprevIf the authors don't draw conclusions from that, I wouldn't either. For example, suppose the main impact of exercise on cancer risk were simply the correlation with time spent outdoors in the sun. Then you would expect to see a chart like this. But a conclusion like 'limit running to 3 hours/week, while walking can be up to 12 hours/week' would be totally wrong. reply throwaway2037 4 hours agoparentprev> 2200 MET-minutes/week is about 3 hours of 8mph running per week Wow, that is a lot of exercise for the average person. That would be 30 mins per day, six days a week. Again, I don't think 99% of people can sustain this for a more than a few months. reply jeffbee 4 hours agoparentprevLuckily, very few people are in danger of extreme fitness. That is what the word extreme means. reply MPSimmons 7 hours agoprevThe Apple watch has a decent cardio fitness monitor. The absolute values aren't precise (since it uses VO2Max estimates, which is really only accurate if you're doing a metabolic stress test), but it's good to keep an eye on trends. reply layman51 7 hours agoparentI wonder how it estimates VO2Max and how often depending on the model. On my Series 3, it seems to make a data point two or three times a month and it’s concerning to me because it seems like it isn’t trending up even though I have started to do more cardio exercises. Instead, it has consistently reported a range that it says is below average. Could it be that it’s just pretty difficult to improve VO2Max in general? Some people’s hearts have to beat harder or they breathe in less than fitter people? reply migueelo 4 hours agorootparentI've also been wondering this. Mine reports a number that's way below what I get from other trackers. I suspect it might use the age and gender predicted max heart rate in the calculations instead of the values set by the user in the workout settings. My MHR has been roughly 25 beats above the average throughout my adult life. As far as I know, above average MHR is not an indicator of poor cardio fitness in it self (even though being fit affects heart rate response). The discussion part of this paper lists some causes for incorrect VO2MAX prediction, and has a line that may support my thesis https://www.apple.com/healthcare/docs/site/Using_Apple_Watch... > In addition to chronotropic incompetence, other medical conditions can also decrease the accuracy of VO2 max estimates on Apple Watch ... medical conditions that severely limit exercise tolerance, preventing patients from reaching heart rates close to their predicted maximum heart rate reply SamPatt 5 hours agorootparentprevThe FitBit vo2max estimate is fairly crude, at least for my model. It requires a ten minute run on level ground at a certain heart rate to \"calibrate\" but tries to estimate it from other exercise too. It leads to the number fluctuating quite a bit between runs and other exercises. I'd argue that how you feel during a run or hard cardio training is a better metric. You can fairly easily judge progress week to week, completely subjectively. reply smileysteve 5 hours agorootparentprevI have a mii band fit; it only calculates vo² max when you run for longer than 10 minutes reach a heart rate over 150 reply Affric 6 hours agorootparentprevSeries 5 does it post every Cardio exercise reply nanidin 6 hours agorootparentAccording to Apple[0] it is only after the specific activities of: Outdoor Walk, Outdoor Run, and Hiking. Which explains why my estimated VO2 max goes down significantly in the winter months when I'm mainly doing cardio indoors and only taking leisurely walks with the dog outdoors. [0] https://support.apple.com/en-us/108790#:~:text=Apple%20Watch.... reply voisin 5 hours agorootparentI wonder why they can’t track cardio indoors. Perhaps because the watch doesn’t link up to most / any treadmills and is generally totally inaccurate with respect to distance and speed? reply andbberger 6 minutes agorootparentit's just the V02 max estimate which is only validated for those scenarios reply nradov 6 hours agorootparentprevVO2 Max is highly trainable, up to a point. But in order to see any significant improvement you're going to have to spend months consistently doing a mix of hard intervals and long zone 2 workouts. There are major differences between individuals in terms of training response and achievable upper limit. reply WheatMillington 6 hours agoparentprevCardio still counts even if you don't measure it and share it with Apple. reply russell_h 6 hours agorootparentI think that Apple end-to-end encrypts fitness data. reply nanidin 6 hours agorootparentprevDoes Apple get your cardio data? It's my understanding is that is encrypted, even when it goes to iCloud for backup. reply nradov 6 hours agorootparentprevCome on now. If it's not on Strava then it didn't happen. reply dailykoder 4 hours agorootparentStrava is not enough. You should make a tiktok livestream while exercising. Otherwise it's not a real workout reply FredPret 6 hours agoparentprevThe poor VO2Max estimates on the Apple Watch is why I'm phasing out of wearing one. I track my actual VO2Max measurements in Apple Health via a shortcut, and I don't need a daily made-up number messing up my data. reply weatherlite 5 hours agorootparentWhy is Vo2Max important for people who aren't professional athletes? It's not that I'm not curious about my number I'm just not sure how helpful it is going to be - I need to understand what pace I can hold 5k, 10k, 21k and eventually 42k. I think the best way to know that is racing again and again and getting a good feel for your aerobic / threshold zone. Whether my Vo2max is 51 or 52 - does it matter to me? reply bpizzi 37 minutes agorootparent> looking at VO2 max in relation to all-cause mortality, we see a very clear trend. Simply bringing your VO2 max from ‘low’ (bottom 25th percentile) to ‘below average’ (25th to 50th percentile) is associated with a 50% reduction in all-cause mortality. When you go from ‘low’ to ‘above average’ (50th to 75th percentile) the risk reduction is closer to 70%! https://peterattiamd.com/category/exercise/vo2-max/ reply beezlebroxxxxxx 49 minutes agorootparentprevUnless you're getting it tested in a lab, the number is not very accurate. Genetics also plays a large part in how much you can raise it. For most casual people it's just a single data point that seems all encompassing so they go: number goes up = good. Most professional athletes don't really care about vo2 max all that much. reply FredPret 5 hours agorootparentprevTo me it's a measurement of roughly how not-dead I am reply ssgodderidge 6 hours agorootparentprevI would love to see how this works. Where can I learn more? I’ve been pretty dissatisfied with the VO2Max on the watch overall. reply FredPret 6 hours agorootparentYou can create a shortcut that prompts you for text input, then appends the current datetime, and saves it to the health metric of your choice. You then export this to your home screen. So when you get good data for your VO2Max (or anything else, really), you can easily save it and look at it on the Apple Health graphs. The only way to get the Watch to stop messing it up with its daily hallucinated number is to turn off all fitness tracking (in the Privacy settings of all places). But then half the reason for having the watch goes away. reply MPSimmons 6 hours agorootparentprevHow often do you do metabolic stress tests? reply BenFranklin100 6 hours agorootparentprevAs the OP mentioned, absolute numbers are garbage, but trends are useful. Once a year just get an actual VO2Max test to peg the watch values too. reply mathieuh 6 hours agorootparentThe trends aren’t really useful either unless you are recording strenuous exercise with the watch. I do a lot of cycling and my fitness in terms of how much power I can put out and for how long and my average heart rate for the same power has increased vastly, yet according to Apple my VO2 max has stayed consistent at around 45 for years since I only record walking exercises with it. I guess there’s not much it can tell if you only hit the mid 80s bpm at most during an exercise. I realise it’s also another estimate but based on my cycling exercises Garmin is estimating my VO2 max at 55 and it does show a progression in my values. reply plorkyeran 6 hours agorootparentprevThe trends are garbage too. According to my watch my VO2Max has swung wildly between 55 and 45 over the last year, w",
    "originSummary": [
      "The article assesses numerous studies examining how cardiorespiratory fitness (CRF) affects mortality, chronic diseases, and overall well-being, underlining the significance of regular physical activity in maintaining good CRF.",
      "It stresses the robust predictive nature of CRF for various health outcomes, suggesting the routine measurement of CRF in clinical settings for health assessment and interventions.",
      "More research is necessary to address existing gaps in the literature and determine a causal link between CRF and health results."
    ],
    "commentSummary": [
      "Emphasis is placed on the significance of cardio fitness for overall health and longevity, recommending Zone 2 training for building aerobic fitness steadily.",
      "Consistent cardio at an easy pace can prevent injury, enhance endurance, and maximize benefits; varying routines and avoiding intensity mixing within the same session are crucial.",
      "The ongoing debate includes the effectiveness of wearing helmets while cycling, exploring the link between exercise, vitamin D levels, and mortality through controlled trials, underlining the need for a comprehensive approach to fitness and health."
    ],
    "points": 328,
    "commentCount": 357,
    "retryCount": 0,
    "time": 1714701501
  },
  {
    "id": 40240724,
    "title": "Crafting an Innovative Rocket Engine: The Journey of E2 Engine Development",
    "originLink": "https://blog.ablspacesystems.com/p/building-e2",
    "originBody": "Share this post Building a Rocket Engine from Scratch blog.ablspacesystems.com Copy link Facebook Email Note Other Discover more from abl Subscribe Continue reading Sign in Building a Rocket Engine from Scratch How I learned to stop worrying and love the burn. Ryan Kuhn May 02, 2024 Share The E2 Engine is a simple, robust, resilient machine. It starts up. It burns smooth. It shuts down. It’s tolerant to a variety of startup sequences and inlet conditions. Recently, a qualification E2 engine achieved a 4x life on total duration and starts. After 28 starts and 1300 seconds of run time, this fleet leader shows no signs of performance degradation. E2 wants to run, and it wants to run for a long time. It wasn’t always like this though; engine development is unforgiving. Thousands of decisions go into designing an engine, into architecting its sequences, and into building its test stands. When you go to start it up, they all need to be right. Otherwise, you’re likely to be exercising the test stand’s fire suppression system. I started the ABL engine program from scratch. Over the years, we have burned through hot walls, hard started chambers, and fine-tuned our turbomachinery. Along the way, we put together an excellent team of engineers and technicians. We built test stands and then we ran those test stands hard. We accumulated hours and hours of hotfire time across fifty different engines. We worked relentlessly to create a flight-worthy engine system and went from nothing to having ten flight-worthy engines tested and installed on a rocket in under four years. I’m excited to talk through the path we took to get there. Engines on Stages The E2 Engine You don’t get to a simple, robust, resilient engine by accident. The E2 engine runs on Jet-A and Liquid Oxygen, the most commonly available propellants in the world. It operates on the robust Gas Generator Cycle, and is powered by a single-shaft turbopump. The RS1 rocket uses E2 in three variants. Stage 2 uses the E2 Vacuum engine. Stage 1 uses E2 Sea Level Radial and Center. Center is a tightly-packaged double chamber variant of Radial mounted at the center of RS1’s aft end. Each generating over 16,000 lbf of thrust in vacuum, these engines are designed, manufactured, and tested in-house at ABL. My Background You might call me an unlikely candidate to lead an engine program from scratch, but I was hired by ABL in 2018 to do just that. My background was in commercial aircraft interiors, web development, semiconductor fab fluid components, and SpaceX Falcon 9 hydraulic systems. Rather than direct propulsion experience, I leaned on lifelong curiosity, mechanical intuition, and engineering practical solutions to a wide range of problems. In college I pursued the air and ground-based mechanical and aerospace engineering coursework, shunning classes on rocket engines and orbital mechanics. It wasn’t an exciting time for spaceflight. The Space Shuttle was heading towards retirement, and the remaining launch vehicles were primarily Russian and American rockets from the 1960s. Unless you were really paying attention, and I wasn’t, you knew nothing of the SpaceX Falcon 1 launches that were moving incrementally closer to orbit. ABL’s E2 Engine Radial Variant In 2015 SpaceX’s repeated attempts to land boosters finally did catch my attention, and I joined the Falcon 9 stage systems team alongside ABL founder Harry O’Hanley. I learned a lot about reusability and what it takes to land rockets on barges and on land. Unfortunately, I didn’t learn as much about engines. I was obviously aware of them and how they worked but didn’t spend much time in or around them. I recall a coworker talking about an engine performance-related question they had asked an interviewee, where they scoffed at the inadequate answer they received, and which I myself would not have been able to do better than. All that is to say, I joined ABL as a blank slate. This meant two things: 1, I had to do a lot of research to figure out where to start. 2, I did not have any predispositions about designing an engine that could hold me back. I was a team of one and I hit the textbooks, NASA monographs, and research papers on rocket engines to start wrapping my head around the problem. Design NASA monographs, describing in detail the design problems central to rocket engines and their components and their solutions engineered in the 1960s, are available to the world at nasa.gov. Reading through these, you end up with thousands of tidbits, ranging through rules of thumb, key ratios, materials that don’t work well in engines, materials that usually work well in engines, and so on. What they lack is a single best set of solutions for the specific type of engine you want to build, test, and operate. This is where the fun starts. Turbopumps Seal Test History Example from NASA Monograph (NASA-SP-8121) Most rocket engine designs do not start with textbooks and monographs. They often have a starting point, whether it be a previous generation of engine within the same company, a legacy technology demonstrator that was crated up after completion awaiting a new owner, or key components or IP purchased from another entity. At ABL we started from a true clean sheet. Some small components – seals, bearings, sensors – were purchased from vendors, but everything else was designed from scratch. And not only the engine itself, but the test infrastructure, the test software, and entire test sites we built from bare dirt. Since our trade space was completely open, we had to move quickly to make key early decisions to limit that space to avoid the paralysis that too many choices can induce. A few things seemed obvious when looking at industry examples and the high level understanding we had of engines and engine manufacturing technology. Example of a Clean Sheet First, engine cycle. All rocket engines combust propellants in a combustion chamber and accelerate the biproducts through a nozzle to generate thrust. The way the propellant gets to the combustion chamber is dictated by the type of engine cycle, each with its own advantages and disadvantages. These range from the pressure fed cycle, where the propellant tanks are held at a high pressure and propellant flows directly from them to the chamber, to the staged combustion cycle, where propellants flow through pumps and turbines and burn a bit of each along the way to self-power the pumps. We chose the gas generator cycle, which can provide moderate efficiency, and where each component can be tested and tuned on its own without requiring the others be present or even ready. With the cycle chosen we split out the four key areas to work on: 1. The turbopump, which takes propellants stored in the rocket at low pressures and pumps them up to the high pressures required to feed the engine’s combustion chamber. 2. The main combustion chamber. 3. The main chamber injector, which mixes the propellants coming into the chamber just so to ensure they mix and burn steadily and efficiently. 4. The gas generator, a small combustion chamber that provides pressurized hot gas to power a turbine that powers the turbopump. From Left to Right: Turbopump, Chamber + Injector, Gas Generator Before getting into any fine details, I gathered equations from textbooks and monographs into an excel spreadsheet that worked sequentially through sizing the requirements for each component. I later learned that this is called a “power balance” or “1D code” in industry. Basically, if you want X thrust, you must flow Y rate of propellants, and you must set the main combustion chamber exit to Z diameter. Parts of it were as simple as this, but when I added in the governing equations for turbopump impeller sizing it became fairly complicated. Time to get into detailed design. To divide up the work, I took the turbopump and main injector, and the other two engineers on the team took the main combustion chamber and gas generator. Turbopump Early Production Turbopumps I dug in on the turbopump, a frightening device spinning at about 50,000 RPM and operating with a complex system of fluid passages and bearings and impellers and turbines. A typical turbopump boosts propellants from around 50 psi to 2000 psi, while pushing many gallons per second into the combustion chamber. By contrast, a Formula 1 race car fuel pump also pumps up 1000s of psi of pressure, but flows less than one gallon per minute. If you look at the cross section of a typical turbopump (for example, the F1 turbopump from the Apollo missions), you’ll see the expected impellers and turbines but will quickly get lost in a sea of complex secondary features such as slingers, balance pistons, labyrinth seals, recirculation channels, multi-stage turbines, bearing coolant systems, and so on. And even for the impellers and turbines, a 2D cross section highly simplifies them, as their sweeping, spiraling blades are one of the most 3D, organic looking features in a rocket engine. F1 Turbopump Cutaway (Rocketdyne R-3896-1A), Credit: NASA I can’t go into the exact details of how our turbopump design shook out, but the general idea was to not add a feature until we were convinced it was needed. Keep it as simple as possible as long as possible. The 1D excel code took impeller and turbine design as far as they could go on paper, spitting out speeds, inlet and outlet sizes and blade angles, and expected efficiencies. There is a big gap between the end of the equations and the gracefully shaped blades of final designs, which is spanned by specialized software. While on the phone with an engineer who was demoing the software we ended up purchasing, I bluntly asked “if we have a set of inlet requirements and exit requirements for an impeller, why can’t the software just give us the best answer?” Using the software required a huge amount of tweaking of parameters and manually adjusting curves until things looked just so. He answered that you don’t know exactly how a design is going to perform until you do time-intensive computational fluid dynamics modelling, so you have to design and iterate based on your experience and intuition. Ultimately, impeller and turbine design is part equation, part rule of thumb, with a sprinkling of art. Wild, for something that I had previously perceived as a triumph of engineering. Injector I interspersed pump design and part procurement with work on the injector for our main chamber. Since I had significant valve design experience, we decided to go with a pintle architecture. Most engine heritage uses showerhead or impinging jet type injectors, where an array of 100s of small holes in the injector spray propellants in a way that they collide with each other and atomize into small particles that burn quickly and efficiently. These designs, however, require a network of complex passages within the injector, and precisely controlled injection hole diameters, angles, and placements. This adds up to an expensive, manufacturing-intensive process, and as far as we could tell from the resources available then, 3D printing was not an appropriate solution to make it simpler, as it could not provide the precise dimensions and surface finishes required. Pintles require none of this – they inject propellants in two “sheets,” one axial, one radial, that collide with each other to provide the atomization needed for good combustion and are designed and manufactured much more like valves than not. It also allowed us to simplify packaging and reducing the total number of parts we had to work with. Main Chamber 3D Printed Main Chambers Main chamber design was another interesting exercise in theory meets reality. We narrowed our design trade space quickly by deciding we would 3D print our chamber and would do so using machines and materials that were well-distributed and understood – nothing cutting edge. This set the maximum size for our printed parts, and set our material as Inconel. Inconel is a nickel superalloy initially developed for use in jet engines. It is very strong, very heat tolerant, very weldable, and was readily available in 3D printers around the country. It is also difficult to machine and has poor thermal conductivity, two challenges we had to take on and live with. The crux of rocket engine combustion chambers is that the combustion inside them reaches about 6,000°F (flirting with the surface of the sun at 10,000°F), and even the best metals get quite soft at 1,200°F and melt at 2,500°F. There are a few ways to make a chamber survive this, but the most common is to run a portion of the propellants through the walls of the chamber to cool it. This sets up an interesting thermal management problem. You have to make the inner wall of the chamber as thin as possible to ensure the cooling effects of the flowing propellant can penetrate it, but not so thin that it ruptures under pressure. You have to make cooling channels in the wall narrow so the propellant flows at high velocity and cools the wall more effectively, but not so narrow that they generate excessive backpressure in the system that requires a huge amount of effort from the turbopump to overcome. You have to let the propellant suck as much heat out of the wall as possible, but not so much that it boils or breaks down. The combustion chamber engineer wrote a slick code that could optimize all of these parameters in a continuous sweep down the length of the combustion chamber, spitting out geometry that he could directly port over to 3D modeling software that would in turn feed the 3D printers. We made a few key assumptions and locked down a design so we could get into test and adjust, if needed. Five years later, we continue to be pleased with the outcome of this sizing. To this day we have not changed from that original chamber cooling solution. The design just works. Build the Hardware One of the most energizing moments during a product development cycle is when the parts you have been designing on a computer screen for months finally show up in person. There’s a general rule for when you first see your parts – they are either larger or smaller than you were picturing them, never exactly how you imagined. Most of our early parts were printed and machined by aerospace manufacturing shops from around the country. These shops are a special type, taking on high-value, low-quantity jobs, and delivering them rapidly to support the pace required by the industry. We had a handful of key partnerships and owe a lot to them. Seeing our early engine parts in person was awesome. First a small print of a chamber section to prove out thickness limits, then a GG, then chamber sections and injector parts. It was feeling very real. The First Development Injector An outlier was the impellers and turbine for our pump. I knew they would not be the kind of parts just any machine shop would take on, and dug around and found a few that specialized in making them. When I received quotes back, it was wake up time. The impellers and the turbine were each quoted at around $18k – more than a small car! Even worse, the lead time was four months. While the cost was high, the lead time is what was ultimately untenable. We were certain to have to go through a few revisions to these designs before they were ready to fly, and adding a four month cycle to each of those revisions would drag out the overall pump development time beyond what was reasonable for a startup trying to move quickly and prove itself. It was time to verticalize. We leased our first 5-axis mill and hired a machinist. These parts were made from hard metals and had small intricate features. The machining wasn’t easy. I think we spent more than the outsourced quotes in broken end mills on the first set. But over time we got better, due to advances in our machining and advances in the design. Machining Our First Impeller At one point, I received a well-deserved earful from our machinist about how tightly spaced the turbine blades were. The program was taking nearly a month to run and required tiny end mills that broke often. We performed a turbine blade count study to see if we could use fewer blades with more space between them. It turned out the performance impact of running fewer blades was minimal, so we cut the number down, allowing our machinist to use larger, less fragile tools. Machine time dropped to less than a day, which was a significant win for turbine cost and machine time. It was also a good lesson in thinking comprehensively about a design’s manufacturability (those passages between the blades looked so big on the computer screen!) in addition to its performance. A Close Up of E2’s Turbine We can now produce impellers and turbines in a matter of days and at a fraction of the cost. There’s an example of the benefit of this that we reference often in tours and when recruiting engineers to our propulsion team. At one point during early pump testing, we kept running into performance issues with our fuel impeller. It wasn’t grabbing the incoming flow effectively and was giving our engine unpredictable performance. While we could combat this and work with it on the test stand, we ultimately concluded that it wasn’t going to work for flight. From the point of making that conclusion, through redesigning the impeller, getting it into the machine shop, getting it built into a pump and balanced (a step in every pump build to ensure everything spins smoothly), and getting the pump back into test to prove out the design change, it took us 10 days. If we had not brought that capability in house, we would have been stuck with either a multi month delay or with transferring the impact of this performance issue into other rocket systems or into the performance of the rocket itself. Big win. Production turbines - what once took weeks, now takes hours We’ve continued to verticalize where it has made sense. We now operate a handful of 3D printers in house, have numerous 5 axis mills and multi-axis lathes. Our turbopump rotors are balanced in house. Many of these processes or technologies seemed daunting when we first learned of them or saw them performed by another party. But once in house, you learn, you do it over and over, and it becomes routine. During my time at SpaceX we saw landing rockets go from insane to routine. The same has happened here at ABL, first for building engines and then for testing them. ABL Machine Shop Build the Team Building an effective team is often more challenging than building a working engine. We’ve strived to stay as small as possible as long as possible, being very thoughtful about who we bring onboard and when. Part of ABL’s philosophy is to produce more than we consume. As we strive to do more with less, we’ve often found less enables us to do more. We started the propulsion program in 2018 with two engineers (myself included). We were a team of five for the first two years of development - all the way through running our first fully integrated engine. Today, it’s a team of fifteen. We’ve made hires that worked and ones that didn’t. We’ve made new grad hires and experienced hires, from inside the industry and outside. We’ve found a few traits to be key indicators of success, which are influenced by our simple engine architecture that allows first principles to prevail: Engineers who don’t stay behind a desk all day, who love getting their hands on hardware, being in the field, helping with tests are almost always more effective than those who do not. Where engineers bring experience, they don’t over-focus on it. They allow their experience to be a guide, a part of the puzzle, not the whole answer. An engineer may own a few specific parts of an engine, of the rocket. They will be most successful if they have a comprehensive understanding and interest in how those parts fit into the bigger picture. How they effect the teams interfacing with them, the build, operation, and performance of the rocket. The best engineers never sit on what they think is right, they quickly take action or they speak up regardless of org structure or seniority. Overall, the best indicator of success is a strong foundation of mechanical and fluid dynamic intuition. From there, it’s easy to fill in the gaps. We expect a lot of each engineer and they all work incredibly hard. And because the team is small, we give each the opportunity to experience real ownership of a large part of the design, build, and test of rocket engines. Test and Iterate We started our first E2 test campaign in summer 2019, a just under a year after we started engine design. Our first test site was on a flat concrete pad at Spaceport America in New Mexico. There we set up our first test stand which took us through pressure-fed (no turbopump, yet) testing of first our GG and then our thrust chamber. In this test campaign we accomplished a few things and learned a few things. We deployed to an austere location, we ran our first combustors, we lit with TEA-TEB (the pyrophoric liquid that is used to start many rocket and jet engines), and we worked with cryogens. We learned that snap rings don’t work well inside of combustion chambers and we learned that pintles melted easily and weren’t the slam dunk we thought they would be. In 2020 we deployed to site 1-56 at the Air Force Research Laboratory (AFRL) near Edwards Air Force Base, where we again set up our pressure fed test stand but also a development rocket tank for pump-fed testing. Having the opportunity to operate at this site significantly accelerated our development program. At AFRL we ran our first turbopump, and it pumped! We had some challenges. We melted turbines and we had power instabilities, but we learned and adapted. Changes were made to the test stand to make the engine more controllable, changes were made to the turbine exhaust to make it more stable, and changes were made to the turbopump to make it more efficient. Turbopump Testing at AFRL In the time between the testing at Spaceport America and AFRL we designed and manufactured a new version of our injector that didn’t use a pintle. Since we had shown we could make a chamber that worked and a GG that worked we were no longer as concerned about other injector types, and took a leap on a manufacturing method that was much less arduous than the traditional method. It wasn’t that we didn’t think we could make the pintle work eventually – I’m confident we could have – it was that we knew this other type of injector was almost certain to work right away. And it did. And we have not changed it since. One of the biggest accomplishments at AFRL was running a fully integrated engine, which we did on our Stage 2 development tank. We had a fully integrated engine, with a pump, GG, and TCA closing the loop and running under their own power. We were in the game. Integrated Engine Testing on Stage 2 Tank 2021 was dedicated to building out a brand new test site in Mojave, CA and getting into testing there. As we did, we implemented a new round of upgrades to the turbopump and matured the design of everything around it on the rocket. At the end of 2021 we started the testing of engines for Flight 1, a campaign that looked dramatically different for us. We had multiple test stands, we were running many, many engines, and we were running them for full flight durations. All of a sudden our total engine run time was measured in thousands of seconds rather than tens of seconds. And they were working. In 2022 we made thrust upgrades to the engine to give us more power. We also kicked off the build of a new engine test site, dedicated for production testing. We would soon have the ability to run engine development and production test campaigns entirely in parallel. In 2023 we repackaged the same engine components into a configuration that was more modular, making it even easier to build and test. More recently, we’ve optimized different key features of the engine, such as the TEA-TEB system to enhance reliability and long term performance. Production and Qualification Engine Testing Looking Forward We’ve built 50 individual engines and operated them on 6 different test stands across 3 sites. These units have racked up many hours of hotfire time and hundreds of starts. E2 iterative development is not complete, and it may never be. There will always be small improvements to manufacturing, performance, mass, or cost to make. We’ve learned a ton and will continue to do so. Problems that seemed like existential crises at the moment look small in retrospect. You assess, you solve, you move on, you don’t make the same mistake again. We’ve had print powder in pump bearings, underperforming volutes, underperforming impellers, melting liners, melting turbines, melting manifolds, melting tubes, chugging pumps, unstable gas generators, countless leaking seals and hard starts. Each solved problem makes our engineers stronger, makes our engine stronger, makes our company stronger. You can only learn and solve problems like these when you test. Our biggest misses have come from not continuing to test when we didn’t absolutely have to, and pushed uncovering issues until further down the line where they were more impactful. Just like when running up a hill in a race, don’t stop and rest when you get to the top, keep the pedal down and get over the crest. Looking forward, we are continuing to build out our team with a blend of talented generalist engineers and propulsion engineers. It is awesome to see the assumptions and institutional knowledge we have built up over the last 6 years both leveraged and challenged. So far the results have been great, and I am excited to be a part of it. E2 Ignition Testing Thanks for reading abl! Subscribe for free to receive new posts and support my work. Subscribe Share",
    "commentLink": "https://news.ycombinator.com/item?id=40240724",
    "commentBody": "Building a rocket engine from scratch (ablspacesystems.com)251 points by sr-latch 14 hours agohidepastfavorite50 comments buescher 11 hours agoNASA's report server is a national treasure, especially the material from the 50s and 60s that he references. Some of the most crisp and succinct technical writing you'll find, and you can infer a lot about how they ran projects. Declassified NRO reports are also very good - you can see the Lockheed Skunk Works principles in action. Example: https://www.nro.gov/Portals/135/documents/foia/declass/WS117... reply mhh__ 8 hours agoparentLots of academic and engineering things from back then are great. I have an incomplete set of the radlab textbooks. They're still useful even today, and have a really careful pace to them because they were written for a generation for whom electricity was still relatively new. One of the things that is a real shame is just how artfully bound those old books are. Leather, thick but smooth paper, etc. reply duped 10 hours agoparentprevIf there's two things I've learned about chemistry from ExplosionsAndFire in YouTube it's - Yellow is bad - The 60s were awesome reply dotancohen 1 hour agorootparentJust yesterday I pointed out a yellow flame to someone as evidence of incomplete combustion on the gas stove. Laughing I showed them your timely comment. Out of context of the thread, your second point was amusingly interpreted to be about LSD. reply sr-latch 3 hours agoparentprevand if you're ever writing 1DoF codes yourself, NASA's CEA for propellant chemistry calculations is freely available https://cearun.grc.nasa.gov/ reply Horffupolde 11 hours agoparentprevEnglish is great for technical writing. reply adrianN 6 hours agorootparentI think English has too many ambiguities to be well suited for technical writing. There are efforts like https://en.wikipedia.org/wiki/Simplified_Technical_English but they don't have a lot of adoption. reply trhway 3 hours agorootparentIn USSR/Russia we had a stand-up comedian (Zadornov) who in one of his jokes was explaining how the conciseness of the language is advantageous on the battlefiled. Recently I had to write in Russian (my native language) a tech doc of many pages. That is after 26 years of tech writing only in English (my 2nd language). That experience resulted in an epiphany for me on why Russia is so technically behind as I suddenly understood what Zadornov was talking about :) In comparison to English, Russian sucks, to say the least, as a technical language. All the things which make Russian great for literature and poetry make it terribly inefficient for tech writing, miles and miles behind English. If anything, I think English is the secret sauce of the success of the Western technical civilization. (For older Russian speakers - if you remember another Zadornov's \"dolbani pljuhalkoj po kuvykalke, prikin'sja vetosh'ju i ne otsvechivaj\" - now I think it really comes from the same weakness of Russian for technical communication) reply z3phyr 44 minutes agorootparentTh soviet mechanical and rf engineering efforts were very well regarded during the early cold war period, and I would argue, much more sophisticated. Language did not stop the USSR. It was the economic policy like banning some disciplines based on ideological grounds like cybernetics etc. Check it out, the things for which a western scientist or engineer is given credit, were discovered/invented much earlier by soviet counterparts! What do you think about Kotelnikov? reply beambot 9 hours agorootparentprevIt probably helps that you have to be exceedingly thoughful about every keystroke & word less you need to completely retype the document or page! reply mhh__ 8 hours agorootparentI'd have imagined you wrote a draft, corrected with the red pen, and then sent it off to (presumably) an army of women to do it properly. reply scoot 9 hours agorootparentprev\"lest\" perhaps? At least you don't have to retype the whole sentence. ;) reply cuSetanta 23 minutes agoprevOh wow, I work for a supplier for ABL and am today in the process of putting some of their stuff into our thermal chamber for cycling. Thats neat. We work for a lot of launcher companies, but ABL is the most interesting for me (even though we do relatively little for them). The containerised approach to the entire system is a really clever adaptation of existing methods to create a rapid launch system. reply datadrivenangel 13 hours agoprev\"At one point, I received a well-deserved earful from our machinist about how tightly spaced the turbine blades were. The program was taking nearly a month to run and required tiny end mills that broke often. We performed a turbine blade count study to see if we could use fewer blades with more space between them. It turned out the performance impact of running fewer blades was minimal, so we cut the number down, allowing our machinist to use larger, less fragile tools. Machine time dropped to less than a day, which was a significant win for turbine cost and machine time. It was also a good lesson in thinking comprehensively about a design’s manufacturability (those passages between the blades looked so big on the computer screen!) in addition to its performance. \" Once again people learn the hard way that it's valuable to have tight feedback cycles and embedded knowledge on your team. reply FredPret 13 hours agoparentAnd also the value of a completely obsessed engineer. If the mech eng designing the parts is also the type to build things in his spare time, this type of machining problem would stand out right away. Of course, not everything can ever be anticipated, so tight feedback loops are fantastic when you can get them. reply numpad0 12 hours agorootparentI'd agree with this take more. It's just cheaper and faster to just know whys and plan ahead than waiting for failures, if you could. I've been tinkering with 3D printers long enough that I've trivially halved print times after seeing long print time predictions. Lots of my brackets and parts fits together on first try and needs no supports. Rapid turnaround did help in _acquiring_ those skills, but now I could just waterfall little projects and tight looping is not that important. reply psunavy03 12 hours agorootparentprevValuing \"completely obsessed\" employees is a great way to build a company with toxic burnout culture. Some people want lives outside of work and are still good at their jobs. reply quartesixte 11 hours agorootparentOr some people are so good at their jobs that even while being completely obsessed, they still have great work-life balance. I interact with a lot of people like the author in the article from this industry. And while yes, their worklife balance skews \"work\", they all have interesting hobbies and find time to hang out with friends, go on adventures, and live a pretty full life. Unless you're on the Starship team at SpaceX. Truly, hats off to y'all at Boca Chica some intense hours you are pulling there. reply nothercastle 8 hours agorootparentEmployees who are married to their job are the best employees. Just like in a real relationship you can treat them like shit and they mostly stay married reply guynamedloren 11 hours agorootparentprevThis may surprise you, but it's possible to be completely obsessed with your work (while you're working!), while also maintaining a work-life balance. reply psunavy03 11 hours agorootparentIt's also a great excuse for abusing people. reply mdorazio 5 hours agoprevGreat post on building extremely complex hardware from scratch. At the same time I hate to be that business guy, but both this blog post and the abl site are missing a good answer to my first question: Why? Given that SpaceX exists and is quickly approaching feasibility of Starship on top of Falcon, what is the primary goal of this rocket system? How will it compete? Who will its customers be? Is it getting its metric ton payload to orbit faster/cheaper/easier? Is this \"from scratch\" engine design superior to existing designs in some way? What is its current ISP? Is Jet-A + LOX a better fuel choice given expected mission parameters? I'd love to see a blog post that tackles these kinds of questions. reply choeger 3 hours agoparentFrom the outside: Diversification is always great. Build a whole ecosystem of small-scale rocket manufacturers instead of one big monopoly. That will foster competition and innovation. From the investor: SpaceX might fail. Even if there Falcons are pretty much unbeatable now, you don't know what's going to happen with Starship. And even the Falcons could conceivably be grounded for years after some hypothetical flaw is found. More likely: With the price reductions made by SpaceX, the market will grow and there will be more than enough clients. From the inside: Because it's a fun challenge and literally rocket science, of course. reply avmich 4 hours agoparentprevSpaceX is certainly giving a hard time to its competitors, but it doesn't mean they shouldn't exist. Some of them may actually follow the same route, designing reusable hardware and reducing launch costs. SpaceX took 20 years to become relying on robustly reusable system; maybe some other companies reach a similar state sooner. reply AndriyKunitsyn 12 hours agoprevNice read. The fact that it is possible to 3d print metal parts that withstand temperatures and pressures of a rocket engine is so exciting. How expensive is it? reply generuso 10 hours agoparentThe material costs (the $300 per kg of titanium suggested in another comment) are only a small part of the overall expense. Electron beam sintering printer time typically costs $100-$200 per hour, and a large build can easily take multiple days. After the printing itself, one will have to remove to loose powder, which for small cooling channels in the walls of the combustion chamber is very challenging and time consuming. After that, some post-processing may also be necessary. One process for achieving the greatest strength is hot isostatic pressing, when the part is baked in a furnace in a retort filled with a very high pressure inert gas. Specifically for rocket engines, it is also desirable to have a layer with the high heat conductivity on the inside, typically made of a copper-based alloy, and the external structure from a higher strength material. This means either bi-metallic printing, which is a rather niche process, or some metal deposition process over the printed part. In addition to this, there is usually quality control, for example, high resolution industrial computed tomography, to make sure that the invisible internal features have been fabricated and cleaned out correctly. In addition to the additive steps, it will also be necessary to machine the features which are impractical or impossible to build sufficiently accurately. Together, these steps add to a significant cost. Some of the above processes can be seen in this video: https://www.youtube.com/watch?v=7pXEf0wHU1Y reply WJW 12 hours agoparentprevMostly depends on the volume of the part, or equivalently it's weight. Complexity you get mostly \"for free\" when it comes to 3d printing. What type of material you need to \"withstand temperatures and pressures of a rocket engine\" is entirely dependent on which part of the rocket engine we're talking about. A fuel injector has radically different requirements than a supporting strut for example. 3d printed titanium goes for 300-400 USD/kg, steel is a bit cheaper at ~150 USD/kg for most inconel grades. reply avmich 12 hours agoparentprevPaul Breed, from Unreasonable Rocket team - https://x.com/unrocket - mentioned, a decade or so ago, that he printed some aluminum engines for regenerative cooling by hydrogen peroxide for ~$1000 . Another story from http://rocketmoonlighting.blogspot.com/2010/ is about a small engine cooled with nitrous oxide - and manufactured entirely on personal money, also quite some time ago. I think these numbers are still indicative of the current prices. reply quartesixte 11 hours agorootparentAluminum is magnitudes cheaper than Inconel. And since volume is cubic, and pricing mostly on based on powder weight, you are about an order of magnitude or two off for the size of engine ABL is producing here. reply avmich 11 hours agorootparentThat's correct, but it's still interesting that you have that opportunity to shave - slash? - costs here. And also the other engine mentioned isn't an aluminum one. reply aaronblohowiak 7 hours agorootparentprevWould $100,000 still be a “steal” for all that? (Especially with repeatability and nimbleness) ? reply quartesixte 12 hours agoparentprevVery. Very. Expensive. Inconel powder is also Not That Great for your health and at the particle-size the printers rocket companies use, you need full PPE to safely handle the loose powder floating about. The machines themselves are also expensive. Think in the millions of USD. EOS, SLM, and Velo3D are key players in this market. They require a fair bit of space, and training to use correctly. You probably need a mechanical engineer who is well-versed in materials science and has a tolerance for finicky machines that constantly breakdown. Then you have the metal powders. Which, also potential million or two. And then you have all the associated infrastructure needed. High voltage power. Gas (Nitrogen, Helium, Argon, etc etc) in the thousands of liters per month. Waste disposal. Safety (some alloys are flammable in their powder form). Climate control (the powders are sensitive to the environment. Humidity will quickly destroy your powder supply). Tooling (the base-plates metal printers used are machined from solid blocks of steel). And last but not least, any of the post-printing work. Heat treat. Coatings. Analysis. CNC Machining. 3D Printing metal on industrial scales is a CAPEX intensive endeavor, and not for the faint of heart. reply jfoutz 12 hours agoparentprevStratasys? I’m not sure about pricing, and the website won’t say. However, a bunch of places rent out time on those machines. Draw up your rocket, and get a quote. Price is generally cc^2 volume Metal is not cheap. Make a few out of plastic to verify dimensions. reply quartesixte 12 hours agorootparentPlastic to verify dimensions is a great approach. But don't forget that the laser sintering of metal powders might result in design constraints not present in plastic printing! reply spxneo 12 hours agoparentprevside question to this: where can i design stuff involving metal parts (presumably in CAD tools) and have it printed en masse? With PCBs? ex) car components reply numpad0 11 hours agorootparentYou can draw up mechanical components in Autodesk Fusion, OnShape, SOLIDWORKS for Makers, or FreeCAD, and send STEP or STL to PCBWay or JLCPCB in China for manufacturing(note that export restrictions may apply if it's literal rockets or otherwise dual-use/controlled in nature). PCB mounting outlines can be exported from above 3D CAD and imported to EDA tools such as Altium and KiCAD; KiCAD is fine unless you're doing DRAM or PCIe. Same PCBWay and JLCPCB takes your design, and optionally assemble PCB with parts for you. That should take you to first 2-3 working units at ~$500 and up to few dozen beta units with zero initial cost and much inflated unit costs, and I guess beyond that involves significant human resourcing and networking problems outside of PoC hardware scope. reply spxneo 11 hours agorootparentthank you so much for this detailed answer. now i just need to invest some time into learning CAD and KiCAD reply Robelius 11 hours agorootparentYou could try a battery powered phone charger since it's a \"relatively simple\" first project. The big hurdle for learning these types of tools is usually \"What buttons do I press to create the output that I want. For the electrical side, there are plenty of schematics online that you can try to copy or use as a starting point. And the CAD side can be a simple box with snap fits. I'd recommend OnShape if you're just starting out since it's the lowest barrier of entry, but Fusion 360 is also good. All in, it should be <$150 for the PCBs + Components + 3D Prints. After you get the satisfaction of seeing your device charge from something you made, then you'll start getting the itch and find more excuses to make things. reply spxneo 10 hours agorootparentI'll follow your recommendation and try the simple stuff. Looks like OnShape is right up my alley. All very exciting feels like I'm \"programming hardware\" ! reply quartesixte 10 hours agorootparentAnd once you are done, services like Xometry allow you to print out the metal by having them source a printing vendor from their network. reply z3t4 4 hours agoprevThe science of pressure chambers have also advanced, we could just pump whatever material, like liquid air into a pressure tank and then load it into the rocket. No mixing or pumping needed just open the valve and let the pressure out and you will have a very cheap and simple rocket. reply sr-latch 2 hours agoparentThis is absolutely not true - injector design is the most important aspect of designing a thrust chamber. Poor mixing of propellants leads to severe combustion instability, which often leads to explosions. Even the earliest space programs did significant testing on propellant choices and injector designs (see Ignition! by John D. Clark) Also, pressure fed rockets have always been a fairly terrible design. Pressure feeding requires heavy tanks, and incurs a big mass fraction (dry mass / wet mass) penalty. Outside of rare cases, it's only used for ground testing. reply z3t4 1 hour agorootparentjust drop the tank when the weight/thrust ratio is too low. Thanks for the book suggestion! (have not read it yet) reply avmich 12 hours agoprevAwesome story, thanks! It seems the design choices are rather conservative, which is entirely justified by the \"from scratch\" part for the first engine. I'm sure subsequent designs will be more bold and adventurous. Keep up great work! reply spxneo 12 hours agoprevnow all that is required is a github for passive radar...for educational purposes reply sr-latch 3 hours agoparenthttps://github.com/krakenrf/krakensdr_docs/wiki/ The SDR world is incredible reply brcmthrowaway 12 hours agoprev [–] Can you use ChatGPT to build a Saturn V? reply WalterBright 4 hours agoparentIf it's like some code I asked ChatGPT to generate, it will blow up on the pad! reply trollerator23 10 hours agoparentprev [–] You can try! reply brcmthrowaway 8 hours agorootparent [–] I would get sent to jail for breaking ITAR reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The E2 Engine created by ABL is a durable rocket engine powered by Jet-A and liquid oxygen.",
      "Despite lacking direct propulsion experience, Ryan Kuhn effectively led the program by prioritizing curiosity, intuition, and practical engineering solutions.",
      "The article details the design process, with a focus on the gas generator engine cycle, turbopump design, challenges faced in developing engine components like combustion chambers and impellers, and the team at AFRL making notable progress in engine development."
    ],
    "commentSummary": [
      "The discussion on abslacesystems.com delves into diverse topics like building a rocket engine, technical writing, Soviet engineering feats, project feedback loops, SpaceX challenges, 3D printing for rocket parts, and metal part design using CAD tools.",
      "It explores pressure chambers for rocket propulsion and critiques using pressure tanks for a low-cost rocket design.",
      "The conversation shares recommendations on software tools, manufacturing firms, and beginner tips in these areas."
    ],
    "points": 251,
    "commentCount": 50,
    "retryCount": 0,
    "time": 1714679688
  },
  {
    "id": 40243168,
    "title": "Developing \"sqlite-vec\": A New Vector Search Solution in SQLite",
    "originLink": "https://alexgarcia.xyz/blog/2024/building-new-vector-search-sqlite/index.html",
    "originBody": "Alex Garcia's Blog Change theme I'm writing a new vector search SQLite Extension 2024-05-02 by Alex Garcia tl;dr — sqlite-vec will be a new SQLite extension for vector search, replacing sqlite-vss. It will be an embeddable \"fast enough\" vector search tool, that can run anywhere SQLite runs - including WASM! It's still in active development, but check out the repo to learn when it will be ready! I'm working on a new SQLite extension! It's called sqlite-vec, an extension for vector search, written purely in C. It's meant to replace sqlite-vss, another vector search SQLite extension I released in February 2023, which has a number of problems. I believe the approach I'm taking with sqlite-vec solves a number of problem it's predecessor has, will have a much nicer and performant SQL API, and is a better fit for all applications who want an embed vector search solution! What sqlite-vec will be sqlite-vec will be a SQLite extension written purely in C with no dependencies. It will provide custom SQL functions and virtual tables for fast vector search, as well as other tools and utilities for working with vectors (quantization, JSON/BLOB/numpy conversions, vector arithmetic, etc.). A quick sample of what vector search will look like with sqlite-vec, in pure SQL: .load ./vec0 -- a \"vector store\" for 8-dimensional floating point numbers create virtual table vec_examples using vec0( sample_embedding float[8] ); -- vectors can be provided as JSON or in a compact binary format insert into vec_examples(rowid, sample_embedding) values (1, '[-0.200, 0.250, 0.341, -0.211, 0.645, 0.935, -0.316, -0.924]'), (2, X'E5D0E23E894100BF8FC2B53E426045BFF4FD343F7D3F35BFA4703DBE1058B93E'), (3, '[0.716, -0.927, 0.134, 0.052, -0.669, 0.793, -0.634, -0.162]'), (4, X'8FC235BFC3F5A83E9EEF273F9EEF273DA4707DBF23DB393FB81EC53E7D3F75BF'); -- KNN style query goes brrrr select rowid, distance from vec_examples where sample_embedding match '[0.890, 0.544, 0.825, 0.961, 0.358, 0.0196, 0.521, 0.175]' order by distance limit 2; /* rowid,distance 2,2.38687372207642 1,2.38978505134583 */ Using sqlite-vec means using pure SQL, just CREATE VIRTUAL TABLE, INSERT INTO, and SELECT statements. This work is exciting - for many reasons! First off, \"written in pure C\" means it will be able to run anywhere. The previous sqlite-vss extension, which had some cumbersome C++ dependencies, was only able to reliably run on Linux and MacOS machines, with binaries in the 3MB-5MB range. By contrast, sqlite-vec will run on all platforms (Linux/MacOS/Windows), in the browser with WebAssembly, and even smaller devices like mobile phones and Raspberry Pis! Smaller binaries too, in the few 100's of KB range. Additionally, sqlite-vec has more control over memory usage. By default, vectors are stored in 'chunks' in shadow tables, and are read chunk-by-chunk during KNN searches. This means you don't need to store everything in RAM! Though if you do want in-memory speed, you could use the PRAGMA mmap_size command to make KNN searches much faster. And finally, sqlite-vec is built in a new \"era\" of vector search tooling and research. There will be better support for \"adative-length embeddings\" (aka Matryoshka embeddings), and int8/bit vector support for binary and scalar quantization. This means more control over the speed, accuracy, and disk space that your vectors take up. Though initially, sqlite-vec will only support exhaustive full-scan vector search. There will be no \"approximate nearest neighbors\" (ANN) options. But I hope to add IVF + HNSW in the future! Demo Enough yappin' let's see a demo sqlite-vec is running right now in your browser! If you open up devtools, you'll see an (un-optimized) 5.9MB sqlite3.wasm file, which is the official SQLite WASM build with sqlite-vec compiled in. There is a movies.bit.db SQLite database also loaded, which is a 2.6MB SQLite database, which has this movies dataset with 4,800 movie overviews in the articles table. The separate vec_movies virtual table is a vector index of those \"overviews\" embedded, with Nomic's 1.5 embeddings model, quantized to binary vectors. Here's a quick sample of what the data looks like: Here we see the articles table has columns like title, release_date, and overview. The overview column here is important - it's a very short sentence describing the plot of the movie. We also have the vec_articles virtual table, which stores embeddings of the articles.overview as the overview_embeddings column. Thy are binary vectors with 768 dimensions, which takes up 96 bytes (768 / 8 = 96). Now let's see how a KNN-style search works! Here's a lil' table select component. Select a movie with the radio button on the left-hand side. The movie ID you selected will now pre-populate the :selected_movie SQL parameter in this KNN SQL query! Those are the 10 closest movies to the one you selected! The \"closest\" one (using hamming distance, because it's a binary vector) will always be the same movie, with a distance of 0. Keep in mind, embedding a single-sentence plot description of a small movie dataset doesn't give the best results (and binary quantization sacrifices even more quality), but the core idea remains. Fast, \"good enough\" vector search, in your browser! More docs about this KNN-style query will come soon, but in case you wanted to poke around at the internals, try adding a EXPLAIN QUERY PLAN to the beginning of the SELECT statement. You'll see the 0:knn \"index\" that vec_movies uses. But what's wrong with sqlite-vss? I won't go into all the details, but there were a number of roadblocks in the development and adoption of sqlite-vss, including: Only worked on Linux + MacOS machines (no Windows, WASM, mobile devices, etc.) Stored vectors all in-memory Various transaction-related bugs and issues Extremely hard and time-consuming to compile Missing common vector operations (scalar/binary quantization) Nearly all of these are because sqlite-vss depended on Faiss. With a lot of time and energy, some of these issues could maybe be solved, but many of them would be blocked by Faiss. Given all this, a no-dependency and low-level solution seemed really enticing. Turns out, vector search isn't too complicated, so sqlite-vec was born! Still not ready, but soon! The core features of sqlite-vec work, but I have very little error handling + testing. I have 246 TODOs in the sqlite-vec.c file, which I'm tracking with a lil script: $ make progress deno run --allow-read=sqlite-vec.c scripts/progress.ts Number of todo_assert()'s: 191 Number of \"// TODO\" comments: 41 Number of todo panics: 14 Total TODOs: 246 ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ (0/246) 0% complete to sqlite-vec v0 Once those 246 TODOs are completed, then the first v0.1.0 of sqlite-vec will be released, with documentation, demos, bindings, and more! I'm aiming for a month or so, but we shall see! I'm looking for sponsors! Is your company interested in the success of sqlite-vec? I'd love to chat! Email me for more information.",
    "commentLink": "https://news.ycombinator.com/item?id=40243168",
    "commentBody": "I'm writing a new vector search SQLite Extension (alexgarcia.xyz)239 points by sebg 8 hours agohidepastfavorite43 comments alexgarcia-xyz 5 hours agoAuthor here — happy to answer any questions! This is more of a \"I'm working on a new project\" rather than an official release, the extension itself is still a work-in-progress. A link to the project: https://github.com/asg017/sqlite-vec I have a pretty specific vision of what v0.1.0 of this extension will look like, but it'll take a few more weeks to get there. This blog post was more for letting users of sqlite-vss (a previous vector search SQLite extension I wrote) know what will be coming next. There will be a much bigger release when that is ready. But in general, I'm super excited to have an easy embeddable vector search alternative! Especially one that runs on all operating system, in WASM, mobile devices, Raspberry Pis, etc. I personally I'm trying to run a lil' semantic search app on my Beepy[0], which is a ton of fun to play with. [0] https://beepy.sqfmi.com/ reply lsb 5 hours agoparentWould this implement indexing strategies like HNSW? Linear scan is obviously great to start out with, and can definitely be performant, especially if your data is in a reasonable order and under (say) 10MB, so this shouldn't block a beta release. Also do you build with sqlite-httpvfs? This would go together great https://github.com/phiresky/sql.js-httpvfs reply alexgarcia-xyz 5 hours agorootparentThe initial v0.1.0 release will only have linear scans, but I want to support ANN indexes like IVF/HNSW in the future! Wanted to focus on fullscans first to make things easier. In my experiments you can get pretty far with linear scans using sqlite-vec. Depends on the number of dimensions of course, but I'd expect it can handle searching 100's of thousands of vectors to maybe a million with sub-second searches, especially if you tweak some settings (page_size, mmap_size, etc.). And if you quantize your vectors (int8 reduces size 4x, binary 32x) you can get even faster, at the expense of quality. Or use Matryoshka embeddings[0] to shave down dimensions even more. Building sql.js-httpvfs would be cool! Though I'm using the \"official\" SQLite WASM builds which came out after sql.js, so I don't think it'll be compatible out of the box. Would be very curious to see how much effort it would be to make a new HTTP VFS for SQLite's new WASM builds [0] https://huggingface.co/blog/matryoshka reply mkesper 3 hours agorootparentHave a look at https://jkatz05.com/post/postgres/pgvector-performance-150x-... fp16 for indices seems to give a nice size compression without reducing quality with HNSW. reply sroussey 4 hours agorootparentprevMight have a look at this library: https://github.com/unum-cloud/usearch It does HNSW and there is a SQLite related project, though not quite the same thing. reply alexgarcia-xyz 4 hours agorootparentThanks for sharing! I've looked into usearch before, it's really sleek, especially all their language bindings. Though I want sqlite-vec to have total control over what stays in-memory vs on-disk during searches, and most vector search libraries like usearch/hnswlib/faiss/annoy either always store in-memory or don't offer hooks for other storage systems. Additionally, sqlite-vec takes advantage of some SQLite specific APIs, like BLOB I/O [0], which I hope would speed things up a ton. It's a ton more work, coming up with new storage solutions that are backed by SQLite shadow tables, but I think it'll be work it! And I also like how sqlite-vec is just a single sqlite-vec.c file. It makes linking + cross-compiling super easy, and since I got burned relying on heavy C++ dependencies with sqlite-vss, a no-dependency rule feels good. Mostly inspired by SQLite single-file sqlite3.c amalgamation, and Josh Baker's single file C projects like tg[1]. [0] https://www.sqlite.org/c3ref/blob_open.html [1] https://github.com/tidwall/tg reply CGamesPlay 1 hour agoparentprevVery exciting and I can’t wait to try it. Dependency issues are the reason I am currently not using sqlite-vss (i.e. not using an index for my vector searches), but man do I wish for better performance. At the risk of fulfilling a HN stereotype, may I ask why you ditched Rust for this extension? reply TachyonicBytes 3 hours agoparentprevReally nice to see Wasm there, usually vector search in sqlite wasn't available in the browser. Did you ever consider making it syntax compatible with pgvector, in order to have a common SQL vector DSL? I am sure the benefits are much smaller than the disadvantages, but I'm curious if it's possible. reply alexgarcia-xyz 3 hours agorootparentIt's not possible to match pgvector's syntax with SQLite. SQLite doesn't have custom indexes, and since I want to have full control of how vectors are stored on disk, the only real way to do that in SQLite is with virtual tables[0]. And there's a few other smaller details that don't match (postgres operators like /, SQLite's limited virtual table constraints, etc.), so matching pgvector wasn't a priority. I instead tried to match SQLite's FTS5 full text search[1] extension where possible. It solves a similar problem: storing data in its own optimized format with a virtual table, so offering a similar API seemed appropriate. Plus, there's a few quirks with the pgvector API that I don't quite like (like vector column definitions), so starting from scratch is nice! [0] https://www.sqlite.org/vtab.html [1] https://www.sqlite.org/fts5.html reply jeanloolz 2 hours agoparentprevI originally added sqlite-vss (your original vector search implementation) on Langchain as a vectorstore. Do you think this one is mature enough to add on Langchain, or should I wait a bit? Love your work by the way, I have been using sqlite-vss on a few projects already. reply xyc 5 hours agoparentprevThis is awesome! We used Qdrant vector DB for a local AI RAG app https://recurse.chat/blog/posts/local-docs#vector-database, but was eyeing up sqlite-vss as a lightweight embedded solution. Excited to see you are making a successor. Would be interested to learn about latency and scalability benchmarks. reply alexgarcia-xyz 4 hours agorootparentThat's great to hear, thanks for sharing! Will definitely have benchmarks to share later Would love to hear about the scale of the vector data you're working with in your local app. Do you actually find yourself with > 1 million vectors? Do you get away with just storing it all in-memory? reply xyc 4 hours agorootparentI don't think we need > 1 million vector yet. But we plan to target folder of local document such as obsidian vault or store web search results which could rack up a large number of vectors. Persistence on disk is also a desired feature when looking at the choices because we don't want to reindex existing files. Benchmark is also not everything, easiness to embed and integrate with existing sqlite apps could make sqlite-vec stand out and help with adoption. reply tipsytoad 2 hours agoparentprevLooks really nice, but the only concern I had — how does the perf compare to more mature libraries like faiss? reply nnx 3 hours agoparentprevInteresting idea. Do you intend to compress the vector storage in any way and do you intend to implement your own vector search algorithms or reuse some already optimized libraries like usearch? reply alexgarcia-xyz 3 hours agorootparentI don't have plans for compressed vector storage. There is support for int8/binary quantization, which can reduce the size of stored vectors drastically, but impacts performance quite a lot. I'd like to support something like product quantization[0] in the future, though! No plans for using usearch or another vector search library, either. I want to keep dependencies low to make compilingeasy, and I want full control for how vectors are stored on-disk and in-memory. [0] https://www.pinecone.io/learn/series/faiss/product-quantizat... reply barakm 5 hours agoprev> Thy are binary vectors with 768 dimensions, which takes up 96 bytes (768 / 8 = 96). I guess I’m confused. This is honestly the problem that most vector storage faces (“curse of dimensionality”) let alone the indexing. I assume that you meant 768 dimensions * 8 bytes (for a f64) which is 6144 bytes. Usually, these get shrunk with some (hopefully minor) loss, so like a f32 or f16 (or smaller!). If you can post how you fit 768 dimensions in 96 bytes, even with compression or trie-equivalent amortization, or whatever… I’d love to hear more about that for another post. Ninja edit: Unless you’re treating each dimension as one-bit? But then I still have questions around retrieval quality reply alexgarcia-xyz 5 hours agoparentAuthor here - ya \"binary vectors\" means quantizing to one bit per dimension. Normally it would be 4 * dimensions bytes of space per vector (where 4=sizeof(float)). Some embedding models, like nomic v1.5[0] and mixedbread's new model[1] are specifically trained to retain quality after binary quantization. Not all models do tho, so results may vary. I think in general for really large vectors, like OpenAI's large embeddings model with 3072 dimensions, it kindof works, even if they didn't specifically train for it. [0] https://twitter.com/nomic_ai/status/1769837800793243687 [1] https://www.mixedbread.ai/blog/binary-mrl reply barakm 5 hours agorootparentThank you! As you keep posting your progress, and I hope you do, adding these references would probably help warding off crusty fuddy-duddys like me (or at least give them more to research either way) ;) reply lsb 5 hours agoparentprevBinary, quantize each dimension to +1 or -1 You can try out binary vectors, in comparison to quantize every pair of vectors to one of four values, and a lot more, by using a FAISS index on your data, and using Product Quantization (like PQ768x1 for binary features in this case) https://github.com/facebookresearch/faiss/wiki/The-index-fac... reply barakm 5 hours agorootparentAppreciate the link; but would still like to know how well it works. If you have a link for that, I’d be much obliged reply lsb 5 hours agorootparentIt depends on your data and your embedding model. For example, I was able to quantize embeddings of English Wikipedia from 384-dimensions down to 48 7-bit dimensions, and the search works great: https://www.leebutterman.com/2023/06/01/offline-realtime-emb... reply TheAnkurTyagi 2 hours agoprevThis is great. we used Qdrant vector DB for an end to end automation for our AI RAG app at https://github.com/rnadigital/agentcloud, but very excited to see you are making a successor. any ETA when it would be ready to use and any quickstart guide? Maybe I can help in writing a blog as well. reply Trufa 6 hours agoprevI love this style of projects, OSS project for a very particular issue. I keep thinking what can I do in the Typescript/Next.js/React ecosystem that's very useful to a technical niche but I haven't had the inspiration yet. reply usgroup 2 hours agoprevShallow insight, but separating the algorithmic/representational and sqlite operational parts into own C projects, is possibly a good idea. I'd expect the rate of evolution to be significantly different between the two parts, and if you are using an algorithm library adopted by others, you may get progress \"for free\". reply elitan 2 hours agoprevVector search in SQLite is what's keeping me from switching from Postgres. reply ComputerGuru 4 hours agoprevI guess this is an answer (not in answer, mind!) to the GitHub issue I opened against SQLite-vss a couple of months ago? https://github.com/asg017/sqlite-vss/issues/124 reply alexgarcia-xyz 3 hours agoparentYes it is! Sorry for not following up there. Actually, when I first read that ticket, it started me down the rabbit-hole of \"how can I make sqlite-vss\" better, which eventually turned into \"I should make sqlite-vec.\" So thanks for helping me go down this path! With sqlite-vec's builtin binary quantization, you should be able to do something like: CREATE VIRTUAL TABLE vec_files USING vec0 ( contents_embedding bit[1536] ); INSERT INTO vec_files(rowid,contents_embedding) VALUES ( (1, vec_quantize_binary( /* 1536-dimension float vector here*/)) ) reply xrd 4 hours agoprevVery excited about this. When running inside the browser, is sqlite-vec able to persist data into the browser-native indexdb? Or, is this part left to the user? If you can share the thinking there I would appreciate it, even if that is, \"no thinking yet!\" reply alexgarcia-xyz 4 hours agoparentIt could! It's based on the official SQLite WASM build, so you can use the same persistent options[0] that are offered there. Not sure if IndexedDB is specifically supported, but localStorage/OPFS VFS is available. [0] https://sqlite.org/wasm/doc/trunk/persistence.md#kvvfs reply samwillis 3 hours agorootparentOP is correct, the official WASM SQLite build is a sync only build and doesn't support async VFSs (which a IndexedDB VFS needs to be as it's an async api, unless you load the whole file into memory). The best option for IndexedDB backed WASM SQLite is wa-sqlite, it offered both a sync and async build and a bunch of different VFSs, including an IndexedDB one. Note however that the async builds add significant overhead and reduce performance. The most performant VFS is the \"OPFS access handle pool\" VFS that Roy developed for wa-SQLite and the official build also adopted as an option. That would be my recommendation for now. reply codazoda 6 hours agoprevThis is a lot like I imagine “readme driven development” to look like. I’m curious if the author started with docs first. reply alexgarcia-xyz 5 hours agoparentThanks for the kind words! I started with code first — the extension itself is already mostly written[0]. But it's one of those \"20% effort for 80% of the work,\" where the last 20% I need to write (error handling, fuzzy tests, correctness testing) will take 80% of the time. But people already have questions about the current status of `sqlite-vss`, so I figured this \"work-in-progress\" blog post could answer some questions. Though I do like the idea of starting with docs first! Especially with SQLite extensions, where it all really matters what the SQL API looks like (scalar functions, virtual tables, etc.). I definitely did a lot of sketching of what the SQL part of sqlite-vec should look like before writing most of the code. [0] https://github.com/asg017/sqlite-vec/blob/main/sqlite-vec.c reply peter_l_downs 5 hours agorootparentYou’ve done a great job communicating the project, in every aspect. Nice work. I’m excited to try this out! reply blizzardman 1 hour agoprevAre you looking for contributor by any chance? reply auraham 5 hours agoprevAwesome work! It would be great if you can share a post of how you developed that extension. I am not familiar with SQLite extensions, so I am not sure how to dig into the github repo. reply alexgarcia-xyz 5 hours agoparentI definitely plan to! I have a much larger list of SQLite extensions I've built here: https://github.com/asg017/sqlite-ecosystem Here's a few other references you may enjoy if you wanna learn more about SQLite extensions: - The single source file for sqlite-vec: https://github.com/asg017/sqlite-vec/blob/main/sqlite-vec.c - sqlean, a project from Anton Zhiyanov which is good base of great SQLite extensions: https://github.com/nalgeon/sqlean - The official SQLite docs: https://www.sqlite.org/loadext.html - The \"hello world\" SQLite extension example: https://www.sqlite.org/src/file/ext/misc/rot13.c reply babox 1 hour agoprevvery cool i like so much sqlite reply zaeger 2 hours agoprevvery good reply goprams 2 hours agoprev [–] > KNN style query goes brrrr Why do devs write comments like this? What does \"goes brrrr\" add to anyone's understanding of the code? It's so annoying. Is it supposed to be amusing or cute? It's not. The rest of the article is great but it's so jarring to see this sort of nonsense in amongst a solid piece of work. It's like when you go to a conference and the presentations are peppered with unfunny \"meme\" pictures. Grow up, people. reply efdee 1 hour agoparentWhy do people write comments like this? What does \"grow up, people\" add to anyone's understanding of the topic? It's so annoying. Is it supposed to make you look better? It doesn't. reply dayjaby 2 hours agoparentprevLanguage evolves. Grow up yourself. reply timmy777 2 hours agoparentprev [–] Keep in mind, a presentation (text, or otherwise) is an art. Culture, background, experiences and ideologies influences art. ... and to address your tone, did you know, you can give feedback with empathy? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Alex Garcia introduces sqlite-vec, a new SQLite extension for vector search, designed to replace sqlite-vss, offering custom SQL functions and virtual tables.",
      "Written solely in C, sqlite-vec targets speed and embeddability, boasting cross-platform support for WebAssembly and mobile devices, along with improved memory management.",
      "The extension, currently in progress, is set for an upcoming initial release; the developer is seeking sponsorship opportunities to advance the project."
    ],
    "commentSummary": [
      "The author is developing a new vector search SQLite extension emphasizing linear scans initially, with future plans for advanced indexing, rapid processing of numerous vectors, and investigating quantization for speedier searches.",
      "Users are exchanging thoughts on related projects and their encounters with vector storage solutions, while the post covers binary vectors, quantization, machine learning models, and the advantages of vector search in databases like SQLite.",
      "Feedback on the project is optimistic, with users eager to contribute and expand their knowledge of SQLite extensions."
    ],
    "points": 239,
    "commentCount": 43,
    "retryCount": 0,
    "time": 1714700670
  },
  {
    "id": 40240037,
    "title": "Microsoft bans U.S. police from using facial recognition AI",
    "originLink": "https://techcrunch.com/2024/05/02/microsoft-bans-u-s-police-departments-from-using-enterprise-ai-tool/",
    "originBody": "(opens in a new window)(opens in a new window)(opens in a new window)(opens in a new window)(opens in a new window) Link Copied AI Microsoft bans US police departments from using enterprise AI tool for facial recognition Kyle Wiggers@kyle_l_wiggers / 8:57 PM UTC•May 2, 2024 Comment Image Credits: Fabrice Coffrini / AFP / Getty Images Microsoft has reaffirmed its ban on U.S. police departments from using generative AI for facial recognition through Azure OpenAI Service, the company’s fully managed, enterprise-focused wrapper around OpenAI tech. Language added Wednesday to the terms of service for Azure OpenAI Service more obviously prohibits integrations with Azure OpenAI Service from being used “by or for” police departments for facial recognition in the U.S., including integrations with OpenAI’s current — and perhaps future — image-analyzing models. A separate new bullet point covers “any law enforcement globally,” and explicitly bars the use of “real-time facial recognition technology” on mobile cameras, like body cameras and dashcams, to attempt to identify a person in “uncontrolled, in-the-wild” environments. The changes in policy come a week after Axon, a maker of tech and weapons products for military and law enforcement, announced a new product that leverages OpenAI’s GPT-4 generative text model to summarize audio from body cameras. Critics were quick to point out the potential pitfalls, like hallucinations (even the best generative AI models today invent facts) and racial biases introduced from the training data (which is especially concerning given that people of color are far more likely to be stopped by police than their white peers). It’s unclear whether Axon was using GPT-4 via Azure OpenAI Service, and, if so, whether the updated policy was in response to Axon’s product launch. OpenAI had previously restricted the use of its models for facial recognition through its APIs. We’ve reached out to Axon, Microsoft and OpenAI and will update this post if we hear back. The new terms leave wiggle room for Microsoft. The complete ban on Azure OpenAI Service usage pertains only to U.S., not international, police. And it doesn’t cover facial recognition performed with stationary cameras in controlled environments, like a back office (although the terms prohibit any use of facial recognition by U.S. police). That tracks with Microsoft’s and close partner OpenAI’s recent approach to AI-related law enforcement and defense contracts. In January, reporting by Bloomberg revealed that OpenAI is working with the Pentagon on a number of projects including cybersecurity capabilities — a departure from the startup’s earlier ban on providing its AI to militaries. Elsewhere, Microsoft has pitched using OpenAI’s image generation tool, DALL-E, to help the Department of Defense (DoD) build software to execute military operations, per The Intercept. Azure OpenAI Service became available in Microsoft’s Azure Government product in February, adding additional compliance and management features geared toward government agencies including law enforcement. In a blog post, Candice Ling, SVP of Microsoft’s government-focused division Microsoft Federal, pledged that Azure OpenAI Service would be “submitted for additional authorization” to the DoD for workloads supporting DoD missions. Update: After publication, Microsoft said its original change to the terms of service contained an error, and in fact the ban applies only to facial recognition in the U.S. It is not a blanket ban on police departments using the service. Please login to comment Login / Create Account TechCrunch Disrupt 2024 Innovation For Every Stage LEARN MORE Sign up for Newsletters See all newsletters(opens in a new window) Daily News Week in Review Startups Weekly Event Updates Advertising Updates By subscribing, you are agreeing to Yahoo's Terms and Privacy Policy. Email Subscribe (opens in a new window)(opens in a new window)(opens in a new window)(opens in a new window)(opens in a new window) Copy Tags AI Azure OpenAI Service Generative AI law enforcement Microsoft police",
    "commentLink": "https://news.ycombinator.com/item?id=40240037",
    "commentBody": "Microsoft bans U.S. police from using enterprise AI tool for facial recognition (techcrunch.com)233 points by coloneltcb 15 hours agohidepastfavorite119 comments megraf 14 hours agoOf course they did. They have to pay separate licensing for the use of DAS – which is developed by M$ and resold from NYPD. https://en.wikipedia.org/wiki/Domain_Awareness_System reply iceyest 14 hours agoparentWow, I had no idea it was this bad. I am not really suprised the lengths American spying goes though. Glad not to be living in New York. >The Domain Awareness System is the largest digital surveillance system in the world I wonder how it compares to China and if facial recognition tech is as pervasive in America as it is in China. reply pfortuny 14 hours agorootparentThere’s London too, totally dystopian. reply vidarh 13 hours agorootparentMost cameras in the UK are private, rarely working well, and usually not networked or easily accessible to the police. It could be better, but the idea of the UK as a surveillance state is seriously overblown outside of maybe a handful of streets. reply phantompeace 11 hours agorootparentDon’t forget ANPRs that can track your vehicle all around the country. Doesn’t even have to cross paths with a police car - they’re at most major junctions and all over the motorway reply winrid 14 hours agorootparentprevYeah I would want to better understand how they arrived at that statement. When I was in Xinjiang there were like 5-10 cameras at every intersection. Surely NYC isn't at that level? reply dpkirchner 13 hours agorootparentI don't know about NYC but it's common to see at least 4 cameras per lighted intersection where I am (small city in the PNW). They're replacing the loops used to sense vehicles, but I'm pretty sure they're also remotely accessible. reply ajma 10 hours agorootparentNot only are they remotely accessible. There publicly accessible (not all of them) https://kingcounty.gov/en/dept/local-services/transit-transp... reply lotsofpulp 13 hours agorootparentprevI don’t think those are necessarily recording cameras. They could just be radar or video vehicle detectors to let the light know if a car is waiting. reply janalsncm 14 hours agorootparentprevIn America we get the worst of both worlds: police won’t admit to domestic spying so they can’t use to solve day to day crimes. But they still spy on us, Constitution be damned. As a result the US has a higher crime rate than many other countries including China. If you don’t trust China’s numbers look at Singapore, which has a population density similar to NYC with an order of magnitude less crime. Singapore is safer at night than NYC is during the day. Why? Cameras. If you commit an offense, you will be caught, without question. reply mrcartmeneses 13 hours agorootparentSingapore isn’t safe “because cameras” it’s safe because it has high social mobility, social housing, high levels of education, high levels of income equality and is generally wealthy and much wealthier than its neighbours. And also it’s a police state where you get sentenced to death for drug smuggling and can be punished for doing drugs in another country while on holiday if you are a citizen, for example. To the naive, Singapore is a paradise, but once you visit for long enough you realise it’s just a super nice prison and it’s not very fun being with the other lags reply ChrisMarshallNY 12 hours agorootparentWilliam Gibson got a lot of hate for his \"Disneyland With the Death Penalty\" essay (in Wired), about 30 years ago: https://en.wikipedia.org/wiki/Disneyland_with_the_Death_Pena... reply janalsncm 9 hours agorootparentThe United States has the death penalty and it’s certainly not Disneyland. In the other hand calling it “Disneyland” probably oversells how fun Singapore is. It is a fairly boring place in my experience. reply r00fus 12 hours agorootparentprev> To the naive, Singapore is a paradise, but once you visit for long enough you realise it’s just a super nice prison and it’s not very fun being with the other lags It's not an American style of life, but I'm sure it's attractive to a lot of people who simply aren't into drugs, partying, or living the high life. reply petre 5 hours agorootparentYes, it's attractive to rich Chinese taking a break from Xi's regime. Others pick Canada or NZ instead. reply temporarara 11 hours agorootparentprevThis. I'm sure there are lots of things I don't like about Singapore, but the fact that doing and especially dealing drugs is kinda hard and may have death penalty level consequences is a non-issue for me. Cultures that promote drug usage are disgusting and once the drugs comes into your neighborhood there is no way to just ignore it if you care about your safety. I wouldn't care too much if it was just some guys using drugs for fun and that was the end of it, but drugs destroy whole communities and I hate it. reply LtWorf 3 hours agorootparentI think that's a USA centric view. Drugs aren't linked to violence unless you make laws to link them. reply jMyles 12 hours agorootparentprev> people who simply aren't into drugs ...are we still having to go over this? There are no such people. Everybody does drugs, unless you adopt a definition of \"drug\" which is designed specifically to accommodate this assertion, rather than apply in some useful way to reality. It's difficult to even create a definition of 'drug' which convincingly excludes survival necessities like water and oxygen, but remains otherwise consistent. There are some drugs which are, for reasons varying from racism to capriciousness to an intemperant desire for greater cartel income, prohibited to varying degrees in various jurisdictions. You can go to Singapore and consume sugar, coffee, nicotine, alcohol, and many other drugs while enjoying the tacit endorsement of your behavior by the local state apparatus. You can also consume cocaine, opioids, and plenty of other drugs so long as you pay black market rates and do so shielded by privilege so as not to run afoul of an investigation. You can even consume cannabis, though of course it is very difficult to smuggle, so you pay a higher premium. reply LordShredda 8 hours agorootparentYou know what we mean by drugs. Not coffee, not alcohol. reply jMyles 8 hours agorootparentNo, I really don't know. Do you just mean drugs which the state prohibits? If so, isn't that begging the question? If a person wants to go to a place where drugs are prohibited, and \"drug\" is defined as a substance which is prohibited, then doesn't every jurisdiction qualify? If a person \"simply isn't into drugs\", with \"drugs\" defined by the local state, isn't that person just in favor of the state-prescribed diet? And it really has nothing to do with drugs? If such a person goes from a place where drug X is prohibited to one where it is not, does their preference suddenly change? One better: if a person goes from a place where drug X is prohibited to one where it is compulsory, do they suddenly fall in love with the drug? Of course not. Because nobody's preferences work that way. Ergo, there is no actual real human who meets the critera, \"simply isn't into drugs\" upon close examination. Sure, there may be people who are pro-prohibition, but I imagine nearly all of them are in a position to financially gain from it. In terms of outcomes, it's horrible policy, especially if a target is reduction of death and disease from drug use. reply jhanschoo 6 hours agorootparentprev> To the naive, Singapore is a paradise, but once you visit for long enough you realise it’s just a super nice prison and it’s not very fun being with the other lags Oh come on, your yardstick is completely ill-calibrated. Among polities that - didn't exist as independent polities until post-WWII - were not modernized/westernized/democratized until post-WWII (thereby excluding NZ) you'd be hard-pressed to name one that you'd be rather born as a random citizen in. Perhaps another of the Asian tigers, like TW. Presumably you have in mind, as Gibson probably did, a country that has had a longer history of independent or democratic rule and economic development. reply skhunted 13 hours agorootparentprevThe claims made in your first paragraph are undermined by your third paragraph. reply bryanrasmussen 12 hours agorootparentthe first paragraph did not claim it was not a prison, it just claimed it was safe not because it was a prison but for other reasons. The third paragraph claimed it was a prison but it did not say the safety came from it being a prison. the third paragraph and first paragraph are not in conflict. reply skhunted 12 hours agorootparentThe fact that it is a prison with much surveillance has little to do with its low crime? This is not a credible belief. reply bryanrasmussen 11 hours agorootparentI wasn't saying it was wrong or not - I just said the two paragraphs were not inherently contradictory. reply skhunted 6 hours agorootparentThat’s why I said “undermined” and not “contradicted”. reply skhunted 12 hours agorootparentprevI know it’s not true in American prisons but generally not much crime occurs in prison. If Singapore is prison like then it seems that this would at least be a contributing factor to its lack of crime. reply observationist 12 hours agorootparentprevNot that US institutions don't lie, but our bureaucracy and government oversight systems come with the benefit that many of the things we track and document can be verified. Other countries simply lie. The numbers you get are not a rough approximation of reality; they're simply part of whatever story that country wants to tell. Crime rate is a statistic for which a majority of countries provide numbers that are completely dissociated from reality. You can, of course, take the numbers seriously, as if the statistics are being published in good faith. Unless you have some sort of independent oversight, however, that isn't beholden to or biased by the country being assessed, then taking those numbers seriously is probably a silly thing to do. The US gets lots of independent verification and validation of crime statistics. They're frequently analyzed at local, state, and federal level by journalists, students, activists, authors, and government officials. At every level an official number is published, it gets challenged, so there are incentives keeping the politicians and bureaucrats honest. They get slammed when they get caught lying, and they get caught lying because the public and the media keep track of things and demand accountability. Some stuff, like total officer involved shootings, dog shootings by officials, abuses of power, and things of that nature, don't get publicly disclosed much of the time, so there are gaps in what we know and what officials are required to disclose. The US isn't perfect, but you can get pretty good numbers that actually correlate with reality. Even other western countries don't always have trustworthy reporting and accounting for government actions. The best you'll ever get is a glowing narrative. reply janalsncm 9 hours agorootparentSingapore has very low levels of corruption. Even lower than in the United States. You can choose to dismiss them, but I think their numbers are reliable. reply dgfitz 12 hours agorootparentprevThis post should be stickied at the top of this topic. reply LtWorf 3 hours agorootparentprevSuch a long text for \"their numbers are better than ours, they are clearly liars\" reply vundercind 14 hours agorootparentprevSame with national ID and our fear of “papers, please”. We have all the downsides—constantly having to provide ID to everyone, government can trivially access all kinds of tracking data tied to that—but none of the benefits of an actual national ID because we have to pretend we don’t have one (we do, it’s just 80% privatized and a massive liability and inconvenience for citizens in ways that it doesn’t need to be) reply forgetfreeman 12 hours agorootparentThe irony of individuals freaking out over the notion of what the wildly underfunded and largely indifferent federal government might do to collect data when private industry is surveilling basically every aspect of their existence for most of every day... reply brokenmachine 10 hours agorootparentPrivate industry can't directly send someone to prison. reply LtWorf 3 hours agorootparentSoon: \"our AI predicts that individual X has a 90% chance of committing a violent crime within the next semester\" reply SpaceManNabs 13 hours agorootparentprev> we do, it’s just 80% privatized and a massive liability and inconvenience for citizens in ways that it doesn’t need to be I am guessing to you are referring to how multiple private organizations can just poll for my social security number and I cant do anything about it? Yeah... reply vundercind 13 hours agorootparentYep. It’s a pain in the ass to deal with from our end, to put it mildly, but easy to piece it all together on the other, so it’s barely an impediment to bad actors. Since there are functionally no restrictions on government using parts of that system to grab all kinds of data from private sources, it’s a national ID connected directly to a crazy-powerful dragnet spying system, too. But inconvenient, insecure, and very hard to gain oversight of. All the bad shit, none of the good. We may as well just have a national ID, it’d be less-bad than what we have now and might provide a jumping off point for making it a lot less-bad. reply DaiPlusPlus 14 hours agorootparentprev> constantly having to provide ID to everyone I live in WA and for the past 5+ years I've only ever presented my driver's license to TSA agents at the airport. ...unless getting carded at the store for buying beer counts? reply vundercind 13 hours agorootparentDoctor, dentist, employers, educational institutions. [edit] all financial institutions, lenders, et c Unless you’re buying everything with cash, your purchasing history will be connected to it, too. Where you drive (license plate scanning and data-sharing is widespread). Facial recognition in stores means paying cash might not even help (seen the ones that highlight your face on a monitor when you walk in the store?) But we have to pretend we don’t have a national ID system any time it might be convenient to a person to have that. reply DaiPlusPlus 12 hours agorootparentRight. I thought the parent post was talking about being challenged by government officials/police (\"papers, please\") on a regular basis - and not about private-sector use. A misunderstanding on my part. reply yardie 13 hours agorootparentprevI literally had to present ID this morning to buy cold medicine. Had to present ID lastweek to enter my kid's school then had to present ID to their pediatrician. reply deathlight 12 hours agorootparentIt's sad how in the state of Washington I have become accustomed to drawing my ID out on every transaction that involves confirming that I'm over 21 whereas cross the border into Idaho there's not a single time I have ever been asked to provide my ID in over 10 years. Clearly in Idaho they can take one look at me and go like well yeah clearly but such visual social technology is beyond the pale in the great creepy state of Washington. I'm a bit sad to think of what the children of today's children will be like when I am very old and they are very young. Maybe one day nobody will know what it was like to live in a society that was able to just tell things by looking at it. reply forgetfreeman 12 hours agorootparentprevAt least two of those instances make perfect sense. Meth's a thing and we don't like unidentified individuals fucking around on school grounds these days. reply kube-system 14 hours agorootparentprev> Singapore is safer at night than NYC is during the day. Why? Cameras. If you commit an offense, you will be caught, without question. There's more to it than that. A sheisty will defeat a camera. reply echoangle 13 hours agorootparentNot when you have a lot of cameras, right? You still have to put it on somewhere without being seen, and you would have to swap all clothes so you can’t be tracked by checking tapes of other locations for your clothes before you put it on. reply lupusreal 13 hours agorootparentprevI think Singapore whipping the shit out of criminals probably has a lot to do with it. Not just Singapore, but most of the rest of Asian countries as well. South Korea treats their prisoners so harshly that the US military has to have a special agreement in place to ensure that US servicemen imprisoned for crimes by South Korea actually get fed more than a starvation diet and don't have to do hard labor: https://www.stripes.com/migration/for-u-s-inmates-in-s-korea... Of course South Koreans know better than American soldiers not to commit crimes in South Korea, because prison there is so awful. Unsurprisingly they have a much lower crime rate than America. reply 8372049 13 hours agorootparentThe big downsides with horrible prison are that criminals will have a huge incentive to do anything to not get caught and convicted, including violently resisting arrest, shootouts, targeting witnesses and so on. It is mostly effective as a deterrent against financial/winning-relates crimes, but mostly ineffective against most other types of crimes. Plus, horrible prison is much less likely to actually reform the person in question. Statistically, the Norwegian system with \"holiday prisons\" works well, with very low recidivism rates for most types of crime. reply infotainment 5 hours agorootparentThat’s why execution is the ideal solution, not prisons. During a brief transition period you may have to deal with violently resisting arrest and the like, but eventually you get to the point where all the criminals are dead. reply lupusreal 10 hours agorootparentprevNorway's intention homicide rate is six times greater than Singapore's. reply bryanlarsen 12 hours agorootparentprevStudies show that the severity of the punishment has little impact crime rates -- the odds of getting caught & punished are much more significant. reply AvocadoPanic 13 hours agorootparentprevIs it the catching or the imprisoning NYC and many other cities are struggling with in current year? reply lupusreal 13 hours agorootparentAmerican criminal subcultures treat going to prison as almost as a rite of passage, rather than something to be avoided at all costs. Sentences are too short and too easy. reply postalrat 14 hours agorootparentprevYou had no idea it was so bad? You do know how mobile phones work and what is tracked don't you? reply beeboobaa3 13 hours agoparentprev> 9,000 CCTV cameras, owned either by the NYPD or private actors I wonder what kind of person volunteers their camera for the surveillance apparatus. reply mikestew 13 hours agorootparentThe business owner who might have had their fill of break-ins, people pooping in front of their store, et al., for one. I'm sure \"private actors\" is not all just Ring doorbells. reply themoonisachees 13 hours agorootparentprevBusinesses point cameras at the public for the sole purpose of surveillance. I'm not talking other businesses, I'm talking business built to sell and launder surveillance footage. reply mlinhares 13 hours agoparentprevYay, even licensed to the Brazilian National Police (that doesn't exist). Did they mean the Federal Police? reply jampa 12 hours agorootparentIt seems to be a mistake. From the original article which Wikipedia is citing: > \"[...]Others, such as the Washington D.C. Metro Police, the Singapore Police Force, and the Brazilian National Police have purchased the DAS software from Microsoft, our software developer, and have used it to secure high-profile governmental and cultural sites, the 2014 World Cup, and the 2016 Summer Olympics. Microsoft has agreed to give New York City 30 percent of the revenue it derives from selling the software to other jurisdictions (Parascandola and Moore 2012);\" But \"Parascandola and Moore 2012\" refers to another article that doesn't say anything about existing uses outside of NYC. (Using Archive because the site has a Geoblock). https://web.archive.org/web/20210812131624/https://www.nydai... EDIT: Just removed the paragraph from Wikipedia. reply andrewmcwatters 14 hours agoparentprevIt reads like literal \"pre-crime.\" reply observationist 13 hours agorootparentA machine learning algorithm known as Patternizr is included in the DAS, which connects potential criminal suspects to other unsolved crimes in order to speed arrests and close old cases.[20][21] The algorithm is trained on a decade of historic police data of manually identified crime patterns. Potential criminal suspects. Who needs civil liberties, anyway? reply petre 4 hours agorootparentSo no more \"Copilot can make mistakes\" kind of disclaimers? Patternizr nevr maks mstakes? reply edot 13 hours agoprevThey are not banning them from using their AI tool. They're banning them from the currently-offered licensing structure. This is a profit-grab, not an ethical restriction. It is of the same vein as Sam Altman going to congress and spooking them in hopes of regulating AI so that they are the only ones allowed to build it. I wouldn't be shocked if we also saw a similar move to ban hospitals, schools, etc. from using the standard enterprise license under the guise of \"we need to provide you with oversight for your vulnerable populations\", and then charging them a hefty surcharge on top of the standard enterprise rate. \"Responsible\" AI is more profitable than standard AI - regardless of the nebulous nature of what \"responsible\" means. reply jMyles 12 hours agoparentSpot on. I'll explore whether this nit is worth picking: > \"Responsible\" AI is more profitable than standard AI - regardless of the nebulous nature of what \"responsible\" means. /s/regardless of/in part owing to reply spxneo 13 hours agoprevAmerican law enforcement openly disregard the foundations from which the country was built upon yet pushes other countries to adopt ideas which it abandoned but won't admit it out of humiliation--that which we call \"democracy\" comes with the very hooks and strings it tells other countries to abandon. Just take a look at the countries that credulously adopted \"democracy\". They are not in a good shape and will probably never find itself out of the moving goal post that Western countries like to place on \"lesser\" countries. America is addicted to these 3 things: Oil, authoritarian labour, drugs. It's law enforcement is not meant to police those addictions at all but rather to ensure its continued uninterrupted supply even if it means violating its own principles that made the country great (about 40+ years ago). It's no wonder many Americans are yearning for past glory. reply advisedwang 13 hours agoprevThis is excellent. There will be blood on our hands if we allow our software to be used to more closely surveil and police people. Not only because the software as it currently exists is flaky, but because of how it will enviably be used. Certain communities will have it deployed against them more, parallel construction will hide when technology is used in violation of rights, police and juries will believe when technology has a \"hit\" that it proves guilt and not understand the caveats, etc etc. Some Microsoft employees have been calling for such limits for years [1, 2] [1] https://www.geekwire.com/2019/microsoft-github-workers-prote... [2] https://www.cbsnews.com/news/microsoft-workers-asking-drop-p... reply rockinghigh 13 hours agoparentThe NYPD has had a system built by Microsoft for 15 years: > Beginning in 2009, the NYPD, in partnership with Microsoft, built a powerful counterterrorism and policing tool called the Domain Awareness System (DAS). The DAS is a central platform used to aggregate data from internal and external closed-circuit television cameras (CCTV), license plate readers (LPRs), and environmental sensors, as well as 911 calls and other NYPD databases. The DAS uses an interactive dashboard interface to display real-time alerts whenever a 911 call is received or a sensor is triggered. The DAS also in- cludes mapping features that make it possible to survey and track targets. The DAS was originally built to support the Lower Manhattan Security Initiative (LMSI) – a public-private partnership – but has since expanded to cover the entire city, giving NYPD personnel direct access to thousands of cameras owned and operated by private orga- nizations. Until the development of the mobility platform and the mobile DAS system described below, the full capabilities of DAS have only been available to the Counterterrorism Bureau and a few other specially trained officers on desktop computers. As the NYPD upgrades its network, access to all DAS capabilities and resources will be expanded to all NYPD’s commands. https://www.nyc.gov/html/nypd/html/home/POA/pdf/Technology.p... reply chefandy 13 hours agoparentprevIf it's like what they did with the military, they're just trying to push them into a more expensive licensing scheme. reply chefandy 14 hours agoprev\"In January, reporting by Bloomberg revealed that OpenAI is working with the Pentagon on a number of projects including cybersecurity capabilities — a departure from the startup’s earlier ban on providing its AI to militaries. Elsewhere, Microsoft has pitched using OpenAI’s image generation tool, DALL-E, to help the Department of Defense (DoD) build software to execute military operations, per The Intercept.\" Hm... to my eye, this just seems to be a great spin on essentially pushing cops or their vendors to sign (more expensive, I imagine) independent service contracts under the guise of oversight. Their principles seem to be as genuine as they were in their plea to the US government to cede control of LLMs, generally, to the big corporate players \"for safety.\" reply SpaceManNabs 13 hours agoparentOpenAI has blatantly ignored most of its principles (which Elon called out surprisingly well). Insane to me that people ignore that. reply chefandy 13 hours agorootparentSeriously. > Insane to me that people ignore that. Yeah for sure, but I don't think this is a super representative crowd, though. People uncritically frolicking through all of these developments with AI-enhanced rose-colored-glasses glued to their faces are over-represented in the SV universe. I know a lot of non-tech-focused folks that are casting a very wary eye towards our impending incredible AI-powered corporate utopia. The only similarly optimistic people I know outside of tech are the devastatingly credulous fellows still trying to figure out how to recoup money on all of the NFTs they bought, and they seem to be much better at generating headlines than societal changes. But maybe I'm the one wearing the rose-colored glasses, here. reply ldng 14 hours agoprevWait, is that a decision a company is allowed to make ? Is it the beginning of cyberpunk ? Feels like they're just protecting their ass while keeping overselling their product. Not that law enforcement should use AI irresponsibly and without boundaries, but I'm not sure it is the responsibility of a company to make that kind of call. reply duxup 14 hours agoparentPresumably if I have a service that I think is inappropriate for a use (for whatever reason, but let's say false positives) I can in fact limit what kind of use cases area allowed. I could argue that my tool used wrong presents a legal risk to ME because I let law enforcement do some terrible things that I know don't work. Semi-related, most cloud services do not allow crypto mining, the reason there being it's not cost effective. So whatever mining does happen is stolen credentials, or some inside company job where dude is using company resources for personal gain ... pretty much all illegal activity. So they ban it and I believe actively try to detect / aggressively disable it. reply RajT88 13 hours agorootparentI think what it's about, is they can squint a little bit and see a near future where there is massive class-action suits against Microsoft for all the harmful uses which inevitably flow from a police department uncritically using hallucinating AI for their day to day work. reply treyd 14 hours agoparentprevYou generally can't force a party to enter into a contract with another party that they don't want to. reply blackeyeblitzar 14 hours agorootparentUtilities and carriers can’t just pick and choose customers totally. For example, your electrical utility cannot deny your service based on your political speech or profession or whatever. Even if they are privately owned. Personally, I think companies above a certain size should not be able to deny customers because they are effectively like public agencies or utility services due to their size. Banks, big tech, large social media platforms should be regulated like utilities in my opinion. reply singleshot_ 13 hours agorootparentWhen it comes to wanting big companies to follow rules (a noble pursuit) many wish that common carrier status could be imposed on a business. But remember why common carrier status exists. Imagine I rob a stagecoach and lie low until the next day, when I catch a train to the next town over. Is the train engineer my accomplice in the robbery? Or my accessory after the fact? Is the train company? He and it might be, except that the train company “holds itself out” as a transporter of goods and services open to all. In exchange, the law grants the company that holds itself out as a carrier for all the freedom not to worry about whether carriage is a criminal act for the customer. But without holding one’s self out as a carrier for all, there is no common carrier status. You cannot hold out a company as sufficiently large; they must hold themselves out as wanting this status. When I see people clamor for social media platforms, I get doubly confused, because not only have these companies never held themselves out as common carriers, they would take nothing they do not already have (under 230) from doing so. reply kube-system 13 hours agorootparentprevOf all the things that could be considered a public utility, Azure OpenAI Service ranks somewhere between Cabbage Patch dolls and Crystal Pepsi. Not only can everyone live without it, almost everyone on the planet does live without it. reply atonse 13 hours agorootparentprevBut utilities also have those rules because they are granted de-facto monopolies in their geographic areas. It's not like I can get electricity from another provider if there is literally only one in my area. And what they're providing (water, sewage, electricity) is deemed essential. Even with banks, if one bank rejects me, I can (hypothetically) go to another one down the street. reply blackeyeblitzar 13 hours agorootparentCompanies can be damaging and anti competitive even if they aren’t total monopolies. Being an oligopoly or having a significant portion of market share is enough. Keep in mind many of these companies have more users than most national governments do - and they have soft power on a similar scale. Another angle is network effects - social media companies face reduced competition due to this and so they too should be regulated above a certain size since there isn’t enough competition or choice there, because the value of access to those particular platforms is very high due to their size. > Even with banks, if one bank rejects me, I can (hypothetically) go to another one down the street. What if they all reject you though? For example what if all banks decide they are better off debanking someone to stay in the good side of politicians who might punish them otherwise (like via regulation)? Some of these services are fundamental and should be forced to provide service no matter what. reply text0404 14 hours agorootparentprevmicrosoft's enterprise AI tool isn't a utility though, and they're not banning police on political or moral grounds. reply blackeyeblitzar 13 hours agorootparent> microsoft's enterprise AI tool isn't a utility though It’s part of an infrastructure as a service platform, which is why I view it as utility. But I also am saying we need to broaden what we call a utility for the modern world. Not having access to large platforms (like social media) or centralized services (banking) or large companies (which have lots of market share and are one of a few options) is very damaging. What are your thoughts on that? > and they're not banning police on political or moral grounds. How do you know this? This seems like it would almost certainly be the result of pressure from political groups. reply pessimizer 12 hours agorootparent> It’s part of an infrastructure as a service platform, which is why I view it as utility. They haven't securely broadened it to the internet or mobile phone service, and you want them to broaden it to individual privately-produced software packages. So the police can't be refused as a customer. reply whaleofatw2022 11 hours agorootparentprevI think in this case, a bigger factor would be this is a 'we will not attest to accuracy in court'. As an odd sort of comparison, this is why you can't just take a magic 8 ball into the courtroom and shake it; 'for amusement only' reply miah_ 14 hours agoparentprevCorporations can make any judgement call about who can or should use their product. Though it does seem rare in the US for companies to prevent others from using their product based on ethical or moral reasons, but more should. reply busterarm 14 hours agorootparentUS banks don't do business with the porn industry pretty famously. reply Alupis 14 hours agorootparentThat is not due to morality though - that is due to the insane level of chargebacks and fraud revolving around payments within the porn industry. reply HeatrayEnjoyer 14 hours agorootparentExcept it is. High shrinkage can be baked into the business model and digital media is a high margin business that can tolerate it. Other industries with high fraud rates don't get cut out wholesale. reply favorited 14 hours agorootparentExactly. Mastercard, Visa & Discover were perfectly happy to process PornHub payments for years - until enough external political pressure was applied. Religious special interest lobbying groups like like Focus on the Family and NCOSE (formerly known as Morality in Media) were celebrating when they stopped, as were Republican politicians like Josh Hawley (https://twitter.com/HawleyMO/status/1337111830976753672). reply Alupis 13 hours agorootparent> until enough external political pressure was applied This doesn't pass the sniff test. You have been able to purchase porn for decades in various mediums - why would a website suddenly invoke the ire of the \"morality police\"? Even more-so when porn is freely available in almost every case when it comes to the internet. Really though, it has nothing to do with morality - it is straight up the risk profile of those websites. There's even a movie about the start of the online paid-porn industry and all the stuff they had to do to figure out how not to bleed money on chargebacks etc. reply pessimizer 12 hours agorootparent> You have been able to purchase porn for decades in various mediums Have you? Or has it been patchwork and limited and volatile, with people being fined and jailed fairly arbitrarily? There were periods in the 80s and 90s where people were going through pornographic tapes and removing the swearing. > suddenly invoke Suddenly, after decades. Is the adjective just to make it sound more unlikely? reply blackeyeblitzar 14 hours agorootparentprevWhat about Visa and Mastercard blocking Wikileaks donations? Should that be allowed just because they are private? reply Suppafly 14 hours agorootparentNot the other guy, but I'm pretty much OK with that. Honestly I'm more OK with them blocking payments to quasi-illegal operations than I am with them blocking payments to presumably legal adult content. Ultimately, in both cases, there other ways to make those payments. Hell I'm more annoyed that I had to use giftcards to buy Minecraft for my kids back in the day, before Microsoft became their parent company, because of some weird financial law in my state. reply Alupis 13 hours agorootparent> Hell I'm more annoyed that I had to use giftcards to buy Minecraft for my kids back in the day, before Microsoft became their parent company, because of some weird financial law in my state. I'm curious to know more about this. I'm not aware of any laws in any state (assuming the US) where it's illegal or forbidden from purchasing video games using a credit card, or even a debit card. reply blackeyeblitzar 13 hours agorootparentprev> I'm more OK with them blocking payments to quasi-illegal operations I know cases have been brought against Wikileaks and Assange, but in my opinion they broke no laws. They acted within the bounds of journalistic freedom. I don’t understand how people are OK with news outlets leaking things like Trump’s tax returns but think that freedom of the press works differently in other circumstances. Why would it be different case by case? reply csande17 14 hours agorootparentprevCompanies can decide who they provide service to, and can give advice about who \"should\" use their products. But I don't think Proctor & Gamble has the legal authority to stop you from using Tide laundry detergent that you legally purchased from a grocery store or on eBay. The current era of tech is all about disguising web services as \"products\" you can \"buy\" (most recently exemplified by the Humane AI pin and Rabbit R1), so I can see how there'd be confusion, though. reply simonw 14 hours agoparentprevForcing companies to sell their surveillance-enabling product to the police seems pretty cyberpunk-dystopian to me. reply cthalupa 13 hours agoparentprev> Wait, is that a decision a company is allowed to make ? Is it the beginning of cyberpunk ? Yes. Freedom of association is literally part of the first amendment. The only limits are around protected classes. How in the world is forcing a company to do business with someone a LESS dystopian solution than allowing them to decide who they do business with? reply SpaceManNabs 14 hours agoparentprev> Wait, is that a decision a company is allowed to make ? Is it the beginning of cyberpunk ? Companies can choose their clients in a non cyperbunk world too. reply flenserboy 13 hours agoprevIt is dangerous to allow groups of any sort to be excluded from licensing. This sort of behavior is far too easily weaponized against minority political, opinion, & affinity groups. While I'm not keen on the police having these tools in their hands, who else will be next? Who will be squeezed out of the economy because everyone who provides a particular sort of software or service decides to enact the same policies? This is why software should be sold, not licensed — a word processor should be treated no differently from a loaf of bread. reply tehwebguy 13 hours agoparent\"First they came for the Police state and I said nothing...\" lmao reply devindotcom 13 hours agoprevFYI, Microsoft now says (we contacted before publishing) the policy we cited \"contained an error\" and was intended to only apply to facial recognition, and has been changed in the official code of conduct. We've updated the headline and body to reflect this change. A mod might want to change the HN hed too. reply deusum 13 hours agoprevNo one has pointed out that the AI companies are being sued for 'stealing' vast troves of copyrighted data, and the case is still in progress. If the police departments are complicit, then does copyright stand to lose de facto as AI will only be increasingly implemented while the case slogs through the courts? reply kcb 13 hours agoparentOne bare minimum way to not get your data \"stolen\" is to not personally make it freely available on the internet. reply alwaysrunning 14 hours agoprevHow exactly are they going to stop the NYPD from using this service? Just not register it in their subscription as a service? And does it really matter? If I can use Azure's cognitive search and host my own model that doesn't stop me from using Azure for the same purpose. reply HeatrayEnjoyer 14 hours agoparentUsing it unofficially through unauthorized workarounds brings doubt into the chain of custody and investigation integrity. reply alwaysrunning 14 hours agorootparentI'm not sure the courts would consider it a workaround, it's a legit way to host your own model. https://learn.microsoft.com/en-us/azure/machine-learning/how... reply Maxious 9 hours agorootparentprevClearview AI created an unregulated facial recognition system and let individual cops use it for free as a \"trial\" Their superiors claim they had no idea https://www.abc.net.au/news/2020-07-13/afp-use-of-facial-rec... reply riffic 14 hours agorootparentprevwhen cops and DAs use parallel construction to build a case, \"integrity\" is a mere technicality. https://en.wikipedia.org/wiki/Parallel_construction reply _DeadFred_ 14 hours agorootparentprevOh no, they will have to use parallel construction, something they TOTALLY don't have experience doing. reply karaterobot 13 hours agoparentprevYou can't stop someone from misusing your technology by updating your terms of service, just like you can't stop a criminal by making their behavior against the law. But I don't hold Microsoft responsible for making the police behave a certain way, or for preventing violations of the rules from happening. As you say, it's just not possible. I'd be satisfied if Microsoft consistently shut down any attempts to use their technology in a systematic way, once they discovered a violation of their terms. reply resource_waste 14 hours agoprevIs this just PR stuff? Obviously enterprise tech companies (Google/MS) are going to be working with States. These State have to enforce the social contract so you are going to have blood on your hands. I don't really have skin in the game, I'm at the whim of these parties. reply nicklecompte 13 hours agoparentI suspect it's legal. The focus seems to be on face and speech recognition and I am sure MSFT's internal metrics on the AI's ability to recognize non-white faces/accents/etc are very bad. Even with LLMs, the existence of lawyers unknowingly using ChatGPT confabulations in court suggests that police officers will do the same with investigations or surveillance. A cop who takes Sam Altman's dumb tweets too seriously might think Azure's GPT-4 service can look at an evidence report and apply its AI Magic. Probably not something Microsoft wants to defend in court. reply warkdarrior 13 hours agoparentprevThis is a great opportunity for open-source models to get a foothold in this lucrative market. reply hm-nah 14 hours agoprevIMHO, speech-to-text to LLMs is borderline spycraft already. “Are you wearing a wire?” The answer doesn’t mean merely recording the conversation. Now it could mean, a personalized “analyst” determining sentiment, correlating events/evidence/facts/(hallucinations)/building a prosecutors case…in real time! reply MeImCounting 14 hours agoprev [–] This seems like a good thing. I would not trust US police to hold themselves to literally any standard of ethics as far as automation of this sort is concerned. I can easily see departments ignoring the outputs that dont fit into their agenda while running running with the outputs they do like. I would like to see more companies restricting what contracts theyll take based on ethics. reply devmor 14 hours agoparentUnfortunately that’s not what’s happening here. The police still have access to this information and tooling. They just have a different license agreement and access point that allows Microsoft to extract more tax dollars. reply Suppafly 14 hours agorootparentThere is probably an actual reason for it other than corporate greed. I'm guessing they have to silo the data separately on their cloud and that costs more. It's the same reason healthcare applications often cost more to host, they can't intermingle data in cloud instances and have to insure that it's only hosted in the US and what not. I imagine having to deal with legal procedures ensures a lot of additional time/money compared to other customers. reply devmor 13 hours agorootparentOh I would certainly bet there's a lot of reasons it costs more money too. I'm just being reductivist and pessimist about it! reply ben_jones 14 hours agoparentprev [–] AI is a giant liability shield. You can blatantly steal shit and profit of it. Of course enforcement agencies are going to have a field day with it. Announcements like this are just a pretense that they aren’t already counting the money. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Microsoft prohibits U.S. police from employing generative AI for facial recognition in Azure OpenAI Service to address concerns about biases and risks in law enforcement technology.",
      "The ban covers real-time facial recognition on mobile cameras for U.S. police but not international law enforcement agencies.",
      "This policy shift stems from criticism of utilizing AI in law enforcement and highlights Microsoft's collaboration with OpenAI across various government entities like the Department of Defense."
    ],
    "commentSummary": [
      "Microsoft has banned the use of their facial recognition AI by U.S. police, sparking discussions on surveillance, drug laws, crime rates, and data collection by private and government entities.",
      "The conversation extends to OpenAI's policy shift on military utilization, holding tech giants responsible, anti-competitive behaviors, and Microsoft's involvement in overseeing facial recognition tech.",
      "Key concerns include biased recognition, ethical issues, and the absence of standards in AI usage by police departments."
    ],
    "points": 233,
    "commentCount": 119,
    "retryCount": 0,
    "time": 1714675891
  },
  {
    "id": 40235968,
    "title": "Backblaze Q1 2024 Drive Stats: Reliable 16TB Models Lead the Pack",
    "originLink": "https://www.backblaze.com/blog/backblaze-drive-stats-for-q1-2024/",
    "originBody": "Backblaze Drive Stats for Q1 2024 May 2, 2024 byAndy Klein // 4 Comments As of the end of Q1 2024, Backblaze was monitoring 283,851 hard drives and SSDs in our cloud storage servers located in our data centers around the world. We removed from this analysis 4,279 boot drives, consisting of 3,307 SSDs and 972 hard drives. This leaves us with 279,572 hard drives under management to examine for this report. We’ll review their annualized failure rates (AFRs) as of Q1 2024, and we’ll dig into the average age of drive failure by model, drive size, and more. Along the way, we’ll share our observations and insights on the data presented and, as always, we look forward to you doing the same in the comments section at the end of the post. Hard Drive Failure Rates for Q1 2024 We analyzed the drive stats data of 279,572 hard drives. In this group we identified 275 individual drives which exceeded their manufacturer’s temperature specification at some point in their operational life. As such, these drives were removed from our AFR calculations. The remaining 279,297 drives were divided into two groups. The primary group consists of the drive models which had at least 100 drives in operation as of the end of the quarter and accumulated over 10,000 drive days during the same quarter. This group consists of 278,656 drives grouped into 29 drive models. The secondary group contains the remaining 641 drives which did not meet the criteria noted. We will review the secondary group later in this post, but for the moment let’s focus on the primary group. For Q1 2024, we analyzed 278,656 hard drives grouped into 29 drive models. The table below lists the AFRs of these drive models. The table is sorted by drive size then AFR, and grouped by drive size. Notes and Observations on the Q1 2024 Drive Stats Downward AFR: The AFR for Q1 2024 was 1.41%. That’s down from Q4 2023 at 1.53%, and also down from one year ago (Q1 2023) at 1.54%. The continuing process of replacing older 4TB drives is a primary driver of this decrease as the Q1 2024 AFR (1.36%) for the 4TB drive cohort is down from a high of 2.33% in Q2 2023. A Few Good Zeroes: In Q1 2024, three drive models had zero failures: 16TB Seagate (model: ST16000NM002J) Q1 2024 drive days: 42,133 Lifetime drive days: 216,019 Lifetime AFR: 0.68% Lifetime confidence interval: 1.4% 8TB Seagate (model: ST8000NM000A) Q1 2024 drive days: 19,684 Lifetime drive days: 106,759 Lifetime AFR: 0.00% Lifetime confidence interval: 1.9% 6TB Seagate (model: ST6000DX000) Q1 2024 drive days: 80,262 Lifetime drive days: 4,268,373 Lifetime AFR: 0.86% Lifetime confidence interval: 0.3% All three drives have a lifetime AFR of less than 1%, but in the case of the 8TB and 16TB drive models the confidence interval (95%) is still too high. While it is possible the two drives models will continue to perform well, we’d like to see the confidence interval below 1%, and preferably below 0.5%, before we can trust the lifetime AFR. With a confidence interval of 0.3% the 6TB Seagate drives delivered another quarter of zero failures. At an average age of nine years, these drives continue to defy their age. They were purchased and installed at the same time back in 2015, and are members of the only 6TB Backblaze Vault still in operation. The End of the Line: The 4TB Toshiba (model: MD04ABA400V) are not in the Q1 2024 Drive Stats tables. This was not an oversight. The last of these drives became a migration target early in Q1 and their data was securely transferred to pristine 16TB Toshiba drives. They rivaled the 6TB Seagate drives in age and AFR, but their number was up and it was time to go. The Secondary Group As noted previously, we divided the drive models into two groups, primary and secondary, with drive count (>100) and drive days (>10,000) being the metrics used to divide the groups. The secondary group has a total of 641 drives spread across 27 drive models. Below is a table of those drive models. The secondary group is mostly made up of drive models which are replacement drives or migration candidates. Regardless, the lack of observations (drive days) over the observation period is too low to have any certainty about the calculated AFR. From time to time, a secondary drive model will move into the primary group. For example, the 14TB Seagate (model: ST14000NM000J) will most likely have over 100 drives and 10,000 drive days in Q2. The reverse is also possible, especially as we continue to migrate our 4TB drive models. Why Have a Secondary Group? In practice we’ve always had two groups; we just didn’t name them. Previously, we would eliminate from the quarterly, annual, and lifetime AFR charts drive models which did not have at least 45 drives, then we upped that to 60 drives. This was okay, but we realized that we needed to also set a minimum number of drive days over the analysis period to improve our confidence in the AFRs we calculated. To that end, we have set the following thresholds for drive models to be in the primary group. Review Period Drive Count per Model Drive Days per Model Quarterly >100 drives >10,000 drive days Annual >250 drives >50,000 drives days Lifetime >500 drives >100,000 drive days We will evaluate these metrics as we go along and change them if needed. The goal is to continue to provide AFRs that we are confident are an accurate reflection of the drives in our environment. The Average Age of Drive Failure Redux In Q1 2023 Drive Stats report, we took a look at the average age in which a drive fails. This review was inspired by the folks at Secure Data Recovery who calculated that based on their analysis of 2,007 failed drives, the average age at which they failed was 1,051 days or roughly 2 years and 10 months. We applied the same approach to our 17,155 failed drives and were surprised when our average age of failure was only 2 years and 6 months. Then we realized that many of the drive models that were still in use were older (much older) than the average, and surely when some number of them failed, it would affect the average age of failure for a given drive model. To account for this realization, we considered only those drive models that are no longer active in our production environment. We call this collection retired drive models as these are drives that can no longer age or fail. When we reviewed the average age of this retired group of drives, the average age of failure was 2 years and 7 months. Unexpected, yes, but we decided we needed more data before reaching any conclusions. So, here we are a year later to see if the average age of drive failure we computed in Q1 2023 has changed. Let’s dig in. As before we recorded the date, serial_number, model, drive_capacity, failure, and SMART 9 raw value for all of the failed drives we have in the Drive Stats dataset back to April 2013. The SMART 9 raw value gives us the number of hours the drive was in operation. Then we removed boot drives and drives with incomplete data, that is some of the values were missing or wildly inaccurate. This left us with 17,155 failed drives as of Q1 2023. Over this past year, Q2 2023 through Q1 2024, we recorded an additional 4,406 failed drives. There were 173 drives which were either boot drives or had incomplete data, leaving us with 4,233 drives to add to the 17,155 failed drives previous, totalling 21,388 failed drives to evaluate. When we compare Q1 2023 to Q1 2024 we get the table below. The average age of failure for all of the Backblaze drive models (2 years and 10 months) matches the Secure Data Recovery baseline. The question is, does that validate their number? We say, not yet. Why? Two primary reasons. First, we only have two data points, so we don’t have much of a trend, that is we don’t know if the alignment is real or just temporary. Second, the average age of failure of the active drive models (that is, those in production) is now already higher (2 years and 11 months) than the Secure Data baseline. If that trend were to continue, then when the active drive models retire, they will likely increase the average age of failure of the drive models that are not in production. That said, we can compare the numbers by drive size and drive model from Q1 2023 to Q1 2024 to see if we can gain any additional insights. Let’s start with the average age by drive size in the table below. The most salient observation is that for every drive size that had active drive models (green), the average age of failure increased from Q1 2023 to Q1 2024. Given that the overall average age of failure increased over the last year, it is reasonable to expect that some of the active drive size cohorts would increase. With that in mind, let’s take a look at the changes by drive model over the same period. Starting with the retired drive models, there were three drive models totalling 196 drives which moved from active to retired from Q1 2023 to Q1 2024. Still, the average age of failure for the retired drive cohort remained at 2 years 7 months, so we’ll spare you from looking at a chart with 39 drive models where over 90% of the data didn’t change Q1 2023 to Q1 2024. On the other hand, the active drive models are a little more interesting, as we can see below. In all but the two drive models (highlighted), the average age of failure for each drive model went up. In other words, active drive models are, on average, older when they fail, than one year ago. Remember, we are testing the average age of the drive failures, not the average age of the drive. At this point, let’s review. The Secure Data Recovery folks checked 2,007 failed drives and determined their average age of failure was 2 years and 10 months. We are testing that assertion. At the moment, the average age of failure for the retired drive models (those no longer in operation in our environment) is 2 years and 7 months. This is still less than the Secure Data number. But, the drive models still in operation are now hitting an average of 2 years and 10 months, suggesting that once these drive models are removed from service, the average age of failure for the retired drive models will increase. Based on all of this, we think the average age of failure for our retired drive models will eventually exceed 2 years and 10 months. Further, we predict that the average age of failure will reach closer to 4 years for the retired drive models once our 4TB drive models are removed from service. Annualized Failures Rates for Manufacturers As we noted at the beginning of this report, the quarterly AFR for Q1 2024 was 1.41%. Each of the four manufacturers we track contributed to the overall AFR as shown in the chart below. As you can see, the overall AFR for all drives peaked in Q3 2023 and is dropping. This is mostly due to the retirement of older 4TB drives that are further along the bathtub curve of drive failure. Interestingly, all of the remaining 4TB drives in use today are either Seagate or HGST models. Therefore, we expect the quarterly AFR will most likely continue to decrease for those two manufacturers as over the next year their 4TB drive models will be replaced. Lifetime Hard Drive Failure Rates As of the end of Q1 2024, we were tracking 279,572 operational hard drives. As noted earlier, we defined the minimum eligibility criteria of a drive model to be included in our analysis for quarterly, annual and lifetime reviews. To be considered for the lifetime review, a drive model was required to have 500 or more drives as of the end of Q1 2024 and have over 100,000 accumulated drive days during their lifetime. When we removed those drive models which did not meet the lifetime criteria, we had 277,910 drives grouped into 26 models remaining for analysis as shown in the tale below. With three exceptions, the confidence interval for each drive model is 0.5% or less at 95% certainty. For the three exceptions: the 10TB Seagate, the 14TB Seagate, and 14TB Toshiba models, the occurrence of drive failure from quarter to quarter was too variable over their lifetime. This volatility has a negative effect on the confidence interval. The combination of a low lifetime AFR and a small confidence interval is helpful in identifying the drive models which work well in our environment. These days we are interested mostly in the larger drives as replacements, migration targets, or new installations. Using the table above, let’s see if we can identify our best 12, 14, and 16TB performers. We’ll skip reviewing the 22TB drives as we only have one model. The drive models are grouped by drive size, then sorted by their Lifetime AFR. Let’s take a look at each of those groups. 12TB drive models: The three 12TB HGST models are great performers, but are hard to find new. Also, Western Digital, who purchased the HGST drive business a while back, has started using their own model numbers of these drives, so it can be confusing. If you do find an original HGST make sure it is new as from our perspective buying a refurbished drive is not the same as buying a new. 14TB drive models: The first three models look to be solid—the WDC (WUH721414ALE6L4), Toshiba (MG07ACA14TA), and Seagate (ST14000NM001G). The remaining two drive models have mediocre lifetime AFRs and undesirable confidence intervals. 16TB drive models: Lots of choice here, with all six drive models performing well to this point, although the WDC models are the best of the best to date. The Hard Drive Stats Data It has now been eleven years since we began recording, storing and reporting the operational statistics of the hard drives and SSDs we use to store data in the Backblaze data storage cloud. We look at the telemetry data of the drives, including their SMART stats and other health related attributes. We do not read or otherwise examine the actual customer data stored. Over the years, we have analyzed the data we have gathered and published our findings and insights from our analyses. For transparency, we also publish the data itself, known as the Drive Stats dataset. This dataset is open source and can be downloaded from our Drive Stats webpage. You can download and use the Drive Stats dataset for free for your own purpose. All we ask are three things: 1) you cite Backblaze as the source if you use the data, 2) you accept that you are solely responsible for how you use the data, and 3) you do not sell this data to anyone; it is free. Good luck and let us know if you find anything interesting. print Published May 2, 2024 Categorized as Cloud Storage, Featured, Featured-Cloud Storage, Hard Drive Stats Tagged B2Cloud About Andy Klein Andy Klein is the Principal Cloud Storage Storyteller at Backblaze. He has over 25 years of experience in technology marketing and during that time, he has shared his expertise in cloud storage and computer security at events, symposiums, and panels at RSA, SNIA SDC, MIT, the Federal Trade Commission, and hundreds more. He currently writes and rants about drive stats, Storage Pods, cloud storage, and more. Related Posts How to Back Up Your Synology NAS to the Cloud April 30, 2024 Backblaze and Parablu Team Up to Elevate Security For Microsoft 365 Users April 25, 2024 Backblaze Wins NAB Product of the Year April 23, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40235968",
    "commentBody": "Backblaze Drive Stats for Q1 2024 (backblaze.com)231 points by TangerineDream 20 hours agohidepastfavorite89 comments MarkG509 17 hours agoI, too, love Backblaze's reports. But they provide no information regarding drive endurance. While I became aware of this with SSDs, HDD manufacturers are reporting this too, usually as a warranty item, and with surprisingly lower numbers than I would have expected. For example, in the Pro-sumer space, both WD's Red Pro and Gold HDDs report[1] their endurance limit as 550TB/year total bytes \"transferred* to or from the drive hard drive\", regardless of drive size. [1] See Specifications, and especially their footnote 1 at the bottom of the page: https://www.westerndigital.com/products/internal-drives/wd-r... reply wtallis 12 hours agoparentThe endurance figures for hard drives are probably derived from the rated number of seek operations for the heads, which is why it doesn't matter whether the operations are for reading or writing data. But that bakes in some assumptions about the mix of random vs sequential IO. And of course the figures are subject to de-rating when the company doesn't want the warranty to cover anything close to the real expected lifespan, especially for products further down the lineup. reply GGO 19 hours agoprevI buy hard drives based on these reports. Thank you Backblaze. reply Scene_Cast2 18 hours agoparentWhere do you buy your drives? Last time I was in the market, I couldn't find a reputable seller selling the exact models in the report. I'm afraid that the less reputable sellers (random 3rd party sellers on Amazon) are selling refurbished drives. I ended up buying a similar sounding but not same model from CDW. reply secabeen 18 hours agorootparentThese are useful data points, but I've found that at my risk tolerance level, I get a lot more TB/$ buying refurbished drives. Amazon has a couple of sellers that specialize in server pulls from datacenters, even after 3 years of minimal use, the vendors provide 5 years of additional warranty to you. reply pronoiac 17 hours agorootparentBuying refurbished also makes it much easier to avoid having the same brand/model/batch/uptime, for firmware and hardware issues. I do carefully test for bad sectors and verify capacity, just in case. reply dehrmann 16 hours agorootparentprevI think you're better off buying used and using the savings for either mirroring or off-site backup. I'd take two mirrored used drives from different vendors over one new drive any day. reply ethbr1 13 hours agorootparentThere was a Backblaze report a while ago that said, essentially, that most individual drives are either immediate lemons or run to warranty. If you buy used, you're avoiding the first form of failure. reply WarOnPrivacy 17 hours agorootparentprev> even after 3 years of minimal use, the vendors provide 5 years of additional warranty to you. The Amazon refurb drives (in this class) typically come with 40k-43k hours of data center use. Generally they're well used for 4½-5yrs. Price is ~30% of new. I think refurb DC drives have their place (replaceable data). I've bought them - but I followed other buyers' steps to maximize my odds. I chose my model (of HGST) carefully, put it thru an intensive 24h test and check smart stats afterward. As far as the 5yr warranty goes, it's from the seller and they don't all stick around for 5 years. But they are around for a while -> heavy test that drive after purchase. reply 2OEH8eoCRo0 12 hours agorootparentprevIndeed- RAID used to stand for Redundant Array of Inexpensive Disks. The point was to throw a bunch of disks together and with redundancy it didn't matter how unreliable they were. Using blingy drives w/ RAID feels counter-intuitive- at least as a hobbyist. reply malfist 14 hours agorootparentprevA lot of those resellers do not disclose that the drive isn't new, even labeling the item as new. GoHardDrive is notorious for selling \"new\" drives with years of power on time. Neither Newegg nor Amazon seem to do anything about those sellers reply hwbunny 10 hours agorootparentprevRefurbed drives have a MUCH HIGHER failure rate. I used to send back lots of drives to Seagate, they come back with the service sticker and that means trouble. YMMV reply nikisweeting 13 hours agorootparentprevLots of good options here: https://diskprices.com/ reply hi_hi 10 hours agorootparentTo save going down this rabbit hole again :-) https://news.ycombinator.com/item?id=39262314 >https://listofdisks.com/ https://www.cpuscout.com/ https://diskprices.com/ https://gpuprices.us/ https://instances.vantage.sh/ https://tvpricesindex.com/ reply dsr_ 12 hours agorootparentprevNote that they list at least one vendor as selling \"New\" drives when they are not even close to being new. reply synack 6 hours agorootparentI think there will eventually be a false advertising lawsuit or some regulatory action against Amazon about this. Until that happens, it’s hard to say for certain which items are used. reply nikisweeting 9 hours agorootparentprevIt's definitely scraped with a few simple queries and not moderated by a human, you have to manually check before buying of course. It just saves a few minutes of time automating the initial search. reply cm2187 18 hours agorootparentprevIn europe lambda tek is my goto for enterprise hardware as a retail customer. reply SoftTalker 17 hours agorootparentprevAnd for stuff like this, many companies will have an approved vendor, and you have to buy what they offer or go through a justification for an exception. reply havaloc 18 hours agorootparentprevB&H has quite a few reply bee_rider 17 hours agorootparentI guess it isn’t that surprising given the path the development took, but it is always funny to me that one of the most reputable consumer tech companies is a photography place. reply fallingsquirrel 17 hours agorootparentSimilar to how the most popular online retailer is a bookstore. Successful businesses are able to expand and I wish B&H the best of luck on that path, we need more companies like them. reply squigz 17 hours agorootparentI'd rather companies stick to one thing and do it well, rather than expand into every industry out there and slowly creep into every facet of society. Like that bookstore that just happens to retail some stuff too. reply ssl-3 16 hours agorootparentB&H seems to be pretty focused on techy things (and cameras of all sorts have always been techy things, though that corner of the tech market that has been declining for a long time now). When they branch out to selling everything including fresh vegetables, motor oil, and computing services, then maybe they might be more comparable to the overgrown bookstore. reply ghaff 12 hours agorootparentThere used to be a much more distinct camera—and all rhe ancillary gear and consumables than there used to be. Though B&H still sells a ton of lighting and audio gear as well as printers and consumables for same. They sell other stuff too but they’re still pretty photo and video-centric, laptops notwithstanding. reply Modified3019 16 hours agorootparentprevI definitely learn towards B&H for electronic things. It’s quite a bit less “internet flea market” that Amazon often is. reply philistine 14 hours agorootparentprevAWB&H alone is a Fortune 500 company reply Wistar 17 hours agorootparentprevI buy most, but not all, of my tech at B&H and have now for more than a decade. Especially peripherals. reply user_7832 13 hours agorootparentprevWhat's the risk of buying Amazon & running a SMART/crystaldisk test? reply speedgoose 15 hours agoparentprevI don’t buy hard drives based on these reports. I buy SSDs and let my cloud providers deal with hard drives. reply bluedino 19 hours agoprev> The 4TB Toshiba (model: MD04ABA400V) are not in the Q1 2024 Drive Stats tables. This was not an oversight. The last of these drives became a migration target early in Q1 and their data was securely transferred to pristine 16TB Toshiba drives. That's a milestone. Imagine the racks that were eliminated reply bombcar 19 hours agoparent> That's a milestone. Imagine the racks that were eliminated I'm imagining about 3/4ths ;) reply djbusby 16 hours agorootparentI'm imagining 4x capacity reply seabrookmx 15 hours agorootparentprev3/4ths of the racks that had 4TB drives, assuming they didn't also expand capacity as part of this. But they run many drive types. reply toomuchtodo 18 hours agoparentprevPerhaps not eliminated, but repurposed with fresh 16TB drives. And the power savings per byte stored! reply Dylan16807 16 hours agoparentprevYeah, but just thinking about it reminds me how annoyed I am that they increased the B2 pricing by 20% last year. Right after launching B2, in late 2015, they made their post about storage pod 5.0, saying it \"enabled\" B2 at the $5/TB price, at 44 cents per gigabyte and a raw 45TB per rack unit. In late 2022 they posted about supermicro servers costing 20 cents per gigabyte and fitting a raw 240TB per rack unit. So as they migrate or get new data, that's 1/5 as many servers to manage, costing about half as much per TB. It's hard to figure out how the profit margin wasn't much better, despite the various prices increases they surely had to deal with. The free egress based on data stored was nice, but the change still stings. Maybe I'm overlooking something but I'm not sure what it would be. In contrast the price increases they've had for their unlimited backup product have always felt fine to me. Personal data keeps growing, and hard drive prices haven't been dropping fast. Easy enough. But B2 has always been per byte. And don't think I'm being unfair and only blaming them because they release a lot of information. I saw hard drives go from 4TB to 16TB myself, and I would have done a similar analysis even if they were secretive. reply philistine 14 hours agorootparentInflation. At the rate it went up the last couple of years, a 20% price increase to put them back on the right side of profits is more than probable. reply Dylan16807 13 hours agorootparentMaybe I wasn't clear, but the hardware costs and the operation costs should all have dropped between 2x and 5x as a baseline before price increases. Inflation is not even close to that level. And those hardware costs already take into account inflation up through the end of 2022. reply justsomehnguy 12 hours agorootparent> e hardware costs and the operation costs should all have dropped between 2x and 5x That would work if they fully recouped the costs of obtaining and running the drives, including racks, PSUs, cases, drive and PSU replacements, control boards, datacenter/whatever costs, electricity, HVAC etc. and generated a solid profit not only to buy all the new hardware but a new yacht for the owners too. But usually that is not how it works, because the nobody sane buys the hardware with the cash. And even if they have a new fancy 240TB/rack units, that doesn't mean they just migrated outright and threw the old ones ASAP. So while there is a 5x lower costs per U for the new rack unit, it doesn't translate to 5x lower cost of storage for the sell. reply Dylan16807 6 hours agorootparentI would sure hope the original units were recouped after 8 years. You can look at their stats and see that the very vast majority of their data is on 12-16TB drives, and most of the rest is on 8TB drives. Even with those not being the very newest and cheapest models, their average server today is a lot denser and cheaper than their brand new servers 8 years ago. reply Moru 14 hours agorootparentprevAlso a storage inflation on the users side. People have more data on bigger drives that wants a backup. reply Dylan16807 13 hours agorootparentThis is B2, the service that charges per byte. More data makes it easier for them to profit. reply gangstead 16 hours agoprevI wonder how the pricing works out. I look at the failure rates and my general take away is \"buy Western Digital\" for my qty 1 purchases. But if you look within a category, say 14TB drives, they've purchased 4 times as many Toshiba drives as WD. Are the vendors pricing these such that it's worth a slightly higher failure rate to get the $/TB down? reply mijamo 16 hours agoparentIf you are a large company owning hundreds of thousands of them and knowing you will have disk failures regardless, maybe. If you own just a few hundreds and a failure costs you money the logic may be completely different. reply Marsymars 14 hours agoparentprevI'd assume so. Also consider that if a drive fails under warranty, and you're already dealing with a bunch of failing drives on a regular basis, the marginal cost to get a warranty replacement is close to zero. reply jpgvm 18 hours agoprevAmazing these have continued. I base my NAS purchase decisions on these and so far haven't led me astray. reply Marsymars 14 hours agoparentHow would they lead you astray? I wouldn't consider a drive failure in a home NAS to indicate that - even their most statistically reliable drives still require redundancy/backup - if you haven't experienced a drive failure yet, that's just chance. reply jpgvm 3 hours agorootparentWell.. that might be true for a lot of normal NAS with 8 drives or less. I on the other hand have a 4U 48 bay Chenbro so drive failures are somewhat significant for me lol. Redundancy wise it's 4 raidz2 vdevs with 12 drives each and backed up to rsync.net I have had 2 drive failures, one was shortly after commissioning and the other happened a few months ago which was pretty random. I'm using HGST drives, specifically 8TB He8 and they have been really solid in operation since 2016. I don't have any spares left now though so when I get back to where the chassis is hosted I will be doing a rebuild onto 16TB drives. On the other hand in my professional life I experienced arrays that had multiple drives fail in quick succession (especially around 2010-2012 era) from less ... reliable brands cough Seagate cough. So I would consider 2 failures from ~1.5M drive hours to be very good and thank Backblaze for convincing me to shell out on these rather more expensive drives. reply paulmd 10 hours agorootparentprevever since backblaze started doing these there has been a dedicated set of seagate fanboys (I know, it’s the oddest thing to fanboy over) who come up with literally any excuse to avoid acknowledging that seagate might have higher than normal failure rates, and that has included throwing shade at “well, you don’t know the failure rate of those Toshibas and WDs in home usage!!!”. reply objektif 14 hours agoparentprevWhich specific ones do you like so far? reply jpgvm 3 hours agorootparentI have had 48 HGST He8 8TB drives online since 2016. 2 failures in that time, one was warranted the other happened recently. reply ncr100 6 hours agoprevhttps://youtu.be/IgJ6YolLxYE This video presents AFR, failure rates, derived from prior backblaze reports, aggregated. Definitely worth a watch if you're interested in this report. reply fencepost 13 hours agoprevAs with every time these come out, Remember that Backblaze's usage pattern is different from yours! Well, unless you're putting large numbers of consumer SATA drives into massive storage arrays with proper power and cooling in a data center. reply Malidir 58 minutes agoprevWhy no Samsung? reply dehrmann 16 hours agoprevI find the stats interesting, but it's hard to actually inform any decisions because by the time the stats come out, who knows what's actually shipping. reply kayson 18 hours agoprevDoes Backblaze ever buy refurbs? I'm guessing not, but I'd be curious to see any data on how failure rates compare after manufacturers recertify. reply jjeaff 17 hours agoparentI can't think of any reason why the lifetime would be any different for a refurb. of course, you need to start from when the drive was originally used. of course, there is probably also some additional wear and tear just due to the removal, handling, and additional shipping of the drives. reply from-nibly 17 hours agoparentprevIn some ways that would be incredibly noisy to test. However it could be a good way to measure the practicality of S.M.A.R.T metrics. Finding out how accurate they are at predicting hdd lifespan would be a great finding. reply SoftTalker 17 hours agorootparentDoes anyone find value in SMART metrics? In my experience, the drives report \"healthy\" until they fail, then they report \"failed\" I've personally never tracked the detailed metrics to see if anything is predictive of impending failure, but I've never seen the overall status be anything but \"healthy\" unless the drive had already failed. reply Sohcahtoa82 17 hours agorootparentThe SMART metrics aren't binary, and any application that is presenting them as binary (Either HEALTHY or FAILED) is doing you a disservice. > I've personally never tracked the detailed metrics to see if anything is predictive of impending failure Backblaze has! https://www.backblaze.com/blog/hard-drive-smart-stats/ reply SoftTalker 16 hours agorootparentFrom that link: From experience, we have found the following five SMART metrics indicate impending disk drive failure: SMART 5: Reallocated_Sector_Count. SMART 187: Reported_Uncorrectable_Errors. SMART 188: Command_Timeout. SMART 197: Current_Pending_Sector_Count. SMART 198: Offline_Uncorrectable. That's good to know, I might start tracking that. I manage several clusters of servers and hard drive failures just seem pretty random. reply vel0city 12 hours agorootparentI've had several hard drives that started gradually increasing a reallocated sector count, then start getting reported uncorrectable errors, then eventually just give up the ghost. Usually whenever reallocated sectors starts climbing a drive is nearing death and should be replaced as soon as possible. You might not have had corruption yet, but its coming. Once you get URE's you've lost some data. However, one time a drive got a burst of reallocated sectors, it stabilized, then didn't have any problems for a long time. Eventually it wouldn't power on years later. reply favorited 13 hours agorootparentprevI've had an M.2 NVMe drive start reporting bad blocks via SMART. I kept using it for non-critical storage, but replaced it as my boot drive. Obviously not the same failure pattern as spinning rust, but I was glad for the early warning anyway. reply Sakos 13 hours agorootparentprevAbsolutely. I've looked at the SMART data of easily over 1000 drives. Many of them ok, many of them with questionable health, many failing and many failed. The SMART data has always been a valuable indicator as to what's going on. You need to look at the actual values given by tools like smartctl or CrystalDiskInfo. Everything you need to evaluate the state of your drives is there. I've never seen an HDD fail overnight without any indication at all. reply hwbunny 10 hours agoparentprevRefurbed drives, at least for Seagate, are terrible. reply sdwr 19 hours agoprevSays the annual failure rate is 1.5%, but average time to failure is 2.5 years? Those numbers don't line up. Are most drives retired without failing? reply scottlamb 19 hours agoparent> Are most drives retired without failing? I'd expect so given that HDDs are still having significant density advancements. After a while old drives aren't worth the power and sled/rack space that could be used for a higher capacity drive. And, yeah, it makes these statistics make more sense together. Edit: plus they are just increasing drive count so most drives haven't hit the time when they would fail or be retired... reply rsync 14 hours agoparentprev\"Are most drives retired without failing?\" Yes, certainly. One can watch both SMART indicators as well as certain ZFS stats and catch a problem drive before it actually fails. I like to remove drives from zpools early because there is a common intermediate state they can fall into where they have not failed out but dramatically impact ZFS performance as they timeout/retry certain operations thousands and thousands of times. reply favorited 13 hours agorootparentWhat's the best way to monitor those ZFS stats? I just rely on scheduled ZFS scrubs, and the occasional `zpool status -v`... reply mnw21cam 11 hours agoparentprevYes, and because of that the numbers on the average time to failure are completely meaningless. The drives the don't ever fail skew the numbers completely. If a fantastically reliable drive were to have 5/5000 drives fail, but they all failed in the first month and then the rest carried on forever, then that would show here as having a lower \"reliability\" than a dire drive where 4000/5000 drives fail after a year. I'd like to see instead something like mean time until 2% of the drives fail. That'd actually be comparable between drives. And yes, it would also mean that some drive types haven't reached 2% failure yet, so they'd be shown as \">X months\". This is what a Kaplan-Meier survival curve was meant for [0]. Please use it. Also, it'd be great to see the confidence intervals on the annualised failure rates. [0] https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator reply codemac 15 hours agoparentprevDrives have warranties, after which the manufacturer doesn't make any claims about it's durability. This could put your fleet at wild and significant risk if things start hitting a wall and failing en masse. You may not be able to repair away if as you're repairing the data you're copying to yet another dying drive. So you have usually a lifetime of drive tput and start/stop values you want to stay under, and depending on how accurate your data is for each drive you may push beyond the drive warranties. But you will generally stop before the drive actually fails. reply bombcar 19 hours agoparentprevObviously yes. At an AFR of 1.5% they'd have to have the drives run for (about) 67 years to have them all retire from failure. (in reality they'd probably have failure rates spike at some point, but the idea stands. And they explicitly said they retired a bunch of 4TBs) reply rovr138 18 hours agoparentprevThey just retired 4TB ones. While they seem to get retired, it's not as quick as we'd think. reply pcurve 14 hours agoprevLooks like WDC reliability has improved a lot in the past decade. Seagate continues to trail behind competitors. I guess they're basically competing on price? Because with data like this, I don't know why anyone running data center would buy Seagate over WD? reply formerly_proven 12 hours agoparentThe WDC models which are only somewhat more expensive than Toshiba or Seagate tend to perform quite a lot worse than those. Models with the same performance are significantly more expensive. reply sakshatshinde 6 hours agoprevCan't thank backblaze enough... reply hwbunny 10 hours agoprevThey are kingkong. After they started publishing these Seagate seemingly stopped selling trash less and less. Had so many Seagate drives going south. Bleh. Would be nice to see SSD drive stats too. There are so many terrible SSDs out there, like SP, which has utter trash controllers. One day your drive gets locked up without any forewarning, and your data just disappears. reply rokkamokka 19 hours agoprevI always click these every time they come up. Can't tell you how much I appreciate them releasing stats like this! reply WarOnPrivacy 16 hours agoprev> A Few Good Zeroes: In Q1 2024, three drive models had zero failures They go on to list 3 Seagate models that share one common factor: Sharply lower drive counts. Backblaze had a lot fewer of these drives. All of theirMaybe your drives were shingled despite labeling suggesting otherwise I'm not ruling that out. The whole debacle was so amazingly tonedeaf that I wouldn't be surprised if they did that behind the scenes. I wrote this at the time: https://honeypot.net/2020/04/15/staying-away-from.html reply freedomben 13 hours agorootparentprevIndeed, and anecdata is weighted so heavily by our minds, even when we are aware of it and consciously look at the numbers. That's what evolution gives us though. The best brains at survival are the ones that learned from their observations, so we're battling our nature by trying to disregard that. I'll never buy another Seagate because of that one piece of shit I got :-D reply redox99 13 hours agoparentprevI've had so many Seagate drives fail that I won't buy Seagate again. If a brand sells bad drives, they should be aware of the reputational damage it causes. Otherwise there is no downside to selling bad drives. reply louwrentius 14 hours agoprev [–] If you buy drives based on there reports, make sure your drives are operating within the same environmental parameters or these stats may not apply reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Backblaze published their Q1 2024 drive stats report, examining 279,572 hard drives, revealing a 1.41% annualized failure rate overall.",
      "The report highlights zero failures in three drive models, discusses drive sizes, manufacturers, and noteworthy findings like the reliability of 6TB Seagate drives and the transition from 4TB Toshiba to 16TB models.",
      "The study indicates a shift towards older drive models failing later in their lifespan, recommending specific drive models for reliability and offering the Drive Stats dataset for free download, accompanied by insights from Andy Klein on cloud storage and computer security."
    ],
    "commentSummary": [
      "The discussion includes topics like the reliability of refurbished hard drives, Backblaze's use of larger and more cost-effective drives, and monitoring SMART metrics to predict drive failure.",
      "Users share mixed responses on buying refurbished drives, cautioning about potential risks, while some rely on Backblaze's drive statistics to guide their purchasing decisions.",
      "Emphasizes the importance of monitoring SMART metrics and retiring drives proactively to prevent data loss."
    ],
    "points": 231,
    "commentCount": 89,
    "retryCount": 0,
    "time": 1714656344
  },
  {
    "id": 40240737,
    "title": "RISC OS 5.30: A Different Computing Era for Raspberry Pi",
    "originLink": "https://www.theregister.com/2024/05/02/rool_530_is_here/",
    "originBody": "OSes 59 Got an old Raspberry Pi spare? Try RISC OS. It is, literally, something else 59 V5.30 arrives – with RPi Wi-Fi support Liam Proven Thu 2 May 2024 // 09:51 UTC The new version of RISC OS, the original native Arm OS, runs on eight or nine Arm-based platforms, including the Raspberry Pi Zero, 1, 2, 3 and 4 – and on that last two, this release supports wireless networking. RISC OS 5.30 comes with quite an assortment of applications, and plenty more are online (click to enlarge) RISC OS 5.30 is the latest release of Acorn's original native operating system for its Arm processors. Original, but not first: As Acornsoft project lead Paul Fellows told the Reg in 2022, what was then called \"Arthur\" supplanted a far more ambitious project called ARX, which never shipped. ROS 5.30 is the first stable release from the RISC OS Open (aka ROOL) project since version 5.28 in 2020. (If you have that, you can upgrade in place.) RISC OS Open project leader Steve Revill, who we interviewed last year, told us: This stable release has been in the works for a long time – we wanted to get it right! The Wi-Fi support comes from a successful combination of partnerships with companies in the RISC OS scene and generous donations from community members to support our bounties. We really hope the welcome addition of Wi-Fi on the Pi makes it easier for people who've never tried this little OS to give it a spin. Acorn's original, and very comprehensive, documentation has been updated for this release, too: There's also a complete User Guide PDF included in our RISC OS Pi download - a 618 page book, also available in print from ROOL or Amazon. We take pride in the quality of our user documentation and believe this sets us apart from many Open Source projects. It also includes a very basic web browser, but one without Javascript – so it's limited (click to enlarge) New owners RISC OS Developments made RISC OS 5 open source in 2018 and it's still in active development under RISC OS Open. That is no trivial project: Although by modern standards this desktop operating system is tiny, significant chunks of it were hand-coded in Arm assembly code – for 20th century hardware. Version 5.30 supports seven platforms: Post-1994 Acorn machines with the IOMD chipset, the Iyonix and Beagleboard hardware we described in 2010, and Elesar's Titanium PC, plus three Arm development boards (the IGEPv5, the OMAP 5432, and the Pandaboard). And, of course, the Raspberry Pi, which is by some distance the cheapest member of the family. For now, RISC OS does not support the Raspberry Pi 5, but it does run on the Pi Zero, 1, 2, 3 and 4, and it's fast and responsive on all of them. We found the new support for the Wi-Fi controllers in the Raspberry Pi 3 and 4 a little clunky – for instance, changes to the network configuration require a reboot. Even so, it's a lot better than nothing. First, you enable the SDIO WLAN interface; then, after a restart, a new Wi-Fi icon appears on the left of the icon bar, which allows you to connect to both 2.4 and 5GHz networks. This is a fairly modernized and refurbished late-1980s single-user GUI-based OS, and that implies some limitations. It was first released the same year as OS/2 1.0, long before Apple System 7 or Windows 3.0. In fact, it'll remind you of Windows 3 on MS-DOS: it's a single-tasking text-mode OS, with networking, on top of which is a graphical desktop that does cooperative multitasking. RISC OS gives applications access to much of the memory map, and so if a program accidentally scribbles over the wrong parts of that address space, the whole computer can freeze up – which in testing our Pi 400 did several times. But saying that, it's an admirably complete OS, in this vulture's opinion, with quite a rich portfolio of applications. RISC OS 5.30 comes with a selection of productivity apps, plus development tools, including a choice of editors, Python, Lua, and a C compiler – and of course with a 32-bit version of BBC BASIC V, a fully structured interpreter which also supports inline Arm assembly language. This is, in a way, a mature OS with an ecosystem and an aftermarket. (Which, we feel we must explicitly spell out, means that quite a few of those third-party applications and drivers will cost you money.) There are emulators that will let you run 20th century Acorn apps that you can find online, but this isn't an emulated vintage environment like Amiga Forever. It's not meant for running games from thirty years ago. This is a native bare-metal OS, built on 1980s roots but updated for 21st century hardware. It's also not an experimental project with little practical use, like Redox OS or Serenity OS, interesting though those are. The RISC OS GUI – called simply the WIMP – will take some getting used to. It has no application menu bars at all, for example. You middle click on things to get at the relevant menu; this GUI only has context menus, nothing else. (What menu is easier to hit than whacking the mouse pointer up to the top of the screen? One where you don't move the mouse at all! The menu is always where your mouse already is.) And yes, there is an icon bar along the bottom where you can bring up menus and windows for running programs and other things; but when you want to do something in an app, you click the menu mouse button in the context of that application rather than look for a menu bar. The idea of having a directory navigation right in the save dialog, so you can choose where to put the file, was a hack invented for the original 128kB Macintosh, because it didn't have enough RAM to show a filer window alongside your app. RISC OS didn't need that: In 1987, it ran on a 32-bit RISC workstation with a meg of RAM, so its Save dialog just has an icon that you drag to the directory window you want. Similarly, this is the OS whose GUI perhaps inspired NeXTstep's Dock, which in turn inspired the Windows 95 taskbar. RISC OS doesn't work like them, because it's often thought they got their ideas from RISC OS. When you run a RISC OS application, all that happens is that it puts an icon in the icon bar. Middle-click that new icon for global options, or in most apps, just left-click it to open a new empty window. The right button doesn't go unused: it's called Adjust and it modifies what a left-click would do. So, for example, left-click a scrollbar to move in that direction, but right-click it to move the other direction. Left-click an \"OK\" button to save your settings, but right-click it to Apply them without closing. It's odd, but in its way, it's more elegant than any other mouse-driven desktop. As you can no doubt tell, the Reg FOSS desk is very fond of RISC OS. It was the first GUI we got to know, on our Archimedes A310 – back when Apple's System 6 was new. That ran best on the Mac IIci, which was $6,269 – in 2024, that's $15¾K or £12½K. When this vulture was 20, our used Archie cost £800 (one kilobuck). RISC OS is a fascinating glimpse into another world. It has almost no influence from the worlds of 32-bit Windows, or MacOS 7 or Mac OS X, or Linux – because they hadn't been invented yet. It has a superbly elegant graphical desktop, but it's almost totally unlike anything else you've ever seen. Imagine if the classic 1980s Motorola 68000 computers – the Atari ST, the Amigas, or the Classic MacOS pre-PowerPC Macs – and their CPUs had kept on developing and evolving into the present day, completely separately from modern world of 64-bit chips and both FOSS and proprietary OSes. That's what RISC OS is: A time-traveler from the 1980s, alive and well, modernized and updated, but almost completely free of any influence from the rest of the 32-bit World Wide Web era. You will find it very disorienting, especially if all you know is post-1990s OSes, but that's part of the fun. Almost everything you could want – web browsers, email, office-type apps, games, dev tools – it's all there, and enough to get you started is here, free and open source. The 2024 release of RISC OS runs fine on a £12 ($15) Raspberry Pi Zero – and the same SD card will boot any model up to the Pi 4 or 400. (But not, for now, the new, 64-bit-only Pi 5.) If you don't have one of those, but have an old Pi 1, 2 or 3 lying in a drawer somewhere, dusty and neglected, dig it out and put RISC OS onto an SD card – even a 2GB card will do – and give it a try. ® Bootnote ROOL is not the only flavor of RISC OS, and the machines listed above aren't the only modern RISC OS hardware in the world – they just happen to be the ones that the RISC OS Open project currently supports. There are some other devices that aren't included in version 5.30 right now, because there are too many outstanding bugs. Some of these have their own separate ports of RISC OS available. This includes the PineBook and PineBook Pro laptops, and the PINE A64 board. In addition, there are two other noteworthy editions. There's a distribution specifically aimed at the Raspberry Pi, called RISC OS Direct, also from RISC OS Developments. We talked to Andrew Rawnsley of RISC OS Developments, who told us: Our expectation is to deliver a version of RISC OS Direct with all this baked in over the summer, but with such a significant piece of engineering, we are making sure to test thoroughly. This is the culmination of a much larger project to modernise RISC OS - new TCP/IP, IPv6, Firewall, Wi-fi, Browser (essential to configure wifi in many instances), and a ready-to-go RISC OS to present it all. Even the UI has seen a lick of paint thanks to Pinboard2. There's also a separately-maintained 26-bit branch of RISC OS for original Acorn hardware, although these days most users run it on x86 PCs or Macs on a commercial emulator called Virtual Acorn. Sponsored: Easing the cloud migration journey Share More about Raspberry Pi RISC More like these × More about Raspberry Pi RISC Narrower topics AVR RISC-V SiFive Broader topics Arm More about Share 59 COMMENTS More about Raspberry Pi RISC More like these × More about Raspberry Pi RISC Narrower topics AVR RISC-V SiFive Broader topics Arm TIP US OFF Send us news",
    "commentLink": "https://news.ycombinator.com/item?id=40240737",
    "commentBody": "Got an old Raspberry Pi spare? Try RISC OS. It is, something else (theregister.com)218 points by m_c 14 hours agohidepastfavorite122 comments ajb 13 hours agoIt was ahead of its time in UX, but rather behind in the foundations. It's a single user system with no real security, and there was no system of shared libraries - to share code between applications, it was usual to put the shared code in a kernel module and call the kernel. Even the standard C library worked this way. Amusingly, when you invoked the system console -which was at a lower level than the gui system, effectively pausing it - the command line appeared at the bottom of the screen and the frozen gui scrolled up as you entered more commands; until you exit the system console. (It was also possible to get a command line in a window, which could do slightly less - I forget exactly what) reply qwerty456127 7 hours agoparent> rather behind in the foundations. It's a single user system with no real security I believe multi-user systems are actually an ancient, outdated rather than a \"modern\" concept. It made sense when computers were huge, expensive and many users shared one even at work, let alone at home. Nowadays computers are almost never shared. Even when people used to have just one home PC per family (during pre-Win7 days) they mostly preferred to disable the sign-in screen and share the whole environment. Nowadays multi-user OS facilities definitely help to build security but they were not designed just for this. Modern security can be done better without an OS-level concept of a user. reply jbverschoor 1 minute agorootparentMulti user is getting replaced by virtualization / containerization. The layers are simply getting shuffled reply justinclift 9 minutes agorootparentprev> Nowadays computers are almost never shared. That doesn't seem very accurate, unless you're meaning strictly personal computers? reply repelsteeltje 1 hour agorootparentprevHaving grown up with RISC OS, I never considered lack of multi user an omission. But the lack of process isolation (or simply: lack of processes) was a real head-ache. No memory protection or hardware access, rudimentary virtual memory \"page mapping\". Any tardy calls to Wimp_Poll() freezing the entire GUI. \"Cooperative multitasking\" also means it's still impossible to do something useful with more than 1 Raspberry Pi core. So: lacking in foundations, yes. But multi user, not so much. reply dvdkon 2 hours agorootparentprevToday, the same OS core is used in phones, laptops, desktops and servers. Explicitly making it single-user means giving up on servers and parts of the market for the former three. You could make the case for removing the concept of users from the kernel and forcing the OS to write some security plugin. The OS could then have a tailored security mechanism for every usecase, but they'd probably end up including users anyway (for the people who do want them). reply rbanffy 1 hour agorootparentOne such case is company-issued phones. They require the company-provided apps to be isolated from any user-specific configuration, creating an effective multi-user system where user apps (such as Apple Music) run with different privileges (and network access) than, say, Teams or Outlook. reply josephg 31 minutes agorootparentI think this is a good idea anyway. I love how if I install some dodgy app on my phone, it can't access the private, stored data of other apps. It can't steal my google or facebook credentials. And it can't cryptolocker my filesystem. My desktop computers are designed with this old \"user security\" model that I don't use at all - since I'm the only user anyway. User security protects ... uh, the operating system I suppose, which I could reinstall in 20 minutes anyway. But we're missing a much more important security boundary - which is between one bad program and all my other stuff. Every program you run today on desktop is inexplicably executed with full permission over all of your private files, and, worse, it has full network access. Its an insanely terrible design. We /could/ retrofit the user security model to help us isolate applications. But personally I think it would be easier to just design and implement something good from scratch. (For the security people in the room, the threat model is a bad program, or single bad npm package gets pulled into a program you run. How do we limit the blast radius?) reply mavhc 44 minutes agorootparentprevEvery app on your phone runs as a different user reply Elyra 4 hours agorootparentprev> Nowadays computers are almost never shared. I find it ironic that Google TV still does not have this feature, it is the one \"computer\" that people probably still share regularly. Sure you can login to multiple accounts and have different \"profiles\", but that just changes home screen recommendations, all the apps and their sessions are shared. So since I share it with a few roommates, we're constantly having to logout and in to our accounts in different apps to access our watch histories, our plex servers, etc. On the other hand, my android phone does multiple accounts perfectly, why can't it work the same way on the TV? reply pjmlp 4 hours agorootparentprevOS security always requires a concept of a user, unless you want to run all processes under the same identity. reply ptman 2 hours agorootparentYou could do with some other concept than users. E.g. namespaces or capabilities. reply pjmlp 43 minutes agorootparentIt is another way to call a \"user\", hence why I used the word indentity instead. reply josephg 26 minutes agorootparentNo its not. There's all sorts of stuff you can do with capabilities that are really awkward to even think about with users. Like, when a database starts, you can hand it the capabilities it needs to access a specific directory, listen on a specific port and append (but not read) a specific log file. User based security is usually done by the folder being owned by the database, or an ACL or something. But with capabilities, you don't need to make a database user or set any flags on the folder, or make sure the database's configuration matches the filesystem permissions. The capability is basically a pre baked file handle that can be used directly. And capabilities can do lots more stuff! They can be more fine grained - eg, \"Whatsapp can only access these specific photos in my camera roll\". And a program can pass a subset of its capabilities to a child process. reply rvense 3 hours agorootparentprevBut if you're sandboxing and running all applications in a separate space, is it really correct to refer to it as \"users\" anymore? reply jl6 3 hours agorootparent3rd party apps are written (controlled) by other people, and you are effectively granting that 3rd party permission to run their code on your device, so the user concept isn’t that far off the mark. Since ~nobody reads the source code of 3rd party apps, security vetting is usually a matter of deciding to trust the app’s author, same as how you would trust anyone else that you give a user account to. reply tech2 2 hours agorootparentYou grant them capabilities, a handle which permits access to a particular directory, one which allows network access (or possibly even further limited than that) etc. No user, just a series of object handles which permit them to perform the task and nothing more. reply jl6 1 hour agorootparentSure, but capabilities and handles are technical terms of art below the level at which regular users need to understand. The idea that an app is another person using your computer is not a terrible abstraction in terms of helping people make sense of what's happening. reply josephg 19 minutes agorootparentMaybe. But they're a bad mental model for software developers because user based security is way more limited than what you can do with capabilities. And they're confusing for users, too. Signal isn't another \"user\" on my phone. Its still me. I just decide what capabilities I grant it on the day. \"Yes, you can use location tracking for now - but only until later in the day.\" reply pjmlp 41 minutes agorootparentprevCapabilites are just another way of doing role management in processes, and require identities for administration anyway. It is yet another way of managing processes identitities. reply josephg 18 minutes agorootparentA capability is not an identity. And the difference matters a great deal in how you build software that takes advantage of capability based security models. reply saagarjha 2 hours agorootparentprevThis is a very Android centric viewpoint ;) reply lproven 1 hour agorootparentUnpacking that witty but cryptic comment: Yes, you can have privilege-based security without user accounts, if you accept that you do not have control over your own hardware because only the OS vendor has administrative rights. In other words: yes, you can have no-sign-in and no user accounts, but it's still there and you don't have admin access to your own computer. Stepping back a level: Smartphone OSes do not show accounts and permissions, but they are still there, just concealed. Same as they still have complex filesystems, but they are hidden. Stepping back another level: This is a bad way to design OSes: when you need to hide away major parts of the functionality, then you shouldn't have that functionality. It should not be in your design in the first place. reply pjmlp 36 minutes agorootparentprevEven iOS has different users for all those processes running on the device. :) reply mikepurvis 7 hours agorootparentprevModern multi-user paradigms also have very weird ideas of what’s shared. Like, installing or updating software is the same permission tier as accessing another user’s documents, wtf? reply eddd-ddde 7 hours agorootparent_Technically_ it doesn't have to be. Super user is super user. It can always access anyone's files. Allowing unrestricted access to super user essentially destroys any sense of security. You can very much allow only access to certain commands under super user, e.g. only allow users to run pacman. Of course now you are trusting that said commands won't leak the permissions. I agree that it's a mess. My personal and biggest issue is not even across user boundaries, but inside a single user. What do you mean my Firefox client can read my .ssh files??? reply mavhc 42 minutes agorootparenthttps://xkcd.com/1200/ reply remedan 1 hour agorootparentprevTo me, a \"modern multi-user paradigm\" is Nix with Home Manager. Where most of my software is installed in my user's environment and not on the system level. Thus, if there were another user on the same machine, we could each manage our own software and updates without affecting the other. reply tedunangst 6 hours agorootparentprevDon't mind me, just updating the software you use to access your documents, nothing to see here, move along. reply wolrah 5 hours agorootparentprev> Like, installing or updating software is the same permission tier as accessing another user’s documents, wtf? To some extent, yes. If I can install software of my own choosing on basically any normal desktop OS that will appear to other users of the system as \"LibreOffice\", \"Firefox\", etc. then I more or less have access to all their data. MacOS is starting to sandbox applications but not by a lot, and of course Windows Store sandboxed apps are more or less dead in the water. reply pjmlp 4 hours agorootparentDo not mix Windows Store (UWP) sandboxes, with Windows sandbox, which not only is pretty much alive, is making its way across Windows 11 updates since last year. Check \"Windows 11 Security\", \"App Isolation\", \"Sandbox\", \"Standard User\", \"Pluton\". https://github.com/dwizzzle/Presentations/blob/master/David%... reply msla 20 minutes agorootparentprevSingle-user means I have to trust everything I run. I mean, some of us here actually remember MS-DOS. It's not a huge secret to us. reply dietr1ch 12 hours agoparentprevSharing code? I like my apps bundling a whole browser to render a couple buttons reply hedora 8 hours agorootparentThe march toward systemd-electron continues. reply qwerty456127 7 hours agorootparentThis can actually make sense - why not make Electron a shared part of the OS instead of so many apps bundling it. As I understand Apple once built its entire GUI system around a flavour of PostScript which was designed for documents typesetting. Now the world just is doing a similar thing with HTML. reply idle_zealot 2 hours agorootparent> why not make Electron a shared part of the OS instead of so many apps bundling it You're describing a system webview, which is a thing on Android, Windows, iOS, and macOS. reply dajtxx 7 hours agorootparentprevBecause I'm old and I'm sick to death of everything being a web app. Because I hate web app programming so it limits my employment options :-) reply josephg 9 minutes agorootparentI'd be more mad about this if building native applications wasn't super shitty these days. Xcode crashes all the time. SwiftUI is terribly documented and buggy. And lots of standard things you see apple do in their apps are basically impossible to do from 3rd party code. And its impossible to debug anything because its all closed source. And windows has about 8 different native UI libraries that all look and feel different, and they're constantly making new ones instead of making one UI be good and well supported. I hate electron with a burning passion. But at least the web has open standards, good debugging tools and modern, performant, well documented and pleasant to use UI libraries like React, SolidJS and so on. Just don't ask about rich text editing on the web. Oh god. Its been decades and its still so shit. reply qwerty456127 7 hours agorootparentprevI feel the same. reply com2kid 7 hours agorootparentprev> This can actually make sense - why not make Electron a shared part of the OS instead of so many apps bundling it. Microsoft did this ages ago, they lost a big anti-trust trial over it. Aside from that, people ship electron because it works the same across different OSs, if you just want to target one OS, you are better off using that OSs native dev toolkit. Although good luck finding anyone who knows how to write native apps for desktops anymore, and if you are on Windows, good luck figuring out what toolkit you are supposed to use now days! And Linux has had the decades long problem of QT vs GTK. So really the only platform with a native toolkit is MacOS, although when it first came out there were actually multiple toolkits to choose from there as well, and now days I think there is some argument over using Swift of not still (not sure, don't keep up). Or you can just use Electron and skip the above mess entirely. reply qwerty456127 6 hours agorootparent> people ship electron because it works the same across different OSs Why does Microsoft build the very VisualStudio installer with Electron then? To me it seems companies ship electron because it's easy to hire a JavaScript developer. reply com2kid 5 hours agorootparent> Why does Microsoft build the very VisualStudio installer with Electron then? > To me it seems companies ship electron because it's easy to hire a JavaScript developer. When I worked at Microsoft, one team I was on, very ironically, had a really hard time finding Windows developers. We actually resorted to drawing straws to see who on the team would have to learn native Windows development! IMHO a large part of the problem is that native Windows development is a career dead end, unless you work at Microsoft, there are relatively few well paying jobs for what is now a niche skillset. reply pjmlp 4 hours agorootparentUnless one works for the games industry, does IT at big corp, embedded devices programming. reply saagarjha 2 hours agorootparentNone of which are particularly sexy, unfortunately. reply pjmlp 44 minutes agorootparentGiven the hoards of candidates for game development and IoT jobs, putting up with the pay and hours, at least two of those are quite sexy. reply pjmlp 4 hours agorootparentprevVisual Studio installer uses WPF. They did indeed try to use Electron, and the backslash was big enough they went back to WPF. reply lelanthran 4 hours agorootparentprev> Although good luck finding anyone who knows how to write native apps for desktops anymore Hi :-) TBH, in a room full of HN regulars, to find a developer to write a native GUI app, you can throw a brick and hire whoever says \"Ow!\" reply pjmlp 4 hours agorootparentprevMicrosoft already did that with Windows 98, it was called Active Desktop. And you're mixing Apple with NeXT (NeXTSTEP) and Sun (NeWS). reply squarefoot 6 hours agorootparentprev> why not make Electron a shared part of the OS instead of so many apps bundling it. Because it's a wrong approach to a problem that was solved decades ago by much smaller and faster system libraries for UI development. reply qwerty456127 5 hours agorootparentApparently these libraries and the languages they are designed to be used with failed to offer sufficiently easy way to implement the UX people want. I myself strongly prefer classic desktop GUIs adhering to the 90s Microsoft and Apple design guidelines, also well-designed (rather than chaotically evolved like JavaScript) programming languages too yet the objective reality seems like that's not what the demand is for - real-life companies and people prefer fast-entry non-proprietary languages like JavaScript and virtually-unlimited expression like what CSS gives. The only libraries I know can technically be good alternatives to Electron are Qt Quick, WPF (and its spinoffs) and JavaFX but they all have downsides which limit their adoption. reply dajtxx 7 hours agoparentprevHome computers didn't need multi-user capability or much in the way of security (other than anti-virus) back then. I'd argue they still don't. I don't think these two things were the problem. I can take or leave shared libraries. They seem to cause a lot of trouble, but so do statics, so I'm on the fence there. But in the context of when this was released it's a non-issue. I'll give you the CLI thing though. If the CLI couldn't be full-featured in a window that was an oversight. reply Symbiote 1 hour agorootparentI don't remember any limitation of the windowed CLI on RISC OS, except it was slower if there was a lot of output. The OS generally had very little usage of the CLI though, since the GUI was present in ROM and booted to the desktop in about 3 seconds. reply pjmlp 4 hours agoparentprevBack in my day we shared code with static libraries, it appears to be quite hip in some Linux distributions nowadays. reply stkdump 5 hours agoparentprevNowadays there is also more and more nothing shared besides the kernel. Look at technologies like electron, docker, flatpak, etc. Similar to multiuser: Security is important, but as applications get more and more powerful it is not about what individual users can and can't do, but what each application can and can't do. reply tom_ 8 hours agoparentprevA previous bit of discussion: https://news.ycombinator.com/item?id=37796610 reply amiga386 11 hours agoparentprevI'm not even sure it was ahead in its UX; it had a three button mouse, and daily operations needed all three mouse buttons. I had to use them for years at school and I feel you could never quite be sure what the third button would do. In some cases, it was like what shift-click does today, in other cases it selected menu items without closing the menu, in other cases it moved windows without bringing them to front and in yet still different cases it opened a _different_ menu to the one the middle-click did. For menus, I feel it was the worst of all systems. The Mac and Amiga had menus consistently at the top of the screen, and the Mac was good for discoverability in that it showed you the menus were there without you having to click a button. Windows also did that, but menus were attached to windows (bleh). RISC OS was worst of all, _every_ menu is a context menu, including app-level menus - and you got different menus depending on whether you middle-clicked on the icon bar icon, or you right-clicked on the icon bar icon. There was no standard file requester, _everything_ was drag and drop; to load a file, you had to drag it onto the application (although yes, default file associations allow you to double-click it). To save, first make sure you've got a filer window of the directory you want to save to open and visible on screen, then middle-click in the window of the file you're working on, navigate to File -> Save -> a tiny box with a file icon appears, you get to type the filename, then you have to _drag_ the file icon to the folder to save. And if you accidentally mouse-out of that box while typing the name, you lose the name. The OS was also ridiculous in some of its APIs, particularly that there were a million and one things under the calls OS_Byte and OS_Word - yes, really, API calls all clustered together because they return a byte or return a word. It's a design holdover from the original BBC Micro's OSBYTE and OSWORD calls. There's also a pile of crap multiplexed behind \"VDU\" calls, and much like terminal emulators, there's a lot of behaviour you can invoke by printing specific control sequences to the text screen, including mode-switching. It had a weird system where _all_ OS calls were either \"I'll handle errors\" (e.g. SWI XOS_WriteC) _or_ \"let the system handle errors\" (SWI OS_WriteC), which in most cases means that if the OS call ever had an error, it stopped and exited your entire program. The problem with this approach is lots of programmers chose to write software that falls over at the slightest provocation, rather than think to deal with every error and decide how much to deal with recovering it. So, for example, let's say you've been working in a paint package on your masterpiece, you save to disk, and there's a read/write error. Goodbye masterpiece. You can get a flavour of its programming environment from http://www.riscos.com/support/developers/prm/ It also had its own filesystem metadata craziness; there were no file extensions, but rather file type metadata saved separately (the Mac also had this madness), and it also saved the \"load address\" along with the file. Nonetheless, what I did like about it was: 1. That the whole OS was ROM-resident and you can boot a device with no media needed at all, within 3 seconds of turning it on. AmigaOS was _nearly_ all ROM-resident, but nonetheless required a boot disk to get to Workbench (all you need on that boot disk to get to Workbench is a ~200 byte program that launches it; it was clearly a deliberate choice to insist on a bootdisk, and I think it would've been better if it didn't) 2. That it pioneered \"an app is a special kind of directory\", so you can keep all your app's assets inside a folder. Mac OS at this time was using the awful resource-fork system to do this, but by Mac OS X it had seen the light and create apps and resource bundles 3. That it had a built-in BASIC interpreter, and this was a very fine BASIC because it had a full assembler built into it and it had fantastic BASIC-to-machine-code interoperability. You could write all the bits that needed to be fast in assembler, while writing the rest in BASIC. There were even a few commercial games released written in BASIC+Assembler. Overall, AmigaOS was a much better OS than RISC OS, but I do still have space in my heart for the plucky British operating system. reply LeoPanthera 11 hours agorootparent> you got different menus depending on whether you middle-clicked on the icon bar icon, or you right-clicked on the icon bar icon Are you sure you're talking about RISC OS? Because you're got the overview right but your details are weird. Right-clicking never pops a menu in RISC OS. Only the middle button does that. That's why the buttons in RISC OS have names. They're not \"left, middle, and right\" they are \"select, menu, and adjust\". The \"everything is an object\" drag-and-drop was absurdly powerful. You're not limited to dragging into a directory, you can equally well \"save\" a file directly into another application, avoiding the middle step of dumping it onto the disk first. Personally, I find that the trend in making all UIs as easy as possible for the beginner to be a step backwards. Yes indeed, beginners can get going quicker, but then you've very quickly learned everything and there's no where to go. The pro user cannot work faster. The pro user cannot do more. We're all stuck behind fisher price interfaces. If you treat users as if they are children, they will always use your software like a child. reply repelsteeltje 1 hour agorootparent> The \"everything is an object\" drag-and-drop was absurdly powerful. You're not limited to dragging into a directory, you can equally well \"save\" a file directly into another application, avoiding the middle step of dumping it onto the disk first. So true! For sure I was biased when I left RISC OS in favour of Windows 3.1 early 90s, but it took me years to get used to the clumsy COPY-CUT-PASTE metaphors still dominant today. reply amiga386 10 hours agorootparentprev> Because you're got the overview right but your details are weird. Having just booted up RedSquirrel, yes, I accept I'm slightly misremembering. Right-clicking doesn't open a menu, it is consistently middle-click. However, it is still somewhat inconsistent. In several applications (for example, !Maestro or a filer icon), left and right click on the icon bar do the same thing, while in others (for example !Edit), only left click opens a new window and right click does nothing. Playing around with RISC OS 3.10 again, I'm also reminded of the nonsense that is menu items with arrows on them (indicating sub-menus, or sub-windows like save boxes) require you to successfully slide the mouse through the arrow to open them. Almost all other menus I've seen will open sub-menus as soon as you land _anywhere_ on the menu item. While drag-and-drop may be powerful, the ergonomics of the UI were atrocious. I don't think anyone at Acorn had heard of Fitts' Law. The tiny save box also required you had exactly the right filer window open, ready and waiting. You couldn't easily change your mind like you can with a file requester (and also with modern MacOS's spring-loaded folders). I also think the OS was designed with the expectation that overlapping windows would be _normal_, and I don't think that's ever been the case, certainly not how I use computers. Most windows I have open are fullscreen, and I switch between them (most commonly with the alt-tab concept that Windows brought). I might have _internal_ windows inside one app's window (for example, multiple code editing windows and terminals in an IDE, or tools, palettes and layers in graphics editing), but only on special occasions do I have two separate _application_ windows visibly open on the same screen, and when I do, they're usually side by side, not overlapping. reply jlarcombe 4 hours agorootparentI still have all my windows overlapping, and I suspect it's because my formative years were spent in RISC OS. Also I still find \"File Open\" dialogs weird - every app having a miniature and less-functional tiny 'Filer' built in... reply ajb 12 hours agoprevAcorn was a curious company. It managed to get incredible amounts of work done, by assigning big projects to individuals instead of teams. My memory is not to be relied on at this lapse of time, but I seem to recall that in the final years there was a browser maintained by one guy, a port of Java by two, and an implementation of directX by another. Obviously all those projects were much smaller back then (around 98) but still, those devs were doing the work of what another company had a team to do. And in fact this does work, as communication overhead is reduced, but in many cases the increase in productivity loses strategically to the slower time to market. reply beeboobaa3 12 hours agoparentFrom personal experience it's amazing how much more productive I am on solo projects vs working with other people. When you're solo you can just go, but in a team everything needs to be discussed or at least communicated. reply DowagerDave 12 hours agorootparentScaling software development has been THE vexing problem since day one. There's no doubt that the most efficient system fits in the head of a single person; the challenge is then what? reply humzashahid98 12 hours agorootparentRelated to this subject is Casey Muratori's video about Conway's law and a possible extension to it. The communication overhead of working in teams, and the fact it's harder to address cross-cutting concerns in them, is a key theme in it. https://www.youtube.com/watch?v=5IUj1EZwpJY There is also Descartes' quote about how a work produced by one master is often better than one in which many are involved, because of the unifying vision. reply ozim 11 hours agorootparentBut one master cannot go to the Moon. It takes coordinated effort of many, many people. The same with making a-bomb, the same with making anything bigger in software. It’s nice that Linus started kernel and GIT but nowadays he’s not writing much code and most likely he would is not able to review personally each and every PR. reply humzashahid98 11 hours agorootparentYou're right. We also have proverbs like \"two heads are better than one\" and \"standing on the shoulder of giants\" so people recognise that both sides are important and have their value. Right now, I'm working on my own on a personal project attempting to do something a little novel and I appreciate being able to go back and refine my ideas/previous code based on things I learn and additional thinking (even rewriting from scratch), when I'm more likely to face friction (like \"stick to the suboptimal approach; it's not that bad\") and cause trouble for teammates if I was working with someone else. So the value of working alone speaks more to me currently than the value of working in teams, but they both have their place. reply codebje 9 hours agorootparentprevEither software development teams are a wonderful metaphor for multithreaded code, or multithreaded code is a wonderful metaphor for software development teams. I'm not sure which. reply calebpeterson 9 hours agorootparentPlot twist: the dining philosophers problem was based on a real-life software development team. reply codebje 6 hours agorootparentEach wants to claim a keyboard and a mouse, but due to new pair programming requirements set by management there is one fewer set of peripherals than there are developers. reply kaspm 12 hours agorootparentprevEfficient but not necessarily better. When I'm solo developing I go back later and I made some questionable decisions a team member would have identified. reply FridgeSeal 6 hours agorootparentMy amusing if cynic take: it’s a function of the number of opinions about how to do it. A project can tolerate 2 easily, 3 in many cases, 4 and above is difficult terrain. To scale up team count, you need to increase the count of “unopinionated, doesn’t really care devs” to prevent too many opinionated devs landing on the same part(s) of the projects and conflicting. Put one or 2 on each pillar of the project - 3 tops if they work together excellently. If a project needs more bodies, drop in unopinionated devs. There’s enough bus factor that they catch each others code, but not so much that it grinds to a halt in communication overhead. reply silisili 12 hours agorootparentprevI've pretty much found the most it scales linearly is 2, and only then in good conditions such as working well together, greenfielding, and something with clear enough boundaries. After that, well, it basically flatlines and even seems to decrease at times. reply LordDragonfang 12 hours agorootparentprevI read The Mythical Man Month recently (first published in 1975), and while some of it is charmingly dated (have a secretary take your code and run it down for you!), it's astonishing how much of its discussion and advice for structuring a team of programmers remains relevant even today. reply xattt 10 hours agorootparentAs a teenager, I misread the title and borrowed it from the library. Imagine my disappointment and embarrassment when I realized that there was no mention of a human-moth hybrid at all. reply VelesDude 8 hours agorootparentprevOld joke but - Two engineers can do in two months what one engineer can do in one month. But this sort of goes with the African proverb, \"If you want to go fast, go alone. If you want to go far, go together.\" reply fidotron 11 hours agoparentprevI recall seeing a Macromedia Director player but never heard of a DirectX port. In any case the lack of hardware floating point in most of their machines was looking like a big mistake by the mid 90s. Their compilers were also way off the pace and that was getting to be a problem. Tbh I think I am slightly bitter about having stuck with Acorn a bit too much, and should have jumped away sooner. It is clear Acorn knew they were toast even before the Risc PC. A lot of these very impressive developments were consequently glorious wastes of time, which is kind of tragic too. reply ajb 9 hours agorootparentI'm not sure if the directX port ever saw the light of day. At that point the top brass were putting their hopes in the 'Network Computer' [1] and Set Top Boxes, so it may have shipped with one of those, or been intended to. [1]https://en.wikipedia.org/wiki/Network_Computer_Reference_Pro... reply repelsteeltje 1 hour agorootparentBrian McCullough's book How the Internet Happened (https://www.amazon.com/How-Internet-Happened-Netscape-iPhone...) angles this in terms of the centralized digital superhighway versus the open distributed internet. (Where thin clients, set top boxes, etc. are in the first group and web browsers and WWW protocol are the second.) Essentially, by 2005 the open internet had won, but the iPhone (or more precisely: the App store) became the dream of a thin client and became the platform NCs had initially targeted — turning the internet into a walled garden with a vengeance. reply VelesDude 8 hours agorootparentprevThe NC is one of those things that feels like it was simply to far ahead of where the technology and the mind set of users was. Nowadays it would have much more traction thanks to SaaS. In tech, one step ahead is an innovator. Two step ahead is a martyr. reply squarefoot 1 hour agorootparent> The NC is one of those things that feels like it was simply to far ahead of where the technology and the mind set of users was. They were just like X dumb terminals that predated them by about a decade: far behind what the technology was offering with storage and computing power becoming cheaper every year. I'm glad they never caught up, and hope the same happens to Chromeboxes/books; I don't want prices of common hardware I use go up because of market shrinkage due to lots of people ditching real computers in favor of dumb terminals where even the simplest service is something that they must access and run remotely with no or reduced local storage/computing power. Sorry for having an unpopular opinion, but to me SaaS is like going back 40-50 years to the mainframes era, and essentially is a way to put everything behind a counter so that users can be charged tomorrow for what today is still free. reply flomo 1 hour agorootparentprevTrivia at this point. But the Oracle Network Computer was a low-end x86 box running FreeBSD and a full-screen Netscape Navigator. Very much a product of its time. (WebTV, later purchased by Microsoft, was the more successful product in this space.) reply ajb 6 hours agorootparentprevYeah, this was back in the days of dial-up. Chromebooks are effectively the modern NC reply VelesDude 3 hours agorootparentA great example. Chromebooks also managed to also take the better ideas of Netbooks and go with them. reply abraae 12 hours agoparentprevObligatory Fred Brooks quote: \"The bearing of a child takes nine months, no matter how many women are assigned.” reply exe34 11 hours agorootparentHah, try getting them to sit through refinement meetings, and I bet it'll take a lot longer! reply gedy 10 hours agoparentprev> but in many cases the increase in productivity loses strategically to the slower time to market. I disagree that it always means slower time to market - if the individual is empowered and minimum process (no PMs, \"grooming\", estimates, etc) a sharp individual can run circles around a full team. reply ajb 7 hours agorootparentWell, \"many cases\" isn't \"always\". A bigger team will get there faster if the development is larger than a certain size, if it's smaller then an individual can win. reply LeoPanthera 13 hours agoprevRISC OS has been limping on thanks to the efforts of some extremely hard-working volunteers, but a roadblock is coming. The Pi 5 drops support for 32-bit ARM code, in which RISC OS written, and since enormous chunks of it are written in assembly, there is no trivial way to port. Even so, it's heartwarming that people continue to put efforts into operating systems that aren't related to Unix or Windows. I'm happy to see people use this, and AmigaOS, and BeOS, and others. Computing shouldn't be a monoculture. reply VelesDude 7 hours agoparentBeOS/Haiku are surprisingly modern considering their age. They made a lot of technology choices at the time that short term were limiting (multi threading everywhere) but long term were very nice. It was just that the short term issues was one of many anchors of them when they needed everything to go their way. Haiku is the closest of these to being daily runner ready but like a lot of systems, lack of driver support prevents it from that lofty goal. It is pretty much the only reason that Linux has been able to go so far. reply fweimer 12 hours agoparentprev> The Pi 5 drops support for 32-bit ARM code Note that EL0 (userspace) support is still present, but RISC OS cannot currently run entirely in userspace. reply z500 13 hours agoprevI kind of went down a rabbit hole last year after the Xerox Star emulator was posted here. It was really cool, but super slow, so I felt like emulating another old system. I ended up writing a GUI calculator for RISC OS in ARM assembly, neither of which I had exposure to before. It was a blast. Very interesting system, it's like visiting a country on the other side of the planet. There's a pretty good emulator, they've got a bundle fully loaded with all kinds of RISC OS tools. https://www.marutan.net/rpcemu/easystart.html reply rbanffy 1 hour agoprevRISC OS is an interesting beast, but I’d LOVE to see RISC/IX ported to the RPi and other small boards. Has anyone saved the sources anywhere? Would whoever now owns IXI IP help with the desktop part? reply repelsteeltje 1 hour agoparentYou mean the Archimedes GUI on top of the BSD kernel? - https://en.wikipedia.org/wiki/RISC_iX I have memories of drooling over those R series machines or anything MIPS/Sparc/Alpha based in the early 90s for that matter. I think ARM systems at the time were never competitive in terms of speed (compared to SUN, SGI, HP), but they were Archimedes' sexy sisters. reply xyzzy3000 14 minutes agorootparentAcorn bought-in a couple of GUIs for RISC iX (Motif mwm/twm in later versions, X.desktop from IXI Ltd in the earlier release), but I'm not sure if any were exclusive, and none looked like the RISC OS GUI - I wouldn't characterise anything about the RISC iX GUI as being particularly 'Archimedes'. There's a video of someone playing around with the GUIs here: https://www.youtube.com/watch?v=8r7vgQsuoT4 If only it had been given the RISC OS GUI with BSD underneath - that would have been way easier to port to modern Unix-like foundations and may have had a healthier-but-niche future as something 'modern' beyond Acorn's existence. reply repelsteeltje 0 minutes agorootparentAh yes, Motif! Now I remember, I stand corrected. Before Gnome, Qt, KDE we of course had motif and X with it's license issues. If I remember correctly, early Slackware CD bundles came with motif as well. lproven 12 hours agoprevThat's my article -- I posted it here too: https://news.ycombinator.com/item?id=40234430 reply sillywalk 7 hours agoparentLink to HN discussion of another of your articles with more RiscOS history. Modernising RISC OS in 2020: is there hope for the ancient ARM OS? 111 points by lproven on Oct 10, 2020hidepastfavorite53 comments https://news.ycombinator.com/item?id=24735766 reply fredley 13 hours agoprevRISC OS was the first graphical OS I ever used - my father ran Sibelius (the very first version I think) on an Acorn computer for engraving music. The three-button mouse approach is totally unique, I'm glad they explain it in this article! reply jnaina 7 hours agoprevStill remember viewing the Acorn Archimedes with the RISC OS for the first time in 1987, when it was launched. Someone wrote a 10 line demo that rolled down the current screen, using a curl motion using bitmap copy/transform, using Basic. Breathtaking performance/speed. Never knew why it bombed in the marketplace. reply repelsteeltje 54 minutes agoparentBest remember that compared to today, there hardly was a market place till late nineties. 70s and early 80s were dominated by hardware enthusiast, 80s game software and a nerdy minority, business PCs had little momentum till late 80s and 90s. All this time, the market was flooded with different types of mutually incompatible micro computers. After DOS compatible PC gained momentum, the only company still (barely) standing was Apple, and it ultimately adopted comorbidity PC hardware too. So I think the reason Acorn (and Commodore, Sinclair, Amstrad, RadioShack as well as Sun, DEC and SGI) went belly up is primarily that while the market was growing, there simply wasn't enough space in the market to compete with wintel dominance. reply btbuilder 6 hours agoprevPress F12 and dust off your Dabs Press BASIC WIMP programming for the Acorn… A machine that you could code up a full GUI application with the BASIC interpreter in ROM, enabling children everywhere to which a C compiler was Unobtainium. reply robin_reala 13 hours agoprevI’ve got fond memories of RISC OS from my school’s room full of Archimedes. I spent a a fair bit of time designing and failing to build a text adventure in Basic, while my teacher tried to get us to learn how to use TechWriter. Best feature was being able to drag a slider to create a RAM disk. Amazingly you can still buy a copy of TechWriter 9.1 for £85! http://www.mw-software.com/software/ewtw/ewtw.html reply bschmidt1 8 hours agoprevRaspberry Pi is amazing. Everything from the size and simplicity of hardware to that of Raspbian. Super moddable, modular, surprisingly performant. As a lot of others do, I use one for most of my sites and apps as a home server. 15+ apps, sites, and APIs running on this thing 24/7/365 over WiFi. What would be the benefit of using RISC OS over Raspbian, or even Ubuntu Server? Is it pure nostalgia like running Windows XP on a Pi? reply sillywalk 7 hours agoparent> What would be the benefit of using RISC OS over Raspbian, or even Ubuntu Server? Is it pure nostalgia like running Windows XP on a Pi? It's small - apps are hundreds of KB to a few MBs and it's fast/responsive. The question is what do you want to use it for? There are apps for most things, but as it's not Unix or Windows it doesn't have a lot of ports of bigger open source apps. reply cess11 50 minutes agoparentprevIt's raw, like if you had a Forth system with a GUI that isn't aggressively obtuse. You can touch everything in it with a BASIC dialect, and at least in some variants drop into assembler and run the hardware somewhat directly. This allows e.g. interesting graphics programming, and there's very little that gets in the way of immediately testing ideas. reply mtillman 3 hours agoprevSingle user issues aside, this looks better than every version of Linux I’ve seen in the past decade. reply BizarreByte 7 hours agoprevUsing RISC OS on an RPI was as close to using a computer for the first time as I've ever gotten again. So many ways of doing things are not bad, but completely alien if you've never done it. It actually gave me an appreciate for how computers must be for those who aren't used to them. reply addamh 6 hours agoprevRISC architecture is gonna change everything reply b800h 13 hours agoprevExcellent, I've been waiting for Wifi support, it's a game changer. Will be mucking around with this at the weekend. reply kaycebasques 11 hours agoprevSkimmed the article, didn't see this important bit mentioned: I'm pretty sure the OS is available through rpi-imager, meaning it's trivial to set up on an RPi and presumably has some notion of official endorsement from RPi org. reply imoverclocked 13 hours agoprevWow, the article itself brings me down memory lane. The one thing that will not bring me into this platform: cooperative multitasking. (Shudder) I’m sure it feels fast though! reply ChrisArchitect 13 hours agoprevMore on official post: https://news.ycombinator.com/item?id=40188348 reply appstorelottery 2 hours agoprevThe most impressive thing to me was the inbuilt BBC Basic, and the availability of every system function to basic with SWI calls. This contrasts with languages like GW Basic or even commercial QuickBasic (QBX) or Visual Basic: second class citizens when it came to accessing system calls under Windows (or dos). In fact, I wrote simple basic code to access the undocumented random number generator within the BCM2835 chip - it worked perfectly under RiscOS and even a dedicated BBC Basic emulutor for the Pico. This is what the code looks like for those that are interested: REM MAP MEMORY SYS \"OS_Memory\",13,&20104000,32 TO ,,,RNG_CTRL% SYS \"OS_Memory\",13,&20104004,32 TO ,,,RNG_STATUS% SYS \"OS_Memory\",13,&20104008,32 TO ,,,RNG_DATA% SYS \"OS_Memory\",13,&2010400C,32 TO ,,,RNG_FF_THRES% SYS \"OS_Memory\",13,&20104010,32 TO ,,,RNG_INT_MASK% REM CHECK WHERE THE REGISTERS ARE MAPPED PRINT “RNG_CTRL MAPPED AT &”;STR$~(RNG_CTRL%) PRINT “RNG_STATUS MAPPED AT &”;STR$~(RNG_STATUS%) PRINT “RNG_DATA MAPPED AT &”;STR$~(RNG_DATA%) PRINT “RNG_FF_THRES MAPPED AT &”;STR$~(RNG_FF_THRES%) PRINT “RNG_INT_MASK MAPPED AT &”;STR$~(RNG_INT_MASK%) REM THESE INTS BECOME REGISTERS R0-R7 RESPECTIVELY A%=1 B%=RNG_DATA% C%=RNG_INT_MASK% D%=RNG_STATUS% E%=RNG_CTRL% F%=&1 G%=&4000000 REM GET A RANDOM NUMBER FROM THE RNG DIM RNG% 30 P% = RNG% [ OPT 1 SWI “OS_EnterOS” LDR R0,[R1] SWI “OS_LeaveOS” MOV PC,R14 ALIGN ] REM INIT THE RNG DIM INIT% 30 P% = INIT% [ OPT 1 SWI “OS_EnterOS” STR R5,[R4] STR R6,[R3] SWI “OS_LeaveOS” MOV PC,R14 ALIGN ] REM LETS INIT… CALL INIT% A%=0 X%=0 FIRST%=0 REM KEEP READING RANDOM NUMBERS UNTIL THEY ACTUALLY BECOME RANDOM REPEAT X%=A% A% = USR (RNG%) PRINT “WARMING UP &”;STR$~(A%) IF FIRST%A% REM DISPLAY RANDOM NUMBERS (RETURNED FROM R0) REPEAT A%=USR (RNG%) PRINT “&”;STR$~(A%);\" \"; UNTIL 1=0 reply wiktor-k 13 hours agoprevIt gives me strong Amiga-vibes and it seems it's open-source now... I like it and will definitely try it out! reply dheera 12 hours agoprevThat UI looks like something out of the late 80s / early 90s. I can smell the VCR head cleaner and scratch-n-sniff stickers and hear the dial-up internet bee-bee-bee-bee-gurglegugrle-dong-ding-dong-ding-whooooooooosh-wheeeeeeeesh-bling! bling whooooo. Can we at least upgrade the fonts, colors, and negative space to make it look more 2020s? reply joosters 12 hours agoparentCan we at least upgrade the fonts, colors, and negative space to make it look more 2020s? Of course YOU can, it's open source, feel free to hack away at it. reply sillywalk 7 hours agoparentprevThere's been the Desktop Modernization Engine / Project https://paolozaino.wordpress.com/portfolio/risc-os-desktop-m... discussion here: https://www.riscosopen.org/forum/forums/1/topics/17696 Development seems to have stalled / slowed down. reply owyn 11 hours agoparentprevI know this is a joke but this OS was first released in 1987... I forgot all about it, but it's pretty cool that you can still run it on hardware. reply 486sx33 11 hours agoprev“ significant chunks of it were hand-coded in Arm assembly code” as all modern OS should be at the kernel level reply saagarjha 2 hours agoparentThank goodness they are not. reply jlarcombe 4 hours agoparentprevgiving them grief now though, because the Pi 5's chipset drops support for 32 bit ARM.. reply guenthert 1 hour agoprev [–] > RISC OS gives applications access to much of the memory map, and so if a program accidentally scribbles over the wrong parts of that address space, the whole computer can freeze up – which in testing our Pi 400 did several times. Enough said. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "RISC OS 5.30 is the latest version of the original Arm OS, compatible with Arm-based platforms like Raspberry Pi, featuring Wi-Fi support and various applications.",
      "The OS, led by Steve Revill from RISC OS Open project, focuses on proper release execution, despite its unique GUI and limitations, offering productivity apps and development tools for modern hardware.",
      "Not supporting vintage application emulation, RISC OS, providing a throwback to a different computing era, is free, open-source, and caters to Raspberry Pi models up to 4 or 400, delivering a distinct user experience from contemporary operating systems."
    ],
    "commentSummary": [
      "The discussion focuses on the user experience of RISC OS, highlighting its lack of multi-user support and the potential for enhancing security measures.",
      "Users express nostalgia and interest in modernizing RISC OS, praising its capabilities on Raspberry Pi, with comparisons to other operating systems like AmigaOS and tools like Electron.",
      "Technical issues like memory map access and worries about future chipset support, along with the potential for hand-coding in Arm assembly code and open-source projects, are also touched upon."
    ],
    "points": 218,
    "commentCount": 122,
    "retryCount": 0,
    "time": 1714679759
  },
  {
    "id": 40241734,
    "title": "Boeing Whistleblower's Death Sparks Investigation",
    "originLink": "https://www.newshub.co.nz/home/world/2024/05/whistleblower-josh-dean-of-boeing-supplier-spirit-aerosystems-dies-of-sudden-illness.html",
    "originBody": "Aviation Whistleblower Josh Dean of Boeing supplier Spirit Aerosystems dies of 'sudden illness' 02/05/2024 Another whistleblower who publicly spoke out about safety issues with Boeing planes has died, less than two months after fellow whistleblower John Barnett died from a gunshot wound police have yet to finish investigating. Joshua Dean, a former quality auditor at Boeing supplier Spirit AeroSystems and one of the first to allege wilful ignorance of manufacturing defects on the notorious 737 MAX, died after a \"short and sudden illness\", the Seattle Times reports. The 45-year-old was reportedly \"known for a healthy lifestyle\" but fell ill and was admitted to hospital a little over two weeks ago due to breathing difficulties. He was subsequently diagnosed with pneumonia and a severe bacterial infection known as MRSA. Despite various treatments, his condition worsened rapidly before it was revealed he had suffered a stroke, and Dean's mother posted on Facebook on April 26 that he was \"fighting for his life\". He died Tuesday morning (local time), the Seattle Times quotes his aunt Carol Parsons as confirming. A Spirit spokesperson said: \"Our thoughts are with Josh Dean's family. This sudden loss is stunning news here and for his loved ones.\" More from Newshub Boeing knew of MAX issue before fatal crashes Want to easily ensure you're not flying on Boeing planes? Here's how Ethiopian Airlines crash has 'suspicious links' to Lion Air crash - expert Watch: Shocking moment engine cover falls off Boeing plane during takeoff Dean and Barnett were both represented by the same legal company in South Carolina. After Barnett died from a gunshot wound in Charleston, the same South Carolina city Boeing has its 787 manufacturing facility, the coroner reported his death appeared to be \"self-inflicted\"; but the police are yet to complete their investigation into his death. \"They found him in his truck. I don't know what to say. I've never experienced anything like this in my life,\" one of his lawyers Brian Knowles told media. Boeing released a statement at the time saying the company was \"saddened\" by Barnett's death. Knowles told the Seattle Times he would not speculate on the close timing and circumstances of the two deaths. “Whistleblowers are needed. They bring to light wrongdoing and corruption in the interests of society. It takes a lot of courage to stand up,” Knowles said. \"It's a difficult set of circumstances. Our thoughts now are with John’s family and Josh’s family.\" A recent deposition and complaint filed by Dean have brought attention to what he alleged was \"serious and gross misconduct by senior quality management\" within Spirit's 737 production line. It came amid heightened scrutiny of a Boeing 737 MAX aircraft and former colleagues of Dean have corroborated some of his claims. Barnett, who was 62 when he died, was a veteran of 32 years at Boeing and was well known for his vocal criticism of the company's alleged decline in production standards. He died amid providing testimony in a high-profile legal case against Boeing, having testified just days prior and shortly before he was scheduled for further questioning.",
    "commentLink": "https://news.ycombinator.com/item?id=40241734",
    "commentBody": "[flagged] Second Boeing Whistleblower Dies in Less Than Two Months (newshub.co.nz)211 points by sarimkx 12 hours agohidepastfavorite71 comments gnabgib 12 hours agoOngoing discussion [0] (367 points, 22 hours ago, 442 comments) [0]: https://news.ycombinator.com/item?id=40230790 reply plasticeagle 11 hours agoprevYou'd think they'd just put him on a plane, they're much better at killing people using planes. reply jtriangle 11 hours agoparentLast I checked they haven't killed any passengers in awhile. Really not since: https://en.wikipedia.org/wiki/Ethiopian_Airlines_Flight_302#... Which was due to MCAS malfunction and pilots responding poorly to it. I'm all for holding Boeing's feet to the fire, but, lets not devolve into hyperbole here. reply rezonant 11 hours agorootparentGood point, the statute of limitations for killing your customers should probably only be two years. That's, like, ancient history in tech years! GPT 4 wasn't even released at that point! We can't keep holding them accountable for such negligence, after all they've been really really good in the two years since- they only let a door fall off of a plane once! And no one died from that! We don't need to mention that if the seat next to the door was filled and they were not wearing their seatbelt, it is highly likely they would have died. EDIT: My bad, it was 5 years ago, not 2 years ago. I skimmed the Wikipedia page too fast, much like Boeing's engineers did while trying to bypass FAA regulations. I should be punished, but Boeing did nothing wrong. reply akerl_ 11 hours agorootparentThis level of snark isn’t really productive to the conversation. Additionally, 2019 was 5 years ago. reply rezonant 9 hours agorootparentOh that's fair, you're right, that accident happened 5 years ago. Even more reason to excuse them from doing engineering badly! That is basically the beginning of the universe! reply Civitello 11 hours agorootparentprevdepends on the goal of the speaker reply 7thaccount 11 hours agorootparentprevNo. In the case of Ethiopian airlines they knew exactly what was going on when MCAS kicked in and almost immediately responded. MCAS pushed the plane down SO hard that something in the tail got stuck. Boeing spent a lot of time trying to blame this on pilots when 1.) they initially didn't even train them on it, and 2.) even if trained...once it happens...you have extremely little time to react. reply pbsladek 11 hours agorootparentprevAgreed. Responding poorly maybe but all blame goes to Boeing. Pilots didn’t even know it existed and only got training and an override in 2021 from your link. reply phyzome 11 hours agorootparentprev5 years is \"a while\"? reply contravariant 11 hours agoparentprevTricky as that would require some kind of quality control. reply coolbreezetft24 11 hours agoparentprevHow many people have died on boeing plane in the past year? reply vandyswa 11 hours agoprevIf the intention is to convince whistleblowers #3 through #30 that they should find a healthier hobby? Then yeah, they don't really care if it's obvious to the peanut gallery, so long as it's obvious to any other witnesses. reply mulmen 10 hours agoparentDo you have any evidence whatsoever to suggest foul play here? reply gigatexal 12 hours agoprev1 whistleblower death is an outlier. Two is an eye raiser. Three will be a pattern and point to Boeing no? reply ndr 11 hours agoparent\"Once is happenstance. Twice is coincidence. Three times is enemy action\" ― Ian Fleming, Goldfinger reply pcloadletter_ 12 hours agoparentprevTwo out of how many though? Yeeeesh reply digitalsushi 12 hours agorootparent2 out of 30 maybe reply pfdietz 12 hours agorootparentprevYeah, this may just indicate there are lots of whistleblowers. reply sslayer 11 hours agorootparentIf its Boeing, I'm not Blowing! reply tomcam 11 hours agorootparentAnd to think I was almost ready to hire you for my branding needs reply dyauspitr 11 hours agoparentprevThough this guy seemed really sick so him taking his own life doesn’t seem insane. Does boeing have the ability to infect people with MRSA? reply codersfocus 11 hours agorootparent>Does boeing have the ability to infect people with MRSA? Reminds the old soviet ricin umbrella injector https://en.wikipedia.org/wiki/Bulgarian_umbrella reply wavefunction 11 hours agorootparentprevMRSA has some benefits to consider alongside its detriments. It is capable of surviving on surfaces so you could apply it to a surface that your target interacts with and be relatively confident that it will be viable when the target contacts it. It's common, so it's not some sort of exotic bio-weapon that screams cloak-and-daggers: you can get MRSA in a hospital even. I think it's pretty resilient too so once your target has contacted the MRSA simple hygienic practices wouldn't neutralize it like taking a shower or bath. But as far as I know it is very treatable by knowledgeable medical professionals if caught early so in that sense it's not a guaranteed attack-vector and a poor choice for an assassination. Depending on a target who doesn't visit the doctor until too late, maybe less of an issue. reply Enginerrrd 11 hours agorootparentIt also colonizes a large fraction of hospital workers so unless you injected it into the victim or got them to inhale a large number of aerosolized bacteria, it won't be effective. Assassination seems unlikely here to me. The presentation of sudden illness with multiple agents flu B, MRSA, and strep all at the same time is unusual. The presentation probably IS consistent with a sudden exposure to a very high number of infectious organisms into his lungs which could overwhelm the immune system and potentially lead to death. It wouldn't be impossible to orchestrate but it sounds difficult, finicky, unreliable, and tough to correctly execute without collateral damage. It would almost certainly require a state actor. It's also plausible he contracted influenza B and suffered from an opportunistic infection of the lungs. By Occam's razor I think the latter is quite a bit more likely. reply moralestapia 11 hours agorootparentprev>and a poor choice for an assassination And yet, it worked. reply Rebelgecko 11 hours agorootparentIs there any evidence he was assassinated or are people just trying to meme it into being the truth? reply moralestapia 10 hours agorootparentI mean the guy died from MRSA. So, that thing is somewhat effective at killing people. reply dyauspitr 9 hours agorootparentIt’s also somewhat treatable if caught early. reply sarimkx 12 hours agoparentprevI hope there is isn't a third! Pretty insane already... reply groby_b 11 hours agoparentprevIt's... just barely probable with two people. Assuming 30 whistleblowers, and an actuarial likelihood of 1.2% death[1] in a given year at their ages, we can use binomial probability[2] to determine the chance of two dying in a given year. The math works out to a 4.4% chance. That's a bit of an eye raiser, but still just believable. (Look, we all believe it can happen we roll a critical in D&D ;) 3 or more dying in a year, we're at 0.5% - I'd really assume that's deliberate. [1] Actuarial tables: https://www.ssa.gov/oact/STATS/table4c6.html [2] Calculator if you want to play with probabilities: https://www.wolframalpha.com/input?i=binomial+probability+ca... reply htss2013 10 hours agorootparentCan we not make a reasonable assumption that whistleblowers against a powerful company have a much higher than average risk of dying? Knowing nothing about them personally we can reasonably conclude: - They get regular death threats. - The opposing law firms probably have PIs digging into their lives for dirt to discredit them. - Their careers in their line of work are over and as far as I know, there's no whistle-blower retirement fund. - Everyone they knew and worked with for many years and who still works there probably cut them off for fear of being associated with them. - Because of all this their sleep is probably horrible, they're much more likely to have unhealthy alcohol habits, and to have financial problems. reply groby_b 9 hours agorootparentMaybe. But these are all random assumptions, and we have no way of taking them into account without a lot of work on the modeling. But the point I was trying to make (badly, based on where the discussion is going :) was that \"two whistleblowers dying is still somewhat probable\". Increased likelihood of death just makes it more probable. reply vinaypai 11 hours agoprevThe assumption here is that Boeing is responsible for these. What exactly does Boeing gain from murdering whistleblowers after the fact? At this point Boeing's reputation is already tarnished and they're already going to be under a microscope, so what does murdering whistleblowers even achieve? reply thenberlin 11 hours agoparentA deterrent to prevent additional whistleblowers from coming forward? If the issues are as massively systemic as facts to date suggest, it seems like there might be a number of other shoes that could drop. Boeing's reputation (as well as that of its higher ups) could get a hell of a lot more tarnished and even breach further into legal culpability territory. I'm not fully convinced that's what's happening here, but two whistleblowers on a massive US company that also happens to be a defense contractor dying in suspicious and unusual ways (especially that first one \"killing himself\" after he said if he died it wasn't suicide and also smack in the middle of his deposition days...) certainly warrants a non-trivial amount of concern and a deeper investigation. reply htss2013 11 hours agorootparentThis assumes the risk of getting caught offing whistleblowers is less than the risk of more whistle-blowers coming out. Does it really make sense that you'd risk exposing a whistle blower murder program as opposed to whatever corporate problems they have? Not to mention, it's a lot easier to smooth over corporate screw ups than getting caught hiring hit men. reply LightHugger 8 hours agorootparentIf these are hits, i would think it's one or two rogue execs or stakeholder with a lot of personal money to blow doing this, OR they are leveraging US military contacts to get it done. It's probably impossible to keep an assassination program from leaking carried out in any other way. reply ssalka 11 hours agoparentprevI think if Boeing care's about their reputation so badly, they'd get out of the defense contractor business. But there's too much money to be made there. Silencing whistleblowers could be very beneficial to their bottom line: * Protects their existing IP, as well as any classified information the whistleblower may have had access to. * Prevents further leaks from the whistleblower, which could impact existing/future contracts, or the company's ability to win them. * Sends a chilling message to future would-be whistleblowers. * Sends a reassuring message to the defense industry: \"we have a zero-tolerance policy for leaking information and your business is safe with us.\" reply remarkEon 11 hours agorootparent>I think if Boeing care's about their reputation so badly, they'd get out of the defense contractor business. Huh? Boeing's defense products are a separate division from commercial aviation. And those products are the best out there. Maybe expensive, I guess, and the usual criticism of scope creep and project management applies. reply ssalka 10 hours agorootparentThat may be the case, but I think many people will hear \"Boeing\" and immediately classify them along with Lockheed Martin, Northrop Grunman, Raytheon etc. reply gedy 11 hours agoparentprevI personally don't think there is a conspiracy, but consider if there others that this benefits. E.g. a union, etc? reply SV_BubbleTime 11 hours agoparentprevI mean… isn’t the obvious answer a “chilling effect”? If you were to subscribe to the idea Boeing murders whistleblowers, how is the most obvious reason for why not to prevent more people from speaking out? reply jxramos 11 hours agoprevIs there any kind of Bayesian modeling to calculate the probability of deaths in a subpopulation like employees of a company over a given period of time? I guess none of this would be public knowledge to know the background death rate of that population, but assuming it was all public where we knew how many deaths per employee was typical could something be calculated to show this pattern is a mathematical outlier of somekind? reply ChrisArchitect 11 hours agoprev[dupe] Discussion over last 24 hrs or so: https://news.ycombinator.com/item?id=40230790 reply sakshatshinde 6 hours agoprevThey got a hit on everyone these days reply afavour 12 hours agoprevHow many whistleblowers have there been and what’s the demographic? Not to be too much of a downer but if there are a good number of whistleblowers and they’re over-represented by retirement age men that have worked in production plants all their lives then there is some level of morbidity to be expected. But also: yeah, when you look at it face on it’s absolutely wild that there have been two whistleblower deaths. reply bfdm 12 hours agoparentThis one was 45 years old. reply sarimkx 12 hours agorootparentAnd what do they even mean by \"sudden illness\"?? reply dyauspitr 11 hours agorootparentMRSA, the article mentions it. reply micromacrofoot 11 hours agorootparentprevInfluenza and MRSA — allegedly the MRSA was contracted before hospitalization, which is odd. Though MRSA would also be an odd way to try to intentionally kill someone. reply afavour 11 hours agorootparentprevThe point still stands. There ought to be some level of statistical analysis to tell you how likely an event this is rather than immediately reaching for the conspiracy theory lever. reply sarimkx 11 hours agorootparentI was not alluding to any conspiracies. In that \"statistical analysis\" the company and more importantly the time must be factored in. Yes, I am sure out of all whistleblowers, all over the world, for all types of stories (war crimes, finance (insider trading), etc) there must many that do just die. But... 1. The same 'set' of whistleblowers (exposing manufacturing faults in Boing aircrafts) 2. In less than two months..! Not to mention not just at any time but during multiple ongoing investigations...? reply bb88 11 hours agoprevI'm asking a question here that I don't know the answer to. Has anyone weaponized MRSA? Not just weaponized it for mass casualties, but able to also only target a single individual with it? reply jxramos 11 hours agoparentI'm not sure, but I think in suspicious deaths like this folks should DNA sequence all viruses and other stuff found on the body of the poor individual. I've been curious about autopsies from a proactive angle. Can the family of the deceased, or even the person in their own will expressed before they died, can they force a municipality or county or whatever agency the coroner operates under to release the body for a private autopsy? reply bb88 8 hours agorootparentAt some point the body has to be released back to the family -- once the forensic autopsy is done, so yes. reply jxramos 1 hour agorootparentyah that's a good point, they can't kidnap and prevent a burial indefinitely. reply xboxnolifes 11 hours agoparentprevI'd be very surprised if organizations such as the CIA or similar haven't at least put some resources toward researching the viability of weaponizing MRSA or similar. reply hyperific 11 hours agoparentprevI don't think we have the technical ability to make a real life FOXDIE yet. reply seoulmetro 9 hours agoparentprevEverything can be weaponised. Most things have been weaponised. reply keernan 6 hours agoprevI keep seeing headlines similar to the above (\"Second Boeing Whistleblower\"). The headline is simply a lie and misrepresentation. Josh Dean was not a Boeing Whistleblower. He is a Whistleblower against his employer, Spirit AeroSystems, not Boeing. reply smashah 11 hours agoprevIn America they allow oligarchs to just whack people left right and centre? reply EasyMark 9 hours agoparent0 proof of that, but on the internet everyone is a detective. reply smashah 2 hours agorootparentBoeing is killing Americans who are trying to keep passengers safe and the feds brush it off reply 2OEH8eoCRo0 11 hours agoprevInformative article from The Economist about disinformation being on the rise...flagged (https://news.ycombinator.com/item?id=40238314) Conspiratorial nonsense...#1 spot. Is anybody else concerned about the health of this forum? reply lamontcg 5 hours agoparentYeah that thread and these Boeing threads are fucking nuts. reply itsthecourier 12 hours agoprevBoeing kills you in so many ways reply dylan604 12 hours agoparentAt least they aren't falling out of windows reply anilakar 11 hours agorootparentAmericans tend to prefer detached houses. A broken ankle is not going to silence a person. reply ansanabria 11 hours agorootparentprevYet reply Lammy 11 hours agoprevFact: Everybody who drinks water will die. reply pdx_flyer 11 hours agoprev [–] No longer a coincidence. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Whistleblower Josh Dean, known for exposing safety issues with Boeing planes, has tragically passed away from an unexpected illness, following another whistleblower's recent death.",
      "Dean, a former quality auditor at Boeing supplier Spirit AeroSystems, accused senior management of overlooking manufacturing defects on the 737 MAX, with some of his claims corroborated by colleagues.",
      "The deaths of both Dean and John Barnett, Boeing veteran, have sparked concerns due to their close timing and circumstances, prompting ongoing police investigations."
    ],
    "commentSummary": [
      "A second whistleblower from Boeing has passed away, sparking discussions about the company's safety procedures and accountability, notably after the Ethiopian Airlines Flight 302 tragedy.",
      "Speculations on foul play and the potential use of MRSA for assassination are circulating, leading to debates about the likelihood, Boeing's role, and the motives behind the whistleblowers' deaths.",
      "Conversations also involve whistleblowers uncovering manufacturing defects, the possible weaponization of MRSA, private autopsies, and the proliferation of disinformation, with some comments addressing resilience, mortality, and conspiracy theories."
    ],
    "points": 211,
    "commentCount": 71,
    "retryCount": 0,
    "time": 1714686445
  },
  {
    "id": 40235916,
    "title": "Second Boeing Whistleblower Dies Abruptly",
    "originLink": "https://www.newsweek.com/boeing-whistleblower-joshua-dean-dies-sudden-illness-1896401",
    "originBody": "Second Boeing Whistleblower Dies Suddenly Published May 02, 2024 at 3:08 AM EDT Updated May 02, 2024 at 2:22 PM EDT",
    "commentLink": "https://news.ycombinator.com/item?id=40235916",
    "commentBody": "Second Boeing Whistleblower Dies Suddenly (newsweek.com)204 points by koolba 20 hours agohidepastfavorite3 comments belter 20 hours agohttps://news.ycombinator.com/item?id=40233213 https://news.ycombinator.com/item?id=40230790 reply dang 16 hours agoparentComments moved to https://news.ycombinator.com/item?id=40230790. Thanks! reply ChrisArchitect 19 hours agoprev [–] [dupe] Discussion: https://news.ycombinator.com/item?id=40230790 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A second Boeing whistleblower has tragically passed away.",
      "This incident was reported on May 2, 2024, with ongoing updates provided.",
      "-"
    ],
    "commentSummary": [
      "A second whistleblower from Boeing has passed away abruptly, according to Newsweek.",
      "The news sparked a discussion on Hacker News, leading to a related thread on the topic.",
      "-"
    ],
    "points": 204,
    "commentCount": 3,
    "retryCount": 0,
    "time": 1714656025
  },
  {
    "id": 40239029,
    "title": "Piccolo: Rust-Based Lua Interpreter with Unique Features",
    "originLink": "https://kyju.org/blog/piccolo-a-stackless-lua-interpreter/",
    "originBody": "Piccolo - A Stackless Lua Interpreter 2024-05-01 History of piccolo A \"Stackless\" Interpreter Design Benefits of Stackless Cancellation Pre-emptive Concurrency Fuel, Pacing, and Custom Scheduling \"Symmetric\" Coroutines and coroutine.yieldto The \"Big Lie\" Rust Coroutines, Lua Coroutines, and Snarfing Zooming Out piccolo is an interpreter for the Lua language written in pure, mostly safe Rust with an eye towards safe sandboxing and resiliency. It uses gc-arena for its garbage collection system, and in fact gc-arena was originally created as part of piccolo. You can try it out below in an the interactive REPL. I'm not much of a web developer, and this is a little homegrown web terminal thingy, so hopefully this works for you. I'm going to be using REPLs like this to demonstrate a lot of what makes piccolo unique, so if it doesn't work or you or you don't have Javascript turned on, then this might be a pretty boring post and I'm sorry! ↓ Type Some Lua Here ↓ [1]You know, if you want to... I find REPLs to be really magical and inviting,[2] and I end up eventually wanting to attach them to everything I ever make.[3]It's possible to like things too much. Not just a REPL but the idea that your program is a sort of Living Thing that understands a Language, and if the normal UI isn't fit for purpose and you're enough of a Witch, you can always just Speak to the program in the way it naturally understands... cast a Spell to achieve your Goals, or maybe just have a Conversation. I think it helps close the gap between the author of a program and the user. I'm not better than the user, who am I to tell them what they can and can't say to the program? I hope you feel the same way about REPLs as I do because there are a lot of them on this page, and I'm going to ask you to type things into them. If you're not into it, well... I'll try and always give you working code that you can copy and paste, but where's the fun in that? I said in my last post in this series that my goal wasn't to try to sell anyone on gc-arena or piccolo[4]yet anyway and I think that's still true here. piccolo is pretty rough around the edges[5]which you probably noticed if you tried to use large parts of the stdlib in the REPL above right now, but it's more complete than you might think[6]The implementation strategy so far has been to do the hardest parts first to prove that the basic idea works, so most of really hard parts of the VM are feature complete. and it has some interesting and unique features. Still, I'm not telling you to go out and replace LuaJIT or Luau or The Ur Lua with piccolo just yet. In this post, I just want to talk about what makes piccolo special, and hopefully you'll find it interesting. In a future post, I'm going to tie all of this together, the way gc-arena and piccolo are designed, how they work now, and how I wish they could work in the future, but this post is going to focus just on piccolo as it works right now. History of piccolo When I was first writing piccolo (in 2019), I had noticed that nobody had quite figured out how to make VMs for certain kinds of languages that could be competitive with C while being implemented primarily in safe Rust. Mostly I'm referring to problems surrounding garbage collection, rather than something like designing fast interpreter loops (which is something I'm not very good at yet!). Languages that have ownership semantics similar to Rust are of course much easier to write VMs for in Rust, because the implemented language can snarf[7]my absolute favorite PLT jargon much of the functionality from the host language. It's pretty easy to express the exact semantics of Rc by just... using Rc itself to implement the language. There are several such scripting languages that try to have matching ownership and mutability semantics with Rust and I think that's honestly a great idea because sharing these core semantics with the host language just removes a huge amount of cognitive burden when crossing the language boundary, and you can make an embedded language this way that feels like it fits in perfectly with the host. However, I also think it's a bit of a disappointment if only Rust-like languages can be easily made using Rust. Certainly this is not actually true, and there are plenty of other Rust runtimes for languages with \"unrestricted ownership\" (proper garbage collection, unrestricted references... the terms for this are a bit all over the place, but what I mean is languages like Lua). At the time at least, when I surveyed language runtimes written in Rust they broadly fell into one of several categories, none of which was what I wanted for piccolo... Languages with ownership semantics similar to Rust, so no \"true garbage collection\" / \"unrestricted ownership\" or whatever you want to call it (dyon, rune, etc...) Language runtimes with true garbage collection (tracing or generational collector) but whose implementations were wildly, infectiously unsafe as they would be in C, due to the nature of garbage collected pointers and their interactions with the Rust stack. Language runtimes for languages that are meant to have proper garbage collection but the implementer used Rc or similar and left the problem of what to do about reference cycles for later (RustPython). I wanted to have a language for Rust that felt as natural as Lua does for C, and one that had true garbage collection[8]and I have extremely bad NIH syndrome ... and frankly I really like just plain vanilla Lua. I think it matches perfectly with Rust because they're so different, I think having an Rust embedded language that frees you from even having to think about ownership is very powerful because it can be used for things where having to think about ownership can be more trouble than its worth. Let each language play to their strengths, and Rust and Lua in a lot of ways have complementary strengths. Since I just looooove Lua so much and I had so much experience with vanilla PUC-Rio Lua (aka The Ur Lua), I decided to try and write an interpreter designed similarly to[9]shamelessly stolen from PUC-Rio's Lua, with a similar sort of garbage collector, but because I was primarily interested in sandboxing untrusted scripts, somehow made of mostly safe Rust.[10]or at least not like this gc-arena was born out of my efforts to solve this problem. But I really don't intend to throw any shade at any of the projects I listed above or any other Rust-implemented language runtime written in a different style. These are hard problems and gc-arena is not a perfect solution. In fact, in the early days of gc-arena and piccolo, I ran into so many seemingly unsolvable problems that I became hopelessly frustrated and gave up on piccolo entirely for about four years. It was only through Ruffle's use of gc-arena and Ruffle contributors helping to get through the roadblocks we encountered that I was eventually able to pick piccolo back up. Today, there are not nearly so many unsolved problems in trying to use gc-arena to implement a language like Lua or ActionScript, but it really came down to Ruffle contributors helping to solve each issue one by one over the intervening years. BUT, even with all of the major roadblocks overcome (pointers to Unsize types, GC finalization, nonstatic Any, a bunch more I've forgotten) influence from the biggest limitation of gc-arena stubbornly remained: the \"Mutation XOR Collection\" design of gc-arena that was talked about in the last post. gc-arena's design requires that code that wants to access garbage collected data do so through special mutation methods, and that collection must ONLY happen when no mutation methods are executing. This \"Mutation XOR Collection\" design means that calls to Arena::mutate must return before garbage collection can safely take place, to prove that no garbage collected pointers exist anywhere on the Rust stack. Unless I were willing to just give up on ever hoping to match Lua runtimes written in C, or were willing to limit the places piccolo could be used,[11]If I were making a runtime that was more limited in purpose, I could instead limit garbage collection to, say, once a frame if I were making a video game or something like Ruffle, or just simply require that continuous execution of Lua not go on for \"too long\", but this would make piccolo much less general. I had to figure out a way to make the entire execution context of piccolo suspendable, just to be able to leave calls to Arena::mutate, so that garbage collection could take place at arbitrary points. At the beginning, this limitation infuriated me, and I spent ages trying anything I could to find an acceptable way around it. It still certainly is limiting, but now that piccolo has gotten further along I think what I've ended up with is actually very cool, and what started out as purely a painful compromise might actually end up being piccolo's \"killer feature\"... A \"Stackless\" Interpreter Design Some of the biggest, most interesting features of piccolo come from its \"stackless\" design, originally born only from necessity due to the limitations of gc-arena. This design is similar to other \"stackless\" interpreters, and the one most people have heard of is Stackless Python, so if you're familiar with it, most of what you know will be applicable to piccolo as well. \"Stackless\" here is jargon that's used in a couple of places, not just in interpreter design. You may have heard that Rust has \"stackless\" coroutines, and the word \"stackless\" as I'm using it here means the exact same thing. It means that piccolo's Lua runtime is not \"stackful\", it does not rely on the Rust function call stack to maintain its execution state, and execution can be suspended at any time. This applies not just for plain interpreted Lua bytecode but also for Rust code executed from Lua (callbacks) in any form, for any depth of Lua -> Rust -> Lua calls. The overriding design decision made early in piccolo's life was that execution of Lua (and ALL callback code called from Lua) must always be able to be suspended, and that execution would be driven from the outside by polling: // This is pseudocode to demonstrate the \"stackless\" or \"trampoline\" VM style, // how this really works inside piccolo is slightly more complex. // Set up the execution of some Lua code and produce an object representing the // execution state. The internal Lua / Callback call stack is reified into this // object, and does not rely on the normal Rust function call stack. let execution_state = ...; // We need to call a method to make progress, and call that method in a loop // until the running Lua code is complete. loop { match execution_state.poll() { None => { // We are not yet done and must continue to call // `execution_state.poll()`. We could suspend the computation at any // time however by just exiting the loop. } Some(result) => { // Our execution is done and we receive the result, and should not // call `execution_state.poll()` anymore. break; } } } This should be extremely familiar to anyone who has ever touched Rust futures, and the similarity is no accident. The core of the design of piccolo is virtually the same as Async Rust: that all long running operations are reified into objects that must be polled to completion. The obvious benefit for piccolo is that it becomes trivial now to exit a call to gc_arena::Arena::mutate and allow for collection, since we can now do so in-between calls to execution_state.poll().[12]In reality this is piccolo::Executor::step, but I wanted to show the similarity to normal Rust futures. What I didn't fully appreciate when I began writing piccolo is that this style of writing an interpreter, though at times more difficult, comes with many other benefits that make it (in my opinion) a worthwhile goal, or at least a very interesting place in the design space and I hope a unique niche that piccolo can fill. Benefits of Stackless Cancellation An obvious side benefit of polling execution in the style of Future is that, just like a Future, execution can be canceled at any time by just... not continuing to call poll. Let's go back to the REPL from above, but this time, let's see what happens when we run some Lua code that never returns. If you don't know much Lua, try typing something like: while true do end or maybe repeat print(\"Look Around You\") until false ↓ Just Endlessly Do It ∞ ↓ You should see a big interrupt button appear, and when you press it, the command should stop. How this works under the hood in this demo is that inside this webpage there is some javascript that looks something like this: this.interval = setInterval((() => { if (this.executor.step(8192)) { this.finish(); } }).bind(this), 0); This is the \"poll loop\" that we talked about above that polls running Lua code to completion. This is still not exactly how it would look when using piccolo directly but it's a little closer... The executor there is a piccolo::Executor object,[13]Well, it's a simplified wrapper for Javascript and Executor::step is called in a loop until the code has completed. Here, Lua execution actually hooks into the normal Javascript event loop, every time the closure is run, the piccolo::Executor is \"stepped\" for 8192 \"steps\". The \"steps\" value here is referred to inside piccolo as \"fuel\" and (more or less) corresponds to a number of Lua VM instructions to run before returning. Since the timeout given to setInterval is 0, we run this function regularly and rapidly but without blocking the main Javascript event loop. When the interrupt button is pressed, the interval is canceled and the executor is dropped, interrupting execution. In fact, every REPL on the page works in the same way and shares the main Javascript event loop, so all of them can execute Lua code concurrently. Interruptable Lua code is not something new to piccolo, PUC-Rio Lua (and most of its forks) have something like this in the form of lua_sethook. This function allows you to, among a few other things, set \"hook\" function that runs every count VM instructions, and one of the things this function can do when run is interrupt running code by calling e.g. lua_error.[14]If you also know that you can call lua_yield from a hook function and mimic what piccolo tasklets do, I know that too, wait just a bit and I'll talk about it. So we can imagine a situation in which we can set up something similar to what piccolo is doing here, either by running Lua code in a different thread and waiting for an interrupt flag to be set in the hook function, or by pumping an event loop from within the hook function or something similar.[15]If you're internally screaming about calling lua_yield from a hook, wait. However, I would argue that the way piccolo is structured makes this effortless and natural due to its stackless design. Since PUC-Rio Lua is written in normal, stackful style, the best thing it can offer is a hook function that will be periodically called by the VM loop, whereas with piccolo the user never loses control to the VM in the first place. piccolo is designed such that a call to Executor::step should always return in a reasonable, bounded amount of time proportional to the \"fuel\" it is given,[16]Ensuring this is true in all cases so that it can be relied on as a security boundary is complex and a WIP, but is 100% a goal of piccolo. so it is not necessary to provide an equivalent to lua_hook at all. Pre-emptive Concurrency One of Lua's best and most defining features is its support for coroutines. Coroutines in Lua can be used to provide seamless cooperative multitasking, and are especially powerful for things like game development where some kind of script must execute concurrently with the running simulation of a video game. However, Lua coroutines only provide cooperative multitasking, the script must decide where and when to yield control to the caller, and a buggy (or malicious) script that does not yield control may need to be interrupted and canceled (via lua_sethook) or might make the game hang. Rust (at least, unstable Rust) also has coroutines, and they are used behind the scenes to implement async. In Rust, like in Lua, these coroutines provide cooperative multitasking, Rust code must decide when to call await, or an implementation of Future::poll must decide when to return. A buggy implementation of Future will hang an async executor thread just like a buggy Lua coroutine might hang a game loop. In piccolo, running Lua code acts very similarly to a Rust \"task\" (a term for something that implements Future and is run on an async \"executor\"), and like Rust tasks, they can easily be run concurrently. However, piccolo works very hard to guarantee that piccolo::Executor::step returns in a bounded amount of time, even without the cooperation of the running Lua code. So, by using several independent piccolo::Executor \"tasklets\" and multiplexing calls to each piccolo::Executor::step, we can give Lua pre-emptive multitasking. It's easier to understand with a demonstration. The two REPLs below are connected to one Lua instance. Instead of a single print function, they have the functions print_left and print_right to print in the left or right REPL console. They share global variables, so we can use this to demonstrate that the two interfaces are running Lua code on the same VM. In the left REPL, type something like this i = 0 while true do i = i + 1 end While that is running in the left REPL, in the right REPL type this: while true do print_right(i) end ↓ These two REPLs are connected to the same Lua instance! ↓ You should notice that it appears that two separate Lua REPLs access the same state, seemingly in parallel! In fact, if you copied the exact code above, the right REPL probably prints values of i seemingly at random, every thousand and a half or so increments. In this demo, this behavior comes from the way that running Lua code is run inside setInterval callbacks... The REPLs here work exactly the same as any of the REPLs above except that they both share a Lua instance, and this really is the only difference. There are two setInterval callbacks calling Executor::step being run by the browser at the same time and each callback is run in a round-robin fashion.[17]I actually don't know how the task scheduler in browsers works exactly, but I think it will execute both REPLs in a simple round-robin way? In a plain Rust environment you could get the same behavior by looping and calling Executor::step for one executor then another in turn, in a simple round-robin scheduler. This is very similar in a way to OS threads which also are pre-emptively scheduled, but instead of using an OS scheduler, we write our own scheduler and execute some running Lua for a time slice via calls to Executor::step.[18]This idea is not new, and other stackless interpreters have called this scheduling idea tasklets. In fact, though you can't observe actual data races here or even an analogue of it within Lua, you can observe a mirror of other race condition problems that OS concurrency primitives are meant to solve, and a custom scheduler for these Lua \"tasklets\" might even want to provide a version of common OS primitives to prevent them and aid scheduling.[19]Stackless Python has a custom version of channels to communicate between tasklets that serves this purpose, but I don't actually know whether these channels affect tasklet scheduling (but they could!). This is also again, VERY similar to how rust Futures (well, tasks) work when running on an async executor. Async tasks can be round robin scheduled or in some more advanced way, but each task has a \"slice\" of execution given to it by calling its Future::poll method. The difference in piccolo is that Lua scripts are not actually aware that they are being scheduled this way, and from the perspective of Lua, this scheduling happens pre-emptively. Rust callbacks in piccolo are more privileged than this and actually receive a piccolo::Fuel object that they can use to consume fuel proportional to their work, and must be trusted to cooperatively schedule themselves (you can always incorrectly write loop {} in a callback, after all), but Lua code cannot break out, at least not on its own. Yet another way to look at this is that piccolo executes Lua code sort of as if there are something like invisible coroutine.yield statements inserted everywhere, but ones that operate on a different level of abstraction from the real coroutine.yield, ones which regular Lua code cannot interact with. Let's imagine we transformed the above code that I asked you to type in the paired REPLs into something like this in plain Lua: -- This example is in a big function loaded in the REPL below so you can easily -- call it, and I'll do the same thing with later examples. I'm not gonna ask -- you to type *that* much. function coroutine_example() local i = 0 local co1 = coroutine.create(function() while true do i = i + 1 coroutine.yield() end end) local co2 = coroutine.create(function() while true do print(i) coroutine.yield() end end) while true do coroutine.resume(co1) coroutine.resume(co2) end end ↓ The above code is loaded here you want to run it ↓ This behaves sort of like the code above but in a much more predictable way. If you know Lua and are comfortable with coroutines, you can probably tell that the above code is pretty much just a convoluted version of a simple for loop, but it's enough to demonstrate the idea. We have two coroutines that execute their own loops independent of each other and we schedule between them, but this time we require that the body of the coroutines decide where to yield to the scheduler to allow the other task to run. Stackless execution in piccolo is almost the same as if we could painlessly automatically insert these calls to coroutine.yield everywhere in the body of our running Lua tasks and use this to pre-emptively rather than cooperatively schedule them. Fuel, Pacing, and Custom Scheduling In the last section where I transformed the code executed in the concurrent REPLs into basic Lua coroutines, you may have noticed a big difference between the two. In the first example, the scheduling between the two Lua REPLs was somewhat random and hard to discern, the left REPL would increment the i value more than a thousand times for every time the right REPL printed the value of i, but in the second example the first task would increment the i value once for every time the second task printed i. The reason for this has to do with how the javascript for this page is actually written, but it's a perfect, simple example of something using piccolo enables: custom tasklet scheduling. I've mentioned \"fuel\" before in the context of piccolo::Executor::step. Here is the real signature of Executor::step inside piccolo: impl Executor { /// Runs the VM for a period of time controlled by the `fuel` parameter. /// /// The VM and callbacks will consume fuel as they run, and `Executor::step` /// will return as soon as `Fuel::can_continue()` returns false *and some /// minimal positive progress has been made*. /// /// Returns `false` if the method has exhausted its fuel, but there is /// more work to do, and returns `true` if no more progress can be made. pub fn step(self, ctx: Context, fuel: &mut Fuel) -> bool { ... } } The method requires a fuel: &mut Fuel parameter to, well, \"fuel\" the VM, and the running VM consumes this fuel as it runs. Fuel is a very simple wrapper around an i32 value (you can see the current implementation here), that is decremented by Executor as it runs Lua code, and also optionally by any Rust callback that it calls. piccolo's ultimate goal is to enable treating all loaded Lua code as potentially malicious, but Rust callbacks are never on the other side of this security boundary. Callbacks are meant to cooperate with the execution system of piccolo and act as part of the security boundary to potentially malicious Lua, and as such, they can consume Fuel or even \"interrupt\" the \"flow\" of fuel to the Executor that calls them. This system makes a lot of sense to provide, and not only strictly for security. piccolo's goal is to enable Lua tasks to run concurrently not only with Rust but with each other, and as such there are many ways we we might want to give certain tasks more or less time to run. We could imagine a game engine where we want to provide a sandbox for running Lua code such that no matter what, if the script is badly written or buggy, that the game simulation can continue without being compromised. Tasks could be scheduled such that they are assigned a certain amount of fuel \"per second\" up to a predefined \"tank limit\", giving them a kind of \"burst\" fuel. In this way, a task that periodically needs a lot of computational power can get it, but a broken task that has infinite looped will always use a much smaller amount of sustainable fuel per frame.[20]You could get funky with this too, make game entities that have scripts attached that always use the maximum fuel get hot in game and make that have a gameplay effect. This might only be fun in something like ComputerCraft... or maybe it's not fun at all and you shouldn't listen to me about gamedev... probably the second one. Besides just \"consuming\" fuel, another thing a Rust callback can do is interrupt fuel. This is quite similar in behavior to just consuming all of the remaining fuel so the difference isn't that important, but it exists to mesh well with the use case described before, where we want to give tasks a sizeable \"reserve tank\" of fuel. \"Interrupting\" fuel flow makes the outer Executor::step immediately return to the Rust code calling it, no matter the amount of fuel currently consumed. This is mostly useful for technical purposes, for example if one Lua task is waiting on some event and cannot possibly currently make any progress, or if some callback must return to the outer Rust caller immediately to take effect. This is what is happening on REPLs on this page when print is called! I noticed when testing the REPL I was writing that calling print in a hot loop slowed down the whole page quite a lot, and mostly just to create and destroy a bunch of output divs faster than they could be read as the output went far past the console scrollback limit. So, to fix this, I made print callbacks in this page always call Fuel::interrupt to make the page more responsive during hot loops. When you call print in a REPL on this page, it immediately yields control to the browser task queue! This is the sort of thing that having deep control over VM scheduling allows you to do: customize it to make it work in many different situations.[21]Yes I know you can also use lua_yield in a callback for the same effect, but crucially, that means you cannot mix these callbacks with normal Lua coroutines. I'm going to talk more about this before the end, I promise. \"Symmetric\" Coroutines and coroutine.yieldto It's tough to talk about coroutines because there tend to not be universally agreed upon definitions, but I'm going to try. You might want to have the wikipedia article on coroutines and possibly this paper open if you want the full extra credit for this section. Lua has what is usually referred to as \"asymmetric coroutines\", which are (as far as I can tell) the most commonly seen type of coroutine. This is also the same sort of coroutine that Rust supports with the (unstable) std::ops::Coroutine trait. As such, this can feel like a fancy term for a simple idea, but it refers to the limitation that coroutines yield only to their caller. It is also possible to instead support fully \"symmetric\" coroutines that can yield to any other coroutine, not just the calling one! This is probably easier to understand expressed as a Lua function that almost provides what we want. Symmetric coroutines work as if we had the following function in Lua: -- We want to yeild control to another coroutine directly, so we need to yield -- control *and* resume another coroutine somehow at the same time. function yieldto(co) -- Unfortunately this doesn't quite work as is, because there is no such -- thing as a \"tail yield\" in standard Lua. This will resume the given -- coroutine, but it will keep around a stack frame for *this* coroutine -- until the entire sequence of coroutines eventually returns, which may -- be *never*. As it is, this \"works\" but it is a stack leak. coroutine.yield(coroutine.resume(co)) end We want a function that can yield to another coroutine by resuming that coroutine and yielding to the caller... whatever that other coroutine would have yielded. The problem is that this function as written is a stack leak: there is no way for normal Lua to \"tail yield\" like it can \"tail return\", the yieldto function as written will consume stack space for the current call to coroutine.resume, only giving the stack space up when the stack of coroutines eventually finishes. True symmetric coroutines do not have this limitation, and can mutually yield to each other without bound. Because of the way piccolo's Executor works, Lua control flow that might normally be expressed as a Rust function call (such as a callback resuming another coroutine) is reified into the structure of the Executor, and the actual Rust control flow always \"jumps back up\" to Executor::step. This is actually the origin of the term \"trampoline style\" when referring to stackless interpreters, that control flow always \"jumps back\" to the same place. In PUC-Rio Lua, coroutine.resume is a normal C function call, so it is impossible to directly support this \"tail yield\" operation and avoid the stack leak, but piccolo's design just so happens to allow easily providing this as a builtin function: coroutine.yieldto, enabling full symmetric coroutines! Let's see how it works... -- I'm deeply sorry for this example... function browse_internet() local be_bored local be_optimistic local be_dooming be_bored = coroutine.create(function() while true do print(\"I'm bored, I think I'll mindlessly browse The Internet\") coroutine.yieldto(be_optimistic) end end) be_optimistic = coroutine.create(function() while true do print(\"Maybe The Internet won't be so bad this time\") coroutine.yieldto(be_dooming) end end) be_dooming = coroutine.create(function() while true do print(\"I think I need a break from The Internet\") coroutine.yieldto(be_bored) end end) coroutine.resume(be_bored) end ↓ You can run the 100% accurate Internet Simulator below ↓ Now, unless you're already pretty familiar with coroutines (or for some unearthly reason you decided to stop reading this and instead go carefully read the paper I linked earlier), you might not know that \"symmetric\" and \"asymmetric\" coroutines are actually of equivalent expressive power. Let's pretend that we don't have coroutine.yieldto and transform the previous example a bit to make up for it. -- I'm still sorry... function browse_internet() -- Because we don't have proper `coroutine.yieldto`, we need some way of -- returning to the outer level and resuming the next coroutine. We can't -- provide this as a function because there's no way around the stack -- leak, but we can provide it as an outer \"runner\". function run_coroutines(co, ...) -- Because we can't provide a normal function, we instead require that -- every coroutine always yield the coroutine that should be run *next*, -- and in this way we avoid the stack leak. I've kept this purposefully -- simple and am not handling resume arguments or errors. local _, next = coroutine.resume(co, ...) if not next then return end return run_coroutines(next) end -- Afterwards, we change every call to `coroutine.yieldto` to -- `coroutine.yield`, and wrap the coroutines in our \"runner\". local be_bored local be_optimistic local be_dooming be_bored = coroutine.create(function() while true do print(\"I'm bored, I think I'll mindlessly browse The Internet\") coroutine.yield(be_optimistic) end end) be_optimistic = coroutine.create(function() while true do print(\"Maybe The Internet won't be so bad this time\") coroutine.yield(be_dooming) end end) be_dooming = coroutine.create(function() while true do print(\"I think I need a break from The Internet\") coroutine.yield(be_bored) end end) run_coroutines(be_bored) end ↓ After the above transform, our simulator is still 100% Accurate ↓ So, the coroutine.yieldto function that piccolo provides doesn't actually make Lua fundamentally any more powerful, instead it is more of a convenience. So why bring this up? Well, besides it being a very neat function to be able to provide, and piccolo being able to provide it in a way that doesn't require any outside \"runner\", I wanted to bring attention to the idea of transforming code like this. It's no coincidence that piccolo has an easy time providing coroutine.yieldto. The above transform takes normal control flow and turns it into control flow via return values. This is very nearly the exact same transform that has already been done by piccolo's stackless design that I've been talking about this whole time. In fact, let's look at the actual implementation of coroutine.yieldto in the code for the coroutine lib inside piccolo: coroutine.set(ctx, \"yieldto\", Callback::from_fn(&ctx, |ctx, _, mut stack| { let thread: Thread = stack.from_front(ctx)?; Ok(CallbackReturn::Yield { to_thread: Some(thread), then: None, }) })).unwrap(); Ignoring some of the unimportant details, we see that the 'yieldto' field is set to a callback function, and that callback function takes a single argument of a Thread. Then, it returns an enum value CallbackReturn::Yield and states which thread to yield to (the normal coroutine.yield function simply sets to_thread to None instead). This is exactly the same as the transform that we've already done above, which shows why this is so simple for piccolo to provide: piccolo::Executor already works like this. The \"Big Lie\" So far I have talked a lot about piccolo's unique design, and how it allows piccolo to have powers that other Lua interpreters can't have. I have been lying to you! The actual truth is rather complicated, and you need the context of everything I've said so far to fully understand it. The real truth is... PUC-Rio Lua can already sort of do about 70% of the same things piccolo can do. In fact, piccolo is not uniquely designed at all, it is the natural conclusion to the way PUC-Rio Lua already works. Let's start by doing something that I think almost nobody who uses PUC-Rio Lua or Luau or LuaJIT knows that they can do.[22]LuaJIT is slightly more complicated because you probably have to disable the JIT to make it work in all cases. We're going to implement tasklets using the plain Lua C API! I don't have the energy to get normal PUC-Rio Lua 5.4 working in a browser with Emscripten, so you won't be able to run these examples interactively, you'll just have to trust me (or set up a C build environment with Lua 5.4 and try them yourself). You'll also have to understand C and the PUC-Rio Lua C API to fully understand these examples, but hopefully I can comment them enough to show what's going on even if you don't. #include#include#include \"lauxlib.h\" #include \"lua.h\" #include \"lualib.h\" // We will set up a \"hook\" function for the Lua VM to periodically call. // // In this case, the \"hook\" function always *yields*, which will only work if // the calling Lua thread is itself a coroutine (and not the main thread). // // We are sort of using the \"hook\" function to *externally insert* calls to // `coroutine.yield` periodically in otherwise unmodified Lua. void yield_hook(lua_State* L, lua_Debug* _ar) { lua_yield(L, 0); } int main(int _argc, char** _argv) { // Open the main Lua state and all of the Lua stdlib. lua_State* L = luaL_newstate(); luaL_openlibs(L); // Create a thread separate from the main one to use as our coroutine // thread. lua_State* co = lua_newthread(L); // Load *unmodified* Lua code, no manual calls to `coroutine.yield` are // necessary here. assert( luaL_loadstring( co, \"while true do\" \" print('hello')\" \"end\" ) == LUA_OK ); // Set our hook function on the coroutine thread, *forcing* the coroutine to // yield whenever the hook is called. // // In this case, the hook will be called every 256 VM instructions. lua_sethook(co, yield_hook, LUA_MASKCOUNT, 256); // Our main loop. // // Every time through the loop, we resume our coroutine. The hook function // *externally* causes the called Lua code to periodically yield without // having to modify our Lua source code to manually add `coroutine.yield` // statements. // // When running this C code with my current version of Lua 5.4, I see 64 // \"hello\" lines for every 1 \"there\" line, showing that execution correctly // periodically returns to C. while (true) { int nresults; assert(lua_resume(co, NULL, 0, &nresults) == LUA_YIELD); lua_pop(co, nresults); printf(\"there\"); } } The example above shows a fully working, if simplistic, Lua tasklet system. In the same way that piccolo's Executor::step function works \"as though\" there are invisible periodic calls to coroutine.yield everywhere, calling lua_yield from a lua_Hook function also (and much more literally) inserts invisible periodic calls to coroutine.yield. This is more or less everything required for a tasklet! PUC-Rio Lua can do about 70% of what piccolo can do, right out of the box! The problem is the last 30%. Let's modify the example above very slightly... #include#include#include \"lauxlib.h\" #include \"lua.h\" #include \"lualib.h\" // Same as last time, we effectively insert invisible periodic calls to // `coroutine.yield`. void yield_hook(lua_State* L, lua_Debug* _ar) { lua_yield(L, 0); } int main(int _argc, char** _argv) { // Open the main Lua state and all of the Lua stdlib. lua_State* L = luaL_newstate(); luaL_openlibs(L); // Create a thread separate from the main one to use as our coroutine // thread. lua_State* co = lua_newthread(L); // Still *unmodified* Lua code with no manual calls to `coroutine.yield`. // // We make one small change though, before calling `print('hello')`, we call // `table.sort` to sort a Lua table. The callback isn't important here, but // what's important is that `table.sort` is a C function which calls a Lua // function (the comparator). // // We put a big for loop in the comparator function just to make sure the VM // spends some actual time here, but no matter what, the same behavior will // eventually occur if you use Lua -> C -> Lua callbacks at all. assert( luaL_loadstring( co, \"while true do\" \" table.sort({3, 2, 1}, function(a, b)\" \" for _ = 1,1000000 do end\" \" return alua_resume -> luaV_execute (the main VM loop) -> sort (in ltablib.c) -> lua_call -> luaV_execute (the main VM loop again, for the comparator) -> yield_hook -> lua_yield PUC-Rio Lua uses the normal C stack for much of its internal state, and calls to lua_yield are expressed as a C longjmp, jumping back up to an upper C frame and popping any call frames in-between from the call stack. So, certain operations are simply disallowed when the inner call to longjmp would destroy essential information about the runtime state. There IS a way around this problem, however. Ultimately, the problem is that the call to table.sort, a C function, in turn calls a Lua function with the C API function lua_call. Any Lua function called this way is disallowed from calling coroutine.yield (or its C equivalent lua_yield). PUC-Rio's C API provides a special version of lua_call to get around this: lua_callk. You can read in more detail about the entire situation in the section of the PUC-Rio Lua 5.4 manual called Handling Yields in C. This does work, and in this way, PUC-Rio Lua provides the ability to yield from situations like this Lua -> C -> Lua sandwich. However, table.sort is not written this way, and in fact none of the stdlib is written this way at all! The reason for this is, frankly, that transforming C code to work this way is enormously difficult. The C code in question must be able to handle a longjmp, when the inner Lua code triggering a longjmp will destroy (not even unwind!) the current C stack up to where it was called, and the only way for the C code to resume is through the lua_KFunction and lua_KContext passed to lua_callk. There are no Drop impls to rely on, no automatic memory management, no coroutines, the C code must be transformed so that it relies entirely on a type pointed to by a lua_KContext for its state, so that it can be suspended at any time.[23]This should sound familiar. This is not the only problem, either. By repurposing normal Lua coroutine yields like this to yield back to C, you take away Lua coroutines from the usable part of the Lua language. If we were to try to use normal coroutines in our tasklet system, the inner lua_yield from the hook function would just yield to the nearest thing that has called lua_resume, which in this case would be the Lua thread which called coroutine.resume and not the top-level C code. I love coroutines,[24]As much or more than I love REPLs! and Lua without coroutines is frankly no longer really Lua, but with enough effort, I think you could get around this problem too! Remember the transform we did before, where we made symmetric coroutines out of asymmetric ones? You can do something similar with the normal Lua C API but wrapping coroutine.yield in a special version that instead returned whether or not it is a real yield or a synthetic one from the lua_Hook function. You would have to go further than this to make it work, restructuring all of the other coroutine methods so that which thread was waiting on the results of which other thread was kept in an array rather than the C stack, so that the coroutine \"tasklet\" system continues to work while providing a similar, inner system for \"normal\" Lua coroutines. You could also do the work of re-implementing every single function in the stdlib that calls back into Lua in such a way that it used lua_callk with a continuation function instead of lua_call, too, so that every C function in the stdlib became suspendable. For good measure, you could also periodically yield long running callbacks even if they didn't call back into Lua, just to make sure that execution always jumped back out to the outermost C code in a bounded amount of time. So lets summarize this list of theoretical changes we can make to PUC-Rio Lua to make a complete tasklet system.[25]The \"30%\" is obviously very generous, in reality, the last 30% makes this \"vanilla\" tasklet system useless in a huge number of cases. It's not that 30% of use cases are non-viable, it's that 30% of the surface area of Lua no longer works, so most uses are non-viable. Use lua_Hook functions to insert synthetic calls to lua_yield within all Lua code. Make all of the stdlib that calls Lua functions suspendable by using lua_callk and continuations. Reimplement the Lua coroutine library making it one level of abstraction up from normal calls to lua_yield, so that normal Lua coroutines can still work. We would need to implement coroutine.resume in a different way that does not use the C stack. We can do a transform similar to implementing \"symmetric\" coroutines over \"asymmetric\" ones here, where we implement \"Lua\" coroutines over our lower level \"synthetic\" yielding. Lua calls to coroutine.yield and coroutine.resume would now both be a yield to the calling C code, and the yielded values would tell the outer C code what to do next (whether to resume another coroutine or yield to whatever the \"upper\" coroutine was). As a side effect, coroutine.yieldto becomes easy to implement. For good measure, keep track of some unit of time cost in all callbacks, and insert calls to lua_yieldk in all long running callbacks so we know that control will always return to the outer calling C code in a reasonable amount of time. We have now reached, very roughly, the current design of piccolo. Rust Coroutines, Lua Coroutines, and Snarfing In the previous section I laid out a rough explanation of how to transform PUC-Rio Lua as it exists today and build a system similar to what piccolo forces by design. However, I am not aware of anyone ever doing anything like this on a grand scale.[26]I know people do tasklet systems in PUC-Rio Lua and Luau, but I think they limit the tasklet code to very simple Lua that doesn't require completely rewriting the Lua stdlib. The reason for this, I think, is simple, and that is that it is just monumentally hard to write C callbacks this way! The same problem exists in piccolo though, which I alluded to near the beginning of this post. In piccolo, long running callbacks are represented by a trait called Sequence which allows them to be suspended. More precisely, it is not so much that they are suspended as it is that their API must mirror the outer Executor API in piccolo: they must be polled to completion. Now, the situation is not nearly as bad here as trying to use lua_callk / lua_pcallk / lua_yieldk in plain C, but fundamentally it can still be more than a little painful. The Sequence trait shares a lot in common with the Future trait, in that both represent an operation that must be polled to completion. Like I said before when I was introducing the \"stackless\" design, this similarity is no accident. I used the slang word \"snarf\" casually near the beginning of this post without really explaining it. As I understand it, snarfing is something from PLT jargon where if you implement an inner programming language B in an outer programming language A, features from language A can be very easily and automatically incorporated into language B. The most common example I see here is actually garbage collection, if you implement a runtime for a garbage collected language within another garbage collected language, and you're okay with the GC semantics from the outer language being reflected in the inner language, then you can snarf garbage collection from the outer language. Think of implementing Lua in something like Go, even though the specifics of the GC semantics in Lua may not be expressible in Go,[27]I don't actually know whether Go can express all of the minutia of Lua weak / ephemeron tables and finalization. it would probably be worth it to just snarf garbage collection from Go and use plain Go pointers as Lua references. Snarfing can also be simpler things like implementing the stdlib of the inner language using the stdlib of the outer language, in PUC-Rio Lua, there is actually a good deal of functionality snarfed from C, most of it bad (like os.setlocale). With all of this context finally out of the way, I can say what I've wanted to say almost from the beginning of this very long blog post: The original design I wanted with piccolo and gc-arena was for Lua to snarf coroutines from Rust. I'm going to talk about this in more detail in a future post because this post is so long already, but Sequence's similarity to Future is because I want to use Rust coroutines to implement piccolo. Think about it... why is PUC-Rio Lua's C interpreter written the way it is? Why do lua_callk and lua_pcallk and lua_yieldk exist at all... they exist because C does not have coroutines. This entire post I have been dancing around this issue without addressing it because I feared it wouldn't make sense without a mountain of context, but the entire time I've been talking about \"reifing state\" that would \"normally be held inside the call stack\" into objects that can be \"polled to completion\"... that is the very core of what a coroutine is. The only real downside to gc-arena and piccolo is having to do this transform manually rather than letting the Rust compiler do it. The pain of using gc-arena and piccolo is THE SAME pain that existed before Rust Async was stabilized, with Future combinator libraries having to fill the gap. In fact, an older version of gc-arena tried to provide combinators like this to try and fill the gap, but making it fit into piccolo in a generic way was just too painful and the combinator library was dropped. piccolo::Sequence actually comes from the remains of this combinator library. And all of this exists solely because I can't figure out how to make a Rust coroutine implement gc_arena::Collect.[28]If I sound agitated, it's because I spent a large amount of my life force trying to make this work somehow. It needs Rust compiler changes. If I could figure this out, all of the problems with gc-arena and piccolo could melt away, and the Rust compiler could do the painful transformation into \"stackless\" interpreter design largely for us. Even the term \"stackless\" is shared with Rust coroutines. Zooming Out I'm gonna spend a bit of time here zooming out some more. Hopefully I won't zoom out so far that I stop even being anchored to reality. I think Rust's really cool core idea is the same that is shared by all systems programming languages: that they are meant to be the last stop in a line of other choices. I honestly don't think every single bit of software needs to be written in Rust or any systems programming language. To me, systems programming languages are languages where if you need to make system A work with system B, no matter what those systems are, you can use them. Rust and C are languages that you're supposed to use when what you're making needs to fit almost anywhere. They're supposed to be the languages with the fewest possible assumptions about how the rest of the world works, becasue they're meant to be a host or glue language to inner systems with better assumptions, which are more fit to purpose. I know that this perspective on systems programming languages is not universal, and that the real world is actually quite resistent to putting things into neat little boxes like this, but I think this perspective is at least a useful one. As such, I always flinch a little when I see people trying to write systems in Rust as though they're trying to figure out the one solution for something, assuming that no other solution would EVER need to exist within the same program, or even need to exist at all. I think one size fits all solutions to problems are not where Rust's strength is. Global async reactors / executors, whole-program garbage collectors,[29]Anything with global variables, really. Hating on global variables might sound kinda 90s, but that doesn't make it wrong. heck even whole program allocators,[30]Maybe Zig has some good ideas here? all of these things always make me some amount of uncomfortable because I just think that.. systems programming languages are meant for making BIG end products or libraries that last a long time, where more than one of these kinds of systems might need to exist at once, or you may need to take them apart and use them a la carte. It's not that I think these are wrong to use or wrong to make, I just don't prefer to use those kinds of solutions myself if I can avoid them because also honestly the user of my library knows better than me. There's a tradeoff in programming between flexibility and fitness for purpose, systems programming is the way it is because it's supposed to be ultimately flexible, it's what you use when a more friendly, more tailored tool isn't flexible enough. I don't like going against that idea when writing code that I want to last for a long time.[31]I also understand that compromises have to be made sometimes, and usability matters, so I genuinely mean no offense to libraries that might choose different priorities, but it might not be what I personally want. One of my favorite parts of pretty much every version of Lua is how painless it is to have multiple copies of the interpreter. If you've ever run into large garbage collector pauses in other languages, this rarely happens in Lua not because its garbage collector is just that good, but because you aren't forced to have just ONE of them in your program, you can have as many of them as you need, each in isolation from each other! Lua is a language that actually meshes very well with my vague ideas about the core idea of systems programming, because PUC-Rio Lua was written to fit almost anywhere. It's actually amazing how neatly it fits into C, how it is fine being the bottom of the hierarchy, it's just a C library and you just call C functions. Two distant parts of a program both use Lua? Different versions of Lua with different loaded libraries? You can make it work! It doesn't read external files unless you tell it to, it doesn't do anything unless you tell it to because it makes very few assumptions. It's your tool, and it fits neatly into whatever program you already have. I think this is why it has remained so popular for so many years. [32]And so popular to integrate into big, complex systems programming projects like video games. I want to make a version of Lua that feels for Rust like PUC-Rio feels for C, but to go even further. I want to make a version of Lua that fits anywhere as much as I possibly can make it. Untrusted input! Cancellation! I want to make a version of Lua with the fewest possible opinions about how the rest of the program is structured. I know piccolo is a little far from that in several ways right now, but that's my ultimate goal. I think stackless interpreters actually fit this idea of being as unobtrusive as possible, of fitting anywhere better than classical language interpreters. Garbage collection systems in general are very often at odds with this idea of fitting anywhere. There can only be one boehm gc in a C program, after all. It's interesting to me that garbage collector systems as a library for C have to be so much slower than garbage collectors written for other languages but written in C. The problem is not that C can't write fast garbage collection systems, the problem is that the C language itself has so few \"global assumptions\" built into it. It's much easier to write a garbage collector for a language that can annotate where GC pointers are on the language's call stack or in heap allocated objects than one like C, where it is extremely difficult to have such things. Progress in systems programming languages seems to happen when new abstractions are invented that give new fundamental powers but do NOT introduce more required assumptions. I think coroutines are one of these, and that all systems programming languages should have a stackless coroutine system because it is the sort of thing that can fit into other systems as much as possible. I think there is also some kind of deep connection between higher level languages whose compilers / interpreters do things like annotate where garbage collected pointers are stored in the language's call stack or automatically insert garbage collector safe points, and the idea of coroutines as a general reification of the call stack itself, letting the language do this manipulation rather than a specialized compiler. I came up with this connection way back in early 2019, but if we could make Rust coroutines implement Collect, then this makes yield / await into an abstraction of the garbage collector safe point. When a Future or Coroutine yields control to the caller, all of the (apparent) stack variables are guaranteed to be stored inside the state of the running coroutine. This would allow gc-arena to easily separate collection and mutation in normal, straight line Rust code that is simply annotated with awaits (or yields) to mark where garbage collection can safely take place in the same way that a higher level language runtime inserts safe points to mark where garbage collection can safely take place.[33]I feel like I'm doing a math proof but I can't really figure out a direct line between A and B but I know they're the same, so I go from A and get as far as I can towards B, and I go from B and get as far as I can towards A, then I wave my arms really wildly up and down so everyone gets it. I think Rust is so close to having some very interesting, novel powers with its coroutines by simply being able to combine existing features together. I can automatically serialize a custom struct with #[derive(Serialize)], and I can automatically transform a function body into a state machine, but what I cannot do is #[derive(Serialize)] this state machine, nor can I #[derive(Collect)] it. Why not?? This deserves its own blog post, but I felt like I couldn't rightly close out this post without at least mentioning it. In my next post I'm going to explore this idea more fully and hopefully try and actually make it \"work\" by pretending to be able to #[derive(Collect)] a coroutine. I think that Rust might need more than just this feature to make this system workable, but if it did work, it could represent a largely painless, general, isolated system for tracing garbage collection in Rust. A garbage collection system that can fit anywhere. Bye!",
    "commentLink": "https://news.ycombinator.com/item?id=40239029",
    "commentBody": "Piccolo – A Stackless Lua Interpreter (kyju.org)203 points by vvoruganti 16 hours agohidepastfavorite22 comments 10000truths 8 hours ago> However, piccolo works very hard to guarantee that piccolo::Executor::step returns in a bounded amount of time, even without the cooperation of the running Lua code. The issue is that there exist ways to make a single bytecode instruction take an unbounded amount of time to execute. For example, Lua's \"string.find()\" is implemented in native code. The interpreter only sees a single OP_CALL opcode, so it will count it as 1 instruction executed. But the actual execution time of the native implementation of string.find() is dependent on its inputs, which are not only variable length strings, but can be maliciously crafted to run in exponential time. Here's an example, shamelessly stolen from Mike Pall himself: string.find(string.rep(\"a\", 50), string.rep(\"a?\", 50)..string.rep(\"a\", 50)) The only way to solve this is to track execution time within the native code as well (i.e. make them fuel-aware), and ensure that they abort immediately if they exhaust the fuel. reply haileys 6 hours agoparent> The only way to solve this is to track execution time within the native code as well (i.e. make them fuel-aware), and ensure that they abort immediately if they exhaust the fuel. You will find that this is precisely the approach Piccolo takes. reply vvanders 5 hours agoprev> ... and frankly I really like just plain vanilla Lua. I think it matches perfectly with Rust because they're so different, I think having an Rust embedded language that frees you from even having to think about ownership is very powerful because it can be used for things where having to think about ownership can be more trouble than its worth. Let each language play to their strengths, and Rust and Lua in a lot of ways have complementary strengths. Tons of awesome technical details in the post and I've always felt that Lua's co-routines are incredibly undervalued. We used to run Lua in all sorts of crazy places and it really punches above its weight. The above quote + zoom out at the end also makes some really good observations. Some of the most powerful systems I've worked on picked a couple core technologies that were complementary with good interop rather than a one-size fits all. Will be interesting to see where Piccolo goes. reply camgunz 1 hour agoprevI really enjoyed the \"Zooming Out\" section, a lot of good food for thought in there. reply wesamco 14 hours agoprevI love how there are many Piccolo REPLs embedded in the article and interleaved with the paragraphs. Piccolo looks amazing, I got a perfect use case for it, and I'm excited for when I get the chance to use it, thank you for working on it. reply rnmmrnm 15 hours agoprevA bit off topic: I just wrote a bunch of lua C debug api code (lua_sethook) to pre-emptively run and context-switch multiple lua \"coroutines\" (well not so \"co\"). Is this library offering a lua implementation more well-designed for this use-case? I got all this code to unload the coroutine stack to store and and reload it before continuing it later. Does having C bindings to this library makes sense? reply interroboink 15 hours agoparentI think yes? My understanding is that the \"stackless\" concept here means that it does not store its execution state in the \"C runtime stack\" (or Rust, in this case). So, there is some blob of memory describing that info, managed by Piccolo, rather than it residing on the \"real\" OS execution stack. In particular, for call chains like: Lua -> C -> Lua -> C (or so), it is normally hard to save/restore the \"C\" parts of that chain, since you need to peek into the not-normally-accessible and platform-specific C runtime stack details. I wonder: how are you doing it in your system? In Piccolo, I imagine it would be easier, since even the \"-> C ->\" portions of the stack are being managed by Piccolo, not the base C runtime. I don't know what the APIs to access that stuff actually look like, though. Aside: have you looked at Pluto/Eris for Lua serialization of coroutines? ---- EDIT Yes, it seems like the section The \"Big Lie\" is right up your alley (: reply rnmmrnm 14 hours agorootparentWell the only thing that's really itching me is the fact that the whole lua debug.* section is documented as >You should exert care when using this library. Several of its functions violate basic assumptions about Lua code (e.g., that variables local to a function cannot be accessed from outside; that userdata metatables cannot be changed by Lua code; that Lua programs do not crash) and therefore can compromise otherwise secure code (from the manual) kinda creeps me out. reply Dylan16807 4 hours agorootparentIf you find a debugger that can't ruin a function's local variables then that debugger sucks. reply rnmmrnm 2 hours agorootparentobviously, but i'm not so keen to turn it into a pre-emptive scheduler either lol. reply rnmmrnm 15 hours agoparentprevreading beyond the first paragraphs, I see this practically what he advocates to do :P reply sanxiyn 7 hours agorootparentShe, not he. kyren is a woman. reply rnmmrnm 7 hours agorootparentTerribly sorry! reply jxyxfinite 14 hours agoprevBeautiful website! Would love to learn more about the stack reply qudat 15 hours agoprevI love the design of the website, nice work! reply klaussilveira 14 hours agoprevI wonder if a Rust reimplementation of Lua will, one day, be faster than LuaJIT. reply PhilipRoman 12 hours agoparentGood point, it is well known that speed of machine code varies greatly depending on what language was used to generate it. Jokes aside, I really don't see a point in this. The safety guarantees of Rust are thrown out of the window the second you call dynamically generated code. reply animaomnium 14 hours agoparentprevClearly that isn't the case, surely Mike Pall would have heard of it, when he brought back LuaJIT from the future, that is. reply __s 10 hours agoparentprevNot without being a JIT. See Cranelift. I've used it to make a Befunge JIT in Rust reply alberth 9 hours agoprev [–] Monospace font Please don’t use monospaced fonts for long-form text. It’s extremely different to read because your eye struggles to see the “shape” of each word, when everything is equally spaced apart. reply anthk 3 hours agoparentAs a Unifont user under UXTerm and reading RSS articles, news, chat, code and everything, I think having a good font size matters. WIth Unifont, start from 12pt and use multiples of 4: 16, 20, 24, 28, 32... until the font doesn't look fuzzy and it perfectly suits your resolution/DPI. Also, as an European I used the read the Teletext (It was almost an electronic newspaper with national/international news/sport results/weather forecasts and so on which worked accesing every page/section by a 3 digit number) on a CRT TV just fine. reply Dylan16807 4 hours agoparentprev [–] Citation needed. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article explores Piccolo, a stackless Lua interpreter developed in Rust, focusing on safe sandboxing and resiliency.",
      "Unique features include symmetric coroutines and custom scheduling, with a spotlight on efficient garbage collection and interrupting running code.",
      "Emphasis on integrating seamlessly with Rust ecosystem and creating unobtrusive interpreters for diverse systems, potential for Rust coroutines in Lua for streamlined asynchronous programming."
    ],
    "commentSummary": [
      "Piccolo, a stackless Lua interpreter, is struggling to ensure limited execution time for bytecode instructions because of functions like Lua's \"string.find()\" implemented in native code.",
      "Emphasizes the importance of monitoring execution time in native code and highlights the compatibility strengths between Rust and Lua.",
      "Comments in the article discuss Piccolo's design, possible applications, and comparisons with other technologies, offering optimization tips for better performance and font legibility."
    ],
    "points": 203,
    "commentCount": 22,
    "retryCount": 0,
    "time": 1714671481
  },
  {
    "id": 40237855,
    "title": "Orangutan Exhibits Healing Behavior Using Plant",
    "originLink": "https://www.bbc.co.uk/news/science-environment-68942123",
    "originBody": "Wounded orangutan seen using plant as medicine Published 18 hours ago Share Media caption, Watch: How Rakus healed his own wound By Georgina Rannard BBC Science reporter A Sumatran orangutan in Indonesia has self-medicated using a paste made from plants to heal a large wound on his cheek, say scientists. It is the first time a creature in the wild has been recorded treating an injury with a medicinal plant. After researchers saw Rakus applying the plant poultice to his face, the wound closed up and healed in a month. Scientists say the behaviour could come from a common ancestor shared by humans and great apes. \"They are our closest relatives and this again points towards the similarities we share with them. We are more similar than we are different,\" said biologist Dr Isabella Laumer at the Max Planck institute in Germany and lead author of the research. A research team in the Gunung Leuser National Park, Indonesia spotted Rakus with a large wound on his cheek in June 2022. They believe he was injured fighting with rival male orangutans because he made loud cries called \"long calls\" in the days before they saw the wound. The team then saw Rakus chewing the stem and leaves of plant called Akar Kuning - an anti-inflammatory and anti-bacterial plant that is also used locally to treat malaria and diabetes. IMAGE SOURCE, ARMAS Image caption, Scientists saw Rakus chewing a medicinal leaf into a paste He repeatedly applied the liquid onto his cheek for seven minutes. Rakus then smeared the chewed leaves onto his wound until it was fully covered. He continued to feed on the plant for over 30 minutes. The paste and leaves then appear to have done their magic - the researchers saw no sign of infection and the wound closed within five days. After a month, Rakus was fully healed. IMAGE SOURCE, SAFRUDDIN Image caption, After a month, the wound on Rakus's cheek was healed The scientists concluded that Rakus knew he was applying medicine because orangutans very rarely eat this particular plant and because of the length of the treatment. \"He repeatedly applied the paste, and he later also applied more solid plant matter. The entire process lasted really a considerable amount of time - that's why we think that he intentionally applied it,\" explains Dr Laumer. The researchers also saw Rakus resting for much longer than usual - more than half of the day - suggesting he was trying to recuperate after the injury. Scientists were already aware that great apes used medicine to try to heal themselves. In the 1960s biologist Jane Goodall saw whole leaves in the faeces of chimpanzees, and others documented seeing great apes swallowing leaves with medicinal properties. But they had never seen a wild animal applying a plant to a wound. Dr Laumer says it is possible that it was the first time Rakus had done this type of treatment. \"It could be that he accidentally touched his wound with his finger that had the plant on it. And then because the plant has quite potent pain relieving substances he might have felt immediate pain relief, which made him apply it again and again,\" she says. Or he could have learned the method from watching other orangutans in his group. The researchers will now be closely watching other orangutans to see if they can spot the same medical skills that Rakus showed. \"I think in the next few years we will discover even more behaviours and more abilities that are very human-like,\" she suggests. The research is published in the scientific journal Scientific Reports. Related Topics Orangutans",
    "commentLink": "https://news.ycombinator.com/item?id=40237855",
    "commentBody": "Wild orangutan seen healing his wound with a plant (bbc.co.uk)202 points by neversaydie 17 hours agohidepastfavorite110 comments balls187 8 hours agoMany years ago I stopped by a zoo in France, with my 7mo son. I was pushing him in the stroller while he was napping, and at the orangutan exhibit, one came up to the window and gestured towards the stroller. I turned the stroller to face the window, and removed the hood. The orangutan blew my son a kiss, waved then went back to the otherside of its enclosure. Made me rethink what I knew about animal intelligence. reply ornornor 3 hours agoparentThe way we treat animals is terrifying as they’re likely much much more sentient than we make ourselves believe (mostly for our own convenience) reply penguin_booze 3 hours agoparentprev> The orangutan blew my son a kiss Excuse being the kill joy: the fellow might be mimicking a gesture it learned from other visitors. That's not to say they don't have an inner life--we're all made of the same substrate, after all. reply noja 2 hours agorootparentIsn't this what humans do? reply madaxe_again 2 hours agorootparentYes. We are particularly accomplished mimics. Originality is exceedingly rare. We like to think of ourselves as different, but we are just inhabited by far more ghosts than animals without language. reply jader201 7 hours agoparentprevReminded me of this tweet I saw a couple weeks ago (from our local zoo): https://twitter.com/amazlngnature/status/1782416298636345452 reply grecy 5 hours agoparentprevI was lucky enough to spend a bunch of time with Chimpanzees and Gorillas in Africa (mostly West Africa). I got to carry them, play with them and just chill with them in the jungle [1]. Staring into their eyes it is impossible to deny we are them and they are us. Their facial expressions, emotions, reactions, playfulness and also anger, jealousy and all the rest are incredible to be a part of. Often I was on the other side of a fence, but the times I was right in the middle of it all are something I will never forget as long as I live. I have no doubt future generations will look back on us as heathens for keeping intelligent animals locked in cages. [1] https://www.youtube.com/watch?v=Mfdo3s8tPUk reply Log_out_ 3 hours agorootparentFuture generations will look at us as monsters propping up highly western economy dependent sanctuaries in economic fragile regions just before the big crash, sentencing sentients to die out of romanticism and a unrealistic view of us in reality. reply mehulashah 5 hours agorootparentprevThese are magnificent creatures. There is so much more left for us to learn about them and us. Phenomenal. reply eellpp 7 hours agoparentprevAfter reading this, the choice of word \"exhibit\" hurts reply gonzo41 7 hours agorootparentnext [9 more] Animal prison. reply bredren 5 hours agorootparentZoos are are a terrible entertainment business. All the primates and megafauna especially, it is a tragedy. I'm certain they'll be looked back on with shame. Even as zoos market videos of animals \"eating birthday cake\" on social media. See also zoos' dark history entwined with ethnological expositions https://archive.is/xsuEU reply trogdor 4 hours agorootparentI’m not sure that’s a fair statement to make categorically. Many zoos provide homes, food, and veterinary care for animals that would not be able to survive in the wild. For example, the mountain lion at Arizona Sonora Desert Museum is blind, and many of their other animals had been kept illegally by their previous owners, were confiscated by Arizona Game & Fish, and needed a place to live so they wouldn’t die. reply xvector 3 hours agorootparentprevFactory farms will be looked back on with 10,000x more shame and horror than zoos. Zoos barely scratch the surface of the immense and unnecessary cruelty we inflict on animals. We torture 70 billion land animals to death every year and the poor things will never even understand why - they're just babies, mentally speaking. It's objectively the greatest evil humans have ever committed, and for our own pleasure at that. We inflict pure hellish suffering because \"tAsTe bUdS gO BRRR!\" It's hard to even comprehend something more comically, absurdly evil, both in terms of motive and sheer scale of suffering. At least zoos do trigger an empathy response and get people to care about animals more. reply vbezhenar 3 hours agorootparentWhat? We eat meat. We need to eat meat. So we need to kill animals for that. Zoos absolutely not required for humans to survive. reply xvector 2 hours agorootparent> We eat meat. Purely for taste. > We need to eat meat. False. 10-20% of the world population does not and we're just fine. > So we need to kill animals for that. No, you want to kill animals because you want tasty food. Not because you \"need\" meat. No one living in an industrialized country does. It's trivial to create a plant-based balanced diet (and a tasty one at that.) > Zoos absolutely not required for humans to survive. Neither are factory farms. They exist to satisfy \"taste buds go brr.\" But that is not worth literally torturing[1] an animal to death. [1]: https://www.youtube.com/watch?v=LQRAfJyEsko reply Voultapher 4 hours agorootparentprevThere is also the grim reality that most pets would run away and not come back if you were to open the \"prison doors\". Think of birds, reptiles, dogs etc. Our interaction with other life on this planet is very human centric, some call this human supremacism. A line is drawn between us and other animals, and the justification is quite arbitrary in my opinion. A line that is used to justify a vast number of atrocities and systemic exploitation. I don't believe there is no meaningful difference between a human and a cricket. But between a human and a dog or pig? That's close enough that I'm willing to extrapolate. reply mrkstu 4 hours agorootparentMy dogs that have escaped the yard go walkabout for a bit and then make their way home. My daughter's covid puppy has zero desire to be away from his human family- he's constantly looking for a human family member to hang out with and won't scamper away given the chance. Now a bird/snake generally will be gone given the chance- but they truly are wild animals, while dogs are bred to be co-dependent with humans. reply Voultapher 4 hours agorootparentIndeed dogs have been genetically modified over millennia to the point many wouldn't even be able to survive in the wild, whether you think that justifies our actions is up to you. In addition the chances they want to escape are smaller than for birds. I looked it up and found this source https://www.petlink.net/blog/chance-dogs-coming-back-run-awa... that says \"The unfortunate reality is that 15% of dogs across the United States go missing.\". For context that's not 15% of dogs want to leave and do so. What's the percentage of dogs that had the chance to leave? Let's say 50%, which would imply that given the chance 1 in 3 dogs left. Albeit not all of the because they disliked their home/prison. Looking at the language used in that article exposes some mental gymnastics. They simultaneously state that dogs could find their way back if they wanted and compare it to loosing a child! No, the dog is not a child, and if the reasons you provide why they \"go missing\" are \"Fear from loud noises, Easy escape routes, Boredom and Prey drive\" I find it hard to ignore that these are adult conscious beings that may not want to live with us, and only do so when given no alternative. reply darth_avocado 16 hours agoprevObviously orangutans are much smarter as a species, they are know to pass down knowledge/behaviors, including tool usage, down generations. But there could be more to this behavior. A dog instinctively knows to eat grass when they have stomach issues and Alaskan Bears eat certain plants to expel parasites after hibernation. How do they know? Zoopharmacognosy is a fascinating subject that needs more research. https://en.wikipedia.org/wiki/Zoopharmacognosy reply Arete314159 7 hours agoparentWhen I had thrush but didn't know it, I craved yogurt and garlic, both of which would be excellent in restoring floral balance. I don't know how bodies know these things, but they do! It's fascinating. reply wil421 14 hours agoparentprevWhen my wife was pregnant the doc said to let them know if she had any cravings for minerals, clay, or something weird like that. Turns out it can mean a vitamin deficiency. Not sure how the mind knows to send out a specific craving besides food and water. reply anigbrowl 12 hours agorootparentThere's a school of thought that says a lot of thinking happens in the gut, which delegates tasks to the brain, so to speak. reply NoPicklez 9 hours agorootparentMy gut instinctively knows I need chocolate right after eating dinner reply taneq 9 hours agorootparentI reckon that's just your tongue in a silly hat with a fake mustache. reply NoPicklez 8 hours agorootparentMy tongue needs to go further down on the decision making food chain reply beretguy 7 hours agorootparentprevMy gut instinctively knows I need chocolate and then not eat dinner reply scotty79 8 hours agorootparentprevI have this after eating a bare steak. I think there's something about eating a lot of something that does not contain carbs that immediately turns on craving for sugar. reply abathur 7 hours agorootparentI had to ponder this for a minute :) I picked up dark chocolate as a snack in early 2020, which I chose very specifically to avoid carbs. I also use cocoa powder to make low-carb ice cream for the same reason. I guess I've used it this way for long enough now that I've broken my default association between chocolate and sugar. reply dylan604 7 hours agorootparentprevI say this after eating a tomato sauce. Of all foods to cause a craving for a follow up, tomato sauces are the top. Next up is smoked meat (bbq) makes me want banana pudding, but I think that's a trained habit vs a craving like tomato sauces. reply b3lvedere 1 hour agorootparentprevThere is communication between your gut and brain. https://my.clevelandclinic.org/health/body/the-gut-brain-con... Your gut microbiome also has more living cells than the rest of your cells combined. https://my.clevelandclinic.org/health/body/25201-gut-microbi... So, it's a team effort :) https://www.explainxkcd.com/wiki/images/1/10/team_effort.png reply hnick 9 hours agoparentprevI'd considered the dog eating grass thing as more of a now-disputed myth, but maybe someone has evidence. Either way my own dog loves munching on the taller/thicker grass every time we walk, like he's a sheep. His favourite foods also include broccoli and bok choy so that could be why. I think a lot of people downplay the role plants play in a dog's diet (not required but helpful especially with processed diets that can lack organ meat etc), and they don't always get fed them at home. reply UniverseHacker 9 hours agorootparentI've had half a dozen dogs, and every single one of them ate grass every chance they could get- in large quantities. I also don't think it has anything to do with having an upset stomach. Dogs seem to really enjoy grass, and make it a regular part of their diet... I know they likely can't extract any calories from it, but it may have other benefits. reply retrac 8 hours agorootparentThere is nutrition in grass which is available to mammals like dogs (and humans). They are mostly cellulose, but plants use carbohydrates and proteins for their own structure and metabolism, and these are present in all plants in at least small amounts. Roots, shoots and fleshy stalks, are better than the leaves. Like an unpleasant, less nutritious lettuce or celery with too much fibre. Those are also hard to get meaningful calories from at like 50 - 100 calories per pound, but there's some calories, and there's other nutrients in them too. reply hnick 6 hours agorootparentThe cellulose aka fibre is useful for them too, for the same reason as us, even if there are not many calories or other nutrients available. Dry food, bones, high protein diets can all benefit from a bit of bulk. I've been told that wild dogs and wolves eat the digestive contents of herbivores which is semi-digested (free range haggis!), which definitely seems true to me given how much both of mine go for rabbit/sheep droppings. This supposedly supplies some more available vegetative nutrients to them. But this came from a pet nutritionist, I'm not sure how strict or scientific their training is. reply shiroiushi 5 hours agorootparentPet dogs that live with cats are well-known to eat cat turds out of the litterbox. reply UniverseHacker 8 hours agorootparentprevMy dog strongly disagrees with you that grass is more unpleasant than lettuce. reply scruple 4 hours agorootparentprevI have a dog that literally only eats grass when he needs to retch something up. He'll go to the back door and whine and gag, whine and gag. If we're upstairs, he'll come fetch me. I don't stop him, either. I've learned that lesson. He's a mutt breed, we think half Yorkshire Terrier and half Miniature Schnauzer, for whatever that's worth. Our full-breed Rat Terrier did no such thing in 17 years of life but the mutt has been doing it since we got him nearly 15 years ago! reply Intralexical 9 hours agorootparentprevYou should get a sheepdog dog to herd your sheep-dog. reply hnick 6 hours agorootparentHah we actually did... the younger border collie also does it, but less. Maybe he's copying his brother from another mother. reply datavirtue 9 hours agorootparentprevMy dog loves Bell peppers. She will eat any piece that hits the ground, quite greedily. reply chrisweekly 9 hours agorootparentmy dogs are the same -- but substitute \"anything even conceivably food-related\" for \"Bell peppers\" reply hnick 6 hours agorootparentprevWe minded a labrador once (notorious appetites), he'd eat capsicums, and on the neighbourhood walks would even go for raw green olives that fell from trees onto the paths. I'm pretty sure they are very bitter which sensible animals interpret as poison but not he. So we had to steer away from those during that season. reply Intralexical 9 hours agoparentprevHumans instinctively know to eat earth and clay when you have certain vitamin or mineral deficiencies, or parasite infections, too. https://en.wikipedia.org/wiki/Geophagia#Humans Of course, these days you're probably better off swallowing a supplement. But where's the fun in that? reply lsaferite 15 hours agoparentprevAnother one that I've been watching recently is chicks knowing to take dust baths with zero interaction with adult chickens. One reason for this behavior is apparently pest control. reply taneq 9 hours agorootparentI find it interesting when people attribute abstract intentions to this kind of behaviour. The chick's probably not thinking \"man I gotta dust myself for mites\", it probably just feels good to get dusty. Now, the reason it feels good is probably that dust baths are correlated with fewer parasites and better overall outcomes, but I doubt the chick knows that. Same thing with cats liking scritches under their chin. They're not thinking \"ah yes I am scent marking this human\", scent glands are probably just itchy. reply iancmceachern 9 hours agoparentprevElephants too reply esalman 7 hours agoparentprevIf you're fascinated by canine instincts, wait till you hear about the super generations of monarch butterflies. reply cpeterso 5 hours agoprevOne of the most adorable TV shows I’ve seen is “Orangutan Jungle School” about the Borneo Orangutan Survival Foundation’s orangutan rescue/rehabilitation/release program. The series is available on a number of streaming services and many clips on YouTube. https://orangutanjungleschool.com/ reply rramadass 5 hours agoparentThis is THE classic Orangutan series to watch. I think some episodes (excerpts?) are available on Youtube Smithsonian Channel. Here is one - https://www.youtube.com/watch?v=V3hJgI2UrSg Anybody who watches these will come away with a deep appreciation of the Natural World, Darwinian Theory and how Human-like (even more than Chimpanzees/Gorillas) Orangutans are. After a while you forget that they are a different species and start laughing/crying/empathizing with these beautiful animals. Very highly recommended to watch with your kids and entire family. reply bifftastic 2 hours agoprevIs this actually new? See here: https://www.nature.com/articles/s41598-017-16621-w - discussed here: https://news.ycombinator.com/item?id=24669593 It's even been in Wild Kratts! reply mattlondon 17 hours agoprevIt would be nice if they could work out what this \"medicinal\" tree is and start mass-producing this paste because that looks like some serious healing going on there! ... or perhaps orangutans are just way better at this sort of thing that human are? I recently tripped up running and had a much smaller cut (compared to the apparent missing-chunk-of-flesh this orangutan had) on my knee that took several weeks to fully heal. I'd pay for this paste! reply JohnMakin 17 hours agoparentIt is already known - > The team then saw Rakus chewing the stem and leaves of plant called Akar Kuning - an anti-inflammatory and anti-bacterial plant that is also used locally to treat malaria and diabetes. reply codethief 14 hours agoparentprev> on my knee Wounds on/near joints are known to heal much more slowly because of the added movement-induced stress. In contrast, facial wounds heal particularly fast¹. For instance, I had a bike accident last September, which resulted in a small wound on my knee and several bigger, more severe injuries in my face. (I basically did a face plant.) The facial injuries healed within 2 weeks. Meanwhile, to this day people ask me what's wrong with my knee because while the wound has closed, I have severe scar tissue and it still looks like the accident had occurred only a few days ago. ¹) Not sure whether it's true but I read somewhere that this might have been selected for during evolution, since the face plays such a crucial role in communication. reply borski 6 hours agorootparenthehe... face... plant... (sorry) reply alphazard 8 hours agoparentprevThere's obviously a lot of nonsense that floats around in the herbal medicine community. But antimicrobial plants are relatively common, and applied topically, they do what you would expect them to. They generally aren't an antibiotic replacement because they aren't orally bio-available. They won't reach an infection in the periphery if you eat them. To make a medicinal plant more effective you can immerse a lot of it in a solvent and use that to extract and concentrate the chemical you care about. These concentrates are commonly referred to as \"essential oils\". None of this stuff was invented by a person, working for a corporation, and so you should not expect to see any of it commercialized, regardless of its efficacy. There will always be a synthetic analog, usually more effective (because it has been designed), and more importantly, patent-able. reply kragen 5 hours agorootparentthat's a tincture or elixir, not an essential oil. essential oils are usually obtained by steam distillation, which does not involve a solvent. and there are lots of tinctures, elixirs, and essential oils commercially available reply observationist 11 hours agoparentprevhttps://en.wikipedia.org/wiki/Fibraurea_tinctoria There are all sorts of beneficial chemicals in nature, way more than we've accurately assessed and documented. Because plants can't be patented, they're left largely unknown - the incentives that might bring access to things like this are almost entirely missing from the modern Western market. reply dudeinjapan 8 hours agorootparentWild plants themselves can’t be patented, but chemicals derived from plants can be, provided they are non-obvious and have utility. Novel plants like GMOs can also be patented. reply RenThraysk 16 hours agoprevIt's interesting for sure, but the mystery is how he gained that knowledge. Is he just repeating what he's seen humans do or his ancestors? reply rramadass 5 hours agoparent> Is he just repeating what he's seen humans do This is most probably the main reason. However there is some debate on whether these learned behaviours can be passed across generations. Here is David Attenborough showing Orangutans washing clothes/using soap/tools/etc. mimicking what they have seen Humans in their environment do - https://www.youtube.com/watch?v=IFACrIx5SZ0 reply shmageggy 3 hours agorootparentThis is an animal living in the wild who did not have contact with humans. The article mentions that he may have learned it from other orangutans, but says nothing about humans. reply observationist 11 hours agoparentprevMore than likely the plant has a noticeable topical anesthetic effect - Orangutans don't have an explicit mechanism for passing abstract knowledge so the phenomena has to be explained subjectively. It might be a particular cooling effect, or something recognizable about the plant that it contrasted against an injury for which it didn't use the plant. Or it observed another ape that did have such a subjective effect and comparison - the Einstein of orangutans might have figured it out generations ago and they've been imitating a successful behavior since then. The plant has been studied and analyzed, with various papers out there, like this one, but there doesn't seem to be anything recognized as effective for people (yet, more studies are probably needed) : https://www.phcogj.com/sites/default/files/PharmacognJ-13-1-... In previous studies, it was found that the Akar Kuning (Arcangelisia flava Merr) contains chemical compounds, including saponins, flavonoids and tannins, besides that the roots also contain glycosides and alkaloids, especially the isokuinolin group, namely berberine, jatrorizin, and palmatin. There are also some minor alkaloids such as columbamine dehidrokoridalmin, homoaromolin and talifendin, and fibraleucin terpenes, and fibraurin has several activities such as antifungal, antiasma, antibacterial, anti-tumor, anti-malarial and anti- inflammatory. reply dudinax 6 hours agorootparentOrangutans (probably) can't pass on \"this will help you heal\", but they might be able to pass on \"Put this on a wound.\" reply snikeris 15 hours agoparentprevAren’t we faced with the same fundamental mystery of human insight? Where does it come from? reply dhosek 5 hours agorootparentI’ve often wondered how the heck did people invent soap? What made them think mix ashes and fat together and use the result to make stuff cleaner? (And that’s ignoring the fact that at a molecular level, soap is pretty amazing in and of itself.) reply dorn-1845 8 hours agorootparentprevWe have systematized these pursuits. Even before modern science, language could be used to crudely pool together individual trial and error as a huge force multiplier. reply elboru 14 hours agorootparentprevTrial and error? reply JohnMakin 13 hours agorootparentBingo. We only know which mushrooms that are tasty or give us a fun time because a lot of people died finding out. reply RenThraysk 27 minutes agorootparentAt some period in our evolution, it gave us pre-dispositions to certain tastes. Like our fondness for sweet & salty is universal. reply polishdude20 3 hours agorootparentprevNot only that. A lot of mushrooms can be eaten in small doses but will cause things like stomach upset well before death. I'd bet we as a species would quickly test mushrooms this way without dieing. reply jjtheblunt 13 hours agorootparentprevThere's also cheating : isn't coffee bean edibility observed from goats? essentially outsourced trial and error is a thing too. reply GreenWatermelon 13 hours agorootparentWhich is exactly what we do with lab mice. reply dudeinjapan 8 hours agoparentprevIt was a prescription from Dr. Zaius https://youtu.be/JlmzUEQxOvA reply frogpelt 15 hours agoprevDogs eat grass when they need to throw up. reply UniverseHacker 9 hours agoparentI don't think this is true- every dog I've had eats grass frequently and it doesn't usually cause them to throw up. Also- dogs can throw up on demand at any time, they don't need anything like grass to help them. This is because it is the natural way that female dogs bring home food for puppies, they vomit up the food to share it with them. I'm pretty sure dogs sometimes vomit on purpose with the explicit goal of sharing food with humans, and/or just to eat it again for fun out of boredom. When my dog is bored he will often start preparing to vomit, and if I tell him 'no', he will stop. reply hirvi74 5 hours agorootparent> Also- dogs can throw up on demand at any time, they don't need anything like grass to help them. How sure are you about this? Everything I am reading seems to disagree with this. reply andoando 8 hours agorootparentprevMine too. Just munches on grass on every walk, never seen her throw up. I think she just likes it. reply UniverseHacker 8 hours agorootparentI think humans have a hard time understanding animal behavior, and consistently underestimate their intelligence because we don’t understand or relate to their motivations and interests. We take our values and motivations as objective facts, and don’t even consider that maybe dogs enjoy eating grass, and also aren’t that interested in the weird sounds humans like to make with their mouths. reply below43 7 hours agorootparentNot sure about the latter statement. Our dog is very interested in any keyword verbalised by us that may relate to receiving food or a walk or a ride in the car (from pretty much any corner of the house). She appears to be permanently listening to every sound we make. reply datavirtue 9 hours agoprevLet's make sure that we ban that plant, immediately. reply dudeinjapan 9 hours agoparentBan catnip too. reply codethief 17 hours agoprevI know I'm humanizing Rakus here but I'd like to think that, when he kisses the tree towards the end of the video, he does so out of gratitude for helping him. :-) reply Intralexical 8 hours agoparentApparently orangutans do kiss to show affection. And the lips are probably full of nerve endings for foraging purposes anyway. So who knows, maybe he was actually kissing that tree to show some kind of positive emotion, whether it's a kind of reciprocal appreciation similar to what we'd consider \"gratitude\" for helping him, something associative on a simpler level for making him feel better, or maybe a different, uniquely orangutan emotion that we don't have the words for. > I know I'm humanizing Rakus here Out of all the species on this planet, orangutans probably have some of the emotions that are the closest to human emotions. Like, would you expect Ancient Greeks to have emotional experiences that are recognisable to you? What about Cro-Magnons? Neanderthals? Australopithecines? Where do they stop being people with rich inner lives, and turn into animals driven by instinct? Orangutans are tree-dwelling (semi)social fruit eaters, like we were not that long ago. If you set the Tree of Life to \"Zoom to Fit\", we'd blend right together. \"Humanizing\" would be if you were talking about an octopus, a fish, or a protozoan. Different humans at different extremes already experience emotions in vastly different ways; I don't think it's unthinkable or odd that the median individual in such a closely related species might have experiences which we can relate to. But also he looked to me like he was chewing in the last couple seconds, so maybe the other commenter saying he was just eating ants was right lol. reply tambourine_man 16 hours agoparentprevProbably eating some crawling ants, but it's a nice thought reply ies7 4 hours agoparentprev> I know I'm humanizing Rakus here Well in Indonesia, orang (h)utan means Forest people reply luxpir 16 hours agoparentprevI like the idea, but he likes the ants more :) reply rdevsrex 16 hours agoprevThat's pretty cool! Perhaps genetic or innate memory is how he knew what to do? Sort of like dogs eating grass when they are sick? reply Scarblac 15 hours agoparentOrang Utan stay with their mothers for six to seven years to learn what to eat and where to find it, and visit their mothers until they reach the age of 15 or 16. (says https://www.worldwildlife.org/stories/5-remarkable-animal-mo...) reply logtempo 16 hours agoparentprevMonkey see, monkey do, and orangutan think and understand. orangutan are quite smart and can associate ideas, communicate and share knowledge. They are considered as one of the most intelligent primate. A shame they're critically endangered. reply WesolyKubeczek 16 hours agorootparentNaked ape see forest ape as competition, naked ape destroy competition. reply anon291 16 hours agoparentprevIt seems odd to me the go to would be 'genetic' memory, when mammals pretty much universally engage in learning and cultural sharing. I'm not sure about where these orangutans live, but my guess would be that, if it is not a thing the orangutans knew, centuries of living alongside humans... they probably picked up a thing or two. They're quite smart creatures. reply Filligree 16 hours agoparentprevMore likely it's something parents or friends taught him. reply mock-possum 16 hours agoparentprev> Dr Laumer says it is possible that it was the first time Rakus had done this type of treatment. “It could be that he accidentally touched his wound with his finger that had the plant on it. And then because the plant has quite potent pain relieving substances he might have felt immediate pain relief, which made him apply it again and again,\" she says. reply ldjkfkdsjnv 16 hours agoparentprev100% genetic reply dh-g 16 hours agorootparent67% incorrect reply datadeft 16 hours agoprev [–] Even the animals know that plants are medicine. Such a wierd thing that lebel these as alternative. reply tombert 16 hours agoparentI don't think people label plants as inherently alternative medicine. Plenty of non-alternative (aka regular) medicine was originally derived from plants (e.g. aspirin). Things typically become labeled \"medicine\" when they've been work and have been tested. \"Alternative\" kind of works like a logical `not`, so applying de Morgan's law that implies that alternative medicine either has not been tested or doesn't work. Obviously that's a kind of reductive take, but I don't think it's fair to say that scientists label all things from \"plants\" as \"alternative\". reply abdullahkhalids 16 hours agorootparentIt's not about what people or scientists say. It's the wrong frame to think about it in. Pharmaceuticals these days run as businesses, and its not financially beneficial for them to even quantify the medicinal qualities of chemically unprocessed plants. Because you can't build a competitive advantage around something like that. So, while they often take inspiration taken from plant based compounds, usually the essential compounds are extracted, a synthetic process developed to manufacture them, and only the medical properties of the resultant product studied via trials. It would do well for publicly funded medicine to reserve some funding for quantifying the medicinal properties of plants used directly. reply tombert 14 hours agorootparentI'm pretty sure that the NIH does do studies into efficacy of plants. This was after a ten second Kagi search: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3059459/. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3847409/ I don't know if that counts as \"publicly funded medicine\", but it's certainly a publicly funded group that studies medicine. reply SomeoneFromCA 14 hours agorootparentUSA's FDA is notorious for not finding therapeutical effect in plant medicines. The same plants which have been in use for 100s years in ex-USSR and Germany. USSR's pharma was relatively prudent and would not peddle non-working solutions for New Age reasons. reply tombert 11 hours agorootparentThe articles I posted were for the NIH, not the FDA. They are two separate organizations, with admittedly some overlap, but still distinct. The NIH researches lots of potential health claims and will publish their data as well. The stuff I linked was two examples that I found in like ten seconds, but there’s tons more. The NIH is much more of a research organization than regulatory, unlike the FDA. I just felt it prudent because the person I was responding to was lamenting about not having a publicly funded research body to investigate potential health benefits for plants that is simply untrue in the US. reply PlunderBunny 7 hours agorootparentprevMy first thought on reading this was of Tim Minchin's epic beat poem 'Storm' [0] - jump to 3:05 if you want to hear the bit that specifically deals with this. (Warning: some language NSFW) [0] https://youtu.be/HhGuXCuDb1U?si=zDhsb_llkkzbMEiB reply TylerE 16 hours agorootparentprevI like throwing those types o. Their ear by asking them if they want alternative medicine or actual medicine. reply Camus134 16 hours agorootparentprev>Things typically become labeled \"medicine\" when they've been work and have been tested Eh, sometimes. But often things labeled \"medicine\" are things that companies can get FDA approval for, patent, manufacture, ensure shelf stability, label with a brand, and sell to customers at a high margin. There are lots of plants that are proven to work but aren't sold to customers because they can't meet some of those criteria. They aren't necessarily worse, just can't be be sold at scale profitably. reply user_7832 16 hours agorootparentI don't know why your comment was dead/flagged, what you're saying isn't a new thing at all. If anyone's interested in similar things look up patent evergreening. As someone using life-saving drugs that are patented despite the creators of the said drug selling the rights for a dollar to prevent pharmaceutical capture, this stuff is irritating at best and deadly at worst. reply monetus 16 hours agoparentprev [–] My uninformed opinion is that anything which hasn't been distilled into a single compound won't be called medicine by the medical industry. How they manage to study those that are only beneficial when converging with other compounds, I have no idea. reply greenish_shores 16 hours agorootparent [–] I have some background in pharmacology, particularly psychopharmacology and I can tell you that it's not even remotely true. There's so much studies out there... so, what kind of studies were you reading? For example - Rapamycin/sirolimus, ephedrine, morphine, antibiotics (e.g. penicillins, cephalosporins), psilocin and psilocybin, are the first naturally occuring (in significant quantities) that came to my mind when thinking about some. Neither of these were studied only in any combination drugs (FDCs). And a lot of close derivatives of naturally occuring substances, like aspirin. This could be really a huge list (of both). reply F-Lexx 16 hours agorootparent [–] How about referencing (with links) a bunch of concrete relevant studies from psychopharmacology then, to back up your claims? reply gwbas1c 15 hours agorootparent [–] This is a casual message board; there's no requirement to put that kind of effort into a post here. You can also do the legwork yourself. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A Sumatran orangutan named Rakus used a plant paste to heal a wound on his cheek, marking the first observation of a wild animal using a medicinal plant for treatment.",
      "The behavior suggests a possible connection to the shared evolutionary history between humans and great apes, showing similarities in medicine use.",
      "Rakus' wound healed in about a month after applying the plant poultice, indicating a potential learned behavior from his group members."
    ],
    "commentSummary": [
      "The discussion explores animal intelligence and behavior, focusing on orangutans and dogs using plants for healing, touching upon ethical concerns in zoos and factory farms.",
      "It highlights the benefits of natural medicines and challenges in pharmaceutical patenting, calling for more research on plant medicinal properties.",
      "The conversation also addresses the limitations of the current pharmaceutical industry, advocating for a deeper understanding of animal behavior and medicinal potential."
    ],
    "points": 202,
    "commentCount": 110,
    "retryCount": 0,
    "time": 1714666145
  },
  {
    "id": 40238509,
    "title": "Hacker Search: Semantic Search Engine for Hacker News",
    "originLink": "https://hackersearch.net",
    "originBody": "Hi HN!I&#x27;m Jonathan and I built Hacker Search (https:&#x2F;&#x2F;hackersearch.net), a semantic search engine for Hacker News. Type a keyword or a description of what you&#x27;re interested in, and you&#x27;ll get top links from HN surfaced to you along with brief summaries.Unlike HN&#x27;s otherwise very valuable search feature, Hacker Search doesn&#x27;t require you to get your keywords exactly right. That&#x27;s achieved by leveraging OpenAI&#x27;s latest embedding models alongside more traditional indexes extracted from the scraped and cleaned up contents of the links.I think there are many more interesting things one could build atop the HN dataset in the age of LLMs (e.g. more explicitly searching for technical opinions, recommending stories to you based on your interests, and making the core search feature more useful). If any of those sound interesting to you, head over to https:&#x2F;&#x2F;hackersearch.net&#x2F;signup to get notified when I launch them!Note: at least one person has built something similar before (https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36391655). Funnily enough, I only found out about this through my own implementation, and I based on my testing, I think Hacker Search generally performs better when doing keyword&#x2F;sentence searches (vs. whole document similarity lookup), thanks to the way the data is indexed.",
    "commentLink": "https://news.ycombinator.com/item?id=40238509",
    "commentBody": "Hacker Search – A semantic search engine for Hacker News (hackersearch.net)198 points by jnnnthnn 17 hours agohidepastfavorite64 comments Hi HN! I'm Jonathan and I built Hacker Search (https://hackersearch.net), a semantic search engine for Hacker News. Type a keyword or a description of what you're interested in, and you'll get top links from HN surfaced to you along with brief summaries. Unlike HN's otherwise very valuable search feature, Hacker Search doesn't require you to get your keywords exactly right. That's achieved by leveraging OpenAI's latest embedding models alongside more traditional indexes extracted from the scraped and cleaned up contents of the links. I think there are many more interesting things one could build atop the HN dataset in the age of LLMs (e.g. more explicitly searching for technical opinions, recommending stories to you based on your interests, and making the core search feature more useful). If any of those sound interesting to you, head over to https://hackersearch.net/signup to get notified when I launch them! Note: at least one person has built something similar before (https://news.ycombinator.com/item?id=36391655). Funnily enough, I only found out about this through my own implementation, and I based on my testing, I think Hacker Search generally performs better when doing keyword/sentence searches (vs. whole document similarity lookup), thanks to the way the data is indexed. v1sea 16 hours agoCan you go into more detail on how it works or provide some references for articles/papers on implementing a system like this? Is it just RAG? Testing it out, I'd say the results for \"graph visualization\" are focused if a bit incomplete. So to me it has high precision, but lower recall. I don't see this searching comments. That could be a nice extension. Thanks for sharing. reply jnnnthnn 16 hours agoparentIt is mostly RAG, although I suppose that doesn't say much about the system: one thing I've found is that the way you clean and process the data substantially changes the quality of the results. I'll write a little blog post sharing some of the learnings! If you feel up for it, you should share your email in the righthand \"Unhappy with your results?\" widget. My plan is to manually look into the disappointing searches and follow-up with better results for folks, in addition to fixing whatever can be fixed. Agreed re: searching comments (which it indeed currently doesn't do). reply lpetting 16 hours agorootparenti am not surprised that 'the way you clean and process the data substantially changes the quality of the results.' can you share anything about your approach here? reply jnnnthnn 16 hours agorootparentI'll write up a little blog post once the traffic dies down a bit! In the meantime, one thing that comes to mind is that simply embedding the whole contents of the webpages after scraping them didn't yield very good search results. As an example, an article about Python might only mention Python by name once. I found that trimming extraneous strings (e.g. menus, share links), and then extracting key themes + embedding those directly yielded much, much better results. reply zarathustreal 12 hours agorootparentIn our RAG pipeline we found that implementing HyDE also made a huge difference, maybe generating and embedding hypothetical user search queries (per document) would help here reply pbronez 10 hours agorootparentHyDE apparently means “Hypothetical Document Embeddings”, which seems to be a kind of generative query expansion/pre-processing https://arxiv.org/abs/2212.10496 https://github.com/texttron/hyde From the abstract: Given a query, HyDE first zero-shot instructs an instruction-following language model (e.g. InstructGPT) to generate a hypothetical document. The document captures relevance patterns but is unreal and may contain false details. Then, an unsupervised contrastively learned encoder~(e.g. Contriever) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, where similar real documents are retrieved based on vector similarity. This second step ground the generated document to the actual corpus, with the encoder's dense bottleneck filtering out the incorrect details. reply isoprophlex 15 hours agoprevThere's at least three of us ;) I built https://searchhacker.news Loving your LLM generated summaries! Very nice user experience to see at a glance what a hit is about. Also your back button actually works, haha. Well done! reply jnnnthnn 15 hours agoparentThank you, and likewise! reply curious_cat_163 15 hours agorootparentFour. :) reply jnnnthnn 15 hours agorootparentWelp, I'm so glad I wrote \"at least one person\", and not \"one person\" in my note in the OP :D reply curious_cat_163 15 hours agorootparentHeh. But, it’s great that you put your stuff out there. Kudos to you! I will play with it some more. reply ofermend 10 hours agoprevSuper cool. Clearly many of us see the need here. I have also been working on a similar demo: https://search-hackernews.vercel.app/ 1. Stack: Vectara for RAG, Vercel for hosting 2. Results show the main story and top 3-4 comments from the story 3. Focused mostly on the search aspect - so if you click it redirects you to the HN page itself. No summaries although it'd be easy to add. Would love to get some feedback and any suggestions for improvement. I'm still working on this as a side project. Example query to try: \"What did Nvidia announce in GTC 2024?\" (regular HN search returns empty) reply codethief 13 hours agoprevNice work! Unfortunately, though, it didn't find what I was looking for in the following real-word test case: The other day I tried to remember the name of an SaaS to pin/cache/ back up my apt/apk/pip dependencies, which I think I had read about either here[0] or here[1]. After quite a bit of time and some elaborate Google-fu, I did end up finding those HN threads again. However, they did not show up on hackersearch.net for me, neither when entering the service's name nor when I searched for \"deterministic Docker builds\" or \"cache apt apk pip dependencies\". [0]: https://news.ycombinator.com/item?id=39684416 [1]: https://news.ycombinator.com/item?id=39723888 reply jnnnthnn 13 hours agoparentThanks for the feedback! I made a cutoff at stories with = 10 upvotes [2]. It only took several hours to build the semantic search on top. And that included time for me to try out and learn several different vector DBs, embedding models, data pipelines, and UI frameworks! The current state of AI tooling is wonderfully simple. In the end I landed on (selected in haste optimizing for developer ergonomics, so only a partial endorsement): - BAAI/bge-small-en as an embedding model - Python with - HuggingFaceBgeEmbeddings from langchain_community for creating embeddings - SentenceSplitter from llama_index for chunking documents - ChromaDB as a vector DB + chroma-ops to prune the DB - sqlite3 for metadata - FastAPI, Pydantic, Jinja2, Tailwind for API and server-rendered webpages - jsdom and mozilla-readability for article extraction I generated the index locally on my M2 Mac which ripped through the ~70k articles in ~12 hours to generate all the embeddings. I run the search site with Podman on a VM from Hetzner—along with other projects—for ~$8 / month. All requests are handled on CPU w/o calls to external AI providers. Query times are >> better latency + cost reply robrenaud 12 hours agorootparentMaybe a combined approach beats either? Let some non-LLM reranker quickly spit out two results, and fill in the rest with the LLM. reply hubraumhugo 16 hours agoprevGreat to see cool stuff being built on top of HN data! Many of us rely on this platform as one of the primary sources of information. > recommending stories to you based on your interests I built this as a service that monitors and classifies HN stories based on your interests (solved my FOMO): https://www.kadoa.com/hacksnack reply jnnnthnn 15 hours agoparentNice! reply Fudgel 5 hours agoprevPretty neat. I think I found a small UI glitch: If I search and then click the browser back button when I'm on the results page, the search button on the home page shows the spinning icon. reply curious_cat_163 12 hours agoprevSo, I played with it some more. I think that this is a good starting point. You can tune various parameters for what you have indexed and it will get better. I am sure it will evolve in interesting directions from here. > e.g. more explicitly searching for technical opinions... Yes, please! I would love to be able to search for strongly held opinions by folks who _know_ what they are talking about. > recommending stories to you based on your interests... I am curious how, in principle, you would you do that? Where do you think the signal that indicates my \"interest\" lies? reply jnnnthnn 12 hours agoparentThank you! To learn your interests we'd at a minimum need to know what HN stories you tend to click or comment on, e.g. by a different reader view or using a browser extension. Presumably your comments and submissions could provide useful signal as well :) reply pjot 15 hours agoprevI’ve done something very similar! But with duckDB as a vector store/engine. https://github.com/patricktrainer/duckdb-embedding-search reply jnnnthnn 15 hours agoparentSweet! I'll try running it tonight. reply HanClinto 12 hours agoprevNice work! Love seeing RAG work get developed! What about using the embeddings for nearest-neighbor search for similar articles? I.E., for any given article, can you use the embeddings of an article to run a search, rather than encoding my query? That would let me find similar / related articles much more easily. reply jnnnthnn 12 hours agoparentThanks! Yup, totally feasible. I might add that! reply xwowsersx 10 hours agoprevThank you so much for this; it's very nicely done. I'm finding exactly what I'm looking for on topics I'm eager to explore further through conversation. Bookmarked. Please let us know if and how we can help. reply jnnnthnn 10 hours agoparentThank you! reply simonw 14 hours agoprevIt looks like you generated an LLM summary of every page you indexed. What model did you use for that, and how much did it cost? reply jnnnthnn 14 hours agoparentHi Simon! Big fan of your blog. I actually generate two summaries: one is part of the ingestion pipeline and used for indexing and embedding, and another is generated on-the-fly based on user queries (the goal there is to \"reconcile\" the user query with each individual item being suggested). I use GPT-3.5 Turbo, which works well enough for that purpose. Cost of generating the original summaries from raw page contents came down to about $0.01 per item. That could add up quickly, but I was lucky enough to have some OpenAI credits laying around so I didn't have to think much about this or explore alternative options. GPT-4 would produce nicer summaries for the user-facing portion, but the latency and costs are too high for production. With GPT-3.5 however those are super cheap since they require very few tokens (they operate off of the original summaries mentioned above). Worth noting that I've processed stories by score descending, and didn't process anything under 50 points which substantially reduced the number of tokens to process. reply avereveard 15 hours agoprevdoesn't seem like it's semantic https://hackersearch.net/search?q=solutions+for+postgres+clu... sure result are about postgres but none was relevant to clustering solutions reply jnnnthnn 15 hours agoparentI suspect this is probably because of a bit of a bias in the indexed dataset: at present, the indexed stories tend to bias toward high-scores ones, and at a glance I don't see that many stories about Postgres clustering in that distribution. reply avereveard 14 hours agorootparentyeah there are only three stories coming up from the site search and none picks up things like citus etc https://hn.algolia.com/?q=postgres+clustering only one is semanthically correct, the other pick up the wrong version of clustering (i.e. k-means instead of multi master writes) but yeah if one doesn't test the hard cases, how does one know it preserves semantics :D reply jnnnthnn 14 hours agorootparentIn fairness, it's probably impossible to unambiguously determine what the intended/desired interpretation is (though intuitively it seems like k-means should be lower likelihood)! reply avereveard 14 hours agorootparentI've tried Hyde and seems to work better. had to do it client side tho. asked chatgpt: \"write one sentence explanation about this topic: solutions for postgres clustering\" which returned \"Solutions for PostgreSQL clustering involve implementing methods such as streaming replication or third-party tools like Patroni to manage and distribute database workloads across multiple servers for enhanced performance and fault tolerance.\" then I searched that: https://hackersearch.net/search?q=Solutions+for+PostgreSQL+c... and results are much better: 1. An overview of distributed Postgres architectures 2. A Technical Dive into PostgreSQL's replication mechanisms 3. Ways to capture changes in Postgres hyde paper is here https://arxiv.org/abs/2212.10496 it's possible that openai embedding are simmetrical, if that the case you need to hallucinate some content and use that as base for the embedding distance calculation. or you can move to asymmetric embedding, or you can try prompting their embedding edit: prompting embedding seems to work, tried searching for “write an article about: solutions for postgres clustering” and results are much better https://hackersearch.net/search?q=write+an+article+about%3A+... you can try prepending \"write an article about: \" to all user searches :D reply jnnnthnn 14 hours agorootparentSweet! Thanks for sharing. A prior implementation had HyDE running on user searches, but I found the results to be hit-or-miss depending on the query type. I definitely want to re-explore that though; I think it should be possible to do so a lot more rigorously now that I have a better sense for what people want to search for. reply jnnnthnn 14 hours agorootparentRelatedly, the \"Follow up queries\" in the righthand floating insert are an attempt at finding a decent balance between recovering from failures & giving users enough control on the queries themselves :) See https://hackersearch.net/search?q=postgres+clustering&period... reply avereveard 14 hours agorootparentyou should really use this visibility to get a thumbs up / down near each result and use that as a validation set :D reply jnnnthnn 14 hours agorootparentTotally! I've come to deeply dislike thumbs up/down UXes, so I'm collecting that by recording clicks on results! reply levkk 16 hours agoprevPretty cool. A little slow for a search engine, have you tried in-database embeddings for semantic search like PostgresML? reply jnnnthnn 16 hours agoparentThanks for trying it out! Agreed it could be faster for uncached queries. The embeddings retrieval itself is actually pretty fast (uses pgvector). However, I found that having a LLM rerank results + generate summaries related to the search query made results more useful, which is what accounts for much of the latency. Maybe I should make that a user-customizable setting! reply montanalow 16 hours agorootparentYou can do all of that in a single SQL query, with pgml.embed() and then pgml.train() a custom reranker with xgboost, to pgml.predict() the conversion score of a search result based on click-through-rate, or other objective. If you'd like free hosting, feel free to reach out. I'm one of the founders at postgresml.org. reply jnnnthnn 16 hours agorootparentSweet. I'll follow-up off HN. Thank you! reply larose 10 hours agoprevNot LLM-based, but I built https://hn.lixiasearch.com . Feel free to use it as a comparison point :) reply beefman 13 hours agoprevCan anyone exhibit a search that works better here than on Algolia? reply jnnnthnn 13 hours agoparentTry any of the examples listed on the landing page. You can easily access the Hlgolia search equivalent by clicking on \"Try Hacker News search instead\" to the right (on desktop). Keyword searches on the Algolia engine will generally result in better recall -- at least when identifying the right keyword is easy, e.g. the name of a company. They likely will require more sifting through results & keyword \"engineering\" however. In my mind the two approaches are complementary. I suppose there's an argument for working more directly towards blending them :) reply beefman 12 hours agorootparentThanks; don't know how I missed that. Trying the examples now, semantic search usually works better. But if I trim extra phrasing (e.g. how do diffusion models work -> diffusion models) they're about the same (but Algolia is much faster). reply Scene_Cast2 16 hours agoprevWhat's your stack like? reply jnnnthnn 16 hours agoparent- TypeScript - Next.js - OpenAI's embeddings and GPT endpoints - Postgres with pgvector (on neon.tech) - Tailwind - tRPC - Vercel for web hosting - Google Cloud products for data pipelines (GCS, Cloud Tasks) reply JPKab 3 hours agoprevAwesome!!! reply robrenaud 12 hours agoprevIt feels pretty good. I did some reading on some high quality posts about chess that I found through it. What was the biggest thing you learned while implementing this? Was anything surprisingly difficult? Was there anything that worked better than you expected? reply jnnnthnn 12 hours agoparentThanks for trying it out! > What was the biggest thing you learned while implementing this? How much the quality of the data and resulting indexes matter. My impression based on this experience is that \"RAG\" might be a cohesive set of techniques, but their application to various domains likely is very domain-specific. > Was anything surprisingly difficult? Evaluating results is very tedious, almost by definition: you need to figure out ground truth by some mechanism and build evaluation datasets from there. To be honest, a lot of this beta was built on \"vibes\" only. > Was there anything that worked better than you expected? In terms of whether something worked better than I expected: modern embeddings are really magical. I'd previously worked with TF/IDF (a decade-or-so ago) and Doc2Vec (6-7 years ago), and while those were surprisingly useful, they really pale compared to what LLM embeddings can encode in very dense representations. reply tinyhouse 16 hours agoprevNice work! I'm sure you know that you can also search Google with site:https://news.ycombinator.com. If I were you I would probably think of a niche where one can get better results by searching HN and other relevant data sources. Another suggestion is not about the search so much but about the UI. One of the worse things about sites like HN is that it's really hard to follow a long thread. If you can fix that by doing some data transformation and build a nice UI for search results, it'd be pretty useful. Good luck! reply jnnnthnn 16 hours agoparentThanks for the feedback! One big distinction with the \"site:https://news.ycombinator.com\" hack is that the search on Hacker Search directly runs against the underlying link's contents, rather than whatever happens to be on HN. We also more directly leverage HN's curation by factoring in scores. Appreciate your suggestions; will look into building those! reply stevenicr 15 hours agoprev [–] Give me a button to remove a story from the search results and you'd saved me many clicks and minutes yesterday. search 'ssh'.. select comments not stories.. omg the thing I am looking for is only a few days ago but I can't get through all the ones from the one story.. page to page.. meh.. anyways I love the privacy terms statement page, I almost used it to check something. reply jnnnthnn 15 hours agoparent [–] Ack! If you're willing to share that example with me over email (jonathan@unikowski.net), I'd love to see what we can do. Maybe good enough semantic search over comments would remove the need for filtering on post type? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Jonathan developed Hacker Search, a semantic search engine for Hacker News, utilizing OpenAI's embedding models and traditional indexes for scraped content.",
      "The search engine enables users to easily discover top links and summaries by entering keywords or descriptions, with plans to expand into recommending stories based on interests.",
      "Despite competition, Hacker Search outperforms in keyword/sentence searches, offering users a superior experience when navigating Hacker News content."
    ],
    "commentSummary": [
      "Enhancing a semantic search engine for Hacker News with OpenAI models and traditional indexes is the primary focus of the discussion.",
      "Topics include data cleaning, document embeddings, and tools like Hyde to improve search accuracy and results.",
      "Plans are in place to utilize GPT models for generating summaries, enhancing user experience, and addressing limitations in the indexed dataset, with the creator open to feedback for future enhancements."
    ],
    "points": 198,
    "commentCount": 64,
    "retryCount": 0,
    "time": 1714669260
  },
  {
    "id": 40242410,
    "title": "Innovative CSS Art: A Single Div Showcase",
    "originLink": "https://a.singlediv.com/",
    "originBody": "A Single Div: a CSS drawing project by Lynn Fisher 2014-2019GitHub#divtoberBuy me a coffee View more single divs ⇨",
    "commentLink": "https://news.ycombinator.com/item?id=40242410",
    "commentBody": "A Single Div (singlediv.com)194 points by techedlaksh 10 hours agohidepastfavorite38 comments Izkata 5 hours agoThis reminds me of something that was posted once, that I haven't been able to find again: A demo where a page returned content, but if you viewed the source it was 0 bytes. If I remember right, it took advantage of a feature that let you reference CSS through the HTTP headers (so it wasn't in the HTML), then the CSS added things to implicit required elements the browser would add to the page. reply chrismorgan 2 hours agoparenthttps://news.ycombinator.com/item?id=34464951 reply pynappo 4 hours agoparentprevmaybe you're referring to the demo from pwnfunction? \"This Site Has No Code, or Does It?\" https://youtu.be/msdymgkhePo reply techedlaksh 4 hours agoparentprevWell thats a fun experiment. If you ever find the link, do share it, would love to tinker around and see what's exactly going on in there. reply gnabgib 10 hours agoprevLots of discussion on this in 2016[1](535 points, 108 comments) and 2014[0](649 points, 141 comments), but also a similar project 16 days ago: Only s[2](135 points, 70 comments) [0]: https://news.ycombinator.com/item?id=8229072 [1]: https://news.ycombinator.com/item?id=12091173 [2]: https://news.ycombinator.com/item?id=40049258 reply techedlaksh 10 hours agoparentI found the \"a single div\" site to be interesting and great inspiration from time to time, so I shared it here. I feel like, it usually benefits the community if we see and discuss old projects from modern web development eyes. reply djbusby 7 hours agorootparentYes. HN is one place where I love a repost. reply techedlaksh 6 hours agorootparentI agree some things should not become spam but once in a while a fresh conversation is what we all need for the things that we like ! reply ChrisArchitect 5 hours agorootparentprevnext [4 more] [flagged] techedlaksh 5 hours agorootparentThese are fun little CSS Art and I agree this might have become a niche topic but that does not mean it's lost. These little graphics in the early days is what pulled me towards learning CSS even better and I hope discussing this now, today, would help new web developers get excited for CSS as it did for me back then. reply ChrisArchitect 4 hours agorootparentnext [3 more] [flagged] techedlaksh 4 hours agorootparentIt seems like you have nothing to discuss regarding the original link and it's obvious from other comments that people still find this interesting contradicting your whole point. reply pjerem 4 hours agorootparentprevIt’s new to me and I enjoyed it. Nice discovery, op, thanks ! reply wodenokoto 4 hours agoprevWhat does single div mean here? I looked at the source and gave up counting the number of divs, so it is not literally \"this page only has one div\", but ... ? reply nevir 3 hours agoparentEach \"picture\" in the list is rendered via css rules defined for a single div. Lots of box shadow shenanigans, usually reply rav 3 hours agorootparentLooking at the CSS for a few of the pictures, it seems like it's more CSS backgrounds specified using lots of gradients. I wonder if it would be faster to implement an SVG-to-CSS-gradient compiler or implement the pictures directly using CSS gradients. reply onehair 2 hours agorootparentWhile what you suggest is true, the incentive for the developer here is to try out all kinds of CSS properties in tandem and rely as much on CSS as possible. To put this all in perspective, imagine developers doing this between 2012 and 2015, where many websites were still designed by using photoshop built & sliced images as backgrounds and applied on divs to achieve glorious designs. (while still needing to load all the pictures needed to achieve such designs. Then come new fascinating CSS properties that can do away with needing static images to make those same designs. Back then, such creations brought aww to fellow developers as it was out of the ordinary. Most often the main concern wasn't performance and efficiency. reply silvestrov 1 hour agorootparentIt's also a good torture test for the browsers \"inspect element\" debugging tool. i.e. the debugger tools are useless in figuring out what the different background and box-shadow rules do. reply vidyesh 5 hours agoprevI am always in awe of anyone doing CSS Art. Thank you for sharing this. I know the current 3D graphic scene is frothing over three.js for everything but there is a place for CSS Art to be used and most people forget it or don't likely dive into it as CSS Art is really hard. I wish it was easier. three.js is great for creating big scenes but I see people creating small single character scenes for their website which can be created with CSS too! I have spent a long time trying to recreate some of the amazing codepens I have seen and it takes my whole weekend and its not even perfect then. :( I still think even though internet is ready for it, I don't think majority of the devices connected to the internet are capable to support WebGL properly. Yes, it differs with audience but silos are bad, create websites accessbile for everyone not just your silo perhaps. reply techedlaksh 5 hours agoparentI agree that there is novelty in doing CSS Art but due to the fact that it requires a lot of deep dive into nitty gritty-ness of css, devs usually don't prefer it whereas three.js is being pushed from niche 3D artist coder community to mainstream and that might just be a good thing. Saying all that, three.js is not a perfect solution to everything and should be considered with it's pros and cons without creating unnecessary bloated websites. reply chilling 3 hours agoparentprevYou can try out my CSS-only bonfire! (: https://megaemce.github.io/Bonfire reply purple-leafy 6 hours agoprevI’m on mobile, am I to understand each graphic is built from a single div? Incredible! Good eye for graphic design too reply spondyl 5 hours agoparentYep, they're all just one div. Well, technically a div plus ::before and ::after pseudo-elements but still, it's all CSS reply karaterobot 7 hours agoprevWell, I'm glad this was posted. I vaguely remember it, but I haven't looked at it in a while. It's incredibly impressive. reply techedlaksh 6 hours agoparentI have been wanting to give this site another look too but could not remember the name until i dig deeper into my \"dump link\" folders. reply poopsmithe 2 hours agoprevWhat is the significance of a single div? What is the meaning, what is the method? Without an about page, I'm left to speculation. reply alfon 2 hours agoparentThe \"about\" page is a comment in the HTML source: > This is a CSS drawing experiment to see what's possible with a single div. To learn more about this project: GitHub: https://github.com/lynnandtonic/a-single-div Mozilla Hacks: https://hacks.mozilla.org/2014/09/single-div-drawings-with-c... Talk from CSSDay: https://lynnandtonic.com/thoughts/entries/talk-illustration-... Codepen: https://codepen.io/lynnandtonic Mastodon: @lynnandtonic@front-end.social Twitter: @lynnandtonic reply NayamAmarshe 49 minutes agoprev\"Coding is art\" and here's the proof! reply guardian5x 58 minutes agoprevWhen i look at the html of the website i see a lot of divs. reply ryanthedev 7 hours agoprevI absolutely loved this site. Thank you for sharing. reply techedlaksh 6 hours agoparentSometimes I just wish, I could sit down and just recreate these little pieces one by one reply PcChip 5 hours agoprevCan someone ELI5 what this is and why it’s impressive? reply vidyesh 5 hours agoparentEach drawing is created with CSS and within a single div. This is not a SVG or PNG etc. Its just pure CSS, even the animations of course. reply techedlaksh 5 hours agoparentprevBasically each graphic is a single `div` element and everything else comes from CSS. Early days of codepen were also used to be filled with these kind of fun little CSS Art. reply exodust 2 hours agorootparentDo we know if these were all hand-coded? I gather there's vector drawing tools that can make these kinds of graphics.. But perhaps those tools can't output CSS attached to one div? reply IneffablePigeon 2 hours agorootparentI would be surprised if any aren’t hand coded. You have to be extremely economical with shapes with this technique - generally (unless I’m out of date on css art techniques) you have your original element, a before and an after element, and an arbitrary number of box shadows of those elements which you can use to duplicate them in different colours and positions. So you have to pick 3 basic shapes to draw with. In some of these you can see that limited shape palette at play but some of them hide it very well. reply omoikane 7 hours agoprevThis reminds me of https://news.ycombinator.com/item?id=8070879 - Rendering Worlds with Two Triangles on the GPU (2014) reply techedlaksh 6 hours agoparentI usually follow Inigo's work but somehow i missed this one. Thanks for sharing this. reply doawoo 7 hours agoprevBrave little toaster! reply andrethegiant 5 hours agoprev [–] Lynn hype reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"A Single Div\" is a CSS drawing project by Lynn Fisher presented on GitHub between 2014-2019, showcasing diverse designs crafted using a single HTML element.",
      "The project provides the opportunity to support the artist by purchasing a coffee and exploring additional single div designs."
    ],
    "commentSummary": [
      "A Single Div\" showcases graphics crafted solely with CSS in one HTML div, sparking admiration for CSS art's technical prowess in web development.",
      "Users engage in conversations about CSS art, highlighting its importance and sharing resources and tools for creating CSS art.\"",
      "The post includes discussions on other projects related to CSS art, extending the exploration beyond \"A Single Div.\"\""
    ],
    "points": 195,
    "commentCount": 38,
    "retryCount": 0,
    "time": 1714691860
  },
  {
    "id": 40241525,
    "title": "Startup Accelerator Newchip Folds: Founders Lose Companies",
    "originLink": "https://techcrunch.com/2024/05/02/they-thought-they-were-joining-an-accelerator-instead-they-lost-their-startups/",
    "originBody": "(opens in a new window)(opens in a new window)(opens in a new window)(opens in a new window)(opens in a new window) Link Copied Featured Article They thought they were joining an accelerator — instead they lost their startups How Newchip’s bankruptcy threatened the cap tables of thousands of startups Mary Ann Azevedo, Christine Hall / 5:38 PM UTC•May 2, 2024 Comment Image Credits: Bryce Durbin / TechCrunch Lacey Hunter thought all was well as she put her startup through the three-month Newchip accelerator. Then the organization filed for bankruptcy in May 2023. Things went from bad to worse later that year when she discovered warrants of her company — rights to buy an ownership stake — had become part of the proceedings, which ultimately forced her to shut down her company. In 2022, Hunter started TechAid, an AI smart-matching tool for humanitarian aid, and was just beginning the accelerator’s curriculum when Newchip filed for bankruptcy. “I made a few friends, but functionally, got nothing from Newchip,” Hunter said. “I was shooting to have the curriculum done by August, but in May, the website went down.” The now-defunct Austin accelerator had filed for bankruptcy amid employee and customer discontent. The court has since ordered the company to auction off the warrants it held in more than 1,000 of the startups that went through the accelerator program. Normally, private companies like startups have control over which investors are allowed to buy shares and the prices they pay. But the bankruptcy court, which works to restore creditors rather than equity holders, isn’t allowing Newchip’s startups to exert that kind of control. Instead, the auctions are ongoing, with the first tranche already sold and upcoming tranches expected to be sold this spring and summer. Founders are outraged — including some, like Hunter, who have actually lost their companies as a result. TechAid fought the sale of the warrants prior to closing the company. Hunter tried to buy them back herself from Newchip, but the organization’s lawyers declined her offer, she told TechCrunch. She had lined up a grant from a bank to help fund her offer, but it ultimately told her no because it was too risky for them to be involved with an unknown warrant holder on her cap table. So Hunter felt she had no choice but to shut TechAid. “There was no path,” Hunter said. “I knew I was not going to be able to raise money. I mean, I couldn’t even get a no-strings-attached grant. I totally get that, but it still sucks.” Newchip’s fall from accelerator grace Newchip started out as an aggregator of top deals from “various equity-based crowdfunding platforms,” according to Silicon Hills News, and later evolved into an accelerator that promised to help startups grow their companies and meet investors — for a hefty fee. It charged startups between a few thousand dollars and $18,000 to $20,000 for its training programs, founders said. Startups also granted Newchip the right to buy $250,000 worth of shares in the company at a later date, but at their current valuation — this type of deal is also known as a warrant. Newchip founder and CEO Andrew Ryan previously faced harsh criticism about his leadership style, including allegations that he could be “abusive” and threatening to employees, according to eight former employees who walked out. (Ryan acknowledged to TechCrunch last year that his leadership style was based on “a military mindset.”) One example involved a meeting of about 15 employees in sales, operations and marketing. Ryan had asked the leaders of each department to read a book on how to help college volunteers be more passionate about volunteering, recalled one person who attended the meeting. Ryan asked two of the company’s leaders to lead the group in a discussion of the book. But many were confused by it and didn’t see how it applied to Newchip’s business. “They were struggling with it. Andrew kept jumping in and interrupting them, and directly challenging them.” And finally, recalled the source, Ryan said, “This was a test for individuals that I’ve asked to do this today. I was going to fire one of you, based on whoever did the worst job.” He then singled out one person, told the room the person was fired, and, this person recalled, Ryan then said, “I do stuff sometimes to see who’s loyal and to see who is going to do what I tell them to do. This was a test and you failed. You’re out.’” After seeing Ryan fire this guy in front of the whole room, “I literally watched all of his direct reports sitting there saying to themselves, ‘I will never trust this man again,’” the source said. Ryan contends that the person who was fired during that meeting had behaved aggressively after being singled out. Ryan also claims that the individual had come unprepared to lead the meeting, which Ryan viewed as an “act of overt insubordination,” telling TechCrunch: “While conducting the termination publicly in that meeting may seem harsh, it was intended to reinforce the gravity of the situation and ensure all managers understood that we took these training sessions and their responsibilities as leaders seriously.” Image Credits: TechCrunch When Newchip (which also did business under the name Astralabs) initially filed for bankruptcy in March 2023, it was a Chapter 11 debt reorg. It then went into Chapter 7 — dissolution and liquidation — two months later. Its Chapter 11 filing revealed that it had $1.7 million in total assets and $4.8 million in total liabilities. But the value of the warrants was apparently not taken into account at that time, a source familiar with internal happenings said. Those warrants were estimated to be valued at an eye-popping just under $500 million by Austin-based VC fund and early Newchip investor Sputnik ATX, according to a document viewed by TechCrunch. “I feel so much stress and embarrassment. I’m a struggling founder and don’t have the money to pay for a lawyer. Here was this accelerator supposed to help founders, and instead it is imposing stress on young founders.” Management had not been keeping up with the warrants to the point where it had missed that some companies had exited or raised money, losing out on the potential upside, noted Kerstin Hadzik, a consultant who was brought in to serve as interim CFO a few weeks after the initial bankruptcy filing. How much did Newchip potentially lose? Sputnik ATX said it identified $54 million in warrant value from companies that had liquidity events “that should have been reported to Newchip but were not,” according to documents viewed by TechCrunch. In Hadzik’s view, Newchip might have also been saved from going into Chapter 7 if Ryan had been willing to step down as CEO and had presented the warrants as assets when initially filing for Chapter 11. The judge repeatedly asked Ryan if he would voluntarily step down and let someone else, such as a chief restructuring officer, run the company. Ryan repeatedly dodged the question, expressing doubt that anyone could do so successfully. Ryan also noted that employees had requested “a new CEO” and later claimed that he “was going to step aside … but the shareholders and investors, as part of them putting capital in, preferred that I stay here to make sure that we have the capital … to continue driving the business.” Ryan also admitted that he was the company’s “major owner and shareholder” and that he had just “terminated all the board” the week before, just after having filed for bankruptcy, according to court documents viewed by TechCrunch. “The judge was offering like a lifeline,” and Ryan “just said no,” Hadzik recalled. In a Zoom interview with TechCrunch back when we first reported on the bankruptcy, and in two LinkedIn posts in 2023, Ryan said that he accepted “full responsibility for the events at Newchip.” Ryan later alleged that there was an attempted coup on the part of an investor but sources say that Ryan had actually asked early investor Joe Merrill to serve as CEO before changing his mind and resuming the role himself. Merrill, who was an early investor in Newchip under its previous model and also co-founder of Sputnik ATX, declined to comment beyond noting that he believed the attempted sale of the warrants was a valid move. Founders fight for their companies One founder, who asked to remain anonymous, told TechCrunch that Newchip had approached her on LinkedIn and told her if she got approved to join, she would get introductions to investors. So she paid a $7,500 deposit and was all set to join Newchip when a founder friend told her to “never pay for introductions.” She decided to hear out Ryan. What convinced her to ask for her money back was that Ryan “blew off our meeting.” He reached out later, but she had already emailed Newchip asking for her deposit back on the basis that she had not started yet. The founder got her money back, but Newchip didn’t void her contract, so she is now part of the bankruptcy lawsuit. That’s when she learned that someone could buy the warrants of her company for pennies on the dollar, and “it could screw your valuation going forward,” she said. “I feel so much stress and embarrassment,” she told TechCrunch. “I’m a struggling founder and don’t have the money to pay for a lawyer. Here was this accelerator supposed to help founders, and instead it is imposing stress on young founders.” There was a period of time when founders could object to their warrants being sold, according to Chad Harding, managing partner at Peak Technology Partners, the investment banking firm tasked by the court to sell the warrants. The deadline for those in the first tranche to object to these sales was January 15, he told TechCrunch. Founders from all over the world, including Australia and Finland, filed objections, according to court documents. “We were in the process of obtaining a refund from Newchip when Newchip went bust,” wrote Veronica Hey, CEO and founder of Australian startup Ok Away. “The contract is therefore null and void and the warrant attached to it is not applicable. None of this will stand up in an Australian court. If you continue to pursue in ‘selling’ this warrant you are selling something that does not exist and there will be repercussions.” But startups’ objections were made in vain when the court overruled them. A bankruptcy court’s goal is to oversee the selling of assets to settle debts. If there is money left over, it’s paid to shareholders. Ryan is the majority shareholder. So the warrants are being sold in three tranches. The first involved 133 companies, including for startups such as Cleanster.com, bitewell, Agshift and Firehawk Aerospace. Combined, those 133 startups had raised over $340 million in funding, according to documents shared by the sales agent with potential investors and viewed by TechCrunch. Ultimately, the sales agent ended up selling 28 warrants in just four companies from the first tranche for a total of about $58,000, presumably at a discount. Successful bidders included Bitewell and ClearForce — startups that bought back their own warrants in advance for $5,000 each, according to an agreement with the trustee — as well as Palm Ventures and Angel Deal Syndicate. The latter purchased the bulk of the warrants, spending $43,000 on warrants in 24 companies, according to court documents viewed by TechCrunch. The second tranche will likely be sold this summer and will include over 1,400 warrants for sale, according to Harding. The bid deadline will likely be late July, Harding said. Founders of those startups included in the second tranche will also have the opportunity to object with a proposed deadline of May 31. Ryan maintains that extensive efforts “have been made to notify stakeholders well in advance.” “This has afforded ample time for interested parties to access information and documents, raise any objections or issues, and prepare for participation in the sale,” Ryan told TechCrunch. When dreams become nightmares Like TechAid’s Hunter, Garrett Temple blames the loss of his company on Newchip’s demise. He, similar to Hunter, also participated in Newchip’s accelerator program from January until May 2023. His startup, Novogiene, was a medical tech company focused on epidemic prevention. Temple put around $7,500 on his credit cards to be part of the program and said that he never spoke with investors. His main reason for doing Newchip was to get investors for a $500,000 round, in part to pay for a small production run of his device so he could send it to universities and medical schools for pilot testing. The meetings with investors were supposed to happen after a demo day that was scheduled for the summer. But when Newchip shut down in May, that demo day, and hence those introductions, didn’t happen. Temple wasn’t able to keep going and ended up dissolving Novogiene in the summer of 2023. As such, his company no longer existed for warrants to be sold to potential investors. Temple said he spoke with his bank about getting money back from the program since he used credit cards. The bank was at first successful in getting $5,000 returned. However, about a month later, Temple noticed that money was no longer in his account and believes Newchip protested the funds. Though Temple has moved on, he still has some intellectual property for Novogiene and says he is hoping at some point to license the technology to someone else or perhaps at another time pick up where he left off. “It was very sad to call it quits because getting the funding to make those units was the only hurdle before making serious progress,” Temple said. “If they connected me with investors like they said, I could have made my invention, gotten efficacy and would be shipping units right now. I really do believe that.” Accelerator operators sell dreams. But that doesn’t always mean that the accelerator will come through. And sadly, the founders who buy into those dreams can be the ones who end up paying the price. Please login to comment Login / Create Account TechCrunch Disrupt 2024 Innovation For Every Stage LEARN MORE Sign up for Newsletters See all newsletters(opens in a new window) Daily News Week in Review Startups Weekly Event Updates Advertising Updates By subscribing, you are agreeing to Yahoo's Terms and Privacy Policy. Email Subscribe (opens in a new window)(opens in a new window)(opens in a new window)(opens in a new window)(opens in a new window) Copy Tags accelerator Andrew Ryan bankruptcy Newchip venture capital",
    "commentLink": "https://news.ycombinator.com/item?id=40241525",
    "commentBody": "They thought they were joining an accelerator – instead they lost their startups (techcrunch.com)191 points by e2e4 12 hours agohidepastfavorite119 comments ultrasaurus 10 hours ago> So she paid a $7,500 deposit and was all set to join Newchip when a founder friend told her to “never pay for introductions.” Hopefully everyone knows this here, but if you paid for an introduction it's a negative signal: just cold email. That being said, I'll make intros for only $6,500 and no warrants. reply pavel_lishin 9 hours agoparentDon't listen to this charlatan. For only $6,499 I'll introduce you to a chap who won't charge you a cent over $6,498 for an introduction. reply hedora 7 hours agorootparentI charge as much as the second lowest bidder. Second lowest because sustainable value extraction is important to me. reply atherton33 10 hours agoparentprevOf note from the article: she complained and was refunded the money after being stood up for the meeting, but they never cancelled the contract she paid to sign that gave them the right to buy her out of her own company for pennies, so once it passed to bankruptcy the creditors still took her company. reply tptacek 7 hours agorootparentThey didn't directly take her company, right? They held on to warrants for some % of the company, which killed her chances of fundraising? reply darkerside 9 hours agorootparentprevWouldn't a contract like this be considered unenforceable? There were no services rendered, no exchange of value reply unyttigfjelltol 8 hours agorootparentThe issue is they dropped a bet-the-company litigation on 1,000 startups that are not positioned to do anything but close operations. reply darth_avocado 5 hours agorootparentprevThe contract SHOULD be unenforceable, however, not sure if bankruptcy court will actually resolve that matter. Maybe it would be a separate lawsuit? reply SpicyLemonZest 5 hours agorootparentIIUC, the problem is that the founders have no good way to force a resolution at all until whoever buys the warrants attempts to exercise them. reply langsoul-com 3 hours agorootparentprevFounder has no money to pay for lawyers. No doubt those fees would bankrupt the company completely. reply morgante 4 hours agoparentprevI don't really understand how these predatory businesses continue to exist. I understand some entrepreneurs are very out of the loop / far from the VC ecosystem, but even just Googling about it you will find lots of clear advice to never pay for introductions or accelerators. reply iamleppert 11 hours agoprevAdd TechStars to the list of accelerators to be avoided at all costs. They make an investment in your company on terms they can claw back the money at any time. Most of these accelerators provide little to no value, in my experience. Unless you need to know what “product market fit” means. Hilarious. reply xpl 9 hours agoparent> They make an investment in your company on terms they can claw back the money at any time It is a loan then, not an investment. reply IncreasePosts 8 hours agorootparentEven worse - loans can't usually be clawed back at any time reply withinboredom 2 hours agorootparentNearly all loans I have seen have a provision in the contract that the entire loan can be called at any time. reply bratwurst3000 1 hour agorootparentIs this a American thing? reply justinclift 35 minutes agorootparentSeems to be common in at least western countries. reply brianoconnor 4 minutes agorootparentDefo not the case here for consumer loans. A German bank cannot tell you to pay back a loan earlier than agreed on in the contract. dryanau 37 minutes agorootparentprevWhat? No they don't! reply singleshot_ 7 hours agorootparentprevPerhaps this is what you meant by usually, but otherwise I suggest reading the fine print on your home line of credit, if you have one. reply fuzzythinker 10 hours agoparentprevAny source to your avoid TechStars advice? reply chambers 9 hours agorootparentThis piece goes into some detail: https://www.founderscoop.com/2024/what-went-wrong-at-techsta.... An erudite and incisive autopsy of a disintegrating accelerator. I'm no longer in the startup scene so I can't vouch for its facts, however. reply fuzzythinker 9 hours agorootparent> https://www.founderscoop.com/2024/what-went-wrong-at-techsta... Thank you, it's what I'm looking for to understand why. reply threeseed 5 hours agorootparentprevThe other problem with advice is that TechStars operates like a franchise. So you don't know which of the many accelerators are problematic or not or what their incentives are. reply moralestapia 10 hours agoparentprevMost VCs add zero value aside from the money. Bootstrapping is always better, if you can do it, of course. Exhibit A: Naval Ravikant, the flagship SV investor, widely regarded to as \"a wise man\", just released a kind of crappy messaging app that flopped. Imagine having unlimited leverage, unlimited money, unlimited reputation, a huge audience already in place and still that not being enough to put out a competent product. Now imagine this guy asking for 20-30% of your company equity in exchange for \"advice\", lol. reply dtnewman 10 hours agorootparentNo one gives away 20% of a company for advice. He gives them capital. Hopefully connections. And founders can take or leave the advice. Anyway, he is a successful entrepreneur having built AngelList. Sure, maybe he isn’t Midas, but a single failure in a startup doesn’t make someone an idiot. But assuming you are referring to AirChat, it seems too early to call it a failure anyway. reply Lewton 40 minutes agorootparent> a single failure in a startup doesn’t make someone an idiot Depends on the failure reply stuffinmyhand 3 hours agorootparentprev> Airchat is an app that lets you chat with anyone, anytime, anywhere. You can share photos, videos, and personal info with other users, but data is not encrypted and may be shared with third parties. If this isn't gonna fail dramatically, I'm eating my hat reply moralestapia 10 hours agorootparentprev>No one gives away 20% of a company for advice. You'd be surprised at how common that is. Wouldn't you be inclined to believe that @naval wouldn't want to use that capital, connections and whatnot to support the single project of its own authorship in its lifetime? The results speak for themselves. I have another theory, VCs freeride on the success and luck of other people's projects, which (sometimes) are so good and so profitable that they can even afford to have someone leeching off them. Just look at how many stories are there where the founders end up with zilch and regret ever taking VC money. Any of the random guys on Twitter that are building and shipping stuff and making 4-5 figures on their side projects is worth more than a 1,000 Denpoks sharing their \"wisdom\" with you. reply gfourfour 7 hours agorootparentThis is kind of a confusing perspective considering VCs are giving you money to pay for the operations of your business. Money is money no matter how stupid the giver is, their money won’t leech value from your company itself. As for founders ending up with nothing, in those cases their investors ended up with much much less than they were hoping to too. Plus there’s plenty of other cases where founders get rich off a worthless company because of the beneficence of VCs. reply knappe 7 hours agorootparent>>Money is money no matter how stupid the giver is, their money won’t leech value from your company itself. Yeah, this isn't true for a number of reasons. 1) The money you accept is given in trade for a percentage of the company and that means influence in the company. That influence almost always comes in the form of board seats which literally drive the direction of the company. I've seen many successful companies do some really stupid things because the investors wanted it that way and it actively hurt the business. 2) Certain investors come with a set of prestige. You're the n a forum which is known for just that. Who you take money from certainly matters. reply threeseed 5 hours agorootparent> That influence almost always comes in the form of board seats Most pre-seed and seed investors don't take board seats. And at Series A and above they are putting in enough money where it seems fair enough. reply rdlecler1 4 hours agorootparentprevYou don’t seem to understand basic economics. When you get on a bus and pay for your ticket, is the rest of the ride a free ride? VCs are running a business too. Most VCs fail to return the capital to their LPs. That’s right, then spend 10 years of their life working with startups and have nothing at the end. They take RISK and they try to DERISK their investment by helping the portfolio company. There’s also a LOT of stories where a VC invests a LOT of money only for the company to get recapped. The founders are given (some say rewarded) with new equity with the VC is wiped out. In many cases a founder will exit handsomely and the early VCs who came in end up with nothing. That’s the risk. VC is simple but it’s not easy. reply CPLX 29 minutes agorootparent> then spend 10 years of their life working with startups and have nothing at the end How do they avoid starving to death after the first few days? reply langsoul-com 3 hours agorootparentprevThe entire point of a vc is to fund many and hope a success works out. Read on what Paul Graham said about ycomb, have shit tons of people dig holes to find treasure. reply nl 8 hours agorootparentprev> just released a kind of crappy messaging app that flopped. I don't think releasing a messaging app that flops is bad? If getting a messaging app to succeed was easy then there would be more successes at it. reply IncreasePosts 8 hours agorootparentHell, google has released about 30 and 33 have failed reply hobobaggins 8 hours agorootparentI just re-read your post and caught it - thank you, made my evening! reply drewcoo 5 hours agorootparentprev> I don't think releasing a messaging app that flops is bad? For an exec, it's a \"learning experience.\" Most startups fail. Take the VC cash, fail, and \"learn\" on their dime. For any employee, it's s short stint to list on a resume that will make them less attractive to recruiters for the rest of their careers. \"Why were you only at FooBarCorp for Baz months?\" (Oh, wonderful - how do I explain without throwing anyone under the bus?) reply ldjkfkdsjnv 10 hours agorootparentprevIf the advice was a slam dunk they would just start the company. There are too many factors for it to be useful, and they know that. Theres too much nuance reply morgante 4 hours agorootparentprevStartups are incredibly hard. If Airchat \"flopped\" it still got way more usage than most bootstrapped startups ever see. reply moomoo11 3 hours agorootparentCan you explain why? Are you suggesting he used the money on marketing instead of product? I don’t know too many bootstrapped companies that are billion dollar plus but most well run bootstrapped companies can end up becoming successful small and mid level companies that make a decent money. At least from a revenue:num_employees, can’t be capital intensive so they have to be profitable from early on. Pls explain I’m interested to learn thanx reply morgante 2 hours agorootparent> Are you suggesting he used the money on marketing instead of product? No, a lot of the marketing/hype just came from Naval and other influencers on Twitter. > most well run bootstrapped companies can end up becoming successful small and mid level companies that make a decent money \"well run\" is doing a lot of work here, but: - Most bootstrapped companies fail, period. - Most of them fail without anyone ever noticing they existed. It's very hard to break through the noise, and I know lots of companies/projects that failed without ever getting more than a handful of users. reply ilrwbwrkhv 7 hours agorootparentprevYa I do not know why people hold Naval up so high. What has he ever done? reply MichaelZuo 10 hours agorootparentprevI've seen this 'Naval' on Twitter, or now X, but there's hundreds of accounts with that shtick, catering to various groups, so it seems doubtful. Are you sure he was 'the flagship SV investor, widely regarded...' among serious SV folks? Or just among the peanut gallery? reply alexeichemenda 8 hours agoprev>\"It was very sad to call it quits because getting the funding to make those units was the only hurdle before making serious progress,” Temple said. “If they connected me with investors like they said, I could have made my invention, gotten efficacy and would be shipping units right now. I really do believe that.\" It's unfortunate to see a founder believe that one accelerator would make or break their company. Typically an accelerator amplifies your existing trajectory - if you're a fast-growing company, you'll get more term sheets from investors than you know what to do with. If you're flat, they won't be attracting investors in any way. It's a founder's job to navigate this instead of relying on the accelerator to find $500k. reply JumpCrisscross 7 hours agoparent> Typically an accelerator amplifies your existing trajectory If only they had a word increasing velocity :) reply CalRobert 6 hours agoparentprevSome accelerators are targeted towards people who do not yet have companies. I can't say anything for or against them but I briefly participated in (and chose to leave) Carbon13, which aims to match people with a cofounder. reply pedalpete 54 minutes agoprevSo is the technicality of a warrant vs a SAFE that lands these founders in this position? As a SAFE is an agreement for future equity, would it also be treated as a warrant in a bankruptcy? Why would NewChip decide on this structure vs the more common? reply jmward01 7 hours agoprevSo the question to the experts here is, what should someone look out for as a potential founder or employee (early or late)? I've so far seen 0 upside from the three startups I have worked at and I am not likely to think of options as an incentive in the future. Is this the new norm? Are the days of equity as compensation dead (even for founders)? reply gnicholas 6 hours agoparentIt’s long been the advice (at least on HN) to assume the value of options are 0 if you are employee. You have no ability to control dilution as a mere employee, so in almost all cases they will be worth nothing. As a founder, you’re in control. Your equity is worth as much as you make it! But the more funding you need to take on, the more diluted you’ll be. Bootstrappers grow slower but remain in complete control, and can’t be screwed by rare events like this one (or more common dilutive events, which VCs may force on you). reply hn_throwaway_99 12 hours agoprevWhat a psychopath. To anyone who may be in this kind of situation, trust your instincts and leave. It will not get better. You will find other, better opportunities elsewhere. Best lesson I ever learned was from a high level exec that had just started at the company where I worked. He quit in 2 weeks. Impressively, he did it without drama or really even causing bad will - he just told the CEO it wasn't a match, and that the longer he stayed the more detrimental it would be to both himself and the company. I wish I had followed his lead - I was too worried about what it would look like to leave a company so soon after joining. reply kayodelycaon 10 hours agoparent> I was too worried about what it would look like to leave a company so soon after joining. You don’t have to put every job on your resume. :) reply bbarn 9 hours agorootparentExactly. I worked for 4 months at a company and just put \"Contractor - various\" in that spot because I did not want the stigma of that company on my resume. reply dylan604 11 hours agoparentprevI once worked at a place where an employee started their first day at the start of the shift. They mumbled their way through it to lunch where they never returned. That's the shortest I've personally seen. Absolutely not a c-suite role or anything management related though. reply slyall 8 hours agorootparentI had somebody sit near to me who wanted to leave at lunchtime but was convinced to stay on and left after the second day. This was on the ads team. The job involved managing ads on our site (via google ads and some other providers). They had never done this sort of thing before[1] and were not up to being thrown in the deep end. [1] I think they had done other sorts of advertising, just not for websites reply dylan604 6 hours agorootparentYou say deep end, but any chance somebody had a crisis of consciousness with working with ad tech. Oh who am I kidding? reply vidarh 5 hours agorootparentprevI've had that as well. I've always wondered how someone felt so out of place they wouldn't stick it out until the end of the day, or give it a few days. reply dylan604 4 hours agorootparentI always tried to think of it as they were interviewing for some other place they really wanted, but that place was taking a really long time to get back to them so they took something else before things got too dire financially. Then the other job finally reached out, so they said c'ya! I really have no idea the actual thought process though. reply vidarh 3 hours agorootparentI wouldn't be surprised at that, but I personally would still at least stay out the day. And sit down with someone and tell them. I guess the chance is small people will remember them, but you never know where you'll come across someone again - I had someone (that I thankfully had better memories of) that had worked for me show apply for a totally different position at a different company where I was the hiring manager without knowing I worked there, for example... reply Paul-Craft 9 hours agorootparentprevI think you're the winner here. The shortest I've seen is someone who left after their first day. reply swader999 9 hours agorootparentI usually stay long enough to get the company mug. reply datascienced 7 hours agoparentprevExecs can have bigger voids due to gardening leave, non competes so it looks OK. But honestly because you can (in theory) lie on your CV it ain’t worth worrying about. So the scenario where you die because you run out of money and need a job because of a gap can be remediated that way. Change the dates or something. reply aitchnyu 5 hours agoparentprevI'm trying to normalize this to my circle and strangers in India, where getting let go is a huge shame. I joined a company and my boss wanted me to be a human copilot, given a task and left for a few hours before getting interrupted. I copy pasted 500 line scripts in Lambda web editor, released certain versions for dev/prod, complained about sql injection and variables named data which overwrite each other depending on state and more crap. reply kcartlidge 11 hours agoparentprevYears ago I took a dev role at a very well known UK organisation, a prestigious brand supposedly good for the career. Their systems turned out to be smoke and mirrors of the most braindead kind. One and a half days in I'd seen enough and I quit. If you know, you know. reply lmm 9 hours agorootparentThis kind of hinting is worse than useless. \"If you know, you know\" is obnoxious. If you mean Autonomy you should say so, and if you don't then your post is just misleading and confusing. reply prawn 8 hours agorootparentI assumed they were suggesting that they knew that it would be best to leave the company and didn't hang around. e.g., \"If you know [hanging around will be a waste of time], you know [enough that you should leave].\" reply paulryanrogers 11 hours agoparentprevWhat was the truth? Really just bad fits or something more sinister? reply hn_throwaway_99 4 hours agorootparentNot really something \"sinister\", but there was a level of chaos/shit-show that was orders of magnitude bigger than I had seen before, and I've worked for startups my whole career. reply blueboo 11 hours agoparentprevthe stupendous risk taken on by founders confounds instincts, and we're already filtering to folks who have resorted to taking money from third- or fourth-tier options reply TimJRobinson 11 hours agoprevI don't understand how they lost their startup though? Doesn't the accelerator only take a small percent? reply pnw 10 hours agoparentI guess the argument is that a $250k warrant sitting in bankruptcy court is a potential liability that would turn off other investors. It's not something I've personally seen, but it doesn't really seem insurmountable for a hot startup. Losing a cofounder is a much more common and potentially painful problem. The CEO's post on LinkedIn made it seem like they found it tough to get funding in general. reply atherton33 11 hours agoparentprev\"Startups also granted Newchip the right to buy $250,000 worth of shares in the company at a later date, but at their current valuation\" reply jojobas 10 hours agorootparentSo the company was essentially signed away even before the bankruptcy. reply atherton33 10 hours agorootparentYup. The bankruptcy just transferred the right to exploit it from the mildly incompetent to expertly ruthless. reply behringer 4 hours agorootparentI prefer the term scam artist over \"expertly ruthless\" reply morgante 4 hours agorootparentI don't think the final buyers are scam artists. They're mercenary, but not deceiving anyone. The scam artists are definitely the people preying on entrepreneurs' dreams to sell a worthless accelerator membership. reply irjustin 8 hours agorootparentprevYeah this is a pretty bad and far far far from founder friendly. reply grensley 9 hours agoprevAnybody else nodding along like... \"mhm mhm Austin...makes sense...\" \"I wonder when the Florida scams are gonna start hitting?\" reply replwoacause 8 hours agoparentI’m curious to know what you mean by this. Does Austin have a reputation for this kind of thing? reply sangnoir 7 hours agorootparentAustin and Miami were supposed to be the new tech start-up hotspots after the great COVID WFH migration. The zeitgeist was the SF Bay area was played out for a multitude of reasons the departees were only too happy to blog about. Austin and Miami also have a bunch of investors without a background in tech - in all, lot's of new players in a high-growth area make it a target-rich environment for those lacking scruples. reply grensley 8 hours agorootparentprevRich people start hyping these places Eager eyed entrepreneurs follow And the sharks are just waiting reply thayne 8 hours agoprevIt seems like if you make a contract and a stock warrant is part of your side, and the other side doesn't keep their side of the deal (for example because they went bankrupt), then that warrant should be void, because the contract was breached. reply SoftTalker 6 hours agoprev> The court has since ordered the company to auction off the warrants it held in more than 1,000 of the startups that went through the accelerator program. I probably don't understand something but how will this possibly benefit creditors? Who is going to pay anything for warrants in startups (most of which will fail, since that's what happens to most startups)? reply tschwimmer 5 hours agoparentI had the same question myself. The article notes that most of the first tranche of warrants went unpurchased which makes sense. However, the article also says that some of the portfolio went on to have an exit or raise later funding or something (the article mentions some company in Australia that seems to be a going concern, but they're claiming that the warrants are invalid). My take is that whoever is overseeing the portfolio has determined that the likely aggregate value of all the warrants > 0 so they are trying to sell them to recover something. In practice most of the warrants are indeed are worth 0 so it's actually not as a big of an issue as this is made out to be. reply SpicyLemonZest 5 hours agoparentprevThe article indicates that buyers of an initial tranche have included startups buying back their own warrants (as the headline one unsuccessfully attempted to find funds for) and VCs who make similar portfolio bets routinely. reply theogravity 4 hours agoprevIt sounds like Newchip had warrants on lots of companies - did all these companies pay $7500, or is this only for seed-stage startups that need to get access to an investor network? reply Paul-Craft 9 hours agoprevI might be having a brain fart right now, but I'd just about swear I've read a similar story before. Can anybody back me up, or am I just mistaken? reply datascienced 7 hours agoprevWould have been nice for the bankruptcy company (what is the term… administrator? receiver?) to try to sell the portfolio of investments as-is to another VC. This may have both kept the startups alive and got them more $ overall for creditors. Maybe this would have been more doable in 2020! reply ldjkfkdsjnv 11 hours agoprevA popular venture studio based out of NYC is like this. They take 60% of the equity from the start, and provide 1M in capital (which is decent amount). The narrative is that they provide significant guidance, follow on capital, etc. But in reality, none of their guidance or follow on capital comes through. For a first time founder its okay for a year, any longer and its really a financial disaster versus just working your way up the corporate ladder. People have no idea how few startups really cash out, and how hard it is when you start from low equity percentages/have bad terms. The horror stories usually dont arise until a startup is actually worth something and has a future. This is usually 2 years+ into the journey. I think the typical founders doing this are actually just people that want to say they own a company at dinner parties. reply CharlieDigital 11 hours agoparent> A popular venture studio based out of NYC is like this Sounds like Fractal. Supposedly, the value add is that they've already done the due diligence and market research on some product idea. They match a team (CEO + CTO) to the idea and provide the funding. Do not know of any well known companies to have come out of this model, but if they're still around, it must be generating some returns for it to be worthwhile. reply presidentender 11 hours agorootparentFractal seems to have addressed the entire market they had identified. It doesn't appear that they're still recruiting founders. I was in their pipeline and interviewing potential business cofounders, but chose to go the traditional venture route - it didn't work out, but I don't think I'd have succeeded at the Fractal business either. reply ldjkfkdsjnv 10 hours agorootparentThey stopped getting funded bc of a mix of interest rates and company underperformance reply paxys 11 hours agoparentprevAt that point they are just hiring an employee. Once you get the $1M what incentive is there to continue to hustle while being a minority shareholder in your own company? reply s1artibartfast 9 hours agorootparentYeah, what's the vc ROI here if they drop 1M and destroy each company. Do some of these go on to exit? reply bawolff 7 hours agoprevI mean, maybe you shouldn't sell of the right to buy your company. The accelerator sounds scummy, but at the same time i can't help but wonder wtf the owners of these companies were doing. Did they just not read the contract? If you own a company i think you have a lot of responsibility for the shitty business deals you make. Its not like we are talking about some senior citizen hoodwinked into signing their home away. reply datascienced 7 hours agoparentEvery contract has some crap in for what seems like unlikely scenarios. If you can’t negotiate it out it is either that or the highway. If these startups could get YC funding they probably would have. So for some it is accept a possible imperfect contract or back to employment. Employment itself being full of contracts with crap clauses as well as common law itself having crap. Show me the perfect contract! reply bawolff 1 hour agorootparent> If you can’t negotiate it out it is either that or the highway That is generally how contracts work. You get something and you give something in return. Nothing comes free. If the deal was better then \"the highway\", you have no cause to complain when the other side comes to collect their part. Especially for a sophisticated party like a company. Things are a little different for individuals like employees where the power imbalance is coercive. However when it comes to a company, as long as it wasn't outright fraud, i have very little sympathy that they are having buyers remorse over a bad deal. reply keepamovin 7 hours agoprevThe court has since ordered the company to auction off the warrants it held in more than 1,000 of the startups that went through the accelerator program. Why should the startups be punished? I think in this case the interests of startup ecosystem should out do those of creditors. Is there no protection for that?? Seems nond to gut startups when an accelerator failed, agains the entire purpose. Seems a great way to destroy economic value. Tho to be brutal a bag of startups is basically economic destruction anyway, on average as most of them fail...but I mean. In this case it's like precrime, they're killing them before they even have a chance. Not fair, not good! reply adapteva 9 hours agoprevYup, got an cold email from them. Marked it as spam and never heard from them again... reply replwoacause 8 hours agoprevThe guy in charge of this sounds like a real piece of work. reply Joel_Mckay 10 hours agoprevRule #14: \"Never outsource economic control structures, or one may end up indentured\" Sometimes one needs to admit they were conned, and start over... When people start out, no one tells them there is an ecosystem of legal-cons that target vulnerable small firms. Even this forum has users the constantly spam people with various funding scams. My condolences, some lessons can take a year or two to recover... =3 reply Terr_ 10 hours agoparentThat reminds me of a book quote, about a semaphore transmission-line company as a kind of fantasy-version analogy for modern telecoms. > \"[My father] was chairman of the original Grand Trunk Company. The clacks was his vision. Hell, he designed half of the mechanisms in the towers. And he got together with a group of other engineers, all serious men with slide rules, and they borrowed money and mortgaged their houses and built a local system and poured the money back in and started building the Trunk. There was a lot of money coming in; every city wanted to be in on it, everyone was going to be rich. [...] > Everything was going fine and suddenly he got this letter and there were meetings and they said he was lucky not to go to prison for, oh, I don't know, something complicated and legal. But the clacks was still making huge amounts. Can you understand that? Reacher Gilt and his gang acted friendly, oh yes, but they were buying up the mortgages and controlling banks and moving numbers around and they pulled the Grand Trunk out from under us like thieves. All they want to do is make money. They don't care about the Trunk. They'll run it into the ground and make more money by selling it.\" -- Going Postal by Terry Pratchett reply Joel_Mckay 8 hours agorootparentI wish the mistakes I've seen over the years were fictional, as even the people I found disagreeable still deserved better treatment. Any CEO worth anything owns their mistakes, adapts, and mitigates future issues. Good luck out there =3 reply jwsteigerwalt 7 hours agoprevWhen is the auction? How can you bid? reply CPLX 11 hours agoprevWhat a fucking mess. I got tons and tons of outreach from these guys for my company. It was pretty well written didn’t come off as overtly scammy unless you already know to run screaming from an accelerator or any other “investor” that wants you to give them money up front. reply emodendroket 11 hours agoparentIf you don't know that what are you doing trying to run a business? reply jltsiren 10 hours agorootparentRunning a business is one of the most common things people do. The barrier of entry is lower than for most jobs. There are millions of entrepreneurs who are well below the average person in their ability to recognize and avoid scams, and scammers are well aware of that. And it doesn't help that businesses often don't have the same legal protections as employees and private individuals. reply CPLX 9 hours agorootparentprevDo you know if lawyers should be paid up front? How about a newly hired employee, do they get a deposit to ensure they will show up on the first day? How about a landlord for your first office? If you make things and sell them to a store does the store pay up front? Can they return them to you if they don’t sell them? Can you get a refund on a hotel if you don’t stay there? How about a flight you don’t take, or a rental car you never pick up? And so on. The world is complicated. How did you learn all that stuff and when? reply bitwize 8 hours agoprev> Newchip founder and CEO Andrew Ryan Name checks out. reply binbag 12 hours agoprevTerrifying. reply _cs2017_ 12 hours agoprev [–] This article is absolute trash since it doesn't explain how the bankruptcy of the accelerator would change the amount of dilution the startups experience. Taking the information in the article at face value, the startups paid the accelerator (partially) with warrants. Those warrants have a fixed exercise price; the courts cannot change that. Whether those warrants are exercised by the accelerator or by the creditors, the dilution will be exactly the same. This is not at all affected by whether creditors bought the warrants for penny on the dollar or for a billion dollars. Maybe there's some reason why warrant owner matters. But the article makes no attempt to state or even hint at such a reason. I wish there was a way to blacklist domains from showing up on my Hacker News feed so I don't have to keep reading this type of junk journalism. reply hn_throwaway_99 11 hours agoparent [–] Why the diatribe? > Maybe there's some reason why warrant owner matters. It's a well understood fact by anyone in the startup world that it does matter, because future investors or acquirers care deeply about the structure of your cap table. Furthermore, the article gives an explicit example of this: > She had lined up a grant from a bank to help fund her offer, but it ultimately told her no because it was too risky for them to be involved with an unknown warrant holder on her cap table. reply _cs2017_ 8 hours agorootparentSo your point is that a good investor should agree to a buyout offer but the bad investor doesn't? In other words, the original warrant holders (the accelerator) would have happily agreed to the founders buyout offer funded by the bank's grant? Why would they do that, if the whole point of an accelerator is to accumulate shares in the startups? reply sroussey 11 hours agorootparentprevThat paragraph makes no sense. A bank is offering her a grant? And this grant was supposed to remove the warrant holder from the cap table, so why would they have a problem with that? And having some unknown warrant holder is the reason to shut down? I call BS. reply rvba 10 hours agorootparentWhat is the business model of an \"AI smart-matching tool for humanitarian aid\"? Spunds like a word salad, or a weekend project for a developer that could be hosted for few bucks per month. reply bbarnett 11 hours agorootparentprev [–] Read your comment and parent. The quote from the article doesn't explain why the warrant owner matters, and you suggest the cap table structure does, which makes sense. However, the structure has nothing to do with ownership of parts of that structure. Why would warrant ownership matter? reply llamaimperative 11 hours agorootparent [–] Because as an investor in a company, you don’t want that company’s founders constantly distracted by a piece of shit investor who has a massive stake in their company. Pretty straightforward. Investors can create tons of havoc, and “bought equity from a bargain bin outside of a bonfire” is probably as good a warning sign as any. reply bbarnett 8 hours agorootparent [–] The size of the warrant is still the structure. In terms of investment, as the warrant is demonstratedly transferable, the investor ownership is always an unknown. This still sounds like structure(size and control of a single warrant) rather than who. The real issue seems to be any large warrant, that is, the uncertainty in the structure. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Newchip, a startup accelerator, declared bankruptcy, leading to the auctioning of warrants from over 1,000 startups, causing founders like Lacey Hunter of TechAid to lose their companies.",
      "CEO Andrew Ryan received criticism for his management style, and missed opportunities in managing warrants, valued at potentially $500 million, worsened the company's financial situation.",
      "Deal Syndicate acquired a significant number of warrants, with more expected to be sold shortly, highlighting the risks associated with accelerator programs through the experiences of founders like Garrett Temple."
    ],
    "commentSummary": [
      "Entrepreneurs in the startup industry encounter challenges like accelerators acquiring companies for minimal sums and concerns about the value offered by accelerators.",
      "Debate arises regarding the involvement of venture capitalists in funding startups, discussing the risks and advantages of VC funding and the struggles of bootstrapped companies.",
      "Noteworthy figures like Naval Ravikant are influential, warrant ownership is crucial in startup investments, emphasizing potential consequences for both founders and investors."
    ],
    "points": 191,
    "commentCount": 119,
    "retryCount": 0,
    "time": 1714685082
  }
]
