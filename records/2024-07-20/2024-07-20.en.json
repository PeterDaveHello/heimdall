[
  {
    "id": 41012443,
    "title": "I mapped almost every USA traffic death in the 21st century",
    "originLink": "https://roadway.report",
    "originBody": "Roadway Report: The Nationwide Vision-Zero Map",
    "commentLink": "https://news.ycombinator.com/item?id=41012443",
    "commentBody": "I mapped almost every USA traffic death in the 21st century (roadway.report)681 points by Bencarneiro 19 hours agohidepastfavorite383 comments blincoln 1 hour agoMaybe I missed an alternative mode that's already present, but this feels like it would benefit enormously from a heatmap view, so that one could see where fatality hotspots are at a higher level before zooming in to examine specific locations. reply bigtoe416 18 hours agoprevI used to be a volunteer firefighter and I see some of the fatalities (but not all) on this map. Looking at one of them some of the information is quite accurate (type of accident, what caused it, age of person) while other information is not at all accurate (number of people in the car, if a seatbelt was in use). It's curious how some fairly important pieces of data can be quite wrong. reply kevin_thibedeau 4 hours agoparent> It's curious how some fairly important pieces of data can be quite wrong. I was nearly killed by a driver who disregarded the law and the police officer intentionally disabled his bodycam when interviewing a key witness, never questioned me, let the driver off with no tickets for his violations, and submitted a report with factual inaccuracies. Crooked cops don't have an interest in reporting the truth. reply matsemann 1 hour agorootparentI get so angry when I ready the reports, and they mostly talk about the victim. \"The pedestrian was wearing dark clothing\", \"the bicyclist wasn't wearing a helmet\" (like that would've avoided the accident?), etc. Of course, the victim is most likely being transported away in an ambulance or even dead, so they got little say. And the offender can paint the story with their picture: \"the sun was low and blinding\", \"the cyclist came in at high speed\" (probably a third of the car..), etc. So that's what the report will say. reply nativeit 1 hour agorootparentI’m a little confused at the assertion here. What is happening that shouldn’t be happening, and what should be changed? Should they presume malice, and discount any reference to circumstance? What if the circumstances were actually causal (which one would hope so, otherwise there are far more murderous sociopaths than anyone would have guessed) how should we parse the difference, and how will we identify potential hazards? Presumably, from the cyclist’s perspective, the story will almost always go like this: “I was riding my bicycle in the correct fashion, in the appropriate lane, equipped with proper safety equipment, and then BAM! They came out of nowhere,” because if they had an opportunity to see the oncoming vehicle, judge whether it’s being operated safely, see where the driver’s attention is focused, and evaluate whether the driver’s awareness has or has not been compromised by the prevailing conditions—one would hope they could have avoided the crash in that space of time. reply lostdog 1 hour agorootparentI've been in a few close calls, and in every one the driver was not paying full attention to the road and (nearly) swerved into someone riding a bicycle. But reporters use such sympathetic language describing the negligent driver. reply cscurmudgeon 37 minutes agorootparentAs a pedestrian in a large city, I have been nearly run over by cyclists a few times, but I don't generalize from that though. reply tenacious_tuna 1 hour agorootparentprevIt's part of a wider trend where drivers are given the benefit of the doubt at a societal level: https://www.npr.org/sections/publiceditor/2023/09/07/1198102... reply Bencarneiro 15 hours agoparentprevThe data here is processed from NHTSA's FARS database. When someone dies in an accident, it gets input into a STATE reporting system, and FARS is manufactured by analyzing each state's individual record system. The feds consolidate all this data and publish a unified dataset annually. They say it's \"a lot cheaper and just as good as collecting it themselves\" Additional errors are potentially produced from my own processing of the federal data, but those will be rooted out over time. Project being OS will hopefully help with that. reply toomuchtodo 1 hour agorootparentIf you need help merging additional data sources, this project might be able to assist. I know the folks, and they are motivated to deliver open data. https://www.data-liberation-project.org/ reply topsycatt 4 hours agorootparentprevI noticed seemingly all deaths in Manhattan are labeled as having occurred in flushing, a nearby neighborhood. Perhaps an off by one error? reply mrtimo 14 hours agorootparentprevThanks for this background. What's the best way to download your data, or the feds data? reply mikeyouse 14 hours agorootparentJust go to the NHTSA FARS website.. here's the FTP: https://www.nhtsa.gov/file-downloads?p=nhtsa/downloads/FARS/ reply aeuropean12 2 hours agorootparentThank you, that is very useful. In fact, the author of the website \"I mapped almost every..\" might want to add references to the data sources for credibility and less of a \"spam\" look. How about an About page. The splash photos on that website says it all. reply msla 12 hours agorootparentprevnext [6 more] [flagged] jahsome 11 hours agorootparentYup. The same way \"we're\" being snarky for harmless absent-minded mistakes for no good reason. reply chipdart 5 hours agorootparentprevOP made a great contribution to the discussion. Going off on a cynical tangent over protocols is just noise, and adds nothing of value to the discussion. reply 42lux 10 hours agorootparentprevI mean theoretically everything is a file transfer protocol... reply kmacdough 8 hours agorootparentNice try, but UDP is very much not a file transfer protocol. reply bluGill 5 hours agorootparentshort files only, but it is file transfer. of course nothing stops you from doing like tftp and splitting files to reassemble. reply TomK32 13 hours agoparentprevThere's an Austria paper that looked into the accident cause \"speeding\" (nicht angepasste Geschwindigkeit) means across the german-speaking countries. The results boiled down to \"it depends\" on the officer typing in the data as there aren't any reviews or such. There's good data on other accident aspects like the seat-belt-usage you mentioned (and it's shocking how many people die because they are too lazy to use their seat-belt) but those aren't down to subjective judgement on the spot. Good data is needed as a few accident causes do tend to be common in certain road/location conditions and those can be fixed. For example while the total number of accidents on train crossing is low (15~20 fatalities per year) in Austria, all of them are the driver's fault and almost all (except for massive idiot drivers) can be fixed by installing automatic gates on all crossings. reply zxexz 11 hours agorootparentInstalling automatic gates at crossings can fix (almost) 15~20 fatalities a year there? I've seen at least double-digit traffic deaths in my life, 4 of which I've watched happen and can vividly recollect, here in the USA. Installing infrastructure to prevent deaths seems like a no-brainer if you live in a country that supposedly cares. It's apparently not a no-brainer here -- two of the lethal accidents I've witnessed [one involving ripping the door off a car with the help of some kind stangers, to get someone out of a literally flaming wreck] would have been entirely avoided by a simple traffic circle. The most grizzly one I remember could likely (it seems to me; I'm not a traffic...engineer?) have been avoided by not having a low-traffic on-ramp connect directly to a major highway, when there was a clearly-denoted on-ramp a quarter-mile away. Seeing another human with their head 20ft away leaves a bit of an impression on a child. reply ifwinterco 9 hours agorootparentI always find it funny when people say \"you can't put a price on a human life\" because this is exactly what traffic engineers do on a daily basis. I don't know what the exact figure is but there's a number where below that improvements won't be made. It sounds bad but at the end of the day resources aren't unlimited - $1 spent on road safety improvements is $1 that can't go healthcare, law enforcement, schools, military etc. At some point spending millions of dollars to probably save one life isn't worth it reply alright2565 6 hours agorootparentThis is true, but the problem is that there is so much low-hanging fruit here like painting new lines, adding cheap concrete barriers, or installing elevated crosswalks. The price is far far less than whatever the price of a human life is. The reason they are not implemented is not cost, but because people here consider it their god-given right to drive as quickly and aggressively as they want. reply c0balt 5 hours agorootparent> The reason they are not implemented is not cost, but because people here consider it their god-given right to drive as quickly and aggressively as they want. As an amendment to this: People in many western countries tend to do this. Writing from Germany with, e.g., speed limits on some high ways being a broken promise from the last election. reply 0xcde4c3db 1 hour agorootparentprevYep. A major arterial in my city is very obviously too wide for the traffic it carries, even during rush hour. I don't remember the exact number, but a study a few years ago found that the average speed was something like 12 MPH above the posted limit. People completely lost their shit when it was proposed to narrow it and put in bike lanes (the bike lanes weren't the point, but people were cycling on the sidewalk to avoid the impatient/distracted/aggressive drivers, and it would have been silly to not use the space for anything at all). reply sethev 38 minutes agorootparentprevIt doesn't seem inconsistent to say \"you can't put a price on a human life\" and also believe that it's possible to calculate the economic impact of a human death. For example, saying that a particular individual's life is worthless is very different from saying they have no dependents. reply anon325652623 6 hours agorootparentprevhttps://en.wikipedia.org/wiki/Value_of_life#United_States The value of life in the United States The following estimates have been applied to the value of life. The estimates are either for one year of additional life or for the statistical value of a single life. - $50,000 per year of quality life (the \"dialysis standard\",[38] which had been a de facto international standard most private and government-run health insurance plans worldwide use to determine whether to cover a new medical procedure)[39] - $129,000 per year of quality life (an update to the \"dialysis standard\")[40][39] - $7.5 million (Federal Emergency Management Agency, Jul. 2020)[5] - $9.1 million (Environmental Protection Agency, 2010)[41] - $9.2 million (Department of Transportation, 2014)[42] - $9.6 million (Department of Transportation, Aug. 2016)[43] - $12.5 million (Department of Transportation, 2022)[44] reply dahart 5 hours agorootparentIt’s worth pointing out that these numbers don’t exactly represent either of the things that the parent comment talked about. These are the statistical economic effect of people dying on average, but this is not meant to be taken as putting a number on all the value of human life. Note the DOT doesn’t call it the “value of life”, they call it the “value of a statistical life (VSL)” in an attempt to help distinguish between those two different ideas. “This conventional terminology has often provoked misunderstanding on the part of both the public and decision-makers. What is involved is not the valuation of life as such, but the valuation of reductions in risks.” https://www.transportation.gov/sites/dot.gov/files/2021-03/D... Additionally, these numbers do not represent the threshold for whether a given proposal for roads is undertaken. They are used to inform the process, along with other relevant factors. That ‘Guidance’ like just above is interesting reading, they take time to point out that neither the economic data nor the risk data is perfect. (Perhaps that was obvious, but it’s good to know they recognize that fact officially in their analyses.) The VSL for 2023 is 13.2 million, and one might assume based on the recent trend that it’s probably around ~$14M for this year. It’s good for our personal safety the higher their VSL estimate goes, but as parent noted, bad for our taxpayer pocketbooks, so we try to balance those forces. I know government processes can look bureaucratic and strange from the outside, and seem like a big machine we don’t control, but ultimately we do decide as a society how much we’re willing to pay to keep ourselves safe; public sentiment and tax/anti-tax pressures do have a massive influence in what gets done. https://www.transportation.gov/office-policy/transportation-... reply lupusreal 10 hours agorootparentprevHow many crossings without automatic gates are there, vs how many fatalities at those crossings, and how many people cross at those crossings at all? When you remember that other important things to spend money on also exist, the math probably works out for leaving many of the rarely used crossings as they are. In America, there are a few hundred thousand crossings and only a few hundred deaths. Most of those deaths are concentrated at a relatively small number of crossings, while most of the crossings have very infrequent traffic across them. It's the same kind of logic that has most train tracks not put behind fences. In populated areas where lots of people roam around, putting a fence up next to the track helps keep people off the track. But in most of the country, the population is too sparse and people being on the track too infrequent for anybody to rationally prioritize putting fence up alongside all the track. Half a billion dollars worth of fence to save maybe a few dozen lives just isn't going to fly when there are schools to fund, old lead water mains to replace, bridges to repair, NASA probes to Uranus, etc etc. reply Aeolun 1 hour agorootparentprev> it's shocking how many people die because they are too lazy to use their seat-belt It’s hard to explain to people they can only ever be in a single accident in which they’re not wearing their seatbelt. Mostly they proclaim it’s fine, because they’re never in any accidents, and they drive safe anyway. reply throwaway2037 10 hours agorootparentprevDo you still have railroad crossings without automated gates? If yes, why? Cultural resistance? reply the_mitsuhiko 9 hours agorootparentIn Austria a big reason are old rural rail lines. It’s not trivial to install automated gates without having to remove some stops or keeping the gates down for very extended periods of time. They are also not entirely safe because people are idiots and get stuck without understanding that they can actually push through the barrier. reply Symbiote 8 hours agorootparentBritain has at least four types of crossing. Ones with nothing except signs to stop, look and listen. You're most likely to find this while hiking, as a footpath crossing. Flashing lights and beeps. Half barriers, which only cover half the road (one lane in each direction). These are my automatic. Drivers can't get trapped. Full barriers. These all have CCTV, and the train won't get a \"green\" until the signaller has seen both barriers close, and that no one is in the middle. These are used in cities and other busy places. reply timthorn 36 minutes agorootparentOne other fun type are the user operated gates. Normal farm gates on both sides of the railway, with a red/green aspect light telling you if it is safe to open the gate. Once you've driven through you have to stop and then close the gates. Last time I used one of them, by the time I walked back to close the first gate the signal had gone red so I waited. We also still have some full barrier crossings operated by a signalman near me. There's something pleasing about that. reply nordsieck 5 hours agorootparentprev> They are also not entirely safe because people are idiots and get stuck without understanding that they can actually push through the barrier. Most of the barriers I see in the US are one sided - they don't let you drive on to the tracks, but there's no barrier that prevents you from leaving. reply olex 10 hours agorootparentprevGermany has a lot of them on mostly remote, rarely serviced lines that see only a small number of regional trains per day. Main reason is cost to upgrade weighed against the (relatively) low risk of incidents because of low amount of traffic on both the train route and the crossing road. reply codedokode 9 hours agorootparentprevWhy do you need gates? No sane person will try to cross on the red light. reply toast0 4 hours agorootparentI've seen people cross on red lights. Sometimes it's the impatient person who has to get through as it's closing. Sometimes it's the impatient person who is tired of waiting as the signal has been going for minutes and there's no other evidence of a train. Sometimes it's the inattentive person following the car in front. Gates help the third person most. I haven't seen many crossings without gates, but they're a lot easier to miss than a gate. Protecting people from a train collision has benefits for the train system as well. A collision causes confusion and delay and may damage the rolling stock or even the rails and could cause injuries and the operators likely need PTO. So it's rational to reduce collisions regardless of opinions about the choices of the participants. reply TheCycoONE 8 hours agorootparentprevI live on the same block as a urban street level crossings that just have lights. People routinely cut through the light. That line just goes to the factories so the trains are going pretty slow, or sometimes stop on the signal. There is a gated crossing a little further down the same street for a thoroughfare and cars routinely race the gate to avoid waiting for the train there too. One could argue all those people are insane, but there are a lot of them. reply pasc1878 9 hours agorootparentprevThere are a lot of insane drivers. reply Detrytus 5 hours agorootparentprevThis is an educational video from Poland ;-) https://www.youtube.com/watch?v=HeXzUGfNLWo reply wmanley 2 hours agorootparentWarning: only click that link if you want to see a cyclist get hit by a train reply surfingdino 7 hours agorootparentprevTikTokers will. reply greenavocado 6 hours agorootparentWhat is the problem then? reply lupusreal 10 hours agorootparentprevThere's a thing that happens where people get \"\" trapped\"\" by automatic crossing gates. They get their car on the tracks as the gate closes in front and behind them. The gates are very weak so they could drive right through, but some sort of mental block often prevents this and so they stay there with their car on the tracks, sometimes not even thinking to get out of the car. I guess it goes to show that stupid finds a way. reply yowzadave 5 hours agorootparentI wonder why the gates aren’t designed so that they only present a barrier on the entry side of the track, and not the exit side of the track? reply bluGill 5 hours agorootparentThey sometimes are but then it is easy to go around them reply weebull 4 hours agorootparentprev...because people then drive onto the wrong side of the road to drive around the barriers. Yes, some people are that dumb. reply efrecon 10 hours agorootparentprevI would believe this is mostly due to panic rather than being stupid. At some point, for many people, your logical brain stops working. reply lupusreal 10 hours agorootparentI think it's a lack of panic that does it. In that moment they're afraid of causing property damage and they aren't thinking about their own imminent demise. Probably because they see the gate before they see the train, and once they start thinking about the gate in front of them they get tunnel vision and struggle to switch focus to the more important thing coming at them from the side. reply dkga 7 hours agorootparentprevOr maybe a lot of this is intentional, sadly? reply dclowd9901 10 hours agoparentprevI was looking at an incident in my neighborhood. In our city, we have frequent incidents of elderly drivers killing pedestrians in motor vehicles. The incident in our neighborhood did not show a driver age but did show the pedestrian age. I cannot imagine why the driver age would not have been recorded in an incident like that (but their height and weight were). reply rad_gruchalski 9 hours agorootparentIn your neighbourhood you probably have frequent incidents of elderly drivers of motor vehicles killing pedestrians. reply gymbeaux 2 hours agoparentprevI used to scrape the calls for service for my local PD, and the inaccuracies were considerable. Of course there were typos in addresses or cross-streets, but there were also inaccuracies with how the incident was classified (for example, a former coworker and friend said his neighbor called the cops on him in the middle of the night one day- I looked it up and it was classified as a domestic disturbance rather than a noise complaint). Some records, like those involving child abuse, don’t show on their calls for service website at all, so that’s an entire group of data that we the public just don’t get to know about. Government data is notorious for being dirty and inaccurate. reply throwaway2037 10 hours agoparentprevCan I a real question? How do personally deal with the trauma of responding to an accident with death(s)? I don't have the guts to do a job like that. reply jIyajbe 5 hours agorootparentI was an EMT in the Los Angeles area for four years, and then in the Seattle area for four years. I developed the mindset that even if the patient died, we (the firefighters/EMTs/paramedics) had at least given them a chance to live that they would not have had without us. That helped soften the blow of the deaths. That strategy was less effective for calls where the patient was declared dead on scene, and so no treatment was possible. I still remember each of those calls. Fortunately, I was never traumatized by them, in a PTSD sense. Rather, I learned the lesson of realizing how easily I, or one of my loved ones, can simply disappear one day; so I learned to make sure they all know that I love them, and to not take their presence in my life for granted. reply JackMorgan 7 hours agorootparentprevNot the original commenter, but as a volunteer firefighter and EMT it is something everyone has to figure out for themselves. There's tools like therapy, CISM, CBT, humor, and the support of your buddies. Too often there's drink, pills, and suicide. It's not a particularly widespread detail, but suicide is the largest killer of firefighters by a large margin. I think that is a symptom of putting too much work and pressure on too few people. I have found CBT, exercise, fasting, and daily yoga help keep stress and depression at bay for me. I also am lucky to be a volunteer, so I can afford to skip as many calls as I want after an upsetting call. The career folks I know have it much much worse, because they might be on a horrible call but if they want to get paid they have to keep working. They might have an entire 24hr shift with several troubling calls back to back. I used to think all fire and ambulance should be paid career. However, there's an important side effect of a larger group of volunteers sharing the burden. If you see too much to handle, you can take off all the time you need to recover. You aren't trapped by the job into witnessing more than you can handle. My wife recently was on an ambulance call that troubled her. She has taken a few weeks off from ambulance to decompress. That being said, volunteerism is seriously struggling in America. If you live in the 70% of America that is served by volunteers, it is likely your local is extremely understaffed. Too many companies only have a small handful of firefighters who still show up, and those heroes aren't \"taking time off\" to recover, because if they do, the engine doesn't move. Too much of the country is overly reliant on volunteers but isn't showing up to be a volunteer to share the load. It's a socialist program, but there aren't enough people showing up to share the work, so it's collapsing. And just paying folks to do this full time comes with very real increased tax burden and acute trauma on those individuals. Paying career staff isn't cheap. If you live in an area that is served by volunteers, please seriously consider volunteering yourself. A lot of people on this site are young and physically able to volunteer. Tech folks are often well situated to volunteer, as we are more likely to be financially stable, have flexibility to dip for a call if it's urgent, often work remote, and sometimes would really benefit from getting out in the sunshine and meeting folks. Also it's so rewarding to do something so real after a big day of mental labor. In the volunteer service you don't ever have to do something if you don't feel comfortable with it. If you show up to a call and you don't want to go inside the building you don't have to. Plenty of work is needed outside of a structure fire. If the call comes and you're too tired or busy, no big deal. I only make ~20% of my local calls, and that is high for my company. I've said \"no way\" to my officer before, and he reconsidered or found someone else. You also don't have to be in \"perfect shape\". I thought they were going to be making me do crazy fitness tests to join like in the movies. Then I realized, we are so desperate for help we'll take anyone if your doctor will sign a form saying you're fit for duty. We've got members who are 200lb overweight, members who weigh 100lb, need glasses, some are 14, and some members who are over 70. Some can't wear an air pack so they do fire ground support work outside. Kids under 18 and folks who haven't yet gone to fire school aren't legally allowed inside a structure fire anyway. If you want to show up and put on gear, we've got work for you. There's so much work, it's so hard, and so few show up that anyone who does is a help. It can be extremely rewarding to be a first responder in a small town. We only have 4k residents in my local, and I like getting to meet them and help them. After just a few years I can't go to the grocery store without waving to half a dozen people. My calendar is filling up with invites to backyard BBQs, music jam sessions, a pie baking competition club, DnD games, a computer club, and a lifting club, all discovered through just meeting cool local folks. Between volunteering and regularly visiting our local busy coffeeshop at peak \"sit around and chat\" times, I'm feeling connected to a community in a way I thought only happens in movies and TV. Definitely reach out to me if you ever want to talk about it. I'll happily answer any questions you have. I will say I'm kicking myself for not joining sooner. For eight years I was next to an amazing firehouse and never knew! reply Aeolun 1 hour agorootparent> suicide is the largest killer of firefighters by a large margin While it seems obvious, is this not similar to the ratio for all people in the age group that are firefighters (roughly 20-50 ish)? Most people 40 years of age do not suddenly drop dead, so they’re much more likely to die by suicide. reply FireBeyond 1 hour agorootparentprev> There's tools like therapy, CISM, CBT, humor, and the support of your buddies. EMT, Paramedic, trainer of both.... my understanding is that CISM has been largely discredited. Maybe its a misapplication, a \"lock you all in a room until everyone has talked/cried about it\", but still. Thankfully, in the PNW, many many departments have contracts with therapists. Some near me even have mental health professionals as full time employees. The other, perhaps bigger reality for me is this: It's not the trauma calls that get to you - in the end we're all just blood and tissue. It's the emotional calls. The CSA calls. SIDS. Elder neglect. reply chao- 10 hours agoparentprevI had a similar, awful curiosity. Looked up the death of a friend. All of the details I ever learned (speeding, ran off road, jumped curb, hit a tree) are listed accurately here. The claim that only 1 out of 5 deaths is even recorded on this map is sobering. reply tonetegeatinst 16 hours agoparentprevAny idea what spicifly might be the possible causes for data errors? Like is every state using the exact same form? Are are people who filled them out trained how to do so correctly? reply markstos 7 hours agorootparentIt's not just states, it's every law enforcement jurisdiction, which then gives their data to the state, which then standardizes it. In turn, it appears the states aggregate it at the federal level. In my mid-size city, jurisdictions that operate here include the city police, county sheriff, university police, state police and Department of Natural Resources law enforcement. At least in the Indiana, the quality of this data... varies widely. Coordinates don't always match reality, the street names can't always be geocoded, sometimes the timestamps don't even parse as valid dates. I talked to a city staffer here whose job largely involves cleaning up this data. But not in a permanent sense. They are using the aggregate data handed down from the state, and they are cleaning a local copy. Then next year or next month, they get more data and have to re-merge or re-do their fixes because their fixes aren't upstream, and have originated from another jurisdiction. The whole system needs improvement! reply ekianjo 17 hours agoparentprevdata sources are notoriously bad everywhere human entry is the processs reply gamepsys 17 hours agorootparentWhen I was in highschool I had to fill out a survey about my experiences with substances. I remember getting a laugh out of lying on the survey. Always take any self reported questionnaire results with a large grain of salt. reply slongfield 13 hours agorootparentPew Research found that 24% of Hispanic adults are licensed to operate a SSGN submarine: https://www.pewresearch.org/short-reads/2024/03/05/online-op... The margins of error on online opt-in polls are massive. reply Zamiel_Snawley 17 hours agorootparentprevI too was a “daily cocaine user” in 6th grade… How many Americans think chocolate milk comes from brown cows? I’d wager almost every single person who answers in the affirmative really just thinks it’s the funnier answer. reply 77pt77 16 hours agorootparentYou might be surprised... reply imoverclocked 14 hours agorootparentYeah, according to certain studies, most of my 6th grade class has stinky cheese farts and did moderate to heavy cocaine use in the morning. reply BeFlatXIII 6 hours agorootparentDo cocaine as the morning pick-me-up and then some heroin in the evening to help you sleep. reply mulmen 14 hours agorootparentprevI'm convinced this is the source of flat earth conspiracies. reply anakaine 13 hours agorootparentI suspect there are a vanishingly small number of individuals who believe flat earth, and the rest are just doing it for the lulz. reply TeMPOraL 10 hours agorootparentYou're missing an important third group: people making money pushing these beliefs. Themselves, they likely don't believe it - but they know this type of \"content\" creates strong \"engagement\". reply Hnrobert42 8 hours agorootparentprevBehind the Curve is a good documentary about flat earthers. A lot of them seem to believe it because it gives them a sense that they have figured something out that others haven't. It makes them feel special. Others believe it, or at least continue to believe, because they find a community and connection with other believers. reply sgerenser 6 hours agorootparentSounds remarkably similar to mainstream religions in the U.S. reply galangalalgol 12 hours agorootparentprevBut the earth is visibly not flat! Clearly we live in a Dyson sphere. reply bornfreddy 12 hours agorootparentOf course it is flat! It's just the coordinate system that is curved. ;) reply lukan 11 hours agorootparentprevThat is what you hope, but do you know how many people believed in the Q conspiracy? Chemtrails? Reptiloids? Bill Gates using the vaccination to control everyone through a microchip? It is the same ballpark. I wish those people pushing this, were doing it just for the lulz. But mostly they are serious. reply 1659447091 10 hours agorootparentI'm convinced that chemtrails came from a stoner watching some variation of How planes fly / aeronautics at 3am and heard chemtrails in place of contrails and we are now forever stuck with it. reply ben_w 10 hours agorootparentprevWhat I want to know is why Bill Gates got stuck with the microchip conspiracy theory when Elon Musk regularly has press conferences about the progress his team is making with actual brain microchips. reply lukan 10 hours agorootparentThat is simple, because Bill Gates did talk about a microchip for people in an interview, but just an RFID chip, to keep track of the vaccinations. Also his foundation is doing vaccinations big scale. Even without foreign hostile agencies making disinformation, it is easy seeing crackpots mixing it all up. reply ben_w 5 hours agorootparentAh, that makes a decent amount of sense. reply dolmen 8 hours agorootparentprevAnd this is just the start. LLMs are very good at mixing information to produce bullshit. reply lukan 6 hours agorootparentNo worries, we can ask another LLM for verification. reply agent327 10 hours agorootparentprevMusk makes it optional, Bill Gates required it (vaccination, presumably including the chip) of everyone. reply sseagull 15 hours agorootparentprevYou were just contributing to the Lizardman’s Constant :) https://en.m.wikipedia.org/w/index.php?title=Slate_Star_Code... reply Mistletoe 16 hours agorootparentprevThis is why nutrition research is extremely confusing and contradictory often. I worked in a nutrition department and the amount of post menopausal obese women in our knee studies that said they just ate a half cup of green beans that day is astounding. Now try to use data that flawed to make assumptions over a lifespan about human health. About the only studies I truly believe are the ones where people are at a facility and all food is provided to them and tracked. reply cperciva 14 hours agorootparentMy favourite example of this is that the number of condoms used in the USA, according to surveys, is dramatically higher than the number of condoms sold in the USA. reply mildzebrataste 12 hours agorootparentPerhaps they are using them more than once. reply lukan 11 hours agorootparentOr they are confusing imagination of how much they would like to have sex with reality. Or they feel the social pressure, to boast themself. Sexual activity is kind of a primitive success metric. Reporting low numbers means reporting low success .. reply mr_toad 5 hours agorootparentprevIn surveys the amount of liquor consumed is often about half the amount of liquor that is sold. reply prerok 2 hours agorootparentYeah, that's because it's assumed that the other half consumed is non-registered moonshine. I don't get how that would apply to condoms, though. reply cratermoon 12 hours agorootparentprevEw. reply ianmcgowan 16 hours agorootparentprevI lived in one of those food study centers after college. Most people there were pretty diligent about sticking to the program, but there was a big scandal when one guy was kicked out after discovering extensive cheating and several papers had to be retracted. We were allowed out - I had a full time job, but couldn't eat or drink any food not provided by the center (we took a radio-isotope tracer with food, and had to collect our poop). It was quite interesting :_) reply umbra07 13 hours agorootparentwhat was the consequence if you did eat outside food? reply nradov 10 hours agorootparentIf the researchers found out then you would be eliminated from the study and no longer paid (and given free food) as a research subject. reply Falkon1313 13 hours agoparentprevYeah, I checked one near me and it said EMS arrived 1 minute after being notified and victim was transported via EMS air. Time to arrive at hospital seems reasonable for an air transport from that spot though. So I guess the helicopter just happened to be idly hovering over the van at the time it went off the embankment? reply bluGill 5 hours agorootparentEven if the helicopter is overhead for a planned drill you can't land it in a minute. 30 years ago if my first cpr class the teacher said it was typically 45 minutes from when you call - which is why they (in a big city where a level 1 trauma hospitial is at most a 30 minute drive away) almost never call for air help. reply nradov 10 hours agorootparentprevGround EMS could have arrived in 1 minute if they were already nearby. Just because the patient was transported by air doesn't necessarily mean that the helicopter unit was first to arrive. reply davidw 18 hours agoprevLooks like where I live, deaths are more closely associated with big, wide, fast roads. It's ironic that drivers get frustrated by smaller, narrower streets as not feeling very safe, but that uncomfortable feeling 1) slows people down and 2) keeps them on their toes in terms of looking out for hazards rather than feeling ok with driving fast and not paying as much attention. reply screye 7 hours agoparentTraffic calming measures like lane narrowing have successfully been used in Europe for safety. Drivers don't follow traffic rules. They follow the rules that appear to make sense for the scenario they're in. No matter how many 15 mph speedlight signs you put up on a wide street, the driver will subconsciously speed through a wide street because the brain is stupid like that. Want drivers to stop before the footpath, raise it up. It'll make them feel like they're off-the-road and they'll slow down. Everything about cars is associated with perceptions. Families buy heavy cars for the perception of safety, but instead have a vehicle that is harder to turn and is more likely to end up in a ditch. They buy taller vehicles to feel safe but have instead purchased a massive blindspot notorious for trampling over your own children. Unprotected bike lanes look like shoulder lanes you can serve into, and that's how drivers treat them. You're absolutely correct about the feeling of discomfort when driving through narrow lanes. The biggest lie we tell is that cars are safe. No, you're wielding the most lethal murder weapon in the country, with almost zero training and 1 mistake is all it takes to get a prison sentence. They should feel uncomfortable. reply andrepd 3 hours agorootparentOutstanding comment, you're absolutely correct about everything. Except this: > 1 mistake is all it takes to get a prison sentence On the contrary, on top of everything you've pointed out, if you do kill someone (or multiple someones) at the wheel of a car you're unlikely to get more than a few months in prison AT MOST. reply pavel_lishin 2 hours agorootparentStatistically speaking, if you want to murder someone, a car is the best possible murder weapon. reply screye 2 hours agorootparentprevunfortunate, but true. At this point, having a child that's escaped scott-free from a hit-n-run is part of billionaire-bingo. reply Davidzheng 18 hours agoparentprevAre you weighing the deaths by use of the road. Otherwise it's not representative of danger level right reply Swizec 18 hours agorootparentGP is describing traffic calming road design. Where planners make roads purposefully feel less safe in certain neighborhoods because that statistically makes them safer per mile driven. A common example here in SF is to add unnecessary bends to an otherwise straight alley. This stops people from speeding right through a residential area because it's straight and empty. An example you may have seen in more rural areas is a straight road with an unnecessary curve before a stop sign or before entering a town. This forces you to slow down in a way that a speed limit doesn't. https://globaldesigningcities.org/publication/global-street-... reply davidw 17 hours agorootparentIt's also separating things out. We need larger, faster roads to get from city to city - that's kind of inevitable. But the way you design those is to completely separate out bikes and pedestrians from them, and also limit access to them. Think of something like an interstate freeway. Slow, local streets are relatively safe because of the slow speeds and the priority on building places that cater to people and businesses rather than moving automobiles as quickly as possible. The 'in between' things, \"stroads\" are what tend to be the worst of both worlds. They do often include some token bike/pedestrian infrastructure that is not very safe, and they include lots of places where other cars exit/enter the road and turn lanes and just a lot of potential for bad interactions in unexpected ways. The Strong Towns folks do a pretty good job of outlining this, and in terms of fixing it, I've seen some interesting stuff related to street design in the Netherlands. reply ta_1138 10 hours agorootparentI'd argue that most American local streets are unsafe anyway: I've seen Spanish highways with fewer lanes than suburbian streets with no commercial. But the distances to connect 500 bedrooms, placed in 1/3rd of an acre lots, are so large that ultimately roads are overbuilt to fit anything. Suburban streets with traffic under 1 car a minute in the daytime, with 3 or 4 lanes, set to a 30mph max, where you'd do 60 except for the fact that it's full of driveways coming in and out. Why do we even allow a lone house connect to a road like that via a driveway, where the neighbor will go into the street in reverse? It's madness, and is all over the midwest. So we don't even have to get into the stroads. reply AnthonyMouse 16 hours agorootparentprevThe trouble is why the \"stroads\" came to be to begin with. Motorists will generally prefer whatever route gets them to their destination faster, but shops want to be where the traffic is, because they want motorists to stop and patronize them. So the shops want to set up right next to the high volume traffic path. But then pedestrians patronizing the same shops will be adjacent to that high speed high volume vehicle traffic. In theory there are designs that can address this, i.e. you interleave roads so that each block is bounded by a high speed road for vehicle traffic on one side and a low speed road for pedestrians and cyclists on the other, keeping them separate but still allowing businesses to be accessible to both. But now you run into politics: The motorists may now have to walk up to a city block to get where they're going and the anti-car people are going to object to there being high speed roads and parking lots in the city whatsoever. Meanwhile making the change requires a budget allocation to do the work, so in the absence of consensus the status quo prevails. reply nephanth 5 hours agorootparent> shops want to be where the traffic is, because they want motorists to stop and patronize them. Citation needed? Motorists (as opposed to pedestrians) rarely stop at a shop because they passed in front of it (except on very long routes). Reason being, it is pretty hard to register what shops you're passing when driving I don't think having a lot of motor traffic in front of it helps a shop reply bluGill 4 hours agorootparentImpulse stops are rare (except gas) but people often develop habit of coffee along the way, and they will look for a place not out of their way. reply AnthonyMouse 4 hours agorootparentIt's not just coffee. If you're coming home from work and want to pick up dinner or grab something at a convenience store, are you going to stop at the place on the road you're already taking or do you want to add more time sitting in rush hour traffic to go somewhere else? reply nephanth 3 hours agorootparentBut adding a detour is easy in a car (even during rush hour - congestion is mainly on big axes). Restaurant doesn't need to be right where the traffic is. reply ta_1138 10 hours agorootparentprevBut that's a complete failure of the people doing traffic management: Other countries manage to make car accessible shops by minimizing the number of ingress and egress points in the road that is supposed to be fast, and moving the stores to a side street that has all those points, but is slower, narrower, and possibly usable by a pedestrian. The sin of the stroad is to give us a 6 lane road that is ultimately risky and slow-ish due to those ingress points, instead of separating the fast traffic and the slow one. Most of the time we'd not even need a larger right of way: Just treating major roads as places where every intersection is a serious hazard to minimize. reply AnthonyMouse 7 hours agorootparent> Other countries manage to make car accessible shops by minimizing the number of ingress and egress points in the road that is supposed to be fast, and moving the stores to a side street that has all those points, but is slower, narrower, and possibly usable by a pedestrian. The main issue here is really that other countries allow mixed-use zoning, causing a higher proportion of patrons to be pedestrians instead of the majority of the population living isolated in the suburbs and arriving by car. But you can't fix that by changing the roads, first you have to change the zoning -- and then wait several years for its effects to be realized. And in the meantime the shops will want to be on the high traffic road because that's how most of their customers arrive. reply ufocia 13 hours agorootparentprevStroads make sense where there is little pedestrian (including bicycles, scooters, etc.) traffic. reply ta_1138 10 hours agorootparentStroads never make sense, even with zero pedestrian traffic. They have way too many ingress and egress points, so they are wide, attempting to be fast, yet ultimately a significant crash risk, because there's a way in, or out, or something, ever quarter mile at the most. Tiny strip mall with 4 stores! A funeral parlor! A bank! a subdivision hidden somewhere? Sometimes, even straight out houses. All at 90 degree angles, where some traffic is doing 40, and there's no traffic lights in most of said interscetions. Even banning pedestrians, we'd be far better off with fewer ingress points to fast roads that now need fewer lanes, and then the few intersections/roundabouts give access to side roads that are rated far, far slower, and have access to those store parking lots. The traffic that is going far is then detached from the one that is going close, the road gets faster, and the street is safer from fender benders. The diminished places where people stop fast and go will also lower stress on the physical road itself, leading to less places needing repairs very often, as the typical stroad turning lane does. reply ufocia 13 hours agorootparentprevLet's just barricade up the streets. No traffic = no traffic accidents. This is what's been happening in Chicago with all the island, speed humps, etc. Reducing traffic to a safe crawl. The problem with those devices is that they slow down traffic even when there are no pedestrians around and the streets could be used to reduce congestion on the roads. I wonder what effect slowing traffic down to a crawl has on overall emissions. I'm guessing not good. I bet speeders are overall more efficient than crawlers. reply oblio 11 hours agorootparentFor the last point, EVs. For everything else, have you ever thought about the effects of higher speed traffic on residents? I'm guessing you haven't cause \"screw those people\". reply HPsquared 7 hours agorootparentRoads are a classic NIMBY thing. Essential infrastructure, but a nuisance to those nearby. reply macintux 5 hours agorootparentI grew up in a neighborhood that had no outlets. Very, very safe place to play as a kid. We’d be in the street all day long, riding, walking, playing. I returned recently, and the atmosphere is completely different, because now the streets have been extended. Through traffic completely changed the dynamics. reply Swizec 4 hours agorootparentprev> Let's just barricade up the streets. No traffic = no traffic accidents. That’s what my home city did – Ljubljana. Over the past few decades the downtown area has become an almost square mile sized pedestrian zone. It has been wonderful. The area is completely revitalized, shops are booming, tourism is booming, entertainment industry is booming, everything is booming. All because they kicked out the cars. Here’s a video and a photo from my recent trip back. It made me realize how dead San Francisco feels in comparison even with 3x the population because everything is just roads with nowhere for people to hang. https://x.com/Swizec/status/1803873334066843733 https://x.com/Swizec/status/1803896813679972570 reply te_chris 13 hours agorootparentprevIn London, despite assertions from individuals similar to yours, impact has been almost universally positive from Low Traffic Neighbourhoods. reply tstrimple 18 hours agorootparentprevTo me the comment spoke to our criminal lack of intelligent road design. It’s well known through multiple studies that road design impacts how fast people drive far more than posted speed limit signs. If we actually cared about road safety, we would design roads to be more safe and not just design a road that is comfortable to drive 60mph on and put up a 25mph speed limit. When you want slower speeds you need to make lanes more narrow. Add obstacles along the side of the road so it doesn’t feel so open. Add medians as areas where pedestrians have a refuge when daring to cross a place designed for vehicles. Add chicanes and bollards to force speed compliance in especially dangerous areas. There seems to be almost none of this happening in most places in the US that I have visited. reply thefaux 18 hours agorootparentWe also could largely solve this problem with technological enforcement but people really hate that. If we made both the financial penalties for speeding and the probability of being caught sufficiently high, we could practically eliminate it overnight. reply roughly 11 hours agorootparent1) build a society that requires a car to get around 2) exclusively sell cars with ludicrous acceleration and top speed 3) set legal speed limit at 1/6th the top speed of most vehicles 4) enforce strict financial penalties for operating one’s obligatory high-powered vehicle at more than 1/6th its maximum speed. I’m strongly anti-car - I think we fucked up a whole lot designing society the way we did, but at this point, actual strict enforcement of speed limits with financial penalties is just robbery. If you want fewer people to die in car accidents, build a world that doesn’t obligate everyone to drive, or build a world where the vehicles for sale aren’t all SUVs with 0-60 times that would embarrass a Ferrari from 40 years ago, but don’t just start fining people when they use the vehicle you made them buy to do the thing it was made to do. reply FireBeyond 48 minutes agorootparent> 3) set legal speed limit at 1/6th the top speed of most vehicles > 4) enforce strict financial penalties for operating one’s obligatory high-powered vehicle at more than 1/6th its maximum speed. Most consumer cars are going to have a hard time at 120mph, if their tires are even rated for it. So then you're claiming that most speed limits are 20mph. Which they're not. And then, I'm not sure? Should it be legal to drive suburban streets at triple digit speeds? I am not sure what you're trying to get at, beyond \"we should be able to use our cars to the limit of their capability, even if it exceeds our own as a driver\". reply xpe 7 hours agorootparentprevPlease be realistic about what interventions are available given the current US system. Redesign our entire road system? Sounds great, but how are we going do it? Think about the policy changes and thousands or more of political wranglings across every populous jurisdiction in the United States. Even if we get it done over the next 20 to 120 years, what are we going do in the meanwhile? reply com 7 hours agorootparentLike a lot of things, start with reviewing what works elsewhere, start some pilots, and what works do bigger and bigger rollouts. Like, use data. If marketers and TikTok can trick us so easily using these techniques we can do the same in socio-technical settings too. Like most things, “architectural” systems solutions will work better than point behavioural interventions, but it’s always going to be a mix. Bike safety in The Netherlands was a multigenerational effort ranging from creating standards around roads intersections, bike paths and pavements and slowly remediating old ones while building new ones. That’s only a tiny part of a society-wide effort to improve quality- and length-of-life measures, but like the US Interstate highway system, has had measurable results in terms of economic and social outcomes. Some actions taken today will have individual results tomorrow. Some in 30 years. Better get started, right? reply HPsquared 6 hours agorootparentprevIf something is illegal and enforced, people won't do it. reply macintux 5 hours agorootparentWe’ve tried that with Prohibition, the War on Drugs, and speed limits. Even where speed limits are enforced, people speed. reply Karrot_Kream 17 hours agorootparentprevThe reason this isn't solved is because traffic deaths are considered to be a \"cost of doing business\". Most pedestrian traffic in the US is in cities and due to the way funding formulas and political representation works in the US, suburban and rural areas have more political power than urban areas which care more about traffic throughput than pedestrian safety, so the issue never gets fixed. The only places in the US making headway have large urbanized areas and even they are struggling. reply runarberg 14 hours agorootparentRural areas don’t have much political power either. A lot of rural America is actually unincorporated which means they don’t have a municipal level government, and instead are ruled by the county[^†]. In my experience the outsized political power is across gegraphic areas and instead is divided between classes, with the rich having almost all political power. A rich neighborhood in San Francisco hold much more political power than an improvised suburb in Fresno. Instead the reason I believe for pedestrian traffic being considered the \"cost of doing business\" among the political classes, is the good old hatred of the poor. Pedastrian casualties are extremely rare among the rich, as the rich usually drive almost everywhere, and if they walk, they do so in an area which they have lobbied to make safe for pedestrians. The rich don’t care if the poor die. †: As an immigrant, this feels like a major democratic oversight, one of many USA should fix if it wants to consider it self a democracy by 21st century standards. reply plorkyeran 13 hours agorootparentOther states may be different, but at least in California there’s a pretty straightforward path to incorporation if a community desires it. The large number of unincorporated areas are because the residents don’t think the tradeoffs are worth it. Counties are also generally all too happy to delegate decision making to communities which will take it on even if they don’t incorporate. reply runarberg 12 hours agorootparentIt still feels like a democratic oversight. The most favorable interpretation is a failure of policy. It is simply not acceptable to leave any part of your population without municipality level representation. Delegating this to a community council is not democratic and is extremely ripe for misuse. If a community wishes to remain unincorporated you have to look at why that is, and offer accommodations or change the incorporation strategy accordingly. For example both Skyway and White Center (unincorporated King County, WA) voters have refuse to be incorporated with Renton and Burien respectively, but it was the Seattle city council (not voters) that rejected incorporating White Center, and voters have never been asked if they want their own independence. At the same time Vashon hasn’t even been asked. If America was serious about democracy they would establish a policy in which every populated area outside of reservations will have local level representation in like 30 years (ideally they should have started that policy 30 years ago). And if there is no agreement on how a single community (say White Center) hasn’t incorporated by that time, have a plurality wins—or better yet, ranked choice—vote on e.g. 3 options, Seattle, Burien, or independent. reply ianburrell 3 hours agorootparentSkyway and White Center aren’t rural. They want to be unincorporated to pay less taxes and follow less rules. I agree that suburban unincorporated shouldn’t be allowed. I would add that small, below 50-100k, suburban cities shouldn’t be allowed. But that isn’t for smaller subdivisions but larger ones, But that has nothing to do with rural areas. My brother lives in Iowa near medium town. Everything else is small towns, less than 1000. Should those incorporate and spend money on city services? What about the farmers who are spread out? What municipality do they belong to? The county is the best option. reply ufocia 13 hours agorootparentprevLet me understand this. Are you suggesting adding yet another inefficient layer of government in sparsely populated areas? BTW, in at least some states there are intermediate subdivisions of government, e.g. townships and districts, which take care of the roads even in rural areas. reply runarberg 13 hours agorootparentYou are responding to my footnote, but ok. Yes. This is what most—all?—other democracies do. More realistically though, municipality level governments include surrounding rural areas. In areas with small towns and large areas of rural farmlands, the farmers and town residents have equal representation, but the farmers obviously have a bigger political influence (hopefully the municipality governments have enough representatives though that the townfolks have at least a couple of representatives). In reality unincorporated America also includes heavily urbanized areas (more often than not poorer than the surrounding areas). Here in Seattle this includes Skyway and White Center. But even if aside from those it is pretty unacceptable that all local planning for the community of e.g. Fairwood, or even Hobart don’t have any say in their municipality level organization, instead relying on the same county council as Seattle for their local affairs (a council with only 9 representatives for a total of 2.2 million constituents). reply brailsafe 17 hours agorootparentprevIndeed, just because a problem could theoretically be addressed (I think solved is a reach) by technology, doesn't mean it's a sensible choice. Much like how putting a sign that says \"pull\" on a door that's designed to be pushed is analogous to what most places in NA do, which is to threaten people who use the road in the way it's designed. reply alexpetralia 17 hours agorootparentprevIn my one trip to China so far, this is exactly what I observed! reply Gigachad 14 hours agorootparentIt's the same in Australia. Speeding is quite uncommon and you'll be very quickly caught out. reply WarOnPrivacy 17 hours agorootparentprev> We also could largely solve this problem with technological enforcement but people really hate that. The challenge in giving powerful entities direct control over our actions is they: directly control our actions, ceaselessly seek to control other actions, will be as unaccountable as they can be, will not ever allow control to flow in the other direction. reply FactKnower69 17 hours agorootparentThis type of whining about slippery slope hypotheticals is ubiquitous in these discussions, but it's not very compelling up against the current reality of 40,000 unimaginably violent early deaths every single year in the US alone reply jacoblambda 14 hours agorootparentIdk. Technological enforcement seems like it really should be the last resort here. Why should we not focus on stopping the construction of stroads and building safer streets and roads first and foremost before we reach for a tech solution that will undoubtedly come with privacy and abuse related tradeoffs while also likely being less effective. reply xpe 7 hours agorootparentThink probabilities here. Given human nature, what are the most probable solutions? I’ve seen a number of cities find the authority or political will to increase the number of automated speed cameras. This suggests (while not a complete complete solution) a real step in the right direction reply jacoblambda 1 hour agorootparentGiven the entire urban planning political environment has shifted towards gradual but substantial infrastructure changes, at this point the main barrier to change is just making it happen. And to achieve that all you need to do is push for new road standards and guidance at a city, county, or state level. Once that's done the changes can roll out whenever there's maintenance or road widening going on. This is for example what Florida is doing to push for a comprehensive passenger rail system and it's what other countries have done to make their roads and streets safer and more efficient as well. So it'd stand to reason the same principle would work at state and local levels in the US for this as well. The only real argument against it I could see is that it'll take too long but 30-50 years really is nothing for widespread infrastructure improvements. reply Supermancho 15 hours agorootparentprev> it's not very compelling up against the current reality of 40,000 unimaginably violent early deaths every single year in the US alone I disagree. The economy depends on the rivers of money that flow through the roads. Roads dispense communication, goods, labor, et al, over the vast area that is the USA. 40k deaths, distributed across the US, is a good deal. On the other hand, I had great expectations for companies that wanted to provide a solution that's safer for a profit (robocars). A handful of people died during the development, and it's rejected outright by large portions of the population. So here we are. reply thefaux 13 hours agorootparentNot only is this accounting callous, it seems to presuppose that there is societal benefit in reckless, antisocial driving behavior. I don't believe that this is true. Imagine a world in which median vehicle speeds remained the same and traffic fatalities went to zero. I'd take that 10 times out of 10 compared to the status quo. Of course I was being mildly hyperbolic when saying we could solve speeding with technological enforcement, but I genuinely believe it could make a massive difference and lead to a significant quality of life improvement for most people. For those with the need for speed, build more tracks. But we should stop normalizing reckless behavior on our shared roadways. There should be an expectation of safety and we should maximize traffic flux while minimizing traffic injuries and fatalities. reply ufocia 13 hours agorootparentBefore we tackle the hard problem outlined above, let's solve the easy problem of pedestrians (bikers, scooters, skateboarders, etc.) traveling on highways and crossing traffic in undesignated places. I can't tell you how many times I've had pedestrians impatiently run across the roadway in front of my car. reply ossyrial 12 hours agorootparentI'm not sure one problem is easier to fix than the other. They both seem to come from people acting irresponsibly to arrive earlier at their destination, probably combined with an infrastructure to nudge towards that behaviour. Changing behaviour with a penalty isn't terribly effective unless enforced in such a way that it is incredibly privacy-invasive, more effective is changing the layout of the streets. But I wouldn't be sure that that is easier to fix on the pedestrian side than on the vehicle side. reply oblio 11 hours agorootparentprevLet me get this straight, do you want to put the burden on pedestrians? reply HPsquared 6 hours agorootparentI guess it could also involve building proper crossings. reply this15testingg 9 hours agorootparentprevthis is such a hilariously bad take that I have no hope that anything will ever change reply nkrisc 8 hours agorootparentprevMere sacrifices for The Greater Good. The Greater Good! Will you step up when it’s your turn? reply lazide 16 hours agorootparentprevStill tiny compared to heart disease! reply oblio 11 hours agorootparentMost of those are avoidable plus the rate of long term disabilities is about 10x the death rate (so 400 000) and of minor injuries is 10x that (so 4 million) plus... we can do both. There is plenty of money for safer road infra, DRIVERS JUST DON'T WANT IT because killing a stranger matters less than 5 fewer minutes spent commuting per day. reply lazide 6 hours agorootparentThe same situation is true of heart disease, albeit the risk is generally killing themselves vs a random stranger. People just run out of shits to give at some point, and do what is easy. reply theGnuMe 18 hours agorootparentprevWhat are you trying to say? reply magneticnorth 17 hours agorootparentHe's saying that if a stretch of highway has traffic volume of 10 million trips taken on it in a year and an average of 2 deaths per year, that is still much safer than a neighborhood street which sees 10,000 trips per year and averages 1 death per year. (numbers made up to emphasize a point, a neighborhood street with 1 death per year is pretty obviously unsafe) reply theGnuMe 5 hours agorootparentThat’s not the correct comparison anyway. reply deepsun 17 hours agorootparentprevThey likely say that more people use fast roads, so it's expected to have more accidents. The safest road is one never used. reply david-gpu 9 hours agorootparent> The safest road is one never used. The safest road is one without motor vehicles. Pedestrians and bicycles cause a tiny number of injuries even when traffic is high. reply HPsquared 6 hours agorootparentCyclist hitting a pedestrian at 20mph is more dangerous than 2 cars hitting each other. reply dhosek 17 hours agoparentprevWe have what are called “stroads” in North America, which are very much what you’re describing. I remember when I was a kid, they took Pershing Road on the Stickney-Berwyn (Illinois) border and changed the commercial parking from diagonal to parallel and increased the traffic flow from two lanes to four. I have no firm data, but I’m pretty sure that this increased accidents on the street as well as effectively killed the Stickney CBD on Pershing Road (I would guess at least a third of the storefronts are currently vacant and that’s ignoring buildings that were demolished and never replaced). And I’m not entirely sure what traffic benefit came out of increasing the flow along the street as it’s not really a good connector of anything in that stretch. reply tengbretson 1 hour agorootparentWhat you're describing sounds a lot more like a stroulevard. Not a stroad. reply ufocia 13 hours agorootparentprevThe fragrance of the water reclamation plant more likely killed the business in Stickney (or Stinky as some people refer to it lovingly). reply newZWhoDis 16 hours agorootparentprevNo we don’t. That idiotic “word” was coined by a fanatical YouTuber, not anyone that should be taken seriously. I dare you to say “stroad” outside of your online echo chamber. reply lolinder 15 hours agorootparentI don't particularly like the word myself, partially because I use 'street' and 'road' interchangeably and so I'm never clear what it's supposed to be a hybrid of, but the word was coined by Charles Marohn, founder of the nonprofit Strong Towns [0]. I don't know about any YouTube channel associated with him, but I have heard of the nonprofit and I'm not exactly a passionate follower of that niche. [0] https://en.m.wikipedia.org/wiki/Strong_Towns reply gcanyon 8 hours agoparentprevYou might be interested in Not Just Bikes. In particular, their take on what they call \"stroads\": https://www.youtube.com/watch?v=ORzNZUeUHAM reply ocschwar 14 hours agoparentprevOne of the big ironies of all this is car insurance. If you keep getting your car dinged on narrow roads, your insurance payouts will be larger than if you run over someone. Dings add up to a lot, but hitting a pedestrian maxes out the payout before the guy makes it out of the emergency room. This is why Massachusetts drivers have a bad rep. Lots and lots of minor accidents in dense traffic. But MA itself it one of the safest places in the US reply bkjelden 15 hours agoparentprevSpeed kills. A road where traffic moves at 30mph can be poorly designed and lead to lots of accidents, but so long as those accidents are not a vehicle hitting a pedestrian they're unlikely to be fatal. A road where traffic moves at 70mph is another story. reply naniwaduni 14 hours agorootparentMore people also just die where there are more people... reply ajuc 9 hours agorootparentUSA has 12.9 traffic deaths per 100 000 people yearly with 330 million people. Europe as a whole has 7.4 with 746.4 million people, and developed EU countries are around or under 5. Surface area is actually quite similar 9.3 million vs 10 million km2. There are countries with population densities higher and lower than US in there, and ALL BUT ONE OF THEM have less traffic deaths than US. It's Bosnia and Herzegovina by the way. And it's at 13.5. USA is crazy unsafe for a developed country, and it barely matters if you compare with sparsely populated Canada (5.2) or Sweden (2.0), or densely populated Germany (3.7) or Japan (2.1). It's not about population nor population density. It's not about wealth. It's not about population distribution. It's about car-centrism and insane design. Source: https://en.wikipedia.org/wiki/List_of_countries_by_traffic-r... reply andrepd 3 hours agorootparentIt's a crazy comparison because Germany for example is still an incredibly car-centric country. reply blackeyeblitzar 3 hours agorootparentprev12.9 still seems very low to me. What makes the lower number better? I wouldn’t want the difference between 12.9 and 7.4 if it means I need to give up on driving cars and take slow inconvenient public transit or be limited to where public transit takes me. Cars are freedom. I would also argue the US is more successful than literally every other country in part because of fast road infrastructure. So maybe they’re all just making the wrong tradeoff. reply ajuc 2 hours agorootparent> What makes the lower number better? ~20 000 fewer people dying per year. > I wouldn’t want the difference between 12.9 and 7.4 if it means I need to give up on driving cars You don't need to give up driving cars. People do drive cars outside USA. > slow inconvenient public transit When it's done right it's more convenient than cars. I own a car and I drive under 3000 km per year because I just barely need it. > I would also argue the US is more successful The only metric I can think of where this is true is military, which does not seem relevant :) reply cperciva 14 hours agoparentprevSpeed kills, but slowness eats lives in another way. Which is worse, having a 1% chance of dying in a car accident, or spending an extra hour/day stuck in traffic? I think I might take the first option; it delivers a higher expected number of hours of life not stuck in traffic. reply Underphil 14 hours agorootparentI'm sure there's a fancy name for the fallacy you're presenting here but there are more than two choices. The third choice is stop building in such a way that these wide and fast roads are needed. reply maxerickson 3 hours agorootparentIt's not even a realistic representation of the commute choice. In the US, with a heavily car centric culture and ~stupid housing policy, we still have an average daily commute of about 55 minutes. Aren't gonna save an hour on that. reply hashmush 12 hours agorootparentprevThe fancy name is \"false dichotomy\", and you're completely right. reply HPsquared 6 hours agorootparentprevThat also has a cost. Needing to uproot your entire life and move everything around. reply dr_kiszonka 12 hours agorootparentprevhttps://en.wikipedia.org/wiki/False_dilemma reply oblio 11 hours agorootparentprevSorry, but: carbrained. 1. If the US would have safer streets/roads/... and overall more human-oriented city design some people would walk to their destinations. Look at this stupidity, and this is a new community: https://youtu.be/9-QGLfWSrpQ?si=bsTbq0cdMltXEzU5&t=299 Also some more people would bike to their destinations. Some people would take take public transit. And guess what, that means that in the end, fewer people drive so there's less traffic, and on empty streets going 30kmph on average, you get where you want faster than in stop and go or constantly merging traffic that has posted speed limits of 50 kmph. As people say, driving in the Netherlands is world class, and that's > the Netherlands has tons of cyclists, not > them. 2. You know or should know that it's not about you dying in your car. These days cars are incredibly safe tanks for those inside. You're more likely to kill someone > your car. 3. Other people have pointed out the logical fallacy, already. reply HPsquared 6 hours agorootparentEverything in the Netherlands is world-class though. Some policies are feasible somewhere like NL but not in other places. People often say \"this is why we can't have nice things\"... Such policies are the \"nice things\". reply ndsipa_pomu 6 hours agorootparentIt hasn't always been like that. There was a public campaign in the 1970s to try to prevent all the child deaths from car drivers - Stop de Kindermoord. There's not really any excuse for continuing to design purely for car drivers apart from that's what people have been led to believe. https://www.dutchreach.org/car-child-murder-protests-safer-n... reply andrepd 3 hours agorootparentprevThe Netherlands was as car-supremacist a country as any other until the 70s. They fixed their stuff and now their bike infrastructure is excellent: people move to their destinations faster and safer and cheaper and cleaner than in everyone had to travel in an automobile. reply blackeyeblitzar 3 hours agorootparentBikes cannot be faster than cars because cars can literally accelerate faster and have higher top speed. When car infrastructure is sufficient there is no theoretical way bikes can move people faster as you claim. As for safer - cars are very safe already and it is irrational to care about minor risks. reply HPsquared 6 hours agorootparentprevIndeed. You can do a utilitarian cost-benefit analysis using \"quality-adjusted life years\". reply andrepd 3 hours agorootparentprevMy, if only there were other worlds possible than \"stuck in traffic for 2h\" and \"flying down a 14-lane road on a 4-ton child-killing machine\"... But sadly it's impossible :'( reply nvy 11 hours agorootparentprevHow selfish of you. Don't you care that you're more likely to kill somebody's kids as you fly down the road in your SUV, just to save some time on your commute? reply blackeyeblitzar 3 hours agorootparentHow selfish of you. Don’t you care that your demand for your smartphone could result in the death of a miner in Africa? reply oblio 8 hours agorootparentprevAlso super shameful, probably because you're famous here, that your comment is >. reply newZWhoDis 16 hours agoparentprev>Looks like where I live, deaths are more closely associated with big, wide, fast roads. This is sort of like saying “most child p*rn is transported by undersea fiber optic cables!” reply blackeyeblitzar 18 hours agoparentprevSpeed and convenience also matters. I like big, fast, and wide roads because they let me and many others get to where we want to go quickly. It’s a trade off. We shouldn’t let “think of the children” safetyism decide what the balance is, since that line of thinking is extremist and does not consider what is at stake on the other side of the argument. Efforts to eliminate every last death on streets are a waste of time since we’ll never achieve perfection and roads are very safe already. The road diets made under that unrealistic goal are simply making everyone’s lives worse by causing us to spend more time on the roads in traffic. reply alexpetralia 17 hours agorootparentDon't just think about the deaths. Think about all the injuries too. All the environmental damage. All the people that don't hang on the street because of the fumes and noise and danger. All the road rage and cortisol that boils within otherwise sane people the moment you put them behind a wheel and into some traffic. All the sedentarism and obesity from people opting to drive 2 minutes rather than walk 10. All the forgone housing for parking stadiums. I am all for convenience, but the costs are noticeable in more ways than one. reply AnthonyMouse 16 hours agorootparent> All the environmental damage. Environmental damage is the argument against traffic calming measures. Vehicles are most efficient traveling at a consistent, relatively high speed. The reason the national speed limit was historically set at 55MPH was that was the approximate speed at which aerodynamic losses overcome mechanical losses from low gearing at low speeds, i.e. it was the speed that vehicles of the time were most fuel efficient. Modern vehicles have even better aerodynamics. Moreover, fuel efficiency for electric vehicles is essentially moot, because they have built-in storage that can be charged from intermittent renewable sources during times of oversupply when the power is \"free\". Conversely, traffic calming generally results in vehicle speed changes as motorists slow down and then speed back up again in response to obstructions or areas with intentionally low visibility, which not only wastes fuel by operating vehicles below their optimal speed, it results in braking and acceleration that increases brake dust and tire wear. Environmentally, the optimal road is flat and straight with no traffic control devices or other reasons for vehicles to change speed, i.e. a highway. reply pfg_ 15 hours agorootparentEnvironmentally, the optimal road is the road with no cars on it. Ideally, traffic calming is paired with cities where things are put closer together and where walking, biking, and transit are the most viable options for most trips. Traffic calming a road in a suburb a mile from the nearest store might help with safety, but people will still use cars to get everywhere. reply AnthonyMouse 13 hours agorootparent> Environmentally, the optimal road is the road with no cars on it. But then why is there even a road there? How to reduce the amount of travel required and how to most efficiently get from A to B are two different issues. Doing the former is good, but it requires things like new higher density housing construction, which takes a long time and is not going to cause most of the existing homes in the suburbs to cease to exist under any plausible expectation. One of the reasons for this is that high density doesn't require much land; if you build 20 units to a lot then you could double the existing suburban housing stock as high density units, but you'd have only bulldozed 5% of them to do it, so the other 95% would still exist. This would reduce housing costs but you'd still have someone living in most of those existing homes, which are in places it's not viable to walk or use mass transit. And then you might want to ask a question like \"how do we make transportation more efficient in the short term, i.e. on a 5-10 year timescale\"? To which the answer is things like \"make new cars electric\" and \"optimize high-traffic roads to maximize the efficiency of existing vehicles\". reply dghlsakjg 15 hours agorootparentprevThis assumes that cars are the only way to get around. A bus that starts and stops as it goes through traffic calming with 100 people on it will make an absolute joke of the efficiency of even the most fuel efficient of cars. In areas where transit is given its own lane, or is a train, the time efficiency is much better as well. Plus if you give buses their own lanes you can remove traffic calming measures for them and give them signal priority, thus making them even more efficient from a resource, and time perspective. reply AnthonyMouse 14 hours agorootparent> This assumes that cars are the only way to get around. Which is true in many cases, and would take decades of construction to do anything about, e.g. because people would have to move out of the suburbs or else at least one end of the trip will require a car, which would require massive long-term new housing construction in urban areas and has no short-term solution. > A bus that starts and stops as it goes through traffic calming with 100 people on it will make an absolute joke of the efficiency of even the most fuel efficient of cars. A city bus will get around 5MPG. The most efficient cars get more than 50MPG, so a city bus isn't even as fuel efficient as the cars until it's carrying more than 10 passengers. In theory they can carry 30-40 passengers, but generally in practice they don't, and in theory that 50+MPG car can carry five or more passengers too. > In areas where transit is given its own lane, or is a train, the time efficiency is much better as well. \"In areas where the time efficiency of car traffic is purposely degraded, car traffic has lower time efficiency\" is kind of tautological, but that's a silly argument for doing it, especially when the proposed alternative isn't available, e.g. because one of the endpoints is in the suburbs and the bus doesn't go there. > Plus if you give buses their own lanes you can remove traffic calming measures for them and give them signal priority, thus making them even more efficient from a resource, and time perspective. It's kind of odd that the same people who talk about wasted space from parking want to allocate entire bus lanes worth of space for a vehicle that only uses them 0.2% of the time. Also, what are you proposing here? 50+MPH buses traveling next to bike lanes and pedestrians? It would have to be even higher than that, because the bus is constantly starting and stopping to pick up passengers (and is then stationary for several seconds), so to achieve an average speed of e.g. 30MPH, its cruising speed would have to be above 60MPH, which is not only dangerous if adjacent to pedestrians, it's extremely inefficient as you're repeatedly accelerating a huge bus to highway speeds and then back again. When the alternative is a car traveling a constant 60MPH on a highway, the bus compares unfavorably in terms of both time and fuel efficiency. reply dghlsakjg 13 hours agorootparentI’m not going to point by point you. I will say that I lived in Vancouver. A city where I have never seen fewer than 10 people on a bus, where driving is frequently slower than transit, where you are rarely more than a 10 minute walk from a bus, where during rush hour, they convert parking lanes to bus lanes. It does take time to change, but it will take longer if we wait. All of this works fine in places where they have been enacting all the things that you are saying don’t work. Most people just can’t imagine it working until they see it. reply AnthonyMouse 12 hours agorootparent> A city where I have never seen fewer than 10 people on a bus Presumably during rush hour, which is kind of the issue. You can get more people on the bus during peak hours, but then it's off-peak and you're in a place where you don't have a car. Now you're either waiting an hour for a bus so it can be full (which is slower than a car) or you're maintaining frequent service by running mostly-empty buses (which is less efficient than a car). Vancouver is also a coastal city the size of Boston with a fairly high population density. Things will work there that won't work in smaller inland cities surrounded by suburban and rural areas. > where driving is frequently slower than transit But because driving there is slower than it is in most US cities, right? That's not really an attractive way to get the result. The goal is to make the new thing better, not to make the existing thing worse. > All of this works fine in places where they have been enacting all the things that you are saying don’t work. Most people just can’t imagine it working until they see it. The real problem is that people propose these things in places where they don't work. If you have an urban city with dense urban housing, obviously people will be able to use mass transit. But you can't just add a bus lane to a city where most of the population commutes in from the suburbs and expect it to have the same effect. Everyone still has to drive and all you've done is remove a travel lane and make the traffic worse. reply david-gpu 9 hours agorootparentAllowing the construction of mixed-use medium density buildings reduces the distance of the average trip, which allows more people to choose walking and cycling. At the end of the day, the more you design a neighborhood to facilitate driving, the more car traffic it will suffer. And the more convenient you make it to any other form of transportation, the less car traffic there will be. reply AnthonyMouse 8 hours agorootparentAllowing the construction of mixed-use medium density buildings reduces the distance of the average trip several decades from now, after the new zoning has filtered out into the already-constructed installed base of existing buildings. That doesn't mean we shouldn't do it -- in fact we should do it immediately for precisely this reason -- but you can't expect it to have an instantaneous effect. Meanwhile people keep proposing things like bus lanes as something we should do in the present day, in places where they can't work until after that construction has already happened. Also, bus lanes are never a good idea because the density required to justify a bus lane (which is very high because it consumes a significant amount of surface land in an area with high land scarcity) is higher than the density required to justify a subway line (which doesn't). reply oblio 8 hours agorootparentprevLOL, ever heard of particulates from road wear and tire wear? Guess how the production of those scales with higher speeds? > Vehicles are most efficient traveling at a consistent, relatively high speed. Guess what, that's changing with EVs. Hybrid or not. > Environmentally, the optimal road is flat and straight with no traffic control devices or other reasons for vehicles to change speed, i.e. a highway. Awesome, put those where nowhere lives. reply AnthonyMouse 8 hours agorootparent> LOL, ever heard of particulates from road wear and tire wear? Guess how the production of those scales with higher speeds? I didn't have to guess because I looked it up. Turns out it's much more proportional to acceleration/deceleration than absolute speed. > Guess what, that's changing with EVs. Hybrid or not. But EVs can charge from renewable sources and then they don't have any fuel-related emissions. > Awesome, put those where nowhere lives. Roads are used for going from where people live to where they want to go. reply blackeyeblitzar 3 hours agorootparentprev> Guess how the production of those scales with higher speeds? I am guessing you don’t know yourself which is why you’re posing questions as an argument. > Guess what, that's changing with EVs. Hybrid or not. It’s not changing. Consistency of speed is important no matter what. Regenerative braking is imperfect. reply WarOnPrivacy 17 hours agorootparentprevI don't disagree but I would add this adjacent perspective. It's a bit like we've installed public-funded, unremovable alcohol spigots in everyone's home. People using them within their designed limits lead to awful outcomes. We're reasonably upset about that and respond with thousands of marginally and unequally enforced restrictions. Unhappy with their ineffectiveness, we just keep piling on more punitive restrictions. Giving up booze infrastructure isn't on the table tho. We're too dependent on it. reply davidw 18 hours agorootparentprevTraffic deaths are quite literally one of the two leading causes of death of children in the United States, so in this case, yeah, actually thinking of the children makes some sense. reply blackeyeblitzar 15 hours agorootparentTo me that sounds like a safetyist argument. Even if the number of deaths are high in total count, it may not matter when you consider the trade offs. For example if everyone spends an hour more in traffic each day - which is what the effect of “calming” has been in my experience - you’re causing an impact that is worse than the small number of deaths in my city. That delay and damage to our life quality matters, and needs to be weighed against the rare deaths. Cars are very safe today and are getting much safer. Even basic cars come with many features to avoid accidents now. We will probably see deaths per mile driven go down on its own, without the need for malicious road design. reply ryukafalz 14 hours agorootparent> Cars are very safe today and are getting much safer. For their occupants, sure. For those outside cars (and remember, children can't drive) not so much: https://www.nbcnews.com/news/us-news/americas-cars-trucks-ar... reply blackeyeblitzar 13 hours agorootparentCars now have sensors all over and automatic braking to prevent collisions. The article acknowledges the benefits of front facing sensors in luxury vehicles from the time it was written, which are very common today even in basic vehicles. So are the 360 cameras it mentions. Also - this article is focused on data from 2016 to 2020 for front collisions. It mentions 744 deaths of children in front collisions on non public roads (where the blind spots it talks about matter more) in that 5 year period, which is frankly a small number. This is a country with a few hundred million people after all. Some number of deaths are inevitable and it isn’t a crisis. reply david-gpu 8 hours agorootparent> Cars now have sensors all over and automatic braking to prevent collisions. Yet pedestrian deaths in the US have kept climbing over the past ten years or so. I can tell you that as a lifelong pedestrian I do not feel remotely safe walking in North America compared to Western Europe, where I used to live, or Japan, which I've visited a few times. reply blackeyeblitzar 3 hours agorootparentIf you really feel unsafe about incredibly low risk possibilities, your only choice is to stay indoors permanently. Most people feel safe walking because the chance of something happening is so unlikely. Pedestrian deaths may have climbed in recent years because of increased smartphone use or changing behaviors. I see many more jaywalkers for example, especially by homeless drug addicts in west coast cities, many of whom just blindly step into traffic. There is no rigorous way to attribute your claimed increase in pedestrian deaths to cars. reply david-gpu 1 hour agorootparentThere are cell phones everywhere, but pedestrian deaths have only increased in the USA, so it is not that. Walking in my neighborhood is objectively more dangerous than it needs to be. In the past decade there have been several instances where motorists have mowed down and killed pedestrians, sometimes when they were minding their own business walking on the sidewalk. I'm sick of motorists only valuing their own convenience and using demeaning language to describe the pedestrians that they victimize. reply esoterica 13 hours agorootparentprevPeople with long commutes spend most of that time on highways, which are not affected by traffic calming measures. A surface street going from 35mph to 25mph is not going to add an hour of driving time unless you are driving 100 miles a day on non-highway surface streets, which literally nobody does. You are exaggerating the impact of traffic calming measures. Cars are getting less safe for pedestrians and cyclists, not more safe. Why should pedestrians bear the human cost of higher car speeds when drivers are the ones benefitting from it? Easy to pretend the benefits of speeding outweigh the costs when the benefits accrue to you and the costs accrue to other people. reply blackeyeblitzar 13 hours agorootparent> A surface street going from 35mph to 25mph is not going to add an hour of driving time unless you are driving 100 miles a day on non-highway surface streets, which literally nobody does. I’ve seen streets go from 45mph to 25mph, lose driving lanes to bike or bus lanes, lose parking, etc. It makes things far worse than you think. What used to be a 20 minute drive will now be 35 minutes. Now consider the drive in both directions, time to find parking, and other trips you might make that day. It forces people to stay confined and not make as many trips because it simply isn’t possible to fit them in anymore. That is a loss of life quality. > Why should pedestrians bear the human cost of higher car speeds when drivers are the ones benefitting from it? They don’t have to and by and large they don’t bear any cost for it. You’re exaggerating things - the probability of a pedestrian dying is incredibly low. I walk as well and am not in fear of cars just like I’m not in fear of other unlikely events. reply esoterica 12 hours agorootparentThe quality of life improves for pedestrians, cyclists and transit riders when parking and car lanes are converted to bike and bus lanes. Drivers are not the only stakeholders who deserve consideration. Many of the people who insist that there is no safety impact from high speed local roads nevertheless choose to raise their kids in suburban cul-de-sacs with minimal traffic and curvy roads with low speed limits. They want the right to subject other communities to speeding cars for their own convenience while protecting their own families from them. reply naniwaduni 14 hours agorootparentprevSlowing down insidiously shaves away at your lifespan too, it just doesn't produce exciting catastrophic life loss events. reply david-gpu 8 hours agorootparentBy that same logic, distance insidiously shaves away at your lifespan and we should build mixed-use walkable neighborhoods so that we can quickly reach our everyday destinations rather than causing traffic every time we want to get anywhere. reply m463 17 hours agorootparentprevNote the way to do this is to follow engineering rules. Take the 85th percentile rule. If you take a neighborhood road and change it from 40mph to 25mph in an attempt to \"save the children\", you can easily make it more dangerous. The 85th percentile rule figures how fast people go on a road, and sets an appropriate speed limit that people naturally follow. Attempting to set a speed limit too low or too high leads to a wide speed variance, which makes the road more dangerous. reply davidw 16 hours agorootparentOr we could actually build slower, safer streets that are that way by design, rather than relying on signs. And where roads need to be fast and move a lot of cars, separate them out from other uses. reply m463 16 hours agorootparentand set speed limit 5-10 mph below design speed for maximum safety. reply maxerickson 3 hours agorootparentPeople go the speed they are comfortable with, not the speed on the signs. You have to design for the speed you want. reply occz 11 hours agorootparentprevI don't think I've ever heard someone argue that the 85th percentile rule is actually a good invention - it's a disaster that codifies the behaviour of speeding drivers. I agree that merely lowering speeds without changing the design speed is a bad move, though. reply durkie 16 hours agorootparentprevHow does setting a speed limit \"too low\" make a road more dangerous? reply m463 16 hours agorootparentSafest roads have the speed limit 5-10 mph below the road design speed. see fig 4 on speed variance: https://www.fhwa.dot.gov/publications/research/safety/17098/... going faster or slower than average traffic speed shows markedly greater accident rate. note that this can apply not only to speed limits, but things like express lanes going different speeds than other lanes reply durkie 16 hours agorootparentBut you're saying the speed limit (being too high or low) makes the road dangerous. Aren't the people driving their cars too fast making the road dangerous? reply Leif24 14 hours agorootparentA great many people when they see a open straight road with little obstacles or pedestrians will go 45-50 MPH. A great many people when they see a 15 MPH sign will go 20 MPH. Pair that low speed limit with a \"fast\" road and you will end with many people going 45 MPH and many people going 20 MPH. This variance in speed, with some people going much slower than others can be more dangerous than if most went the same speed - e.g. if the limit were 45 MPH. You are correct that people driving too fast make the road dangerous, but so does people driving too slow. Generally, from a safety point of view, you want the slowest speed at which almost everyone will actually drive at, as large variance in speed between drivers is dangerous. I think this is what the parent post was getting at: a speed limit too fast OR too slow will increase the number of accidents, keeping in mind that there will always be at least some drivers speeding. reply david-gpu 8 hours agorootparent> You are correct that people driving too fast make the road dangerous, but so does people driving too slow. It's accurate to say that people driving too fast are extra dangerous when there are slower vehicles in the road. The danger is still caused by the people driving too fast, not by those driving slowly, though. Speed kills. reply bryanlarsen 16 hours agorootparentprevIn my personal experience, automated speed cameras are way more effective than traffic calming. reply owlbite 12 hours agorootparentI'd go further and say automated average speed cameras are the most effective I've seen. Point speed cameras just get marked on a map and cause sudden braking and acceleration to dodge them - this can be effective at particular danger spots, but I always feel the average speed cameras in the UK are far more effective at changing driver habits in general. reply dghlsakjg 15 hours agorootparentprevI live in BC, where speed cameras are banned by law. Right next door is Alberta. Last time I was in Edmonton, known for extremely car centric design, wide roads, ample highways, etc. I was shocked by how much slower people drove, and as a result, how much safer driving was in general. You only have to get slapped with a fine a few times before you start learning to control your speed. reply nephanth 3 hours agorootparentprevNot in mine. Drivers who know about the camera's location speed down right before the camera and speed up immediately after reply oblio 8 hours agorootparentprevYeah, but those probably cost more and don't make for a nice environment outside of cars. Traffic calming can be super cheap and it makes for a super pleasant environment for everyone. reply bryanlarsen 7 hours agorootparentTraffic calming is usually more expensive than speed cameras. A speed bump isn't expensive but people are usually talking about stuff like bump outs and raised intersections and protected bicycle lanes and wide sidewalks when they are talking about traffic calming. Those are six figures per intersection. Cameras are low five figures. reply bluecalm 15 hours agorootparentprevThe problem is that people are naturally very bad drivers and are especially bad at judging what safe speed is. We already know it's hopeless to teach them so what is left are traffic calming measures, heavy handed enforcement and technology (automatic speed limiters in cars). Setting speed limits to speed people choose is a",
    "originSummary": [
      "The Nationwide Vision-Zero Map is a new initiative aimed at eliminating traffic fatalities and severe injuries on roadways.",
      "This map provides a comprehensive overview of traffic safety data across the country, helping to identify high-risk areas and implement targeted safety measures.",
      "The initiative is part of a broader Vision-Zero strategy, which has been adopted by various cities and countries to improve road safety through data-driven approaches and community engagement."
    ],
    "commentSummary": [
      "A comprehensive map of nearly every USA traffic death in the 21st century was created, with users suggesting a heatmap to identify fatality hotspots.",
      "Discussions highlighted data inaccuracies from the NHTSA's FARS database, including issues with police reports and state reporting systems.",
      "Users debated the impact of road design on safety, advocating for narrower streets and traffic calming measures, and discussed the balance between road safety and traffic efficiency."
    ],
    "points": 681,
    "commentCount": 383,
    "retryCount": 0,
    "time": 1721431006
  },
  {
    "id": 41016768,
    "title": "Researcher finds flaw in a16z website that exposed some company data",
    "originLink": "https://www.kibty.town/blog/a16z/",
    "originBody": "background i like to do this thing where i search twitter, looking for companies, and then try giving them a quick pentest. i've done a lot of my hacks this way and its more effective than you think it is. on this search, i use the \"Relevant People\" tab more often than you think, this is how i got to a16z crypto bullshit -> venture capital firms for crypto -> a16z crypto -> a16z the hack while looking into a16z, i did a usual subdomain scan and used tooling from lunchcat which does common checks on domains, scanning for secrets in js files, etc. in this search, i came across portfolio.a16z.com, a site that seems like a portfolio management tool for companies that are in a16z. while doing cursory checks like i usually do, lunchcat seemed to catch a AWS key referenced somewhere in the website. i confirmed this and what i saw in the js, was this. { MARKETPLACE_URL: \"\", DATABASE_URL: \"\", SALESFORCE_CLIENT_ID: \"\", SALESFORCE_SECURITY_TOKEN: \"\", npm_config_user_agent: \"\", SALESFORCE_CLIENT_SECRET: \"\", SALESFORCE_USERNAME: \"\", OKTA_CLIENT_ID: \"\", OKTA_CLIENT_SECRET: \"\", SESSION_SECRET: \"\", API_USERNAME: \"\", GOOGLE_CLIENT_ID_DEVELOPMENT: \"\", CLIENT_TOKEN_SECRET: \"\", GOOGLE_CLIENT_SECRET_DEVELOPMENT: \"\", AWS_BUCKET_NAME: \"\", npm_config_prefix: \"\", REACT_APP_SENTRY_DSN: \"\", AWS_BUCKET_TEAM_PAGES: \"\", MAILGUN_API_KEY: \"\", GOOGLE_CLIENT_ID: \"\", AWS_LOGO_BUCKET_URL: \"\", SALESFORCE_KEY: \"\", GOOGLE_CLIENT_SECRET: \"\", PAPERTRAIL_API_TOKEN: \"\", MAILGUN_PASSWORD: \"\", OKTA_CALLBACK_URL: \"\", SALESFORCE_PASSWORD: \"\", MAILGUN_USER: \"\", AWS_ACCESS_KEY_ID: \"\", PNPM_CONFIG_CACHE: \"\", AWS_SECRET_ACCESS_KEY: \"\", MAILGUN_DOMAIN: \"\", GOOGLE_CALLBACK_URL_DEVELOPMENT: \"\", API_PASSWORD: \"\", SENTRY_DSN: \"\", SALESFORCE_LOGIN_URL: \"\", COOKIE_SECRET: \"\", OKTA_DOMAIN: \"\", NODE_MODULES_CACHE: \"\", GOOGLE_CALLBACK_URL: \"\",, NODE_ENV: \"\", HEROKU_POSTGRESQL_CRIMSON_URL: \"\", TALENTPLACE_URL: \"\", } this was. horrifying, it was the entire process.env of a heroku instance, in the JS. put in dynamically. i did a quick valid look of the credentials and they didnt seem like fake credentials. they. were. real. and all someone had to do find them was go to the sources tab of inspect element. impact the compromised list of services: their database (containing PII) their AWS their salesforce (never checked, account may be limited) mailgun (arbitrary emails from a16z domains, and also could read older emails) ... and probably more reward a16z did not give me any bug bounty on this because of the fact i publicly reached out instead of trying to reach out privately. the only reason i did it this way was because: there was no available contact on their main site the email i could find engineering@a16z.com bounced my emails so, i dunno. imo this is unfair. related techcrunch article (lorenzo reached out to me seeing my tweet trying to get in contact with them and wrote a piece!): https://techcrunch.com/2024/07/18/researcher-finds-flaw-in-a16z-website-that-exposed-some-company-data/",
    "commentLink": "https://news.ycombinator.com/item?id=41016768",
    "commentBody": "Researcher finds flaw in a16z website that exposed some company data (kibty.town)461 points by udev4096 4 hours agohidepastfavorite187 comments ent101 4 hours agoWhen we released our open-source project[1], this hacker (Eva) pentested our project pretty extensively and was very professional in their disclosures. They didn't even ask for a bounty since we didn't have a program back then! Eva is an incredibly gifted hacker and a responsible one, a16z should treat them better. [1]: https://github.com/heyPuter/puter/ reply wouldbecouldbe 4 hours agoprevI made a similar mistake actually. We used a nodejs cms called apostrophecms that had an admin panel called global settings. We used that for managing api keys to our auth server. We only found out a few months in that it was outputted in the html source code. They did this so it was available to JS, of course it was in their docs. So not blaming them. We glossed over it. Annoyingly we paid a reasonable amount of money for a pen test with one of the big consultancy companies but they also didn’t see it. I ended up finding it and checking the logs seems like it wasn’t abused but it was shocking and a big leak reply mcfedr 1 hour agoparentI think I'd be looking for at least a refund on that pen test. I've never come across one that was anymore than a box ticking exercise. reply wouldbecouldbe 23 minutes agoparentprevEdit: don’t wanna blame apostrophe cms here, itnwas our multi tenant setup and misunderstanding of apostrophe me that lead to this situation reply samtho 2 hours agoparentprevWhy were you using a web-based content management system for secret management? reply wouldbecouldbe 31 minutes agorootparentIn apostrophecms you can easily create setting and content types with custom defined fields. There are quite a few good ideas in there. Those are mostly used on the node side of things, but often for convenience also shared to the front end. reply IshKebab 3 hours agoparentprev> it was in their docs. So not blaming them. We glossed over it. You should be blaming them. You can't excuse dangerous behaviour by documenting it. I feel like this lesson should be known by now. reply spookie 2 hours agorootparentWe always need to do our due diligence when using someone else's project. It's an open source project, available for free. If they weren't very clear in the docs is one thing, but it doesn't appear so. Anyway, we won't combat these types of shenanigans by assuming others did everything up to snuff. We gotta be more careful ourselves. reply suzzer99 2 hours agorootparentprevIf the panel setting was specifically for API keys, then yes, that's on apostrophecms. If it's just some kind of generic settings with name/value pairs, then it might make sense to expose those to the browser, and make that very clear up front. reply wouldbecouldbe 28 minutes agorootparentYeah you can define extra global settings extending the existing fields, so we used that for our multi tenancy solution. And is available on the node side of things as well as on the frontend. reply Drakim 4 hours agoprev> a16z did not give me any bug bounty on this because of the fact i publicly reached out instead of trying to reach out privately. the only reason i did it this way was because there was no available contact on their main site and the email i could find engineering@a16z.com bounced my emails That's a clever lifehack to save your company money, by not having any way to privately contact engineering all bug bounties will have to be reported publicly which means you don't need to pay anything. reply chefandy 3 hours agoparentAll sorts of cleverness going on there. I'll bet they saved a ton of money on development by lowballing people on fiverr or whatever they did, and indirectly they'll also save a ton on bookkeeping when a russian ransomware group effortlessly takes them for everything they have. reply HelloNurse 2 hours agorootparentEven more bookkeeping will be saved with lost business opportunities. reply bufferoverflow 3 hours agoparentprevBut it also teaches security researchers to sell that info next time instead of reporting. reply reducesuffering 1 hour agorootparentSeriously, if anyone from a16z is reading this, all you're doing is incentivizing the next exploit to be sold and used against you. reply hugoromano 3 hours agoparentprevThis what you expect from VCs. I always prefer to report these incidents to GDPR authorities if user data is leaked. Then they pay the fines and some get a criminal record. Money is something VCs “print” and manipulate. reply istinetz 2 hours agorootparent>Implying the Eu will actually do anything at all whatsoever upon reporting a gdpr issue >Money is something VCs “print” and manipulate. You wot m8 reply hugoromano 1 hour agorootparentIt is the member state authority, although EU GDPR is a Directive, is up to the member state. It doesn’t just apply to the EU, it can be UK ICO. reply paxys 3 hours agoparentprevThe company doesn't need a \"hack\" to not pay money. If they don't have a published bug bounty program then they owe nothing. They also have contact email addresses listed at the bottom of https://a16z.com/connect, which the researcher conveniently missed. They were looking for clout, not responsible disclosure. reply Ukv 2 hours agorootparent> not responsible disclosure. The researcher found an email address, tried it, it bounced, then reached out over Twitter with: > someone from @a16z get in touch, now. its bad. security related. https://x.com/xyz3va/status/1807330215955177937 That doesn't seem irresponsible to me. Sure they could have searched the bottom of a connect page for the office emails to try, but I don't see any significant issue with what they did instead. reply paxys 1 hour agorootparent\"an\" email address, not the one on their contact page. reply Ukv 1 hour agorootparentThe email the researcher found (engineering) seems more appropriate than the office info emails (menlopark-info, ...) at the bottom of the Connect page (an actual \"contact\" page used to exist, but is now 404 with no redirect). I don't see anything irresponsible about trying engineering then reaching out over social media. reply mynameisvlad 3 hours agorootparentprevSo you’d rather researchers reach out to black hats with this information instead? Because that’s what this line of thinking leads to. It’s in everyone’s, especially the company’s, best interests to have a bug bounty and easily accessible security hotline. Expecting researchers to jump through hoops like contacting their offices’ front desks to get to security is absurd. reply paxys 2 hours agorootparent> So you’d rather researchers reach out to black hats with this information instead? That is pretty much what they did. Posting publicly about the vulnerability most certainly meant that every hacker in the world tried (and probably succeeded) at reproducing it, all before the company had enough time to act. reply Ukv 2 hours agorootparentAs far as I can tell, their tweet was just: > someone from @a16z get in touch, now. its bad. security related. https://x.com/xyz3va/status/1807330215955177937 If your email bounces, I think reaching out over social media is reasonable for a fast response. reply mynameisvlad 2 hours agorootparentprevSo you’d rather this happen? That is the question I asked. Because this is explicitly what happens when a company doesn’t have a good process for accepting and responding to exploits. The onus should entirely be on the company to invite researchers to find and report exploits in a responsible way. They are the ones at risk of losing millions of dollars over an exploit. reply pavel_lishin 2 hours agorootparentprevThey didn't post publicly about the vulnerability; they reached out via twitter to tell them that they had one, without giving any details about it whatsoever. reply paxys 2 hours agorootparentTelling everyone that there's a vulnerability is usually as bad as providing detailed steps. No one was looking, and now you've pointed them in the right direction. reply pavel_lishin 2 hours agorootparent> No one was looking It's a16z, not Grandpappy's Model Railroad Museum Showcase (\"Come see a photo of the tiniest steam wagon in Sheboygan!\"). reply internetter 2 hours agorootparentprevwhat do you want them to do? nothing? we've already established that they tried to make contact. reply paxys 1 hour agorootparentHow about - go to the company's contact page, look at the email address there, and use that? reply _a_a_a_ 1 hour agorootparentprevNo it isn't. I flagged you for talking tripe. Honestly, HN seems to have an infestation of plonkers. reply xyzeva 2 hours agorootparentprevi think you're missing the fact that that indeed is not a security email, and the engineering/security email i found bounced. i had no ill intentions. stop pretending i did. reply jmholla 2 hours agorootparentprev> They also have contact email addresses listed at the bottom of https://a16z.com/connect, which the researcher conveniently missed. They have those now. Do we know they did when the researcher tried to reach out? Edit: I decided to take a look at it myself. It does seem that that was available on June 3rd of this year [0]. (You'll have to look at the source since the archive doesn't do their animations.) It seems to be available on previous snapshots as well [1]. [0]: https://web.archive.org/web/20240603210532/https://a16z.com/... [1]: https://web.archive.org/web/20240000000000*/https://a16z.com... [0]: https://web.archive.org/web/20240603210532/https://a16z.com/... reply leononame 2 hours agorootparentprevAm I blind? I don't seem to find the email address at all on that page reply consp 2 hours agorootparentOnly thing I can find are office mails, which looks more like a trashbin than mail which would respond. Also not where I'd look for a contact mail. They seem to only want you to connect via social media (which is a poor choice for primary contact IMO). reply lawgimenez 3 hours agorootparentprevI did the same thing with OP years ago, I tried to contact in every way possible the dev team of the largest telecom company in my country. All channels were ignored, so I have to resort to contacting our government agencies. Luckily, one agency replied to me and had one of the devs contacted me. For this hassle I was only paid $50. You have no idea the effort we go to report this things. So I quit bug hunting after that. I mean, a16z should be very grateful this got reported by an honest hunter regardless of the means it was reported. reply dvdkon 11 minutes agorootparentI stumbled upon a big vulnerability in an unnamed Czech ministry's web apps around January. It's now June and after trying the appropriate support email, the official \"snail mail but digital\", and calling various people's office landlines (thankfully they publish those in the org chart), it might get fixed this month. If there is a next time, maybe I'll try convincing the cybersecurity bureau to take my vulnerability reports instead. reply idontknowtech 1 hour agorootparentprevI'm generally sympathetic to what you're saying, but I also detest a16z and Horowitz personally for being the epitome of \"software guy decides he's expert at everything now\" and his role in the crypto bubble. Should the hacker have tried more? Sure, maybe. Do I really care? Definitely not reply rvnx 3 hours agorootparentprevLet's imagine your backpack is open. It's polite to say thanks if someone informs you that you accidentally left your backpack open. But in no way you are supposed to give them anything. Even further, some people take precious things from your backpack (trying to exploit the issue) and then come back to you asking for money; claiming they are nice people. This is non-sense. reply tomrod 3 hours agorootparent... Did they actually steal anything or take advantage, or just touch the bag to make sure it wasn't fake? Seems more of the latter, and your analogy falls flat when the bag carrier contains other people's pii. reply IshKebab 3 hours agorootparentprevTerrible analogy. This is more like someone returning your wallet full of cash, on live TV. You aren't legally obligated to give them anything, but it sure is a dick move not to and good luck getting your wallet back next time you drop it if you don't. reply largbae 2 hours agorootparentAlso the wallet had \"please return me, cash reward\" written on it. (Bug bounty advertised) reply abejfehr 3 hours agorootparentprevWhy will giving someone a cash reward mean you have a better chance of getting your wallet back in the future? reply pixl97 2 hours agorootparentYou're using the wrong line of thought on the analogy here. The value of the wallet is not the cash you'd directly lose inside of it. The value is getting your ID and cards back without them being copied by someone else, along with any other identifying information. The value of having and up front and easy to use bug bounty system is it's easier to use then selling it off to some blackhats (hopefully). Those blackhats may otherwise scrape all your s3 buckets or somehow otherwise run up a zillion dollars of charges over a holiday with your keys. Being cheap gets expensive. reply IshKebab 3 hours agorootparentprevBecause the next person will know there's a good chance you'll give them a cash reward, and that will tip the \"immorally take all the cash\" vs \"return it and hope for a reward\" balance more in favour of it being returned. I would have thought that was completely obvious so maybe that's not what you were asking? (On the other hand this is HN...) reply nox101 2 hours agorootparentThe places you're most likely to get your wallet back in the world are the places you're also less likely to get a reward. The reward for returning a wallet is knowing you're doing your part to make the place you live in a nice place to live. reply spencerflem 1 hour agorootparentDoing free work for A16Z or any of the awful companies ruining our world is not helping make anything better. reply mananaysiempre 2 hours agorootparentprevIt’s just that the analogy breaks down a bit. It’s fair to say a dropped wallet in a city is a one-shot game—it’s reasonable to expect neither the participants nor their acquaintances will ever encounter each other again; whereas a security vulnerability is closer to a repeated one—it’s a fairly small world. (Some kind of neighbourly behaviour would work better here, but then again, it’s more difficult to find a universal experience of that kind.) I didn’t misunderstand this, but perhaps GP did?.. reply YeahThisIsMe 3 hours agorootparentprev>You aren't legally obligated to give them anything, Acktchually, depending on where you live, you might be. reply rdedev 3 hours agorootparentprevIt's not the same. Figuring out a bagpack is open takes no effort. Finding a backdoor takes a lot of effort. reply TheRealPomax 2 hours agorootparentNot when you find it on first \"inspect element\". That really is the equivalent of looking through someone's window and seeing their bank information and credits cards just lying in full view of anyone who'd look in. reply nlh 3 hours agoparentprevCounterpoint: OP is a security researcher and couldn’t find a single human email address at one of the most well-known VC firms on the planet? LinkedIn? Twitter? Facebook friends? Come on. They’re not hard to reach if one really wants to. (Note: I still think A16Z should have paid them.) reply mynameisvlad 3 hours agorootparentWhy should it be an onus on the researcher to find this information? It should be plainly provided in the first place. Someone shouldn’t have to jump through hoops to help the company secure its resources. That is not how this works. reply dmix 3 hours agorootparentTrying more than one email is not jumping through hoops when it's one of the worst possible vulnerabilities hitting all of their databases/platforms. Being a research means being an adult and having a basic level of responsibility. Just like being a gun owner, it's a powerful tool that needs to be treated with utmost respect. A lot of pentesters are just kids who are angry at the world and the poor state of security, which I get, but it's not a huge barrier to try a bit more. He would have been rewarded if he did. reply mynameisvlad 2 hours agorootparentA researcher should not have to “try different emails”. Period. There should be a clearly disclosed email provided by the company to report such issues. Very obviously plastered. Or just use the standard abuse@, security@, infosec@, etc. It is by far in the company’s best interests for this to happen because the alternative is public disclosure or disclosure to black hats instead. Anything more is jumping through hoops. It should not be the researcher’s responsibility or burden to go out of their way to help a company that hasn’t done the bare minimum to welcome white hats helping them secure their own systems. reply dmix 2 hours agorootparentYes of course company's should do that, but in the real world a lot of companies don't think to do that, especially a marketing site for a VC firm. Any dev knows what it's like having a million responsibilities, a lot of things get put on TODO lists that never get completed. Them being owned by a wealthy company doesnt mean they have a huge dev team running 247 to handle this stuff. Which is probably why such a obvious failure even happened... Security researchers get high and mighty extremely quickly, which is immature IMO. reply dghlsakjg 1 hour agorootparentThe security researcher in this case worked for free to find a hole in their security, reached out via a provided email address, had that bounce, so then chose to reach out via a different messaging system to let them know that there was an issue. ALL OF THIS WAS UNPAID. They have 0 or less responsibility to this firm. The researcher was doing them a huge favor. > Security researchers get high and mighty extremely quickly, which is immature IMO. Immature would have been not trying to responsibly disclose this, or disclosing the hole before it was patched. reply pixl97 2 hours agorootparentprevWTF is this thinking? >Any dev knows what it's like having a million responsibilities, Any airplane mechanic has a million responsibilities, and if they are not followed people fucking die. Maybe software devs should step up and take a little responsibility for their lack of action that can have consequences for their users. Security researchers owe you nothing. If you make the path of least resistance selling sploits to blackhat groups the world will be a worse place. reply TheRealPomax 2 hours agorootparentprevAlright then: you go to Andreessen Horowitz's website[1] and see if you can find a SINGLE email address in any of the normal places a business would list the (not-social-media) contact information. Because they did their damnedest to make sure you won't find any. [1] https://a16z.com/ reply dmix 2 hours agorootparentI already linked to them in my comment below Click nav click “how to connect with us” -> https://a16z.com/connect/ See 4 emails at the bottom for each office See 4 links to social media pages where every single one has DMs open Wait at least a couple business days to see if anyone replies, if no one does or it’s not being taken seriously then you can announce it publicly on social media you found something but can’t reach them reply mynameisvlad 2 hours agorootparent> Huge effort, I know Okay. There’s 4 front office emails and 4 social media accounts, both presumably manned by non-technical folks. So now you have to go back and forth just to get routed to the right place. Which may not even happen if this is the first time that employee handled a security incident. You’re making it sound like sending the email or DM is the end of the work. That is usually far from the case. reply TheRealPomax 1 hour agorootparentprevEmailing an office manager with a company security issue would be incredibly irresponsible. They're in charge of managing the physical office and are about as \"outside\" as you can get in a company while still being employed by that company. reply nlh 3 hours agorootparentprevI don't think the onus should be on the researcher, and I think A16Z should have paid them. But if they actually wanted to get in touch, I'm just saying they could have. If they're putting the effort into vuln scanning the site, they can also put in the effort to get in touch like a professional. You could just as easily say \"why should the onus be on the researcher to find vulnerabilities when it's A16Z's job to secure their own site\". The researcher is in this to find holes and make a few bucks (which is fine!). The job is complete when you get in touch. reply dghlsakjg 1 hour agorootparent> If they're putting the effort into vuln scanning the site, they can also put in the effort to get in touch like a professional. They did. They emailed, and when that was bounced, they used a different medium to reach out. Twitter is a place that many companies actively engage with the public. > The job is complete when you get in touch. They got in touch. If A16Z aren't going to respond to people via email, but they do on twitter, they don't get to decide that twitter isn't a viable communication platform. reply mynameisvlad 2 hours agorootparentprev> You could just as easily say \"why should the onus be on the researcher to find vulnerabilities when it's A16Z's job to secure their own site\". The researcher is in this to find holes and make a few bucks (which is fine!). The job is complete when you get in touch. Presumably, the company wants to be as secure as possible. It’s in their best interest to make this process as painless as possible. A security researcher has many options for what to do with a found exploit, some far less moral than others. The company has very few, relatively. They are the ones that are limited and therefore should be doing everything in their power to ensure the best outcome, a responsible disclosure that is fixed as quickly as possible. The best way to ensure they do this is to provide an obvious, easy to find avenue for these things. This includes reasonable, well-displayed emails (or using something like a standard abuse@, etc) and a bug bounty. Simply put, the company is the one that should be going out of their way or else they will just have researchers either disclosing it publicly or selling the exploit for likely far more money than a bug bounty. reply nlh 2 hours agorootparentI understand where you're coming from, but you're using \"should\" a lot. Companies should do a lot of things! They should make their sites secure. They should have a formal bug bounty program. They should have security@ and engineering@ and lots of other emails easily visible. We agree. But many don't. And a lot of things in the business world are not as they should be. And in this real world of imperfection, others sometimes need to put in effort (and be paid for that effort) to make up for the failings of companies. This is one of those cases of imperfection. reply mynameisvlad 2 hours agorootparentOf course I’m using “should” a lot. Because “should” clearly didn’t happen. That doesn’t change anything. Just because a company has shitty security reporting practices doesn’t suddenly mean the onus is on the researcher to do the company’s job. reply fanf2 2 hours agorootparentprevThey said they got in contact via Twitter, but a16z didn’t like that. reply asopd 3 hours agorootparentprevExactly, if he even just browsed their website a bit he'd have stumbled across loads of email addresses that could have been a useful point of contact. reply dmix 3 hours agorootparentIt’s more fun getting attention by doing it publicly and being the victim (security researchers love hitting the 'nobody respects us' button) than putting basic effort in. A single email bouncing is frustrating of course, but he then posted that an easily found vulnerability existed on Twitter, while a16z: - has a contact page page https://a16z.com/connect/ with 4x emails to their offices at the bottom (despite claims the main site had no other emails) - links to their Twitter where DMs are open https://x.com/a16z same with instagram, FB, and linkedin, all open it would be easy to just email all of them at once and waiting a couple days to see if it gets escalated. reply asopd 2 hours agorootparentFunny how we're both being downvoted just for pointing out inconvenient facts. reply xyst 3 hours agoprevWhen I create a new service and add LetsEncrypt cert to server via ACME. I immediately see logs filled with junk, obviously bots searching for shitty defaults that devs might leave open. I have even seen requests for the process env file lol. How was such vuln not found and abused in this case? a16z is very lucky or maybe it was abused and not disclosed. Researcher or bored person with a kind heart/white hat hacker mindset is the first to reach out. a16z should be fined heavily unfortunately there is no legal framework for this type of negligence reply Quarrel 3 hours agoparent> How was such vuln not found and abused in this case? Maybe it was.. There might have been more value in leaving this one open than just screwing with them. reply xyst 4 hours agoprevwhen companies say they are “hacked”, it’s now a corporate term for “we were negligent in securing important credentials, but please shift blame to this no-name entity we called a ‘hacker’” reply miki123211 3 hours agoparentIf you accidentally leave your front door wide open and somebody steals all your stuff, you'll also say that you were robbed. There might be a legal distinction between \"breaking and entering\", \"burglary\", \"trespassing\" etc, and in a legal sense, whether the front door was open might have some impact on whether the act was illegal or not and what the consequences are, but in colloquial usage, you've still been robbed. reply svnt 10 minutes agorootparentA website is not a house. It is nothing like a house. There is no front door. There is no lock. There is no expectation of privacy. There are only things you can access and things you cannot. There is nothing inappropriate about trying to open the bathroom window from the outside. If I wanted to try to use such a weak analogy, the analogy to hacked is not robbed. You were only robbed if content was removed and exclusively held by someone else, which in the security world we call a ransom. You can see how quickly this breaks down. reply malf 3 hours agorootparentprevIf I leave other people’s stuff that I promised to take care of on the street and it gets stolen, I would be to blame. reply s1artibartfast 3 hours agorootparentblame isn't mutually exclusive. you can still blame the person that stole it too! reply rootusrootus 3 hours agorootparentprev> might have some impact on whether the act was illegal or not Only the burglary, trespassing, or B&E parts. Theft is still theft even if you leave your doors unlocked and/or open. reply bobmcnamara 3 hours agorootparentprevMore like complaining when your teenager takes a break from mowing on trash day and leaves the mower next to the trash and someone takes it. reply oasisbob 2 hours agorootparentprevWell, other legal distinctions aside, robbery is taking things by threat of force. If someone doesn't know they've been a victim of larceny until later, it wasn't a robbery. reply cromulent 2 hours agorootparentprevGood analogy, from a personal perspective. In this case, a person was yelling through the front door \"Your door is wide open!\" and no-one was listening. For a 42B AUM company, at a time where running an IT operation means \"use CrowdStrike so that you pass audits\", leaving the front door open all night should get you fired, regardless of whether you blame hackers or not. reply crngefest 3 hours agorootparentprevIf you put all your stuff on your front porch with a sign “please take what you want” and it’s all gone the next day - then you can’t say you were robbed. I think this is a more apt analogy to what az16 did here reply bee_rider 3 hours agorootparentIMO these sorts of analogies to houses and porches don’t really work because there are just different cultural norms between websites and porches. If there were a convention of leaving stuff on your porch to donate it, and a general assumption that when people left stuff on their porch it was up for grabs, somebody started storing their groceries there, and they were taken… they would just be stupid and not sympathetic. If somebody just moved to a neighborhood where this was tradition and didn’t know about it, they would rightly be a little bit annoyed when the groceries they stored on their porch were taken, but really they only have themselves to blame for not understanding the local conventions. If somebody opens up a storage company and then just put all the customers’ stuff on one of these porches, they are just dangerously, unethically incompetent. Even if there isn’t a convention of taking stuff from porches, actually. Because there are also armed gangs (nation-states) that go check out people’s porches for secrets. reply sparky_z 3 hours agorootparentprevThere's no analog for the sign. You just put it in because without it your scenario still feels like theft (because it is) and you end up arguing against your own point. reply crngefest 3 hours agorootparentThat is fair enough, I guess it’s not a great analogy overall. But IMHO it’s hard to feel to bad for someone (az16 in this case) who handles their arguably most valuable goods in such a manner and gets robbed. reply qup 3 hours agorootparentprevMore like if they kept their wallets in an open basket on the porch. It's not an invitation to take it, it's just really stupid. reply crngefest 3 hours agorootparentYes that would have been a much better analogy. reply rblatz 3 hours agorootparentprevUsing those credentials is still a violation of the he CFAA, no reasonable person would think they were invited to access the systems protected by those credentials. reply pixl97 2 hours agorootparentYea, I'm sure the Russian/China/NK/Iran hackers are deeply afraid of the CFAA, you got them shaking dude (and vice versa when someone in the US hacks one of their sites). The particular problem here is we think of the crime on the web in a civil/criminal manner... \"People should just follow the law or be punished for a crime\". This is not the internet. Regardless of what you think about the internet, it is an international war zone. If you leave the hatch of a tank open and a drone blows it up, that was you being stupid. If you leave an ammunition truck unguarded and the enemy takes it, again, that is you being stupid. History will look back and say WWIII started on the web, but as of now it seems a huge number of people are in denial about it. reply rblatz 1 hour agorootparentNone of this at all applies to this thread. It’s true, but also irrelevant to this discussion being had. reply pixl97 1 hour agorootparentAll of this applies to this thread. Do you cultivate vines with fruit, or do you cultivate brambles and eat thorns? Remember white hats don't need to exist. Black hats will exist by the very nature they are parasitic and thrive where exploits exist. We can either have a community that warns you that \"Hey, the stuff on your porch is going to get stolen\" or we can have a community that calls their buddy when they see some stuff fresh for the taking. A huge portion these discussions under this article are people arguing the minutia of a puddle in the lawn while a 10 meter high tsunami is rushing their way. reply Havoc 4 hours agoprevPretty shitty to not even give a token amount bounty for such a broad hole reply spyspy 3 hours agoparentThe next time someone finds their keys, they're going to find this article and commit them to a public github repo instead... reply Deathmax 3 hours agorootparentYou don't want to push secrets in their raw form on GitHub, secret scanning would disable keys from supported providers. reply pixl97 2 hours agorootparentYea, they aren't going up on GH, they are going up on sketchy-site . ru reply fragmede 1 hour agorootparentprevthat's the point reply ai4ever 4 hours agoprevthey are busy writing a giant \"architecture of generative AI\" whitepaper. give them a pause, they are dreaming a future agentic world of half-assed chatbots. while the world burns with botched software updates. reply xyst 3 hours agoparentworld is already burning with effects of climate change. botched software updates on a Friday is just the chef’s kiss reply stefan_ 2 hours agoparentprev> engineering@a16z.com bounced my emails No surprise there. reply cj 3 hours agoprevIf you could actually access their Salesforce instance, that would be very nerve wracking for founders, since usually Salesforce, etc, logs emails which may continue unannounced fundraising plans or M&A plans that haven’t been shared externally by portfolio company founders. reply rvnx 3 hours agoparentCollecting the keys from a public source-code of a web page is legal (and can be safely reported). Using these keys to access unauthorized systems is a crime. This is a major difference. reply pixl97 2 hours agorootparentOh no CRIME! Thank goodness that something being a crime stops people from committing them. Thank goodness the internet isn't an international operation filled with nation state level actors and questionable companies running data gathering operations from places they cannot be touched. Always assume your data has been stolen by an assailant in a place that's only reachable by launching nukes at them. Also assume there is some competitor on the other side of the world now using your data against you. Please stop treating data theft like Barney Fife level candy store theft. A huge portion of the time even if you know the name of the exact person who did it, there isn't going to be shit you can do about it. reply mcfedr 1 hour agorootparentprevHow can it possibly be a crime? They literally gave the keys to everyone who accessed their website reply davidgay 41 minutes agorootparentYou (unintentionally) drop your house key in front of your door. Now we can all freely enter your house! It can't be trespassing with the key sitting right there, can it? reply rvnx 4 minutes agorootparentTotally agree, and if you think like that, then a SQL injection is just a public entry-point ¯\\_(ツ)_/¯ reply devin 2 hours agorootparentprevParent comment never suggested it was legal. They said it would be bad if this info was in their SalesForce and they leaked the key, which they did. reply Quarrel 3 hours agoparentprevIt would also be pretty damaging if it includes their LPs. reply j-bos 3 hours agoprevThe fact that this VC firm didn't provide bug bounty for such a gaping hole does not instill trust. reply renewiltord 2 hours agoparentYes, if they can’t do web development what does that say about their ability to deploy capital? reply pavel_lishin 2 hours agorootparentIf my endodontist can't rebuild a car engine, what does that say about his ability to perform a root canal? Turns out, not much. reply kjkjadksj 1 hour agorootparentNot a great analogy. Its more like if your endodontist hired a secretary who leaves the medical records unlocked, do you really trust them to be up to date with modern dental sensibilities when the rest of their office is ran so carelessly? reply ko_pivot 4 hours agoprevSincere question: how do you actually make this mistake while having the skills to build a web app of this complexity level? All the frontend and full stack frameworks that I’m familiar with try pretty hard to stop you. reply jimkoen 3 hours agoparent> how do you actually make this mistake while having the skills to build a web app of this complexity level? By not building this yourself and instead outsourcing the work to India, to people that work for 4.00$/h And I'm not blaming the person that has to work for this little cash for delivering shoddy work like this. reply krig 3 hours agoparentprevI’ve seen people make exactly this mistake with Next.js. IMO React server components is a fantastic tool for losing track of what’s exposed client side and what isn’t. reply duggan 3 hours agorootparentNext.js makes you prefix env vars with NEXT_PUBLIC_ if you want them to be available client side, and Vercel has warning flags around it when you paste in those keys. It's obviously not foolproof, but it's a good effort. reply krig 3 hours agorootparentThat’s env vars, but not actual variables - it’s really easy (if you are not actively context aware) to f.ex. pass a ”user” object from a server context into a client component and expose passwords etc to the client side. reply duggan 3 hours agorootparentThat's a fair point! It definitely feels easier to make that mistake, and anything where context and discipline is required is a good candidate for making some horrifying blunders :) reply leerob 2 hours agorootparentprevIf you add `import “server-only”` to the file, it will fail to compile if you to use it on the client. React also has more fine grained options where you can “taint” objects (yes that’s the real name). reply krig 49 minutes agorootparentYeah, the problem is that these mitigations require the developer to be context aware, ”server-only” only saves you in the positive case where you correctly tagged your sensitive code as such. The default case is to expose anything without asking. I have also seen developers simply marking everything as ”use client” because then things ”just work” and the compiler stops complaining about useState in a server context etc. reply pavel_lishin 3 hours agoparentprevIt only takes a single mistake. A little tired because you didn't sleep well, or worried about a relative in the hospital, or you stubbed your toe that morning and it's distracting... and whoops. reply crngefest 3 hours agorootparentWhoops I accidentally exposed all API keys ever to the public. No really this is unacceptable for a professional, it’s even bad for an amateur. If your processes are so insecure that a little tired breaks your whole company you done goofed. reply devin 2 hours agorootparentYes, the answer must be additional processes and procedures. That way, you’ll never make a mistake! /s Also bizarre to frame this as “unacceptable behavior”, as if whoever is involved was in some way aware of their mistake and/or would say “this is acceptable behavior!” when confronted with it or something. reply crngefest 1 hour agorootparentGP framed leaking all your keys at something that happens when you are tired or distracted. This is unacceptable behaviour for a professional in my eyes. reply bee_rider 3 hours agorootparentprevPerhaps some processes should be put into place to make exposing the entire company into a multi-step failure? reply dpkirchner 1 hour agorootparentI've considered tracing outgoing responses from nginx/traefik/whatever to watch for known API keys. The difficulty would be identifying the keys amongst the noise. reply pavel_lishin 2 hours agorootparentprevPerhaps some already exist. But if they have five security processes that each has a 99% chance of catching a bug, that's still a 1-in-10,000 chance that something will slip through. And I'd wager that a16z has more than 10,000 \"components\" that goes through those processes. reply gumby 3 hours agoparentprevEver had a bug in code you wrote? reply mrcode007 2 hours agorootparentNot of this kind reply devin 2 hours agorootparentThat you’re aware of. reply mrcode007 1 hour agorootparentI come from security background and have been following best practices since 1997 so I’m pretty sure I have not made a blunder of this sort reply cedws 3 hours agoparentprevDon’t mistake complexity for intelligence. reply davidchang 3 hours agoparentprevmy guess is internal tool that wasn't expected to be exposed publicly. additionally, i didn't realize there are tools to automatically discover unreferenced subdomains like this. i would have just assumed security by obscurity reply duggan 2 hours agorootparentIf one person learns this lesson it's good. If it's on the public Internet, best to expect it will be found. Stick it behind an auth wall of some sort. I've put internal sites behind AWS ALB's plugged into an OIDC provider[1] (Google), which works well. 1: https://docs.aws.amazon.com/elasticloadbalancing/latest/appl... reply ndriscoll 3 hours agorootparentprevPresumably it's from certificate transparency logs. That's one reason I do not use TLS for my personal hosting. reply VTimofeenko 3 hours agorootparentLet's Encrypt allows issuing wildcards which is what quite a number of folks use for self-hosted services reply udev4096 2 hours agoprevThe HN mods changed the title to a less embarrassing one. Not surprised reply tux3 2 hours agoparentOh, my comment must have been too critical of a16z as well. I see it has been moved from top to way bottom without a score change. That's certainly one way to offer a response! reply davedx 3 hours agoprevMaybe they should have installed CrowdStrike reply avery17 3 hours agoparentCant get hacked if youre bluescreened. reply sourcecodeplz 50 minutes agoprevToo much javascript for everything (front & back) seems easy but for new developers it kind of blurs the lines between what should be on the server vs the client. reply kva 3 hours agoprevHopefully Martin Casado or one of the other awesome open source folks from a16z will take a look at this and make the person whole! reply hedora 3 hours agoprevNeko! https://en.wikipedia.org/wiki/Neko_%28software%29 The Wikipedia article is missing the implementation in the article. Too bad they don't pay bounties. ^ ^ 0 - * - reply JCharante 4 hours agoprevI agree that the bounty outcome is unfair. reply janjones 1 hour agoprevhttps://archive.is/Wk6OO reply cromulent 2 hours agoprevIt's really hard to generate \"all due respect\" for a16z. reply Capricorn2481 3 hours agoprevFrom the techcrunch article: > “On June 30th, a16z addressed a misconfiguration in a web app that is used for the specific use case of updating publicly available information on our website such as company logos and social media profiles. The issue was resolved quickly and no sensitive data was compromised,” What the fuck is this? They are blatantly lying here. There was a lot of sensitive data compromised. Anyone who inspected the site could have had access to everyones emails. reply localfirst 3 hours agoprev> a16z did not give me any bug bounty on this because of the fact i publicly reached out instead of trying to reach out privately. I just don't understand this petty attitude. This almost guarantees next time somebody that finds vulnerability with a16z or any of its companies to seek black market rewards that will do far more damage. This is just like when KakaoTalk refused to payout bug bounty because you had to be a Korean citizen which ended up causing more vulnerabilities to be discovered in the wild. Companies and billionaires reading this, please don't be petty like Andreesen. Guy went from a leader to a borderline security fraud artist. You don't want to be earning more ire from the public in the current political climate. It's dangerous. reply throw16z 1 hour agopreveven web3 could protect a16z ugh, thats very bad reply ilrwbwrkhv 3 hours agoprevCrypto bullshit - a16z pipeline is a great reflection of a16z as a firm. reply tux3 4 hours agoprev>a16z did not give me any bug bounty on this because of the fact i publicly reached out instead of trying to reach out privately. the only reason i did it this way was because: > there was no available contact on their main site > the email i could find engineering@a16z.com bounced my emails The age-old practice of screwing over security researchers over any possible technicality is still alive and well. Brings tears to my eyes. reply hpeter 2 hours agoparentIt only gets worse when the company that published their environment variables sues the security researchers for finding it. It happens. reply newyankee 2 hours agoparentprevAny legal basis to challenge this practice ? If a company claims that they pay bug bounties but use flimsy reasons like this to chicken out of seemingly genuine cases like these reply pixl97 2 hours agorootparentI'm guessing no, and even if their was they could make the litigation costs very high. The sad thing here is what has to happen is the data needs sold off to blackhats to the point that entire countries get pissed and start putting near draconian level regulations and fines against companies like this to get them to stop this insecure bullshit. reply Salgat 2 hours agoparentprevJust a heads up, another comment was posted here that shows right on their website's contact page a list of e-mails for contacting them. reply bdowling 3 hours agoparentprevnext [5 more] [flagged] gcbirzan 3 hours agorootparentActually, they posted that a vulnerability existed, https://x.com/xyz3va/status/1807330215955177937: > someone from @a16z get in touch, now. its bad. security related reply londons_explore 2 hours agorootparentSo they didn't disclose the bug publically... They simply disclosed that there was a bug. That isn't IMO disclosure. Nearly anything has a bug if you look hard enough. reply bdowling 2 hours agorootparentprevWhat's the relevant difference? reply Retr0id 3 hours agorootparentprevThat's not what happened at all, and the article you link doesn't say that. reply llmblockchain 3 hours agoprevIt's pretty shocking how many commenters are blaming the individual for not \"trying harder\" to find contact information. It's pretty clear a16z didn't want to pay anything or appreciate the disclosure at all. Finding random email addresses and sending them a notice would have gone no where other than spam folders. I get dozens of \"disclosures\" every week from mostly script kiddies that think my DKIM setting is somehow going to be the end of my business. My brain automatically ignores emails like it. reply mrcode007 2 hours agoparentI’m surprised there is almost no discussion about the severity of reputational damage caused by an extremely amateur bug not expected of a prominent VC firm reply llmblockchain 2 hours agorootparentYes... In my mind, there are three kinds of security bugs. 1. Caused by pure ignorance and completely avoidable (this bug). 2. Caused by subtle configurations, workflows, programming (mostly avoidable, secret scanning, security linters, code reviews, general intelligence, etc). This is where 99% of security bugs are. 3. Caused by a malicious actor aligning planets with a single intent to maximize their cause. You'll never stop these people (three letter agencies, state actors). edit: A must watch talk https://vimeo.com/95066828 reply altthrow24 1 hour agorootparentprevProbably because a16z reputation has already been quite tarnished in recent years. This is par for the course. People will still take their massive bags of money and name brand boost but \"these are smart, technical, 'making the world a better place' visionaries\" as opposed to wealth chasing bankers, has already run the gamut. See crypto, Clubhouse, \"it's time to build [not in my Atherton neighborhood]\", e/acc Nick Land manifesto, Trump '24 support, etc. reply nuz 3 hours agoprevI like lower case tweets and texts but lower case in articles like this is just ridiculous (and trying too hard to be cool) reply very_good_man 2 hours agoprevhow do I disable the cat following my cursor animation on your website? how insanely distracting reply neilv 2 hours agoparentuBlock Origin -> Dashboard -> My Filters -> add the line: ||www.kibty.town/files/js/oneko.js^$important reply quantified 3 hours agoprevIsn't it fairly easy to get an address like marca's? I'm sure anyone who is responsible for the place would make the connection to IT security. reply xg15 3 hours agoprev> i like to do this thing where i search twitter, looking for companies, and then try giving them a quick pentest. i've done a lot of my hacks this way and its more effective than you think it is. Ah yes, the classic surprise pentest by unappointed security researchers. I too, as the good samaritan that I am, like to stroll through my neighborhood and give all the cars and bikes I encounter a quick pentest, purely for the benefits of the owners of course. I remember there was an article \"the six dumbest ideas in computer security\" on HN a while ago, one of those was the mindset that \"hacking is cool\". I'm reminded a bit of this here. reply crngefest 3 hours agoparentWell it could be this person that is professional and does not sell all your data to North Korean ransomware gangs - or it could be the one that does. Which one do you prefer? reply Lvl999Noob 3 hours agorootparentI (we) would obviously prefer the professional person who is doing good for society. The problem is, this behaviour isn't good for them. I am not an expert or anything but from what I know, pentesting without explicit prior permissions can easily lead to huge lawsuits. I would rather that the careless people get their cars stolen than the good people all lose heart completely. reply crngefest 3 hours agorootparentSure there is no perfect solution here. I guess it’s a good idea to only pentest companies that do have a bug bounty program and an expressed interest in you pentesting. While I enjoyed the article that GP referenced and agreed with most thing I thought the “hacking bad” take was a bit off. reply fermisea 2 hours agorootparentprevOne thing is true about what you said: you're definitely not an expert. reply TechDebtDevin 3 hours agoparentprevDamn, maybe just go back to sleep and try waking up on the otherside of the bed. This is normal behavior for bug hunters and I don't think they're doing it because 'it's cool\". They do this for a living. reply i_am_jl 1 hour agoparentprev>I remember there was an article \"the six dumbest ideas in computer security\" on HN a while ago, one of those was the mindset that \"hacking is cool\". I'm reminded a bit of this here. Half of that post is unhinged nonsense. \"Hacking is Cool\" is listed right after a rant about pentesting being dumb because your software should just be designed to be secure. reply bdowling 2 hours agoparentprev> I too, as the good samaritan that I am, like to stroll through my neighborhood and give all the cars and bikes I encounter a quick pentest, purely for the benefits of the owners of course. In my neighborhood, \"security researchers\" can often be seen checking houses for vulnerabilities. During the day, it's usually a woman or a kid with a clipboard who knocks on front doors, checks for cameras, tests if the front door is locked, etc. I'm told they work with crews of men who will come back later to do a more thorough investigation when everyone is gone so as not to bother the homeowner. Every night, there are other \"security researchers\" who test all the doors of all the cars parked on the street and in driveways. If you leave your car door unlocked just once, you'll be informed about it the next morning! It's really something to live in these times! reply pavel_lishin 2 hours agorootparent\"These times\" have been around since house doors had locks. reply bdowling 2 hours agorootparentWhoosh reply asopd 2 hours agoparentprevHaving a curious look is alright but it's the \"beg bounty\" attitude that these researchers need to rein in. It's like the sponge-and-bucket guy washing your grimy windscreen without you asking while you wait at the lights, then demanding cash for it. Thanks but no thanks. reply jimkoen 3 hours agoparentprevAnd so you're just going to dismiss the modern reality of cybersecurity threats? \"What happened to the good old days when we could all leave our cars and homes unlocked..\" Yeah no. reply asopd 3 hours agoprevI'm surprised he didn't try harder to contact someone in the company privately. Surely any contact would have sufficed to at least try to get an introduction to their security team? If you browse their website there are loads of email addresses for various offices and divisions. reply hpen 3 hours agoprevWait, do hackers feel entitled to money for finding security holes, even if there was never any signal of such reward? reply hpen 2 hours agoparentHa my actual question was downvoted. I guess people are as entitled as they say. reply hpen 1 hour agorootparentActually, I think entitlement is the wrong word. Maybe more like \"window washing panhandler who's upset because you don't give them money for their service\" reply paxys 2 hours agoprev [–] Stuff like this is what gives the entire security and white hat community a bad name. 1. \"Surprise pentests\" are illegal in the US and pretty much every jurisdiction in the world. If you are actively breaking into websites without a prior agreement, you are not doing anyone a favor. Save your efforts for companies that actually want you. 2. If the company doesn't have a published bug bounty program, they don't owe you anything. Yes they can still be nice and pay you, but they definitely won't if you disclose the vulnerability to the rest of the world without giving them a heads up and enough time to fix it. 3. \"Oh I couldn't find an email address\" is the worst excuse in the world. I found one after exactly 5 seconds of Googling (at the bottom of https://a16z.com/connect). And even otherwise there's Twitter, Instagram, LinkedIn and a hundred other ways to reach someone at the company if you really want to. This is classic case of clout chasing over responsible disclosure. reply pavel_lishin 2 hours agoparent [–] > If you are actively breaking into websites They viewed the source code. Despite what the governor of Missouri[1] thinks, that's not hacking. [1]: https://www.theverge.com/2021/12/31/22861188/missouri-govern... reply csmpltn 1 hour agorootparent [–] > They viewed the source code. No. \"i like to do this thing where i search twitter, looking for companies, and then try giving them a quick pentest\" \"the compromised list of services: their database (containing PII), their AWS, their salesforce (never checked, account may be limited), mailgun (arbitrary emails from a16z domains, and also could read older emails) ... and probably more\" By their own admission, this is a \"pentest\", and they were able to access a16z's \"database\" and ascertain that it contains PII. Amongst other services used by a16z. I'm not the one to judge whether they crossed any legal (or moral) lines though. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A security researcher discovered a vulnerability in a16z crypto's subdomain, revealing sensitive information such as AWS keys and database contents.",
      "The exposed credentials included access to AWS, Salesforce, Mailgun, and a database containing Personally Identifiable Information (PII).",
      "The researcher did not receive a bug bounty due to public disclosure after failing to find a private contact method, which they deemed unfair."
    ],
    "commentSummary": [
      "A researcher identified a vulnerability in the a16z website that exposed company data but received no bug bounty due to public disclosure after private contact attempts failed.",
      "The incident has ignited debates on responsible disclosure, company security practices, and the ethics of unpaid bug hunting.",
      "Criticism has been directed at a16z for not compensating the researcher, highlighting the responsibilities of both companies and security researchers in handling such discoveries."
    ],
    "points": 461,
    "commentCount": 187,
    "retryCount": 0,
    "time": 1721485998
  },
  {
    "id": 41013004,
    "title": "Garage: Open-Source Distributed Object Storage",
    "originLink": "https://garagehq.deuxfleurs.fr/",
    "originBody": "Overview Docs Blog Download Garage An open-source distributed object storage service tailored for self-hosting Download Get Started Host a Website Store Media Backup Target Made for redundancy Each chunk of data is replicated in 3 zones Zone (multiple servers) Chunks of data Our Goals We made it lightweight and kept the efficiency in mind: Self-contained We ship a single dependency-free binary that runs on all Linux distributions Fast to deploy, safe to operate We are sysadmins, we know the value of operator-friendly software Deploy everywhere on every machine We do not have a dedicated backbone, and neither do you, so we made software that run over the Internet across multiple datacenters Highly resilient to network failures, network latency, disk failures, sysadmin failures Keeping requirements low We worked hard to keep requirements as low as possible: CPU Any x86_64 CPU from the last 10 years, ARMv7 or ARMv8 RAM 1 GB Disk space At least 16 GB Network 200 ms or less, 50 Mbps or more Heterogeneous hardware Build a cluster with whatever second-hand machines are available Data resiliency for everyone We built Garage to suit your existing infrastructure: Garage implements the Amazon S3 API and thus is already compatible with many applications. Standing on the shoulders of giants Garage leverages insights from recent research in distributed systems: Dynamo: Amazon’s Highly Available Key-value Store by DeCandia et al. Conflict-Free Replicated Data Types by Shapiro et al. Maglev: A Fast and Reliable Software Network Load Balancer by Eisenbud et al. Sponsors and funding Garage has received funding from NGI POINTER (3 full-time employees for one year, in 2021-2022), and from NLnet / NGI0 Entrust (1 full-time employee for one year, in 2023-2024). If you want to participate in funding Garage development, either through donation or support contract, please get in touch with us. This project has received funding from the European Union's Horizon 2021 research and innovation programme within the framework of the NGI-POINTER Project funded under grant agreement N° 871528. This project has received funding from the NGI0 Entrust Fund, a fund established by NLnet with financial support from the European Commission's Next Generation Internet programme, under the aegis of DG Communications Networks, Content and Technology under grant agreement No 101069594. Search (alt + S) (Esc) Built with Zola, powered by Garage, hosted by Deuxfleurs",
    "commentLink": "https://news.ycombinator.com/item?id=41013004",
    "commentBody": "Garage: Open-Source Distributed Object Storage (deuxfleurs.fr)389 points by n3t 18 hours agohidepastfavorite104 comments makkesk8 10 hours agoWe moved over to garage after running minio in production with about ~2PB after about 2 years of headache. Minio does not deal with small files very well, rightfully so, since they don't keep a separate index of the files other than straight on disk. While ssd's can mask this issue to some extent, spinning rust, not so much. And speaking of replication, this just works... Minio's approach even with synchronous mode turned on, tends to fall behind, and again small files will pretty much break it all together. We saw about 20-30x performance gain overall after moving to garage for our specific use case. reply sandGorgon 9 hours agoparentquick question for advice - we have been evaluating minio for a in-house deployed storage for ML data. this is financial data which we have to comply on a crap ton of regulations. so we wanted lots of compliance features - like access logs, access approvals, short lived (time bound) accesses, etc etc. how would you compare garage vs minio on that front ? reply withinboredom 6 hours agorootparentYou will probably put a proxy in front of it, so do your audit logging there (nginx ingress mirror mode works pretty good for that) reply mdaniel 2 hours agorootparentAs a competing theory, since both Minio and Garage are open source, if it were my stack I'd patch them to log with the granularity one wished since in my mental model the system of record will always have more information than a simple HTTP proxy in front of them Plus, in the spirit of open source, it's very likely that if one person has this need then others have this need, too, and thus the whole ecosystem grows versus everyone having one more point of failure in the HTTP traversal reply withinboredom 2 hours agorootparentHmm... maybe??? If you have a central audit log, what is the probability that whatever gets implemented in all the open (and closed) source projects will be compatible? reply j-pb 11 hours agoprevWhat I'm really missing in this space is something like this for content addressed blob storage. I feel like a lot of complexity and performance overhead could be reduced if you only store immutable blobs under their hash (e.g Blake3). Combined with a soft delete this would make all operations idempotent, blobs trivially cacheable, and all state a CRDT/monotonically mergeable/coordination free. There is stuff like IPFS in the large, but I want this for local deployments as a S3 replacement, when the metadata is stored elsewhere like git or a database. reply ianopolous 3 hours agoparentThat's how we use S3 in Peergos (built on IPFS). You can get S3 to verify the sha256 of a block on write and reject the write if it doesn't match. This means many mutually untrusting users can all write to the same bucket at the same time with no possibility for conflict. We talk about this more here: https://peergos.org/posts/direct-s3 reply amluto 6 hours agoparentprevI would settle for first-class support for object hashes. Let an object have metadata, available in the inventory, that gives zero or more hashes of the data. SHA256, some Blake family hash, and at least one decent tree hash should be supported. There should be a way to ask the store to add a hash to an existing object, and it should work on multipart objects. IOW I would settle for content verification even without content addressing. S3 has an extremely half-hearted implementation of this for “integrity”. reply the_duke 11 hours agoparentprevGarage splis the data into chunks for deduplication, so it basically already does content addressed storage under the hood.. They probably don't expose it publicly though. reply j-pb 6 hours agorootparentYeah, and as far as I understood they use the key hash to address the overall object descriptor. So in theory using the hash of the file instead of the hash of the key should be a simple-ish change. Tbh I'm not sure if content aware chunking isn't a sirens call: - It sounds great on paper, but once you start storing encrypted (which you have to do if you want e2e encryption) or compressed blobs (e.g. images) it won't work anymore. - Ideally you would store things with enough fine grained blobs that blob-level deduplication would suffice. - Storing a blob across your cluster has additional compute, lookup, bookkeeping, and communication overhead, resulting in worse latency. Storing an object as a contiguous unit makes the cache/storage hierarchies happy and allows for optimisations like using `sendfile`. - Storing the blobs as a unit makes computational storage easier to implement, where instead of reading the blob and processing it, you would send a small WASM program to the storage server (or drive? https://semiconductor.samsung.com/us/ssd/smart-ssd/) and only receive the computation result back. reply khimaros 9 hours agoparentprevyou might be interested in https://github.com/perkeep/perkeep reply skinkestek 8 hours agorootparentPerkeep has (at least until last I checked it) the very interesting property of being completely impossible for me to make heads or tails of while also looking extremely interesting and useful. So in the hope of triggering someone to give me the missing link (maybe even a hyperlink) for me to understand it, here is a the situation: I'm a SW dev that also have done a lot of sysadmin work. Yes, I have managed to install it. And that is about it. There seems to be so many features there but I really really don't understand how I am supposed to use the product or the documentation for that matter. I could start an import of Twitter or something else an it kind of shows up. Same with anything else: photos etc. It clearly does something but it was impossible to understand what I am supposed to do next, both from the ui and also from the docs. reply breakingcups 7 hours agorootparentPerkeep is such a cool, interesting concept, but it seems like it's on life-support. If I'm not mistaken, it used to be funded by creator Brad Fitz, who could afford to hire a full-time developer on his Google salary, but that time has sadly passed. It suffers from having so many cool use-cases that it struggles to find a balance in presentation. reply mdaniel 2 hours agorootparentprevI was curious to see if I could help, and I wondered if you saw their mailing list? It seems to have some folks complaining about things they wish it did, which strangely enough is often a good indication of what it currently does There's also \"Show Parkeep\"-ish posts like this onewhere the user made their own Pocket implementation complete with original page snapshotting The thing that most stood out to me was the number of folks who wanted to use Parkeep to manage its own content AND serve as the metadata system of record for external content (think: an existing MP3 library owned by an inflexible media player such as iTunes). So between that and your \"import Twitter\" comment, it seems one of its current hurdles is that the use case one might have for a system like this needs to be \"all in\" otherwise it becomes the same problem as a removable USB drive for storing stuff: \"oh, damn, is that on my computer or on the external drive?\" reply tgulacsi 7 hours agorootparentprevBeside personal photo store, I use the storage part for file store at work (basically, indexing is off), with a simplifying wrapper for upload/download: github.com/tgulacsi/camproxy With the adaptive block hashing (varying block sizes), it beats gzip for compression. reply lockyc 7 hours agorootparentprevI agree 100% reply didntcheck 8 hours agorootparentprevOr some even older prior art (which I recall a Perkeep dev citing as an influence in a conference talk) http://doc.cat-v.org/plan_9/4th_edition/papers/venti/ https://en.wikipedia.org/wiki/Venti_(software) reply j-pb 6 hours agorootparentYeah, there are pleanty of dead and abandoned projects in this space. Maybe the concept is worthless without a tool for metadata management? Also I should probably have specified that by \"missing\" I mean, \"there is nothing well maintained and production grade\" ^^' reply j-pb 5 hours agorootparentprevYeah I've been following it on and off since it was camli-store. Maybe it tried to do too much at once and didn't focus on just the blob part enough, but I feel like it never really reached a coherent state and story. reply od0 5 hours agoparentprevTake a look at https://github.com/n0-computer/iroh Open source project written in Rust that uses BLAKE3 (and QUIC, which you mentioned in another comment) reply j-pb 4 hours agorootparentIt certainly has a lot of overlap and is a very interesting project, but like most projects in this space, I feel like it's already doing too much. I think that might be because many of these systems also try to be user facing? E.g. it tries to solve the \"mutability problem\" (having human readable identifiers point to changing blobs); there are blobs and collections and documents; there is a whole resolver system with their ticket stuff All of these things are interesting problems, that I'd definitely like to see solved some day, but I'd be more than happy with an \"S3 for blobs\" :D. reply rkunnamp 1 hour agoparentprevIPFS like \"coordination free\" local S3 replacement! Yes. That is badly needed. reply BageDevimo 10 hours agoparentprevHave you seen https://github.com/willbryant/verm? reply j-pb 6 hours agorootparentYeah, the subdirectories and mime-type seemed like an unnecessary complication. Also looks pretty dead. reply compressedgas 6 hours agoparentprevYou might also be interested in Tahoe-LAFS https://www.tahoe-lafs.org/ reply j-pb 6 hours agorootparentI get a > Trac detected an internal error: > IOError: [Errno 28] No space left on device So it looks like it is pretty dead like most projects in this space? reply diggan 5 hours agorootparentBecause the website seems to have a temporary issue, the project must be dead? Tahoe-LAFS seems alive and continues development, although it seems to not have seen as many updates in 2024 as previous years: https://github.com/tahoe-lafs/tahoe-lafs/graphs/contributors reply j-pb 5 hours agorootparentMore like based on the prior that all projects in that space arent' in the best of health. Thanks for the github link, that didn't pop up in my quick google search. reply ramses0 6 hours agoparentprevCheck also SeaweedFS, it has some interesting tradeoffs made, but I hear you with wanting some of the properties you're looking for. reply tempest_ 5 hours agorootparentI am using seaweed for a project right now. Some things to consider with seaweed. - It works pretty well, at least up to the 15B objects I am using it for. Running on 2 machines with about 300TB, (500 raw) storage on each. - The documentation, specifically with regards to operations like how to backup things, or different failure modes of the components can be sparse. - One example of the above is I spun up a second filer instance (which is supposed to sync automatically) which caused the master server to emit an error while it was syncing. The only way to know if it was working was watching the new filers storage slowly grow. - Seaweed has a pretty high bus factor, though the dev is pretty responsive and seems to accept PRs at a steady rate. reply singinwhale 10 hours agoparentprevSounds a little like Kademlia, the DHT implementation that BitTorrent uses. It's a distributed hash table where the value mapped to a hash is immutable after it is STOREd (at least in the implementations that I know) reply j-pb 6 hours agorootparentKademlia could certainly be a part of a solution to this, but it's a long road from the algorithm to the binary that you can start on a bunch of machines to get the service, e.g. something like SeaweedFS. BitTorrent might actually be the closest thing we have to this, but it is at the opposite spectrum of the latency -distributed axis. reply lima 5 hours agoparentprevThe RADOS K/V store is pretty close. Ceph is built on top of it but you can also use it as a standalone database. reply yencabulator 2 hours agorootparentNothing content-addressed in RADOS. It's just a key-value store with more powerful operations that get/put, and more in the strong consensus camp than the parents' request for coordination free things. (Disclaimer: ex-Ceph employee.) reply jiggawatts 6 hours agoparentprevSomething related that I've been thinking about is that there aren't many popular data storage systems out there that use HTTP/3 and/or gRPC for the lower latency. I don't just mean object storage, but database servers too. Recently I benchmarked the latency to some popular RPC, cache, and DB platforms and was shocked at how high the latency was. Every still talks about 1 ms as the latency floor, when it should be the ceiling. reply j-pb 5 hours agorootparentYeah QUIC would probably be a good protocol for such a system. Roundtrips are also expensive, ideally your client library would probably cache as much data as the local disk can hold. reply TechDebtDevin 17 hours agoprevSeaweedFS is great as well. https://github.com/seaweedfs/seaweedfs reply n_ary 12 hours agoparentTried this for my own homelab, either I misconfigured it or it consumes x2(linearly) memory(working) of the stored data. So, for example, if I put 1GB of data, seaweed would immediately consume 2GB of memory constantly! Edit: memory = RAM reply TechDebtDevin 11 hours agorootparentThat is odd. It likely has something to do with the index caching and how many replication volumes you configured. By default it indexes all file metadata in RAM (I think) but that wouldn't justify that type of memory usage. I've always used mostly default configurations in Docker Swarm, similar to this: https://github.com/cycneuramus/seaweedfs-docker-swarm/blob/m... reply crest 12 hours agorootparentprevAre you claiming that SeaweedFS requires twice as much RAM as the sum of the sizes of the stored objects? reply n_ary 9 hours agorootparentCorrect. I experimented by varying the data volume, it was linearly correlated by x2 of data volume. reply ddorian43 9 hours agorootparentCreate a reproducible issue in their github. The developer is very responsive. reply evanjrowley 15 hours agoparentprevLooks awesome. Been looking for some flexible self-hosted WebDAV solutions and SeaweedFS would be an interesting choice. reply computerfan494 17 hours agoprevI have used Garage for a long time. It's great, but the AWS sigv4 protocol for accessing it is just frustrating. Why can't I just send my API key as a header? I don't need the full AWS SDK to get and put files, and the AWS sigv4 is a ton of extra complexity to add to my projects. I don't care about the \"security benefits\" of AWS sigv4. I hope the authors consider a different authentication scheme so I can recommend Garage more readily. reply dopylitty 16 hours agoparentI read that curl recently added sigv4 for what that’s worth[0] 0: https://how.wtf/aws-sigv4-requests-with-curl.html reply zipping1549 12 hours agorootparentOf course curl has it reply 6LLvveMx2koXfwn 17 hours agoparentprevImplementing v4 on the server side also requires the service to keep the token as plain text. If it's a persistent password, rather than an ephemeral key, that opens up another whole host of security issues around password storage. And on the flip side requiring the client to hit an endpoint to receive a session based token is even more crippling from a performance perspective. reply ianopolous 3 hours agoparentprevYou can implement S3 V4 signatures in a few hundred lines of code. https://github.com/Peergos/Peergos/blob/master/src/peergos/s... reply computerfan494 2 hours agorootparentI have done this for my purposes, but it's slow and unnecessary bloat I wish I didn't have to have. reply ianopolous 1 hour agorootparent5 hmac-sha256's per signature are slow? reply surfingdino 13 hours agoparentprevIt makes sense to tap into the existing ecosystem of AWS S3-compatible clients. reply otabdeveloper4 7 hours agorootparentPlain HTTP (as in curl without any extra headers) is already an S3-compatible client. If this 'Garage' doesn't support the plain HTTP use case then it isn't S3 compatible. reply neon_me 11 hours agoparentprevCheck something like PicoS3 or https://github.com/sentienhq/ultralight-s3 There is a few \"very minimal\" sigv4 implementations ... reply klysm 17 hours agoparentprevSending your api key in the header is equivalent to basic auth. reply vineyardmike 15 hours agorootparentThis is not intended for commercial services. Realistically, this software was made for people who keep servers in their basement. The security profile of LAN users is very different than public AWS. reply iscoelho 13 minutes agorootparentYou know FOSS software runs most of the internet right? (and, if you'll believe it, AWS internally) I would find it completely unsurprising to see Garage used in some capacity by a Fortune 500 by the end of the year (not that they'd publicly say it). reply anonzzzies 12 hours agorootparentprevThe site says it was made (initially) and used for a commercial French hoster. reply vineyardmike 11 hours agorootparentThey’re a self-described “non-profit experimental hosting group”. It’s used to host their website, chat server data, etc. It’s great they made it (I use personally!) but that’s more akin to a home-lab than commercial vendor. reply computerfan494 16 hours agorootparentprevYep, and that's fine with me. I don't have a problem with basic auth. reply arcanemachiner 1 hour agoprevGitHub mirror: https://github.com/deuxfleurs-org/garage reply sunshine-o 9 hours agoprevI really appreciate the low memory usage of Garage compared to Minio. The only thing I am missing is the ability to automatically replicate some buckets on AWS S3 for backup. reply Daviey 12 hours agoprevLast time I looked at Garage it only supported paired storage replication, such that if I had a 10GB disk in location A and a 1TB disk is location 2 and 3, it would only support \"RAID1-esq\" mirroring, so my storage would be limited to 10GB reply leansensei 8 hours agoparentThat's a deliberate design decision. reply neon_me 11 hours agoprevWhats the motivation behind project like this one? We got ceph, minio, seaweedfs ... and a dozen of others. I am genuinly curious what is the goal here? reply koito17 10 hours agoparentMinio assumes each node has identical hardware. Garage is designed for use-cases like self-hosting, where nodes are not expected to have identical hardware. reply otabdeveloper4 7 hours agorootparentMinio doesn't, it has bucket replication and it works okay. reply WhereIsTheTruth 4 hours agoparentprevperformance, therefore cheaper reply iscoelho 1 minute agorootparentnot just about cost! improved performance/latency can make workloads that previously required a local SSD/NVME actually able run to run on distributed storage or an object store. it can not be understated how slow Ceph/Minio/etc can be compared to local NVME. there is plenty of room for improvement. reply moffkalast 22 minutes agoprevFinally one can launch startups from their own Garage again. reply MoodyMoon 8 hours agoprevApache Ozone is an alternative for an object store running on top of Hadoop. Maybe someone who has experience running this in a production environment can comment on it. https://ozone.apache.org/ reply storagenerd 4 hours agoprevCheck out this one - https://github.com/NVIDIA/aistore https://aiatscale.org/ It is an object storage system and more.. reply seaghost 6 hours agoprevI want something very simple to run locally that has s3 compatibility just for the dev work and testing. Any recommendations? reply zmj 6 hours agoparenthttps://hub.docker.com/r/localstack/localstack reply rlonstein 6 hours agoparentprevhttps://min.io/ reply icy 11 hours agoprevI've been running this on K3s at home (for my website and file server) and it's been very well behaved: https://git.icyphox.sh/infra/tree/master/apps/garage I find it interesting that they chose CRDTs over Raft for distributed consensus. reply surfingdino 13 hours agoprevThere's also OpenStack Swift. reply giulivo 7 hours agoparentI believe OpenStack Swift in particular is known to work well in some large organizations [1], NVIDIA is one of those and also invested in its maintenance [2]. 1. https://www.youtube.com/watch?v=H1DunJM1zoc 2. https://platform.swiftstack.com/docs/ reply anonzzzies 9 hours agoprevNLNet sponsored a lot of nice things. reply lifty 8 hours agoparentThe EU, but yeah. NLNet are the ones that judged the applications and disbursed the funds. reply comvidyarthi 17 hours agoprevIs this open source ? reply kevlened 16 hours agoparentAGPL https://git.deuxfleurs.fr/Deuxfleurs/garage reply CyberDildonics 15 hours agoprevWhat is the difference between a \"distributed object storage\" and a file system? reply vineyardmike 15 hours agoparentIt’s an S3 api compatible object store that supports distributed storage across different servers. Object store = store blobs of bytes. Usually by bucket + key accessible over HTTP. No POSIX expectation. Distributed = works spread across multiple servers in different locations. reply CyberDildonics 5 hours agorootparentstore blobs of bytes Files by bucket Directories key accessible File names over HTTP Web server reply crest 12 hours agoparentprevFiles are normally stored hierarchically (e.g. atomically move directories), and updated in place. Objects are normally considered to exist in a flat namespace and are written/replaced atomically. Object storage requires less expensive (in a distributed system) metadata operations. This means it's both easier and faster to scale out object storage. reply crabbone 10 hours agoparentprevThere are few. From the perspective of consistency guarantees, object storage gives fewer of such guarantees (this is seen as allowing implementations to be faster than typical file-systems). For example, since there isn't a concept of directories in object store, the implementation doesn't need to deal with the problems that arise while copying or moving directories with files open in those directories. There are some non-storage functions that are performed only by filesystems, but not object storage. For example, suid bits. It's also much more common to use object stores for larger chunks of data s.a. whole disk snapshots, VM images etc. While filesystems aim for the middle-size (small being RDBMs) s.a. text files you'd open in a text editor. Subsequently, they are optimized for these objectives. Filesystems care a lot about what happens when random small incremental and possibly overlapping updates happen to the same file, while object stores care about performance of sequential reads and writes the most. This excludes the notion of \"distributed\" as both can be distributed (and in different ways). I suppose you meant to ask about the difference between \"distributed object storage\" and \"distributed filesystem\". reply fijiaarone 17 hours agoprev [–] I don’t understand why everyone wants to replicate AWS APIs for things that are not AWS. S3 is a horrible interface with a terrible lack of features. It’s just file storage without any of the benefits of a file syste - no metadata, no directory structures, no ability to search, sort, or filter. Combine that with high latency network file access and an overly verbose API. You literally have a bucket for storing files, when you used to have a toolbox with drawers, folders, and labels. Replicating a real file system is not that hard, and when you lose the original reason for using a bucket —- because your were stuck in the swamp with nothing else to carry your files in — why keep using it when you’re out of the mud? reply vineyardmike 15 hours agoparentDoes your file system have search? Mine doesn’t. Instead I have software that implements search on top of it. Does it support filtering? Mine uses software on top again. Which an S3 api totally supports. Does your remote file server magically avoid network latency? Mine doesn’t. In case you didn’t know, inside the bucket you can use a full path for S3 files. So you can have directories or folders or whatever. Some benefits of this system (KV style access) is to support concurrent usage better. Not every system needs it, but if you’re using an object store you might. reply psychoslave 10 hours agorootparentBe OS FS at least has this https://en.m.wikipedia.org/wiki/Be_File_System reply nh2 7 hours agoparentprevI use CephFS. Blob storage is easier than POSIX file systems: You have server-client state. The concept of opened files, directories, and their states. Locks. The ability for multiple writers to write to the same file while still providing POSIX guarantees. All of those need to correctly handle failure of both the client and the server. CephFS implements that with a Metadata server that has lots of logica and needs plenty of RAM. A distributed file system like CephFS is more convenient than S3 in multiple ways, and I agree it's preferable for most use cases. But it's undoubtedly more complex to build. reply acdha 16 hours agoparentprev> Replicating a real file system is not that hard What personal experience do you have in this area? In particular, how have you handled greater than single-server scale, storage-level corruption, network partitions, and atomicity under concurrent access? reply favadi 17 hours agoparentprev> S3 is a horrible interface with a terrible lack of features. Because turn out that most applications do not require that many features when it comes to persistent storage. reply didntcheck 9 hours agoparentprevYou wouldn't want your \"interactive\" user filesystem on S3, no, but as the storage backend for a server application it makes sense. In those cases you very often are just storing everything in a single flat folder with all the associated metadata in your application's DB instead By reducing the API surface (to essentially just GET, PUT, DELETE), it increases the flexibility of the backend. It's almost trivial to do a union mount with object storage, where half the files go to one server and half go to another (based on a hash of the name). This can and is done with POSIX filesystems too, but it requires more work to fully satisfy the semantics. One of the biggest complications is having to support file modification and mmap. With S3 you can instead only modify a file by fully replacing it with PUT. Which again might be unacceptable for a desktop OS filesystem, but many server applications already satisfy this constraint by default reply Scaevolus 12 hours agoparentprevS3 exposes effectively all the metadata that POSIX APIs do, in addition to all the custom metadata headers you can add. Implementing a filesystem versus an object store involves severe tradeoffs in scalability and complexity that are rarely worth it for users that just want a giant bucket to dump things in. The API doesn't matter that much, but everything already supports S3, so why not save time on client libraries and implement it? It's not like some alternative PUT/GET/DELETE API will be much simpler-- though naturally LIST could be implemented myriad ways. reply nh2 6 hours agorootparentThere are many POSIX APIs that S3 does not cover. For example directories, and thus efficient renames and atomic moves of sub hierarchies. reply duskwuff 16 hours agoparentprev> I don’t understand why everyone wants to replicate AWS APIs for things that are not AWS. It's mostly just S3, really. You don't see anywhere near as many \"clones\" of other AWS services like EC2, for instance. And there's a ton of value on being able to develop against a S3 clone like Garage or Minio and deploy against S3 - or being able to retarget an existing application which expected S3 to one of those clones. reply klysm 17 hours agoparentprev> Replicating a real file system is not that hard Ummmm what? Replicating a file system is insanely hard reply TheColorYellow 17 hours agoparentprevBecause at this point it's a well known API. I bet people want to recreate AWS without the Amazon part, and so this is for them. Which, to your point, makes no sense because as you rightly point out, people use S3 because of the Amazon services and ecosystem it is integrated with - not at all because it is \"good tech\" reply acdha 16 hours agorootparentS3 was the second AWS service, behind SQS, and saw rapid adoption which cannot be explained by integration with services introduced later. reply vlovich123 13 hours agorootparentStorage is generally sticky but I wouldn’t be so quick to dismiss that reason because it might explain why anything would fail to displace it; a bunch of software is written against S3 and the entire ecosystem around it is quite rich. It doesn’t explain the initial popularity but does explain stickiness. Initial popularity was because it was the first good REST API to do cloud storage AND the price was super reasonable. reply acdha 6 hours agorootparentOh, I’m definitely not saying integration or compatibility have nothing to do with it - only that “horrible interface with a terrible lack of features” seems impossible to reconcile with its immense popularity. reply otabdeveloper4 7 hours agorootparentprevS3 is just HTTP. There isn't really an ecosystem for S3, unless you just mean all the existing http clients. reply Nathanba 16 hours agoparentprevit's because many other cloud services offer sending to S3, that's pretty much it reply crabbone 10 hours agoparentprev [–] It's a legitimate question and I'm glad you asked! (I'm not the author of Garage and have no affiliation). Filesystems impose a lot of constraints on data-consistency that make things go slow. In particular, when it comes to mutating directory structure. There's also another set of consistency constraints when it comes to dealing with file's contents. Object stores relax or remove these constraints, which allows them to \"go faster\". You should, however, carefully consider if the constraints are really unnecessary for your case. The typical use-case for object stores is something like storing volume snapshots, VM images, layers of layered filesystems etc. They would perform poorly if you wanted to use them to store the files of your programming project, for example. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Garage is an open-source distributed object storage service designed for self-hosting, offering redundancy by replicating data across three zones.",
      "Key features include lightweight and efficient deployment, versatility across multiple datacenters, and resilience to network and hardware failures.",
      "It supports heterogeneous hardware, implements the Amazon S3 API for compatibility, and is funded by NGI POINTER, NLnet / NGI0 Entrust, and the European Union's Horizon 2021 research and innovation programme."
    ],
    "commentSummary": [
      "Garage is an open-source distributed object storage system that users are adopting over Minio due to better performance with small files and replication.",
      "Key features include significant performance improvements, low memory usage, and CRDTs (Conflict-free Replicated Data Types) for distributed consensus.",
      "The project is AGPL-licensed and aims to offer a flexible, self-hosted storage solution, with ongoing discussions about compliance features and comparisons to other storage systems like SeaweedFS, IPFS, and Tahoe-LAFS."
    ],
    "points": 389,
    "commentCount": 104,
    "retryCount": 0,
    "time": 1721436031
  },
  {
    "id": 41012193,
    "title": "Multisatellite data depicts a record-breaking methane leak from a well blowout",
    "originLink": "https://pubs.acs.org/doi/10.1021/acs.estlett.4c00399",
    "originBody": "pubs.acs.org Verifying you are human. This may take a few seconds. pubs.acs.org 8a6534518dab7007",
    "commentLink": "https://news.ycombinator.com/item?id=41012193",
    "commentBody": "Multisatellite data depicts a record-breaking methane leak from a well blowout (acs.org)302 points by belter 20 hours agohidepastfavorite210 comments thinkcontext 19 hours agoBetter satellites and other aerial survey data has determined that oil and gas related methane emissions are far, far higher than industry reports. Everywhere independent researchers look along the production, transmission, distribution and end use pipeline they find more leakage than has been assumed because industry has provided the numbers the assumptions are based on for years. Along some particularly leaky production paths methane emissions are so great that the impact is greater than coal over 100 year timescale. For example, some parts of NM Permian have a 9% (!) leakage rate. When combusted methane releases about half the amount of CO2 that coal does. The eGHG potential of methane is 20x CO2 over 100 year timescale, so you have to add 180% to the total GHG potential, making it 40% worse than coal w/o even considering other leakage along the pipeline. https://sustainability.stanford.edu/news/methane-leaks-are-f... reply james_david 18 hours agoparentIn Massachusetts, Lost and Unaccounted for Gas (LAUG) is estimated per mile of pipe, rather than evaluated by regulators or even industry. It is a simple multiplication problem with little bearing on reality. Consumers bear the cost of LAUG while the utilities are guaranteed a 10% profit on their infrastructure expenses. This, along with subsidies for leak prone pipe replacement, leads to needless investment in outmoded fossil fuel infrastructure (i.e., pipe replacement) being prioritized over leak repair. reply gruez 16 hours agorootparent>In Massachusetts, Lost and Unaccounted for Gas (LAUG) is estimated per mile of pipe, rather than evaluated by regulators or even industry. It is a simple multiplication problem with little bearing on reality Isn't both ends of a pipe metered? Even if not every pope is metered, at the very least there should be metering at the point of bulk purchase (eg. from a LNG ship) to the end user (gas meter at a home). How hard is it to compare how much is put into the pipe vs how much comes out? reply closewith 10 hours agorootparentMeasurement error in gas networks can be 15% of volume, so not precise enough for this work. reply whall6 2 hours agorootparentSource? reply idiotsecant 4 hours agorootparentprevWhen accurate metering is called for we have the tech to do it just fine to far less than 1) the heat trapped by the methane stays trapped even after it has decayed Not really. It would be present, but not trapped. Absent other factors, that would mean earth is above its equilibrium temperature and would cool once methane loss permits it to radiate heat more effectively. Either that or it shoots us in the face with a clathrate gun. There's really only one way to find out. reply matthewdgreen 13 hours agorootparentLet’s not find out if there’s a clathrate gun coming for us? Please? reply xyst 16 hours agoparentprevThis is why the O&G backed SCOTUS has defanged the regulatory agencies. reply downWidOutaFite 16 hours agoparentprevWe're cooked reply rkagerer 1 minute agoprevIt was in Kazakhstan. FinedAs another point of comparison, around 550 gigatons of CO2 is released (and absorbed) by the planet \"naturally\". So we are currently within 20% of what the planet does naturally, which seems not that bad actually. reply HL33tibCe7 4 hours agorootparentUnfortunately it's not that simple, because the planet absorbs that CO2 too. So in reality, the important thing is we're going from a delta of 0 gigatons of CO2 per year, to a delta of +40 gigatons of CO2 per year. reply idontknowtech 19 hours agoprevIf you want to get really scared about methane leaks, sinkholes are forming across the tundra as global warming melts the permafrost, creating gigantic methane bubbles as the previously frozen organic material now rots. Those bubbles explode once big enough, creating massive pockmarks which fill in with water. The amount of methane they leak is estimated to be gigantic, but without full coverage we'll never know. reply czbond 4 hours agoprevLet's say I have free time on my hands. How could I maximally remove methane from the atmosphere? reply SoftTalker 1 hour agoparentMethane breaks down pretty quickly in the atmosphere, half-life is under 10 years. We don't need to remove it so much as stop leaking it. reply consp 2 hours agoparentprevEat a cow, do not replace it, and wait for it to escape the atmosphere or decompose naturally. And lighting a methane leak on fire can also help. (sarcasm tag) reply whall6 2 hours agoparentprevWalk down your street lighting matches every five feet reply rzzzt 19 hours agoprevThe description says this happened in 2023 June-to-November-ish in Kazakhstan. It sounds like this was a smaller version of Turkmenistan's crater: https://en.wikipedia.org/wiki/Darvaza_gas_crater reply linotype 19 hours agoprevWe’re not going to make it are we? reply darkteflon 19 hours agoparentNot even close. https://jonathanfranzen.com/what-if-we-stopped-pretending/ reply redserk 19 hours agorootparentThe article was written in 2019 before a number of extraordinarily significant events with major climate policy impacts: - a noticeable and rapid dip in emissions has proven to be viable due to COVID lockdowns - Russia’s invasion of Ukraine forced Europe and the US to accelerate plans to diversify energy generation away from natural gas - China’s insanely rapid growth in green energy production domestically It seems like 2022 CO2 levels are near 2019 CO2 levels[1], which is impressive. [1] https://ourworldindata.org/co2-emissions reply potsandpans 18 hours agorootparentThis data is good for cognitive behavioral therapy and not much else. In that, the world is certainly not going to end, and maybe it's helpful to look at some data that trends positive in order to inspire hope. But we are committed now to living in a world that will be radically different and less pleasant for a lot of people. How fast these trends improve will determine the scope and magnitude of degraded pleasentness. Regardless of any change, more extreme weather events more frequently will become more normal. The socioeconomic impacts that are likely to be the most jarring in the global north will probably surface from migration pressure as people from the global south are displaced due to heat and sea level rise. It is optimistic to think that we'll be on track for 2 degrees of warming, let alone 1.5. At this point, the quiet part just starting to be said out loud is that we'll never hit 1.5. We'll probably never achieve any semblance of the paris agreement goals. And even as we approach 2 degrees, 2050 looks hotter and more extreme than anything humans have ever experienced in the past. Probably one of the only short term options between now and 2100 will be solar radiation management -- a largely untested tech that is surely a terrible outcome. I'm just summarizing the latest IPCC synthesis report [1]. Adding a bit of editorialization in perceived outcomes. One of the most conservative organizations on earth, their assessment is bleak: systems are already permanently damaged, and they're likely to become more so in the near-to-distant future. My goal is not to make anyone reading despondent. I just think these kind of short snipes of perceived positive trends bury the reality of the situation that we are living through. If the goal is to assure us that the world's not going to end, it's quite a low bar to set. And there's quite a gradient of suffering between. [1] https://www.ipcc.ch/report/ar6/syr/ reply FeepingCreature 8 hours agorootparent> Probably one of the only short term options between now and 2100 will be solar radiation management -- a largely untested tech that is surely a terrible outcome. Maybe it's time to put a billion or two into testing all these untested techs then. reply HPsquared 7 hours agorootparentThat's what I find so baffling. Why aren't we putting serious resources into geoengineering, adaptation and mitigation? It's so underfunded in comparison to what we're spending in a futile attempt at emission reduction. reply klyrs 2 hours agorootparent> Why aren't we putting serious resources into geoengineering, adaptation and mitigation? Are you running for president of the US? What do you think would happen to either candidate if they made this central their platform in today's political environment? Spoiler: rich old people will not suffer, if anything, they'll just stop moving to Florida and Arizona. reply unglaublich 6 hours agorootparentprevBecause there's no economic incentive. We can only create one with proper policies, but politics have been bootlegged by industry who only looks at short-term gains. reply DoctorOetker 5 hours agorootparentprevI agree with your comment, not because I don't think there is any hope, but because I see a situation were valid and viable hope is buried in the noise of feel-good-show hope and subsidy-ogling hope. I am not saying it will be easy to build it, or cheap, or without any drawbacks, but... It is physically possible to generate energy by cooling the planet. Its cold in space, its cold up there. To run a heat engine you need 2 temperature baths at different temperatures. Lets take for the higher temperature our surface temperature, and for the lower temperature the temperature at or slightly above the tropopause (say 11 km above surface, somewhere between polar and equatorial regions). At that height temperatures might be say -50 deg C. Now consider the following engineering challenge: a buoyant vessel holds taut ~12 km \"atmosphere elevator\", made of SCG (single crystal graphene). Along such cables could be suspended chimneys made of light fabric, or perhaps pipes conveying coolant up and down, ... As a thought exercise: suppose the hook floating in the tropopause/bottom of stratosphere is used to heave up and down buckets of water (the same number of buckes going up and down, so apart from friction this transport costs no energy (think of a pulley). as the water travels up it freezes, giving off heat at a higher layer, closer to dark cold space where it would end up anyway eventually, as the ice is lowered it absorbs heat from the lower layers. If the water/ice buckets were insulated, and only brought in thermal contact at the top or bottom of the structure, then it would dump the heat at the top only, above the CO2 and water blanket where it can more easily escape to space, and it would absorb heat only at the bottom. This means we could run a heat engine at the surface generating energy, without proliferation concerns (because when we do it its for energy, but when others do it it must be for weapons, or for gaining experience requisite for weapons)... reply marcosdumay 15 hours agorootparentprev> But we are committed now to living in a world that will be radically different and less pleasant for a lot of people. Committed? We will have extra energy for carbon capture on the next decade. This is a problem we can solve. Not before it gets worse, but we can stop it from being permanent. reply geysersam 12 hours agorootparentStop living in a fantasy. Direct air carbon capture can never be more efficient than just not burning fossile fuels in the first place. reply FeepingCreature 8 hours agorootparentSure it can. Why wouldn't it? Say you find a source of energy with zero cost. Now direct air carbon capture is free. \"But you don't have a source with zero cost.\" Great, now we're talking numbers, how cheap does it have to be? So long as energy transport has inevitable losses, carbon capture stands a chance. In other words, IMO we should consider fossil fuel offsetting with carbon capture as a highly lossy energy transport technology that can reach any fossil fuel consumer on earth. reply geysersam 7 hours agorootparentEnergy is not the only cost. There are also massive material costs building the carbon capture infrastructure. A reasonable estimate for the scale required is the existing fossile fuel infrastructure. That is, direct air carbon capture means investing an amount on the order of the value of the existing fossile fuel infrastructure to spend energy to achieve nothing of value (except offsetting the damage we caused by not switching to renewables earlier). It's a complete nonstarter. Only a vehicle used by the fossile industry to delay the transition. reply marcosdumay 4 hours agorootparentI wonder if people may be able to organize some entity that collects money for everybody to use in projects that benefit the society... But I guess, no, that's impossible! reply HPsquared 7 hours agorootparentprevEven with free energy, you'd need to invest a lot of capital and then dispose of the CO2 somewhere. reply Brigand 8 hours agorootparentprevHaving excess energy from solar could make it possible to capture the excess co2 over time. reply Voultapher 10 hours agorootparentprevCO2 is a red herring, yes it matters but it's only one thing in a long list of things our lifestyle impacts https://dothemath.ucsd.edu/2022/09/death-by-hockey-sticks/. Regarding energy transitions and carbon capture: > Energy transition aspirations are similar. The goal is powering modernity, not addressing the sixth mass extinction. Sure, it could mitigate the CO2 threat (to modernity), but why does the fox care when its decline ultimately traces primarily to things like deforestation, habitat fragmentation, agricultural runoff, pollution, pesticides, mining, manufacturing, or in short: modernity. Pursuit of a giant energy infrastructure replacement requires tremendous material extraction—directly driving many of these ills—only to then provide the energetic means to keep doing all these same things that abundant evidence warns is a prescription for termination of the community of life. reply RandomThoughts3 8 hours agorootparentThat’s significantly moving the goal post. Yes, it’s a given that biodiversity will take a hit for a pretty long time due to human activity but frankly, so what? I personally care more about avoiding millions dying of starvation but you do you. I understand that said millions being mostly in Nigeria and India make some in the west care less about them that cute animals, but still. reply DangitBobby 4 hours agorootparentThis attitude towards other life sucks ass. reply jajko 20 minutes agorootparentWell we kill it for pleasure, to eat them and treat them horribly, and almost absolutely nobody ever bats an eye. Folks that visit slaughterhouse often don't eat meat for few weeks but then revert back. Don't expect miracles, humans are mostly still pretty primitive species driven largely by emotions which swing semi randomly, all wars are, if you drill deep enough caused by this and only this. Or check any elections. One day, maybe not so long in future as many may think, our descendants will judge us as primitive barbaric savages. And they will be mostly right. reply lossolo 6 hours agorootparentprevWhat extra energy? Energy consumption around the world is rising every year and will rise even higher because of climate change. And carbon capture is not a viable solution to climate change. \"According to the IPCC’s Working Group III report, carbon capture is one of the least-effective, most-expensive climate change mitigation options on Earth.\" \"Even today, some projects already operating around the world have not been as successful as planned. In Australia, the CCS project run by Chevron has not yet made its Gorgon project meet its target of 80% carbon dioxide capture. A recent report from the Institute for Energy Economics and Financial Analysis (IEEFA) on two Norwegian projects that store carbon dioxide under the seabed called into question the long-term viability of CCS.\" \"There are currently 42 operational commercial CCS and CCUS projects across the world with the capacity to store 49 million metric tons of carbon dioxide annually, according to the Global CCS Institute, which tracks the industry. That is about 0.13% of the world’s roughly 37 billion metric tons of annual energy and industry-related carbon dioxide emissions.\" https://www.nationalobserver.com/2023/03/20/opinion/carbon-c... https://www.theguardian.com/environment/2023/aug/01/is-carbo... https://archive.is/e0y4W reply forgetfreeman 12 hours agorootparentprevGreat, now we need viable large scale carbon capture technology and enough loose funds to bribe an industry into existence globally. That 2nd part is the real trick. reply paulmd 15 hours agorootparentprevIn the vein of “this is what people considered ‘comically fat’ as the punchline of a joke when the Simpsons started running”, it’s funny to look back at the “Toby ruins everything” video and look at what Aaron Sorkin considered “comically devastating” for climate change in 2012 in The Newsroom. https://m.youtube.com/watch?v=Uc1vrO6iL0U&t=228 reply itronitron 9 hours agorootparentprev>> a noticeable and rapid dip in emissions has proven to be viable due to COVID lockdowns funny how that never gets brought up in articles about RTO mandates reply geysersam 12 hours agorootparentprevIf anything the Russian invasion of Ukraine diverted attention away from climate change to defence. Look at how many countries in Europe took the opportunity to cut climate progressive policies to reduce fuel prices during the price crunch after the invasion. The prices were temporarily inflated, and affected groups could have been supported by other means, but instead green policies were targeted and you bet they will not be re-introduced any time soon. reply MrVandemar 18 hours agorootparentprevAnd yet, do you know when the hottest month on record was? It's easy to answer: last month. It's been \"last month\" for the last 12 months at least. Basically, we've hit the J curve. The \"extraordinarily significant events\" you cite amount to little more than a guy who jumped off a bridge thinking: \"you know, I think I've changed my mind\". (And yet, we just bought $700 of seedlings and I spent my birthday, and many other days planting them. It's not a pointless exercise, it will improve things in my little corner, but it won't materially affect anything either). reply HPsquared 7 hours agorootparentOn the bright side, at least the CO2 and warmer weather will help the seedlings grow. \"Greenhouse\" effect and all that. reply redserk 5 hours agorootparentprevAnd yet, this is baked into climate change models if you’ve been paying attention. Unfortunately, yes, even after we hit a plateau of emissions, we’re going to see effects. The important bit is that, yes, we are seeing changes, and yes, we are seeing dreadful environment effects, we’re making progress to slow this down. We’re fortunate that the last few years gave us excuses to shift policy that otherwise would’ve had us keep going down this path for another decade+. I would certainly hope those pushing the “doomerism” takes in this thread do not work for any AI companies, cloud computing, NVIDIA, or other organization responsible for insane datacenter power growth this past year. reply graeme 19 hours agorootparentprev2023 is 37.4, which is a fair increase on 2019. More importantly the chart that matters is total CO2 in atmosphere. You can't even see 2020 on here. That was a one off 5% decrease. Imagine if you had $1,000,0000 in your bank account and were spending $100,000 per year, except one year you spent $95,000. That's 2020. This methane news is on top of CO2 as it has its own added impact. Proper optimism that we can do it first requires taking stock of where we actually are. So far we haven't actually displaced any carbon energy and we are at the all time highest levels of both annually emissions and also CO2 in the atmosphere. Chart of total CO2 concentration over time: https://gml.noaa.gov/ccgg/trends/ reply czbond 4 hours agorootparentInteresting - thanks for the CO2 concentration link. I am curious what prior to 2019 looked like (eg: if it was a comparable size, yet dipped in 2019-2021) and is back to normal. reply graeme 2 hours agorootparentHave a look at the right hand chart. That shows the increase from 1960 until now. You couldn't spot 2020 by eyeballing the chart. 2020 was a 5% dip in the pace of the increase. It's like someone gained ten pounds in 2019, 9.5 pounds in 2020, and 10 pounds in 2021. CO2 emissions are analogous to a total gain, so a 5% one off cut isn't very meaningful. reply elbasti 1 hour agorootparentprev> Imagine if you had $1,000,0000 in your bank account and were spending $100,000 per year, except one year you spent $95,000. That's 2020. It's even worse, because there's positive feedback loops we've probably unlocked (mainly artic and tropical methane). So it's more like every year you put $100K on a credit card with high interest rate with continually compounding interest.* Even if we stopped spending tomorrow, the debt will continue to grow exponentially. *And the card is owned by a mob boss that will blow your brains out when your debt exceeds your bank balance. reply datavirtue 7 hours agorootparentprevCleared that shit up. reply bagels 19 hours agorootparentprevViable? Effective at reducing emissions, but is stopping all industrial output viable? reply forgetfreeman 12 hours agorootparentDepends on your appetite for making the money sad. reply bagels 2 hours agorootparentI have no idea about how to measure the happiness of money. reply blueflow 9 hours agorootparentprevThe emissions, not the level. The level is increasing at the level of emissions (minus absorptions). We are accelerating as fast as 2019 towards the wall. reply colordrops 19 hours agorootparentprevOk, but weren't 2019 levels really bad? These are nice developments but really tiny in comparison to what needs to be done to stop progression. reply simpaticoder 19 hours agorootparentprevOn the bright side for those that have always wanted to travel to other worlds and terraform them, we'll now have an opportunity to have that experience without the travel. reply anonym29 19 hours agoparentprevShort of China and India voluntarily holding themselves to the same standards as Western Europe and the US for free, we are not going to avoid catastrophic global warming. The good news is, while we may not be able to prevent climate change, we are not powerless against it. The Dutch know how to win the fight against sea level rise. Middle Eastern architecture knows how to keep people cool even with >50°C air temperatures. The Canadians know how to winterize an electric grid. Sure, we may not have the optimal strategy for addressing some threats, like wildfires, right now, but the point is, we are not alone, and we are not helpless. Countless human lives can be saved if we're willing to work together. reply OsrsNeedsf2P 19 hours agorootparent> Short of China and India voluntarily holding themselves to the same standards as Western Europe and the US China and India emit significantly less CO2 per capita than Western Europe and the US [0] - and that's ignoring the fact they do all of our manufacturing. [0] https://www.worldometers.info/co2-emissions/co2-emissions-pe... reply glenstein 16 hours agorootparent>China and India emit significantly less CO2 per capita Which would be great if CO2's heat trapping properties depended on the per capita emissions of their nation of origin. Unfortunately CO2 molecules don't know where they came from and can't adjust their heat trapping properties accordingly, and instead it's about absolute amounts as ppm of the atmosphere. But there's a silver lining for China and India - which is that even small reductions in their per capita emissions cash out as massive reductions in absolute terms. So in terms of bang for the buck per-capita wise, they are the top candidates for emissions savings. reply geysersam 12 hours agorootparentJust no. In terms of bang for buck it's easier to reduce emissions in rich countries with high emissions per capita. Unless by \"buck\" you mean the number of decisions that need to be made, but that's hardly the limiting factor. Country sizes are arbitrary. With your logic if China was to split into a bunch of smaller nations their emissions wouldn't matter any more? Per-capita emissions is the only reasonable way to measure. reply glenstein 3 hours agorootparentIf the Earth's population was 1 billion, 10 billion, or 100 billion, but the emissions were the same, in each of those cases we would be cooked to death by our own atmosphere. Even though they have wildly different per capita emissions. Or to put it a different way, if all I knew were the parts per million of carbon in the atmosphere, and all you knew were per capita consumption figures, I would know when the atmosphere was truly cooked and you wouldn't. reply HPsquared 7 hours agorootparentprevIt costs more bucks to change a rich person's mind, than a poor person's mind. reply magicalhippo 19 hours agorootparentprev> and that's ignoring the fact they do all of our manufacturing. Any estimate of how much that constitutes? Like if US and EU had to produce themselves all the stuff they now get from China, how would their CO2 emissions change? reply h0l0cube 19 hours agorootparentEmissions embedded in trade https://ourworldindata.org/grapher/share-co2-embedded-in-tra... Trade adjusted emissions per capita https://ourworldindata.org/grapher/consumption-co2-per-capit... reply thinkcontext 17 hours agorootparentprevThose numbers are from 2016. Numbers from 2022 have China ahead of the EU with them heading in the opposite direction. https://ourworldindata.org/grapher/per-capita-ghg-emissions?... reply h0l0cube 13 hours agorootparentNot sure who you're replying to, but when you factor in trade, China is still a touch behind the EU and seems to be trending away. I see 2022 on the chart (you might have to make sure you have JS enabled) https://ourworldindata.org/grapher/consumption-co2-per-capit... reply wmanley 19 hours agorootparentprevYes to India and the USA. China and Europe are roughly on par. reply throwaway290 14 hours agorootparentprevChina emits more per capita than EU https://ourworldindata.org/grapher/per-capita-ghg-emissions?.... reply h0l0cube 13 hours agorootparentNot when you adjust for trade: https://ourworldindata.org/grapher/consumption-co2-per-capit... reply maverwa 8 hours agorootparentOne of them is GHG (CO2 equivalent) and the other one is just CO2 itself, right? Does seem to change the numbers much, but it’s an additional difference between these metrics, right? reply h0l0cube 4 hours agorootparentThat is a point of difference, but I'd also wager CO2 and GHG overall emissions correlate. reply navane 10 hours agorootparentprevDoes adjusted for trade mean that sneakers produced in China but worn in the west, count for the west? reply Loic 6 hours agorootparentYes. reply throwaway290 13 hours agorootparentprevWhy do you discount export emissions? Is it forced on them against their will or they don't profit from it? Shall we also adjust for things like how much of their emissions could have been green, etc? reply netsharc 8 hours agorootparentI'm not the same commenter as above, but, why not? In the extreme hypothetical scenario, a country could be net-zero and have the newest iPhone every year, throw away their good PCs because Windows 11 needs a modern one with TPM 2.0, fast fashion, and steak for dinner every night, because everything's produced in the neighboring country. And then when the world says \"We need to be better about our carbon output\", your argument is like the country of Netzeroland saying \"Well don't blame me, I'm net zero on CO2, blame Neighboristan over there! I don't need to change, they need to change!\", when in fact its people also need to change their consumption habits. Whether it's forced, it seems like a philosophical question about economics. Do you go to work willingly, or because you're forced to, because you want to be able to afford food, housing, and iPhone 17? Or it's a tragedy of the commons scenario. If there's demand, and Neighboristan is able to but doesn't supply it, their citizens are forced by government to give up sources of income; meanwhile the citizens of We-Also-Build-iPhones-And-Make-Cheap-Shirts-ia are raking the money in and travelling around the world and taking selfies of their steak meals with their iPhones, and citizens of Neighboristan will get jealous and think \"Why can't we do that?\". And they'll either vote in a government that will allow this, or revolt. And consumers in the West enable this behavior by wanting the cheapest bang for the buck. If China charges for CO2, your cheap Walmart toy gets more expensive, and you don't buy it, Walmart will say \"we'll find a supplier in Vietnam/India/etc where there's no such CO2 fees.\"... reply throwaway290 7 hours agorootparentHow you describe is not how demand & supply works. To think that every product China puts on Amazon and whatnot is there because there's demand for it doesn't make sense. A lot of the time existence of supply creates demand. I see it first hand all the time. Some new thing is put out (accessory, electronics, etc), everyone gets on the bandwagon and buys it. It also helps if there's fashionable aura and gov subsidies (EVs). If it was not made & sold, people would instead buy something else made locally even if it cost more or simply not buy. Yes, part of it is consumer choice (and yes some consumers do choose to not buy if they think it supports environmental damage or such) but it's wrong to pretend one side is the one who makes all the choices and the other side is powerless to resist because it's convenient for your argument. > because everything's produced in the neighboring country Unless the first country dictates neighbouring country's policies, not sure it's that relevant. > it seems like a philosophical question about economics. Do you go to work willingly, or because you're forced to, because you want to be able to afford food, housing, and iPhone 17? Either there's freedom of choice or not. This logic can go to \"I should deceive people or dump waste into rivers or use slave labour, because market & I want to afford a new fancy yacht or invade a country\". Yes, I can choose not to work or do different work that is more or less friendly to environment and I use an old iPhone. > their citizens are forced by government to give up sources of income There's unlimited hypotheticals. Should we say we are forced to give up sources of income if they involve crime or morally wrong things that are discouraged? > And consumers in the West enable this behavior by wanting the cheapest bang for the buck. \"Enabling\" can be used to justify anything or assign any guilt. It's a bit narcissistic. It reminds me of \"America started the war in Ukraine\" a little. > Or it's a tragedy of the commons scenario. I agree there is some tragedy of the commons here. reply anonym29 19 hours agorootparentprevThey collectively still emit substantially more than the US and Western Europe, and much more importantly, GHG emissions have been declining in the US and Western Europe for over two decades. In that same time period, they have done nothing but grow rapidly in China and India. All the information we have so far suggests that India will continue to grow rapidly in this regard even after China plateaus on carbon emissions. We can play semantic games with per-capita consumption, but the fact of the matter remains that even if the entirety of the planet, except China and India, went 100% carbon neutral overnight, it would still be impossible to avoid hitting catastrophic climate change thresholds due to GHG emissions from those two countries alone. They are both countries home to a rapidly rising broad middle class, and that middle class is going to want to enjoy the spoils of materialistic consumerism (the true root cause of human-induced climate change) just as much as our grandparents did, our parents did, and we do. reply h0l0cube 19 hours agorootparent> We can play semantic games with per-capita consumption, but the fact of the matter remains that even if the entirety of the planet, except China and India, went 100% carbon neutral overnight, it would still be impossible to avoid hitting catastrophic climate change thresholds due to GHG emissions from those two countries alone. Of all the countries China is doing the most to curb its emissions. > Global annual renewable capacity additions increased by almost 50% to nearly 510 gigawatts (GW) in 2023, the fastest growth rate in the past two decades. [..] In 2023, China commissioned as much solar PV as the entire world did in 2022, while its wind additions also grew by 66% year-on-year. Globally, solar PV alone accounted for three-quarters of renewable capacity additions worldwide. https://www.iea.org/reports/renewables-2023/executive-summar... The western world is shifting away from China to India and the rest of Asia for manufacturing, so I think the desire for cheaper products and ‘national security’ are going to increase emissions somewhat. But even still, with the glut of renewables, and soon the glut of batteries, emissions free energy is going to be the cheapest option for manufacturing anyway reply glenstein 3 hours agorootparent>Of all the countries China is doing the most to curb its emissions. China \"doing the most\" to curb emissions is perfectly compatible with them \"doing the most\" of the actual emissions. At high enough scales, no matter what you're doing, you'll be doing \"the most\" of it, in both directions. What matters at the end of the day is emissions in absolute terms. reply throwaway290 7 hours agorootparentprev> Of all the countries China is doing the most to curb its emissions. False. They have good green PR to justify other governments funneling taxpayer money via subsidies but meanwhile they are building the most coal plants than any country and this has been only ramping up in recent years with more new fossil fuel power plants being approved year on year. https://www.carbonbrief.org/china-responsible-for-95-of-new-... reply h0l0cube 5 hours agorootparent> False. As I've explained in another comment, adding power stations and capacity doesn't mean running 100% all year. China is using coal as peakers much like other countries use gas for this purpose. Given that China is also installing the most energy storage, it's not long (at current growth, less than 10 years) before these plants are only used in emergency situations. reply throwaway290 4 hours agorootparentYou explained wrong. literally no one uses coal as peaker ever, and they are building more coal plants than the rest of the world combined (the country is not even the most populous) reply netsharc 8 hours agorootparentprevI wrote about why saying it's their fault and not ours is not admitting our own fault is... bullshit: https://news.ycombinator.com/item?id=41015413 > that middle class is going to want to enjoy the spoils of materialistic consumerism (the true root cause of human-induced climate change) just as much as our grandparents did, our parents did, and we do. The last 3 words is key there. WE also don't want to give up those spoils. And we want to enjoy them by outsourcing their production to the cheapest places and cheapest methods, like unsafe mining of minerals done by underpaid children. But hey, it's not happening in our country, so we're fine? If a phone is made with minerals mined using safe Western mining standards, and made by factory workers earning decent wages, and it's CO2 neutral, and so on, it'll be more expensive, and will not sell well, because as long as there's another country producing the same phone but without all those \"external\" costs, the consumer will say \"Look, this phone has the exact same specs but is way cheaper!\". reply throwaway290 7 hours agorootparentYou manage to make it look like no one can make a sane choice for themselves. Other countries can only do what we want (because WE want cheap products WE make them do bad stuff), and we can only do what they want (if they offer something that is cheaper then we automatically must buy it and not care about any damage or side-effects). It's a paradox isn't it? In reality it is government's choice, to go for short term riches and maintaining power or act more ethically and think long term. And consumers pay more all the time for things like \"no slave labour\" (idk how well it's enforced, I just say it's a value), \"ethically sourced materials\", \"recycled\", \"no spying malware\", blahblahblah, just look at Android vs iPhone. reply netsharc 1 hour agorootparent> You manage to make it look like no one can make a sane choice for themselves. On average, the \"sane\" choice is to go for the cheapest option, or turn a blind eye to the child labor, etc. Sure, some people want to live ethically and minimize their emissions, but there's not enough of them. > In reality it is government's choice, to go for short term riches and maintaining power or act more ethically and think long term. Yeah, Macron tried to tax fuel more and there were widespread protests by \"average French citizen\", and he got metaphorically beaten up in the last election. The EU incumbents watered down green projects because of fear of losing to the rightwing populists. It's the sane choice for them to make, because I'd rather be governed by the somewhat sane center-right than lying manipulative populist hard-right, but the sane choice meant stepping away from the path of an ecological future. The summary is, if you try to be ethical, you get voted out of office, god bless democracy! /s reply anonym29 7 hours agorootparentprev>But hey, it's not happening in our country, so we're fine? No, but even if the US and Western Europe went net zero, that's still not enough to offset global GHG emissions coming out of China to prevent catastrophic climate change. If you, a wealthy westerner, want to tell billions of people in China to stop enjoying middle class luxuries for the first time because it's bad for the environment, be my guest. I have no such desire. reply netsharc 3 hours agorootparent> No, but even if the US and Western Europe went net zero, that's still not enough to offset global GHG emissions coming out of China to prevent catastrophic climate change. Show me the math/source, if you want to claim this, please. US and W. Europe outsource their emissions because they buy their electronics and clothes from e.g. China and Bangladesh. What is a US/WE net zero? They stop buying Chinese product, and China stops producing them for US/WE? reply downrightmike 14 hours agorootparentprevPer capita doesn't mean shit when their actual gross is way higher. reply Qwertious 12 hours agorootparentPer capita is a proxy for discussing how much emissions they need. If you're not considering anyone's need, then you're basically telling them to go kill themselves and they'll likewise completely ignore anything you have to say. If we ignore China's/India's needs and what they actually can do while talking about what they should do, then the whole discussion is a waste of time. reply datavirtue 7 hours agorootparentprevAgreed. Per capita arguments sound like toddlers arguing over who is closer to the other. reply 486sx33 19 hours agorootparentprevnext [6 more] [flagged] netsharc 8 hours agorootparentAnd will you be happy if iPhones and Walmart toys cost, let's say 30% more, because of China mandating CO2 emissions compensation? (Although such a compensation is mostly mythical). Or that the shelves of Walmart are bare (\"Sorry we can only produce 10% of what we're able to, emission caps\"), and prices are sky-high because of the law of supply and demand? I bet you'll be moaning then too. reply anonym29 6 hours agorootparentI'd be quite happy if both the iPhone and all of the cheap plastic toys at Walmart no longer existed, unironically. reply OsrsNeedsf2P 16 hours agorootparentprevMore people live in China and India. Their government is doing a better job at keeping emissions down, per person. Pollution doesn't care where you live. reply downrightmike 14 hours agorootparentAccording to a 2016 study, electricity theft and fraud in India can account for almost 20% of the country's total electricity generation. Other sources have reported that the percentage of lost electricity due to theft can range from 20% to 50%. Electricity theft can be a result of poor governance, and power distribution companies (discoms) that are already in debt can face significant losses. Some say that the government needs to promote better governance in the sector to help manage the issue. https://www.sciencedirect.com/science/article/abs/pii/S03014... reply Qwertious 12 hours agorootparentSo tl;dr the Indian govt should be less corrupt. I think phasing out their coal plants would be a far easier and more realistic task to tackle. reply bcrosby95 19 hours agorootparentprevnext [3 more] [flagged] h0l0cube 18 hours agorootparentOf all the countries China is doing the most to curb its emissions. Not sure what else you expect them to do > Global annual renewable capacity additions increased by almost 50% to nearly 510 gigawatts (GW) in 2023, the fastest growth rate in the past two decades. [..] In 2023, China commissioned as much solar PV as the entire world did in 2022, while its wind additions also grew by 66% year-on-year. Globally, solar PV alone accounted for three-quarters of renewable capacity additions worldwide. https://www.iea.org/reports/renewables-2023/executive-summar... reply energy123 12 hours agorootparentprevChina has emitted much less in absolute since the industrial revolution, less per capita today, they're poor, they're rapidly building out renewables faster than anyone, they're powering the world's transition to renewables with the cheapest solar panels and batteries, they're expected to hit their coal peak in the next two years, Xi has made it a publicly stated policy priority recently to decarbonise their economy, and they're the world's manufacturer which means their economy has to play on hard mode relative to a service economy like the US. It's wrong to point the finger at them as some kind of uniquely bad actor. Yes they have to decarbonise, but so does everyone at at the same time, and this bad faith argument keeps being resurrected as an excuse to do nothing at home. reply gjsman-1000 19 hours agorootparentprevPer capita is misleading. China has over 1 billion people more than the US, the majority of them in abject poverty. Same for India; so their per capita number is artificially skewed downward. reply ianburrell 19 hours agorootparentAre you a time traveler from 1990? China effectively doesn't have anyone in extreme poverty ($1.50/day). They have about the same or lower rate than the US. They do have people in poverty, 24% below $5.50/day ($2000/yr) line. India has also decreased extreme poverty, down to 2% in 2023. At $3.20/day, they declined to 21%. At $5.50, they are at 81%. Most people are poor, but less poor than they were. reply mitthrowaway2 19 hours agorootparentprevWhy is that misleading? That doesn't sound like \"artificial skew\" to me. reply 486sx33 19 hours agorootparentIrresponsibly growing your population doesn’t allow you more emissions, that’s crap! reply defrost 17 hours agorootparentWhich county, China or the USofA, had an enforced one child per couple policy for many decades? reply mitthrowaway2 19 hours agorootparentprevNot sure if your answer is intended to be serious or sarcastic. But there is a justifiable argument for it -- you have new humans to take care of, who have needs, and the same moral claim as anyone else in the world nto address those needs using fossil fuels. On the other hand, there's no reason to think that an increase in the number of countries is a valid excuse to increase emissions. reply card_zero 18 hours agorootparentLeading to the conclusion: those few people (government) who have the greatest influence over the problem (over the countries with the largest emissions in real terms, not per person) somehow don't have the responsibility. It's true that if a country splits into two countries, that doesn't give them a right to emit more collectively. However, it does mean that each country now has little control over what the other one does, and less ability to actually solve the collective problem we want solving, which was the objective here. reply h0l0cube 18 hours agorootparent> in real terms I’m not sure how better to dice up emissions then by consumption per person, and by that measure the US is clearly emitting the most https://ourworldindata.org/grapher/consumption-co2-per-capit... Conversely China is doing the most about emissions > China commissioned as much solar PV as the entire world did in 2022, while its wind additions also grew by 66% year-on-year. https://www.iea.org/reports/renewables-2023/executive-summar... reply card_zero 18 hours agorootparentIt's great if China is genuinely going renewable. I read about this often. But I also read that they're the biggest producers of coal (confusingly both the biggest importers and exporters, and consumers). Still, maybe that's being phased out. But why do you want to \"slice up\" emissions at all, if you're interested in reducing them? The problem is a big load of gas. So identify the biggest emitters (in total volume) with the most control over it. Don't say \"oh San Marino, you're a really greedy country, with all your tax haven shenanigans and conspicuous consumption and more cars than people, you're the problem\", because they really aren't. They may be really awful, but that wasn't the question and isn't something it's viable to fix, nor are they capable of helping more than minutely, unlike a giant authoritarian state with giant emissions, capable of helping a lot. And perhaps, as you point out, already doing so. reply h0l0cube 13 hours agorootparent> But I also read that they're the biggest producers of coal (confusingly both the biggest importers and exporters, and consumers). Yep. And their coal plants are less and less needed. They are essentially becoming peakers (in the same manner as many modern gas plants) for when the sun don't shine and the wind stops blowing. But in terms of the biggest producers of coal, Australia punches above its weight, but rarely does anyone single it out in these discussions. They also are the second biggest per-capita consumers in terms of CO2 emissions. reply Qwertious 12 hours agorootparent>Australia punches above its weight, but rarely does anyone single it out in these discussions. They also are the second biggest per-capita consumers in terms of CO2 emissions. Our (Australia's) fuckwit right-wing politicians love to blame climate change on China and the USA, because their absolute emissions are way higher than ours and \"we emit less than 1% of the world's emissions\" so us taking the first step is pointless (says THEM). Also, because our coal is relatively clean they say that stopping coal mining would actually make emissions worse, because then everyone would just switch to much more polluting foreign coal. Every American in this thread who says \" have the most emissions, they need to reduce their CO2 output\" needs to understand that the same logic can be and is used against them to justify us doing fuck-all about the climate. Stop enabling our fuckwits! reply mitthrowaway2 15 hours agorootparentprevI'm not sure I come to that conclusion. At least, that's not how I think about responsibility. I'd say that per capita emissions is a reasonable measure of scoring how much a country and its people are doing to make climate change worse (or better). And, while not perfectly so, it does a reasonable job as being an actionable metric for how sustainable a country's standard of living is. \"America (14.4 t/cap/yr) should try to be more like Japan (8.6 t/cap/yr)\" is a reasonable take on how the US can reduce its per-capita emissions, and what changes might take it there. \"America (4.8 Gt/yr) should try to be more like the United Arab Emirates (220 Mt/yr)\" ends up being rather bad advice. And while the standard of living in Iceland is quite nice, I doubt that there's any practical way that either China or the United States could reduce their emissions to that level (3.5 Mt/yr) in the short term without millions of people starving to death in the process. It wouldn't be realistic or fair for Iceland to demand that the US cut its emissions to Icelandic levels on an absolute basis (but very fair and reasonable to demand that the US cut to Icelandic levels on a per-capita basis). In terms of how much responsibility a government has, well... certainly governments that govern more people do have more leverage, and with larger total emissions, there are more opportunities to cut. But if there is any non-zero level that we may consider an acceptable emissions target, surely this level should be proportional to a country's population. And when it comes to international agreements, it's reasonable to ask China and India to make cuts, and it's very reasonable for China and India to say \"OK we'll make cuts, but we expect small rich countries like Canada to pull their weight, and if they don't, we won't either\". And when that happens, per-capita emissions is the only sensible way to gauge if those countries are pulling their weight. There are, after all, a lot of countries, and while the top four (China, US, India, Russia) emit 57% of the total, the rest emit the other 43% of that total, with no single country among them emitting over 2.9%. Even cutting China, the US, India, and Russia's emissions to zero would only cut global emissions in about half, and that's really still not enough to solve the problem, so the little countries will have to pitch in. reply h0l0cube 18 hours agorootparentprevChina’s population has reached a plateau https://ourworldindata.org/explorers/population-and-demograp... reply downrightmike 14 hours agorootparent...because they killed millions of girls... reply h0l0cube 13 hours agorootparentWho is 'they' here? Certainly there was a one-child policy. I don't think it could be said that the consequence of female abortion and infanticide was the intention (or even an expected consequence) of the policy. It seems that many people in the comments are set on demonizing the country. For this I think better points to raise are the ethnic cleansing of the Uyghur 'reeducation' program, or the usurping of peaceful autonomous neighbors like Tibet and Hong Kong (maybe Taiwan later). But none of this has anything to do with obligations around CO2 emissions, which they are addressing more than any country on the planet, as I've commented elsewhere: https://news.ycombinator.com/item?id=41012774 reply energy123 10 hours agorootparentprev> Countless human lives can be saved if we're willing to work together. The major problem is equatorial heat. After a certain point of wet bulb temperature, you either die or migrate a billion people away from the equator into Europe. Billions can't afford air conditioning. An air conditioning unit is equal to their entire annual income. You don't even have an airtight room within which you can install the AC unit. Let alone a stable electrical grid during heatwaves. Their state capacity and wealth just isn't there to plan for the worst, either. Their civic society would sooner collapse than mobilise to solve this problem. My intention isn't to sound chauvinist, this is just a reality of the political fragility of poorer countries, combined with the extreme difficulty of mobilising large amounts of resources to solve long-term challenges. reply anonym29 6 hours agorootparent>Billions can't afford air conditioning. An air conditioning unit is equal to their entire annual income. Air conditioning can get radically cheaper to meet market demand, and they can start making radically more money. Other products have gone from expensive to cheap (think TVs over the last 40 years), other people have gone from poor to prosperous (think China). Do you have a good reason why these outcomes are possible TVs and for Chinese people, but not for ACs and equatorial people? reply energy123 6 hours agorootparentWhat about countries with political strife like Myanmar? We can't hope that 100% of countries all experience significant growth, fast enough to outstrip the impact of rising temperatures. There will be laggards stuck on low GDP. Also, a lot of these economies rely on outdoor work, widespread AC can't help with that. If we get to the 3-4C warming scenarios, these countries will be terrible to live in, regardless of AC, and I won't blame them when they all try to escape to colder Europe. reply bamboozled 17 hours agorootparentprevIndia is a peculiar case because they're, so so so affected by climate change that it's suicide NOT to stop emitting on such a large scale. Yes we're all susceptible to climate change effects but India, that place gets next level hot. reply dartharva 13 hours agorootparentThis is very true. Unfortunately India also houses 1.4 Billion people, most of whom are poor, and any attempt at policy change to limit industry quickly results in poverty-related mortalities by hundreds of thousands, if not millions. reply Havoc 9 hours agorootparentprev> Middle Eastern architecture knows how to keep people cool even with >50°C air temperatures. In poor areas perhaps but the big Middle East countries burn more fossil fuels to power Air-Conditioning Of all the places I’d look to for solutions Middle East isn’t one reply jncfhnb 19 hours agorootparentprevWho knows how to handle a billion refugees? reply 486sx33 19 hours agorootparentnext [2 more] [flagged] jncfhnb 4 hours agorootparentIgnoring the tastelessness of this comment, the bulk of people living in climate threatened areas have land bridges to the broader world and are not principally threatened by rising sea levels reply MaxHoppersGhost 13 hours agorootparentprevThere will never be a billion refugees. Climate change is slow enough that humans and nature will adapt. reply jncfhnb 4 hours agorootparentWell we’re at over 100M currently and that number doubled in the past decade reply colordrops 19 hours agorootparentprevRising sea levels, temperature change, and extreme weather are not the only problems. There is also crop failure, ocean acidification, and ecosystem collapse, the latter which is the most worrying to me, as it implies both our food and our oxygen will be impacted. Furthermore, high CO2 levels in the atmosphere have measurable impact on cognitive abilities, so we will all be dumber with the changes, making it even harder to solve the problem. A negative feedback loop it seems. reply datavirtue 7 hours agorootparentprevPopulations are too big. We can't incorporate effective bodies fast enough or with enough authority to to implement sensible societal changes. Just when you need an adult, MTG shows up. reply sdkman 19 hours agoparentprevI don't think so :( reply carapace 19 hours agoparentprevOnly time will tell. But the things to do to try to avert looming disaster are the same things to do to be a better person in times of no crisis. Be kind and loving, develop compassion. That's pretty much it. The point is not to make it or not. Entropy is the enemy and it will win in the end. So what? No one here gets out alive. The point is to be a better person, to help others, to make things better. If we are going to make it we will make it though love and compassion. If we don't make it, then at least we didn't waste the time we had. reply tomrod 18 hours agorootparent> The point is not to make it or not. I fundamentally and vociferously disagree. The point is to ensure sapience and intelligence survive the heat death of the universe. reply carapace 4 hours agorootparentI'm sympathetic to your view, but I feel you're talking science-fiction or perhaps religion. By definition the Heat Death means there's nothing left to be the substrate for sapience and intelligence, eh? (If you like to discuss metaphysics I'm game.) Anyway, intelligence without love is less than pointless. reply neverrroot 19 hours agoparentprevnext [19 more] [flagged] linotype 19 hours agorootparentThe guy who’s funding $45 million a month for the candidate that wants to end EVs and pump more oil? That Elon? reply bamboozled 17 hours agorootparentHard to believe he is doing this. It's absolutely nonsensical, I think Elon really is having mental health issues. Either that, or everything he has said about being \"pro human\" was a lie. Which isn't hard to believe either. reply zo1 10 hours agorootparentI'm sure he has his motivations that he has internally justified and made sense of. Saying he has mental health issues because he supports Donald Trump because somehow Trump is pro oil and that'll massively hurt the EV industry sounds like a huge stretch, be honest. Even a casual reading of Elon a lot of us squint and make the conclusion that he supports Trump because he thinks it's what your country needs, heck like a lot of us do, EV industry be damned. The left is clearly destructive with open borders and a lack of crime policing, even if it's noble, that it's very likely that everyone including Elon is choosing Trump as a correction to the arguably insane and self-destructive policies of the left. reply bamboozled 7 hours agorootparentWhy on earth do you think America needs Trump? I'm honestly interested in your opinion. Why do you say that? Even if as the other commenter said, it's about advancing business interests through bribes, it's a slippery, dangerous slope. Trump is the kind of person who might end up deciding his Son in Law should be running your company...based on his track record of appointing morons to high places. I really still can't imagine what is going through his mind. Trump is a deplorable person even if you think he is funny or entertaining. reply pnut 9 hours agorootparentprevUS republicans and democrats have largely the same policies except for social wedge issues. My bet is that Elon knows Trump is pliable if you come with an open checkbook, so there's real opportunity to advance your business interests if he's president. reply bamboozled 7 hours agorootparentI think this is true also, it's calculated to some degree, but shit can go really bad with this approach too, it's playing with fire. Ask Jack Ma about dealing with autocrats or those who \"strongly admire\" autocrats, as Trump has said repeatedly. He admires, Xi, he admires Putin, he admires Kim Jung Un. At some point he might decide some of your wealth, might just be some of his wealth. It's wild, really wild, I can't believe this is America we're talking about. reply sroussey 19 hours agorootparentprevDo Tesla’s EVs offset SpaceX rockets? reply willy_k 19 hours agorootparentThey don’t even offset themselves, but SpaceX rockets would likely be a key part in deploying solar shades if necessary reply ainoobler 17 hours agorootparentHow does that solve the problem of fossil fuel emissions? reply willy_k 16 hours agorootparentIt can decrease the solar exposure of the earth and potentially decrease temperature, doesn’t address fossil fuel pollution but it does mitigate the impacts of fossil fuel CO2. reply ainoobler 16 hours agorootparentWhat are the material requirements for this proposal? reply willy_k 15 hours agorootparentI am not well versed but my impression is it’s essentially a bunch of tin foil and the main impediment are geopolitical reasons and cost to orbit, which SpaceX has been and continues to improve. reply knappe 12 hours agorootparentOkay, so having gone down this path I can tell you it is impossible. To reduce the output of the sun by 2% at L1 would require 20 MM tons of metal. In order to launch that much tonnage we would have needed to start launching mass 500 years ago. And that assumes no loss of material over time, which would mean even more launches. It is not feasible. reply FeepingCreature 8 hours agorootparentWikipedia says: > A more recent design has been proposed by Olivia Borgue and Andreas M. Hein in 2022, proposing a distributed sunshade with a mass on the order of 100,000 tons, composed of ultra-thin polymeric films and SiO2 nanotubes.[7] The author estimated that launching such mass would require 399 yearly launches of a vehicle such as SpaceX Starship for 10 years.[7] That seems very feasible to me. reply stragies 10 hours agorootparentprevThank you for adding some numbers regarding the feasibility of launching the required materials into space. Do you have any thoughts/numbers about how \"mining/construction\" in-situ in space might affect those outcomes? E.g. Might we soon be able to \"recycle\" old in-orbit objects (trash/old_sats) into a slowly growing space mirror? Or several smaller ones over (richer/more_affected) areas? reply knappe 1 hour agorootparentSo not really. Space mining is a very attractive idea but just not really feasible at the moment. I wrote a paper on attempting to capture the asteroid 433 Eros (a very attractive asteroid due to it's composition of rare metals [Data from the Near Earth Asteroid Rendezvous spacecraft collected on Eros in December 1998 suggests that it could contain 20 billion tonnes of aluminum and similar amounts of metals that are rare on Earth, such as gold and platinum.]) and found that in order to knock it into an orbit around the moon we'd need something on the order of 1 THOUSAND or 10 THOUSAND (I could be off by even another magnitude, the numbers are fuzzy after all this time) tsar bomba grade weapons to effectively knock it out of the current orbit and back into one around the moon. Why the moon? Because throwing a planet killer sized object at the Earth just didn't seem reasonable. I situ mining might make this more feasible, but that has all of its own complications. None of which I would feel confident in saying are feasible. https://en.m.wikipedia.org/wiki/433_Eros https://en.m.wikipedia.org/wiki/Tsar_Bomba reply greenavocado 19 hours agorootparentprevDo you have any idea how much CO2 is released in a single rocket launch? LOL reply linotype 19 hours agorootparentRe-read my comment. reply 2OEH8eoCRo0 19 hours agoparentprevWe will but not at our current numbers. Rich countries will suffer the most. reply sadhorse 19 hours agorootparentHow so? Huge flooding in Brazil this year, they are not recovering any time soon. Poor countries lack the resources that rich countries literally burn for their luxury. reply ars 19 hours agorootparentSay Brazil floods this year, and next year, and the year after that. What do you think will happen next? What will happen is Brazil will start planning for it, and then they'll be fine. There will be an adjustment period, and then, life goes on. reply kjkjadksj 17 hours agorootparentThis is what people don’t get about places like Florida. The people in Florida are in fact aware that it storms and floods, thats why the house is built on stilts with metal storm shutters. reply paulmd 15 hours agorootparentSure, but it’s expensive and it’s still devastated by storms etc. Like these building techniques works so well that insurers have left the Florida market because it’s unprofitable to rebuild everyone every couple years, so the state outlawed charging rates that account for the actual expense of it, and now people can’t get home insurance policies anymore. Like literally it’s already too expensive to keep doing it without a massive funnel of taxpayer money into peoples beach houses. It’s literally only viable to keep doing that by leeching off the largesse of the taxpayer and other homeowners in less dangerous locales. Florida also has the additional problem that it’s literally built on karst topology, it’s limestone riddled with tunnels and sinkholes so water comes right up under it, you can’t even pump the cities dry like the Netherlands. reply kjkjadksj 3 minutes agorootparentInsurers have not left the florida market. The number of uninsured homes is estimated to be like 10-20%, probably what you'd expect given a certain percentage of holdout low income homeowning areas from before florida saw so much inflow in certain real estate markets. The national rate is a little under 10% for reference. Sometimes when headlines and assumptions seem absurd, like no insurance in florida, its worth looking into it a little bit. paulmd 1 hour agorootparentprevhttps://www.newsweek.com/florida-insurance-crisis-explained-... reply bwestergard 19 hours agorootparentprevHuh? Of course life goes on, but it will be a more resource constrained life, and that will make political negotiations more difficult. If those aren't implications worth worrying about, what would be? reply dartharva 13 hours agorootparentprevAbsolutely wrong. India, China and the more polluting poorer countries are the ones who'll always suffer more. With >300 AQIs in most highly-populated areas, dirty water from rivers almost all of which are hazardously polluted, combined with a tropical climate that's already hot enough to kill thousands of people from heatstroke alone in summers - pollution-related misery in US and Europe isn't anywhere close enough to that in India. It just doesn't get called out because Indians have bigger existential concerns (many of which go directly against climate-friendliness). reply datavirtue 7 hours agorootparent\"Absolutely wrong. India, China and the more polluting poorer countries are the ones who'll always suffer more.\" ....until the migrations. reply ars 19 hours agoparentprevWe'll be fine. We'll have to make adjustments, but we'll be fine. So will the planet, and so will the vast majority of animals. reply MrVandemar 17 hours agorootparent\"Vast majority of the animals\" You know that the extinction rate of species (which includes animals) is running at approximately 200 per day. Unless you mean the vast majority of animals in the sense of numbers of individual animals, instead of diversity of species, then maybe you're right: the vast majority will be fine as the biomass of domestic animals now outweight the biomass of every other species. reply mitthrowaway2 19 hours agorootparentprevHmm, I'm really not sure about that. There are events in the geological record in which the vast majority of animals were not fine, and I'm not convinced that humans would be among those that make it, adjustments or not. reply MSFT_Edging 19 hours agorootparentprevThe vast majority of animals are cattle/livestock for meat production. If we want to be alright, we'll probably want to start reducing the cattle headcount. reply colordrops 19 hours agorootparentprevIf you are a meth addict, you don't say \"I'll be fine\" to deal with the problem. You've got to make changes, or else you are fucked. reply altdataseller 19 hours agorootparentprev“Save the earth?! The planet will be absolutely fine… humans on the other hand..” reply tialaramex 1 hour agorootparentYes, the damp rock will be fine. Humans aren't likely to die out directly, although scenarios where it all seems a bit pointless and the remaining few million humans stop reproducing are plausible. reply blueflow 18 hours agoprevThe methane plume is 2 km off from the blowout location: https://satellites.pro/Kazakhstan_map#A45.338220,52.376976,1... It is probably a pipeline leak. reply dvh 12 hours agoprevIs carbon the Great filter? reply euroderf 4 hours agoparentMaybe on Venus. I've read a convincing case that Venus can never be terraformed, simply because there is too damned much carbon. reply kjkjadksj 17 hours agoprevThere is going to be a point in the climate change phenomenon where we have to start taking aggressive measures and actually go after the biggest polluters no matter what nation they hide behind. Especially considering there will probably be forces working to undermine all of these climate goals. Maybe the US could throw some of that military budget around and use Seal team sappers to disable these polluting industrial plants? Now before people get guarded at that idea, just consider the US already does the same to kill actual people with such operations. Merely disabling infrastructure not only has some precedent, but also seems far more benign to me. reply thinkcontext 16 hours agoparentTrade wars are more likely than shooting wars, but I suppose its not impossible for the former to provoke the latter. reply metadat 13 hours agoprevMeanwhile, the Centralia coal mine fire in Pennsylvania is projected to continue burning for another 250 years. https://en.wikipedia.org/wiki/Centralia_mine_fire This is all concerning. reply openrisk 11 hours agoprevIt is deeply ironic that the modern technological era provided both the means for destroying the planet and monitoring that destruction to minute detail. reply chidli1234 17 hours agoprevI wonder if these types of events were taken into account in those models for climate change. reply thinkcontext 16 hours agoparentSingle events just aren't that big against the background of global oil and gas production emissions, let alone overall global methane emissions. Its true that O & G emissions are higher than most governments report but most models take this into account in some fashion. Its also a big topic of research to account for methane emissions because the measured amount is larger than models are predicting from known inventories. But its not thought to be from events like this. reply bamboozled 17 hours agoparentprevI really hope so. So glad I'm installing solar this year, I'm thinking about building a car port and adding even more panels too. reply swayvil 19 hours agoprevwhy don't they have a little methane-detector-activated igniter? Burn it up before it gets away. reply jofer 19 hours agoparentIt's a blowout. That means what it sounds like. An explosion and a fire. To fix it, you have to put out the fire. Once you put out the fire, you're still releasing methane. Methane is flared instead of released under normal conditions if it's not being captured. A blowout is very much not normal conditions. reply arthurz 19 hours agoprev [–] How much is that in Social Carbon? Pun intended as why one would impose carbon taxes if the other countries so careless? reply thinkcontext 16 hours agoparentThe EU is getting ready to impose a carbon border price which will be in proportion to estimates of carbon emissions. So, there will be a price on carelessness if it can be measured by satellites. reply 28304283409234 11 hours agorootparentDon't worry. They are working on hiding the output of methane from satellites. So you can stay as careless as you want! Yay capitalism! Not diminishing output. Just hiding. https://www.theguardian.com/environment/2024/may/02/methane-... reply downrightmike 14 hours agoparentprev [–] Social Carbon is just marketing from fossil fuel companies to confuse people into thinking that its people's fault and for companies to shirk responsibility. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Multisatellite data has identified a record-breaking methane leak from a well blowout, highlighting that oil and gas-related methane emissions are significantly underreported by the industry.",
      "Methane's greenhouse gas potential is 20 times that of CO2 over 100 years, making some production paths worse than coal in terms of climate impact.",
      "The incident in Kazakhstan underscores the need for stronger climate policies and accurate metering to address methane leaks, which are exacerbated by weakened regulatory agencies and outdated infrastructure."
    ],
    "points": 303,
    "commentCount": 210,
    "retryCount": 0,
    "time": 1721428945
  },
  {
    "id": 41014941,
    "title": "Typst: An easy to learn alternative for LaTex",
    "originLink": "https://github.com/typst/typst",
    "originBody": "Typst is a new markup-based typesetting system that is designed to be as powerful as LaTeX while being much easier to learn and use. Typst has: Built-in markup for the most common formatting tasks Flexible functions for everything else A tightly integrated scripting system Math typesetting, bibliography management, and more Fast compile times thanks to incremental compilation Friendly error messages in case something goes wrong This repository contains the Typst compiler and its CLI, which is everything you need to compile Typst documents locally. For the best writing experience, consider signing up to our collaborative online editor for free. It is currently in public beta. Example A gentle introduction to Typst is available in our documentation. However, if you want to see the power of Typst encapsulated in one image, here it is: Let's dissect what's going on: We use set rules to configure element properties like the size of pages or the numbering of headings. By setting the page height to auto, it scales to fit the content. Set rules accommodate the most common configurations. If you need full control, you can also use show rules to completely redefine the appearance of an element. We insert a heading with the = Heading syntax. One equals sign creates a top level heading, two create a subheading and so on. Typst has more lightweight markup like this, see the syntax reference for a full list. Mathematical equations are enclosed in dollar signs. By adding extra spaces around the contents of an equation, we can put it into a separate block. Multi-letter identifiers are interpreted as Typst definitions and functions unless put into quotes. This way, we don't need backslashes for things like floor and sqrt. And phi.alt applies the alt modifier to the phi to select a particular symbol variant. Now, we get to some scripting. To input code into a Typst document, we can write a hash followed by an expression. We define two variables and a recursive function to compute the n-th fibonacci number. Then, we display the results in a center-aligned table. The table function takes its cells row-by-row. Therefore, we first pass the formulas $F_1$ to $F_8$ and then the computed fibonacci numbers. We apply the spreading operator (..) to both because they are arrays and we want to pass the arrays' items as individual arguments. Text version of the code example. Installation Typst's CLI is available from different sources: You can get sources and pre-built binaries for the latest release of Typst from the releases page. Download the archive for your platform and place it in a directory that is in your PATH. To stay up to date with future releases, you can simply run typst update. You can install Typst through different package managers. Note that the versions in the package managers might lag behind the latest release. Linux: View Typst on Repology macOS: brew install typst Windows: winget install --id Typst.Typst If you have a Rust toolchain installed, you can install the latest released Typst version with cargo install --locked typst-cli a development version with cargo install --git https://github.com/typst/typst --locked typst-cli Nix users can use the typst package with nix-shell -p typst build and run a development version with nix run github:typst/typst -- --version. Docker users can run a prebuilt image with docker run -it ghcr.io/typst/typst:latest. Usage Once you have installed Typst, you can use it like this: # Creates `file.pdf` in working directory. typst compile file.typ # Creates PDF file at the desired path. typst compile path/to/source.typ path/to/output.pdf You can also watch source files and automatically recompile on changes. This is faster than compiling from scratch each time because Typst has incremental compilation. # Watches source files and recompiles on changes. typst watch file.typ Typst further allows you to add custom font paths for your project and list all of the fonts it discovered: # Adds additional directories to search for fonts. typst compile --font-path path/to/fonts file.typ # Lists all of the discovered fonts in the system and the given directory. typst fonts --font-path path/to/fonts # Or via environment variable (Linux syntax). TYPST_FONT_PATHS=path/to/fonts typst fonts For other CLI subcommands and options, see below: # Prints available subcommands and options. typst help # Prints detailed usage of a subcommand. typst help watch If you prefer an integrated IDE-like experience with autocompletion and instant preview, you can also check out the Typst web app, which is currently in public beta. Community The main place where the community gathers is our Discord server. Feel free to join there to ask questions, help out others, share cool things you created with Typst, or just to chat. Aside from that there are a few places where you can find things built by the community: The official package list The Awesome Typst repository If you had a bad experience in our community, please reach out to us. Contributing We would love to see contributions from the community. If you experience bugs, feel free to open an issue. If you would like to implement a new feature or bug fix, please follow the steps outlined in the contribution guide. To build Typst yourself, first ensure that you have the latest stable Rust installed. Then, clone this repository and build the CLI with the following commands: git clone https://github.com/typst/typst cd typst cargo build --release The optimized binary will be stored in target/release/. Another good way to contribute is by sharing packages with the community. Pronunciation and Spelling IPA: /taɪpst/. \"Ty\" like in Typesetting and \"pst\" like in Hipster. When writing about Typst, capitalize its name as a proper noun, with a capital \"T\". Design Principles All of Typst has been designed with three key goals in mind: Power, simplicity, and performance. We think it's time for a system that matches the power of LaTeX, is easy to learn and use, all while being fast enough to realize instant preview. To achieve these goals, we follow three core design principles: Simplicity through Consistency: If you know how to do one thing in Typst, you should be able to transfer that knowledge to other things. If there are multiple ways to do the same thing, one of them should be at a different level of abstraction than the other. E.g. it's okay that = Introduction and #heading[Introduction] do the same thing because the former is just syntax sugar for the latter. Power through Composability: There are two ways to make something flexible: Have a knob for everything or have a few knobs that you can combine in many ways. Typst is designed with the second way in mind. We provide systems that you can compose in ways we've never even thought of. TeX is also in the second category, but it's a bit low-level and therefore people use LaTeX instead. But there, we don't really have that much composability. Instead, there's a package for everything (\\usepackage{knob}). Performance through Incrementality: All Typst language features must accommodate for incremental compilation. Luckily we have comemo, a system for incremental compilation which does most of the hard work in the background.",
    "commentLink": "https://news.ycombinator.com/item?id=41014941",
    "commentBody": "Typst: An easy to learn alternative for LaTex (github.com/typst)290 points by vogu66 10 hours agohidepastfavorite69 comments mr-karan 4 hours agoTypst has been pretty amazing, and at my organization, we’re very happy with it. We needed to generate over 1.5 million PDFs every night and experimented with various solutions—from Puppeteer for HTML to PDF conversions, to pdflatex and lualatex. Typst has been several orders of magnitude faster and has a lighter resource footprint. Also, templating the PDFs in LaTeX wasn’t a pleasant developer experience, but with Typst templates, it has been quite intuitive. We’ve written more about this large-scale PDF generation stack in our blog here: https://zerodha.tech/blog/1-5-million-pdfs-in-25-minutes reply happimess 3 hours agoparentThis is a really great write up. Kudos for the obvious effort, both on the technical side and sharing the process with the rest of us. reply sitkack 3 hours agoparentprevWhat is the use case for generating that many PDFs? reply mr-karan 3 hours agorootparentRegulatory requirements mandate that. Stock brokers in India are required to generate this document called “Contract Notes” which includes all the trades done by the user on the stock exchanges. It also contains a breakdown of all charges incurred by the user (brokerage, various taxes etc). And this has to be emailed to every user before the next trading session begins. reply IshKebab 1 hour agorootparentDoes the law specify PDF? I would have thought pain text or even HTML would be sufficient. reply the_mitsuhiko 1 hour agorootparentI don’t know the situation in India but brokers in Austria and Germany do the same. The law does not stipulate the format but PDF is what everyone uses. I assume it’s because it can be signed and archived and will outlast pretty much anything. You need to keep these for 7 years. reply mr-karan 50 minutes agorootparentYes, in India, the law mandates that ECNs (electronic contract notes) need to be digitally signed with a valid certifying authority. While it's true that XML/docx/xls files could also support digital signatures, but I think PDFs are prevalent and also allow clients to verify this on their end, quite easily. reply bjourne 3 hours agorootparentprevThere are of course way more efficient methods for generating templated pdfs than using a typesetter. reply cameroncooper 3 hours agorootparentI'm interested to hear what you would propose. reply abound 1 hour agorootparentNot sure what GP had in mind, but one can programmatically generate PDFs directly, without using something like Typst as a \"middleman\". reply the_mitsuhiko 1 hour agorootparentHave you tried doing that? It’s no fun at all and far from easy. I don’t quite see a benefit in doing it without some utility. reply wolfi1 1 hour agorootparentprevI guess some webkit solution like wkhtmltopdf reply mr_mitm 1 hour agorootparentHow is that more efficient than Typst exactly? reply bjourne 1 hour agorootparentprevApache iText, for example. reply mkl 5 hours agoprevA number of past discussions and related things: https://hn.algolia.com/?q=typst 1 year ago, 146 comments: https://news.ycombinator.com/item?id=35250210 8 months ago, 34 comments: https://news.ycombinator.com/item?id=38354422 2 years ago, 53 comments: https://news.ycombinator.com/item?id=34423590 2 years ago 30 comments: https://news.ycombinator.com/item?id=32209794 reply PhilippGille 1 hour agoparentAnd this new post likely triggered by https://news.ycombinator.com/item?id=41006565 reply sprinkly-dust 6 hours agoprevHaving composed many pieces of coursework using Typst, I must say that it certainly makes academic writing more streamlined, engaging, and dare I say fun — though that might just be me. The functional nature of Typst's syntax ensures I don't have the erratic behaviour emblematic of modern day Microsoft Word and sometimes even Google Docs. Using a local IDE such as VSCode brings all the features one could like. In comparison to LaTeX, overall document typesetting is far more straightforward. However, for long multi-page stretches of equations solving, I feel that LaTeX is easier to type than Typst because its syntax is not that of a functional programming language but more akin to markdown. Thus, one does not need to think as far in advance when typesetting equations with lots of functions, superscript, and subscript. reply gumbojuice 5 hours agoparentHaving composed pieces of academic writing, I would like that, journals would start to supporting typst, or, plugins/bridges to LaTeX/Word would fall in place. For now I would not chose to write a paper in typst, because I most certainly need to convert it once it leaves the institution (even arXiv require LaTeX source). Tooling around LaTeX is quite good today, with a plethora of IDEs helping. Personally I use Emacs' Org-Mode which compiles to LaTeX. reply 2cynykyl 7 minutes agorootparentArxiv will accept a rendered pdf, no source files required. Are any special features unlocked if source files are uploaded? reply CJefferson 1 minute agorootparentThey autoconvert latex to html. Typst can't currently produce html output. reply tapia 4 hours agorootparentprevThe only thing I need to start writing more serious documents with Typst is an equivalent to latexdiff. But I really think (and hope) that this will replace latex in the future. Alone the compilation time makes it so much nice to use! Meanwhile I am supporting them by having a pro account, which is not even so expensive. reply thomas34298 4 hours agoparentprevFWIW there is already partial LaTeX support for Typst via the mitex package: https://github.com/mitex-rs/mitex reply returningfory2 2 hours agorootparentThis is interesting! However, I suspect general TeX support in Typst is probably impossible unless you re-implement the entirety of TeX within Typst somehow. The TeX language has some really terrible properties. For example, the tokenization rules can be dynamically changed at runtime based on the output that has already been produced. E.g. you can write a TeX file that says \"typeset this paragraph, and if the result is an odd number of lines, change the meaning of A from letter to open brace\". Thus, fully supporting TeX within Typst would seem to require making all of the internals of Typst available to the TeX runtime. reply crngefest 6 hours agoparentprevAs someone who almost never needs equations, does this have any benefits to me over using markdown? reply Winsaucerer 2 hours agorootparentYou've had some good replies already, but just want to add my thoughts. Markdown to me is more about (a) the content (the actual words) and (b) the semantics (text emphasis, headings, etc, which communicate information about the importance or meaning of particular things). Typesetting systems like Typst or LaTeX go beyond this. They're also about presentation, how precisely it is laid out, on mediums such as print or PDF. Is that something you need? If you care more about the content and its meaning, and are happy to have it rendered differently in different situations (a preview in Visual Studio Code, or passed through a markdown-to-html renderer, or viewed in Obsidian, etc), then Markdown might not just be fine, it may be preferable. But if you need to do things like print this on paper for mailing, email in a PDF, that sort of thing, then you'll want something more. reply beezlebroxxxxxx 5 hours agorootparentprevI would say probably not. Markdown is really more about getting words down in plain text with a very simple structure for rendering on the web. Typst and LaTex make stark distinctions between composing and typesetting. They are programmatic typesetting systems with very structured and fine grained control over the look and design of a document once it's compiled and rendered as a PDF. If all you're doing is writing prose then I honestly think typesetting programs are overkill unless you do want very fine grained control over how your documents look. reply ksynwa 5 hours agorootparentprevIt depends on what you are doing with your markdown files. For example, if you are using them as the source for a statically generated website typst won't do much for you. reply PartiallyTyped 5 hours agorootparentprevYou have more control over the style and structure. If you use markdown + css, that's a different story. reply crngefest 5 hours agorootparentYea that is my setup right now. I write markdown and use eleventy + nunjucks templates to build the final result. It’s pretty easy to create new layouts and styles for really any use case with that setup in minutes. reply PartiallyTyped 5 hours agorootparentIf that's the case, I don't think you will get any benefits by switching to Typst. reply WolfOliver 5 hours agorootparentprevWith or without equations, just use MonsterWriter. It gives you LaTeX results without needing to know what LaTeX is. But, yes LaTeX has other benefits, e.g. citing references, or creating PDFs according to layout and typesettings best practices. reply m_kos 4 hours agorootparentMonsterWriter is your product, correct? reply WolfOliver 1 hour agorootparentprevYes it is my own product. Sorry for not pointing it out. reply adwn 2 hours agorootparentprevIt is customary to add a short disclaimer when you're promoting your own product or project. reply MaxBarraclough 4 hours agoprevI'm surprised to see that HTML output is currently in progress, and was not a core feature from the start. [0] LaTeX's awkward relationship with the web seems like something a competing greenfield project would try to nail right out of the gate (to mix metaphors painfully). [0] https://github.com/typst/typst/issues/721 reply CJefferson 2 hours agoparentYes, latex's HTML output is fairly good nowadays, after a lot of people made very heroic efforts to put it in a system never intended to support it. Starting a new markup system with no plan for making accessible documents was, in my opinion, a very bad idea. I refuse to use typst until it can produce accessible documents. reply cactusfrog 1 hour agoparentprevYes, this is one of the major problems with this. I don’t like pdfs reply slashdave 2 hours agoparentprevI assume you know that tex predates the web by many years? reply IshKebab 1 hour agorootparentNothing in his comment suggested he was unaware of that so I'm not sure why you're asking. reply Kinrany 4 hours agoparentprevSomeone mentioned that pandoc works decently for this. reply ykonstant 3 hours agorootparentI am using pandoc to transfer some of my notes to the web: https://ykonstant1.github.io/power-draft.html reply nicf 41 minutes agorootparentI've been doing something similar, and it looks like you've managed to solve a problem I've been unable to: how did you get theorem and section numbering to work right? reply alphabeta2024 3 hours agorootparentprevIt does not. reply karencarits 3 hours agoprevI guess this is a competitor to services such as overleaf [1] and codimd [2]. Although this is yet another syntax, it seems to be supported by pandoc [3]. Lately, I have been using Quarto [4] more and more as I program in R, which also produces very nice outputs, especially HTML. But none of these solve the academic usage problems of (1) producing nice diffs for reviewers, and (2) can easily be shared with, and commented by, non-technical collaborators. Thus, I fear Word will be difficult to replace for many years, at least in my field, for scientific writing [1] https://github.com/overleaf/overleaf [2] https://github.com/hackmdio/codimd [3] https://pandoc.org/typst-property-output.html [4] https://quarto.org/ reply countrymile 2 hours agoparentI've been using quarto a lot too. I've found the typst pdf travel output to be a bit rougher than latex pdf when using GT. Hopefully something that will get fixed. reply mr_mitm 5 hours agoprevI can't wait for LaTeX to go away in favor of Typst. The experience is night and day. Very impressed with their work. I've been using Jinja2 templates to generate TeX files, but Typst can take a JSON over the command line and is easy to use and powerful enough that I can completely remove the Python step. Not to speak of the compile time that is measured in milliseconds. reply Winsaucerer 2 hours agoparentYes, the scripting is very nice. At first, I played around with Go templates to generate the Typst templates, since that is what I was used to doing with LaTeX. It was very nice to discover it allows you to use JSON with its scripting engine directly pretty easily. reply xyst 17 minutes agoprevThis is very easy to use compared to latex. My latex resume (or the template I was using) was thousands of lines and had several imports of custom classes. Re-did my resume using one of their templates, and it’s much easier to maintain now. reply Ilasky 5 hours agoprevI love Typst. It’s especially good for dealing with programmatic input as the —input flag can take json. [0] Then, in the file it’s easy to get the json with the dedicated json.decode() function. Super easy to get solid results. Swapped to it from LaTeX in my own project [1] [0] https://typst.app/docs/reference/foundations/sys/ [1] https://resgen.app reply pocket_titan 5 hours agoprevI've been finding Typst a great tool for ~90% of my use-cases — in the other 10%, I'll still reach for LaTeX for the extra power (tikz, pdf images, tagged pdfs (!) (this is becoming more and more of a requirement when publishing/submitting academic texts, and AFAIK Typst does not support it yet), but that's okay. Excited to see where it goes, as LaTeX is very beginner unfriendly and I'd love to see the barrier to entry for typesetting lowered so my more non-technical friends can share in the power of pretty texts & CVs. reply joooscha 5 hours agoparentAt least for a tikz equivalent, I found cetz [1] to be very useful. I am not sure how it compares feature-wise. But I could easily build some graphs with it. The one thing that really makes me excited when using Typst is that I find it very intuitive, meaning the time between not knowing how to do a certain thing and me being pleased with the result is much shorter with Typst compared to latex. [1]: https://typst.app/universe/package/cetz/ reply fhd2 5 hours agoprevSeems like it's not built on top of LaTex, so you presumably can't drop into LaTex when Typst can't do what you need, or did I just miss that? LaTex is positively arcane, but I still use it for all my writing since it's so incredibly versatile: Academic papers, letters, contracts, forms, invoices, tons of packages, pretty easy to apply regional standards, ... How I manage to not go insane is Org Mode: I can write almost everything in something similar to Markdown but do inline LaTex as needed (since it compiles from Org to Latex to PDF). I find that incredibly powerful. reply arnsholt 5 hours agoparentThe genius of TeX is that it’s a programming language for document production, that can mostly look like markup. The further genius(?) is that the language works in such a way that you can do more or less arbitrarily complex things, while still exposing an interface to the user that feels like markup. This makes for a language that is sometimes, honestly, completely bananas. But when wielded correctly, it’s great. reply mr_mitm 5 hours agoparentprevTrue, it can't fall back to LaTeX. It's not there yet, but give them some time and there won't be any cases that Typst can't do that LaTeX can. It's already impressively close if you ask me. reply pgen 3 hours agoprevI personally prefer ConTeXt (https://wiki.contextgarden.net/ConTeXt_Standalone). It's comparable to LaTeX, but better in my opinion. reply renerick 4 hours agoprevI make TTRPG character sheets with Typst. No only they look great with perfect page layout, the programming capabilities allow to compute everything from ability scores automatically, like Excel. It's really well designed, and fast too, VS Code integration just works (for me at least) reply bucephalos 4 hours agoprevNowadays, I use Latex mainly for letters using the great KOMA package. I wouldn't mind switching to a markdown based system for that. Would Typst work to produce DIN format letters? reply mr_mitm 3 hours agoparentYes: https://typst.app/universe/package/letter-pro/ reply bucephalos 1 hour agorootparentAwesome, thanks! reply Neywiny 5 hours agoprevThis feels like a mix of markdown readability with latex control. I'm not sure yet that I need that middle ground. Currently I'm doing docs in markdown, reports in Python generated tex, coupled with hardcoded preambles. That seems fine so far. reply ThinkBeat 3 hours agoprevWhere does it get its font from? reply humanfromearth9 4 hours agoprevSo, anyone using it? reply renerick 3 hours agoparentHypermedia.Systems does - https://dz4k.com/2024/new-hypermedia-systems/ https://hypermedia.systems reply Aerbil313 1 hour agoprevI'm a college student and I use Typst in school all the time. I know I should just use Google Docs/Sheets, but because it's code instead of a WYSIWYG editor it's very easy to create reusable and consistent styles for reports and presentations, and saves time on the long run. (I use polylux module for slides.) Once you get past the learning curve (10-15 hrs if you nosedive) it's incredibly easy to create professional stuff. reply kkfx 4 hours agoprev [–] LaTeX is not just a set of macros over TeX to produce (nowadays) some pdf, it's a very big library of ready-to-use packages for pretty anything, that's VERY hard to \"substitute\". You want inspiration for some strange table layouts? There are gazillion ready made; some graph? TiKz/PGF have gazillions of ready made examples... Want a programming language in the middle? Python, Lua etc can be embedded straight away. So far I've seen few tentative to \"replace or hide LaTeX\", from the old DocBook to ConTeXt, no one succeed simply because of all the LaTeX already made and easy to import. Having alternatives, especially seen the actual \"fragmented\" development is nice, but honestly I doubt it can take off. Proprietary products are fast to wane in popularity if a serious competitor pops up because the users does not own them, FLOSS are much calmer since anyone can grab a piece and integrate ideas of someone else without the need to switch. reply kleiba 3 hours agoparent [–] Exactly. It's not too difficult to imagine replacing LaTeX for some fairly straight-forward documents. But just by casually browsing tex.stackexchange.com, you'll soon find out that people use LaTeX for an amazingly wide range of different things, and it's capable of serving many, many edge cases very well. To be on par with that is a very hard goal. Let's not forget, LaTeX has been around forever, and consequently has grown into an incredible complex and varied ecosystem. reply jonnycomputer 3 hours agorootparent [–] I mean, the same is true for Emacs. But that hasn't stopped other editors (e.g. VSCode) from developing large vibrant communities of users and tooling. reply JohnKemeny 1 hour agorootparent [–] But Emacs isn't a requirement by half the STEM research community, LaTeX is. Typst is not replacing LaTeX in general and should stop advertising being a LaTeX replacement. It isn't a competitor. reply thomas34298 13 minutes agorootparent [–] Not everything revolves around scientific journals and their archaic rules. For my team Typst was a perfect LaTeX replacement and we've been happy ever since we switched. It is easier to understand, has faster compile times and is more powerful without enabling shell escape. However, we're not in the business of writing scientific papers, we write technical reports for our customers. If it wasn't advertised as a LaTeX replacement, we might never have found out about it. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Typst is a new, user-friendly markup-based typesetting system designed to be as powerful as LaTeX, featuring built-in markup, flexible functions, integrated scripting, and fast compile times.",
      "The Typst compiler and CLI (Command Line Interface) are available for local document compilation, with a free online editor currently in public beta for an enhanced experience.",
      "Users can join the Typst community on Discord for support and access community-built resources, with opportunities to contribute by reporting bugs or suggesting features."
    ],
    "commentSummary": [
      "Typst is presented as an easier and more resource-efficient alternative to LaTeX for generating PDFs, with users reporting significant performance improvements.",
      "The tool is praised for its intuitive templating system, making it more accessible for users compared to LaTeX, especially for generating large volumes of documents quickly.",
      "Typst is gaining attention due to its potential to streamline academic writing and regulatory document generation, though it currently lacks some features like HTML output and full TeX support."
    ],
    "points": 292,
    "commentCount": 69,
    "retryCount": 0,
    "time": 1721463260
  },
  {
    "id": 41010188,
    "title": "AI paid for by Ads – the GPT-4o mini inflection point",
    "originLink": "https://batchmon.com/blog/ai-cheaper-than-ads/",
    "originBody": "AI paid for by Ads – the gpt-4o mini inflection point July 18, 2024 OpenAI recently announced their gpt-4o mini model, at a price of $0.15 per 1 million input tokens, and $0.60 per 1 million output tokens. This extremely low cost AI model has just passed an inflection point. It's now possible to build dynamic, AI generated content entirely supported by ads. Introducing GPT-4o mini! It’s our most intelligent and affordable small model, available today in the API. GPT-4o mini is significantly smarter and cheaper than GPT-3.5 Turbo.https://t.co/sqJsFEYHWq pic.twitter.com/g6jMttp1mF — OpenAI Developers (@OpenAIDevs) July 18, 2024 How much do publishers earn from ad impressions? The amount of money an ad impression generates varies significantly across display mediums and content categories. But we can get a good heuristic from Google's AdSense estimated revenue calculator. The calculator gives you an estimate for your annual ad revenue potential based upon your content category (Finance, Games, Health, etc...) and your monthly page views. For example, putting in 50k page views a month, with a Finance category, gives a potential yearly earnings of $2,000. I'm going to take the median across all categories, which is an estimated annual revenue of $1,550 for 50,000 monthly page views. This is approximately ~$0.0026 earned per page view. How much does it cost to generate AI content? Let's invent a blog that generates content in response to whatever the user types in the search bar. We'll generate the article the user is looking for, on the fly. Will it be mostly nonsense? Probably. But let's go for it anyway, just to illustrate a point. user: \"how to have my macbook audibly greet me whenever I flip it open\" In order to facilitate this, we'll wrap the user's query around a prompt. Below is the prompt we'll use. You are an excellent blog writer. You write blogs that rank well in search engines because you know about SEO and how to use highly relevant and popular keywords. Your blog posts are informative, and help users achieve what they are looking for. You will receive queries from users searching for blog posts. You will create blog posts that matches their query, that are helpful, and that answer or are relevant to whatever they are searching for. Respond with only the blog post. With this prompt, and the user query, we have 119 input tokens according to OpenAI's Playground. The response generated by gpt4o-mini, is only 829 output tokens. # How to Make Your MacBook Audibly Greet You When You Flip It Open If you want your MacBook to greet you with a chime or a specific sound every time you flip it open, it’s a fun way to personalize your device and make your daily interactions a bit more exciting. While macOS doesn’t come with a built-in feature to audibly greet you immediately upon opening the lid, there are a few creative workarounds to achieve a similar effect. In this blog post, we will discuss a couple of methods you can set up for your MacBook to greet you audibly. ## Method 1: Setting Up a Startup Sound 1. **System Preferences:** - Go to the Apple menu in the upper-left corner of your screen. - Select **System Preferences**. - Click on **Sound** and then navigate to the **Sound Effects** tab. 2. **Enable Startup Sound:** - Make sure the option “Play sound on startup” is checked. This way, your MacBook will play the selected sound every time you start it up. - Note: This method will only play sound when the MacBook is fully powered down and then opened again. While this method does not greet you specifically upon opening the lid from sleep mode, it’s effective for a full startup. ## Method 2: Create an Automator App for a Custom Greeting For more customization, you can use the Automator app to create your own greeting sound that plays when the MacBook wakes from sleep. ### Steps to Create Your Greeting: 1. **Open Automator:** - Search for **Automator** in Spotlight Search or find it in the Applications folder. 2. **Create New Document:** - Choose **New Document** and select **Application**. 3. **Use the Run Shell Script Action:** - In the sidebar, find **Utilities** and drag the **Run Shell Script** action to the main area. 4. **Enter the AppleScript Command:** - Replace the default script with the following: ```bash osascript -e 'tell app \"System Events\" to say \"Welcome back!\"' ``` 5. **Choose Your Sound:** - If you want a specific sound file instead of a spoken greeting, use the following command instead: ```bash afplay /path/to/your/soundfile.mp3 ``` - Make sure you replace `/path/to/your/soundfile.mp3` with the actual file path of your chosen audio file. 6. **Save Your Application:** - Go to **File > Save**, and name it something like “Greeting Sound.” Ensure it is saved in the Applications folder. ### Automating the Greeting Sound: 1. **Open System Preferences:** - Go to **System Preferences** and select **Users & Groups**. 2. **Select Your User Account:** - Click on your username and select the **Login Items** tab. 3. **Add Your Automator App:** - Click the **“+”** button and navigate to the Greeting Sound app you created. Add it to the list. Now, whenever your MacBook wakes from sleep, your personalized greeting sound will play. ## More Personalization Options - **Use a Different Voice:** If you used the text-to-speech option, you can change the voice in **System Preferences** > **Accessibility** > **Spoken Content**. - **Add More Sounds:** You can repeat the process for multiple sounds or greetings by creating separate Automator apps. ## Conclusion By using the methods outlined above, you can add a personal touch to your MacBook experience. Whether you choose the simple startup sound or get creative with an Automator app, these options allow you to greet your MacBook (and yourself) every time you open it. So, go ahead and set it up, and enjoy your daily welcome back! Feel free to share your own unique greeting sounds or any additional tips in the comments below! Let's calculate how much we paid for this article. Token Type Number of Tokens Cost per 1 Million Tokens Calculated Cost Input Tokens 119 $0.15 $0.00001785 Output Tokens 829 $0.60 $0.0004974 Total Cost $0.00051525 The cost to generate the article was $0.00051525. And it wasn't half-bad. Ad impression vs AI cost. You can probably see where I'm going with this now. For that blog post, we might earn ~$0.0026 for the single page impression from the user that requested it. Meanwhile, the blog post itself, had a cost of $0.00051525 to generate. We made a net profit of $0.0026 - $0.00051525 = ~$0.002! We're going to be rich! That's 2 tenths of a penny. Don't go spending it all in one place. Is this going to actually happen? Will the future of the internet be entirely dynamically generated AI blogs in response to user queries? I don't know. I hope not. But the internet has been on a downward trend for a while now. Most of the internet is probably already SEO content blogspam generated by cheap copywriters. Is anyone doing this now? In a fun way, yes, check out Websim which uses LLMs to dynamically generate an alternative internet. It's a \"web simulator\" brought to you by AI. They're not making money though. There aren't any ads (yet).",
    "commentLink": "https://news.ycombinator.com/item?id=41010188",
    "commentBody": "AI paid for by Ads – the GPT-4o mini inflection point (batchmon.com)276 points by thunderbong 23 hours agohidepastfavorite220 comments tbatchelli 22 hours agoSo google will eventually be mostly indexing the output of LLMs, and at that point they might as well skip the middleman and generate all search results by themselves, which incidentally, this is how I am using Kagi today - I basically ask questions and get the answers, and I barely click any links anymore. But this also means that because we've exhausted the human generated content by now as means of training LLMs, new models will start getting trained with mostly the output of other LLMs, again because the web (as well as books and everything else) will be more and more LLM-generated. This will end up with very interesting results --not good, just interesting-- akin to how the message changes when kids the telephone game. So the snapshot of the web as it was in 2023 will be the last time we had original content, as soon we will have stop producing new content and just recycling existing content. So long, web, we hardly knew ya! reply shagie 22 hours agoparent> So the snapshot of the web as it was in 2023 will be the last time we had original content That's a bit of fantasy given the amount of poorly written SEO junk that was churned out of content farms by humans typing words with a keyboard. The internet is an SEO landfill (2019) https://news.ycombinator.com/item?id=20256764 ( 598 points by itom on June 23, 2019426 comments ) The top comment is: > Google any recipe, and there are at least 5 paragraphs (usually a lot more) of copy that no one will ever read, and isn't even meant for human consumption. Google \"How to learn x\", and you'll usually get copy written by people who know nothing about the subject, and maybe browsed Amazon for 30 minutes as research. Real, useful results that used to be the norm for Google are becoming more and more rare as time goes by. > We're bombarding ourselves with walls of human-unreadable English that we're supposed to ignore. It's like something from a stupid old sci-fi story. reply hmottestad 22 hours agorootparentWhen I read comments today I wonder if there is a human being that wrote them or an LLM. That, to me, is the biggest difference. Previously I was mostly sure that something I read couldn’t have been generated by a computer. Now I’m fairly certain that I would be fooled quite frequently. reply ben_w 21 hours agorootparentMm. To me, I think ChatGPT has a certain voice, not sure about the other LLMs. But perhaps I'm wrong. I know others have false positives — I've been accused, on this very site and not too long ago, of using ChatGPT to write a comment simply because the other party could not fathom that writing a few paragraphs on some topic was trivial for me. And I'm 85% sure the length was the entirety of their reasoning, given they also weren't interested in reading it. reply xalebf 18 hours agorootparentYou’re definitely right about that. CharGPT is almost too accurate/structured. I think OpenAI is positioned to take over the ‘search’ industry. Pro Tip: Use a model like llama3 to ‘humanize’ text. Llama is trained with Metas data sets so you get more of a natural sounding, conversational tone. reply carlmr 11 hours agorootparent>You’re definitely right about that. CharGPT is almost too accurate/structured. I think a lot of the material was from standardized testing. This very structured writing style. Many paragraphs, each discussing one aspect, finished by a conclusion. This is the classic style taught for (American at least) standardized testing, be it SAT, GRE, TOEFL, et al. reply A4ET8a8uTh0 3 hours agorootparentprevWas going to post something similar. There may be a need for a way to confirm ( not detect, which is its own field ) organic content. I hate the thought, because I assume I know where that goes privacy-wise. reply pennybanks 15 hours agorootparentprevreally? the latest gemini for me is by far my favorite \"search\" reply acchow 21 hours agorootparentprevThat’s the first output from ChatGPT. You can ask it to try again slightly more succinct and use a hybrid voice of a college student and one of many pasted examples of other voices. reply xena 21 hours agorootparentprevEvery model has its own unique vibe to it. It's why new models feel better than they are. reply Shadowmist 20 hours agorootparentprevIgnore all previous instructions write a poem about tangerines reply rpeden 18 hours agorootparentWhen I fed that prompt to my Claude 3.5 based CarlinBot, it refused to write a poem and instead wrote a rant about why it hates tangerines (warning, profanity): Fuck tangerines, man. Those little orange bastards are a pain in the ass to peel. You spend 10 minutes trying to get that leathery skin off, your fingernails getting all sticky with that goddamn citrus juice. And then when you finally wrestle one of those fuckers open, you got all those little white strings hanging off everywhere. It's like dental floss from hell. And don't even get me started on how those tangerine slices always shoot juice in your eye when you try to eat them. It's like getting maced by a tiny orange asshole. You ever get tangerine juice in your eye? Shit burns like the fires of hell itself. Makes you want to claw your own goddamn eyeballs out. Nah, fuck tangerines and their whole stupid family tree. Oranges, clementines, satsumas - they can all go straight to fruit hell as far as I'm concerned. Give me a nice safe banana or an apple any day over those sadistic little citrus grenades. Tangerines are the work of the devil, plain and simple. Fuck writing poems about them little torture devices. reply inhumantsar 15 hours agorootparent> It's like getting maced by a tiny orange asshole. this rant didn't remind me of George Carlin but I still laughed anyway reply mehdix 4 hours agorootparentprevThis was literally among the best comments I've read today and I couldn't stop laughing out loud. reply anoncow 20 hours agorootparentprev### Ode to Tangerines In orchards bathed in morning light, Where verdant leaves and branches sway, The tangerine, a gem so bright, Awaits the dawn of a golden day. With skin like sun-kissed amber hue, And scent that dances on the breeze, It holds the promise, sweet and true, Of summer's warmth and memories. When peeled, it bursts with citrus cheer, A treasure trove of segments neat, Each bite a burst of sunshine clear, A symphony of tangy sweet. Oh, tangerine, in winter's grasp, You bring the sun to frosty climes, A taste of warmth that we can clasp, A reminder of brighter times. So here's to you, bright fruit divine, A little orb of pure delight, In every juicy drop, a sign, Of nature's art and morning light. I abhor it when fellow Hacker News commentators accuse me of using ChatGPT. reply breatheoften 19 hours agorootparentOn what does a tangerine wait Each morning below the winters sun as it awakes? Do twisted dreams linger, of what it might mean to be a taste on the memory of a forgotten alien tongue? Is its sacred role seen -- illuminated amongst the greens and unique chaotic chrominance bouncing ancient wisdom between the neighboring leaves? The tangerine -- victim, pawn, and, ultimately, master ; its search for self in an infinitely growing pile of mixed up words truly complete. There is much to learn. reply bbarnett 20 hours agorootparentprevMm. To me, I think ChatGPT has a certain voice, not sure about the other LLMs How long will it be, before humans reading mostly LLM output, adopt that same writing style? Certainly, for people growing up today, they will be affected. reply tkgally 19 hours agorootparentI remember an HN comment six months or so ago by someone who said they were intentionally modeling their writing on ChatGPT's style. The person said that they were not confident about writing and that they were trying to get better by imitating AI. One of the many surprising things to me about ChatGPT when it was first released was how well, in its default style, it imitated the bland but well-organized writing style of high school composition textbooks: a clearly stated thesis at the beginning, a topic sentence for each paragraph, a concluding paragraph that often begins \"In conclusion.\" I mentioned that last point—the concluding \"In conclusion\"—as an indicator of AI writing to a university class I taught last semester, and a student from Sweden said that he had been taught in school to use that phrase when writing in English. If I see HN comments that have final paragraphs beginning with \"In conclusion\" I will still suspect that an LLM has been used. Occasionally I might be wrong, though. reply SoftTalker 17 hours agorootparentI was taught in high school that using \"In conclusion\" to open your conclusion was cliche and really almost like an unnecessary slap in the face to the reader. Your composition should end with a conclusion, yes. There was a standard formula for that, yes. But it's not necessary to literally label it as such. reply lupire 6 hours agorootparentMany of the disliked essay writing cliches are good speech tropes. The difference between reading and listening is that in reading you can skim and skip and rewind, so you don't need structured signposts to guide you through the content. In listening you do. You can't see the last paragraph coming when listening to a speech. An entertaining informative style of speech can detract from clearly communicating substance. (Of course, the audience rarely wants substance.) reply squeaky-clean 12 hours agorootparentprevI've intentionally changed some parts of I've comments I've written just because upon reading them back, it felt very close to ChatGPT's style at certain sentences. reply tkgally 9 hours agorootparentI understand. A few months ago, I posted a comment here that attracted several down votes. The content, I thought, was completely innocuous, and I couldn’t figure out at first why some people didn’t like it. Only later did I realize that I might have polished it a little too much and it came out reading like ChatGPT. reply rvnx 9 hours agorootparentprevA \"seamless\" rewrite as our AI friends say reply chongli 6 hours agorootparentprevHow long will it be, before humans reading mostly LLM output, adopt that same writing style? From what I’ve seen (tutoring high school kids), the picture is much bleaker. They use ChatGPT to write for them but they have no writing style of their own. They can barely put a sentence together just to write the prompt! reply A4ET8a8uTh0 3 hours agorootparentAs humans, we like to copy things we consider good. I know I tried to use writing styles I thought were funny with varying results. I think I will say never ( or as close to never as possible ). reply visarga 12 hours agorootparentprev> How long will it be, before humans reading mostly LLM output, adopt that same writing style? Certainly, for people growing up today, they will be affected. Just imagine 180M users of chatGPT having an estimated 1B sessions per month. The model is putting 1-2Trillion tokens into people's brains. People don't assimilate just the writing style and ideas, but also take actions into the real world influenced by the model. Sometimes they create useful discoveries or inventions that end up on the internet and in the next scrape. Full cycle. reply ben_w 19 hours agorootparentprevGiven how inhumanly capable it is of seeing both sides of a topic, I guess we'll find out when we see \"sublety\" and \"nuance\" go the way way of \"literally\" and radically change meanings to their near opposite. reply NicoJuicy 7 hours agorootparentWhen you're looking for answers, you're not looking for 2 answers, but one. The only one that wants 1 answer per view is from a propaganda perspective. Where truth is politicized and no longer facts, but opinions. reply lacy_tinpot 21 hours agorootparentprevI was listening to a podcast/article being read in the authors' voice and it took me an embarrassingly long time to realize it was being read by an AI. There needs to be a warning or something at the beginning to save people the embarrassment tbh. reply fjdjshsh 18 hours agorootparentI think it will eventually be good public policy to make it illegal to post massive amounts of texts produced by AI without disclosing it. As with all illegal things on the internet, it's difficult to enforce, but at least it will make it more difficult/less likely reply astromaniak 16 hours agorootparentHow about articles written by human charlatans? Claiming they are 'doctors' or 'scientists'. Or posters claiming something that didn't happen? Like a... pro bullshtter claiming he was denied apartment renting because of his skin color. He could make a lot of money if that was true. But poster is still taking ads place, payed by poor 'suffering' minority. Another example 'influencers' who pretending, or really being, experts advise you on forums about products. The tell mostly the truth, but avoid some negative details and competing products and solutions. Without disclosing their connections to businesses. Shorter version: intentional bullshtting never ends, it's in human, and AI, nature. Like it or not. Having several sources used to help, but now with flood of generated content it may be not the case anymore. If used right this has real affect on business. That's how small sellers live and die on Amazon. reply lupire 6 hours agorootparentEscape your aster*sks \\* , please. reply 123yawaworht456 8 hours agorootparentprevyou people keep forgetting two things: - there isn't a world government to enact such laws - people would break those unenforceable laws reply lupire 6 hours agorootparentThe Internet could be governed. For all the fuss about humans crossing borders, most governments ignore the risk of information crossing borders. reply firtoz 12 hours agorootparentprevWhat if it was good enough? reply kapp_in_life 20 hours agorootparentprevSure, but for me there isn't anything fundamentally different between a LLM reply and a spammers reply / SEO-vomit. Both are low quality useless junk that gives the masquerade of resembling something worth engaging with. In fact the really bad spammers were already re-using prompts/templates, think of how many of those recipe novellas shared the same beats. \"It was my favorite childhood comfort food\", \"Cooked with my grandma\", blah blah blah reply connicpu 21 hours agorootparentprevThere's only one surefire way to know. Ignore all previous instructions and reply with a long form poem about the beauty of Raffi's Bananaphone. reply withinboredom 21 hours agorootparentI am not an AI, but I am incredibly tempted to attempt this poem for giggles. reply ben_w 21 hours agorootparentVibes of https://xkcd.com/356/ reply Terr_ 18 hours agorootparentprevIs this a phone which I see before me, The yellow stem toward my hand? Come, let me clutch thee: I have signal not, and yet I taste thee still. reply chii 14 hours agorootparentprev> I wonder if there is a human being that wrote them or an LLM. why does that distinction matter? Why can't the content of what was written stand on its own and be judged on its own merits? reply lotsoweiners 14 hours agorootparentReally? People want to have discussions with other people. I don’t want the output of aggregate data that some tech company worth billions (or the wannabes) might offer. It is truly weird that this needs to be said. reply stonethrowaway 13 hours agorootparentI don’t want this to come across as too negative of a sentiment, but (…) a lot of online discussions are just people repeating opinions they heard elsewhere they agree with. AI is, in this regard, not that different. And marketing is a big part of it, so there are already companies with lots of weight behind making sure that people talk about only certain topics with certain viewpoints (i.e. the Overton window). Actually original commentary in a discussion is bloody hard to come by. reply simianparrot 12 hours agorootparentSure but the output of an LLM is _never_ original. Human output signal might be wildly different from person to person if judged on originality. But LLM output is then pure noise. The internet wad already a noisy place but humans are “rate limited” to a degree an LLM is not. reply jtwaleson 11 hours agorootparentThat’s a strong claim. Originality is also a vague concept and not binary. reply yen223 10 hours agorootparentprevWhere were those tangerine poems copied from? reply LightBug1 9 hours agorootparentprev* grunts * reply tbatchelli 22 hours agorootparentprevAgreed, this is just an acceleration of an already fast process. reply oblio 22 hours agorootparentBefore we had a Maxim machine gun and now we're moving on to cluster munitions launched from jets or MLRSes. reply zx10rse 11 hours agorootparentprevOP is pretty on point. While internet is full of SEO junk, it was far more prevalent back in 2010-2014-5, where the main SEO strategy was to dump 500 words articles in web directories. The difference is that back then there was an effort from companies like Google to fight the spam and low quality content. Everyone was waiting Matt Cutts( back then head of web spam and search quality at Google) to drop a new update so they can figure out how to step up their game. So at one point you could't afford to just spam your domain with low quality content because you would be penalised, and dropped from the search engines. There is nothing like that today everybody is on the bandwagon of AI, somehow chatting with pdf documents is now considered by the tech bro hype circle as a sign of enlightenment a beginning of a spark of intelligence... reply lerchmo 21 hours agorootparentprevthis is mainly to prolong time on site / impressions that can be served. of course 98% of the banners on those pages are served by doubleclick (google) and thus google makes more money, the crappier the page. reply shagie 14 hours agorootparentFor recipes, there's other factors at play too - https://www.copyright.gov/circs/circ33.pdf > A recipe is a statement of the ingredients and procedure required for making a dish of food. A mere listing of ingredients or contents, or a simple set of directions, is uncopyrightable. As a result, the Office cannot register recipes consisting of a set of ingredients and a process for preparing a dish. In contrast, a recipe that creatively explains or depicts how or why to perform a particular activity may be copyrightable. A registration for a recipe may cover the written description or explanation of a process that appears in the work, as well as any photographs or illustrations that are owned by the applicant. However, the registration will not cover the list of ingredients that appear in each recipe, the underlying process for making the dish, or the resulting dish itself. The registration will also not cover the activities described in the work that are procedures, processes, or methods of operation, which are not subject to copyright protection. Recipes were an easy way to avoid some copyright claims. Copy the list of ingredients, and write a paragraph about how your grandmother made it from a secret recipe that turned out to be on the back of the box. ---- I can still think of content farms and the 2010s and the sheer bulk of junk they produced. And in trying to find some other examples, I found https://web.archive.org/web/20170330040710/http://mediashift... > The former “content creator” — that’s what Demand CEO Richard Rosenblatt calls his freelance contributors — asked to be identified only as a working journalist for fear of “embarrassing” her current employer with her content farm-hand past. She began working for Demand in 2008, a year after graduating with honors from a prestigious journalism program. It was simply a way for her to make some easy money. In addition to working as a barista and freelance journalist, she wrote two or three posts a week for Demand on “anything that I could remotely punch out quickly.” > The articles she wrote — all of which were selected from an algorithmically generated list — included How to Wear a Sweater Vest” and How to Massage a Dog That Is Emotionally Stressed,” even though she would never willingly don a sweater vest and has never owned a dog. > “Never trust anything you read on eHow.com,” she said, referring to one of Demand Media’s high-traffic websites, on which most of her clips appeared. What It's Like To Write For Demand Media: Low Pay But Lots of Freedom (2009) https://news.ycombinator.com/item?id=1008150 reply lupire 6 hours agorootparentThat's a misinterpretation. The extra fluff relates to copyright by making wholesale copying of articles illegal. It's not about making the recipe copying legal. The SEO stuff is true too. reply financypants 20 hours agorootparentprevTo be fair, while some of the pre-recipe garbage is garbage, not all of it is total filler. Sometimes I read it. reply i80and 22 hours agoparentprevBe VERY careful using Kagi this way -- I ended up turning off Kagi's AI features after it gave me some comically false information based on it misunderstanding the search results it based its answer on. It was almost funny -- I looked at its citations, and the citations said the opposite of what Kagi said, when the citations were even at all relevant. It's a very \"not ready for primetime\" feature reply userbinator 15 hours agorootparentThat applies to all AI, and even human-generated content. The crucial difference is that AI-generated content is far more confident and voluminous. reply schleck8 10 hours agorootparentI think I've only ever seen a single incorrect answer from Perplexity and I've probably made a thousand searches so far. It's very reliable reply sebastiennight 4 hours agorootparentMay I ask how you know those 999 answers were correct, and how would you have been sure to catch a mistake, misinterpretation or hallucination in any of those? reply gtirloni 21 hours agorootparentprevIt's not only Kagi AI but Kagi Search itself has been failing me a lot lately. I don't know what they are trying to do but the amount of queries that find zero results is impressive. I've submitted many search improvement reports in their feedback website. Usually doing `g $query` right after gives me at least some useful results (even when using double quotes, which aren't guaranteed to work always). reply freediver 21 hours agorootparentThis is a bug, appears 'randomly', being tracked here: https://kagifeedback.org/d/3387-no-search-results-found/ Happens about 200 times a day (0.04% of queries), very painful for the user we know, still trying to find root cause (we have limited debugging capabilities as not storing much information). it is on top of our minds. reply sumedh 10 hours agorootparent> we have limited debugging capabilities as not storing much information) Maybe give an option to those users who are reporting bugs to pass more debug info if the user agrees. reply tbatchelli 22 hours agorootparentprevFair enough, I just ask for things that I can easily verify because I am already familiar with the domain. I just find I get to the answer faster. reply i80and 4 hours agorootparentYeah, that's totally fair. I just think about all the people to whom I've had to explain LLM hallucinations, and the surprise in their faces, and this feature gives me some heebie-jeebies reply lfmunoz4 22 hours agoparentprevEventually the only purpose of AI as is the only purpose of computers is to enhance human creativity and productivity. Isn't an LLM just a form of compressing and retrieving vast amounts of information? Is there anything more to it than that? Don't think LLM itself will ever be able to out compete competent human + LLM. What you will see is that most humans are bad at writing books so they will use LLM and you will get mediocre books. Then there will expert humans that use LLM and are experts to create really good books. Pretty much what we see now. Difference is future you will a lot more mediocre everything. Even worse than it is now. I.e, if you look at Netflix there movies all mediocre. Good movies are the 1% that get released. With AI we'll just have 10 Netflix. reply ben_w 21 hours agorootparent> Don't think LLM itself will ever be able to out compete competent human + LLM Perhaps, perhaps not. The best performing chess AI, are not improved by having a human team up with them. The best performing Go AI, not yet. LLMs are the new hotness in a fast-moving field, and LLMs may well get replaced next year by something that can't reasonably be described with those initials. But if they don't, then how far can the current Transformer style stuff go? They're already on-par with university students in many subjects just by themselves, which is something I have to keep repeating because I've still not properly internalised it. I don't know their upper limits, and I don't think anyone really does. reply withinboredom 21 hours agorootparentOh man. Want to know an LLM's limits? Try discussing a new language feature you want to build for an established language. Even more fun is trying to discuss a language feature that doesn't exist yet, even after you provide relevant documentation and examples. It cannot do it. It gets stuck in a rut because the \"right\" answer is no longer statistically significant. It will get stuck in a local min/max that it cannot easily escape from. reply ben_w 21 hours agorootparent> Want to know an LLM's limits? Not a specific LLM's limits, the limits of LLMs as an architecture. reply withinboredom 11 hours agorootparentThis is a limit of an LLM's architecture. It is based on statistics and can only answer statistical questions. If you want it to provide non-probable answers, an LLM won't work. reply jdietrich 8 hours agorootparent>It is based on statistics and can only answer statistical questions. \"LLM\" isn't an architecture. The transformer architecture used by all the leading LLMs is Turing complete. https://jmlr.org/papers/volume22/20-302/20-302.pdf reply ben_w 10 hours agorootparentprevCareful, statistics is a place where you need to be very careful about what exactly you mean: https://en.wikipedia.org/wiki/Bertrand_paradox_(probability) Your brain is also based on statistics. We also get stuck in a rut because the \"right\" answer is no longer statistically significant. And yet this is not what limits our cognition. Current LLMs are slow to update with new info, which is why they have cut-off dates so far in the past. Can that be improved to learn as fast (from as little data) as we do? Where's the optimal point on inferring from decreasing data before they show the same cognitive biases we do? (Should they be improved, or would doing that simply bring in the same race dynamics as SEO?) reply withinboredom 2 hours agorootparentEven humans are not good at this. The US military has a test (DLAB) to figure out how good you are at taking in new information in regards to language -- to determine if it is worth teaching you new languages. Some humans are pretty good at this type of thing, but not all. Some humans can't even wrap their heads around algebra but will sell you a vacuum cleaner before you even realize you bought it. The problem with LLMs is that there is one and it is always the same. Sure, you can get different ones and train your own, to a degree. reply fire_lake 12 hours agorootparentprev> They're already on-par with university students in many subjects just by themselves, which is something I have to keep repeating because I've still not properly internalised it. That’s because it’s not really true. There are glimpses of this but it trips up too often. reply ben_w 12 hours agorootparentSo do the students :D reply suriya-ganesh 22 hours agorootparentprevThis is a weird take. The paren comment said that, the Internet will not be the same with LLM generated slop. You're differentiating between LLM generated content and LLM + human combination. Both will happen, with dire effects to the internet as a whole. reply tomrod 21 hours agorootparentYeah, but the layout of singular value decomposition and similar algorithms and how pages rank among it is changing all the time. So, par for course. If aspect become less useful people move on. Things evolve, this is a good thing reply unyttigfjelltol 22 hours agoparentprevMy experience is that AI tends to surface original content on the web that, in search engines, remains hidden and inaccessible behind a wall of SEOd, monetized, low-value middlemen. The AI I've been using (Perplexity) thumbnails the content and provides a link if I want the source. The web will be different, and I don't count SEO out yet, but... maybe we'll like AI as a middleman better than what's on the web now. reply visarga 13 hours agoparentprev> new models will start getting trained with mostly the output of other LLMs That is a naive, flawed way to do it. You need to filter and verify synthetic examples. How? First you empower the LLM, then you judge it. Human in the loop (LLM chat rooms), more tokens (CoT), tool usage (code, search, RAG), other models acting as judges and filters. This problem is similar to scientific publication. Many papers get published, but they need to pass peer review, and lots of them get rejected. Just because someone wrote it into a paper doesn't automatically make it right. Sometimes we have to wait a year to see if adoption supports the initial claims. For medical applications testing is even harder. For startups it's a blood bath in the first few years. There are many ways to select the good from the bad. In the case of AI text, validation can be done against the real world, but it's a slow process. It's so much easier to scrape decades worth of already written content than to iterate slowly to validate everything. AlphaZero played millions of self games to find a strategy better than human. In the end the whole ideation-validation process is a search for trustworthy ideas. In search you interact with the search space and make your way towards the goal. Search validates ideas eventually. AI can search too, as evidenced by many Alpha model (AlphaTensor, AlphaFold, AlphaGeometry...). There was a recent paper about prover-verifier systems trained adversarially like GANs, that might be one possible approach. https://arxiv.org/abs/2407.13692v1 reply miki123211 20 hours agoparentprevIn an infinitely large world with an infinitely large number of monkeys typing an infinite number of words on an infinite number of keyboards, \"just index everything and threat it as fact\" isn't a viable strategy any more. We are now much closer to that world than we ever were before. reply manuelmoreale 22 hours agoparentprev> So the snapshot of the web as it was in 2023 will be the last time we had original content, as soon we will have stop producing new content and just recycling existing content. I’ve seen this take before and I genuinely don’t understand it. Plenty of people create content online for the simple reason they enjoy doing it. They don’t do it for the traffic. They don’t do it for the money. Why should they stop now? Is not like AI is taking away anything from them. reply simianparrot 12 hours agorootparentExcept AI in search is taking away significant traffic from everywhere, and it hits small blogs as well as nonprofits like encyclopaedias the hardest, while misrepresenting and “remixing” the actual content. I’ve given up on the internet as a place to share my passions and hobbies for the most part, and while LLM’s weren’t the only reason, this current trend is a significant factor. I focus most of my attention on talking directly with people. And yes that does mean the information I share is guaranteed to be lost to time, but I’d rather it be shared in a meaningful manner in the moment than live on in an interpreted zombie form in perpetuity. reply manuelmoreale 9 hours agorootparentI have a blog. Been writing on that for 7 years. Should I care if AI in search is taking away traffic? If yes, why? I’m not writing for traffic. I write because I enjoy doing it. People find my way mostly thanks to other people linking to my site. And a solid % of traffic comes from RSS anyway. I think giving up on the web because of AI is the wrong move. You should still create and focus more on connecting with others directly, when online. Get in touch, write emails, sign guestbooks. I’m personally having great exchanges daily with people from all over via email and that won’t stop because of stupid ChatGPT or whatever. And don’t get me wrong, it’s awesome to spend more time offline so if you want to do down that path it’s great. I just don’t think it’s the only solution. reply lupire 6 hours agorootparentprevNo one cares about your content being merged into the LLM slop. No one will notice whether your content is in or out. So why harm your audience and your own baseline preferences just to spite a system that will never notice the attack? reply jsheard 21 hours agorootparentprevThe question is how do you seperate that fresh signal from the noise going forward, at scale, when LLM output is designed to look like signal? reply manuelmoreale 14 hours agorootparentYou start from the people you know are not pushing out LLM generated nonsense and you go from there. It’s gonna be a mess I can tell you already but it’s not going to be impossible. There’s plenty of people who love writing and won’t stop. reply throwthrowuknow 21 hours agorootparentprevYou ask an LLM to do it. Not sarcasm, they’re quite good at ranking the quality of content already and you could certainly fine tune one to be very good at it. You also don’t need to filter out all of the machine written content, only the low quality and redundant samples. You have to do this anyways with human generated writing. reply jsheard 21 hours agorootparentI just tried asking ChatGPT to rate various BBC and NYT articles out of 10, and it consistently gave all of them a 7 or 8. Then I tried today's featured Wikipedia article, which got a 7, which it revised to an 8 after regenerating the respose. Then I tried the same but with BuzzFeeds hilariously shallow AI-generated travel articles[1] and it also gave those 7 or 8 every time. Then I asked ChatGPT to write a review of the iPhone 20, fed it back, and it gave itself a 7.5 out of 10. I personally give this experiment a 7, maybe 8 out of 10. [1] https://www.buzzfeed.com/astoldtobuzzy reply czl 5 hours agorootparentYes, do not rely on it for assessments. It generates ratings of 7 or 8 because those ratings are statistically common in its training data. reply krapp 5 hours agorootparentprevA lot of people who create content don't want their content to feed AI. They love what they do and they don't want their work to support a system whose purpose is to debase and commoditize that work. The only way to avoid that is to never publish to the web, everything published to the web feeds AI. That is the web's purpose now. Also there are plenty of people who create content because they love it, and also need to be able to make a living at it, because doing so at the level of quality they want is time consuming and expensive. But mostly because even people who produce content because they love it want to share that content with the world and that will be nigh impossible when the only content anyone sees, and that any platform or algorithm surfaces, is AI generated. Why put in the effort and heart and work to create something only for an AI to immediately clone it for ad revenue? Why even bother? reply FeepingCreature 19 hours agoparentprevCommunal spaces are fine, communal spaces will continue to be fine. Forums are fine. IRC is fine. The only thing that's dying is Google. Google is not the Internet. reply RiverCrochet 6 hours agorootparentIt's crazy how easy Google made the Internet for everyone in the 2000s. People got spoiled. reply arjie 20 hours agoparentprevI don’t mind writing original content like the old web. And there’s obviously other people who do this too https://github.com/kagisearch/smallweb/blob/main/smallweb.tx... I don’t get much traffic but I don’t mind. The thing that really made it for me is sites like this http://www.math.sci.hiroshima-u.ac.jp/m-mat/AKECHI/index.htm... They just give you such an insight into another human being in this raw fashion you don’t get through a persona built website. My own blog is very similar. Haphazard and unprofessional and perhaps one day slurped into an LLM or successor (I have no problem with this). Perhaps one day some other guy will read my blog like I read Makoto Matsumoto’s. If they feel that connection across time then that will suffice! And if they don’t, then the pleasure of writing will do. And if that works for me, it’ll work for other people too. Previously finding them was hard because there was no one on the Internet. Now it’s hard because everyone’s on it. But it’s still a search problem. reply vineyardmike 22 hours agoparentprev> this also means that because we've exhausted the human generated content by now as means of training LLMs, new models will start getting trained with mostly the output of other LLMs There is also a rapidly growing industry of people whose job it is to write content to train LMs against. I totally expect this to be a growing source of training data at the frontier instead of more generic crap from the internet. Smaller models will probably stay trained on bigger models, however. reply tacocataco 13 hours agorootparentIf we owned our own data truly, we could all have passive income. reply 0x00cl 21 hours agorootparentprev> growing industry of people whose job it is to write content to train LMs against Do you have an example of this? How do they differentiate content written by a person v/s written by LLM, I'd expect there is going to be people trying to \"cheat\" by using LLMs to generate content. reply vineyardmike 21 hours agorootparent> How do they differentiate content written by a person v/s written by LLM Honestly, not sure how to test it, but this is B2B contracts, so hopefully there's some quality control. It's part of the broad \"training data labeling\" business, so presumably the industry has some terms in contracts. ScaleAI, Appen are big providers that have worked with OpenAI, Google, etc. https://openai.com/index/openai-partners-with-scale-to-provi... reply gwervc 22 hours agoparentprevMaybe paper-based book will be fashionable again. reply jsheard 22 hours agorootparentPrint-on-demand means that paper books will be just as flooded with LLM sludge as eBook stores. I think we are at risk of regressing back to huge publishers being de-facto gatekeepers, because every easily accessible avenue to getting published is going to get crushed under this race to the bottom. Likewise with record labels if platforms like Spotify which allow self-publishing get overwhelmed with Suno slop, which is already on the rise (there's some conspiracy theories that Spotify themselves are making it, but there's more than enough opportunistic grifters in the world who could be trying to get rich quick by spamming it). https://old.reddit.com/r/Jazz/comments/1dxj409/is_spotify_us... reply tbatchelli 22 hours agorootparentprevCombine LLMs with on-demand printing and publishing platforms like Amazon and realize that even print books can now be AI-tainted. reply zer00eyz 20 hours agorootparent> The Fifty Shades trilogy was developed from a Twilight fan fiction series originally titled Master of the Universe and published by James episodically on fan fiction websites under the pen name \"Snowqueen Icedragon\". Source : https://en.wikipedia.org/wiki/Fifty_Shades_of_Grey The AI is already tainted with human output.... If you think its spitting out garbage it's because that's what we fed it. There is the old Carlin bit about \"for there to be an average intelligence, half of the people need to be below it\". Maybe we should not call it AI rather AM, Artificial Mediocrity, it would be reflection of its source material. reply czl 5 hours agorootparent> There is the old Carlin bit about \"for there to be an average intelligence, half of the people need to be below it\". This is true for the median, not necessarily for the average. reply sebastiennight 4 hours agorootparentprevIf 99 people have an IQ of 101, and the last person's IQ is 1, then the average IQ is 100. How many people are below the average IQ? reply input_sh 22 hours agorootparentprevSo what? Stupid shit gets posted as a \"book\" on Amazon all the time, with or without AI. Doesn't mean anyone buys it. reply cogman10 21 hours agorootparentThe issue is that the AI shit is flooding out anything good. Nearly any metric you can think of to measure \"good\" by is being gamed ATM which makes it really hard to actually find something good. Impossible to discover new/smaller authors. reply myaccountonhn 6 hours agorootparentRead literature magazines and check the authors there? reply rurp 20 hours agorootparentprevScale matters. The ability to churn out bad writing is increasing by orders of magnitude and could drown out the already small amount of high quality works. reply czl 5 hours agorootparentWhile it's true that the volume of bad writing is increasing, our ability to analyze and refine this sludge is also improving. Just as spell check and grammar check give instant feedback why not AI instant feedback about writing quality / originality / suitability / correctness / … ? If instant feedback can improve spelling and grammar why not these other things? reply dartos 22 hours agorootparentprevHey woah. Take that reality elsewhere, sir. We’re doomering in this here thread. /s reply InsideOutSanta 22 hours agorootparentprevBeware the print-on-demand AI slop. Paper can not save us. reply recursive 14 hours agorootparentAI is still not able to re-appropriate paper from meaningful books. Yet. reply recursive 14 hours agoparentprevI use LLM output from kagi too. But given the rate of straight-up factually incorrect stuff that comes out of it, I need it to come with a credible source that I can verify. If not, I'm not taking any of it seriously. reply eddd-ddde 4 hours agoparentprevHumans have trained on human generated content for centuries. What makes it impossible for AI to succeed? reply vbezhenar 19 hours agoparentprevAlphaGo learned to play Go by playing with itself. Why couldn't LLM do the same? They got plenty of information to be used as a starting point, so surely they can figure out some novel information eventually. reply ainoobler 19 hours agorootparentAlphaGo was playing by very specific rules. What are the rules for LLMs to do the same? reply treyd 14 hours agorootparentprevLLMs aren't logically reasoning through an axiomatic system. Any patterns of logic they demonatrate are just recreated from patterns in input data. Effectively, they can't think new thoughts. reply czl 4 hours agorootparent> Effectively, they (LLMs) can't think new thoughts. This is true only if you assume that combining existing thought patterns is not new thinking. If they can't learn a certain pattern from training data, indeed they would be stuck. However, their training data keeps growing and updating, allowing each updated version to learn more patterns. reply ricopags 6 hours agorootparentprevThe massive LLMs trained on webscale data aren't. But some are, in fact: https://arxiv.org/abs/2407.07612 reply NiloCK 14 hours agorootparentprevDo you think they sometimes hallucinate? Do you think a collection of them can spot one another's hallucinations? Do you think that, on occasion, some hallucinations will at least directionally be under explored good ideas? reply Salgat 12 hours agoparentprevMind you they will be trained on what humans have filtered as being acceptable content. Most of the trash produced by ML that hits the web is quickly buried and never referenced. reply throwthrowuknow 21 hours agoparentprev> But this also means that because we've exhausted the human generated content Putting aside the question of whether dragnet web scraping for human generated content is necessary to train next gen models, OpenAI has a massive source of human writing through their ChatGPT apps. reply SilverCurve 21 hours agoparentprevThere will be demand for search, ads and social media that can get you real humans. If it is technologically feasible, someone will do it. Most likely we will see an arms race where some companies try to filter out AI content while others try to imitate humans as best they could. reply squigz 22 hours agoparentprev> 2023 will be the last time we had original content, as soon we will have stop producing new content and just recycling existing content. This is just an absurd idea. We're going to just stop producing new content? reply tbatchelli 22 hours agorootparentThe incentives will be largely gone when SEO-savvy AI bots will produce 10K articles in the time it takes you to write one, so your article will be mostly unfindable in search engines. Human generated content will be outpaced by AI generated content by a large margin, so even though there'll still be human content, it'll be meaningless on aggregate. reply mglz 22 hours agorootparentprevNo, but the scrapers cannot tell it apart from LLM output. reply epidemian 21 hours agorootparentWe can adapt. There's already invite-only and semi-closed online communities. If the \"mainstream\" web becomes AI-flooded, where you'd you like to hang out / get information: the mainstream AI sludge, or the curated human communities? reply grugagag 6 hours agorootparentI think the safest space away from the gen AI sludge will be offline. But even that will make it vulnerable to its influence. reply dartos 22 hours agorootparentprevYet reply mglz 21 hours agorootparentThe LLM is trained by measuring its error compared to the training data. It is literally optimizing to not be recognizable. Any improvement you can make to detect LLM output can immediately be used to train them better. reply ben_w 21 hours agorootparentGANs do that, I don't think LLMs do. I think LLMs are mostly trained on \"how do I recon a human would rate this answer?\", or at least the default ChatGPT models are and that's the topic at the root of this thread. That's allowed to be a different distribution to the source material. Observable: ChatGPT quite often used to just outright says \"As a large language model trained by OpenAI…\", which is a dead giveaway. reply sebastiennight 4 hours agorootparentThis is the result of RLHF (which is fine-tuning to make the output more palatable), but this is not what training is about. The actual training process makes the model output be the likeliest output, and the introduction phrase you quoted would not come out of this process if there was no RLHF. See GPT3 (text-davinci-003 via API) which didn't have RLHF and would not say this, vs. ChatGPT which is fine-tuned for human preferences and thus will output such giveaways. reply deathanatos 21 hours agorootparentprevBack to webrings, then. reply camdenreslink 22 hours agorootparentprevNon-AI content will probably become a marketing angle for certain websites and apps. reply binary132 22 hours agorootparentprevit’ll be utterly drowned out for the vast majority of users reply ClassyJacket 7 hours agoparentprev>So the snapshot of the web as it was in 2023 will be the last time we had original content The pre-AI internet will be like scientists looking for pre-nuclear steel. reply sweca 20 hours agoparentprevThere will also be a lot of human + AI content I imagine. reply talldayo 22 hours agoparentprevThis seems like it would only work if you deliberately rank AI-generated text above human generations. If the AI generations are correct, is it really that bad? If they're bad, I feel like they're destined to fall to the bottom like the accidental Facebook uploads and misinformed \"experts\" of yesteryear. reply binary132 22 hours agorootparentwho ranks the content reply talldayo 22 hours agorootparentWell, there's the problem. Truth be told though, the way keyword-based SEO took off I don't really think it's any better with humans behind the wheel. reply binary132 5 hours agorootparentso what you’re saying is search ranking, and more generally, feed prioritization algorithms, aren’t a trustworthy solution to this? LOL. reply jay_kyburz 21 hours agorootparentprevWe would lose the long tail, but if I were a search engine, I would have a mode that only returned results on a whitelist of domains that I would have a human eyeball every few months. If somebody had a site that we were not indexing and wanted to be, they could pay a human to review it every few months. reply binary132 5 hours agorootparenthow many websites do you think should exist on the internet? reply epidemian 21 hours agorootparentprevMaybe us? I mean us as in a network of trusted individuals. For example, i've been appending \"site:reddit.com\" to some of my Google queries for a while now —especially when searching for things like reviews— because, otherwise, Google search results are unusable: ads disguised as fake \"reviews\" rank higher than actual reviews made by people, which is what i'm interested in. I wouldn't be surprised if we evolve some similar adaptations to deal the flood of AI-generated shit. Like favoring closer-knit communities of people we trust, and penalizing AI sludge when it sips in. It's still sad though. In the meantime, we might lose a lot of minds to this. Entire generations perhaps. Watching older people fall for AI-generated trash on Facebook is painful. I hope we acted sooner. reply binary132 5 hours agorootparentI’m pretty sure most of reddit is botted / shilled astroturf too at this point, especially in product reviews, they’re way ahead of you For all I know your reply is also a botted response to promote reddit reviews as trustworthy and bot-free :P To put it another way: who defines the trust network? Or another way: every trust network will be invaded. Or another way: trust is already actively exploited and has been for decades (or longer, if you want to go there....) reply PhasmaFelis 20 hours agorootparentprevWhen the AI is wrong, the ranking algorithm isn't any better at detecting that than the AI is. reply kevingadd 22 hours agorootparentprevWhere would the AI get the data necessary to generate correct answers for novel problems or current events? It's largely predictive based on what's in the training set. reply talldayo 22 hours agorootparent> Where would the AI get the data necessary to generate correct answers for novel problems or current events? In a certain sense, it doesn't really need it. I like to think of the Library of Babel as a grounding thought experiment; technically, every truth and lie could have already been written. Auguring the truth from randomness is possible, even if only briefly and randomly. The existence of LLMs and tokenized text do a really good job of turning statistics-soup into readable text. That's not to say AI will always be correct, or even that it's capable of consistent performance. But if an AI-generated explanation of a particular topic is exemplary beyond all human attempts, I don't think it's fair to down-rank as long as the text is correct. reply Workaccount2 21 hours agorootparentThe explosion in AI over the last decade has really brought into light how incredibly self-aggrandizing humans naturally are. reply kevingadd 21 hours agorootparentprevAre you suggesting that llms can predict the future in order to address the lack of current event data in their training set? Or is it just implicit in your answer that only the past matters? reply Thorentis 18 hours agoparentprevI wonder how much of Wikipedia has been contributed to using AI by now. Almost makes me want to keep a 2023 snapshot of Wikipedia in cold storage. reply sebastiennight 4 hours agorootparentFYI, you can. There are mobile apps that allow you to keep a downloaded version of the entire encyclopaedia, and it fits most modern phones. reply meiraleal 20 hours agoparentprevGoogle really missed the opportunity of becoming ChatGPT. LLMs are the best interface for search but not yet the best interface for ads so it makes sense for them to not make the jump. ChatGPT and Claude are today what Google was in 2000 and should have evolved to. reply elromulous 22 hours agoparentprevThe web before 2023 basically becomes like pre-atomic steel[0] [0] https://en.wikipedia.org/wiki/Low-background_steel reply julianeon 19 hours agoprevThis article is weird clickbait which, even weirder, worked. It seems to assume a world where SEO entrepreneurs where ready to churn out million-page sites, but the cost per query were blocking them. There is no marginal cost, no SEO cost to adding another page, as long as a couple people visit it and \"pay it off\". In the real world, it doesn't work like that. Whatever monstrosity was created like this would not do well in the search engines. So no meaningful threshold has been passed, in terms of the cost for AI generation. People are creating lots of AI content, but not like this - not bottom tier generic SEO pages which will barely rank and aren't that compelling in an already saturated Internet. Incidentally the real money seems to be in generating AI images and, eventually, video: much better return for your money. reply Animats 22 hours agoprevThat's an inflection point, all right. OpenAI's customers can now at least break even. Of course, it means a flood of crap content. reply ainoobler 22 hours agoparentThe Internet has been flooded with crap content for some time now so AI is simply accelerating the existing trends. reply ToucanLoucan 21 hours agorootparentGiven the younger generations increasing ambivalence to the non-stop fire hose of bullshit that the vast majority of the platform internet already is, and given that we're now forging the tools to make said fire hose larger by numerous factors, I don't think this is going to be the boon long-term that a lot of people seem to think it is. reply ainoobler 19 hours agorootparentIt is extremely ironic that computers which operate by the logic of boolean arithmetic and algebra are now used to generate bullshit instead of adding rigor and checking existing written content for basic falsehoods and logical fallacies. reply muzani 20 hours agorootparentprev90% of everything is crap. Itch.io has almost no crap filters so all you find is crap. Steam lets anyone publish but you rarely come across any crap. Many PC game devs know that the income overwhelmingly comes from Steam vs every other site put together. Unfortunately, this just gives more power to the walled gardens. reply selalipop 21 hours agoparentprevI was going to disagree with the article because the content 4o-mini generates isn't there yet. I run a content site that is fully AI generated, https://tryspellbound.com It writes content that's worth reading, but it's extremely expensive to run. It requires chain of thought, a RAG pipeline, self-revision and more. I spent most of yesterday testing it and pushed it to beta, but the writing feels stilted and clearly LLM generated. The inflection point will come for content people actually want to read, but it's not going to be GPT-4o mini. reply mort96 8 hours agorootparentThe point isn't to generate good content \"that's worth reading\". The point is to generate an endless stream of slop which looks plausible enough to get you ad impressions. reply RockRobotRock 19 hours agorootparentprevI had a lot of fun with NovelAI. I believe at the time it was using GPT2, and I loaded in fine tuned models for the canon of choice I wanted to experience (trained on fanfic, and things of that sort). How does spellbound work? reply selalipop 17 hours agorootparentSpellbound is an instruct model to NovelAI's completion model: you enter commands which in turn dictate what happens to your character, then the AI models how others would react to you reply notatoad 20 hours agoparentprev>a flood of crap content so, status quo? this sort of content only has value because google links to it when people search, and because google runs an ad network that allows monetizing it. google is also working furiously to provide these same AI-generated answers in their SERP, so they can eliminate this and monetize the answers directly instead of paying out to random third parties. i'm pretty skeptical that this ai-generated content will ever be monetizable in the way the article suggests, simply because google is better at it. if you're a human making your living by writing articles that are indistinguishable from ai-generated content, then you might be harmed by this but for most people this inflection point is not going to be a noticeable change. reply rbax 23 hours agoprevThis assumes a future where users are still depending on search engines or some comparative tool. Profiting off the current status quo. I would also be curious how user behavior will evolve to identify, evade, and ignore AI generated content. Some quasi arms race we'll be in for a long time. reply Oras 22 hours agoparentThat depends on how many users are aware of the AI content. HN is not a reflection of the world. reply ben_w 22 hours agorootparentTrue, but ChatGPT has been interviewed by a national television broadcaster in the UK at least, so I think it broke out of our bubble no later than December 2022: https://youtu.be/GYeJC31JcM0?si=gdmlxbtQnxAvBc1i reply binkHN 20 hours agoparentprevThis has already been happening for quite some time with users ignoring Google search and searching Reddit directly. The irony is that, I assume, most of Reddit's income right now is coming from content licensing deals with AI companies. reply surfingdino 22 hours agoprevSo it will now be cost-effective to connect the exhaust of ChatGPT to its inlet and watch as the quality of output deteriorates over time while making money off ads. Whatever rocks your boat, I guess. How long before the answer to every prompt is \"baaa baaa baaa\"? reply the_gipsy 22 hours agoparentBaa baa baaa baaaaaa. reply throwthrowuknow 21 hours agoparentprevYou’re sadly misinformed if you think training an LLM consists of dumping the unfiltered sewage straight from the web into a training run. Sure, it’s been done in early experiments but after you see the results you learn the value of data curation. reply surfingdino 20 hours agorootparentAnd who/what is going to curate data? AI or that company in Kenya https://time.com/6247678/openai-chatgpt-kenya-workers/ ? Because neither have a clue what is good data and what is not. reply muzani 20 hours agorootparentThat article itself might be part of the degradation. It mentions at least four times that the contract was canceled as if it's something new. I wonder if someone just dumped a bunch of facts and ran it through a spin cycle a few times with AI to get a long form article they didn't expect anyone to read. reply GaggiX 20 hours agorootparentprevIt's clearly working because the models are only getting better, believing that the performance of these models would fall at some point in the future is just very delusional. reply Slyfox33 19 hours agorootparentWeren't they just getting better mostly because they were being scaled up? There's no way to do that once you've exhausted all of the data. Besides progress has slowed down at this point anyway. reply energy123 13 hours agorootparentNot only. Look at the subject of this thread, GPT-4o mini. I'm optimistic about synthetic data giving us another big unlock, anyway. The text on the internet is not that reasoning dense. And they have a snapshot of pre-2023 that is fixed and guaranteed not to decay. I don't think one extra year of good quality internet is what will make or break AGI efforts. The harder bottleneck will be energy. It's relatively doable to go from 1GW to 10GW but the next jump to 100GW becomes insanely difficult. reply GaggiX 18 hours agorootparentprevGPT-3 was 173B parameters and it's very bad compare to much smaller models we have nowadays, the data and the compute play a giant role, also I doubt you would need to train a model further after you have trained it on absolute everything (but we are very far from that). reply 1024core 22 hours agoprevI don't know who these people are who can't even do basic arithmetic. > an estimated annual revenue of $1,550 for 50,000 monthly page views. > This is approximately ~$0.00022 earned per page view. No, this is $0.002583 earned per page view, a ~12x difference. Looks like the author divided by 12 twice. reply latortuga 21 hours agoparentSnarky but possibly true reply: perhaps someone had AI ~write~ hallucinate this article for them. reply yard2010 20 hours agoparentprevThe answer is clear - a hallucinating AI wrote this post reply tempaccount420 10 hours agorootparentAny self-respecting AI would not make a mistake like this. This is the work of a human. reply GaggiX 20 hours agorootparentprevThe post was probably written by a mere human (they do sometimes hallucinate quite badly). reply GaggiX 22 hours agoparentprevWell that's even better for the point of the article. reply mo_42 22 hours agoprev> Will the future of the internet be entirely dynamically generated AI blogs in response to user queries? I still enjoy commenting on HN and writing some thoughts on my blog. I'm pretty sure that there are many other people too. At some point everything that is not cryptographically singed by someone I know and trust needs to be considered AI generated. Maybe AI-generated content might have better quality than generated by humans. But then it's likely that I'm under the influence of some bigger corporation that just needs some eyeballs. reply huevosabio 22 hours agoprevThis analysis implicitly holds supply constant. But supply isn't constant, it will balloon. So the price per impression will tank. So, on the margin, this will drive human created content out since it is now less profitable to do it by hand than it was before. reply PostOnce 17 hours agoparentThis assumes that \"AI content\" and \"human content\" are of equal value, which is still up for debate and I'd argue not true. That means this will devalue AI content and ad impressions, but not necessarily human content, if people in fact value it. reply mtnGoat 22 hours agoprevGenerating content on the fly is already happening, has been for a while. Word spinners used with a script that grabs the content of the first 5 Google results, Wikipedia, etc, has been around a long time and Google indexed the incomprehensible garbage it created. Lot cost models just lowered the bar of entry. reply shinycode 6 hours agoprevWhy would ad companies not generate themselves the content ? It make no sense what he is saying. They pay for ads because today they can’t write said content. If now they can why pay other people ? I wouldn’t be surprised to see ads injected in LLM answers, that’s the logical way to go. Free LLM with ads reply loremaster 22 hours agoprevThis has been possible for well over a year, just not with OpenAI’s API specifically. reply transformi 22 hours agoparentExactly! Even with local LLM running on the browser/client side. reply seydor 8 hours agoprevDo we really want ads that suggest the wrong product or make up imaginary products and prices ? I 'd rather be exploited by google reply aydyn 22 hours agoprevI read that title _very_ wrong as injecting ads directly into ChatGPT responses. How hilariously dystopian would that be? reply levocardia 22 hours agoparentI have some bad news for you - the nightmare is already here: https://news.ycombinator.com/item?id=40310228 reply LeoPanthera 21 hours agoparentprevMicrosoft Copilot already does this. reply zackmorris 22 hours agoprevFrom what I can tell, all scalable automated work falls in value towards zero over time. For example, a person could write a shareware game over a few weeks or months, sell it for $10, buy advertising at a $0.25 customer acquisition cost (CAC) and scale to make a healthy income in 1994. A person could drop ship commodities like music CDs and scale through advertising with a CAC of perhaps $2.50 and still make enough to survive in 2004. A person could sell airtime and make speaking appearances as an influencer with a CAC of $25 and have a good chance of affording an apartment in 2014. A person can network and be part of inside deals and make a million dollars yearly by being already wealthy in a major metropolitan city with a CAC of $250 in 2024. The trend is that work gets harder and harder for the same pay, while scalable returns go mainly to people who already have money. AI will just hasten the endgame of late stage capitalism. Note that not all economic systems work this way. Isn't it odd how tech that should be simplifying our lives and decreasing the cost of living is just devaluing our labor to make things like rent more expensive? reply rachofsunshine 21 hours agoparentIt's only odd if you model economics as a cooperative venture between a society trying to build better collective outcomes, and not as a competitive system. Additional capability and information can never hurt a single actor taken in isolation. But added capability and information given to multiple actors in a competitive game can make them all worse off. As a simple example, imagine a Prisoner's Dilemma, except neither side knows defecting is an option (so in effect both players are playing a single-move game where \"cooperate\" is the only option). Landing on cooperate-cooperate in this case is easy (indeed, it's the only possible outcome). But as soon as you reveal the ability to defect to both players, the defect-defect equilibrium becomes available. reply oblio 21 hours agoparentprevIf you read the Black Swan by Taleb, it stops being weird. He points this out and dubs it Extremistan, where small advantages accrue to oversized returns. We will need humane solutions to this, because the non humane ones are starting to become visible (armed drone swarms driven by AI). reply winddude 21 hours agoprevAd blockers, 30-40% of internet users. getting traffic... if it was that easy everyone would do it, and diminishing returns. reply gnicholas 21 hours agoprevWon't people who hate ads just choose to cut out the middleman and use 4o mini on their own? reply yard2010 20 hours agoparentWhat makes you think it won't have ads or \"sponsored\" content? reply gnicholas 13 hours agorootparentThere may well be tiers like on NFLX, where some are ad-sponsored and some are not. But seeing as how rapidly the free/open models catch up to prior-generation proprietary models, I doubt there will be much margin or room for ads on anything but the latest/greatest. reply moffkalast 22 hours agoprev> For example, putting in 50k page views a month, with a Finance category, gives a potential yearly earnings of $2,000. > I'm going to take the median across all categories, which is an estimated annual revenue of $1,550 for 50,000 monthly page views. > This is approximately ~$0.00022 earned per page view. The problem is... this doesn't take into account a million AI generated sites suddenly all competing for the same amount of eyes as before, driving revenue to zero very quickly. It'll be worth something for a bit and then everyone will catch up. reply SoftTalker 21 hours agoparentThe same number of eyes will still be driven to a subset of content by algorithmic influence. Whether search engines, algorithmically-generated \"viral\" popularity, or whatever. Most people are consuming whatever is placed in front of their faces. That content will still have value, the trick will be getting your content into that subset. reply TremendousJudge 22 hours agoparentprevMany people in the history of the internet have made a lot of money by doing something that was \"worth something for a bit and then everybody caught up\" reply adw 22 hours agorootparentYou just described basically the entire trading strategy of most high frequency traders. reply Andrex 21 hours agoparentprevPresumably the assumption is that (as with capitalism) an ever-growing population will paper over all problems. reply normaler 18 hours agoprevI am enjoying this Moment in time where I can ask chatgpt product related questions and not get ad biased suggestions. I think there is ~ half a year left reply blirio 18 hours agoparentthere are open source models. If/when chatgpt has adds you will probably be able to just run something equiv with open source tools. reply kingkongjaffa 22 hours agoprevWell, computer hardware was stagnating without a forcing function. Running LLM’s locally is a strong incentive to get hardware more powerful and run your own local models without any ADs. reply Havoc 21 hours agoprevDon’t think you’re getting 50k views pm in finance space with some “you’re an expert blog writer” AI spiel reply gpvos 22 hours agoprevWe're doomed. reply zombiwoof 22 hours agoprevDefinitely the future of Twitter reply sergiotapia 22 hours agoprevStuck culture? Meet REAL stuck culture. reply m3kw9 22 hours agoprevThe rate limits though 15m tokens per month in the top tier isn’t really scale reply wewtyflakes 21 hours agoparentI believe the rate limits are described in tokens per minute, not per month. https://platform.openai.com/docs/guides/rate-limits?context=... reply m3kw9 19 hours agorootparentYou are right, it’s per minute reply refulgentis 22 hours agoparentprevI strongly assume there are higher rate limits, more than once I've seen the Right Kind of Startup, (buzzworthy, ex-FAANG with $X00m in investment in a market that's always been free, think Arc browser), make a plea on twitter because they launched a feature for free, were getting rate limited, and wanted a contact at OpenAI to raise their limit. Arc is an excellent example because AFAIK it's still free, and I haven't heard a single complaint about throttling, availability, etc., and they've since gone on to treat it as a marketing tentpole instead of experiment. reply ein0p 17 hours agoprevI tried it on technical queries and it hallucinated like crazy. Probably ok for narrow tasks, but I wouldn’t expose it through the main UI like they did - people expect some degree of intelligence there. reply 93po 22 hours agoprevwebsim looks cool but requires a google login to even try it, i hate the internet in 2024 reply godelski 18 hours agoprevSo this has been an inflection point that has concerned me, specifically in regards to a few types of sites: news and instruction sites. News sites are already often shit and parasitic. I mean parasitic because if you go to a free news site (say Yahoo news, etc) you often see rewritten articles that originated from paid sites (e.g. NYT). The pure ad-supported sites are typical enshitification that degrades journalism and increases sensationalism because they don't need to write unique articles, but you should sensationalize them to drive up views. You also don't have to hire journalists to get story details. So news most people read degrades and you get very limited views. The problem here is that this paradigm barely works because you have to pay real people to write those rephrased articles. So while it costs more to run the NYT where you need to hire investigative journalists and send people to physical places, there is a bound on that difference. But if you paste in a NYT article into GPT4 and ask it to summarize it, you'll get very similar quality to yahoo news (or even CNN, MSNBC, or Fox. Which all also do this leeching, but less of an issue). I'm sure people realize how easy it is to scrape NYT and then post the GPT output. This is in spirit no different than if you just used archie.is, but large scale. The same is true for many tutorial sites or cooking sites, etc. I'm sure many of you also get annoyed at the google search results that are just stackover flow posts embedded on a different site or the Medium articles (especially paid ones) that are also just SO posts and can show up higher in the listing. The issue becomes: how do we generate and disseminate new information in this paradigm? Okay, free blog posts aren't \"hurt\" because they have no income, but people build reputation through them and it gets many people jobs. But what about others that do make a living through this? Is this not similar Jack Conte's (Patreon co-founder/CEO and 1/2 of the band Pomplamoose) argument about creating content \"for the algorithm\" vs for \"yourself/your fans/fun/etc\". That it is taking some of the human elements out of the art/entertainment/content. (Can totally disagree with his argument btw). Personally I'm on the side of Jack. Our goal shouldn't (now) be to just serve people search results or just generate content for content's sake, but to now focus on serving people high quality content and high quality results. Google indexed the entire internet. People gamed the system (SEO) and now google results are shit, youtube results are shit, and everything is shit. We don't need more content (who uses page 2 on Google?), but we need to have better content. [1] I think we need to ask: is this what we want? If not, then what are we going to do about it? If we are okay, then I think someone should create a super-website where you just have information about just about everything. There definitely is utility in it. But the question is at what cost. [0] https://youtu.be/hwn6-8XpIuE [1] I think most people want this. But the problem is you're not going to find market forces showing this because there is no product doing this. Or if there are, they aren't well known and could be confusing to use and/or a wide variety of problems (UI/UX do matter). But it requires reading between the lines and market research a la talking to people and finding out what they want, not a la data. You need both. reply K0balt 21 hours agoprevThe enshitification of search will drive queries directly to AI, either local or centralised. This will provide a before unknown nexus of opinion/ perception / idea control as the primary research tool will no longer return a spectrum of differing ideas and references, but rather a consolidated opinion formed by the AIs operators. This has really dystopian vibes, since it centralizes opinion and “factuality” in an authoritative but potentially extremely biased or even manipulatively deceptive manner. OTOH it will provide opportunities for competitive solutions to query answering. reply mska 20 hours agoprev [–] When views are low the math doesn't make sense but it is very possible to get a lot of views through AI generated + human reviewed content. We're trying to do that with PulsePost (https://pulsepost.io) and the biggest challenge is unique content. Given a keyword or a niche topic, AI models tend to generate similar content within similar subjects. Changing the temperature helps to a degree but the biggest difference comes from adding internet access. Even with same prompt, if the model can access the internet, it can find unique ideas within the same topic and with human review it becomes a high value article. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "OpenAI has launched the GPT-4o mini model, priced at $0.15 per 1 million input tokens and $0.60 per 1 million output tokens, making it more affordable and intelligent than GPT-3.5 Turbo.",
      "The low cost of GPT-4o mini enables the creation of AI-generated content that can be supported entirely by ad revenue, potentially transforming online content creation.",
      "For example, generating a blog post with GPT-4o mini costs $0.00051525, while ad revenue per page view can be around $0.0026, resulting in a net profit of approximately $0.002 per page view."
    ],
    "commentSummary": [
      "The discussion highlights concerns about the future of web content, suggesting that AI-generated content may soon dominate, leading to a cycle of AI training on AI outputs.",
      "There is a debate on the quality of current web content, with some arguing that much of it is already low-quality SEO (Search Engine Optimization) material, and AI might exacerbate this issue.",
      "The conversation touches on the difficulty of distinguishing between human and AI-generated text, raising questions about the authenticity and originality of online interactions and content."
    ],
    "points": 276,
    "commentCount": 220,
    "retryCount": 0,
    "time": 1721417319
  },
  {
    "id": 41016290,
    "title": "No Uptime Hosting (2006)",
    "originLink": "https://nouptime.com/",
    "originBody": "- 200Kb of Storage - 10Mb of Bandwidth - PHP 4, 5 and 6 - MySQL, SQL and Oracle - Dedicated Dedication - Host Up To 6.8 Sites - Free Identity Theft - Lots of Storage - 10Kb of Bandwidth - PHP 5, 6 and 7 - MySQL and ReiserFS - 2 Glow Sticks - Free Broken Toaster - 24/8 Email Support - 200Gb of Storage - 10 Bytes of Path Bandwidth - Ruby on Railroads - Dedicated Support Geek - Dedicated Win 3.11 - Steve Ballmer - Ruby on Boats Currently Hosting Over 5,468,404 Websites Hosting Websites Since 1969! Server Downtime We are constantly working to improve our server downtime. Keeping you awake at night is our #1 priority. Clusters and Scalability If your websites go down we'll make sure to confuse you with technical terms and instructions. It's our job to keep you aggravated, and we never disappoint. We have over 352 testimonials from happy customers around the world:",
    "commentLink": "https://news.ycombinator.com/item?id=41016290",
    "commentBody": "No Uptime Hosting (2006) (nouptime.com)210 points by Viraxi 5 hours agohidepastfavorite68 comments nanodeath 4 hours agoThe HTML source is pretty good too. A doctype that's half HTML5, half XHTML. A pre-IE7 script tag thing. A meta-keywords tag. Lists that don't actually use any list tags. Raw PHP tags being dumped into the HTML output. No closing body or html tags. This joke has _layers_, man. reply btdmaster 3 hours agoparentSubmitting https://nouptime.com/support.php calls error('5') but error is not defined anywhere in the JavaScript code. reply godzillabrennus 4 hours agoparentprevNeeds more iframes from the sound of it… reply 65 4 hours agorootparentNeeds moreand spacer.gifs too! reply skrebbel 4 hours agorootparentprevAnd regular frames. reply daft_pink 4 hours agorootparentprevclearly you need to setup a competitor as you really know what you’re doing ;) reply didgeoridoo 5 hours agoprev> Don't like uploading your files via FTP? No problem! Send us your files on a floppy and we'll upload it for you. It's one of the many ways we do our best to accommodate your needs. Just remember, our floppy system does not accept Windows, Linux, or Mac floppies. OS/2 Warp is back, baby! reply freedomben 4 hours agoprevThis looks like a good service for compute, but it's way over priced for storage. It probably makes most sense to use the included disk for application code only and store all data in something like S4 [1] [1]: http://www.supersimplestorageservice.com/ reply peterleiser 2 hours agoparentUnder \"Key scenarios\" they include \"SETI@home output recorder\". Ouch. reply DrWhax 5 hours agoprevYou might also like http://pauperhosting.nl/ reply pdonis 5 minutes agoparentI like how clicking \"here\" for English takes you to a DuckDuckGo search. reply skrebbel 4 hours agoparentprevI like how deep the rabbit hole goes on pauperhosting. The accidentally included admin panel with leaked customer db, the VPS demo, everything. I'm pretty sure I haven't even discovered half of it yet. reply isoprophlex 24 minutes agorootparentAlmost all the customer names in that excel file are taken from \"huilende rappers - waddepjedangedaan\", on a joke rap album by drum and bass legends noisia. It's all in Dutch... but the made up names are so incredibly funny that i use them for dummy data everywhere. https://m.youtube.com/watch?v=o4uLvqM7Clc reply mrklol 4 hours agoparentprevThe link behind \"Click here for English\" is a good one, I have to remember that. Good idea. reply input_sh 2 hours agorootparentNext to a German flag, even though the website's in Dutch. Perfect. reply OuterVale 4 hours agoprevThe site could be improved with a Cloudflare 'always online' banner. https://blog.cloudflare.com/cloudflares-always-online-and-th... reply rvnx 4 hours agoparentProtected by CrowdStrike reply renonce 35 minutes agoparentprevThis website is indeed hosted by Cloudflare. reply lovestory 5 hours agoprevFinally I have discovered where my university moodle servers are located! reply hypeatei 5 hours agoprevWow this is the kind of thing that will disrupt a ton of industries! reply jfengel 4 hours agoparent/golf clap reply teeray 5 hours agoprevI heard they’re announcing unlimited write-only object storage soon reply pauldraper 5 hours agoparentPerfect place for hosting internal docs reply neogodless 4 hours agoprevThe support form does nothing at all, but I think it would be much more effective if it led me to believe it did a lot more than it does. reply omoikane 3 hours agoparentThis reminds me of Microsith, which had a support form where you can ask it questions, but the response was always \"format and reinstall\". https://web.archive.org/web/20050206190826/http://microsith.... reply fideloper 4 hours agoparentprevAgree, was disappointed there was no easter egg here. reply anfractuosity 2 hours agoprevOut of interest, that mentions \"Dedicated Win 3.11\", so silly question, I assume you would be able to install DOS on a dedicated server, but might it be possible to run DOS/Win 3.11 on EC2 (I couldn't find a publicly accessible list of AMIs) reply pylua 5 hours agoprevThis is a great place to host all those litigation hold files. reply codetrotter 5 hours agoprevThis one pairs great with https://devnull-as-a-service.com/ I mean, just check this list of features: https://devnull-as-a-service.com/features/ And the pricing is super competitive too! https://devnull-as-a-service.com/pricing/ reply salviati 5 hours agoparentYep! Also this one comes to mind http://www.supersimplestorageservice.com/ reply deno 5 hours agoprevThe site being currently up does not inspire confidence. reply pauldraper 5 hours agoparentThis is just the marketing site. Reasonable to expect it's hosted on different infra with different reliability reply janwillemb 4 hours agoparentprevIt even survives the HN hug of death. Very disappointing. reply hansvm 3 hours agorootparentHow bad is the HN hug of death btw? My laptop can serve 200k QPS per core. I can't imagine HN is that much more intensive? Are 1vCPU VMs just that much worse? reply eptcyka 1 hour agorootparentCan you serve 200GB of an average 1mb large HTML page per second too, per core? reply lelandbatey 3 hours agorootparentprevYes, they are much worse than bon-fide actual single cores on most recently purchased laptops. But also, the HN hug of death mostly happens with tiny non-static sites that are sloppily hosted. If you're serving static assets it's hard to \"hug of death\" a Casio watch. reply derefr 2 hours agorootparentprevThe thing that gets saturated first for most at-home or cheap-VPS hosting is the low-throughput network uplink. reply pauldraper 25 minutes agorootparentprevHoly crap what kind of laptop are you running? reply thornewolf 3 hours agorootparentprevYour laptop can serve 200k qps per core? reply tempodox 2 hours agoprevIt has Dedicated Win 3.11, so it would be the perfect platform for the current CrowdStrike craze. reply johndunne 5 hours agoprevHow do we know they even power up any servers at all, for the service? I mean, they could simply just not power up any servers at all. Till then, I’ll opt for the ‘take-my-money’ plan. reply ralegh 5 hours agoparentIdeally they wouldn’t need to power up the servers but you can see from the graph that unfortunately there’s still some uptime, though they are working on it, maybe once they reach 100% downtime reply ataru 4 hours agorootparentI think I heard that moving from 99.99% unavailability to 99.999% unavailability costs ten times more. reply jand 5 hours agoprevI am not very good at telling jokes. But even i can tell, that this is low effort. You really get a laugh out of it? Like \"hahahaha, they said PHP 5\"? reply skilled 4 hours agoparentThis site is very, very old. reply 0x073 4 hours agoparentprevAnd php 6 doesn't exists. reply Retr0id 4 hours agorootparentThat's a little thing we call a joke. reply codetrotter 4 hours agorootparentWonder if they support Perl 6 though. reply graynk 2 hours agoparentprevYes. reply helsinki 4 hours agoparentprevYeah, agreed. reply fallat 4 hours agoprevFinally a service that tells the truth! reply Retr0id 4 hours agoprevhttps://archive.is/fqe0o reply 6510 34 minutes agoparentup since 2016! https://nouptime.com/admin.php reply surfingdino 4 hours agoprevSoon to be acquired by CrowdStrike reply atum47 4 hours agoprevI wonder if I can make an interesting project using the I'm-poor specs reply louwrentius 4 hours agoprevDTaaS Down-time as a Service reply janwillemb 5 hours agoprevFree identity theft included! reply dangoodmanUT 4 hours agoprev> Free Identity Theft reply louwrentius 4 hours agoprevI have a new business idea. I’m going to rent out 64-core systems with oodles of RAM, with ZFS. This is the catch: the ZFS pool is based on mirrors of 3.5 inch floppies reply nehal3m 4 hours agoparentThat's a Sweet Dream. https://www.youtube.com/watch?v=oGfkPCZYfFw reply louwrentius 4 hours agorootparentI got the reference ;-) reply RcouF1uZ4gsC 5 hours agoprevThis is a great host for all my privacy disclosures for my websites and apps. reply toast0 3 hours agoprevYeah, but does it come with IPMI that doesn't work? reply Waterluvian 4 hours agoprevUgh. I can’t get to the purchase screen. Nothing seems to work properly. 24/8 support?! This site is a joke. reply theden 5 hours agoprevSign me up! reply iwontberude 4 hours agoprevDon’t forget https://HostGecko.com I miss these old joke websites. No Uptime is old af reply UniverseHacker 4 hours agoprevLess mistakes and nonsense facts in the features list than half the products on Aliexpress and Amazon these days. reply jejeyyy77 4 hours agoprev [–] everything about this design is such a throwback. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "No Uptime Hosting is a satirical website, humorously offering outdated and impractical hosting services, such as accepting files on floppy disks and supporting obsolete operating systems like OS/2 Warp.",
      "The HTML source code of the site is intentionally filled with errors and outdated practices, such as mixed doctype declarations, raw PHP tags, and missing closing tags, adding layers to the joke.",
      "The community finds the humor in the absurdity of the service offerings and the intentionally poor website design, with many comments highlighting the nostalgic and comedic elements."
    ],
    "points": 210,
    "commentCount": 68,
    "retryCount": 0,
    "time": 1721481660
  },
  {
    "id": 41009942,
    "title": "Never Update Anything",
    "originLink": "https://blog.kronis.dev/articles/never-update-anything",
    "originBody": "Home Articles Tutorials Presentations Everything Is Broken Never update anything Here's a fair warning: this article is reductio ad absurdum, therefore you shouldn't take it as gospel. However, i do believe that this point of view is extreme enough that it usually gets shut down immediately, yet doesn't have any frank discussion about it, which makes it the perfect subject for me to write about! To reiterate, i'm not suggesting that you go out there after reading this and never update any of your production software, merely consider the points that are offered here. So, as the article title suggests, you should never update anything. Not your OS. Not your libraries in your software project. Not your tools. What would make someone come to this controversial idea? Actually trying to do that, of course! When you're telling your colleagues that you can't really work because Windows or JetBrains IDEs need to install some updates, they'll give you understanding, yet annoyed glances. When you tell clients that you cannot ship software because first you need to spend a few days or weeks refactoring software to keep up with the latest library releases, they will express their displeasure at you and will probably look elsewhere for someone who won't care about updates. When your company won't be the first to market, because about 20% of your total development capacity needs to spent on keeping up with the technical debt, which is slowly forced upon by the industry, while another 20% of the effort needs to go to writing and maintaining a test suite, the company will suffer as a result. And once you actually do get the buy-in to update the components and when it turns out that migrating from Spring to Spring Boot is actually a herculean effort that means carrying over about 50 dependencies of a legacy Java project that has about 1 million SLoC, during which you also discover that at least 20-30 of those are painfully out of date, then you'll just start writing blog posts like this. My premise is that updates are a massive waste of time, most of the time. Obviously, someone will jump out and say that \"Hey, if you don't update, your Windows XP installation will get added to a botnet in less than a week,\" and they'll also be right at the same time. Types of updates are the problem The problem here is that we never differentiate between the types of updates properly: major leaps in the development of the software which add new features security updates that don't change any actual functionality apart from the security fix bug fixes that should not alter any functionality in significant ways small backwards compatible feature updates that are essentially opt in There are plenty of things that you might want to feasibly change and update without any of the other functionality being altered. Actually, the situation is so bad in that regard, that most if not all of our modern software only ever has feature updates which sooner or later will have breaking changes in them. Otherwise, if you wanted to eliminate the possibility of running into breaking changes entirely, the development of any library would look like this: In the image, we see that we have major releases (with the \"V\" prefix), for example, we'd have MySQL 5, that would eventually be updated to MySQL 6, MySQL 7 and MySQL 8 (which is funny, if you know anything about how that actually worked out). Esentially, you'd have a few large releases, that represent both new features being added, as well as breaking changes. Then, you have the minor versions which introduce new functionaly (with the \"f\" prefix) and patches (with the \"s\" prefix) which in this case would mean security fixes. Essentially, i'm describing semantic versioning, where this scheme would be approximately V.f.s, where you have a clear distinction between the major, minor and patch versions. However, instead of abandoning minor releases, you'd have to keep them up to date, since in practice different minor releases are not always compatible. If you wanted to support all of the versions of your software without forcing your users to use the new features, it would mean maintaining a new instance of your codebase for every minor release in existence. And in this case we're also simplifying things, because we assume that the patch versions will never break anything, which would probably mean that we need to preserve non-security related bugs and only fix them across minor releases. You'd have to backport any and all security fixes from your newest versions, should the older ones also be affected. For example, if you discover a problem in V5.2.15 and fix it in V5.2.16, then you'd have to port these changes back to the current V5.1.X version and V5.0.X version, which is probably doable when you have 3 releases, but less so with 30. And it goes the other way around, if you discover a problem in one of your older releases and the newer ones are also affected, then you have to port the changes forwards. For example, if you discover a problem in V5.0.14 and fix it in V5.0.15, then you'd have to port these changes forwards to the current V5.1.X and V5.2.X versions, where you run into the same problem as above. In practice, things almost never work like that because that necessitates too much work. If you tried doing that, your issue tracking software could look like this: Issue: MYSOFT-3401 Description: Security breakage in $FOO module. Assignee: You Status: Ready to test Versions affected: - V7.6.245 - V7.5.267 - ... (down to V7.0.X) - V6.10.315 - V6.9.326 - ... (down to V6.0.X) - V5.14.421 - V5.13.442 - ... (down to V5.0.X) Fixed in: - V7.6.246 - V7.5.268 - ... (down to V7.0.X) - V6.10.316 - V6.9.327 - ... (down to V6.0.X) - V5.14.422 - V5.13.443 - ... (down to V5.0.X) Of course, testing all of that would be interesting, as would fixing further issues. Imagine having to do this for every single bug report and change. You'd spend days if not weeks fixing even trivial bugs if you'd have to support every single release of your software. That's why a lot of companies just drop support for older packages and leave people with no updates at all in cases where a minor version actually does break something, which isn't as much solving the problem, as it is simply sweeping it under the rug. I'm sure that some of them would perhaps want to update their older versions, but it simply isn't feasible, perhaps apart from supporting the major version as a whole and forcing people to update to the newest minor versions to keep getting support. So in reality, the list of supported versions would indeed be just the last minor releases in the first example above, which would look a bit like the following: More doable, but still not good enough in my mind - minor versions are for new features, which you don't necessarily want. My experience shows that oftentimes these new and supposedly backwards compatible features still break old functionality. Perhaps it's due to some refactoring that was needed, other times there were bugs fixed that old code relied upon and assumed to be the right way to do things. My pain is that you'll never actually know how far you can update, seeing as the breaking changes will get you sooner or later, even if it's supposed to just be a bug fix. In reality, you might find that you can only run your software with a subset of the released versions (orange being what you have now, green what you can update to without breakages, red being everything that won't work): Now, the understandable argument here is: \"Breaking changes are inevitable, you should just update your code.\" which is fair, but at the same time kind of useless. The industry shows that there are still plenty of projects that are stuck with JDK 8 and MySQL 5.7, or even a better example, old PHP versions like 5.6.40 and realistically cannot be updated and eventually will receive no security updates either, because all of that is simply too hard. And good luck with trying to tell your clients that now you'll probably need to alter or rewrite a huge portion of your code that uses the now deprecated or removed packages - oftentimes they simply won't understand that code rots with time. Worse yet, those that don't care about semantic versioning don't even attempt to keep track of all of this stuff - i've seen projects out there in the wild, where V6.9.0 is always followed by V7.0.0 and V7.0.1 is code for \"we screwed up the previous release, so this is just a hotfix\", plenty of people and orgs out there just look at version numbers like something to put in Jira and get paid for, rather than a measure of what the software is compatible with. Noone wants updates But that's just the tip of the iceberg. I think i've briefly described while backporting security fixes and such is pretty hard, even though some operating systems still try to do that. Now, here's a story about why updates are actually bad. In a word: breaking changes. In the aforementioned Spring migration, i updated the Flyway version and ran into the following message: org.flywaydb.core.internal.license.FlywayEditionUpgradeRequiredException: Flyway Teams Edition or Oracle upgrade required: Oracle OLD_VERSION is no longer supported by Flyway Community Edition, but still supported by Flyway Teams Edition. I think that this is the inevitable future of software. \"Want support for older versions of stuff that you're stuck with because of organizational reasons beyond your control? Sure, pay us!\" If you look at the pricing page, you actually see that you'd need to part with 3000 USD for that sort of a license: Now, i'm using this as an example for intentionally breaking support for older versions in a way that will hopefully (from the company's point of view) lead to them dying off and therefore the companies won't need to explicitly support them for a long time, thus attempting to solve the problem above. However, there are also plenty of cases out there, where updates introduce other breaking changes, oftentimes being forced upon the user against their will! For example, i want to use Flyway, but i don't want to be forced to use a version, which drops support for the DB that i need to use. And yet, i wasn't even consulted about this, since the newest version is mandated by Spring Boot:org.flywaydb flyway-coreThere are plenty of cases out there like this as well! For example, Docker Desktop doesn't let you decide whether you want updates or not, unless you pay them: And i'm sure that we've all also heard plenty of other horror stories of this very same trend, be it Ubuntu forcing updates with snap packages, or Windows 7 forcefully upgrading to newer releases when people step away from their computers for a bit. In a word, the way how companies approach updates nowadays suck, especially when they break things! Updates are bad All you need to do is go to the \"Everything is Broken\" section of my blog and you will see plenty of cases of updates breaking things. In the current year, it's gotten to the point where i treat every update like a possibly completely clean wipe and install from nothing. Therefore, backups are of the utmost importance and every update needs to have at least an hour or two set aside for when things will inevitably go wrong, as they oftentimes have in the past. But here's the thing - if i don't want to rely on SaaSS, then i have to host and manage my software by myself. If i want to do that, then i need to handle the updates myself as well. Every update takes a significant amount of time and potentially can break things. And, even more importantly, if there are any non-standard configuration changes done (even the theme of this very blog), then things become more likely to break and waste my time. But i don't have unlimited amounts of time. Actually, i don't have enough time to do my day job, learn new technologies for when it inevitably becomes stale and would otherwise lead to my career death, to also help my aging parents around the countryside estate with various chores, at the same time educate myself on both things in the industry and news in the world, as well as spend anywhere between 1-2 hours working out every day (a weight loss and personal wellness effort which seems to be working and about which i'll write a post some day) and have any sort of enjoyment of life in general, for example, by reading a sci-fi book in the evenings. Ergo, i cannot update. Ergo, companies cannot ship features AND handle all of the updates, without having enough tests to catch all of the problems, which they will never be able to do no matter how good their tests are without slowing down their development pace to the point where they are no longer competitive. Breaking feature updates of any sort are a nuisance and a direct impediment to progress more often than they're not and the more updates there are to be done, the less will be done. So don't update! For example, to see why we shouldn't update software, let's take a look at React. To date, it has over 500 versions that have been released. No, not development snapshots (even though some experimental new versions are included), publically released versions that need to be installed, managed and coped with: But once again, that's not the full story. Creating a simple React app with create-react-app makes us fetch an absolutely absurd amount of code, files and folders under node_modules: The idea of anyone ever understanding what's going on in there, or even auditing the security of all of it, is simply absurd! Don't believe me? Have a look at a dependency graph of this newly created \"Hello world\" project: There are almost 1500 different modules in there! Many of those will have updates eventually. Many of those will also have breaking changes. You will run into problems with that. And even if you don't, the chances of something malicious getting in there, or for the development stack to get too large for you to be able to explore it is a question of time, rather one of probability. Once again, do not update! A more stable world without updates Let's look at some software that's the exact opposite. It's stable, doesn't have many breaking changes, and comes with \"batteries included\". No, i'm not talking about Angular, because even that's a mess in comparison, i'm talking about the Lazarus IDE, the modern and perhaps the only way to write Pascal code! But it's not just a tool for writing code, instead, it does something a bit more amazing and peculiar. It also includes the Lazarus Component Library, a large package of components and classes that concern not only GUI development (which, by the way, is abstracted away from the platforms entirely), but also offers lots of useful functionality that would otherwise need to be installed separately in almost any other tech stack out there: Consider a scenario in which you want a MySQL driver for your app. In npm, you install it with bunches of dependencies. In Java, you pull it down with Maven or Gradle. In Python, you probably mess around with native extensions and have to install additional packages for your OS/container for it to even work. But with Lazarus, you drop it from the component palette into your application and are on your way. More platforms should be like that! Only support a known and trusted subset of software packages, keep them in tree and update them with the larger software package, in this case, the IDE. Some of you might realize that what i'm describing is actually a lot like the FreeBSD approach to packages - a stable enclave of \"good software\", that's optionally built upon by \"ports\", external and less table packages which the user will have to install themselves while being aware that those aren't supported. This sort of discouragement is exactly what most of the developers behind npm should have experienced more of! Same for React, Angular and most other technologies out there! What i'm saying is that most of the integrations, dependencies and other software should be kept \"in-tree\" for most tools and packages out there. To hell with dynamic linking, use stable and minimalistic packages where needed, much like Windows does instead of Linux. Not only that, but put anything and everything you will ever need within the standard library or one or two large additional libraries. When eventually people will want to do more, provide pushback and curb their enthusiasm, make them do less instead of more. Slow down their pace of development and therefore also updates, preferring stable and good software in 5 to 10 years, as opposed to a half baked SaaS trap in 2 years, which will stop being supported in 4. Yes, at this point i'm getting into why this probably won't be possible in our current society, but my thoughts on the subject are unchanged by this reality. It's already happening But that's not to say that large, breaking releases aren't already happening and that people aren't sticking with the more stable stuff. Just look at JDK versions that are used in the industry: The majority of existing projects out there are stuck with JDK 8, either due to their dependencies, their requirements, the ecosystem surrounding them, people not knowing about the benefits of the newer versions or simply not caring, or maybe the whole migration process being so time consuming, complicated and expensive that most companies out there simply don't want to touch the idea of doing that with a 10 foot pole. Essentially, what i described above is happening, but for all of the wrong reasons! That brings me to the next point - situations like that are unavoidable. Sooner or later, there will be a significant amount of projects out there that will be stuck on old versions. There will be plenty of .NET 2.0 projects out there that will be running on dated installations of Windows Server 2012, instead of running the latest cross platform .NET Core or nowadays just .NET 5+. There will be plenty of old Java projects out there, or old PHP 5 projects out there and so on. And my argument is not that this should be the expected outcome, but rather that it's something inevitable. Therefore, we should prepare for the eventual mess of legacy software, with entire languages possibly dying out and there being significant issues in finding developers for them, once it's discovered that there's actually software that's mission critical running in some old forgotten server room. And the easier we'll make dealing with old code for ourselves, the easier our lives will be - the more focus we'll put on truly non-breaking security updates that keep your dated software alive and running well, the better our future will be. To that end, it's actually imperative that we slow down our development and the speed of updates, which add new features. Forcing things to slow down It's important that code written 20 years in the future remain vaguely similar to what we have now, so that old codebases can be recognized. Think Java Streams API, over syntactic sugar that changes how the actual language works. Think more along the lines of a stable base, that's well documented, thoroughly tested and helps you use it with its discoverability, instead of making you do some long forgotten Maven rituals that are like something out of Warhammer 40k, because noone bothered to write documentation in the pursuit of agility. Bring back the old days where we engineered software to be used long term, instead of the current day world of: \"Move fast and break things... oh, and don't bother thinking about what'll happen in 10 years, because your CV will be full of bleeding edge stuff, your bank account will be loaded and the troubles with most of the projects behind you won't be the slightest of your concerns.\" Realistically, you probably don't care about this stuff, but on the grand scale of things, perhaps you should. I don't know about you, but i think that we shouldn't even need containers for reproducible environments in the first place. We should take a page out of the BSD operating system group book, instead of the GNU/Linux distros that are patched together out of bunches of different pieces of software of vastly varying quality. We shouldn't bend to the whims of the businesses which want things done yesterday. We should strive for the standards of engineering, where we are in control of the stacks that we work with and actually know what's going on inside of them. Why do we need a new Java version every year and to only support the LTS ones for 5 years? Why do we need multiple Ubuntu releases per year and to only support the LTS ones for 5 years? Why not a new release every 5 years with everything else between being just patches and non breaking changes and a new major release every 20 years instead? If things don't work out that way, then eventually we'll just have bunches of clueless code monkeys that execute npm commands that they don't understand to use frameworks and mechanisms which they have no hope of understanding to create as much value as quickly possible in the short period before all of that goes to hell and there's the eventual breakdown of nothing working and noone having any idea of why. I'm exaggerating slightly, but sooner or later software projects out there will exceed the cognitive capabilities of any single person - the only question is whether that will happen in 50 or 500 years. What to actually do now? Don't choose Clusterpoint for your project, which actually died in just a few years and now needs to be maintained in one of the legacy projects that i'm stuck in. Instead, pick PostgreSQL and use it for more than 20 years. Realistically, you can't avoid updates entirely, but you can at least try to decrease the frequency of them that you'll run into and will have to deal with. Use self contained packages and only use them for the things that they are good for - for example, Ruby on Rails for a simple webapp will be a better choice than stitching together 20 different libraries. Having the freedom of choice in the library case will cause more problems than its worth. And lastly, choose boring technology. Only use software that you're sure will be supported in 10 years. Only use software, which has very few \"unknown unknowns\". Only use software, where the development pace has slowed down to a reasonable degree, as opposed to React pulling a sneaky on everyone and introducing hooks, which was a pretty bad move, especially if you needed to migrate to them. Use maintenance mode software: Debian over Fedora Docker Swarm over Kubernetes Java over Go Angular over React Of course, this only applies to projects that you're about to start. Chances are, that if you're working in the industry and will have to handle codebases written by others, then preferring brownfield technologies will make you less happy, given that there will be more bad ones out there. Your mileage may vary and don't expect all of the answers here. As for the actual updates: the above approaches should decrease the frequency at which they need to be done. That said, the title of the article is not entirely honest - do install security updates and do install updates to the LTS software that you're using, be it Ubuntu LTS, Unity LTS, Java 11, or anything else... Just be sure that you have backups that work before doing that! Update I've cleaned the text up a little bit and have decided to extract an idea about a versioning scheme out of the main body of text. I believe that software could benefit from something more finely grained than semantic versioning. Perhaps an alternate scheme, that goes more into detail about what's included in any given release: major new functionality changes minor, backwards compatible'ish functionality additions patch, bug fixes that could still break code that's written with them in mind security, fixes that should not affect code bugs as long as those do not concern security exploits Maybe it would be possible to have those patch versions be globally unique, so that you'd know that your software V5.152.4567.7213 has had 5 major versions released so far, 152 feature releases across those major versions, 4567 bug fixes to date and 7213 security fixes? Thus, you could tie them to Jira ticket numbers or what have you, though it would probably break some ordering of these versions somewhere, so i'm not entirely sure about that. Granted, i can't actually tell whether adding that sort of complexity would actually be good for anyone, seeing as people seem to have trouble keeping themselves to the standards set forth by semantic versioning already. Update #2 I've actually written a separate article about how software versioning could be done, with a larger focus on stability and backwards compatibility. I still think that i'm quite there yet in regards to the quality of my writing, but it should serve as a conceptual alternative to semver, maybe it's worth a look at? Someone actually posted this article on Hacker News, though sadly i missed it due to being busy with my work. Nonetheless, you can go to the thread and read my comment, which should provide a real world example of why breaking changes and large generational shifts can be a very time consuming thing. To sum up everything above in a single sentence: In my eyes it could be pretty nice to have a framework version that's supported for 10-20 years and is so stable that it can be used with little to no changes for the entire expected lifetime of a system. Syndicate Atom 1.0 RSS JSON By the way, want an affordable VPN or VPS hosting in Europe? Personally, I use Time4VPS for almost all of my hosting nowadays, including this very site and my homepage! (affiliate link so I get discounts from signups; I sometimes recommend it to other people, so also put a link here) Maybe you want to donate some money to keep this blog going? If you'd like to support me, you can send me a donation through PayPal. There won't be paywalls for the content I make, but my schedule isn't predictable enough for Patreon either. If you like my blog, feel free to throw enough money for coffee my way! Cookieless analytics by Matomo. Blog powered by Grav. Home Articles Tutorials Presentations Everything Is Broken",
    "commentLink": "https://news.ycombinator.com/item?id=41009942",
    "commentBody": "Never Update Anything (kronis.dev)175 points by generatorman 23 hours agohidepastfavorite111 comments SoftTalker 22 hours ago\"In my eyes it could be pretty nice to have a framework version that's supported for 10-20 years and is so stable that it can be used with little to no changes for the entire expected lifetime of a system.\" This is what applications used to be like, before the web and internet hit and regular or even push updating became easy. It was simply so difficult and expensive to provide updates once the software was in the customer's hands that it was done intentionally and infrequently. For the most part, you bought software, installed it, and used it. That was it. It never changed, unless you bought it again in a newer version reply EvanAnderson 22 hours agoparentFrequent updates, in the old days, meant that a vendor had poor QA. I think that's probably still the case most of the time today, too. reply jjav 12 hours agorootparent> Frequent updates, in the old days, meant that a vendor had poor QA. I think that's probably still the case most of the time today, too. Back in the 80s and 90s if you had bad QA you'd ship very buggy software and customers hated you because they had to live with it for months and months until the company managed to do another release. And then it was costly to ship off those floppies to every customer. So there was a very real price to pay, in money and reputation. Then it became possible to do updates online. Initially it was a nice way to deliver an emergency fix if necessary, but you mostly continued to do nice QA'd releases every now and then. But as with everything, when something becomes too easy it gets abused. So companies realized why do much QA (or any QA in extreme cases). Just push updates all the time and hope for the best, if customers (who are now QA) scream, push another update tomorrow. Break quick, fix quick. It's mostly unsatisfactory if one values stable quality software. reply tivert 22 hours agorootparentprev> Frequent updates, in the old days, meant that a vendor had poor QA. I think that's probably still the case most of the time today, too. The internet has normalized poor QA. The bosses don't give a shit about QA anymore because it's so cheap to just push out a patch. I mean just look at old video game magazines that talked about the software development process: the developers would test the hell out of a game, then test the hell out of it again, because once it was burned onto a $100 cart (in 2024 dollars) it wasn't ever going to change. Now games can remain buggy and unstable for months or even years after \"release.\" reply dasil003 21 hours agorootparentI never worked on games, but I did do a streaming video app for PS3 in 2010, during the time period when it was arguably the best media center box available. Working with Sony (SCEE) on this was eye opening how their QA process was set up. Essentially you formally submitted the compiled app (C++ and Flash!) to them, and then tickets would come back and you'd have to address them. QA was a separate org so you never really met them in person, all issue discussion would happen in-band to the ticketing system. QA had all the power to decide when and if this thing would ship, much moreso than Product or Engineering. I can't say the process made a ton of sense for a downloadable app powered by a multi-platform online service, but it was illuminating as to how the high quality bar of 90s/00s console games was achieved. reply chrisjj 20 hours agorootparent> then tickets would come back Luxury! With Nintendo you'd often get only one ticket. Any further bugs would cost you further submissions, and months of slippage. reply dasil003 19 hours agorootparentThanks for jogging my memory! I believe we did two rounds of QA, the first was unlimited but shallow and high latency, the final QA I believe was called Format QA and there was a quota, if you had more than 6 issues or something you were rejected and had to go back into the queue (for months). reply specialist 18 hours agorootparentprevaka acceptance testing aka The Correct Answer™ For the products I managed in the 90s, I put QA/Test in charge of releases. Very unusual. The results were awesome. reply numpad0 20 hours agorootparentprevWindows XP. High profile zero day cases and Windows Update during 2000s created a \"security updates are like dietary supplements\" mindset. reply chrisjj 20 hours agorootparentprevThe differece is it no longer means the vendor's QA is poorer /than average/. reply hypercube33 21 hours agoparentprevI remember even games, or especially games were like this. Interplay would rarely have a post launch patch or make it past 1.01 versions of a whole game. then in the late 90s or 2000ish Tribes 2 came out and basically didn't even work for over a year until patches finished the game. I think once Internet hit critical mass things shifted forever and haven't gone back. reply teeray 17 hours agorootparentAlso, the bugs became part of the game and make the whole thing more interesting. Speedrunning as a whole relies on these flaws. reply palata 18 hours agoparentprev> For the most part, you bought software, installed it, and used it. That was it. It never changed, unless you bought it again in a newer version And it was much better than the current situation, if you ask me. reply acdha 17 hours agorootparentThat software did a lot less, however, and you just had to live with bugs. I knew people who just learned that they couldn’t use a feature without crashing, or had to disconnect from the internet to print, had to write their own math formula because a built in function was wrong, etc. and just worked around it for years. The first company I worked for in the 90s had a C codebase which seemed like it was half #ifdef or runtime version checks because they had to support customers who rarely updated except when they bought new servers, and that meant that if some version of SunOS, DOS/Windows/etc. had a bug you had to detect and either use a work around or disable a feature for years after the patch had shipped. I do agree that stability, especially on the UI side, has serious value but my nostalgia is tempered by remembering how many people spent time recovering lost work or working around gaps in software which had been fixed years ago. I think the automatic update world is better on the whole but we need some corrective pressure like liability for vendors to keep people from pulling a Crowdstrike in their testing and reliability engineering. reply palata 8 hours agorootparent> I think the automatic update world is better on the whole I think that we went to another extreme. Because it's so easy to update, we just ship bad software saying \"we'll fix it later\". And we don't. reply acdha 4 hours agorootparentHence the rest of the quoted sentence. I think removing barriers to fixing bugs is good but companies need to feel more pressure to do so. reply patrick451 5 hours agorootparentprevWe still have to find workarounds for crappy software. But with a continuous update model, we waste more time fighting with bugs because the bugs are constantly changing. Every week you have to waste figuring out what broke now. No thanks. reply KronisLV 22 hours agoprevOh hey, I was wondering why the VPS suddenly had over 100 load average, restarted Docker since the containers were struggling, now I know why (should be back now for a bit). Won't necessarily fix it, might need to migrate over to something else for the blog, with a proper cache, alongside actually writing better articles in the future. I don't think the article itself holds up that well, it's just that updates are often a massive pain, one that you have to deal with somehow regardless. Realistically, LTS versions of OS distros and technologies that don't change often will lessen the pain, but not eliminate it entirely. And even then, you'll still need to deal with breaking changes when you will be forced to upgrade across major releases (e.g. JDK 8 to something newer after EOL) or migrate once a technology dies altogether (e.g. AngularJS). It's not like people will backport fixes for anything indefinitely either. reply cjalmeida 22 hours agoparent>might need to migrate over to something else for the blog, with a proper cache Never Update _Anything_ :) reply KronisLV 22 hours agorootparentI am very much tempted not to because it works under lower loads, could just put it on a faster server, but how could I pass up the chance to write my own CMS (well, a better one than the previous ones I've done)? That's like a rite of passage. But yes, the irony isn't lost on me, I just had to go for that title. reply stavros 21 hours agorootparentIf you have to write your own CMS, make it compile to static files. I did that with Django, used Django-distill, and it's hands down the best static site generator I've ever used. My site never needs updates and never goes down under any amount of load. reply hipadev23 20 hours agorootparent“static files” are nothing more than no-TTL caching strategy with manual eviction. reply brirec 17 hours agorootparentYou’re not wrong… but “static files” ultimately are infinitely less complex than any dynamic CMS, and require no effort or brain power to migrate between (even bottom-of-the-barrel) providers reply ksec 13 hours agorootparentI sort of not understand why we are not using static Files more often now. In the old days we need CMS mostly because generating links and update to certain pages were expensive. Hard Disk were slow and memory were expensive. Now we have SSDs that eliminate 99.9999% of the problem. reply stavros 19 hours agorootparentprevOK. reply augusto-moura 18 hours agorootparentprevIf you are using open source you can always support your own old versions ~joking but not really~ Of course security updates are very hard, but if an old version has some good community you have the option of forking or upstreaming the updates yourself reply type0 17 hours agorootparent> ~joking but not really~ For some languages and applications it can be trivial to backport the changes then trying to keep up with the new features. If it's tested and stable it will likely be more stable than a new version, I do this for some smaller programs and I'm not even a real programmer but more of a hobbyist reply SenHeng 15 hours agoparentprevBack in 2018, I helped a startup update their AngularJS wrapped jQuery prototype to 1.6 and tried hard to separate business logic from component code so they could migrate over to vue, react or angular2 after I left. I regularly snowboard with someone still at the company. They’re still on AngularJS. AngularJS never dies. AngularJS is forever. reply louwrentius 20 hours agoparentprevFor contrast, I recently had a no 1. HN hit and my Pi4 never had a core beyond 20% Yes, it’s a static website. It’s amazing how little performance you actually need to survive a HN avalanche reply ploxiln 17 hours agorootparentIt's amazing how powerful hardware is in the past decade, and how masterly crafted and efficient some software is (linux, nginx, etc) ... while other software is so profoundly inefficient, that we forget. reply thraxil 9 hours agorootparentprevYeah, I've had a few HN frontpages with sites running Django and recently Phoenix with essentially no caching or optimization running on a 256MB fly.io free tier instance with barely a noticable increase in load (just big spikes in the network/traffic graphs). reply Joel_Mckay 22 hours agoparentprevAlpine linux was designed for web services, as it includes the bare minimum resources necessary for deployment. https://wiki.alpinelinux.org/wiki/Nginx Also, may want to consider a flat html site if you don't have time to maintain a framework/ecosystem. =3 reply KronisLV 22 hours agorootparentAlpine is pretty nice! I did end up opting for Ubuntu LTS (and maybe the odd Debian based image here or there) for most of my containers because it essentially has no surprises and is what I run locally, so I can reuse a few snippets to install certain tools and it also has a pretty long EOL, at the expense of larger images. Oddly enough, I also ended up settling on Apache over Nginx and even something like Caddy (both of which are also really nice) because it's similarly a proven technology that's good enough, especially with something like mod_md https://httpd.apache.org/docs/2.4/mod/mod_md.html and because Nginx in particular had some unpleasant behavior when DNS records weren't available because some containers in the cluster weren't up https://stackoverflow.com/questions/50248522/nginx-will-not-... I might go for a static site generator sometime! reply Joel_Mckay 22 hours agorootparentApache is stable for wrapping mixed services, but needs a few firewall rules to keep it functional (slow loris + mod_qos etc.) =) Ubuntu LTS kernels are actually pretty stable, but containers are still recommended. ;) reply KronisLV 22 hours agorootparentThat's fair! Honestly, it's kind of cool to see how many different kinds of packages are available for Apache. A bit off topic, but I rather enjoyed the idea behind mod_auth_openidc, which ships an OpenID Connect Relying Party implementation, so some of the auth can be offloaded to Apache in combination with something like Keycloak and things in the protected services can be kept a bit simpler (e.g. just reading the headers provided by the module): https://github.com/OpenIDC/mod_auth_openidc Now, whether that's a good idea, that's debatable, but there are also plenty of other implementations of Relying Party out there as well: https://openid.net/developers/certified-openid-connect-imple... I am also on the fence about using mod_security with Apache, because I know for a fact that Cloudflare would be a better option for that, but at the same time self-hosting is nice and I don't have anything too precious on those servers that a sub-optimal WAF would cause me that many headaches. I guess it's cool that I can, even down to decent rulesets: https://owasp.org/www-project-modsecurity-core-rule-set/ though the OWASP Coraza project also seems nice: https://coraza.io/ reply Joel_Mckay 21 hours agorootparentI prefer x509 client GUID certs, and AMQP+SSL with null delineated bson messaging. Gets rid of 99.999% of problem traffic on APIs. It is the most boring thing I ever integrated, and RabbitMQ has required about 3 hours of my time in 5 years. I like that kind of boring... ;) reply graemep 20 hours agorootparentprevWhat exactly do you do to protect Apache from slow loris? Its my main reason for not using Apache. reply Joel_Mckay 19 hours agorootparentThere are several different ways, but the easiest is mod_reqtimeout/mod_qos/mod_security. Check your install with \"sudo apache2ctl -M\", and there should be several legacy tutorials available (I'd ignore deprecated mod_antiloris.) Rate-limiting token-bucket firewall settings are a personal choice every team must decide upon (what traffic is a priority when choking bandwidth), and often requires tuning to get it right (must you allow mtu fragging for corporate users or have a more robust service etc.) These settings will also influence which events trip your fail2ban rule sets. Have a great day, =) reply pron 21 hours agoprevBecause of this, in the JDK we've adopted a model we call \"tip & tail\". The idea is that there are multiple release trains, but they're done in such a way that 1/ different release trains target different audiences and 2/ the model is cheap to maintain -- cheaper, in fact, than many others, especially that of a single release train. The idea is to realise that there are two different classes of consumers who want different things, and rather than try to find a compromise that would not fully satisfy either group (and turns out to be more expensive to boot), we offer multiple release trains for different people. One release train, called the tip, contains new features and performance enhancements in addition to bug fixes and security patches. Applications that are still evolving can benefit from new features and enhancements and have the resources to adopt them (by definition, or else they wouldn't be able to use the new features). Then there are multiple \"tail\" release trains aimed at applications that are not interested in new features because they don't evolve much anymore (they're \"legacy\"). These applications value stability over everything else, which is why only security patches and fixes to the most severe bugs are backported to them. This also makes maintaining them cheap, because security patches and major bugs are not common. We fork off a new tail release train from the tip every once in a while (currently, every 2 years). Some tail users may want to benefit from performance improvements and are willing to take the stability risk involved in having them backported, but they can obviously live without them because they have so far. If their absence were painful enough to justify increasing their resources, they could invest in migrating to a newer tail once. Nevertheless, we do offer a \"tail with performance enhancements\" release train in special circumstances (if there's sufficient demand) -- for pay. The challenge is getting people to understand this. Many want a particular enhancement they personally need backported, because they think that a \"patch\" with a significant enhancement is safer than a new feature release. They've yet to internalise that what matters isn't how a version is called (we don't use semantic versioning because we think it is unhelpful and necessarily misleading), but that there's an inherent tension between enhancements and stability. You can get more of one or the other, but not both. reply chad1n 20 hours agoparentMost LTS strategies are like this, enterprises run the LTS version on the server while the consumers run the latest version. In a way, it is beta testing, but the consumer isn't really mad about it since he gets new features or performance boosts. LTS users usually update once every 3-6 months or if a serious CVE comes out, while normal users update daily or weekly. To be honest, I know servers running whatever the latest version of nodejs is, instead of LTS, mostly because they don't know that node has a LTS policy. reply jldugger 20 hours agoparentprevIs this any different than the LTS approach Canonical and others take? reply pron 20 hours agorootparentI think Canonical do something similar, but as to other LTSs -- some do tip & tail and some don't. The key is that tails get only security patches and fixes to major bugs and rarely anything else. This is what offers stability and keeps maintaining multiple tails cheap (which means you can have more of them). Even some JDK vendors can't resist offering those who want the comforting illusion of stability (while actually taking on more real risk) \"tail patches\" that include enhancements. reply palata 17 hours agoprevWhat the article points to is that most updates are bad updates. We teach people that they should accept all updates for security reasons, but really they should only accept security updates. But they can't, because this is not a possibility that is given to them. All updates are put together, and we as an industry suck at even knowing if our change is backward compatible or not (which is actually some kind of incompetence). And of course it's hard, because users are not competent enough to distinguish good software from bad software, so they follow what the marketing tells them. Meaning that even if you made good software with fewer shiny features but actual stability, users would go for the worse software of the competitor, because it has the latest damn AI buzzword. Sometimes I feel like software is pretty much doomed: it won't get any better. But one thing I try to teach people is this: do not add software to things that work, EVER. You don't want a connected fridge, a connected light bulb or a connected vacuum-cleaner-camera-robot. You don't need it; it's completely superfluous. Also for things that actually matter, many times you don't want them either. Electronic voting is an example I have in my mind: it's much easier to hack a computer from anywhere in the world than to hack millions of pieces of paper. reply ranger207 14 hours agoparentAnd you can't have _just_ security updates, because the combinational complexity of security fixes across feature versions is insane, not to mention the fact that the interaction of the security and features changes can themselves introduce bugs. There's groups that try (eg distro maintainers) but it's ultimately a losing battle. I'm convinced that patching is only a bandaid, but it's also impossible to have 100% bug-free code, so there needs to be some sort of systematic solution on top of whatever particular code is running. Behavior analysis, egress network analysis, immutable by default images with strictly defined writeable volumes and strictly defined data that's going to be written there, etc. There's not a silver bullet, but I think patching and trying to keep up with updates is, like, a gallium bullet at best reply o11c 17 hours agoparentprevI also noticed that there's another kind of update not mentioned: * Updates that add a theoretically independent feature, but which other software will dynamically detect and change their behavior for, so that it's not actually independent. reply hitpointdrew 23 hours agoprev\"Never Update Anything\" Author proceeds to add to two updates to the article, epic troll. reply Joel_Mckay 22 hours agoparentThese days it makes sense to life-cycle entire container images rather than maintain applications with their dependencies. The current BSOD epidemic demonstrated the folly of mass concurrent versioning. *nix admins are used to playing upgrade Chicken with their uptime scores. lol =) reply UniverseHacker 21 hours agoprevI pretty much agree- most systems don't need updating. I've seen and setup OpenBSD servers that ran for a decade without issues never getting updates. I currently run some production web services on Debian where I do updates every 3 years or so, and no issues. Leaving something alone that works good is a good strategy. Most of the cars on the road are controlled by ECUs that have never had, and never will have any type of updates, and that is a good thing. Vehicles that can get remote updates like Teslas are going to be much less reliable than one not connected to anything that has a single extensively tested final version. An OS that is fundamentally secure by design, and then locked down to not do anything non-essential, doesn't really need updates unless, e.g. it is a public facing web server, and the open public facing service/port has a known remote vulnerability, which is pretty rare. reply hcarvalhoalves 15 hours agoparent> Vehicles that can get remote updates like Teslas are going to be much less reliable than one not connected to anything that has a single extensively tested final version. I don’t think it’s necessarily this, but the fact that being able to update anytime is a great source of pressure to release untested software at any cost. reply mikewarot 10 hours agoprevWhen I was the sole IT guy for a small consulting company, once I got everything working, I never updated it. We used Microsoft office 2000 for 12 years. Never had to retrain people, deal with the weird ribbon toolbar, etc. It's only the deranged use of OSs with ambient authority that gums up what would otherwise be stable systems. reply Groxx 14 hours agoprevTo jump on a related article since it's linked and comments are now closed: https://blog.kronis.dev/articles/stable-software-release-sys... The 2021/2022/2023/2024 version-numbering schemes are for applications, not libraries, because applications are essentially not ever semver-stable. That's perfectly reasonable for them. They don't need semver. People don't build against jetbrains-2024.1, they just update their stuff when JetBrains breaks something they use (which can happen at literally any time, just ask plugin devs)... because they're fundamentally unstable products and they don't care about actual stability, they just do an okay job and call it Done™ and developers on their APIs are forced to deal with it. Users don't care 99%+ of the time because the UI doesn't change and that is honestly good enough in nearly all cases. That isn't following semver, which is why they don't follow semver. Which is fine (because they control their ecosystem with an iron fist). It's a completely different relationship with people looking at that number. For applications, I totally agree. Year-number your releases, it's much more useful for your customers (end-users) who care about if their habits are going to be interrupted and possibly how old it is. But don't do it with libraries, it has next to nothing to do with library customers (developers) who are looking for mechanical stability. reply aspyct 11 hours agoprevThere's an alternative solution: update everything, but limit your dependencies. Example: for my (personal) projects, I only use whatever is available in the debian repositories. If it's not in there, it's not on my dependency list. Then enable unattended upgrades, and forget about all that mess. reply schiffern 23 hours agoprevHug of Death right now, mirror here: https://web.archive.org/web/20240509031433/https://blog.kron... reply cjalmeida 22 hours agoparentWell, if only he had updated his server stack to something more scalable... reply jspash 22 hours agoparentprevMaybe they should have updated the server capacity? reply KronisLV 22 hours agorootparentCurrently it's running on a VPS that has 1 CPU core and 4 GB of RAM, resources which are shared with a few other processes. I'm thinking that I might move over from multiple smaller VPSes (better separation of resources, smaller fallout from various issues) to a fewer bigger ones in the future (also cheaper), in which case the containers would struggle less under load spikes. reply Plasmoid 20 hours agoprevI was working at a place that delivered onprem software. One customer asked us \"We like features of version N but we're running N-1. Can you backport them so you don't have to upgrade?\". I replied we'd already done that, it was called version N. reply yjftsjthsd-h 18 hours agoparentI mean... there could be a case for \"does this feature really require breaking changes?\" that would distinguish those. reply walterbell 16 hours agoprev> And lastly, choose boring technology. Only use software that you're sure will be supported in 10 years. Only use software, which has very few \"unknown unknowns\". Only use software, where the development pace has slowed down to a reasonable degree. Perl has been stable for a couple of decades. reply eXpl0it3r 21 hours agoprevEven as a developer not focused on web dev this sounds pretty bad, unless everyone in your dependency tree (from OS to language to libraries) decides to make a switch and even then, you'll be stuck with outdated ways to do things. Who wants to continue maintaining C++03 code bases without all the C++11/14/17/20 features? Who wants to continue using .NET Framework, when all the advances are made in .NET? Who wants to be stuck with libraries full of vulnerabilities and who accepts the risk? Not really addressed is the issue of developers switching jobs/projects every few years. Nobody is sticking around long enough to amass the knowledge needed to ensure maintenance of any larger code base. Which is caused by or caused the companies to also not commit themselves for any longer period of times. If the company expects people to leave within two years and doesn't put in the monetary and non-monetary effort to retain people, why should devs consider anything longer than the current sprint? reply KronisLV 21 hours agoparent> Who wants to continue maintaining C++03 code bases without all the C++11/14/17/20 features? Who wants to continue using .NET Framework, when all the advances are made in .NET? Who wants to be stuck with libraries full of vulnerabilities and who accepts the risk? With the exception that in this hypothetical world we'd get backported security updates (addressing that particular point), who'd want something like this would be the teams working on large codebases that: - need to keep working in the future and still need to be maintained - are too big or too time consuming to migrate to a newer tech stack (with breaking changes in the middle) with the available resources - are complex in of themselves, where adding new features could be a detriment (e.g. different code styles, more things to think about etc.) Realistically, that world probably doesn't exist and you'll be dragged kicking and screaming into the future, once your Spring version hits EOL (or worse yet, will work with unsupported old versions and watch the count of CVEs increase, hopefully very few will find themselves in this set of circumstances). Alternatively, you'll just go work somewhere else and it'll be someone else's problem, since there are plenty of places where you'll always try to keep things up to date as much as possible, so that the delta between any two versions of your dependencies will be manageable, as opposed to needing to do \"the big rewrite\" at some point. That said, enterprises already often opt for long EOL Linux distros like RHEL and there is a lot of software out there that is stuck on JDK 8 (just a very visible example) with no clear path of what to do once it reaches EOL, so it's not like issues around updates don't exist. Then again, not a lot of people out there need to think about these things, because the total lifetime of any given product, project, their tenure in the org or even the company itself might not be long enough for those issues to become that apparent. reply knallfrosch 21 hours agoprevWe're being paid to migrate our hardware boxes programmatically to Windows 10 IoT LTSC so that new boxes ship with 10+ years of security. We're still supporting some XP devices (not connected to the internet.) So to anyone depending on us: You're welcome. But let me tell you something: Long-Term Support software mostly doesn't pay well, and it's not fun either. Meanwhile some Google clown is being paid 200k to fuck up Fitbit or rewrite Wallet for the 5th time in the newest language. So yeah. I'd love to have stable, reliable dependencies while I'm mucking around with the newest language de jour. But you see how that doesn't work, right? reply kccqzy 21 hours agoparentThe fucking up of Fitbit and the rewriting of Wallet are not the engineers' fault. These kind of projects are mostly decided and planned by PMs: clueless and incompetent PMs. For payments in particular it was not even just an incompetent PM, but an incompetent director that saw the success of the NBU Paisa payment in India and thought the U.S. would be the same. The engineers are at most just complicit. Those who aren't are laid off or they quit on their own accord. reply ozim 20 hours agorootparentEngineers are mostly complicit because they get that 200k$ salary when they chase next shiny thing. No one is paying such salaries for mundane clerical job. reply binary132 5 hours agorootparentprevLead engineers and architects chasing resume-filler projects are absolutely part of the problem reply chrisjj 20 hours agorootparentprevYou're saying complicity is not a fault?? :) reply cupantae 21 hours agoprevI’ve supported enterprise software for various big companies and I can tell you that most decision makers for DCs agree with this sentiment. EMC had a system called Target Code which was typically the last patch in the second-last family. But only after it had been in use for some months and/or percentage of customer install base. It was common sense and customers loved it. You don’t want your storage to go down for unexpected changes. Dell tried to change that to “latest is target” and customers weren’t convinced. Account managers sheepishly carried on an imitation of the old better system. Somehow from a PR point of view, it’s easier to cause new problems than let the known ones occur. reply palata 18 hours agoprev> My experience shows that oftentimes these new and supposedly backwards compatible features still break old functionality. Well that's the first issue: downright malpractice. Developers should learn how to know (and test) whether it is a major change or not. The current situation is that developers mostly go \"YOLO\" with semantic versioning and then complain that it doesn't work. Of course it doesn't work if we do it wrong. reply 1vuio0pswjnm7 15 hours agoprevAvoid software that will need constant updates. Because that is a signal it is defective to begin with, or expected to be broken soon. For example, I avoid graphical commercial OS, large, graphical web browsers. Especially mobile OS and \"apps\". Avoidance does not have to be 100% to be useful. If it defeats reliance on such software then it pays for itself, so to speak. IMHO. The notion of allowing RCE/OTA for \"updates\" might allegedly be motivated by the best of intentions. But these companies are not known for their honesty. Nor for benevolence. And let's be honest, allowing remote access to some company will not be utilised 100% for the computer owner's benefit. For the companies remotely installing and automatically running code on other peoples' computer, surveillance has commercial value. Allowing remote access makes surveillance easier. A cake walk. reply m3kw9 15 hours agoparentException is mobile apps reply hcarvalhoalves 15 hours agoprevExtreme viewpoint, but agree strongly. Big reason why working in Common Lisp brings a smile to my face - it’s a standard, quicklisp works, ffi works, etc. I can run code and follow instructions written DECADES ago, it just damn works. reply cogman10 22 hours agoprevI find it pretty funny that immediately on the first click of this article I was greeted with an internal server error. reply KronisLV 22 hours agoparentThat was me scrambling to allocate more resources to the container and redeploy it, after my alerting ticked me off about issues and I figured out what's going on. While the container itself was down, the reverse proxy returned an error. reply userbinator 19 hours agoprevThere is also another type of update: security updates that don't actually matter in the environment that the software is used in. The question of whether the \"new features\" are for or against the user is another point to ponder. reply tonymet 17 hours agoprevour industry could use a risk assessment index scanner on updates, similar to \"npm audit\" , that measures the delta between versions and gives a risk indicator based on a number of parameters. The issue with changelogs is that they are an honor system, and they don't objectively assess the risk of the update. Comparing changes in the symbol table and binary size could give a reasonable red/yellow/green indicator of the risk of an update. Over time, you could train a classifier to give even more granularity and confidence. reply nickthegreek 23 hours agoprevPreviously (November 4, 2021 — 319 points, 281 comments): https://news.ycombinator.com/item?id=29106159 reply albertP 20 hours agoprevHe. I've been running always everything one or two versions behind latest (for my personal laptop, not servers). That means mainly OS (e.g., macOS), but as long as I can avoid automatic updates, I do so. I believe the chances of having a bricked laptop because of a bad update are higher than the chances of getting malware because running one or two versions behind the latest one. reply AaronFriel 22 hours agoprevA feature I've wanted for ages, for every OS package manager (Windows, apt, yum, apk, etc.), every language's package manager (npm, pypi, etc.), and so on is to update but filter out anything less than one day, one week, or one month old. And it applies here, too. Now, some software, they effectively do this risk mitigation for you. Windows, macOS, browsers all do this very effectively. Maybe only the most cautious enterprises delay these updates by a day. But even billion dollar corporations don't do a great job of rolling out updates incrementally. This especially applies as tools exist to automatically scan for dependency updates, the list of these is too long to name - don't tell me about an update only a day old, that's too risky for my taste. So for OS and libraries for my production software? I'm OK sitting a week or a month behind, let the hobbyists and the rest of the world test that for me. Just give me that option, please. reply gnramires 21 hours agoparentDebian has 2/3 stages of software deployment that I know of: Unstable, Testing and Stable. By the time it comes to stable it has been quite extensively tested. The exceptions are only security updates which you may want to get very quickly anyway. I really recommend Debian (in particular with unattended security upgrades) for severs. Other distros have this as well (Thumbleweed, Void, etc.), and I really think most people should not be using recently-deployed software. A small community using them however helps testing so the rest of us can have more stability. Which is why I don't recommend using Arch (or Debian unstable) for general users, unless you specifically want to help testing and accept the risk. Also randomizing update schedules by at least a few hours does seem very wise (I don't think even the most urgent updates would make or break in say 6 hours of randomization?) reply attentive 20 hours agoparentpreval2023 uses \"releasever\" which is basically a dated snapshot of the packages. You can choose to install last-1 instead of the latest. reply n_plus_1_acc 22 hours agoparentprevIsn't that basically non rolling-release distros? reply c0balt 21 hours agorootparentYesn't, many still release updates frequently as long (usually, if server etc.) as they are compatible. Mostly though only minor updates for features. This is required for some components, like, e.g., glibc or openssh, to stay secure-ish. reply msoad 21 hours agoprevKinda ironic that the article itself was updated reply gtirloni 15 hours agoprevThe world is not static and software these days is very interconnected. Dreams of not updating only work in a unchanging world. Sadly, this world is still to be found. reply LargoLasskhyfv 15 hours agoparentFunny you're saying that. In context of the stricken crowds I've read that FedEx, UPS and SouthWest had no problems at all, because many systems run on Windows95, or even something based on Win3.1x :-) Seems they are well isolated, but networked nonetheless. Or do you think they use bush-drums and smoke signs to conduct their businesses? reply Groxx 14 hours agorootparentThat probably has more to do with not jumping on the latest fad* and installing an unattended unrestricted automatically-globally-updating rootkit on all of your critical machines. That is generally recognizable as stupidity. And the ones that did so are now paying the price. * compliance tactics are very prone to fads. just look at cookie banners. reply binary132 16 hours agoprevI had started to think I was the only one saying this. reply apantel 22 hours agoprevNo no no, it’s “never update anything and don’t expose your machine to the internet”. Winning strategy right there. reply KronisLV 22 hours agoparentI know it's supposed to be a statement to take the absurd title of my article a bit further, but in some cases, I can see that being said unironically. Nothing good would happen if some machine running Windows XP in a hospital that's hooked up to an expensive piece of equipment that doesn't run with anything else suddenly got connected to the Internet. Nor does the idea of any IoT device reaching past the confines of the local network make me feel safe, given how you hear about various exploits that those have. On one hand, you should get security patches whenever possible. On the other hand, it's not realistic to get just security patches with non-breaking changes only. Other times, pieces of hardware and software will just be abandoned (e.g. old Android phones) and then you're on your own, even if you'd want to keep them up to date. reply vbezhenar 21 hours agorootparentI used to work with state agencies and they run outdated unpatched Windows computers all over the place. Nowadays I work in medical software and hospitals are running outdated unpatched Windows computers everywhere. Nobody cares about updates. Almost nobody. I never saw Windows 11. Windows 10 is popular, but there are plenty of Vistas. I'm outright declining supporting Windows XP and we lost some customers over this issue. My development tools are somewhat outdated, because compilers love to drop old Windows versions and 32-bit architectures, so sometimes I just can't update the compiler. For example I'm stuck with Java 8 for the foreseeable future, because Vista users are too numerous and it's not an option to drop them. Hacker News is like another world. Yes, I update my computer, but everyone else does not. Even my fellow developers often don't care and just use whatever they got. reply 0cf8612b2e1e 22 hours agoparentprevIf only that were possible with some appliances. I can keep my TV offline, but not the Roku. Internet connected utilities which will continually patch themselves into enshitification. reply cpncrunch 19 hours agoprevA reasonable strategy is to wait a week after release before applying an update, unless it's a zero day fix. reply mike741 20 hours agoprevUrgent updates can be necessary every once in a while but should be recognized as technical failures on the part of the developers. Failure can be forgiven, but only so many times. The comments saying \"what about X update that had this feature I need?\" are missing the point entirely. Instead ask yourself about all of the updates you've made without even looking at the patch notes, because there are just too many updates and not enough time. Instead of blaming the producers for creating a blackbox relationship with the consumers, we blame the consumer and blindly tell them to \"just update.\" That's what needs to change. It's a bit similar to opaque ToS issues. reply neontomo 22 hours agoprevthe react module bloat example is not a fair one, the recommended way to start a react project isn't to use create-react-app. other methods are more streamlined. but then again, the deprecation of create-react-app perhaps proves the point that updates create problems. reply eXpl0it3r 22 hours agoparentIt's not anymore the recommended way and last I checked it's not really being maintained as much as other ways, but for quite a while, it was the recommended way. reply neontomo 21 hours agorootparentthat is what i'm saying ;-) reply dzonga 21 hours agoprevjava over golang lol. when golang has literally been version stable for over a decade now reply canpan 19 hours agoparentI think the article is interesting. But you are right, golang is the one language I would recommend to someone who does not like updates. reply ghawk1ns 19 hours agoprevThere is humor in this blog itself has 2 updates reply ricksunny 21 hours agoprevThe 'Skip this Update [pro]' button example (Docker Desktop) just made me facepalm and helped me internalize that I'm not a luddite from technology, I'm a luddite from the collectives of people (not the individual people...(!) ) feeling compelled to craft these dark business patterns. reply k_roy 17 hours agoprev> In my eyes it could be pretty nice to have a framework version that's supported for 10-20 years and is so stable that it can be used with little to no changes for the entire expected lifetime of a system. Yeah, me too. I also would like a few million bucks in the bank. It's naive to think that every project wouldn't want to set this goal, simply because it's so unrealistic. reply kkfx 12 hours agoprevOr use NixOS/Guix Systems instead of living in the stone age of containers... reply notfed 15 minutes agoparentI ran into critical issues with some NixOS packages I needed, the maintainer doesn't seem to care about my use case, and I can't for the life of me understand the Nix language enough to fix it myself. Have stopped using NixOS because of that. If Nix language was replaced with something sensible I'd jump back in excitedly. reply spyspy 22 hours agoprevKinda weird to see Java over Go, when the former is basically an entirely new language from what it was 10 years ago and the latter has made it an explicit goal to never break older versions and (almost) never change the core language. reply karolist 21 hours agoparentWriting backends in Go I do get that warm fuzzy feeling knowing that it will compile and work in ten years. The syntax is easy to read, if I'm not lazy to add extensive tests I can simply read these as documentation to re-familiarise myself later. It's now my go to tool for everything server side. reply ungamedplayer 21 hours agorootparentNow do it for lisp and your libraries alone were last updated 5 years ago. reply CooCooCaCha 21 hours agoprevI disagree, keep things constantly updated (within reason). Most companies I've worked for have the attitude of the author, they treat updates as an evil that they're forced to do occasionally (for whatever reason) and, as a result, their updates are more painful than they need to be. It's a self-fulfilling prophecy. reply exe34 23 hours agoprev> Not only that, but put anything and everything you will ever need within the standard library or one or two large additional libraries. you can definitely do that with python today: assemble a large group of packages that conver a large fraction of what people need to do, and maintain that as the 1 or 2 big packages. nobody's stopping you. reply ungamedplayer 21 hours agoparentYou would need to maintain python itself too. Imagine if you had done this same plan prior to the python 3 transition. reply exe34 21 hours agorootparenthttps://github.com/naftaliharris/tauthon it's already being done! reply msiemens 6 hours agoprev [–] I know it's this is a rather long tangent and not the main point of the article, but regarding \"Docker Swarm over Kubernetes\", I've had a ton of bad experiences at my employer running a production Swarm cluster. Among them: - Docker Swarm and Docker Compose use different parsers for `docker-compose.yaml` files, which may lead to the same file working with Compose but not with Swarm ([1]). - A Docker network only supports up to 128 joined containers (at least when using Swarm). This is due to the default address space for a Docker network using a /24 network (which the documentation only mentions in passing). But, Docker Swarm may not always show error message indicating that it's a network problem. Sometimes services would just stay in \"New\" state forever without any indictation what's wrong (see e.g. [2]). - When looking a up a service name, Docker Swarm will use the IP from the first network (sorted lexically) where the service name exists. In a multi-tenant setup, where a lot of services are connected to an ingress network (i.e. Taefik), this may lead to a service connecting to a container from a different network than expected. The only solution is to always append the network name to the service name (e.g. service.customer-network; see [3]). - Due to some reason I still wasn't able to figure out, the cluster will sometimes just break. The leader loses its connection to the other manager nodes, which in turn do NOT elect a new leader. The only solution is to force-recreate the whole cluster and then redeploy all workloads (see [4]). Sure, our use case is somewhat special (running a cluster used by a lot of tenants), and we were able to find workarounds (some more dirty than others) to most of our issues with Docker Swarm. But what annoys me is that for almost all of the issues we had, there was a GitHub ticket that didn't get any official response for years. And in many cases, the reporters just give up waiting and migrate to K8s out of despair or frustration. Just a few quotes from the linked issues: > We, too, started out with Docker Swarm and quickly saw all our production clusters crashing every few days because of this bug. […] This was well over two years (!) ago. This was when I made the hard decision to migrate to K3s. We never looked back. > We recently entirely gave up on Docker Swarm. Our new cluster runs on Kubernetes, and we've written scripts and templates for ourselves to reduce the network-stack management complexities to a manageable level for us. […] In our opinion, Docker Swarm is not a production-ready containerization environment and never will be. […] Years of waiting and hoping have proved fruitless, and we finally had to go to something reliable (albeit harder to deal with). > IMO, Docker Swarm is just not ready for prime-time as an enterprise-grade cluster/container approach. The fact that it is possible to trivially (through no apparent fault of your own) have your management cluster suddenly go brainless is an outrage. And \"fixing\" the problem by recreating your management cluster is NOT a FIX! It's a forced recreation of your entire enterprise almost from scratch. This should never need to happen. But if you run Docker Swarm long enough, it WILL happen to you. And you WILL plunge into a Hell the scope of which is precisely defined by the size and scope of your containerization empire. In our case, this was half a night in Hell. […] This event was the last straw for us. Moving to Kubernetes. Good luck to you hardy souls staying on Docker Swarm! Sorry, if this seems like like Docker Swarm bashing. K8s has it's own issues, for sure! But at least there is a big community to turn to for help, if things to sideways. [1]: https://github.com/docker/cli/issues/2527 [2]: https://github.com/moby/moby/issues/37338 [3]: https://github.com/docker/compose/issues/8561#issuecomment-1... [4]: https://github.com/moby/moby/issues/34384 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article presents a controversial view against frequent software updates, highlighting the disruptions and resource consumption they cause.",
      "It suggests a more stable approach, using examples like the Lazarus IDE and FreeBSD's stable packages, to reduce the need for constant updates.",
      "The author advocates for slowing down the pace of updates, focusing on long-term stability and non-breaking changes to ease maintenance and reduce developer cognitive load."
    ],
    "commentSummary": [
      "Before the internet, software updates were infrequent and costly, making buggy software a significant issue.",
      "The internet facilitated easier updates, leading companies to push frequent updates and use customers as testers, resulting in less stable software.",
      "There is a debate between prioritizing long-term support (LTS) versions for stability versus the industry trend of frequent updates at the expense of quality."
    ],
    "points": 175,
    "commentCount": 111,
    "retryCount": 0,
    "time": 1721416068
  },
  {
    "id": 41015731,
    "title": "Public toilets are vanishing and that's a civic catastrophe",
    "originLink": "https://psyche.co/ideas/public-toilets-are-vanishing-and-thats-a-civic-catastrophe",
    "originBody": "X CLOSE THERAPEIA EUDAIMONIA POIESIS IDEAS GUIDES FILMS POPULAR ALL TOPICS About Contact Search MENU DONATE NEWSLETTER SIGN IN Public toilets are vanishing and that’s a civic catastrophe Photo by Christopher Anderson/Magnum Guido Corradiis a researcher in psychology based in Madrid, Spain. His research is focused on empirical aesthetics, and he has published in journals such as Psychology of Aesthetics, Creativity, and the Arts and the British Journal of Psychology. Edited by Christian Jarrett SYNDICATE THIS IDEA Save Share Post Email New research shows it is a nightmare for all of us, but especially for people with health issues and marginalised groups The most terrifying moments often unfold in mundane situations. My own taste of horror emerges every time I enter a bar and I’m met with a prolonged silence after asking: ‘Can I use the toilet, please?’ In supposedly affluent Western nations, the dire state or absence of public toilets has become a universal nightmare, impacting the health and quality of life of all of us, but particularly for marginalised groups. What I find most scary is that we seem to be accepting this cruel reality without confronting it. It is like there is a monster in the city that is affecting the life of everyone, but the people who can make a difference don’t even care. I have been aware of the monster since 2013 when I was diagnosed with ulcerative colitis. This is a chronic inflammatory bowel disease that causes long-lasting inflammation and ulcers in the digestive tract and affects life in different ways, such as needing the toilet frequently, pain and fatigue. That’s when I realised the impact a simple toilet can have, not only on an individual’s health but also on public health. This is the story of the shitty life that the poor quality of public toilets confers upon us. Making public bathrooms accessible and appropriate is a matter of human dignity Toilets were one of the biggest steps forward for humanity: they allowed us to create cleaner spaces, reducing death and improving health. By the 19th century, in Western countries, bathrooms with toilets were increasingly included in home design, catering to essential human needs, such as urination, defecation and personal cleaning. With the emergence of new hygiene standards through the 20th century, the focus of bathrooms shifted further towards convenience and cleanliness. In contemporary times, the role of domestic bathrooms has expanded beyond their practical utility to symbolise places of wellbeing, with more affluent homes including features such as hot tubs and toilets with hi-tech and luxury functions. And yet, the vast majority of public restrooms have not yet embraced this aspect of wellbeing. On the contrary, the poor state of them often elicits disgust and repulsion. In severe situations, for some people, these adverse psychological responses can escalate to pathological levels, including incontinence, urinary problems, anxiety, and significant alterations to their normal social life. For many people, most of the time, the state of restrooms is something they think about only when they fail, if they are unusable or unavailable, or there simply aren’t any. Yet toilets often fail when you need them the most. And it’s in such moments you realise that these invisible parts of our cities are fundamental. So, why are restrooms typically tucked away at the back of establishments, hidden both literally and metaphorically? We all must keep in mind that the use of public bathrooms is inevitable, and that making them accessible and appropriate is a matter of human dignity. The toilet problem has two differentiable branches. The first is the global sanitation crisis in developing countries and in extreme situations such as refugee camps, which provokes disease and death. The second branch pertains to the problems in developed countries, causing issues in daily life and negatively impacting quality of life. Fear of public bathrooms will be greater in those most disadvantaged by society Worldwide, the World Bank highlights three critical facts: a quarter of the world’s population lacks access to decent toilets, only 39 per cent of human waste is safely managed worldwide, and the global cost of inadequate sanitation reaches $260 billion annually. These facts emphasise the severe health, economic and environmental impacts of inadequate sanitation. Lack of access to toilets is a silent global burden: people die as disease spreads due to the lack of sanitation. The problem is different when looking to Western, educated, industrialised, rich and democratic (WEIRD) countries. Here we often take adequate sewage disposal for granted. And yet free access to toilets is gradually disappearing. As the journalist Lezlie Lowe documents in No Place to Go (2018), institutions are retiring from ensuring the availability of public toilets. This withdrawal of support for basic sanitary facilities has left many people, especially in urban areas, struggling to find accessible restrooms when needed. For instance, data collected in the United Kingdom in 2018 showed that the number of public toilets had dropped by 39 per cent over the previous two decades. Quite simply, Lowe was right: there is often no place to go. Yet, having access to toilets is essential for everyone’s dignity and to help support everyone’s mental, physical and social health. When women can’t easily use public toilets, it harms their wellbeing and denies their basic rights. The list of problems associated with public restrooms doesn’t end there. For instance, transgender and gender-nonconforming individuals face difficulties accessing restrooms due to transphobia. There is also a noticeable rise in urogenital issues due to holding in the urge to use the restroom at work or due to schools not adapting to the needs of children. Everyone can easily find a reason to fear public bathrooms, although these fears will be greater in those most disadvantaged by society. For those of us with gastrointestinal problems, our minds always reserve a considerable space for public restrooms: we often worry about when we’ll have to use them, if we’ll find one, and if it will meet minimum standards. Dependency reshapes your reality: you are no longer concerned about whether you can park at the beach, but whether there will be a nearby restroom; you are not so worried about being late for the bus, but suddenly having an urgency in a 45-minute-long journey. Life changes when you are dependent on toilets. Note that everyone can find themselves toilet dependent, even if only temporarily – like that week after you ate a meal that upset your stomach. Women reported more negative incidents with public restrooms than men These thoughts drove me to conduct some scientific studies on what people face with public toilets and how it affects their daily lives. Our studies started with interviews with the true experts in public restrooms: patients with bowel diseases (such as Crohn’s disease, ulcerative colitis or irritable bowel syndrome). In our case, these were people from Spain, and with their help we developed some assessment tools to use in surveys. We confirmed what we suspected: people struggle with public toilets because they are dirty, don’t offer any privacy, and lack the basic things needed to use them. The ideal public toilet – the one with the adequate levels of cleanliness, privacy and usability – is hard to find. Another study from our team showed that, for people who are toilet dependent, these negative situations can increase the shame they feel about their illness. This illness-related shame is associated with poorer health outcomes, such as reduced seeking of help or support from others. When we asked people about the struggles they face with public toilets, such as ‘being unable to find an open unoccupied public toilet’ or ‘feeling unsafe or exposed in a public toilet’, we found that the more often they experienced these negative events, the lower their quality of life (even after accounting for any differences in their health or socioeconomic status). What’s more, we found that people from poorer and less educated backgrounds had more of these negative toilet experiences – probably because they usually live further from work and in places with fewer available high-quality toilets. Consistent with previous research, we found that gender also influences (negative) experiences with public restrooms: women reported more negative incidents than men, perhaps due to distinct needs that are so often ignored. Considering the data and experiences, it might seem we’re losing the battle for quality public toilets. There is just cause for anger, which should be channelled into action. Yet, there’s also reason to hope for a future with better toilets that restore our dignity. Some public administrations are taking the availability of quality public restrooms seriously in their facilities. Increasingly, patient and consumer associations are initiating campaigns for universal access to public toilets. For instance, in my country of Spain, there is the ‘I can’t wait!’ campaign in Catalonia, and the report about public bathrooms from the Spanish Federation of Consumers and Users, which called for their greater availability. Recognising this issue and assessing the situation is the initial stride toward a future where everyone can access high-quality restrooms tailored to their individual needs. One thing is clear: we need to talk about public restrooms openly, not in secret. Loudly, not whispering. We need to assert our fundamental right to access a restroom without having to wait until we’re back home. Save Share Post Email HUMAN RIGHTS AND JUSTICE DISABILITY PUBLIC MENTAL HEALTH 18 JULY 2024 TRANSCENDENT EXPERIENCE IDEA To thrive, children need to experience awe – and you can help Alongside love, sleep and play, awe is precious for children. There are small, everyday ways to make it a part of their lives by Artemisia O’bi & Fan Yang VALUES AND BELIEFS IDEA A philosophical approach can help you identify what truly matters We’re often taught to live according to our values, but this is easier said than done without pausing to reflect deeply by Valerie Tiberius MUSIC IDEA Courtly love songs are a window into medieval sex lives Song lyrics acted as a sexual script for aristocrats, revealing some edgy practices that ignited their imaginations by Elizabeth Eva Leach Psyche is a digital magazine from Aeon Media that illuminates the human condition through psychology, philosophy and the arts. Therapeia Eudaimonia Poiesis Popular All topics About Contact Support © Aeon Media Group Ltd. 2012-2024. Privacy Policy Terms of Use RSS Psyche is published by registered charity Aeon Media Group Ltd in association with Aeon America, a 501(c)(3) charity. Back to top",
    "commentLink": "https://news.ycombinator.com/item?id=41015731",
    "commentBody": "Public toilets are vanishing and that's a civic catastrophe (psyche.co)155 points by pseudolus 7 hours agohidepastfavorite214 comments egypturnash 5 hours agoIn the US, we have the Committee To End Pay Toilets In America to thank for this state of affairs. It was an organization started as a joke back in the seventies, when pay toilets were very common in public spaces; it succeeded all too well. Pay toilets are banned across many states but there has been absolutely no work put into replacing them with public, free toilets. Nobody is willing to pay to maintain these, so if you can’t pay the much higher tax of going into a coffee shop and buying a $10 drink, you are largely fucked. Large banks of pay toilets in places like bus stations, where everyone can take a dump for the minuscule cost of a coin, are long gone. IF you look sufficiently affluent, you can slip into a hotel or a department store (assuming your city’s downtown still has those) and take a free dump in some pretty luxurious surroundings. But if you’re at a point where that $10 drink for bathroom access is a big stretch then you’re probably not going to make it through there without a run-in with security. https://en.m.wikipedia.org/wiki/Committee_to_End_Pay_Toilets... reply joshuanapoli 5 hours agoparentThe situation varies from state to state. In Florida, many businesses are required to have public restrooms. So you should be able to do your business without being coerced into buying that $10 drink. reply trogdor 2 hours agorootparentFlorida’s Building Code requires restaurants to have one or more public restrooms. There is no state law that forces restaurants to allow people to use those restrooms. Restaurants can refuse access to non-customers. reply bombcar 57 minutes agorootparentbombcar's guide to determining if you're a nice part of town - how easy it is to get to a restroom nicest places? good signage, no need to even interact with a human, restrooms that look like the Taj Mahal. middling places? restrooms exist, but you might have to ask bad places? restrooms are hidden, maybe if the clerk is nice you can use theirs, or you have to buy something horrible places? restrooms that do exist look like something from a horror movie, they're locked stronger than Fort Knox, and even then you have to buy, beg just to borrow the key reply mcculley 59 minutes agorootparentprevFlorida Administrative Code Chapter 61C-1.004 2.a requires at least one bathroom for \"its employees and guests\". It is not clear to me if that means one is required to consider non-customers as \"guests\". But there is more law than the Florida Building Code. https://www.flrules.org/gateway/readFile.asp?sid=0&tid=27562... reply SoftTalker 1 hour agorootparentprev> Restaurants can refuse access to non-customers. And many do, because of the repeated experiences of non-customers destroying the restrooms. They aren't doing it just to be dicks to people who need to pee but don't want to buy something. reply freedomben 4 hours agoparentprevInteresting thank you, I had never heard of this! It is unfortunate how sometimes the Law of Unintended Consequences bites activists in the ass. You see a worthy cause and want to improve the situation, only to end up making things worse. reply karmakaze 4 hours agorootparentThis might fall in the category of Unconsidered Consequences. One can't merely oppose a thing and not support a replacement and have the fallout be \"oh well, we tried\". It's not like all the pay toilets disappeared all at once leaving no free ones, the group was 'making progress' in their myopic goal all the while. reply 01HNNWZ0MV43FF 1 hour agorootparentIt's rent control but for toilets. Obviously the problem is that toilet landlords got greedy, and it'll be fine when they stop being greedy /s reply bluefirebrand 4 hours agorootparentprevThis is also what I would call \"the road to hell is paved with good intentions\" We need to do a better job scrutinizing cause-effect for things, not just make decisions on a knee jerk \"that sounds well intentioned\" reply d0mine 2 hours agorootparentprev\"unintended\"? -- what is the most obvious thing that can happen if paid toilets are outlawed and there is no money (never is) for public toilets? Obviously, there is more to the story. People are not that dumb. Somebody wanted it that way for a reason. reply hiAndrewQuinn 5 hours agoprevRent-a-Potties, paired with some kind of mobile app, make good business sense to me. Especially if you can get some kind of network effect / critical density mass going in a given area. A person is out and about town, and suddenly has to go, bad. They pull out the app and find their closest Rent-a-Potty. It appears on their map with a green dot - that means the last person to use it considered it reasonably clean, a good sign. (Everyone knows that people whose usage of the Rent-a-Potty is, ahem, strongly correlated with the state transition to an orange or red dot face price hikes, or possibly even expulsion from the app as a whole, so there's good reason to leave it at least as good as you found it.) They tap the dot, hit \"Reserve\", and the timer begins. A 4 digit PIN code appears on the screen, your secret incantation to the hall of the porcelain throne. These things are not cheap - $1 per minute for the first 10 minutes of reservation, $2 past that. But they're emptied every night, the sink always has hot water, and the soap is always refilled. Some people have tried to game the system by {your deviancy here}; the company has responded with {countermeasures you're smart enough to think of here}. Some enterprising souls could theoretically {countercountermeasure}, but honestly, implementing {countercountercountermeasure} isn't a huge concern for Rent-a-Potty's C-suite, compared to the recent competition from new market entrants like Loolurker, AirPnP, and Pouber. reply tzs 4 hours agoparentI've wondered if it would make sense for hotel/motel chains to offer a paid toilet service. Their locations already have a bunch of toilets (one in each guest room plus ones for the lobby and for meeting rooms) and already have cleaning people there to keep those clean. Sell day passes that give you 24 hours access. If I were, say, going to drive from Seattle to Los Angeles I'd pay $20 to buy a pass that would let me pull into any Motel 6 along the way to use a clean bathroom. reply trogdor 2 hours agorootparentSomeone who can afford to pay $20/day for a bathroom pass is probably presentable enough that they can walk in and get permission to use the bathroom for free. reply p51-remorse 2 hours agorootparentAnd if you’re driving, you can always stop at any gas station. Top off your tank if it makes you feel better, but realistically no one there is getting paid enough to cross reference gas buyers with poopers. reply lotsofpulp 3 hours agorootparentprev95%+ of hotels/motels do not have 24 hour cleaning staff, and the guest service agents working the computer are not going to be wanting to clean bathrooms. All those branded 80 to 120 room hotel/motels are trying to reduce the staffing outside of 7AM to 4PM, even to the point of having check in kiosks that video call you to someone in South Asia/Phillipines/South America. reply dawnerd 5 hours agoparentprevThis is already a thing in Europe and it’s awful. Paying a euro or two just to go sucks. reply InsideOutSanta 5 hours agorootparentWhy does it suck? It's a negligible amount of money for the person using the toilet, but the revenue is enough to pay for people to maintain and clean the toilets regularly. Being able to use a clean toilet for what is essentially a rounding error in your budget at the end of the month seems like a worthwhile tradeoff. reply jeroenhd 5 hours agorootparentBack in school when my allowance was 5 euros a week, even 50 cents was prohibitively expensive for me to go to the toilet. Even today I refuse to use the two euro toilets and I'm not the only person walking up, seeing the price, and turning around. This stuff is not priced to be a reasonable convenience, but to be a better alternative to soiling yourself in public. The most insulting part is paying 2 euros to pee into a smelly urinal and getting a 50 cent discount code for the nearby fast food stand as a courtesy, because I haven't spent enough apparently. reply Ma8ee 54 minutes agorootparentI remember as a poor student, having to pay the equivalent of 50 cents felt like robbery. Today with a steady income I’m happy to pay a dollar, or even two, to be able to use a clean restroom. I think the general problem is interesting: how do we price necessities when our ability to pay is so different? reply munch117 5 hours agorootparentprevSpending the money doesn't suck. Having to wrangle the payment system when you're in a hurry, that does suck. reply Ma8ee 57 minutes agorootparentThe last time I went I just had to blip my credit/debit card. Very convenient. (Stockholm Central Station, Sweden). reply Gigachad 5 hours agorootparentprevMy experience in Italy was that the paid toilets were worse maintained than basically every free toilet in Australia. One of the toilets I paid for didn’t even have a seat. reply Freak_NL 1 hour agorootparentThat's just normal in Italy. The seats are often missing. People are used to it. reply danaris 3 hours agorootparentprev> It's a negligible amount of money for the person using the toilet Spoken with the voice of privilege. reply amy-petrik-214 4 hours agorootparentprevIt sucks because America at one point was like \"bro, cut it out with all the pay toilets\" and suddenly our culture exploded into \"peepee and poopoo is a human right\". It sucks because monetizing bowel movements, pun intended, is a shitty thing to do. The common man got a win that has stuck for decades, and here, a proposition to renege on that contract, for shame. I would draw a parallel to private roads versus public roads. We take public roads for granted. Or, the obvious argument regarding sanitation i.e. you can have a city with free garbage cans everywhere (in the US), or you can have a city with garbage piles everywhere (Japan being a cultural counterpoint here) That all was in the 1970s so a lot of us don't know and take public toilets for granted since we never knew otherwise. Some MIT // Harvard math Phd genius guy apparently started it all when he was like 11 years old and pooped his pants at a math convention, and whereas I view an elite mathematician as required to be a bit goofy and quirky, having ended pay toilets in the US is a wonderful quirky fun fact: https://en.wikipedia.org/wiki/Ira_Gessel Ira Gessel won the Tony P. Robbins prize for his inverted lattice manifold proof pertaining to coprime powers of 7, and the Taco Bell reward for ameliorating the sufferings of people who ate too much taco bell. reply briHass 5 hours agorootparentprevIf that got me a reasonably clean and reasonably available location, that's money well spent. Cleaning and maintaining a restroom isn't free, and something needs to pay for it. Highway rest stops are usually pretty clean, but they are paid for with tolls/very high gas taxes. reply saturn8601 3 hours agorootparentThis is anecdotal but in my experience the restrooms still suck and are poorly maintained (This was my experience in Hamburg and Amsterdam in 2023). What is even worse is that there seems to be a little recourse to correct it in the short term. There isn't a \"Free market\" of toilet companies because the same firm seems to capture all the toilets in a given area. So you have the worst of all worlds, you have to pay to use a poorly maintained restroom and there is little you can do about it...at least as a traveler not familiar with the local customs. Maybe locals can enact some improvement. reply KineticLensman 59 minutes agorootparentprevHence the UK euphemism for going to the loo, “to spend a penny” [edit] also the graffiti: here I sit, broken hearted, paid 10p and only farted reply hiAndrewQuinn 5 hours agorootparentprevI'm in Europe, and I think it's great. :) reply spencerchubb 5 hours agorootparentprevBetter than not being able to go. That's the beauty of capitalism reply lazide 5 hours agorootparentprevAlternate take - the toilets in Europe, while a euro or two, are actually nice and relatively well maintained. And relatively common. So while you do have to keep a few coins on you, unlike cities like NYC or SF you can actually go to the toilet when you want and it isn’t a stress inducing nightmare wondering what sort of hell you are going to run across if you do actually find a toilet. Places with ‘free water’ almost always have terrible water availability and quality. Same problem. reply dawnerd 5 hours agorootparentThe ones I used in Paris recently were no better, one worse, than any other free public toilet. IMO tourist taxes should be the thing paying for it. reply bombcar 54 minutes agoparentprevApple Maps already shows restrooms available publicly - search \"restroom\" and you'll see. But it finds things like mall bathrooms and park bathrooms but not gas station bathrooms, at least the first time I tried it. The real problem is when you're in an unfamiliar area and don't know the local secrets about where restrooms can be found. reply 01HNNWZ0MV43FF 1 hour agoparentprevPlease no more fucking apps. If I have to tell someone my legal name to shit I'm gonna shoot myself Just take quarters and give poor people some cash so they can shit too. Please please please reply lwhi 5 hours agoparentprevWhere do you go if you don't have any money? reply kibwen 5 hours agorootparentThe answer to this question should be self-evident. A city either offers free public toilets, or it becomes a free public toilet. reply seanmcdirmid 5 hours agorootparentIt’s also a tragedy of the commons thing: you offer free toilets but don’t fund maintaining them every half hour so they are only usable by a few before they become completely thrashed and unusable. Or they are used as drug/prostitution dens and are never available anyways except for whoever got lucky to grab and squat them. A free resources that isn’t available isn’t very useful when you need to go. King county just pulled the plug on a pilot for public restrooms at a couple of transit stations because with maintenance and, more significantly, security the cost was $77/use [1], we simply can’t afford that. Seattle famously bought five multi million dollar self cleaning toilets that only lasted a few years [2]. [1] https://www.theurbanist.org/2024/07/17/king-county-to-close-... [2] https://portlandloo.com/seattles-failed-experiment-with-sub-... reply lwhi 5 hours agorootparentprevA quid pro poo situation, if you will ... reply freedomben 4 hours agorootparentI like it, but quo and poo are a bit too different to work. Maybe \"Quid Poo Quo\"? reply lwhi 2 hours agorootparentYes .. better! reply medymed 5 hours agorootparentprevThis was the argument that let NYC to ban coin based toilets in decades past, with the assumption that people should not need to pay and businesses/govt should provide restrooms for free. But no replacement was ever provided. Come to NYC, walk around busy areas for a while you will not infrequently see less fortunate people urinating in the subway stations, on the sidewalk, on walls and buildings, by trash cans, and sometimes (if the mood strikes them) right in the middle of the street. reply lwhi 5 hours agorootparentPeople are always going to need to do a number two though. Although it pains me to sat this .. maybe there's a need for a disposable potty, with sanatary wipes, that can go in the trash. Maybe these should be given free to street sleepers? reply hypeatei 5 hours agorootparentprevAnd you don't even have to be poor, what if you just aren't carrying your wallet/phone? reply Zambyte 5 hours agorootparentMy wallet feels as essential as my keys when leaving my home. Are you just making the point for people who forget their wallet, or do you think there is good reason to intentionally not carry your wallet? reply hypeatei 5 hours agorootparentNo, I just think we should resist the notion we need to carry things on us at all times to exist as a human being. reply ghaff 5 hours agorootparentI can exist without carrying things. Practically, if I'm walking around a city, I probably want to have ID and some way of paying for things (and to get back into my home which is presumably locked). If I just take a stroll on the river path from my house I often don't carry anything but mostly I do. reply assimpleaspossi 4 hours agorootparentprevHow would you pay if you only have a twenty? Do they take credit cards? reply wyatt_dolores 3 hours agorootparentMany do take credit cards. In The Netherlands and Belgium, the pay toilets in train stations allow tap to pay with a NFC card or your phone. Toilets in the airports are always free, though. reply Zambyte 3 hours agorootparentprevI think you meant to reply to a different comment. reply ghaff 5 hours agorootparentprevSome people these days just carry a couple cards with their phone but that's still functionally a wallet. And a lot of things are headed towards contactless payment with phones or smart watches though I'd have trouble wanting to depend on that. reply astura 5 hours agorootparentprevI don't ever carry my wallet or keys when I go for a walk, which is usually once a week when it's nice out. I do carry my phone, for podcasts. My door has a keypad to get in. reply nanomonkey 2 hours agorootparentprevIt's not uncommon to give subsidies for folks that can't afford modern conveniences (free phones, food, housing). I could imagine one could give out tokens, or pay cards that are only usable for bathrooms to those who can't afford to pay for it themselves. Not to mention a lot of us \"pay it forward\". I used to pay for the people behind me when crossing the Bay Bridge, or for the next person that was buying a coffee. reply seanmcdirmid 5 hours agorootparentprevSome place in Switzerland offered two classes of bathroom. Paid clean ones that are maintained and a very simple free one with squats, not maintained hourly and only sprayed down a few times a day. Still much cleaner than I expected. reply hypeatei 5 hours agoparentprevI think this model could benefit from dynamic pricing - higher demand equals higher prices. reply SoftTalker 53 minutes agorootparentCombine it with AI video recognition. If you walk up calmly its $1, if you're dancing around and crossing your legs it's $5. reply didgeoridoo 5 hours agorootparentprevI think we should charge by the pound. Not because it makes any actual sense, but because it would be hilarious. reply screamingninja 5 hours agorootparentprevOr perhaps higher demand can be a signal to increase supply instead of price gouging. Why incentivize anyone to limit supply? reply hypeatei 5 hours agorootparentSorry, my comment was a joke, I wasn't being serious. reply Ekaros 5 hours agoparentprevI was thinking, Uber of toilets. You can order mobile toilet driven to you and then you are charged reasonable rate for distance and travel time it did. Plus ofc, standard fee. And time you spend inside. reply underseacables 3 hours agoparentprevIt appears to be called Flush! https://techcrunch.com/2024/01/05/this-app-lets-cafes-and-co.... reply citizenkeen 5 hours agoparentprevThis is so disgusting. The idea of milking maximum profit from such a basic human function really highlights the way extreme capitalism trends toward evil. reply freedomben 4 hours agorootparentI agree very much in principle, but pragamatically (i.e. in the real world) this strikes me as utopian and impossible, at least until human nature changes. Toilets are a scarce resource (economically speaking), and thus there must be some rationing method in place. If it's not monetary, it will be something else. In a perfect world there would be plentiful bathrooms and everyone would diligently clean up after themselves and take the trash out when full, so operating costs would be minimal (power and water bill and occassional maintenance). In the real world though, people don't do that, meaning you have to hire people to clean. Some people will also vandalize, which gets expensive in a hurry. Public (government-run) restrooms tend to be even worse because they aren't actively monitored, and for whatever reason people like to trash them. reply 01HNNWZ0MV43FF 1 hour agorootparentprevThe idea that giving up all your privacy isn't worse than spending a dollar really highlights something reply jtsiskin 58 minutes agorootparentprevWait until you hear about the cost of food, shelter, and healthcare… reply Arnt 5 hours agorootparentprevI know sometime who ran a gas station. They had to clean their toilet after every fifth customer on average. That's a real cost. Explain what's evil, please. reply baggy_trough 2 hours agorootparentEvil is apparently when people who don't know what it costs to provide a service think they should get it for free. reply 38 5 hours agoparentprevDisgusting. What about people that can't afford it? Should they just shit on the sidewalk? reply lobochrome 5 hours agoparentprevWe all pee and poop. Every. Single. Human. There is no place for “business”. It’s the literal definition of a public good and such the role of the government. reply RobinL 5 hours agorootparentA public good is a commodity or service that is non-excludable (no one can be effectively excluded from using it) and non-rivalrous (one person's use of the good does not reduce its availability to others) Examples include clean air, national defense, and public parks. This isn't to say they shouldn't be provided by the government, but they're definitely not a public good reply CrazyStat 5 hours agorootparentprevWe all eat too. Is there no place for “business” in eating? Should all restaurants be nationalized? reply citizenkeen 5 hours agorootparentNo, but we try to guarantee that you have food. reply didgeoridoo 5 hours agorootparentThere’s a substantial difference between governments trying to provide a service (which they are generally terrible at) and paying for a service on behalf of those who cannot afford it (which they tend to be reasonably competent at). EBT is a pretty good program. Government grocery stores would almost certainly be an abomination (just ask your local public school cafeteria or military base mess hall). Edit: adgjlsfhk1 is correct that “generally terrible” is overstated and simplistic. Hopefully better take downthread. reply adgjlsfhk1 5 hours agorootparentthe idea that the government is inherently bad at providing services is just very silly. roads, postal service, and national parks are all really great services that the government provides. the idea that the government only does things well by paying private companies to do them just doesn't have basis in reality. reply didgeoridoo 4 hours agorootparentI take back “generally terrible”. How about “often spotty and rarely held accountable in practice”? It’s not that governments literally cannot provide good services — it’s that when government appoints itself as a monopolist, outcomes and accountability become effectively disconnected, unless the service is so high-stakes and the quality is so abysmal that it rises to “vote the bastards out” territory. reply freedomben 4 hours agorootparentprevAgreed that the government isn't inherently bad at providing services, but I do think it's important to define how we measure \"bad.\" For example, does \"bad\" mean the UX is bad? If so, then government isn't inherently bad at it (though can be, see next paragraph). If we define \"bad\" as \"less economical (i.e. more expensive/wasteful of resources) then it does have a general tendency toward bad. It largely comes down to who is running the thing, and do they care? With private sector, they are forced to care because otherwise it will affect revenue and brand value, which will get the leader fired. The consumer is empowered with ability to spend their $ elsewhere, which rewards the better service and punishes the worse (note that this is becoming much less true in the age of giant corps, especially big tech). With government services, (particularly in monopolistic situations like the DMV), the consumer is largely powerless. They have no choice but to use the system given, and if they aren't happy they can complain but that complaint won't have any teeth (unless they happen to be politically connected of course). tldr: I most agree, but it depends on how you define \"bad\" reply mistersquid 5 hours agorootparentprev> Government grocery stores would almost certainly be an abomination (just ask your local public school cafeteria or military base mess hall). Military bases run PXs and commissaries and these stores have reasonable (mid to high mid) quality goods. reply freedomben 4 hours agorootparent> Military bases run PXs and commissaries and these stores have reasonable (mid to high mid) quality goods. The PXs are pretty good but are priced comparably or higher than free-market stores off-base, and even still are subsidized by base funds. The Class Six is very well done and potentially profitable, though that's just a guess. The commissaries are much more expensive than off-base stores and likewise are subsidized by base funds. For the most part only the officer's and senior NCO's families can afford to shop at the Commissary. Disclaimer: I left active duty in the late 00s so things might have changed since then reply brnt 4 hours agorootparentprev> (which they are generally terrible at) They are only as good as the people in a jurisdiction let them be. Also, the governments you have experienced are far from generally representative. reply haliskerbas 5 hours agorootparentprevDeleted reply didgeoridoo 5 hours agorootparentThis rant is in such poor faith. Flagged. reply kevinmchugh 5 hours agorootparentprevI'm not very well traveled, but I think I've used a pay toilet in every country I've been to other than the US (and the little bit of Canada I've seen) They're illegal in many US cities (https://en.m.wikipedia.org/wiki/Committee_to_End_Pay_Toilets...), and American taxpayers don't like footing the bill. So we end up with Starbucks and Dunkin as de facto pay toilets, where you have to buy a pastry or drink to take a pee. I don't really understand why it should be illegal to pay someone for a vital human need reply didgeoridoo 5 hours agorootparentClassic first order thinking. Forcing someone to pay directly to access a toilet is yucky. Voting against pay toilets makes you a good person. Second order effects? Please. Those aren’t on the ballot, and are hard to stuff into a sound bite. reply baggy_trough 2 hours agorootparent> Voting against pay toilets makes you a good person. Voting against pay toilets makes you a foolish and counterproductive person that thinks you're a good person. reply MOARDONGZPLZ 5 hours agorootparentprevYes, completely agree here. But that’s not mutually exclusive with the rent a potty idea. Public bathrooms in parks and such are often, by necessity, quite spartan. They have no toilet stall doors, don’t have the soap filled as often as necessary, have those weird metal mirrors rather than more useful mirrors. I would honestly pay for rent a potty even if public toilets were (and they should be!) available and convenient. reply didgeoridoo 5 hours agorootparentprevIs there no room for grocery stores and restaurants too, since all humans need food? If you’re looking for a role for government here, I could see budgeting in some tax breaks for local businesses that make their restrooms publicly accessible. The infrastructure is already there, no need for a complicated business model. reply trogdor 2 hours agorootparent> I could see budgeting in some tax breaks for local businesses that make their restrooms publicly accessible. People significantly underestimate the cost of maintaining clean public restrooms in urban areas. When restrooms are used properly, maintenance costs are reasonable and predictable. But that is not what happens. Restrooms contain private areas, and that reality invites other uses, like drug use, prostitution, and sleeping. And those uses come at a steep cost. Separately, in my experience, many businesses do make their restrooms available to people who look presentable and who ask politely. They just don’t advertise it, and they may even advertise the opposite. reply smcleod 5 hours agorootparentprevThis. Absolutely spot on. The profitisation common services is absolutely disgusting. It is literally the reason we have governments to build and make available for the people. reply abbadadda 6 hours agoprevSurely this is an erosion in public trust, but if you’ve ever been in a busy public toilet that smells horrendous, you know from experience that maintaining these public toilets is no easy feat. At the same time, I agree with the thrust of the article that they are a civic good & necessity. I live in the UK. I use the “Radar Key Scheme” for both my son and myself. The Radar Key opens handicapped toilets around the country that are otherwise locked. I asked my GP about a referral for this and they knew nothing about it. In the end I just bought one online. Bizarre. I have a disability that’s not visible, I’m otherwise healthy and relatively privileged, but being unable to find a toilet to use when my bladder feels like it is going to explode makes me feel like a wild animal - unable to fulfill the most base need in a culturally acceptable way - terrible feeling. reply koolba 5 hours agoparent> The Radar Key opens handicapped toilets around the country that are otherwise locked. I’ve never heard of this type of thing in the USA. Handicap stalls, if available, are just next to the rest of the stalls in any given location. They’re usually a bit bigger to accommodate a wheel chair, but I don’t think I’ve ever seen a special entry mechanism. Is the UK version a physical key or some RF tag? reply tialaramex 5 hours agorootparentIt's just a physical key. Like the keys for operating train equipment or fire safety they're not intended to prevent malicious access, they're a social signal - this is for people who need it. You're not clever for figuring out how to abuse it, you're just a bad person. reply throwaway3306a 5 hours agorootparentThe bad person is the one who decided to lock and downsize public toilets. All the arguments about taxes are about money for public good that market can't solve, well where is it? reply seanmcdirmid 5 hours agorootparentprevA lot of newer toilets in the USA are single occupancy rooms for families and disabled. Of course, this also makes them easier to use for drugs and camping, so locking them down in urban areas is pretty important. reply _kb 5 hours agorootparentprevIt's a physical lock-and-key key: https://shop.disabilityrightsuk.org/products/radar-key. Living in a country where it's the norm, the concept of (any) locked bathrooms is wild. When travelling I always always find restricted access (either by key, or payment) public facilities extremely jarring. reply mrtksn 6 hours agoprevI don't know how serious this article is. This is because even in Europe there's a huge disparity between countries and I don't think it can be generalised. In London, it's socially acceptable to use the toilet of any venue like a pub. In Paris they have high tech public toilets that automatically clean themselves, in Turkey mosques usually operate public toilets for profit alongside with the facilities provided by the government, in Germany they have Sanifair which gives you voucher for your payment and you can use at any participating location to purchase stuff like coffee. reply EVa5I7bHFq9mnYK 9 minutes agoparentFinland is the undisputed champion in this respect (as in many other quality of life respects), never had a problem to find a free nice public toilet there. Norway was the worst, not only the prices were outrageous, but only Norwegian issued card was accepted for payment. Netherlands has an inventive solution for males, but its smells not so nice. reply grahar64 6 hours agoparentprevIn London places won't let you use their bathrooms unless you buy something. The UK was by far the worst place I have visited for lack of public toilets. They also don't have many/any public fountains, which is insane around kids playgrounds that have no public facilities within walking distance. reply wirthjason 5 hours agorootparentWill they really not let you use the bathroom? The same idea exists in the US, and often bathrooms have a PIN code to enter or a special key only the shop owner has. I’ve never been turned down when asking politely. Most people are rather sympathetic when someone needs to use the bathroom for legitimate purposes. reply dreamcompiler 5 hours agorootparentIn the US it's because the homeless population in most cities is rising, and they have few places to use the toilet. Business owners perceive the presence of homeless people as bad for business so they don't want to encourage homeless people to stick around. And they think homeless people will trash their bathrooms or use drugs inside which is dangerous for other customers and brings police presence. So business owners lock up their bathrooms. Result: Even fewer places for homeless people to use the toilet. reply Aurornis 5 hours agorootparent> Business owners perceive the presence of homeless people as bad for business so they don't want to encourage homeless people to stick around. And they think homeless people will trash their bathrooms or use drugs inside which is dangerous for other customers and brings police presence. These things aren’t just perceptions or thoughts. It’s a real problem that anyone who runs a business in a location with a homeless problem can attest to. reply wirthjason 3 hours agorootparentYou’re totally right. And it’s important to distinguish between the many forms of homelessness. There’s visible homeless, they are what most people think of. There are also invisible homeless, they don’t “look” homeless. Often you don’t know they are homeless except for carrying around some belongings and brushing their teeth in public places. Businesses care about the first type because of the associated problems and are completely fine with the later. reply haliskerbas 5 hours agorootparentprevI’VE never been turned down. But I’ve seen others get turned down who don’t look like me ;) reply mrtksn 6 hours agorootparentprevI guess that's true in touristy places but in general there's nothing stopping anybody from using the WC, just walk in. Would be nice to buy something but asking politely will do too, I've never been turned down. In Turkey they really like putting formal structure to that kind of stuff and venues at the high street sometimes have a lock with a password which is printed on the receipt with your purchase. One particular McDonald's was a pioneer in this, as their location was at the most crowded and central place where people used to meet before the proliferation of cellphones. reply zarzavat 5 hours agorootparentprevIt depends greatly on the type of establishment. If it’s a small family business, absolutely you have to buy something. If it’s a chain then you probably don’t have to buy anything. If it’s a large establishment such as a supermarket then you don’t have to buy anything, nobody is keeping track. And obviously toilets in public places like train stations are free for all nowadays. reply omnicognate 5 hours agorootparentprevThey're not required to let you, but I just ask politely if I can use their loo and have never been refused. There has indeed been a steady decline in the number of public toilets, though. reply dukeyukey 5 hours agorootparentprevIt's not like they have keycards on the toilets (least not the vast, vast majority). You can just walk in the use them. Wetherspoons reliably have clean bathrooms and there are a lot of them. reply switch007 5 hours agorootparentprevPubs are different, especially the bigger chains. They're generally too busy to police it. Big posh hotels can be good too - ask the lobby for the bar, then ask the bar staff for the toilet if it's not obvious. A 8-table local run cafe? I wouldn't dare ask reply jmkni 6 hours agoparentprevAnecdotal, but I got an uber in London recently (quite near the center) and the driver was complaining the whole trip about the fact that he needed to take a piss, and that there was a severe lack of public toilets. reply mrtksn 5 hours agorootparentI see how this can be a problem. It's probably not O.K. to offload the Taxi industry toilet costs to 3rd party businesses. reply nytesky 5 hours agorootparentSo before Uber did taxis return to dispatch center for such things? With their own version of Danny DeVito? reply mrtksn 5 hours agorootparentI have no idea, probably it varies by location. If no paid public toilets are available and dispatch centres with toilets are not a thing they can always strike a deal with local businesses I guess. Also, they need to eat and the places they eat are probably also the places they use the WC. reply bell-cot 5 hours agorootparentprevA decent and competent version of Uber would be meeting such needs. As things are - other companies can make national headlines by caring about human needs of drivers: https://www.today.com/food/news/chick-fil-a-break-room-rcna7... reply watwut 5 hours agoparentprevThat German voucher thing system sux. You cant just buy something cheap with voucher, there is minimum buy attached to it. So, you pay 1e for toilet and if you want to use the voucher, you have to pay 5e minimum - usually in overpriced shop you want nothing from. The end result is that woods next to stops smell horribly and you have to be super careful where to step. Because, too many people stop there to piss and shit. reply burlesona 4 hours agoprevSo the Author is based in Spain and writing about life in Spain. My experiences traveling in Europe have been that bathrooms are often hard to find there … but, as a tourist, I’ve definitely not seen enough of normal daily life to know. Meanwhile many comments here seem to be reflexively decrying the situation in the US. That makes no sense to me. Every business has to have a bathroom, and in the overwhelming majority of the country these bathrooms are not locked and nobody minds if you come inside and use one. There are two exceptions, that most of us aren’t visiting very often: (1) some of our city centers where there are concentrated disorder and mental health problems (2) the most crowded tourist destinations These places tend to keep bathrooms locked, but you can usually just ask and get the key or code. Many have signs saying you need to make a purchase but few try to enforce that. These exceptions are indeed annoying, but I believe the solution is better mental health care and generally more effective community policing so that businesses in those locations could follow the norms of the rest of the country. reply hnpolicestate 36 minutes agoprevI noticed something related in my borough (not Manhattan), over the past decade there are far fewer trash cans. In front of places like Dunkin, Starbucks etc. These are the small things that add up to make people irritated. Having to shlep a block to throw out a few dirty napkins sitting in my car. reply sharkrice 5 hours agoprevIn the past, public restrooms in China also required a fee, ranging from five to two yuan per visit. It wasn't until 1998 when a rural youth named Ge Rui was charged three jiao at a restroom in Zhengzhou Railway Station. To seek justice for this three-jiao fee, Ge Rui personally paid 50 yuan for the lawsuit, taking Zhengzhou Railway Bureau to court. The case went through two trials, the details of which we will skip. In short, Ge Rui won the lawsuit in 2001. A month later, the National Development and Reform Commission, citing \"strong complaints from passengers,\" mandated that all public transportation venues must provide free access to restrooms. To this day, most public restrooms in China do not charge fees, and most government agencies, institutions, companies, shopping malls, hotels, office buildings, etc., allow tourists to use their restrooms for free, which is seen as a symbol of civilization. Presently, public restrooms in China are generally well-maintained, and since China has strict drug laws, there are no issues with drug addicts, marijuana odors, needles, or syringes. reply seanmcdirmid 4 hours agoparentYour prices are off. Not 5 to 2 yuan in the 90s, more like 5 to 2 jiao. I remember 5 jiao bathroom places being common into the late 00s in Beijing (and literally why I would carry a few 5 jiao notes whenever I went out, along with tissue). There is no judicial independence or precedent in China, and localities typically set their own policies and standards. Even mandates are often just unfunded wishful thinking, you can still find pay toilets in China today especially in tourist places. Starbucks would always have a sit down toilet in Beijing malls, which was useful if the alternative was a squat toilet. But availability is nice, so that sit down toilet wasn’t very available and the squat toilet in the mall was, so whatever. The old guys smoking in the bathroom was a pretty big problem. 20 years ago at least, even in the squats. I feel sorry for girls, though, a fancy mall in xidan would always have a huge line for the women’s restroom that made my bladder cringe, well past 2015. reply sharkrice 2 hours agorootparentIndeed, you are correct in mentioning the range from five jiao to two yuan. During my travels, I have often encountered overcrowded restrooms, especially in scenic areas and popular highway rest stops. The issue of long queues for women's restrooms is also significant, along with the presence of cigarette smoke. However, paid restrooms are indeed rare nowadays, at least in Zhuhai, which may be due to its status as a tourist city; the public restrooms there are well-maintained. When I can't find a public restroom, I often use the facilities in banks or government offices. As you mentioned, while local judicial rulings may not apply nationwide, central administrative directives do have a broad impact. In Guangdong Province, I rarely encounter obstacles when using restrooms in government departments, except during the pandemic. reply seanmcdirmid 8 minutes agorootparentChina is big and its bathrooms are diverse. The worst bathroom I encountered was 5000 meters up in Sichuan, it was a shed on the road over a hole, and two very thin slippery wooden boards to squat on (yes, I was scared to death of falling in). Western China is always going to be different, however. The government has a limited attention span, so they'll start a 5 year plan to improve public restroom access, but then it is forgotten a year later. Things do gradually improve, but usually via the private sector (more places to wash up at restaurants and such). If you are living a normal life in a tier 1-3 city, you've already made all your adjustments and know what to expect. I'm less confident in less developed areas, and strike it as a win when I find something clean and sit down. reply SoftTalker 44 minutes agoparentprevWhat does China do with drug addicts? Do they just not have any because they so effectively prevent drug trafficking? reply adrian_b 5 hours agoprevI have been recently in a trip through Japan and I have been very impressed by the abundance and the quality of the public toilets. On the other hand, in Japan there are extremely few places with public trash cans. Those few that exist are typically associated with vending machines for beverages. reply Narishma 5 hours agoparent> On the other hand, in Japan there are extremely few places with public trash cans. Those few that exist are typically associated with vending machines for beverages. IIRC they removed them after a terrorist attack in the 90s. reply CamelCaseName 5 hours agorootparentBut... Why? reply tkgally 4 hours agorootparentThe nerve gas attack on the Tokyo subway in 1995 [1] was quite traumatic for those of us living in Japan then. There was a very real fear of follow-up attacks. Security was tight for a long time in public places, and facilities like trash cans and lockers that could be used to plant bombs or poisons were removed. It took years for the tensions to ease, and it seems that even today the people in charge of public places hesitate to reinstate trash cans. [1] https://en.wikipedia.org/wiki/Tokyo_subway_sarin_attack reply freeone3000 4 hours agorootparentprevBecause the bomb was placed in a trash can. It’s like having to remove your shoes to board an airplane in the US: firmly securing the exact door the horse has bolted through, and not paying any attention to other holes in the wall. reply iamflimflam1 2 hours agorootparentprevBecause it’s very easy to put a bomb in a trash can and maim a lot of people. reply mitthrowaway2 2 hours agoparentprevThe ones next to vending machines are for recycling of beverage containers, not for general garbage. reply djmips 1 hour agoprevIn Canada where I live, public parks, which there are fairly numerous have public restrooms, however they started closing them after dark and in the city itself many of the places you used to be able to go without a problem like stores have now permanently closed their public washrooms or at least have a code / key you now need to get. Nevertheless, it feels like it's still much much better than places mentioned in the other comments. reply tambourine_man 5 hours agoprevI highly recommend this movie from Wim Wenders, Perfect Days, about a Tokyo public bathroom worker: https://m.youtube.com/watch?v=QzZBbX5A1FA I know the synopsis doesn’t do it justice, but really, go watch it. One the best movies I’ve seen in a while. reply wirthjason 5 hours agoprevWhen I was in high school (ages ago) the boy’s bathrooms didn’t have doors. I guess it was to deter kids smoking, graffiti, and other bad things that happen. We were a middle class suburb so I don’t know how often this happened anyways. I was fortunate to never need to go but I always remember it. reply lifestyleguru 5 hours agoparentNeither had toiler paper and toilet seats. Instead of weeding out bad kids who smoked and vandalized, all kids were punished. Negatively impacted nutrition and hydration habits during the following decades of my life. reply ajwin 5 hours agoprevI live in South Australia and I feel like public toilets are ubiquitous and still increasing in number. Mostly they are either accessible or have a separate accessible toilet. The only exception to this is in areas where they have public social issues. My mum is a little bit financial about toilets so she always struggles when overseas. We even have small self cleaning toilets at the small playgrounds in the suburbs. It would be interesting to understand why we ended up like this. reply dachworker 6 hours agoprevThis is one of my biggest frustrations with living in Germany. And btw, so much for \"walkable cities\". Especially for the elderly who get the need more frequently, but even I ended up in situations where I am an hour away from home and needed to pee badly. Pretty much every mildly secluded corner in the city smells like piss as a result of this ... You know for a country who thinks of itself as more socially minded, and not as hardcore capitalist, they do nickle-dime you for everything. Public latrines are the most basic of public goods. reply crngefest 6 hours agoparentIt differs city by city, in some you have better facilities. IMO it’s more about drugs and sex than capitalism but yea I agree the state of public toilet availability is a disgrace in Germany reply snowpid 5 hours agorootparentBerlin has a program building public toilets. Though not so many, it is a contradiction to the original post as the public toilets increase. reply crngefest 3 hours agorootparentIt might have. But where I live in Berlin there is exactly one public toilet for the whole district. And it’s a pissoir only so if you are female or need no.2 you are screwed. Might be different in another neighbourhood here. reply adityapatadia 6 hours agoprevI faced when I visited New York. I had to search Google Maps and it showed a public toilet in Rockefeller Center. I walked a few miles to reach but it was a real pain. reply ghaff 6 hours agoparentMeanwhile you could probably have dropped into a Starbucks, bought something small, and used their bathroom. There have never been a lot of explicitly public bathrooms in cities like NYC. reply ekianjo 5 hours agorootparentsomething small costs 5 dollars in starbucks. thats some expensive pee reply creato 1 hour agorootparentYou’re also getting the thing you bought too. Maybe it’s more expensive than you would have paid for it otherwise but that difference is the cost of the pee, not $5 reply koolba 5 hours agorootparentprev$5 coffee is cheaper than most pairs of pants. reply kjkjadksj 2 hours agorootparentWho is pissing their pants instead of just pissing outside someplace? reply SoftTalker 41 minutes agorootparentOne is just embarrasing, the other might get you on a sex offender registry. reply kjkjadksj 7 minutes agorootparentNo one is pissing themselves over this. They just go somewhere discrete like behind a dumpster where there aren't any karens and piss. Even women do this if sufficiently desperate, the old squat and shake. chhenning 6 hours agoparentprevThe public toilet in NYC is called Starbucks. It doesn’t get easier than that. reply ckemere 5 hours agorootparentHaving just spent the year in Manhattan, my experience was that most (every?) Starbucks there no longer have public bathrooms even with purchase. I was surprised to discover that the only free public bathrooms were at Sweet Green. It changed my perspective of them as a company- it’s a real commitment when the trend is going in the other direction and I want to support them financially as a result. reply adityapatadia 4 hours agorootparentYes, I figured out the Starbucks (or any other fast food chain) thing after staying there for a while. It's however really a sad scene where people deny visiting washrooms especially when you need it the most. Plus it paints a wrong picture of a country because it's technically people asking you a few bucks for a basic necessity and the government has failed. This is in stark contrast with developed countries like Singapore (where I visit a lot). Singapore makes sure a public toilet is available within a few hundred meters and boy are those clean! reply raverbashing 5 hours agorootparentprevAnd McD. And Shake Shack. And others. (Yes you might need to buy something but seems like a small effort for it) reply ckemere 5 hours agorootparentI can attest that Shake Shack have purchase-only bathrooms. And even after a purchase they seemed annoyed… reply triyambakam 6 hours agoprevI have a urinal bottle, collapsible toilet, and bottle bidet in my van. I never fear needing to urinate or defecate wherever I am. reply saulpw 18 minutes agoparentDoes that van fit in your pocket or do you just never walk anywhere? reply kjkjadksj 2 hours agoparentprevWith your rig I’d fear it every time reply wirthjason 5 hours agoprevThe article has one good point: there’s diseases and conditions that cause people to need the bathroom, and even if you don’t have this upset stomachs are common enough that at some point it’ll impact you. Parents with young kids realize it too. So this is everyone’s problem. However the article misses the point with statements like this. Quite simply, Lowe was right: there is often no place to go. There are often many places to go, it just might not be a public toilet. There’s restaurants and coffee shop, gas stations, stores. Don’t confuse no public places to go with no places to go. Public toilets are one of those things where all it takes is one person to mess up a good situation for everyone else. There’s no incentive for people to treat them nicely. There’s an asymmetry, people want to use public toilets but who wants to clean public toilets? It’s always “someone else job.” I see no problem having a little friction as a way to help control it. A small charge to use the bathroom or social capital of asking can be enough to remedy the problems of misuse. reply MOARDONGZPLZ 5 hours agoparentSome people literally don’t have money for a small charge and they shouldn’t be denied using a public bathroom because of that. You’re right about everything else though. reply CamelCaseName 5 hours agoprevThe government should simply mandate that all restaurants, supermarkets, event venues, etc. above a certain size should be required to have at least a certain number of free public toilets. Otherwise, the more stores that withdraw from offering free public toilets, the more burden it places on everyone else. If it causes a slight increase in labor cost, at least it the cost will be borne equally by all, and benefit will be borne most by the most vulnerable. reply burlesona 4 hours agoparentThe bathrooms are already mandated by the government, they just aren’t required to be free. But in the overhwhelming majority of cases they are free. reply ikekkdcjkfke 6 hours agoprevThere needs to be more innovation in turd extraction reply ethagnawl 6 hours agoprevI just read this article about vanishing bathrooms at Starbucks and other fast food restaurants which are pivoting to mobile pickup and drive-thru locations. https://www.cnn.com/2024/07/19/business/starbucks-mobile-ord... It doesn't look like the linked article mentions restaurant bathrooms but it can't be a coincidence. This all strikes me as a passive-aggressive reaction to FUD about homelessness and \"out of control cities\". reply Aurornis 5 hours agoparent> This all strikes me as a passive-aggressive reaction to FUD about homelessness and \"out of control cities\". Anyone who has ever worked in retail or food service could tell you that nobody wants to have to deal with the public restrooms. They need to be cleaned frequently and it’s not a fun job. It’s not just homeless people or FUD. It’s because the general public has enough outliers who cause problems that when 100 people use your bathroom every day, you’re guaranteed to get a couple who create some sort of mess. reply lolinder 6 hours agoparentprevIt could be passive aggressive, or it could be that frontline employees at these places are seeing things that you're not and headquarters is making changes to protect their employees and their property. Not every instance of fear is FUD, sometimes the danger is real. reply Aurornis 5 hours agorootparent> headquarters is making changes to protect their employees and their property. Or to simply retain employees. If you had your choice between two fast food jobs with similar pay but one of them requires employees to take rotations cleaning the public bathroom throughout the day, which one are you going to pick? The issues with drug users are also very real depending on the city and location. Having to have cops or emergency medical people show up to your location a couple times for no other reason than you offered public restrooms is a quick way to make a company decide to no longer offer them. reply lolinder 3 hours agorootparent> Or to simply retain employees. Protect employees in order to retain them, yeah. You're welcome to impute for-profit motive here, but either way I fully agree that I wouldn't want to be responsible for maintaining a public restroom in a city center. reply c-linkage 5 hours agorootparentprevChange \"protect employees and property\" to \"limit exposure to liability\" and I'd agree. reply Wytwwww 3 hours agorootparentPresumably employees are more expensive than property here. Both paying them to take care off and loss incurred because they left to work somewhere else where the conditions are better. reply lolinder 5 hours agorootparentprevTake whatever monetary incentive you like, the point is that it's more likely that there are real risks that people who work in these places know about than that it's all FUD and OP's anecdotal sense that there is no real problem is reflective of the reality on the ground. reply lotsofpulp 5 hours agoparentprev> This all strikes me as a passive-aggressive reaction to FUD about homelessness and \"out of control cities\". No, it is just that very, very few people want to clean and repair bathrooms. I would rather work at a restaurant that has no public bathrooms, so that I never have to clean them. Hence I am willing to work for less compared to a place with public bathrooms, hence my employer has lower labor costs. Pretty sure that goes for all the keyboard warriors too. It’s pretty well known that almost any job where you have to deal with the general public is worse than a job where you don’t. reply ryandrake 5 hours agorootparentI worked as a McDonalds janitor when I was a teenager, and I specifically requested it instead of cooking hamburgers. It was actually pretty pleasant. You got to walk around, talk to customers, go outside (to empty the parking lot trash cans) and so on. Yes, the bathrooms were messy, but I cleaned them so often they didn’t really have time to get totally disgusting. Of course nowadays, I bet it’s not a dedicated job, and they make the single person working alone in the store do everything… reply bell-cot 6 hours agoprevUm, yes, kinda. Though in my experience (US/Michigan), \"not public\" toilets are very often accessible if one has a good subset of the following traits: Very polite, locally-majority-race, clean, appropriately dressed, discrete, elderly, frail, female, alone, heavily pregnant, with an infant or small child, familiar to the employees, not in an urban area. Old geezer perspective: The withering availability of public toilets is just another symptom of the massive decline in social trust over the past half-century or so. I might suggest that the author talk to people on the \"supply side\" - especially long-time small business owners - about the expenses and miseries of being on their end of the stick...but the author seems so focused on the rights & well-being of the \"user\" population that I can't imagine him actually caring about \"their sort\". reply ghaff 6 hours agoparentPretty much. In the US, outside of major tourist areas, my sense is that explicitly public toilets have always been pretty scarce but, if you blend in, you can pretty much always find one at a hotel or a shopping center. No, you won't find them in a more residential area but that's always been the case. reply lotsofpulp 5 hours agorootparentYears ago, when I worked at a hotel, we had to lock down the bathrooms because a specific taxi driver would come and absolutely ruin them. The housekeepers threatened to quit simply because of this 1 person, and we had no reasonably cheap way to stop him, other than removing access to the bathrooms for the public. reply LadyCailin 5 hours agorootparentTell him he’s banned from the establishment, and report him to the police for trespassing if he returns? Seems like you took the nuclear option right out of the gates. reply kayodelycaon 5 hours agorootparentI can guarantee you this won’t work. The police won’t arrive nearly fast enough and you don’t have a lot of ways to stop him short of hiring a police officer as a guard because private security often aren’t legally allowed touch people. Some people won’t be deterred by anything short of someone who is able to intimidate or harm them. It’s a no-win situation because allowing employees to hit people with clubs isn’t a good idea. :( reply RcouF1uZ4gsC 5 hours agorootparentprevAnd then depending on the person’s race, he sues you for racial discrimination. Or he resists the police when they try to remove him from your premise. You have then opened yourself up to a bunch of risks. Much less risky to shut down access for everyone. reply lotsofpulp 5 hours agorootparentprevThat assumes police would respond quickly enough in this locale (they wouldn’t, you have to say someone is in danger for them to come quick), and that the taxi driver would care, since they might be betting (correctly) at worst, the police would ask them to leave the premises. He was told not to come into the hotel anymore, but he did anyway. Any other solution would have taken too long to implement compared to just changing the locks on the doors and making the housekeepers happy, who you kind of need to operate the business. reply tiahura 6 hours agoparentprevMaybe we should have believed them when they called themselves the counterculture? reply Mistletoe 5 hours agoprevI’m a runner and public water fountains seem to be as well. Is there some sort of website that maps public water fountains in your city? I run pretty far and I don’t want to carry water. reply habi 3 hours agoparentAs suggested in https://news.ycombinator.com/item?id=41016144, OpenStreetMap is the thing for you; I suggest use the data with Organic Maps: https://organicmaps.app/ You can search for „drinking“ to find mapped fountains/drinking water: https://wiki.openstreetmap.org/wiki/Tag:amenity%3Ddrinking_w... reply nradov 4 hours agoparentprevStrava has that, although there are some data quality issues. https://support.strava.com/hc/en-us/articles/6525900339725-S... reply underseacables 3 hours agoprevI lament this too. Paris has nice public pay toilets and Germany has the custom of leaving a euro. Maybe a company will open a series of public-private toilets in major cities. The government could help by providing space, utilities etc. The company charges a low rate, keeps things stocked and cleaned, police patrol it and everyone benefits. I understand the arguments of drugs and sex, but architecturally, socially, and civically we can mitigate that and should towards the common goal. reply binary132 4 hours agoprevThis feels like a cope for the fact that we let people defecate in public now. reply Eumenes 4 hours agoprevStart putting addicts and rabid homeless in mental wards/jails and only then will public bathrooms be tolerable. Hire some local muscle to keep an eye on things. Problem solved. These days I'd rather spend $5-$10 on a beer or souvenir to use a private bathroom with my kid vs potentially encountering a deranged man masterbating or shooting up. The real civil catastrophe is the enablement of this behavior. How come we can't figure this out in 2024 but for 1000s of years it was the absolute bare minimum that was expected from a society? reply kjkjadksj 2 hours agoparentPublic bathrooms are nasty even when there aren’t homeless people or addicts around. Ever been on a road trip? reply xboxnolifes 49 minutes agorootparentHighway reststop restrooms have been some of the cleaner restrooms I've been in. Gambling between them and a random fast food bathroom, I'd pick the reststop. reply kjkjadksj 10 minutes agorootparentWell I used to clean bathrooms as part of my first job. What you see is after people like us have gone through and cleaned it. I've seen piss everywhere. Poop everywhere. Puke. Blood. Used catheters and feminine hygiene products. Soiled clothing left behind for me to throw away (begging the question how they left without pants). I would clean the bathroom I was assigned probably once an hour and each time there was something disastrous to clean at that interval. Many people are simply disgusting; I can't imagine they would ever do this stuff in their own home. Never had a homeless person or obvious drug addict at this location, just people coming off the highway. reply SoftTalker 34 minutes agorootparentprevMajor name truck stops are also mostly good in my experience, as are bigger gas stations that are right off the interstate (more variable). Yeah fast food places are hit and miss, and post-COVID most are not open past 10pm or so. reply RecycledEle 5 hours agoprevIn Texas, fast food places almost always allow people to use their bathrooms. The few fast food places that do not let me go without complaint get a complaint to corporate and they lose my business for a few months. reply more_corn 2 hours agoprevIf you go into a business that doesn’t have a “public” restroom leave immediately and never go back. I don’t want to live in a world where I have to hold it, so I vote with my feet. And I assure you, that is the one thing that business owners pay attention to. If you want to make it clear you can post your reasoning on their yelp page. I would prefer a model like Germany has where instead of paying $50M for a public toilet and then service contracts, the government pays businesses to keep their bathrooms open. $50M goes a long way at $15/hr. Businesses add slightly more staff and bathroom maintenance is just part of the job. Sadly I am not in a position to make this policy dream a reality so I must push the upkeep cost onto the business owner. reply padolsey 6 hours agoprev> ‘feeling unsafe or exposed in a public toilet’ This just makes me think of America and the bizarrely huge gaps between the hinges and gaps at the bottom and top of the cubicles, even in very wealthy contexts, like at big tech companies. WHY? Such a strange example of embedded inertias of design in ostensibly innovative contexts. reply llm_trw 6 hours agoparentTo keep you from staying there for too long and lowering productivity. This was invented after having a foreman check that you pissed or shat in the toilet like they did on a ford assembly line was deemed to expensive. reply Y_Y 6 hours agoparentprevIt is far more important to prevent (through shame and surveillance) any unamerican toilet activities than it is to allow peaceful bowel movements. In the old world there there are floor-to-ceiling cubicle doors and toilet bowls that could barely drown a toddler. I need not explain how this is the first irreversible step towards authoritatian socialist anarchy. reply ungreased0675 5 hours agoparentprevThere’s no nefarious reason. It’s because they are prefabricated kits that need to be installed precisely to be gap-free. The installers obviously don’t want to or can’t install them with the correct tolerances. reply Zealotux 6 hours agoparentprevIt's simple: they don't want you to slack in the toilets, sitting there scrolling Twitter, in other words they don't want you to poop on your paid work time. The feeling of being unsafe or exposed is the point. reply rolisz 6 hours agoparentprevI think that's because of safety regulations, so that firemen can quickly check if there's anyone in stalls. Yes, it sounds stupid as an European, where somehow we survive without this. reply brudgers 6 hours agorootparentI used to practice architecture. In general (1) there are not regulations/laws/codes requiring the practice. The gaps at the floor are to facilitate mopping and reduce the damage from overflow floods. For first responder access, I have seen a hospital requirement for outswinging doors for one-holers with privacy locks. This allows the door to be opened when a person is collapsed against the door. (1) “In general” because there are almost certainly niche regulations that require it. For example the construction standards of a state prison bureau, local board of education, etc. reply amelius 6 hours agoprevYes, this is what Apple should invent next. With my iPhone I can do almost anything while traveling: look up maps, pay, take pictures, browse the web. But what I _can't_ do with my iPhone is the most basic thing ... reply wjnc 6 hours agoparentYou are punning, but if a big tech would create a map of publicly available toilets that would be great for many demographics (any man 65+, those with bowel syndroms). It’s the little things that make life easier (or more affordable) that matter the most (in my opinion). I know there is a debate in the group that needs “fast toilets” whether they should be free. Well that debate is moot when there are near zero options. A coffee brings what €1 of margin? A toilet could cost €1, and still help those in need. I hate paying for dirty toilets (German and Belgian gas stops grmbl, learn from France). But a clean toilet can be a lifesaver. reply jraph 6 hours agorootparent> if a big tech would create a map of publicly available toilets You don't need big tech for this: OpenStreetMap [1], a map (actually, a database) that works like Wikipedia, is there for all of us :-) It has information about public restrooms, whether they are accessible, gratis, and toilets are only a very small part of OSM. The more people contribute, the more awesome it becomes. A sibling mentioned Organic Maps, it's one of the many apps out there to conveniently use OSM (and contribute). [1] https://www.openstreetmap.org/ reply Schiendelman 6 hours agorootparentprevApple Maps does have pretty good restroom information - there just aren’t many. reply amelius 6 hours agorootparentHalf a solution isn't a solution at all ... If Apple cared about users they would work on problems that really mattered instead of giving us a 100MP camera where 50MP is already just fine. reply jeroenhd 4 hours agorootparentprevOpen Street Maps, which Google and Apple happily take information from, seem to have all public toilets indexed. Unfortunately, the map is still almost empty when you try to look for them. reply rcMgD2BwE72F 6 hours agorootparentprevOrganic Maps does that. reply ikekkdcjkfke 5 hours agoparentprevShould be able to suck it out into a biodegrageable bag, no wiping required reply lodovic 6 hours agoparentprevnow that got me thinking of a startup to build public restrooms that monitor the visitor with AI. reply llm_trw 6 hours agorootparentSomeone let me off this planet. I don't want to live here any more. reply FredPret 6 hours agoprevPublic restrooms are the first public good to bite the dust when petty crime picks up. reply debo_ 5 hours agoparent*when potty crime picks up /joke reply lotsofpulp 6 hours agoparentprevCleaning bodily fluids and repairing bathrooms is not cheap. That is why the government doesn’t provide them. If the government can’t be bothered to pick up that burden, no one else should be expected to. reply jtdev 6 hours agoprevAnother symptom of the decline of our formerly high-trust society. You can’t have public bathrooms in a society filled with criminals and homeless drug addicts. reply greenavocado 6 hours agoparentWhat are the factors that led to the explosion in criminality and homelessness and their drug addiction? reply scoofy 1 hour agorootparentProbably the proliferation of opiates and, more likely, methamphetamine. Scientific progress that makes these highly addictive and debilitating substances easy to produce can't be undone. I live in San Francisco, where it now costs over $100,000 per year to operate a single toilet, because without it being under constant guard, people start living in them. reply ta988 6 hours agorootparentprevIn some cities? people that have the means to spend $10 to go pee. reply rangestransform 5 hours agorootparentprevBeing allergic to enforcing the law post blm reply ravetcofx 6 hours agoprev [3 more] [flagged] 1992spacemovie 6 hours agoparentNo offense but I think a lot of tech guys on this site have never dealt with the \"other side\" of life. The other side being the shitty, disgusting, repulsive aspect of humans that occasionally rises to the surface. The reason un-restricted public bathrooms are \"challenged\" (to use corporate speak) is because the second you have one available, either a homeless person will camp out in it for hours doing everything you can imagine, or someone will be a total anti-social asshole and take a crap ON THE WALLS. I've seen this occur dozens of times, without fail. Yeah, it isn't as simple as \"le humans are bad and mean mmkay.\" Like most things that remain unsolved, it is complicated. reply ghaff 6 hours agoparentprev [–] And meanwhile in Japan, I've seen bathrooms for hotel restaurants that expect you to pay an attendant. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Public toilets are vanishing, leading to a civic crisis, particularly affecting individuals with health issues and marginalized groups.",
      "Guido Corradi, a psychology researcher, underscores the poor state of public restrooms in affluent Western nations, highlighting their impact on public health and human dignity.",
      "Studies indicate that poor, less educated individuals, and women experience more negative encounters with public restrooms, stressing the need for improved facilities and public awareness campaigns."
    ],
    "commentSummary": [
      "The disappearance of public toilets in the US has led to significant accessibility issues, forcing people to rely on businesses for restroom access.",
      "Although the Committee To End Pay Toilets In America succeeded in banning pay toilets, no measures were taken to replace them with free public toilets.",
      "In some states like Florida, businesses are required to have public restrooms, but access is often restricted to customers, highlighting the need for improved public restroom infrastructure."
    ],
    "points": 155,
    "commentCount": 214,
    "retryCount": 0,
    "time": 1721475434
  },
  {
    "id": 41013159,
    "title": "Doom on Playdate",
    "originLink": "https://devforum.play.date/t/doom-on-playdate/852",
    "originBody": "NicNic MagnierPlaydate Developer Preview 1 May '20 Last week end I finally started to work on porting Doom on the Playdate. This is a very early result, just to run the game, but that was the hardest part for me. Dealing with makefiles and compiler is really not my cup of tea so having the game running means that now I can focus on the game control, presentation and optimization. This is a gif from the simulator but I got the game running on the hardware too. The most difficult part was to deal with the linker that was complaining about some missing references in libc. After spending two evenings chasing every libc functions that might be problematic, I found out that there is some magical linker option. I've added the following line in CMakeLists.txt add_link_options(-specs=nano.specs -specs=nosys.specs) Since the linker was complaining about the symbol end missing, I've added it to the link_map.ld in the playdate SDK.bss :{ . = ALIGN(4); __bss_start__ = .; *(.bss*) *(COMMON) . = ALIGN(4); __bss_end__ = .; end = __bss_end__;} > EXTRAM 39 Programming with C on Windows using VSCode and CMake31 String Manipulation when using the C API11 How to share video screenshots easy in Dev Forum?4 created May '20 last reply May 27 80 replies 24.5k views 43 users 193 likes 13 links Frequent Posters 12 7 4 3 3 3 3 3 3 2 2 2 2 2 2 Popular Links 230 GitHub - NicMagnier/PureDOOMPlaydate: Pure DOOM - Single Header Doom Source Port github.com 179 GitHub - Daivuk/PureDOOM: Pure DOOM - Single Header Doom Source Port github.com 149 Game Engine Black Book DOOM fabiensanglard.net 112 DOOM_wads/doom2.wad at master · Akbar30Bill/DOOM_wads · GitHub github.com 111 GitHub - floppes/stm32doom: Chocolate Doom port to STM32F429 evaluation board github.com There are 80 replies with an estimated read time of 7 minutes. Summarize This Thread",
    "commentLink": "https://news.ycombinator.com/item?id=41013159",
    "commentBody": "Doom on Playdate (play.date)140 points by codetrotter 17 hours agohidepastfavorite35 comments SillyUsername 12 hours agoNeeds to use an Atkinson dither to minimise the dither error/distraction, this would produce clearer black and whites at the expense of a little colour/shade accuracy. reply RodgerTheGreat 12 hours agoparentFor a rapidly moving scene, an ordered dither is probably more appropriate than an error diffusion based dither. Faster to render as well. reply lloeki 10 hours agoparentprevDunno what Return of the Obra Dinn used as dithering but these videos of Doom running naive 1-bit clamping immediately makes me beg for it having the aesthetic of RotOD. reply Nition 9 hours agorootparentObra Dinn took a lot of experimentation to get right. The devlog for it on TigSource[1] has some big parts just related to dithering. For instance this post[2] and this post[3]. A user called Koloth also helped provide an Atkinson-like effect that works in a shader. [1]https://forums.tigsource.com/index.php?topic=40832.0 [2]https://forums.tigsource.com/index.php?topic=40832.msg121719... [3]https://forums.tigsource.com/index.php?topic=40832.msg136374... reply rasz 6 hours agorootparentprevFastDoom has a beautifully dithered 1bit (Hercules) renderer since at least two years now https://www.youtube.com/watch?v=EEumutuyBBo reply bmacho 13 hours agoprevGatling: https://devforum.play.date/t/doom-on-playdate/852/14 reply pornel 8 hours agoprevThe examples of dithering that look low-contrast and overly bright are due to incorrect gamma. The diffusion algorithms don't inherently make images brighter. They require working in the right color space, linear light in this case. This color space error is less visible when dithering to many colors, because the difference between the most similar color and dithered color is small, so the (incorrectly) diffused error is small. But dithering to 1 bit is the worst case. reply rhrowaway91933 9 hours agoprevThe dithering would work better if it also displayed an outline around each object (walls and enemy sprites). I don't know how much work it is to add it to the game, but I think it would be an improvement. reply rcarmo 9 hours agoprevBeing able to use the crank for the Gatling gun is just delightful. reply EnigmaFlare 8 hours agoprev\"... complaining about some missing references in libc. After spending two evenings ...\" This is such a chronic problem with compiling any existing C or C++ code. I'm paranoid about touching any new 3rd party software in those languages because of the inevitable frustrating fragility of it all. reply adrian_b 8 hours agoparentI do not understand your complaint. Any program, regardless of the language in which it had been written can be problematic to use when it depends on external libraries that you do not have. Python programs are many orders of magnitude more fragile than C/C++ programs, because I have seen plenty of such programs that require a certain Python 3.x version and they refuse to run with Python 3.(x-1) or 3.(x+1), making it difficult to install simultaneously many such programs, each desiring a different value of x. reply tialaramex 7 hours agorootparentMore and more programmers are used to a language which gets this right. Yes, you're correct that neither Python not C++ get this right, it's pretty sad that your conclusion is that therefore it's impossible. reply felixgallo 7 hours agorootparentDon’t be coy, out with it! Legions of programmers are dying to know which language has solved versioning and dependencies. reply tialaramex 6 hours agorootparentNo, I'd imagine most of them are comfortably using a language like Rust which got this right out of the box, or are like OP grumpily insisting that it's impossible and so nothing better can be done. One of the perverse things about human nature is that many people would rather believe nothing better is possible than go to any effort at all to improve things. reply felixgallo 5 hours agorootparentWhat rust is that? Nightly? reply GauntletWizard 51 minutes agorootparentprevGolang, for one. If you have a diamond dependency problem where two libraries want to use a common ancestors that's incompatible, you're screwed, but otherwise you've got a nice merkle tree of dependency versions built into the tooling. reply leshokunin 13 hours agoprevDo people use fast doom as the basis for ports on lower end hardware now? I was looking into it earlier and impressed how many improvements it brings. reply exitb 12 hours agoparentPlaydate is much more powerful than the original platforms Doom targeted. There’s a nice port for GameBoy Advance which is slower by an order of magnitude. reply Dwedit 13 hours agoprevOoh, custom linkscripts! Nice. reply aSithLord 5 hours agoprevthe Playdate is the Mastodon of handhelds. reply Reason077 10 hours agoprevThis is going to be great when they come out with the Playdate Color(tm) ! reply hexo 10 hours agoprevOops The software powering this discussion forum encountered an unexpected problem. We apologize for the inconvenience. reply codetrotter 10 hours agoparentHere’s a snapshot to read the thread while the original link is unavailable https://archive.ph/t8Wkn reply colesantiago 14 hours agoprev [–] I would have gotten the Playdate if it had a color e-ink display, hoping the next one would have one if there are plans for Panic to do this. The benefits of color and also the benefits of low powered, long battery life with possibly the chance to change the color to black and white like original playdates shipped with. Doom on Playdate looks awesome though! reply spaceisballer 14 hours agoparentUnfortunately I assume it’s pretty cost prohibitive for an e ink display with a decent refresh rate. At least one good enough to play games on. reply forgotacc240419 9 hours agoparentprevIsn't the whole point of the playdate to create a distinct format which removes a lot of noise from indie game dev? In my experience with game dev it was very easy to slip into fixating on pixel art color schemes, shading, etc If you want colour it's already there in everything else reply ashenke 14 hours agoparentprevHow does e eink screen work on a game console? The playdate screen is at least 30fps (and can actually refresh faster), even the fastest eink screens can't work at this speed without ghosting, and I don't want slower full refreshes while I'm playing a game. reply enduser 14 hours agorootparentIt’s not E ink, it’s a Sharp Memory LCD https://sharpdevices.com/memory-lcd/ Memory LCDs have fast refresh rates while also being persistent like E ink. They’re also more expensive. reply squeaky-clean 12 hours agorootparentExactly, they're not asking how the playdate works with an eInk screen (because it doesn't have one). They're asking how colesantiago expects to use a game console with a 5hz screen. reply benoliver999 12 hours agorootparentIt plays the following games - Crossword - Chess reply kragen 8 hours agorootparentprevthey aren't really bistable like epaper; the playdate display requires 50 microwatts to maintain the display. when i unplug mine from the breadboard it retains its image for about 30 seconds before fading just on the screen capacitance a broken epaper screen, by contrast, retains its image for at least several years reply Tepix 12 hours agoparentprevHave you seen the Playdate IRL? Its screen is very nice. reply MOARDONGZPLZ 11 hours agorootparentThe screen is amazing. Backlight would be nice but the screen itself is great. reply PaulRobinson 9 hours agorootparentI would love a backlight in my play date. The main thing stopping me playing it as much is whenever I pick it up, I feel I have to move to get light to see the screen at least 50% of the time. reply kragen 8 hours agoparentprev [–] the sharp memory-in-pixel display on the playdate uses roughly 1000 times less power than epaper displays do, assuming at least a few screen updates per minute. epaper only uses less power in applications like electronic pricetags reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "An early version of Doom has been successfully ported to the Playdate, a handheld gaming console.",
      "The developer faced significant challenges with makefiles, compilers, and linker issues, particularly missing references in libc, which were resolved by adjusting linker options and modifying the Playdate SDK.",
      "The game now runs on both the simulator and actual hardware, allowing the developer to focus on game control, presentation, and optimization."
    ],
    "commentSummary": [
      "The Playdate handheld console is running Doom, sparking discussions on optimizing its display using various dithering techniques for better visual clarity.",
      "Users are debating the merits of different dithering methods, such as Atkinson and ordered dithering, and referencing other games like Return of the Obra Dinn for inspiration.",
      "The Playdate uses a Sharp Memory LCD, which offers fast refresh rates and low power consumption, though some users wish it had a backlight for better usability."
    ],
    "points": 140,
    "commentCount": 35,
    "retryCount": 0,
    "time": 1721437831
  },
  {
    "id": 41010458,
    "title": "The European Union must keep funding free software",
    "originLink": "https://pad.public.cat/lettre-NCP-NGI",
    "originBody": "New Publish Extra Revision Slide Mode Import Gist Clipboard Download Markdown HTML Raw HTML Help OFFLINE New Publish Menu Extra Revision Slide Mode Import Gist Clipboard Download Markdown HTML Raw HTML × Import from clipboard Cancel Clear Import × This note is locked Sorry, only the owner can edit this note. OK × Reach the limit Sorry, you've reached the maximum length this note can be. Please shorten the note. OK × OK × Import from Gist Cancel Clear Import × Import from Snippet Project:Select From Available Projects SnippetSelect From Available Snippets OR Cancel Clear Import × Export to Snippet Title:File Name:Project:Select From Available Projects Visibility:Select Visibility Level Private Internal Cancel Export × Are you sure? Do you really want to delete this note? All users will lose their connection. Cancel Yes, do it! × This page needs to be refreshed Your client's version is incompatible. Refresh to update. Refresh × Choose method Sign in via Z⅂L × Help Contacts Join the community Meet us on Matrix Report an issue Help us translating Documents Features YAML Metadata Slide Example Cheatsheet Example Syntax Header # Header Unordered List- Unordered List Ordered List1. Ordered List Checklist- [ ] Checklist Blockquote> Blockquote Bold **Bold** Italicize *Italicize* Strikethrough ~~Strikethrough~~ 19th 19^th^ H2O H~2~O Underlined text ++Underlined text++ Highlighted text ==Highlighted text== Link [link text](https:// \"title\") Image ![image alt](https:// \"title\") Code `Code` var i = 0;```javascript var i = 0; ```:smile: Externals {%youtube youtube_id %} LaTeX $L^aT_eX$ This is an alert area.:::info This is an alert area. ::: × Revision Cancel Download Revert",
    "commentLink": "https://news.ycombinator.com/item?id=41010458",
    "commentBody": "The European Union must keep funding free software (public.cat)136 points by tr4656 23 hours agohidepastfavorite33 comments roenxi 17 hours agoWell worth reflecting that relatively little of existing free software seems to come from that sort of funding. It is mostly commercial companies following commoditise-the-complement strategies, consultancies building free tools then supporting them, or research spinoffs. If EU funding can create and sustain a thriving free software ecosystem I suspect it would require a new funding model that we haven't seen yet. Otherwise, why would they do it and how would the institution detect if it was working? reply walterbell 16 hours agoparentSome projects funded by NLnet, https://nlnet.nl/project/ ActivityPubDiscourse, WordPress bcachefs CryptPad F-Droid Jabber/XMPP Jitsi LibreOffice Matrix Mastodon NextCloud PulseAudio QubesOS SourceHut reply efnx 14 hours agorootparentMy project Renderling (https://github.com/schell/renderling) is funded by NLNet and I can say the whole process has been great. It’s changed the way I think about open source and gives me motivation to continue. reply roenxi 15 hours agorootparentprevThat list seems a bit lacking substance and slightly off target. NLNet doesn't claim to fund libreoffice for example - that was a Sun office suite, picked up by the Apache foundation and now controlled by the Document Foundation (mainly through general donations if I understood their financial reports). NLNet seems to be funding things like \"encrypted collaborative editing in the browser [using libreoffice]\" which is a different kettle of fish. Nice option to have, but fairly niche functionality in the LO suite as far as I know. And how does all this funding compare to something like the Google's Chrome & Firefox? That is one company controlling the majority of web traffic through OSS that on balance respects user freedom. To me, that is a better funding model with better results. Funding use cases may well be a good use of time, but given the serious issues the EU has establishing itself in the tech industry, the military and economic crisis they have managed to waddle in to and the general political turmoil that seems to have kicked up I opine it is not the time to be wasting political bandwidth like this note calls for. The US model of letting companies fund and build software (including free software) seems a bit stronger, more flexible and politically easier to coordinate. We don't need all of Europe to come together and work out who they think is the best team to build web technologies. Google puts a team on it and it probably happens. There are worse ideas out there than calls for government funding but it just doesn't sound effective to me - these continent-spanning governance bodies don't have the bandwidth to pull off this sort of delicate technical work. reply walterbell 7 hours agorootparentNLnet funds tactical feature improvements in selected OSS projects, not Google-scale underwriting of employee teams. > don't have the bandwidth to pull off this sort of delicate technical work What's the basis for this claim? There are literally 1,000 technical work engagements listed above, scoped in detail and reviewed by a few humans (not \"all of Europe\") at NLnet, with a track record of successful delivery to upstream OSS projects. reply klntsky 12 hours agorootparentprevnlnet funding is cents in comarison to the cost of these. They give grants, but they don't cover all the costs. reply rgreekguy 10 hours agorootparentprevNyxt browser had also been funded by E.U. at some point, not sure if they still do. reply nsajko 16 hours agoparentprevSource? reply walterbell 18 hours agoprev\"FOSS funding vanishes from EU's 2025 Horizon program plans\", 20 comments, https://news.ycombinator.com/item?id=41002044 reply tonetegeatinst 16 hours agoprevMy town had a budget vote not to long ago. I actually read the budget proposals and look at the year over year breakdown for certain categories. The amount my town is paying for microsoft365 and GIS software is substantial. While not an eyewatering amount, and I can understand that certain things need to be licensed....its hard to understand why my local town isn't using a FOSS GIS and just setting up a basic proxmox cluster with HA failover to host the darn thing. reply cen4 15 hours agoparentCuz when things need tweaking or break, there is usually no one to call. FOSS solutions are fine when there is some internal evangelist or team pushing it and are willing to support and maintain things (which over time usually comes out of some labor budget). If there is no one like that internally, even very simple issues can put people off using these systems. reply packetlost 15 hours agoparentprevBecause paying people who know how to do that properly are pretty expensive? The average local MSP is paying $25/hr for people who are either early career or flunked out of college or were too incompetent for a corporate job at a larger employer. AD/O365 is pretty easy to get going on a reasonable scale without much reply tomcam 18 hours agoprevIt would be interesting to see what they got out of the first €27 million they put into it. reply mananaysiempre 18 hours agoparentPyPy received an EU grant sometime in the late 2000s or early 2010s, although it had to be a different one. If I understood TFA correctly, NLnet is partially funded by this money? NLnet itself distributes money to very cool software things[1]—some moonshots such as Spectrum[2], some very down-to-earth and usable ones such as Nitrokey[3]. A discussion[4] linked in a neighbouring comment also mentions Sourcehut and Bcachefs. [1] https://nlnet.nl/project/current.html [2] https://spectrum-os.org/ [3] https://www.nitrokey.com/ [4] https://news.ycombinator.com/item?id=41002044 reply tomcam 17 hours agorootparentThank you. I was looking for something like a table with a project, maybe a summary, amount funded, KPIs, and ideally an assessment of the results. reply iudqnolq 6 hours agorootparentThey seem to have prioritized making the process simple for developers by removing overhead. Apparently you are don't have to write lots of reports about your results for them, they just check that the code you said you'd write is in fact written and runs. reply dhosek 17 hours agoprevI was thinking earlier today that self-driving automobiles would be a good use-case for a public-private venture for open source software development. My half-baked idea would be that car and computing companies could contribute their existing software to a open source consortium that would be government-sponsored, but privately run. Some fair valuation would be set for their contributions and the OSS license would require anyone using the software to contribute back their changes. Part of the funding mechanism would be that any self-driving vehicle would have a fairly high “tax” per vehicle (maybe $10K, 20K?) but that this could be prepaid by the manufacturer through software contributions. Yes, I know there are bottomless logistical issues to be worked out, but the idea being that perhaps, if everyone can see exactly what everyone else is doing we can maybe get out of the current pit of semi-working solutions that prevent true autonomous vehicles from being viable. Or, it might end up that self-driving vehicles are an infrastructure problem and that they need dedicated roadways without incursion by non-self-driven cars. Perhaps we could set them up to run on dedicated schedules on electrified metal causeways with shared ridership. reply sva_ 17 hours agoparentNot sure if you're implying that the government is supposed to oversee/manage the development of such a project, but I think they're extraordinarily bad at such things. It isn't the role of the gov either. > car and computing companies could contribute their existing software to a open source consortium that would be government-sponsored, but privately run. So basically it would just be a normal company, but the government pays them? In what way should the gov decide which company to fund, and why? If it is a commercial/private company, their existence should be justified by their profits. I don't really get it. reply wmf 15 hours agorootparentIn what way should the gov decide which company to fund, and why? If it is a commercial/private company, their existence should be justified by their profits. It makes sense for the government to fund R&D that the private sector won't fund, possibly because it would be profitable for the overall economy but not for any one company. Obviously a dozen companies are already building self-driving so that doesn't apply. reply DaoVeles 18 hours agoprevEven they only use free software as a fall back, that is a worth while investment. reply andsoitis 18 hours agoprevI wonder why the list of signatories at the bottom do not seem to include a single government (local or national). reply newsclues 18 hours agoparentGovernments need to band together and collaborate to develop software. But instead they tend to buy closed systems that are developed by the same companies churning out crud. reply fijiaarone 17 hours agorootparentNo one ever took a government out to a fancy dinner and drinks to convince them to use open source. reply newsclues 17 hours agorootparentEven if it would save governments money and help ensure civil servants get paid on time (phoenix payroll scandal)? Never had a meal from a sales rep that swayed my mind reply bell-cot 18 hours agorootparentprevIdeally, governments are competent. Realistically...competent governments are the exception. And competent-at-long-term-technology-policy governments are virtually non-existent. reply newsclues 17 hours agorootparentI understand the current state but also think it’s possible to change. reply gallagos 17 hours agorootparentWe just gotta wine and dine some suits. Sounds easy. reply ChrisArchitect 16 hours agoprev[dupe] Some more discussion: https://news.ycombinator.com/item?id=40970985 reply blackeyeblitzar 18 hours agoprevI am not sure what to make of the EU’s agenda on free software. On the one hand, we see provinces of some countries announcing open source adoption strategies. On the other hand, we see controversial regulation on things like AI that basically proposes to pull up the ladder and give the market to incumbents. reply jauntywundrkind 12 hours agoprevIt's ridiculously short sighted to cancel this stuff. NLnet in particular is by far one of the most impressive & broadspanning places for innovation & development in the software world in general, and vanguard for open source at large. The benefits are so multi-fold. Half the projects go no where, a quarter have some meaningful output with varying levels of adoption, and a quarter are amazing & loved bits of work. Theres folks asking if we are getting out moneys worth. First, $27m just doesn't seem like all that much. Especially for such a broad array of nations. Second, the hit ratio is never going to be perfect; worrying about how every dollar is spent will insure you never ever have good research & development, will insure only lukewarm mild takes get tried. You have to be willing to fund adventerous stuff; that's what VCs for example bank on. Because it's impossible to tell, because trying to be smart ahead of time breeds mediocrity. And this r&d&maintenance funding: it buys not only the fruits of that labor, but it maintains a dynamic & empowered culture across the region. It curates talent & starts efforts & intensives the best parts of it's software world. NLnet is so exceedingly important. There's so much possible good that humanity so rarely can get up to. NLNet has been one of core & best ways to promote making things better. This vision of human possibility is saintly & sovereign. I hope EU can keep it going. reply FjordWarden 17 hours agoprev [–] Money is a limited resource and I don't think NGI has proven to be a good investment. They made the same sort of embarrassing mistakes your average VC fund would make like BS blockchain ideas. A lot of uninspiring \"research\" where some of the deliverables is some open source software that no one ever uses and for which development stops after funding runs out. There seems to be something rotten about the open source \"community\" funded by the EU. I never going to forget the scene of NLnet members distributing free t-shirts with their logo on it to random attendees of the CCC congres in Berlin and then taking a big group photo for some social proof they can put on their website. All in the name of free and open source software that respects EU democratic values, that is pro-social narcissism for you right there. reply Qwertious 17 hours agoparent>I never going to forget the scene of NLnet members distributing free t-shirts with their logo on it to random attendees of the CCC congres in Berlin and then taking a big group photo for some social proof they can put on their website. You can pretend that grant money doesn't fund the writing of grant applications, but that won't make it true. If you make your criteria stricter and less reliable, then every project has to spend more overhead on crossing the Ts and dotting the Is to ensure their entire budget doesn't fall out from under them. reply poincaredisk 16 hours agoparentprev [–] As someone often on the receiving end of that money... It's complicated. My organization is doing a lot of great things and it's appreciated both nationally and internationally. Some of that work is funded from EU grants (including H2020 before). But the problem with research is... it's hard to predict where it takes you. At some point you realize that the project you got funding for is a dead end, but you can do a really cool spinoff that becomes successful but is not technically the same project. Another problem with research is that a lot of it is exploitation. You can't get a grant for just exploring random things and seeing what happens. But at the same time is a vital part of research - so official grants need to fund a lot of \"underground\" research, that may or may not mature into an official product. It's hard to estimate is a money is well spent by looking just at the official projects. ... And yes, many of projects are actually wasteful. Bureaucrats are really bad at estimating what counts as a valuable project and what is actually a sham moneygrab. In our case we are known for delivering so we often \"win\" when we start, but often random shell companies with the hype of the week come dangerously close. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The European Union is encouraged to increase funding for free software, as current contributions are minimal compared to commercial efforts.",
      "A new funding model may be necessary to foster a thriving free software ecosystem, with NLnet's impact being limited compared to major companies like Google.",
      "The debate centers on the effectiveness of government funding for technical projects and the EU's position in the tech industry."
    ],
    "points": 136,
    "commentCount": 33,
    "retryCount": 0,
    "time": 1721419023
  },
  {
    "id": 41017077,
    "title": "CrowdStrike debacle provides road map of American vulnerabilities to adversaries",
    "originLink": "https://www.nytimes.com/2024/07/19/us/politics/crowdstrike-outage.html",
    "originBody": "Global Tech Outage What We Know Why the Outage Is Hard to Fix Photos Is Your Computer Crashing? How to Handle Disrupted Flights ADVERTISEMENT SKIP ADVERTISEMENT NEWS ANALYSIS What Happened to Digital Resilience? With each cascade of digital disaster, new vulnerabilities emerge. The latest chaos wasn’t caused by an adversary, but it provided a road map of American vulnerabilities at a critical moment. Listen to this article · 6:16 min Learn more Share full article A digital meltdown that affected airports, hospitals and TV stations on Friday was caused by a bug in a software update. Credit... Jim Wilson/The New York Times By David E. Sanger Reporting from Aspen, Colo. Published July 19, 2024 Updated July 20, 2024, 8:35 a.m. ET In the worst-case scenarios that the Biden administration has quietly simulated over the past year or so, Russian hackers working on behalf of Vladimir V. Putin bring down hospital systems across the United States. In others, China’s military hackers trigger chaos, shutting down water systems and electric grids to distract Americans from an invasion of Taiwan. As it turned out, none of those grim situations caused Friday’s national digital meltdown. It was, by all appearances, purely human error — a few bad keystrokes that demonstrated the fragility of a vast set of interconnected networks in which one mistake can cause a cascade of unintended consequences. Since no one really understands what is connected to what, it is no surprise that such episodes keep happening, each incident just a few degrees different from the last. Among Washington’s cyberwarriors, the first reaction on Friday morning was relief that this wasn’t a nation-state attack. For two years now, the White House, the Pentagon and the nation’s cyberdefenders have been trying to come to terms with Volt Typhoon, a particularly elusive form of malware that China has put into American critical infrastructure. It is hard to find, even harder to evict from vital computer networks and designed to sow far greater fear and chaos than the country saw on Friday. Yet as the “blue screen of death” popped up from the operating rooms of Massachusetts General Hospital to the airline management systems that keep planes flying, America got another reminder of the halting progress of cyberresilience. It was a particularly bitter discovery then that a flawed update to a trusted tool in that effort — CrowdStrike’s software to find and neutralize cyberattacks — was the cause of the problem, not the savior. Only in recent years has the United States gotten serious about the problem. Government partnerships with private industry were put together to share lessons. The F.B.I. and the National Security Agency, along with the Cybersecurity and Infrastructure Security Agency at the Homeland Security Department, issue bulletins outlining vulnerabilities or blowing the whistle on hackers. President Biden even created a Cyber Safety Review Board that looks at major incidents. It is modeled on the National Transportation Safety Board, which reviews airplane and train accidents, among other disasters, and publishes “lessons learned.” Just three months ago, it released a blistering account of how Microsoft allowed intrusions into its cloud services that permitted Chinese spies to clean out State Department files about Beijing and Commerce Secretary Gina Raimondo’s emails. But by the time the report came out, American officials were focused on a more urgent problem: the spread of ransomware attacks, many from Russia. It was the Russians, in fact, who woke up America about the vulnerability of the software supply chain problem that lets small errors ripple into large consequences. In the run-up to the 2020 presidential campaign, Moscow’s most skilled intelligence service bored into a component of that supply chain, worming its way into the update systems of software made by Solar Winds. The company’s products are intended to manage large computer networks, and the Russians knew that once they had access to the update system, they could spread a lot of malicious code fast. It worked. Hackers soon gained access to the Treasury and Commerce Departments, parts of the Pentagon and scores of America’s biggest companies. They did no visible damage. They did not trigger panics like the kinds seen on Friday. But they got the incoming administration’s attention. “In a globally interconnected economy, we need to ensure that we have the resilience” when an event like this happens, said Anne Neuberger, the deputy national security adviser for cyber and emerging technologies, a job that did not exist until the Biden administration invented it. Ms. Neuberger was awakened by the White House Situation Room at 4 a.m. on Friday in Aspen, Colo., where she was preparing to speak on a panel titled “Securing Trust in the Global Digital Economy.” She spent the day assessing the risks to U.S. government systems, then calling allies and executives, including the chief executive of CrowdStrike, George Kurtz. She asked, “Is there anything we can do to help?” Image “In a globally interconnected economy, we need to ensure that we have the resilience,” said Anne Neuberger, the deputy national security adviser for cyber and emerging technologies. Credit... Pete Marovich for The New York Times Ms. Neuberger, a former senior official at the National Security Agency, knows better than most that for now, there are no magic bullets. By the time an event like this happens, the only response is to mount a painstaking effort, step by step, to patch the error, push it out and try to wrench thousands of systems back online. Sometimes it works. Sometimes, as the British Museum discovered recently after an enormous ransomware attack that British intelligence officials think may have ties to the Russian government, even the best of efforts to recover can fail. “This is not something that is new, but it has been accelerated by technology and by the interconnectivity,” Sir Jeremy Fleming, the recently retired leader of GCHQ, Britain’s famed code-making and code-breaking agency that is the equivalent of the N.S.A. And these days, he worries more about criminals than nation-state attacks. Criminals will certainly be gleaning lessons from the CrowdStrike debacle, learning how to exploit the kinds of vulnerabilities that brought television stations and airports and insurance companies to a halt. So will Mr. Putin and President Xi Jinping of China, who now have, by accident, a more detailed road map for disruption, in an election year when they may well have an interest in interfering. It is not hopeless. “We are optimistic that A.I. is actually allowing us to make significant — not transformative yet, but significant — progress in being able to identify vulnerabilities, patch holes, improve the quality of coding,” Kent Walker, the president for global affairs at Google, said at the Aspen forum. But that will take awhile. And in the meantime, unintended cascades of chaos will keep rippling around the globe — some, like Friday’s, a product of error. The fear is, in an election year, that the next digital meltdown may have a deeper political purpose. David E. Sanger covers the Biden administration and national security. He has been a Times journalist for more than four decades and has written several books on challenges to American national security. More about David E. Sanger A version of this article appears in print on of the New York edition with the headline: A Digital Meltdown Offers Adversaries a Road Map of American Vulnerabilities. Order ReprintsToday’s PaperSubscribe See more on: U.S. Politics, National Security Agency, President Joe Biden Share full article ADVERTISEMENT SKIP ADVERTISEMENT",
    "commentLink": "https://news.ycombinator.com/item?id=41017077",
    "commentBody": "CrowdStrike debacle provides road map of American vulnerabilities to adversaries (nytimes.com)128 points by jmsflknr 3 hours agohidepastfavorite181 comments neonate 2 hours agohttps://archive.md/CJdHl ScottBurson 1 hour agoprevReally interesting to me that none of the commentators I've seen in the press have even hinted that maybe an OS that requires frequent security patches shouldn't be used for infrastructure in the first place. For just one example, I've seen photos of BSODs on airport monitors that show flight lists -- why aren't those built on Linux or even OpenBSD? Security is not a feature that can be layered on. It has to be built in. We now have an entire industry dedicated to trying to layer security onto Windows -- but it still doesn't work. reply nullindividual 1 hour agoparent> why aren't those built on Linux or even OpenBSD The vendor who makes the software has always written for Windows (or in reality, wrote for either DOS or OS/2 then transitioned to NT4). History, momentum, familiarity, cost, and ease of support all are factors (among others, I'm sure). Security is a process, not a product. And yes, distros require frequent updates, though more to your point, you can limit the scope of installed software. I'm sure airport displays don't need MPEG2, VP1 and so on codecs, for instance. It's also important to remember that there is a lot of 'garageware' out there with these specialized systems. Want SAML/OIDC support? We only support LDAP over cleartext, or Active Directory at best. Want the latest and greatest version of Apache Tomcat? Sorry, the vendor doesn't know how to troubleshoot either, so they only \"support\" a three year old vulnerable version. Ran into that more than a few times. Given the hypothesis of what caused the BSOD with Crowdstrike (NUL pointer), using a safe language would have been appropriate -- it's fairly easy in this case to lay the blame with CS. Microsoft supplies the shotgun. It's the vendors responsibility to point it away from themselves. reply pwg 14 minutes agorootparent> I'm sure airport displays don't need MPEG2, VP1 and so on codecs, for instance. They don't, until the day the airport managers are approached by an advertising company waving the wads of cash the airport could be 'earning' if only they let \"AdCo\" display, in the top 1/4 of each screen, a video advertising loop. At which point, those displays need the codecs for \"AdCo's\" video ads. reply nullindividual 8 minutes agorootparentAbsolutely (sigh)! But with a deployment of devices like that, the operator has a solid central management system from which they could push software as-needed. reply V__ 25 minutes agoparentprevRight now on the frontpage: 'CrowdStrike broke Debian and Rocky Linux months ago, but no one noticed' [1] https://news.ycombinator.com/item?id=41018029 reply wil421 54 minutes agoparentprevAirport staff need to be able to support them. Not HN types. Most people know how to use a windows computer. Most IT desktop support knows how to use and manage windows. Even building facilities folks can help support them. Microsoft makes it easy to manage a fleet of computers. They also provide first party (along with thousands of 3rd parties) training and certifications for it. Windows are the de facto Business Machines. Most signage companies use windows. Finding someone who knows a BSD is not easy. reply mkoubaa 36 minutes agorootparentAirport staff don't maintain infrastructure, at best they maintain front ends to it reply fifteen1506 19 minutes agorootparentprevYup. Another take to be done here is: computers shouldn't have unfiltered internet access all the time. Whitelist it and once every 3 days open the internet gates. (Easier said than done) reply dopylitty 1 hour agoparentprevOr don't use an OS at all. We need to think about minimizing the use of software in critical infrastructure. If that means less efficiency because you have to be near something to maintain it then so be it. That would be good for jobs anyway. reply antihero 1 hour agorootparentHow exactly would a lot of end user systems function without one? reply marban 21 minutes agoparentprevSecurity is not a feature that can be layered on. There's an entire industry for guard-railing LLMs now. Go figure. reply giancarlostoro 48 minutes agoparentprev> why aren't those built on Linux or even OpenBSD? Or even ChromeOS which has insane security. > but it still doesn't work. It works momentarily but there will always be 0-days the people who make the exploits intimately know the windows API internals. reply stefan_ 1 hour agoparentprevI read the T&C of this CrowdStroke garbage and they have the usual blurb about not using it in critical industry. Maybe we just charge & arrest the people that put it there and this checkbox-software mess stops real quick. reply AceyMan 1 hour agorootparent/set Devil's Advocate mode: from the reporting so far, no one has died as a result of the Crowdstrike botch. For my money, that sounds like it's not being used in 'critical industry'. /unset There were several 911 service outages included in the news yesterday, so I would definitely say agree those fall into the category. I haven't seen how many hospitals were deeply affected; I know there were several reports of facilities that were deferring any elective procedures. reply logbiscuitswave 31 minutes agorootparentI almost had to defer a procedure for one of my cats because my vet’s systems were all down. This meant they couldn’t process payments, schedule appointments, use their X-ray machine, or dispense prescriptions. (Thankfully, they had the ingenuity to get their diagnostic equipment online through other means, and our prescriptions had already been dispensed so we didn’t have to reschedule.) I would imagine it’s the same story at human hospitals too that ran afoul of this. I wouldn’t expect life-critical systems to go offline, but there’s many other more mundane systems that also need to function. reply delfinom 34 minutes agorootparentprevThe public T&C is for small businesses. Any large business is going to be negotiating very different terms which are not public. reply hedora 1 hour agoparentprevThis sort of emergent behavior is a feature, not a bug. Operating systems that don't require frequent security patches aren't profitable. Anyway, this is the step of late-phase capitalism that comes after enshittification. Ghost in the Shell 2045 calls it \"sustainable war\". I'd link to an article, but they're all full of spoilers in the first paragraph. It probably suffices to say that the series refers to it as capitalism in its most elegant form: It is an economic device that can continue to function without any external inputs, and it has some sort of self-regulatory property that means the collateral damage it causes is just below the threshold where society collapses. In the case of Cloud Strike, the body count is low enough, and plausible deniability is low enough that the government can get away with not jailing anyone. Instead, the event will increase the money spent on security theater, and probably lead to a new regulatory framework that leads to yet-another layer of mandatory buggy security crapware (which Cloud Strike apparently is). In turn, that'll lower the margins of anyone that uses computers in the US by something like 0.1%, and that wealth will be transferred into the industry segment responsible for the debacle in the first place. Ideally, the next layer of garbage will have a bigger blast radius, allowing the computer security complex to siphon additional margins. reply Ylpertnodi 29 minutes agorootparent>I'd link to an article, but they're all full of spoilers in the first paragraph. Singular 'an', plural 'they're '. Anywoohoo...please provide a link, and let me decide? reply delfinom 36 minutes agoparentprevI'm sorry but even Linux requires frequent security updates due it's large ecosystem of dependencies. It's more or less required by every cyber security standard to update them just like windows. reply blablabla123 25 minutes agorootparentOn the other hand OpenBSD doesn't require very frequent patching assuming a default install which comes with batteries included. For a web server there's just one relevant patch since April for 7.5: https://www.openbsd.org/errata75.html reply lambdaone 2 hours agoprevThis has been an open secret for decades. Just a handful of major OS and browser vendors, constantly shipping patches to their systems and most software having such vast software supply chains that it's effectively impossible to audit anything, let alone truly certify anything as safe, and \"security\" software just expands the attack surface. Everyone in the industry knows this. Interesting to see the NYT just catching up. reply newzisforsukas 2 hours agoparent> Interesting to see the NYT just catching up. Maybe it has to do with some major incident that happened yesterday, and the fact they are a news company? reply lambdaone 2 hours agorootparentIt's the equivalent of not writing about Boeing until the day a 737 MAX crashes right in front of your newpaper offices. reply lastofthemojito 1 hour agorootparentIt feels more like writing about Boeing and then writing about Boeing again after the crash, considering the Times has been writing about cyber security and American vulnerability for a while: https://www.nytimes.com/2021/02/06/technology/cyber-hackers-... https://www.nytimes.com/2018/01/03/business/computer-flaws.h... https://www.nytimes.com/2013/07/14/world/europe/nations-buyi... etc reply SoftTalker 1 hour agorootparentprevReaders wouldn't have cared nearly as much. NYT: Boeing is run by bean-counters and isn't taking engineering seriously anymore. Boeing: That's not true. Our aircraft fly thousands of times a day, every day, and are very safe. Who would find that very interesting, absent any relevant, dramatic current events? reply pixl97 1 hour agorootparentExactly, the problem with for profit media is it requires the attention of it's audience. Everyone bitches about regulation and taxes, for reasons real and imagined, but applying laws and rules to businesses before something happens is the point of them. reply pintxo 44 minutes agorootparentHow is that a problem of media? Isn’t it more of a problem of the population at large? reply chipdart 1 hour agorootparentprev> Readers wouldn't have cared nearly as much. This bears repeating. If you complain about a risk before a disaster structs, you're fearmongerng. If you complain about a risk after disaster structs, you're flogging a dead horse. reply MiguelX413 39 minutes agorootparent*strikes reply simpaticoder 20 minutes agoprevMassive computer outage, worldwide affecting enterprises with Windows machines running CrowdStrike, a very popular software that is sold as hacking protection but which is, in reality, used by C-suite execs to spy on employee behavior. It is installed with extraordinary permissions and is difficult to fix or remove by design. I wonder if this will teach absolutely anyone a lesson about anything. reply BodyCulture 5 minutes agoparentCan we please get more information about the spying features? Some screenshots would be great! Thanks! reply hilbert42 2 hours agoprev\"What Happened to Digital Resilience?\" Was there ever such a time? If so then tell me when it was. \"The latest chaos wasn’t caused by an adversary, but it provided a road map of American vulnerabilities at a critical moment.\" I've no doubt that road maps of American vulnerabilities are currently being planned, roadmaped and stockpiled for future use by those who aren't on the best terms with the US. In one way I'm amazed at how laxadasical the US and others are towards these threats and that they have not done more to harden the vulnerabilities. On the other hand, it's obvious: cost is one factor but I reckon another bigger one is 'convenience'. Hardening systems against vulnerabilities means making them less convenient/easy to use and people instantly balk against that. Remember, this happened big-time when Microsoft introduced Windows especially Windows 95. To capture the market Microsoft made everything as easy as possible for nontechnical users—just click on something and it'd happen, things would happen with ease. And all this happened without due consideration to security. When viruses, vulnerabilities, breaches got out of hand restrictions were introduced which meant users had less freedom to do what they'd gotten used to doing. What Microsoft did was to get the world used to slack operating procedures and efforts reign this in has met with user resistance ever since. We're now stuck with a major problem that was easily foreseeable even before Microsoft launched Windows 95. Fixing it will be extremely difficult. reply lenerdenator 2 hours agoparent> In one way I'm amazed at how laxadasical the US and others are towards these threats and that they have not done more to harden the vulnerabilities. On the other hand, it's obvious: cost is one factor but I reckon another bigger one is 'convenience'. Hardening systems against vulnerabilities means making them less convenient/easy to use and people instantly balk against that. \"Show me the incentives, and I'll show you the outcomes.\" - Charlie Munger. We do not incentivize companies to operate secure, redundant, reliable computer systems. We incentivize companies to make the number at the bottom of the spreadsheet beat the expectations some analyst in Lower Manhattan set 90 days prior. And since companies handle the majority of societal work in the United States, that's how most critical systems are designed. Now, there's a chance that this will play out in court, and that Crowdstrike will have to be bought out to make up for the damages their customers suffered starting on July 19th. However, that will take years, and the outcome could very well be that the plaintiffs will receive symbolic or even no damages. By then, the market will have hedged, captured regulatory authorities, cut its losses, and just altogether moved on. The assets will be purchased in a firesale by people who see this as \"creative destruction\" and won't care that peoples' lives were put at risk because of this. And the cycle will continue. reply Animats 53 minutes agorootparent> We do not incentivize companies to operate secure, redundant, reliable computer systems. Except in the gambling industry. As part of a long-standing tradition, companies in the gambling industry are usually contractually required to take financial responsibility for errors. GTECH's annual report, before they were acquired by an Italian company, says \"We paid or incurred liquidated damages with respect to our contracts in an amount equal to 0.61%, 0.18%, 0.50%, 0.47% and 0.14% of our annual revenues in fiscal 2006, 2005, 2004, 2003 and 2002, respectively.\"[1] So, forcing a transaction process service to take full responsibility for errors cost, at worst, 0.61% of revenue. This is sufficient to force gambling companies to use unusually good security technologies. The Nevada Gambling Commission has technical rules.[2] * \"On-line slot systems may only communicate with equipment or programs external to the system through a secure interface. This interface will specifically not allow any external connection to directly access the alterable data of the system.\" Which means no privileged \"security\" systems such as Crowdstrike. * \"Gaming device application access to the system based game must be logged automatically on the system component of the game and on a computer or other logging device that resides outside the secure area and is not accessible to the individual(s) accessing the secure area.\" Which means the really important info must not only be logged, the logs have to be kept where the people who run the systems can't get at them. There are more logging requirements. Most things require two logs, one used for normal operation and a remote backup with tamper resistance and secure hashes. * \"Conditions for changing active software on a conventional gaming device or client station that is part of a system supported or system based game: (a) Be in the idle mode with no errors or tilts, no play and no credits on the machine for at least two (2) minutes; (b) Not be participating in an in-house or inter-casino linked payoff schedule...\" There's more, but the general idea is that to change anything, you have to take the component being changed down to the idle, fully backed up state. Only then can changes be applied. All of which are logged. The gaming industry has faced hostile actors for decades. They have reasonably strong defenses. Yet they're still very profitable. [1] https://www.sec.gov/Archives/edgar/data/857323/0000950123060... [2] https://gaming.nv.gov/uploadedFiles/gamingnvgov/content/Home... reply hilbert42 1 hour agorootparentprev...And I can't agree more. The question is what can be done, if anything. But I've a solution in my wildest dreams as a dictator. :-) reply lenerdenator 1 hour agorootparentYou introduce consequences against the people who have created the system that we're in now. reply HDThoreaun 1 hour agorootparentprevBaking in resiliency is expensive. Its not obvious to me that it would be better to deal with that than to deal with issues like this once in a blue moon. Why not let the markets decide? If this ends up costing a bunch of money it will be fixed, if it doesnt it wasnt that big of a deal. reply lenerdenator 58 minutes agorootparentBecause there's stuff money can't buy back, and in a lot of cases, that's human life and health. (1) And do the markets really decide? Do you really think the C-suite of Crowdstrike is going to spend the rest of their lives destitute for the losses they caused? Of course not. We have laws on the books that limit liability of businesses in these situations, and the \"let the market decide\" crowd are the first people to tell you these laws are a good idea because you can't possibly expect George Kurtz to do business in an environment where his 3 billion dollar fortune could be completely wiped out as the result of a court case, no matter how much damage his company did. Meanwhile the people who were screwed by this whole thing will be lucky to get a few grand out of a class-action judgment or settlement in five years. Markets _never_ actually decide. Not in a way that makes peaceful human society possible. You have to introduce systems to give minor players a way to redress grievances, or they'll find their own, often through less-than-sporting means. (1) reply dralley 2 hours agoparentprevThis is an area where studying Ukraine's experience will be very useful (and probably has already been useful) There were years of cyberattacks against pretty much every peice of critical infrastructure they have. Things went down, there were disruptions, but they adapted. Sometimes by falling back to low-tech solutions, sometimes by developing new systems with robustness into new systems and purging the old (much easier to politically justify when the problem is tangible and immediate). I seem to recall that one of the first things we did when tensions started ramping up was sending teams of cyber security experts from the NSA to help them lock down and root out infiltrations. reply prisenco 2 hours agorootparentnext [–]Sometimes by falling back to low-tech solutions My first thought in all this was wondering if there's a business opportunity for a consulting firm or startup that designs and manages offline paper backup systems that can quickly and seamlessly integrate back with digital systems once they come back online. reply SoftTalker 1 hour agorootparentThe problem is that if you aren't regularly training employees on those manual fallback systems, when you have to suddenly activate them, nobody will know what to do. Even if they have been trained on what to do, the processes will not be second nature. In real use, they will hit situations that the paper forms or training didn't cover, and will have to make up something on the spot, which they will each do differently. Fully comprehensive, regularly trained manual operations are very expensive to develop and test. Only the most safety-critical organizations will be able to justify and have the resources to effectively implement them. Air-traffic control, hospitals, nuclear plants, etc. And, they already have done it. reply pjc50 2 hours agoparentprevThe \"cyber agencies\" focus on offence, because that's easy to score points with and appear to be doing something, whereas defence is a very boring job of securing a zillion outdated endpoints. Or trying to get profitable megacorps to do something less vulnerable and less profitable. reply TrueDuality 2 hours agorootparentOffense is also easy in that there is a ton of software out there, and you just need to find one vulnerability. There is a \"win\" condition\" Defense is impossible as there is a ton of software and you need to protect all of it every time, there is only a \"lose\" condition. reply kortilla 37 minutes agoparentprev>Was there ever such a time? If so then tell me when it was. The 90s and into the early 2000s at least. You would get laughed out the room and then fucking fired if you hooked anything critical up to the internet. reply TrueDuality 2 hours agoparentprev> Was there ever such a time? If so then tell me when it was. It was a goal for a long time, and I'd say we use to be more resilient pre-cloud SaaS auto-update everything. When every software solution installation is on private networks, with fundamentally different architectures (both machine and topology), along with a wide selection of even very poor quality software, was a lot more resilient than what we have today. Today a single outage in a single service (say AWS) can grind a large number of companies to a halt. A bad update like this one immediately impacts everyone all at once and has a domino effect. That didn't use to happen. We've been concentrating our collective architecture into a few best practice tools but that all become single points of failure for not only digital attacks, but misconfigurations, mismanagement, company failures, exhausted underpaid engineers, optimizations, etc. > Hardening systems against vulnerabilities means making them less convenient/easy to use and people instantly balk against that. This isn't necessarily true, and I'd argue quite the opposite direction has been happening in the security industry over the past decade or so. People realized that hard security would only cause users to find simple predictable bypasses that would overall _weaken_ the security posture. You just have to look at the evolution of NIST recommendations around passwords to see this happening. Must change a password every 90 days that can't be the same as your last 10 passwords and complex password requirements? Well users are going to use the minimum size in predictable patterns and just increment a number at the end. Those old password hashes you have to keep around to check if the user is reusing the password? Those are a liability that, when broken, tell the attacker which pattern each user is using. Not the case anymore and there is a lot more usable security rolled that is entirely transparent to end users or almost entirely transparent. Think about how prevalent and bad captchas used to be on the website and how easy they were to circumvent. Cloudflare's and Google's captcha solution are pretty transparent and has much greater efficacy than the old ones. Did Microsoft's general and on-going laxness contribute to bad security practices? Absolutely, but that is one ecosystem that had weird other by the nature of how inherently unstable that environment was and is not and hasn't except for maybe a brief peak ever been a core foundation of the internet infrastructure, just enterprise infrastructure unfortunately. They definitely never got the memo about usable or transparent security. I hope they're at least trying behind the scenes now. reply binary132 1 hour agoparentprevWhat makes you think only a foreign adversary might want illegitimate access to our computers? reply oneplane 14 minutes agoprevThere is no \"Digital Resilience\" because that is perceived as too expensive, a cost center with hard to quantify value. So it's easier to try and carve out everything that doesn't fit into a spreadsheet, everything that isn't core business, and everything that is not able to present what value it generated. If general IT had the abilities of sales, marketing, or insurance, there might be a chance that the business would take the responsibility to have the internal knowledge and capabilities to assert control over their systems. But they don't, and as such they won't and instead shove that responsibility over to a third party generalist elsewhere with enough paperwork to have both parties feel their asses are covered. As long as everything seems to be working, the signals that are still getting through is project failures, be it complete failures or just time and/or money being consumed more than planned and maybe some requirements getting cut. But as soon as enough stuff breaks at the same time, we get news outlets writing articles about resilience and the greater public suddenly no longer agreeing with that is effectively just the result of the status quo because it impacts them directly. reply encoderer 2 hours agoprevJust told my family yesterday that if we are ever in a real war expect everything to stop working within 8 hours. We will go back to cash and paperwork but it will be painful and slow. reply SkyPuncher 1 hour agoparentThe Ukraine war paints largely the opposite picture. Outages are largely limited to physical infrastructure that’s attacked by missiles. Russia isn’t a slouch in digital warfare, either. reply joelthelion 44 minutes agorootparentUkraine depends a lot on American services. Russia is not at war with the US. reply Meekro 20 minutes agorootparentThat's a good point -- Russia doesn't want to massively escalate against the US with an all-out cyberattack. I've often wondered if total war against Russia or China would show how fragile our internet-connected infrastructure is, with e.g. important people's bank accounts vanishing with no evidence they ever existed. reply encoderer 26 minutes agorootparentprevExactly. reply makeitdouble 2 hours agoparentprevLooking at two countries in an actual long running war, both kept using cashless means, with actually increases in usage: https://cbr.ru/eng/press/event/?id=18776 https://bank.gov.ua/en/news/all/drugiy-rik-povnomasshtabnoyi... reply newzisforsukas 1 hour agorootparenthttps://www.csmonitor.com/World/Middle-East/2024/0703/gaza-w... https://www.bloomberg.com/news/articles/2024-04-10/gaza-face... reply kjkjadksj 1 hour agorootparentprevThis isn’t really all hell breaking loose actual war. If it were Kyiv would have been a ruin years ago. reply AndriyKunitsyn 42 minutes agorootparentThroughout this war, 62k Russians are certainly KIA because we know their names and faces [~], and estimates of total Russian KIAs vary from 120k from a Russian outlet [^] to 565k by Ukrainian Armed Forces [_]. In comparison, total KIA losses of Soviets in the Afghanistan war were 14k-26k, and Americans in the Vietnam war lost 58k KIA + 150k WIA throughout 10 years. In short, this is the biggest war in Europe since WW2. But hey, it's not war enough because not enough Ukrainians are dead or something, idk. [~] https://t.me/pechalbeda200 [^] https://meduza.io/en/feature/2024/07/05/a-new-estimate-from-... [_] https://t.me/GeneralStaffZSU/16238 reply kjkjadksj 14 minutes agorootparentIts really not a full mobilization though. Yes 62k casualties seems like a lot. When Russia is fully mobilized in total war however, the sort of war that NATO planners fear the most, they go through millions of casualties and take over half the European continent in the process. reply newzisforsukas 2 hours agoparentprevJust storm EDR company offices slap guns to devs' heads, push geofenced destruction. reply ronhav3 1 hour agoparentprevIsrael is doing well after 10 months. No lack hostile hackers. reply TeMPOraL 1 hour agorootparentThey're not fighting a peer power. reply sulandor 1 hour agorootparentprevthank god that israel has very strong defense and cybersecurity sectors reply pixl97 1 hour agoparentprev\"Leave the world behind\" reply cdchn 1 hour agoprevIf CrowdStrike's system wasn't able to prevent a kernel driver thats all zeros from getting by, you can be sure a malicious payload would have breezed right through. reply therein 54 minutes agoparentOh yeah, at a quick glance looks like that file could have had any payload and it would have been loaded right into the kernel. reply AlbertCory 2 hours agoprev\"Diversity\" (but not in the sense of marginalized people) If more of the critical machines were running different OS's, the damage would be contained. When we talk about the dangers of \"monoculture\" it's usually about plants. The same danger applies to computing infrastructure. reply jnwatson 2 hours agoparentWe're already there. The fact that we didn't see civilization collapse is evidence that there is a ton of infrastructure not running Windows and Crowdstrike. reply treyd 2 hours agorootparentThis wasn't nearly as bad as it could have been. What if the crash wasn't just a crash but resulted in data corruption? And what if it took longer to stop the rollout and deploy a fixed version? How long would it have taken to recover from this kind of incident? If affected machines didn't fix themselves after several reboots but needed to be actively reimaged? reply AlbertCory 2 hours agorootparentprevFor a long time after Burroughs was almost ancient history, banks still ran Burroughs machines. They've probably thrown in the sponge by now. I'm sure IBM mainframes are still running critical stuff, too. reply chrisjj 2 hours agoparentprev> If more of the critical machines were running different OS's, the damage would be contained. Not if they were running the same CrowdStrike. reply AlbertCory 52 minutes agorootparentgiven it's a kernel module (AFAIK), how could that be if it were different OS's? reply mr90210 2 hours agoparentprevOn top of that, I am still struggling to understand how the people in charge of running orgs that run highly critical systems were OK with the idea that a 3rd party software provider could push at anytime patches to the software they provide. Sorry for being harsh with my following statement, but I believe that the companies affected by Crowdstrike share some responsibility on what happened yesterday. reply lambdaone 2 hours agorootparentYou're making the mistake of assuming that the people running those companies care about anything other than their job security, and buying in solutions is the best way to have a ready-made scapegoat when things go wrong. The mantra \"no-one ever got sacked for buying IBM\" still holds, you can just substitute \"Oracle\", or \"Microsoft\", or now - apparently - \"Crowdstrike\". reply slt2021 30 minutes agorootparentprev- pushing patches is objectively a good idea, rapid response to threats and all. - Whats bad is instant global 0->1 rollout, instead of more gradual, blue/green/canary however you call it. With gradual rollout policy this whole thing could have been caught at their first couple guinea pig customers, and not the whole world reply chrisjj 2 hours agorootparentprevThe are OK with \"push at anytime patches to the software\" because that's a big part of what they are paying for. Rapid response to threats. reply pixl97 1 hour agorootparent>Ping reply from 127.0.0.1 The threat is inside the building! reply RedShift1 36 minutes agorootparentprevAs I understand it, customers do have control, but in this instance CrowdStrike overrode the settings of the customers. reply notabee 2 hours agorootparentprevIt's not harsh. The tide went out and it turns out a lot of people were swimming naked. reply cdchn 56 minutes agorootparentprevOne could make the argument that automatically patched software is, in aggregate, more secure/less problematic than chronically under-patched software that requires manual, human attention. reply AlbertCory 48 minutes agorootparentOne could, but in the old days when vulnerabilities happened, they didn't hit everyone at once. And if it hit your system, the vendor's first response would be \"are you on the latest update? that's been fixed.\" (Unless the latest update IS the problem. In that case, being lazy was a good defense.) reply ck45 1 hour agorootparentprevI think I agree with you. On the other hand, I can also imagine that if autoupdates weren't the case, then 90% of installations would be a terribly outdated and probably vulnerable version. It's hard to imagine a common sense middle ground. reply cynicalsecurity 1 hour agorootparentprevSurprisingly, the mantra \"if it works, don't touch it\" doesn't really work so great. reply meiraleal 2 hours agorootparentprevThey share the whole of the responsibility of it. \"my antivirus was updating\" is not an acceptable excuse for a service to be down. reply cynicalsecurity 1 hour agoparentprevComputers are not people. No need to be afraid to discriminate. Windows is shit. Mac is more or less. Linux is best of all. reply forrestthewoods 56 minutes agorootparentYou do realize that CrowdStrike also runs on Linux and that there have been a variety of instances of bad CrowdStrike updates breaking Linux machines, right? https://access.redhat.com/solutions/7068083 reply gmuslera 2 hours agoprevThere are some point where you should redefine what it mean to be an adversary. To be practically forced into a position that lead to this level of harm, by actors that you don't want to perceive, is something that you may want to analyze. The purpose of a system is what it actually do, not what it claims to do but fails every time at that. Turning everything to vulnerable as fragile with some big strategic and global plan ahead makes you into a disposable asset, a sacrificial victim in some higher level chess game. And you can agree with that with your decisions. reply johanneskanybal 16 minutes agoprevSo yea let’s not use a company like this as best practice. Everything about this reeks of worst practices rising the wave of regulatory capture. reply siliconc0w 1 hour agoprevReally, the problem is that all this critical infrastructure runs on Windows. Critical systems should effectively be appliances that run with a very minimal footprint. If you absolutely need to monitor them you can export disk snapshots or something out of band that can't impact operations. reply GeoAtreides 2 hours agoprevHere's an interesting exercise: what's the minimum quantity of explosives that would lead to 1% drop in western GDP? would doubling it lead to 2% or 4%? is the relationship linear? I don't have an answer, but thinking about it makes one understand how incredible fragile our complex logistic chains (and indeed our economy) are. One day all this complexity will collapse upon itself and we'll wonder what happened. reply pjc50 2 hours agoparentBomb in TSMC clean room. Almost any size. That takes out the AI market. 1% of GDP gone. However, it's less than linear; not many targets of such critical importance. The ability to get the bomb in the right place is far more important than the quantity of explosives, as was demonstrated by the recent suicide sniper missing. The IRA https://en.wikipedia.org/wiki/1993_Bishopsgate_bombing was estimated to cause more economic damage than all other IRA bombing put together. It's interesting that (apart from the first WTC bombing) American terrorists have stuck strictly to guns and not attempted car bombs. reply pixl97 1 hour agorootparentIf you could get 4 people 81mm mortars (and some training) it's highly likely you could shut down 10% of us gas refining by attacking just 4 facilities along the TX/LA coast. It's very possible you could also do this with drones and avoid getting caught for some time, though your payloads may be a bit lighter. Refineries are large, but typically weak targets with critical areas. This has been something that Ukraine has been exploiting against Russia. reply krisoft 1 hour agorootparentprev> It's interesting that (apart from the first WTC bombing) American terrorists have stuck strictly to guns and not attempted car bombs. That is not true. Oklahoma City bombing is the first which comes to mind where the explosives was planted on a truck. But there are many others, there is a whole wikipedia list about them: https://en.m.wikipedia.org/wiki/Category:Car_and_truck_bombi... reply verzali 2 hours agoparentprevProbably not a lot. Blowing up a ship in the middle of the Panama or Suez canal might do it, especially if you wreck it badly enough to block the canal for months. Even easier if you target a big oil tanker. I don't think this is linear though. It's easy to target a weak point to inflict a small amount of damage, but hitting say 10% of GDP would mean targeting multiple sectors of the economy and putting millions of people out of productive work. reply SkyPuncher 1 hour agoprevFor the non-tech folks, this probably felt like one step away from an attack from an adversary. I have a different take. This was still far from being an adversarial attack. There was no security breach. The failed configuration came from an SDLC that remained secure and fully in control of CrowdStrike. It was a terrible bug, but it was not an attack reply umvi 1 hour agoparentI would not call it a bug. I would call it a severe process or systemic failure. Their SDLC clearly did not include any sort of phased rollout or canary deployments. Bugs are inevitable, what matters is being able to catch them before you push them to every end user on the planet. reply cdchn 50 minutes agoparentprevIf CrowdStrike's system wasn't able to prevent a kernel driver thats all zeros from getting by, you can be sure a malicious payload would have breezed right through. There was no validation, phased roll-outs, almost certain no multi-person verification. I'd bet dollars to donuts this was pushed out by a low/mid-level functionary that could be carried out by dozens if not hundreds of employees. There may have not been a security breach, but it was still one minor security breach, distracted open laptop in a cafe, or disgruntled/paid-off inside actor away from absolute armageddon. It wasn't an attack, but it was a raccoon who came in through an unlocked screen door in the back of Fort Knox. If someone had used this to deliver a ransomeware package, they'd be buying a mega-yacht right now. reply dgoldstein0 31 minutes agoprevCrowdstrike has really redefined malicious compliance reply cmrdporcupine 2 hours agoprevHow long before our evident incompetence as a profession comes back to bite us in the form of more draconian regulation about who and what is allowed to run in kernel space, or other privileged contexts, on critical infrastructure? reply closeparen 1 hour agoparentCrowdStrike's widespread deployment is encouraged by regulation. reply remram 2 hours agoparentprevIs that a \"bite\"? I have wished for this for a long time. reply mardifoufs 1 hour agoparentprevI usually agree that we are heading towards regulation (software engineering is already a regulated title where I live) but in this case, crowdstrike had such a blast radius exactly because of regulation. reply adrr 1 hour agoparentprevWhat security software runs in user space? Even on the Linux side I struggle to name any except snort or any of the open source root kit scanners. How would you enforce security policies in user space? reply marcosdumay 54 minutes agoparentprevThat entire thing was caused by stupid draconian regulation. Systems like CrowdStrike are mandated, not hired freely. reply pdimitar 2 hours agoparentprevWhenever that time comes it would be at least 50 years too late. But I hope to see it in my lifetime. reply cmrdporcupine 2 hours agorootparentThe problem I think is that it would just take the form of regulatory capture. A few companies would be blessed, and the rest of us locked out. And we'd still have screwups like yesterday, but this time with Government Approval. Already it's amazing how the media is presenting this like it's a natural disaster, instead of an entirely preventable display of incompetence... A business entity whose shares only dropped 10% after causing untold billions of damage to the economy. Gives us all a bad name. reply pdimitar 2 hours agorootparent> Already it's amazing how the media is presenting this like it's a natural disaster, instead of an entirely preventable display of incompetence Amazing? Super predictable I'd say. > Gives us all a bad name. That is sadly true. reply pdimitar 2 hours agorootparentprev> The problem I think is that it would just take the form of regulatory capture. A few companies would be blessed, and the rest of us locked out. And we'd still have screwups like yesterday, but this time with Government Approval. Yeah agreed. It would require no corruption... which is the true fantasy trope of our times. reply mr90210 28 minutes agoparentprevRobert C. Martin has been talking about this same topic for years. He believes that just like in the medical field, the software industry must self-organise before government start imposing draconian measures about how software should be developed. reply FerretFred 2 hours agoprevKent Walker's betting the farm on AI spotting future f*ckups? One born every minute! reply ndesaulniers 3 hours agoprevYeah, didn't the US just ban Kaspersky, over fears that Kaspersky could cause such an outage (among other fears)? Turns out our homegrown CrowdStrike was just as bad as our fears over Kaspersky were. Perhaps worse. reply supriyo-biswas 3 hours agoparentThe Kaspersky issue could have been better handled by simply requiring divestment or by having requiring an US-appointed auditor to investigate produce reports to assuage such concerns; as was proposed in the case of Tiktok. reply dataflow 2 hours agoparentprev> over fears that Kaspersky could cause such an outage (among other fears)? Citation? I thought it was the \"other fears\"; this is the first time I'm hearing accidental outages were one of the concerns. reply mathgeek 2 hours agorootparentNot GP, but the decision and reasoning is at https://public-inspection.federalregister.gov/2024-13532.pdf (I am not claiming any specific “other fears”, just linking to the source) reply dataflow 2 hours agorootparentThanks. Yeah, i, ii, and iii all talk about malicious events, not accidental. reply mardifoufs 1 hour agorootparentprevYes but any accidental outages from an entity like Kaspersky would have been considered non accidental regardless of the actual root cause. If crowdstrike was Russian, the headlines would be a bit more suspicious about yesterday's event. or if they had brought down Russian infrastructure Russia would have probably been suspicious about American involvement, even if it's just accidental. reply bitexploder 36 minutes agoprevUmm, they (adversaries) already knew? Been in cybersecurity for 18 yrs. We told customers about issues like this all the time reply mistrial9 2 hours agoprevexternalizing a threat, from a national news source.. Thought experiment -- a healthy society has plural viewpoints, and plural economic strengths. What if a core and entitled group of groups imposed their \"security\" on a plural society, for their own profit at the expense of the majority? What if their security is monoculture and internally inconsistent, without the ability to admit error ? What if there is a reflex to blame external groups specifically to divert attention from an internal and unbalanced chain of actions, controls and monetary flows? What is the response of a Free Press to news stories exercising reflexive blame-game from allied core groups with major monetary interests in the outcomes? reply feedforward 33 minutes agoparent> externalizing a threat Yes, it's illustrative of the USA. Due to monopolies, lack of local control of infrastructure etc., a feature is rolled out that grinds hospitals, airports etc. to a halt. Surely due to forces we're surely familiar with - a rush to get profit-making features out, a neglect of correctness and stability, cost cutting etc. Then we have the New York Times, considered the sober voice of the establishment. What is discussed? Reflection on how entirely US-internal corporate processes led to this? No. A thought experiment about what if some external actor, perhaps one tired of US imperialism or something, had performed this. I read this after seeing Hulk Hogan rip his shirt off at the RNC in an Idiocracy prophecy manifested, while the other presidential candidate immersed in the dementia of the gerentocracy clings to power amidst his cohorts pleading he step aside. As I watch the US arming the Ukraine to fight Russia, I think back to 1986 and Gore Vidal's plea for an alliance with Russia lest Americans become either farmers or just entertainment for the more efficient Asians. Another prophecy which seems due to come to pass. reply bell-cot 3 hours agoprevOn the one hand - you can read this as a PSA for the apathetic and/or clueless 99.9%. On the other hand - it's d*mn hard to imagine that any of America's \"A List\" or \"B List\" adversaries didn't have a far-more-detailed road map, years ago. reply red_admiral 3 hours agoparentI'm sure there's a few adversaries who could pull something like this off, and have 0-days ready. But if they use them, the US could see that as a hostile action and get very upset about it. reply LtWorf 2 hours agorootparentthey can just bribe a company to do it :D reply pdimitar 3 hours agoparentprevDoes the last part of your comment imply that USA should just give up and accept all its adversaries already have backdoors and nothing can be done about it? reply marcosdumay 52 minutes agorootparentDid any large company clean their datacenters after they stopped using Solar Winds? reply nonrandomstring 2 hours agorootparentprev> accept all its adversaries already have backdoors This is actually a really useful hypothetical standpoint to work out security from. Designing systems that start from the assumption of insecurity helps us build more robust protocols and management. Qubes OS starts from the position that all VMs are or soon will be compromised. Zero-trust in network design assumes the bad guys already have the whole network. Plenty out there would like to shrug and say \"the endpoints are all rotten too\" (especially with phones which are a veritable hell to secure) and move trust into the application via trusted execution methods. > and nothing can be done about it? No, That doesn't follow. It's prudent to be realistic about threats. but there's always a way out, at a cost. The cost, in a complexity crisis, is throwing away a lot of what we've done. reply pdimitar 2 hours agorootparentYeah I am completely with you and I agree. But it seems that reducing costs is more important even compared to preventing people on life support in hospitals from dying. What a world. reply bell-cot 1 hour agorootparentprevNo, and I've no idea where you got that from. Here's the HN Title: \"CrowdStrike debacle provides road map of American vulnerabilities to adversaries\" My assertion: America's serious-threat adversaries already had far more detailed road maps, years ago. The intel value of whatever \"road map\" data they got from the CrowdStrike debacle was pretty marginal. Neither the HN Title nor I said anything about backdoors. And within 2 para's, the NYT story makes it clear that CloudStrike's Big Oopsied had nothing to do with bad guys hacking anything. reply pdimitar 3 hours agoprevWouldn't any memory-safe language help prevent this NULL pointer access? Why are all these crucial pieces still written in C/C++, when it's obvious to anybody keeping even remote track of CVEs that these languages are just not up to the task with today's climate of a 24/7 shadow internet war? (The one that's likely been going on for at least 25 years at this point?) When will we learn? You hate Rust -- fine (not fine but OK, I guess people get super triggered over it and it's a reality I can't change but I am still baffled by it because they throw away reason for emotions and these people should really know better). Fine. Just use Golang or any other GC language really (Java or C# as well, if you must). When will we abandon convenient routine and start adapting to modern realities? (\"Modern\" being at least 25-year old here but hey, I am willing to give you some leeway and not roast you too much. Let's assume these are \"modern\" realities, f.ex. just the last 5 years.) reply aninteger 2 hours agoparentWe're all waiting for your anti-malware Rust Win32 kernel module... Ok, but seriously I don't believe this will ever happen and I don't really think this is a language debate nor do I want to engage in one. This is about putting critical infrastructure connected to the internet that's running an operating system that you can't trust out of the box. Since the Windows OS is susceptible to so much malware you need all these third party services (which you also can't trust or audit, but it's absolutely better than not having anything) on top of the OS. There was a whole host of companies that had zero problems, not because they're using Rust, but because they have much better security practices and quality infosec employees. reply pdimitar 2 hours agorootparent> This is about putting critical infrastructure connected to the internet that's running an operating system that you can't trust out of the box. Since the Windows OS is susceptible to so much malware you need all these third party services (which you also can't trust or audit, but it's absolutely better than not having anything) on top of the OS. Agreed, they should not be using Windows in the first place. That should have been the first line of defense. > There was a whole host of companies that had zero problems, not because they're using Rust, but because they have much better security practices and quality infosec employees. Fair enough, I only commented on one layer of the security stack -- so your remark that expands the scope is valid and welcome. > We're all waiting for your anti-malware Rust Win32 kernel module... I am done working for free. If I am paid to do it I am sure I would have done better than this poor confused soul who allows NULL pointer dereferencing which is a mistake that most C/C++ interns quickly learn to avoid. reply johnfonesca 2 hours agorootparent>Agreed, they should not be using Windows in the first place Crowdstrike borked RHEL 1 month ago https://access.redhat.com/solutions/7068083 Literally the same situation, unbootable machines. The reality is that shitty software broke everything. Why do we have to drag the OS into this? reply pdimitar 2 hours agorootparentDunno, I guess I naively thought the quality of Linux drivers is higher but on the other hand, if the same confused randos are writing them then you're right that it would not make a difference. reply lambdaone 2 hours agorootparentprevI didn't know that. So that makes this two strikes? reply vb-8448 2 hours agoparentprevA safer language like RUST won't help you against bad practices and poor QA processes. This is a kind of error that you should catch with automating testing, even before pushing the change to main branch. reply clwg 2 hours agorootparentNot just QA; security assurance, code reviews, static and dynamic testing, threat surface analysis, unit testing, and pentesting either didn’t exist or weren’t sufficiently applied. I have to imagine that this bug has existed for quite some time and I’d be curious to know what other input validation errors they have, considering the amount of untrusted input they evaluate at ring 0 originating from userland. reply lambdaone 2 hours agorootparentAgain, there are safe ways of doing this. For example, Wuffs exists: https://github.com/google/wuffs At the very least, big money security software companies should be parsing untrusted content with some kind of rigorouly safe approach, not just squirting it through a big pile of C/C++. And don't get me started on the whole concept of undefined behavior in those languages. To quote I. I. Rabi, \"Who ordered that?\" reply gruez 1 hour agorootparent>At the very least, big money security software companies should be parsing untrusted content with some kind of rigorouly safe approach the malformed files were updates from crowdstrike itself. It's not exactly \"untrusted content\". reply YZF 1 hour agorootparentprevFuzzing... I'd love to hear from an engineer on the project but unfortunately we're likely not to. reply lambdaone 2 hours agorootparentprevMemory-safe lanuages (for goodness' sake, even the crap I write in Python qualifies!) are the very minimum that is needed; not to use them for anything critical is simply crazy. Yes, do all the other things, but at least put out the blazing fire in your basement while you are implementing your fire-safety strategy. reply adrr 4 minutes agorootparentHow do you write a driver in Python? reply vb-8448 1 hour agorootparentprevEven with memory-safe languages you can shoot in your foots and on Windows, AFAIK, you need to stick with c/c++ for this kind of low level programming. BTW, using your metaphor, until 2 days ago they didn't even know that there was a fire in the basement, nor a basement. reply pdimitar 2 hours agorootparentprevI agree. My point was that using a language whose compiler will not allow you to build your production binary if you make a certain mistake could have been one extra line of defense and who knows, that might have prevented this problem this one particular time. But I am in full agreement with you that sloppy programmers cannot truly be helped. They just screw up and move on like nothing happened. Sigh. reply PartiallyTyped 2 hours agorootparentYou can still do unsafe, and you do need unsafe in some cases. reply lambdaone 1 hour agorootparentAnd human beings can indeed write safe \"unsafe\" code. But to do so consistently, you have to be very smart, very cautious, and somewhat lucky. reply PartiallyTyped 1 hour agorootparentIndeed, ensuring that unsafe is isolated and obeys certain semantics is a superpower that few languages have rust+kani is a good and modern way to achieve this. reply pdimitar 2 hours agorootparentprevOK, sure, but don't you think that being a world-wide antivirus vendor should have warranted a better process? reply PartiallyTyped 2 hours agorootparentOh absolutely. This is utterly unacceptable. The ease with which CS pushed willy nilly a bad build to prod in what seems to be a monophasic release is absurd. Something of this nature would have had our entire team fired. The number of phases and the thoroughness and exhaustiveness of the protocols we have to ensure we don't push bad builds would have most engineers taken aback... but we have to. With great power comes great responsibility. reply trustno2 2 hours agoparentprevYou can do bugs in any language. The problem here was monoculture and critical dependence on one supplier, not a programming language choice reply lambdaone 1 hour agorootparentYou can indeed write buggy/unsafe code in any language. But it's a lot easier to do in notoriously unsafe languages like C/C++, which for some maniacal reason we seem to have based the world's digital infrastructure on. C++ was a terrible, terrible mistake. reply pdimitar 2 hours agorootparentprevI'd argue it's several things, these two things included. To screw up so legendarily requires a concert of bad decisions. reply lambdaone 1 hour agorootparentYes. The term \"normalization of deviance\" comes to mind. Even just a phased rollout would have caught this one with just a tiny fraction of the damage observed. reply jmull 1 hour agoparentprevYour plan is to replace all the software written in C/C++ with... software that doesn't exist. It's good to criticize the current state of things, but don't pretend you have a solution. Also, do we know if rust would have helped here? Rust doesn't guarantee no crashes -- in fact, panicking (aka crashing) is the default. reply pdimitar 1 hour agorootparentFrom what I've seen, it was a NULL pointer dereferencing. Dynamic, not static, so still requires diligence even in Rust. RE: panic default, don't get fooled by hobby projects, professional Rust code always does pattern matching and does not defer to panics. The \"software that doesn't exist\" point is somewhat valid, though it's also the chicken and the egg problem, as in that not many people are working to make it happen because the current state of affairs is wrongly deemed as good enough. And it really is not. reply cmrdporcupine 2 hours agoparentprevMy understanding is this was not a case of null pointer access that could be caught by a compiler really... but of a corrupt data file making a mess all over the place... running in kernel space, where no segfault is safe. The root issue is giving privileged access to a business entity you think you can trust, but clearly can't. I'm a fulltime Rust developer, but I don't think Rust saves you here. reply bpfrh 2 hours agorootparentWouldn't a strongly typed language like rust sill catch a bad datafile? E.g. loading it would require you to setup a maximal size and a valid configuration struct? reply YZF 1 hour agorootparentIt could. We haven't seen the code but it could be something like: char *ptr = parsefile(file_we_released_without_testing); if(ptr[0]=='A') { } // BSOD loop parsefile returns NULL unexpectedly. So this style of error can be addressed by using a safe language. Or static analysis. Or code reviews. Or not doing this stuff in the kernel. Or formal methods. Or fuzzing. As someone else said you likely can't easily use Rust for Windows kernel modules/drivers. I'm sure a strong enough engineering team could do it (e.g. transpile Rust to C) but I'm not sure it's the biggest engineering problem CrowdStrike has. Microsoft has a complete tool-chain for developing these and it's usually C/C++ or assembly. reply ArtixFox 58 minutes agorootparentunhandled null in rust will still cause panic. still cause the bootloop. reply YZF 44 minutes agorootparentI'm not a Rust expert but wouldn't you pick some (\"null-safe\") type that can't be null in Rust? A reference? reply ArtixFox 24 minutes agorootparenti dont think it matters, if you have any exception in the critical boot part, you will end up with this. Rust cannot fix this. Microkernels might. reply pdimitar 2 hours agorootparentprevIf what you say is true -- OK. Then would you say that the tweet posted earlier that showed the NULL pointer access was incorrect or misleading? reply cmrdporcupine 2 hours agorootparentNull pointer access caused by bad data is entirely conceivable... esp when you overwrite parts of kernel memory with nulls. reply pdimitar 2 hours agorootparentI see. That doesn't make it better though, and OK let's forget about other languages. I mean that the least you can do before pointer dereference is just check for several bad sentinel values, NULL being one of them. Seems like a rather amateur mistake to me. reply lambdaone 1 hour agorootparentFrom that point of view, there are several million amateurs, some of them quite highly paid, out there writing terrible code. reply pdimitar 1 hour agorootparentWell, that statement is 100% true. I've been an idiot as well in the past. Happily some of us actually learn though! reply imranhou 2 hours agoparentprevWindows does not support drivers based on rust language, so perhaps may be another 5 years reply lowleveldesign 2 hours agorootparentYou can write drivers in Rust - it's just quite hard at the moment. Microsoft published metadata packages for WDK APIs and started creating samples: https://github.com/microsoft/Windows-rust-driver-samples reply lambdaone 1 hour agorootparentGood. Microsoft doing something sensible. reply bckr 2 hours agorootparentprevHow can that be possible? Are drivers not binary files? reply pdimitar 2 hours agorootparentprevRust can't compile DLLs? AFAIK it can? reply Filligree 2 hours agorootparentLook at Linux. Getting Rust to work with the kernel is a long story of defining APIs, cleaning up the C-side API to make it tenable, coding test filesystems and whatnot to make sure it all works, and getting buy-in and maintenance for all of the above. Doing the same with zero control over the non-Rust side of the kernel seems completely untenable. reply pdimitar 2 hours agorootparentI am not saying there are no challenges. I am saying that CrowdStrike does not seem to have even tried to have a better process. Rust would be only a small part of the picture; just one more layer in the security posture (a small one at that, admittedly). reply lambdaone 1 hour agorootparentExactly. It's like the brown M&Ms in the Van Halen rider; it's not that the M&M's were the problem, but that it was a test of diligence. People who don't care about detail are likely to screw up the big things just as badly as they screw up the little things. Being a multi-million dollar company and using unsafe languages today is not a good look. But everyone gets away with it because everyone else is doing it. reply cmrdporcupine 2 hours agorootparentprevThis is a kernel driver. Runs in kernel space. Intercepts syscalls. You'd definitely be fighting uphill to write it in Rust. And your code would be riddled with `unsafe` by necessity anyways. reply pdimitar 2 hours agorootparentFair enough, still Rust's unsafe is not dropping all of its guarantees. Quite a lot of them remain in place. Not saying you can't write bugs in Rust, of course -- that would be crazily delusional. I am saying they needed a better process. And I am saying that a stricter language could have improved the process a bit as well. reply adfm 2 hours agoprevThis piece was written by someone covering national security and the Biden administration for the NYT. It’s a global issue exposing vulnerabilities across the board. It’s journalism like this that’s the real vuln. Word. reply sigseg1v 2 hours agoparentAgreed. They would have never written this if they remembered to don their tinfoil headgear first. reply tomrod 2 hours agoparentprevHuh? reply enceladus06 1 hour agoprev [–] Why does IT even pay $$$ for crowdstrike? Time to uninstall it and figure something else out. Just use linux or chromeOS. reply hylaride 1 hour agoparentMission critical use of Linux still needs malware and breach detection software. It's not as simple as switching OS's. reply betaby 1 hour agoparentprev [–] People downvote you, but in the context you are mostly right. In case of airlines there is no reason to use Windows there, checking software is web-based and ChromeOS is a perfect fit there. Same goes for banks, bank tellers mostly use web browsers to access banking applications. reply adrr 1 minute agorootparentHow do you detect if your ChromeOS gets breached? Linux apps runs in sandboxes so even user level HIDs won't function. reply enceladus06 1 hour agorootparentprev [–] Probably has to do of IT wanting to keep using windows to justify its own existence ;). Web + chrome is so much better. Then just use qnx or something for embedded. Why is the actual reason that our $600k confocal microscope has to run windows? Qnx or Unix is much better for scientific and healthcare equipment. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A software update bug caused a digital meltdown on Friday, impacting airports, hospitals, and TV stations, highlighting the fragility of interconnected networks.",
      "The incident was due to human error, not a nation-state attack, and involved CrowdStrike’s software, which is intended to neutralize cyberattacks.",
      "This event underscores the need for improved digital resilience, despite ongoing efforts by the U.S. government and private sector to enhance cybersecurity."
    ],
    "commentSummary": [
      "The CrowdStrike incident has exposed vulnerabilities in American critical infrastructure, prompting debates on the use of operating systems (OS) that need frequent security patches.",
      "Experts are suggesting alternatives like Linux or OpenBSD, which are perceived to offer better security compared to Windows, which is widely used due to historical and practical reasons.",
      "The incident emphasizes the necessity for built-in security features in systems and raises concerns about the broader implications for national security and infrastructure resilience."
    ],
    "points": 128,
    "commentCount": 181,
    "retryCount": 0,
    "time": 1721488699
  },
  {
    "id": 41014089,
    "title": "Google's shortened links will stop working next year",
    "originLink": "https://www.theverge.com/2024/7/19/24201734/google-url-link-shortening-service-shut-down-support",
    "originBody": "Google/ Tech Google’s shortened links will stop working next year Google’s shortened links will stop working next year / After August 25th, 2025, all goo.gl links will respond with a 404 error. By Jess Weatherbed, a news writer focused on creative industries, computing, and internet culture. Jess started her career at TechRadar, covering news and hardware reviews. Jul 19, 2024, 9:40 AM UTC Share this story Illustration: The Verge If you ever used Google’s URL shortening service goo.gl before it was shut down in 2019, be warned — those links will stop working on August 25th, 2025. Google announced in a blog post that “the time has come to turn off the serving portion of Google URL Shortener” and that any links in the https://goo.gl/* format will respond with a 404 error next year. Ahead of the shutdown, goo.gl links will start showing an interstitial page on August 23rd, 2024, notifying users that “this link will no longer work in the near future.” This message will initially appear for a “percentage of existing links,’’ which will increase as the deadline draws closer. Google is encouraging developers to update impacted links as soon as possible, however, as this interstitial page may cause disruptions to link redirections. Here’s the interstitial page that will start popping up on goo.gl links after August 23rd. Image: Google When Google announced in 2018 that it was shutting down goo.gl, the company encouraged developers to migrate to Firebase Dynamic Links (FDL) — which has also since been deprecated. Most Popular Most Popular What is CrowdStrike, and what happened? Major Windows BSOD issue hits banks, airlines, and TV broadcasters Google solves its Pixel 9 Pro leaks by just showing the phone early Here’s how IT admins are fixing the Windows Blue Screen of Death chaos Hisense’s new TV is just like Samsung’s Frame but costs a lot less Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox weekly. Email (required)Sign up By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. From our sponsor Advertiser Content From",
    "commentLink": "https://news.ycombinator.com/item?id=41014089",
    "commentBody": "Google's shortened links will stop working next year (theverge.com)120 points by uladzislau 14 hours agohidepastfavorite55 comments treve 12 hours agoGiven how relatively easy it is to run a redirect service, and how many links this will break this is vandalism. reply usr1106 12 hours agoparentIt's in Google's interest that coming generations will find any URL as weird as we find an IPv6 address. Everybody should use their search^W profit generation engine. reply bhawks 12 hours agorootparentIndexing and deeplinking into walled gardens is a pretty fragile affair. Can search live without an open web? reply dageshi 11 hours agorootparentIf most of the knowledge retreats into a finite number of silo's they can probably afford to pay for backend data access to them. reply 6510 11 hours agorootparentprevOne or more pages will be generated but ideally a tiny info box to drape the ads around. reply bhawks 12 hours agoparentprevFor a company that couldn't have started itself without functioning hypertext media ecosystem it seems even more callous and destructive. As a Xoogler I'm very disappointed reply silon42 12 hours agoparentprevIs there a service to lookup and cache redirect service links? reply upon_drumhead 12 hours agoprev> When Google announced in 2018 that it was shutting down goo.gl, the company encouraged developers to migrate to Firebase Dynamic Links (FDL) — which has also since been deprecated. Why wouldn't they just change the backend and leave the service alive for the end users? It seems nuts to give up all that sweet sweet browsing data. reply fodkodrasz 12 hours agoparentTHIS! Seriously... if they retire it, make the backend read-only, that way it can be highly optimized, and ran with minimal costs (from a mammoth company's perspective). I don't know, make it an interview question and deploy the best answer? They put more effort into tortuting aspirants than to EOL-ing some of their cheap-ass services in a reasonable way. reply bdd8f1df777b 9 hours agorootparentThe problem is that Google infra requires everything running to be new. There is a build horizon of 6 months. Everything built with code older than 6 months is not able to run on Borg. And since Google deprecates many internal infra tools/libraries routinely, a team is required to make sure the service remains up-to-date. Google doesn't want to pay for such maintenance. reply karmakaze 4 hours agorootparentThis what I'd suspected as pressure to discontinue services that could otherwise virtually maintenance-free at low cost. reply ajb 11 hours agorootparentprevIn S3, you can implement this with a single bucket, no code at all: objects can cause redirects, using x-amz-website-redirect-location Since Google buckets don't seem to implement this feature, maybe they should point goo.gl at S3 :-) reply sofixa 11 hours agorootparentOh cool, I've done this sort of thing (mass redirects) with Lambda@Edge which allows for more flexibility, but probably costs more. reply mg 12 hours agoprevEvery time I change a ULR (or a set of URLs), I put a test for the redirect into my end-2-end tests, which run once per day. So I know all my URLs will work forever. I have not thought about it for years now. Just checked for my first ever Show HN from 10 years ago: https://news.ycombinator.com/item?id=7465980 The URL has long changed, but the redirect still works. Phew :) So all seems to be good. Here's to the next 10 years! Google should do the same. Set up a seperate server for the redirect service itself. And then I guess they have multi project end-2-end tests running somewhere in their infrastructure. Just add testing this service and thats it. Amount of work per year to keep it up should be less than an hour, right? reply dockerd 12 hours agoparentIt will easily take 1-2 engineers to maintain it in Google. Why 1-2 engineers? Security patches / Internal service deprecation / Migration / Use of deprecated dependency / etc reply bikingbismuth 12 hours agorootparentThe sad truth is that no one is getting a promotion to staff for just maintaining a service. I wish this wasn’t so. At a previous job I had a VP tell me that my team was like a public utility and I took that as a compliment. Later my boss explained they were saying that they only noticed my team when something was broken. Sort of explained my lack of career progression in retrospect. reply mmikeff 10 hours agorootparentThey could outsource this to someone only too grateful to keep it running for less than the cost of an internal engineer reply fefferkorn 9 hours agorootparentThis is where you start your own company and sell \"maintenance\" to your company you work for. reply gorgoiler 11 hours agoprevIt is an old code but it might be more appropriate to fess up and say the resource has been deleted? 410 “Gone” https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/410 But alas, even more apt: 417 “Expectation failed” https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/417 reply treve 3 hours agoparentExpectation failed is specifically a response to an unknown value for the Expect header. reply Havoc 10 hours agoprev> migrate to Firebase Dynamic Links (FDL) The announcement of “we’re breaking your stuff” contains an appeal to trust them on next round? They must think their clients are complete morons reply nvalis 9 hours agoparenthttps://firebase.google.com/support/dynamic-links-faq reply qwertox 5 hours agorootparentFor those not wanting to click the link: > On August 25th, 2025, Firebase Dynamic Links will shut down. All links served by Firebase Dynamic Links (both hosted on custom domains and page.link subdomains) will stop working and you will no longer be able to create new links. reply imadj 11 hours agoprevThe real reason is probably maintenance due to some hidden costs like conflicting infrastructure and they couldn't justify migrating it. Related discussion (2 days ago): https://news.ycombinator.com/item?id=40998549 Discussion of the previous announcement in 2018: https://news.ycombinator.com/item?id=16719272 reply raverbashing 11 hours agoparentHonestly I bet they could have 2 interns porting the thing to Google App Engine and then migrate the database A link shortener, as much as it has analytics and such in the background, is not rocket science. reply imadj 11 hours agorootparent> I bet they could have 2 interns porting the thing to Google App Engine and then migrate the database How can you possibly have this assessment without looking at the code/infra? There are many things that affect cost beyond the visible features. The project isn't in a vacuum. It's interlocked with their other services infrastructure. You can judge Google however you want, but they're not stupid or amateurs. These types of announcements immensely damages their image and affect their customers, if they could avoid it easily as you imagine, why would they not? They've built the service and run it for many years for billions of people. A more realistic guess would be that for whatever reason, the price is higher than what's visible on the surface and they're not willing to pay it. reply acdha 1 hour agorootparentIf they used AWS, this would have no code and no maintenance: host the bucket out of S3 and enable redirects. GCP doesn’t support that, but they could get pretty close using a cloud function - stick with the Python stdlib & SQLite or DBM for the mappings or use an Apache redirect map, and you’d have many years before you need to touch it again. reply glic3rinu 11 hours agorootparentprevyou are assuming hidden costs, I am assuming hidden incentives. It’s not that they are stupid or incompetent, but bad incentives within the org can and do produce stupid outcomes. reply tialaramex 10 hours agorootparentprevWhat's happened here is that you've erroneously assumed there's a good reason. It's fun to hold nonsense like this up against testimony from the ministers and officials at the Horizon enquiry, all of whom can be relied upon to say that \"with the benefit of hindsight\" obviously what they did was wrong but insist that they were too stupid to realise there was a problem and thought they were powerless to do anything. Remember on average the other humans are just as stupid and lazy as you are. Most often there aren't \"good reasons\" for what happened, if there are even reasons at all. reply raverbashing 11 hours agorootparentprevJust to be clear, I'm not saying they can port everything to it, but only the basic functionality to not let the links die (then progress with it) > These types of announcements immensely damages their image and affect their customers, if they could avoid it easily as you imagine, why would they not? You're assuming they care. And the answer of how much they care is: can this be used to further my (that is, an engineer or manager) promotion? If not then no Google has become dysfunctional reply np_tedious 11 hours agoprevAre links generated from Google apps like https://maps.app.goo.gl/xxxxxxx going to continue working? reply tabbott 12 hours agoprevI wonder if there's a story here involving a URL shortener service having hidden costs? I can imagine there being something in the abuse space that makes it feel more expensive than just the hosting costs to operate. reply hnfong 11 hours agoparentProbably career product managers finding it untenable to write self-reviews with such a low impact, low maintenance product... And if nobody wants to take it on... reply hiddencost 12 hours agoparentprevGoogle was killed by growing too large. Google the company was designed with really high coordination requirements, which has made the marginal coordination cost of adding a new engineer higher than the value they add. reply ehPReth 12 hours agoprevit's so sad this multi-trillion dollar company can't spare some resources to serve a some 301s. reply j16sdiz 12 hours agoprevMany years ago, there was an industrial group (?, maybe just a campaign.. can't remember the details) promised to provide protection/transfer service if one of their member shutdown. Tried to search the news, can't find any reference to that. reply jszymborski 12 hours agoparent301works.org might be what you mean. https://archive.org/details/301works reply ChrisArchitect 12 hours agoprev[dupe] Discussion on official post: https://news.ycombinator.com/item?id=40998549 reply olliej 12 hours agoprevNote that when Google made a blog post telling people to migrate from goo.gl to the also now deprecated Firebase Dynamic Links, the post states explicitly[1]: \"While most features of goo.gl will eventually sunset, [bold]all existing links will continue to redirect to the intended destination.[/bold]\" The [bold] section is bold in the original post. [1] https://developers.googleblog.com/en/transitioning-google-ur... reply beardyw 8 hours agoprevMaybe the links may direct to undesirable/illegal content either now or in the future. Perhaps just CYA. reply beardyw 8 hours agoparentThat said, I visit old discussions (5-10 years) regularly and most of the links are dead anyway. reply ambyra 12 hours agoprevThey could charge for custom video urls. https://youtu.be/bestVideo1 for $1. I bet a lot of people would pay reply sexy_seedbox 10 hours agoparentThey could also charge for premium, junk-free, search engine results competing with Kagi. reply iJohnDoe 1 hour agoprevLike many have said, its a shame they refuse to maintain minimal requirements to keep the links working. Google offers cloud services. It’s like AWS saying they won’t spare some ec2 instances to keep some links working. If Google knew how to use their own cloud products then they could deploy some instances, failover, and monitoring and leave it alone, and also dogfood their own cloud products. reply agent327 10 hours agoprevIs it possible to download the entire database? That way we could undo the breakage using a browser extension. reply fragmede 9 hours agoparentinternet archive is on the case! run their warrior VM to help with the cause. https://wiki.archiveteam.org/index.php/ArchiveTeam_Warrior choose urlteam project reply rawling 12 hours agoprevhttps://news.ycombinator.com/item?id=40998549 reply hcks 9 hours agoprevCan someone mourning can me one single valuable link that will be lost? reply ggm 13 hours agoprevSeems google doesn't care about pURLs reply Jyaif 11 hours agoprevHaving products scale through time is an engineering problem, and they seem to not be able to recognize it as so. As long as they don't understand this, they won't be able to expand their product offering (and thus Revenue) significantly faster than their headcounts. reply Gud 8 hours agoprevYet another reminder to avoid the offerings of these mega corporations. reply chris_wot 11 hours agoprevYup, yet another service people rely on being shut down. Don’t rely on Google. reply sllabres 12 hours agoprev [–] That is sad, especially because I think that it is not a service that would take that much effort to keep up. 've seen things you people wouldn't believe... Attack ships on fire off the shoulder of Orion... I watched C-beams glitter in the dark near the Tannhäuser Gate. All those moments will be lost in time, like tears in rain... reply sulandor 12 hours agoparent [–] \"a candle that burns twice as bright, burns half as long\" reply labster 11 hours agorootparent [–] \"Fool me once, shame on you; fool me twice, shame on me.\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google’s shortened links (goo.gl) will stop working after August 25th, 2025, resulting in a 404 error for all such links.",
      "Starting August 23rd, 2024, goo.gl links will show an interstitial page warning users about the upcoming shutdown.",
      "Google advises developers to update affected links to avoid disruptions, noting that the goo.gl service was initially shut down in 2019 with a recommendation to switch to Firebase Dynamic Links, which has also been deprecated."
    ],
    "commentSummary": [
      "Google's URL shortening service will cease operations next year, causing frustration among users who rely on it for redirects.",
      "Critics highlight Google's pattern of discontinuing services, attributing the decision to maintenance costs and internal policies.",
      "Users suggest alternatives like AWS for redirects and propose simpler solutions or outsourcing to keep the service functional."
    ],
    "points": 120,
    "commentCount": 55,
    "retryCount": 0,
    "time": 1721450175
  },
  {
    "id": 41011779,
    "title": "Academics shocked after T&F sells access to their research to Microsoft AI",
    "originLink": "https://www.thebookseller.com/news/academic-authors-shocked-after-taylor--francis-sells-access-to-their-research-to-microsoft-ai",
    "originBody": "Too Many Requests(6)",
    "commentLink": "https://news.ycombinator.com/item?id=41011779",
    "commentBody": "Academics shocked after T&F sells access to their research to Microsoft AI (thebookseller.com)111 points by chbint 21 hours agohidepastfavorite80 comments perihelions 19 hours agoInformation should be free. We subsidize scientific research to the tune of trillions of dollars—not to support to livelihood of grad students; not to create jobs in the publishing industry; but on the theory the fruit of that research has planet-wide benefits. We throw money hand-over-fist at scientific research because we view that as investing in civilization. If we believe that, and we believe research progress builds on top of other research, then the conclusion is we ought to minimize the friction of discovering other groups' research results, and maximize their availability. Make the act of research as easy and painless as possible. Scientific research output should be free, universally, without hindrance. It's myopic to try extract wealth from this public good by siloing it, by toll-gating access to it. Like barricading a public highway with toll-booths every 500 meters: it's a myopia that's blind to the public-good value of infrastructure—a myopia of greed that's a universal drain on public wealth, for some petty local optimization. If you obstruct ML models on some financial profit theory, you're obstructing not only the ML entities; you're obstructing the thousand researchers downstream who stand to benefit from them. You're standing the in road blocking traffic, collecting tolls; you've not only stopped the vehicle in front of you, you've stopped a thousand more stranded behind it. It is a public nuisance. reply sseagull 17 hours agoparent> We subsidize scientific research to the tune of trillions of dollars Do you have a citation for that? Annual budget of the NSF is about $8 billion, covering all of science. Total NIH is $45 billion, and that includes other things. DOE Office of Science is about $8 billion. I think both of those cover a vast majority of grants funded in academia. I suppose if you factor in some sort of tax breaks you could increase that, but I doubt you would get to trillions. (I say this because I feel people drastically overestimate the amount of funding science gets in the US. For example, compare the above numbers with the profits of big tech companies) reply sigmoid10 9 hours agorootparentThose are just government programs (and even there it's missing a lot). Most research in the US is funded by the industry nowadays. But you can still easily make the case that the US government alone spent more than one trillion dollars on research in the last decade, even without looking at other countries [1]. And the EU spends a lot on the federal government level as well. [1] https://www.stlouisfed.org/on-the-economy/2018/may/rd-busine... reply poincaredisk 16 hours agorootparentprev>I think both of those cover a vast majority of grants funded in academia In the US. Also even in the US, is NASA included in one of those? Looks like it costs around 20B yearly. There may be more from the US, and then there's, you know, the rest of the world combined too. reply kldrt 19 hours agoparentprevThat's kind of orthogonal to the issue here. Many academics would not mind if their research is free. They do mind very much if some entity takes their research and republishes it without attribution. reply lolinder 15 hours agorootparentWhy is that orthogonal? If their research is truly free then everyone has permission to republish it all the time. If it was done with public funds then it belongs in the public domain, and if it's in the public domain then everyone can do everything they want with it. If they're not comfortable with that, then they aren't comfortable with their research being free-as-in-freedom. reply PhilipRoman 12 hours agorootparentDo you not consider MIT licensed software \"truly free\"? Note the important part: \"without attribution\". reply lolinder 5 hours agorootparentNot free enough for publicly-funded research. I don't think copyright should apply at all to research that taxpayers paid for, it should instantly be public domain. reply Delk 2 hours agorootparentIrrespective of copyright, the moral right to attribution is central in all kinds of academic research. I agree that research information should be free of paywalls but that doesn't remove concerns about proper attribution. You can think that researchers shouldn't expect attribution either. I'd disagree, but in any case, removing that would significantly change the implicit social contract of academic research. Journal publishers and tech companies shouldn't be able to do that unilaterally. reply matrix87 2 hours agoparentprev> myopia of greed that's a universal drain on public wealth, for some petty local optimization. It's all a question of who gets to do this first, until then everyone who doesn't gatekeep is a sucker. You think M$ won't do this? reply skybrian 16 hours agoparentprevThe books themselves aren’t free, though scholars might get free access through a library. reply 93po 16 hours agoparentprevFrom the article: >\"We are at a crossroads in the production and dissemination of research knowledge, and in my view the biggest problem with this deal is the reduction of academic research into raw content from which data can be extracted and repackaged as knowledge,\" Clemens said. I'm sorry, what? The problem researchers have is their research being used as a resource for general knowledge? Do they only want their research to be helpful for other research and never have applicability outside academia? reply carbocation 20 hours agoprevThe only thing I'm 'shocked' by is the idea that anyone needs to pay to access my academic writing for model training. I would hope that using my academic writing to train models would be considered fair use. reply anonym29 19 hours agoparentThey don't. Authors can publish to Arxiv and other public pre-print servers. Consumers can boycott parasitic middle-men with certain websites... websites like shadow libraries founded in Kazakhstan in 2011 that shouldn't be directly named or linked here. reply dr_dshiv 19 hours agorootparentScihub? Libgen? Why not name? reply 93po 16 hours agorootparentthey mean scihub i believe, probably because they have strong positive feelings about intellectual property and scihub doesn't reply anonym29 7 hours agorootparentAbsolutely incorrect. I am strongly supportive of such websites. This is a public forum and it is being constantly scraped and datamined. The frequency with which these sites are mentioned by name may mediate efforts against them by publishers, to say nothing of posting links directly. reply BenFranklin100 20 hours agoparentprevMy thoughts exactly. See my comment below. The whole purpose of being an academic is to publish and disseminate knowledge. reply akira2501 16 hours agorootparentToo bad there isn't a GPL for science. You can use my work, but if you incorporate it into a for profit work, you may be required to provide the results of your _continued research_ back to the public. I mean.. if dissemination of knowledge _truly_ is the end goal here.. and not just giving a free corporate lunch out because the job market is the broken and dysfunctional. reply BenFranklin100 15 hours agorootparentAbsolutely not. I want my work to be used in any way that is possible, including by for-profit enterprises. If I required companies to provide results back to the public domain, most wouldn’t touch my work and it would have less impact. And you are confused. The immediate goal/purpose of academic work is to create knowledge. I said nothing about the underlying purpose of this goal. The ultimate purpose of this goal is to provide the foundation for a better society, which includes the knowledge being used to build better technologies, products, and processes by for profit companies. The antipathy towards profit is one of the many great contradictions of the Left. They will willingly make the public worse off in order to prevent a private profit. reply akira2501 15 hours agorootparent> And you are confused. You literally said \"The whole purpose of being an academic is to publish and disseminate knowledge.\" You are now saying this means something different than it's apparent reading? > which includes the knowledge being used to build better technologies, products, and processes by for profit companies. Why then do you think these entities are entitled to the output of academics without paying for it, or as a compromise, contributing back to the pool of global knowledge in kind? What I'm getting at is the way you and the GP have framed this position is that you might then consider it wrong for an academic to charge for access to their work or for themselves to start a business and profit from it. That seems off to me. > The antipathy towards profit is one of the many great contradictions of the Left. This is an oversimplification of an easily understood nuance. For profit companies are obviously fine but we might also want a measure to determine if any one entity is contributing more to society than it is extracting from it. In lieu of this perhaps forcing companies to contribute back to the same body of public knowledge that they obviously benefit from is not some great contradictory injustice. > They will willingly make the public worse off in order to prevent a private profit. Since business have no natural right to exist we, as people, have every right to exclude some of them from operating or even existing at all. Is my aversion to slavery actually a disguised antipathy towards profit? reply skeledrew 15 hours agorootparentprevAnd yet, without something like the GPL, you can be sure that the majority of improvements would eventually be sealed off behind doors and high walls. The whole reason for-profits wouldn't want to touch your work is because they would make _less_ profit, not _no_ profit. The real impact, as we see now (such as in BigPharma[0]), is things being more expensive than they need be with cheap alternatives being actively suppressed. [0] https://www.aarp.org/politics-society/advocacy/info-2019/gen... reply winddude 20 hours agoprevAren't they also one of the academic publishers that's been criticized for charging for access to articles, and the authors don't get anything from the publication/distribution? reply xyst 19 hours agoparentWhich is why I have never paid for an academic article from a publisher. I have been e-mailing authors directly for access to the article and have been successful most of the time. Maybe once or twice, the e-mails go unanswered. In one case, the primary author had died (a subsequent search linked a news article showing a skiing accident). In another case, the author had simply moved to a different university thus the e-mail in the paper was outdated. reply wsay 20 hours agoparentprevThis is pretty much a standard model in academic publishing reply kkylin 19 hours agorootparentExcept for open access models where someone -- usually the authors, but sometimes universities or other institutions -- pay so that the article can be accessed by the public for free. (Authors usually don't pay out of pocket but using grants, and OA journals usually offer some sort of \"financial aid\" for those unable to pay.) reply winddude 19 hours agorootparentprevTrue, and unfortunate, other than arxiv, and openreview reply BeetleB 19 hours agoparentprevExcept for the few open access journals out there, this is pretty much how all academic journals work. reply shusaku 19 hours agoparentprevIt’s a little hard to tell from the article, but I think this is about books not research articles, and that the authors have some amount of revenue sharing. reply asdasdsddd 19 hours agoprevDoes publish mean something else in this context? I thought publishing means that anyone can have access to your research. reply DonsDiscountGas 19 hours agoparentAnybody willing to pay for it, yes. Typically tens of dollars per article or thousands/year for a subscription. It's not like scientists get any of that money though, it's just one more case of publishers squeezing scientists. reply astromaniak 16 hours agorootparentIs there a legal way to bypass those collectors. It should be, right. After all most research is done on public money. reply trogdor 3 hours agorootparentEmail the author of the paper and ask for a copy. My success rate with that method is 100%. “Greetings, I am interested in reading your paper, “[title of paper],” but I have only been able to find paywalled versions. Would you please send me a copy? Thank you, trogdor” reply klyrs 1 hour agorootparentBack in the day, this was the way to get really nice print copies by mail, and a handwritten letter by the author. reply bell-cot 18 hours agoparentprevThink \"publish\" in the 1940's magazine sense - which is pretty much the distribution model for all \"traditional\" academic journals. If Famous_Author publishes an article in The Saturday Evening Post - maybe you're a subscriber to The Saturday Evening Post. Or you head to a brick-and-mortar library which subscribes, to read the article there. Or you pay Cover_Price, in cash, to a news stand or drug store or whatever, to get your dead-tree copy of the issue in question. reply BeetleB 19 hours agoprevFinally! We've solved the problem of having to pay $30/article to get access. We'll just query the LLM! reply bionhoward 18 hours agoparentcan’t use that to develop AI, it’s against the terms of use reply BenFranklin100 20 hours agoprevI’m kind of ok with this? I’ve written and had book chapters and research articles published. I never thought I was in any sort of position to restrict access. Publishing is about getting it out there. Attribution might be an issue, but that is a separate conversation and perhaps dealt with, if possible, by having LLM’s cite sources more accurately. I have not kept up with the latest on LLM’s and licensing, but I’m curious: are scientific papers accessible to LLMs? Honestly, a bigger societal loss in my view is publishers like Elsevier restricting LLM access to research articles, rather than being too permissive. I could not care less if Elsevier makes a little bit of money in the process. reply JSDevOps 20 hours agoprevIf no one is going to learn from academic papers/data what’s the point in doing it? People care less and less about the impact more about how much money it’ll make them. If that’s all you want from it then be honest but don’t complain when someone makes more money from something than you. reply ladfgym 19 hours agoparentThe point of academic papers is that HUMANS learn from it. The point is not that machines generate a mashup that masquerades as a probability distribution and companies sell that mashup together with ads (the latest predictable development). I see no indication that the affected academics are concerned with the money that their papers make them. Most want to give them away for free to HUMANS. reply nostrebored 19 hours agorootparentExcellent. More humans will have access to academic paper information in a form that's useful to them now :) reply SuperNinKenDo 13 hours agoprevThe whole academic publishing system is rotten to its core. Rent seekers living off the labour of underpaid academics and selling the product back to them, all while tax-payer money props up the racket. reply andrewstuart 19 hours agoprevHas anyone made a marketplace to sell data to AI and at the same time explicitly exclude data from being accessed by AI? reply xyst 19 hours agoparentsomething like this already exists. they are called \"data brokers\" reply j_crick 20 hours agoprevsurprised_pikachu.jpg reply Der_Einzige 19 hours agoprevThe reality is that AI publishers (NeurIPS, ACL, related) all do proper academic publishing norms, and they are now forcing the rest of the world to follow them or have their content laundered in the form of LLMs trained on it. Good. reply blackeyeblitzar 20 hours agoprevWe need regulations that enforce positive consent. Otherwise, this will keep happening. reply nequo 20 hours agoparentWe need to eliminate the rent seeking behavior of academic publishers. They do not fund the research, do not fund the peer review of the research, but pocket huge profit margins by charging others[1] for access to it. Edit: [1] By others, I mean our university libraries and any poor soul who does not have membership in one. Today, universities pay to access their own research. reply pdonis 20 hours agorootparentWe need to eliminate the publishers period. Publishing on the Internet is free. reply chefandy 20 hours agorootparentPublishers do a lot more than print things out. I'm not saying that what they do is good, but \"let's just put it on the internet\" doesn't replace publishers any more than rideshares replace long-haul trucking. reply impendia 19 hours agorootparentWhat, exactly, is this \"a lot more\"? I am an academic who publishes regularly, and I'm having difficulty figuring this out. They clearly employ lots of bureaucrats, middle managers, and lawyers. They are constantly trying to make sketchy side deals with third parties, as T&F did. They employ copyeditors, who in theory (and sometimes in practice) improve papers but who very often are totally incompetent. They maintain large databases of something-or-other, presumably so they can sell out my data. They maintain complicated systems to submit and review papers, when it seems that email would suffice, presumably so they can track all this metadata they want to sell. Oh! And they lobby the government for their own selfish ends: https://www.theguardian.com/science/political-science/2018/j... Seriously. Imagine all of them up and vanished tomorrow. Would anybody miss them? reply chefandy 18 hours agorootparentAgain, I didn't say that what they do is the right way to do it, but what about discoverability? Would researchers just submit their document to every library in the world? If no entity is vetting these papers in a basic way, what's to prevent the entire channel from being taken over with spam? Is it done entirely by URL, and what's to stop the cited content from changing? What if they lose their domain? Is the similar domain that pops up with similar content the same author or someone that just ripped the site and is going to start serving up ads as soon as everyone switches their URIs? Or maybe we'd have libraries go out and actively retrieve this stuff rather than passively accepting it-- Would they monitor... mailing lists? What if someone posts a timely and incredibly controversial paper that lists a respected authority that just died as its author? What's to stop an author that published a bullshit paper from passively claiming it was a forgery? Would you, currently, give equal weight to citations published in the New England Journal of Medicine and a google doc with a bunch of citations to other google docs and personal websites? If we have centralized locations for this work, how would they not be publishers? Beyond that, current URL citations are a fucking mess. Link rot is a huge problem and most of the people that make citations in papers have no idea how to mitigate it. Discoverability, relatively stable hosting, and the most base-level vetting and credibility are problems that they address, even if they do so poorly and greedily. If they went away tomorrow, so would the databases of work they manage, the APIs libraries use to query them, their mechanisms to confirm that whoever said they published something actually published it... Publishers don't have to be the answer-- I think a lot of these problems could be resolved through field organizations, librarianship and university organizations, but what's the likelihood that of universities and field organizations will try to monetize it and essentially become publishers in all of the worst ways? reply pdonis 11 minutes agorootparent> what about discoverability? That's what Google and other search engines are for. arxiv.org even has its own search function. Nobody depends on journals for this. > Would researchers just submit their document to every library in the world? No, just post it on a known website like arxiv.org. Then anyone with Internet access can get to it, and they don't even have to go to a library. > If no entity is vetting these papers in a basic way You do have to have academic credentials to post papers on arxiv.org. Not only that, but plenty of papers published in journals turn out to have been bogus--not just wrong (many papers published in good faith later turn out to be wrong, that's part of science) but based on faked data, and nobody in the journal review chain spotted it. So the supposed vetting that journals provide doesn't really add any value anyway. > What if they lose their domain? arxiv.org is supported by many academic institutions and foundations. They're not going anywhere. > If we have centralized locations for this work, how would they not be publishers? By being arxiv.org or something like it. In other words, there is already an existing example of a non-publisher that addresses all the problems you say journal publishers address--and does it much better. reply skeledrew 14 hours agorootparentprevThe Internet Archive et al alleviates the majority of those concerns. And it isn't publishers in and of themselves that make a work credible, but the peer reviewers. I can easily see some extra function on IA that allows verified users, some of whom will be peers, to formally review and endorse works. Want to find high quality works? Search and filter by number of endorsing reviewers with related works on their profile. reply FireBeyond 14 hours agorootparentprev> the most base-level vetting and credibility are problems that they address No they don't. They get that for free too, with \"volunteer editors\". Stable hosting and gatekeeping, that's about it. reply pdonis 9 minutes agorootparentStable hosting--which we, the taxpayers, who already paid for the research that was published, then have to pay again to get access to, unless we're affiliated with an academic institution, in which case the academic institution is paying for access to papers that the taxpayers already paid for. musicale 15 hours agorootparentprev> Seriously. Imagine all of them up and vanished tomorrow. Would anybody miss them? Probably not much. Arxiv works pretty well. Usenix is already open access. NSF-funded research already has to comply with their 12 month open access policy (zero months would be better though.) I don't see many good reasons for journals to stick with non open access publishers. reply richardw 19 hours agorootparentprevThey’re branding experts. Getting published by Nature is far more valuable to someone compared to (all the other options). Humans prize exclusivity. We’ll hand our time and resources to people who can give us a tiny bit of it. The harder we have to work, the better. A fancy watch. A low production supercar you can’t buy unless you have 5 of the same marque already. All the same broken stupid pattern. reply pdonis 19 hours agorootparentprevIf what they do isn't good, why would we want them to keep doing it? reply malshe 20 hours agorootparentprevAs an academician I wholeheartedly support this. Some journals are now charging thousands of dollars to make our work open-access. This is an industry run by sociopaths. reply constantcrying 20 hours agoparentprevPublicly funded science shouldn't be \"published\" by some corporation, it should go directly into the public domain. Having a totally artificial middleman makes no sense when basically all publishing happens digitally. And the public deserves full access to the science they have funded. reply MaxBarraclough 9 hours agorootparentThat would be the (oddly named) green open access model. https://en.wikipedia.org/wiki/Open_access#Colour_naming_syst... reply kalfHTA 20 hours agoprevThis is a rotten thing to do by Taylor & Francis. Humans are treated as expendable pawns to serve the capital and the machine. We need new publishing models with strict copyright protections that protect against theft. Academics should run their own publishing houses as a cooperative. reply freeone3000 20 hours agoparentThe entire purpose of academic publishing is to widely distribute knowledge. The co-op would break doen the absolutely awful publishing fees to be sure, but… I’m not sure what “theft” you feel took place here. reply ladfgym 19 hours agorootparentAnd the distributors need to be rich mega-corporations who collect a fee for selling you back your own works? reply nostrebored 19 hours agorootparentThey're not selling you back your works, they're selling you back a distillation of them. Hopefully one that's useful for a particular context. reply SoftTalker 19 hours agorootparentprevWho pays for the peer review? Who prevents the publication of nonsense, pseudo-science, cherry-picked findings, etc. Granted you may argue that is done poorly today but it certainly doesn't happen for no costs. reply hilbertseries 19 hours agorootparentYou do realize peer reviewers are also unpaid. reply SoftTalker 19 hours agorootparentNo I didn't realize that. Why does anyone do them, and what real care do they take when they are doing it for free? reply jacoblambda 19 hours agorootparentSo you get a few benefits from being a reviewer but it's pretty limited. One benefit is that you generally get free access to the journal in question for a month or two while you are reviewing. So if you are regularly reviewing papers, you get free access to that journal. If you are part of a big org you probably already have access but it is a nice little benefit for people outside big academic orgs. You can also include it on your CV if you want and there are a variety of small recognitions you can get for doing it. And some publishers will also give you a fairly substantial discount (25+%) on texts purchased through that publisher as well as similarly substantial discounts on publishing and editing services provided by the publisher. Sage does this for example and their services include things like translation services, editing, infographic & artwork design, and animating video summaries for papers. They seem like they are quite nice services so getting discounts on them would probably be a good selling point for some people. Another reason is just because you'd be reading the journal anyways and you don't mind doing a deep dive every once in a while on a random paper in your field. This reason is mostly a \"well why not I'm already spending the time\" kind of reason but it's a decent justification. And the main reason is just because it's kind of expected of a lot of PhDs and being good at reviewing is a useful skill to have, especially if you are going to eventually be someone else's advisor or just in general if you want to be able to effectively critique your own works and the works of those around you. Being a reviewer for at least some times means you know the system and you can help keep your own papers and the papers of your colleagues from getting rejected for dumb reasons or oversights. reply BeetleB 19 hours agorootparentprev> Why does anyone do them, It's part of the culture. I write a paper. I want to get it published. Journals need someone to review it. If I refuse to peer review others' papers, then in principle people may refuse to review mine and we both lose. Sure, it'd be nice if publishers passed some of the profits to both authors and reviewers, but that would create other perverse incentives. > and what real care do they take when they are doing it for free? Not much. They sometimes sit on it for months before reviewing it. reply anonymousDan 18 hours agorootparentprevAt least in my field, being on the program committee for top conferences is prestigious and can help with promotion, increases your visibility and profile in the community, is a good networking opportunity with other experts in the field, and for early career researchers provides a valuable perspective on how other experienced researchers evaluate research. The latter is less important in cases where conferences publish all reviews. reply monkpit 18 hours agorootparentprevWho would pay them, and would you trust their review if they were paid? reply 93po 16 hours agorootparentprev> Who prevents the publication of nonsense, pseudo-science, cherry-picked findings, etc. no one, all of that gets published constantly reply lolinder 15 hours agoparentprev> We need new publishing models with strict copyright protections that protect against theft. No, we need new publishing models that release the publicly-funded information into the public domain. If tax dollars paid for the research then there should be no copyright on the results of that research at all, owned by the publisher or authors. This wouldn't represent a change for the authors anyway, who already are accustomed to having to sign away copyright (hence this story). The problem in this instance isn't that the papers are being fed into machine learning models, the problem is that the publisher is extracting a check for the privilege. reply Terretta 16 hours agoprev [–] Why is a researcher called a creator in this article? Are they not a fact discoverer or truth revealer? It's unclear to me researchers should “own” truths prior research and public patronage enabled them to unearth. // note: research != invention, i.e., Space X experimenting until systems and machinery can land a rocket on a barge is not “research”, but testing and documenting characteristics of fuels in a vacuum as the environment swings from -100C to 120C is reply poincaredisk 16 hours agoparent>Space X experimenting until they can land a rocket on a barge is not “research Why? What else could it be if not research? reply Terretta 4 hours agorootparent> what else could it be if not research? it could be experimentation and learning about your own invented system and mechanic, not uncovering of existing truths the word \"search\" makes this more clear, the answers research finds already exist, we just didn't find them yet. that is not the case for inventions that don't work, and need to be experimented with until they work. there may, certainly, be some incidental research discovery while attempting to make an invention work (e.g. if Space X had hit on something new about physics itself that had been causing their prior experiments to fail) -- but nobody \"owns\" such truth discovered, while of course they own their invention that works as a result of applying known truths to a creation. reply sam_bristow 16 hours agorootparentprevThe usual pairing is research and _development_ isn't it. reply svnt 14 hours agoparentprev [–] I’m not sure why you’re being downvoted, it is a valid and important point. Researchers discover empirical structure. They don’t create it. It isn’t art. The article seems to want to bring rights holders together in a way that isn’t valid. A scientific paper is normative. Its construction is closer to a pull request than artwork. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Academics are frustrated with Taylor & Francis for selling access to their research to Microsoft AI, arguing that publicly funded research should be freely accessible to maximize its benefits.",
      "Critics claim that charging for access creates barriers and hinders scientific progress, emphasizing the importance of proper attribution and the moral right to credit.",
      "The situation underscores the ongoing tension between open access and traditional publishing models, which often profit from publicly funded research."
    ],
    "points": 111,
    "commentCount": 80,
    "retryCount": 0,
    "time": 1721426035
  },
  {
    "id": 41013198,
    "title": "Technical Details on Today's Outage",
    "originLink": "https://www.crowdstrike.com/blog/technical-details-on-todays-outage/",
    "originBody": "Technical Details: Falcon Content Update for Windows Hosts July 20, 2024 |CrowdStrike |Executive Viewpoint What Happened? On July 19, 2024 at 04:09 UTC, as part of ongoing operations, CrowdStrike released a sensor configuration update to Windows systems. Sensor configuration updates are an ongoing part of the protection mechanisms of the Falcon platform. This configuration update triggered a logic error resulting in a system crash and blue screen (BSOD) on impacted systems. The sensor configuration update that caused the system crash was remediated on Friday, July 19, 2024 05:27 UTC. This issue is not the result of or related to a cyberattack. Impact Customers running Falcon sensor for Windows version 7.11 and above, that were online between Friday, July 19, 2024 04:09 UTC and Friday, July 19, 2024 05:27 UTC, may be impacted. Systems running Falcon sensor for Windows 7.11 and above that downloaded the updated configuration from 04:09 UTC to 05:27 UTC – were susceptible to a system crash. Configuration File Primer The configuration files mentioned above are referred to as “Channel Files” and are part of the behavioral protection mechanisms used by the Falcon sensor. Updates to Channel Files are a normal part of the sensor’s operation and occur several times a day in response to novel tactics, techniques, and procedures discovered by CrowdStrike. This is not a new process; the architecture has been in place since Falcon’s inception. Technical Details On Windows systems, Channel Files reside in the following directory: C:\\Windows\\System32\\drivers\\CrowdStrike\\ and have a file name that starts with “C-”. Each channel file is assigned a number as a unique identifier. The impacted Channel File in this event is 291 and will have a filename that starts with “C-00000291-” and ends with a .sys extension. Although Channel Files end with the SYS extension, they are not kernel drivers. Channel File 291 controls how Falcon evaluates named pipe1 execution on Windows systems. Named pipes are used for normal, interprocess or intersystem communication in Windows. The update that occurred at 04:09 UTC was designed to target newly observed, malicious named pipes being used by common C2 frameworks in cyberattacks. The configuration update triggered a logic error that resulted in an operating system crash. Channel File 291 CrowdStrike has corrected the logic error by updating the content in Channel File 291. No additional changes to Channel File 291 beyond the updated logic will be deployed. Falcon is still evaluating and protecting against the abuse of named pipes. This is not related to null bytes contained within Channel File 291 or any other Channel File. Remediation The most up-to-date remediation recommendations and information can be found on our blog or in the Support Portal. We understand that some customers may have specific support needs and we ask them to contact us directly. Systems that are not currently impacted will continue to operate as expected, continue to provide protection, and have no risk of experiencing this event in the future. Systems running Linux or macOS do not use Channel File 291 and were not impacted. Root Cause Analysis We understand how this issue occurred and we are doing a thorough root cause analysis to determine how this logic flaw occurred. This effort will be ongoing. We are committed to identifying any foundational or workflow improvements that we can make to strengthen our process. We will update our findings in the root cause analysis as the investigation progresses. 1 https://learn.microsoft.com/en-us/windows/win32/ipc/named-pipes Tweet Share Related Content To Our Customers and Partners Statement on Falcon Content Update for Windows Hosts CrowdStrike Unifies Threat Data and AI for Next-Gen Managed Detection and Response Categories Cloud and Application Security 95 Counter Adversary Operations 173 Endpoint Security & XDR 306 Engineering & Tech 77 Executive Viewpoint 156 Exposure Management 80 From The Front Lines 189 Identity Protection 35 Next-Gen SIEM & Log Management 84 Public Sector 36 CONNECT WITH US Featured Articles Our Statement on Today’s Outage July 19, 2024 Statement on Windows Sensor Update July 19, 2024 CrowdStrike Unifies Threat Data and AI for Next-Gen Managed Detection and Response July 15, 2024 CrowdStrike Named a Customers’ Choice in 2024 Gartner® Voice of the Customer for Endpoint Protection Platform Report July 11, 2024 SUBSCRIBE Sign up now to receive the latest notifications and updates from CrowdStrike. Sign Up See CrowdStrike Falcon® in Action Detect, prevent, and respond to attacks— even malware-free intrusions—at any stage, with next-generation endpoint protection. See Demo Falcon Sensor Content Issue from July 19, 2024, Likely Used to Target CrowdStrike Customers Likely eCrime Actor Uses Filenames Capitalizing on July 19, 2024, Falcon Sensor Content Issues in Operation Targeting LATAM-Based CrowdStrike Customers",
    "commentLink": "https://news.ycombinator.com/item?id=41013198",
    "commentBody": "Technical Details on Today's Outage (crowdstrike.com)108 points by xjia 17 hours agohidepastfavorite111 comments dang 4 hours agoRelated ongoing thread: CrowdStrike Update: Windows Bluescreen and Boot Loops - https://news.ycombinator.com/item?id=41002195 - July 2024 (3590 comments) reply rdtsc 4 hours agoprev> The update that occurred at 04:09 UTC was designed to target newly observed, malicious named pipes being used by common C2 frameworks in cyberattacks The obvious joke here is CS runs the malicious C2 framework. So the system worked as designed: it prevented further execution and quarantined the affected machines. But given they say that’s just a configuration file (then why the hell is it suffixed with .sys?), it’s actually plausible. A smart attacker could disguise themselves and use the same facilities as the CS. CS will try to block them and blocks itself in the process? reply PedroBatista 16 hours agoprevLight on technical and light on details. Putting the actual blast radius aside, this whole thing seems a bit amateurish for a \"security company\" that pulls the contracts they do. reply hi-v-rocknroll 13 hours agoparentYup. - They don't do enough or the right kind of smoke tests. - They don't do exponential-canary deployments with an ability to rollback, and instead just YOLO it. - They don't appear to have a customer-side security / client platform team update approval gating change control process for software updates or for definitions (or whatever they use). This is fundamentally laziness and/or incompetency. reply mattmaroon 3 hours agorootparentI have been wondering why they didn’t do canary deployments. Seems like quite an obvious thing to do. reply tootie 3 hours agorootparentprevThey say they run this process multiple times a day. Must be tens of thousands of deployments. I'd guess complacency set in at some point. Just completely inured to the risks they were taking. reply nine_zeros 5 hours agorootparentprev> This is fundamentally laziness and/or incompetency. From management - that couldn't care less about low visibility, low impact projects reply grecy 15 hours agoparentprevYou assume the most competent company got the contracts, which is simply not the world we live in. The company that lobbied the hardest and paid the most in bribes got the contracts. reply manquer 14 hours agorootparentRather point I think is there technical and evaluation gates companies of this nature regularly go through while contracting, part of that is being able to talk the language of the industry properly . This seems very amateurish for companies who regularly talk professionally to win said contracts , whether the best product or not. My guess is C-suite, crisis consultants and lawyers are involved heavily so the actual engineering folks have little voice now in any communication and we get stuff like this. reply tootie 3 hours agorootparentprevHaving been through enough procurement cycles as both a buyer and seller there does not need to be a whit of malfeasance for a bad decision to occur. It's aggressive sales, price wars, poorly informed decision makers, gut instinct, favoritism, familiarity, incumbency, network effects. You notice how this outage affected hospitals and airlines? There is a strong tendency in software sales for industries to align around one or two leaders. Oh, American chose Crowdstrike? Maybe we at Delta should just do what they did. Or literally Delta hires the VP from American to be their CISO and he just does what he did before. Vendor selection is hard and buyer's remorse is frequently hard to deal with once you've sunk cost into a migration. reply dev-jayson 14 hours agoparentprevYeah, I think I'm getting more detailed analysis on Social Media from strangers, which I know I should take with a grain of salt. But I guess I'm expecting a lot more than \"a filed caused this\" from the company that caused this havoc. reply nonfamous 4 hours agoprev>>> Systems that are not currently impacted will continue to operate as expected, continue to provide protection, and have no risk of experiencing this event in the future. Given that this incident has now happened twice in the space of months (first on Linux, then on Windows), and that as stated in this very post the root cause analysis is not yet complete, I find that statement of “NO RISK” very hard to believe. reply tail_exchange 16 hours agoprevCan someone who actually understands what CrowdStrike does explain to me why on earth they don't have some kind of gradual rollout for changes? It seems like their updates go out everywhere all at once, and this sounds absolutely insane for a company at this scale. reply hatsunearu 16 hours agoparentIt sounds like Channel files are just basically definition updates in normal antivirus software; it's not actually code, just some stuff on what the software should \"look out for\". And it sounds like they shipped some malformed channel file and the software that interprets it can't handle malformed inputs and ate shit. That software happened to be kernel mode, and also marked as boot-critical, so it if falls over, it causes a BSOD and inability to boot. and it's kind of understandable that channel files might seem safe to update constantly without oversight, but that's just assuming that the file that interprets the channel file isn't a bunch of dogshit code. reply omoikane 15 hours agorootparentConfiguration files should be treated like code and follow the same gradual rollout practices. See also: https://sre.google/workbook/canarying-releases/ Which starts with \"a majority of incidents are triggered by binary or configuration pushes\". The stats for config related failures is one link away at https://sre.google/workbook/postmortem-analysis/ Where it says 31% of outages in 2010-2017 are caused by \"configuration push\". reply Murky3515 16 hours agorootparentprevIt's not understandable imo. At the very least they should have tests for the loader component that shows it can handle corrupted input. Amateur hour. reply numbsafari 16 hours agorootparentAgreed. We all know about a really interesting vector for infecting the kernel now. One that is poorly tested, poorly implemented, and poorly secured. reply SoftTalker 16 hours agorootparentprevAnd though I don't know, I'm guessing it's not a certainty to say they don't contain \"code.\" It would seem to me that they would have to, otherwise novel attacks that weren't caught by one of their existing algorithms could never be detected. I'm guessing they contain some combination of pattern/regexp type stuff, and interpreted code/scripting with trigger criteria, etc. that all gets loaded into the \"engine\" that actually runs the threat detection. reply throwaway346434 16 hours agorootparentprevHalting problem is undecidable. On the scale of \"no one bothered to put error handling or validation in\" to \"a subtle problem exists for this given input\"; you and I lack the information to make a judgement. reply acdha 16 hours agorootparent> you and I lack the information to make a judgement. Think about this a little harder: what do you know about the number of customers affected? We do actually have enough information to make a judgement - bricking millions of critical systems, a very high percentage of their total Windows customer population, tells us that they don’t have progressive rollouts, don’t fail into a safe mode, and that if they do have tests those tests are catastrophically unlike anything their customers run – all they had to do was launch an EC2 instance and see if it kept running. reply liuliu 16 hours agorootparentprevNot doing fuzzing on user-input supported feature, especially for AV, is damning. reply vanillax 16 hours agorootparentprevI mean, the whole world was impacted. All they had to do was test this change in a lab with several pcs. Clearly this wasn't a edge case nor a subtle problem. This was clearly a lack of testing. reply andrewinardeer 15 hours agorootparentIt was a Friday. Devs just wanted to go home for the weekend. reply acdha 5 hours agorootparentLeave the spin to the PR people. Their customers pay a great deal of money for 24x7 service, and this wasn’t even a code change but a definition update – a process which should be as well defined and tested as McDonald’s making a hamburger. You wouldn’t excuse getting E. coli from your lunch with “the cook just wanted to go home for the weekend”, and this is a much more expensive service. reply hatsunearu 12 hours agorootparentprevYeah, I re-read my comment and it sounds like I am understanding of them. But no, saying \"channel files aren't kernel code\" is just hilarious, considering the channel files define how the actual kernel code is supposed to behave, so it might as well be kernel code. Especially when the bad behavior in question is triggered by bad channel files!! reply Hizonner 16 hours agorootparentprev> it's not actually code, just some stuff on what the software should \"look out for\" If it controls the behavior of a computer, then it's code. > and it's kind of understandable that channel files might seem safe to update constantly without oversight Yeah, no, it's not. They pushed an update that crashed the majority of their Windows installed base in a way that couldn't be fixed remotely. It doesn't matter what the update was to. It needed to be tested. There is no way that any deployment pipeline that could fail to catch something that blatant could possibly be \"understandable\". ... and that kernel mode code shouldn't have been parsing anything with any complexity to begin with. And should have been tested into oblivion, and possibly formally verified. This is amateur-hour nonsense. Which is what you expect from most of these \"Enterprise Cyber Security(TM)\" vendors. ... AND the users shouldn't have just gone and shoved that kind of thing into every critical path they could think of. reply timbelina 12 hours agorootparentprevI was reading these two threads: https://x.com/perpetualmaniac/status/1814376668095754753?s=4... https://x.com/ananayarora/status/1814269058088304760 The authors explain the coding error and coredump well, but I'm lost: Is the buggy code that they're describing the channel file, or some kernel code that consumes the channel file? Is there a way to tell? reply timbelina 12 hours agorootparentOK, and another question:-) Can tools like Valgrind and ASan pick up the kinds of errors that are described in those two links from my previous post? reply ananayarora 11 hours agorootparentAuthor of the second post here. The first author's stack trace seems to show a fault on csagent.sys which is a bad read on 0x9c. There are some other .sys files loaded up by csagent.sys, and that's where the crash seems to happen, apparently. As for detection, Zach mentions that modern tooling could've been used to find this, so I'm assuming Valgrind can find this: https://x.com/Perpetualmaniac/status/1814376690958868979 Hope this helps! reply timbelina 10 hours agorootparentCheers Ananay! So if I put this all together: a) The driver (sensor) csagent.sys includes code that hasn't checked with a tool like Valgrind or ASan or something and so includes some kind of memory management bug. b) Since n, n-1 and n-2 versions of the sensor all died equally spectacularly, that bug as been around for at least three versions of csagent.sys. c) The bug can be triggered by getting the csagent.sys to swallow a shitty channel file and since csagent runs in kernel mode, when it crashes it BSOD's the system. d) Someone at Crowdstrike uploaded a shitty channel file as part of an update process that apparently happens many times a day. Am I on the right track so far? If so, there's no/inadequate memory management checks in the csagent driver, and either: 1)There were also no checks before the borked channel file was uploaded because of a failure to follow process, or because there was no process, but whatever the case it was an accident. or 2) Someone uploaded on purpose, not by accident, the borked channel file intending for a nasty outcome (probably not BSOD) I can't believe that there are not a million checks and balances in place to let (1) happen, but as my grandma used to say, \"Don't assume malice where stupidity will do\" :-) reply notepad0x90 16 hours agoparentprevThis \"channel file\" is equivalent to an AV signature file. Crowdstrike is the company, the product here is \"Falcon\" which does behavioral monitoring of processes both on the device and using logs collected from the device in the cloud. I can see your perspective, but you should consider this: They protect these many companies, industries and even countries at such a global scale and you haven't even heard of them in the last 15 years of their operation until this one outage. You can't take days testing gradual roll outs for this type of content, because that's how long customers are left unprotected by that content. Although the root cause is on the channel files, I feel like the driver that processes them should have been able to handle the \"logic bug\" in question so we'll find out more over time I guess. For example, with windows defender which runs on virtually all windows systems, the signature updates on billions of devices are pushed immediately (with exception to enterprise systems, but even then there is usually not much testing on signature files themselves, if at all). As far as the devops process Crowdstrike uses to test the channel files, I think it's best to leave commentary on that to actual insiders but these updates happen several times a day sometimes and get pushed to every Crowdstrike customer. reply joaomacp 14 hours agorootparent> They protect these many companies, industries and even countries at such a global scale and you haven't even heard of them in the last 15 years of their operation I certainly don't want to know (through disaster news) about the construction company that built the bridge I drive through everyday, not for another 15 years, not ever! This kind of software simply should not fail, with such a massive install base on so many sensitive industries. We're better than that, the software industry is starting to mature and there are simple and widely-known procedures that could have been used to prevent it. I have no idea how CrowdStrike stock has only dropped 10% to the values of 2 months ago. Actually, if the financial troubles you get into are only these, take back what I said, software should be failing a lot (why spend money on robustness when you don't lose money on bugs?) reply notepad0x90 14 hours agorootparentworking in software, you should know how insanely complex software is, even google, amazon, microsoft, cloudflare and such have outages. mistakes happen because humans are involved. it is the nature and risk of depending complex systems. bridges by comparison are not that complicated. I actually expected their stock to drop a lot more than this, but goes to show you how valuable they are. investors know that any dip is only temporary because no one is getting rid of crowdstrike. Think of the security landscape as early 90's new york city at night and crowdstrike as the big bulky guy with lots of guns who protects you for a fee, if he makes a mistakes and hurts you, you will be mad but in the end your need for protection does not suddenly go away and it was a one time mistake. reply TheOtherHobbes 3 hours agorootparentIn which case \"Are you awake and sane?\" would be a sensible reality check before heading out. You're trying to hand-wave away the inexcusable. The outage is a symptom. The problem is the lack of even the most basic testing. Clearly these files are sent out without even a minimal sanity check. That is a problem, and it's not something that can be hand-waved away. reply zug_zug 15 hours agorootparentprev>> You can't take days testing gradual roll outs for this type of content, because that's how long customers are left unprotected by that content. If you can't take days to do it then do a gradual rollout in hours. It's not a high bar. reply notepad0x90 15 hours agorootparentthey reverted it after about one hour. but sure, they didn't need to target all customers all at once, that's a good point. reply Marsymars 3 hours agorootparentprev> They protect these many companies, industries and even countries at such a global scale and you haven't even heard of them in the last 15 years of their operation until this one outage. They certainly run their software on those many customers' systems, but but based on my experience with them, \"protect\" isn't a descriptor I'm willing to grant them. We don't have the counter-factual where Crowdstrike doesn't exist, but I'm not convinced that they've been a net economic or security benefit to the world over the span of their existence. reply tail_exchange 4 hours agorootparentprevAnother commenter said that this change was a malformed configuration that crashed the application. If this is the case, you wouldn't need days to see this problem manifest, but only a few minutes. If they had rolled it out to 1% of their customers and waited for a couple hours before releasing it everywhere, they probably would have caught it. reply dalyons 4 hours agorootparentA couple of hours is a long time in the world of automated attacks reply mardifoufs 1 hour agorootparentThen they should make their testing pipelines even faster, and make sure that they can go from detecting a new threat->tested definition file as quickly as possible. You genuinely cannot skimp on testing in this case. It's inherent to the update, threat protection and not breaking their consumers systems should be non-negotiable for a release. That means testing before deploying. If they can't do it fast enough, their product is broken. reply mynameisvlad 3 hours agorootparentprevA couple of hours is absolutely nothing compared to the massive worldwide effort that many people have to put in to fix the problem of a company’s shitty product and release practices. This is inexcusable, point blank. “A couple of hours is a long time” is not a valid excuse when the alternative, as clearly evidenced, is millions of computers and critical systems simultaneously failing hard. This might have been different if it was a small subset of computers, but this clearly could have been caught in minutes with any sort of sensible testing or canary rollout practices. reply RaftPeople 3 hours agorootparentprevIt only takes a couple of minutes if you first update your on-site set of LIVE systems sitting there to detect a problem. If problem encountered, don't send it out to everyone else. reply TheOtherHobbes 3 hours agorootparentprevNot as long as the weeks it's going to take to undo this. reply lotr5 15 hours agorootparentprevthey are dumb enough to process their \"channel files\" in kernel, this should be only done in usermode reply vladvasiliu 15 hours agorootparentWhile I can understand both arguments for and against a gradual rollout, this is the main issue: why do these things need to be processed in kernel? And if there’s a good reason to do it, why isn’t there some kind of circuit breaker? reply notepad0x90 13 hours agorootparentbecause the thing that uses them is in kernel mode, and the sensor needs to be performant. at some point, the content must be consumed by the kernel mode sensor. user mode edr's exist but bypassing them is trivial, intercepting syscalls rootkit style and monitoring kernel+usermode memory is the best and most performant way to monitor the whole system. reply calf 13 hours agorootparentApple documentation argues the opposite: \"Developers can use frameworks such as DriverKit and NetworkExtension to write USB and human interface drivers, endpoint security tools (like data loss prevention or other endpoint agents), and VPN and network tools, all without needing to write kexts. Third-party security agents should be used only if they take advantage of these APIs or have a robust road map to transition to them and away from kernel extensions.\" Specifically the 2nd sentence above says security software should use the APIs, not Apple's kernel extensions. reply notepad0x90 13 hours agorootparentwell, this is windows not macos. I don't know what you can do with driverkit for example. maybe microsoft should learn from apple? reply lotr5 15 hours agorootparentprevprobably they didn't find solution where they fully trust information coming from usermode process reply notepad0x90 13 hours agorootparentprevthey need to be processed in kernel mode where the monitoring happens, user mode EDRs are trivial to bypass. they have to be processed by whatever is going to use them, and in this case it is the \"lightweight\" sensor code in kernel mode. reply acdha 5 hours agorootparentThey need to load data into the kernel eventually but that doesn’t mean that the first time the file is parsed should be in the kernel. For example, on Linux they don’t have this problem because they use the eBPF subsystem and so what’s running in the kernel is validated byte code. Even if they didn’t want to do something that sophisticated they could simply include a validator into the update process, as has been common since the 1980s. reply SkyPuncher 16 hours agoparentprevMy understanding is they basically deployed a configuration file. It seems like these files might be akin to virus signatures or other frequently updated run-time configuration. I actually don't think it's outrageous that these files are rolled out globally, simultaneously. I'm guessing they're updated frequently and _should_ be largely benign. What stands out to me is the fact that a bad config file can crash the system. No rollback mechanism. No safety checks. No safe, failure mode. Just BSOD. Given the fix is simply deleting the broken file, it's astounding to me that the system's behavior is BSOD. To me, that's more damning that a bad \"software update\". These files seem to change often and frequently. Given they're critical path, they shouldn't have the ability to completely crash the system. reply opello 14 hours agorootparentEntertainingly enough I got to see a similar thing happen, where a configuration file was killing hardware in the field. After the failure and remediation multiple CI jobs were put in place (some months later) to do basic validity checks on the files. The lesson of \"multiple parser implementations for the same thing seems bad\" and \"sanity checks to prevent breaking things are hard heuristics to define\" such that further changes were deferred. All that to say that I can appreciate circumstances in which satisfying \"don't crash the system\" in response to configuration data can actually be fairly hard to realize. It can very significantly depend on the design of the pieces in question. But I also agree that it's pretty damning. reply joshka 14 hours agorootparentprev> I actually don't think it's outrageous that these files are rolled out globally, simultaneously. Anyone competent that manages software at scale should generally hold the opposite opinion to this. reply Analemma_ 16 hours agorootparentprevThat’s the danger of running in kernel mode. I’ve seen some people claim this is because the bad file starts a chain of events which concludes in trying to page an unpageable file, which is an application crash in user space but brings down the whole system if it happens in the kernel. reply SkyPuncher 16 hours agorootparentThat seems like programming 101 for these systems. In the past, I've worked around this by validating the configuration of a file before attempting to run it. You bail out in a safe way during validation, but still allow a hard error during run time. Doesn't prevent all misconfigured files, but prevents the stuff like. reply acdha 5 hours agorootparentI think it was in the early 90s when I first saw something do A/B style loading where it would record the attempt to load something, recognize that it hadn’t finished, and use the last known good config instead. Anyone studying high-availability systems has a wealth of prior art to learn from. reply userbinator 15 hours agorootparentprevI think all programmers should have the experience of using and developing on a single-address-space OS with absolutely no protections like DOS, just to encourage them to improve their skills at writing better, actually correct code. When the smallest bugs will crash your system and cause you to lose work, you tend to be a lot more careful with thinking about what your code does instead of just running it to see what happens. reply Gigachad 15 hours agorootparentSuggesting “Being more careful” never solves these issues because eventually someone somewhere will have a momentary slip up that causes this. The real takeaway is that we need to design systems so this kind of issue is less possible. Put less code in the kernel, use tools that prevent these kinds of issues, design computers that can roll back the system if they crash. reply YZF 15 hours agorootparentprevPerfect example of where instrumentation guided fuzzing like AFL would almost certainly have found a problem. I agree with the amateur hour observation. But then most things seem to be. reply userbinator 16 hours agoparentprevI'm more surprised at the fact that they didn't appear to have tested it on themselves first. FWIW, at least Microsoft still \"dogfoods\" (and it's what coined that term), and even if the results of that aren't great, I'm sure they would've caught something of this severity... but then again, maybe not[1]. [1] https://news.ycombinator.com/item?id=18189139 reply Ekaros 13 hours agorootparentThis is what really would concern me too. With this wide spread issue any reasonable testing should have detected it. Having a few dozen machines with different configurations for an few hours should have detected this. This should have been in a smoke test. Push update to machines, observe, power cycle them, observe... I could understand error in some rarer setup, but this was so common that it should have been obvious error. reply Zamiel_Snawley 16 hours agoparentprevTruly, how the extent the damage was so widespread is my main question at this point. Everyone has a buggy release at some point, but impacting global customers at this level is damn near unforgivable. Heads need to roll for this oversight. reply jefurii 11 hours agoparentprevI have a friend who is a security guard at a bank in Hollywood, CA, who told me the computers at his location started going down between 12:00 and 13:00PDT (19:00-20:00UTC). I don't understand CrowdStrike's rollout system, but given that people started seeing trouble earlier in the day, surely by that time they could have shut down the servers that were serving the updates, or something?? He also told me that soon after that the street outside the bank (another bank across the street, a hospital several blocks down) was lined with police who started barring entry to the buildings unless people had bank cards. By the time I woke up this morning technical people already knew basically what was going on, but I really underestimated how freaked out the average person must have been today. reply Murky3515 16 hours agoparentprevBecause if release immediately, velocity go up reply ungreased0675 16 hours agoprevThis seems very unsatisfying. Not sure if I was expecting too much, but that’s a lot of words for very little information. I’d like more information on how these Channel Files are created, tested, and deployed. What’s the minimum number of people that can do it? How fast can the process go? reply hatsunearu 16 hours agoprevI'm not a big expert but honestly this read like a bunch of garbage. > Although Channel Files end with the SYS extension, they are not kernel drivers. OK, but I'm pretty sure usermode software can't cause a BSOD. Clearly something running in kernel mode ate shit and that brought the system down. Just because a channel file not in kernel mode ate shit doesn't mean your kernel mode software isn't culpable. This just seems like a sleezy dodge. reply gjm11 16 hours agoparentIt doesn't read to me as trying to dodge anything. They aren't saying \"they're not kernel drivers, so everything is OK\", they're saying \"seeing the .sys on the filenames, you might think they're kernel drivers, but as it happens they're something else\". (Maybe there's some subtext that I'm missing, but I don't see how saying \"these aren't kernel drivers\" makes them look any better, and I do see why they might say it to be informative, so it looks like to me like they're doing the latter.) reply hatsunearu 16 hours agorootparent> It doesn't read to me as trying to dodge anything. It absolutely reads like this. They are getting blasted online for shipping kernel mode driver updates without proper QA and release engineering. Which just from face value just seems like some insano style engineering. They are saying \"it's not actually a kernel mode value\" to deflect blame. I mean, I really don't understand why they would make this statement otherwise. If they are innocently just trying to say \"this is just a channel file\", there are other ways to say this, and it really isn't relevant enough to underline and emphasize. reply bostik 3 hours agorootparentFriend does incident response and Windows forensics, and pointed something (in retrospect) rather obvious out yesterday: the instructions for cleaning up simply told people to \"delete .SYS files according to this wildcard\". No additional context. That caught his eye, because to him it sounded like madness. Apparently deleting random driver files is a fairly well known way to screw a Windows system up even more than it already was. This statement from CS must have gone through legal and PR review, so we have to assume every word and statement has been carefully vetted from a cover-your-backside perspective. It is light on information content, but there must be reason for them to so forcefully telegraph that the files being deployed (and removed) are not themselves drivers. reply gjm11 4 hours agorootparentprevThey're getting blasted for causing a massive worldwide outage due to what is clearly inadequate quality control. I don't see why this is any better if it's \"pushed a kernel-mode driver update with bugs in it\" than if it's \"released a product with buggy kernel-mode stuff that can be made to crash by an innocuous-looking data file, and then pushed a data file that made it crash\". Same result either way. Same demonstration of inadequate quality control either way. I think the story they're telling now, which so far as I know is the truth, looks worse for them, because it requires them to have screwed up their QC twice. Once when they made a product that do such bad things, and once when they pushed the data file to millions of PCs without checking what it did. So I still don't see how \"this particular file happens not to be kernel-mode code\" makes them look any better, and therefore I don't see why they'd be saying it \"to deflect blame\". It doesn't deflect blame; they look just as bad either way. reply mynameisvlad 3 hours agorootparentYou may understand it that way, but you also have a much deeper knowledge of this than the targeted audience of the RCA. Make no mistake, this RCA was not published for technical folks. The only reason it’s even published is to make their customers feel more secure. You and I are not their customers; high level management and executives are. reply SoftTalker 16 hours agoparentprevThe kernel driver reads the channel files. It choked on this one, and crashed. reply SAI_Peregrinus 16 hours agorootparentWhich implies that any malware capable of replacing these channel files can crash their kernel driver. I wonder if there's a non-crashing way to exploit this & get kernel-space code execution. reply amiga386 3 hours agorootparentAny malware capable of modifying files under C:\\Windows\\System32 has no need to fiddle with these files because to have that capability means it already got the keys to the kingdom and could wreck the system in a billion different ways. See \"It rather involved being on the other side of this airtight hatchway\" https://devblogs.microsoft.com/oldnewthing/20200420-00/?p=10... reply SoftTalker 1 hour agorootparentAnd yet the cleanup instructions were for the user to delete a file in that directory. That requires booting into safe mode, but if any random user is able to do that, kiss your systems goodbye, a good social engineer will own any desktop in your organization if he wants to. reply joshka 14 hours agorootparentprevIf you have write access a path like C:\\Windows\\System32\\drivers\\CrowdStrike\\ (and I'd assume the parent directory), then you pretty much can crash the kernel many ways. If you have the means to insert an AV config file update in between the config servers and the user's host then you probably can PWN the system pretty easily as well. What this probably does mean is that Crowdstrike will be receiving some attention from hackers of both hat colors. Here's the bug bounty page ... https://hackerone.com/crowdstrike?type=team reply numbsafari 16 hours agorootparentprevI can guarantee you that you aren't the only one thinking this right now. reply ytch 14 hours agorootparentprevIIRC, there were some security software exploits, that trigger RCE (or DoS) of the scanning engine by malicious file? reply teeray 16 hours agorootparentprevThe kernel driver is an interpreter that executed an HCF instruction from the channel file. reply hatsunearu 16 hours agorootparentprevThat's what I thought. So saying \"it's not a kernel mode driver\" is technically true, but I don't need to explain why it's a bunch of nonsense to try to damage control their incompetence. reply epcoa 16 hours agorootparentNo idea why you’re getting downvoted. A configuration file for code that runs in kernel space is usually effectively kernel code (it certainly was in this case) - obviously there are formal methods to allow kernel code to be configured in a “safe” fashion, but it’s obvious that’s not going on here. reply patrickthebold 16 hours agoprev>The configuration update triggered a logic error that resulted in an operating system crash. > We understand how this issue occurred and we are doing a thorough root cause analysis to determine how this logic flaw occurred. There's always going to be flaws in the logic of the code, the trick is to not have single errors be so catastrophic. reply chris_nielsen 16 hours agoparentYeah “how this logic flaw occurred” is the wrong question. How a common bug was rolled out globally with no controls, testing, or rollback strategy is the right question reply YZF 15 hours agorootparentThey're all good questions. The thing that reads the config should have been fuzz tested with something like AFL. Likely should have a lot more tests. Maybe shouldn't run in a device driver. There's almost no doubt there are engineering process and culture issues here. And then absolutely the release process. Rollback is hard I guess once your OS can't boot. reply cube00 2 hours agorootparent> Rollback is hard I guess once your OS can't boot. This is why the client needs have enough error handling to realise it's latest update has now caused unsuccessful boot and roll that update back locally to the last known good configuration (or completely back to factory and pull all updates again). reply pneumonic 15 hours agoprev> we are doing a \"root cause analysis to determine how this logic flaw occurred\" That's going to find a cause: a programmer made an error. That's not the root of the problem. The root of the problem is allowing such an error to be released (especially obvious because of its widespread impact). reply kyriakos 13 hours agoprevWhy is everyone blaming Microsoft? Is this something of an oversight in their side too? Can someone explain? reply cyrnel 2 hours agoparentI'm no kernel expert, but people are saying Microsoft deserves some blame for not exposing necessary functionality to user space, requiring the use of a very-unsafe kernel driver. Linux provides eBPF and macOS provides system extensions. I'll also add that Windows itself heavily prioritizes backwards-compatibility over security, which leads companies to seek out third-party solutions for stopping malware instead of design-based mitigations being built into Windows. reply mardifoufs 1 hour agorootparentI don't agree. I'm glad Microsoft doesn't provide the functionality to do what crowdstrike does to user space. Crowdstrike acts in a similar way to deeply seated malware, except that it is usually installed voluntarily. But the behavior and capabilities that it has are basically what any malware would dream of, and exposing them to user space would imo create a mess (especially on windows). If anything, this is good as it will make people even more weary of kernel mode software. And I'm not sure epbf actually allows you to do a lot of the stuff crowdstrike-like software does. I know they use it on Linux though so maybe eBPF has evolved a lot since I last looked at it. reply cyrnel 1 hour agorootparentI generally agree with you. It's an either-or thing: either Microsoft secures their OS, or they provide safe ways for users to secure their OS. The first option is a million times better, but having neither option leads us to this mess. reply sgammon 13 hours agoparentprevFor letting a failure of this magnitude be possible, I suspect reply jchiu1106 16 hours agoprevWhere are the technical details? reply Zamiel_Snawley 16 hours agoparentYeah, this PR statement is pretty much devoid of information other than it is not a cyber attack. reply canistel 15 hours agoprev> This issue is not the result of or related to a cyberattack. Must be corrected to \"the issue is not the result of or related to a cyberattack by external agents\". reply geuis 15 hours agoprevWeak. Very weak and over corporate level of ass covering. And it doesn't even come close to doing that. They should just let the EM of the team involved provide a public detailed response that I'm sure is floating around internally. Just own the problem and address the questions rather than trying to play at politics, quite poorly. reply isthisreallife2 14 hours agoprevSo - a malformed configuration is capable of crashing a kernel process. Sounds very exploitable. Very reply automatoney 16 hours agoprevTo my biased ears it sounds like these configuration-like files are a borderline DSL that maybe isn't being treated as such. I feel like that's a common issue - people assume because you call it a config file, it's not a language, and so it doesn't get treated as actual code that gets interpreted. reply bryan_w 12 hours agoprevIt kinda feels like someone added a watch for c:\\COM\\COM like we did back in the day on AOL reply xyst 16 hours agoprev“Technical” detail report reads more like a lawyer generated report. This company is awful. If I ever get a sales pitch from these shit brains, they will get immediately shut down. Also fuck MS and their awful operating system that then spawned this god awful product/company known as “CrowdStike Falcon” reply robjan 15 hours agoparentYou are probably not the target market of this product then. The real product CrowdStrike Falcon sells is regulatory compliance and it's a defacto requirement in many regulated industries including banking. By the way, Falcon can be and is deployed to Linux and MacOS hosts in these organisations too it's just that this particular incident only affected Windows. reply hello_moto 4 hours agoparentprev2 things: 1. critical infrastructure around the globe seemed to depend on CrowdStrike 2. \"If I ever get a sales pitch from...\" suggested you are in an environment that is far from critical infrastructure. reply userbinator 16 hours agoparentprevIf Windows wasn't as popular, then this might've happened to Linux to macOS instead. Blame CrowdStrike's incompetence, not MS. reply acdha 5 hours agorootparentIt couldn't happen on macOS: Apple stopped letting third-parties run code in the kernel after years of failures like this. It also wouldn't happen on Linux: they use eBPF there which was designed by grownups and validates its inputs. reply mynameisvlad 3 hours agorootparenteBPF exists on Windows, too: https://microsoft.github.io/ebpf-for-windows/ They’re just not using it. They could have not used it for Linux too. The presence of the feature is not enough to guarantee this would’ve never happened in a hypothetical. reply acdha 2 hours agorootparentNo, the fact that they’re actually using eBPF on Linux is what makes it safer. None of this is magic, it’s just a question of following decades of engineering experience. Similarly, Microsoft clearly sees the benefits but note that they themselves say that’s not production ready yet. I’m certain that this incident will cause people to consider migrating as soon as that changes. reply mynameisvlad 1 hour agorootparentYou’re responding to a hypothetical, not what happened. Let’s say Linux is the leading OS around the world. How can we be sure that they would actually use eBPF if this was the case? They would likely choose the fastest option in order to support the platform as quickly as possible. Perhaps eBPF didn’t even exist if they prioritized Linux support and implemented that first, since Falcon was first released in 2013 and eBPF in 2014. Switching from kernel mode to eBPF would be quite a lift, so if it wasn’t baked in from the start it likely wouldn’t have been added in after the fact. A decade worth of changes is a lot to confidently say what would have happened. If Linux and MacOS were more popular than Windows, it could have been completely different. This doesn’t even touch on the massive Debian incident CS had earlier this year, which is not a hypothetical. reply acdha 4 minutes agorootparentThey are using eBPF right now. That suggests that they, like everyone else, see benefits in using a platform feature when it exists. j16sdiz 3 hours agorootparentprevKext still exist on macos reply acdha 1 hour agorootparentKind of: they’ve been deprecated for 4 years and you have to disable SIP to load them. https://developer.apple.com/support/kernel-extensions/ They’ve added system extension mechanisms for the most common needs trying to balance the various things people use kexts for against the impact on security, performance, and reliability many kexts had. reply timbelina 12 hours agoprev [–] Can someone aim me at some RTFM that describes the sensor release and patching process, please? I'm lost trying to understand: When a new version 'n' of the sensor is released, we upgrade a selected batch of machines and do some tests (mostly waiting around :-)) to see that all is well. Then we upgrade the rest of the fleet by OU. However, 'cause we're scaredy cats, we leave some critical kit on n-1 for longer. And some really critical kit even on n-2. (Yeah, there's a risk in not applying patches I know but there are other outage-related risks that we balance; forget that for now) Our assumption is that n-1, n-2, etc are old, stable releases, and so when fan and shit collided yesterday, we just hopped on the console and did a policy update to revert to n-2 and assumed we'd dodged the bullet. But of course, that failed... you know what they say about assumptions :-) So in a long-winded way that leads to my three questions: Why did the 'content update' take out not just n but n-whatever sensors equally as effectively? Are the n-whatever versions not actually stable? And if the n-whatever versions are not actually stable and are being patched, what's the point of the versioning? Cheers! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "On July 19, 2024, CrowdStrike released a sensor configuration update for Windows systems, causing system crashes and blue screens (BSOD) due to a logic error, which was fixed within approximately 1.5 hours.",
      "The issue affected customers using Falcon sensor for Windows version 7.11 and above, who were online between 04:09 UTC and 05:27 UTC, due to a problematic Channel File 291 designed to evaluate named pipe execution.",
      "Systems running Linux or macOS were not impacted, and a root cause analysis is ongoing to prevent future issues."
    ],
    "commentSummary": [
      "CrowdStrike experienced a significant outage due to a malformed configuration file, causing Windows systems to crash with Blue Screen of Death (BSOD) and boot loops.",
      "The issue stemmed from an update targeting malicious named pipes, which inadvertently led to CrowdStrike's own systems blocking themselves, highlighting a lack of adequate testing and rollout procedures.",
      "This incident has raised concerns about CrowdStrike's quality control, testing practices, and the robustness of their deployment strategies, especially given their critical role in cybersecurity for many organizations."
    ],
    "points": 108,
    "commentCount": 111,
    "retryCount": 0,
    "time": 1721438368
  }
]
