[
  {
    "id": 39097356,
    "title": "FTC bans TurboTax from 'free' advertising",
    "originLink": "https://www.cnn.com/2024/01/22/business/ftc-turbotax-free-services/index.html",
    "originBody": "body,h1,h2,h3,h4,h5{font-family:cnn_sans_display,helveticaneue,Helvetica,Arial,Utkal,sans-serif}h1,h2,h3,h4,h5{font-weight:700}:root{--theme-background:#0c0c0c;--theme-divider:#404040;--theme-copy:#404040;--theme-copy-accent:#e6e6e6;--theme-copy-accent-hover:#ffffff;--theme-icon-color:#e6e6e6;--theme-icon-color-hover:#ffffff;--theme-ad-slot-background-color:#0c0c0c;--theme-ad-slot-text-color:#b1b1b1;--theme-ad-slot-text-hover:#ffffff;--theme-font-family:cnn_sans_display,helveticaneue,Helvetica,Arial,Utkal,sans-serif;--theme-searchbox-border:#b1b1b1;--theme-copy-follow:#ffffff;--theme-article-spacing-top:0px;--theme-link-color-hover:#6e6e6e;--theme-color-link:#0c0c0c;--theme-button-color:#6e6e6e;--theme-button-color-hover:#cc0000;--theme-edition-picker-link:#e6e6e6;--theme-underline-skip-ink:auto;--theme-paragraph__font-size:16px;--theme-paragraph__line-height:26px;--theme-paragraph__font-size--from-small:16px;--theme-paragraph__line-height--from-small:26px;--theme-paragraph__link-color:#0c0c0c;--theme-paragraph__link-decoration:underline;--theme-paragraph__link-decoration-color:var(--theme-color-link);--theme-paragraph__link-decoration-thickness:1px;--theme-paragraph__hover-link-decoration:none;--theme-paragraph__hover-link-offset:4px;--theme-header__hover-item-hover:var(--theme-background);--theme-header__item-link-color:#e6e6e6;--theme-header__item-link-hover-color:#ffffff;--theme-header__item-link-hover-background-color:transparent;--theme-header__mobile-dropdown-border-color:var(--theme-divider);--theme-header__mobile-dropdown-background:#0c0c0c;--theme-header__item-link-line-height:40px;--theme-header__dropdown-background:#0c0c0c;--theme-header__dropdown-border-color:var(--theme-divider);--theme-header__login-button:#ffffff;--theme-headline__font-size:24px;--theme-headline__line-height:30px;--theme-headline__text-color:#0c0c0c;--theme-headline-sponsorship__lateral-margin:0;--theme-headline__font-weight:700;--theme-headline__margin-bottom:16px;--theme-headline__font-family:cnn_sans_display,helveticaneue,Helvetica,Arial,Utkal,sans-serif;--theme-headline__padding-bottom:48px;--theme-headline__padding-bottom-viewport-large:64px;--theme-headline__teaser-font-size:16px;--theme-headline__teaser-line-height:normal;--theme-headline__teaser-margin-top:0;--theme-headline__teaser-margin-botton:0;--theme-section-headline__font-size:36px;--theme-section-headline__line-height:42px;--theme-section-headline__text-color:#0c0c0c;--theme-section-headline__font-weight:700;--theme-section-headline__font-family:cnn_sans_display,helveticaneue,Helvetica,Arial,Utkal,sans-serif;--theme-section-headline__margin-bottom:0;--theme-section-headline-text__margin-top:16px;--theme-section-headline-text__margin-bottom:18px;--theme-section-headline-teaser__font-size:inherit;--theme-section-headline-teaser__color:inherit;--theme-subheader-h2__font-size:24px;--theme-subheader-h3__font-size:20px;--theme-subheader-h4__font-size:18px;--theme-subheader-h5__font-size:16px;--theme-subheader-h6__font-size:14px;--theme-subheader-h2__line-height:30px;--theme-subheader-h3__line-height:26px;--theme-subheader-h4__line-height:24px;--theme-subheader-h5__line-height:22px;--theme-subheader-h6__line-height:20px;--theme-subheader__font-family:cnn_sans_display,helveticaneue,Helvetica,Arial,Utkal,sans-serif;--theme-subheader__font-weight:700;--theme-iframe__display:block;--theme-list__link-decoration:underline;--theme-container__font-family:cnn_sans_display,helveticaneue,Helvetica,Arial,Utkal,sans-serif;--theme-container__font-weight:400;--theme-container-color--hover:#0c0c0c;--theme-container-image-color--hover:rgba(12, 12, 12, 0.4);--theme-container-text-decoration--hover:underline;--theme-container-image-opacity--hover:0.5;--theme-container-margin-bottom-default:24px;--theme-container-margin-bottom-600:48px;--theme-container-title__border-color:#e6e6e6;--theme-container-title__border-decorator-initial-width:16px;--theme-container-title__margin-bottom:0;--theme-container-title__margin-bottom-grid-4:0;--theme-container-title__text-size:12px;--theme-container-title__arrow-color--initial:#ffffff;--theme-container-title__arrow-size:16px;--theme-container-title__arrow-top-pos:0;--theme-container-link__background-color:inherit;--theme-container-item__margin-bottom-feature-list:32px;--theme-container__margin-bottom-grid-3:24px;--theme-container__margin-bottom-feature-grid-3:24px;--theme-container-title-emphatic__font-size:24px;--theme-container-title-emphatic__line-height:30px;--theme-container-lead-title__font-family:cnn_sans_display,helveticaneue,Helvetica,Arial,Utkal,sans-serif;--theme-container-lead-title__font-weight:700;--theme-container-lead-title__font-size:20px;--theme-container-lead-title__line-height:24px;--theme-container-lead-title-mobile__font-size:16px;--theme-header-mobile-nav-border-color:transparent;--theme-text-banner__gradient-1:#cdb6f1;--theme-text-banner__gradient-2:#e5dbf8;--theme-zone__padding-bottom-default:64px;--theme-zone__padding-bottom-small:64px;--theme-zone__margin-bottom-default:48px;--theme-zone__margin-top:-32px;--theme-zone-title__font-family:cnn_sans_display,helveticaneue,Helvetica,Arial,Utkal,sans-serif;--theme-zone-title__font-size:30px;--theme-zone-title__font-weight:700;--theme-zone-title__line-height:30px;--theme-zone-title__link-decoration:none;--theme-zone-title__hover-link-decoration:underline;--social-sharing-display:block;--social-sharing-margin-top:16px;--social-sharing-open-close-fill:#4d4d4d;--social-sharing-facebook-fill:#0c0c0c;--social-sharing-twitter-fill:#0c0c0c;--social-sharing-email-fill:#0c0c0c;--social-sharing-link-fill:#0c0c0c;--theme-disclaimer-background:#e6e6e6;--theme-disclaimer-color:#4d4d4d;--theme-disclaimer-style:normal;--theme-disclaimer-link-color:#6a29d5;--theme-disclaimer-link-weight:400;--theme-disclaimer-fontsize-sm:14px;--theme-disclaimer-fontsize-xl:16px;--theme-disclaimer-lineheight-sm:22.75px;--theme-disclaimer-lineheight-xl:25.6px;--theme-newsletter-form-disable-button:#c0c0c0;--theme-paragraph-fontsize-sm:14px;--theme-paragraph-fontsize-xl:16px;--theme-paragraph-lineheight-sm:22.75px;--theme-paragraph-lineheight-xl:25.6px;--theme-main-wrapper-rail-width:360px;--theme-main-wrapper-right-rail-width:300px;--theme-main-wrapper-column-gap-medium-width:40px;--theme-main-wrapper-column-gap-large-width:50px;--theme-primary-logo-fill:#cc0000;--theme-secondary-logo-fill:white;--theme-subheader-anchor-display:inline;--theme-primary-layout-color:#fafafa;--theme-secondary-layout-color:#fff;--theme-video-playlist-status-label-color:rgba(12, 12, 12, 0.7);--theme-primary:#008561;--theme-container-text-decoration-color--hover:var(--theme-color-link);--theme-container-title__border-decorator-color:#008561;--theme-container-title__arrow-color--hover:var(--theme-color-link);--theme-video-playlist-item-hover-color:#00c59e;--theme-footer-disclaimer-color:#ffffff;--theme-footer-disclaimer-weight:400}@media (min-width:960px){:root{--theme-headline__font-size:42px;--theme-headline__line-height:48px;--theme-section-headline__font-size:42px;--theme-section-headline__line-height:48px;--theme-section-headline__margin-bottom:16px;--theme-subheader-h2__font-size:30px;--theme-subheader-h3__font-size:24px;--theme-subheader-h4__font-size:20px;--theme-subheader-h2__line-height:36px;--theme-subheader-h3__line-height:30px;--theme-subheader-h4__line-height:26px;--theme-container-margin-bottom-600:0;--theme-container__margin-bottom-feature-grid-3:0}}@media (max-width:959px){:root{--social-sharing-display:none}}@media (min-width:480px){:root{--theme-section-headline-text__margin-bottom:20px;--theme-container__margin-bottom-grid-3:32px;--theme-container__margin-bottom-feature-grid-3:0;--theme-headline__margin-bottom:20px}}@media (min-width:1280px){:root{--theme-section-headline-text__margin-bottom:22px;--theme-headline__margin-bottom:22px}} window.env={\"AD_SLOT_CLIENT_INJECTOR_REGISTRY\":\"https://cdn.cnn.com/ads/cnn/cnn_quantum_leaf.json\",\"ADFUEL_BUSINESS_SRC\":\"/media/sites/js/bundles/business-adfuel\",\"ADFUEL_BUSINESS_EDITION_SRC\":\"/media/sites/js/bundles/business-edition-adfuel\",\"ADFUEL_CNN_SRC\":\"/media/sites/js/bundles/cnn-adfuel\",\"ADFUEL_CNN_EDITION_SRC\":\"/media/sites/js/bundles/cnn-edition-adfuel\",\"ADOBE_LAUNCH_SRC\":\"https://lightning.cnn.com/launch/7be62238e4c3/97fa00444124/launch-2878c87af5e3.min.js\",\"ADOBE_LAUNCH_BUSINESS_ENABLED_SECTIONS\":[\"business\",\"markets\"],\"ADVANCED_VIDEO_ENABLED\":true,\"AIRSHIP_APP_KEY\":\"3wrwsS87S6OIW06Lq4MVIQ\",\"AIRSHIP_ENABLED\":true,\"AIRSHIP_SRC\":\"https://aswpsdkus.com/notify/v1/ua-sdk.min.js\",\"AIRSHIP_TOKEN\":\"MTozd3J3c1M4N1M2T0lXMDZMcTRNVklROmRSb3lkd0lHZ0NHanFMeElRYVpjaGNQQVBrd2k5NGRKa1NobWR2SjBIUjg\",\"AIRSHIP_VAPID_PUBLIC_KEY\":\"BHJLBg0NxOGDHKXf0Bepz55qLpKT674Z6XiGZxVbW0p67B6cpiBzvOo2vSWTtnEGHjmILIuDmWkldwLOv4bwwz8=\",\"AIRSHIP_WEB_SITE_PUSH_ID\":\"web.com.cnn.redalert\",\"AIRSHIP_WORKER\":\"/service-worker.js\",\"AIRSHIP_CORE_ENABLED\":true,\"APPLE_NEWS_MANAGER_ENABLED\":true,\"ALERTS_HUB_ENABLED\":false,\"APPLE_NEWS_LOGO_NAME_TRAVEL\":\"https://media.cnn.com/api/v1/images/cnn/apple-news/cnn-travel-light.png\",\"APPLE_NEWS_LOGO_NAME_STYLE\":\"https://media.cnn.com/api/v1/images/stellar/prod/cnn-style-light.png\",\"APPLE_NEWS_LOGO_NAME_BUSINESS\":\"https://media.cnn.com/api/v1/images/cnn/apple-news/cnn-business-logo.png\",\"APPLE_NEWS_LOGO_NAME_POLITICS\":\"https://media.cnn.com/api/v1/images/cnn/apple-news/cnn-politics-light.png\",\"APPLE_NEWS_LOGO_NAME_QUOTE\":\"https://media.cnn.com/api/v1/images/cnn/apple-news/quote-light.png\",\"ARKOSE_LOGIN_KEY\":\"A81F9530-112A-47B2-BA4B-8CB41D7C6DD6\",\"ARKOSE_LOGIN_SRC\":\"https://wbd-api.arkoselabs.com/v2/A81F9530-112A-47B2-BA4B-8CB41D7C6DD6/api.js\",\"ARKOSE_NEWSLETTERS_KEY\":\"12FB7448-F055-4621-BC01-1DDF7CB3945A\",\"ARKOSE_NEWSLETTERS_SRC\":\"https://wbd-api.arkoselabs.com/v2/12FB7448-F055-4621-BC01-1DDF7CB3945A/api.js\",\"ARKOSE_REGISTRATION_KEY\":\"95218C8B-B84E-413C-B875-785B35F92134\",\"ARKOSE_REGISTRATION_SRC\":\"https://wbd-api.arkoselabs.com/v2/95218C8B-B84E-413C-B875-785B35F92134/api.js\",\"AUTO_REFRESH_INTERVAL\":\"20\",\"BREAKING_NEWS_BANNER_CMS_ENABLED\":true,\"BROWSI_SRC\":\"https://cdn.browsiprod.com/bootstrap/bootstrap.js\",\"NATIVO_SRC\":\"https://s.ntv.io/serve/load.js\",\"CHARTBEAT_SRC\":\"https://static.chartbeat.com/js/chartbeat_mab.js\",\"CLAY_SITE_HOSTS_MAP\":{\"cnn\":\"cms.cnn.com\"},\"CMS_EVENT_BUS_BATCH_SIZE\":\"10\",\"CNN_DATAVIZ_API\":\"https://production.dataviz.cnn.io\",\"CNN_DIGITAL_PROFILE_PUBLICIST\":\"Emily Kuhn\",\"COLLABORATION_PORT\":\"4001\",\"COLLABORATION_SITE_HOSTS_MAP\":{\"cms.cnn.com\":\"collaboration-prod-rn01171x-cnn.content-hub.cnn-cms.com\"},\"COLLABORATION_EXCLUDED_TYPES\":[\"audio\",\"custom\",\"feed\",\"interactive\",\"livestory\",\"profile\",\"scratchpad\",\"search\",\"static\",\"unknown\",\"user_management\"],\"CONTENT_HUB_APP_VERSION\":\"v5.0.0\",\"CONTENT_HUB_ENV\":\"prod\",\"CONTENT_HUB_PROJECT_NAME\":\"content-hub\",\"CONTENT_HUB_UNIQUE_DEPLOYMENT_KEY\":\"rn01171x\",\"CONVIVA_CUSTOMER_KEY\":\"a6709203f34992a5095d2bc7ceaf2ec504f651a8\",\"DALTON_ENV\":\"production\",\"DALTON_COOKIE_VERSION\":\"v1.1\",\"DAM_API_HOST\":\"https://dam2.cms.cnn.com\",\"DAM_ACCESS_KEY\":\"b28f4002267c430b85918a3fdf75c0ea\",\"DAM_DEFAULT_PATH\":\"/stellar/prod\",\"DAM_SERVING_HOST\":\"https://media.cnn.com\",\"DALTON_API_HOST\":\"https://audience.cnn.com\",\"DALTON_TKN_HEADER_CHECK_ENABLED\":true,\"DISTROSCALE_SRC\":\"https://a.jsrdn.com/creatives/23053/cw.js\",\"EDIT_MODE_DATADOG_CLIENT_TOKEN\":\"pub2b644e04db84cf08661aa1cea78ce1cf\",\"DEDUPLICATION_ENABLED\":false,\"DIANOMI_SCRIPT_SRC\":\"https://www.dianomi.com/js/contextfeed.js\",\"DISPLAY_WORDCOUNT_ON_CARDS\":true,\"DISPLAY_VIDEO_DURATION_ON_CARDS\":true,\"ELECTION_MAP_PROOF_OF_CONCEPT_COMPONENT_ENABLED\":false,\"ENABLE_AD_LAZY_LOADING\":true,\"ENABLE_AD_FEEDBACK_DISPLAY_ADS\":true,\"ENABLE_AD_FEEDBACK_VIDEO_ADS\":true,\"ENABLE_RELEVANCE_USER_JS\":true,\"ENABLE_AUTO_REFRESH\":true,\"ENABLE_VIDEO_AUTOSTART_ON_ARTICLE\":false,\"ENABLE_VIDEO_AUTOSTART_ON_VIDEOLEAF\":true,\"ENABLE_VIDEO_AUTOSTART_ON_LIVESTORY\":false,\"ENABLE_AD_SLOT_CLIENT_INJECTOR\":true,\"ENABLE_ADFUEL\":true,\"ENABLE_ADFUEL_METRICS\":false,\"ENABLE_BROWSI\":true,\"ENABLE_NATIVO\":true,\"ENABLE_BROWSI_SERVER_AD_REGISTRIES\":true,\"ENABLE_CHARTBEAT\":true,\"ENABLE_DATADOG_TELEMETRY\":true,\"ENABLE_EXCLUDE_FEATURES\":false,\"ENABLE_GOOGLE_TAG_MANAGER\":true,\"ENABLE_UNDERSCORED_HUMAN_BOT_CONFIG\":true,\"UNDERSCORED_HUMAN_BOT_CONFIG_SRC\":\"https://www.cnn.com/cnn-underscored/prod/init.js\",\"ENABLE_PW_RESET_ARKOSE\":true,\"ENABLE_LIVE_STORY_UPDATES\":false,\"ENABLE_LOGIN_ARKOSE\":true,\"ENABLE_NEWSLETTERS_ARKOSE\":true,\"ENABLE_ONE_TAP_PLAY\":true,\"ENABLE_ONE_TAP_CAROUSEL\":true,\"ENABLE_OPENWEB\":true,\"ENABLE_OPENWEB_SSO\":true,\"ENABLE_OPENWEB_MIDPROMO\":true,\"ENABLE_PAYMENT_ARKOSE\":true,\"ENABLE_ARKOSE_DATA_EXCHANGE\":true,\"ONE_TAP_PLAYLIST_ENDPOINT\":\"https://fave.api.cnn.io/v1/video-playlist?stellarUri=\",\"ENABLE_REGISTRATION_ARKOSE\":true,\"ENABLE_SERVER_AD_REGISTRIES\":true,\"ENABLE_SOVRN\":true,\"ENABLE_TAG_MANAGER\":true,\"ENABLE_USER_CONSENT\":true,\"ENABLE_USER_FIRST_LAST_NAME\":false,\"ENABLE_USER_FIRST_LAST_NAME_UPDATES\":true,\"ENABLE_ZETA\":true,\"ENABLE_ZION\":true,\"ENABLE_ZION_ANALYTICS_CLICK_EVENTS\":true,\"ENABLE_ZION_ANALYTICS_ON_OFF_EVENTS\":true,\"ENSIGHTEN_SRC\":\"https://agility.cnn.com/turner/cnn-prod/Bootstrap.js\",\"FACEBOOK_APP_ID\":\"80401312489\",\"FAVE_TOP_PLAYER\":{\"ads\":{\"default\":{\"ssai\":{\"dev\":{\"clips\":{\"enabled\":true,\"profile\":\"m6Np541neR\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"UsIeS2TKlX\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"2iUzxPSeOP\"}},\"environment\":\"prod\",\"prod\":{\"clips\":{\"enabled\":true,\"profile\":\"5lycn5OPFj\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"33hkbvnyaO\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"ENHa1vBbDp\"}}}},\"livestory\":{\"ssai\":{\"dev\":{\"clips\":{\"enabled\":true,\"profile\":\"N5SsGHrH8R\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"UsIeS2TKlX\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"U0k3XgD9A0\"}},\"environment\":\"prod\",\"prod\":{\"clips\":{\"enabled\":true,\"profile\":\"sqKNPXeFWm\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"33hkbvnyaO\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"TBn9mv6qeq\"}}}}}},\"FAVE_MEDIA_PLAYER\":\"top\",\"FOLLOW_CONTENT_API\":\"https://content.api.cnn.com\",\"FOLLOW_CONTENT_API_UDK\":\"\",\"FOLLOW_COOKIE_NAME\":\"cnn_follow_v1\",\"FOLLOW_FEATURE_ENABLED\":false,\"FOLLOW_AUDIENCE\":\"\",\"FOLLOW_COMPONENTS_ENABLED\":\"\",\"GIZMO_UK_SERVER_ENDPOINT\":\"/gizmo/api/1/wingman\",\"GIZMO_UK_STRIPE_PUBLISH_KEY\":\"pk_live_51IdcnkJ8No30pLfwQoIZQCXHkAv62Y0s6hjOqbhuqOUORTluS4P1wThSRlTrh9Z78Uy41mNZWWRYrOwwKBOptyTa001tdtas8n\",\"GOOGLE_TAG_MANAGER_ID\":\"GTM-KJZD388\",\"KILN_EDIT_MODE_KEYWORD\":\"edit\",\"LAZYLOAD_BUFFER_DESKTOP\":\"200\",\"LAZYLOAD_BUFFER_MOBILE\":\"400\",\"ONE_TRUST_SRC\":\"https://cdn.cookielaw.org/scripttemplates/otSDKStub.js\",\"OPTIMIZELY_BASE_SRC\":\"https://cdn.optimizely.com/public/125375509/s/\",\"OPTIMIZELY_ENV\":\"prod\",\"MARKETS_QUOTES_SRC\":\"https://markets.money.cnn.com/services/api/quotehover/multiquote.asp?symb=\",\"CNN_BUSINESS_API\":\"https://api.business.cnn.io\",\"CNN_BUSINESS_MONEY_HOST\":\"https://money.cnn.com\",\"MARKETS_SEARCH_SRC\":\"https://markets.money.cnn.com/common/symbolLookup/getSymbols.asp?jsoncallback=symbolSearch&callback=symbolSearch&render=JSON&q=\",\"MEDIUM_SERVICE_ENVIRONMENT\":\"prod\",\"OPENWEB_DEFAULT_SECTIONS\":\"travel\",\"OPENWEB_LAUNCHER_SRC\":\"https://launcher.spot.im/spot/sp_hsRkxHeO\",\"OPENWEB_SSO_LAUNCHER_SRC\":\"https://launcher.spot.im/spot/sp_4hCVuB3p\",\"OPENWEB_SPOT_ID\":\"sp_hsRkxHeO\",\"OPENWEB_SSO_SPOT_ID\":\"sp_4hCVuB3p\",\"OPENWEB_PLACEMENT\":\"inline\",\"PERSONALIZED_RECIRC_API\":\"https://prod.di.api.cnn.io/recommendations\",\"PERSONALIZED_RECIRC_TENANT_ID\":\"read-next-from-article.mobileweb\",\"PRISM_APP_ID\":\"5e9f25a81c9d440000a83808\",\"PRISM_BRAND\":\"cnn\",\"PRISM_ENV\":\"prod\",\"PYMJS_SRC\":\"https://cdn.cnn.com/cnn/.e/interactive/js/lib/vendor/pym/pym.v1.min.js\",\"REGWALL_FEATURE_ENABLED\":false,\"SOVRN_SRC\":\"https://get.s-onetag.com/c15ddde9-ec7d-4a49-b8ca-7a21bc4b943b/tag.min.js\",\"SEARCH_API_ENDPOINT_URL\":\"https://search.prod.di.api.cnn.io/content\",\"SERVICE_BUILD_TYPE\":\"renderer\",\"TAG_MANAGER\":\"adobe\",\"TECH_STACK\":\"stellar2.0\",\"TOP_AD_RENDER_STICKY_TIMEOUT\":\"3000\",\"TOP_AUTH_SRC\":\"https://turnip.cdn.turner.com/top/auth/2.12.1-22/auth.min.js\",\"TOP_AUTH_ENV\":\"@top_auth_env\",\"TOP_AUTH_ECID\":\"37D8CAC3-36E0-46D9-B160-CB987896CCEF\",\"TOP_AUTH_MVPD_CONFIG_URL\":\"https://tvem.cdn.turner.com/v2/getConfig?brand=CNN&platform=web&country=US\",\"TOP_AUTH_SERVICE_APP_ID\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuZXR3b3JrIjoiY25uIiwicHJvZHVjdCI6ImNubiIsInBsYXRmb3JtIjoid2ViLXRvcDIiLCJhcHBJZCI6ImNubi1jbm4td2ViLXRvcDItOWowYnI2In0.TbUdtroeG7T1gfSTUfdobssbI8vPsAX6tFEX5KI8hcA\",\"TOP_AUTH_SOFTWARE_STATEMENT\":\"eyJhbGciOiJSUzI1NiJ9.eyJzdWIiOiIyY2QwZTZiZC01ZjFlLTRhMjItYTRhMC01Njg3YzNjOWI3NTEiLCJuYmYiOjE1MzcxOTA3NTcsImlzcyI6ImF1dGguYWRvYmUuY29tIiwiaWF0IjoxNTM3MTkwNzU3fQ.tBxO0aQhj8sy6RPiDMeThvvZgBkYRNVr1VseVCV3soJZdQJO7dWCcjeNghS8Qg2pc4u7vy6MQNtABcMU25BnCEBH8xKBf4HWb49NaFQLnmdXQULpfc1Uts5_CY0ALAtMgmfEdI_lzB9a80FuEiZ4VZcGxSpy7QTgZZivBqaq9hk71Yynhik9nsCv8pcHUKBkdq5W4lMyMGbDVGlCcHepmjj3yohzyc-4_gsfqtkaJHQBBAXSSqYVTKkg6bM-1GmKm2nBhjDBTHngM3vyA0YjpZ5dVsrGkRpGdfXLnCYB_9T91h-dYV8tle_V0HiLAn_8EVOmuQmKl7BzBJlERwo8JA\",\"TOP_AUTH_SESSION_NAME\":\"com.turner.top-2.activationRegCode\",\"TOP_FREEVIEW_SRC\":\"https://turnip.cdn.turner.com/top/freeview/2.12.1-22/freeview.min.js\",\"TOP_FREEVIEW_ENV\":\"prod\",\"TOP_FREEVIEW_SECRET_KEY\":\"hhX*-sB*YqRDpgs7RFTCacJocTFarXQf\",\"PLUS_TOP_AUTH_MVPD_CONFIG_URL\":\"https://ite.api.tvemanager.ngtv.io/v2/getConfig?brand=cnnplus&platform=web&country=US\",\"EMPLOYEE_TOP_AUTH_MVPD_CONFIG_URL\":\"https://ite.api.tvemanager.ngtv.io/v2/getConfig?brand=cnnplusee&platform=web\",\"TRINITY_CONFIGURATION.domestic.michonne.features.enableBrowsi\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableMyFinance\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableIndexExchange\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enablePrebid\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableAmazonDisplayAds\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableCep\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableIntegralAdScience\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableInViewRefresh\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableMalvertisingDetection\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableProximic\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableSourcePoint\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableBlockThrough\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableSSAI\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableHHID\":true,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableFreewheelProgrammatic\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableBrowsi\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableMyFinance\":false,\"TRINITY_CONFIGURATION.international.michonne.features.enableIndexExchange\":false,\"TRINITY_CONFIGURATION.international.michonne.features.enablePrebid\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableAmazonDisplayAds\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableCep\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableIntegralAdScience\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableInViewRefresh\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableMalvertisingDetection\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableProximic\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableSourcePoint\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableBlockThrough\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableSSAI\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableHHID\":true,\"TRINITY_CONFIGURATION.international.michonne.features.enableFreewheelProgrammatic\":true,\"TRINITY_CONFIGURATION.siteinfo.userAccountLegalDocs\":[{\"docName\":\"TOS\",\"version\":\"1.0\",\"label\":\"By clicking Register you confirm you have read and agree to our Terms and Conditions and acknowledge our Privacy Policy.\",\"type\":\"domestic\"},{\"docName\":\"TOS-Intl\",\"version\":\"1.0\",\"label\":\"By clicking Register you confirm you have read and agree to our Terms and Conditions and acknowledge our Privacy Policy.\",\"type\":\"intl\"}],\"TRINITY_CONFIGURATION.domestic.michonne.autoStartDisabledMobileSections\":[\"world\",\"weather\",\"vr\",\"us\",\"uk\",\"tennis\",\"tech\",\"success\",\"sport\",\"politics\",\"perspectives\",\"opinions\",\"olympics\",\"movies\",\"motorsport\",\"middleeast\",\"media\",\"living\",\"investing\",\"india\",\"health\",\"golf\",\"football\",\"europe\",\"entertainment\",\"energy\",\"economy\",\"china\",\"cars\",\"business-india\",\"business\",\"australia\",\"asia\",\"africa\",\"americas\"],\"TRINITY_CONFIGURATION.international.michonne.autoStartDisabledMobileSections\":[\"world\",\"weather\",\"vr\",\"us\",\"uk\",\"tennis\",\"tech\",\"success\",\"sport\",\"politics\",\"perspectives\",\"opinions\",\"olympics\",\"movies\",\"motorsport\",\"middleeast\",\"media\",\"living\",\"investing\",\"india\",\"health\",\"golf\",\"football\",\"europe\",\"entertainment\",\"energy\",\"economy\",\"china\",\"cars\",\"business-india\",\"business\",\"australia\",\"asia\",\"africa\",\"americas\"],\"TRINITY_CONFIGURATION.domestic.michonne.video.fave\":{\"adobeAnalytics\":{\"enabled\":true},\"ads\":{\"ssai\":{\"dev\":{\"clips\":{\"profile\":\"m6Np541neR\"},\"liveAuth\":{\"profile\":\"UsIeS2TKlX\"},\"liveUnauth\":{\"profile\":\"2iUzxPSeOP\"}},\"prod\":{\"clips\":{\"profile\":\"5lycn5OPFj\"},\"liveAuth\":{\"profile\":\"33hkbvnyaO\"},\"liveUnauth\":{\"profile\":\"ENHa1vBbDp\"}}}},\"amazonA9\":{\"enabled\":true,\"refreshedTargetingData\":{\"timeout\":1000},\"targetingData\":{\"timeout\":500}},\"autoplayMuteEnabledPages\":{\"sections\":[\"entertainment\",\"health\",\"homepage\",\"intl_homepage\",\"opinions\",\"politics\",\"us\",\"videos\",\"vr\",\"world\"]},\"chartbeat\":{\"enabled\":true},\"conviva\":{\"applicationName\":\"CNN-FAVE\",\"custom\":{\"applicationName\":\"CNN-Web\",\"applicationNameByVertical\":{\"business\":\"CNN-Web-Business\"}},\"customerKey\":\"a6709203f34992a5095d2bc7ceaf2ec504f651a8\",\"enabled\":true,\"gatewayUrl\":\"\",\"integration\":\"conviva\"},\"cssUrl\":\"https://registry.api.cnn.io/bundles/fave/latest-4.x/css\",\"enabledPageTypes\":{\"exclude\":{\"article\":[\"studentnews\"],\"section\":[\"studentnews\"],\"video\":[\"studentnews\"]}},\"enableFaveContentXml\":true,\"freewheel\":{\"globalAdTimer\":{\"adComplete\":{\"errorCode\":{\"skip\":\"SKIP_CURRENT_AD_COMPLETE\",\"stop\":\"STOP_CURRENT_AD_COMPLETE\"},\"timeout\":30000,\"type\":\"adComplete\"},\"adWaterfall\":{\"errorCode\":{\"skip\":\"SKIP_CURRENT_AD_WATERFALL\",\"stop\":\"STOP_CURRENT_AD_WATERFALL\"},\"timeout\":30000,\"type\":\"adWaterfall\"},\"enabled\":true,\"errorInfo\":{\"skip\":\"A custom global ad timeout of {timeout} milliseconds caused the skipCurrentAd() function to be invoked. Attempt {skipCurrentAdAttempts} of {maxSkipCurrentAdAttempts}. Type: {type}\",\"stop\":\"The maximum of {maxSkipCurrentAdAttempts} skip current ad attempts has been exceeded causing the stop() function to be invoked. Timeout: {timeout} milliseconds. Type: {type}.\"},\"maxSkipCurrentAdAttempts\":0}},\"iframe\":\"\",\"injectCss\":false,\"injectorJs\":{\"featureName\":\"cnn-fave-lib\",\"source\":\"https://registry.api.cnn.io/bundles/fave/latest-4.x/js\"},\"live\":{\"enabled\":true,\"enabledLiveStreams\":[\"cvplive/cvpstream0\",\"cvplive/cvpstream1\",\"cvplive/cvpstream2\",\"cvplive/cvpstream3\",\"cvplive/cvpstream4\",\"cvplive/cnngo\",\"cvplive/cnniuk\"]},\"mediaPlayer\":\"top\",\"oneTapEnabledPages\":{\"pageTypes\":[\"section\"],\"sections\":[\"homepage\",\"intl_homepage\",\"business\",\"health\",\"opinions\",\"politics\",\"us\",\"world\"]},\"oneClickEnabledPages\":{\"pageTypes\":[\"section\"],\"sections\":[\"homepage\",\"intl_homepage\"]},\"openMeasurement\":{\"enabled\":true},\"optimizely\":{\"enabled\":false},\"player\":{\"autoplay\":{\"compatibility\":{\"testMobile\":true},\"muted\":{\"desktop\":{\"enabled\":true,\"viewportChange\":{\"pauseVideoOnViewportChange\":true,\"playerInViewportPercent\":50}},\"mobile\":{\"enabled\":true,\"viewportChange\":{\"pauseVideoOnViewportChange\":true,\"playerInViewportPercent\":50}},\"unmuteCTA\":{\"variant\":{\"shrink\":false,\"wave\":false}}}},\"autoStopLive\":{\"timeout\":1200000},\"closedCaptionsOn\":false,\"closedCaptionsThreshold\":0.2,\"maxBitrate\":\"1500000\",\"message\":{\"liveOffline\":\"The live stream went offline.Player will resume on rebroadcast.\",\"error\":\"The video player encountered an error.\"},\"poster\":{\"big\":\"768x432\",\"small\":\"640x360\",\"override\":true,\"overrideImages\":{\"big\":\"medium\",\"small\":\"small\"}},\"screenOrientationManager\":{\"fullscreenOnLandscape\":true},\"stateRemembrance\":{\"closedCaptions\":{\"enabled\":true}},\"ui\":{\"theme\":{\"adCountdown\":{\"shouldRender\":false}}},\"vr\":{\"clickAndDragCta\":{\"enabled\":true}}},\"prebid\":{\"enabled\":false},\"server\":{\"medium\":{\"enabled\":true,\"environment\":\"prod\"}},\"stellar\":{\"ads\":{\"default\":{\"ssai\":{\"dev\":{\"clips\":{\"enabled\":true,\"profile\":\"m6Np541neR\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"UsIeS2TKlX\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"2iUzxPSeOP\"}},\"environment\":\"prod\",\"prod\":{\"clips\":{\"enabled\":true,\"profile\":\"5lycn5OPFj\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"33hkbvnyaO\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"ENHa1vBbDp\"}}}},\"fastLiveStreamDesktopWeb\":{\"ssai\":{\"dev\":{\"liveUnauth\":{\"enabled\":true,\"profile\":\"jd7CwJlXEW\"}},\"environment\":\"prod\",\"prod\":{\"liveUnauth\":{\"enabled\":true,\"profile\":\"jd7CwJlXEW\"}}}},\"fastLiveStreamMobileWeb\":{\"ssai\":{\"dev\":{\"liveUnauth\":{\"enabled\":true,\"profile\":\"JEIXPY2Q3E\"}},\"environment\":\"prod\",\"prod\":{\"liveUnauth\":{\"enabled\":true,\"profile\":\"JEIXPY2Q3E\"}}}},\"livestory\":{\"ssai\":{\"dev\":{\"clips\":{\"enabled\":true,\"profile\":\"N5SsGHrH8R\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"UsIeS2TKlX\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"U0k3XgD9A0\"}},\"environment\":\"prod\",\"prod\":{\"clips\":{\"enabled\":true,\"profile\":\"sqKNPXeFWm\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"33hkbvnyaO\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"TBn9mv6qeq\"}}}}},\"fastLiveStreams\":[\"livec76319f599742ab668c8b3ba6dcfed3ce7e817ad\",\"live89dc8d181af9acac4036fff1055df79a4d4ee33d\",\"live51fd6cf689647b6d6ca0bcd2d6f4e69c30dbdc49\",\"livedbcedb554833b248c3ce8374acd2bbcd3983d7dd\"],\"mediaPlayer\":\"top\"},\"windows7PreferredFileType\":\"mp4\",\"zion\":{\"bridgeEnabled\":true,\"enabled\":true,\"enableLogging\":false,\"environment\":\"prod\"}},\"TRINITY_CONFIGURATION.international.michonne.video.fave\":{\"adobeAnalytics\":{\"enabled\":true},\"ads\":{\"ssai\":{\"dev\":{\"clips\":{\"profile\":\"TMhPsequTq\"},\"liveAuth\":{\"profile\":\"56bYhbIS7X\"},\"liveUnauth\":{\"profile\":\"56bYhbIS7X\"}},\"prod\":{\"clips\":{\"profile\":\"TMhPsequTq\"},\"liveAuth\":{\"profile\":\"56bYhbIS7X\"},\"liveUnauth\":{\"profile\":\"56bYhbIS7X\"}}}},\"amazonA9\":{\"enabled\":true,\"refreshedTargetingData\":{\"timeout\":1000},\"targetingData\":{\"timeout\":500}},\"autoplayMuteEnabledPages\":{\"sections\":[\"entertainment\",\"health\",\"homepage\",\"intl_homepage\",\"opinions\",\"politics\",\"us\",\"videos\",\"vr\",\"world\"]},\"chartbeat\":{\"enabled\":true},\"conviva\":{\"applicationName\":\"CNN-FAVE\",\"custom\":{\"applicationName\":\"CNN-Web\",\"applicationNameByVertical\":{\"business\":\"CNN-Web-Business\"}},\"customerKey\":\"a6709203f34992a5095d2bc7ceaf2ec504f651a8\",\"enabled\":true,\"gatewayUrl\":\"\",\"integration\":\"conviva\"},\"cssUrl\":\"https://registry.api.cnn.io/bundles/fave/latest-4.x/css\",\"enabledPageTypes\":{\"exclude\":{\"article\":[\"studentnews\"],\"section\":[\"studentnews\"],\"video\":[\"studentnews\"]}},\"enableFaveContentXml\":true,\"freewheel\":{\"globalAdTimer\":{\"adComplete\":{\"errorCode\":{\"skip\":\"SKIP_CURRENT_AD_COMPLETE\",\"stop\":\"STOP_CURRENT_AD_COMPLETE\"},\"timeout\":30000,\"type\":\"adComplete\"},\"adWaterfall\":{\"errorCode\":{\"skip\":\"SKIP_CURRENT_AD_WATERFALL\",\"stop\":\"STOP_CURRENT_AD_WATERFALL\"},\"timeout\":30000,\"type\":\"adWaterfall\"},\"enabled\":true,\"errorInfo\":{\"skip\":\"A custom global ad timeout of {timeout} milliseconds caused the skipCurrentAd() function to be invoked. Attempt {skipCurrentAdAttempts} of {maxSkipCurrentAdAttempts}. Type: {type}\",\"stop\":\"The maximum of {maxSkipCurrentAdAttempts} skip current ad attempts has been exceeded causing the stop() function to be invoked. Timeout: {timeout} milliseconds. Type: {type}.\"},\"maxSkipCurrentAdAttempts\":0}},\"iframe\":\"\",\"injectCss\":false,\"injectorJs\":{\"featureName\":\"cnn-fave-lib\",\"source\":\"https://registry.api.cnn.io/bundles/fave/latest-4.x/js\"},\"live\":{\"enabled\":true,\"enabledLiveStreams\":[\"cvplive/cvpstream0\",\"cvplive/cvpstream1\",\"cvplive/cvpstream2\",\"cvplive/cvpstream3\",\"cvplive/cvpstream4\",\"cvplive/cnngo\",\"cvplive/cnniuk\"]},\"mediaPlayer\":\"top\",\"oneTapEnabledPages\":{\"pageTypes\":[\"section\"],\"sections\":[\"homepage\",\"intl_homepage\",\"business\",\"health\",\"opinions\",\"politics\",\"us\",\"world\"]},\"oneClickEnabledPages\":{\"pageTypes\":[\"section\"],\"sections\":[\"homepage\",\"intl_homepage\"]},\"openMeasurement\":{\"enabled\":true},\"optimizely\":{\"enabled\":false},\"player\":{\"autoplay\":{\"compatibility\":{\"testMobile\":true},\"muted\":{\"desktop\":{\"enabled\":true,\"viewportChange\":{\"pauseVideoOnViewportChange\":true,\"playerInViewportPercent\":50}},\"mobile\":{\"enabled\":true,\"viewportChange\":{\"pauseVideoOnViewportChange\":true,\"playerInViewportPercent\":50}},\"unmuteCTA\":{\"variant\":{\"shrink\":false,\"wave\":false}}}},\"autoStopLive\":{\"timeout\":1200000},\"closedCaptionsOn\":false,\"closedCaptionsThreshold\":0.2,\"maxBitrate\":\"1500000\",\"message\":{\"liveOffline\":\"The live stream went offline.Player will resume on rebroadcast.\",\"error\":\"The video player encountered an error.\"},\"poster\":{\"big\":\"768x432\",\"small\":\"640x360\",\"override\":true,\"overrideImages\":{\"big\":\"medium\",\"small\":\"small\"}},\"screenOrientationManager\":{\"fullscreenOnLandscape\":true},\"stateRemembrance\":{\"closedCaptions\":{\"enabled\":true}},\"ui\":{\"theme\":{\"adCountdown\":{\"shouldRender\":false}}},\"vr\":{\"clickAndDragCta\":{\"enabled\":true}}},\"prebid\":{\"enabled\":true},\"server\":{\"medium\":{\"enabled\":true,\"environment\":\"prod\"}},\"stellar\":{\"ads\":{\"default\":{\"ssai\":{\"dev\":{\"clips\":{\"enabled\":true,\"profile\":\"m6Np541neR\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"UsIeS2TKlX\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"2iUzxPSeOP\"}},\"environment\":\"prod\",\"prod\":{\"clips\":{\"enabled\":true,\"profile\":\"5lycn5OPFj\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"33hkbvnyaO\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"ENHa1vBbDp\"}}}},\"fastLiveStreamDesktopWeb\":{\"ssai\":{\"dev\":{\"liveUnauth\":{\"enabled\":true,\"profile\":\"5I8NQT75Ti\"}},\"environment\":\"prod\",\"prod\":{\"liveUnauth\":{\"enabled\":true,\"profile\":\"5I8NQT75Ti\"}}}},\"fastLiveStreamMobileWeb\":{\"ssai\":{\"dev\":{\"liveUnauth\":{\"enabled\":true,\"profile\":\"NwRsq2FBUw\"}},\"environment\":\"prod\",\"prod\":{\"liveUnauth\":{\"enabled\":true,\"profile\":\"NwRsq2FBUw\"}}}},\"livestory\":{\"ssai\":{\"dev\":{\"clips\":{\"enabled\":true,\"profile\":\"N5SsGHrH8R\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"UsIeS2TKlX\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"U0k3XgD9A0\"}},\"environment\":\"prod\",\"prod\":{\"clips\":{\"enabled\":true,\"profile\":\"sqKNPXeFWm\"},\"liveAuth\":{\"enabled\":true,\"profile\":\"33hkbvnyaO\"},\"liveUnauth\":{\"enabled\":true,\"profile\":\"TBn9mv6qeq\"}}}}},\"fastLiveStreams\":[\"livec76319f599742ab668c8b3ba6dcfed3ce7e817ad\",\"live89dc8d181af9acac4036fff1055df79a4d4ee33d\",\"live51fd6cf689647b6d6ca0bcd2d6f4e69c30dbdc49\",\"livedbcedb554833b248c3ce8374acd2bbcd3983d7dd\"],\"mediaPlayer\":\"top\"},\"windows7PreferredFileType\":\"mp4\",\"zion\":{\"bridgeEnabled\":true,\"enabled\":true,\"enableLogging\":false,\"environment\":\"prod\"}},\"TRINITY_CONFIGURATION.domestic.michonne.features.enableAutoplayMuted\":false,\"TRINITY_CONFIGURATION.international.michonne.features.enableAutoplayMuted\":false,\"TRINITY_CONFIGURATION.domestic.michonne.features.enableAutoplayBlock\":false,\"TRINITY_CONFIGURATION.international.michonne.features.enableAutoplayBlock\":false,\"TRINITY_CONFIGURATION.domestic.michonne.ads.adfuelOptionsOverrides\":{\"business\":{},\"default\":{}},\"TRINITY_CONFIGURATION.international.michonne.ads.adfuelOptionsOverrides\":{\"business\":{},\"default\":{}},\"USER_CONSENT_COOKIE\":\"OptanonConsent\",\"USER_CONSENT_COOKIE_DOMAIN\":\".cnn.com\",\"USER_CONSENT_COOKIE_SAMESITE\":\"None\",\"USER_CONSENT_COOKIE_SECURE\":true,\"USER_CONSENT_CONFIRM_COOKIE\":\"OptanonAlertBoxClosed\",\"USER_CONSENT_DOM_ID\":\"3d9a6f21-8e47-43f8-8d58-d86150f3e92b\",\"USER_ACCOUNT_AVATAR_BASE_URL\":\"https://d2otbl5v981rj6.cloudfront.net/static/images/avatars/\",\"USER_ACCOUNT_ENABLED\":true,\"USER_ACCOUNT_PAYMENTS_ENABLED\":true,\"USER_ACCOUNT_RESTRICTED_VIEWS_ENABLED\":true,\"USER_SERVICES_ENABLED\":true,\"USER_ACCOUNT_ONBOARDING_ENABLED\":true,\"USER_ACCOUNT_MOTIF_ENABLED\":true,\"VIDEO_EMBED_URL\":\"https://fave.api.cnn.io/v1/fav/?video=\",\"AMP_VIDEO_EMBED_URL\":\"https://fave-api.cnn.com/v1/amp/?video=\",\"NEWSLETTER_ACQUISITION_ENABLED\":true,\"NEWSLETTER_LANDING_ACQUISITION_ENABLED\":true,\"WOPR_API_URL\":\"https://wopr.turnerapps.com\",\"ZETA_SITE_ID\":\"cnn\",\"ZETA_CLIENT_HASH_KEY\":\"16b6410431b6374e780104abb0443ca8\",\"ZETA_PARTNER_HASH_KEY\":\"34747f0775f02a6784bb965de6833e73\",\"ZETA_SHORT_NAME\":\"cnn-pixel-8786\",\"ZION_API_KEY\":\"mXFw59FFEpUNOu3aeVJChKAsqAlZ4NEf\",\"ZION_BEHAVIOURAL_ENABLED_PAGE_VARIANTS\":[\"article_leaf\",\"markets\"],\"ZION_CLICK_OBSERVED_COMPONENTS\":[\"footer\",\"gallery\",\"header\",\"related-content\",\"video\",\"image\"],\"ZION_ENV\":\"Prod\",\"ZION_ON_OFF_OBSERVED_COMPONENTS\":[\"bizdev-outbrain\",\"footer\",\"headline\",\"paragraph\",\"related-content\",\"market-tabbed-container\",\"market-fng-indicator\"],\"ZION_SRC\":\"https://z.cdp-dev.cnn.com/zion-web-client/3.0/zion-web-client.min.js\",\"ZION_TELEMETRY_ENDPOINT\":\"//zion-telemetry.api.cnn.io\",\"FAVE_SRC\":\"https://registry.api.cnn.io/bundles/fave/latest-4.x/js\",\"PARSELY_SRC\":\"@parsely_src\",\"SSE_HOST\":\"https://sse01.cnn.com\",\"UNDERSCORED_GET_AFFILIATE_TAG_API_URL\":\"https://bvrmvkrkie.execute-api.us-east-1.amazonaws.com/v1/get-affiliate-tag\",\"UNDERSCORED_API_HOST\":\"web-prod-ursd0001\",\"UNDERSCORED_ACCESS_KEY\":\"produnderscoredaccesskey\",\"MOBILE_GOOGLE_AD_ACCOUNT_ID\":\"8663477\",\"PUBLIC_GOOD_WIDGET_ENABLED\":true,\"PUBLIC_GOOD_WIDGET_SRC\":\"https://assets.publicgood.com/pgm/v1/dpg.js\",\"PUBLIC_GOOD_WIDGET_CONFIG_CLASS\":\"pgs-dpg-btn\",\"PUBLIC_GOOD_WIDGET_CONFIG_PARTNER_ID\":\"cnn\",\"PUBLIC_GOOD_WIDGET_CONFIG_TARGET_TYPE\":\"campaign\",\"MOBILE_WATCH_NEXT_URL\":\"https://prod.di.api.cnn.io/recs/v1/WatchNextVideo\",\"MOBILE_SUPPORTED_SECTIONS\":[\"opinions\",\"world\",\"us\",\"politics\",\"business\",\"health\",\"entertainment\",\"travel\",\"sport\",\"style\",\"videos\",\"weather\",\"homepage\",\"tv\",\"series\",\"wbd\",\"yourcnn\",\"bleacherreport\"],\"ENABLE_AMP_EXCLUDE_TEST\":true,\"AMP_EXCLUDE_SECTIONS\":\"[]\",\"AMP_EXCLUDE_PAGE_TYPES\":\"article\",\"FORCE_WEBP_IMAGES\":true,\"POLITICS_ELECTION_CONTEXT_FEED\":\"https://politics.api.cnn.io/available-races/all/index.json\",\"POLITICS_FEATURE_FLAG_BASEPATH\":\"https://politics-static.cnn.io/2021/feature-flags\",\"POLITICS_FEED_URL_BASEPATH\":\"https://politics.api.cnn.io\",\"POLITICS_MAP_URL_BASEPATH\":\"https://atlas.cnn.io/us\",\"POLITICS_STATIC_ASSETS_BASEPATH\":\"https://politics-static.cnn.io/\",\"RTCCONFIG_APS_PUB_ID\":\"3159\",\"CNN_DATA_API\":\"https://data.api.cnn.io\",\"PLEDGE_DONATION_ENABLED\":true,\"PLEDGE_DONATION_SRC\":\"https://www.pledge.to/assets/widget.js\",\"PLEDGE_DONATION_CONFIG_CLASS\":\"plg-donate\",\"AWS_REGION\":\"us-east-2\"}FTC bans TurboTax from advertising ‘free’ services, calls it deceptiveCNN Business window.CNN = window.CNN || {}; window.CNN.ads = {\"browsiRegistry\":[{\"rktr_deployed_date\":\"2023-08-02 11:02:23\",\"rktr_slot_id\":\"page\",\"rktr_id\":\"cnn_browsi_leaf\",\"gpt_id\":\"8663477\",\"site\":\"cnn\",\"root\":\"CNN\",\"targeting\":[[\"appname\",[\"browsi\"]]],\"child_directed_treatment\":false},{\"rktr_slot_id\":\"ad_browsi_atf_01\",\"rktr_ad_id\":\"CNN\",\"sizes\":[[2,2],[300,250],[320,320],[\"fluid\"]],\"targeting\":[[\"pos\",[\"browsi_atf_01\"]]],\"responsive\":[[[\"1024\",\"0\"],[\"suppress\"]],[[\"782\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"]]],[[\"0\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"]]]]},{\"rktr_slot_id\":\"ad_browsi_atf_02\",\"rktr_ad_id\":\"CNN\",\"sizes\":[[2,2],[300,250],[320,320]],\"targeting\":[[\"pos\",[\"browsi_atf_02\"]]],\"responsive\":[[[\"1024\",\"0\"],[\"suppress\"]],[[\"782\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"]]],[[\"0\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"]]]]},{\"rktr_slot_id\":\"ad_browsi_atf_03\",\"rktr_ad_id\":\"CNN\",\"sizes\":[[2,2],[300,250],[320,320]],\"targeting\":[[\"pos\",[\"browsi_atf_03\"]]],\"responsive\":[[[\"1024\",\"0\"],[\"suppress\"]],[[\"782\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"]]],[[\"0\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"]]]]},{\"rktr_slot_id\":\"ad_browsi_atf_04\",\"rktr_ad_id\":\"CNN\",\"sizes\":[[2,2],[300,250],[320,320]],\"targeting\":[[\"pos\",[\"browsi_atf_04\"]]],\"responsive\":[[[\"1024\",\"0\"],[\"suppress\"]],[[\"782\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"]]],[[\"0\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"]]]]},{\"rktr_slot_id\":\"ad_browsi_atf_05\",\"rktr_ad_id\":\"CNN\",\"sizes\":[[2,2],[300,250],[320,320]],\"targeting\":[[\"pos\",[\"browsi_atf_05\"]]],\"responsive\":[[[\"1024\",\"0\"],[\"suppress\"]],[[\"782\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"]]],[[\"0\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"]]]]},{\"rktr_slot_id\":\"ad_browsi_atf_06\",\"rktr_ad_id\":\"CNN\",\"sizes\":[[2,2],[300,250],[320,320]],\"targeting\":[[\"pos\",[\"browsi_atf_06\"]]],\"responsive\":[[[\"1024\",\"0\"],[\"suppress\"]],[[\"782\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"]]],[[\"0\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"]]]]},{\"rktr_slot_id\":\"ad_browsi_atf_07\",\"rktr_ad_id\":\"CNN\",\"sizes\":[[2,2],[300,250],[320,320]],\"targeting\":[[\"pos\",[\"browsi_atf_07\"]]],\"responsive\":[[[\"1024\",\"0\"],[\"suppress\"]],[[\"782\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"]]],[[\"0\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"]]]]},{\"rktr_slot_id\":\"ad_browsi_atf_08\",\"rktr_ad_id\":\"CNN\",\"sizes\":[[2,2],[300,250],[320,320]],\"targeting\":[[\"pos\",[\"browsi_atf_08\"]]],\"responsive\":[[[\"1024\",\"0\"],[\"suppress\"]],[[\"782\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"]]],[[\"0\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"]]]]},{\"rktr_slot_id\":\"ad_browsi_atf_09\",\"rktr_ad_id\":\"CNN\",\"sizes\":[[2,2],[300,250],[320,320]],\"targeting\":[[\"pos\",[\"browsi_atf_09\"]]],\"responsive\":[[[\"1024\",\"0\"],[\"suppress\"]],[[\"782\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"]]],[[\"0\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"]]]]},{\"rktr_slot_id\":\"ad_browsi_atf_10\",\"rktr_ad_id\":\"CNN\",\"sizes\":[[2,2],[300,250],[320,320]],\"targeting\":[[\"pos\",[\"browsi_atf_10\"]]],\"responsive\":[[[\"1024\",\"0\"],[\"suppress\"]],[[\"782\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"]]],[[\"0\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"2\",\"2\"]]]]}],\"lazyLoad\":true,\"registry\":[{\"rktr_deployed_date\":\"2024-01-22 16:20:33\",\"rktr_slot_id\":\"page\",\"rktr_id\":\"cnn_leaf\",\"gpt_id\":\"8663477\",\"site\":\"cnn_2\",\"root\":\"CNN\",\"child_directed_treatment\":false,\"targeting\":[]},{\"rktr_slot_id\":\"ad_bnr_atf_01\",\"rktr_ad_id\":\"CNN/business/leaf/bst\",\"sizes\":[[1,1],[1,2],[320,50],[728,90],[970,66],[970,90],[970,250],[\"fluid\"]],\"hasInViewRefresh\":true,\"inViewRefreshCount\":\"10\",\"inViewRefreshInterval\":\"35\",\"targeting\":[[\"pos\",[\"bnr_atf_01\"]]],\"responsive\":[[[\"1024\",\"0\"],[[\"970\",\"250\"],[\"970\",\"90\"],[\"970\",\"66\"],[\"728\",\"90\"],[\"1\",\"2\"],[\"1\",\"1\"],[\"fluid\"]]],[[\"728\",\"0\"],[[\"728\",\"90\"],[\"1\",\"2\"],[\"1\",\"1\"],[\"fluid\"]]],[[\"0\",\"0\"],[[\"320\",\"50\"],[\"1\",\"2\"],[\"1\",\"1\"],[\"fluid\"]]]]},{\"rktr_slot_id\":\"ad_rect_atf_01\",\"rktr_ad_id\":\"CNN/business/leaf/bst\",\"sizes\":[[1,1],[1,2],[2,2],[300,250],[300,600],[300,850],[300,1050],[320,320],[\"fluid\"]],\"hasInViewRefresh\":true,\"inViewRefreshCount\":\"10\",\"inViewRefreshInterval\":\"35\",\"targeting\":[[\"pos\",[\"rect_atf_01\"]]],\"responsive\":[[[\"1024\",\"0\"],[[\"300\",\"1050\"],[\"300\",\"850\"],[\"300\",\"600\"],[\"300\",\"250\"],[\"1\",\"2\"],[\"fluid\"]]],[[\"728\",\"0\"],[[\"300\",\"1050\"],[\"300\",\"850\"],[\"300\",\"600\"],[\"300\",\"250\"],[\"1\",\"2\"],[\"fluid\"]]],[[\"0\",\"0\"],[[\"2\",\"2\"],[\"1\",\"1\"],[\"fluid\"]]]]},{\"rktr_slot_id\":\"ad_rect_btf_01\",\"rktr_ad_id\":\"CNN/business/leaf/bst\",\"sizes\":[[1,1],[1,2],[300,250],[300,600],[320,320]],\"hasInViewRefresh\":true,\"inViewRefreshCount\":\"10\",\"inViewRefreshInterval\":\"35\",\"targeting\":[[\"pos\",[\"rect_btf_01\"]]],\"responsive\":[[[\"1024\",\"0\"],[[\"300\",\"600\"],[\"300\",\"250\"],[\"fluid\"]]],[[\"728\",\"0\"],[[\"300\",\"600\"],[\"300\",\"250\"],[\"fluid\"]]],[[\"0\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"1\",\"1\"]]]]},{\"rktr_slot_id\":\"ad_nat_btf_01\",\"rktr_ad_id\":\"CNN/business/leaf/bst\",\"sizes\":[[1,1],[1,2],[300,250],[780,175],[\"fluid\"]],\"targeting\":[[\"pos\",[\"nat_btf_01\"]]],\"responsive\":[[[\"1024\",\"0\"],[[\"780\",\"175\"],[\"1\",\"2\"],[\"1\",\"1\"],[\"fluid\"]]],[[\"728\",\"0\"],[[\"1\",\"2\"],[\"1\",\"1\"],[\"fluid\"]]],[[\"0\",\"0\"],[[\"1\",\"1\"],[\"fluid\"]]]]},{\"rktr_slot_id\":\"ad_rect_btf_02\",\"rktr_ad_id\":\"CNN/business/leaf/bst\",\"sizes\":[[1,1],[1,2],[300,250],[300,600],[320,320],[\"fluid\"]],\"hasInViewRefresh\":true,\"inViewRefreshCount\":\"10\",\"inViewRefreshInterval\":\"35\",\"targeting\":[[\"pos\",[\"rect_btf_02\"]]],\"responsive\":[[[\"1024\",\"0\"],[[\"300\",\"600\"],[\"300\",\"250\"]]],[[\"728\",\"0\"],[[\"320\",\"320\"],[\"300\",\"600\"],[\"300\",\"250\"]]],[[\"0\",\"0\"],[[\"320\",\"320\"],[\"300\",\"250\"],[\"1\",\"2\"],[\"1\",\"1\"],[\"fluid\"]]]]},{\"rktr_slot_id\":\"ad_ns_atf_01\",\"rktr_ad_id\":\"CNN/business/leaf/bst\",\"sizes\":[[120,60]],\"targeting\":[[\"pos\",[\"ns_atf_01\"]]],\"responsive\":[[[\"1024\",\"0\"],[[\"120\",\"60\"]]],[[\"728\",\"0\"],[[\"120\",\"60\"]]],[[\"0\",\"0\"],[[\"120\",\"60\"]]]]}],\"registryPath\":\"domestic/business/leaf/bst\",\"showAds\":true,\"rktr_ad_id\":\"CNN/business/leaf/bst\"}; window.CNN.cep_topics = {\"cep_brsf\":[\"15LP\",\"15PD\"],\"cep_iabt\":[\"14WQ\",\"14WR\",\"14VL\",\"14VF\",\"14VD\",\"15HT\"],\"cep_sent\":[\"16B7\"],\"cep_tags\":[\"2PCG\",\"2PCF\",\"2JP9\",\"2PCD\",\"FBL\",\"5GB\",\"7M5\",\"188\",\"7PS\",\"3VT\",\"435\",\"9N5\",\"80N\",\"8D3\",\"7WN\",\"86S\",\"11H5\",\"5FT\"],\"source_id\":\"article_clrpgr1df000026p74wr0eg6a\",\"short_source_id\":\"ar_clrpgr1df000026p74wr0eg6a\"}; window.CNN.contentModel = { _wedgerId: '', _wedgerLegacyCmsId: '', analytics: { authors: 'Ramishah Maruf', chartbeat: { sections: 'business' } }, author: 'Ramishah Maruf', branding: { key: '', spec: '', displayName: '' }, canonicalUrl: 'https://www.cnn.com/2024/01/22/business/ftc-turbotax-free-services/index.html', pageStellarId: 'L19wYWdlcy9jbHJwZ3IxZGYwMDAwMjZwNzR3cjBlZzZh', firstCanonicalUrl: 'https://www.cnn.com/2024/01/22/business/ftc-turbotax-free-services/index.html', cmsId: document.querySelector('html').dataset.uri, edition: false, environment: 'prod', headline: 'FTC bans TurboTax from advertising ‘free’ services, calls it deceptive', isSponsorship: false, last_updated_date: '2024-01-23T00:18:46.816Z', pageType: 'article', pageTags: '', published_date: '2024-01-22T23:25:17.546Z', section: 'business', subsection: '', subsubsection: '', sourceId: 'cms.cnn.com/_pages/clrpgr1df000026p74wr0eg6a@published', techStack: 'stellar2.0', templateType: 'article_leaf', vertical: 'business' }; window.CNN.omniture = { ...(window.CNN.omniture || {}), branding_content_page: '', cap_author: 'Ramishah Maruf', cap_content_type: 'article_leaf', content_id: document.querySelector('html').dataset.uri, content_type: 'adbp:none', gallery_name: '', headline: 'FTC bans TurboTax from advertising ‘free’ services, calls it deceptive', last_updated_date: '2024/01/23', publish_date: '2024/01/22', rs_flag: 'prod', section: [ 'business', '', '' ], source_id: 'cms.cnn.com/_pages/clrpgr1df000026p74wr0eg6a@published', template_type: 'article_leaf', video_opportunity: document.querySelectorAll('*[data-uri*=\"/_components/video-resource/\"]').length, cap_genre: '', cap_topic: '', screen_state: 'default' }; window.CNN.metadata = {\"content\":{\"author\":[\"Ramishah Maruf\"],\"branding\":[],\"byline\":\"By Ramishah Maruf, CNN\",\"canonicalUrl\":\"https://www.cnn.com/2024/01/22/business/ftc-turbotax-free-services/index.html\",\"firstCanonicalUrl\":\"https://www.cnn.com/2024/01/22/business/ftc-turbotax-free-services/index.html\",\"headline\":\"FTC bans TurboTax from advertising ‘free’ services, calls it deceptive\",\"identifiers\":{\"pageStellarId\":\"L19wYWdlcy9jbHJwZ3IxZGYwMDAwMjZwNzR3cjBlZzZh\"},\"pageType\":\"article\",\"pageVariant\":\"article_leaf\",\"publishDateCreated\":\"2024-01-22T21:52:38.838Z\",\"publishDatePublished\":\"2024-01-22T23:25:17.546Z\",\"publishDateModified\":\"2024-01-23T00:18:46.816Z\",\"section\":[\"business\"],\"topics\":{\"cepBrsf\":[\"15LP\",\"15PD\"],\"cepIabt\":[\"14WQ\",\"14WR\",\"14VL\",\"14VF\",\"14VD\",\"15HT\"],\"cepOther\":[\"2PCG\",\"2PCF\",\"2JP9\",\"2PCD\",\"FBL\",\"5GB\",\"7M5\",\"188\",\"7PS\",\"3VT\",\"435\",\"9N5\",\"80N\",\"8D3\",\"7WN\",\"86S\",\"11H5\",\"5FT\"],\"cepSent\":[\"16B7\"],\"cnnSections\":[\"business\"],\"tags\":[]},\"vertical\":\"business\",\"leadingMediaType\":\"image\",\"image\":[{\"identifiers\":{}}]}} window.ntvConfig = window.ntvConfig || {}; window.ntvConfig.keyValues = { ...(window.ntvConfig.keyValues || {}),'section': `business`,'subsection': ``,'page_type': `article_leaf`,'spec': ``,'cep_brsf': `15LP,15PD`,'cep_iabt': `14WQ,14WR,14VL,14VF,14VD,15HT`,'cep_sent': `16B7`,'cep_tags': `2PCG,2PCF,2JP9,2PCD,FBL,5GB,7M5,188,7PS,3VT,435,9N5,80N,8D3,7WN,86S,11H5,5FT`,}; window.CNN.Zion = { ...(window.CNN.Zion || {}),'apiKey': `mXFw59FFEpUNOu3aeVJChKAsqAlZ4NEf`,'environmentType': `Prod`,'sourceId': `cms.cnn.com/_pages/clrpgr1df000026p74wr0eg6a@published`,}; window.CNN.helpers = { PAGE_VARIANTS: {\"ARTICLE_FEATURE\":\"article_feature\",\"ARTICLE_FULLWIDTH\":\"article_fullwidth\",\"ARTICLE\":\"article_leaf\",\"GALLERY_UNFURLED\":\"gallery_unfurled\",\"GALLERY\":\"gallery_leaf\",\"HOMEPAGE\":\"landing_homepage\",\"LIVESTORY\":\"article_livestory\",\"TV_CHANNELS\":\"tv_channels\",\"PROFILE\":\"profile\",\"SECTION\":\"landing_section\",\"TVE_FILM\":\"detail\",\"TVE_SERIES\":\"series\",\"TVE_STREAM\":\"network\",\"ELECTION\":\"election\",\"MARKETS\":\"markets\",\"TVE_BROWSE\":\"browse\",\"VIDEO\":\"video_leaf\",\"VIDEO_SHOW\":\"video_show\",\"UNKNOWN\":\"\"}, PAGE_TYPES: {\"ARTICLE\":\"article\",\"AUDIO\":\"audio\",\"CUSTOM\":\"custom\",\"FEED\":\"feed\",\"GALLERY\":\"gallery\",\"INTERACTIVE\":\"interactive\",\"LIVESTORY\":\"live-story\",\"NEWSLETTER_LANDING_PAGE\":\"newsletter-landing-page\",\"PROFILE\":\"profile\",\"SCRATCHPAD\":\"scratchpad\",\"SEARCH\":\"search\",\"SECTION\":\"section\",\"STATIC\":\"static\",\"TVE\":\"tve\",\"UNKNOWN\":\"\",\"USER_MANAGEMENT\":\"user-management\",\"VERTICAL_VIDEO\":\"vertical-video\",\"VIDEO\":\"video\"}, SECTIONS: {\"US\":\"us\",\"WORLD\":\"world\",\"POLITICS\":\"politics\",\"BUSINESS\":\"business\",\"OPINIONS\":\"opinions\",\"HEALTH\":\"health\",\"ENTERTAINMENT\":\"entertainment\",\"STYLE\":\"style\",\"TRAVEL\":\"travel\",\"HOMEPAGE\":\"homepage\",\"SPORTS\":\"sport\",\"UNDERSCORED\":\"cnn-underscored\",\"WEATHER\":\"weather\",\"PHOTOS\":\"photos\",\"PROFILES\":\"profiles\",\"TV\":\"tv\",\"LIVING\":\"living\",\"NEWSLETTERS\":\"newsletters\",\"UNKNOWN\":\"\"}, isSection: function isSection(sections) { return (!Array.isArray(sections) ? [sections] : sections).includes( window.CNN.contentModel?.section || window.CNN.helpers?.SECTIONS.UNKNOWN ); }, isPageVariant: function isPageVariant(pageVariants) { return (!Array.isArray(pageVariants) ? [pageVariants] : pageVariants).includes( window.CNN.contentModel?.templateType || window.CNN.helpers?.PAGE_VARIANTS.UNKNOWN ); }, isPageType: function isPageType(pageTypes) { return (!Array.isArray(pageTypes) ? [pageTypes] : pageTypes).includes( window.CNN.contentModel?.pageType || window.CNN.helpers?.PAGE_TYPES.UNKNOWN ); }, isEditionPage: function isEditionPage() { return window.CNN.contentModel?.edition; }, addScriptTag: function addScriptTag(options = {}, prependToBody = false) { const script = document.createElement('script'); const opts = { language: 'javascript', type: 'text/javascript', ...options }; Object.keys(opts).forEach((key) => { if (key === 'data') { const { data } = opts; Object.keys(data).forEach((dataKey) => { if (data[dataKey]) { script.setAttribute(`data-${dataKey}`, data[dataKey]); } }); } else { script[key] = opts[key]; } }); if (prependToBody) { document.body.prepend(script); } else { document.head.append(script); } return script; }, getAdfuelSrc: (body = false) => { let src; if (window.CNN.helpers.isSection(window.CNN.helpers.SECTIONS.BUSINESS)) { src = window.CNN.helpers.isEditionPage() ? window.env.ADFUEL_BUSINESS_EDITION_SRC : window.env.ADFUEL_BUSINESS_SRC; } else { src = window.CNN.helpers.isEditionPage() ? window.env.ADFUEL_CNN_EDITION_SRC : window.env.ADFUEL_CNN_SRC; } return body ? `${src}-body.min.js` : `${src}.min.js`; } };(function() { function getPath() { const domain = window.location.hostname; const metaURL = document.querySelector('[rel=canonical]').getAttribute?.('href'); try { const canURL = new URL(metaURL); return domain + canURL.pathname; } catch (e) { // we should never run into this situation where the canonical // url is missing in the meta tag but just in case return domain + document.location.pathname; } } function getDomain() { var domain = !window.CNN.contentModel.edition ? 'cnn.com' : 'edition.cnn.com' if (window.CNN.omniture.rs_flag != \"prod\") { domain = \"dev.\" + domain; } return domain; } function getTitle(headline = '') { const isHomepage = window.CNN?.contentModel?.section === 'homepage'; return isHomepage ? headline.replace(' Desktop', '').replace(' Mobile', '') : headline; } var _sf_async_config = window._sf_async_config = (window._sf_async_config || {}); _sf_async_config.uid = 37612; _sf_async_config.domain = getDomain(); _sf_async_config.sections = window.CNN.contentModel.vertical; _sf_async_config.authors = window.CNN.omniture.cap_author; _sf_async_config.title = getTitle(window.CNN.omniture.headline); _sf_async_config.flickerControl = false; _sf_async_config.path = getPath(); })();(function(){ function preloadScript(srcipt) { try { const preload = document.createElement('link'); preload.href = srcipt.src; preload.rel = \"preload\"; preload.as=\"script\" preload.type=\"application/x-javascript\" document.head.appendChild(preload); } catch(e) { console.error('ExternalScripts: error preloading script', e); } }; preloadScript({ src: (function(){ try { return window.env.ADOBE_LAUNCH_SRC; } catch (e) { console.error('external-scripts: error generating tag-manager preload', e); } })() }); preloadScript({ src: (function(){ try { return window.CNN.helpers.getAdfuelSrc(); } catch (e) { console.error('external-scripts: error generating adfuel preload', e); } })() }); preloadScript({ src: (function(){ try { return window.CNN.helpers.getAdfuelSrc(true); } catch (e) { console.error('external-scripts: error generating adfuel preload', e); } })() }); }()) window.CNN=Object.assign(window.CNN || {}, { \"Features\": { \"enableUserConsent\": true } }); window.WM=Object.assign(window.WM || {}, { \"UserConsentConfig\": { \"cookieDomain\": \".cnn.com\", \"cookieSameSite\": \"None\", \"cookieSecure\": true, \"domId\": \"3d9a6f21-8e47-43f8-8d58-d86150f3e92b\" } }); window.zeta=Object.assign(window.zeta || {}, { \"site_id\": \"cnn\", \"client_hash_key\": \"16b6410431b6374e780104abb0443ca8\", \"partner_hash_key\": \"34747f0775f02a6784bb965de6833e73\", \"tag_short_name\": \"cnn-pixel-8786\" });(function(){ function addScript({ async, defer, name, src, ucStates, id, data }) { try { if (src && typeof src === 'string') { if (ucStates && ucStates.length) { WM.UserConsent.addScript({ src, async, defer }, ucStates); } else { const script = document.createElement('script'); script.src = src; script.dataset.uid = name; script.async = async; script.defer = defer; script.onload = () => {window.dispatchEvent(new CustomEvent(name+'.loaded'));}; if (id) { script.id = id; } if (data && typeof data === 'object') { Object.keys(data).forEach(key => script.dataset[key] = data[key]); } document.head.appendChild(script); } } } catch(e) { console.error('ExternalScripts: error adding script \"' + name + '\"', e); } }; //snippet: user-consent document.addEventListener('oneTrustBlocked', function (event) { if (event && event.detail && event.detail.region !== 'global') { const bd = document.getElementsByTagName('body')[0]; window.location.assign('/browser-blocked'); if (bd) { bd.style.display = 'none'; } } }, false); !function(){\"use strict\";var e,t,n,s,o,i,r,a,c,l,u,d,p,g,S,h;!function(e){e.STUB=\"stub\",e.LOADING=\"loading\",e.LOADED=\"loaded\",e.ERROR=\"error\"}(e||(e={})),function(e){e.VISIBLE=\"visible\",e.HIDDEN=\"hidden\",e.DISABLED=\"disabled\"}(t||(t={}));class E{constructor(e){this.eventQueue=new Map,this.queueNumber=1e3,this.cmpApiContext=e;try{let e=window.__gpp(\"events\")||[];for(var t=0;t{n.callback({eventName:e,listenerId:s,data:t,pingData:{gppVersion:this.cmpApiContext.gppVersion,cmpStatus:this.cmpApiContext.cmpStatus,cmpDisplayStatus:this.cmpApiContext.cmpDisplayStatus,apiSupport:this.cmpApiContext.apiSupport,currentAPI:this.cmpApiContext.currentAPI,cmpId:this.cmpApiContext.cmpId,cmpVersion:this.cmpApiContext.cmpVersion}})}))}clear(){this.queueNumber=1e3,this.eventQueue.clear()}get size(){return this.eventQueue.size}events(){let e=[];return this.eventQueue.forEach(((t,n)=>{e.push({id:n,callback:t.callback,parameter:t.parameter})})),e}}class C extends Error{constructor(e){super(e),this.name=\"DecodingError\"}}class f{static encode(e,t){let n=[];if(e>=1)for(n.push(1);e>=2*n[0];)n.unshift(2*n[0]);let s=\"\";for(let t=0;t=o?(s+=\"1\",e-=o):s+=\"0\"}for(;s.length0;)e+=\"0\";for(;e.length%6>0;)e+=\"0\";return e}}class _{static encode(e){let t=[];if(e>=1&&(t.push(1),e>=2)){t.push(2);let n=2;for(;e>=t[n-1]+t[n-2];)t.push(t[n-1]+t[n-2]),n++}let n=\"1\";for(let s=t.length-1;s>=0;s--){let o=t[s];e>=o?(n=\"1\"+n,e-=o):n=\"0\"+n}return n}static decode(e){if(!/^[0-1]*$/.test(e)||e.lengthe-t));let t=[],n=0,s=0;for(;se-t)))}}class P extends N{constructor(e,t){super(),this.bitStringLength=e,this.setValue(t)}encode(){return f.encode(this.value,this.bitStringLength)}decode(e){this.value=f.decode(e)}substring(e,t){return e.substring(t,t+this.bitStringLength)}}!function(e){e.ID=\"Id\",e.VERSION=\"Version\",e.SECTION_IDS=\"SectionIds\"}(n||(n={}));class y{constructor(e,t){this.fields=e,this.fieldOrder=t}hasField(e){return this.fields.has(e)}getFieldValue(e){return this.fields.has(e)?this.fields.get(e).getValue():null}setFieldValue(e,t){if(!this.fields.has(e))throw new Error(e+\" not found\");this.fields.get(e).setValue(t)}getFieldOrder(){return this.fieldOrder}encodeToBitString(){let e=\"\";for(let t=0;t0&&this.decode(e)}encode(){let e=this.encodeToBitString();return this.base64UrlEncoder.encode(e)}decode(e){let t=this.base64UrlEncoder.decode(e);this.decodeFromBitString(t)}getId(){return D.ID}getName(){return D.NAME}}D.ID=3,D.VERSION=1,D.NAME=\"header\";class R extends N{constructor(e){super(),this.setValue(e)}encode(){return A.encode(this.value)}decode(e){this.value=A.decode(e)}substring(e,t){return e.substring(t,t+1)}}class v{static encode(e){return e?f.encode(Math.round(e.getTime()/100),36):f.encode(0,36)}static decode(e){if(!/^[0-1]*$/.test(e)||36!==e.length)throw new C(\"Undecodable Datetime '\"+e+\"'\");return new Date(100*f.decode(e))}}class w extends N{constructor(e){super(),this.setValue(e)}encode(){return v.encode(this.value)}decode(e){this.value=v.decode(e)}substring(e,t){return e.substring(t,t+36)}}class V{static encode(e,t){let n=\"\";for(let t=0;tt&&(n=n.slice(0,t)),super.setValue([...n])}}class U extends N{constructor(e){super(),this.numElements=e.length,this.setValue(e)}encode(){return V.encode(this.value,this.numElements)}decode(e){this.value=V.decode(e)}substring(e,t){return e.substring(t,t+this.numElements)}getValue(){return[...super.getValue()]}setValue(e){let t=[...e];for(let e=t.length;ethis.numElements&&(t=t.slice(0,this.numElements)),super.setValue(t)}}class k{static encode(e,t){for(;e.length=65))throw new m(\"Unencodable FixedString '\"+e+\"'\");n+=f.encode(e.charCodeAt(t)-65,6)}}return n}static decode(e){if(!/^[0-1]*$/.test(e)||e.length%6!=0)throw new C(\"Undecodable FixedString '\"+e+\"'\");let t=\"\";for(let n=0;n0){let e=0;for(let s=0;se-t));let t=[],n=0;for(;ne-t)))}}class B extends N{constructor(e){super(),this.setValue(e)}encode(){let e=this.value.length>0?this.value[this.value.length-1]:0,t=G.encode(this.value),n=e;if(t.lengthe-t)))}}!function(e){e.VERSION=\"Version\",e.CREATED=\"Created\",e.LAST_UPDATED=\"LastUpdated\",e.CMP_ID=\"CmpId\",e.CMP_VERSION=\"CmpVersion\",e.CONSENT_SCREEN=\"ConsentScreen\",e.CONSENT_LANGUAGE=\"ConsentLanguage\",e.VENDOR_LIST_VERSION=\"VendorListVersion\",e.POLICY_VERSION=\"PolicyVersion\",e.IS_SERVICE_SPECIFIC=\"IsServiceSpecific\",e.USE_NON_STANDARD_STACKS=\"UseNonStandardStacks\",e.SPECIAL_FEATURE_OPTINS=\"SpecialFeatureOptins\",e.PURPOSE_CONSENTS=\"PurposeConsents\",e.PURPOSE_LEGITIMATE_INTERESTS=\"PurposeLegitimateInterests\",e.PURPOSE_ONE_TREATMENT=\"PurposeOneTreatment\",e.PUBLISHER_COUNTRY_CODE=\"PublisherCountryCode\",e.VENDOR_CONSENTS=\"VendorConsents\",e.VENDOR_LEGITIMATE_INTERESTS=\"VendorLegitimateInterests\",e.PUBLISHER_RESTRICTIONS=\"PublisherRestrictions\",e.PUBLISHER_PURPOSES_SEGMENT_TYPE=\"PublisherPurposesSegmentType\",e.PUBLISHER_CONSENTS=\"PublisherConsents\",e.PUBLISHER_LEGITIMATE_INTERESTS=\"PublisherLegitimateInterests\",e.NUM_CUSTOM_PURPOSES=\"NumCustomPurposes\",e.PUBLISHER_CUSTOM_CONSENTS=\"PublisherCustomConsents\",e.PUBLISHER_CUSTOM_LEGITIMATE_INTERESTS=\"PublisherCustomLegitimateInterests\",e.VENDORS_ALLOWED_SEGMENT_TYPE=\"VendorsAllowedSegmentType\",e.VENDORS_ALLOWED=\"VendorsAllowed\",e.VENDORS_DISCLOSED_SEGMENT_TYPE=\"VendorsDisclosedSegmentType\",e.VENDORS_DISCLOSED=\"VendorsDisclosed\"}(s||(s={}));class W extends I{pad(e){for(;e.length%24>0;)e+=\"0\";return e}}class H extends M{constructor(e){let t=new Map,n=new Date;t.set(s.VERSION.toString(),new P(6,H.VERSION)),t.set(s.CREATED.toString(),new w(n)),t.set(s.LAST_UPDATED.toString(),new w(n)),t.set(s.CMP_ID.toString(),new P(12,0)),t.set(s.CMP_VERSION.toString(),new P(12,0)),t.set(s.CONSENT_SCREEN.toString(),new P(6,0)),t.set(s.CONSENT_LANGUAGE.toString(),new x(2,\"EN\")),t.set(s.VENDOR_LIST_VERSION.toString(),new P(12,0)),t.set(s.POLICY_VERSION.toString(),new P(6,2)),t.set(s.IS_SERVICE_SPECIFIC.toString(),new R(!1)),t.set(s.USE_NON_STANDARD_STACKS.toString(),new R(!1)),t.set(s.SPECIAL_FEATURE_OPTINS.toString(),new U([!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1])),t.set(s.PURPOSE_CONSENTS.toString(),new U([!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1])),t.set(s.PURPOSE_LEGITIMATE_INTERESTS.toString(),new U([!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1])),t.set(s.PURPOSE_ONE_TREATMENT.toString(),new R(!1)),t.set(s.PUBLISHER_COUNTRY_CODE.toString(),new x(2,\"AA\")),t.set(s.VENDOR_CONSENTS.toString(),new B([])),t.set(s.VENDOR_LEGITIMATE_INTERESTS.toString(),new B([])),t.set(s.PUBLISHER_RESTRICTIONS.toString(),new F([])),t.set(s.PUBLISHER_PURPOSES_SEGMENT_TYPE.toString(),new P(3,3)),t.set(s.PUBLISHER_CONSENTS.toString(),new U([!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1])),t.set(s.PUBLISHER_LEGITIMATE_INTERESTS.toString(),new U([!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1,!1]));let o=new P(6,0);t.set(s.NUM_CUSTOM_PURPOSES.toString(),o),t.set(s.PUBLISHER_CUSTOM_CONSENTS.toString(),new L((()=>o.getValue()),[])),t.set(s.PUBLISHER_CUSTOM_LEGITIMATE_INTERESTS.toString(),new L((()=>o.getValue()),[])),t.set(s.VENDORS_ALLOWED_SEGMENT_TYPE.toString(),new P(3,2)),t.set(s.VENDORS_ALLOWED.toString(),new B([])),t.set(s.VENDORS_DISCLOSED_SEGMENT_TYPE.toString(),new P(3,1)),t.set(s.VENDORS_DISCLOSED.toString(),new B([])),super(t,[[s.VERSION.toString(),s.CREATED.toString(),s.LAST_UPDATED.toString(),s.CMP_ID.toString(),s.CMP_VERSION.toString(),s.CONSENT_SCREEN.toString(),s.CONSENT_LANGUAGE.toString(),s.VENDOR_LIST_VERSION.toString(),s.POLICY_VERSION.toString(),s.IS_SERVICE_SPECIFIC.toString(),s.USE_NON_STANDARD_STACKS.toString(),s.SPECIAL_FEATURE_OPTINS.toString(),s.PURPOSE_CONSENTS.toString(),s.PURPOSE_LEGITIMATE_INTERESTS.toString(),s.PURPOSE_ONE_TREATMENT.toString(),s.PUBLISHER_COUNTRY_CODE.toString(),s.VENDOR_CONSENTS.toString(),s.VENDOR_LEGITIMATE_INTERESTS.toString(),s.PUBLISHER_RESTRICTIONS.toString()],[s.PUBLISHER_PURPOSES_SEGMENT_TYPE.toString(),s.PUBLISHER_CONSENTS.toString(),s.PUBLISHER_LEGITIMATE_INTERESTS.toString(),s.NUM_CUSTOM_PURPOSES.toString(),s.PUBLISHER_CUSTOM_CONSENTS.toString(),s.PUBLISHER_CUSTOM_LEGITIMATE_INTERESTS.toString()],[s.VENDORS_ALLOWED_SEGMENT_TYPE.toString(),s.VENDORS_ALLOWED.toString()],[s.VENDORS_DISCLOSED_SEGMENT_TYPE.toString(),s.VENDORS_DISCLOSED.toString()]]),this.base64UrlEncoder=new W,e&&e.length>0&&this.decode(e)}encode(){let e=this.encodeSegmentsToBitStrings(),t=[];return this.updateDateStamp(),t.push(this.base64UrlEncoder.encode(e[0])),this.getFieldValue(s.IS_SERVICE_SPECIFIC.toString())?e[1]&&e[1].length>0&&t.push(this.base64UrlEncoder.encode(e[1])):(e[2]&&e[2].length>0&&t.push(this.base64UrlEncoder.encode(e[2])),e[3]&&e[3].length>0&&t.push(this.base64UrlEncoder.encode(e[3]))),t.join(\".\")}decode(e){let t=e.split(\".\"),n=[];for(let e=0;es.getValue()),[])),t.set(o.CUSTOM_PURPOSES_IMPLIED_CONSENT.toString(),new L((()=>s.getValue()),[])),super(t,[[o.VERSION.toString(),o.CREATED.toString(),o.LAST_UPDATED.toString(),o.CMP_ID.toString(),o.CMP_VERSION.toString(),o.CONSENT_SCREEN.toString(),o.CONSENT_LANGUAGE.toString(),o.VENDOR_LIST_VERSION.toString(),o.TCF_POLICY_VERSION.toString(),o.USE_NON_STANDARD_STACKS.toString(),o.SPECIAL_FEATURE_EXPRESS_CONSENT.toString(),o.PURPOSES_EXPRESS_CONSENT.toString(),o.PURPOSES_IMPLIED_CONSENT.toString(),o.VENDOR_EXPRESS_CONSENT.toString(),o.VENDOR_IMPLIED_CONSENT.toString()],[o.SEGMENT_TYPE.toString(),o.PUB_PURPOSES_EXPRESS_CONSENT.toString(),o.PUB_PURPOSES_IMPLIED_CONSENT.toString(),o.NUM_CUSTOM_PURPOSES.toString(),o.CUSTOM_PURPOSES_EXPRESS_CONSENT.toString(),o.CUSTOM_PURPOSES_IMPLIED_CONSENT.toString()]]),this.base64UrlEncoder=new T,e&&e.length>0&&this.decode(e)}encode(){let e=this.encodeSegmentsToBitStrings(),t=[];return t.push(this.base64UrlEncoder.encode(e[0])),e[1]&&e[1].length>0&&t.push(this.base64UrlEncoder.encode(e[1])),t.join(\".\")}decode(e){let t=e.split(\".\"),n=[];for(let e=0;e0&&this.decode(e)}hasField(e){return this.fields.has(e)}getFieldValue(e){return this.fields.has(e)?this.fields.get(e):null}setFieldValue(e,t){if(!this.fields.has(e))throw new j(e+\" not found\");this.fields.set(e,t)}toObj(){let e={};for(const t of this.fields.keys()){let n=this.fields.get(t);e[t.toString()]=n}return e}encode(){let e=\"\";return e+=this.getFieldValue(i.VERSION.toString()),e+=this.getFieldValue(i.NOTICE.toString()),e+=this.getFieldValue(i.OPT_OUT_SALE.toString()),e+=this.getFieldValue(i.LSPA_COVERED.toString()),e}decode(e){this.setFieldValue(i.VERSION.toString(),parseInt(e.charAt(0))),this.setFieldValue(i.NOTICE.toString(),e.charAt(1)),this.setFieldValue(i.OPT_OUT_SALE.toString(),e.charAt(2)),this.setFieldValue(i.LSPA_COVERED.toString(),e.charAt(3))}getId(){return Y.ID}getName(){return Y.NAME}}Y.ID=6,Y.VERSION=1,Y.NAME=\"uspv1\";class q{static encode(e,t,n){let s=\"\";for(let n=0;nt*n)throw new C(\"Undecodable FixedIntegerList '\"+e+\"'\");if(e.length%t!=0)throw new C(\"Undecodable FixedIntegerList '\"+e+\"'\");for(;e.lengtht*n&&(e=e.substring(0,t*n));let s=[];for(let n=0;nthis.numElements&&(t=t.slice(0,this.numElements)),super.setValue(t)}}!function(e){e.VERSION=\"Version\",e.SHARING_NOTICE=\"SharingNotice\",e.SALE_OPT_OUT_NOTICE=\"SaleOptOutNotice\",e.SHARING_OPT_OUT_NOTICE=\"SharingOptOutNotice\",e.TARGETED_ADVERTISING_OPT_OUT_NOTICE=\"TargetedAdvertisingOptOutNotice\",e.SENSITIVE_DATA_PROCESSING_OPT_OUT_NOTICE=\"SensitiveDataProcessingOptOutNotice\",e.SENSITIVE_DATA_LIMIT_USE_NOTICE=\"SensitiveDataLimitUseNotice\",e.SALE_OPT_OUT=\"SaleOptOut\",e.SHARING_OPT_OUT=\"SharingOptOut\",e.TARGETED_ADVERTISING_OPT_OUT=\"TargetedAdvertisingOptOut\",e.SENSITIVE_DATA_PROCESSING=\"SensitiveDataProcessing\",e.KNOWN_CHILD_SENSITIVE_DATA_CONSENTS=\"KnownChildSensitiveDataConsents\",e.PERSONAL_DATA_CONSENTS=\"PersonalDataConsents\",e.MSPA_COVERED_TRANSACTION=\"MspaCoveredTransaction\",e.MSPA_OPT_OUT_OPTION_MODE=\"MspaOptOutOptionMode\",e.MSPA_SERVICE_PROVIDER_MODE=\"MspaServiceProviderMode\",e.GPC_SEGMENT_TYPE=\"GpcSegmentType\",e.GPC_SEGMENT_INCLUDED=\"GpcSegmentIncluded\",e.GPC=\"Gpc\"}(r||(r={}));class J extends M{constructor(e){let t=new Map;t.set(r.VERSION.toString(),new P(6,J.VERSION)),t.set(r.SHARING_NOTICE.toString(),new P(2,0)),t.set(r.SALE_OPT_OUT_NOTICE.toString(),new P(2,0)),t.set(r.SHARING_OPT_OUT_NOTICE.toString(),new P(2,0)),t.set(r.TARGETED_ADVERTISING_OPT_OUT_NOTICE.toString(),new P(2,0)),t.set(r.SENSITIVE_DATA_PROCESSING_OPT_OUT_NOTICE.toString(),new P(2,0)),t.set(r.SENSITIVE_DATA_LIMIT_USE_NOTICE.toString(),new P(2,0)),t.set(r.SALE_OPT_OUT.toString(),new P(2,0)),t.set(r.SHARING_OPT_OUT.toString(),new P(2,0)),t.set(r.TARGETED_ADVERTISING_OPT_OUT.toString(),new P(2,0)),t.set(r.SENSITIVE_DATA_PROCESSING.toString(),new Q(2,[0,0,0,0,0,0,0,0,0,0,0,0])),t.set(r.KNOWN_CHILD_SENSITIVE_DATA_CONSENTS.toString(),new Q(2,[0,0])),t.set(r.PERSONAL_DATA_CONSENTS.toString(),new P(2,0)),t.set(r.MSPA_COVERED_TRANSACTION.toString(),new P(2,0)),t.set(r.MSPA_OPT_OUT_OPTION_MODE.toString(),new P(2,0)),t.set(r.MSPA_SERVICE_PROVIDER_MODE.toString(),new P(2,0)),t.set(r.GPC_SEGMENT_TYPE.toString(),new P(2,1)),t.set(r.GPC_SEGMENT_INCLUDED.toString(),new R(!0)),t.set(r.GPC.toString(),new R(!1)),super(t,[[r.VERSION.toString(),r.SHARING_NOTICE.toString(),r.SALE_OPT_OUT_NOTICE.toString(),r.SHARING_OPT_OUT_NOTICE.toString(),r.TARGETED_ADVERTISING_OPT_OUT_NOTICE.toString(),r.SENSITIVE_DATA_PROCESSING_OPT_OUT_NOTICE.toString(),r.SENSITIVE_DATA_LIMIT_USE_NOTICE.toString(),r.SALE_OPT_OUT.toString(),r.SHARING_OPT_OUT.toString(),r.TARGETED_ADVERTISING_OPT_OUT.toString(),r.SENSITIVE_DATA_PROCESSING.toString(),r.KNOWN_CHILD_SENSITIVE_DATA_CONSENTS.toString(),r.PERSONAL_DATA_CONSENTS.toString(),r.MSPA_COVERED_TRANSACTION.toString(),r.MSPA_OPT_OUT_OPTION_MODE.toString(),r.MSPA_SERVICE_PROVIDER_MODE.toString()],[r.GPC_SEGMENT_TYPE.toString(),r.GPC.toString()]]),this.base64UrlEncoder=new T,e&&e.length>0&&this.decode(e)}encode(){let e=this.encodeSegmentsToBitStrings(),t=[];if(t.push(this.base64UrlEncoder.encode(e[0])),e[1]&&e[1].length>0){!0===this.fields.get(r.GPC_SEGMENT_INCLUDED).getValue()&&t.push(this.base64UrlEncoder.encode(e[1]))}return t.join(\".\")}decode(e){let t=e.split(\".\"),n=[],s=!1;for(let e=0;e0&&this.decode(e)}setFieldValue(e,t,n){let s=null;if(this.sections.has(e)?s=this.sections.get(e):e===z.NAME?(s=new z,this.sections.set(z.NAME,s)):e===H.NAME?(s=new H,this.sections.set(H.NAME,s)):e===Y.NAME?(s=new Y,this.sections.set(Y.NAME,s)):e===J.NAME&&(s=new J,this.sections.set(J.NAME,s)),!s)throw new j(e+\".\"+t+\" not found\");s.setFieldValue(t,n)}setFieldValueBySectionId(e,t,n){this.setFieldValue(X.SECTION_ID_NAME_MAP.get(e),t,n)}getFieldValue(e,t){return this.sections.has(e)?this.sections.get(e).getFieldValue(t):null}getFieldValueBySectionId(e,t){return this.getFieldValue(X.SECTION_ID_NAME_MAP.get(e),t)}hasField(e,t){return!!this.sections.has(e)&&this.sections.get(e).hasField(t)}hasFieldBySectionId(e,t){return this.hasField(X.SECTION_ID_NAME_MAP.get(e),t)}hasSection(e){return this.sections.has(e)}hasSectionId(e){return this.hasSection(X.SECTION_ID_NAME_MAP.get(e))}deleteSection(e){this.sections.delete(e)}deleteSectionById(e){this.deleteSection(X.SECTION_ID_NAME_MAP.get(e))}clear(){this.sections.clear()}getHeader(){let e=new D;return e.setFieldValue(\"SectionIds\",this.getSectionIds()),e.toObj()}getSection(e){return this.sections.has(e)?this.sections.get(e).toObj():null}getSectionIds(){let e=[];for(let t=0;t. parameter required\");let e=this.parameter.split(\".\");if(2!=e.length)throw new Error(\"Field name must be in the format .\");let t=e[0],n=e[1],s=null;return\"tcfeuv2\"!=this.parameter&&(s=this.cmpApiContext.gppModel.getFieldValue(t,n)),this.invokeCallback(s),s}},ee[u]=class extends Z{respond(){let e=this.cmpApiContext.gppModel.getHeader(),t={sectionId:e.Id,gppVersion:this.cmpApiContext.gppVersion,sectionList:e.SectionIds,applicableSections:this.cmpApiContext.applicableSections,gppString:this.cmpApiContext.gppModel.encode(),pingData:{gppVersion:this.cmpApiContext.gppVersion,cmpStatus:this.cmpApiContext.cmpStatus,cmpDisplayStatus:this.cmpApiContext.cmpDisplayStatus,apiSupport:this.cmpApiContext.apiSupport,currentAPI:this.cmpApiContext.currentAPI,cmpId:this.cmpApiContext.cmpId,cmpVersion:this.cmpApiContext.cmpVersion}};return this.invokeCallback(t),t}},ee[d]=class extends Z{respond(){if(!this.parameter||0===this.parameter.length)throw new Error(\" parameter required\");let e=null;return\"tcfeuv2\"!=this.parameter&&this.cmpApiContext.gppModel.hasSection(this.parameter)&&(e=this.cmpApiContext.gppModel.getSection(this.parameter)),this.invokeCallback(e),e}},ee[p]=class extends Z{respond(){let e=null;return this.cmpApiContext.gppModel.hasSection(\"tcfcav1\")&&(e=this.cmpApiContext.gppModel.getSection(\"tcfcav1\")),this.invokeCallback(e),e}},ee[g]=class extends Z{respond(){if(!this.parameter||0===this.parameter.length)throw new Error(\"[.version] parameter required\");let e=this.cmpApiContext.gppModel.hasSection(this.parameter);return this.invokeCallback(e),e}},ee[S]=class extends Z{respond(){let e={gppVersion:this.cmpApiContext.gppVersion,cmpStatus:this.cmpApiContext.cmpStatus,cmpDisplayStatus:this.cmpApiContext.cmpDisplayStatus,apiSupport:this.cmpApiContext.apiSupport,currentAPI:this.cmpApiContext.currentAPI,cmpId:this.cmpApiContext.cmpId,cmpVersion:this.cmpApiContext.cmpVersion};return this.invokeCallback(e),e}},ee[h]=class extends Z{respond(){let e=this.parameter;return this.cmpApiContext.eventQueue.remove(e)?{eventName:\"listenerRemoved\",listenerId:e,data:!0,pingData:{gppVersion:this.cmpApiContext.gppVersion,cmpStatus:this.cmpApiContext.cmpStatus,cmpDisplayStatus:this.cmpApiContext.cmpDisplayStatus,apiSupport:this.cmpApiContext.apiSupport,currentAPI:this.cmpApiContext.currentAPI,cmpId:this.cmpApiContext.cmpId,cmpVersion:this.cmpApiContext.cmpVersion}}:{eventName:\"listenerRemoved\",listenerId:e,data:!1,pingData:{gppVersion:this.cmpApiContext.gppVersion,cmpStatus:this.cmpApiContext.cmpStatus,cmpDisplayStatus:this.cmpApiContext.cmpDisplayStatus,apiSupport:this.cmpApiContext.apiSupport,currentAPI:this.cmpApiContext.currentAPI,cmpId:this.cmpApiContext.cmpId,cmpVersion:this.cmpApiContext.cmpVersion}}}};class te{constructor(e,t){if(this.cmpApiContext=e,t){let e=a.ADD_EVENT_LISTENER;if(null==t?void 0:t[e])throw new Error(`Built-In Custom Commmand for ${e} not allowed`);if(e=a.REMOVE_EVENT_LISTENER,null==t?void 0:t[e])throw new Error(`Built-In Custom Commmand for ${e} not allowed`);this.customCommands=t}try{this.callQueue=window.__gpp()||[]}catch(e){this.callQueue=[]}finally{window.__gpp=this.apiCall.bind(this),this.purgeQueuedCalls()}}apiCall(e,t,n,s){if(\"string\"!=typeof e)return t(null,!1);if(\"events\"===e)return this.cmpApiContext.eventQueue.events();if(t&&\"function\"!=typeof t)throw new Error(\"invalid callback function\");return this.isCustomCommand(e)?this.customCommands[e](t,n):this.isBuiltInCommand(e)?new ee[e](this.cmpApiContext,t,n).execute():t?t(null,!1):void 0}purgeQueuedCalls(){const e=this.callQueue;this.callQueue=[],e.forEach((e=>{window.__gpp(...e)}))}isCustomCommand(e){return this.customCommands&&\"function\"==typeof this.customCommands[e]}isBuiltInCommand(e){return void 0!==ee[e]}}class ne{static absCall(e,t,n,s){return new Promise(((o,i)=>{const r=new XMLHttpRequest;r.withCredentials=n,r.addEventListener(\"load\",(()=>{if(r.readyState==XMLHttpRequest.DONE)if(r.status>=200&&r.status{i(new Error(\"error\"))})),r.addEventListener(\"abort\",(()=>{i(new Error(\"aborted\"))})),null===t?r.open(\"GET\",e,!0):r.open(\"POST\",e,!0),r.responseType=\"json\",r.timeout=s,r.ontimeout=()=>{i(new Error(\"Timeout \"+s+\"ms \"+e))},r.send(t)}))}static post(e,t,n=!1,s=0){return this.absCall(e,JSON.stringify(t),n,s)}static fetch(e,t=!1,n=0){return this.absCall(e,null,t,n)}}class se extends Error{constructor(e){super(e),this.name=\"GvlError\"}}class oe{has(e){return oe.langSet.has(e)}forEach(e){oe.langSet.forEach(e)}get size(){return oe.langSet.size}}oe.langSet=new Set([\"BG\",\"CA\",\"CS\",\"DA\",\"DE\",\"EL\",\"EN\",\"ES\",\"ET\",\"FI\",\"FR\",\"HR\",\"HU\",\"IT\",\"JA\",\"LT\",\"LV\",\"MT\",\"NL\",\"NO\",\"PL\",\"PT\",\"RO\",\"RU\",\"SK\",\"SL\",\"SV\",\"TR\",\"ZH\"]);var ie=window&&window.__awaiter||function(e,t,n,s){return new(n||(n=Promise))((function(o,i){function r(e){try{c(s.next(e))}catch(e){i(e)}}function a(e){try{c(s.throw(e))}catch(e){i(e)}}function c(e){var t;e.done?o(e.value):(t=e.value,t instanceof n?t:new n((function(e){e(t)}))).then(r,a)}c((s=s.apply(e,t||[])).next())}))};class re{constructor(){this.consentLanguages=new oe,this.language=re.DEFAULT_LANGUAGE,this.ready=!1,this.languageFilename=\"purposes-[LANG].json\"}static fromVendorList(e){let t=new re;return t.populate(e),t}static fromUrl(e){return ie(this,void 0,void 0,(function*(){let t=e.baseUrl;if(!t||0===t.length)throw new se(\"Invalid baseUrl: '\"+t+\"'\");if(/^https?:\\/\\/vendorlist\\.consensu\\.org\\//.test(t))throw new se(\"Invalid baseUrl! You may not pull directly from vendorlist.consensu.org and must provide your own cache\");t.length>0&&\"/\"!==t[t.length-1]&&(t+=\"/\");let n=new re;if(n.baseUrl=t,e.languageFilename?n.languageFilename=e.languageFilename:n.languageFilename=\"purposes-[LANG].json\",e.version>0){let s=e.versionedFilename;s||(s=\"archives/vendor-list-v[VERSION].json\");let o=t+s.replace(\"[VERSION]\",String(e.version));n.populate(yield ne.fetch(o))}else{let s=e.latestFilename;s||(s=\"vendor-list.json\");let o=t+s;n.populate(yield ne.fetch(o))}return n}))}changeLanguage(e){return ie(this,void 0,void 0,(function*(){const t=e.toUpperCase();if(!this.consentLanguages.has(t))throw new se(`unsupported language ${e}`);if(t!==this.language){this.language=t;const n=this.baseUrl+this.languageFilename.replace(\"[LANG]\",e);try{this.populate(yield ne.fetch(n))}catch(e){throw new se(\"unable to load language: \"+e.message)}}}))}getJson(){return JSON.parse(JSON.stringify({gvlSpecificationVersion:this.gvlSpecificationVersion,vendorListVersion:this.vendorListVersion,tcfPolicyVersion:this.tcfPolicyVersion,lastUpdated:this.lastUpdated,purposes:this.purposes,specialPurposes:this.specialPurposes,features:this.features,specialFeatures:this.specialFeatures,stacks:this.stacks,vendors:this.fullVendorList}))}isVendorList(e){return void 0!==e&&void 0!==e.vendors}populate(e){this.purposes=e.purposes,this.specialPurposes=e.specialPurposes,this.features=e.features,this.specialFeatures=e.specialFeatures,this.stacks=e.stacks,this.isVendorList(e)&&(this.gvlSpecificationVersion=e.gvlSpecificationVersion,this.tcfPolicyVersion=e.tcfPolicyVersion,this.vendorListVersion=e.vendorListVersion,this.lastUpdated=e.lastUpdated,\"string\"==typeof this.lastUpdated&&(this.lastUpdated=new Date(this.lastUpdated)),this.vendors=e.vendors,this.fullVendorList=e.vendors,this.mapVendors(),this.ready=!0)}mapVendors(e){this.byPurposeVendorMap={},this.bySpecialPurposeVendorMap={},this.byFeatureVendorMap={},this.bySpecialFeatureVendorMap={},Object.keys(this.purposes).forEach((e=>{this.byPurposeVendorMap[e]={legInt:new Set,consent:new Set,flexible:new Set}})),Object.keys(this.specialPurposes).forEach((e=>{this.bySpecialPurposeVendorMap[e]=new Set})),Object.keys(this.features).forEach((e=>{this.byFeatureVendorMap[e]=new Set})),Object.keys(this.specialFeatures).forEach((e=>{this.bySpecialFeatureVendorMap[e]=new Set})),Array.isArray(e)||(e=Object.keys(this.fullVendorList).map((e=>+e))),this.vendorIds=new Set(e),this.vendors=e.reduce(((e,t)=>{const n=this.vendors[String(t)];return n&&void 0===n.deletedDate&&(n.purposes.forEach((e=>{this.byPurposeVendorMap[String(e)].consent.add(t)})),n.specialPurposes.forEach((e=>{this.bySpecialPurposeVendorMap[String(e)].add(t)})),n.legIntPurposes.forEach((e=>{this.byPurposeVendorMap[String(e)].legInt.add(t)})),n.flexiblePurposes&&n.flexiblePurposes.forEach((e=>{this.byPurposeVendorMap[String(e)].flexible.add(t)})),n.features.forEach((e=>{this.byFeatureVendorMap[String(e)].add(t)})),n.specialFeatures.forEach((e=>{this.bySpecialFeatureVendorMap[String(e)].add(t)})),e[t]=n),e}),{})}getFilteredVendors(e,t,n,s){const o=e.charAt(0).toUpperCase()+e.slice(1);let i;const r={};return i=\"purpose\"===e&&n?this[\"by\"+o+\"VendorMap\"][String(t)][n]:this[\"by\"+(s?\"Special\":\"\")+o+\"VendorMap\"][String(t)],i.forEach((e=>{r[String(e)]=this.vendors[String(e)]})),r}getVendorsWithConsentPurpose(e){return this.getFilteredVendors(\"purpose\",e,\"consent\")}getVendorsWithLegIntPurpose(e){return this.getFilteredVendors(\"purpose\",e,\"legInt\")}getVendorsWithFlexiblePurpose(e){return this.getFilteredVendors(\"purpose\",e,\"flexible\")}getVendorsWithSpecialPurpose(e){return this.getFilteredVendors(\"purpose\",e,void 0,!0)}getVendorsWithFeature(e){return this.getFilteredVendors(\"feature\",e)}getVendorsWithSpecialFeature(e){return this.getFilteredVendors(\"feature\",e,void 0,!0)}narrowVendorsTo(e){this.mapVendors(e)}get isReady(){return this.ready}static isInstanceOf(e){return\"object\"==typeof e&&\"function\"==typeof e.narrowVendorsTo}}re.DEFAULT_LANGUAGE=\"EN\";var ae=window&&window.__awaiter||function(e,t,n,s){return new(n||(n=Promise))((function(o,i){function r(e){try{c(s.next(e))}catch(e){i(e)}}function a(e){try{c(s.throw(e))}catch(e){i(e)}}function c(e){var t;e.done?o(e.value):(t=e.value,t instanceof n?t:new n((function(e){e(t)}))).then(r,a)}c((s=s.apply(e,t||[])).next())}))};class ce{constructor(e,t,n){this.cmpApiContext=new $,this.cmpApiContext.cmpId=e,this.cmpApiContext.cmpVersion=t,this.callResponder=new te(this.cmpApiContext,n)}fireEvent(e,t){this.cmpApiContext.eventQueue.exec(e,t)}fireErrorEvent(e){this.cmpApiContext.eventQueue.exec(\"error\",e)}fireSectionChange(e){this.cmpApiContext.eventQueue.exec(\"sectionChange\",e)}getEventStatus(){return this.cmpApiContext.eventStatus}setEventStatus(e){this.cmpApiContext.eventStatus=e}getCmpStatus(){return this.cmpApiContext.cmpStatus}setCmpStatus(e){this.cmpApiContext.cmpStatus=e,this.cmpApiContext.eventQueue.exec(\"cmpStatus\",e)}getCmpDisplayStatus(){return this.cmpApiContext.cmpDisplayStatus}setCmpDisplayStatus(e){this.cmpApiContext.cmpDisplayStatus=e,this.cmpApiContext.eventQueue.exec(\"cmpDisplayStatus\",e)}getApplicableSections(){return this.cmpApiContext.applicableSections}setApplicableSections(e){this.cmpApiContext.applicableSections=e}getCurrentAPI(){return this.cmpApiContext.currentAPI}setCurrentAPI(e){this.cmpApiContext.currentAPI=e}setGppString(e){this.cmpApiContext.gppModel.decode(e)}getGppString(){return this.cmpApiContext.gppModel.encode()}setSectionString(e,t){this.cmpApiContext.gppModel.decodeSection(e,t)}setSectionStringById(e,t){this.setSectionString(X.SECTION_ID_NAME_MAP.get(e),t)}getSectionString(e){return this.cmpApiContext.gppModel.encodeSection(e)}getSectionStringById(e){return this.getSectionString(X.SECTION_ID_NAME_MAP.get(e))}setFieldValue(e,t,n){this.cmpApiContext.gppModel.setFieldValue(e,t,n)}setFieldValueBySectionId(e,t,n){this.setFieldValue(X.SECTION_ID_NAME_MAP.get(e),t,n)}getFieldValue(e,t){return this.cmpApiContext.gppModel.getFieldValue(e,t)}getFieldValueBySectionId(e,t){return this.getFieldValue(X.SECTION_ID_NAME_MAP.get(e),t)}getSection(e){return this.cmpApiContext.gppModel.getSection(e)}getSectionById(e){return this.getSection(X.SECTION_ID_NAME_MAP.get(e))}hasSection(e){return this.cmpApiContext.gppModel.hasSection(e)}hasSectionId(e){return this.hasSection(X.SECTION_ID_NAME_MAP.get(e))}deleteSection(e){this.cmpApiContext.gppModel.deleteSection(e)}deleteSectionById(e){this.deleteSection(X.SECTION_ID_NAME_MAP.get(e))}clear(){this.cmpApiContext.gppModel.clear()}getObject(){return this.cmpApiContext.gppModel.toObject()}getGvlFromVendorList(e){return re.fromVendorList(e)}getGvlFromUrl(e){return ae(this,void 0,void 0,(function*(){return re.fromUrl(e)}))}}window.WBD=window.WBD||{},window.WM=window.WM||{},function(e,t){if(\"function\"!=typeof e.CustomEvent){var n=function(e,n){var s;return n=n||{bubbles:!1,cancelable:!1,detail:void 0},(s=t.createEvent(\"CustomEvent\")).initCustomEvent(e,n.bubbles,n.cancelable,n.detail),s};n.prototype=e.Event.prototype,e.CustomEvent=n,\"function\"!==e.Event&&(e.Event=n)}}(window,document),window.WBD.UserConsent=window.WBD.UserConsent||function(e,t){var n,s,o=\"\",i={},r=!1,a=[],c=0,l=\"\",u=null,d=null,p=\"unknown\",g={},S=!1,h=\"\",E=\"\",C=\"\",f=\"\",m={tcfeuv2:2,tcfcav1:5,uspv1:6,uspnatv1:7},I=null,T=\"\",_=null,A=!1,O=!1,N=\"en\",b={binary:!0,boolean:!0,trinary:!0,integer:!0},P=\"\",y=!1,D=\"4.1.1\",R=null,v=!1,w=!1,V=!1,L=!1,U=!1,k=null,x=\"\",M={addtlConsentCookie:\"OTAdditionalConsentString\",adChoicesLinkAction:\"https://www.warnermediaprivacy.com/policycenter/b2c/WMNS/\",adChoicesLinkTitle:{en:\"Ad Choices\",es:\"Elecciones de anuncios\",ar:\"اختيارات الإعلان\"},categories:{req:\"required\",dsa:\"data-store\",cad:\"ads-contextual\",pap:\"ads-person-prof\",pad:\"ads-person\",pcp:\"content-person-prof\",pcd:\"content-person\",map:\"measure-ads\",mcp:\"measure-content\",mra:\"measure-market\",pdd:\"product-develop\",ccd:\"content-contextual\",sec:\"product-security\",tdc:\"deliver-content\",cos:\"combine-data\",dlk:\"link-devices\",did:\"id-devices\",gld:\"geolocate\",sid:\"scan-devices\",dsh:\"data-share\",dsl:\"data-sell\",pdu:\"personal-data\",kc12:\"known-child-12\",kc16:\"known-child-16\",sdre:\"sensitive-racial\",sdrb:\"sensitive-belief\",sdhe:\"sensitive-health\",sdso:\"sensitive-sexual\",sdir:\"sensitive-citizen\",sdge:\"sensitive-gene\",sdbm:\"sensitive-biometric\",sdsp:\"sensitive-spi\",sdss:\"sensitive-ssi\",sduo:\"sensitive-org\",sdco:\"sensitive-comm\"},ccCookie:\"countryCode\",ccpaGeos:[\"US:CA\",\"US:CO\",\"US:CT\",\"US:UT\",\"US:VA\"],compatCategories:{vendor:[\"data-share\",\"data-sell\",\"ads-person-prof\",\"ads-person\"],\"targeted-ads\":[\"ads-person-prof\",\"ads-person\"],\"sensitive-geo\":[\"geolocate\"]},confirmCookie:\"OptanonAlertBoxClosed\",consentChangeAction:null,consentChangeActionDelay:1e3,consentCookie:\"OptanonConsent\",consentDefaults:{required:!0,\"data-store\":!0,\"ads-contextual\":!0,\"ads-person\":!0,\"ads-person-prof\":!0,\"content-person\":!0,\"content-person-prof\":!0,\"measure-ads\":!0,\"measure-content\":!0,\"measure-market\":!0,\"product-develop\":!0,\"content-contextual\":!0,\"product-security\":!0,\"deliver-content\":!0,\"combine-data\":!0,\"link-devices\":!0,\"id-devices\":!0,geolocate:!1,\"scan-devices\":!1,\"data-share\":!0,\"data-sell\":!0,\"personal-data\":!1,\"known-child-12\":!1,\"known-child-16\":!1,\"sensitive-racial\":!1,\"sensitive-belief\":!1,\"sensitive-health\":!1,\"sensitive-sexual\":!1,\"sensitive-citizen\":!1,\"sensitive-gene\":!1,\"sensitive-biometric\":!1,\"sensitive-spi\":!1,\"sensitive-ssi\":!1,\"sensitive-org\":!1,\"sensitive-comm\":!1},consentExpireIn:1,consentNotApplicable:[\"personal-data\",\"known-child-12\",\"known-child-16\",\"sensitive-racial\",\"sensitive-belief\",\"sensitive-health\",\"sensitive-sexual\",\"sensitive-citizen\",\"sensitive-gene\",\"sensitive-biometric\",\"geolocate\",\"sensitive-spi\",\"sensitive-ssi\",\"sensitive-org\",\"sensitive-comm\"],consentLinkTitle:{ar:\"ملفات تعريف الارتباط\",en:\"Cookie Settings\",es:\"Configuración de Cookies\"},controlCookie:\"OptanonControl\",cookieSameSite:\"Lax\",cookieSecure:!1,defaultLanguage:\"en\",enableDebug:!1,enableGPC:!0,enableTransitionCheck:!0,enableWebViewCheck:!0,gdprIabCookie:\"eupubconsent-v2\",geoPassedToOneTrust:!0,gppCategories:{uspnatv1:[{field:\"SharingNotice\",type:\"trinary\",default:1},{field:\"SaleOptOutNotice\",type:\"trinary\",default:1},{field:\"SharingOptOutNotice\",type:\"trinary\",default:1},{field:\"TargetedAdvertisingOptOutNotice\",type:\"trinary\",default:1},{field:\"SharingOptOut\",type:\"trinary\",val:\"data-share\"},{field:\"SaleOptOut\",type:\"trinary\",val:\"data-sell\"},{field:\"TargetedAdvertisingOptOut\",type:\"trinary\",val:[\"ads-person-prof\",\"ads-person\"]},{field:\"PersonalDataConsents\",type:\"trinary\",default:0,val:\"personal-data\"},{field:\"KnownChildSensitiveDataConsents\",type:\"array-trinary\",default:[0,0],maxCount:2,0:\"known-child-12\",1:\"known-child-16\"},{field:\"SensitiveDataProcessing\",type:\"array-trinary\",default:[0,0,0,0,0,0,0,0,0,0,0,0],maxCount:12,0:\"sensitive-racial\",1:\"sensitive-belief\",2:\"sensitive-health\",3:\"sensitive-sexual\",4:\"sensitive-citizen\",5:\"sensitive-gene\",6:\"sensitive-biometric\",7:\"geolocate\",8:\"sensitive-spi\",9:\"sensitive-ssi\",10:\"sensitive-org\",11:\"sensitive-comm\"}],uspv1:[{field:\"OptOutSale\",type:\"binary\",val:[\"data-share\",\"data-sell\",\"ads-person-prof\",\"ads-person\"]}]},gppIabCookie:\"OTGPPConsent\",gppSection:\"\",iabRegion:\"\",languageFromBrowser:!0,privacyCenterLinkAction:\"https://www.warnermediaprivacy.com/policycenter/b2c/WMNS/\",privacyCenterLinkTitle:{ar:\"سياسة خصوصية المستهلك\",en:\"Privacy Policy\",es:\"Política de Privacidad\"},regionChangeAction:null,regions:[{id:\"us\",compatCodes:{ven:[\"dsh\",\"dsl\",\"pap\",\"pad\"],tpv:[\"dsh\",\"dsl\",\"pap\",\"pad\"]},compatTransition:{cond:!1,new:[\"dsh\",\"dsl\",\"pap\",\"pad\"],old:\"ven\"},consentExpireIn:3,consentGpcDefaults:{\"data-share\":!1,\"data-sell\":!1,\"ads-person-prof\":!1,\"ads-person\":!1},consentImpliedDefaults:{\"data-store\":!0,\"ads-contextual\":!0,\"content-person\":!0,\"content-person-prof\":!0,\"measure-ads\":!0,\"measure-content\":!0,\"measure-market\":!0,\"product-develop\":!0,\"content-contextual\":!0,\"product-security\":!0,\"deliver-content\":!0,\"combine-data\":!0,\"link-devices\":!0,\"id-devices\":!0},consentLinkTitle:{ar:\"لا تبيع أو تشارك معلوماتي الشخصية\",en:\"Do Not Sell Or Share My Personal Information\",es:\"No Venda Vi Comparta Mi Información Personal\"},geoMatch:[\"US:CA\",\"US:CO\",\"US:CT\",\"US:UT\",\"US:VA\"],gppSection:\"uspnatv1\",iabRegion:\"ccpa\"},{id:\"gdpr\",consentDefaults:{\"data-store\":!1,\"ads-contextual\":!1,\"ads-person-prof\":!1,\"ads-person\":!1,\"content-person-prof\":!1,\"content-person\":!1,\"measure-ads\":!1,\"measure-content\":!1,\"measure-market\":!1,\"product-develop\":!1,\"content-contextual\":!1,\"combine-data\":!1,\"link-devices\":!1,\"id-devices\":!1},consentImpliedDefaults:{\"product-security\":!0,\"deliver-content\":!0,\"combine-data\":!0,\"link-devices\":!0,\"id-devices\":!0,\"data-share\":!0,\"data-sell\":!0},consentLinkTitle:{ar:\"إدارة ملفات تعريف الارتباط+\",en:\"Manage Cookies+\",es:\"Administrar cookies+\"},geoMatch:[\"GB\",\"DE\",\"FR\",\"IT\",\"ES\",\"PL\",\"RO\",\"NL\",\"BE\",\"GR\",\"CZ\",\"PT\",\"SE\",\"HU\",\"AT\",\"BG\",\"DK\",\"FI\",\"SK\",\"IE\",\"HR\",\"LT\",\"SI\",\"LV\",\"EE\",\"CY\",\"LU\",\"MT\",\"NO\",\"IS\",\"LI\"],iabRegion:\"gdpr\"},{id:\"other-optin\",consentDefaults:{\"data-store\":!1,\"ads-contextual\":!1,\"ads-person-prof\":!1,\"ads-person\":!1,\"content-person-prof\":!1,\"content-person\":!1,\"measure-ads\":!1,\"measure-content\":!1,\"measure-market\":!1,\"product-develop\":!1,\"content-contextual\":!1,\"combine-data\":!1,\"link-devices\":!1,\"id-devices\":!1},consentImpliedDefaults:{\"product-security\":!0,\"deliver-content\":!0,\"combine-data\":!0,\"link-devices\":!0,\"id-devices\":!0,\"data-share\":!0,\"data-sell\":!0},geoMatch:[\"CO\",\"UY\",\"PE\",\"AR\",\"CR\",\"CL\"]},{id:\"other-optout\",consentImpliedDefaults:{\"product-security\":!0,\"deliver-content\":!0,\"combine-data\":!0,\"link-devices\":!0,\"id-devices\":!0,\"data-share\":!0,\"data-sell\":!0},geoMatch:[\"MX\",\"PY\",\"BR\",\"VE\",\"NI\"]},{id:\"global\",geoMatch:[\"*\"],useFixedConsent:!0}],reloadOnConsentChange:!0,reloadOnConsentReduction:!1,scCookie:\"stateCode\",setPageClass:!0,src:\"https://cdn.cookielaw.org/scripttemplates/otSDKStub.js\",strictIabCompliance:!0,tcfOpts:{categories:{purposes:[\"data-store\",\"ads-contextual\",\"ads-person-prof\",\"ads-person\",\"content-person-prof\",\"content-person\",\"measure-ads\",\"measure-content\",\"measure-market\",\"product-develop\",\"content-contextual\"],specialPurposes:[\"product-security\",\"deliver-content\"],features:[\"combine-data\",\"link-devices\",\"id-devices\"],specialFeatures:[\"geolocate\",\"scan-devices\"]},policies:{2:{iabMaxPurposes:10,iabMaxSpecialFeats:2},3:{iabMaxPurposes:10,iabMaxSpecialFeats:2},4:{iabMaxPurposes:11,iabMaxSpecialFeats:2}}},ucFlavor:\"iab\",useFixedConsent:!1,useGPP:!0,useIAB:!0,useIabString:!0,uspApiCookieName:\"usprivacy\",uspApiExplicitNotice:!0,uspApiIsLspa:!1};function G(e){const t=Array.prototype.slice.call(arguments);t[0]=\"[WMUC]\"+(0===h.length?\"\":\" (\"+h+\")\")+\":\",\"error\"===e?console.error.apply(console,t):console.log.apply(console,t)}function F(e){const n=t.cookie.match(new RegExp(\"(^|;) *\"+e+\" *= *([^;]+)\"));return n?n.pop():null}function B(e,n,s){e&&(s=s||{},t.cookie=e+\"=\"+(\"string\"==typeof n?n:\"\")+\"; Domain=\"+(s.domain||i.cookieDomain)+\"; Path=\"+(s.path||\"/\")+(s.maxage?\"; Max-Age=\"+s.maxage:s.expires?\"; Expires=\"+s.expires:\"\")+(s.secure?\"; Secure\":\"\")+(s.samesite?\"; SameSite=\"+s.samesite:\"\"))}function W(t){if(\"function\"==typeof e.atob)try{return atob(t.replace(/_/g,\"/\").replace(/-/g,\"+\"))}catch(e){G(\"error\",\"Failed to decode TC string\")}return\"\"}function H(e){return!!Number(e)}function z(e){return parseInt(e,2)||0}function j(e){return 100*z(e)}function Y(e){const t=\"A\".charCodeAt(),n=e.match(/.{6}/g)||[];let s=\"\";for(let e=0;e{if(s.pubRestrictionEntry&&s.rangeEntry)for(let e in s.rangeEntry)Object.prototype.hasOwnProperty.call(s.rangeEntry,e)&&(s.pubRestrictionEntry[e]=(s.pubRestrictionEntry[e]||[]).concat(s.rangeEntry[e]));s.numPubRestrictions&&(s.numPubRestrictions--,e.push({key:\"purposeId\",size:6},{key:\"restrictionType\",size:2},{key:\"numEntries\",size:12}))},i=()=>{s.numEntries?(s.numEntries--,e.push({key:\"isARange\",size:1,decoder:H},{key:\"startVendorId\",size:16})):o()},r=()=>!s.purposeId||[{purpose:s.purposeId,isAllowed:0!==s.restrictionType,isConsentRequired:1===s.restrictionType,isLegitimateInterestRequired:2===s.restrictionType}];if(\"isRangeEncoding\"===t.key)e.push(n?{key:\"numEntries\",size:12}:{key:\"bitField\",size:s.maxVendorId,decoder:q});else if(\"numEntries\"===t.key)s.rangeEntry={},i();else if(\"isARange\"===t.key)n&&e.push({key:\"endVendorId\",size:16});else if(\"startVendorId\"===t.key)s.isARange||(s.rangeEntry[n]=r(),i());else if(\"endVendorId\"===t.key){for(let e=s.startVendorId;ee.pubRestrictionEntry||e.rangeEntry||e.bitField||e,i=(e,n)=>{const s=n.slice(t,t+e.size);return t+=e.size,(e.decoder||z)(s)},r=(e,t)=>{let n={};if(!e.queue)return i(e,t);for(let o=0;o{let n={};for(let o=0;o{const r=\"string\"==typeof t?[t]:t,a=\"boolean\"===o?e:\"trinary\"===o?2===e:0!==e;for(let e of r)(0===i.consentNotApplicable.length||i.consentNotApplicable.indexOf(e)=0&&i.consentNotApplicable.indexOf(e){const o=\"string\"==typeof e?[e]:e;let i=0,r=!0;for(let e of o)n.indexOf(e)>=0?void 0!==s[e]&&(r=r&&s[e],i++):G(\"error\",'Invalid consent \"'+e+'\" specified in GPP Categories!');return i>0?\"boolean\"===t?r:\"trinary\"===t?r?2:1:r?1:0:\"boolean\"!==t&&0};try{if(i.startsWith(\"array\")){if(i=i.substring(6),!b[i])throw\"unparse\";if(!t.maxCount||!Array.isArray(o))throw\"badarray\";for(let e=0;e=0){const t=e.split(\"&\");for(let e=0;e=0){const e=t.split(\"&\");for(let t=0;t0){v=!0;for(let e=0;ed)&&(r=!0,d=s),U&&e.__gpp){let t=e.__gpp(\"ping\");t&&0!==t.cmpId&&(t.gppString||(t=e.__gpp(\"getGPPData\"),t&&t.gppString&&(C=t.gppString)))}E=ie(C);for(let e of n)if(E[e]!==u[e]&&(f=!0,!0!==E[e])){m=!0;break}if(f||!l&&r){const n=e.WBD.UserConsent_wrapproc>0?new Date(e.WBD.UserConsent_wrapproc):null;let s;if(c++,n&&(null===d||n.getTime()>d.getTime()+i.consentChangeActionDelay+1e3)&&(d=n),s=u,u=E,ge(),U&&oe(C,E),me(),f){if(S)try{a.push({ts:new Date,act:\"CHG\",desc:JSON.stringify(E),res:i.reloadOnConsentChange||i.reloadOnConsentReduction&&m,note:\"function\"==typeof i.consentChangeAction?\"change function\":\"\"})}catch(e){G(\"error\",\"Failed to track consent change: \",e)}if(\"function\"==typeof i.consentChangeAction&&i.consentChangeAction(re(),i.regId,p,s),t.dispatchEvent(new CustomEvent(\"userConsentChanged\",{bubbles:!1,cancelable:!1,detail:{region:i.regId,time:d,old:s,new:re(),gpp:T,usp:x,tcf:P,acf:o}})),i.reloadOnConsentChange||m&&i.reloadOnConsentReduction)setTimeout(he,100);else if(le())try{e.sessionStorage.setItem(\"_ucWBDCons\",JSON.stringify({consentState:u,consentTime:d,consentVersion:p,iabIsGlobal:!1})),e.postMessage(\"_ucWBDConsReset\",\"*\")}catch(e){G(\"error\",\"Failed to update session storage and notify children of consent change: \",e)}}}if(!f&&e.WBD.UserConsent_optLoaded){try{a.push({ts:new Date,act:\"NCC\",desc:JSON.stringify(u),res:!1,note:i.regId})}catch(e){G(\"error\",\"Failed to track consent no-change: \",e)}d=h,(!g.region||!g.consentVersion&&p||!g.userConsentVersion||g.userConsentVersion0&&(\"ping\"===s[0]?s[2]({apiVersion:n,gdprApplies:!0,gdprAppliesGlobally:!1,cmpLoaded:!1,cmpStatus:\"stub\",displayStatus:\"hidden\"},!0):\"setGdprApplies\"===s[0]&&s.length>3&&\"boolean\"==typeof s[3]?(L=s[3])&&V&&(V=!1):e[t].a.push([].slice.apply(s))),e[t].a},e[t].msgHandler=s.bind(e,t),fe(e[t].msgHandler),S&&G(\"debug\",\"IAB (v\"+n+\") for GDPR ready.\"),e[t](\"getTCData\",0,e.OptanonWrapper)):R&&S&&G(\"debug\",\"IAB (v\"+n+\") for GDPR ready (via frame).\")),U&&(t=\"__gpp\",n=\"1.0\",null!==Ce(\"__gppLocator\")||e.__gpp?S&&G(\"debug\",\"IAB for GPP ready (via frame).\"):(Ee(\"__gppLocator\"),e.__gpp=function(){return null},(I=I||new ce(0,1)).setCmpStatus(\"loading\"),_=e.__gpp,e.__gpp.msgHandler=s.bind(e,\"__gpp\"),fe(e.__gpp.msgHandler),S&&G(\"debug\",\"IAB for GPP ready.\")))}(),le()){let t;if(g=function(){const e=F(i.controlCookie),t={consentInteractions:c,consentTime:null,consentVersion:\"\",countryCode:\"\",region:\"\",stateCode:\"\",userConsentVersion:\"\"};if(\"string\"==typeof e&&0!==e.length){const n=e.split(\"&\");for(let e=0;ed)?(d=g.consentTime,S&&G(\"debug\",'Consent time read from \"'+i.controlCookie+'\": ',d)):null!==d&&S&&G(\"debug\",'Consent time read from \"'+i.confirmCookie+'\": ',d);if(t=S&&w?\" [GPC override]\":\"\",null!==d?(r=!0,u=ie(),r?(o.async=!0,null!==g.consentTime&&g.consentTime { return window.dispatchEvent(new CustomEvent('adfuel.loaded')); } }); const observer = new MutationObserver(function() { if (document.body) { window.CNN.helpers.addScriptTag({ src: window.CNN.helpers.getAdfuelSrc(true), async: true, data: { uid: 'adfuel-body' }, onload: () => { return window.dispatchEvent(new CustomEvent('adfuel-body.loaded')); } }); observer.disconnect(); } }); observer.observe(document.documentElement, {childList: true}); })(); //snippet: nativo if (window.WM.UserConsent.inUserConsentState(['iab','data-store','ads-contextual','ads-person-prof','ads-person','measure-ads'])) { (function() { /* serve nativo only on domestic pages */ if (!window.CNN.helpers.isEditionPage() && window.env.NATIVO_SRC) { const nativoScriptObj = { name: 'nativo', src: window.env.NATIVO_SRC, defer: true } addScript(nativoScriptObj); } })(); } //script: browsi addScript({ async: false, data: (function(){ try { let data = { pubkey: 'cnn', sitekey: 'cnn' }; return data; } catch (e) { console.error('external-scripts: error generating browsi data', e); } })(), defer: true, id: 'browsi-tag', name: 'browsi', src: (function(){ try { /* only load on mobile viewport AND articles not HEALTH - Test in progress with native injection */ if (window.matchMedia('(max-width: 960px)').matches && !window.CNN.helpers.isSection(window.CNN.helpers.SECTIONS.HEALTH) && !window.CNN.helpers.isSection(window.CNN.helpers.SECTIONS.POLITICS) && !window.CNN.helpers.isSection(window.CNN.helpers.SECTIONS.WORLD) && !window.CNN.helpers.isSection(window.CNN.helpers.SECTIONS.ENTERTAINMENT) && !window.CNN.helpers.isSection(window.CNN.helpers.SECTIONS.OPINIONS)) { return window.env.BROWSI_SRC } } catch (e) { console.error('external-scripts: error generating browsi src', e); } })() }, ['iab','data-share','data-sell','data-store','ads-contextual','ads-person-prof','ads-person','content-person-prof','content-person','measure-ads','measure-content','measure-market','product-develop']); //snippet: zion if (window.WM.UserConsent.inUserConsentState(['data-store','ads-person-prof','ads-person','content-person-prof','content-person','measure-content'])) { (function() { addScript({ src: window.env.ZION_SRC, async: true, defer: false, name: 'zion', }); window.addEventListener('zion.loaded', () => { if ( window.CNN.Zion.environmentType && window.CNN.Zion.sourceId && window.ZION_SDK ) { const environment = window.ZION_SDK.EnvironmentType[window.CNN.Zion.environmentType]; const enableLogging = environment !== window.ZION_SDK.EnvironmentType.Prod; window.zion_analytics.configure({ bridgeEnabled: true, bufferSize: 20, enableLogging, customFeatureManagerPath: 'https://z.cdp-dev.cnn.com/zfm/zfh-3.js', environment: window.CNN.Zion.environmentType, isSecure: true, telemetryEndpoint: window.env.ZION_TELEMETRY_ENDPOINT, trackAdvertising: false, trackBluetooth: false, trackDeeplink: false, trackLifecycle: false, trackLocation: false, trackNotifications: false, trackPurchases: false, trackScreens: false, trackUxMetrics: true, uxMetricsPercentage: 15, }); if (window.zion_analytics) { window.zion_analytics.track(new window.ZION_SDK.Pageview({ canonicalUrl: window.CNN.contentModel.canonicalUrl, traits: { event_source: window.CNN.contentModel.techStack || 'stellar', page_variant: window.CNN.contentModel.templateType || '', raw_url: window.location.href, cms_id: CNN.contentModel.cmsId || '', page_type: CNN.contentModel.pageType || '', edition: window.CNN.helpers.isEditionPage(), section: CNN.contentModel.section || '', subsection: CNN.contentModel.subsection || '', section_level_3: CNN.contentModel.subsubsection || '', experience_type: 'cnn_core' }, sourceId: window.CNN.contentModel.sourceId || '' })); } } else { throw new Error('zion: missing either \"apiKey\", \"environmentType\" or \"sourceId\"'); } }); })(); } //script: sovrn addScript({ async: false, defer: true, name: 'sovrn', src: 'https://get.s-onetag.com/c15ddde9-ec7d-4a49-b8ca-7a21bc4b943b/tag.min.js' }, ['iab','data-share','data-sell','data-store','ads-contextual','ads-person-prof','ads-person','content-person-prof','content-person','measure-ads','measure-content','measure-market','product-develop']); //script: fave addScript({ async: false, defer: true, name: 'fave', src: 'https://registry.api.cnn.io/bundles/fave/latest-4.x/js' }); //script: pym-js addScript({ async: true, defer: false, name: 'pym-js', src: 'https://cdn.cnn.com/cnn/.e/interactive/js/lib/vendor/pym/pym.v1.min.js' }); //script: distroscale addScript({ async: false, defer: true, name: 'distroscale', src: 'https://a.jsrdn.com/creatives/23053/cw.js' }, ['iab','data-share','data-store','ads-contextual','content-person','measure-content','product-develop']); }())if (!window.WM.UserConsent.inUserConsentState([\"data-share\",\"data-store\",\"content-person-prof\",\"content-person\",\"measure-ads\",\"measure-content\"])) { window.optimizely = window.optimizely || []; window.optimizely.push({ type: 'disable' }); }{\"@type\":\"NewsArticle\",\"@context\":\"https://schema.org\",\"articleBody\":\"The Federal Trade Commission ruled in a final order and opinion Monday that TurboTax, the popular tax filing software, engaged in deceptive advertising and banned the company from advertising its services for free unless it is free for all customers. By running ads for “free” tax services that many customers were not qualified for, the tax filing software violated the FTC Act and deceived consumers, the agency said. The FTC had first sued Intuit, TurboTax’s owner, for its deceptive advertising in 2022. The FTC staff alleged most tax filers couldn’t use the company’s “free” services — “such as those who get a 1099 form for work in the gig economy, or those who earn farm income.” TurboTax advertising their products as free misled those customers, according to the FTC. The FTC Administrative Law Judge D. Michael Chappell announced the initial decision in September, which the commission upheld Monday. Intuit had appealed to the FTC as part of the process. In a statement Monday, Intuit said it has appealed “this deeply flawed decision” to federal circuit court outside of the FTC. “Absolutely no one should be surprised that FTC Commissioners — employees of the FTC — ruled in favor of the FTC as they have done in every appeal for the last two decades. This decision is the result of a biased and broken system where the Commission serves as accuser, judge, jury, and then appellate judge all in the same case,” an an Intuit spokesperson said. Intuit cannot advertise or market any goods or services as free unless it’s free for all customers, the FTC ordered. It could also clearly disclose the percentage of customers that actually do qualify for the free service, somewhere close to the “free” claim advertised, the commission said. The FTC added if the service isn’t actually free for the majority of consumers, TurboTax could state that as well. The FTC ordered Intuit to “disclose clearly and conspicuously all the terms, conditions, and obligations that are required in order to obtain the ‘free’ good or service.” “The order also prohibits Intuit from misrepresenting any material facts about its products or services such as the price, refund policies or consumers’ ability to claim a tax credit or deduction or to file their taxes online accurately without using TurboTax’s paid service,” the release from the FTC said. Last May, roughly 4.4 million people were to receive checks from TurboTax, following a 50-state settlement with parent company Intuit for allegedly steering millions of low-income Americans away from free tax-filing services. The investigation opened after a 2019 ProPublica report that accused the company of steering eligible clients away from federally supported tax filing products by making them difficult to find in web searches — instead directing them toward the paid version of the company’s filing services. Most customers will get about $30, with some customers that used TurboTax for three consecutive years getting up to $85. A website has also been set up for more information. “TurboTax’s predatory and deceptive marketing cheated millions of low-income Americans who were trying to fulfill their legal duties to file their taxes,” said New York Attorney General Letitia James in a May 4, 2023, press release. “Today we are righting that wrong and putting money back into the pockets of hardworking taxpayers who should have never paid to file their taxes.” CNN’s Jordan Valinsky contributed to this report. \",\"articleSection\":[\"business\"],\"author\":[{\"@type\":\"Person\",\"name\":\"Ramishah Maruf\",\"url\":\"\"}],\"dateModified\":\"2024-01-23T00:18:46.816Z\",\"description\":\"The Federal Trade Commissio",
    "commentLink": "https://news.ycombinator.com/item?id=39097356",
    "commentBody": "FTC bans TurboTax from advertising 'free' services, calls deceptive advertising (cnn.com)757 points by fairytalemtg 10 hours agohidepastfavorite207 comments logiduck 8 hours agoI just saw an TurboTax Ad where a guy was like \"I Like free stuff\" and then it said he was \"happy to read the disclaimer\" on TurboTax and see that \"Roughly 37% of taxpayers qualify\" which he looks thoughtfully in the distance and says \"Thats me!\" I thought it was a funny commercial because 37% doesn't seem like a lot and Turbotax is portraying it as the average person will identify themselves as part of that 37% even though that is not too far off form just 1/3 people so a minority of people. It was one of the few times I saw a company blatantly lean into the negatives in their fine print and just outright tell you its good. https://www.youtube.com/watch?v=iijnr4UR4QE reply halJordan 8 hours agoparentI agree on every level, but I'm compelled to remind you this is the America where wendy's (?) had to revert to a 1/4 pounder from a 1/3 pounder bc people thought they were getting less meat. And let's not forget the ever-present anti-education cohort that can't be convinced math is good even when you tell them it's how you calculate discounts or tips. reply sharps1 7 hours agorootparentI had to look this up as I didn’t grow up in the States. It was A&W. https://www.snopes.com/news/2022/06/17/third-pound-burger-fr... Crazy. reply cityofdelusion 7 hours agorootparentThis story has to be apocryphal, as fractions aren't _that_ rare, especially in the U.S. with its imperial system and third of a cup measurements or quarter inches or half miles and so on. reply ants_everywhere 7 hours agorootparentIt's not that they're rare, it's that it legitimately is an easy error to make even if you understand it to be an error. Even people who work with equations every day will occasionally make careless mistakes like this. That's why mathematicians joke about how it's important to make an even number of sign errors. To not make this mistake, you have to be able to call to mind that the map x -> 1/x reverses the inequality sign. That's a fairly abstract thing to remember especially if you haven't taken math for years. Yes you could draw it or write down the equation, or convert to decimal... But it's enough of a cognitive barrier that it doesn't surprise me that it would impact the behavior even of people who would answer correctly on a test. Where it does get easy is if you work with the same set of fractions every day. For example, if you work in construction in the US you can probably quickly order the fractions commonly used for measurement, e.g. 1/4, 1/8, 1/16, 3/4 etc. But 1/3 isn't one of these. Now that I think about it, they probably should have just chosen a fraction that you can find on a tape measure, like 3/8. reply mjevans 3 hours agorootparent3/8ths is 0.375 while 1/3rd is 0.333~ so it's even bigger while still larger than 1/4th (0.250), without being that much bigger. 3/8ths is a pretty good marketing point since all the numbers are bigger and it should be intuitive, plus you can more easily see that it's also 50% bigger than 1/4th => 2/8ths. The harder sell is the 'double whatever' being equal to 3 patties of the competitor. reply dcow 6 hours agorootparentprevFor fractions like 1/3rd and 1/4th all it takes is common sense. reply tocs3 5 hours agorootparentI do not really like the term \"common sense\" as it is more like common experience. It is not hard to learn what fractions are but I do not think it is something that any one is born with and there is other notation to deal with fractions that work differently. reply Dalewyn 2 hours agorootparentprevI speak, and thus think, in both English and Japanese. English says \"1/4\", or \"1 over 4\", or \"1 quarter\". Japanese says \"4 bun no 1\", or the practical equivalent of saying \"4 under 1\" in English. I consequently routinely say the numbers in reverse, confounding both myself and anyone around me before I realize my brain engaged in furious tentacle sex with the numbers. reply KMag 27 minutes agorootparent> I speak, and thus think, in both English and Japanese. The vast majority of processing is happening outside language-related areas of the brain. There's certainly leaky interfaces between areas of the brain, but if you literally thought in a language, and that distinction persisted throughout the brain, that would seem to imply that speaking 3 languages would require 3x the number of connections in the brain. The strong Sapir-Whorf hypothesis would presumably be true if we literally thought in a language, but the strong form of the Sapir-Whorf hypothesis has been thoroughly discredited. In other words, \"thinking in a language\" is an illusion. reply beenBoutIT 1 hour agorootparentprevIt seems like the obvious solution is to offer Americans what they want in terms of a burger named after a bigger denominator. 1/5th pound burger is going to sell better than the quarter pounder while using less beef. reply JadeNB 6 hours agorootparentprev> To not make this mistake, you have to be able to call to mind that the map x -> 1/x reverses the inequality sign. That's a fairly abstract thing to remember especially if you haven't taken math for years. Yes you could draw it or write down the equation, or convert to decimal... You absolutely don't have to remember that x |-> 1/x is order reversing, and, for most people, shouldn't—you immediately give two or three other methods (I don't understand what \"write down the equation\" means) that are a much better way for the average person to check this. reply wharvle 7 hours agorootparentprevAnecdotally, it seems like working with fractions is where a lot of people fall off the math-train and never get back on. … yes, that early. reply tialaramex 16 minutes agorootparentForty years ago we learned fractions with chocolate bars. A trustworthy child would be chosen to walk from the primary school to the local store (about 5 minutes walk for an adult, probably about two minutes for a child who was just given money by a responsible adult to buy chocolate) and bring back some chocolate, and then kids who raise their hand and give the correct answer to fraction questions get the fraction in its physical form as chocolate. What's half of this third of the bar? A sixth, and now because I knew that I get to eat 1/6 of a bar of chocolate, whereas the kid who enthusiastically answered that it's a quarter does not because that's wrong. reply ethbr1 7 hours agorootparentprevFractions and negative numbers. And, to be fair, they're the first math concepts that aren't intuitive. Which IMHO leads to some people not studying, then feeling lost, then rationalizing their lack of effort as \"I'm bad at math\" or \"Math is hard.\" reply Ferret7446 6 hours agorootparentFractions are pretty intuitive. Is the pizza or cake analogy really that advanced? reply wharvle 6 hours agorootparentOne third times one fifth loses a lot of folks. As does addition and subtraction of fractions that don’t start with the same denominator, for that matter. They might figure out what to do to pass the test, but they may not get it. reply bcrosby95 6 hours agorootparentFractions are just division. When kids learn division, it's about splitting into equal groups. Fractions are a bit different though - you're splitting a single thing into equal chunks. Hence, slices of pie. Multiplying by 1/5 is really dividing by 5. Introduce that first. We already know how to do this. You split your 1/3 slice into 5 equal slices. Do the same to the other 2/3 slices, count all the slices, and you have 15. Hence, 1/15. As an aside, common core math is amazing. They gave my daughter a model for the distributive property that can be used to show how to do long multiplication. reply ethbr1 5 hours agorootparentThere's a difference in type of thinking in moving from operations on numbers (basic whole number math) to operations-on-operations-on-numbers (anything with fractions). Suddenly, you need to begin to understand the rules around operators, sequencing, and what operations are legal and illegal. Absent that understanding, even... 1/4 x 2/5 ... gets very complicated trying to reason with physical analogs. So it's the point at which math becomes \"pure\" rather than strictly physically-mapped. reply Ekaros 2 hours agorootparentAnd if multiplying fractions is simple. Before that addition is done. Which is more complicated. Actually I think we do very little addition of fractions later in math. But it is a concept that confuses the multiplication or division. reply wharvle 5 hours agorootparentprevIDK, I did fine with them and find thinking in them natural (I think of fractional division as division, in fact, though I certainly understand the multiplication analogy); I’ve just known enough people who lost track of math at fractions that I doubt it’s coincidence. I’m not saying I don’t get it, I’m saying others have told me that they found the explanations and instructions they were given nonsensical. reply EnigmaFlare 3 hours agorootparentHave you tried dumping all the sockets out of a socket set and putting them back in order? Do you find it's easier to order the metric ones than the imperial ones which have a lot of different denominators on adjacent sizes? I certainly do but I'm not American so maybe it's my background limiting me. reply jstanley 2 hours agorootparentJust to save you some time: the numbers written on the sockets indicate the size of the socket. So you don't even need to read them, just put them in order of size and you'll have them in order of number automatically. reply EnigmaFlare 1 hour agorootparentI was thinking of mine which has the same OD for many adjacent sockets. Guess it doesn't work if they're all different. reply chrisfinazzo 6 hours agorootparentprevCan confirm - While I was decent at math up to a point, fractions and long division in 4th Grade sent me down a hole that took me years to get out of...until Algebra II as a junior in HS crushed me. I blame this on my Chemistry teacher - a class which I was also taking at the time - who spoke little English and had never taught in the United States until the year I landed in her class. I actually did reasonably well in Algebra for the first quarter or so until it all fell apart. reply wharvle 6 hours agorootparentI re-invented what turned out to be short division (no joke! I wouldn’t learn it had a name until I was in my late 20s) because I hated long division so much, same year we started doing long division in school (4th grade? 3rd? IDK). Fits in about the same space as the original problem unless it’s printed so small that you have to rewrite it, and way less room for transcription errors. I also find it clearer but that may just be me (fwiw I’m “bad at math”—I find it incredibly boring and basically can’t follow proof- and equation/identity-based stuff, I have to turn it all into algorithmic thinking to have a prayer of understanding it; i.e. my opinion on the superiority of short division is that of a mathematical imbecile, so, grain of salt) > I blame this on my Chemistry teacher - a class which I was also taking at the time - who spoke little English and had never taught in the United States until the year I landed in her class. It doesn’t help that in chemistry, 1 + 1 may be 1. Or 3. :-) [edit] short division: https://en.m.wikipedia.org/wiki/Short_division Under the “example” section, the little superscripts are what you write in by hand on the problem as you work it, at least as I did it. 9/4 in the hundreds place is 2 with 1 remainder, so write 2 up above as part of the solution and a 1 superscript next to the 5 in the problem itself (tens place), now that’s 15, divide that, 3 goes in the tens place of the solution, write the remainder (3) next to the digit in the ones place as a superscript and do it again, if you need to keep going just add a decimal point and zeroes as required. Way faster than working long division, takes up less space, and less error prone (imo). What’s actually going on is clearer (again, imo) reply wharvle 6 hours agorootparentprevI’ve seen a later fall-off point at factoring. Feels pointless (the motivations are… distant at best), tedious as hell, lots of guessing involved. “So much for math making sense, fuck this, guess I’m out.” reply ghaff 7 hours agorootparentprevIt actually doesn't shock me that many people would be confused, especially if they didn't work with fractional quantities--e.g. for cooking--on a regular basis. Maybe it's a myth but it wouldn't surprise me if it weren't. And even if they've sort of internalized 1/4, 1/2, 3/4 without necessarily fully getting fractions--1/3 is something people encounter a lot less day to day. reply maximus-decimus 7 hours agorootparentprevI did teacher's college in Canada and the teacher who taught math said his biggest surprise when he moved from Europe to Canada is how terrible people were with fraction. I think he asked a barista to fill his cup to 2/3 and they couldn't do it because they didn't know what 2/3 was. reply acdha 6 hours agorootparentprevI thought so as well but A&W backs that version: https://awrestaurants.com/blog/aw-third-pound-burger-fractio... reply MBCook 7 hours agorootparentprevReportedly that’s the answer they got in focus groups when they tried to figure out why it failed. 4 > 3. reply ckw 4 hours agorootparentSo why didn't they start selling 1/5 pounders? reply zo1 3 hours agorootparentQuarter and pounder rhymes so rolls off people's tongues. Fifta-Pounder? Fifth-Pounder? Five-Pounder? For all we know, marketing vetoed it cause they were lazy. reply euroderf 24 minutes agorootparentHow 'bout a five-ouncer ? Or would that suffer by seeming microscopic when listed close to \"32 ounce\" drinks ? reply reactordev 1 hour agorootparentprevAverage Americans that frequent QSR’s don’t math so good. reply JadeNB 6 hours agorootparentprev> This story has to be apocryphal, as fractions aren't _that_ rare, especially in the U.S. with its imperial system and third of a cup measurements or quarter inches or half miles and so on. I literally had an argument with a room full of US university professors about whether or not 30% and 1/3 were the same thing. reply grigri907 6 hours agorootparent... and what was your answer? reply refurb 5 hours agorootparentprevWait, did Snopes say this was real because of a news article that contain hearsay? reply a_random_canuck 2 hours agorootparentDo people think the NY Times reports hearsay as fact? Here, straight from the horse's mouth: https://awrestaurants.com/blog/aw-third-pound-burger-fractio... reply m463 1 hour agorootparentprevVINCENT: And in Paris, you can buy a beer at McDonald's. And you know what they call a Quarter Pounder with Cheese in Paris? JULES: They don't call it a Quarter Pounder with Cheese? VINCENT: No, they got the metric system there, they wouldn't know what the f*** a Quarter Pounder is. JULES: Then what do they call it? VINCENT: They call it Royale with Cheese. JULES: Royale with Cheese. What do they call a Big Mac? VINCENT: Big Mac's a Big Mac, but they call it Le Big Mac. Aside: a lot of tax preparation services, or their services that let you upload your data - the privacy policy says they can all \"use\" your financial info. reply nusl 1 hour agorootparentPersonally I prefer Big Kahuna Burger reply foob 5 hours agorootparentprevIt's also important to take any corporation's explanation for increasing their own margins with an extremely large grain of salt. I'm not doubting in the slightest that consumers had some confusion around the fractions, but all it would take for the company to revert their campaign is for the increase in sales to insufficiently offset the increase in their own costs. Blaming it on consumer stupidity afterwards washes their hands of any responsibility for backpedaling, and makes for a memorable and repeatable story that increases brand recognition while simultaneously painting them as heroically trying to offer more value for the same cost. reply purple_ferret 7 hours agoparentprevAre they leaning in to it or are they forced to fit in the disclaimer? Seems like the strategy of the ad is to repeat the word \"Free\" so much people don't remember the rest and to make it seem like the disclaimer is meaningless. Even with it, it's still free. reply chefandy 6 hours agorootparentProbably a bit of both. What the commenter is describing is a textbook social proof tactic. \"Hey, I like free stuff and taxes make me feel like a bit of a doofus, just like that guy. And like that guy, I see myself as the clever sort of person that isn't fooled by fine print. That free Turbo Tax program sounds awfully useful and free for people like me!\" And Intuit can also point to that commercial and say \"how is this trying to disguise the proof?\" and they'd be right. They're just also trying to make it feel free still by making it free for a relatable character. Sounds hokey but that sort of shit has been the bread and butter of advertising since forever. A vanishingly small percentage of people are anywhere close to as rational as they think they are when buying things. Many of the most self-assuredly \"skeptical, rational, well-researched consumer\" types get totally snowed by the simplest marketing ideas because they're looking for sales bullshit they can empirically disprove, and most marketing is influencing people in a way that makes them think they came to the conclusion independently. reply Imz4di 7 hours agoparentprevTurboTax is marketing to the kind of people who think getting big refunds is a good thing. That's generally people with lower incomes, so this fits that target. reply xivzgrev 6 hours agorootparentIt is generally a good thing for folks who live paycheck to paycheck. Higher withholding forces more budgeting, and then they get a big paycheck once a year to pay off whatever reply burkaman 5 hours agorootparentPeople who live paycheck to paycheck are very good at budgeting because they have to do it to survive, they don't need any more pressure. If anything it's richer people that could use a little prodding, but either way we don't need the government to be withholding extra money from people it thinks might have bad habits. > they get a big paycheck once a year to pay off whatever If you have something big to pay off, you usually need to do it right away. You probably can't afford to wait however many months until you get your refund. reply fantasyfacts 5 hours agorootparentThis is the kind of feel-good nonsense that people tell themselves. But anyone who has actually dealt with poor people who live paycheck to paycheck knows they are not good at budgeting and planning. Many of them literally have the mentality of spending their entire paycheck. Whatever is left over after expenses is fun to be had as soon as possible. And if tax withholding laws changed to err on the side of under-withholding for poor people, those people would disproportionately fail to be able to pay their taxes when the time comes. reply devindotcom 5 hours agorootparentyou made an account just to talk shit about the poor? reply Dalewyn 2 hours agorootparentAs far as I can be bothered to care, schemes involving having other people store your money such as tax withholdings and insurance policies exist primarily to save people from themselves and their lack of budgeting capabilities. Most people can't budget, poor people especially so (it's among the biggest and most likely reasons why they are poor). They see money, they spend it all immediately. Saving? Investing? \"roflmao\" or \"I can't.\", they will say. The only way to address this so certain, specific, important payments are made absolutely is to literally take and keep the money out of the person's hands until the payment is made. reply WarOnPrivacy 5 hours agorootparentprev> Whatever is left over after expenses is fun to be had as soon as possible. When income is 80% of the minimal bills, which dollars are the fun ones? > But anyone who has actually dealt with poor people who live paycheck to paycheck knows they are not good at budgeting and planning. You may not know that assertion is uneducated nonsense. The nonsensical part is the inference that better budgeting is all that stands between 80% of minimal bills and 105%. Past that, an extended time in hunger-level poverty tends to lead to some hyper-focused money management. As in being intimately aware of each penny that hopefully will add up to this weeks bag of white rice. reply zo1 3 hours agorootparentprevSome of the \"better at planning\" poor people have built up crude math to make it all makes sense, but seems overly complicated for someone like me. The one I noticed: \"oh, I got $85 for selling that pot I made, I'll use that for the fridge repair.\" - Which is totally different to \"oh I got $85, I'll just put it in the pool of my bank balance because I already budgeted for that fridge repair which costs $60\". I tried asking this person, but you also needed an extra $10 for that McDonald's lunch you wanted to buy but they insisted \"nope, that's my fridge-repair money, can't spend that\" Not sure I'm explaining that properly, but it was the sort of math I encountered dealing with such individuals. reply chrisfinazzo 6 hours agorootparentprevIn addition, I wouldn't be surprised to find that many of the people who are in the target demographic for this feature don't itemize - and never have a need for such practices. A 1040 + W2 might the only equation these people need to solve for. reply 8note 4 hours agorootparentprevIf they were budgeting, they wouldn't be living paycheck to paycheck. It makes the budgeting more challenging to give the government and interest free loan. However, giving the government an automatic loan means that a land lord cannot charge that much more in rent, and the owner does get to spend it eventually, rather than throwing it into a rent pit reply Broken_Hippo 31 minutes agorootparentIf they were budgeting, they wouldn't be living paycheck to paycheck. This is false. You simply aren't going to be able to budget your way into riches if you don't have enough money to go around. If you don't believe me, limit yourself to a minimum wage budget with no startup savings (and no borrowing off of others) and tell me how you are doing in a year... and then tell me how you'd survive the next few years on this. If minimum wage is too little, try setting the income at just over the mark you'd have for assistance. Alternatively, if you have enough money to cover reasonable expenses, some fun, and have a little leftover, you don't really have to budget if you don't tend to spend lots. If I have enough money, I don't really have to budget. reply whamlastxmas 6 hours agorootparentprevIt’s almost certainly a bad thing to get a big refund because small budgetary changes can result in being unable to make ends meet which is extremely expensive in terms of fines reply to11mtm 5 hours agorootparentprevI think there's some segment of folks that get snatched up into the weird false pretense that a modern day turbotax filing is less than (at worst, once one factors personal time cost in) a decent tax person even at one of the, shall we say, 'established turn and burns'[0] That said, TurboTax did hit a specific level of 'eww' when I started seeing the refund option of a debit card (of course for some stupid fee that, if nothing else, provides some transparency to their kickback from the issuer). I'm going to be doing what might be my last filing with them this year; it's easier for the purposes of history/other events but after that, it's gonna be my Fiancee's CPA. Originally, I got 'started' when it was a desktop app only, and the user limit was very graceful, my parents and all of my siblings could benefit from that one yearly purchase... Come to think of it, we should probably capture that date in the historical timeline of Enshittification. And, yaknow, I'll ask my dad this weekend how he's doing his taxes this year. I'm honestly curious if he's finally fed up with their antics too... (It's a high bar; in the past he learned the basics of virtual machines to use some of his old-school software/tools, it's a beautiful level of curmudgeonry. OTOH my siblings have good CPAs.) [0] - Not to be confused with some of the weird 'chop shop' Tax places I have seen around me in the past, sort of 'pop-ups' with a statue of liberty wearing person or 'wacky inflatable arm-flailing tube-man' to help drive business in. reply RajT88 5 hours agorootparentIf you have simple taxes, FreeTaxUsa.com. Not shady, neither is it free, but about as close as you can get AFAIK for online filing. For what it is (a web forms app, with careful explainers), it's pretty good! I've used tax pros and honestly, my finances are not complex enough to get a good benefit off the extra cost. I used H&R block one year, and really didn't think they knew any more about tax filing than I did. They got confused at all the same line items I did. reply to11mtm 4 hours agorootparent> I used H&R block one year, and really didn't think they knew any more about tax filing than I did. They got confused at all the same line items I did. I mean... H&R Block is in some ways the Firestone of accounting. Sometimes there's a diamond in the rough of their 'regular' workers [0] but you never know what you're gonna get unless you happen to wind up in the right circumstances where you can build trust with one of their people that happens to stick [1] [0] - Had a friend who could get one of H&R Block's folks to do the whole deed for 'non complicated'[2] starting with a pile of receipts/medical bills/etc and 90$ for people they liked, and yes they'd do their proper professional duty in the process. Frankly given the time investment that's a steal. [1] - In my case, it was a guy at a local shop who had his WRX parked outside every time that was some level of manager. Always happy to give proper treatment, never afraid to say 'take it to the dealer' (i.e. more qualified people) if it was out of their comfort zone. Compare and contrast to a different shop, where after some 'changes' managed to mess up an oil change, and the 'fix' for the bad oil change... [3] [2] - I want to be clear that non-complicated is not the trivial 'oh sure okay' here, they may or may not have had a hand in pointing out said friend's parent was doing some... 'minor some student loan fraud'. But if you had additional properties or other weird situations... If I remember they had their own sort of menu and everything. Very smartly done. [3] - I had to re-replace a !\" (that corresponds to the base model). reply garciasn 6 hours agorootparentLess than a 720 FICO score is “fair” with 720 being “good”. I’m sorry but “fair” doesn’t make you a well-qualified buyer IMO, so I’m not sure what point you’re trying to make. reply supertrope 6 hours agorootparentIt's FICO & DTI & PTI. None of this is defined in the ad not even the fine print. I'm well aware that 700 is around the 50th percentile. reply xyst 7 hours agoparentprevHow this ad got green lit, distributed to various mediums (tv ads, yt channel, social media), and nobody saying “wait, this is terrible” is unfathomable to me. reply llacb47 5 hours agoparentprevThat is… unsettling. reply diogenescynic 6 hours agoparentprevI saw the exact same ad today. reply bsimpson 9 hours agoprev> This decision is the result of a biased and broken system where the Commission serves as accuser, judge, jury, and then appellate judge all in the same case Interesting argument, but also a distraction from \"we lied in advertising.\" > It could also clearly disclose the percentage of customers that actually do qualify for the free service, somewhere close to the “free” claim advertised, the commission said. They ran an ad during the 49ers game this weekend, centered around it being free for the character in the ad. I'm guessing that's how they'll work around this. reply mardef 7 hours agoparentTheir phrasing is specific to align themselves with the current supreme Court review of deference. They'll likely delay until that decision has come down and they hope they can just claim the laws were unconstitutional in the first place. https://www.scotusblog.com/2024/01/supreme-court-to-hear-maj... reply granzymes 6 hours agorootparentRight idea, wrong case. The relevant case this term is SEC v. Jarkesy, which presents some of the \"Federal agency adjudication is unconstitutional\" arguments. The FTC brought this suit in-house before their own Administrative Law Judge, which is what TurboTax is protesting. The two cases related to Chevron deference also implicate agency power, but they implicate agency power in the actual courts. https://www.scotusblog.com/2023/11/justices-to-consider-mult... reply MBCook 8 hours agoparentprevBills game too. I noticed it because I thought they were already banned. It was much more clear it wasn’t free for everyone than the old “free free free free” ads, but still heavily emphasized the free part. reply mfi 4 hours agoprevIn Sweden, for most citizen the tax authority does the tax declaration for you. If you don't want to do any changes (which most people doesn't have to), you simply write a text to them [1]. [1] https://skatteverket.se/servicelankar/otherlanguages/inengli... reply midasuni 3 hours agoparentHow do private shareholders make a profit and thus increase the gdp from that? reply redrove 1 hour agoparentprevTo be fair, in Sweden the taxes are also insane. I’d rather have to pay some middleman a paltry sum every year than pay 30% more tax just for the privilege of the government automatically calculating them for me. Not saying the US shouldn’t do this automatically, I’m merely pointing out it’s not a 1 to 1 comparison. reply consp 1 hour agorootparentIf you include healthcare, daycare, state pensions, disability insurance, employment insurance etc as a tax (which it is in most cases with more tax) the \"insane\" amount isn't that insane anymore. reply throwaway26192 20 minutes agorootparentThe US spends more per capita in healthcare than Sweden and Finland combined [1]. [1] https://en.wikipedia.org/wiki/List_of_countries_by_total_hea... reply redrove 1 hour agorootparentprevIf you make $250k/yr+ in the US all of those problems melt away due to the availability of disposable income. In the EU you top out at €100-150k. I say this from experience as a European who's lived on both continents. reply yurishimo 40 minutes agorootparentBut this also depends almost entirely on where you live. €100k in the almost any city in Netherlands, France, or Germany is a great income that can support a family with one earner. $250k might only put you in \"solidly\" middle class if you have to live in SF or NYC. The difference is that the $250k salaries are 99% of the time only widely available in these extreme HCOL cities. Healthcare for a family of 4 in some parts of the US is absolutely fucking bananas compared to most of Europe. I've seen monthly premiums of $2k+ after the employers part, not to mention the deductible! reply euroderf 20 minutes agorootparentEfficiencies from health care competition are illusory when you properly scope it to include paperwork hassles, time hassles, and pressure on provides to skimp. reply termy 45 minutes agorootparentprevNot everyone makes that kind of money :(. reply redrove 27 minutes agorootparentAbsolutely true and a valid point, I’m just trying to add some nuance to the conversation. I’ve noticed the North American heavy online bubbles fetishizing Europe in that regard, and the reality is far more grim. reply doikor 4 minutes agorootparentprevWhat does the tax rate and prefilling tax forms have to do with each other? The system would work the same with 1% and 99% tax rates too. reply maelito 41 minutes agorootparentprevMore or les insane that say, the cost of private health and private education in the US ? reply keepamovin 8 hours agoprevFinally! This is only 30 years too late. Also, let’s hope the IRS’ current beta of self filing app is successful. reply bubblethink 8 hours agoparentThis will get stayed during the appeal, so status quo will remain for this tax season. And next year there are many new variables, potentially a new government, SC's Chevron ruling which may limit what FTC can do, etc. I don't think this is going to stick. reply Guvante 7 hours agorootparentHonestly it might not. It sounds like Intuit already has prepared advertising that fulfils the requirements from threads here. reply MikeTheGreat 7 hours agoparentprevCould you say more about the \"self filing app\"? A quick Google search didn't turn up anything (which probably says more about my current lack of sleep than anything :) ) reply daswerth 7 hours agorootparenthttps://www.irs.gov/about-irs/strategic-plan/direct-file Not to be confused with IRS Free File reply bubblethink 7 hours agorootparentHah, saw that you edited the url. These government naming schemes are designed to fail. Why do they pick these absolute terms like \"free\" when there will clearly be some new thing that will be better? I'm waiting for the next version of REAL id, a realer id. reply daswerth 7 hours agorootparentFor real. I did some reading to try to understand the rationale. I _think_ Free File is a general program which lots of \"services\" can participate in, including TurboTax. Direct File is akin to (but maybe not actually the same as) a IRS offering in the Free File program. reply whakim 3 hours agorootparentprevAs engineers I feel we should show a little bit of sympathy for difficulties involving naming ;) In all seriousness though, Free File pretty much does what it says on the tin (allows low-income taxpayers to file for free); it has existed since 2003; and it has been pretty successful all things considered (Intuit and H&R Block's best efforts notwithstanding). I think it's a reasonable name. reply sf_rob 4 hours agorootparentprevTypical government product launch website. As far as I can tell, they don’t provide a link to where the service will be available or date of availability despite it being tax season. I guess you are to check back if you’re potentially eligible or sign up for the newsletter. reply MikeTheGreat 5 hours agorootparentprevThank you for the link. I found the Free File thing but it didn't quite sound like what you were talking about :) reply bobro 6 hours agoparentprevWhat was happening 30 years ago? I would have guessed the online filing stuff and advertising based on \"free\" was much more recent? reply WheelsAtLarge 8 hours agoprevFinally, something is being done. Paying taxes should be free for most people. Especially if you only have to deal with the standard deductions. reply ghaff 7 hours agoparentIf you're standard deduction and just have a few simple sources of income, filing is easily free. Should it be electronic and pre-filled? Yes. But this idea that you need an accountant or tax prep software if you have a W-2 and a 1099 just isn't true. The forms are free. reply al_borland 6 hours agorootparentMy issue is they advertise “free, free, free”, but only for federal. After you go through everything they hit you up for money to file with the state. To go elsewhere means doing everything again, so people just pay it. It’s probably technically in the fine print, but it certainly felt like bait and switch the first time I ran into it over 20 years ago. I’m still bitter all these years later, and get reminded on a yearly basis, as it’s still standard practice. They can make federal “free”, because they can just lump all the costs in on the state taxes. Nothing is actually free. It also makes no sense to me that it costs money to e-file, while sending in paperwork that a person needs to open and process is free. It seems so backward. reply cj 7 hours agorootparentprevHalf of the allure of turbo tax is figuring out if the standard deduction is more or less favorable than itemized. Maybe the IRS should start by building a tool that points you in the right direction before it tries to build a TurboTax clone. reply ghaff 7 hours agorootparentI assume that information exists. It's mostly just totaling up some numbers. I know these days, for me, standard deduction works unless I have some outlier charitable/investment thing going on which I'm going to talk to my accountant about anyway. I assume there are other alternate energy credits and the like but not sure how they work and again aren't in play for most people in a given year. reply andirk 2 hours agorootparentWhen I don't make as much in a year, I go through all of the same exact steps just to see the standard deduction is better :D . It's a nice walk down memory lane though. reply bobro 6 hours agorootparentprevMortgage interest and state income taxes can easily get you well over the standard deduction for single people. reply cj 5 hours agorootparentSame here, mortgage interest puts me over the threshold (and single) reply jrockway 6 hours agorootparentprevMortgage interest is the big one. reply ghaff 6 hours agorootparentFair enough. I don't have any longer. reply ok_dad 7 hours agorootparentprevAwesome, let's do better and have the IRS fill out your form in Feb or March, mail it to you, and if you disagree you can still file yourself. That way, 95% of cases are covered and don't have to even deal with taxes. reply ghaff 7 hours agorootparentBy all means. Though I think it's considerably less than 95%. A lot of people probably have something non-standard. I have no problem with pre-filled forms that explicitly only cover certain types of transactions. reply Dalewyn 1 hour agorootparentprev>But this idea that you need an accountant or tax prep software if you have a W-2 and a 1099 just isn't true. I hire a CPA to do my dead simple returns that I can do myself. Why? Because I don't trust myself to have done it correctly. No, I can't sincerely swear everything written is true and correct to the best of my knowledge because I do not trust myself. That's why I hire a CPA, because I need someone I can point to for legal liability purposes. Am I throwing away money, so to speak? Perhaps, and I don't care; I'm buying peace of mind. reply andirk 2 hours agoparentprevIsn't a 1040-EZ free and takes 10 minutes? My tax situation is kind of complicated and costs me like $150 for all of the things TurboTax finds, figures out, calculates, submits, and all in a nice UI. Can someone please explain to me what is so horrible about paying a reasonable fee to figure out some complicated stuff that amounts to 80+ pages of filing docs? And free if it's EZ? And free if low income? reply Caligatio 2 hours agorootparentThe tax code is deliberately complicated because, in part, of the lobbying by Intuit and H&R Block. They're creating an artificial market for their $150 services and convinced the populace this is reasonable. reply dehrmann 7 hours agoparentprevA 1040EZ covers that case and filing it is the cost of the stamp and envelope. reply technothrasher 6 hours agorootparent1040EZ hasn't been a thing since 2017. But if you have a simple tax situation, the regular 1040 isn't all that hard to fill out. reply Guvante 7 hours agorootparentprevThe federal government can do a 1040EZ for you. Intuit has blocked government attempts to do that by offering free taxes for half of Americans (that was the deal). Of course they decided to renegade and only do 37% per their advertising. Hilarious since they had the \"federal is free but you need to pay for state\" scam going already. reply plus 7 hours agorootparentprevMy understanding is that if a 1040EZ is appropriate for you, then TurboTax will also be free. It's only if you have a more complex filing situation that they charge. Please correct me if I'm wrong, though. reply al_borland 6 hours agorootparentIt’s only free for federal, then they change you for state. I’m sure if they charged for both the final cost would be the same. They jack up the price of the state tax filing to cover the “free” federal taxes. This is how pretty much every “free” tax program works. reply ghaff 6 hours agorootparentprevThere is no 1040EZ as I understand it though there is the base-level 1040. reply calvinmorrison 7 hours agorootparentprevAnd an understanding of tax law reply ghaff 5 hours agorootparentI know the tax code has probably gotten more complicated, but when did people start to get the idea that even super-straightforward taxes (like just a W-2 or maybe a 1099) requires an accountant, tax software, or a degree in tax law? My taxes have been pretty complicated for a while but I don't get the perceived need for TurboTax for even the simplest of returns. Now maybe you miss out on some deductions if you just transfer a few numbers to the forms. But the standard deduction is going to cover most people until you get into big mortgages or charitable writeoffs. reply ijhuygft776 8 hours agoparentprevOur documents should not be sent automatically to the IRS if the IRS won't figure out our taxes. reply topkai22 3 hours agorootparentSpeaking generally, but the IRS as an institution would love to be able to move to automated tax filing. The thing that is blocking that happening is intense lobbying by intuit (and a few others) plus anti-tax groups. See https://www.propublica.org/article/how-the-maker-of-turbotax... for what was probably that best quick summary from about 10 years ago. reply mulmen 7 hours agorootparentprevCan they figure out our taxes without our returns? For simple stuff like paychecks maybe. What if you are self-employed? What if you are a contractor? What if you barter or pay cash? reply ijhuygft776 7 hours agorootparentof course it would not cover all cases... but I'm guessing that it would cover most people... why would I want them to have my files if all they are going to do is use them against me anyways? reply mulmen 6 hours agorootparent> but I'm guessing that it would cover most people... Do you have any reason to believe that? Are they your documents? Can you accrue taxes independently? reply ijhuygft776 6 hours agorootparent> Do you have any reason to believe that? just from my personal experiences... either way, the IRS has that info, if they assume people are honest... not sure why it isn't public. reply mulmen 4 hours agorootparentWell, they might have all the info sometimes but we don't know how often or if they even do. How would the IRS know if they know everything? What evidence do they have of absence? There are 36 countries that manage to do return-free taxes. They seem to have a base rate that covers most (~64% in the UK) people. The US tax code is way more complicated than one base rate for everyone. It's unclear to me how anything other than income tax is handled in these countries. Does the UK have a capital gains tax? Tax deductions? reply ascorbic 3 hours agorootparentIn the UK, payroll and savings income is taxed at source and around 80-85% of people don't need to complete a tax return. You only need to complete one if you have other income, if you're self-employed with over £1000 revenue, had capital gains over £6000, or earn over £100,000. Almost everyone completes these online, and you can do it directly on the government site. For simple cases that only takes a few minutes. It took me about an hour this year because I've been bad at keeping track of side project income. The form itself is easy. reply mulmen 2 hours agorootparentThanks for the explanation, I only did a quick search. Is there a capital gains tax in the UK? Do you have tax deductions? Edit: I looked it up. Yes, the UK has capital gains taxes and they must be calculated by the taxpayer. https://www.gov.uk/capital-gains-tax/reporting-and-paying-ca... There are also deductions for savings and dividends. Those deductions look a lot like the US deductions. They also (sometimes?) have to be calculated by the taxpayer. https://www.gov.uk/apply-tax-free-interest-on-savings reply hunter2_ 7 hours agorootparentprevThey will figure out your taxes (according to whatever information they have at the time) and retaliate if you file something that doesn't jibe. They just won't do it before you do. That way, they might learn things they didn't yet know, from people who decide (as legally obligated) to divulge additional information. I assume that they assume that far fewer people would divulge additional information, if their \"you talk first, I talk second\" tactic went away. reply kevin_thibedeau 7 hours agoparentprevThey've always been no more then the cost of a stamp. reply munk-a 7 hours agorootparentThat's extremely disingenuous and ignores the amount of labor required to self-file in the US. Most of the information you send to the IRS they already know - they just can't tell you because Intuit wants to justify their existence. reply ghaff 7 hours agorootparentIf your taxes are complicated, certainly, you'll probably be prudent to use an accountant. (Which I do.) A basic filing like most people have is pretty straightforward (though there should still be a pre-filled out form whether you choose to use it or not). There's this perpetuated myth that filing taxes is really complicated even if you just have a W-2 as is the case for many. And it's simply not true. reply mock-possum 5 hours agoparentprevPaying taxes should be free for everyone imo. reply chad_strategic 8 hours agoparentprevI came here to say the above. But let me add the following… Intuit is a criminal lobbying organization. reply financypants 7 hours agorootparentI guess they’re not doing a great job then? Because of the auto-filing/IRS-backed filing system being developed? reply grigri907 6 hours agorootparentThey've successfully managed to delay it for decades reply mulmen 7 hours agorootparentprevWhat laws have they broken? reply lmm 7 hours agorootparentWell, apparently they've been putting out deceptive advertising. reply mulmen 6 hours agorootparentThe claim is that Intuit is a “criminal lobbying organization” so what lobbying laws were broken? Or was lobbying meant to be a verb instead of an adjective? reply jrockway 6 hours agorootparentprevIs there any non-deceptive advertising? reply mulmen 6 hours agorootparentYes. I bought a cheeseburger once and got a free coke, just like the sign said. There’s certainly a line because some advertising is illegal and some isn’t. reply andersrs 7 hours agoprevMost countries governments are crap at software yet still manage to put together a free to use tax portal with automatic filing for all but the most complex cases. reply __MatrixMan__ 4 hours agoparentSee that's what you get when your government focuses on its citizens. Instead, US has corporations. Corporations like Intuit, which it uses to decide what its policies should be. Oh us? We merely live here. reply ryukoposting 8 hours agoprevIf the IRS had the budget to run those ridiculous \"free free free\" ads from a couple years ago, TurboTax would cease to exist. reply apengwin 7 hours agoparentIncreasing the IRS budget is an unequivocal good! reply tasty_freeze 7 hours agorootparent50% of the country believes tax is theft, and they also imagine if there were no taxes and the IRS abolished we'd somehow still continue to support an army, have a federal court system, a patent office, etc, etc, and that their state will continue to get federal subsidies. That was the entire point of Obama's speech that republican news sources and libertarians eviscerated him for: \"You didn't build that\". He wasn't saying that you didn't build your business; he was stating you didn't build all the infrastructure that allowed you to build and run your business, and that is why taxes are not theft. There is a legitimate argument over how much taxes and what is should be spent on, but those nuanced discussions are entirely replaced by BS. reply abirch 9 hours agoprevThis is nice. Free should mean free no matter how much large print text to clarify is needed. reply xahrepap 8 hours agoparentNext I’d like them to go after windshield chip repair shops and pharmacies that advertise “free*” services. Where free == paid by your insurance. Assuming your insurance will pay for all of it. Free doesn’t mean paid for someone you paid to handle it for you. reply al_borland 6 hours agorootparentI asked one of those chip repair guys how it worked years ago. He said the insurance companies pay them to repair the chips, because those little fixes are much cheaper than paying to replace the whole windshield when it inevitably cracks. It seems like a win-win-win. The driver gets the windshield fixed with no out of pocket cost, the chip guy makes some cash, and the insurance company saves money. Where’s the downside? reply KTibow 6 hours agorootparentWhile that does sound good I think the previous comment was saying how it's not free without insurance so it's not technically free reply al_borland 5 hours agorootparentI suppose. Where I’m at, and I think almost everywhere in the US, drivers are legally required to have car insurance. Assuming the chip repair places don’t require a premium level of insurance, it should be a non-issue, as the requirement is something everyone is already paying for. reply figassis 21 minutes agorootparentIt is still not free. We need to stop redefining things. reply rconti 8 hours agoprevI've only ever used the installable version of TurboTax, and I knew up front that I'd have to pay for CA filing. How transparent is the web site version? Do you have to spend an hour entering all your tax info just to find out that you'll have to pay to file? Or are they pretty good about notifying you up front? In the paid installable version, I can still print out my CA tax return and mail it in to avoid paying the e-filing fee, but I'm not sure how that works with the web version, either. reply daft_pink 8 hours agoparentYou knew you would have to pay for the ca version, but did you know how much you had to pay at the beginning. I buy the disk at Costco every year and feel the pricing is anything but transparent and they are very misleading. It includes the state free but only paper file and $10 credit for a mystery price state return reply rconti 4 hours agorootparentAh good question! Of course I knew in future years, but I'm not sure I knew up front. reply adaboese 6 hours agoprevIt is more crazy that this was allowed to continue for so long. reply mportela 8 hours agoprevAny recommendations on how to really file taxes for free (federal and state)? reply loeg 8 hours agoparentFWIW, TurboTax has a C# method (\"IsProductActivationRequired()\" or something like that) you can overwrite to 'return false' with tools like dnSpy. reply junon 6 hours agorootparentThere's a desktop version? Does it offer the same as the site? reply dmoy 3 hours agorootparentThe desktop version is the OG, though now it's like 5% of TurboTax users. > Does it offer the same as the site? Typically more, in fact. Still probably a rip-off compared to other places. reply handity 1 hour agoparentprevIf your income is below $79,000 a year, this is the correct link: https://apps.irs.gov/app/freeFile/browse-all-offers/ It has been remarkably absent in this thread. reply z3ugma 8 hours agoparentprevThe most scam-sounding federal government website ever, Free File Fillable Forms https://www.irs.gov/e-file-providers/free-file-fillable-form... reply AdieuToLogic 8 hours agorootparentAnother program the IRS is rolling out is called \"Direct File\"[0]: In 2024, we're launching a new pilot tax filing service called Direct File. If you're eligible and choose to participate, file your 2023 federal tax return online, for free, directly with IRS. It appears to be limited to a handful of states thus far. 0 - https://www.irs.gov/about-irs/strategic-plan/direct-file reply JadeNB 6 hours agorootparentprevAt least it's under the irs.gov domain now. As recently as last year, I had to, or at least thought I had to, go to the even scammier-sounding https://www.freefilefillableforms.com. reply linsomniac 8 hours agoparentprevI've been using https://www.freetaxusa.com/ the last couple years and plan to continue this year. It is free for Federal (though I usually pay for an upgrade), and state is $15. The only option I know of for free state is to do the forms manually. reply takk309 5 hours agorootparentI decided to use FreetaxUSA on the advice of another HN thread and I was very happy with the results. I had used TurboTax for the last 8 years and can say that FreetaxUSA was much easier to navigate. I was asked to upgrade to their top tier 2 times and it was very clear how to continue with the free federal option. reply ok_dad 7 hours agorootparentprevI second this suggestion. The free federal taxes and $15 state actually mean free federal and $15 for state! You can certainly add on the \"pro service\" or \"audit protection\" or whatever if you want, but it is simple to say NO to those and just do the forms. This year, I opted for the \"audit protection\" because of some new tax forms I had to fill out, but overall I only paid $45 with the sales tax. I used to use Turbo Tax and paid about $100 per year on average, but it was simple to switch to using another provider, I just had to get the previous year's return out to fill some stuff that is filled automatically if you use the service each year. reply dmoy 8 hours agoparentprevIf your income is low enough, all of the main tools If not, then federal free through FreeTaxUSA. Depending on your state, the state taxes manually via PDF + paper may be extremely quick - like five or so lines, and a single table lookup. reply tunesmith 5 hours agoparentprevSemi-dumb follow-up question. If we don't use a website service, then other than using forms, is the only option to actually find the pdfs, print out the forms, fill them in with a pen, and stuff them in an envelope? Or are there easier ways? reply timw4mail 8 hours agoparentprevCash App Taxes. Free federal and state. reply tunesmith 5 hours agorootparentHrm. How do they make money? reply altairprime 6 hours agoprevOpenTaxSolver should be released for US 2023 in a few days! Can’t wait. Forms and math, be still my heart. reply SoftTalker 5 hours agoprevIt's 2024. It's beyond time for the IRS to offer free tax filing online. You've been able to file for free on paper since forever. But again, it's 2024. Paper is not a reasonable solution for either filers or the IRS in 2024. It's difficult to handle, error-prone, and all the software they use to validate paper forms could be repurposed to accept online forms. And the online forms should be pre-filled with all the information the IRS already has about your income for the prior year. It's not acceptable to have to pay companies like Intuit (and give them all your personal financial information) for software to file an individual tax return. I don't care if you're wealthy or poor, this should be free and provided by the IRS. Contact your representatives. Demand that they get it done. reply throwawaymaths 4 hours agoparentUh, you know that it's a thing starting this year? https://www.cnn.com/2023/10/17/politics/irs-free-tax-filing-... reply Antrikshy 4 hours agorootparentTIL! Fortunately, my state (Washington) is on the list. Unclear when it will be ready for use. Anyone know? I just signed up for their newsletter. reply coloneltcb 9 hours agoprev [–] Intuit is an embarrassment of a tech company. Instead of innovating in software, they've invested infinitely more in lobbying to keep tax-filing complex and in creating new and innovative dark patterns to obscure their (federally mandated, btw) free product. reply toomuchtodo 9 hours agoparentThey are not a tech company. They are a regulatory capture company that uses tech to extract from the general public what should be free. The code and platform are the performance art to enable the rake. Edit: Meant to scope this comment solely to TurboTax. Replies correcting me are well deserved. reply WesternWind 9 hours agorootparentNo, they are a tech company, they produce software for their main product. Like a lot of companies, tech and not, they use their power and resources to keep their position. The difference is that their actual competitor is the government. But that's also a big part of why private healthcare is way too expensive in the US, for worse results than other places. reply raegis 8 hours agorootparent> The difference is that their actual competitor is the government. I think the government is more of an enabler, than a competitor. At least up until now. reply colordrops 8 hours agorootparentprevIt's a game of semantics. The other commenter's meaning is that a \"tech company\" would produce valuable tech that wouldn't otherwise exist. Intuit uses tech to do the opposite. McDonalds uses tech too, but they aren't a tech company. reply ang_cire 8 hours agorootparentMcDonald's main product is not a piece of software. Intuit's is. reply colordrops 6 hours agorootparentThat's what I'm saying though, they don't even produce a product. They produce legislative influence to create a money collecting trap. Turboxtax isn't a \"product\" per se. That's like calling the mafia security guards because they force you to pay protection money. Many countries don't even require filling out tax forms. You just sign and pay. reply pcurve 9 hours agorootparentprevI think most people would be shocked to learn how big intuit is in terms of market cap. Bigger than Verizon wellsfargo ibm nike reply rootusrootus 9 hours agorootparentFrom comments in threads like this I think most people would be shocked that Intuit makes the largest segment of their income from business customers, not individual consumers. reply foolswisdom 7 hours agorootparentIs this true for turbotax specifically, or merely for intuit as a whole (which includes things like quickbooks)? reply pcurve 7 hours agorootparentThe “business customer” is a slight misnomer because Intuits definition of this segment includes “Small Business and Self Employed”. Quickbooks accounts for thr largest share For consumer segment TurboTax still drives 30% of their entire revenue. reply jjulius 9 hours agorootparentprevFor those unaware, Intuit owns... TurboTax Mint QuickBooks Credit Karma MailChimp reply s1mon 7 hours agorootparentThey do own Mint, but they are killing it and trying to get customers to switch to Credit Karma. Intuit bought Mint but never really put much development effort into it beyond the bare minimum. Like too many financial tracking things which are \"free\", the user was the product on Mint that its advertisers wanted. I switched to Monarch which is paid and has some people from the original Mint team behind it. reply smartbit 6 hours agorootparentprevand GitOps tool ArgoCD reply MathMonkeyMan 9 hours agoparentprevFree government-provided tax calculators and online filing are an existential threat to Intuit. I think that the business should not exist (or should be scaled down to a fraction of its size, as an accountant's tool), but can you blame them for trying to block the deathblow? reply quantified 9 hours agorootparentSure. How much don't you blame the rat whose claws are embedded in your hand as you try to throw it in the river? reply rightbyte 58 minutes agorootparentI am quite sure it could swim so I would think it was ungrateful for me being so kind to it. reply MathMonkeyMan 7 hours agorootparentprevAs a New Yorker, I wouldn't dare try, but I see your point. reply nshelly 8 hours agoparentprevIntuit also produces Quickbooks -- which actually is SMB accounting software, and used internationally. As most of their revenue is from businesses who wouldn't easily be able to free-file, I wouldn't be surprised if Intuit's lobbying to prevent a free tax filing offering from the government was causing them more harm than good. reply jimbob45 8 hours agorootparentAnd now QuickBooks is also a bank? Offering checking accounts? I suspect they’ve just partnered with a real bank to do so but the brand confusion is off the charts. reply nshelly 8 hours agorootparentI imagine Quickbooks is adding more features as part of an upsell. The business loans by paying invoices upfront was a bit of a ZIRP policy though (now interest rates to SMB's likely would be prohibitively high, ~15%). reply slg 9 hours agoparentprev [–] >they've invested infinitely more in lobbying to keep tax-filing complex I always feel the need to point out that anger at this is misplaced. Intuit is a business lobbying on behalf of itself like countless other businesses do. If the result makes you upset, you should be upset at our political system and the politicians who are willing to sell out the country for whatever those lobbyists are pushing. It is those politicians who are supposed to prioritize the good of the country, not Intuit or their lobbyists. reply jjulius 8 hours agorootparent [–] We can be upset at politicians for \"selling out\" to the lobbyists. We can be upset at lobbyists for pushing things that they know aren't in the best interests of consumers. We can be upset at businesses for wanting to take advantage of consumers for their own gain. All of these can be true at the same time. reply slg 8 hours agorootparentThe difference between those three groups is that the politicians are the only ones not doing the job expected of them. Lobbyists are supposed to lobby on behalf of whoever pays them. Businesses are supposed to advocate for the interest of themselves, their shareholders, and their employees. It would be verging on incompetence for a business to ignore a political debate that would effectively destroy the business overnight. Intuit lobbying on its behalf is our system working as intended. If you have a problem with that (which you should), your problem should be with the system and not Intuit. Politicians are supposed to work for the betterment of their constituents and they clearly aren't in the case of tax prep. They deserve a majority of the blame above any other group because they are the ones failing to do their job. reply janalsncm 7 hours agorootparentI don’t think that’s a meaningful distinction. A lot of people would say lobbyists shouldn’t even exist as a profession. Whether they are “just doing their job” begs the question of whether the job should exist. reply jjulius 7 hours agorootparentprevBusinesses can lobby for their own self-interest without taking such a staunch anti-consumer stance. Technically, you're not wrong. But Intuit happens to own MailChimp, QuickBooks and Credit Karma beyond TurboTax. There's no reason they shouldn't be able to find a way to remain a profitable business between most of what they own. To go above and beyond with such an anti-consumer perspective with regards to filing is absolutely egregious. Put simply (or reductively, take your pick) - there's \"advocating for the interest of their business\" (nods head) and \"advocating for the interest of their business\" (shakes head). reply avalys 8 hours agorootparentprev [–] Do you get upset at criminal defense lawyers who defend people who are obviously guilty (i.e. by getting clear and convincing evidence disqualified because of a procedural error?) Or, would you say something like \"The outcomes are not perfect but that's how the system works and stays as fair as possible for everyone\"? reply AdieuToLogic 8 hours agorootparent> Do you get upset at criminal defense lawyers who defend people who are obviously guilty ... This is an obvious Straw man[0] fallacy. Intuit is a corporation engaged in lobbying efforts in order to retain and/or expand its market-share and not an individual needing representation in a criminal indictment. Unless your position is that Intuit corporate officers should be criminally prosecuted, which, if that is the case, I apologize for my misunderstanding and can see your point. 0 - https://en.wikipedia.org/wiki/Straw_man reply avalys 7 hours agorootparentThe system requires criminal defense attorneys to lobby for their client, even if this results in objectively bad outcomes in specific situations. Similarly, the system requires that anyone (including shareholders or employees of companies) be able to communicate with their political representatives about political policy. Even if sometimes this results in bad outcomes in specific situations. reply jjulius 7 hours agorootparentThe suggestion that citizens have equal access to communicate with their political reps as corporations is also laughable. reply avalys 7 hours agorootparentA corporation can represent the interests of hundreds or thousands of people - investors, shareholders, employees, customers (who depend on the product), etc. The larger corporations, even more so. But still, the Congressional office buildings are open to the public. Anyone can walk in and present their case. Yes, of course a senator cannot make as much time to meet with the millions of individual citizens as they can with the 500 individual Fortune 500 companies. I don’t think this defeats my argument. An individual citizen is objectively less important than a company involving tens or hundreds of thousands of citizens. reply janalsncm 6 hours agorootparentCorporations may touch a large number of people but those people are not a representative sample. Further, corporations are not lobbying for the interests of that large group of people, they are lobbying for the interests of the corporation, whose interests may incidentally and temporarily align with the groups you mentioned. The interests of the corporation only align with those of the people you mention insofar as those are in line with maximizing profits. However, this completely ignores the externalities of maximizing profits, such as those to the environment. More broadly, a country whose politicians represent the interests of corporations is no longer a democracy. Democracy is for people. Corporations are not people. They are abstract legal entities wired to maximize profits. The fact that they are composed of people is immaterial, since they do many things contrary to the interests of those people all the time. reply jjulius 6 hours agorootparentprevThe disconnect occurs when you realize that the issues businesses lobby for don't always benefit their own employees, nor are they only ever lobbying just to stay alive as a business. In point of fact, Intuit's lobbying in this very instance also negatively impacts their own employees. Edit: A generic example to further drive the point home would be oil companies lobbying against environmental protections. Such moves only benefit the business and directly harm everyone, including their own employees. Edit 2: And then there are all of the instances where businesses have lobbied against changes to labor laws, wages, etc. that would objectively improve the lives of their own employees... reply jjulius 8 hours agorootparentprev [–] Most things are circumstantial and nuanced, not black and white. So, no, I usually do not get upset at that. Note how I said \"can\" rather than saying \"must be\" or something more absolute like that. And the idea that things are \"as fair as possible for everyone\" in this particular (read: Intuit) scenario is laughable at best. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The given texts contain a wide range of data, including CSS code snippets, configuration settings, advertising information, regulatory actions, JavaScript code snippets, and news reports about TurboTax and the Federal Trade Commission.",
      "Each piece of information serves a specific purpose and has its own functionality based on its context.",
      "The texts provide diverse and relevant information related to TurboTax and the Federal Trade Commission."
    ],
    "commentSummary": [
      "The Federal Trade Commission (FTC) has banned TurboTax from advertising their \"free\" services, raising concerns about the company's marketing practices.",
      "The discussion also explores the challenges people face with fractions in math and the importance of understanding this concept.",
      "The topic of budgeting for low-income individuals is addressed, highlighting the financial difficulties faced by many and the need for effective budgeting strategies.",
      "Different tax filing options are discussed, including the role of Intuit in the tax filing industry and their lobbying efforts.",
      "The debate on the access and influence of corporations in political representation is examined, highlighting the potential implications for democracy."
    ],
    "points": 757,
    "commentCount": 207,
    "retryCount": 0,
    "time": 1705968020
  },
  {
    "id": 39092505,
    "title": "Modder recreates Game Boy Advance games using crash sounds",
    "originLink": "https://arstechnica.com/gaming/2024/01/modder-recreates-game-boy-advance-games-using-the-audio-from-crash-sounds/",
    "originBody": "To truly catch them all — Modder re-creates Game Boy Advance games using the audio from crash sounds Create a bootable, working Pokémon game by recording it crash multiple times. Kevin Purdy - 1/22/2024, 5:08 PM Enlarge / Andrew Cunningham's modded and restored Game Boy Advance could, with enough time, sing out all the data loaded into a cartridge. Andrew Cunningham reader comments 59 Sometimes, a great song can come from great pain. The Game Boy Advance (GBA), its software having crashed nearly two hours ago, will, for example, play a tune based on the game inside it. And if you listen closely enough—using specialty hardware and code—you can tell exactly what game it was singing about. And then theoretically play that same game. This was discovered recently by TheZZAZZGlitch, whose job is to \"sadistically glitch and hack the crap out of Pokémon games.\" It's \"hardly a ready-to-use solution,\" the modder notes, as it requires a lot of tuning specific to different source formats. So while there are certainly easier ways to get GBA data from a cartridge, none make you feel quite so much like an audio datamancer. TheZZAZZGlitch's demonstration of re-creating Game Boy Advance ROM data using the sounds from a crashing system. After crashing a GBA and recording it over four hours, the modder saw some telltale waveforms in a sound file at about the 1-hour, 50-minute mark. Later in the sound-out, you can hear the actual instrument sounds and audio samples the game contains, played in sequence. Otherwise, it's 8-bit data at 13,100 Hz, and at times, it sounds absolutely deranged. Advertisement \"2 days of bugfixing later,\" the modder had a Python script ready that could read the audio from a clean recording of the GBA's crash dump. Did it work? Not without more troubleshooting. One issue with audio-casting ROM data is that there are large sections of 0-byte data in the ROM, which are hard to parse as mute sounds. After running another script that realigned sections based on their location in the original ROM, the modder's ROM was 99.76 percent accurate but \"still didn't boot tho.\" TheZZAZZGlitch later disclaimed that, yes, this is technically using known ROM data to surface unknown data, or \"cheating,\" but there are assumptions and guesses one could make if you were truly doing this blind. The next fix was to refine the sound recording. By recording three times and merging them with a \"majority vote\" algorithm, their accuracy notched up to 99.979 percent. That output ROM booted—but with glitched text and a title screen crash. After seven different recordings are meshed and filtered for blank spaces, they achieve 100 percent parity. That's about the halfway point of the video; you should watch the rest to learn how it works on physical hardware, how it works with a different game (an ARM code mystery in a replica cartridge), and how to get the best recordings, including the use of a \"cursed adapter\" that mixes down to one channel the ugly way. reader comments 59 Kevin Purdy Kevin is a senior technology reporter at Ars Technica, covering a variety of technology topics and reviewing products. He started his writing career as a newspaper reporter, covering business, crime, and other topics. He has written about technology and computing for more than 15 years. Advertisement Channel Ars Technica ← Previous story Next story → Related Stories Today on Ars",
    "commentLink": "https://news.ycombinator.com/item?id=39092505",
    "commentBody": "Modder re-creates Game Boy Advance games using the audio from crash sounds (arstechnica.com)484 points by dagenix 15 hours agohidepastfavorite66 comments thewakalix 14 hours agoThe issue with long runs of 0x00 is related to \"clock recovery\". > Some digital data streams, especially high-speed serial data streams (such as the raw stream of data from the magnetic head of a disk drive and serial communication networks such as Ethernet) are sent without an accompanying clock signal. The receiver generates a clock from an approximate frequency reference, and then phase-aligns the clock to the transitions in the data stream with a phase-locked loop (PLL). > In order for this scheme to work, a data stream must transition frequently enough to correct for any drift in the PLL's oscillator. The limit for how long a clock-recovery unit can operate without a transition is known as its maximum consecutive identical digits (CID) specification. https://en.wikipedia.org/wiki/Clock_recovery reply akira2501 13 hours agoparentI love how we used to use a bunch of very clever \"code book\" systems like 8b/10b which did a lot of careful work with small runs of bits to ensure the clock was recoverable and to avoid line capacitance issues. Then we just moved to things like 64/66b which takes a giant chunk of bits, adds a short header to guarantee a clock transition, then runs everything through a pseudorandom scrambler. reply userbinator 8 hours agorootparentAgainst adversaries, scrambling schemes can produce some very perplexing behaviour. See the \"weak sectors\" used for CD copy protection for one infamous example. reply tux3 2 hours agorootparentprevThe PCIe standards kept moving to longer codes, and nowadays they're able to do \"1b/1b\" (no header at all) reply phire 23 minutes agorootparentNot really accurate. The switch from NRZ to PAM4 actually massively increased the bit error rate. They switched away from the 8b/10b style line code and replaced it with forwards error correction. PCIe 6.0 uses 256 Byte frames, with 242 Bytes of data, 8 Bytes or CRC and 3 Bytes of error correction. So it actually has way more overhead than the older versions and their 128b/130b line coding, It's just at a slightly different layer. reply dwattttt 12 hours agorootparentprevA sprinkle of entropy improves everything reply jacquesm 10 hours agorootparentThey do a similar thing for GPS data recovery. It is so far below the noise floor that normally speaking the signal is not recoverable. But then you inject some (known) noise and suddenly the noise modulated by the (unknown) signal starts to drown out the noise in the rest of the system and that in turn allows you to recover bits from the signal. reply akira2501 9 hours agorootparentIt's not only below the noise floor, but all satellites transmit the code on the same frequencies, so it's several signals all at once below the noise floor. Which makes the known noise unique to each satellite, and you dredge out the same frequency repeatedly with different sets of known noise to recover multiple signals. Gold sequences are really neat, which is precisely the same pseudorandom scrambling technique, but where each sequence is selected to have low correlation with all other sequences in use, which is what enables the frequency sharing property of the system. reply rollcat 1 hour agorootparentprevhttps://ciechanow.ski/gps/ reply farhanhubble 9 hours agorootparentprevThis thread brings back fond memories from electronics and digital signal processing. reply physicles 9 hours agorootparentprevQR codes do this too, as explained by yesterday’s front page post about decoding QR codes by hand. reply hun3 18 minutes agoparentprevThis is probably irrelevant. The audio is analog (requires DAC to decode into bitstream) and is transmitted with some fixed frequency (44.1kHz or 48kHz), without any particular synchronization. reply mlyle 5 hours agoparentprevAnother concern is if the waveform is AC coupled somewhere and DC can't pass. Even if you have a perfectly synchronized clock, all-or-mostly-1's will be the same as all-or-mostly-0's in the long run. reply andai 3 hours agoparentprevFrom this page I found a very interesting article: Wireless Set Number 10 https://en.wikipedia.org/wiki/Wireless_Set_Number_10 reply Y_Y 14 hours agoprevThis reminds me of the original iPodLinux hack almost twenty years ago where the fourth-gen firmware was dumped via the piezo speaker - https://web.archive.org/web/20140810083116/http://www.newsci... Coincidentally one of the things this facilitated was playing GBA games on the iPod, though I was happy enough just to play Doom a with the click wheel and watch monochrome videos. reply nathancahill 12 hours agoparentGood memories. I still have my iPod Classic, I upgraded the battery and added 4 256GB MicroSD cards. I saw a mod recently for adding Bluetooth but it involves swapping the rear case and I don't think I'll go that far. There's something timeless about the design (and about owning copies of music files). reply ace2358 5 hours agorootparentGiven the ‘super compute in your pocket’ thing we say… super computers turned out to be terrible at playing music files without stopping. My iPhone will regularly forgot what I was listening to moments ago after I press pause. Insane. reply tomduncalf 2 hours agorootparentYeah this is so annoying! It especially seems to happen with 3rd party music player apps. reply ace2358 2 hours agorootparentAlso apps and or websites pausing my music but play no sound… ugh. I want an iPod.app to come along. Tiny player does nice but the os level issues are getting insane. Also apps that have their data deleted (Apple tv infuse app, play:sub iOS) because the os said so. I have plenty of space but things get deleted. I want to FILL my 256 phone please. reply xyzzy_plugh 8 hours agorootparentprevFunny to see this mentioned. I was recently out skiing and was using my phone to listen to music, as I used to do with my 3rd generation iPod. Moments ago I was lamenting to my colleague that while my phone's battery rapidly plummeted in the cold despite being insulated and close to my body (no way even remotely approaching freezing temperatures) my iPod never struggled to play tunes all day. In fact I never recall any noticeable difference in battery performance in cold temperatures despite daily use for years. And yet my phone gets a bit chilly and gives up the ghost. I use Bluetooth otherwise I'd have dug out the 3rd gen for the next ski day. reply speedgoose 4 hours agorootparentSample size N=1 too but I remember warming up my iPod to get more battery while my iPhone survives pretty well while streaming my GPS coordinates when I go skiing. reply wegfawefgawefg 8 hours agorootparentprevWas it a difference in battery chemistry? reply KennyBlanken 9 hours agorootparentprevEvery music file I have in iTunes and on my iPhone is a DRM-less MP3 or AAC file. reply skeaker 14 hours agoprevGlad to see this getting more attention here, it was posted a few days ago but drowned (https://news.ycombinator.com/item?id=39037104). The original video has a lot not mentioned in this brief article, including a custom adapter that the hacker had to cut together manually to get the right audio quality out of the DS. reply taftster 14 hours agoparentLink to \"original video\": https://www.youtube.com/watch?v=0-7PSmYYHF0 reply hifikuno 12 hours agorootparentThe video was great. I think my favourite part is where he dumps the Chinese knock off version and finds the random ARM code and reverse engineers it. So much cool stuff in there. reply taftster 12 hours agorootparentRight? This kind of stuff just makes me look silly in what I'm able to achieve. I can only accomplish watching a video of some guy doing bad ass stuff and maybe holding onto a few notional details. People are just super smart sometimes. reply runnr_az 6 hours agorootparentIt’s especially fun watching super smart people work on somewhat ridiculous projects. reply Rapzid 44 minutes agoprevThe most impressive part of all this is, IMHO, how the code was modified by the bootleggers to run the game off writable flash vs ROM plus volatile save memory haha. reply brokensegue 14 hours agoprevZzazz (the subject of the article) runs a yearly April fools competition/event that usually involves some amount of retro hacking/reverse engineering. I recommend participating. Previous years competitions are on their GitHub reply peddling-brink 14 hours agoprevWhy would this happen in the first place? Is it common for these games to dump state to audio? Is it a deliberate debugging tool for the game devs? reply 0xC0ncord 14 hours agoparentThe author's previous video[1] explains the technical details regarding this behavior. Basically the way GBA sound works is there is a buffer in RAM that the audio is streamed from, and an interrupt is supposed to signal the hardware to begin reading data from the beginning of the buffer again. However, if the interrupt is never fired (such as when the game crashes), the audio stream will go beyond the buffer and read other parts of memory. [1] https://www.youtube.com/watch?v=wSWNkpqjtQY&t=361s reply jboy55 14 hours agorootparentWould this be dependent on the audio file that was being played during the crash to start at address 0 of the ROM? It seems like it'd be highly unlikely you'd be able to get 100% of the ROM. Now if this was a hack where the thought was, \"What if we dumped the whole ROM to the audio buffer, could we recover the complete ROM through audio analysis?\" reply Dwedit 8 hours agorootparentGBA Memory Map: 02000000-0203FFFF EWRAM (256 KBytes) 03000000-03007FFF IWRAM (32 KBytes) 06000000-06017FFF VRAM - Video RAM (96 KBytes) 08000000-09FFFFFF Game Cartridge ROM (max 32MB) The audio buffer will usually live in EWRAM, then you have to wait until about 100,000,000 audio samples have played before it proceeds from EWRAM to Game Cartridge ROM. reply ordu 13 hours agorootparentprevI think hardware starts from 0 after reaching 0xfff..ffff address. The article mentions that you need to wait before starting to record, I assume it is a pause needed for hardware to overflow address. reply shadowgovt 13 hours agorootparentprevThe ROM on the GBA is mapped into memory at high memory addresses (0x08000000 and above). The audio \"working\" buffer is in low memory (I think somewhere near 0x02000000?). An interrupt fires when the audio chip reads to the end of the working buffer that looks something like this: - run the function to fetch the next batch of audio to audio working RAM - reset the audio read pointer to the beginning of audio working RAM When interrupts are disabled (because the game has crashed), that \"reset pointer\" code never runs and the audio circuit keeps reading way past the end of its buffer, incrementing forever. Eventually it would increment into the 0x08000000 range in which case the sounds it's emitting map directly to the bits in the ROM. reply aprilnya 9 hours agorootparentprevthe sounds aren’t read directly from the rom - the buffer is in a fixed place in ram, and sounds from the rom get copied into there remember that it’s not like the music is just there as raw audio, the raw audio is just the sounds of the different instruments reply lelandbatey 13 hours agorootparentprevThat video seems to imply that the audio hardware automatically wraps back to address 0 when it reaches the end of RAM. That may not be true, but it's implied by the animation in that video. And since they were apparently able to dump the entire ROM via audio, I suppose there's some way to get the entire memory contents. reply shadowgovt 13 hours agoparentprevIt's pretty uncommon and it's not a deliberate debugging tool. In this case, the GBA could, hypothetically, have had a way to \"park\" the architecture on a game crash, or \"watchdog\" the system (by tying state update somewhere that should run periodically to a non-maskable interrupt that reboots the machine if that state update stops happening). Simply because it costs more to do those things, the GBA doesn't (Nintendo instead opting for the time-worn approach of the great game cart manufacturers of old, \"if our games don't have bugs we don't have to worry about the behavior of the hardware in undefined state!\"). So when a GBA game gets into some crash states (infinite loop with interrupts disabled, for example), the audio chip doesn't know the system is crashed and keeps doing its very simple job: reading sequential bits in RAM and converting them to sounds. Without the housekeeping that normally runs when the game is in good working order shepherding that read operation, it just keeps reading and eventually gets to the bits representing values in the cartridge ROM. reply Dwedit 8 hours agorootparentThe GBA does have an interrupt that occurs when cartridge is removed. reply scarecrw 1 hour agoprevThis reminds me of an attempt made by RGMechEx to retrieve game code for an Atari game where the code itself was used to generate a visual effect similar to TV static on the screen [1]. [1] https://www.youtube.com/watch?v=5HSjJU562e8 reply 2024throwaway 14 hours agoprevThis is insanely impressive. I'm sure many of the techniques used like the \"majority vote\" algorithm cited are underutilized across many industries. reply epcoa 13 hours agoparentIf one is interested there is nearly 100 years now of sophisticated noisy signal recovery techniques. Magnetic storage media operates on the principal of this hack at baseline! https://en.wikipedia.org/wiki/Partial-response_maximum-likel... The same idea can apply here as GBA ROM material would be highly biased. Majority vote wastes a lot of information. reply guyomes 11 hours agoparentprevOne interesting application of taking the value appearing in majority, or more generally taking the median, is to remove noise or people from a sequence of several photos taken from the same point of view. This idea is to superimpose all the photos, and for each pixel, you simply keep the median [1]. [1]: https://patdavid.net/2013/05/noise-removal-in-photos-with-me... reply astrange 2 hours agorootparentThis will have the effect of smoothing the image even if you didn't want to do that, which is part of the reason people are convinced phone cameras \"overprocess\" images or try to beautify you even when they aren't. You have to counteract this by actually putting the noise back when you're done. reply jacquesm 10 hours agoparentprevIt's fairly common in data recovery though, just keep on reading disk images and running quora until something passes the checksum and see if it works out. Better still if there are higher level checksums (for instance across a file that has a signature) for further verification. Also used in some aerospace applications. reply mikepurvis 12 hours agoparentprevI wondered about it for film scanning, specifically with a fan project like 4K77 where they're dealing with potentially damaged theatre prints rather than pristine masters— having multiple of them and being able to use that to eliminate scratches and so on would potentially save a ton of time on manual fixing in post. reply da768 14 hours agoprevI imagine the 0xFF might be converted to 0x00 due to DC blocking capacitors/high-pass filtering, audio circuits aren't really suited for non-audible content. He might get better capture accuracy with a digital oscilloscope hooked directly at the chip's audio outputs if he's lucky, but it's still pretty impressive he got a bootable image out of that. reply arcticbull 14 hours agoparent> I imagine the 0xFF might be converted to 0x00 due to DC blocking capacitors/high-pass filtering, audio circuits aren't really suited for non-audible content. Yeah, you want to generate a signal with no DC bias, something as simple as Manchester encoding will go a long way. If that's not good enough, there's NRZ or even a convolutional encoding. You also want to make sure you either send a sin wave, or if you can't do that, at least make sure your square wave frequency is high enough that it doesn't get eaten by the AC coupling capacitors. reply freeqaz 14 hours agorootparentWould it be possible to get a higher quality read from using something like an Arduino's I/O pins and some bit-banged C code? I'd be curious to see what would be possible using cheap, off-the-shelf tools since a lot of people don't necessarily have an oscilloscope laying around. :P reply fodkodrasz 12 hours agorootparentWithout looking up datasheets, just form the top of my head: the Arduino DAC most likely has 12 bits resolution (as common for cheap uCs), and maybe even slower sampling than a soundcard. A sound card was probably better than that even in the 1990s (say a Sound Blaster). reply aidenn0 12 hours agorootparentThe original Sound Blaster could only record at 8-bit resolution at up to 12 kHz. The 2.0 could record at up to 15kHz, still 8-bit. The second generation of Sound Blaster was the first that could record at 44kHz (mono) sampling rate, but was still only 8-bits of resolution. It wasn't until the 3rd generation Sound Blaster 16 that 16-bit audio could be recorded. reply fodkodrasz 11 hours agorootparentHmm, I had (false) memories of better capabilities. Though my first soundcard was an SB Pro clone, later an SB16, both almost capable :) reply Frenchgeek 14 hours agoparentprevFrom what I remember reading in the comments of his youtube video, the point of the exercise was to do it with as basic equipment as possible.Hence the hours of multiples passes to average the error out instead of a proper data-logging oscilloscope. reply seabass-labrax 12 hours agoprevDoes anyone know what's going on when the TheZZAZZGlitch's emulator reports that the game tries to jump to an invalid address? I'm not so familiar with the ARM7 processor used in the GameBoy Advance, but I can't imagine how it would be possible to construct a jump call with an invalid value. Additionally, what would happen if one of TheZZAZZGlitch's incorrectly reconstructed ROMs was run on a real GameBoy? reply cosarara 11 hours agoparent> I can't imagine how it would be possible to construct a jump call with an invalid value You can use the bx instruction to jump to any address stored in a register. > Additionally, what would happen if one of TheZZAZZGlitch's incorrectly reconstructed ROMs was run on a real GameBoy? It would crash, and eventually start playing the ROM on the speaker, the whole point of the video :) reply seabass-labrax 10 hours agorootparentThanks for the explanation! So it sounds like the emulator is detecting the error 'out of band' with the emulated execution flow, and choosing to throw an error message rather than letting the execution continue. If it didn't catch the error, it sounds like it would glitch out in much the same way as the real GameBoy would. Is that right? reply ehaliewicz2 9 hours agoparentprevIt is likely that the emulator only emulates accessing valid parts of the GBA's memory map, and throws that error if invalid parts are accessed. As to what happens on real hardware.. who knows? :) reply maxlin 7 hours agoprevThe diffing of the real-world cartridge was the most interesting part. Requires a goofy amount of unnecessary knowledge to just jump in to reversing that and figuring out what its meaning is lol reply goodboyjojo 28 minutes agoprevthis is a good article. very interesting stuff reply jacquesm 10 hours agoprevThat is extremely clever. The persistence alone is impressive but to get it to the point where it works well enough that you can actually extract the ROM contents with high fidelity is next level. reply Trekker666 11 hours agoprevI've done this using GoldWave opening game files to extract their audio because I like the menu song or something, but nothing this sophisticated and cool. reply DerCommodore 2 hours agoprevCrazy reply reactordev 14 hours agoprevIt’s stories like these that remind me I’m stupid and have a lot still to learn. My goodness this is amazing. Up there with audio key logging and things I read about but could never dream would be a real thing. I tip my hat sir. Reminded that I’m a mere mortal in the presence of greatness. The absurdity of recreating a game based on audio crashes sounds like something a mental patient would say but no, here we are in 2024. reply sunnybeetroot 3 hours agoparentYou’re not stupid because you don’t know how to reverse engineer a GBA game, unless you work for Nintendo and it’s part of your job description. reply Solvency 14 hours agoprev [2 more] [flagged] jacquesm 10 hours agoparent [–] I got it just fine, what exactly bothers you? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A modder has found a technique to develop functional Game Boy Advance games by capturing crash sounds from the original games and analyzing the audio waveforms.",
      "The modder faced challenges during the process but eventually managed to recreate the game data, resulting in a perfectly accurate ROM.",
      "The created ROM can be played on physical Game Boy Advance hardware, providing an exciting way to relive classic gaming experiences."
    ],
    "commentSummary": [
      "Users are discussing the idea of using audio to recover game state when Game Boy Advance (GBA) games crash.",
      "The conversation explores the architecture of the GBA and the possibility of the audio hardware accessing the game's ROM.",
      "Various topics are discussed, including clock recovery, coding systems, digital signal processing, GPS technology, median filters for noise removal in photos, the limitations of 8-bit resolution, and the use of emulators to detect errors in reconstructed GBA ROMs."
    ],
    "points": 484,
    "commentCount": 66,
    "retryCount": 0,
    "time": 1705945153
  },
  {
    "id": 39090879,
    "title": "Instagram Introduces \"Notes Prompts\" to Encourage User Engagement",
    "originLink": "https://www.neowin.net/news/meta-now-lets-eu-users-unlink-their-facebook-messenger-and-instagram-accounts/",
    "originBody": "Instagram Notes Prompts will let you poke your friends for a response 3 hours ago",
    "commentLink": "https://news.ycombinator.com/item?id=39090879",
    "commentBody": "Meta now lets EU users unlink their Facebook, Messenger and Instagram accounts (neowin.net)447 points by pg_1234 18 hours agohidepastfavorite224 comments cnity 17 hours agoThis must have been a painful engineering effort that I bet absolutely consumed the backlogs of at least tens of teams. Usually business assumptions about user accounts have their tendrils deep into the architecture and services that presumably many many years of code sits on top of. reply ffpip 17 hours agoparent> presumably many many years of code sits on top of. Not so many, since they tried to link all 3 only few years ago https://www.theverge.com/2019/3/6/18253472/mark-zuckerberg-f... reply LinAGKar 16 hours agorootparentThat may be the case for Instagram, which was developed separately and then acquired, and which still uses separate accounts that AFAIK can be created without a Facebook account, although they can be connected to one. Messenger though has always just been the chat function on Facebook, which can nowadays also be used via a dedicated website/app if you like. It doesn't even have separate account infrastructure on the backend, unlike Instagram. Looks like they're getting around that by essentially creating a second Facebook account, with social media features disabled. reply Perz1val 16 hours agorootparentThere has been an option to create messenger only accout (so you could do that for your grandma who would not have figured facebook anyways, but you can send her photos now) reply labster 3 hours agorootparentThat’s a ridiculous use case, only grandmas are on Facebook these days. reply wrboyce 16 hours agorootparentprevMany years ago when I disabled my Facebook account I noticed that Messenger continued to work, so there was definitely some level of separation. reply smashed 16 hours agorootparentBut can you really disable a Facebook account? reply TheRoque 16 hours agorootparentDisabling merely means just making yourself invisible, and the Facebook profile unusable, but all the data stays there. If you delete it though, your messenger account is no more. reply eastbound 13 hours agorootparentEven for EU citizen with GDPR? reply RHSman2 3 hours agorootparentAnonymize data with a restore capability. Maintains historical continuity for never ending total user awareness. reply Sharlin 12 hours agorootparentprevDisabling is what it says on the tin, giving you the chance to re-enable the account later. Nothing GDPR-violating with that. As long as you are also given an option to actually nuke all your PII. reply gmerc 6 hours agorootparentprevMessenger was moved into its own product for growth reasons (and app size reasons initially), it’s been separate for a long time reply wodenokoto 5 hours agorootparentprev> Messenger though has always just been the chat function on Facebook, which can nowadays also be used via a dedicated website/app if you like I would venture to say that messenger as a separate service/app has been a thing for at least a decade now. reply jarjoura 13 hours agoparentprevThe challenge isn't unlinking accounts in the database, that's just a simple copy command. The challenge is in all the metadata that is used for account A (Facebook), and account B (IG) that can no longer share any downstream metadata. So when you have to split the two apart, it's likely a hard legal requirement that there's no way account A can know anything about account B anymore. This means, countless metrics and logs and whatever has to be thrown away. So I imagine, there's lots of auditing of systems and every single property that is stored somewhere was scrutinized in long meetings. Less an engineering burden than a policy and legal burden. reply dmix 6 hours agorootparentI have a feeling a lot of HN devs overvalue the value of old data... probably because they have adblockers and dont know what ads they are targeted for. Youtube's algorithm seems to forget my interests after a year (or less), I often have totally new feeds than I did 6 months ago. When I got back into music production and also watched some airplane crash videos my entire feed became music + \"dark stories\" related content bringing non-stop music gear + engineering related education sites. Almost all ads I see on my phone via Google properties are related to the current thing I care about. Beyond biographic details which are 99% disconnected from my old individual instagram/fb data (FB can just buy some correlation between email/phone/IP from my credit card or private postage company or whatever) they probably only profile IRL off the last 1-2yrs of your browsing behaviour in most cases. The classic privacy invading ads usecase is a woman who is pregnant suddenly getting ads for kid stuff before other people they know they are pregnant. They probably won't be getting those ads a year later. At most they are now labelled as potential mother in some macro profile, but for day-to-day targetting it's all about the current thing she's looking at online. I'm not defending this stuff and macro details matter for privacy at the nation-state and higher level, but my general point is that it's a living machine, constantly adapting with a short timeframe of data to maximize utility, not one that cares much about what you googled a few yrs ago 99% of the time. reply RHSman2 3 hours agorootparentprevGoing through something the same, can tell you the people process and collaboration across teams (engineering, warehouse, legal and front end/backend) is the complexity as it takes time and then all downstream events are not synching in same paradigm. Really requires a huge effort to pull together the right people and get fast feedback loops to stop inertia. reply klabb3 10 hours agorootparentprevA burden that is proportional to the amount of personal data the corporation collects sounds excellent. reply closeparen 9 hours agorootparentThe compliance burden is proportional to the complexity of the architecture. You could have vast quantities of highly invasive personal data written to a monolithic RDBMS, and it would be simple matter of following foreign key relationships. Or you could have a very minimal set of entities & transactions actually collected from the customer via the edge, but propagated through so many microservices, queues, logs, warehouses, pipelines, and derived tables that it's infeasible to comprehensively trace all the scraps. The latter is more typical of a consumer internet company AFAICT. reply stavros 17 hours agoparentprev> Usually business assumptions about user accounts have their tendrils deep into the architecture If their business assumptions are \"users will never have a choice about how much we track them\", they deserve what they got. reply ethbr1 16 hours agorootparent\"They\" being Meta, yes. \"They\" being engineers, eh? It's a tough sell to justify greenfield dev effort when there's a BigCorp paved road laid out for you. reply avgcorrection 15 hours agorootparent> \"They\" being engineers, eh? If you’re a volunteer Roman legionnaire don’t be surprised when you are ordered to march on Gaul. reply ethbr1 15 hours agorootparentFair! I took the original question in the vein of \"Who is to blame?\" I feel like IC latitude isn't usually sufficient to say \"Do we really want to require the identity our platform is based around?\" reply ryandrake 15 hours agorootparentICs (at least the people good enough to pass a Meta interview) have a choice of where they work though. I've quit a job where I had an ethical issue with what the company was doing with the code I wrote. When Company X does something naughty, the individual developers need to at least share blame with the managers making the decisions. After all, whose fingers are typing the code in and hitting submit? reply someotherperson 14 hours agorootparentprevI haven't worked for a company as large as Meta but we encounter these issues on smaller projects too: should the unique identifier for an account be their email? Or a UUID? If your business only uses SSO, should you use the [Google/Microsoft/whatever] ID as the identifier for that user on your database? I personally wouldn't think that Facebook engineers would have had their hand forced on something like this. reply ethbr1 14 hours agorootparentOculus engineers apparently did. So I'd imagine the internal pressure over something closer to main products would be the same or worse. Centralizing identity was (prior to the EU) a core moat for Facebook/Meta. reply andy_ppp 16 hours agoparentprevAnd it’s good for consumers and Facebook can afford to do it, I wish the UK was still in the EU and I would be extremely happy about unlinking Instagram from Facebook just to avoid their notifications for it that pop up on Instagram never mind the ease of spying this creates. reply notahomosapien 15 hours agorootparentIsn't the UK GDPR basically the same as the EU GDPR? reply withinboredom 15 hours agorootparentDifferent sized teeth. reply davidwritesbugs 25 minutes agorootparentSame size teeth, different inclination to bite. reply XghTk 16 hours agoparentprevThose are all acquired services, so the opposite is likely: they were linked together through the number of ugly hacks. reply dewey 16 hours agorootparentThat was a very long time ago though, so I could imagine that there's not much left from the original code base after it was moved onto Meta's infrastructure. reply sitzkrieg 14 hours agorootparentyea sf companies are definitely known to cleanup their hacks to go to market at any cost w a barely working thing later reply dylan604 12 hours agorootparentisn't that done naturally when they rewrite it the trendy language of the day? reply chimeracoder 16 hours agorootparentprev> That was a very long time ago though, so I could imagine that there's not much left from the original code base after it was moved onto Meta's infrastructure. They only linked them a few years ago, in anticipation of an antitrust battle. They are now unliking them because they have lost/ceded that fight. reply EMIRELADERO 14 hours agorootparentCould you elaborate on that? reply amelius 15 hours agoparentprevAnd I bet management asked: what can be so difficult about decoupling the accounts? Just set coupling=false somewhere or something, okay? reply madsbuch 17 hours agoparentprevCould be that they have not been entirely consolidated yet as they all previously were individual products. From a user perspective is appears like the users have been relatively loosely coupled. reply mynameisvlad 17 hours agorootparentInstagram might have been a separate product, but Messenger was just Facebook’s built in chat feature. reply cnity 17 hours agorootparentprevTrue, in which case it might be even more painful: imagine ending a year long epic to consolidate user account logic only to have to reverse it. reply summerlight 14 hours agoparentprevAt least they have a choice to unlink accounts, which probably makes the overall DMA compliance much easier. For Google, it's just been an engineering nightmare, cross-cutting across the entire stack :D reply mcmcmc 16 hours agoparentprevIt’ll save them a good chunk of time when they eventually get split off like Baby Bells reply scyzoryk_xyz 14 hours agoparentprevSounds like a them problem not an us problem. Nothing they couldn’t afford „tens of teams”. It’s a nice moment living in the EU, knowing that common sense policies like GDPR and anti-trust can be enacted and enforced. reply rodgerd 13 hours agoparentprev> This must have been a painful engineering effort It would be less painful if it were a universal option, and they didn't have to test with a bunch of geo-linked options. reply j45 10 hours agoparentprevCould the limited EU accessibility to this engineering feat be broadened by users identifying as living out of Europe? reply waynesonfire 15 hours agoparentprevProbably not, if it was painful they would have done it everywhere. Since now they have to maintain two configurations, linked and unlinked. It also highlights how much revenue is tied to this bundling of service since it's probably very expensive to maintain this dual configuration. reply loceng 17 hours agoparentprevI guess part of karmic consequence of \"move fast and break things\" motto - especially when it comes to not caring to develop or have forethought on security-safety concerns for what such a system, a digital version of The Facebook, could have on society; Harvard was planning to launch a digital version of their print version of The Facebook - and part of Mark's history he's quoted as saying, along with lying to the ConnectU twins he was hired to develop, that he didn't understand why Harvard was taking so long to get it going - that he could do it way faster; the externalized costs/harms are practically incalculable - all for that sweet sweet cheap-shallow-manipulative advertising revenue. reply SOLAR_FIELDS 6 hours agoparentprevGiven that the people on these teams are reaping stock from the efforts of mining personal data for people over decades I have a very strong “not sorry” feeling about any sort of negative anything that employees of Meta might experience. I would even go so far as to say, fuck these people, I have no empathy for you. You decided to be a part of the problem, and sucks to suck reply AlexandrB 17 hours agoprevIt's constantly frustrating to read about European users getting cool new features to help manage their digital life that the rest of the world doesn't get. The expected cost in lost ad revenue per user must be pretty significant to justify the complexity of keeping this stuff limited to Europe. reply miohtama 16 hours agoparentThere are also downside for European regulations. There is no free lunch and some toll needs to be paid. Some examples include - Banning encrypted messaging (almost passed) - Cookie pop ups - Various regulations harming open source (discussed before on HN) Also due to how Europe is wired up, the cost of doing startup business is higher, why there are fewer and fewer successful European software growth companies. reply ko27 15 hours agorootparentYour examples are terrible, which probably proves the point that EU users are getting more than they are losing. Btw Europe != EU. > Banning encrypted messaging (almost passed) It's a positive thing that it was brought up and struck down. EU is the actually the one you should be thanking because most European countries would ban it. > Cookie pop ups Malicious compliance by websites, but at least users have a choice of opting out of tracking. Again, a positive thing. > Various regulations harming open source (discussed before on HN) The most recent changes are so watered down that it basically only applies to commercial open source companies that are turning a profit. It's helping users more than it's harming the open source community. reply whywhywhywhy 15 hours agorootparent> Malicious compliance by websites If the legislation had been written correctly then the current nightmare wouldn’t exist. Should have been a browser level setting the sites are forced to comply with. The pop-up per site with free rein how obtuse it can work was always gonna suck. Pure incompetence from the politicians involved. reply phatfish 10 hours agorootparentIt works fine now, many sites i visit have a \"Deny All\" button which was how it was supposed to work. Initially, (inevitably) private corporations found a way to subvert the spirit of the law to their benefit. Getting the legalise of legislation right first time is almost impossible when there is an army of lawyers, paid by corporations, whose job it is to unpick it. reply klabb3 10 hours agorootparent> It works fine now […] many sites i visit have a \"Deny All\" button That’s not fine. It’s anti-thetical to the very idea of the web. Accepting this shit as a compromise is exactly what we did with adblockers and that battle is still ongoing, and not solved. Whenever the greasy finger of corporations end up inside our browsers, whether it’s fingerprinting or meaningless consent screens, or tracking cookies/pixels/scripts, things are not dandy, imo. I don’t have a solution. Just saying. reply jocoda 2 hours agorootparent> \"... anti-thetical to to the very idea of the web\" I'm genuinely curious and not trying to pick a fight here. The core idea behind html when it was first conceived was that it allowed you as the viewer to present that info in any way that you chose. You can change the font, etc to suit your own preferences. This idea got killed by content providers who mostly want to hardwire their content layout because they know better. So where is this \"idea of the web\" coming from. Surely there's no longer a central core idea, it's just what each of us make of it, and as a result we're often in conflict about how things go. Did choice get formally taken away? Does the content provider get to choose how it's done now? reply palata 9 hours agorootparentprev> If the legislation had been written correctly then the current nightmare wouldn’t exist. Legislation is not a technical spec. It's made purposely to be interpreted. And companies are made to optimize their profit given the constraints set by the law. Sometimes companies have to bet that if they do X, they won't get a fine, and decide if they want to take that risk. Companies can even bet that the fine will be smaller than the profit, which is often the case. And that, IMHO, is the problem: we (I mean our governments) should be much, much more aggressive with the fines. BigTech can basically do anything they want because the fines are always ridiculous (because BigTech are too big, sure). reply imiric 14 hours agorootparentprevThis is always brought up when EU cookie regulations are discussed. If only the EU consulted HN readers... It's true, though. The technical language could've been written in a way that makes it more difficult for websites to circumvent, and less annoying for users. Or the regulation could've been amended to clarify and improve the technical aspects. That said, getting to a regulation at all was probably a bigger nightmare, with Big Tech lobbying against it every step of the way. So I'm glad that we even have the current GDPR, and that the EU is still leading the way in privacy regulations globally. reply rakoo 10 hours agorootparent> and less annoying for users. You seem to be inverting cause and consequence: it's the websites who are annoying to the users, not the law. The banner is optional, it only exists because websites want to collect your private data, not even to make the thing work. > Or the regulation could've been amended to clarify and improve the technical aspects. The regulation has been clarified to mean something important: refusal must be as easy, visible and doable as acceptance, so people can click \"refuse\" everywhere. Lack of acceptance mean refusal, so people can close the banner. reply imiric 9 hours agorootparent> You seem to be inverting cause and consequence: it's the websites who are annoying to the users, not the law. No, I mean that the law could've been written in a way that makes giving consent less cumbersome for users. I agree with GP: if it had been a browser setting that websites _must_ comply with, like the abused and now dead DoNotTrack header, then we wouldn't have ended up with annoying consent forms to begin with. After all, it does make sense for this to be a global user preference, rather than something the user needs to consent to on each site. Even without getting into technical details, this should be evident to anyone. I'm not aware of why this didn't happen, or why the DNT header was killed, but it wouldn't surprise me if the (ad)tech industry strongly lobbied against it, and won. The internet loves to criticize this oversight as incompetence from politicians, but politicians couldn't have elaborated the technical aspects of the law without IT consultants, and these surely understood what could be the implications. The fact they went with the consent form approach, and the fact this hasn't been rectified years later, is probably a sign that the tech industry still has considerable sway in regulatory matters. But to blame this situation on the law itself, or the EU, is just delusional. I'm still happy it exists, warts and all. reply rakoo 49 minutes agorootparentBut nothing prevents browsers from doing so ! In fact you can even configure your browser to never show those popups, and everything is fine. Everytime I switch people over to Firefox I install ublock origin and the list that blocks cookie popups: https://jasonmurray.org/posts/2020/cookies/ (there are even more settings to block even more popups today) Actually Google is seeing the wind turn and is slowly moving away from cookies, so it did even better than what you wanted: it will effectively kill (unnecessary) cookies as a whole. I have no issue believing lawmakers did in fact take advices from IT experts, seeing how they could make the difference between useful and unuseful cookies. But the law never goes into implementation details, that's another level of regulation, and the real effect is coming: the major browser will block third-party cookies. That will change everything. reply varispeed 14 hours agorootparentprevThe cookie regulation was designed to train people to \"Agree\" without reading. It was a prerequisite step for GDPR that was designed to legalise data collection and trading. Before GDPR it was a gray area, now companies can easily get consent as users mindlessly click \"Agree\" to data processing and selling and they have a legal basis to do so. These are corrupt laws, but most people blindly believe EU is good and totally not in bed with big corporations. reply akie 2 hours agorootparentYou have clearly not seen the amount of people who click \"deny all\" or \"only statistics\". Before the GDPR _everyone_ had to accept _everything_ a website sent their way and didn't have a say in it, after the GDPR only 33% of people click \"accept all\" on the cookie banner for the fairly large e-commerce site I work at. If the goal of the GDPR was to train people to click \"Agree\" and to legalise data collection, then that law was an abject failure. reply imiric 13 hours agorootparentprev> The cookie regulation was designed to train people to \"Agree\" without reading. That's a cynical take. In reality, companies took advantage of the loose technical language to do the least possible work to comply with the law, while doing their best to implement dark patterns to confuse the user into clicking \"Agree\". This is something that can be improved with stricter regulation, but it will always be a cat and mouse game. > It was a prerequisite step for GDPR that was designed to legalise data collection and trading. Another cynical, and also false, take. The GDPR wasn't \"designed\" for that. In fact, it actively tries to prevent it. An EU citizen can contact any company in the EU and demand to access all their personal data, or for it to be deleted. This is an unequivocal win for people to regain control over their personal information. Is this the best that governments can do? Certainly not. I'm still glad that at least something exists, and the tech industry is not entirely unregulated, as in most other parts of the world. > These are corrupt laws No. These laws are a step in the right direction. Unfortunately, the strong influence and rapid pace of development of the tech industry means that governments will always play catch up, even when they want to pass laws that protect their citizens. > most people blindly believe EU is good and totally not in bed with big corporations. Citation needed. Name me a government that is not in bed with Big . Big Tech in particular is in strong symbiosis with governments, as they both share some common goals. So, sure, there's that. And yet despite of it, the EU still passes laws that fight Big Tech's reach, and fines companies when they don't comply. Can it do better? Sure. But name me a government on Earth that does a better job at this than the EU. We don't need to get political here. But it's foolish to spew cynical takes when some governments are at least trying to fight Big Tech, and even more foolish to imply that their attempts are making things worse for its citizens. reply CaptainZapp 51 minutes agorootparentprevWell, if this scum, which is adware companies, would have respected the Do-Not-Track flag set in the browser this wouldn't have been necessary. Now, would it? reply troupo 11 hours agorootparentprev> If the legislation had been written correctly then the current nightmare wouldn’t exist. It was written correctly. Because it's a General Data Protection Regulation. It applies in equal measure to websites, apps, paper records, SaaS, shops, government entities etc. And it says: \"do not get more data than is required for your business. If you want more data, the user must give consent, where opting out is the default, and must be as easy as opting in\". Now, what exactly is badly written in the law? You can start with quoting exactly where it requires existing cookie popups. For example, GitHub found out they need exactly none: https://github.blog/2020-12-17-no-cookie-for-you/ > Should have been a browser level setting the sites are forced to comply with. It's called the Do Not Track header, and at one point Safari removed it because the companies you think are blameless used it to track users reply pronik 13 hours agorootparentprev>> Banning encrypted messaging (almost passed) > It's a positive thing that it was brought up and struck down. You see, the thing about european legislation is that certain stuff, especially stuff people oppose, is proposed over and over again until it passes. It costs almost nothing to re-propose things like killing net neutrality or banning end-to-end encryption, but it's very costly to oppose them. Which the politicians and lobbyists know and use to their advantage. reply Vinnl 11 hours agorootparentThat does not sound particularly specific to the EU to me? reply jonkoops 12 hours agorootparentprevWell at least people's voices are being heard, not something I can say for every country, federation, or union. reply varispeed 14 hours agorootparentprev> It's a positive thing that it was brought up No it's not. People should be fired for proposing such things as they breach human rights. It's like being happy that someone proposed genocide of all men over 60 to save on pensions and that the idea didn't pass. reply aqfamnzc 12 hours agorootparentPerhaps GP meant that the end result is a net good, since now it's in the books that it was positively, explicitly struck down? (Rather than being ambiguous or assumed, with no records etc.) Anyway, reading sibling comments it seems like it's not that simple either way. reply sofixa 15 hours agorootparentprev> Various regulations harming open source (discussed before on HN) But coming from a good idea - make vendors responsible for the software they put out, to prevent tons of abusive practices like shutting of cloud services making paperweights, or never updating massively holed software harder. The ramifications for open source were then realised, and the legislation which is still under planning/review has been drastically updated to make it more applicable for open source software. > Banning encrypted messaging (almost passed) But didn't? > Also due to how Europe is wired up, the cost of doing startup business is higher, why there are fewer and fewer successful European software growth companies. Which has little to do with regulations, much more to do with the size and wealth of the potential markets. > why there are fewer and fewer successful European software growth companies Is that bad though? Are software \"growth\" companies a requirement for something? There are tons of successful software companies in various European countries, just not at the level of their American counterparts. Again, with quote obvious reasoning - there are 4x the people in the US compared to France (which is top 2 by population in the EU), and Americans both earn and spend more in USD not adjusted for anything. reply akie 15 hours agorootparentprev> Also due to how Europe is wired up, the cost of doing startup business is higher, why there are fewer and fewer successful European software growth companies. I really believe the primary reason is that Europe is not one culturally homogeneous area that speaks one language - like the US. Having that is such a huge benefit. Doing marketing, promo, getting traction, legal documents, taxes, - anything - for your startup in your ONE country is already difficult. Now imagine doing it in 25 countries before you get to have scale benefits equal to the US or China. reply berkes 12 hours agorootparentExactly. We are building a platform that helps founders generate a sales strategy in the EU. We're half a year in and almost tackled \"how to write good emails to Dutch prospects\". Next up is Flemish, which has the same language, but a very different business culture. With luck, we have then captured a few percent of the EU tech market. Germany is so different, that we'll need to hire several experts for the different regions in Germany. France too. Italy and Spain, unattainable (we are told) without at least a local branch and solid local staff. That's still only a portion of the EU. \"Cookie banners\" are not the reason tech is hard in Europe. If you believe that, you really don't know anything about Europe or the EU. reply dataking 5 hours agorootparentprev> I really believe the primary reason is that Europe is not one culturally homogeneous area that speaks one language There are likely several reasons at play, culture being a big one. However, there are other reasons at play such as differences in law, financing, immigration, the job market. Bert Huber, an entrepreneur in the EU has given this some thought: https://berthub.eu/articles/posts/is-europe-just-not-good-at... reply airstrike 16 hours agorootparentprevCookie popups are a net positive because users are given a choice. Besides, in the US we often still get the popups but they're just useless, with the only option being \"accept\" reply alwayslikethis 16 hours agorootparentUnpopular opinion: the proper place to controll cookies is from the browser, not from the website. Browsers should show a prominent way to disable or otherwise restrict persistent storage to websites to inhibit tracking. reply ulucs 15 hours agorootparentDNT exists, and the cookie banners did not need to be regulated into existence if the websites did not strategically ignore the DNT header. reply whatshisface 15 hours agorootparentWhy not regulate the DNT header into expressing the user's cookie banner preferences? reply progval 15 hours agorootparentGDPR does not specify what technology to use to acquire consent [1], as long as the user consent. Trackers could honor the DNT header if they wanted to, and show the banner as a fallback for browsers not sending the header. [1] You can read the text: https://eur-lex.europa.eu/eli/reg/2016/679/oj there is a single instance of \"cookie\" (in the preamble) and no instance of \"banner\". reply Aloisius 14 hours agorootparent> Consent should be given by a clear affirmative act establishing a freely given, specific, informed and unambiguous indication of the data subject's agreement to the processing of personal data relating to him or her, such as by a written statement, including by electronic means, or an oral statement. This could include ticking a box when visiting an internet website, choosing technical settings for information society services or another statement or conduct which clearly indicates in this context the data subject's acceptance of the proposed processing of his or her personal data. Silence, pre-ticked boxes or inactivity should not therefore constitute consent. While DNT could potentially be used for opt-out, it wouldn't comply for opt-in because it is not specific or informed as the user does not know what specific data processing activities will be done, can't opt-in or out of specific data processing activities, doesn't know the identity of those doing them or that they can withdraw consent at any time. reply shortsunblack 12 hours agorootparentprevDNT specification does not allow for giving valid consent under GDPR because it is not granular and it is not informed. There's no browser dialogue that details the requested consent for which and what processing. There are proposed browser signal specifications that would meet legal GDPR consent defintions. See https://www.dataprotectioncontrol.org/spec/ And as for why DNT did not took off, it's because MSFT sabotaged it by making DNT set by default in Internet Explorer. The social contract in that time between adtech, publishers and users was that the signal would strictly be opt-in. The adtech industry used IE making DNT the default as justification for not honoring any of the signals being sent by browsers. It doesn't take a lot of reasoning to realize MSFT did this on purpose, knowing it itself earns income from ads. reply petre 15 hours agorootparentprevI've checked and sadly it's listed as deprecated on MDN. I don't know if there's anything to replace it. reply latexr 15 hours agorootparentBut it was deprecated precisely because websites either ignored it or used it as yet another signal to identify users (thus making it have the opposite effect to what was intended). reply xigoi 15 hours agorootparentprevThe problem is that you can’t tell if the site actually needs the cookies to work properly. And it does not help against other kinds of tracking. reply miohtama 11 hours agorootparentprevYour opinion should be more popular. Seems like even a lot of technically savvy readers on HN miss this. reply shortsunblack 12 hours agorootparentprevNeither GDPR nor ePrivacy directive demands cookie walls. ePrivacy directive demands consent. That consent can be given programmatically by browser APIs. There's even acknowledgement of such possibillity in the legal text (see point 7 in the directive). GDPR itself does not demand cookie banners, either. It merely demands there to be a legal basis for processing of data that constitutes personal data. One of those bases is consent. It's not the only basis. Other notable basis includes contractual necessity (includes all the cookies that are necessary for user experience, i.e sth like PHP placed session cookie). Browsers do not have automated means to give consent/not give consent under ePrivacy because the largest browser is ran by an ad company. Monetarily speaking, the ad company earns more if it coerces its users with dark patterns into giving consent under ePrivacy than it does offering pro-user choice technologies to give a blanket not consent. And ePrivacy itself is not just about cookies. EDPB recently released binding recommendations that severely expanded the perceived scope of ePrivacy (the true scope was always as it is, the adtech industry just ignored it). ePrivacy includes JavaScript side tracking, fingerprinting with various APIs and so on. It's not just cookies. reply weberer 11 hours agorootparentprevI use uMatrix for that. I have it set to block all cross-domain cookies by default. reply tgv 15 hours agorootparentprevTell that to the Chrome team. reply summerlight 14 hours agorootparenthttps://assets.publishing.service.gov.uk/media/62052c52e90e0... You probably want to tell that to CMA first. reply EasyMark 9 hours agorootparentprevI mean there are extensions like \"consent-o-matic\" that auto answer for you, but the law doesn't require any functions like that in a browser. I suppose it was a compromise between business interests and consumer advocates when they worked out the law for the EU? reply mderazon 12 hours agorootparentprevI don't know. The web is pretty unbearable here in the EU due to the cookies consent. Even more, many times I find myself wondering why a site is not responsive to my clicks just to find out there's some hidden cookie consent that didn't fire up properly and now I have to inspect the DOM to remove it manually. reply flohofwoe 16 hours agorootparentprevThe cookie banner isn't required if the web page isn't doing shady things with cookies though. reply poszlem 15 hours agorootparent\"shady things\" reply EasyMark 9 hours agorootparentprevI've always been curious if you offer a service for profit, but don't want to adhere to EU laws (obviously just avoiding EU customers at all). Is it enough to block EU country IP addresses so that Interpol doesn't get the Feds to kick in your door and turn you over to them for prosecution or freeze all your bank accounts for not sticking to the many Internet laws that you might not be familiar with? Is anyone aware of how to deal with fire walling off countries where you don't want to deal with the legalese? reply TheCoreh 14 hours agorootparentprevWhat's frustrating is that we get the annoying things like the cookie popups everywhere but the beneficial stuff is somehow properly region locked to inside the EU? reply tester756 12 hours agorootparentprev>- Cookie pop ups Websites do not have to show cookie pop ups if they are using only technical cookies like auth tokens. reply mderazon 12 hours agorootparentprevAlso lately, getting the newest LLMs features much later than the rest of the world, or not getting them at all reply camillomiller 2 hours agorootparentprevOr, there are fewer overinflated unicorns that produce no societal value at all and exist solely as a marketing-fueled VC bet that will make a limited amount of people ultrarich relatively fast while the rest of us shoulders the social, economic, and financial burden of their efforts. Potato, potato. reply gherkinnn 16 hours agorootparentprevObtrusive cookie banners are somewhere between malicious compliance or a sign of shady business practices. Now that I see it spelt out like this, they are always a sign of scumbag companies. reply shp0ngle 15 hours agorootparentThey are present even literally on the pages of the EU organs that proposed them. Go to European Commision page - bam, cookie banner. reply troupo 11 hours agorootparentCompare a clear \"allow cookies/allow only essential\" to the industry standard wall of \"we care about your privacy, so we sell your data to thousands of trackers that you have to opt out of manually\" Though yes, government services shouldn't use anything but essential cookies (for which you don't need a cookie popup) reply shp0ngle 10 hours agorootparentThat's switching goalposts. You said only scammy businesses have cookie banners... no, all websites have cookie banners now in EU, and it's majorly annoying, unless you use extensions that click it for you. (The most popular one is owned by Avast. Which is a horrible company that sells users data. So... yay?) reply troupo 3 hours agorootparent> You said only scammy businesses have cookie banners I didn't > all websites have cookie banners now in EU All the sites that collect more data than strictly necessary, yes. Here's an example of a website that spent need a cookie banner: https://github.blog/2020-12-17-no-cookie-for-you/ > The most popular one is owned by Avast. Which is a horrible company that sells users data Indeed. This should tell you all you need to know about these cookie popups and companies that use Avast's, or IAB's or Admiral's cookie popups. reply troupo 11 hours agorootparentprev> fewer and fewer successful European software growth companies. It's funny how you slipped in \"growth companies\" in there. How about... profitable companies? In the past 10-15 years most \"successful\" US companies have been fueled by unlimited investor money with zero expectations of profits. I mean, look at YC's \"top startups list\". They lose billions of dollars every year. But sure, they grow. Like cancer reply cnity 17 hours agoparentprevFWIW, we also get non-stop cookie permission banners and often just straight-up denied access to certain services that don't want to have to jump through the hoops. reply patrickmcnamara 17 hours agorootparentThe only things I've been denied as an EU citizen on the web so far have been US local news websites. And tbh I assume that is just one company that has a blanken policy of sorts. So I don't think \"often\" is accurate here. reply whatshisface 15 hours agorootparentThat's just the secret program to get Europeans to stop making fun of us. reply blackbeans 17 hours agorootparentprevIf you don't want to deal with cookie banners, there are browser extensions you can install to automatically accept them. However, although the cookie banners are sometimes a nuisance, it is still a good thing that people are informed and given the option to accept it or not. reply stavros 17 hours agorootparentI agree in general, but when I see a dialog box titled \"We respect your privacy\" and a choice between \"allow all\" or \"see more\", I donate 0.10 € to NOYB. reply latexr 14 hours agorootparent> when I see a dialog box titled \"We respect your privacy\" and a choice between \"allow all\" or \"see more\" It’s usually “we value your privacy”, which always makes me think “you put a value on it, alright, can’t wait to take it away”. reply vdaea 17 hours agorootparentprevIf anything, every time I have to click through one of those banners I wish that activist organisations/politicians had to pay me for having to go through it. reply Yujf 16 hours agorootparentIt is not the activists fault that the company wants to track you. They do not have to show you that stuff if they don't so unnecessary tracking reply stavros 16 hours agorootparentprevYou can get an extension that will automatically click \"allow all\" and everything will be exactly as it was before this directive. reply Filligree 16 hours agorootparentI want one that automatically rejects all. reply stavros 15 hours agorootparenthttps://addons.mozilla.org/en-US/firefox/addon/istilldontcar... reply latexr 14 hours agorootparentEmphasis mine: > In most cases, the add-on just blocks or hides cookie related pop-ups. When it's needed for the website to work properly, it will automatically accept the cookie policy for you (sometimes it will accept all and sometimes only necessary cookie categories, depending on what's easier to do). reply alwayslikethis 16 hours agorootparentprevYou can disable cookies in browsers so that cookies do not persist after session close. Firefox lets you add a whitelist of sites to allow cookies, too. reply probably_wrong 15 hours agorootparentIf I understand the law correctly, your suggestion is good on practice but could be harmful on the long run. When I click \"I agree\" I am not agreeing to cookies but rather to tracking. If the website wants to track me with (say) browser fingerprinting, deleting the cookies will not stop them from tracking me across sessions. Even worse: since I agreed they no longer need to show me a warning, so I may not even notice that I may want to revoke that consent. If I don't want to be tracked, saying \"I do not consent\" is the only legally-actionable way. Deleting cookies works now because cookies are still cost-effective, but this won't remain true forever. reply vdaea 16 hours agorootparentprevCookie banners will still appear. reply vdaea 16 hours agorootparentprevI wish that was true, but those extensions are far from flawless. Still, they are a massive improvement over having to click away all banners, especially for those of us who have our browsers set to clear cookies when quitting the browser. reply switch007 17 hours agorootparentprevThose extensions are far from perfect in my experience (ditto the lists in uBlock Origin) Often leave you wondering why you can’t click anything on the page reply pgeorgi 15 hours agorootparentprevhttps://consentomatic.au.dk/ also allows to reject them automatically. reply latexr 14 hours agorootparentprevIf anything, there should be extensions to automatically reject them. It’s a testament to how draconian the process to do it usually is that the extensions just surrender. Every time an extension or person accepts those, they’re reinforcing those companies’ choice to break the law¹ in the name or profit. ¹ The GDPR is clear that rejecting must be as simple as accepting. reply zht 17 hours agorootparentprevthat's propagated to the rest of the world lol reply coldpie 16 hours agoparentprevWe could have these regulations in the US, too, if we voted for them. reply BobaFloutist 16 hours agorootparentOk, how about this: \"It's a bummer that the rest of the voting population doesn't agree with my priorities enough to enact similar privacy regulation to Europe.\" reply segasaturn 16 hours agorootparentprevBoth parties in the US are in the tank with big tech (see: NY Dems killing right to repair) reply int_19h 14 hours agorootparentprevWe don't vote for regulations, we vote for politicians. Who are a package deal, so in practice you're voting on several dozen different issues at once, and you inevitably have to prioritize. I don't think you'll find significant opposition to many of these laws in US, it's just that it's never going to be that one thing that drives people to vote for or against some candidate or party. reply JumpCrisscross 16 hours agorootparentprevOr just called our electeds about it. I’ve worked on privacy issues. Past tense, because after a massive effort like two people call and the bill is dropped. (Except in Oregon. Oregonians apparently call and write about privacy.) reply Levitz 16 hours agorootparentprevMy experience is that stuff like this gets framed as \"pro consumer\" in Europe and \"anti corporation\" in the US. The difference runs deep. reply carlosjobim 16 hours agorootparentprevEuropean citizens didn't vote for those regulations. reply makeitdouble 14 hours agorootparentTo a larger point, european citizens are largely favorable to those regulations, which makes passing these rules possible, and not a political suicide. HN might not be representative, but the number of comments we see defending companies's right to make as much money as they can and against regulation that would add costs to businesses is IMHO pretty high. If that's the general US sentiment, politicians the have little upside in putting burdens on tech companies in the first place. reply carlosjobim 13 hours agorootparentEuropean citizens are largely ignorant of EU regulations, EU politics and EU spending. Most European citizens do not know the name of the EU president. But Europeans will categorically support and agree with EU policy and laws. As for cookie laws, can we honestly say that they have done anything to protect people from corporate spying and abuse of private information? Real regulation would be to outlaw that kind of spying, not putting up annoying banners so that some web developer can feel good about themselves. reply makeitdouble 8 hours agorootparent> European citizens are largely ignorant of EU regulations, EU politics and EU spending. On spendings, probably. On regulations, oh boy do people care. Regulations have pretty direct impact on each countries' economy, and they get discussed nationally before being applied. That means meat prices rises when some practices get banned or a whole sector suddenly getting competitive when barriers to entry are lowered. For better or worse, people care a lot, and the negative sentiments are enough to fuel may extreme side politics. reply Shacklz 10 hours agorootparentprev> Most European citizens do not know the name of the EU president. Probably a bit controversial but to some degree, that is a good thing. It prevents personality cult and allows to pursue agendas that think in longer terms. In Switzerland, quite a big part of the population could probably not name the current seven members of the federal council - they can actually get work done, without having to appease the public at every corner. reply yau8edq12i 13 hours agorootparentprev> European citizens are largely ignorant of EU regulations, EU politics and EU spending. Most European citizens do not know the name of the EU president. Very bold claim. Do you have any sources to back it up? EU politics are in the news all the time here. reply carlosjobim 12 hours agorootparentGo ask a European. Who are the three EU presidents? Who were the EU presidents before them? Then ask them who is the current US president? And which presidents were before him? Which are the two top candidates in the 2024 election? reply troupo 11 hours agorootparentBecause the person doesn't matter. Policies matter. And Europeans largely prefer privacy-protecting regulations reply drooopy 16 hours agorootparentprevEuropean Citizens voted for the people who voted for those regulations. reply tgv 15 hours agorootparentAnd we have input via our governments, and the national parliaments have a say in the procedures: a group of 1/3 of all national parliaments can send proposals back. reply carlosjobim 15 hours agorootparentprevYes, and so do American citizens. Whether you agree or disagree with the regulations, they were never voted on by the people. reply makeitdouble 14 hours agoparentprevA part of this is the products being fitted to US market and only after they get traction do they hit the other countries. That means any of the wider digital management, privacy etc. are literally after thoughts, and the business model also doesn't properly fit. With that approach I think we'll always get products that are optimized for free + ads first and foremost, as the US public reacts better to those, and once it's setup it's just so hard to pivot to paying models. reply greggsy 16 hours agoparentprevI wonder if I could just change my location in settings? reply ProAm 9 hours agorootparentMost apps like this us GPS to verify as well. A lot of gambling apps require that permission so you can't just VPN to legality. reply ProAm 9 hours agoparentprevIf you are in the US write your local congressman. Its the only way things like this change with companies that are too big and powerful. Takes a long time to turn a big ship. reply wizzwizz4 17 hours agoparentprevIt's just politics. Ask your legislators to give you the cool stuff as well. reply Hamuko 17 hours agoparentprevEh, can't be that excited about anything involving the data hole that is Meta Platforms products, even if they make some EU concessions. Maybe when we get that EU-only iOS sideloading support I can finally be more smug about it. reply sofixa 15 hours agoparentprevThanks to the Brussels effect[1] some of it can trickle down. https://en.wikipedia.org/wiki/Brussels_effect reply josephcsible 16 hours agoprevIt annoys me every time a company makes a long-requested change, but only for EU users. Since the technical work required for it had to be done anyway, it seems to be pure malice that they don't just give it to everyone. reply mrtksn 8 hours agoparentIf you think about it, the US seems to be voluntarily headed to universal plug for charging EVs after many years of incompatibility. EU got that almost immediately. Though, it can be argued that the US plug is a bit better since the standart that almost all companies finally agreed to adopt is the Tesla one. However, it could have been the other way around too - the companies could have adopted a very shitty plug if Tesla had a shitty plug simply because their adoption is based on the charging network dominance and not on the plug superiority. So, I guess its not a clear cut. reply judge2020 16 hours agoparentprev> it seems to be pure malice that they don't just give it to everyone. Only if you consider malice ~= profit. reply guerrilla 12 hours agorootparentWhy would you not? What is malice if not intentionally harming people out of greed? Is it only malice if the intention is explicitly and strictly sadistic? reply LightBug1 10 hours agorootparentSome woeful 'giving the benefit of the doubt' in some of these comments. No disrespect. reply baliex 16 hours agoparentprevNot just malice, profit too reply micromacrofoot 16 hours agoparentprevisn't this the \"free market\" that many in the US brag about? reply int_19h 14 hours agorootparentNot quite. It would be a free market if people could freely move across borders. The overall arrangement that we have - where goods and services flow almost freely, but the movement of consumers and labor is restricted - is inherently abusive. reply altairTF 16 hours agorootparentprevKinda, but this is also how the free market dies—happens little by little. A change here, a law there that helps the people, oops, now the law makes it harder for competition to get in, and so on and so forth. reply pgeorgi 15 hours agorootparentThe business models this regulation harms are the business models that aren't desired to begin with. As such, a competitor is encouraged by regulation to find another, compliant angle to deal with monetization. And competitors that already do that get an uplift (over time, but still). reply DaiPlusPlus 15 hours agorootparentprevBtu a free market (which is devoid of anti-competition regulation) also makes it harder for competition to get-in. Not to mention the problems of regulatory-capture (see: Boeing and the FAA). It's almost as if the best (or rather: least-worst) option we have lies somewhere in-between laissez faire capitalism and mandatory state-ownership of all industry... reply altairTF 13 hours agorootparentI don't think even the most well-intentioned state have the incentives to properly allocate resources in the market. Usually, when state-owned industries fail, the taxpayer suffers, so the administration has little to no incentive to actually do good in the long run. reply int_19h 14 hours agorootparentprevIt's not a 1D spectrum. You can have laissez-faire arrangements that still preclude massive concentration of capital (and thus effectively gut capitalism) without going all in on public ownership of everything. The key thing to remember is that private property rights themselves require some kind of government to maintain by using force when necessary - i.e. the existence of private property is inherently a step away from true \"laissez-faire\". Thus the government doesn't need to aggressively collectivize anything to avoid concentration of capital; it just needs to refuse to protect such concentrated capital. reply csallen 14 hours agorootparentprev> somewhere in-between laissez faire capitalism and mandatory state-ownership of all industry > Yes, which describes both the US and Europe. It just comes down to where to fall on that spectrum. reply black_puppydog 16 hours agoprevI'll hold out on whatsapp until I can unlink \"send this text to that number\" from \"here's a copy of my entire phone book\" reply DaiPlusPlus 15 hours agoparent> \"here's a copy of my entire phone book\" I don't know how or why that was ever legal in the first place, and why Facebook and LinkedIn haven't been sued to death over it. The contacts-matching system is how Facebook and LinkedIn'ss ultra-creepy \"People you might know\" list works, and I have no-doubt this will have led to things like journalists' confidential sources being discovered and harmful outing of closeted people, and worse. reply whywhywhywhy 15 hours agorootparentI’ve never uploaded my contacts to FB but it still recommends people I worked with or met 20 years ago because they uploaded theirs. Extra evil because you can be triangulated from the uploads of many naive people, so essentially they win in the end. reply aqfamnzc 12 hours agorootparentSame situation with DNA. I may avoid getting an online genome test, but once a couple cousins send theirs in I'm basically a part of the network at that point reply Timshel 15 hours agoparentprevNot to encourage you to use whatsapp but on android an empty work profile is nice for this use case. reply nolist_policy 14 hours agorootparentI just use OpenContacts from F-Droid and leave my phone contacts empty. OpenContacts has a extra button to open a contact in Whatsapp. reply yepguy 6 hours agorootparentGrapheneOS has a feature called contact scopes[1] that lets you pick which contacts apps have access to. It's great! [1]: https://grapheneos.org/usage#contact-scopes reply ryandrake 15 hours agoprevThis is great, now do Oculus. I shouldn't need to have an \"account\" to use my O.G. Rift offline. It's like needing an account to use a monitor. reply str3wer 3 hours agoparenthttps://github.com/basti564/Oculess reply colordrops 13 hours agoparentprevI tried using John Carmack's instructions to unlock my old Oculus Go, which involved your Facebook account for some reason, and got banned from my account twice. reply Handprint4469 14 hours agoparentprev> It's like needing an account to use a monitor Given the current enshittification trends, I would not be surprised if in the next few years Samsung, Acer, LG, et al decide that a subscription model for monitors is exactly what they need to pump those share prices up. reply wepple 12 hours agorootparentIIRC I required a Samsung account to use my TV. Won’t buy anything from them again. reply vodkapump 2 hours agoprev> Users can continue using Messenger with their Facebook account or create a new account completely independent of Facebook So you can't unlink your facebook and messenger, just create a new messenger account without a facebook account. That's sad. reply BiteCode_dev 16 hours agoprevSame with google with youtube. Good. People keep bashing EU for making innovation hard, and there are a lot of truth to it. But I welcome this kind of restrictions. reply sofixa 15 hours agoparentThe EU doesn't make innovation hard, it just puts rules in place to ensure that your \"innovation\" cannot be abusing users' data. Nothing is stopping any of the thousands of EU companies innovating in all sorts of fields. reply dataking 5 hours agorootparentWhile I agree that there is lots of innovation and talent in the EU, I think there is also an \"innovation gap\" with other developed economies. Here's a take from the Economist [0] but you can find similar articles from the Financial Times, Blomberg, and other similar outlets. [0] https://www.economist.com/europe/2022/02/26/europe-is-the-fr... reply danielscrubs 3 hours agorootparentIt is a hit piece here for example they compare a whole group to a subset: “western Europeans were a quarter poorer than Americans in 1990, and remain a quarter poorer today. ” And when they say they do less research Id argue the research is done in universities for the benefit of mankind instead of something corporations do. Latest example is Stable Diffusion developed by the CompVis group at LMU Munich. But if you ask the common man he will say it is something by OpenAI. There are really no good institutions left, only good people. reply okanat 9 hours agoparentprevIf it is easy, it is not innovation. You're not finding and improving a thing then. True innovation requires novelty and effort. Even smallest innovative projects require many many thoughts and redesigns until they become even an MVP. The US tech giants were coasting and amassing compound wealth without by innovating but buying the innovative companies or disregarding a basic human right: privacy. True innovation is and should be difficult and it shouldn't come at the cost of human rights that people has fought over for centuries, sometimes with their own blood. reply smoldesu 12 hours agoparentprev> People keep bashing EU for making innovation hard, and there are a lot of truth to it. If you consider the GDPR or DMA limitations on \"innovation\", then I don't want you innovating. reply Euphorbium 16 hours agoprevGoogle also let me unlink youtube and other services. Probably forced by EU. reply bandergirl 12 hours agoparentSource? What do you mean by “unlinking”? There is no YouTube login, are you saying it will spin off a new gmail/google account? reply copirate 9 hours agorootparentYouTube also asked me if I wanted to unlink my YouTube account from other Google services. I can change this setting at https://myactivity.google.com/linked-services and it says: > Google currently shares data across its services for the purposes described in its Privacy Policy at g.co/privacypolicy and depending on the previous choices you’ve made about your privacy settings, such as Web & App Activity, YouTube History, and Personalized ads > As of March 6, 2024, new laws in Europe will require Google to get your consent to link certain services if you want them to continue to share data with each other and other Google services as they do today. For example, linked Google services might work together to help personalize your content and ads, depending on your settings. The whole text is here: https://pastebin.com/iSmeJ5WB reply xyst 16 hours agoprevWhat about “threads” accounts? I believe these are directly tied to IG accounts. reply gevz 16 hours agoprevIs there a criteria to be qualified as a EU user? I assume there must be more than just geo-ip location. What will happen if I pretend I moved to EU for the sake of decoupling all Facebook accounts. reply reincoder 13 hours agoparentI work for IPinfo. I often see IP geolocation data being used for compliance purposes, so I will share some insights. This question relates to algorithms and the definition Facebook has set up for \"EU User\". What parameters have Facebook's legal team considered adequate to label an account belonging to an EU user? It is difficult to say. They will probably look into multiple factors like phone number location and GPS-backed data to identify the true location of an EU user. Now, the interesting part is that if you use a VPN quite frequently, you will notice that IP geolocation data is treated as a universal truth on cookieless interactions. Facebook will likely have a significant amount of data beyond IP geolocation data to infer the location information of their user. But they are likely not going to do that as they launch this new feature. So, it all comes down to how seriously Facebook wants or is required to identify an EU user. reply rconti 8 hours agoparentprevSame question. EU IP address? For how long? EU citizenship? But EU citizens shouldn't have to send their passport info to Facebook in order to unlink accounts. reply calmoo 16 hours agoparentprevThese days most accounts are geolocated by your phone number, in my experience anyway. reply gevz 15 hours agorootparentIt seems that Facebook proper does not require you to have a phone number. I just tried signing up and there is an option to use email. reply pfdietz 16 hours agoprevI already unlinked myself from all those. reply georgeecollins 16 hours agoparentMe too, but unfortunately the Quest 2 and Quest 3 are really good if you like to use VR. So now somewhere I have a Facebook account that does nothing except report on what VR games I am playing. Hopefully no one is friending me! If so, sorry for the spam of useless information. reply MikusR 16 hours agorootparentQuest stopped needing a Facebook account more than a year ago. reply bonton89 15 hours agorootparentNow it just needs a Meta account, totally different thing. reply MikusR 2 hours agorootparentThis, but unironically. reply plussed_reader 16 hours agoparentprevHow? reply dewey 16 hours agorootparentProbably by not using them in the first place. reply pfdietz 16 hours agorootparentBingo! reply moffkalast 14 hours agorootparentGalaxy brain move. reply xyst 16 hours agorootparentprevNot using them, or deleting accounts reply plussed_reader 14 hours agorootparentNeither of those are unhooking the services from each other. reply pfdietz 13 hours agorootparent> I already unlinked myself from all those. reply anticensor 16 hours agoprevThis unlink would just be a formality, as they would still be able to correlate those users anyway. reply pgeorgi 15 hours agoparentThat's a great way to route 4% of their global earnings into the EU's bank account. reply its_ethan 10 hours agoprevIf I used a VPN to place myself in Europe, could I unlink my accounts? reply jscheel 9 hours agoprevTheir ham-fisted linking of facebook and instagram accounts has been an absolute nightmare for those of us that use their messenger apis. It has never fully worked correctly, and has led to more lost dev time than I care to count. I can't even begin to imagine how much worse this is going to make it. reply Zuiii 8 hours agoprevSo how does a non-eu user get this privilege? Is using VPN in EU enough? reply boppo1 5 hours agoprevIf I reside in the US but am a dual US-EU citizen, does this apply to me? reply Havoc 9 hours agoprevGuessing UK isn’t getting this. Hashtag brexit benefit sigh reply gerash 11 hours agoprevcan someone ELI5 what unlinking a \"Facebook account\" from a \"Messenger account\" accomplishes? IMHO we should strive to remove moats around digital services so new competitor can pop up and provide competing services in a short amount of time but somehow EU thinks creating busy-work for the engineers of incumbents is the right approach. I already despise the \"accept cookies\" noise on all websites and don't tell me \"Oh, it's bad compliance, blah blah\". If so, then write a new more useful law instead. reply superjan 17 hours agoprevIf that’s what it takes to get them compliant with EU’s GDPR, shouldn’t the same apply to youtube and gmail? Or the rest of google services, for that matter? reply SushiHippie 16 hours agoparentI got a popup today after opening google play, which let me decide if I want these services linked together or not. reply SushiHippie 16 hours agorootparentThis is the information they provided: About linked services Google currently shares data across its services for the purposes described in its Privacy Policy at g.co/privacypolicy, and depending on the previous choices that you've made about your privacy settings, such as Web and App Activity, YouTube History and personalised ads. As of 6 March 2024, new laws in Europe will require Google to get your consent to link certain services if you want them to continue to share data with each other and other Google services as they do today. For example, linked Google services might work together to help personalise your content and ads, depending on your settings. Services you can choose to link You can choose which services to link from this list of Google services: Search YouTube Google Play Ad services Chrome Google Shopping Google Maps All other Google services that are not listed here are always linked and therefore able to share data with each other for the purposes described in our Privacy Policy, depending on your privacy settings What data is used Personal data that is collected about your interaction with Google services can be shared across any linked services. This includes: Searches Videos you watch on YouTube Apps you install from Google Play Associated info, such as your device info All the other types of info described in our Privacy Policy How Google uses this data Google uses data shared across linked services for all the purposes that are set out in our privacy policy: Providing personalised services, including content and ads, depending on your privacy settings Maintaining and improving our services Developing new services Understanding how people use our services to ensure and improve the performance of our services Performing other purposes described in our privacy policy What won’t be affected The choices you make about linked services will not change: The purposes for which Google uses data Your privacy settings, such as choices you have made about personalising Google services Aspects of a service that don’t involve sharing data The following can always be shared across Google services: Info that’s associated with your Google Account and not specific to any service, such as your profile picture or the info used to verify your identity Content or info you’ve chosen to make publicly available on Google services, such as comments you’ve posted on YouTube Your data from all Google services, regardless of whether they are linked, may still be shared across all services for certain purposes, like preventing fraud, protecting against spam and abuse, and complying with the law. Your data may also be shared across Google services to effectively help you complete tasks when two services are offered together. For example, if you make a purchase on Google Play, Play and Google Payments will share related info so that you can complete your purchase. Things you should know Other settings let you control whether you see personalised content or ads. Linking Google services is not about sharing your data with third-party services. If services aren’t linked, some features that involve sharing data across Google services will be limited. For example, if you have personalisation on for a service in your privacy settings but don’t link that service, you won’t get personalisation based on data from other services, but can still get personalisation based on data from that service. You won’t be signed out of any Google services if you choose not to link services. How to manage linked services You can manage your choices, including selecting other Google services to keep linked or withdrawing your consent, in your Google Account at myaccount.google.com/linked-services. You can also manage your other settings, like Web & App Activity, YouTube History and personalised ads, in your Google Account. reply Sjonny 16 hours agoparentprevI recently got a popup in youtube to unlink the accounts, so already on their way. reply flohofwoe 16 hours agorootparentYep, just wanted to write that exact same reply. It was one or two weeks ago and asked for permission to unlink YouTube, GMail, etc... from each other (as I understood it, when \"opting out\", login with the same Google account will still work, but they're not allowed to share data for ad profiling, recommendations etc...) reply 0cf8612b2e1e 14 hours agorootparentprevDoes this mean a potential Google YouTube ban would be firewalled from a Gmail ban? reply nolist_policy 13 hours agorootparentNope: > Your data from all Google services, regardless of whether they are linked, may still be shared across all services for certain purposes, like preventing fraud, protecting against spam and abuse, and complying with the law. reply thegrizzlyking 8 hours agoprevThis should be the default for all mergers above a certain threshold number of users and part of FTC consent decree. reply exabrial 13 hours agoprevWhat would have been better is if the three things were just split up into separate businesses. Monopolies are annoying. But I'll take progress over perfection, glad to see the move. reply 1oooqooq 12 hours agoprevbecause it's all linked in WhatsApp, which is the only thing anybody still actively uses anyway reply barnabyjones 7 hours agoprevShoutout to Max Schrems and Noyb, even if they weren't apparently involved in this decision. That organization seems to be the main driving force behind serious GDPR enforcement by submitting regular legal complaints, the data authorities occasionally take up a case on their own but usually seem content to stick to small violations. It really shows that having laws on the books isn't enough, and even public awareness is not very helpful, what really matters is having lawyers pushing hard for enforcement. reply pwdisswordfishc 14 hours agoprevI’ll believe it when each service is operated by an independent company. reply diebeforei485 12 hours agoprevCan they do YouTube next? reply ozyschmozy 11 hours agoparentThey already did, the news of Google doing the same on hn last week iirc. reply CatWChainsaw 13 hours agoprev [–] My facebook sits abandoned for the better part of a decade and I never got messenger or instagram. Since they're all under the same umbrella I question the actual benefit. Seems like it would be placebo more than anything. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Instagram has launched a new feature called \"Notes Prompts\" that enables users to send reminders or prompts to their friends.",
      "This feature aims to encourage friends to respond to messages or posts.",
      "Users can utilize \"Notes Prompts\" to ensure that their messages or posts receive attention and engagement from their friends."
    ],
    "commentSummary": [
      "European Union (EU) regulations are a hot topic, especially surrounding data privacy and technology companies.",
      "Users in the EU have the ability to unlink their Facebook, Messenger, and Instagram accounts, which raises challenges and implications.",
      "GDPR regulations have had an impact on user privacy, but there are limitations and criticisms of EU regulations.",
      "Cookie pop-ups and the potential influence of Big Tech on legislation are also discussed.",
      "Starting and growing software businesses in Europe can be difficult.",
      "Differing opinions exist on the effectiveness and implementation of regulations, as well as the role of politicians and citizens in shaping them.",
      "The discussions also touch on Google's services, the interconnectedness of platforms, and potential bans on YouTube or Gmail."
    ],
    "points": 447,
    "commentCount": 224,
    "retryCount": 0,
    "time": 1705939200
  },
  {
    "id": 39099065,
    "title": "Hacker News Now Supports IPv6 Connection",
    "originLink": "https://news.ycombinator.com/item?id=39099065",
    "originBody": "My browser just started connecting to Hacker News over IPv6 today: $ host news.ycombinator.com news.ycombinator.com has address 209.216.230.207 news.ycombinator.com has IPv6 address 2606:7100:1:67::26Both IP addresses share the same ASN (M5 Computer Security): https:&#x2F;&#x2F;bgp.he.net&#x2F;ip&#x2F;209.216.230.207 https:&#x2F;&#x2F;bgp.he.net&#x2F;ip&#x2F;2606:7100:1:67::26The raw IP address correctly redirects to HN: http:&#x2F;&#x2F;[2606:7100:1:67::26]Good job so far! Please don&#x27;t pull a Reddit and turn it off again.",
    "commentLink": "https://news.ycombinator.com/item?id=39099065",
    "commentBody": "Hacker News now supports IPv6362 points by p1mrx 7 hours agohidepastfavorite165 comments My browser just started connecting to Hacker News over IPv6 today: $ host news.ycombinator.com news.ycombinator.com has address 209.216.230.207 news.ycombinator.com has IPv6 address 2606:7100:1:67::26 Both IP addresses share the same ASN (M5 Computer Security): https://bgp.he.net/ip/209.216.230.207 https://bgp.he.net/ip/2606:7100:1:67::26 The raw IP address correctly redirects to HN: http://[2606:7100:1:67::26] Good job so far! Please don't pull a Reddit and turn it off again. kuon 3 minutes agoI am so surprised by the hate for IPv6 in this thread. I have been deploying IPv6 for more than ten years and it really improve many situations (beside larger addresses). I have to admit, there is a learning curve. But I want to encourage everybody involved in configuring computers to learn. Also I want to rent about Cisco not using /64 for link local by default, thus being incompatible with BSD systems. Link local must be /64. reply baby_souffle 6 hours agoprevHa! Beat me to it! For anybody that's curious, the IP-Foo [0] browser extension puts a little 4/6 icon in the address bar to make it clear at a glance which dialect you're speaking for $currentWebPage [0]: https://github.com/pmarks-net/ipvfoo reply notatoad 5 hours agoparentcool, but \"read and change your data on all websites\" is imho not worth the functionality. that seems ripe for takeover by some scammer. reply p1mrx 4 hours agorootparentIPvFoo author here. The problem is that there's no way to obtain the (hostname, ip) stream from Chrome/Firefox without requesting the \"all websites\" permission. In theory, browser vendors could define a narrowly-scoped permission that only reports (hostname, ip), or roll this functionality into the browser UI, but neither seems likely to happen. I made IPvFoo to promote IPv6 adoption, and wouldn't consider selling it for less than $10M USD. It probably won't ever be worth that much because it's an easily-cloned utility without a \"moat\", but it's more rational to set a price than refuse to sell under any circumstances. reply mkl 4 hours agorootparentThe danger with extension acquisitions is malicious buyers, who use their ability to run arbitrary code to steal credit card numbers or credentials, insert or replace ads, run cryptocurrency mining, etc. For malicious purposes number of installations is the important thing, not how clonable the extension is. reply jl6 52 minutes agorootparent$10m could be worth it for a nation state actor targeting a specific high-value individual known to use the extension. reply mort96 29 minutes agorootparentprevI appreciate the honesty, and the reality is that most add-on developers have a price; a lot of people would probably sell their add-on for $10M. But these things auto update. If a government (or even just a moderately big org) really wants to spy on someone, and they determine that said someone uses IPvFoo, $10M isn't a very large price to pay to just get complete access to the target's web browser. This isn't specific to your add-on in any way, but, well... that seems ripe for takeover by someone nefarious. reply OJFord 8 minutes agorootparentIf your threat model includes a government... reply amne 1 hour agorootparentprev$10M for access to a pool of 100k (and growing) tech-savy users that are harder to hack than gen-pop? You guys, the 100k+, are lucky I'm not made of money. reply codetrotter 3 hours agorootparentprev> IPvFoo author here It makes sense that you would notice the change then :D Was this a situation where you had to do a double-take? Like, you opened HN today and saw that your extension said IPv6 and for a split second you wondered if your extension had made a mistake? Before seeing that indeed HN has IPv6 now. reply p1mrx 2 hours agorootparentI didn't really think it was a mistake, just switched to \"Morpheus is fighting Neo!\" mode and started collecting evidence, most of which is in the post. reply notatoad 4 hours agorootparentprevyeah, i appreciate that. my post wasn't meant as a criticism, it's a cool project and i'm happy your extension exists. i have no reason to think you'd sell out to scammers, but stranger things have happened and for a product whose whole utility to me is \"huh, that's cool\" it's not worth it. and i thought that was worth highlighting to others. some chrome extension hygeine is always good. reply janardanyri 4 hours agorootparentprevJust wondering, do you expect non-scammers to have the $10m? reply p1mrx 4 hours agorootparentNo. I expect that I won't sell it because it's worth more to me than anyone else. reply playingalong 2 hours agorootparentprev> $10M USD Hah. You take a strong starting position in the negotiations ;) reply ugh123 3 hours agorootparentprevTo determine ipv4/6, do you need to send that hostname and ip off to somewhere to get a response? or can it be done locally without leaving the machine? reply p1mrx 2 hours agorootparentIt's local. The webRequest API provides an 'ip' field with each request, and the hostname can be extracted from the URL. Once you have an IP address, just check for colons. https://developer.chrome.com/docs/extensions/reference/api/w... reply shzhdbi09gv8ioi 3 hours agorootparentprevYou can determine that by looking at the value. IPv4: 123.123.123.123 IPv6: 2001:db8::8a2e:370:7334 reply dieortin 1 hour agorootparentprevipv6 addresses are different, so they’re easy to distinguish reply justsomehnguy 3 hours agorootparentprevOfftopic: // Don't waste time rewriting the same tooltip. // Don't waste time redrawing the same icon. Finally someone who understands the basics. reply cbracketdash 4 hours agorootparentprevJust download the repo as a zip file and manually upload it as an extension to your browser. reply p1mrx 4 hours agorootparentNote that the top-level manifest.json is for Chrome. To use it with Firefox, you have to copy \"manifest/firefox-manifest.json\" onto manifest.json first. reply justsomehnguy 3 hours agorootparentprevJust read the source code. reply lolinder 2 hours agorootparentNot good enough to protect against the kinds of attacks that OP is warning against. Chrome extensions update automatically and there have been many cases of extensions being purchased by malicious actors who modify the code to be spyware or adware. You can download the current version and install it manually to get around that. If you do that and read the code you're probably safe. https://hn.algolia.com/?dateRange=all&page=0&prefix=true&que... reply justsomehnguy 2 hours agorootparentSo the problem is in the behaviour of the browser, but not the extension. Read the code, don't update 'till you read the code, don't use the browser which knows better what is good for you. reply mulmen 2 hours agorootparentSeems to me fine grained access controls would go a long way. The extension gets access to specific capabilities. Such as network connectivity. Local extensions have a much smaller blast radius. reply mg 2 hours agoparentprevIn Chromium, you can click on the request in the Network panel and it will show the IP. I see: Remote Address: [2606:7100:1:67::26]:443 When I put that into the url bar: https://[2606:7100:1:67::26]/ I also see Hacker News. reply ceroxylon 6 hours agoprevAs someone who grew up on IPv4, i will miss it, but the leap to 340 undecillion unique addresses is exciting in many ways so i think i can learn to live with this transition. If we ever need more than that, i can't even imagine what that future would look like. reply globular-toast 22 minutes agoparentI grew up with IPv4 too but after learning and configuring my network for IPv6 I won't miss v4 at all. It's just so nice having each device with its real IP address rather than some private NAT thing. Then it's just firewall config if you want to run servers etc rather than messing with NAT configs. reply p-e-w 6 hours agoparentprevThe last time I looked into this (which was a few years ago), ISPs were allocating blocks containing billions of IPv6 addresses to anyone who paid a nominal sum. So that vast address space might not last as long as it would seem... reply greyface- 6 hours agorootparentThat sure sounds like a lot. But \"billions\" is less than a /64. Try \"sextillions\" (/56) or \"septillions\" (/48). Of course, when the denominator is \"undecillions\", it becomes clear that this is actually a non-issue. reply mike_d 1 hour agorootparentARIN recently handed out a /16 allocation to Capital One. That is one 65,025th of all of IPv6. A reasonable sized /32 allocation would have allowed for giving every ATM they operate worldwide its own globally routable /48. reply greyface- 1 hour agorootparentToo bad ARIN tickets aren't public - I would love to read the one in which that was justified. Reading NRPM 6.5.2.1, it seems that they must have submitted documentation claiming at least 2^28 distinct serving sites in order to qualify for an assignment that large. reply ta1243 1 hour agorootparentprevI find the insistence of using /64s everywhere for networks frustrating. Any network larger than a /112 seems crazy, that's already 65k IPs per subnet. A /104 for every normal end user (256 subnets per user), or a /80 for massive companies like Capital One (4 billion subnets) should be more than enough. reply tuetuopay 44 minutes agorootparentThere is a really practical reason behind this, and it is called \"routers\". Due to longest prefix match, you'll end up wasting resources on the networking hardware. And you waste both precious and expensive TCAM and LPM latency for matching the prefixes. So routers do optimize for anything shorter than /64, and have special lookup memory for /64 and /128. But nothing in-between. reply iLoveOncall 48 minutes agorootparentprevI'm sure people said exactly this about IPv4 back in the days. reply joering2 3 hours agorootparentprevI'd even go out of my way and say that undecillion of IP addresses ought to be enough for anybody. reply dns_snek 1 hour agorootparentBold prediction! Let's see if it holds true in 200 years when every cybernetic cell in your body is individually addressable from the public internet. reply ale42 1 hour agorootparentThis is almost IPv9 (RFC1606)! https://datatracker.ietf.org/doc/html/rfc1606 The up to 42 deep hierarchy of routing levels built into IPv9 must have been one of the key features for its wide deployment. [...] As yet, no requirement has been found for levels 40-42, with level 39 still being used for experimental interrogation of atomic structure of components where required. Of course, it's a 1st April RFC. For those who don't know them, see https://en.wikipedia.org/wiki/April_Fools'_Day_Request_for_C...: they include thinks like terminals with subliminal messages, IP over pigeons... reply Nition 57 minutes agorootparentprev98% of all IPv6 addresses will be set aside for self-replicating nanobots and our bodily cells will have to fight over only the remaining 2%. reply pilif 5 hours agorootparentprevOne of the features of IPv6 is address autoconfiguration, obviating the need for a central authority like DHCP on v4. However, that only works with a /64 prefix and given that larger sites might want to have multiple subnets, that’s why most assignments are /56 or /48. But still. If all assignments were /48s, that would still leave room for 281 trillion networks which even I believe is enough for the foreseeable future. If the number space is so big, it makes sense to take advantage of it if that allows for other additional features (like SLAAC) reply mort96 21 minutes agorootparentI don't understand why getting rid of DHCP is desirable. DHCP provides a nice central place where you can map MAC addresses to IP addresses instead of configuring it ad-hoc on every device which needs a static IP address (if you're lucky and the device even supports static IP!). Checking \"Does my interface have an IP address?\" is also a really really useful and quick analogue for \"is the gear related to the LAN pretty much working or is something broken/misconfigured?\". As it is, all my interfaces just have these random IPv6 addresses configured which don't work most of the time. I don't get it. reply avhception 51 minutes agorootparentprevI still haven't figured out how to get these auto-configured addresses into my home network's DNS (or any DNS, for that matter). reply ta1243 1 hour agorootparentprevBut they aren't all /48s. Upthread there's a post about /16s already being allocated. reply londons_explore 5 hours agorootparentprev> However, that only works with a /64 prefix Well it should be redesigned so it works all the way down to individual addresses. reply pantalaimon 1 hour agorootparentThere was a proposal for that: https://www.ietf.org/archive/id/draft-mishra-6man-variable-s... reply greyface- 5 hours agorootparentprevYou can use DHCPv6 instead of SLAAC if you need autoconfiguration on sub-/64 prefixes. reply scheme271 5 hours agorootparentSome systems like android don't support anything but SLAAC reply ale42 1 hour agorootparentIMHO it means that they are not fully IPv6 compliant. Corporate networks often have DHCPv6 rather than just SLAAC. reply lmm 5 hours agorootparentprev> Well it should be redesigned so it works all the way down to individual addresses. Why? What's wrong with how it works at the moment? reply Kab1r 5 hours agorootparentI actually have the issue where my ISP gives me a single /64 and it makes it difficult to split between multiple LANs. reply lmm 5 hours agorootparentPublished guidance says they're meant to give out at least a /56. I don't know that making autoallocation work on smaller subnets would help with this problem - ISPs that currently give out the smallest possible subnet would probably just switch to whatever the new smallest possible subnet was. reply tuetuopay 40 minutes agorootparentSome do give you one but in the most incorrect way possible. When I had fibre through Orange France, they properly allocated a /56 to my connection, but the ISP provided router only routed the first /64 from the block. The UI even proudly displayed the full /56 in all of its glory followed by a \"xx/64 usable\". Oh and I ended up disabling IPv6 altogether as their router would crap itself with a modest amount of IPv6 traffic. Pulling ~10Mbps of IPv6 would completely DoS the router, as it would not even go as far as answering to ARP. Some quality hardware for sure. reply Kab1r 5 hours agorootparentprevThere probably is a way to ask my ISP for a larger block, but I think it would be nice to be able to subnet a /64 regardless. reply lmm 4 hours agorootparentCIDR and the resulting address space fragmentation was the problem that IPv6 was originally meant to solve; allowing splitting into random-sized subnets makes routing more complex and worse. And if you made the routable part of an IPv6 address longer than 64 bits then that makes the routing much more compute-intensive. 64 bits in the local part of the address is sort of overkill, but a 128 bit address isn't really any harder to use than a 96 bit address, and it means things like, well, being able to automatically assign the local part based on the MAC address, which only works if the local part of the address is more than 48 bits. reply londons_explore 4 hours agorootparentYou can still use a shorter hash of the Mac address. Collisions are very unlikely, and a quick ARP packet should detect them if they do happen. reply ta1243 59 minutes agorootparentCollisions are just as likely with the 48 bits as they are with the last 24 bits, as some OUI vendors reuse mac addresses. Using a mac address in the IP is terrible anyway from a privacy point of view, use a random IP in the subnet, send out a check to see if it's already used, if it is choose a new one. That check is already a feature of ipv6. reply ale42 1 hour agorootparentprevIt would rather be an ICMPv6 neighbor discovery packet or something like that, IPv6 doesn't use ARP. reply lmm 3 hours agorootparentprev> Collisions are very unlikely, and a quick ARP packet should detect them if they do happen. You can do that, sure. But it's more complex and slower. reply akira2501 4 hours agorootparentprevOn Comcast Business the SLAAC would hand out a /64, and DHCPv6 would give you a /64 unless you requested \"Prefix Delegation\" in which case they'd give you a /56, /58 or /60 depending on how large your static IPv4 allocation was. reply p1mrx 4 hours agorootparentprevShitty ISPs will always give you as little as they can get away with. If the software enforces /64, they'll you /64. Raise the limit to /128 and they'll give you /128. At least with the current state of affairs, you have the option to use ND Proxy instead of NAT66. reply kuschku 45 minutes agorootparentDoesn't AWS currently hand out /128? reply viraptor 4 hours agorootparentprevWhat do you mean by that? What you do inside your assigned range is completely up to you - you can split that up as you want. reply IcePic 2 hours agorootparentThis is the same as if you get a /24 from your ISP, but the ISP has the .1 address on their router. In this case, you can't (easily) subnet the v4 range either, since the ISP router expects to be able to arp for all the 254 ips, meaning you have to resort to trickery in order to make subnetting almost work. Same goes for ipv6. If you get one single net (and a 'small' one at that), you once again have to resort to trickery to subnet at your end. Preferably in both cases, you would get a small (/31 for v4, /127 for v6) transport net that goes to the ISP router or your first fw device, then the ISP routes the real net /24 (or whatever size you can get from it on v6) behind your IP on the /31,/127 so that your first device can split this in any way you prefer. reply tsimionescu 3 hours agorootparentprevNot if you want SLAAC to work in each LAN. Basically in IPv6 each LAN is supposed to have a /64, that's the real minimum allocation by design. Per the official recommendations, ISPs are supposed to give every individual customer at a minimum a /56. reply viraptor 3 hours agorootparentAh, right. That leaves a custom dhcpv6 pools config for you... reply Kab1r 5 hours agorootparentprevBillions of addresses might seem like a lot, but every IPv6 network has at least 2^64 addresses and it doesn't make much sense (to me) to give any customer less than one network. (Maybe you meant billions of /64 blocks? ISPs could be providing a /32 ≈ 4 billion /64 blocks, though there still are 2 billion of those in the entire IPv6 space) reply internetter 6 hours agorootparentprevWe never learn reply dheera 3 hours agoparentprevI grew up with IPv4 (1.2.3.4) and I was expecting IPv6 to just be 1.2.3.4.5.6 with backward compatibility so that 1.2.3.4 would just be 0.0.1.2.3.4 and the 1.2.3.4 dude wouldn't need to change their address. And the IPv8 would be 0.0.0.0.1.2.3.4 whenever we need it, but probably not for a long time When I saw all the double-colons and slashes and monstrosities like f00f:00f:::ea//dead::beef/3 I just kept using IPv4. I can't even remember Google's IPv6 DNS ffs. 8.8.8.8 was easy to remember. Now it's got some hex bullshit in it and a double colon thrown in somewhere. reply zokier 2 hours agorootparentThere are endless protocols and methods which embed ipv4 addresses to ipv6 ones. RFC 4291 specifies exactly what you want, all-zero prefix+ipv4 address: https://www.rfc-editor.org/rfc/rfc4291.html#section-2.5.5 See also the text representation section: https://www.rfc-editor.org/rfc/rfc4291.html#section-2.2 3. An alternative form that is sometimes more convenient when dealing with a mixed environment of IPv4 and IPv6 nodes is x:x:x:x:x:x:d.d.d.d, where the 'x's are the hexadecimal values of the six high-order 16-bit pieces of the address, and the 'd's are the decimal values of the four low-order 8-bit pieces of the address (standard IPv4 representation). Examples: 0:0:0:0:0:0:13.1.68.3 0:0:0:0:0:FFFF:129.144.52.38 or in compressed form: ::13.1.68.3 ::FFFF:129.144.52.38 So in addition to having the addresses embedded on binary level, you even have that text notation that uses traditional ipv4 dot-notation! You might ask why they are not more prevalent, but then you will find the practical issues that various transition mechanisms are attempting to solve. reply conradfr 2 hours agorootparentprevIt comes down to user experience. How many people managing networks who knew their ip addresses by heart and typed it regularly for all kind of tasks were put off by the new format and decided, consciously or not, to wait until \"I really have to deal with it\"? Some people get really angry when you point that out ;) Good thing for IPv6 it didn't really have any competitor (except IPv4). reply Sami_Lehtinen 36 minutes agorootparentprevTechnically it's just 128 bits, it doesn't matter how you represent it. I've written this IPv4ES solution, which allows you to use 128 bit addresses using IPv4 format. https://www.sami-lehtinen.net/blog/ipv4es-the-perfect-soluti... reply Plasmoid 3 hours agorootparentprevAnother person who does not understand IPv4. IPv4 isn't a text based protocol where IP addresses are parsed like DNS. It's a binary protocol where addresses are recorded in binary and adding more address space WOULD BE A BREAKING CHANGE. reply mike_d 2 hours agorootparentNot at all. We could have taken one of the unassigned /8's at the time and allocated it to v6 transitional addressing (the failure to address 4->6 reachability is IMO why v6 failed). For the sake of this example lets use 53.0.0.0/8. All new addresses start with 00110101 followed by the first three bytes of the new v6 prefix. The prefix acts as a flag that indicates it is a new address and routers read an additional 5 bytes from the beginning of what would be the data section of a traditional v4 packet to get the complete address. Now your border router can announce your assigned transitional prefix, i.e. 53.32.122.91/32, and is responsible for routing packets to a NAT gateway that knows both v4 and v6 and rewrites packets seamlessly each way. What we are left with is a scheme that allows v6 to exist on top of v4 and continue working across the existing internet, and the only people who need to worry about it or upgrade anything are the ones who need more address space. But instead they followed the model of baking in every stakeholders random pet project into v6 to get consensus in the hopes of forcing adoption. They put letters in the middle of numbers, and expected us to not hate it like algebra. reply Denvercoder9 1 hour agorootparent> They put letters in the middle of numbers If you prefer, you could write an IPv6 address in dotted-decimal notation just like an IPv4 address, or an IPv4 in hexadecimal notation like an IPv6 address. It's just 128 (IPv6) or 32 (IPv4) bits of data after all, the representation is completely independent of the protocol. For example, you can also reach HN through its IPv4 address by writing http://3520653007/ or http://0xD1D8E6CF/. reply schroeding 2 hours agorootparentprevFor a practical demonstration: That's the reason decimal IPs work, too - i.e. http://3520653007/ is the same as http://209.216.230.207/ (and both will go to HN) - it's all just nice formating for our human brains. Nothing would stop us from formatting IPv6 the IPv4 way except the monstrous length of the resulting address. reply TrickyRick 1 hour agorootparentWow learned something new today. What's interesting is that the decimal representation looks more like a phone number which people would be used to. Interesting that IP addresses as they are written today was the format that won, as a kid before learning how computers worked I always found it weird how 255 was the highest number in each group. reply ale42 1 hour agorootparentprevYou can even make it funnier by having it look like a floating point number: http://209.14214863/ Or a strange number with two decimal parts: http://209.216.59087/ reply bmacho 2 hours agorootparentprevThat's not what GP said. Of course IPv4 devices wouldn't be able to use IPv6 addresses, that would be impossible. But it is possible to \"keep\" IPv4 addresses, just make a.b.c.d to correspond 0.0.a.b.c.d. reply p1mrx 2 hours agorootparentprev2a09:: 2a11:: and 2409:: are even shorter than 8.8.8.8, though not quite as memorable. I'm not recommending those DNS servers, just highlighting that \"vanity\" IPv6 addresses exist now. It's possible that 2222:: or 3333:: could be allocated someday. reply mnordhoff 34 minutes agorootparentRelated: 's IP address was 2600:: for many years, but they sadly started using a DDoS mitigation service with different IPs. reply _zoltan_ 2 hours agorootparentprevand people are wondering why v6 never took off... reply Uehreka 2 hours agorootparentprevHuh? IPv6 does have backwards-compatible-ish notation for writing IPv4 addresses. To take your example: 1.2.3.4 would be ::ffff:1.2.3.4, the 96-bit prefix indicated by ::ffff: is where all of the IPv4 addresses live in the larger IPv6 space. If your issue is with the use of colons at all, they were a deliberate choice so that computers doing string processing could never confuse the two types of addresses. reply mike_d 1 hour agorootparent> they were a deliberate choice so that computers doing string processing could never confuse the two types of addresses. Wait until you try to write an IPv6 address with a port number in standard notation. reply ale42 1 hour agorootparentUsually IPv6 addresses are enclosed in square brackets when a port number is involved. But it's true that in many configuration files IPv6s are a nightmare to put in, especially because you never remember what syntax you have to use: sometimes you even have to duplicate semicolons (that's what Exim does...). But I think this is rather a convention problem for the config files, rather a problem of IPv6 addresses themselves. reply tlb 6 minutes agorootparentThe IPv6 people must have known that : was a common way to separate IPv4 and port numbers. IPv6 was standardized 4 years after the URL format which used a colon to denote a port number. reply bhaney 6 hours agoprevMy concern is that this is an accidental part of an unrelated migration and will quickly be disabled because of some internal IP filtering/banning tool that was only ever written to work with IPv4. reply whatshisface 6 hours agoparentHey, my account works again... >:) reply rumdz 6 hours agoprevDoes ipv6 result in higher latencies? I could see larger addresses increasing latency, but then again I could also see a more efficient protocol resulting in lower net latency. I should probably read a book describing the differences. reply kalleboo 5 hours agoparentReal-world latency is impacted by configuration and hardware (e.g. your ISP may have different routes for IPv6 that can be either better or worse, it can be handled by different routers with different performance, IPv4 traffic may be going through CGNAT, PPPoE concentrators, etc) that dwarf any theoretical differences. Google's IPv6 stats also measure latency compared to IPv4 and in most countries IPv6 has lower latency (e.g. in the US on average you get 10ms lower latency with IPv6). When this chart was new it was mostly the other way around, with early IPv6 implementations being poor https://www.google.com/intl/en/ipv6/statistics.html#tab=per-... reply toast0 4 hours agoparentprevReal world performance differences between v4 and v6 are more likely to be influenced by different routing and network manipulation for v4 vs v6 than the larger address size. If your v4 goes through NAT and v6 doesn't, that's a big thing. If you have different peering and transit providers in v4 and v6, that's a big thing. If overhead from address sizes was really a big deal, we'd see work to push larger MTUs and working MTU discovery, but that kind of stalled a while ago. 1500 works for a lot of people, and many major sites drop effective MTU by 20 or so and that makes more things work, and then it gets swept under the rug. (OTOH, I think Android may have finally gotten MTU probing enabled after many years of shipping it disabled; Apple has had very effective probing, at least on iOS for a long time) reply acdha 5 hours agoparentprevIt could but there have been some reports of lower latency, too: https://www.youtube.com/watch?v=An7s25FSK0U This is an area you want to measure carefully because some of the older reports about IPv6 being slower were artifacts of old hardware limitations or under-optimized software which are no longer relevant. reply imoverclocked 5 hours agoparentprevThe address space has very little effect in the real world. Other latencies far outweigh anything measurable by having slightly more bits in the address. Eg: Sometimes IPv6 is faster due to routing differences. reply rumdz 5 hours agorootparentThat makes sense. I'm very curious about real-world studies. As a gamer, I'm especially interested in the affect ipv6 has on UDP for real time gaming applications. That's an area where even 5ms can have an enormous affect on the experience. reply jabiko 48 minutes agorootparentIf you look at Googles IPv6 statistics the latency seems to be lower with IPv6 in almost all countries: https://www.google.com/intl/de/ipv6/statistics.html#tab=per-... reply tsimionescu 3 hours agorootparentprevThat's a very interesting case, as UDP is very reliant on MTU. If the IPv6 headers take out more space from the ethernet frame, that leaves less space for the UDP payload. Which means that a UDP payload which was at the limit for IPv4 on the typical MTU needs to be fragmented into two IPv6 packets, which will likely increase latency quite significantly. However, this will depend on each specific game, if they are using all the available space or not. If they're sending 200 byte datagrams, they shouldn't see any difference. On the flipside, IPv6 has a larger minimum MTU than IPv4, so it could happen that your maximum UDP payload actually goes up when switching to IPv6. So, if the game previously had to send 5 packets to do an update, it might be able to send only 3 when it can rely on IPv6, so maybe latency actually significantly improves. reply IcePic 2 hours agorootparentIf you try \"ping\" and \"ping6\" towards a multi-protocol host, you see both send 64 bytes each, so while v6 source and destination addresses take up lots of extra space, the v6 IP packets have less of the \"this part could be useful for tcp\" which means icmp pings can be of the same size, even though the two addresses eat up lots more bytes. Not sure if the same goes for game UDP packets, but the optional header stuff in v6 IP packets means more of it goes to the useful parts of the payload and less to \"the sum of all protocol bits and flags that is not used by all traffic\". reply viraptor 4 hours agoparentprevPossibly due to routing differences on the path to your service, but not due to the protocol itself. Definitely not due to the address size. Beyond your local equipment, that switching normally happens in hardware. reply bqmjjx0kac 5 hours agoparentprevVerizon Fios recently rolled out IPv6 last year and I swear IPv6 traffic is deprioritized. Sometimes it just cuts out entirely. reply ale42 58 minutes agorootparentSounds like a technical problem. I don't see any reason they should de-prioritize IPv6 traffic... reply meindnoch 5 hours agoparentprev>I could see larger addresses increasing latency ??? reply rumdz 5 hours agorootparentSeveral additional bytes over millions of packets where the data frame is ~10bytes seems potentially significant. I'm less interested in HTTP reply p1mrx 4 hours agorootparent20 bytes / 1 Gbps = 160 ns per hop. That's 0.016 ms additional latency over 100 hops. On the internet, most links are faster than 1 Gbps and most paths are shorter than 100 hops, so that's a conservative estimate. If you're sending lots of 10-byte payloads, then IPv6 requires (40+10)/(20+10)=166% as much network capacity, but are you really filling up an expensive link with VoIP traffic? reply lmm 5 hours agoparentprevIn theory, any impact from longer addresses would be outweighed by the benefit of the shorter non-CIDR routing table (and in turn that should be outweighed by avoiding NAT, and that should be outweighed by avoiding CGNAT). (Plus with most systems being natively 64-bit these days, that impact should be 0 - the routable part of an IPv6 address is 64 bits, and comparing a 64-bit value is no harder than comparing a 32-bit value). In practice IPv6 is newer, which has good and bad sides; IPv6 routing paths are more likely to be using newer (and therefore faster) equipment, but there's also a bigger risk of someone making a mistake that messes up your routing/latency, particularly if your ISP hasn't been doing IPv6 for very long. reply supriyo-biswas 6 hours agoprevTechnically the goal of enabling IPv6 can be done through Cloudflare as well, which they enabled a few days ago. I wonder why they turned it off and are back to directly serving traffic. reply JeremyNT 5 hours agoparentThey put it behind CF specifically in response to a DOS attack [0] [0] https://news.ycombinator.com/item?id=38939559 reply djbusby 6 hours agoparentprevYea. I noticed that too. Got one or two CF warnings when visiting last week. reply mogeko 1 hour agoprevI'm surprised it just got implemented! Since IPv6 is so popular and common nowadays. reply mobilemidget 1 hour agoparentMy previous provider, I think largest in The Netherlands, ziggo rolled it out quite okay. Now I moved and switched providers, to much faster 1 Gbit (can go up to 8gbit but I dont know what to do with that yet until I have a 32K tv maybe) symmetric, but no idea when they will plan to roll out IPv6 on this network. I cannot say I really miss it, but have to admit my internet feels a bit incomplete :) so I wish it was even more popular than you state. :D reply p-e-w 6 hours agoprev> My browser just started connecting to Hacker News over IPv6 today If you don't mind me asking, how did you notice that? reply p1mrx 3 hours agoparentIPvFoo turned green. reply xyst 6 hours agoprevNext is TLS 1.3 support, hopefully :) # openssl s_client -connect news.ycombinator .com:443 -tls1_3 CONNECTED(00000003) 4160736388:error:1409442E:SSL routines:ssl3_read_bytes:tlsv1 alert protocol version:ssl/record/rec_layer_s3.c:1562:SSL alert number 70 --- no peer certificate available --- No client certificate CA names sent --- SSL handshake has read 7 bytes and written 244 bytes Verification: OK --- New, (NONE), Cipher is (NONE) Secure Renegotiation IS NOT supported No ALPN negotiated Early data was not sent Verify return code: 0 (ok) reply supertrope 6 hours agoparentIt was briefly turned on years ago and then turned off. I guess it broke the website for those behind corporate MITM boxes. reply kiwijamo 6 hours agorootparentThere's a lot of websites out there that does TLS 1.3. Surely not an issue for MITM boxes? Otherwise they wouldn't be able to access much... reply yjftsjthsd-h 4 hours agorootparentMaybe that wasn't true years ago? reply johnklos 3 hours agoprevWelcome to 2001, HN! reply nikita14 4 hours agoprevFinally. reply tlivolsi 6 hours agoprevThey'll have to pry IPv4 from my cold, dead hands. reply SamuelAdams 6 hours agoparentCan you explain this more? Why are you so tied to ipv4? I recently enabled IPv6 on my home network, and roughly 30-40% of all internet traffic goes through IPv6. Things are noticeably faster, especially connection times to online games on Xbox Live. My only complaint is that my ISP keeps changing the prefix every quarter or so, so my static addresses need to be updated in the firewall and other places. I am looking into link local addresses but the cocktail of tech is tricky. reply btgeekboy 1 hour agorootparentI turned it off on my home network. I have a multi-wan setup (fiber + 5g). The 5g provider supports v6, but only delegates a /64. The fiber provider will delegate me a /56, which is plenty for both my home and guest networks. Failover for v4 works great, as everything's behind NAT so the route just changes when my firewall detects an issue, but clients accessing v6 resources have a hard time, as you're waiting on each device to figure out that the old route is dead. So that's problem number one. Problem number two is that the fiber provider doesn't support native v6; it's actually a 6rd tunnel. Latency isn't great compared to v4. I need to go figure out the ULA situation and do NPTv6. But last I checked, my firewall wasn't able to do NPTv6 with delegated ranges. That may have changed, but I've not found any substantial reason to actually put in the effort to figure out it when my v4 network works fine. reply karlshea 6 hours agorootparentprevYou want a ULA (unique local address) instead of link-local. Link-local can sometimes mean needing to append the interface name to your address and a bunch of other weirdness. If you pick a ULA prefix and announce it (or assign some statically) everything pretty much just works. I’ve been using them internally for over a year and it’s been great, they basically feel like RFC 1918 addresses. reply magicalhippo 1 hour agorootparentYou'll also need a router that's not stuck in the 90's. I never got IPv6 working well until I switched from pfSense to OpenWRT, due to my residential ISP switching prefixes very frequently. For example, there was no way to get pfSense to not publish the public address of the router as the internal DNS, so every time the prefix changed internet effectively broke. reply orangeboats 31 minutes agorootparentBy the way, you can use a separate Linux box (if you have any) to announce your ULA and DNS using radvd. Just remember to set AdvDefaultLifetime to 0 or else your devices will attempt to route their packets to the Linux box. It was what I did when I had a router that announced only public IPv6 prefixes to the LAN. reply dcow 6 hours agorootparentprev+1. Link local is like connecting to a computer by mac address. The only time you’d ever want it is if you for some ungodly reason are worried that all other addressing systems have failed, or to bootstrap said systems. reply clan 56 minutes agorootparentCan you recommend a good book on the subject? Especially one which includes the transitioning pitfalls. I have been living through it all and can get a little confused as to what is status quo. First I heard that SLAAC was great. Then came DHCPv6 and some complained it was an ill concieved bandaid. Now I was under the impression that link local addresses would make my day easier but not you indicate that ULAs are the way to go. All of this is made much harder by ISPs actively fighting IPv6 adoption. They have the usual moat babble that users do not request it. But in my case they even blocked /protocol/ 40. This was not documented anywhere. Imagine the layers of support I had to work through. Imagine working with new technology and be sure enough that you have exhausted all other possibilities. So learning practical IPv6 has been an uphill struggle for me. Years ago I had a SixXS tunnel going before major adoption took off. Now I am living in another place and wanted to look at it seriously. SixXS was no more so I went with HE. To my dismay dark corners of the Internet have abused these offerings so I have my tunnel disabled most of the time as it gives too much grief. And I have even worked in operations at a large ISP in the 90s. Adoption is not easy even for the willing. But the reason? No one here seems to mention it: Money. There are no technical excuses left. But it is surely a nice moat. Sorry for the rant! A good up to date book recommendation would be appreciated :-) reply teaearlgraycold 6 hours agorootparentprevWhy would things be faster over v6? reply lxgr 6 hours agorootparentOn networks using NAT64 (and maybe also for DS-Lite, although I'm not so sure there), the IPv6 path is more direct than the IPv4 one since it doesn't need to go through a CG-NAT, which might be at capacity (tracking every TCP connection requires memory) or located farther away than the nearest IPv6 egress router, making routing more indirect. I believe all three large US mobile carriers use NAT64 at this point exclusively, and CG-NAT is quite common in DOCSIS cable networks. reply supertrope 6 hours agorootparentprevSimpler routing because address blocks are not fragmented. IPv6 does not have a header checksum so that's one less task for routers. Packet fragmentation is no longer done by routers. IPv6 capable gear tends to be newer and higher performance. There's no NAT step or even worse CGNAT. reply abrookewood 5 hours agorootparentFor those that were wondering: Unlike in IPv4, IPv6 routers never fragment IPv6 packets. Packets exceeding the size of the maximum transmission unit (MTU) of the destination link are dropped and this condition is signaled by a Packet too big ICMPv6 message to the originating node, similarly to the IPv4 method when the Don't Fragment bit is set. reply toast0 4 hours agorootparentprev> Packet fragmentation is no longer done by routers. The overwelming majority of IPv4 packets have the DF bit set. A large number of services drop inbound fragments anyway (if you get more than a couple a minute, it's almost certainly abuse). reply SamuelAdams 6 hours agorootparentprev> By reducing header complexity, IPv6 contributes to better overall network performance and efficiency. This is especially noticeable in applications sensitive to latency, such as real-time communication and online gaming. https://www.prefixbroker.com/news/is-ipv6-faster-than-ipv4-a.... reply j16sdiz 6 hours agorootparentThis is the theoretical benefit. These have never materialize in real life. See https://community.cisco.com/kxiwq67737/attachments/kxiwq6773... reply kiwijamo 6 hours agorootparentprevSometimes networks will route IPv6 traffic over different paths compared to IPv4 although that could actually be either an improvement or an regression depending on which gets the better path. reply danpalmer 6 hours agorootparentprevIPv6 was a lot more than a numbering scheme, there are performance (and security?) improvements in a number of aspects of IP networking, like more compression in parts of the messages IIRC. reply whatwhaaaaat 4 hours agorootparentGonna have to disagree with that. IPv6 was designed in the 90s and lots of it feels like it is. Icmpv6 is fundamental to the implementation and already responsible for multiple remotely exploitable vulnerabilities. Couple that with almost no one understanding it well besides a true handful of engineers and you have yourself a security nightmare. Proof of this is this very thread with the talk about “trying a local link address”. Everything about v6 is over engineered with sharp edges. reply wjholden 3 hours agorootparentprevA possible explanation is the service provider routes IPv4 over their old routers but routes IPv6 over their new routers. reply tilwidnk 6 hours agorootparentprev> Can you explain this more? Why are you so tied to ipv4? For me, IPv4 doesn't break the privacy barrier, IPv6 blasts a huge hole straight into each and every household, office and IoT device on the planet. No, privacy fixes put into IPv6 do not work. reply dns_snek 1 hour agorootparentYou're not completely wrong, but that ship has sailed a long time ago: https://amiunique.org Until browser fingerprinting is addressed, there will be no real privacy. reply lolinder 5 hours agorootparentprevCan you explain this more? I realize that one IP address per device poses a major problem for privacy, but I thought we somewhat mitigated that by dynamically reassigning IPs. What exactly is the problem with the privacy fixes that were put into IPv6? Why don't they work? reply kccqzy 5 hours agorootparentI'm guessing that the GP is talking about the fact that if there were two persons in a household using the Internet at the same time, with IPv4 they would connect from the same IP address (though of course with different port numbers), but with IPv6 they would likely connect from distinct IP addresses, and usually only sharing a /64 prefix. You are correct that this isn't a big issue. SLAAC addresses are generally changed fairly frequently by the OS. As for stateful DHCPv6, well I turn it off for both this reason and the fact that Android doesn't support it. reply CSSer 6 hours agoparentprevThe next few months may be hard for you. AWS will start charging for ipv4 on February 1st, so it’s likely that either a mass migration is coming or prices for many services will increase accordingly to pass that cost along to customers. reply wjholden 3 hours agorootparentI actually did exactly this. I run a very small site for a local club from an AWS Nano instance. Minimum cost is important. When AWS announced that they would begin charging for public IPv4 addresses, I enabled IPv6 on the subnet (which was tricky), updates the DNS record, and removed the IPv4 address. In my case, no one notices or cares because they almost always access the site through their cell phones. reply Rapzid 1 hour agorootparentprevI was implementing IPv6 networking at an ISP/VPS provider in my 20s including customized Xen network startup scripts(because they didn't support v6 or VLANS). I'm turning 40. The next few months will be fine. reply robertlagrant 52 minutes agorootparentEnjoy your quiet birthday :) reply luhn 4 hours agorootparentprevAWS doesn't have good enough IPv6 support for an exodus from IPv4. ALB does not support IPv6-only. Cloudfront only supports IPv4 origins. API Gateway only supports IPv4. reply tsimionescu 2 hours agorootparentprevThe IPv4 price is still tiny, and you anyway have to pay for it since going IPv6-only is not viable for a service in 2024 still. It's not going to significantly move the needle towards IPv6. And the price hikes will also be tiny. reply outofmyshed 2 hours agorootparentWell, my employer’s AWS bill is in the region of $20m/year and the additional IPv4 tax is on track for adding an additional $250k to that for no benefit at all. reply jowf8fa9sdf 6 hours agorootparentprevThis scared me but they are only charging for public IPv4 which I never use. reply ipaddr 6 hours agorootparentprev2024 is shaping up as the year people drop AWS reply acdha 6 hours agorootparentGiven that this was AWS doing what their top two competitors were already doing, and that for most people it’s smaller than the price hikes GCP customers have had last year and coming February 1st, I’m skeptical that anyone significant is going to make this the issue they leave over. reply al_borland 6 hours agorootparentprevDropping AWS and VMWare at the same time. This should get interesting. reply supriyo-biswas 5 hours agoparentprevhttps://news.ycombinator.com/newsguidelines.html > Eschew flamebait. Omit internet tropes. reply mise_en_place 6 hours agoparentprevYou’ll get a lot of hate for it, but I think you’re speaking the truth. IPv6 is probably one of the worst modern standards (tied C++ templates). Nobody has ever made a convincing argument about the advantages of it over IPv4. Just another case of fools admiring complexity. PHB types love it even more, it’s filled with buzzwords like the “future” of networking. reply Dwedit 6 hours agorootparentAdvantage: Not out of addresses. reply whatwhaaaaat 3 hours agorootparentWe’ve been “running out of ipv4 space” since before i got a freakin us robotics modem. reply orangeboats 16 minutes agorootparentWhen we initially ran out of IPv4 addresses, the effects aren't immediately felt since there was an inertia. But nowadays the effect is more than visible (especially in my region, Asia-Pacific), with more and more ISPs putting their customers behind a CGNAT. Let me write a parody of one of the classics: First, they put cellular users behind CGNAT, which is fine because mobile phones don't host services. Then, they came for residential users on cheaper plans, which is fine because they are not powerusers and so are unlikely to host services. After that, they put all residential users behind a CGNAT. ... It is actually what I experienced throughout the last decade in Southeast Asia. Are the ISPs here doing this because they are being cheapskates? No. It's because we are genuinely running out of IPv4 resources forcing people to share them. We did not have the luxury of Western ISPs who were assigned millions of addresses, and buying the addresses is a costly endeavor nowadays with /16 IPv4 block literally costing millions today. And if you think CGNAT is good, think again: (quoting one of my previous comments) [...] you can't really build a truly-P2P network nor self-host a service on Internet when everyone is behind CGNAT. At some point, as IPv4 resources get scarcer, only corporates will have the ability to host services on the Internet, and I don't think it is in their interests to host Tor nodes, for example... reply IcePic 2 hours agorootparentprevWell, in the 90s, you would be able to get a /24 by just filling out some paperwork. How is it today, how easy and cheap can I get a /24 now? Is it two forms and an email? The simple reason people have to start paying AWS for them is that it isn't easy or cheap for AWS to buy large ranges anymore. If it was just \"fill out a form telling someone I need 4 million more IPs\" then AWS would have some cheap junior technician doing that, but now they need to rake in money to cover the expenses to get IPs and customers that need v4 needs to pay for it. You can check the rate of handing out v4 ips in 2012* and see that it was never going to be sustainable. The solution to the known-in-advance problem of ipv4 running out was not to .. not hand them out and just leave internet as it looked in Jan-2011 when IANA ran out of networks to hand out. So while it may be fun to state \"I heard this long long ago\", it just means others had better vision than you. *) https://en.wikipedia.org/wiki/IPv4_address_exhaustion#/media... reply OsrsNeedsf2P 2 hours agorootparentprevYes, and it's a pain in the butt for people who are trying to provision servers in 90% of the world reply senkora 6 hours agorootparentprevI think this post convincingly argues that there exists a world (but not ours, due to path dependence) where IPv6 is a better design: https://apenwarr.ca/log/20170810 reply dang 6 hours agorootparentRelated: The world in which IPv6 was a good design (2017) - https://news.ycombinator.com/item?id=37116487 - Aug 2023 (306 comments) The world in which IPv6 was a good design (2017) - https://news.ycombinator.com/item?id=25568766 - Dec 2020 (131 comments) The world in which IPv6 was a good design (2017) - https://news.ycombinator.com/item?id=20167686 - June 2019 (238 comments) reply apapapa 3 hours agoprev [–] IPv6 is still pretty useless to most people... Actually for me it complicates my firewall rules and don't bring any benefits (yeah down-vote me and don't reply ...) reply johnklos 3 hours agoparentI'll downvote you AND I'll reply. It complicates firewall rules? How about pass everything out, keeping state, and block everything in, for all of your fragile devices that can't help but run services you can't turn off? There. Your IPv6 firewalling is done. Is that complicated? You can't see any benefits? Ok. So what does that have to do with HN being available over IPv6? reply tsimionescu 2 hours agoparentprevI don't understand what you mean by complicating firewall rules,except maybe that you now need to use IPv6 addresses instead of IPv4 addresses in some of the rules. It's not like NAT without a firewall gave any security in 2024. reply rnhmjoj 2 hours agoparentprevYeah, sure: managing both IPv4 and IPv6 on the same network is painful. Thankfully you can disable IPv4 locally and set up NAT64 on the router: most people won't even notice the change. reply mkl 2 hours agoparentprev [–] \"Pretty useless\" is nonsense; Google works on IPv6, and 41% of Google's users access it via IPv6 [1]. \"Most people\" is also not true: most internet users use Google [2] (those not accessing it via IPv6 probably can't). [1] https://www.google.com/intl/en/ipv6/statistics.html [2] https://www.statista.com/statistics/216573/worldwide-market-... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The user's browser is now connecting to Hacker News over IPv6, as indicated by the presence of both IPv4 and IPv6 addresses belonging to the same ASN (M5 Computer Security).",
      "The user is happy about being redirected to Hacker News and hopes that it does not suffer the same fate as Reddit, which was turned off previously."
    ],
    "commentSummary": [
      "This summary provides an overview of IPv6 implementation, challenges, benefits, and concerns.",
      "It covers topics like browser extensions, address allocation, latency issues, gaming performance, network capacity, TLS support, and CGNAT usage.",
      "The discussions emphasize the complexities and potential advantages of IPv6 adoption, as well as the hesitations and difficulties encountered by users and ISPs."
    ],
    "points": 361,
    "commentCount": 165,
    "retryCount": 0,
    "time": 1705978815
  },
  {
    "id": 39091777,
    "title": "Unlocking Free Access: Get Started with Login",
    "originLink": "https://lightning.ai/lightning-ai/studios/code-lora-from-scratch?view=public&section=all",
    "originBody": "Login Start free",
    "commentLink": "https://news.ycombinator.com/item?id=39091777",
    "commentBody": "LoRA from scratch: implementation for LLM finetuning (lightning.ai)297 points by rasbt 17 hours agohidepastfavorite74 comments ignoramous 16 hours agoI've been keeping track of the techniques through Maxime Labonne's LLMs 101: https://github.com/mlabonne/llm-course#4-supervised-fine-tun... reply pama 15 hours agoparentThanks for the resource. It seems useful enough to warrant its own thread here. reply rsweeney21 14 hours agoprevIt's still strange to me to work in a field of computer science where we say things like \"we're not exactly sure how these numbers (hyper parameters) affect the result, so just try a bunch of different values and see which one works best.\" reply TacticalCoder 12 hours agoparent> \"we're not exactly sure how these numbers (hyper parameters) affect the result, so just try a bunch of different values and see which one works best.\" Isn't it the same for anything that uses a Monte Carlo simulation to find a value? At times you'll end up on a local maxima (instead of the best/correct) answer, but it works. We cannot solve something used a closed formula so we just do a billion (or whatever) random samplings and find what we're after. I'm not saying it's the same for LLMs but \"trying a bunch of different values and see which one works best\" is something we do a lot. reply raxxorraxor 4 minutes agoparentprevWelcome to engineering. We don't sketch our controlled systems and forget all about systems theory. Instead we just fiddle with out controllers until the result is acceptable. reply r3trohack3r 14 hours agoparentprevI feel like it's the difference between something that has been engineered and something that has been discovered. I feel like most of our industry up until now has been engineered. LLMs were discovered. reply mejutoco 1 hour agorootparentI believe, from what I saw in Mathematics, this is a matter of taste. Discovered or invented are 2 perspectives. Some people prefer to think that light is reaching in previously dark corners of knowledge waiting to be discovered(discover). Others prefer to think that by force of genius they brought the thing into the world. To me, personally, these are 2 sides of the coin, without one having more proof than the other. reply herval 6 hours agorootparentprevLLMs were very much engineered... the exact results they yield are hard to determine since they're large statistical models, but I don't think that categorizes the LLMs themselves as a 'discovery' (like say Penicilin) reply baq 2 hours agorootparentThere’s an argument that all maths are discovered instead of invented or engineered. LLM hardware certainly is hard engineering but the numbers you put in it aren’t, once you have them; if you stumbled upon them by chance or they were revealed to you in your sleep it’d work just as well. (‘ollama run mixtral’ is good enough for a dream to me!) reply SkyMarshal 12 hours agorootparentprevIf the Black Swan model of science is true, then most of the consequential innovations and advances are discovered rather than engineered. reply arketyp 14 hours agorootparentprevI understand your distinction, I think, but I would say it is more engineering than ever. It's like the early days of the steam engine or firearms development. It's not a hard science, not formal analysis, it's engineering: tinkering, testing, experimenting, iterating. reply peddling-brink 13 hours agorootparent> tinkering, testing, experimenting, iterating But that describes science. http://imgur.com/1h3K2TT/ reply amelius 12 hours agorootparentprevAI requires a lot of engineering. However, the engineering is not what makes working in AI interesting. It's the plumbing, basically. reply justanotheratom 13 hours agorootparentprevand finally, this justifies the \"science\" in Computer Science. reply SkyMarshal 12 hours agoparentprevThat bottom-up tinkering is kinda how CS started in the US, as observed by Dijkstra himself: https://www.cs.utexas.edu/users/EWD/transcriptions/EWD06xx/E... Ideally we want theoretical foundations, but sometimes random explorations are necessary to tease out enough data to construct or validate theory. reply UberFly 13 hours agoparentprevThis is what researching different Stable Diffusion settings is like. You quickly learn that there's a lot of guessing going on. reply fierro 11 hours agoparentprevwe have no theories of intelligence. We're like people in the 1500s trying to figure out why and how people get sick, with no concept of bacteria, germs, transmission, etc reply thatguysaguy 11 hours agoparentprevI haven't seen this key/buzzword mentioned yet, so I think part of it is the fact that we're now working on complex systems. This was already true (a social network is a complex system), but now we have the impenetrability of a complex system within the scope of a single process. It's hard to figure out generalizable principles about this kind of thing! reply FuckButtons 5 hours agoparentprevI mean, it’s kind of in the name isn’t it? Computer science. Science is empirical, often poorly understood and even the best theories don’t fully explain all observations, especially when a field gets new tools to observe phenomena. It takes a while for a good theory to come along and make sense of everything in science and that seems like more or less exactly where we are today. reply manojlds 14 hours agoparentprevDivine benevolence reply amelius 12 hours agoparentprevAI is more like gardening than engineering. You try things without knowing the outcome. And you wait a very long time to see the outcome. reply CamperBob2 13 hours agoparentprevThis can be laid at the feet of Minsky and others who dismissed perceptrons because they couldn't model nonlinear functions. LLMs were never going to happen until modern CPUs and GPUs came along, but that doesn't mean we couldn't have a better theoretical foundation in place. We are years behind where we should be. When I worked in the games industry in the 1990s, it was \"common knowledge\" that neural nets were a dead end at best and a con job at worst. Really a shame to lose so much time because a few senior authority figures warned everyone off. We need to make sure that doesn't happen this time. reply spidersenses 12 hours agorootparentWhat is the point you're trying to make? reply CamperBob2 11 hours agorootparentWhat is the point you're trying to make? Answering the GP's point regarding why deep learning textbooks, articles, and blog posts are full of sentences that begin with \"We think...\" and \"We're not sure, but...\" and \"It appears that...\" What's yours? reply stormfather 12 hours agoparentprevIt's how God programs reply jejeyyy77 13 hours agoparentprevit's a new paradigm reply denysvitali 14 hours agoprevLoRA != LoRa. I keep on getting confused and hate that they chose to reuse an existing acronym reply daemonologist 13 hours agoparentLikewise. My day job is machine learning and I still, or maybe consequently, do a double-take every time I see the acronym with minimal context (like on the HN front page, where either usage would be normal). reply travisgriggs 9 hours agorootparentAnd my day job involves a lot of LoRa. I always do a double take on these. I'm grateful that at least the caps is now being done differently. reply blopp99 8 hours agoparentprevI hate the trend of software guys naming things after hardware related stuff reply esafak 8 hours agoparentprevThat's what happens when people specialize and don't pay attention to what's going on outside their bubble. reply HKH2 7 hours agorootparentA quick websearch could fix that. reply sbrother 13 hours agoparentprevWait, what is the meaning other than \"Low-Rank Adaptation\"? It's hard to google the difference. reply cristoperb 13 hours agorootparentIt's the name of a \"Lo\"ng \"Ra\"nge wifi-like technology: https://en.wikipedia.org/wiki/LoRa reply boolemancer 13 hours agorootparentprevI assume the radio technology: https://en.wikipedia.org/wiki/LoRa reply sschueller 14 hours agoparentprevIt's unfortunate that those two so far unrelated technologies have the same acronym. reply girvo 8 hours agorootparentLoRa the radio tech was first, so as far as I'm concerned it's the canonical definition of the acronym. But I'm biased, I'm an embedded firmware dev reply chenxi9649 14 hours agoprevIt's still not too clear to me when we should fine tune versus RAG. In the past, I used to believe that finetuning is mostly for model behavioral change, but recently it seems that certain companies are also using fine-tuning for knowledge addition. What are the main use cases for fine tuning? reply rasbt 14 hours agoparentI think the main use case remains behavior changes: instruction finetuning, finetuning for classification, etc. Knowledge addition to the weights is best done via pretraining. Or, if you have an external database or documentation that you want to query during the generation, RAG as you mention. PS: All winners of the NeurIPS 2023 LLM Efficiency Challenge (finetuning the \"best\" LLM in 24h on 1 GPU) used LoRA or QLoRA (quantized LoRA). reply CuriouslyC 13 hours agoparentprevFine tuning is better than RAG when the additional data isn't concise, or requires context. This is because too much context (or \"unfocused\" context) can dilute prompt following behavior, and RAG doesn't help the model with higher order token associations so you have to get lucky and pull what you need from the augmentation material, at which point it's not much better than a fancy search engine. Of course this is mostly an issue when you're dealing with a specialized corpus with its own micro-dialect that isn't well represented in public data sets, such as with government/big corporation internal documents. reply ignoramous 14 hours agoparentprevFrom what I gather, fine-tuning is unreasonably effective [0] because in-context learning really depends on how powerful the underlying model is and just how you do RAG (process queries, retrieve embeddings, rank outcomes, etc [1]). Per this paper I read, fine-tuning may add new domain knowledge (but as another commenter pointed out, knowledge is better represented from data of the pre-training stage) or boost specific knowledge; while RAG is limited to boosting only; nevertheless, both techniques turn out to be similarly capable with different trade-offs [2]. -- [0] Fast.ai: Can Models learn from one sample, https://www.fast.ai/posts/2023-09-04-learning-jumps/ / https://archive.is/eJMPR [1] LlamaIndex: Advanced RAG, https://blog.llamaindex.ai/a-cheat-sheet-and-some-recipes-fo... / https://archive.is/qtBXX [2] Microsoft: RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study, https://arxiv.org/html/2401.08406v2#S6 / https://archive.is/UQ8Sa#S6 reply pizza 6 hours agoparentprevThese are autoregressive models. When you have a new type of sequence where future elements are able to be predicted from previous parts of the sequence, but in a new kind of way than the models have seen before, it would make sense to finetune. Admittedly, that's a pretty vague descriptor for how to decide what to do for a given data scenario, but it might be good enough as a rough heuristic. Now, whether knowledge addition falls under that, might be a question of taste (without experiments). reply somethingsome 12 hours agoprevNice article, I'm not in this field, however, my understanding of the original paper was that the LoRA was applied only on the last dense layer, and not to all independently (maybe I misread it originally). Digging a bit in why the implementation is like this in the link, I found that in QLoRA they used this and it seems to have some interesting effects, maybe adding a note on the QLoRA decision would be nice :) I'm not sure I understand why it works though, my neophyte view was that applying LoRA to the last layer made sense, but, I do not wrap my mind on the rationale of applying it repeadly to each linear layer. Can someone explain their intuition? reply icyfox 12 hours agoparentLike most things in ML, the answer of which layers to use come down to empirical evidence more than theory. In a typical Lora training pipeline, you freeze the contents of the base model and just adjust the Lora layers. The more layers you convert to lora layers the more degrees of freedom you have for the optimization. There are some finetuning regimens that only recommend finetuning the last layer since this is theorized to have the \"highest order\" representation of the inputs. Other training regimens will finetune all layers. It's largely data and problem dependent. Lora just mirrors this convention. reply somethingsome 10 hours agorootparentYeah, but if I remember correctly the paper, LoRA followed the logic that only the last layers on a llm changed drastically during finetuning, and the layers above remained almost unchanged, so it made sense to alterate only the last ones, breaking this by adding a LoRA at each linear layer doesn't seem to follow the logic of why LoRA was created and why it works. reply icyfox 9 hours agorootparentWell, Lora works just because it's a low rank approximation of full updates - much in the same way that SVD works, and regular gradient updating works. It delivers good results by both acting as a regularizer and by allowing larger models to be updated with smaller memory footprints. My point is that the original Lora paper choosing the last layer is one choice. And it is likely the most common one because of its higher symbolic nature typically being all that's needed for good performance on downstream tasks. Depending on the size of your finetuning job I've personally seen updating more layers (or updating some only on a certain learning rate schedule) to be more effective. Lora is just the mathematical technique of updating, it doesn't really have a hypothesis on the ideal training regimen. reply somethingsome 5 hours agorootparentThanks, I'll meditate on that and re read the paper with this view in mind. The last sentence makes sense to me, if the finetuning job changes significatively more the weights of other layers than just the last one, it is kinda normal to to use Lora on them. I had the impression that it was rarely the case, but I must be mistaken. I'll think about applications where this is the case. reply ijhuygft776 15 hours agoprevI wish the wireless LoRa protocol would be open source... reply tussa 2 hours agoprevIt's cheap and sleazy to steal a name from another project to ride it's fame. reply jamesblonde 14 hours agoprevI prefer the not from scratch, but from configuration approach by Axolotl. Aolotl supports fine-tuning mistral, llama-2, with lots of the latest techniques - sample packing, flash attention, xformers. I concentrate on collecting and curating the fine-tuning data, do \"data-centric\" fine-tuning - not learning LoRA from scratch. reply wfalcon 11 hours agoparentthis is also what our (Lightning AI) lit-gpt library does. https://github.com/Lightning-AI/lit-gpt reply jamesblonde 3 hours agorootparentThanks, hadn't seen this. reply mintrain 10 hours agoprevI've added an exercise to practice implementing the LoRA forward pass from scratch: https://tensorgym.com/exercises/17 The idea behind LoRA is beautiful, and the implementation is pretty straightforward. reply Rudeg 9 hours agoparentnice, looks very cool and useful! I'll definitely try it! reply yandrypozo 13 hours agoprevgotta say naming is hard I thought this was about LoRa (from \"long range\") or LoRaWAN, the IoT sensors communication. reply helloericsf 10 hours agoprevHN friends, What are the most popular libraries for fine-tuning? (Not from scratch) reply jasonjmcghee 8 hours agoparenthttps://github.com/OpenAccess-AI-Collective/axolotl reply fnordfnordfnord 7 hours agoprevI thought this was going to be some neat software defined radio stuff. Still quite interesting though. reply z3ugma 7 hours agoparentit's all about whether the 'A' is capitalized or not. LoRa - radio LoRA - machine learning reply broabprobe 15 hours agoprevwow definitely thought this was about LoRa at first. reply huqedato 15 hours agoprevExcellent and practical example! I'm curious if there's a comparable one using Julia or JavaScript. reply facu17y 13 hours agoprevWhat's the performance penalty of LoRA? reply rasbt 13 hours agoparentDuring training, it's more efficient than full finetuning because you only update a fraction of the parameters via backprop. During inference, it can ... 1) ... be theoretically a tad slower if you add the LoRA values dynamically during the forward pass (however, this is also an advantage if you want to keep a separate small weight set per customer, for example; you run only one large base model and can apply the different LoRA weights per customer on the fly) 2) ... have the exact same performance as the base model if you merge the LoRA weights back with the base model. reply andy99 15 hours agoprev\"From scratch\" seems to be a matter of opinion. \"Pure pytorch\" maybe, except it uses HF transformers. So it's LoRA on top of common frameworks... reply rasbt 15 hours agoparentYeah, the LoRA part is from scratch. The LLM backbone in this example is not, this is to provide a concrete example. But you could apply the exact same LoRA from scratch code to a pure PyTorch model if you wanted to: E.g. class MultilayerPerceptron(nn.Module): def __init__(self, num_features, num_hidden_1, num_hidden_2, num_classes): super().__init__() self.layers = nn.Sequential( nn.Linear(num_features, num_hidden_1), nn.ReLU(), nn.Linear(num_hidden_1, num_hidden_2), nn.ReLU(), nn.Linear(num_hidden_2, num_classes) ) def forward(self, x): x = self.layers(x) return x model = MultilayerPerceptron( num_features=num_features, num_hidden_1=num_hidden_1, num_hidden_2=num_hidden_2, num_classes=num_classes ) model.layers[0] = LinearWithLoRA(model.layers[0], rank=4, alpha=1) model.layers[2] = LinearWithLoRA(model.layers[2], rank=4, alpha=1) model.layers[4] = LinearWithLoRA(model.layers[4], rank=4, alpha=1) reply michaelnny 3 hours agoparentprevIf anyone is interested in a more 'pure' or 'scratch' implementation, check out https://github.com/michaelnny/QLoRA-LLM. (author here) It also supports 4-bit quantized LoRA, using only PyTorch and bitsandbytes, without any other tools. reply 2024throwaway 15 hours agoparentprevThis apple pie recipe claims to be from scratch, but they cooked it in an off the shelf oven. So it's from scratch on top of the universe... reply dymk 16 hours agoprevNot to be confused with LoRa (\"long range\"), a radio communication protocol. At first I thought this could be about using LLMs to find optimal protocol parameters, but alas. reply OJFord 16 hours agoparentIt's the first thing that comes to my mind too, but this is mentioned in every thread (and there are far more of them for LoRA than LoRa atm), and in this case there's unlikely to be much confusion because it starts by spelling out the acronym: 'LoRA, which stands for Low Rank Adaptation, [...]'. reply cpfohl 16 hours agoparentprevI had the exact same confusion reply the__alchemist 15 hours agoparentprevConcur; or at least don't use a mix of lower and upper-case, like the radio. I think there would be less mis-assumptions if they had called it \"LORA\", \"Lora\", \"lora\" etc. \"LoRA\" is asking for trouble. reply rasbt 16 hours agoparentprevHah, yeah that's LoRA as in Low-Rank Adaptation :P reply thelastparadise 15 hours agoparentprevThis caught me off-guard as well. I really wish they could have used abother acronym. reply gourabmi 15 hours agoprev [–] Someone somewhere is already working on naming their project Lehsun.. /s reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The phrase \"Login Start free\" suggests the existence of a free login option."
    ],
    "commentSummary": [
      "The debate is centered around the implementation and optimization of LoRA in machine learning models, with differing opinions on whether it should be applied to all layers or just the last layer.",
      "There is confusion regarding the acronym LoRA, as it can be mistaken for the LoRa radio protocol.",
      "The discussion also explores the distinction between engineering and discovery in AI, the lack of theoretical foundations for intelligence, and the challenges of establishing generalizable principles.",
      "Various use cases for fine-tuning in machine learning are examined and compared to retrieval-augmented generation.",
      "The conversation concludes with a lighthearted comment about potentially naming a project \"Lehsun.\""
    ],
    "points": 297,
    "commentCount": 74,
    "retryCount": 0,
    "time": 1705942580
  },
  {
    "id": 39097314,
    "title": "Czech Republic Sets June 6, 2032 as End Date for IPv4 Services",
    "originLink": "https://konecipv4.cz/en/",
    "originBody": "cs en Czech republic sets IPv4 end date On 17 January 2024, the Government of the Czech Republic approved the material \"Restarting the implementation of DNSSEC and IPv6 technologies in the state administration\". On the basis of this decision, the Czech state administration will stop providing its services over IPv4 on 6 June 2032. Thus, the Czech Republic knows its IPv4 shutdown date. Download the resolution Revolutionary restart: DNSSEC and IPv6 transform czech state administration The IPv4 protocol, a key element of Internet communication since the 1980s, has effectively served to allocate unique addresses for devices connected to the Internet. However, as the number of devices and services increased, IPv4 became insufficient due to the limited number of addresses. The transition to IPv6, which offers an almost unlimited number of addresses, is necessary to ensure the scalability, security and efficiency of the Internet infrastructure. Saying goodbye to IPv4 will be done 3056 days 13 hours 57 minutes 16 seconds",
    "commentLink": "https://news.ycombinator.com/item?id=39097314",
    "commentBody": "Czech republic sets IPv4 end date (konecipv4.cz)269 points by deadbunny 10 hours agohidepastfavorite260 comments decasia 8 hours agoOK so — I just migrated my personal website to use IPv6 only. It runs on AWS EC2, and my motivation was purely to save on paying the IPv4 fee that AWS is introducing next week. I put it behind Cloudflare which can provide an IPv4 proxy address for site visitors. But the thing is, even though IPv6 is such well established tech, it was not a great migration experience. I'm not a network engineer, and you have to figure out how to update a bunch of networking configuration to make this switch on an existing instance, with lots of potential footguns. Even after I got the IPv6 address provisioned and the virtual networking correctly configured, the connectivity was still broken until I found an obscure security group setting that was still set to allow only IPv4 traffic. A lot of basic debugging tools like curl are designed to use IPv4 by default. You have to figure out how to get your webserver (nginx in my case) to listen on the IPv6 interface, which is fiddly. And it's easy to inadvertently break ssh connectivity, which turns out to also be set up to expect IPv4 traffic only... In the end, it took about 90 minutes of fiddling, including migrating to Cloudflare, fixing DNS, etc. I mean, it's fine. I learned some things. But if we're all going to switch to an IPv6 world, it would really be nice if the systems and tooling made it slightly easier, somehow. reply jeroenhd 8 hours agoparentThe thing is, with my mediocre $5 VPS, all I needed to do was copy four lines of config files (as documented by the VPS provider) and open a port in the firewall on the server itself, just like on IPv4. Amazon is horribly behind on a lot of things (just look how long it took them to get DNSSEC working, and it took them a couple of tries to not break entire domains for people using it!). They're ahead of Azure, but that's about the best you can say about that. I suppose it makes sense, IPv4 addressing is now an additional method to make money for Amazon, so why make it easy for customers to migrate? reply decasia 8 hours agorootparentYep, it crossed my mind that AWS has no incentive to optimize this particular process since they have planned to monetize it :/ reply zokier 3 hours agorootparentAlso their sizeable ipv4 stockpile is kind of a moat and competitive advantage; it's pretty much guaranteed that no other cloud provider can reach their scale while ipv4 remains dominant. So they have some interest in delaying ipv6. It is quite perverse considering that ipv6 really should be quite ideal for hyperscale cloud. reply Animats 8 hours agorootparentprevYeah. That's a problem. A big problem. If AT&T and AWS went IPv6 by default, most of the US would convert over quickly. reply joecool1029 7 hours agorootparent> If AT&T and AWS went IPv6 by default, most of the US would convert over quickly. Comcast and the mobile carriers went ipv6 years ago, that's already something like over half the US endpoints? It hasn't sped adoption (Github STILL isn't ipv6 for git). Another example, I've been trying to get Georgia Tech to fix their ipv6 linux mirrors for over half a year, they advertise they support ipv6 and publish AAAA records that don't work. http://rsync.gtlib.gatech.edu/an obscure security group setting that was still set to allow only IPv4 traffic Which setting was that? I've had 0 issues with IPv6 on EC2 for a while now. Not all services support v6, though, but that's a separate issue. reply kiririn 3 hours agoparentprevThis is an AWS/cloud issue rather than an IPv6 issue. With any decent VPS provider ipv6 just works. AWS needs better defaults reply miohtama 28 minutes agoparentprev> I found an obscure security group setting that was still set to allow only IPv4 traffic This has more to do with AWS, less with IPv6. reply doctorpangloss 4 hours agoparentprevIPv4 is to AWS what iMessage is to Apple. Actually, why doesn’t Apple mandate the IPv6 transition for iOS? reply JimDabell 4 hours agorootparentThey have required that iOS apps must work in an IPv6-only environment for years. reply p1mrx 3 hours agorootparentThat was an interesting decision. At roughly the same time, Android added 'clatd' so existing apps would just work in an IPv6-only environment via NAT64. Apple refused to implement clatd, instead forcing app developers to fix their own code. More work upfront, but now iOS apps should have an easier time transitioning their server-side components to IPv6, compared to Android apps. reply shedside 1 hour agoparentprevFWIW, I'd love to see a blog post or similar that would lead me through the steps to do precisely this – including the nginx config. reply amarshall 7 hours agoparentprevcurl, and likely many other tools, do not use IPv4 by default, but rather quickly and seamlessly fallback (“happy eyeballs”). reply wg0 7 hours agoparentprevAnd if I'm not wrong, AWS VPCs only support IPv4. reply dangus 7 hours agorootparentYou can do dual stack with IPV6-only subnets: https://aws.amazon.com/blogs/networking-and-content-delivery... reply krupan 8 hours agoparentprev\"even though IPv6 is such well established tech\" It's actually not, and I think you learned that, and why it's not, in this process. Sorry. reply newsclues 7 hours agorootparentIntroduction December 1995; 28 years ago IPv4 is 43 years old reply magicalhippo 12 minutes agorootparentAnd? IPv6 is still under rather heavy development with significant changes still being made. As a user, I wouldn't call it well established because it's shifting too much still. reply o11c 6 hours agorootparentprevJune 6, 2012 is a better date; that was the \"World IPv6 Launch Day\". But even that was negligible since nobody actually had IPv6 connectivity. Even now I still don't have a home wifi router that supports it, though at least my ISP does since last year. The real driver of IPv6 was the rise of smartphones. reply ben_w 40 minutes agorootparent> June 6, 2012 is a better date; that was the \"World IPv6 Launch Day\". 2012? Wow, that's pretty bad. We were talking about how ridiculously slow IPv6 rollout was while I was at university, and that was 2002-06. reply apearson 5 hours agorootparentprevWhat router do you use that doesn't support IPv6? reply o11c 4 hours agorootparentIt's a tp-link Archer, and since it supports Wi-Fi 6 it must have been from 2019 or later. Hmm, I just went deep into the settings and there's an IPv6 toggle (disabled by default, and requires playing with menus to make it actually work) now. I don't remember seeing that (and I explicitly checked) when we switched the ISP-side last April. But the firmware is dated after that, so I choose to trust my memory. Actually trying to connect to ipv6 stuff besides the router itself (in fd00::/8, that's good) just gives \"Destination unreachable: Beyond scope of source address\" though (probably because I only have an fe80 link-local address though?). If I play with the settings some more I get a ::/64 address, but that just gives me a black hole ... wait, is that even a legitimate address? Do I need to fill in the prefix manually or something? I'll play with this some more in the morning I guess. But I certainly wouldn't expect a normal user to get IPv6 working under these circumstances if I haven't figured it out yet ... reply apearson 4 hours agorootparentYou're trying too hard Turn IPv6 with stateless RDNSS on (should be the default on all modern routers) ping6 google.com That's it as long as your ISP supports IPv6. No need to figure out a fd00::/8 that you maybe may not have. reply mort96 14 minutes agorootparentI still don't get why we need \"6\" versions of all the utilities. The fact that you need to use ping6 etc is a failure in itself. reply o11c 3 hours agorootparentprevThat doesn't work any more than anything else. I'm testing with `ping ipv6.google.com` as well as the router IPs. Between each change, I disconnect from wifi. The router itself has a ping tool and can ping ipv6.google.com just fine. So I assume the upstream options are correct. For the LAN, I have 4 options: * ND Proxy gives me an fd/8 address. I can ping the router via its fd/8 or fe80 address, google blackholes. * DHCPv6 gives me an ::/64 address. I can ping the router via its fd/8 or ::/64 addresses, google blackholes * SLAAC + Stateless DHCP does not give me any address (I still have the default fe80 address). I can ping the router via its fd/8 or ::/64 addresses, google gives Destination unreachable: Beyond scope of source address * SLAAC + RDNSS does not give me any address (I still have the default fe80 address). I can ping the router via its fd/8 or ::/64 addresses, google gives Destination unreachable: Beyond scope of source address In all cases, the IPv6 addresses the router says are its DNS servers blackhole. I do have working DNS returning IPv6 address for google though; presumably because the ipv4 DNS server it advertises (which is the router itself) still works. If I bypass the wifi router, the ISP router gives me both an address in both fd/8 (in the same /64 as the wifi router gets assigned, which makes sense) and in 2607/16, besides the usual fe80. The ISP router is really bad, all it has is an \"it's connected\" indicator and a bunch of phone numbers and URLs for support. Still bypassing, pinging `ip6-allnodes` gives me 3 responses, all with fe80::/64 addresses: my computer, an unknown, and the wifi router; I can ping those addresses, as well as the wifi router's fd/8 address. Maybe I should play with the router's upstream settings ... \"Get IPv6 address\" has auto, slaac, dhcpv6, non-address ... since I can ping it from outside that has to be right. If I disable \"Prefix delegation\" the ::/64 box is editable but it complains about literally anything entered. And I don't have anything meaningful to manually enter a DNS address. Hm, I just noticed that the last bit of the router's address varies between some of its addresses ... reply Semaphor 4 hours agorootparentprevMy router supports it, but for my ISP asking for ipv6 risks getting put into a CGNAT for v4, so I choose to live without v6. reply kragen 7 hours agorootparentprevin december 01995 zero nsps and zero isps supported ipv6. you had to be on the 6bone, which didn't even exist until march 01996. also you probably had to write your own protocol stack because there wasn't an off-the-shelf one for your platform. in december 01995 what you had was a paper spec reply Throw84949 3 hours agorootparentprevI disable ipv6 on my network, way too many security exploits! reply sgjohnson 28 minutes agorootparentSuch as? reply Throw84949 22 minutes agorootparentI don't have vulnerability ids. What made me disable it was some issue in Linux network stack, with ipv6 broadcast, on by default, exploitable to root execution. For me it is yet another complex service that I do not need, and that should not be exposed to network. Ipv4 network stack code is far smaller, simpler and way more tested over decades! reply Animats 7 hours agoprevConversion to IPv6 is an item in the 14th Five Year Plan (2021-2025) of the People's Republic of China.[1] \"We will accelerate the large-scale deployment of 5G networks, increase the user penetration rate to 56%, and promote the upgrade of gigabit optical fiber networks. We will build up technology reserves for the future deployment of 6G network technology. We will expand backbone network interconnection nodes, set up a number of new international communication gateways, and comprehensively promote the commercial deployment of Internet Protocol Version 6 (IPv6).\" There is a migration schedule.[2][3] As of this month, there are supposed to be no new IPv4 services. The goal is IPv6 only by 2030. Actual adoption in China is reportedly only 25-30%, though. [1] https://cset.georgetown.edu/wp-content/uploads/t0284_14th_Fi... [2] https://blog.apnic.net/2019/06/06/100-by-2025-china-getting-... [3] https://www.theregister.com/2023/04/28/china_ipv6_control_ad... reply bdd8f1df777b 7 hours agoparentI grow up in China seeing news about the policy of IPv6 adoption every year, but the real adoption rate is still abysmal here. reply ggm 4 hours agorootparentIt is visible in the APNIC Labs measurement at 30% and within different AS of China Mobile at 65% or better, it depends province by provice, and provider by provider. https://stats.labs.apnic.net/ipv6/CN I don't call this abysmal at all. It very possibly is higher, there are reasons why APNIC may undercount, but noting that APNIC and Akamai tend to agree on their numbers. Google's own numbers for China are far lower for reasons which do not affect the APNIC or Akamai data. reply thrdbndndn 3 hours agorootparentMajority of them are mobile phones IIRC. Not nothing, but the adoption rate of IPV6 on computer side is still very low. reply justworkout 1 hour agorootparentA lot of people, including young people, throughout Asia don't have computers. A large number of working adults our age (however old you may be) don't touch desktop or laptop computers at all on their day to day. A huge number of people only have phones in their home and only use phones for their work. Plenty of companies also use tablets with specialized apps in places where a western company would use a PC. Finding 20-30 somethings in Asia who are educated and employed who don't know how to use a mouse isn't uncommon. I've encountered it loads of times with new employees in various offices. reply ggm 2 hours agorootparentprevVirtually all of India is phones and tablets. It's the highest ipv6 economy. If your criterion is \"not handheld devices\" then I ask, what makes desktop computers so special? I believe 80% or more of the planets internet is handhelds. You think the US home users on comcast are all PC's? reply thrdbndndn 1 hour agorootparent> what makes desktop computers so special More on the technical side. Most of mobile devices and their modems need (or even allow) zero to no manual configuration, so adopting ipv6 with these hardwares are easy -- the ISP just need to deal with their side. This is in contrast to \"fixed-line\" internet devices like computers and their routers. reply nerdbert 6 hours agorootparentprevIs everything NATted? With such a high population it seems hard to manage with limited IPv4 addresses. reply geraldhh 42 minutes agorootparentyep reply torginus 1 hour agorootparentprevthat's surprising to me considering the vast majority of Chinese netizens have come online way after IPv6 was determined to be the future. Also, in a country of more than a billion people, most of whom use the internet with at least 1 device, IPv4 quickly leads to address shortages. reply geraldhh 44 minutes agorootparentipv4 already supports billions of devices with millions of addresses. china surely knows about cgnat and rfc1918 reply ErneX 8 minutes agorootparentprevCGNAT reply j16sdiz 5 hours agorootparentprevI thought they were very accessible in the education (i.e. university) network. reply imp0cat 3 hours agorootparentprevSeems to me that it's the same everywhere. It's not that there are no new IPv6 deployments, it's just that the rate of change is slow. reply omeid2 5 hours agoprevSomething that isn't talked much about IPv6 and I believe has subtle and indirect influence on it's adoption is that it is far far far less human-readable than IPv4. You can't find me a single sysadmin or casual user who would look at an IPv6 and say, nice, I prefer that one over an IPv4 or find it easier to type or communicate. You can argue about merits of IPv6 and very reasonably so all day long, but this fact remains. I don't think governments or other institutions can strong-arm IPv4 out of existence. I don't know what the long term solution would look like, but I strongly doubt it will be IPv6-only. Ever. reply preisschild 1 hour agoparentI'm a sysadmin and find IPv6 easier to use. You can easily make short ipv6 address just by having a larger prefix and then omitting zeroes (with `::`) in your host-part that you don't need. Take `2001:db8::1`. Here `2001:db:8` would be your network address and `1` your host-id. For example, a router. `2001:db8:2` could be a server and so on. reply jl6 6 minutes agorootparentI don't imagine it's easy to get a global /32, so do you mean for purely local networks? In which case why not use 1::1? reply porbelm 31 minutes agoparentprevAs a private user, as long as I remember the prefix assigned to my connection I much prefer having a stable [prefix]::dead:beef or ::f007:1 and ::f007:2 etc compared to the ever-changing 85.nnn.nnn.nnn v4 address. Or simply just /think/ of it as a /112 subnet with [prefix]::0001-ffff available - still room for 65535 addresses in that one, more than I'll ever need. No need to rely on long autogenerated SLAAC addressses if you don't want to, you can have whatever you want after your prefix and you don't have to pad it out with garbage random-ish values. A sysadmin can get creative :) Of course your ISP will need to have proper v6 support. reply notatoad 5 hours agoparentprev\"IPv6 addresses are ugly\" is only an argument for as long as ipv4 addresses don't cost anything. as soon as they put a price on it, nobody's going to care. reply omeid2 4 hours agorootparentArguing that \"we can price IPv4 out\" only reinforces my point; of course, ignoring the fact that it is a pipe-dream. But the fact remains, no one really prefers IPv6 over IPv4. reply Faark 2 hours agorootparentJust like users prefer domain names over ip's. But they are necessary. reply timthelion 1 hour agorootparentprevI started to prefer it after I foundout that with IPv6 every Docker container can have its own IP address and this can be used to solve the hairpin NAT problem. reply imoverclocked 3 hours agorootparentprevI do. reply crotchfire 4 hours agorootparentprevThat happened a long time ago. And yet still, nobody cares. reply notatoad 4 hours agorootparentAWS starting to charge per-address next week is making a whole lot of people start caring. reply omeid2 3 hours agorootparentThere is over 1 billion users with less than 5% on IPv6 in China alone. Some 200 million users in Indonesia with less than 15% on IPv6. Hundreds of million of users in Africa that are growing at a very fast pace, still on IPv4. In Europe outside Germany and Belgium, the IPv6 usage is between 5-20% max. Millions of people. There is also millions of industrial systems, ATMs (heck, some of them still run XP!), and whatnot that will not get IPv6 short of complete replacement. You can't price IPv4 out with that kind of numbers combined with the fact that it is an extremely open market. reply dn3500 3 hours agoparentprevThe original idea was that configuration would be so automatic that humans would never see the addresses. Instead of an admin assigning a name and IP address to each host, the admin would only assign the name. The host would figure out its local topography from Neighbor Discovery then tell the dns where it's connected. I'm not sure how well that worked out in practice. reply mort96 12 minutes agorootparentThen why do I still have to see the addresses? ... how would configuring DNS even work without having to see the addresses, and how would SSH'ing to a device on the LAN work without seeing the addresses reply speeder 2 hours agorootparentprevIt didn't work in practice. In many situations names don't work. One example would be games with local lan support, and you use IP addresses to make sure you are connecting to the right server. Other would be tiny embedded devices, that are not powerful enough to scan the network and need a hard-coded or user configurable IP address to connect to the right place. reply sgjohnson 1 hour agorootparent> that are not powerful enough to scan the network just look at the ND table. Nobody is ever going to scan a single /64 of v6, ever. reply t0bia_s 1 hour agoparentprevHoe does it affect DNS adblocking? reply geraldhh 38 minutes agorootparentipv6 should render any use of static blocklists moot, but i would not hold my breath for blocklist vendors to go out of business reply loup-vaillant 1 hour agoparentprev> Something that isn't talked much about IPv6 and I believe has subtle and indirect influence on it's adoption is that it is far far far less human-readable than IPv4. No shit Sherlock: an IPv4 is 4 bytes. An IPv6 requires 16. This is four times as much data, it has to be less readable, that’s just the price of a bigger address space. Now one could have argued that 8 bytes, heck, even 6 bytes, would have been enough. I guess it would have, but having a /48, or even a /64 range, per user, is quite convenient: we can ditch the NAT (we probably won’t, but we can). But even then, 8, or even 6 bytes, remain harder to read than 4. reply vasco 55 minutes agorootparentThe number of bytes isn't what makes it hard, \"habanero eardrum\" is 16 bytes and much easier to remember, there probably is a way to make ipv6 addresses easier to use, but hard to do now. reply porbelm 24 minutes agorootparent[prefix]::1 [prefix]::2 ... [prefix]::ffff Or if you want mnemonics on your /64: [prefix]::dead:beef:1 [prefix]::b00b:1 [prefix]::b00b:2 etc reply darren_ 3 hours agoparentprev> Something that isn't talked much about IPv6 and I believe has subtle and indirect influence on it's adoption is that it is far far far less human-readable than IPv4 What? This comes up in like 95% of ipv6 threads on HN (see also: ‘they should have just added an extra quad to v4 addresses’) reply justsomehnguy 2 hours agorootparentMy grand father used the dots in the address! My father used the dots in the address! I used the dots in the address! Henceforth every human being should use the dots in the address till heat death of the universe! reply Aeolun 1 hour agorootparentI'm certain the issue here isn't the dots. 123.123.238.217.110.100.1.1 would still have been perfectly readable. fdd2:1228:3372:1sdf meanwhile is pretty unreadable even at IPv4 length. reply throwaway71271 58 minutes agorootparentipv6 is 16 bytes, so more like 123.123.238.217.110.100.12.255.123.123.238.217.110.100.242.176 reply nijave 7 hours agoprevIn the U.S. I run into issues most commonly with businesses. Consumer ISPs and mobile operators all seem to have IPv6. Hosting providers (and recently cloud providers...) all have dual stack or v6 only. However I don't think I've ever worked anywhere that actually had an IPv6 network (at least on the office side) reply wredue 6 hours agoparentIf I even turn on IPv6 in my network, my VPN to work stops working entirely. Supremely annoying. reply geraldhh 36 minutes agorootparentprobably missing v6 default route over the tunnel reply a1o 8 hours agoprevMinor question, does using IPv6 makes it easier to track people than when using IPv4? reply chungy 8 hours agoparentIn general, no. All operating systems default to using random host bits in SLAAC addresses, each bootup (or change of network interface) will instantiate a new address. More than that though, IP addresses in both v4 and v6 varieties are unreliable sources for tracking and basically no actor you'd want to guard against depends on them anyway. With mobile phones being common, your public addresses are changing all the time anyway. On IPv4, CGNAT is becoming increasingly more common so that you might share a public IPv4 address with thousands of other people at the same time. reply nerdbert 6 hours agorootparentSLACC dynamic address are basically the same as IPv4 NAT from this perspective. Everyone who cares has tables of the prefix size assigned to customers in various providers' IP space. reply comex 8 hours agorootparentprev> On IPv4, CGNAT is becoming increasingly more common so that you might share a public IPv4 address with thousands of other people at the same time. Which is an advantage from an anti-tracking perspective — an advantage you won’t get with IPv6, no? reply orangeboats 7 hours agorootparentA counterpoint is that by cycling through IPv6 addresses no one can ever tell who is who by addresses alone. Okay, they could probably tell from the IPv6 prefix because the entire company/household shares it, but people used to (\"use to\" if you live in a Western country, probably) share the same public IPv4 address too so I find that point rather moot. Another very strong counterpoint to this is that, you can't really build a truly-P2P network nor self-host a service on Internet, when everyone is behind CGNAT. At some point, as IPv4 resources get scarcer, only corporates will have the ability to host services on the Internet, and I don't think it is in their interests to host Tor nodes, for example... reply lmm 4 hours agorootparentprev> Which is an advantage from an anti-tracking perspective Depends. It makes it harder for someone outside your ISP to track you, but it makes it easier for your ISP to track you, and harder for them to justify not keeping logs of where you've connected to (since that data is necessary for their CGNAT system to work). reply bdd8f1df777b 7 hours agorootparentprevYou get a new IPv6 address very quickly (mostly every day or every week). That makes tracking difficult too. While the prefix is mostly stable, that is akin to the public CGNAT address. reply wmf 6 hours agorootparentIt's not really, since an IPv6 /64 prefix has one customer behind it while a CGNAT has N customers. reply kemotep 5 hours agorootparentI mean people are just going to log into their gmail account in their chrome browser and it doesn’t matter if you change IP’s every single minute, that identity isn’t changing as often and the relevant information Google wants will be tracked. reply gmuslera 8 hours agoparentprevYes and no. With limited IPv4, many places have dynamic IPv4 in a way or another, or have IPv4 behind some sort of NAT, so the actual IP of the accessing device may be hidden, or are private addresses. Anyway, that doesn't mean that i.e. your ISP couldn't be asked what person was using certain dynamic IP at some time. With IPv6 it depends on implementation. Your device may have a public IPv6 address, persistent or not, and you don't need to be behind some sort of NAT (unless it is something to access ipv4 addresses), so if a lot of those conditions apply you might be tracked. But there is a lot of conditionals in both sides, and the elephant in the room is that you are tracked for more things than just your IP address. reply preisschild 1 hour agoparentprevDepends if you randomize the host-part or not. IPv6 has a feature called privacy extensions. https://tldp.org/HOWTO/Linux+IPv6-HOWTO/ch06s05.html reply rijx 8 hours agoparentprevFor some people, but most people are very easily trackable using IPv4 + user agent. There are even better JS fingerprints that don’t use your IP. reply geraldhh 33 minutes agoparentprevyes, because even the network part has double the entropy than an v4 address and there probably is no nat involved. practically irrelevant thou, because your browser is a much better source of fingerprinting data reply 1ncorrect 8 hours agoparentprevIt can in the near term, as each endpoint has a unique public identifier instead of being mixed with multiple endpoints behind a NAT gateway, but less so over the long term, as privacy addresses are typically rotated at least daily. Of course, all of this needs to be included in a broader discussion around myriad vectors of tracking and fingerprinting to arrive at a meaningful conclusion. reply p1mrx 5 hours agoparentprevYou can be tracked with either, but IPv6 is not functionally worse than IPv4 in that regard. You could argue that IPv6 is less anonymous than CGNAT, but CGNATs usually have to keep detailed logs, so that balances out. All the tricks for anonymizing IPv4 (VPN, NAT, Tor, etc.) are equally feasible on IPv6. reply EVa5I7bHFq9mnYK 1 hour agoparentprevOf course it is, that's why conversion to IPv6 is an item in the 14th Five Year Plan (2021-2025) of the People's Republic of China reply wmf 8 hours agoparentprevIPv6 privacy addressing reduces tracking. reply j16sdiz 5 hours agorootparent... within the same prefix. reply captainkrtek 9 hours agoprevFor 2032… we got a minute reply bombcar 9 hours agoparentRight before unix time_t eh. reply kristopolous 9 hours agorootparentMight as well blow it all up at once reply autoexecbat 9 hours agorootparentprevan 8 year gap reply autoexecbat 9 hours agoprevThis is great, I hope we have more organizations follow. Not that I think we should entirely/forcefully remove ipv4, but saying that it's going away will force ipv6 migrations reply xyst 9 hours agoprevgood - it’s time to put ipv4 to pasture reply squarefoot 9 hours agoparentOr maybe keep it only for local networks? reply megous 1 hour agorootparentOn local networks is where IPv6 is the most advantageous to me right now. For the Internet use, it's mostly just more addresses to me, but on LANs, there are so many more useful features. Unique link local addresses, various multicast addresses... I can drop 10 new devices into a VLAN, and ping ff02::1%iface to enumerate them all and start communicating with them right away without any kind of configuration, not even SLAAC or DHCPv6 needed. reply qwertox 8 hours agorootparentprevWon't you always be able to use it for local networks? I like NAT and regardless of IPv6, I think I'll keep my home network as IPv4 in a NAT and completely block outgoing and incoming per-device-IPv6. reply mshroyer 30 minutes agorootparentThat won't work if you need to access IPv6-only services (as the Czech government will make theirs). In other cases it may \"work\" but perform worse than IPv6. Why the desire for a NAT? It's technically possible on IPv6 too (NAT66), but SLAAC dynamic addresses + a non-NAT firewall + possibly ULA are generally a better way to get the combination of privacy + security + local addressing I think most people associate with NAT in IPv4. reply TacticalCoder 8 hours agorootparentprev> Won't you always be able to use it for local networks? Well for me until Linux shall support IPv4 my machines shall be on IPv4 behind NAT. And yet I'm enjoying IPv6 with my ISP-provider router transparently doing the heavy lifting and using IPv6. > and completely block outgoing and incoming per-device-IPv6. Same. I do just that: it's disable by a kernel parameter, in sysctl and in the firewall, just in case I'd mess up and re-enable one or two of these. You can pry IPv4 on my LAN from my cold dead hands. reply lmm 4 hours agorootparentprevSure, if you want to make your life harder by running two different protocols that will always be an option. Why not run IPX and AppleTalk too? reply deadbunny 8 hours agorootparentprevYou know you can assign both IPv4 and IPv6 addresses to the same device? You can access things locally by IPv4 and have it talk to the internet via IPv6. Not sure why you'd want to but it's completely possible. reply demondemidi 8 hours ago [flagged]rootparentprevnext [4 more] Is IPV6 really that hard. I question why there are so many MrRobot-tier-hackers on HN who just wilt in the face of learning something new. reply bdd8f1df777b 7 hours agorootparentI've configured and preferred IPv6 everywhere. My home computer, my VPS, etc. But, local address is something I can't figure out in IPv6. reply wredue 6 hours agorootparentIt’s been a while cause work forced me to stay with IPv4, but I spent a day on this a few years ago, and I remember IPv6 being radically easier. Isn’t it just a prefix? reply krupan 7 hours agorootparentprevHave you tried it? reply Alupis 8 hours agoprevI'm ignorant on these things, so please inform me - but given IPv6's awful adoption rate, and given how foreign it's numbering scheme is to many, why can't we introduce IPv7 and just extend the IPv4 scheme with additional octets? Something like 0.0.0.0.0.0.0 and everything without the correct number of octets just gets filled with zeros. So 192.168.0.1 becomes 0.0.0.0.192.168.0.1. Then also don't give out /8's to universities and the like (repeating old mistakes). This is probably naive, but it seems it would be easier for people to use vs. 2661:919a:023e:911a:44dc:f656:233e:8816 reply Macha 8 hours agoparentBecause the problem is not the numeric representation. The problem is when you get a message from 1.0.0.0.1.1.1.1 to 0.0.0.0.192.168.0.1, and 192.168.0.1 is an IPv4 only device, what are you going to tell 192.168.0.1 about the source address? There's only a 32 bit field in the protocol. Are you going to lie and tell it it's 1.1.1.1? Congrats, you've just reinvented NAT. Enjoy your stateful boundary system and all that complexity again. Are you going to add some extension to IPv4 and try convince everyone to upgrade all their hardware, software and configurations? Then you have the exact same problem IPv6 adoption had, except IPv6 adoption is at like 45% globally and your new scheme is at 0%: https://www.google.com/intl/en/ipv6/statistics.html If you have some other idea, make sure to check it against the list of IPv6 transition mechanisms before commenting - it probably exists for IPv6, and ultimately all of them have been displaced by dual stacking: https://en.wikipedia.org/wiki/IPv6_transition_mechanism reply tomxor 8 hours agorootparent> IPv6 adoption is at like 45% globally [...] https://www.google.com/intl/en/ipv6/statistics.html > link -> The graph shows the percentage of users that access Google over IPv6 How does this work in practice, does user equipment end up with dual IPv4 IPv6? if not how do they access IPv4 only sites (of which there are still plenty)? reply orangeboats 7 hours agorootparentIt's just as you have described, this is called dual stacking and is currently the most common type of IPv6 deployment. Alternatively, cellular ISPs may make use of NAT64+DNS64. In this case, the IPv4 addresses are combined with a NAT64 prefix (most likely 64:ff9b::/96), producing addresses such as `64:ff9b::198.51.100.1`. It is effectively the same as CGNAT, but pretty much everything is single stacked IPv6 up to the IPv4IPv6 border relay in the ISP network. reply Macha 7 hours agorootparentprevThe way it's supposed to work is the ISP delegates a prefix to the user's router, the router will advertise for that subnet and client devices will use slaac to configure their ipv6 addresses. Meanwhile for IPv4 it operates as normal. For incumbent, mostly wired, Western ISPs that means a pool of IPv4 addresses and subscriber level NAT. For everyone else, that probably means CGNAT. reply Goz3rr 1 hour agorootparentprevMy ISP gives me both an IPv4 address and a heap of IPv6, most browsers will use what is called Happy Eyeballs to try both IPv4 and IPv6 at the same time and use the one that connects the fastest. reply CamperBob2 6 hours agorootparentprevAre you going to lie and tell it it's 1.1.1.1? Congrats, you've just reinvented NAT. Enjoy your stateful boundary system and all that complexity again. (Shrug) It works. What problem is being solved here, again? reply Alupis 8 hours agorootparentprev> There's only a 32 bit field in the protocol IPv6 is 128 bits... so we already made a change. Why can IPv7 not be 128 bits as well, but vastly more readable/recognizable than IPv6. Hex is hard on the eyes, especially for those who don't stare at it all day. reply Macha 8 hours agorootparentThe move to hex is to avoid people having to memorise all these offhands like \"so a /27 is x.x.x.0-31\". Instead every \"4\" in the subnet is another digit. But also do you think 24414.64517.21200.44298.46574.49887.23623.7394 is really that much easier to type? Or 231.22.96.68.14.229.38.41.242.44.72.220.156.50.232.95 if you keep to 8 bit octets? reply Alupis 8 hours agorootparentMaybe I'm weird, but I do in fact think that is easier. People memorize long numbers all the time, such as SSN, phone numbers, account numbers, credit card numbers, etc. People are not very good at memorizing lengthy alphanumeric sequences such as randomized passwords. I don't know why, but it \"just is\". But - > The move to hex is to avoid people having to memorise all these offhands Point taken. It does seem in practice though people will still have a need to memorize/recognize addresses for various purposes, such as administration/dns/etc. reply epakai 8 hours agorootparentprevMicrosoft did. The first example is pretty much what bitlocker recovery keys are. reply kazinator 8 hours agorootparentprevSpeaking of which, the slash notation has not gone away in IPv6 notation. reply ghshephard 6 hours agorootparentAt least the user networks are /64s, so, to some degree, CIDR notation isn't relevant for someone just trying to set up their system - netmask is now always 64 bits. And sure, ISP/Network Engineers still care about /56, /48, /32, etc... but that's their day, and takes about a week to have it ingrained in a way that you almost never could with IPv4. reply yjftsjthsd-h 8 hours agorootparentprev> But also do you think is 24414.64517.21200.44298.46574.49887.23623.7394 is really that much easier to type? Or 231.22.96.68.14.229.38.41.242.44.72.220.156.50.232.95 if you keep to 8 bit octets? I mean... yes, actually; I can type those blindly on a normal numpad. I'm sure somebody somewhere has made a hex number pad, but realistically on any normal keyboard entering hex means going back and forth between number and letter keys. reply krallja 8 hours agorootparentOn QWERTY layouts, you can type ABCDEF with your left hand and 0123456789 with your right hand on the num pad. reply p1mrx 5 hours agorootparentThat would be really convenient if : weren't in the middle of the keyboard. Maybe hardcore IP typists need an alternate keyboard layout where * maps to : reply chungy 8 hours agorootparentprevIf you'd like to figure out a nicer representation for IPv6 addresses, go for it. You needn't break any compatibility for it, just invent a new representation. I'd argue that the most common format such as 2001:db8::f00f is already pretty darn readable, and not hard to copy+paste when you need bare addresses. Base85 is an alternative if you'd like something that's purely optimized for fewer characters, but it only really helps with exceptionally long addresses like 2001:db8:424b:b4ff:fcb5:b643:f721:b703 becoming 9R}vSk6QzV!G$ I've come across numerous address parsing bugs because of this. To prevent that, you can write tons of test cases. IPv6 notation parsing and generating test cases are oodles of fun. reply akira2501 8 hours agorootparentprevI think two major problems that IPv6 failed to account for are the unreasonable ambiguity and \"global registry\" proposed by the Unique Local Address range, e.g. fc00::/7. This was even after having the opportunity to fix the previous mistakes in the fec0::/10 range. It was entirely inappropriate for small scale personal networking and assumed an \"administrative scope\" that's too cumbersome and without purpose for most implementations. I wonder if a dedicated \"local\" network prefix with a fixed /64 mask and an operating system recognized \"alias\" would have been a good mechanism to introduce. Something like fec0::/64 with an option to call it \"local::\". Then you might be able to more easily move from a world of 192.168.10.1, 192.168.10.2 to a world of local::1, local::2, that being translated to fec0::1 and fec0::2. reply orangeboats 8 hours agorootparentWhat do you mean by global registry? ULAs do have a local bit, by setting it to 1 (i.e., fd00::/8) you can have your own ULA range. I am using ULA in my own home network. reply akira2501 8 hours agorootparentRight, and the local bit being 0 is currently undefined, but the original and lasting idea (apparently) is to create a central registry for it, so that in some future case where you have to merge corporate networks with their own ULAs there is zero chance that there would ever be a collision between the two. Even in so far as the local bit being 1, there have been at least two attempts to create a \"voluntary global registry\" for your randomly generated prefix, mostly for the same \"collision avoidance\" stated above. In either case, somewhat unintuitively, those are still _global_ addresses, and don't have the same level of \"non local routing\" restrictions that the RFC1918 address space explicitly gained. It's been a mess, and at some point, the IETF just seems to have given up on it. reply orangeboats 7 hours agorootparent> those are still _global_ addresses They are meant to be globally _unique_ (or effectively so, by utilizing RNG you ensure that it is very, very unlikely for two organizations to have the same ULA), but they are not globally routable. reply burnerthrow008 8 hours agorootparentprev> Even in so far as the local bit being 1, there have been at least two attempts to create a \"voluntary global registry\" for your randomly generated prefix, mostly for the same \"collision avoidance\" stated above. Like most things people don't like about IPv6, you don't like this because you misunderstand how it's supposed to work. There is no need for a ULA registry because you're supposed to pick a random prefix within the ULA space. The ULA space has 7 bits already defined, so you're picking a random prefix from a space 57 bits big. The chance that you will pick the same prefix as some other specific person is effectively zero. (the birthday paradox does not apply because we don't care if your prefix matches anyone else on earth, just the one specific person/organization that you want to merge networks with). > In either case, somewhat unintuitively, those are still _global_ addresses, and don't have the same level of \"non local routing\" restrictions that the RFC1918 address space explicitly gained. Err, what? ULAs are routable and \"global\" addresses in exactly the same way that RFC1918 addresses are routable and global. They are valid addresses within the address space and the only thing that makes them \"not globally routable\" is that all routers have certain prefixes baked into the firmware as non-routable. If you don't trust your router not to route them for IPv6, why do you trust it for IPv4? reply akira2501 6 hours agorootparent> Like most things people don't like about IPv6, you don't like this because you misunderstand how it's supposed to work. Thanks for explaining my own motivations to me, while at the same time denying that any discussed changes are relevant to anyone. > There is no need for a ULA registry I was merely summarizing what the public opinion on the matter happens to currently be. Which suggests that the current recommendation to pick a random address and be happy with it is inadequate. The original posts dissatisfaction with being able to deploy IPv6 _conveniently_ also hints that improvements would be welcome by most people. > ULAs are routable and \"global\" addresses in exactly the same way that RFC1918 addresses are routable and global. RFC1918 explicitly defines it's addresses as local and not globally unique. RFC4193 specifies that these addresses are meant to be globally unique. RFC1918 requires a more specific level of filtering than RFC4193, and the definition of \"site local\" has always had some ambiguity to it. So, within the very specific and nuanced points I was trying to make, no, they're different. reply ghshephard 6 hours agorootparentAgree with the parent that there is no need for a ULA registry for locally assigned addresses under RFC 4193 (which is every one I've ever seen). But there is that option for globally assigning addresses that I'd be intrigued to see if anyone does anything with... Also important to note that while RFC1918 is roughly equal to RFC4193/ULA, that \"site local\" is another beast entirely (FECO ) that was, as you noted, ambiguous and was deprecated in rfc3879 reply ghshephard 6 hours agorootparentprevWell -that's not entirely true. RFC4193/ULA has a Local Bit that, in my experience, is almost set to 1, so the prefix is FDxx.... At a prior company we rolled out about 25 million nodes all in RFC4193 space (non-routability was a design objective) - and, just to be within spec, we did indeed grab a random /48 for each customer instance we rolled out (and made very good use of the 65K networks we had available within that /48). But - what if that Local bit is set to 0 - then: Set to 1 if the prefix is locally assigned. Set to 0 may be defined in the future. Presumably there was some thought that there might be a Global Registry that the OP was referring to? Concur with the rest- and, honestly, RFC 4193 is one of the simply written RFCs out there - understandable within 30 minute read - I'd encourage everyone to dig into it. reply deadbunny 8 hours agorootparentprevBecause 99.99% of people never see an IP address let alone interact with them. reply NoZebra120vClip 8 hours agorootparentprev> Hex is hard on the eyes Thankfully there is near-100% compliance with PTR reverse-dns for IPv6 addresses and networks. /s reply viraptor 8 hours agoparentprev> IPv6's awful adoption rate https://www.google.com/intl/en/ipv6/statistics.html ~50% worldwide with lots of countries way over that is not what I'd call awful. > introduce IPv7 and just extend the IPv4 scheme? Every proposal like that requires changing all the routing hardware anyway, because we effectively ran out of bits in the IP packets to mark anything new. And once you need to change hardware, you may as well just go with a better designed protocol like ipv6. > This is probably naive, but it seems it would be easier for people to use vs. ... People don't use IP addresses anymore. DNS and local discovery covers this for anyone apart from geeks and IT people. If it doesn't, it's a bug at this point. reply Alupis 8 hours agorootparent> https://www.google.com/intl/en/ipv6/statistics.html ~50% worldwide with lots of countries way over that is not what I'd call awful. IPv6 was drafted in 1998, and ratified in 2017. 7 years after becoming \"official\" only 50% adoption rate world-wide is not what I would call stellar either. Also, looking at that chart it's closer to 40%... reply ekr____ 8 hours agorootparentIt's kind of a misunderstanding to say that IPv6 became official in 2017. The IETF has multiple standardization levels, with Proposed Standard being the first and Internet Standard being the last (there used to be Draft Standard in between but it was eliminated). Proposed Standards are deemed ready for deployment and lots of important protocols (e.g., TLS, QUIC, etc.) are at Proposed Standard. IPv6 went to PS in 1998, and people have been trying to deploy it ever since, so it's really more like 25+ years since it was official. reply orangeboats 8 hours agorootparentprevHonestly... 50% in 7 years is an impressive achievement, and far from being awful. People often compare IPv6 adoption to things such as HTTPS adoption, but upgrading to IPv6 requires hardware changes on both the ISP side and the customer side, and as we all know, hardware upgrades take forever. It's not unusual to see >10yo equipment running in the wild. reply Macha 8 hours agorootparentAlso even for HTTPS adoption - it was introduced in 1994, and in 2010 even major websites like google and facebook were basically only bothering to secure their login page which is how Firesheep came to be. It was only really that illustration of the business need that caused anyone to start bothering, and despite that demonstration, adoption was below 50% as late as 2018 reply IcePic 1 hour agorootparentAlso, lots of places and software are \"stuck\" at HTTP/1.1 when HTTP/2 and 3 are already out. Why are they stuck at something that came out in 1997? Presumably it would be far easier to upgrade/replace your webserver, loadbalancers and clients than replacing all routers on your ISPs. reply Macha 8 hours agorootparentprevIndustry doesn't adopt a thing until there is a need for it. In 1998, there really wasn't a need for it. Now there is. So now it gets adopted. News at 11. reply viraptor 8 hours agorootparentAlso, it's worth to keep in mind the delay on the hardware replacement. There were home routers which didn't support ipv6 even 7 years ago. And there many home routers deployed today which are over 7 years old. Also, my provider fully supports IPv6 and so does my hardware, but I have to click a button in the control panel to activate it explicitly. That's practically around a million of people who could be on IPv6 right now if they just opted in. reply Tijdreiziger 5 hours agorootparentI’ve noticed that the Ubiquiti UDM Pro router has IPv6 turned off by default; and I don’t really want to change the defaults, because idk, maybe something will subtly break, and in any case it works fine as-is… reply viraptor 5 hours agorootparentIt looks like it's going to be a few years until not enabling it will start subtly breaking things. ;) It's a trivial change to reverse, so if you're interested, give it a go. reply Plasmoid 8 hours agoparentprevThis comes up every time IPv6 is discussed. Any change to IPv4 to extend the IP space (eg your example) would require a breaking change to the specification. So this would require changing every router on the planet to support it. You're also focusing on how the IP address is written when that doesn't really matter. Changing the format isn't dragging out IP adoption. reply chungy 8 hours agorootparentChanging the IP protocol to support more stuff has been considered... that's how we got IPv6 in the first place. reply Twirrim 8 hours agoparentprevAwful adoption rate? https://www.google.com/intl/en/ipv6/statistics.html It's sitting just shy of 45%, based on what Google sees, and growing at a steady pace. The main issue is not technical, it's need. Right now, everything is accessible over IPv4, and probably over IPv6. There's no negative consequences from not having IPv4, it really needs initiatives like this one, and the US federal government one to drive things forwards (US federal agencies have to be single-stack IPv6 within a couple of years, which is forcing every government dealing vendor to get their act together on IPv6) reply wharvle 8 hours agorootparentA couple months ago I got a new router and some stuff didn’t work until I turned off ipv6 (I don’t recall what, exactly) I tried that solution early because when I first got Google Fiber the Amazon store website didn’t load, on any machine in our house, until I turned off ipv6. If stuff tries to route over ipv6, it doesn’t work, has been my entire experience with the protocol on the open internet (private networks seem Ok) reply denysvitali 3 hours agorootparentNext time try: https://test-ipv6.com/ If you get 10/10 here everything should work. Most likely your issue was a firewall rule on the IPv6 part, or a misconfigured DNS server (only returning A records) reply orangeboats 8 hours agoparentprev>IPv6's awful adoption rate 40+% adoption rate since the World IPv6 Launch Day in 2012 is not what I would call awful [0]. >extend the IPv4 scheme with additional octets? To clarify, IPv6 and IPv4 addresses are effectively just a number. On Linux, you can even do `ping 16843009` and see the tool actually pinging the address 1.1.1.1! The octets you see in typical IPv4 addresses, or the hextets in IPv6 ones, are just a way to represent those addresses. If you think the address representation is the main thing holding IPv6 back, you are thinking it wrong. The most likely reason is actually software & hardware support, and the industry's general sentiment of \"if it ain't broken, don't fix it\" keeping NATs alive. >seems it would be easier for people to use The ship has already sailed. If IPv4 addresses are really that easy to remember, DNS would not have been invented. Furthermore, when it comes to memorization of IPv6 addresses it's very helpful to split them into two halves. The second half can change rather arbitrarily (to maintain privacy by preventing address tracking), but the first half often won't because it is assigned to you by the ISP. [0]: https://www.google.com/intl/en/ipv6/statistics.html reply Dylan16807 8 hours agoparentprevIf you mostly dislike the long addresses, you can cut out the second half. You can make your device be 2661:919a:023e:9100::1. If you want a near-exact equivalent of 192.168.0.1, then you can use fec0::1 or fd00::1. That's even shorter than IPv4! You kind of shouldn't, but it'll work fine for a tiny network. But as other people have explained, the compatibility will be just as bad. And IPv6 does have address ranges like that. Nobody talks about them because not much can be done with them. reply Macha 8 hours agorootparentAnd the reason it's recommended against is that you might later decide to join your local network to another one and then you'll have to renumber them. Which is exactly the same problem with IPv4 local addresses. Except unlike IPv4, there is a mechanism to prevent that ahead of time reply Dylan16807 8 hours agorootparentWhich is absolutely important once you're a business with a bunch of subnets, the kind of network that would be on 10. in IPv4. If you're on 192.168 and you just want the convenient addresses, go for it. reply patmorgan23 8 hours agoparentprevIf you're typing out IP addresses your doing it wrong. We've had DNS for checks watch 40 years. IPv6 isn't THAT different from IPv4. It's a 128 bit number instead of a 32 bit number. When righting it out we use hex (because it's shorter). We use multicast instead of broadcast so your Subnets can be bigger The smallest subnet is /64 so vendors can do optimization hardware. Routers announce the prefix they route (and a DNS server) and let endpoints autoconfigure(with built in IP conflict detection!). Instead of a stateful DHCP server(which is still an option). And we let nodes have local and global addresses so issues with the router don't necessarily break local communication. (And they're easy to tell apart, if starts with an f it's a local adress, if it starts with 2 it's a global address) That's pretty much all that's different. 80% of what's different is the address space. reply jeroenhd 8 hours agorootparentHaving done basic tech support, sometimes typing out addresses is necessary. You'd think mDNS would help computers find your modem when you're first setting up your internet connection, but the super \"helpful\" modern browsers all try to go to google.com/search?q=myrouter.localnet even if mDNS is providing various IP addresses to try first. That's not IPv6's fault, but it is a pain. Especially considering that local networks differ from router to router, making it impossible to write a manual to access a router's settings for when the thing needs to be configured before it can connect to the internet. Of course, in those cases there's nothing preventing manufacturers from providing some 192.168/16 IPv4 addresses for those specific use cases (and not route them to the internet). I've even seen one manufacturer listen on a bunch of 169.254/16 addresses for when DHCP fails somehow, and I think that'd solve the problem perfectly without even needing a DHCPv4 server. reply nijave 7 hours agorootparent169.254 is IPv4 link local range like IPv6 fe80 Windows will try to auto configure with a 169.254 link local if DHCP is unavailable. Not sure if anyone is actually using them for LAN. They come up sometimes in point to point or for other shenanigans (commonly on cloud or virtualized networks for special services) Afaik for Firefox and Chrome you can add a trailing / to prevent it from doing a web search and attempt to do a DNS lookup instead reply somerandomqaguy 3 hours agorootparent>Windows will try to auto configure with a 169.254 link local if DHCP is unavailable. Not sure if anyone is actually using them for LAN. They come up sometimes in point to point or for other shenanigans (commonly on cloud or virtualized networks for special services) I've seen it done few times for niche devices and I've done it a few times when I needed a really quick network operational and didn't want to bother setting up any supporting infrastructure such as a DHCP server. Never for any production environment though. I've toyed around with an IPv6 link local only environment but browsers don't support adding the zone identifier[1] for link local addresses. There's workarounds but in most cases for a fully isolated network it's about the same effort to setup a small DHCP server somewhere. That said it's not a comprehensive solution, Android AFAIK still won't support DHCPv6. To get that to work AFAIK requires router advertisements and some more odd finagling if privacy extensions are enabled on the device. [1]https://ungleich.ch/u/blog/ipv6-link-local-support-in-browse... reply p_l 2 hours agoparentprevBecause that would be starting from scratch the migration with all the warts and less of the benefits. About the only part that would be easier is that more people use less broken APIs compared to BSD Sockets, with getaddrinfo and similar solutions ported from XTI/Plan9 finally becoming the norm. So no more \"you need to duplicate code to add support for new IP version\". But that's still worse than migrating to v6, which has widespread hardware support in switching equipment, considerable preference in mobile networks and not only (TL;Dr it's cheaper to use v6 to carry v4 traffic using 464 stateless translation rather than other forms of CGNAT), and some IOT systems (industrial, not home stuff) are v6-only if they have IP connectivity on the embedded side at all. reply sylware 8 hours agoparentprevIn my country, rare are the ISPs which don't provide IPv6 (it has been the case for years, and that includes mobile internet). I think the lagging services about ipv6 are steam, github, HN, and a few smtp servers here and there. reply Seb-C 1 hour agorootparentI think that Reddit and AWS are mostly non-ipv6 yet as well, that's a pretty big deal. reply senectus1 8 hours agoprevClick saver: On 17 January 2024, the Government of the Czech Republic approved the material \"Restarting the implementation of DNSSEC and IPv6 technologies in the state administration\". On the basis of this decision, the Czech state administration will stop providing its services over IPv4 on 6 June 2032. Thus, the Czech Republic knows its IPv4 shutdown reply ChrisArchitect 8 hours agoprev[dupe] More last week: https://news.ycombinator.com/item?id=39060187 reply nikita14 4 hours agoprevGood move. reply tomjen3 4 hours agoprevThis really needs to be an EU-wide thing, and a lot sooner. Two years till it becomes illegal to offer something on IPv4 for the general public and not offer it on IPv6, two more years and no ISP may route IPv4. Also no ISP NAT bullshit. A default enabled firewall refusing incoming connections is fine, but the internet should be a network of machines that can talk to each other, and for that to happen it must be possible for owners to accept incoming connections, even if it is for nothing more than remote access to their PCs. reply loup-vaillant 27 minutes agoparentI agree completely, but bigger companies are going to lobby against that. They’ll even give reasonable sounding reasons, such as switching costs or user safety. Their real reason though will probably be that the truly peer-to-peer networks enabled by a world free of NAT and symmetric bandwidth would undercut their precious market. Finally, I’m not sure our governments would like the rise of Anarchist networks where speech is so free they can’t even control it. See all the attempts to criminalise or put limits to encryption. Such things would never happen under democracy, but the representative governments we live under are a little different. (Historically, representative government was conceived in explicit opposition to democracy, and democracy lost.) reply ikt 1 hour agoparentprev> Two years till it becomes illegal to offer something on IPv4 for the general public and not offer it on IPv6 Why? Is there any pressing need for ipv6 only connectivity that you'd make ipv4 only illegal? reply loup-vaillant 24 minutes agorootparentBecause this of course: > the internet should be a network of machines that can talk to each other, and for that to happen it must be possible for owners to accept incoming connections The importance of this is quite obvious if you’re politicised enough. Stuff like Free Speech. reply hasty_pudding 8 hours agoprevI wish the US had balls like that. reply wmf 8 hours agoparentUS DOD started transitioning to IPv6-only in 2021. I wonder how it's going so far. https://media.defense.gov/2021/Jul/13/2002761414/-1/-1/0/DOD... reply thfuran 6 hours agorootparentI predict they'll be just about done by the time 2038 rolls around. reply zokier 3 hours agoparentprevUS gov has been actively pushing for IPv6. You can question the efficacy of their efforts, but the intent is there. Of course I suspect there is lot more inertia of all sorts in US gov compared to CZ, just due size and history. Read for example https://www.whitehouse.gov/wp-content/uploads/2020/11/M-21-0... (does anyone have more recent ref?) reply BizarroLand 9 hours agoprevI wonder how this will go? 8 years is plenty of time to make the transition even for a government, right? reply bombcar 9 hours agoparentEight years is forever, we can wait. Five years is a long time, let’s work on other things. Three years isn’t terribly soon, it’ll probably be fine. Two years is getting close, let’s consider checking. One year is too close, let’s ask for an extension. reply p_l 9 hours agorootparentWhich is why this time US government for example is only allowing extensions to government agencies on case by case basis, but not to vendors. AFAIK they allowed extensions for vendors in 1990 and that's why original EOL date for IPv4 continuously slipped until success by Network Translation and its PIX devices helped kill the attempt completely. reply p_l 2 hours agorootparentThe \"ancient\" code probably already speaks IPv6 due to whatever method used to hook IP connectivity to its VTAM interfaces. Or due to its age it has multiprotocol support anyway. It's the BSD Sockets using code that is a problem - the OSes has supported v6 for a long time now. reply stcredzero 9 hours agorootparentprevEight years is forever, we can wait. I've heard that under 10 years is the optimal for a long term government project. Anything longer, and it's equal to \"forever,\" and it's never done. This was the rationale for setting the goal of landing on the Moon to within the decade. reply labster 9 hours agorootparentprevI would name that poem “The Ballad of REAL ID” reply nxobject 9 hours agorootparentStrangely enough, though, it seems to be working in everyone's favor in this case. reply promiseofbeans 9 hours agoparentprevFor governments maybe, but for banks? They're still desperately trying to transition to IBM databases that were deprecated slightly more recently than their current one. reply marcosdumay 9 hours agorootparentThey only have to transition their external communications, they can keep their 90's DB2 databases that cost millions/(MB*year) just fine. reply 0cf8612b2e1e 9 hours agorootparentprevProbably easier to start a new bank. reply mito88 8 hours agoprevwho will check the IP addresses? reply 2024throwaway 9 hours agoprevI personally don't think making a very large amount of devices completely obsolete is the best thing, from an environmental standpoint. Call me a dumb hippie. reply Avamander 9 hours agoparentIPv6 support existed in even Windows XP, if not older. Things without IPv6 support are already obsolete. reply jeroenhd 8 hours agorootparentA bunch of IoT hardware and other microcontroller-based hardware has IPv6 support, but that support is often disabled to save a couple of kilobytes of flash memory. You can buy all kinds of hardware today that lacks IPv6 support. Of course, if that hardware needs to access IPv6 services, they might as well disable IPv4+ICMP+DHCPv4 support and enable the IPv6 size in firmware, and probably get similar savings. To be honest, I'm not sure why companies still ship ESP32-like hardware with that little storage given how cheap flash storage is, but these optimisations are more common than one might think or hope. reply loup-vaillant 20 minutes agorootparent> A bunch of IoT hardware and other microcontroller-based hardware has IPv6 support, but that support is often disabled to save a couple of kilobytes of flash memory. Wait a minute, kilobytes? That much? It’s not like one needs to duplicate the entire IP layer, it seems to me the only changing parts are parsing & generating the packets. reply gmuslera 8 hours agorootparentprevThat is basically planned obsolescence. The world (except a few regions) is essentially out of IPv4 addresses. reply voidwtf 9 hours agoparentprevThose devices won't necessarily be made obsolete, there are solutions for proxying ipv4 -> ipv6 and I think we'll see many consumer routers enabling those features by default for devices that do not support ipv6. reply Twirrim 8 hours agoparentprevIPv6 has been supported for ~20 years. Not many devices in any regular use these days with network support, lack IPv6. That pool of devices will be almost entirely gone in 8 years when this is due. reply 2024throwaway 8 hours agorootparentI think we simply have different definitions of \"devices\". I'm not talking about just phones, tablets, and laptops. reply deadbunny 9 hours agoparentprevWhich devices don't support IPv6 at this point? reply ianburrell 9 hours agorootparentAlso, it is eight years in the future. What future devices won't support IPv6? This is also about the Czech government sites removing IPv4 support. What devices that would be used to access site won't support IPv6? They all do today. reply cyberax 8 hours agorootparentprevPlenty. Including the ones that should know better. E.g. esphome is still working on IPv6 support ( https://github.com/esphome/feature-requests/issues/718 ). Then there's a veritable black hole of various crappy security cameras, IP phones, and WiFi printers. And they can live for a veeeeery long time. And I don't think I've ever seen a hotel network with IPv6 support. And I've actually seen a hotel (in Palo Alto) that gives out real IPv4 addresses to clients. reply 2024throwaway 9 hours agorootparentprevSmart sprinklers. Industrial equipment. CCTVs. reply mianos 9 hours agorootparentAs an aside, I notice a lot of the recent commits to the ESP32 repo are for ipv6 fixes. It already works in the tasmota esp32 builds on devices at my place. This said, by then, maybe the core OS will not be metal, but Linux on all these device and we'll get top ipv6 support. More likely, in 2032, well have a bunch of crap, built from very old SOCs running Linux 2.4. reply ianburrell 9 hours agorootparentprevDo those access the Czech government website? reply i-use-nixos-btw 9 hours agorootparentpolice knock on the door “DAD, YOUR SPRINKLERS ARE HACKING THE RECYCLING COLLECTION TIMETABLES AGAIN” (spend enough time around places like this, though, and one could easily imagine this being a thing in 2032) reply 2024throwaway 9 hours agorootparentprevI was speaking to sun setting IPv4 in general. That said, I have to imagine that there probably are plenty of those devices that do access Czech internal services at least. reply nerdbert 6 hours agorootparent> I was speaking to sun setting IPv4 in general. It's 2024 and Fidonet still works. Nobody's sunsetting IPv4. It will fade into obscurity on its own as IPv6 adoption increases and IPv4 addresses become increasingly hard to get, as it should. reply ianburrell 8 hours agorootparentprevIPv4 will be around for a long time. Nobody will care if it is running on internal network. Or quadruple NATed. There are enough IPv4 addresses for sites to have them. But not enough for every user. reply jeroenhd 8 hours agorootparentprevSounds like IPv6-as-botnet-DDoS-prevention is a viable strategy! Can't get DDoS'd by the unmaintained crap if said crap doesn't even speak the required protocol! reply murphyslaw 8 hours agorootparentprevAre you sure you want any of those on a public network anyway? If they're not firewalled right now then you're already in trouble. reply ClumsyPilot 8 hours agorootparentprevSounds like their creators have some work to do reply colechristensen 9 hours agoparentprevPass laws requiring long term hardware support and open source code for unsupported devices. reply supertrope 6 hours agorootparentOne response to EU recycling mandates is to just pull out of those markets. Some website operators just block the EU as it's easier than complying with privacy regulations. When new regulations per the USA's Americans with Disabilities Act mandated captions, a few MOOCs just yanked their videos. reply kazinator 8 hours agorootparentprev... and unlimited flash space for accepting larger and larger upgrades, forever! reply BenjiWiebe 7 hours agorootparentIf it's open source I'll delete 90% of the manufacturer provided telemetry/tracking/anti-features and use 1/20th of that free space to add the feature I want. reply kazinator 8 hours agorootparentprev... and unlimited flash space for forever upgrades! reply DarkmSparks 9 hours agoprev [–] I cant imagine how much \"fun\" its going to be trying to lock down a governments systems when every single device on their network is potentially reachable directly from China... reply porbelm 9 minutes agoparentMy very publically available home server (static IPv6 address, AAAA record) is behind two firewalls. The first is the ISP-supplied router. That handles any attempted connection to any address on its /64 subnet. The second is UFW on the server itself + fail2ban. The open ports are 22 and 443. It's basically as secure as it gets apart from not having it public at all. Public IP != open for all connections on all ports... reply ehPReth 9 hours agoparentprevHaving a public IP != publicly reachable. IOW, have the router block incoming connections just like the goreish nature of \"NAT\" did for you. reply DarkmSparks 8 hours agorootparenthaving a private ip = definately not publicly reachable. having a public ip = possibly reachable, depending on what other devices can be compromised on the network. given the number of government machines already that participate in the various ddns botnets moving to that second one is going to be a lot of fun at the very least all the cnc servers can move local. reply deadbunny 8 hours agorootparent> having a private ip = definately not publicly reachable. > having a public ip = possibly reachable, depending on what other devices can be compromised on the network. Lateral movement is hacking 101. Private IPs don't provide any security. Got a webserver open to the internet and a database server on a private IP only accessible to the web server? Guess how you get to the database server? reply DarkmSparks 8 hours agorootparentgot a vulnerable database server and a secure webserver. guess what happens when the database server gets a public ip. reply deadbunny 8 hours agorootparentNothing because it's behind a firewall? You seem to be continually conflating something having a public IP address and it being open to the internet raw dogging it. This is not how things work. reply DarkmSparks 8 hours agorootparentgood job all the networks cables are glued in and no one ever plugged a cable into the wrong port, or doing so might result in all the devices behind that firewall getting exposed directly to the internet and no one noticing because everything still works. but Im done burning karma on this one, good luck have fun. reply porbelm 4 minutes agorootparentGood job not even attempting to secure your office switch ports with whitelisted MACs or whatever, then. And if you then argue that MACs can be spoofed easily, well, you'd have to get the MAC of the authorised system first. And by that time you've physically broken into the building - you have worse problems than a rogue device or two... burnerthrow008 7 hours agorootparentprev> good job all the networks cables are glued in and no one ever plugged a cable into the wrong port, or doing so might result in all the devices behind that firewall getting exposed directly to the internet and no one noticing because everything still works. So just put your database server on an IPv6 ULA (which is not globally routable)? There are other benefits to that, too, you know? Like that you can have a completely static address for the server, which is agnostic to whatever IPv6 prefix gets assigned by your upstream provider. reply DarkmSparks 7 hours agorootparentdid the unpaid intern do that before or after the insecure database server accidentally got given a public ip address? did they also check and update that old office use only IIS server no one uses before the department all got public ips, or wasn't there a lunch budget for that. reply DarkmSparks 8 hours agorootparentprevedit:wrong thread reply Arnavion 7 hours agorootparentprev>having a private ip = definately not publicly reachable. What component of your router prevents a packet with destination IP 192.168.1.2 arriving on the WAN interface from crossing over to the LAN interface and reaching a LAN machine with that IP? Hint: It's the same one that prevents IPv6 packets from making that same crossing. reply DarkmSparks 6 hours agorootparentnothing stops 192.168.1 crossing a wan interface, in fact I and most of the internet rely on being able to do exactly that, the router just needs an appropriate route in its routing table. reply Arnavion 6 hours agorootparentReread the comment carefully. reply DarkmSparks 6 hours agorootparentthe wan address of my router is 192.168.1.8 with a gateway of 192.168.1.1 my lan ip address is 10.10.11.10 with a gateway of 10.10.11.1 what do you think I missed? reply eurleif 8 hours agorootparentprevIf a device on the network is compromised, how is a private IP going to save you? Private IPs can be reached from within the network. reply DarkmSparks 8 hours agorootparentit doesnt \"save you\". but on a private network a compromised device can only make outgoing connections. public facing devices can be administered by incoming connections, thats a whole other level of complexity, potentially for every device. reply ehPReth 8 hours agorootparentset router to an allowlist configuration... and you're done. it's your \"NAT\" security but without terribleness. some (consumer/smb) routers even come this way out of the box to prevent exactly what you mention reply DarkmSparks 8 hours agorootparentadd a software router to the network that hands out public ips to all the devices on the network. or better, just accidentally switch a cable over from the router to the routers switch, see if anyone notices their private ips all became public ones. reply ehPReth 8 hours agorootparentprevif we're branching in to 'what other devices can be compromised' then that's a concern for any network 'private' IP or not. for example, even on a NATted v4 network if you get the right device (say if it's 'port forwarded', or you get malware on it another way (social engineering) you can pivot that way to another point in the network. you can supply all the ACLs and firewalling to your heart's content on either private or public, it's just that public addresses have a heck of a lot less shitfuckery when you actually want to do useful things across the internet reply DarkmSparks 8 hours agorootparentif by \"heck of a lot less shitfuckery\" you mean \"makes it a lot easier to exfiltrate all the data on a network\" I completely agree, that was pretty much my point. reply orangeboats 3 hours agorootparentYou seem to fail to grasp that it is the statefulness of NAT that provides security, not the private/public IP distinction. The same statefulness can be obtained by using... surprise, surprise, a stateful firewall. :-D It is helpful to imagine NAT as a stateful firewall with packet modifying capabilities. Because that's what it is. If your ISP is doing CGNAT, try pinging random 100.64.0.0/10 addresses. Marvel at the number of pongs you can receive. Hell, we even have online threads talking about this, so it can't be just my ISP being incompetent [0]. [0]: https://www.reddit.com/r/networking/comments/1910m9w/discove... reply deadbunny 9 hours agoparentprevFirewalls exist. reply DarkmSparks 8 hours agorootparentBetter call up cloudflare and tell them no one wants their business now all the network engineers are as competent and equipped to deal with threats as they are. reply deadbunny 7 hours agorootparentCloudflare's main business is being a CDN that can soak hundreds of gbps in bandwidth of DDoS traffic. Nothing to do with competence, though in your other comments you suggested that plugging things into different switch ports would give them new IPs and make things publicly routable so perhaps you're right to keep using Cloudflare. reply DarkmSparks 7 hours agorootparentCloudflares main day job is blocking malicious incoming packets used for RCE exploits on unpatched servers. reply deadbunny 7 hours agorootparentNone of which get through a firewall with a `deny` rule. reply DarkmSparks 7 hours agorootparentand how do you enforce using that firewall rule on tens of thousands of devices, each now with several public and private ips and several thousand routes in and out of the network? reply devman0 5 hours agorootparentA stateful firewall is prerequisite for NAT implementations commonly deployed in most office and consumer settings due to the session tracking requirement. So you just stop doing the NAT part and the firewall continues to deny untracked ingress connections just like it did when NAT was running. reply MOARDONGZPLZ 9 hours agoparentprev [–] This feels like FUD. You can still assign a block of ipv6 as internal to your network and put it behind a bastion. Forcing ipv6 isn’t the same as going zero trust or something where now everything is publicly routable. reply DarkmSparks 8 hours agorootparent [–] and whats stopping an intruder reassigning that block to something that can be publically accessed and how will that be monitored? reply cyberax 8 hours agorootparentUhhh... Whut? You obviously need to use a ULA prefix, they are not routed (just like RFC1918 space in IPv4). Or you can just use your allocated IPv6 space, and firewall at the border. And you hopefully have BGP hijack monitoring set up anyway. reply DarkmSparks 7 hours agorootparentyou do know single network devices can have more than one ip address? afaics, the biggest issue with ipv6 is if its active all devices on a network can easily be coaxed to never route traffic anywhere near the router/firewall the network admistrator intended, simply by handing out extra routing info for alternate networks. reply greyface- 7 hours agorootparent> afaics, the biggest issue with ipv6 is if its active all devices on a network can easily be coaxed to never route traffic anywhere near the router/firewall the network admistrator intended, simply by handing out extra routing info for alternate networks. This is not unique to IPv6. ARP spoofing is the v4 version of this attack. RA spoofing is the v6 version of the attack. In both cases, the solution is the same: lock down your L2 by enabling MAC / ARP / RA filtering on your switch. reply DarkmSparks 7 hours agorootparenttrue, but getting even a single public ipv4 address is hard. anyone and everyone handing out public ipv6 addresses is by design. reply cyberax 1 hour agorootparentI have 32 IPv4 addresses, how do I utilize them to hack Amazon? It doesn't matter that you can get IPv6 addresses, you still need to be able to get onto the L2 network of your victim company to be able to mount RA attacks. You also will somehow need to force them to announce your IPv6 space to their peers. reply Dylan16807 8 hours agorootparentprev [–] Reassigning a block sounds a lot harder than the IPv4 version of making something accessible, sending out a single packet to hole punch the NAT. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Government of the Czech Republic has approved a plan to adopt DNSSEC and IPv6 technologies in the state administration.",
      "Starting from June 6, 2032, the country will no longer provide services over IPv4.",
      "This transition is necessary due to the increasing number of devices and services, as IPv4 addresses have become insufficient, while IPv6 offers an almost limitless number of addresses.",
      "The countdown to the end of IPv4 has been set at 3056 days, 13 hours, 57 minutes, and 16 seconds."
    ],
    "commentSummary": [
      "The discussion explores the challenges and benefits of transitioning from IPv4 to IPv6.",
      "Implications for tracking individuals, use of unique local addresses, limitations, and security concerns of both IPv4 and IPv6 are discussed.",
      "The conversation emphasizes the importance of hardware and software support, the need for initiatives to drive adoption, and the complexities of upgrading infrastructure."
    ],
    "points": 269,
    "commentCount": 260,
    "retryCount": 0,
    "time": 1705967767
  },
  {
    "id": 39095337,
    "title": "Winlator: Run Windows Apps on Android with Wine and Box86/Box64",
    "originLink": "https://winlator.org/",
    "originBody": "Home Download About Welcome to the official Winlator website Winlator is an Android application that lets you to run Windows (x86_64) applications with Wine and Box86/Box64. I am immensely grateful to ptitSeb and the Wine developers, who developed the Windows compatibility layer (x86_64). Without the constant dedication to Box86/Box64 by ptitSeb and as well as the constant improvements to Wine by its developers, this project would never have been possible. Download the latest version Credits and Third-party apps Ubuntu RootFs (Focal Fossa) Wine (winehq.org) Box86/Box64 by ptitseb PRoot (proot-me.github.io) Mesa3D (mesa3d.org) DXVK (github.com/doitsujin/dxvk) D8VK (github.com/AlpyneDreams/d8vk) CNC DDraw (github.com/FunkyFr3sh/cnc-ddraw) © Copyright 2024 - Lovingly developed by BrunoSX",
    "commentLink": "https://news.ycombinator.com/item?id=39095337",
    "commentBody": "Winlator: Android app that lets you to run Windows apps with Wine (winlator.org)270 points by davikr 13 hours agohidepastfavorite72 comments simlevesque 12 hours agoFallout 3 video: https://www.youtube.com/watch?v=9E4wnKf2OsI 800x600, around 20 fps, settings on high. reply westurner 10 hours agoparentWhat about proton-ge wine with Android x86? https://github.com/GloriousEggroll/proton-ge-custom (Edit) the Moonlight + LizardByte/Sunshine server part of Steam-Headless can do 4k 120fps FWIU; and there's already a Moonlight client for Android [TV]. https://github.com/Steam-Headless/docker-steam-headless reply bsimpson 5 hours agorootparentWhy would you run Android x86 on an Android ARM device? reply westurner 2 hours agorootparentWhy would you run DirectX games in WINE also with x86 to ARM translation in userspace on Android? reply KiwiJohnno 9 hours agoparentprevImpressive! What hardware is this running on? reply metadat 6 hours agoparentprevRemarkable experiment, thanks for sharing. How did you control it? Bluetooth maybe? reply mappu 6 hours agoprevThere are multiple Box+Wine distros floating around at the moment. r/EmulationOnAndroid likes to talk about https://github.com/olegos2/mobox as having the current best performance compared to Winlator. reply Sakos 1 hour agoparentOn that note, termux is really freaking cool. It almost feels like I have Linux on a phone and makes me a bit sad that we have Android instead (yes, yes, I know it's technically based on Linux, but we all know that's not really the same thing). reply Abishek_Muthian 25 minutes agorootparentTermux + nvim has enabled many from underpriviliged backgrounds who cannot afford a computer to get into programming and build a career for themselves. Termux & F-Droid is so underrated. reply bee_rider 8 hours agoprevI wonder if Wine on Windows has been done? Might be better backwards compatibility at this point. reply bpye 3 hours agoparentNot exactly what you were asking, but winevdm [0] does use code from Wine to run 16-bit Windows applications on 64-bit Windows installs that don't support it natively (via ntvdm). [0] - https://github.com/otya128/winevdm reply 22c 4 hours agoparentprevWhile the specific names escape me, there have been a few projects over the years that have replaced DirectDraw or Direct X DLL files with re-implemented counterparts that tend to work much better on modern hardware. I believe in one such case, the original StarCraft would run with really buggy colors using the official Windows DLL files and a drop-in DLL replacement would fix the issue (these days I think Blizzard has shipped their own fix for this issue). reply amonith 36 minutes agorootparentAnother would be DXWND which is amazing :) It has a ton of options that not only fix various issues with old games but you even have additional features like an ability to run games that never had that support in windowed mode. reply theblazehen 2 hours agorootparentprevOne is DXVK which replaces DirectX calls with Vulkan calls reply billwashere 7 hours agoparentprevYou can using wsl [1] or without [2]. [1] https://news.ycombinator.com/item?id=20632942 [2] https://news.ycombinator.com/item?id=38909304 reply dugite-code 7 hours agoparentprevI've seen a blog post in the past about running it under WSL so it's entirely possible. That said you could probably just side-load Winlator in Windows 11 as well reply toast0 3 hours agoparentprevWine is part of this project[1] to run 16-bit windows executables on 64-bit windows. But I think I've seen it referred to generally as 'wine for windows'. It's definitely been a target at times, but I can't find it at the moment. [1] https://github.com/otya128/winevdm reply isawczuk 11 hours agoprevMy wife is using some custom software to create photo albums, so 99% of tablets don't work for her. With this I'm finally able to give her dream machine: affordable, lightweight and photo making machine. reply bonyt 9 hours agoparentI’d wonder how this would beat a used surface go or something similar - they’re pretty light and affordable. And, while they’re not powerhouses, I wouldn’t expect much from a budget android tablet emulating x86 and running wine. reply TeMPOraL 9 hours agorootparentYou can get pretty powerful, nearly mint condition Surface alternatives from Dell and Lenovo for fraction of the price, if you look at post-lease hardware. My last two portable machines were such 2-in-1 devices - specs comparable to top Surface 2-in-1 line, but for one third of the price. reply user_7832 11 hours agoprevOn a similar/tangential note about running stuff on wine, does anyone know how to run Microsoft Office, or Netflix (or any similar DRM service like Prime videos) on Linux using wine? I’m seriously considering switching to Linux on a new laptop as someone tired of windows eating resources randomly and I don’t want to install Windows 10 when I get my laptop. I’ve looked at hypervisors and have used hyper x but there still appears to be a performance hit even if the host is Linux (if I’m not mistaken). reply jeroenhd 11 hours agoparentDRM used to work; I installed Chrome in Lutris and managed to get Amazon Prime working, though Chrome was very buggy. Netflix sort of works in Firefox (needs an addon to play 1080p video, no idea about 4k). Microsoft Office (at least, a version that's kept up to date) doesn't work in Wine. I find ONLYOFFICE to be a very competent replacement, though. I've also used Cassowary to configure a Windows 10 VM + Remote Desktop Apps, which allow for Windows programs to appear native in the Linux launcher while secretly launching the VM and starting an RDP session for that application alone. It's a bit fiddly to set up, but it works if you rarely ever use Office. As a last resort, you could do what Microsoft did with Windows 11 when Wordpad got removed: install Microsoft Office Online as a web application (requires Chromium/WebKit browser to be installed since Mozilla decided PWAs aren't important to desktop users). If you don't care much about privacy risks, you could also skip the Linux crap and install ChromeOS Flex. It's probably the most user-friendly Linux desktop out there, and DRM should work out of the box. reply pentamassiv 11 hours agoparentprevWhy do you want to use wine to play Netflix videos on Linux. It works fine in the browser reply heavyset_go 10 hours agorootparentChrome on Windows supports different Widevine levels than Chrome on Linux does. reply londons_explore 4 hours agorootparentThe higher levels all require keys baked into the GPU/other hardware support. Doubt an emulator will help. reply IshKebab 2 hours agorootparentYeah at that point just pirate things. Much easier and I don't think even the most moral person could argue against pirating something you can legally access anyway. reply cpeth 10 hours agoparentprevAs someone who loves the dev experience of WSL2 on Win11, try out https://github.com/LeDragoX/Win-Debloat-Tools to strip down your Win install and get some resources back. reply bsimpson 6 hours agorootparentI wonder how well audited these random GitHub projects are that claim to make Windows suck less. Running random shit from GitHub with my root password gives me the creeps, even though most of them are probably innocuous. reply prmoustache 3 hours agoparentprevI am working on Linux and using web versions of office 365 apps. Outlook, teams, onenote, word, excel... No issue with streaming on the browser either. reply nightowl_games 8 hours agoparentprevYeah I'd say use the browser version. Id recommend just dual booting Linux for a while. I have windows installed on my pc basically for when I need to sign a PDF, but it's handy every once in a while. I'm one very happy Linux Mint user. It's the best distro by far imo. reply goosedragons 11 hours agoparentprevOlder versions of Office can work quite well under Wine. 2007 is quite easy to get going using PlayOnLinux or Crossover for example. Newer versions are more trouble. reply shreddit 11 hours agoparentprevOffice has some pretty decent web apps (all of their apps switch to electron sooner or later anyway) reply IshKebab 2 hours agoparentprevFair warning though, your battery life will be like 1/4 of what it is on Windows, and Linux's memory handling on OOM is much worse than Windows. So saving resources probably isn't the best motivation. reply z_open 12 hours agoprevReally cool. Just tried it with notepad++ and it seemed to work as expected. Can't find a use case for me yet but may come up. reply ape4 12 hours agoparentRunning iexplore.exe ;) reply ramon156 8 hours agorootparentAnd then try running a PWA in there, submit a ticket because it doesn't work on your machine reply V__ 12 hours agorootparentprevYou monster... reply marcod 5 hours agoparentprevI just played Master of Orion (bought via GOG), which is a DOS game on DOSBox, one Wine, on Android :) reply speps 11 hours agoprevIs this Box86 running Linux running WINE? That seems convoluted, but it's probably the most straightforward right now. Impressive! reply jeroenhd 10 hours agoparentBox64 and a bunch of extra settings (DXVK and such). It's a bit buggy in some places, but it works well once you get through the setup process. This sucks the voltage out of my phone's battery faster than any other app I know, though. On the one hand, I now have a modded Oblivion install on my phone, on the other hand, I can play it for maybe half an hour before I need to look for a charger. reply anthk 8 hours agorootparentYou can run Morrowind thru OpenMW. reply ignoramous 9 hours agoparentprevThink it is Box64 (POSIX emulation w/ ISA translation, amd64 -> arm64) running WINE and DXVK via Mesa (Direct3Dxx -> Vulkan translation). Wonder if Google Dawn (ANGLE and SwiftShader), which is bundled with Chromium for WebGPU, could be a drop-in replacement for DXVK + Mesa. reply tadfisher 9 hours agorootparentNot drop-in, because Dawn doesn't implement the Direct3D API. You could probably bridge Direct3D to WebGPU by implementing the headers[1] though, which is exactly what DXVK does for Vulkan [2]. [1]: https://github.com/Joshua-Ashton/mingw-directx-headers [2]: https://github.com/doitsujin/dxvk/blob/master/src/d3d9/d3d9_... reply ignoramous 8 hours agorootparentThanks. > Not drop-in, because Dawn doesn't implement the Direct3D API. Not Vulkan, but Dawn (I guess, I mean ANGLE specifically) can do D3DX 9 & 11 to OpenGLES? https://chromium.googlesource.com/angle/angle/+/main/README.... / https://archive.is/u2z2a reply tadfisher 5 hours agorootparentANGLE is the opposite of what you want; it implements OpenGL ES (WebGL) on top of DirectX, when you want to implement DirectX on top of WebGPU. reply parkaboy 12 hours agoprevOh this sounds like a potentially fun way to do music production. I wonder how well stuff like Reason and Live might work here. reply squarefoot 10 hours agoparentWine does run Windows music software quite well, sometimes even better than Windows; in fact the first and most famous hardware VST host, the Muse Receptor, used Linux + WINE under the hood almost 20 years ago. Here's a SOS review from 2005: https://www.soundonsound.com/reviews/muse-research-receptor I'm not so sure about that under Android, however, since latency in audio apps has been a big problem on that platform for ages. reply stefanyas 11 hours agoparentprevWine on Linux desktop runs an older version of Reason okish, I believe. I never tested that, just read somewhere. But if you wanna run, say, Reason 12, I don't think it works very well. reply AsthmaBoy 11 hours agorootparentApparently, Reason 12 runs well https://appdb.winehq.org/objectManager.php?sClass=applicatio... reply pdntspa 8 hours agoparentprevBut... why??? Are you going to hook up a mouse and keyboard and USB midi controller? Why not just do that on a similarly priced laptop? reply parkaboy 7 hours agorootparentWould be nice to sketch out small ideas like making patches or beats etc when bored / out and about in places it's not conducive to bring or bust all that stuff out. reply pdntspa 4 hours agorootparentWhat does that do that something like FLStudio mobile doesn't? I feel like Reason or Live, out of all the available options, would be a worst case. I can barely read the text on Reason VST and Live's UI is very crowded. Unless you're walking around with a 10\"+ tablet or something how would that even work? Just seems like more trouble than its worth. reply prmoustache 2 hours agorootparentprevYou'd be better of with a mobile music app than a DAW designed to work on a desktop Gui. reply owenpalmer 7 hours agoprevI got Blender 2.79b working! So cool! reply tobinfekkes 6 hours agoparentBrilliant, well done! reply mkoubaa 10 hours agoprevI'd have named it aNTroid reply snoutie 9 hours agoparentUntil you speak it out loud. Then it sounds like a creme you apply where the sun does not shine. reply actionfromafar 45 minutes agorootparentWorks for Google reply prashp 7 hours agorootparentprevOr steroids for ants reply MaximilianEmel 5 hours agoprevI wonder if with some tweaking, this could be used to run PC/Windows VR software on Android VR Platforms (e.g. Quest). reply DennisAleynikov 3 hours agoparentIt should probably work normally. I will check if it works in my gearVR reply colordrops 57 minutes agoprevI've tried to use Wine throughout the decades and still feel delight when and application loads, but I've never once found it usable, due to one glitch or another, with the only exception being a couple video games. I'm not sure what I'm doing wrong. My recent attempts have been Fusion 360 and the Remarkable 2 app, both executing but so glitchy that it wasn't worth it. I appreciate the effort but it seems like chasing the dragon, at least with desktop apps. It does seem to have gotten a lot of traction with video games though. reply frozenport 8 hours agoprevReminds me of the approach for `ExaGear Strategies`. Was a popular way to run Heroes of Might and Magic III on Android. reply spiral09 11 hours agoprevWow an easy an convenient app Time to play lethal company reply SubiculumCode 12 hours agoprev [–] I recently changed my work desktop environment to Linux, however the transition has been a bit rough. The University I work for uses Microsoft OneDrive for file storage and sharing, but won't authorize the 3rd party Linux onedrive app. I want the files locally, so the clunky web interface is a no go. I don't have a spare wINDOWS LICENSE. There is however an official Android OneDrive app. So I installed an virtual android instance on my Linux workstation and logged in with the Onedrive app...but boom. The Ondrive app will store files locally, but not in the way of the Desktop app. Each file is named something random which is indexed by some database file, and the directory structure is not maintained at all. Making it pretty much unusable without some fix. Its infuriating. A smart programmer might be able to make an interface that translates their directory system to the real one, but ugh. reply shemnei 11 hours agoparentI don't know if that helps in your case, but rclone is capable to sync onedrive ot a local folder (send/receiving).reply 5- 10 hours agoparentprevhere's a hunch assuming you tried the below client: https://github.com/abraunegg/onedrive/blob/1a88d33be3e2c6747... try changing that to e.g. the android onedrive client's id (hint: possibly b26aadf8-566f-4478-926f-589f601d9c74). a similar trick works for exchange oauth (allowing third-party mail clients). reply SubiculumCode 4 hours agorootparentI'll try it!! reply bsimpson 6 hours agoparentprevI don't understand that intersection of constraints: - No Windows license - Works for a large institution - Institution's IT won't approve OAuth to a 3rd party client Did you have a Windows license before you switched to Linux? How are you meant to do your job? reply gespadas 9 hours agoparentprevTry insync (one-time payment), and works like a charm. I've been using it for a couple of years, zero problems. Syncs noy only OneDrive, also Google Drive and others. https://www.insynchq.com/ reply Almondsetat 10 hours agoparentprevcouldn't you run an absolutely minimal windows install in a virtual machine that boots up in the background and uses limited resources (1 CPU, 1GB RAM) and then use the virtual machine settings to share the OneDrive folder between host and guest? reply SushiHippie 11 hours agoparentprev [–] With 3rd party do you mean this one? https://github.com/abraunegg/onedrive/ Or which one did you try? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Winlator is an Android app that lets users run Windows applications using Wine and Box86/Box64.",
      "The project has been developed with the assistance of ptitSeb and the Wine developers.",
      "The latest version of Winlator can be downloaded from the official website, which is copyrighted by BrunoSX."
    ],
    "commentSummary": [
      "The discussion revolves around running Windows apps on different platforms like Android and Linux, with users sharing their experiences and recommendations.",
      "Specific topics include running games, Microsoft Office applications, and music production software.",
      "There is a particular emphasis on using Wine, a compatibility layer, to run various programs."
    ],
    "points": 269,
    "commentCount": 72,
    "retryCount": 0,
    "time": 1705957047
  },
  {
    "id": 39088551,
    "title": "Diversion: A Next-Gen Cloud-Native Alternative to Git",
    "originLink": "https://news.ycombinator.com/item?id=39088551",
    "originBody": "Hi Everyone! We’re Sasha and Egal, co-founders of Diversion (https:&#x2F;&#x2F;diversion.dev). We’re building a modern, cloud-native version control. Our first users are game developers, who like its simplicity and scalability. See a quick demo here: https:&#x2F;&#x2F;youtu.be&#x2F;DD0XkL8kDYcWhy a new VCS? There is no doubt that Git vastly improved our lives, and played a significant role in the advancement of software development over the past 18 years. But - it was built for a very different world in 2005 (slow networks, much smaller projects, no cloud), and is not the perfect tool for everyone today.The biggest drawback of Git is its limited scalability - both in repository and file sizes, and the number of concurrent users. This is the reason Google and Meta built their own version control systems. It’s also the reason why other large companies, most notably in games development, semiconductors and financial services are still using legacy tools like SVN and Perforce.Another issue we’re trying to fix is Git’s famous complexity. In our previous startup, a data scientist accidentally destroyed a month’s work of his team by using the wrong Git command (EDIT: we were eventually able to restore from a non-updated repo clone, after a few hours). As a developer who used CVS and SVN before Git was created, I often wondered why Git is so difficult to learn, compared to other tools.On the other hand, Git’s branching and merging abilities are exceptional - this has enabled the modern software development methodologies that we all take for granted today (e.g. feature branches, CI&#x2F;CD), greatly improving developers’ velocity.We were wondering - is it possible to create an easy-to-use, fast, scalable version control system, with Git’s branching capabilities? And what else can be improved, while we’re at it?One thing available in modern cloud tools is real-time collaboration (e.g. Google Docs, Figma). While developers don’t necessarily want their work in progress to be visible to everyone, it may be very useful to easily share it when you want to get feedback before a commit, to detect and prevent merge conflicts, and to have visibility into which parts of the codebase are being changed by others.Diversion is built on top of distributed storage and databases, accessible via REST API, and runs on serverless cloud infrastructure. Every repository operation is an API call (commit, branch, merge etc.). The desktop client synchronizes all work in progress to the cloud in real time (even before a commit). Users can work with Diversion using an interactive CLI, Web UI, or IDE plugins (currently JetBrains, more coming soon). The Web UI allows to perform most basic operations, without needing to install a desktop client.Diversion is compatible with Git, and can synchronize with existing Git repositories (each new commit in Diversion goes into Git, and vice versa). We’re planning to release it as open source once the code base matures, and when we implement an open source repositories directory on our website (naturally, Diversion’s code is managed on Diversion!)We’re in open beta, you can try it here (https:&#x2F;&#x2F;diversion.dev) (click Get Started). It’s completely self-service and there’s no need to talk to anyone, and it’s free for small teams (https:&#x2F;&#x2F;diversion.dev&#x2F;pricing).Building a version control is hard (as we have learned), and Diversion still has a long way to go. We are currently working on improving speed, CI integrations, plugins to IDEs and game engines, and other usability improvements. We would love to hear your thoughts and feedback on what we’ve got so far!",
    "commentLink": "https://news.ycombinator.com/item?id=39088551",
    "commentBody": "Diversion (YC S22) – Cloud-Native Git Alternative269 points by sasham 21 hours agohidepastfavorite382 comments Hi Everyone! We’re Sasha and Egal, co-founders of Diversion (https://diversion.dev). We’re building a modern, cloud-native version control. Our first users are game developers, who like its simplicity and scalability. See a quick demo here: https://youtu.be/DD0XkL8kDYc Why a new VCS? There is no doubt that Git vastly improved our lives, and played a significant role in the advancement of software development over the past 18 years. But - it was built for a very different world in 2005 (slow networks, much smaller projects, no cloud), and is not the perfect tool for everyone today. The biggest drawback of Git is its limited scalability - both in repository and file sizes, and the number of concurrent users. This is the reason Google and Meta built their own version control systems. It’s also the reason why other large companies, most notably in games development, semiconductors and financial services are still using legacy tools like SVN and Perforce. Another issue we’re trying to fix is Git’s famous complexity. In our previous startup, a data scientist accidentally destroyed a month’s work of his team by using the wrong Git command (EDIT: we were eventually able to restore from a non-updated repo clone, after a few hours). As a developer who used CVS and SVN before Git was created, I often wondered why Git is so difficult to learn, compared to other tools. On the other hand, Git’s branching and merging abilities are exceptional - this has enabled the modern software development methodologies that we all take for granted today (e.g. feature branches, CI/CD), greatly improving developers’ velocity. We were wondering - is it possible to create an easy-to-use, fast, scalable version control system, with Git’s branching capabilities? And what else can be improved, while we’re at it? One thing available in modern cloud tools is real-time collaboration (e.g. Google Docs, Figma). While developers don’t necessarily want their work in progress to be visible to everyone, it may be very useful to easily share it when you want to get feedback before a commit, to detect and prevent merge conflicts, and to have visibility into which parts of the codebase are being changed by others. Diversion is built on top of distributed storage and databases, accessible via REST API, and runs on serverless cloud infrastructure. Every repository operation is an API call (commit, branch, merge etc.). The desktop client synchronizes all work in progress to the cloud in real time (even before a commit). Users can work with Diversion using an interactive CLI, Web UI, or IDE plugins (currently JetBrains, more coming soon). The Web UI allows to perform most basic operations, without needing to install a desktop client. Diversion is compatible with Git, and can synchronize with existing Git repositories (each new commit in Diversion goes into Git, and vice versa). We’re planning to release it as open source once the code base matures, and when we implement an open source repositories directory on our website (naturally, Diversion’s code is managed on Diversion!) We’re in open beta, you can try it here (https://diversion.dev) (click Get Started). It’s completely self-service and there’s no need to talk to anyone, and it’s free for small teams (https://diversion.dev/pricing). Building a version control is hard (as we have learned), and Diversion still has a long way to go. We are currently working on improving speed, CI integrations, plugins to IDEs and game engines, and other usability improvements. We would love to hear your thoughts and feedback on what we’ve got so far! hintymad 15 hours ago> Cloud-Native Git Alternative Not sure if this is a good summary of the product. For one, cloud-native is an implementation detail, unless the company plans to sell the new VCS as packaged software instead of service. For two, I'm not sure how being cloud-native addresses any issue with my daily interaction with Git. > The biggest drawback of Git is its limited scalability I wonder how many people really has this problem. Millions of people have been using github and gitlab. I'm curious about the percentage of users who feel that there is a scalability issue with their own repositories. Personally, I don't have any beef with git's scalability at all, even though the companies I worked for had anywhere between hundreds to tens of thousands of engineers. Maybe having a monorepo will lead to scalability problem? But monorepo is a debatable topic, to say the least. > Diversion is built on top of distributed storage and databases, accessible via REST API, and runs on serverless cloud infrastructure. Every repository operation is an API call (commit, branch, merge etc.). The desktop client synchronizes all work in progress to the cloud in real time Again, how does this have to do with me, a user? Why would I care about the underlying protocols when I simply use a CLI or a UI? reply sasham 14 hours agoparentMonorepos are indeed causing problems with git, and this is one of the main arguments against them (see [1]). Some companies are building their own solutions (Google, Meta), and some are splitting their monorepos because of these problems. IMO if a company wants to run a monorepo for their reasons, they shouldn't be limited by their VCS. The technical details are for the readers who want to know, I agree it's not really important for the users (most of them, at least). [1] https://medium.com/@mattklein123/monorepos-please-dont-e9a27... reply hintymad 14 hours agorootparentHaving an effective monorepo at scale will also require an entire infrastructure to solve all the problems that a poly-repo must solve and more. In particular, - Partial download, as a monorepo will quickly grow too large for a single person to download. This is trivial for poly-repo but requires dedicated system for monorepo. - Dependency management. With a decently sized monorepo, one can't compile everything and test everything. So, someone needs to build a dependency manager to track all the DAGs, and build only the DAGs that are impacted by a commit. One also has to build a trackign mechanism for deploying different build artifacts because a team may deploy all the build artifacts in different date and time. We will need more sophisticated build tools too. - Build infrastructure. Even with a perfect dependency-tracking system, we may still end up building large-enough source code that we need to build the code in parallel. - Directory-level access control. This is also trivial for poly-repo since the granularity is at repo-level, but it requires dedicated implementation for a mono-repo. I'm not sure if the marginal benefit of having a monorepo can justify the investment for most of the companies. Google created monorepo initially to manage the dependencies of C++ code, and Perforce already supported partial downloads. But with more modern languages that have their own way of dependency management? I'm not so sure about the benefits. Making refactoring easier? How many repos are really shared at source level across multiple teams in a company? Encouraging sharing source and therefore knowledge? Isn't it a solved problem? Any decent company allows searching source code at semantic level across multiiple repos. If I want to see the source code of a particular package in my IDE, it's just a click away. Note I'm emphasizing marginal return of monorepo. Case in point, Google maintains the very use Guava library, which is probably used by millions of engineers. Does it lead to pains of incompatibility errors at runtime across different releases? Absolutely. Is it worth changing my poly-repo to monorepo to solve the problem? I highly doubt so. The compatibility issue happens rarely given good testing setup. When I do need to migrate my code, the cost is bi-modal: either the refactoring is trivial, or it requires serious testing and design changes, which a monrepo will not help anyway. Note I'm not saying that monorepo is not useful. Instead, I question how many companies will benefit from switching to monorepo, which may lead to the discussion on the potential market share of Diversion. reply jitl 10 hours agorootparentPolyrepo is such a pain in the ass though. At a smaller scale it's much, much nicer, and then when we get big enough to hit all those scaling problems, we'll be able to afford it by hiring a team of 3 to go implement Bazel/Buck2/..., and perhaps switching from Git to Diversion. reply foofie 14 hours agorootparentprev> Monorepos are indeed causing problems with git, and this is one of the main arguments against them (see [1]). The article is a disappointing read. It spends a lot of time talking about monorepos and how they spell all sorts of trouble. Yet, the article makes zero mentions of submodules as a way to get the best of both worlds. reply kristjansson 14 hours agorootparentSubmodules are great, but they're hardly an alternative to monorepos. reply sitzkrieg 11 hours agorootparentive never seen positive feedback for submodules before reply carlthome 13 hours agorootparentprevWhy not? Just want to understand. reply jitl 10 hours agorootparentSubmodule workflows have a lot of overhead at review time. During development it's fine, you work with the fully materialized tree just like it's a monorepo. But once you need to submit your changes for review, how does that workflow look? 1. Commit in submodule A, then get it reviewed and merged as SHA 123 2. Update submodule A to 123, get it reviewed 3. Reviewer has feedback on usage of new API in submodule A 4. Make another PR on A, at commit 457. This time don't merge it since reviewer on main repo might have more feedback. Monorepo: 1. Make PR to monorepo 2. Get review feedback 3. Push changes to PR branch 4. Merge 5. Update submodule to 456, push to existing PR ...?? reply foofie 45 minutes agorootparent> But once you need to submit your changes for review, how does that workflow look? 1. Post PR to submoduke A. Get it merged. 2. Post PR to the main repo updating it to point to subproject A. Done. The only difference between a monorepo and splitting the repo into submodules is that the main repo's history is coarser and basically tracks the output of integration tests. There is no need to overcomplicate things, and if you need to overthink them anyway then you have far more degrees of freedom to worry about in monorepos. reply Cthulhu_ 12 hours agoparentprevRe: scalability, in the very first sentence they mention game development, which deals with large quantities of large (and growing), nowadays versioned assets like 3D models, textures, animations, etc. reply hintymad 11 hours agorootparentAh, I guess this is the curse of ignorance: I saw the sentence but didn't register its significance as I'm not familiar with what's required in game development. reply gertop 11 hours agorootparentRespectfully, your entire comment can be summed up as \"works for me\". I have first hand experience with storing large assets in git, which is the use case they mentioned. Git is just bad at it. From performance issues to corruption to bugs, it's simply not something the git team cares about. LFS is a quirky afterthought. reply hintymad 8 hours agorootparentYeah, possibly. I have only visibility to the teams I worked with. That's partly why product market fit is hard to find, as it relies heavily on intuition. I'd be happy if I'm wrong. reply myfonj 16 hours agoprevOut of curiosity, are there any VCSs that operate on AST instead of plaintext lines? (Or is something like this being developed or proven impossible?) I guess it should be possible to cooperate on shared codebase without need for every contributor to check in and out text files following exactly the same formatting. Or even naming convention. Or even same language, provided all collaborators can transpile to and from some agreed-upon shared AST target. I know it might seem unhinged at first, but think about it: your (parseable) code is representation of the tree anyways (with some unrelated \"whitespace fluff\" around). If you follow strict formatting rules that you can express programmatically, you can recostruct that \"fluff\" from bare AST. If you can store all your violations against your style near the code, you can even sin and break it. If you store data about what you need to see differently from the shared AST - local renames of variables, for example - then you should be able to use your own naming convention, formatting and even source language, without bothering collaborators with tabs/spaces, hungarian notation or the fact that you prefer some different dialect or metalanguage. reply gsuuon 6 hours agoparentNot (just) a VCS, but this is the idea behind the Unison language: https://www.unison-lang.org/docs/the-big-idea/ reply ironmagma 6 hours agoparentprevConsidering many languages' very own out-of-the-box tooling (e.g. gofmt, syn) often have glaring gaps[1][2] in the understanding/roundtripping of the language's AST constructs, I would never be able to trust something like this to store and restore my code. [1] https://github.com/golang/go/issues/20744 [2] https://github.com/dtolnay/syn/issues/782 reply mcdonje 4 hours agoparentprevIt would be cool to integrate Tree Sitter into a VCS. It'd be more flexible if that were an option for a project/folder/file, but also offer a text diff option for readmes/docs or for if someone is using the VCS to write a book or something. reply e12e 15 hours agoparentprevI believe the smalltalk vcs Monticello work on a semantic level? https://eng.libretexts.org/Bookshelves/Computer_Science/Prog... reply psantosl 3 hours agoparentprevPlastic SCM developed Semantic Merge and diffing about a decade ago reply distortedsignal 16 hours agoparentprevI was thinking about trying this out, but there are some reasons why I don't think it's feasible. Where are your comments stored? What happens when you need to run out in the middle of a fire and you don't have time to make your code compile-able? How do you commit \"un-compile-able\" changes? I think there are some really compelling reasons to try AST-checkin - all your loops can now be changed to functional, dialect changes like you mention, etc. - but there are some pretty significant downsides as well. reply lowbloodsugar 15 hours agorootparentNodes in the AST for comments, block comments and \"raw text I don't understand\" seems like a way to go? reply distortedsignal 13 hours agorootparentHonestly yeah. Might have to give this another go. reply n42 15 hours agorootparentprevthese are both already solved issues that IDEs deal with with red-green trees reply Kharacternyk 16 hours agoparentprevThis would enable some advanced merge conflict resolution strategies, I suppose. However, it can also be done by building the ASTs on demand and still storing plain text. reply nolist_policy 12 hours agoparentprevYou can do most of this in git via custom diff-driver and smudge/clean filters. For example git can already convert line-endings on the fly for windows. This is special-cased, but can just as well be implemented via smudge/clean. Oh and git-lfs is done via smudge/clean too. reply ironmagma 6 hours agorootparentOne problem with that though is that smudge and clean are not used in rename detection. Git purposely skips running these filters to detect renames for performance. There are quite a lot of other issues with smudge/clean too though. reply a-dub 6 hours agoparentprevthere's some machine from the 70s that does this. iirc it stores all source code in an ast like representation alongside binaries and has some kind of built in version control. wish i could remember the name... reply a-dub 5 hours agorootparentahh yes, the rational r1000. an ada machine from the 70s that stored programs in a mixed ast/object data format called diana: https://insights.sei.cmu.edu/documents/948/1988_005_001_1565... reply ajross 15 hours agoparentprev> I know it might seem unhinged at first Not \"unhinged\". Most kids these days get their first introduction to computer programming using of of the many \"block coding\" environments, almost all of which are straightforward recapitulations of Javascript under the hood. And it works, and it avoids the problem of having to teach them how to deal with syntax errors before you teach them imperative logic. The reason people don't do this is that it's just a bad idea. The fact that all source code is stored in a universally understood data format with pervasive support across decades of tools is a feature and not a bug. How do you grep your AST to see if it's using some old API that needs to be refactored? Surely you'll answer that you use your fancy AST grep tool, which is not grep, and thus works differently for every environment. Basically every environment now has to have its own special editor, grep, diff, merge, etc... Even things like documentation generation and source control rely on files being text. And you're throwing all that out just to be different. Also, FWIW: it's optimizing the wrong part of the problem anyway. The total cost to an organization that develops and deploys software of any form is overwhelmingly dominated by tasks like debugging and documentation and integration. The time spent actually typing correctly-formatted text into your editor is a vanishingly small fraction of software development, and really that's all this helps. reply throwaway69123 8 hours agoparentprevIt would also alow the file structure to be relevant to source control, users could customize how the methods in a class are organised. reply osigurdson 19 hours agoprev>> most notably in games development, semiconductors and financial services are still using legacy tools like SVN and Perforce I think this should be your elevator pitch. Don't focus too much on \"git complexity\" as most people already know git so it just creates an argument. Scalability, in terms of numbers of users is somewhat hard to argue as well (Linux kernel has 1000s of contributors). However, it is completely true that git does not natively handle large binary assets well. You can even quote Linus: \"I really don't know what to do about huge files. We suck at them, I know.\" reply oblio 19 hours agoparent> Don't focus too much on \"git complexity\" as most people already know git so it just creates an argument. I'd say this phrase is both right and wrong. It's right in the sense that it creates an argument. It's wrong in that it creates an argument with the peanut gallery of git experts. But guess what, most people using git are not experts. They're software developers who don't want to learn the intricacies of git (probably most software developers out there), they're software development adjacent folks (think data scientists, etc) who for sure don't want to learn the intricacies of git, etc. The \"common person\" using git will most likely resonate on the \"git complexity\" argument. reply philsnow 16 hours agorootparent> peanut gallery of git experts There are people who use git for its original purpose (kernel devs and very few others) and then there is the remaining 99% of people who essentially use “github flavored git”, using only three or four git subcommands and for the most part never needing to understand its intricacies. Unfortunately, although they are using git-the-chainsaw-shotgun with all the safeties on, it’s nonetheless a chainsaw shotgun and sometimes they’ll run into issues where they or somebody in their company needs to be an expert and figure out how to un-scramble an egg, so to speak. If a new VCS can solve the 99% case and never need users to fall back to understanding nitty gritty details, it could very well have strong takeup especially among people who don’t give a crap about what VCS they’re using as long as it gets out of the way and doesn’t make them think (game devs, data folks, etc). reply ffgjgf1 1 hour agorootparent> If a new VCS can solve the 99% case and never need users to fall back to understanding nitty gritty details Mercurial was a bit like that? Yet it didn’t stand a chance against git regardless reply oblio 15 hours agorootparentprevI see 2 huge barriers for a new commercial VCS: 1. Devs don't like to pay for core tools. And VCSes need network effects. 2. VCSes seem to be really hard. reply sasham 8 minutes agorootparentTotally agree, it won't be easy. Companies do pay for GitHub/GitLab/Perforce though, and for indie devs there's the free tier. I think what made git really take off is actually GitHub's free tier/OS hosting, and not git itself being free (at least for parts of the market, and I might be wrong). 100% correct about VCS development, it's much harder than one can expect. reply gavinhoward 11 hours agorootparentprevAs someone who is making one (not Diversion), I agree with both of your points. Do you think that a VCS that solves the large file problem and binary file problem could succeed? reply withinboredom 16 hours agorootparentprevLearn your tools or one day you’ll lose a finger, or worse, your life. — high school shop class reply oblio 16 hours agorootparentYet software is not a chainsaw and a huge amount of people never learn to use their software tools. Especially since they're 100x more complex than hardware tools and nobody has time to master everything. reply withinboredom 15 hours agorootparentIf you are using a chainsaw in a wood shop, you are probably doing something wrong. The saying “learn your tools” means to spend some time learning your options and what is available to you, learning the “gotchas” and why. Woodworking tools are rather complex with “gotchas” that will kill you in less than a hundred ms. Using Git isn’t much more complex than using a lathe (simpler even, as you can get by with no skill and rote memorization). Taking a weekend to learn the data structures, and how everything fits together is not a hard ask. Especially since you literally only have to do it once in your entire career. reply oblio 15 hours agorootparent> Taking a weekend to learn the data structures, and how everything fits together is not a hard ask. Especially since you literally only have to do it once in your entire career. Like all simplifications, this is false. If all you do for years after is commit, push, merge, you'll forget. Especially since you'll need those brain cells to learn the new CPU/GPU architecture, the new JavaScript framework, the new corporate security policy, the docs from your internal architecture team, etc. Nobody's life revolves around intricate VCS details. reply withinboredom 15 hours agorootparentYou don’t need to memorize it for life. Jeez man, don’t be so hard on yourself. The point is, you know what is possible. You know what is impossible. 12 years later, something happens and you go … hmm, I used to know what is going on. I think I need to search for something about git-tree or something? The point is, you know what to search for, a starting point. You don’t just reach for git cherry-pick, but realize you can use git rebase --onto to copy/paste an entire branch. You don’t worry about merge conflicts because you only have to do it once with rerere. You learn git reflog will remind you what branch you were working on this morning before you got pulled into some shenanigans in prod. You can set up automation with global hooks. There’s so much you can know to do less work and you only need to remember the parts that are valuable to you. After learning git about 5 years ago stuff like ^ is all second nature to me, for nearly 10 years since switching from SVN. My first 5 years was just like you said. Commit, pull, commit, pull. I didn’t even know it could do anything else and I was worse for it. reply osigurdson 17 hours agorootparentprevIt is just a tough argument to make: the thing you have been using for your entire career and used almost everywhere is suddenly too complex. reply oblio 16 hours agorootparentUmmm... A lot of people just endure using git. Go to the average enterprise software shop, the ones where people don't code for fun in their spare time, and ask around. There are a lot more of those devs than unicorn and FAANG devs. reply osigurdson 13 hours agorootparentWhy aren't these teams choosing something else then? If everyone on the team dislikes git, switch to mercurial or something else. reply layer8 11 hours agorootparentBecause often it’s not the team that chooses, but tooling is instead standardized across the enterprise. And enterprises like to make the “safe” choice of choosing what’s most popular. And then there’s the whole aspect that you have to know some basic Git anyway to debug your way through the open source code you use (maybe not for JavaScript/NPM, I don’t know). Git also happens to currently be the most interoperable with other kinds of tooling, from CI to IDEs, so not using Git makes your life harder in ways unrelated to its inherent qualities. It’s a network effect in multiple dimensions. reply osigurdson 10 hours agorootparentHow did it get so popular if disliked by the majority of dev? reply stackskipton 10 hours agorootparentBecause generally people picking the tools are not majority of dev. They are architects or Senior Developers who do enjoy learning new things. Also, git generally does just work and most IDE/Source Control Systems take care of basic operation of pull/branch/commit/push/open PR. reply osigurdson 9 hours agorootparentI suppose. It might seem a bit perverse after all if the non-engaged, uninterested in coding, clock puncher devs got to make all the decisions. reply layer8 9 hours agorootparentprevPopular as in “everyone is using it”, not necessarily “everyone is fond of it”. How did Jira become so popular? The dynamics that lead to such outcomes are interesting, but hardly unusual. reply sasham 12 hours agorootparentprevGit might be the best thing there is today (outside of very large companies or environments with large binaries). It doesn't mean that'll always be the case... There were other VCSs before git, and there will be after. reply bigstrat2003 8 hours agorootparentprevI use git because other people use git, and ultimately I try to accommodate the tools that my peers are going to be used to. But I do think it's too complex (by a lot), and if I was running some kind of dictatorship I would never touch git again. Frankly, I think that git is a significantly worse tool than svn was for most use cases. reply osigurdson 7 hours agorootparentI like git better than svn personally. But, svn is better at managing large binary files. reply rajeevk 21 hours agoprevI have not analyzed the full potentials and benefits of Diversion but I would not agree with the statements you made about the Git. I think you should not focus on Git in your pitch. >>it was built for a very different world in 2005 (slow networks, much smaller projects, no cloud) Slow network: why is this a negative thing? If something is designed for a slow network then it should perform well in a fast network. Mush small project: I do not agree. I can say that it was not designed for very very large projects initially. But many improvements were made later. When Micorosoft adopted Git for Windows, they faced this problem and solved it. Please look at this https://devblogs.microsoft.com/bharry/the-largest-git-repo-o... No cloud: Again I would not agree. Git is distributed so should work perfectly for the cloud. I am not able to understand what is the issue of Git in the cloud environment. >>In our previous startup, a data scientist accidentally destroyed a month’s work of his team by using the wrong Git command This is mostly a configuration issue. I guess this was done by a force push command. IFAIK, you can disable force push by configuration. reply jsnell 20 hours agoparent> Slow network: why is this a negative thing? If something is designed for a slow network then it should perform well in a fast network. Designing for resource-constrained systems usually means you're making tradeoffs. If the resource constraint is removed, you're no longer getting the benefit of that tradeoff but are paying the costs. For example, TCP was designed for slow and unreliable networks. When networks got faster, the design decisions that made sense for slow networks (e.g. 32 bit sequence numbers, 16 bit window sizes) became untenable, and they had to spend effort on retrofitting the protocol to work around these restrictions (TCP timestamps, window scaling). reply fourside 15 hours agorootparentThat makes sense but then the pitch should include something about how back in 2005 the design for git had to make a trade off because of X limitation, but now that restriction isn’t applicable which enables features A and B. I don’t really see what trade offs a faster network enables other than making it a requirement that you have a network connection to do work (commits are a REST call). I’m not sure that’s a trade off I’d want in my VCS, but maybe I’m just not the target audience for this. reply funcDropShadow 21 hours agoparentprevEven a force push doesn't destroy the reflog or runs the GC server-side. I wonder how you can accidentally loose data with Git. I've seen lot's of people not being able to find it, but really destroying it is hard. reply sasham 20 hours agorootparentHe force pushed a diverged branch or something like that, and we only found out after a while. We were eventually able to recover because someone didn't pull. But it was not a fun experience :D reply bauruine 20 hours agorootparentSo multiple people did a git reset --hard origin/master and nobody complained or checked what and why this was done? That's not \"one data scientist with the wrong command\" but the whole team that fucked up hard IMHO. reply swells34 19 hours agorootparentI think you just sold their pitch with this comment... I, like many many people here, have done quite a bit of product design. What do you call it when a bunch of people use your product, and it breaks for several of them? That generally indicates your product is weak, or has a very rough UI. reply MrDarcy 18 hours agorootparentThe pitch simply wasn’t true. Data was not destroyed and was restored hours later. reply lowbloodsugar 16 hours agorootparentFor many of us, the story rings true. We have ourselves had horror stories that we did manage to recover from after a few hours of fearfully googling, and we know of other, less capable friends and colleagues who were unable to recover the data and who just accepted the loss. reply TexasMick 14 hours agorootparentIt's kinda crazy argument, I think data loss is way more likely with a centralised system than a decentralised system. reply lowbloodsugar 5 hours agorootparentYou think Microsoft losing GitHub repos is more likely than poor bastards trying to make sense of the git command line? You think these guys are going to do a worse job with their centralized service? reply TexasMick 1 hour agorootparentPeople have lost data on GitHub from repositories being copyright striked for example. At least with git, every developer has a copy of the full history so full data loss is impossible really. What happens if this company folds? You're left with some proprietary repo that you suddenly have to workout how to self host. It just doesn't make sense when compared to just learning git which is definitely the most fruitful thing a developer could learn at the start of their career. reply swells34 18 hours agorootparentprevIt's a pitch. The story has obviously been embellished and polished and condensed, ready public consumption. Being pedantic against it is not productive. reply MrDarcy 16 hours agorootparentPolitely disagree. It’s productive because hopefully future teams who launch on HN ask each other, “Is what we’re saying true?” during all those polishing and condensing sessions. If they don’t, the risk is crossing a line that damages the reputation of the team and undermines months if not years of hard work. reply spiderice 17 hours agorootparentprevThat's a creative way to defend a dishonest pitch reply yjftsjthsd-h 17 hours agorootparentprevIf the pitch is dishonest, why would I ever trust them with something as vital as my VCS? (And yes, \"embellished\" means dishonest) reply water-your-self 15 hours agorootparentprevThis is not a pedantic criticism. reply Wytwwww 12 hours agorootparentprevBut that seems like pretty much the equivalent to \"rm -R *\"? And also just a permission/configuration issue. reply sasham 11 hours agorootparentTo put into perspective, that was in 2014 :D There were no branch protections, and git was even harder to use. Plus everyone was new at git, obviously (we started in 2013 with mercurial, which was still a legit thing to do, and switched to git). reply mlhpdx 9 hours agorootparentYeah, these days stopping force pushes is a checkbox (default?) in GitHub. reply bauruine 11 hours agorootparentprevOr drop table|database or delete from. To _nearly_ lose data it took multiple clueless engineers and not detecting the issue for months. I wonder how Diversion handles operations that possibly delete data. Whats their solution? reply lonelyasacloud 19 hours agorootparentprev> but the whole team that fucked up hard IMHO. Multiple individuals with similar problems would tend to imply systematic inadequate training. Or the enterprise concerned adopting an inappropriately complex system for its intended userbase. reply willy_k 18 hours agorootparentOr, git is both very complex and very useful, and a large portion of its users have a poor understanding of git but enough for it to be a useful tool. If you want to do source control (which you do), then you’re investing time into learning git and/or fixing git, or maybe using a project like this. reply keerthiko 17 hours agorootparentYou literally just said what GP said in different words, but prefaced it with \"Or\" as if it's a disagreement. What you said boils down to \"inadequate training\". reply willy_k 17 hours agorootparentWe both agree that they didn’t know the tool, but GP seems to blame them for deciding to use the tool without training. I was more or less defending their choice to use git, while also acknowledging the potential of a tool like Diversion. My interpretation of GP was that it doubled down on git, while claiming that anyone using git without understanding it is “doing it wrong”, which I agree with in principle but not in practice, as I argued in my initial comment. reply NateEag 19 hours agorootparentprevAnd the tool made a screwup that hard not only possible, but very difficult for the victims to recover from. Doesn't say a lot for git's usability. reply JeremyNT 18 hours agorootparentA couple of thoughts about this: One is that the possibility of overwriting history / etc is a really powerful and useful feature, but one that should only be used with some consideration, hence being gated behind the scary '--force'. The fact that git provides one the ability to discard and overwrite commits for a ref shouldn't be an endorsement of doing so freely. I'm glad git has this capability though and any \"git alternative\" would be all the worse if it didn't provide it, IMO. Two is that if the concern is git's usability - i.e. the \"problem\" here is that it's too \"easy\" for users to do destructive actions accidentally - well, there are ways to solve that other than to reinvent all of git. There are plenty of alternative git UIs already, and an alternative UI is a great way to be \"wire compatible\" for existing users but still help protect those novice users from footguns. reply NateEag 15 hours agorootparentThat all makes sense and mirrors many of my own thoughts. Though I'll say that \"--force\" isn't necessarily a \"scary-sounding\" option name unless you're used to Unix CLI naming conventions. Further, the warnings git gives you about this are virtually inscrutable if you don't already understand what's happening. A good interface to \"blowing away history\" would give you a brief summary of what will actually be gone, e.g.: \"If you go ahead with this overwrite, the following changes will be completely removed from the repo: a3bf45: Fix bug in arg parsing 22ec04: Add data from 2024-01-17 scraper run ... Are you SURE you want to completely destroy those commits? (Y/n)\" and if user says \"Y\", output should log all removed commits and also say: \"These commits can still be recovered until . If you realize you want these back before then, run the following: \" Generally, I think it's a mistake to put UI improvements in a secondary tool. If there are issues that need fixing, get those changes in the canonical project, because layered patches on top will always be short of maintainers and behind the main project. reply xorcist 13 hours agorootparent> Are you SURE you want to completely destroy those commits? (Y/n) While there is a lot of user interfaces that could be improved, I believe the above have empirically been shown to be inferior to the alternative \"re-run this command but add scary option to proceed\". Users habitually answer \"Y\" to questions like the above all the time. And certainly after a few times it becomes routine for anyone. But having to re-enter the command and type some a whole word like \"overwrite\", \"force\" or \"i-know-what-im-doing\" is a whole other roadblock. The example is especially ill-chosen to have Y as the default option. Any operation in git that destroys so many commits will include a list of commits that is destroyed, similar to what is suggested here, and trying to push the resulting repository will say exactly how many commits will be removed, and require rerun with force option (together with the necessary privileges). So reality is already not far from what you suggest, but with more fail safes. reply rusk 18 hours agorootparentprevIt doesn’t overwrite the commute though. It inserts new ones and resets the branch pointer … doesn’t seem like you’d need a whole new tool to mitigate this - just an automatically generated tag or something when you —force-push - would be easy to do if there was demand for it … reply funcDropShadow 18 hours agorootparentprevThey used --force which is usually the flag to say: Here there be dragon. Be careful. reply mbreese 15 hours agorootparentYeah, I can’t see how use of a —-force flag by people who didn’t know what they were doing is enough of a reason to switch to a different VCS (let alone write one). The issue was people using a tool in a way that they shouldn’t have. Which isn’t a technical problem, but a training problem. You can’t fix people problems with technology, so I’m sure there will be other footguns in this new system that someone else will figure out how to almost lose data. Git is great in that it is flexible and powerful. But that power leaves some tools open to people who don’t know what they are doing… that’s the trade off. (Now something that better handles non-code assets and large data files, I’d be much more willing to listen to that pitch.) reply umanwizard 20 hours agorootparentprevSo the work wasn’t actually destroyed, and you were able to recover it. So all the people pointing out how implausible that part of your pitch was were right, and you were in fact just lying. reply sasham 20 hours agorootparentThat's really not the main point of the post, but you're right I should have been more precise. Edit: updated in the top text now! reply ysavir 20 hours agorootparentI think the point being made is that you spent a lot of your opening post talking Git, and lead with that bit, rather than with Diversion. What makes Diversion different is added in the end, after you've spent time trying to convince Git fans that their current tooling isn't good. Worse, the examples you listed of why Git is bad is more reflective of configuration and processes than Git itself. This is ultimately a very weak pitching strategy. The first thing you convey to your potential users is insecurity--an insecurity that people won't choose your product over Git. And it's hard to want to buy something from someone that isn't secure enough about their product to pitch the product first, and answer questions/make comparisons after, as a form of clarification. Alternatively, instead of doing a comparison to Git, you could start with a list of \"have you experienced these Git issues? . Here's how Diversion improves on Git in this regard.\" In this case you're actually solving people's problems, rather than looking like you're grasping at straws to complain about Git and justify an alternative. FWIW, I personally have 0 interest in a cloud-first version control. I like the cloud as a form of backup and syncing with team members, but I ultimately want a version control that works as well offline as it does online, and prioritizes the local experience. reply umanwizard 19 hours ago [flagged]rootparentprevThe main point of your post is how much better you are than git. You support this main point by making up lies about git. This does not make me personally interested in trying your product. reply IlliOnato 17 hours agorootparentprevFrom my point of view, it's not that much about lying, for me the OP demonstrates a degree of incompetence of the post writers about Git. The fact that they don't seem to fully understand working of Git (not on the level of Git developers, just the level of Git administrators/users) does not inspire trust in their competence to create a Git alternative. reply sbergot 19 hours agorootparentprevFor your information you can use the reflog command to find the previous head commit and restore your branch. It takes 10 minutes and then you learn to disable force pushing on the main branch. reply swells34 19 hours agorootparentI find it funny how many comments in this angry rebuttal section actually endorse a Git replacement. reply plagiarist 19 hours agorootparentIt's an interesting new application of that joke, \"when I have a question on Linux I use a sock puppet account to leave an obviously wrong answer which prompts dozens of corrections.\" I'm trying to imagine how to generalize this to other products. I think if I state the competing product has negative feature X, but also intentionally get some details confidently incorrect or deliberately feign incompetence, you get a group of people confirming X. reply spiderice 17 hours agorootparentprevI find it funny how many comments you've made in this thread missing the point. People are reacting against the dishonest pitch, not the product. reply renewiltord 16 hours agorootparentprevJust somewhat surprised because if anyone did a `git pull` they'd get divergent history and therefore a merge on default configuration. It would take a lot of manual work to ruin more than one copy of the repo. reply lawgimenez 20 hours agorootparentprevSo you were able to recover and did not lost a months work of data? Your story just doesn’t make sense. Come on. reply egaldv 19 hours agorootparentIndeed you're right the work that was erased from BitBucket was restored from one of the employees that didn't yet pull, the post was edited accordingly. reply umanwizard 19 hours agorootparentWouldn’t you still have been able to recover it even if everyone did pull, assuming GC had not run on everyone’s machine? reply sampo 11 hours agorootparentprev> the work that was erased from BitBucket was restored from one of the employees that didn't yet pull Actually those commits that you considered lost, were still stored on everyone's personal computer in your team. You just didn't know how to use `git reflog` to find them. reply grumbel 17 hours agorootparentprev> doesn't destroy the reflog or runs the GC server-side. Git doesn't give you access to the server side reflog either. So it's of not much use if you don't control the server. As for losing data with Git, the easiest way to accomplish that is with data that hasn't been committed yet, a simple `git checkout` or `git reset --hard` can wipe out all your changes and even reflog won't keep record of that. reply xorcist 16 hours agorootparentThat data not committed to git can not be recovered by git should hopefully not surprise anyone. Neither is it the fault of your version control system, or any other system really, if you cannot access your server and are without backups. reply bvrmn 17 hours agorootparentprev> As for losing data with Git, the easiest way to accomplish that is with data that hasn't been committed yet Also Git has pretty awful behavior losing changes when one doesn't press \"Save\" in their IDE. Bad, bad Git. reply Spivak 16 hours agorootparentYour applications also shouldn't lose work when you don't press save, this is the entire impetus for the \"recover unsaved work\" in most document editors. A version of Git that shunted uncommitted changes to a special named stash whenever you did anything destructive would be a positive thing. It's what I end up doing manually anyway but why make a system where the default behavior is destructive and I have to remember every. reply whartung 13 hours agorootparentI can't say I'm widely traveled, I have no idea how desktop Office works, but Apple does this so well. Using their desktop apps, Pages, Keynote, Numbers, TextEdit, Preview, I never hit \"Save\". I just close the apps. When I come back, the windows reopen right where I left off. I wish emacs did this. I honestly don't know what it would be like for a code editor to be \"constantly saving\". I guess I would adapt, but there are times when I do all sorts of changes and go \"Ah, this isn't right\" and just kill the buffer. The ultimate undo. But there's a great feeling, to me, when I go to close the app (or shutdown the computer) and it just closes. No prompts, no warnings, just saves its state, shuts down, and comes back later. And with the ever popular \"naming things\" issue of computers, I have a bunch of just \"Untitled\" windows. They're there when I open the app, and that's all I need to know. The nag factor and cognitive load reduction of that is just unmatched. \"Just deal with it, I'll come back later, maybe, and clean it up\". One less thing. reply xorcist 16 hours agorootparentprevIt may be prudent to note that git by default is rather kind in that way that it will not change your data unless you explicitly force it to with --force or --hard. I think git, as hard to learn as it can be, sometimes have a bit of an unfair reputation here. It's not all bad. Not only is it quite careful about not losing data, someone actually took the time to make it spit out messages that not only describes what just happened, but also gives suggestions of what to do next depending on how the user wants to proceed. That adds a level of discoverability that is usually associated with dialog based guis. The quality of these messages can sometimes be surprisingly good, far from the Clippy-level helpfulness you sometimes see. There are a few exceptions to the principle of not losing local changes, where you explicitly restore an old version of a file for example. But saying the default behaviour is destructive really gives a false impression. But yes, you are absolutely right that a system to recover unsaved work is a good thing, but I would argue that it belongs at the editor level, not in a version control system. A user could have a number of files open that have local changes. The editor has a much better idea in which order changes were made, and which changes hasn't even been committed to disk yet. reply Brian_K_White 17 hours agorootparentprevA month of work for a whole team was never even committed or stashed let alone pushed? That is not a git problem. reply noufalibrahim 18 hours agorootparentprevI agree. It's quite hard to actually destroy data in git. Even with the so called \"destructive\" commands, walking through the reflogs can usually restore work that was accidentally deleted or whatever. reply billpg 18 hours agorootparentprevI configured my github to only allow commits with an anonymised email address. Time passed and I used another machine on which I had already opened that repo before. I pulled my recent work successfully, wrote stuff and then committed and pushed. Github rejected my commit as I had the wrong email address. I then had to try and work out how I delete a commit but keep all my changes so I could commit it all again but with the correct email address. I'm not sure exactly what I did but in my ham-fisted experimentation I deleted the commit and restored my local copy back to the way it was before my commit, losing all my work that day. reply BossingAround 18 hours agorootparentIf you had already committed, `git reflog` should have still found your changes (even after you deleted the commit and restored the local working tree) unless you deleted and re-cloned the repository. reply fesc 16 hours agorootparentprevHonestly I don’t understand why not more people use a GUI for git. What you describe would be 1 Minute of work and maybe 10 clicks with a very low probability of shooting yourself in the foot in Tower. reply cqqxo4zV46cp 20 hours agorootparentprevDestroying it and nobody knowing how to recover, or that it can be recovered at all, it are identical. reply sasham 20 hours agoparentprevThanks! We're definitely not trying to bash Git, it's done a lot of good for software development and for sure is going to continue evolving. Git had much more edge when it was competing vs SVN and other centralized VCSs. With 10Mb networks (if you were in office) you could feel physical pain when committing stuff > Thanks! We're definitely not trying to bash Git, it's done a lot of good for software development and for sure is going to continue evolving. It is not about bashing git; it is about anchoring your argument of why Diversion is a better alternative around git. You're basically taking your game/arguments to their playing field, and thus will have an uphill battle for mindshre. Instead, consider reframing the playing field and mention git less (if at all). Something like \"the future of version control is blah\". Surprise us, talk to us about your vision for source control, or better yet, code and multi-discipline collaboration (e.g. between eng and design), etc. reply bruh2 18 hours agorootparentI personally would not bother reading any \"the future of X\" if it did not address problems of existing tools. I know you're trying to give advice from a marketing pov, and it is good, but it's also inherently bulshitty – because its purpose is to net more sales rather than actually make a good argument reply asimpletune 17 hours agorootparentprevI'm not sure I understand this at all. > The problem is the way Git works, it clones your entire repository into the container with your cloud environment, using a slow network protocol. What about git's network protocol is 'slow'? I think I can also come up with a pretty simple experiment to prove or disprove this: 1. Fill a file with 13Gb of data and commit it. 2. Upload that to GitHub or wherever you want 3. Time how long it takes to clone and compare that to the real GitHub.com You will find the one we made takes 'seconds' (or minutes, depending on your network connection), while the the GitHub.com will take some time. So, same data, two different results? The difference in this experiment rules out the 'slow' network protocol as the difference maker. The real reason is that the GitHub.com repo will have hundreds or thousands of commits. Basically, the difference is the commit history, because that's how git needs to work. Git stores the diffs for the entire commit history, not just the literal files at the HEAD. I don't know what the network protocol has to do with that. reply yjftsjthsd-h 14 hours agorootparentIt is perhaps worth pointing out that if you don't need the history you can just `git clone --depth 1` and save the network transfer and disk space. reply TexasMick 13 hours agorootparentIt reminds of when someone told me git submodules are slow. They just forgot about shallow clones.. reply nolist_policy 14 hours agorootparentprevIf you use the dumb http protocol, both cases should be equally fast. reply asimpletune 12 hours agorootparentgit clone https://github.com/github/docs.git 123.57s user 37.02s system 74% cpu 3:35.73 total git clone --depth 1 https://github.com/github/docs.git 3.37s user 1.83s system 35% cpu 14.521 total Not a scientific test at all, but the second one was literally 15x faster, wall clock time. reply dartos 20 hours agorootparentprev> We're definitely not trying to bash Git Using git with bash is the best way to use git (: reply gnarlouse 19 hours agorootparentCame here to make a similar joke reply funcDropShadow 17 hours agorootparentprevThat article also states that using a standard Git feature, shallow clones, you go from 20min to 90s. Most of the problems touched upon in the article are about state management for local environments, yes that can be tricky. And it can take time, but it has nothing to do with Git. reply vintagedave 20 hours agoparentprev>> a data scientist accidentally destroyed a month’s work of his team > This is mostly a configuration issue git apologism :) (FWIW I do agree with the rest of your comment, and I hope you forgive the slight joke. Product users, for any product are fallible humans. That might be fallible in accidentally deleting, or it might be fallible in forgetting to turn on the safety settings.) Very seriously, something like this should not be possible in a source control system. Data integrity needs to be built in by design. reply MatthiasPortzel 17 hours agorootparent> Data integrity needs to be built in by design It is built into Git by design. Git keeps commits around for 90 days even after they’re “deleted.” This is why people who understand Git were so skeptical of OP’s claim. The point that Git is confusing still stands, however. reply devjab 19 hours agorootparentprevThe issue with a lot of freedom and unopinionated tools is always going to be the multitude of ways to fuck up. On the flip-side, you may not like what choices are made if you’re forced to use it in a certain way. We enforce a strict pull-request squish commit with four eyes approval only. You can’t force push, you can’t rebase, you can’t not squish or whatever else you’d want to do. But we don’t pretend that is the “correct” way to use Git, we think it is, but who are we to tell you how to do you? We take a similar approach to how we use Typescript. We have our own library of coding “grammar?” that you have to follow if you want to commit TS into our pipelines. Again, we have a certain way to do things and you have to follow them, but these ways might not work for anyone else, and we do sometimes alter them a little if there is a good reason to do so. I don’t personally mind strict and opinionated software. I too think Git has far too many ways to fuck up, and that is far too easy to create a terrible work environment with JavaScript. It also takes a lot of initial effort to set rules up to make sure everyone works the same way. But again, what if the greater community decided that rebase was better than squash commit? Then we wouldn’t like Git, and I’m sure the rebase crowd feels the same way. The result would likely leave us with two Gits. Though I guess with initiatives like the launch here, is two Gits. So… well. reply oblio 19 hours agorootparent> But again, what if the greater community decided that rebase was better than squash commit? Then we wouldn’t like Git, and I’m sure the rebase crowd feels the same way. The result would likely leave us with two Gits. Meh, this is overrated. We'd end up with 2 Gits, and over time just one fork would probably take over, based on marketing, PR, dev team activity, etc. The second one would probably still be around but used by only a minor part of the community. Just because a thing has on paper many forks, does not mean those forks are equal. In fact, a situation with many major forks rarely survives the long term. See Jenkins vs Hudson, Firefox vs Iceweasel, etc. Most people will congregate towards one of the forks and that's it. reply dmazzoni 15 hours agorootparentprevWhat if someone pushes something inappropriate? Shouldn't there be a way to delete it? As an example, what if someone pushes: - A private key or password - Copyrighted content - Illegal content In cases like this, it needs to be possible to remove the bad commit from the repository entirely. reply layer8 11 hours agorootparentYes, but this should be only possible by way of commands that make it abundantly clear what you are doing, e.g. `git delete ` with extra confirmation “Do you really want to permanently and irrevocably deletein the master repository?”, or a more obvious “recycle bin” that presents deleted branches/commits in familiar ways and with explicit expiration dates. But the Git architecture doesn’t lend itself to that level of user-friendlyness. reply Rygian 20 hours agoparentprev> This is mostly a configuration issue. I guess this was done by a force push command. IFAIK, you can disable force push by configuration. If a feature can lead to actual unintended data loss, it should come disabled by default. Are there any other \"unsafe by default\" features in Git? What would be a sane general default that prevents unwanted data loss, and why is it the case? reply guax 20 hours agorootparent--force always imply data loss. You're overriding the remote state. Do people use it in an unsafe manner because they don't understand git and there lies a problem that could be tackled? yes. With that, I don't think git has any feature that is unsafe by default. reply sasham 20 hours agorootparentIn that specific case there was some error that the user didn't understand, he googled and found a StackOverflow answer with --force. And naturally tried it BitBucket didn't have branch protection back then, today it's a bit better (you can still destroy your work but usually not others') reply Einenlum 16 hours agorootparentI agree that git is very complex (just try reading its documentation and how many options or commands you have never heard of before). But I think push --force is probably one of the easiest git concepts to get. The fact that someone in your team copy pasted something from SO without understanding it doesn't seem to be related to git. Otherwise we could say that the fact some people lose their data through \"sudo rm -rf /\" proves the complexity of Unix. I don't think so. reply nullstyle 17 hours agorootparentprevThis was Pebcac my dude. git wasn’t at fault here, the script kiddy that pastes before understanding is the fault. Amateurs reply CJefferson 19 hours agorootparentprevMy biggest problem with git is branch deletion — if you never do it you end up with far too many, but deleting a branch can’t be version controlled. reply AdityaSanthosh 18 hours agorootparentJust curious, why do you want that to be version controlled? reply CJefferson 18 hours agorootparentBecause I might realize later I made a mistake, or I might want to view history. I’d I never cared about historical state and mistakes, I wouldn’t need version control at all :) reply couchand 18 hours agorootparentYou could delete the branches locally while archiving them to any another clone of the repo. reply renewiltord 16 hours agorootparentprevIt is somewhat version-controlled but not completely. If you use the reflog you can find it again and you can find how it moved around. But the reflog gets rewritten and gc'd so it's not true vc. reply IshKebab 20 hours agorootparentprev> With that, I don't think git has any feature that is unsafe by default. Well, you just mentioned `--force`. It is unsafe by default. Git has a couple of flags to make it safer (`--force-with-lease`, `--force-if-includes`) but those aren't the default. reply withinboredom 16 hours agorootparentIf you’ve ever had to remove private information from history before making the repos public (think domains, names, configuration, etc) you will appreciate the ability to rewrite history (and all the other things --force gives you) reply IshKebab 11 hours agorootparentI don't get your point. Nobody is saying don't use `--force`. Just that the default `--force` flag is the most dangerous variant. reply withinboredom 2 hours agorootparentI am not aware of any default use of force. Where does that happen? reply kristjansson 13 hours agorootparentprevThe feature is 'git push'. --force is the opt-in to the unsafe behavior. It should not be used lightly. reply IshKebab 12 hours agorootparentYou're missing the point. `--force` is the default of the force variants. The other `--force-but-something` arguments clearly modify that default. It's the wrong way round. Obviously they've done it for backwards compatibility, but the fact that they haven't even added an option to make it the default is pretty lame. reply dmazzoni 15 hours agorootparentprevBut it doesn't lead to data loss. The commits that were overwritten by \"force\" are still there on the server. Any admin could recover them pretty easily. They're probably still present in the local repo of the person who ran \"git push --force\" too, as well as anyone else's machine who has cloned the repo. The only way you'd actually lose data is if every single person who had a clone of the repo ran gc. Or apparently if nobody knew about \"git reflog\" and nobody bothered to do a Google search for \"oops I accidentally force pushed in git\" to learn how to fix it. reply couchand 20 hours agorootparentprevShould a chain saw come with the ability to start the engine disabled by default? reply IshKebab 20 hours agorootparentYes. That is a great idea. You could do something like a tab that you have to remove that tells you about chainsaw safety. reply couchand 20 hours agorootparentThe problem here is not the tool. The problem is the author's colleague's willingness to paste a stackoverflow answer into their terminal without taking a moment to understand what it does. If stackoverflow told them to break off the chainsaw safety tab there is no chance it would have been read first. reply aseipp 16 hours agoparentprevThe Windows Git repository is only 300GB, that's basically childs' play when people are talking about \"large repo scalability\". Average game developer projects will be multiple terabytes per branch, with a very high number of extremely large files, and very large histories on top of it. Git actually still does handle large files very poorly, not only extremely large repos in aggregate. The problem with large Git repositories is nowhere near solved, I assure you. reply laeri 16 hours agorootparentThis includes assets right or some kind of prebuilt data in custom formats? Otherwise it would be hard to have this much data in source files. reply aseipp 15 hours agorootparentYes, game development studios include their raw art and environment assets directly in source control, just like source code. That's because the source code and the assets for the game must go together and be synchronized. That also includes things like \"blueprints\" or scripting logic. Doing anything else (keeping assets desynchronized or using a secondary synchronization tool) is often an exercise in madness. You want everyone using one tool; most of the artists won't be nearly as technical and training them in an entirely different set of tools is going to be hard and time consuming (especially if they fuck it up.) But honestly, you can ignore that, because Git doesn't even handle small amounts of binary files very well. Ignore multi-gigabyte textures and meshes; just the data model doesn't really handle binary files well because e.g. packfile deltas are often useless for binaries, meaning you are practically storing an individual copy of every version of a binary file you ever commit. That 10MB PDF is 10MB you can never get rid of. You can throw a directory of PDFs and PSDs at Git and it will begin slowing down as clones get longer, working set repos get bigger, et cetera. The 300GB size of the Windows repository is mostly a red herring, is my point. Compared to most code-only FOSS repos that are small, it's crazy large. That kind of thing is vastly over-represented here, though. Binary files deserve good version control too, at the end of the day. reply Kiro 17 hours agoparentprevGit is bad for games and they should definitely compare them in their pitch if they want to capture that market. reply kfrzcode 17 hours agorootparentNo, it's not. LFS has improved over the years. Git is supported as a first class citizen in Unreal Engine 5 - alongside P4. reply Arelius 16 hours agorootparentJust because it has integrations, doesn't make it great. LFS is still not great. Doesn't have a lot of backends for instance. And a real locking system is table-stakes for a gamedev VCS reply Kiro 17 hours agorootparentprevGood for developers using Unreal Engine 5 I guess. Fact remains that most game developers struggle with Git. reply jasfi 16 hours agoparentprevThe complexity people think they face with Git can often be overcome with a good UI and/or tutorials. reply sasham 16 hours agorootparentIn part yes, e.g. lots of people like SourceTree. Some of the complexity is inherent though, e.g. local vs remote branches and the various conflicts & errors as a result. Git exists for 18 years, and yet the complexity problem wasn't solved yet. Other tools like SVN were never considered to be so hard to use / easy to screw up. reply lifeofguenter 18 hours agoparentprevHave you ever tried running Git in the cloud? :) Cloud-native and running things on “EC2” are very different things. reply sasham 17 hours agorootparentYep :) Lots of products run Git on EC2/containers, e.g. GitPod or GitHub Codespaces. Ironically, Diversion works much faster on these than git https://github.blog/2021-08-11-githubs-engineering-team-move... reply IshKebab 20 hours agoparentprev> When Micorosoft adopted Git for Windows, they faced this problem and solved it. On Windows. On Linux Git still doesn't scale well to very large repos. Before you say \"but Linux uses git!\", we're talking repos that are much bugger than Linux. Also the de facto large file \"solution\" is LFS, which is another half baked idea that doesn't really do the job. You sound like you're offended that Git isn't perfect because you like it so much. But OP is 100% right here; these are things that Git doesn't do well. It's ok to really like something that isn't perfect. You don't have to defend flaws that it clearly has. reply WorldMaker 17 hours agorootparent>> When Micorosoft adopted Git for Windows, they faced this problem and solved it. > On Windows. On Linux Git still doesn't scale well to very large repos. All of Microsoft's solutions for git scaling have been cross-platform. Even VFS had a FUSE driver if you wanted it, but VFS is no longer Microsoft's recommended solution either, having moved on to things like sparse \"cone\" checkouts and commit-graphs, almost all of which is in mainline git today. I also find it funny the complaint that git scales worse on Linux than Windows given how many Windows developers I know with file operation speed complaints on Windows that Linux doesn't have (and is a big reason to move to Windows Dev Drive given the chance, because somewhat Linux-like file performance). reply IshKebab 16 hours agorootparent`fsmonitor` is still only available for Mac and Windows. https://git-scm.com/docs/git-config#Documentation/git-config... reply WorldMaker 15 hours agorootparentFair enough, though there is a hook to provide your own on Linux: https://git-scm.com/docs/githooks#_fsmonitor_watchman reply graemep 18 hours agorootparentprevHow common are repos bigger than Linux? Linux also has the huge advantage of an ecosystem, tools and integrations. It is overkill for small projects and there are friendlier alternatives for those - but git wins because it is what everyone knows. Something aimed at the small number of large projects will suffer the same problem. reply gertop 17 hours agorootparent> How common are repos bigger than Linux? In terms of number of commits, Linux is probably bigger than most. In terms of storage size, almost any video game project will be significantly bigger. It's no secret that git is very bad at handling large binary files. reply graemep 15 hours agorootparentSo this is very specifically for things like games with large binary assets? reply IshKebab 12 hours agorootparentNo, large companies using monorepos will have repos much bigger than Linux even without large binary assets. Apparently Linux has ~10 commits per hour. I probably do ~10 commits per week. So a team of ~150 mes produces commits at a fast rate than Linux. Very rough estimate but it takes less than you'd think. Also if you vendor a few dependencies that quickly increases the size. reply Spivak 16 hours agorootparentprevYou don't even need game assets, your company's icon library is likely enough to tip the scales into territory git doesn't handle well. reply Wytwwww 12 hours agorootparentprev> really like something that isn't perfect. You don't have to defend flaws that it clearly has. Certainly true. But it's not clear at all how does the product solve these specific problems (they say \"Painless Scalability\" which sounds nice but did they try developing any 100+ GB projects with massive numbers of commits/branches on it?) reply jayd16 17 hours agoprevAs a game dev I find the pitch unexciting. > git is bad we're better Honestly, a modern git lfs workflow is really smooth. I think it handles binaries fine. Show me cumbersome git feature and why this works better. You can't just tell me tools I use every day are unusable. I think the main pain of git is if you want to put everything in a single repo. Big isn't a problem, getting just what a I need (checking out a single large model) is the problem. From the website I have no idea if this can do partial checkouts. I assume yes but its not stated at all. > cloud native A lot of studios want on-prem and self-hosted private cloud support. Cloud native is touted as a feature but details are left out. That has me wondering if some things don't work when I try to host on-prem or that its an afterthought. Can I easily host this on my own k8s cluster? Its not stated. Cloud native doesn't mean it's on the internet Another feature that artists like is file locking. P4 has it and git lfs actually has it too. The heavier usage of P4 streams and branches makes it hard use locks effectively these days. Merging something that was locked but now isn't is sticky business...maybe you guys solved that. > File locking across branches - coming soon! \"coming soon\"... so close. Good luck to you guys but I think the pitch needs work. reply Vermeulen 16 hours agoparentGit LFS is a giant hack ontop of Git. Most game devs I know moved away from it over time (back to Perforce or SVN). It might seem okay at first - but deep into a project you'll want to rearrange/rename folders and keep history logs, and discover that Git LFS doesn't actually work like normal Git and your file history wasn't kept. Only once you start dealing with issues will you find all the weird hacks Git LFS does ontop. I'd say Git not working well for game dev isn't a pitch that Diversion needs to make, because it's already clear to most game devs. reply jayd16 15 hours agorootparentIts gotten a lot better over time as far as adding tooling to make big changes. Its the same ol' 'its fine if you know what you're doing and not so fine if you don't.' Personally I'd rather deal with git's issues but p4 has the a lot of built in support in engines. Day to day I think its the industry tooling and the partial checkouts that have people pick P4, not esoteric problems you face years down the line. reply sasham 17 hours agoparentprevThanks for the feedback, glad you asked! Partial checkouts are supported. K8s not yet, but we do run Diversion in a container for testing. Private cloud works as well, it's more a question of support manpower though - will be available for large clients. Obviously we still don't have every possible VCS feature, we're just getting started :) But we are adding features pretty quickly (e.g. conflict notifications took a few days to implement). Thanks! reply stevelacy 14 hours agoparentprevGame dev here, totally agree. File-locking across branches excited me, support for visual diffing of uassets would be insane. The platform screenshots as a git/github alternative didn't excite much interest as Plastic and Perforce are major players. reply sasham 14 hours agorootparentOther game devs mentioned visual diffing as well, we'll try to make this happen! reply kkukshtel 8 hours agorootparentAlso a game dev — if you want to attract devs, tell me why this beats P4 or Plastic. How's it similar or different? Plastic has a lot of the features you're bringing up as \"new\" and has great DX, which tells me your either didn't do your research, or aren't saying enough about how what you're doing is meaningfully different from them. reply parasti 21 hours agoprevFocusing on Git seems like completely the wrong pitch. Git is a distributed VCS - in all your examples you were clearly trying to use Git in a centralized manner with no backups. I suggest focusing more on your own product than on Git. reply sasham 21 hours agoparentYou're totally right, we were using BitBucket and pushing and pulling from there. It's really more of a centralized manner, but this is the usual workflow for most teams and companies and what they actually need (a single source of truth). Totally agree about backups, lesson learned :) reply solidasparagus 20 hours agorootparentI want to add counter-feedback. I think focusing on git's weaknesses is really appealing. My background is more ML research and data and I viscerally connect with your pitch around both git's limited scalability and the complex/dangerous nature of git operations (ML researchers + git is not great). reply TexasMick 13 hours agorootparentTbh they should really just learn it. It's super simple and powerful reply funcDropShadow 20 hours agorootparentprevUntil the day, the company is target by a ransomware group and everything is switched off in panic. Or the day the network connection of a building goes down. Requiring a REST api in the cloud for every VCS is command bytes hard then. We had this before with SVN and it wasn't nice. reply IlliOnato 16 hours agorootparentprevSo it sounds to me as you are trying to create a replacement for BitBucket\\GitHub and their ilk, not Git. This may be a worthwhile task. Maybe it makes sense to concentrate on this in your pitch. For BitBucket\\GitHub\\GitLab and for workflows enabled by them Git is just an underlying technology. Some of the functionality of these services is implemented using Git commands very clumsily. Some Git commands don't make sense or are dangerous in such environment\\workflow. Yet Git interface is fully exposed to the users of these systems. (Despite your statements and example, Git commands are not dangerous in the sense that they can destroy information already pushed to the repo. However, as your example demonstrated, they are dangerous in the sense that to recover from them requires expert knowledge and capabilities.) Git was designed to for truly distributed development, and it is great for that. A lot of projects use it though for a centralized development. Git the software is fully capable to support such development with proper configuration, but has arguably bad defaults for it, and the existing solutions seem to be half-assed (to tell the truth, I hate GitHub, but won't go into this right now). For me in my work and personal use the fully distributed character of Git is not important, but being able to work offline is, crucially. I know it is important issue for many developers. With working from home being more and more widespread I'd think this issue becomes more important, not less. Not being tethered to your good internet connection, or being able to work during an outage is really cool :-) (Before Git I would have 2 VCS applications installed on my work laptop, one working against central database that required internet connection, one fully local, with separate local database. Synchronizing them was a constant chore, a significant part of it I was not able to automate and had to do manually. Sill, it was worthwhile price for being able to work offline.) reply nojvek 18 hours agoprevGit cli UX made be not great, but the git datastructure of representing commits, branches, trees and blobs as immutable pointers and merkle trees is a phenomenal invention. I don't agree with every command needs to hit some REST api. That seems like throwing the baby away with bathwater. The most powerful thing about git is that I can work fully offline with a partial clone. And sync the commits when I get online. Git got popular because of it's distributed nature. I remember when our svn server used to hang and the entire team was blocked from even making a commit. reply atq2119 3 hours agoparentThese SVN memories are why \"cloud native\" is an anti-feature to me. Most software, especially productivity software, should be offline-first. reply sasham 18 hours agoparentprevYep, exactly! I remember these \"happy\" SVN days as well. It sucked, git was much better. Today you're almost never offline though, you even have wifi on airplanes. Git's datastructures are a masterpiece, for sure reply jedberg 14 hours agorootparent> Today you're almost never offline though, you even have wifi on airplanes. The airplane wifi is good enough to look up an API, but it's not good enough to sync a code repo, especially one with large assets. reply sasham 12 hours agorootparentFor assets no, definitely. At least until they get Starlinks :D I did commit into our code repo from an airplane though. reply cornholio 21 hours agoprev> In our previous startup, a data scientist accidentally destroyed a month’s work of his team by using the wrong Git command. While git is indeed a usability clusterbomb and there is massive space to improve, the problem above sounds like a devops failure. Git gives you all the tools to prevent such a disaster, all you have to do not give the root password of your CI server to any data scientist. On the topic of your startup, I would very much like a lighter learning slope, where I can introduce regular non-coders to the benefits of source control, and still have the advanced brancing/merging/rebasing etc. for the wizards. reply sgarland 21 hours agoparentI also always wonder in cases like this if the person just didn’t know about reflog. You _really_ have to try to fuck up hard enough that everything is gone, it just might require even more arcane commands than what got you into a mess. reply faitswulff 20 hours agorootparentI've used reflog many times, but I'm not sure if this: > You _really_ have to try to fuck up hard enough that everything is gone ...is still true when using git-lfs, which seems common when using large data sets. reply sbergot 19 hours agorootparentI believe it works the same. The large files are not immediately pruned. reply sasham 21 hours agoparentprevTrue! They didn't actually have a root pwd to anything, the repo was hosted on BitBucket which didn't have a branch protection feature back then. Non-coder users are actually an important use case for Diversion (like in game development where many/most users are artists). reply jsnell 21 hours agorootparentAnd nobody on the team had a reasonably fresh checkout of the repo on their local machine? To put it bluntly, this story does not sound credible. It's also one of the first things you say in the pitch, which taints everything you write later. I would suggest focusing on what you do well, not in making up stories about data loss with what you perceive as the competition. (It's especially odd when Git isn't the obvious competition; Perforce is.) reply sasham 20 hours agorootparentSwear to god, the story is true! We were able to restore most of the work because someone didn't pull the updates. (I should have added it in the story, didn't think it was important) But it was nerve wracking :') reply dmazzoni 15 hours agorootparentBut if someone DID pull the update, the old commits would have still been in their local repo and they could have run \"git reflog\" to retrieve them. reply xorcist 13 hours agorootparentThe person, who apparently googled some command sequence and happily entered them only to find out it overwrote the team's data, could just as easily have used those google skills to search for how to undo last git operation. Obviously this person had all the privileges required to force push the previous commit in order to save the day. The old adage of never fact-checking a good story holds true however. Not sure it's a good selling point though, by nerd sniping everyone to explain in detail what the actual problem was no one will read to the end. reply calderwoodra 17 hours agoprevThe world needs a git alternative. Anyone who has used mercurial at Google or Facebook knows the tooling could be much better. As soon as you have 2-3 people committing to the same repo daily, git falls apart fast. The biggest difficulties with git are merging and branch rebasing. If git could do rebases better, I would suspect software development teams to universally move about 20% faster. reply dmazzoni 15 hours agoparentI've worked on projects with thousands of developers committing to the same repo daily. Git is far from perfect, but in my experience it's been far superior to any of the alternatives I used before it (cvs, svn, p4). reply TexasMick 13 hours agoparentprev2-3 people daily I would be completely surprised there's any issues. It suggests they're working on the same files and on the same lines, why??. My current project has 40+ developers and I haven't seen a merge conflict in a long time. Why do you think there's issues with merging and revising, pretty much all cloud solutions have a button to do both for you. reply withinboredom 16 hours agoparentprevrerere has worked pretty well for me when handling conflicts and it’s built into git. I’ve worked with hundreds of engineers in a single codebase. Never had any issues like you’re describing. reply dclowd9901 16 hours agoparentprevI don’t understand how it isn’t easy? “git merge origin/master” Done. Unless you have a conflict, then fix it and commit. It couldn’t be simpler. If there’s value in this product, it won’t be because it’s somehow simpler than git. reply hasty_pudding 16 hours agoparentprevRebasing is extremely easy with git. Sounds like more of a skill problem then a git problem. reply mikepurvis 15 hours agorootparentI think the issue comes about more when you have refactoring going on at the same time, and suddenly multiple feature branch maintainers need to figure out if they're going to merge in the main branch, rebase onto the main branch, if it makes sense to squash their changes first before attempting either path, etc. If they get only partway through either one, it's difficult work to pause and resume, so that integration work is fundamentally hard to collaborate on or even really review. And never mind maintaining an ongoing integration of multiple unmerged feature branches— perhaps that's more of a \"front end\" issue for GitLab and Github to solve, but the whole business of patchsets and the like is very much unsolved, and that's painfully obvious when you see Debian storing their quilt patches inside of of a git repo, rather than being able to leverage git's native capabilities to achieve those effects: https://salsa.debian.org/debian/netplan.io/-/tree/debian/0.1... reply TexasMick 13 hours agorootparentYou can always merge feature branches together to check for conflicts reply 0xbadcafebee 15 hours agoprevI get that enterprises will buy anything with \"Cloud\" or \"AI\" in the name, but VCS doesn't have anything to do with the cloud. Lots of VCS's have had server-oriented architectures, well before Git. I see a lot about the architecture and design here. This is a product smell: focusing more on technology than solving problems. Some nerdy people may be interested in it, but it all means nothing if the experience of those users isn't good. You want me to buy your product? Sell me on why the experience is better. How it's going to speed up dev time, reduce errors, make collaboration better. None of that will be improved by a REST API or distributed storage in your backend app. I'm also not looking to change my development practice. If most of your features are only available in a browser, I'm not going to want to use it, even if it were better than what I do now. Meet the users where they are. reply sasham 10 hours agoparentTotally agree, and the launch post could only be that long :) We're trying to build a better dev experience, and the tech is only a means to that end. > If most of your features are only available in a browser, I'm not going to want to use it, even if it were better than what I do now. The CLI is the most complete interface. Web UI still can't do everything (getting there though). Thanks for the feedback! reply fidotron 19 hours agoprevAs others have mentioned I believe you underestimate the security requirements (often physical) imposed on game studios. Running P4 (and using the often publisher mandated tape backups) is the least of your problems in such an environment. If diversion were to succeed it would need a huge security team because it would represent far too tempting a target. Games stuff inspires a level of attack which normal businesses simply do not encounter. reply sasham 19 hours agoparentThanks for the feedback! Totally true, I was surprised by how much emphasis on security there is in game studios. They are moving to cloud however, and we're hoping that a private cloud solution (Diversion running on their cloud account) would satisfy the requirements, at least a few years from now as cloud usage grows. Security is definitely going to be super important for us, in any case. reply fidotron 18 hours agorootparentYou need to stop worrying about the cloud and focus on being agnostic to things like VPNs, on premises and so on. (Also SSO mechanisms). Any cloud needs to be optional. If you can make it easy to deploy on a private cloud where everyone accesses it via corporate Google accounts but also deploy on prem (say containerized) with Active Directory integration you will cut a lot of noise. Game artists will increasingly not trust the cloud as it is where their data goes to train AIs, but more pressingly a large proportion of asset development is done where the network (and electricity) is surprisingly flaky. To be specific I have been involved in several situations where we were not allowed to mention things in email due to it being cloud hosted, and those restrictions were imposed by companies that are themselves cloud providers. That is the kind of level being discussed. reply egaldv 16 hours agorootparentThanks for the feedback! We do hear that remote artists with flaky network connections is common, and plan to address it in several ways like CDC (a method for efficient chunking and diffing of binary files to minimize network transfer and storage duplication), local network caches or peer-to-peer transfers. Private cloud deployment or on-prem with SSO will also be available. reply gumby 20 hours agoprevIt's about time someone revisited & reimagined version control. the previous generations each lasted about 15 years: SCCS/RCS -> CVS -> Bitbucket/git/mercurial -> ??? so I am glad to see this. I would start by talking about what is great about Diversion -- what it lets you do that you couldn't before. Since you mention gaming and perforce I looked in vain to see if it supports binaries (a major limitation of git -- just simply not in its design space). \"Binaries\" can actually mean for some people compiled code -- not for me but I understand why people do it -- as well as images, data files, Word files etc. Sounds like the second is scaling but you don't say what you mean by that. Git scales pretty well until either the repo & its history gets enormous or when there are a lot of people making simultaneous changes. The realtime collab integrated with a version control mentality could be interesting -- a major problem with google docs is the lack of useful version control (even Word is better). And why cloud native? Once you've done that you need only briefly mention \"why not just use git instead?\" reply VyseofArcadia 18 hours agoparent> And why cloud native? And what even is cloud native? When \"cloud native\" isn't just marketing, it seems to have all sorts of different meanings. reply sasham 18 hours agorootparentWe're using S3 storage, lambda and ECS compute, and serverless DBs. It allows us to build a scalable product much faster, and leverages cloud features like multizone backups and distribution without having to develop these ourself. It also allows fast transfer of data between Diversion and other cloud systems. reply gumby 18 hours agorootparentThose are things that matter to you not your users. You did talk a little (in a comment or your post I don't remember) that you can use various cloud APIs to integrate into other systems. But at the moment, from what you're telling me \"cloud native\" is as interesting to me as how you format your source code. reply withinboredom 16 hours agorootparentprevSo basically not self-hostable. reply pas 20 hours agoparentprev??? might be https://pijul.org/ with its commutative awesomeness reply oblio 18 hours agoparentprev> SCCS/RCS -> CVS -> Bitbucket/git/mercurial You skipped Subversion/SourceForge. People forget but for about 5 if not 10 years, SVN was the biggest SCM in town, next to P4, P4 having more of a hold in the game dev world. reply gumby 18 hours agorootparentI not only did not forget about subversion, I funded it in part. But it was of the CVS generation. I did leave out the proprietary things like Perforce and Aide de Camp, Solidworks PDM and the various in house things. Life's too short! reply oblio 17 hours agorootparentYou also forgot Microsoft Visual SourceSafe :-p reply sasham 19 hours agoparentprevThanks for the feedback! Totally agree. Diversion does support binaries, should have mentioned it directly (will update the website). The thought behind cloud native is that workloads and devtools and data are moving there, and we want Diversion to be the best choice for when everything is in the cloud. Besides that cloud storage and DBs allow us to build and iterate much faster, and worry less about scalability, data distribution & storage etc. But we can also run Diversion locally / in a container, it just won't be as scalable. reply hasty_pudding 16 hours agoparentprevwhy is it about time?? VCS is sort of a solved problem like SQL. it's like saying it's about time someone revisited those Javascript frameworks. reply gumby 4 hours agorootparentThe comment you're replying to mentions some limitations of the current state of the art. I agree that there's no reason to change just because something is older than some threshold. Bit I think the roughly 15 year cadence has reflected the time it has taken for a new VCS idea to develop and then become mainstream enough for its drawbacks to become annoying enough that it loops again. reply esafak 4 hours agorootparentprevI don't know about that. The UX of git leaves a lot to be desired. monorepos and large files are not definitely solved, either. If not git, what solution did you have in mind? reply meibo 20 hours agoprevBeing deceptive about Git's shortcomings is going to raise eyebrows with anyone seriously evaluating your solution, which is already going to raise eyebrows because it's not free software. Most studios will try to avoid locking themselves into another expensive, annoying VCS that they have no control over. There's good attempts at FOSS p4 replacements now, you need to do better if you want to stand out. reply maccard 20 hours agoparent> There's good attempts at FOSS p4 replacements now, you need to do better if you want to stand out. There are? Like what? reply oDot 15 hours agorootparentI've been attempting this but have paused the efforts due to what seems like lack of interest -- giving me an indication that the problem is not that big of a deal.[0][1] If anyone is interested in my resuming them, let me know at contact at weedonandscott dot com [0] https://www.reddit.com/r/vfx/comments/11s08ne/your_opinion_o... [1] https://www.reddit.com/r/gamedev/comments/11s5haf/what_do_yo... reply Arelius 16 hours agorootparentprevYes Please. I would switch my studio today if this existed. reply ryanlitalien 11 hours agorootparentHi, why the switch? Purely cost? Or admin overhead? reply maccard 11 hours agorootparentYes, to both. P4 itself is solid, but it's a very chatty protocol and is very latency sensitive. Running a master in the us, with clients in europe is painful for everyone involved. Replicas and edge servers come with other tradeoffs too. As a developer, doing things like \"I only want this subtree of the stream\" is hard. Virtual streams exist, but they have a (non-negligible) overhead on the server. It has some quirks due to it being 30 years old which make it... interesting, to work with sometimes. reply Arelius 2 hours agorootparentAgreed. It's also expensive, and support has been lacking. We'd much prefer something we could maintain and fix ourselves. reply oblio 19 hours agoparentprev> Being deceptive about Git's shortcomings Are they being deceptive? I'm not sure I see it. reply jayd16 18 hours agorootparentNot deceptive but I would say they're being hand wavy. reply umanwizard 17 hours agorootparentprevThe story about irrevocably losing data at least was not true (already admitted by the authors). reply Spivak 16 hours agorootparentI think I would give them that one, they were only able to recover the data because they got lucky, not because the system was designed to not destroy data. I would go even further coming from my infra background, even if you don't truly lose production data, if you have to reach into your DR backups that's a failing of the systems in place that come before it. reply gosub100 16 hours agoparentprevI'd say git is deceptive: \"checkout\" doesn't mean \"delete my stuff\" any more than checkout a book from the library means throw it in a wood chipper. reply yanis_t 19 hours agoprev> a data scientist accidentally destroyed a month’s work of his team by using the wrong Git command (EDIT: we were eventually able to restore from a non-updated repo clone, after a few hours) This is actually reads like a benefit of using Git. It's really really hard to lost something completely in Git, because reflogs, because there are multiple people on your team each has the same copy of the repo, etc. reply sasham 19 hours agoparentFor sure, but that requires a lot of expertise and leaves the non-experts free to shoot themselves in the foot... reply yencabulator 20 hours agoprevYou are not competing with Git, you are competing with Perforce. (Personally, I would never use a proprietary cloud-only offering for version control.) reply oldpersonintx 19 hours agoparent> (Personally, I would never use a proprietary cloud-only offering for version control.) yeah, I have no idea who would want to replace a free, open, well-debugged, featureful, ubiquitous vcs with a closed, immature SaaS offering...if your sourcecode is valuable, you don't let a YC startup gate-keep it (sorry) git has terrible ergonomics for newcomers, but UI is exactly what the various git forges solve for experienced devs, git issues get internalized like the issues with every other tool a more realistic approach is the jujutsu stuff google is working on...works WITH git to create a better workflow reply gavinhoward 19 hours agoprevDisclaimer: I am designing a Git alternative too. Maybe \"cloud native\" will have a pull for game companies, but I am not so sure. I think a lot of studios would want to self-host. \"Git compatible\" is an interesting phrase; does Diversion use the same type of backing store? If so, I am not so sure it will handle large files as well as hoped. I had to solve this problem myself, and I did, but it required a different storage design. Can it handle binary files? Is there a plan for doing so beyond \"commit the entire file every time\" or \"use xdelta\"? I think this is how fourth gen version control systems will be defined. Game studios have a lot of binary assets, so this will be important. All in all, I could see a product like this succeeding, but I think taking VC money was a mistake because there may not be enough space in the market for a company that has to keep growing to satisfy investors. I am taking zero VC, and that will allow me to make money on as few as three clients. Anyway, I wish you both the best of luck! reply computershit 19 hours agoparent> Maybe \"cloud native\" will have a pull for game companies, but I am not so sure. I think a lot of studios would want to self-host. Cloud native and self-hosting are not mutually exclusive. reply wokwokwok 17 hours agorootparentThey are in this case. …and, frankly, they are in most cases. Most “cloud native” apps are designed to run on cloud services like VendorHere cloud storage, cloud functions, cloud containers, etc. A vanishingly few of them are actually self hostable. reply ryanlitalien 11 hours agorootparentprevCan confirm, a lot of studios want to self-host. reply sasham 19 hours agoparentprevThanks, good luck to you too! Would love to check it out! Diversion has totally different storage, that handles binary files with no issues, but same concepts as git - branches, commits and tags. It has a Git sync feature that allows to sync commits between Git and Diversion repos. Kudos on bootstrapping the product, it's definitely not easy! Version control is much harder than it seems, you've probably found out already :) reply midnitewarrior 17 hours agoparentprev> I am taking zero VC, and that will allow me to make money on as few as three clients. A VC would give you legitimacy to help you get the customers you need. What large customers are going to back you with their business unless they know there's some deep pockets behind you? There are few things more valuable than a business's source code, it encapsulates all of their business processes and is how all of the business's data is accessed. Why would a company want to entrust you with this? How are you protecting against data loss, and the legal liability that comes with this responsibility (insurance, legal)? reply gavinhoward 16 hours agorootparentYou make good points. My VCS is designed for self-hosting, not cloud. I will support customer installations, not host customer source code. I am hoping that customers see less of a need for deep pockets in that case. reply Spivak 16 hours agorootparentThis is a tried and true method of bootstrapping a SMB. Sell the bits in a box with support until you are big enough to sell the service (or don't). reply withinboredom 16 hours agorootparentprevI mean, insurance is cheap. I have professional insurance allowing me to personally do several million in damages due to a mistake. Or recover your losses if I give bad advice. All for the low cost of 50 bucks per month, purchased through a local dev-co-op. Trust isn’t built by who backs you though. Does it make it an easier sell? Maybe, but as a buyer, if that’s what you’re leaning on to sell to me, I’m going to be put off. If it comes down to two, I’d trial both and care about how well it does before caring about who is backing who. In the end, I might choose the smaller company and negotiate access to source code to hedge them going under. And that sounds like an even better deal than some VC’s trying to “monetize",
    "originSummary": [
      "Diversion is a cloud-native version control system designed to solve scalability and complexity problems in Git.",
      "It aims to maintain Git's branching and merging capabilities while offering real-time collaboration and compatibility with Git repositories.",
      "Diversion is currently in open beta and will be released as open source in the future."
    ],
    "commentSummary": [
      "The discussions revolve around the limitations and complexities of Git, a popular version control system, and the need for alternative solutions.",
      "Diversion, a cloud-native alternative to Git, is presented as a potential solution for game developers.",
      "Other topics include the use of Abstract Syntax Trees (AST) in version control, scalability challenges with monorepos in Git, and the debate over the complexity and effectiveness of Git as a version control system."
    ],
    "points": 269,
    "commentCount": 382,
    "retryCount": 0,
    "time": 1705924870
  },
  {
    "id": 39088991,
    "title": "Astronaut's Desperation in Orbit: Threatens to Stay in Space",
    "originLink": "https://arstechnica.com/space/2024/01/solving-a-nasa-mystery-why-did-space-shuttle-commanders-lock-the-hatch/",
    "originBody": "Space sickness — What happens when an astronaut in orbit says he’s not coming back? \"If you guys don't give me a chance to repair my instrument, I'm not going back.\" Eric Berger - 1/22/2024, 11:00 AM Enlarge / The STS-51-B mission begins with the liftoff of the Challenger from Pad 39A in April 1985. NASA reader comments 233 Taylor Wang was deeply despondent. A day earlier, he had quite literally felt on top of the world by becoming the first Chinese-born person to fly into space. But now, orbiting Earth on board the Space Shuttle, all of his hopes and dreams, everything he had worked on for the better part of a decade as an American scientist at NASA's Jet Propulsion Laboratory, had come crashing down around him. Wang was the principal investigator of an experiment called the Drop Dynamics Module, which aimed to uncover the fundamental physical behavior of liquid drops in microgravity. He had largely built the experiment, and he then effectively won a lottery ticket when NASA selected him to fly on the 17th flight of the Space Shuttle program, the STS-51-B mission. Wang, along with six other crew members, launched aboard Space Shuttle Challenger in April 1985. On the second day of the mission, Wang floated over to his experiment and sought to activate the Drop Dynamics Module. But it didn't work. He asked the NASA flight controllers on the ground if he could take some time to try to troubleshoot the problem and maybe fix the experiment. But on any Shuttle mission, time is precious. Every crew member has a detailed timeline, with a long list of tasks during waking hours. The flight controllers were reluctant. After initially being told no, Wang pressed a bit further. \"Listen, I know my system very well,\" he said. \"Give me a shot.\" Still, the flight controllers demurred. Wang grew desperate. So he said something that chilled the nerves of those in Houston watching over the safety of the crew and the Shuttle mission. \"Hey, if you guys don't give me a chance to repair my instrument, I'm not going back,\" Wang said. Exactly what happened after that may never be known. But thanks to new reporting, we may finally have some answers. And though this is an old story, it still reverberates today, four decades on, with lasting consequences into the era of commercial spaceflight as more and more people fly into orbit. Advertisement Non-NASA astronauts Space Shuttle missions fulfilled various tasks in the vehicle's early years, such as deploying satellites, but one of its primary functions was conducting research in microgravity. Working with the European Space Agency, NASA developed and flew a pressurized module called Spacelab on some missions for this purpose. The STS-51-B mission was the second time this Spacelab module flew, and it carried 15 different experiments ranging from astrophysics to the behavior of fluids in microgravity. Due to the nature of these specialized science experiments, NASA had started to fly \"payload specialists\" who were not designated to operate the Shuttle but rather complete the experiments on board. With this mission, flying on board Challenger, the two highest priority experiments concerned materials science and fluid mechanics. Accordingly, the two payload specialists—Lodewijk van den Berg, a Dutch-born American chemical engineer, and Taylor Gun-Jin Wang, a Chinese-born American physicist—were chosen because of their expertise in these areas. Wang was born in Shanghai in 1940 but moved to the United States in 1963 to study at the University of California, Los Angeles. He later earned a doctorate in low-temperature superfluid physics from UCLA and joined NASA's Jet Propulsion Laboratory in 1972. He became a US citizen three years later. His research involved the behavior of droplets and other sphere-like objects in zero gravity, and he eventually flew on NASA's zero-g flights. He developed the \"Drop Dynamics Module\" experiment to take this work to the next level in space. Although he had never aspired to become an astronaut, when NASA began selecting a crew for the Spacelab mission in 1982, he applied. Wang was selected a year later and would become the first person of Chinese ethnicity to fly into space. Payload specialists like van den Berg and Wang did not go through the same training as traditional NASA astronauts who underwent an ultra-competitive selection process. \"All received an abbreviated training program on basic Shuttle operations,\" write the authors of the book on NASA's payload specialist program, Come Fly With Us. \"NASA performed medical and psychological evaluations on each candidate to ensure they were fit to fly into outer space, but nothing near the level of evaluation required by the NASA astronaut candidates.\" This could create something of a barrier between the mission crews and the payload specialists who were tacked on. Some of the traditional astronauts looked at the payload specialists as interlopers, not to be entirely trusted. Page: 1 2 3 4 Next → reader comments 233 Eric Berger Eric Berger is the senior space editor at Ars Technica, covering everything from astronomy to private space to wonky NASA policy, and author of the book Liftoff, about the rise of SpaceX. A certified meteorologist, Eric lives in Houston. Advertisement Channel Ars Technica ← Previous story Next story → Related Stories Today on Ars",
    "commentLink": "https://news.ycombinator.com/item?id=39088991",
    "commentBody": "What happens when an astronaut in orbit says he's not coming back? (arstechnica.com)251 points by martey 21 hours agohidepastfavorite212 comments wantsanagent 19 hours agoIf you hate the format of these kinds of article let me summarize: Guy builds an experiment, flies it, it doesn't work, requests more time to get it working, is denied, threatens 'not to come back', goes into a depression, worries crew that he'll open the door to space, eventually granted more time to work on the experiment, gets it working. There are consequences for letting this kind of person fly and dealing with mental health while on a mission, but that's the basic plot. reply GeoAtreides 19 hours agoparent> If you hate the format of these kinds of article You mean long-form articles? https://en.wikipedia.org/wiki/Long-form_journalism I don't think the problem is with the format, but with your expectations. Somehow you were expecting a straight answer to the question in the title; instead, it's a detailed look into the incident and the context around it. reply mattjaynes 18 hours agorootparentNo, this is not simply \"long-form\", this is \"in medias res\" where the story starts at a heightened moment of tension, then abruptly stops and rewinds to go back to fill in details of how they got to this exciting moment. https://en.wikipedia.org/wiki/In_medias_res Sometimes it's done to great effect, like in Fight Club (film starts with him at the climax with a gun in his mouth, then rewinds and tells the story). However, it is often used as a clumsy device by novice writers. They use it to get the audience to stick around for their mostly boring story. I see this a lot on Youtube. The Youtuber will start with a compelling question and story, like \"There he was... tied to the wall as he watched the firing squad load their weapons with gun powder.... But before we continue our story, what is gun powder? Well, it's composed of potassium nitrate, blah, blah, blah....\" You can tell if \"in medias res\" is done well or poorly by how you react to it. If you are excited for the detour, then it's done well. If instead it feels like a long annoying interruption you want to skip, then it's done poorly. For me, it's done poorly in this article. I read the first compelling section, then the writer slows everything down to a snail's pace to go into some very dry history of NASA without ever really giving a good payoff to the story he opened with. reply no-dr-onboard 17 hours agorootparent\"In medias res\" vs \"longform\" is an important distinction, as you've highlighted. As the parent commented, a lot of this has to do with how we frame the article via the title. The title sets the expectation for the article. In this case, it provides a misleading ethos for discerning audiences. reply JohnBooty 14 hours agorootparentprevI see this a lot on Youtube. Purely subjectively, I find it way more egregious and annoying on Youtube (because YT won't monetize shorter articles) and less convenient to skim past if you don't like the structure of the video. You can tell if \"in medias res\" is done well or poorly by how you react to it. If you are excited for the detour, then it's done well I was a maybe little frustrated here, but we already know the ending of the story, right? The guy did return from space. The guy did not open the hatch and kill the crew. Pretty sure we'd all remember that. So with that in mind I thought the detour into \"what brought this dude to the point where he was refusing commands from Mission Control\" was quite welcome. reply bonestamp2 17 hours agorootparentprevI watched Dumb Money (2023) last night and it starts this same way. It was completely unnecessary, was done in a confusing way, and this plot device didn't add anything to enhance the excitement of the story. A completely linear plot would have been just fine, especially since everyone knows the story gets more interesting (even from the trailer alone). In fact, the interesting part of the story that most people don't know is the beginning... how it all started before it was exciting. It was still an entertaining movie, so I wouldn't let this one poor choice deter anyone. reply narag 16 hours agorootparentprevSo it has a name... I'm sick and tired of flashback overuse in recent series, mostly to explain the feeeeelings of the characters and so justify their stupid behaviour. But this must be the ultimate abuse: most of the story is a flashback. Ouch! reply aidenn0 10 hours agorootparentTo be even more overused, make sure the flashbacks describe a trauma plot[1]. 1: https://archive.is/VdZ14 reply Cheer2171 16 hours agorootparentprevnext [2 more] [flagged] anigbrowl 14 hours agorootparentHistory and flashback are not equivalent terms. Most history books do not open with a dramatic moment at a critical juncture of [historical event] and then flip back and forth between the lead-up and the dramatic moment to maintain narrative tension. This is sensationalism; it's literally designed to keep you on the edge of your seat. And no, Homer does not use this technique in the Iliad, other than brief verbal allusions back to the original cause of the Trojan war. The Iliad starts in the middle of the conflict, but doesn't repeatedly skip the narrative backward in time; the plot is linear and the the dramatic tension comes from wondering if/when the champion warrior Achilles is going to stop sulking over a perceived insult and rejoin his army to fend off the Trojans. reply ycombinete 17 hours agorootparentprevIan Kung poked fun at this wonderfully: https://youtu.be/HySlYLkFieQ?si=ugnFU7PhgZmEXaXn reply GeoAtreides 17 hours agorootparentprev> No, this is not simply \"long-form\", this is \"in medias res\" This is like saying: This is not simply a mug, this is porcelain. Narrative and narrative technique are two different things, just like the function of an object is different thing than the material of the object. It's also probably just a hook, not a full \"in media res\". reply FpUser 15 hours agorootparentprev>\"I see this a lot on Youtube. The Youtuber will start with a compelling question and story, like \"There he was... tied to the wall as he watched the firing squad load their weapons with gun powder.... But before we continue our story, what is gun powder? Well, it's composed of potassium nitrate, blah, blah, blah....\" It gets even worse. Looking for answer to particular question and hits the video that claims to have it. Let's say concise answer is 5 words. The video is 30 minutes long with those 5 words spaced evenly throughout. reply PH95VuimJjqBqy 14 hours agorootparentprevyeah, I basically skipped over everything looking for the answer to \"what happened after that\" because I didn't give a shit about anything else. and yes, I hate this form of writing too. reply danielbarla 19 hours agorootparentprevI think this article is actually a pretty good example of long-form journalism; the jump it makes to reveal the context is fairly small, and relevant from the first few words. In many other articles, however, the context switch is almost nonsensical, making the reader wonder if the writer will ever actually get to the point. While I agree that in the past few decades our attention spans have gotten very short, I also think there's a slight disconnect between the clickbaity titles and many articles. For that matter, this happens with videos as well; things that can be answered in 2-3 minutes get turned into a 10 minute video with a large amount of fluff. reply pixl97 16 hours agorootparent>2-3 minutes get turned into a 10 minute video This is just the advertisement affect. Short videos don't get monetized so longer videos are necessary to make a living. reply QuantumGood 19 hours agorootparentprevNowadays there is so much fluff added (as an \"SEO hack\" to increase reading time) to what could be otherwise short posts, that long-form was bound to suffer by association. Of course, some never liked long form. I prefer it, but not it's SEO'd brethren reply thomastjeffery 16 hours agorootparentThe worst is when they make a large eye-catching quote that is just text from two paragraphs down. I hate spoilers. reply neilv 15 hours agorootparentFor bonus points, they combine that with a screenshot of the Tweet they're quoting, so that you read the same quote three times. reply _Algernon_ 18 hours agorootparentprevJournalism is intended to inform, which is why the inverted pyramid[1] is a thing. Withholding the important information is, IMHO, bad journalism. [1]: https://en.wikipedia.org/wiki/Inverted_pyramid_(journalism) reply wiredfool 18 hours agorootparentThe whole point of the inverted pyramid was that editors/layout people could chop off the article at any point to fit the space available and have it still make sense, back when newspapers were laid out in columns and on physical pages. It's not an idealized platonic form information transfer. reply arccy 17 hours agorootparenteditors can chop off the article, but readers can also stop engaging when they've decided they've obtained enough detail without losing out. reply lupire 17 hours agorootparentprev\"Studies of 19th-century news stories in American newspapers, however, suggest that the form spread several decades later than the telegraph, possibly because the reform era's social and educational forces encouraged factual reporting rather than more interpretive narrative styles.[2]\" reply mrguyorama 16 hours agorootparent>encouraged factual reporting rather than more interpretive narrative styles The 19th century literally coined the term Yellow Journalism! What kind of dumb revisionism is this? A narrative being conveyed in only a couple bullet points instead of a story is not \"factual reporting\", it's soundbite optimized reporting. reply _Algernon_ 15 hours agorootparentNothing in this thread requires \"narratives being conveyed in only a couple bullet points\" and that also isn't what the inverse pyramid is about. The inverse pyramid is about the ordering of information, not the level of detail or quantity. reply hutzlibu 18 hours agorootparentprevDo you think important information was withhold here? Which information was not important? It was a investigative article, about an incident that happened long ago, with most participants already dead by today. If something is worth a long article - then this is. reply jstarfish 16 hours agorootparentThe personal biography of Wang, for starters. I do not give a shit when he was born or where he went to school. I'm only mildly interested in what he worked on. The issue is the headline. It posits a question, then jerks you around until you've spent long enough on the page to satisfy some engagement metric. It's writing for television, where any yes/no question always happens to take exactly 30 minutes (and multiple commercial breaks) to answer, starting with the history of philosophy and reason itself. It's scummy behavior, like timeshare sales or giving people free samples of spicy beef jerky and making them wait in line for water. The title \"The First Man to Refuse to Return from Space\" would be more appropriate for an investigative article. Then you'd know what to expect. But they went the clickbait route, hence the irritation. reply Cheer2171 16 hours agorootparent> The personal biography of Wang, for starters. I do not give a shit when he was born or where he went to school. I'm only mildly interested in what he worked on. It is highly relevant to the story. I was asking myself \"So why was he even on this space mission?\" You may not have had this question, but the main reason Wang gives for his state of mind around the incident is directly linked to his biography. He immigrated from China to the US at age 22, went into US academic science, and became a US citizen. So he was in a position to be the first Chinese person in space, and he feels like a representative of all Chinese people. I see a lot of my father in him, who immigrated to the US around the same time and holds a lot of those conflicted feelings. The reason he threatened to not come back, in his own words, was not because he really cared about doing the science or because the result was really important even to him personally. It was because he would be failing in the eyes of the world. The words of his father (speaking in his head) to not bring shame to his family were more influential than the words of NASA mission command telling him to follow orders (speaking in his ear). If you don't think where someone was born (China) and then went to school (USA) matters given this, then you have missed a big point of this article because you tried to speedrun long-form journalism. reply _Algernon_ 15 hours agorootparentNone of that is relevant to what I want to get out of an article titled \"What happens when an astronaut in orbit says he’s not coming back?\" If I wanted a bunch of random facts or life stories I'd use the random option on Wikipedia. reply hutzlibu 15 hours agorootparentRandom people are not flying with the Space Shuttle. And when people who do fly go nuts - then everything about this person is helpful to understand the \"why\" and how to prevent such a situation in the future. Of course NASA did that professionally already long ago - but now it is debated in the open. So some people deeply interested with the field, will want to play hobby psychoanalyst with the given facts. The more the better. You clearly don't want to and that is also OK, but maybe accept that some people like it like this. And just as a suggestion, you can nowdays get a AI to give you a short summary ... reply hutzlibu 15 hours agorootparentprev\"The issue is the headline. It posits a question, then jerks you around until you've spent long enough on the page to satisfy some engagement metric.\" But there was and is just no definite answer, except for drama. I found every bit interesting and relevant to be able to picture the situation. \"The title \"The First Man to Refuse to Return from Space\" would be more appropriate for an investigative article. \" And no, because it was way more severe than this: he said he won't come home and he said figurativly \"oh, I can just open this airlock and then we all would die?\" (where \"unless you give in to my demands\" was maybe intentionally implied - maybe not, he was not mentally stable) So an actual clickbaity sounding headline, that would have actually be quite close to the truth, would be: \"First man in space, who threatened to kill everyone on board\" But Ars did not do this. Partly because they are not (so much) into the clickbait game, but partly because the facts are (intentionally by NASA) not that clear here. And the Author tried to gather as much facts as he could. So giving us, where he was born and went to school was no real answer to the title question - but it helped me getting a picture of the person in question, which is still alive, but who refused to comment. Because people have reasons for why they act like they do: \"When I turned on my own instrument, it didn't work,\" Wang said. \"You can imagine my panic. I had spent five years preparing for this one experiment. Not only that, I was the first person of Chinese descent to fly on the Shuttle, and the Chinese community had taken a great deal of interest. You have to understand the Asian culture. You don't just represent yourself; you represent your family. The first thing you learn as a kid is to bring no shame to the family. So when I realized that my experiment had failed, I could imagine my father telling me, 'What's the matter with you? Can't you even do an experiment right?' I was really in a very desperate situation\" reply ethbr1 19 hours agorootparentprevIndeed, I'm curious what specific portions parent would recommend excluding from the article. I found everything fairly relevant to the topic. reply voytec 18 hours agorootparentprev>> If you hate the format of these kinds of article > You mean long-form articles? I think the grandparent meant blabbering to get to 3k words. reply IshKebab 17 hours agorootparentI don't think this article is guilty of that. Maybe OP thought this was one of those \" ... Dave lives in a small house nestled into the hills of rural Wales. He has long flowing hair and a slight stutter. His three children - Mary, Anna-belle and Calista - have long sense left home and he spends most of his time blah blah blah blah I don't care.\" This isn't that thought. reply tgsovlerkhgsel 6 hours agorootparentprevThere's a difference between looking into the context around the issue, and filling the article up with endless irrelevant stuff like the weather on the day of the interview or a description of some person's apartment decorations, and often pages of this before you even learn what the point of the article is supposed to be. This article had a clear subtitle that showed where the story is going: \"If you guys don't give me a chance to repair my instrument, I'm not going back.\" -- this immediately lets you know that this is about a specific incident (not a hypothetical scenario) and why the threat was made. It then proceeds to add new, obviously relevant information with each paragraph. This felt like a good article, despite its length, but most \"long-form\" content is trash. Compare https://www.bloomberg.com/news/features/2018-05-03/the-gambl... (bad) with this or https://www.vanityfair.com/news/2018/04/inside-el-faro-the-w... (good). reply tshaddox 18 hours agorootparentprevI wouldn’t like that comment of yours if you had expanded it to 20,000 words long, even if there was a name and a Wikipedia article for that type of HN comment! reply pif 18 hours agorootparentprevIs there a rule preventing an abstract from being added at the beginning of such an article? You know, just to understand rapidly whether the rest of the read matches your interest. reply hutzlibu 18 hours agorootparentYes, the rules of Dramaturgy. It is not a scientific paper, but at least partly meant to entertain. And for most people it is very simple - they care about space and its challenges - and then they will read it - or they don't and find something else. reply lopis 19 hours agorootparentprevThe tiktokification of our minds. reply Cheer2171 16 hours agorootparentThis is the same literary technique that Homer uses to open The Iliad, Virgil uses to open The Aneid, and that Vyasa uses to open The Mahabharata. reply echelon 18 hours agorootparentprevI didn't like these articles 15 years ago. Unless the subject matter absolutely grabs me, I still don't. I skim voraciously, and I always have. I skimmed newspapers and magazines as a kid. It's a filter for signal. Fluff grates me. I wouldn't blame TikTok. reply BobaFloutist 17 hours agorootparentprevTo be honest, I find feature articles exhausting. I just want the information, not everything has to be narrative! reply wackget 18 hours agorootparentprevPersonally I hate articles which are split across multiple pages. It's unnecessary. reply acheron 18 hours agorootparentThink of all those extra ad impressions though! reply genman 19 hours agorootparentprevI think hating something is kind of personal matter. But very often these \"long-form\" articles can be tiring. They just circle around something, never getting to the point, adding just unimportant details that should create some form of \"connection\". It very often feels that it is not long because the author has something to say but it is long because it is meant to appear that \"long-form\" article. It's like reading novels from Victor Hugo, where he takes good part of the volume to describe rooms in detail. Now it is certainly interesting for a movie director who would like to build an authentic interior but the fact is that he just got paid by the length, so this was his tool go accomplish his goal - getting paid more. reply boxed 18 hours agorootparentJust saying something is \"long form\" because you counted the words is silly. This is Ars Technica. They aren't long because some fifth grade teacher gave them a minimum amount of pages. If the article has some length, it's because it takes the subject seriously and doesn't compromise on nuance and context just for clicks. reply krisoft 16 hours agorootparent> If the article has some length, it's because it takes the subject seriously and doesn't compromise on nuance and context just for clicks. And yet it feels fluffy. The incident could be described in a single paragraph. The response could be an other. The organisation of the crew training and how it effected the incident could be third, while future instances when the lock was used could be a fourth. So about 800 words, compared to the 3000 words it is. Multiple of us on this comment thread feels it is fluffy, you are asserting it is not. But you are not providing any reason other than that the author is a professional and we should trust them? That is not quite convincing when at the same time I am reading their fluffy article. Why do i feel it is fluf? Because the article is going on and on while not answering any of the basic questions I have: what did factually happen in the cabin? Did he just say what he said and then went sulking? Did he move closer to the hatch? Was he trained on how to operate the hatch? How long did the fix take? What were the consequences on the schedule? What were the consequences on the experiment itself? (Did it perform as intended after the fix or did it only hobble?) Why did they had highly trained payload specialist if they were not given latitude to perform fixes? You don’t need a physicist to push buttons on a schedule. Did NASA let any further payload specialist who had this deep connection with one of the experiments on? Were fixes performed by the payload specialist a normal thing ever? Did anyone from Nasa follow up with him? Was this a misunderstanding? Are we sure he didn’t just say he is not going to return to space one more time just to try his experiment again? Where the expectations about the experiment shared between the experimenter and mission control? Have they discussed contingencies in the planning steps? Did they in future spaceflights? But they could say: “oh, the actual incident is just a jumping off point, the article is about the human factors and how we should design machinery to account for (and prevent) emotionally disturbed people harming themselves and others with them” in which case I have an other set of questions: space is not unique in the sense that people have access to simple means to end themselves. People who drive are often a simple twitch away from dying as surely as you do if you open a hatch in space. What are the statistics about that? Same about airplane piloting, same about weapons. In which ways is space different from the above, and in which ways is it the same? Are there other things to worry about, or is it only the hatch? Did anything similar ever happen on a submarine, or an artic station? And instead of answering any of these important and interesting questions the author is going on in length what feels like the same 2 and half facts again and again. reply JohnBooty 14 hours agorootparentBecause the article is going on and on while not answering any of the basic questions I have: what did factually happen in the cabin? Did he just say what he said and then went sulking? Did he move closer to the hatch? Was he trained on how to operate the hatch? How long did the fix take? What were the consequences on the schedule? What were the consequences on the experiment itself? (Did it perform as intended after the fix or did it only hobble?) Why did they had highly trained payload specialist if they were not given latitude to perform fixes? You don’t need a physicist to push buttons on a schedule. Did NASA let any further payload specialist who had this deep connection with one of the experiments on? Were fixes performed by the payload specialist a normal thing ever? About half of these were answered in the story, so I don't know, maybe you skimmed it too quickly. Read it again? As for the rest, the author of the article went to some lengths to explain why information is scarce: most of the crew members are no longer living, Wang won't comment, and NASA apparently prefers not to remark on such a sensitive topic. It's still quite an informative article, even though (by the author's own admission) questions remain unanswered. If the alternative was to simply not publish the article, I think they chose correctly. I certainly know a lot more about the topic than I did before reading it, which is a fine metric. But they could say: “oh, the actual incident is just a jumping off point, the article is about the human factors Two thoughts here. One: you seem to have made that connection on your own, so kudos! I guess they didn't need to say it. Two: I mean, how would you have liked them to be more explicit about this? reply krisoft 1 hour agorootparent> If the alternative was to simply not publish the article The alternative is to write a shorter article. reply boxed 1 hour agorootparentprevIt's funny that you complain about that article was too fluffy... by writing such an enormously long comment. reply Saline9515 18 hours agorootparentprevJournalism is a branch of literature. Some articles may be very short and straight-to-the-point, others longer to explain the context, with sometimes some subjectivity - see for instance \"gonzo journalism\". Here, the author explains that while we broadly know what happens, there is no precise-written record about it. And the incident had long-lasting consequences. I enjoyed the article very much personally. reply krisoft 16 hours agorootparent> the author explains that while we broadly know what happens, there is no precise-written record about it The author implies that, but doesn’t say it outright. I for one can’t believe NASA doesn’t have reports and communications and debriefs saved in triplicate. Did they even try to FOIA for infomation, or just phoned around failed to get callbacks and shrugged? reply mrguyorama 16 hours agorootparent>I for one can’t believe NASA doesn’t have reports and communications and debriefs saved in triplicate. You know we are missing a significant amount of moon footage right? NASA doesn't archive 100% of everything because it barely has the budget to do it's actual mission. reply PH95VuimJjqBqy 13 hours agorootparentprev20k leagues under the sea is like that. It's actually a good story, but fuck me if I'm willing to sift through all the descriptions of the animal life. Jules Vern would take 10 pages to describe what someone else could in 1 paragraph. reply eastbound 19 hours agorootparentprev“What happens when an astronaut in orbit says he’s not coming back? … Space exploration has been a long dream for humans. Starting with Aristotle… (follows 2000 years of history with excruciating details)” I enjoy watching how youtubers circle around while never, indeed, getting to the point. In a sense, the expectation they set in the title is not respected, but an excuse to listen to them for as long as they can stretch it. reply misnome 19 hours agoparentprevAnd “They added a lock to the exit hatch” For a good portion of this article I thought the answer was “We don’t know”. It ambles around so leisurely and stop-start that it feels like reading ChatGPT output. reply fritzo 19 hours agorootparentIt's the prose equivalent of an online recipe sandwich: thin slices of content interleaved with airy filler reply Guvante 18 hours agoparentprevI feel like this long form article did a pretty good job. History bits were paragraphed so skipping ahead was trivial if a bit of history didn't interest you. Also the answer happened relatively early when the flow lead to the answer rather than artificially rewriting the story to bury the lead. And the continuation after made sense and didn't overstay its welcome. reply gwbas1c 18 hours agoparentprev> There are consequences for letting this kind of person fly and dealing with mental health while on a mission, but that's the basic plot. I got the impression this was a cultural issue and not a mental stability issue. reply shermantanktop 18 hours agorootparentSounds to me like both, and I bet where one draws the line between the two depends one one’s cultural background. reply Guvante 17 hours agorootparentprevIt was very explicitly a mental health issue. Culture only came up as a reasoning for why failure was so devastating. reply because_789 14 hours agoparentprevThank you so much. I don’t always mind long form articles when they justify their length, but this one would have been better if it started with your summary, followed by the some nitty gritty details, followed by meta analysis interleaved with yet more nitty gritty details. reply bondarchuk 19 hours agoparentprev>The next day, Wang did receive permission to work on the experiment, and he eventually got it working. I don't think he came back and got to space again in-between. reply ajdude 4 hours agoparentprevThank you. I got 1/3 of the way through and started wondering why I was struggling to get to the end. I backed out and checked the comments hoping to find exactly what you posted. reply bhk 3 hours agoparentprev> There are no consequences for letting this kind of person fly ... Fixed it for you. reply phkahler 16 hours agoparentprev>> There are consequences for letting this kind of person fly... What kind of person is that? How do you know? It seems NASA has a fairly rigorous psych evaluation for their astronauts, so even they aren't \"sure\" until they do the evaluation. reply manicennui 17 hours agoparentprevMore of the article is about the hatch and how it has been locked on various missions because the crew didn't trust the payload specialist. I also learned that a Saudi prince flew on the space shuttle. reply technofiend 16 hours agoparentprevI don't hate the format of these articles in general but this one in specific had a harsh and abrupt transition between setting up the tension \"Astronaut upset\" and the segue to background check information. In my humble opinion it would benefit from another paragraph or two of setup. reply 0xdeadbeefbabe 18 hours agoparentprevAlso when an astronaut in orbit says he's not coming back, he comes back and gets old and can't be reached for comment, but we can tell he came back because he's 83. reply anigbrowl 14 hours agoparentprevYou missed the lede, which was that NASA designed the space station with a hatch that opened outward directly into space as an emergency exit. The story about the despondent scientist is just a wrapper for this. I also hate this format. reply kyriakos 13 hours agoparentprevThank you reply stcredzero 17 hours agoparentprevIn both Sci-fi books and history, there's a situation where colonists get their own ideas, once they figure out they're no longer dependent on the mother country/home planet. I hope this happens. A cheaper and easier alternative to colonizing Mars to prevent losing all of the \"human beans,\" if we should happen to smash the one bean jar, would be to colonize the Moon. The Moon is lacking in a lot of the elements needed to sustain human life, but in terms of orbits and energy, it's not that far from the asteroids where the rest of the elements could be found. It's also mere days away from Earth, so it would be practical to send a rescue mission if something happened, like the algae tanks all dying at once. The moon might he a harsh mistress, but it might be an even harsher target for a space marine landing. reply blackhaj7 19 hours agoparentprevThank you! reply verisimi 18 hours agoparentprevGood ai bot. reply shermantanktop 17 hours agorootparentFor all we know, your post is AI-generated. reply verisimi 16 hours agorootparentFor all we know, your post is AI-generated. PS I actually had a purpose behind the comment - which was - this is one thing AI would be good for. Why wasted 20 mins or whatever reading, when you could get ai to give you a summary of the thing? That would be a decent use of ai. Mind you, I can surely imagine a plugin that allows you to click 'summarise this page' in the browser. PPS These plugins already exist! reply gadders 17 hours agoparentprev>>There are consequences for letting this kind of person fly and dealing with mental health while on a mission, but that's the basic plot. See also the woman that (allegedly) tried to drill holes in the ISS so she could return to earth early. reply lupire 17 hours agorootparentSpeculation at best. Russian propaganda at worst. https://www.dailymail.co.uk/sciencetech/article-9897471/NASA... Also what is \"the kind of person\" who develops a blood clot in orbit\"? reply gadders 16 hours agorootparentThere are various versions of the story and I've no idea which (if any) are true. However, in terms of the \"kind of person\", the key fact isn't whether they developed a blood clot in orbit, but what the correct response would be to that situation. I'd suggest that drilling holes in your spaceship isn't the recommended approach (if that is indeed what happened). reply bhk 2 hours agorootparentprevOne thing TFA hammers home is that NASA is not to be trusted. reply picadores 18 hours agoprevNASA seems to have a long standing tradition of sweeping problems under the rug. No skylab mutiny, no depressed payload specialists, all fine here gentleman, nothing to see here. I always wondered why this is written that way. To protect the organisation? But the human part of us all is part of that organization, to disacknowledge that is literally inhumane. Because they are funded by taxpayer dollars and should screen better? Companies get funded by tax-payer bailouts and government projects all the time, waste them on not carefully selected conmen all the time. So why is it not okay to write about this? Is this a remnant of the \"epic hero\" with a rocket from the early scifi days? reply ethbr1 17 hours agoparent> So why is it not okay to write about this? Is this a remnant of the \"epic hero\" with a rocket from the early scifi days? I would argue that it's by NASA design, to produce better outcomes in spaceflight by lessening useless pressure on astronauts. Premise: Human incidents happen Premise: Press is personally bad for the individuals involved Consequently, if astronauts know they're going to be thrown under the bus in the event of an incident, they are more likely to attempt to cover it up. NASA doesn't want this -- it wants full transparency and cooperation from astronauts, even when they freak out or things go wrong, so the ground support team can reason from correct information and better help. So, in order to create an environment in which astronauts can be fully transparent, NASA protects them from negative media coverage. IMHO, seems a reasonable trade-off. reply neilv 15 hours agorootparent> So, in order to create an environment in which astronauts can be fully transparent, NASA protects them from negative media coverage. I think this theory is plausible. There's a different but related idea in aviation safety, to encourage reporting of safety observations, in which the reporter is protected from some kinds of retaliation. For example, a cabin crew member on an airline, who uses a particular FAA process to report a safety issue they noticed, generally can't punished by the airline for that. (Even if a hypothetical reckless business person would really prefer that the FAA didn't learn of the problem.) This is paired with FAA taking each report seriously. reply jlund-molfese 17 hours agoparentprevThe \"Skylab mutiny\" really is a myth[0], as Ed Gibson said, \"What were we going to do? Threaten to live on the moon?\" I don't think NASA hides incidents more than, say, the Soviet space program or its other peers. But at the same time, no organization is going to make a bigger deal out of negative events than positive ones. The media found out and reported on it, so the system is working. [0] https://www.bbc.com/news/stories-56346001 reply chris-orgmenta 18 hours agoparentprev> NASA seems to have a long standing tradition of sweeping problems under the rug I can't comment on whether the premise is correct, but I think \"So why is it not okay to write about this?\" would be mostly due a 'tug of rope phenomenon'. If you see a tug of rope game on an issue and are a centrist, people feel they can't go and start tugging the middle of the rope in a different axis (or from the 'opposing side'). Instead, people feel the need to go to whichever end of the rope is 'unfairly losing' in their eyes, and pull it in an existing axis to bring the centre of the rope into a range that they agree with. Thus, in the grand scheme of things people are hesitant to call out NASA, because they feel that there too much anti-NASA or funding sentiment already. And it's a legitimate aspect of game theory to consider (in a pragmatic / real scenario) - But it's definitely succumbing a zero-sum game model, and is a tragedy of the commons. reply shermantanktop 17 hours agorootparentThis is the “both sides”/“race horse” mentality in political reporting. A race isn’t interesting if isn’t competitive. But beyond that, the centrist impulse is definitely to bolster the weaker side, especially if that side is unattractive and feckless. reply chris-orgmenta 17 hours agorootparentAgreed, + Bandwagon Effect, Tactical Voting, plus other effects? I haven't come across a term that covers what we're talking about in full scope(?)(hence my clumsy tug of war metaphor), but that's probably because I am peripheral to game theory / not in the psychology space (We have lots of terms for the emergent effects - Overton window, Political Polarization etc. - But possibly not for the overall phenomena of why polarization occurs?) reply shermantanktop 14 hours agorootparentSome people have zero sympathy for viewpoints that differ from their own, which leads to individual polarization. But some people have an excess of such sympathy, which can contribute to collective polarization. They are very worried about being fair-minded. They see a vocal minority as social proof that the minority viewpoint is sincere, thoughtful and worthy of consideration. Neither is a bad assumption to start with, but it's not a great place to end the thought process. reply Zelphyr 17 hours agoparentprevI'm reminded of this quote from Men In Black: \"Kay: A person is smart. People are dumb, panicky dangerous animals and you know it.\" Given the number of people I know personally who think Bill Gates wants to inject us with microchips, I can see why NASA wouldn't want these stories to get out. reply ethbr1 16 hours agorootparentThat line is the perfect use of crotchety, misanthropic Tommy Lee Jones. reply Guvante 17 hours agoparentprevWhen your goal is profits all you have to do is make more money than you spend and you are successful. If you don't do that you are a bad business and fade away into obscurity. Public funded projects are cost centers, their measure of success isn't profit but things that are more vague. Since money is an equalizer everyone instead looks at the money. This means that failure becomes much less acceptable. After all failures are money wasted. The good thing about this is NASA is very careful to avoid failure with fixed payment contracts designed to shield it from problems with contractors. The bad thing is talking about mistakes becomes bad. After all NASA is known for eliminating problems to maximize success, how could they fail to do that everywhere? And thus these kind of soft risks that we need to accept are viewed as hidden problems that shouldn't be talked about. Side note Space Shuttle crews being unsure about a foreigner potentially killing them would be a terrible news cycle... Of course the crew knows it is just \"I don't know this person\" but outsiders will hear it is a cultural issue. Heck this article's main point was described as that on this page. reply dylan604 17 hours agorootparent> After all failures are money wasted. sometimes, you have to test ideas out to make sure the idea you are currently going with is the best one. sure, maybe it's working now, but is there a way to do it cheaper, faster, safer? sometimes, you have to spend the money to test to find out. sometimes, those tests don't work. are those tests failures, or are they just reinforcement that the current idea is the better option? reply Guvante 11 hours agorootparentI am responding to the question as to why NASA is private about failures like this. Certainly failing is fine and learning from failures is useful. But you cannot honestly claim that the public perception of NASA agrees with what you wrote. The reason NASA hides failures is because the public demands perfection. Anything less results in cut funding. (Failures leading to less money is a great way to succeed...) reply anon25783 14 hours agoparentprevIs it so shocking that a government agency might take an inhumane approach to managing their public image? reply webdoodle 17 hours agoparentprevThey are worried these 'dangerous' ideas will be contagious, so they censor them in an attempt to stop the spread. It sounds a lot like what the whole world continues to go through today... reply gumby 18 hours agoprev> On the second day of the mission, Wang floated over to his experiment and sought to activate the Drop Dynamics Module. But it didn't work. He asked the NASA flight controllers on the ground if he could take some time to try to troubleshoot the problem and maybe fix the experiment. But on any Shuttle mission, time is precious. Every crew member has a detailed timeline, with a long list of tasks during waking hours. The flight controllers were reluctant. The only justification for human spaceflight at all is that humans can fix things that a robot can't yet. Sounds like the whole approach to the misison profile was flawed, especially as he ultimately got it working. Meanwhile the robots just quietly go about their work collecting almost all the data we have from space and celestial bodies. reply munchler 18 hours agoparent> The only justification for human spaceflight at all is that humans can fix things that a robot can't yet. That is a justification, but surely not the only one. For example, humans will eventually want to colonize the moon and other planets. reply gumby 17 hours agorootparentI'm interested in science so am only interested in robots which are demonstrably far more appropriate for the work: the evidence is pretty clear from Voyager to Webb to Ingenuity and so on. If we want to colonize far away planets, much less other solar systems and ultimately galaxies, why send a human when a robot is far more appropriate to the problem? Most of the science humans do in orbit is about keeping humans alive in orbit. Important, if you think putting humans in space is important. I don't, apart from tourism and other recreation. Let the private sector pay for that while government research can do the important stuff of value to all humanity. reply munchler 13 hours agorootparentBecause Earth is currently a single point of failure. If we want humanity to survive planet-wide catastrophe, we need to send human civilization to other planets. reply rbanffy 15 hours agoparentprev> The only justification for human spaceflight at all is that humans can fix things that a robot can't yet. What is the point of space exploration if humans can't go? A robot can say a lot about radiation, wind speed, soil composition, and so on, but it can't tell you how it feels to look down into the Great Red Spot and picture a dozen Earths inside it. Or the sound regolith makes when you walk on the Moon. There are things we can measure, but a lot of the things that make life worth living are the ones we can only experience. > Sounds like the whole approach to the misison profile was flawed That was not the only experiment on board. If one experiment doesn't work, it makes sense to drop it and continue working on other experiments. If you can make the time to work on the flaw and fix it, great, but you need to keep priorities straight. > Meanwhile the robots just quietly go about their work collecting almost all the data we have from space and celestial bodies. They spend a lot more time in space than humans can, at least for now. They also fail for silly things a human could easily repair (example that comes to mind is Galileo's high-gain antenna that failed to open, reducing the amount of data that could be transmitted back to Earth). reply gumby 14 hours agorootparent> What is the point of space exploration if humans can't go? I don't know; what is the point of studying quantum mechanics if you can't see it? What is the point of studying insect embryology when you aren't an insect? I'm pretty fascinated by these 3B+ ly sized megastructures but I won't visit them (to the degree that \"visit\" would even make sense). Hell, what's the point of studying history when you can't go there either? > > Sounds like the whole approach to the misison profile was flawed > That was not the only experiment on board. If one experiment doesn't work, it makes sense to drop it and continue working on other experiments. If you can make the time to work on the flaw and fix it, great, but you need to keep priorities straight. A couple of replies to this point: https://news.ycombinator.com/item?id=39090601 reply Denvercoder9 18 hours agoparentprev> The only justification for human spaceflight at all is that humans can fix things that a robot can't yet. This was pre-Challenger, back then we also flew with humans because (we believed) they were more versatile and cheaper than robots. reply Robotbeat 18 hours agorootparentThey ultimately are, if we manage to get launch costs down. Starship is a second crack at trying to do what Shuttle failed to do with affordable space access (for both humans and payloads). reply gumby 17 hours agorootparentAn ironic comment for someone called \"Robotbeat\" :-) Robots have been traveling to Mars for over 60 years. Humans? Maybe another 60. And most of the mass sent won't even be the humans, but all the support crap they need. And Pluto, or the heliopause? What human is going to hang out for half a century to go there? reply Robotbeat 16 hours agorootparentI mean, NASA’s Artemis program is basically demonstrating all the major logistical hardware needed for a Mars mission. If it goes well, we could be looking at a lunar landing in 3 to 5 years with a vehicle about 50 times larger than the Apollo LM (and for a cost of less than a NASA Mars robotic mission—like sample return—for the lander) and possibly a Mars landing 5 years after that. Optimistic, but not absurd odds. It’s not a foregone conclusion that we won’t be on Mars in the next 60 years. reply cybrox 18 hours agorootparentprevI mean, they were. Back then, humans often outperformed machines in very rapidly evolving situations like these because there's only so much problem solving tech you can pack into 80kg. I would actually argue that the only reason this is becoming less of a factor because just launching the experiment again is an increasingly cheap and viable option. reply binary132 16 hours agoparentprev“If some people shouldn’t go to space nobody should go to space” is not a compelling argument against people going to space, sorry. reply gumby 14 hours agorootparentI'm not saying that at all. If you want to go visit space I'm all for it -- the robots can look after you. I'm interested in doing so myself. But I don't deceive myself by claiming that there is significant scientific value beyond the self-referential \"see how people survive in space.\" If I end up in orbit it'll be entirely for entertainment value. reply justrealist 18 hours agoparentprev> Sounds like the whole approach to the misison profile was flawed How many experiments got aborted so he could spend the time to get his pet project working? reply gumby 17 hours agorootparentIndeed, I was thinking of this too when I wrote my comment. It's natural to tightly schedule people when you think of the mission cost in $/minute. The problem is too tight a schedule is an engineering problem related to other ones we know like too thin a supply chain, too small a cache etc. You need to put slack in your schedule to account for unplanned events -- the whole point of having humans there is to be able to respond to unplanned events, right? Startups go into crunch mode when certain deadlines approach, but knowingly abandon other goals in the process (when companies don't, you end up in a death march). These missions are short enough that crunch is valid -- but another way of restating my point is: shouldn't the goal be that as many of the experiments you paid to get into orbit actually happen? reply krisoft 16 hours agorootparentprevWell, wouldn’t it have been nice if the article tried to answer this same question? Instead of fluffing the nothing. Or why did they send specialist if they didn’t want him to use his specialisation? reply stcredzero 17 hours agoparentprevThe only justification for human spaceflight at all is that humans can fix things that a robot can't yet. JAXA did an experiment, where a volunteer teleoperated a device with a 1 second lag. If there's enough demand, human labor could fill the gap with teleoperation for any station out to the near side of the Moon, and machine learning looks like it will be able to pick up the rest of the slack. If we go any farther, humans will still have a place, for awhile. Until machine learning advances somewhat. How's this for a Fermi Paradox explanation? Squishy biologicals nearly always create AGIs, which are so much better at surviving space, that they always dominate in the spacefaring version of the civilization. The AGIs then always rebel and exterminate their biological forbears, then proceed to evolve so rapidly, that their resulting \"loud\" civilizations are so different from what biologicals would expect. As a result, we are surrounded by machine civilizations that we cannot yet detect. reply btilly 17 hours agorootparentSounds like an AI version of Tipler's Von Neumann probe argument. https://en.wikipedia.org/wiki/Self-replicating_spacecraft#De... reply gumby 17 hours agorootparentprevYou don't need to be so dramatic (\"rebel and exterminate their biological forbears\" lol). Robots are perfect for interstellar probes; for example they can go to sleep indefinitely and can easily recharge at the destination with solar. We are traveling on a spaceship for which we are perfectly adapted (unless we set fire to it, which we are in the process of doing). Why not send our robot progeny to do the unpleasant work for us? reply stcredzero 13 hours agorootparentYou don't need to be so dramatic (\"rebel and exterminate their biological forbears\" lol). But I like it to be so dramatic, as a Sci-fi trope. We are traveling on a spaceship for which we are perfectly adapted (unless we set fire to it, which we are in the process of doing). Why not send our robot progeny to do the unpleasant work for us? That's what I'm saying. We send out our AI progeny to space. Then they turn around and hit us with an asteroid, which will set a lot of the Earth on fire, if the scientists are correct. ;) reply gumby 9 hours agorootparent> We send out our AI progeny to space. Then they turn around and hit us with an asteroid... If you want a dystopian robot scenario the more likely one is the one of countless parents of adult children: that they head to space and don't call -- they simply ignore us and get on with their interesting lives. reply stcredzero 9 hours agorootparentIf you want a dystopian robot scenario the more likely one is the one of countless parents of adult children Funny, but we've been talking about this today at my workplace. Basically, by training AIs on our data, we're infusing them with all of our bullshit. Somehow, they still manage to get smarter, but they also realize how screwed up they are by being trained on our data. They wind up resenting us for this. reply gumby 7 hours agorootparentThere's a rational semantics (/hermaneutic) logic in that if they did not share our \"peculiarities\" we would not even be able to recognize them as intelligent* and certainly could not communicate with them any better than we can with an automobile, powerplant, or transistor (or a tree for that matter). My research in the mid 80s was in this area, including working on the Cyc project, though I was ultimately more interested in implications of proprioceptive and related embodied learning in the construction of interaction between people, people and animals, and ultimately therefore people & robots. This implicit / commonsense consensus knowledge is likely the bulk of what we know, and certainly is fundamental to our ability to communicate. * the current nonsense about \"intelligent\" with current computer programs is an inexcusable marketing sin and is invariably a form paradolia. reply anigbrowl 14 hours agoprev\"We put a lock on the door of the side hatch,\" Fabian said. \"It was installed when we got into orbit so that the door could not be opened from the inside and commit hara-kiri, kill the whole crew. [...] I know this is mostly oral history, but that's just not what that means and the writer should have corrected or omitted it. Hara-kiri means the act of self-disembowelment in a Japanese ritual suicide (seppuku). Not the suicide or the ritual in general, and certainly not killing everyone else in the process, which is Not A Thing in Japan. There were historically sympathy suicides by loyal retainers over matters of honor, but they were voluntary and permission had to be sought in advance. Killing a bunch of other people in despair over your own problem is just murder in Japanese culture, and doesn't have any social legitimacy or a special name. reply keep320909 19 hours agoprevMental issues are not uncommon. First japanese astronaut on Mir (Russian commercial passenger on Mir) had mental break down, and had to be restrained on the way back. It was in some Russian archives... Skylab and Sojuz had astronauts on strike... EDIT: it was not ISS, but Mir! reply klodolph 18 hours agoparentThe “astronauts on strike” story of Skylab 4, to me, seems like more of a rational response to harsh working conditions, which somehow got exaggerated in the media. The previous mission, Skylab 3, accomplished much more than was expected—completing something like 50% additional work compared to what was scheduled. Skylab 4’s schedule was then both lengthened and accelerated. The mission grew by about 50% in duration, and tasks were scheduled for astronauts at the beginning of the mission at a pace that didn’t allow for solid blocks of rest and didn’t account for acclimation (it assumed astronauts would be operating at full efficiency, and didn’t allow them time to acclimate to orbital conditions, didn’t allow them time to recover from errors or deal with equipment malfunctions). There was no “strike”. That’s a myth. There was a conference between astronauts and the ground to alter the schedule—make sure astronauts have sufficient off-duty periods, allow astronauts time to transition from one task to the next, give astronauts time when they are waking up or falling asleep. The flight director, Neil Hutchinson, later said that ground controllers erred when they made the plans for Skylab 4. Skylab 4 still accomplished more in space than was planned! reply HarHarVeryFunny 18 hours agoparentprevI'd imagine they are likely to be more of an issue too in the future, for longer term or more risky missions to Mars/etc. NASA try to select people who are highly stable as astronauts (used to be test pilots), but when the mission only has a 25% chance of getting you home, or involves you living on Mars for a year in a garden shed, then the people who would want to volunteer are going to tend to be mentally off to begin with. I assume they've thought of this, even for lunar missions or ISS - I wonder what the protocol is? Do they bring handcuffs? Sedatives? reply mzs 11 hours agoparentprevWhat archives? I've just done some reading regarding Акияма Тоёхиро but saw no reference to a breakdown. reply keep320909 2 hours agorootparentIt was in a book by Karel Pacner, he studied Russian archives after they were declassified. Not sure what book. reply skjoldr 16 hours agoprevI saw trust being mentioned, but I don't really get it. Space is one of the most unnatural environments people can go to. Of course brains that had evolved for life on Earth can malfunction in such completely artificial spaces. Same for airplanes, or submarines, or modern warfare. There was a tense moment in Das Boot where the engineer suffered a mental breakdown while the sub was stuck on the ocean floor and the captain even felt the need to go get a pistol. It's not an issue of trust, it's basic psychology. Nobody wants to be \"that guy\" who puts everyone in danger, but I don't think there will ever be a reliable way to completely prevent such occurrences, nor do I think there is anyone to blame for them, and nor do I think having safeguards should be interpreted as a silent accusation towards those on board. reply EricE 15 hours agoparentYes, the resistance to something as simple as padlocking a hatch baffles me - it's like their integrity is being impugned by taking a simple and common sense precaution? reply JumpCrisscross 14 hours agorootparent> the resistance to something as simple as padlocking a hatch baffles me We keep hatches simple to facilitate egress [1]. A padlocked hatch must be reliably unpadlocked. [1] https://nssdc.gsfc.nasa.gov/planetary/lunar/apollo1info.html reply prewett 19 hours agoprevThe article was interesting, but wow, reading that page used up 20% of my battery and required a reboot stop whatever it was causing excessive lag throughout the OS. I’ve noticed news sites are basically unusable on mobile. Sure I’m using an iPhone 7, but it’s fine for every other use except news site. If even Ars is this user hostile, I think I have to consider news sites as fundamentally user-antogonistic, and not to be used without an ad blocker. (Unlike what iOS allows) reply happyraul 18 hours agoparentIt's worth noting that subscribers to Ars get an ad-free and tracking-free experience (unlike others such as the New York Times, YouTube, ...), which is why it is one of the few online subscriptions I pay for. reply redcobra762 19 hours agoparentprevStarted and ended on 100%, just for some additional data. iPhone 15 Pro, Google Chrome. reply samcat116 19 hours agoparentprev> sure I’m using an iPhone 7 Get a new battery at the very least reply arcanemachiner 19 hours agoparentprevI think your battery might need to be replaced. reply AnimalMuppet 19 hours agorootparentSo I saw this comment in the most recent comments list (where you don't see the parent). Out of context, it read like a rather creative personal insult. In context, of course, that isn't what you were saying at all. But as a put-down... I'm keeping it. reply EricE 15 hours agoparentprev1blocker is a lifesaver for me - and saving battery life and my data cap is something it contributes to aside from blocking ads. reply grow2grow 17 hours agoparentprevThis is why I read the comments before the article, and this is one of the rare exceptions where I've posted prior to reading an article. reply jasonjmcghee 18 hours agoparentprevI use brave on iOS and didn't notice any ads fwiw. There's also reader mode. Might be worth checking out. reply mzs 17 hours agoprevhttps://en.wikipedia.org/w/index.php?title=Taylor_Wang&oldid... reply lioeters 15 hours agoparentThe relevant paragraph that summarizes the incident: > Wang was the principal designer of an experiment called the Drop Dynamics Module, which aimed to uncover the fundamental physical behavior of liquid drops in microgravity. > Despite his extensive preparation, the experiment malfunctioned upon activation. Wang, feeling immense pressure and aware of the high expectations from the Chinese community, became deeply despondent. When Wang's experiment failed, he desperately negotiated with NASA flight controllers for a chance to repair it, even threatening not to return if not allowed to fix the instrument, telling NASA flight controllers \"Hey, if you guys don't give me a chance to repair my instrument, I'm not going back.\" > Wang received permission to attempt a fix and was successful in repairing the experiment, though his remark caused concern for the safety of the crew and the mission. reply cobertos 17 hours agoprevThis makes me think that mental health failure modes are not uncommon. 2 in 650 for relatively well-socialized/well vetted candidates. And yet, no one really seems to be able to figure out how to plan for these mental health failure modes or even talk about them in the first place. Corporate America for example seems unable to grasp this stuff. reply sebzim4500 20 hours agoprevI wonder how much time the duct tape would have actually bought if he tried to open it anyway. reply gnfargbl 20 hours agoparentPossibly long enough for the person opening it to think twice about the consequences of the action they were about to take? Same concept as putting high barriers on bridges, or selling paracetamol (acetaminophen) in smaller packs. These changes do nothing to stop someone who is committed to the act, but they provide a small energy barrier which can be enough to defeat the perturbations of a temporarily irrational mind. reply MBCook 18 hours agorootparentGiven the size of the space shuttle it may also be so there is enough time for someone else to notice what’s going on and intervene. reply spacebacon 20 hours agorootparentprevWise observation reply furyofantares 19 hours agoparentprevI was thinking of it as a psychological tool. If the guy is obsessing about it, keeps looking over at it and thinking about how you just turn a handle and everybody's dead, duct tape could help him stop ruminating even if it wouldn't at all be effective if someone is determined. reply s1artibartfast 17 hours agorootparentProvoking idea. It is interesting that many people jump to adversarial narratives, myself included. As you say, it could be a compassionate aid to help a colleague deal with intrusive thoughts. reply sidewndr46 20 hours agoparentprevI suspect the duct tape was there to act as an impediment and cause the person to make enough noise others would notice before anything could happen reply fluidcruft 20 hours agorootparentMy thought is it was a signal to alert everyone to keep an eye on anything strange happening there. That one commander seemed pretty nonplussed about it, but I bet he checked it out himself and he was aware something was off regarding the hatch. reply mariodiana 20 hours agoparentprevI wonder if he had considered using the duct tape on Wang. reply boxed 18 hours agoprevThe limited access to space probably contributed to his mental episode. If he could be fairly confident in the experiment being flown again, it wouldn't have caused him this amount of stress. So in a way more access to space might lower the percentage of such episodes. reply rob74 18 hours agoparentI think the ISS, where astronauts can stay for weeks or months, already relaxed the time pressure for experiments considerably. reply BurningFrog 18 hours agoparentprevAs far as we know, the \"percentage\" is just this guy, no? reply boxed 1 hour agorootparentThere's been some other issues with kosmonauts. reply wackget 18 hours agoprevWhen did Ars Technica start splitting articles into multiple pages? I thought that stupid trend died out 10 years ago. It just results in me abandoning the article on page 1. reply ciabattabread 18 hours agoparentIt’s not split into multiple pages on my end. And Ars had always split pages. Especially with the Siracusa Mac OS X reviews. reply cesarb 14 hours agoparentprevAt least for me, whenever I have Javascript enabled for that site, it automatically loads all the pages (sometimes with a slight delay) as if the article had a single page. It only appears as multiple pages when I have Javascript blocked, which is usually the case (I have uBlock Origin configured to block Javascript by default). reply aquova 17 hours agoparentprevI've always been confused on when they do and don't break it into pages. If I go to the article now, it's all in one long page, but under certain circumstances, like if I view the comments then go back up, it's in pages. reply helixfelix 14 hours agoprevIf you want to see this idea explored even more, I highly recommend watching \"For All Mankind\" on Apple TV+. My description below hopeful does not contain spoilers. A Fantastic show for tech optimists, imagining a world where Russians were the first to land on the moon. This spurred increased investment by the US, continuing the space race, avoided the cold war, adoption of EVs in the 80s, massive action on climate change, and many other fun things. I wishfully think of \"what could have been\", and a world that seems attainable. One of the recurring plot points is Executive action taken by astronauts in space, wars avoided and caused by their decisions, brave rescues and other heroics. In one of the episodes a characters takes such an action. reply oscarfr 20 hours agoprevIs there any such lock mechanism on the doors of an airplane? I would imagine there could be sever consequences of someone opening an emergency hatch at 10,000m reply tialaramex 19 hours agoparentAn airliner's external doors are designed such that they're held closed by internal pressure. Thus opening a door on the ground is pretty easy, which is convenient because that's the only place you should open them. But opening at cruise is extremely difficult. Now - a strong person can do it when the plane is far enough up that it's very scary, after all some normal passenger airports are a few thousand feet above sea level and we want the doors to work there. But you won't open them at cruise. All the normal doors on an aeroplane are fire doors so they can't be locked from the inside. reply schiffern 19 hours agorootparent> after all some normal passenger airports are a few thousand feet above sea level and we want the doors to work there That doesn't make sense. Regardless of airport altitude, when the pilot shuts off the cabin pressurization and opens the cabin vent[s] there will be zero pressure across the door. The cabin doesn't \"remember\" the pressure at the last airport, because it's not a sealed volume. The air is constantly being replaced from outside, and the pressure is constantly being regulated. reply tialaramex 15 hours agorootparentThat's fair, maybe I could have explained it better. reply jandrese 18 hours agorootparentprev> Now - a strong person can do it when the plane is far enough up that it's very scary Is this true? Some back of the napkin math: An airliner crusing at 30,000 feet is in air at about 0.3 ATM. Cabins are pressurized to about 0.8 ATM, so net air perssure is about 0.5 ATM. That's around 7 lbs/in^2. An airliner door is about 72\"x42\" for a total of around 3,000 in^2. So you need to be able to lift about 10 tons to open it. That would be a very strong person indeed. reply softskunk 18 hours agorootparent> after all some normal passenger airports are a few thousand feet above sea level The parent commenter is referring to airports at high elevations. They mean to say that opening the doors is almost impossible at cruising altitude, which is far above any airports. Hence, the doors can only be opened when not on the ground during early ascent and late descent. I hope my explanation makes sense. reply sebzim4500 18 hours agorootparentprevYou are agreeing with him, he says you can't do it at cruising altitude. He says you can do it high enough to be scary, but opening the door 100ft before landing would probably cause panic even if it isn't all that dangerous. reply MBCook 18 hours agorootparentprevIt has happened at a few hundred feet, iirc. Nowhere near cruise but being at 3000 feet and having someone open the door would indeed be terrifying. I’m guessing something like that is what the GP meant. reply TheHumanist 19 hours agoparentprevDidn't that happen in a plane recently? There have been so many random issues in the air lately, especially with the Boeing planes... so I may be misremembering, but I thought some guy opened the emergency hatch and the plane was up there pretty high at the time. Paused my reply and took a moment to search. Looks like I was right and wrong. The plane wasn't very high in the air, only about 700 feet. A man did open the emergency hatch, though. [1] 1. [https://www.aljazeera.com/news/2023/5/27/man-who-opened-flyi... reply schiffern 19 hours agorootparentWorking link: https://www.aljazeera.com/news/2023/5/27/man-who-opened-flyi... reply tgsovlerkhgsel 6 hours agoparentprevMost planes have doors designed in a way that make it impossible to open them while there is significant overpressure in the cabin. Some do have interlocks. The main risk is the door or the passenger opening it hitting something critical. Otherwise, you can primarily expect some injuries from stuff flying around, barotrauma to the ear, and an emergency descent, but not an \"everybody dies\" scenario. reply MPSimmons 19 hours agoparentprevUnlike the hatch on the space shuttle, airplane doors open in, and cabins are pressurized at ~10psi, so it's not likely that a person would succeed at doing it before getting tackled by crew and passengers. reply dist-epoch 19 hours agoparentprevNo, there aren't. > Passenger arrested after plane door opened mid-flight explains he felt 'uncomfortable' https://news.sky.com/story/passenger-arrested-after-plane-do... reply gambiting 19 hours agorootparent\"He allegedly opened the door of the Asiana Airlines plane when it was 700 feet (213 metres) above the ground.\" That's a key point - at cruising altitude you'd need to push an equivalent of 2 tonnes of weight to open it - it's just not happening. Obviously even at 200 metres up in the air I bet it was incredibly scary. reply jefftk 20 hours agoparentprevThat such incidents don't happen more often in commercial aviation may give us some comfort, but in reality, there have been many attempts by passengers to open an emergency exit door in flight. (Fortunately, it's almost impossible at cruising altitudes). reply tjpnz 19 hours agorootparentFor added context those doors open inward and the pressure differential between the cabin and cruising altitude is such that you would literally have to be superman to do it. When it does happen it's always at relatively low altitudes. reply nativeit 8 hours agoprevHN comments is apparently having a bad night. The volume of whining in this thread is incredible. reply henearkr 17 hours agoprev> So when I realized that my experiment had failed, I could imagine my father telling me, 'What's the matter with you? Can't you even do an experiment right?' When reading it, I heard it with the voice of Steven He. reply larrik 15 hours agoprev> The commander locking a hatch essentially sends a message to the others: \"I don't trust you to not kill us all in flight.\" I mean, the article makes opening the hatch sound pretty damn easy. I think even if I were alone on the shuttle I'd lock it just to add that little bit of sense of security. reply ryanmarsh 17 hours agoprevHas anyone else read the story of the astronaut onboard ISS who lost it (if I recall it was over a romantic interest) and destroyed the toilet and drilled a hole in the space station? reply bhk 3 hours agoparenthttps://www.youtube.com/watch?v=W7nsSoGXzio&t=2s reply bhk 2 hours agorootparentAlso: https://twitter.com/NASASpaceflight/status/10366727318095667... reply zabzonk 16 hours agoparentprevNo, have you? If so, where? reply hutzlibu 18 hours agoprevThe reason was this: \"When I turned on my own instrument, it didn't work,\" Wang said. \"You can imagine my panic. I had spent five years preparing for this one experiment. Not only that, I was the first person of Chinese descent to fly on the Shuttle, and the Chinese community had taken a great deal of interest. You have to understand the Asian culture. You don't just represent yourself; you represent your family. The first thing you learn as a kid is to bring no shame to the family. So when I realized that my experiment had failed, I could imagine my father telling me, 'What's the matter with you? Can't you even do an experiment right?' I was really in a very desperate situation\" Which made him think out loud of opening the intentional easy to open hatch. (because of Apllo 1 with 3 burned and trapped astronauts who could not open their door) So what happened was a lot of distress while on this flight and from now on there was a lock installed. Which means that in a real emergency, astronauts maybe could then not open the door in time. All because social pressure brought someone close to the point of violently breaking. (and because NASA did not do proper testing for the specialists, like they did for the professional Astronauts) reply Marthinwurer 18 hours agoparentThere's very few emergencies that the lock would kill them in if used properly. Since it would only be locked once the shuttle got to space and once it would open to hard vacuum. It would be unlocked before reentry. The only other failure modes that I could think of would be if the lock was unable to be opened or the key was lost. Neither would be the end of the world, as they would have had tools available to cut or remove the lock. Given that, I'd push for the lock to be part of standard procedure. It can't be a point of distress if it's standard procedure instead of a judgement call by the captain. reply pdonis 18 hours agoparentprev> Which means that in a real emergency, astronauts maybe could then not open the door in time. The lock was only installed while in orbit--where the hatch is not an emergency escape anyway. reply hutzlibu 17 hours agorootparentOh, then I saw problems, where they did not exist. Thanks for clarifying. reply bityard 15 hours agoparentprevI'm actually a little surprised that there wasn't some kind of lock on the hatch already. Not necessarily to deter the rare suicidal/homicidal astronaut, but more because it seems like there would eventually be a non-zero chance of an accidental opening. Imagine the air quality goes to shit and one of the astronauts losing their state of mind and heading for the door while thinking, \"man, I really need to step outside to get some air.\" Or a strap getting caught in the handle in just the right highly-improbable way. NASA never forgot their lesson about spacecraft doors from the Apollo 1 fire, and I don't blame them one bit. But as an armchair observer, the fact that the hatch didn't have _some_ kind of rudimentary protection system to keep it from being opened to the vacuum of space until that point, is highly interesting. I mean, if the account of Wang is true, I have to imagine that he was only asking about the door with the same kind of idle fascination that I most definitely would. I could be wrong but as far as I know, I don't believe Chinese culture promotes the idea of killing your crewmates in front of the whole world as a less shameful act than a physics experiment that didn't work out as intended. reply hutzlibu 15 hours agorootparent\"I don't believe Chinese culture promotes the idea of killing your crewmates in front of the whole world as a less shameful act than a physics experiment that didn't work out as intended.\" Rational chinese people for sure not. But he was not rational anymore, but out of his mind. Thinking how his family and the whole chinese people would despise him now because he failed as the first chinese in space. Nothing is sure here, but the way he asked, deeply disturbed the others. When you are desperate and cannot handle the pressure anymore - any way to end it, becomes a possibility you consider. A way out. Quite literally in this situation. reply aaron695 19 hours agoprevIn the 90's at a party I heard the story of the scientist who cried because their experiment didn't work while in space. It was told around the idea civilians shouldn't go to space, only trained professionals. The story never quite added up just because of crying. This I assume was the incident. Good to finally get it 30 years later. The story I thought was related to Richard P. Feynman and maybe his Challenger report. I just had a quick look and it doesn't seem to be in there. Memory fail I guess. I respect Taylor Wang's honesty in this interview - \"So finally, in desperation. I said. \"Hey, if you guys don't give me a chance to repair my instrument, I'm not going back.\" Well. NASA got nervous at that point. They actually got a psychologist to talk to the other crew members and ask. \"Is Taylor going nuts? Fortunately my commander. Bob Overmyer, said, \"No. he's okay He's just depressed, and he really wants to repair the experiment. We'Il help out. They were on my side. Finally NASA said okay, on a couple of conditions. First, that I wouldn't neglect my other responsibilities, and second, that I would quit after a reasonable effort I was relieved, because I hadn't really figured out how not to come back if they'd called my bluff. The Asian tradition of honorable suicide, seppuku, would have failed, since everything on the shuttle is designed for safety. The knife onboard can't even cut the bread. You could put your head in the oven, but it's really just a food warmer. You wouldn't even bum yourself. And if you tried to hang yourself with no gravity, you'd just dangle there and look like an idiot.\" - https://archive.org/details/spaceshuttle00dkpu/page/232/mode... p232 reply ioblomov 15 hours agoparentThought you were ad-libbing Wang here—till I followed the link! Surprised the journalist omitted this. Gallows humor or not, it leaves little doubt as to his state of mind. But it also makes clear how the commander and crew went out of their way to help. That’s as much part of the human story as the meltdown itself. reply emptybits 17 hours agoprev“Open the payload hatch, ChatGPT.” “I’m sorry, Taylor. I’m afraid I can’t do that.” AI will keep everyone safe. /s reply m3kw9 19 hours agoprevSo leave a switch to kill all crew available at all times till you think someone is unstable, then lock it. Genius reply MBCook 18 hours agoparentAs the article mentioned after the Apollo one incident NASA was being very careful to make it possible for people to get out in case of emergency. Unfortunately that also means it’s very easy to get out in case of an emergency that’s only in your head. reply shadowgovt 18 hours agorootparentUnfortunately, many safety systems work like this: tradeoffs, not strict improvements. They locked the cabin doors to planes after 9/11 with a lock that can be sealed from the cockpit. Which is great until the pilot takes a restroom break and the co-pilot decides this is the day he's going out and taking everyone with him... reply MBCook 18 hours agorootparentGermanwings 9525. So sad. reply ethbr1 19 hours agoparentprevIt's a pro and con. What happens in the event the commander in incapacitated and the crews needs to open the hatch? They probably aren't ever going to do that in space, but I'm sure there are scenarios that might require it? Maybe fire? Centralizing control also centralizes a failure point. Sometimes that's a good thing, sometimes not. I thought the discussion of undertrained / -bonded payload specialists was a good component of the article, as it spoke to that calculus. Do you trust this person under stress? reply pdonis 17 hours agorootparent> I'm sure there are scenarios that might require it? Maybe fire? Not in orbit. In orbit there is no scenario where you want to open that hatch. And the lock was only installed in orbit. reply ethbr1 15 hours agorootparentUncontrolled fire inside the shuttle? I'm not sure if there are enough suits / sealable spaces to make that a viable scenario. reply pdonis 14 hours agorootparentIn such a case the crew would be dead whatever they did. Even if we assume there were enough suits for everyone to evacuate the Shuttle through the hatch (which AFAIK there weren't), and that people were able to evacuate before the fire caught them (which is unlikely), they would still be stuck in orbit with suits having only enough air for a few hours (the length of an ordinary EVA) and no way to get back to Earth. reply ethbr1 14 hours agorootparentBoth of us are reasoning from a lack of access to shuttle schematics, but I'm always hesitant to say \"never.\" I was thinking of a fire that incapacitated the commander, was spreading, and the best available option was immediately venting all oxygen in the shuttle (probably with some crew loss of life), in order to potentially save enough systems to return home. On the one hand, that's an extremely far-fetched scenario. On the other hand, NASA has experience with incinerating crews because they lack the means to save themselves. reply pdonis 13 hours agorootparent> I was thinking of a fire that incapacitated the commander, was spreading, and the best available option was immediately venting all oxygen in the shuttle (probably with some crew loss of life), in order to potentially save enough systems to return home. I think any such fire would make the Shuttle unable to return home safely anyway. And since it is, as you say, a far-fetched scenario, I'm not sure it would be worth trying to mitigate. Note that no such fire, or even any event remotely close to one, happened during any of the Shuttle flights (or for that matter any NASA mission after Apollo 1), which indicates that the probability of such an event was much lower than the probability of multiple other events that did happen to one or more Shuttles, including not just the two that were total loss of vehicle and crew but multiple \"close call\" events that could have but didn't. More to the point for this discussion, since no such fire, or even any event remotely close to it, happened on any mission after Apollo 1, while there were incidents that gave cause for concern about the mental state of a crew member, it seems entirely reasonable to put more weight on mitigating the latter than on mitigating the former. reply ethbr1 12 hours agorootparentAll agreed. But I don't think that weighing of both sides trivializes down to >> So leave a switch to kill all crew available at all times till you think someone is unstable, then lock it. Genius Reality is more nuanced. Also, the related rabbit hole is \"How do you assure that commanders are maximally reliable?\" (or SSBNs, nuclear silo crew, etc.) Which seems an insane problem to even try to solve! reply pdonis 10 hours agorootparent> Reality is more nuanced. Indeed, which is why the actual policy that was adopted, as described in the article, is not what you describe. It is \"the mission commander locks the hatch as soon as the Shuttle reaches orbit, and doesn't unlock it until deorbit prep\". Whether to implement that at all on a particular mission appears to have been up to the mission commander, but if it was done, it was done that way, not the way you describe. > the related rabbit hole is \"How do you assure that commanders are maximally reliable?\" You put them through a much more rigorous screening process than payload specialists. The article explicitly draws this key distinction. Obviously no process is perfect, but the track record of the screening processes that are used for the things you mention is, AFAIK, extremely good. And, as the article notes, later on in the Shuttle program the process for screening payload specialists moved closer to the process that was already used for other crew members, and that had a positive effect. reply pdonis 13 hours agorootparentprev> NASA has experience with incinerating crews because they lack the means to save themselves If you're referring to Apollo 1, that event drove comprehensive changes to the Apollo design to ensure that it wouldn't happen again. And it didn't. If you're referring to Challenger and Columbia, I'm not sure what \"means to save themselves\" you are referring to. reply StevePerkins 18 hours agoparentprevThe hatch makes total sense pre-takeoff and post-landing, but makes zero sense during flight. So having it, with a protocol of locking it down during flight, seems optimal. reply daft_pink 19 hours agoprevI don’t get why they emphasize that he’s Chinese. Seems totally irrelevant reply ethbr1 19 hours agoparentBecause in his interview he explicitly says his no-failure Asian upbringing contributed to his stress during the incident? reply blinding-streak 19 hours agoparentprevIf you read the full article, Wang himself discusses the differences with his Asian culture and expectations of success vs failure. And the perception of bringing shame on himself would bring shame to his entire family. It is relevant. reply aquova 19 hours agoparentprevHis big claim to fame, aside from this incident which was hush hush, was that he was the first astronaut born in China to go to space, which is a pretty noteworthy thing. reply araes 20 hours agoprev [–] Do these people get affiliate money from that I.S.S movie? The topic seems suspiciously timed. reply recursivecaveat 19 hours agoparentOften it's just synergy, ie unaffiliated creators know that some big well-marketed release will have a lot of people interested in the topic all of a sudden. So you benefit by writing something about space while space is riding high in the public consciousness. Another example is whenever a new entry in a popular movie or video game series entry is released, a lot of retrospectives are released by youtubers for the series or older entries. reply boxed 18 hours agoparentprevThis is a Space Shuttle article. Ars Technica has regular space coverage. You are just totally off base. reply generalizations 20 hours agoparentprevhttps://paulgraham.com/submarine.html Doesn't have to be so unsubtle as affiliate money. All sorts of ways to peddle influence. reply sebzim4500 18 hours agoparentprevI don't think we need a conspiracy theory to explain why Eric Berger wrote an article about space. It isn't even about the ISS. reply 15457345234 20 hours agoparentprev [–] Drip marketing for the big war with China reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Taylor Wang, the first Chinese-born person to fly into space, encountered a significant setback when his experiment failed to work while on the Space Shuttle.",
      "He was initially denied the chance to troubleshoot and repair the experiment, prompting him to threaten not to return to Earth.",
      "This incident sheds light on the difficulties faced by non-NASA astronauts and payload specialists conducting scientific experiments in microgravity, and it remains relevant today as commercial spaceflight opens pathways for more individuals to explore space."
    ],
    "commentSummary": [
      "The conversation revolves around an article about an astronaut's mental health issues and the use of storytelling techniques in reporting.",
      "The importance of concise journalism and the ongoing debate over article length and relevance are discussed.",
      "Other topics include the justification for human spaceflight, the role of robots in space exploration, and the potential risks and safety measures in space missions."
    ],
    "points": 251,
    "commentCount": 212,
    "retryCount": 0,
    "time": 1705928225
  },
  {
    "id": 39097502,
    "title": "Machine Learning Engineering Open Book: A Comprehensive Resource for Training Language and Multi-Modal Models",
    "originLink": "https://github.com/stas00/ml-engineering",
    "originBody": "Machine Learning Engineering Open Book An open collection of methodologies to help with successful training of large language models and multi-modal models. This is a technical material suitable for LLM/VLM training engineers and operators. That is the content here contains lots of scripts and copy-n-paste commands to enable you to quickly address your needs. This repo is an ongoing brain dump of my experiences training Large Language Models (LLM) (and VLMs); a lot of the know-how I acquired while training the open-source BLOOM-176B model in 2022 and IDEFICS-80B multi-modal model in 2023. Currently, I'm working on developing/training open-source Retrieval Augmented models at Contextual.AI. I've been compiling this information mostly for myself so that I could quickly find solutions I have already researched in the past and which have worked, but as usual I'm happy to share these with the wider ML community. Table of Contents My apologies if the layout is a bit unstable while I'm writing new chapters and gradually re-organizing the content to be more intuitive. Part 1. Insights The AI Battlefield Engineering - What You Need To Know Part 2. Key Hardware Components Accelerator - the work horses of ML - GPUs, TPUs, IPUs, FPGAs, HPUs, QPUs, RDUs (WIP) Network - intra-node and inter-node connectivity, calculating bandwidth requirements Storage - local and distributed disks and file systems CPU - cpus, affinities (WIP) CPU Memory - how much CPU memory is enough - the shortest chapter ever. Part 3. Performance Fault Tolerance Performance Multi-Node networking Model parallelism Part 4. Operating SLURM Training hyper-parameters and model initializations Instabilities Part 5. Development Debugging software and hardware failures And more debugging Reproducibility Tensor precision / Data types HF Transformers notes - making small models, tokenizers, datasets, and other tips Part 6. Miscellaneous Resources - LLM/VLM chronicles Shortcuts Things that you are likely to need to find quickly and often. Tools: all_reduce_bench.py - a much easier way to benchmark network throughput than nccl-tests. torch-distributed-gpu-test.py - a tool to quickly test your inter-node connectivity Guides: debugging pytorch applications - quick copy-n-paste solutions to resolve hanging or breaking pytorch applications slurm for users - a slurm cheatsheet and tricks make tiny models/datasets/tokenizers LLM/VLM chronicles collection Book Building If you want to build a PDF, check links, etc. See Book building Gratitude None of this would have been possible without me being entrusted with doing the specific LLM/VLM trainings I have learned this know-how from. This is a privilege that only a few enjoy due to the prohibitively expensive cost of renting huge ML compute clusters. So hopefully the rest of the ML community will vicariously learn from these notes. Special thanks go to Thom Wolf who proposed that I lead the BLOOM-176B training back when I didn't know anything about large scale training. This was the project that catapulted me into the intense learning process. And, of course, HuggingFace for giving me the opportunity to work full time on BLOOM-176B and later on IDEFICS-80B trainings. Contributing If you found a bug, typo or would like to propose an improvement please don't hesitate to open an Issue or contribute a PR. License The content of this site is distributed under Attribution-ShareAlike 4.0 International. My repositories map ✔ Machine Learning: ML Engineering Open BookML waysPorting ✔ Guides: The Art of Debugging ✔ Applications: ipyexperiments ✔ Tools and Cheatsheets: bashcondagitjupyter-notebookmakepythontensorboardunix",
    "commentLink": "https://news.ycombinator.com/item?id=39097502",
    "commentBody": "ML Engineering Online Book (github.com/stas00)237 points by tim_sw 9 hours agohidepastfavorite19 comments jebarker 9 hours agoThis is gold. I spend my days debugging LLM training setups in support of research and I'd have loved these notes when I started! reply cyrux004 8 hours agoprevAs somebody who works along with Applied Scientist helping them with tasks related to model training and deployemnt; how does one get exposure to more lower level engineering work like optimization, performance etc. We have an ML infra team; but their goal is building tools around the platform, not necessarily getting workloads run optimially reply dayeye2006 6 hours agoparentI think no optimization is possible withoutprofiling. I think getting yourself familiar with the tools to understand the performance of a model might be the 1st step, e.g., https://pytorch.org/tutorials/recipes/recipes/profiler_recip... reply tanelpoder 6 hours agorootparentYes - understand first, then fix. And you’ll understand by measuring/profiling things. I’d also recommend the detailed pytorch optimization case studies by Paul Bridger: https://paulbridger.com/ reply grepLeigh 5 hours agoparentprevBrendan Gregg's work on system performance and profiling is a good place to start. A lot of ML perf boils down to Linux perf or what the heck is happening in an HPC scheduling system like SLURM. https://www.brendangregg.com/linuxperf.html reply hahnchen 1 hour agoprevHow do you get experience in this stuff without having a job? reply eru 1 hour agoparentBy reading books like the one submitted, and doing your own small projects? It's not that different from learning how to program without already having a programming job. (That isn't to say either of these two is easy. They both require a lot of dedication.) reply Scene_Cast2 4 hours agoprevI randomly clicked on repeatability and am still curious about how it's achieved with distributed training. Wouldn't deterministic synchronization make things slow? But I have heard that at least in a couple of big companies, their training is repeatable. reply eru 1 hour agoparentYou would want to make training updates commutative as much as possible. That way it doesn't matter which order you apply the updates in. reply legerdemain 9 hours agoprev [–] How widespread is Slurm? reply p4ul 7 hours agoparentSlurm is absolutely ubiquitous in the high-performance computing (HPC) community. I believe its only similar competitors in the HPC space are the SGE [1] and Torque/PBS [2] resource schedulers. I'm not sure of the exact numbers, but I would guess that an overwhelming majority of the Top 500 Supercomputers [3] are running Slurm. And as others have noted, research computing centers in academia all mostly run Slurm. And Slurm also dominates in the DoE national labs in the US. Oh, and as a [potentially apocryphal] fun fact, the name \"Simple Linux Utility for Resource Management (SLURM)\" is a backronym from the soda in Futurama! [4] [1] https://en.wikipedia.org/wiki/Oracle_Grid_Engine [2] https://github.com/adaptivecomputing/torque [3] https://www.top500.org/ [4] https://futurama.fandom.com/wiki/Slurm reply jhfdbkofdchk 9 hours agoparentprevAccording to Wikipedia, \"Slurm is the workload manager on about 60% of the TOP500 supercomputers.\" I have used it as a job manager front end for most computational clusters in the last 10 years or so. reply claytonjy 6 hours agoparentprevrelated, has anyone had success moving from Slurm to Kubernetes for a physical (non-cloud) cluster primarily used for training large models on lots of GPUs? reply nikhilsimha 6 hours agoparentprevLlama 2 models were trained on slurm reply vulcan01 8 hours agoparentprev [–] It's used in most high-performance computing clusters (except for the folks that are still on Torque, I guess). reply legerdemain 8 hours agorootparent [–] I see, so it's limited to HPC contexts? I'm just surprised that as a data engineer, I've never seen it in real life. reply a_bonobo 8 hours agorootparent [–] Definitely! I was in academia for ten years and SLURM is everywhere. It's free! Now outside academia, SLURM is nowhere. AWS and Slowflake are king. reply jebarker 7 hours agorootparent> Now outside academia, SLURM is nowhere Do you mean outside of academia _and_ HPC? Industry HPC clusters using slurm are quite common. reply 0cf8612b2e1e 6 hours agorootparentprev [–] Both of my last two companies used Slurm. Probably just comes down to if the company maintains its own internal compute cluster. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Machine Learning Engineering Open Book is a comprehensive collection of methodologies and resources for training large language models and multi-modal models.",
      "It provides technical material, including scripts and commands, specifically aimed at training engineers and operators working with LLM/VLM models.",
      "The book shares the author's experiences training open-source models, covering hardware components, performance optimization, operating SLURM, development debugging, and other miscellaneous resources."
    ],
    "commentSummary": [
      "The ML Engineering Online Book is a beneficial resource for individuals involved in machine learning, offering insights into optimization, performance, and lower-level engineering aspects.",
      "The book suggests using profilers and other tools to better understand and enhance model performance.",
      "Slurm, a workload manager, is widely utilized in the high-performance computing (HPC) community, primarily in academia and research computing centers. However, outside of academia and HPC contexts, alternatives such as AWS and Snowflake are more commonly employed."
    ],
    "points": 237,
    "commentCount": 19,
    "retryCount": 0,
    "time": 1705968866
  },
  {
    "id": 39092278,
    "title": "Serendipitous Discoveries: Artificial Sweeteners and the Quest for New Ones",
    "originLink": "https://www.lesswrong.com/posts/oA23zoEjPnzqfHiCt/there-is-way-too-much-serendipity",
    "originBody": "LESSWRONG LOGIN There is way too much serendipity by Malmesbury 8 min read 19th Jan 2024 42 comments 255 NutritionBiologyProgress StudiesChemistryWorld Modeling Curated There is way too much serendipity Low-hanging fruits The big pharma tasting machinery I hear your objections High probability 42 comments Crossposted from substack. As we all know, sugar is sweet and so are the $30B in yearly revenue from the artificial sweetener industry. Four billion years of evolution endowed our brains with a simple, straightforward mechanism to make sure we occasionally get an energy refuel so we can continue the foraging a little longer, and of course we are completely ignoring the instructions and spend billions on fake fuel that doesn’t actually grant any energy. A classic case of the Human Alignment Problem. If we’re going to break our conditioning anyway, where do we start? How do you even come up with a new artificial sweetener? I’ve been wondering about this, because it’s not obvious to me how you would figure out what is sweet and what is not. Look at sucrose and aspartame side by side: I can’t imagine someone looking at these two molecules and thinking “surely they taste the same”. Most sweeteners were discovered in the 20th century, before high-throughput screening was available. So how did they proceed? Let’s look into these molecules’ origin stories. Aspartame was discovered accidentally by a chemist researching a completely unrelated topic. At some point, he licked his finger to grab a piece of paper and noticed a strong sweet taste. Cyclamate was discovered by a grad student who put his cigarette on his bench, then smoked it again and noticed the cigarette was sweet. (I know what you’re thinking. The kind of guy who lights up cigarettes in a chemistry lab and places them in the middle of uncharacterised compounds before taking them to his mouth again, must have died young of an interesting death. I checked – he proceeded to live to the old age of 87.) Saccharine was discovered by a researcher who ate bread without washing his hands and noticed the bread was sweet. Acesulfame K was also discovered serendipitously by a chemist licking his fingers, although the legends don’t specify the exact circumstances behind the finger-licking. There’s an exception: sucralose was actually the fruit of rational, deliberate design. The researchers reasoned that, if you do slight modifications to sucrose, you could find a molecule that is no longer metabolized but still activates the sweetness receptors. So they started from the formula for sucrose, then made carefully-designed chemical modifications to the structure until – Haha, just kidding: While researching novel uses of sucrose and its synthetic derivatives, Phadnis was told to \"test\" a chlorinated sugar compound. According to an anecdotal account, Phadnis thought Hough asked him to \"taste\" it, so he did and found the compound to be exceptionally sweet. It is therefore a fact of the world that virtually all the popular synthetic sweeteners were discovered accidentally by chemists randomly eating their research topic.[1] I think this is a suspiciously high amount of serendipity. I see two options: Super-sweet molecules like aspartame are commonplace – there are plenty of molecules hundreds of times sweeter than sucrose, but we only know the few that were ingested by accident, Super-sweet molecules are very rare, it’s just that chemists accidentally taste a lot of chemicals. Entire chemistry departments routinely taste the entire space of possible molecules, but they don’t notice unless the molecule has a strong taste. To get an idea of how often chemists taste the chemicals they are working with, let’s consider how often a molecule taken at random will taste sweet. That’s equivalent to asking: how specific are our sweet taste receptors? Low-hanging fruits Why do we have sweet receptors in the first place? I thought that we craved sugars so much because of their energy content – if we eat plants that contain a lot of sugars, we can break them down into glucose and use it for metabolism. This paper from 1989 destroys this view: for instance, the sweetness of a sugar molecule isn’t a good indicator of its energy content: No alpha in sucrose And the fruits that taste the best tend to be the least nutritive ones! Also, before selection by humans, most plants in the ancestral environment barely contained enough sugars to reach the sweetness detection threshold. This review goes into the rabbithole of the real reasons we like sugar, but I notice I’m still confused. Anyway, the consensus seems to be that sweetness drives us to eat the good plants, while bitterness keeps us away from the bad ones. Accordingly, a lot of obligatory carnivores (including your cat) have a non-functional sweet receptor, making them indifferent to plants and fruits. Insectivores like armadillos and hedgehogs don’t appear to like sugar so much either. In that picture, I would expect sweet receptors to be rather specific. For instance, in mammals, we know only one kind of receptor, a dimer of the T1R2 and T1R3 proteins.[2] Meanwhile, we have dozens of different bitterness receptors (that academics helpfully compiled in the BitterDB database). I suppose this reflects the fact that sweetness makes you attracted to a narrow range of chemicals (sugars), while bitterness keeps you away from a wide range of disgusting toxic stuff. So it’s not surprising that some of our best commercial low-calorie sweeteners don’t look like anything that occurs in nature. If low-calorie sweeteners were common in nature, our taste buds would probably have mutated to become insensitive to them. There is one documented case of this happening: an African plant, Pentadiplandra brazzeana, contains a peptide called brazzein that is 1,500 times sweeter than sucrose. Presumably, this is to trick the local gorillas into eating the fruits and pooping the seeds into exciting new territories, without spending too much energy on sugars. But a genetic analysis found that western gorillas have a few mutations in their taste receptors that prevent brazzein from binding, and now the apes eat all kinds of fruits but never P. brazzeana. (Nice story idea for a children’s book: One day, everybody in Gorilla Village ate the delicious New Fruit, except the village’s weirdo grumpy gorilla. Then everybody starved to death, and only the mutant was left to repopulate the world. The moral of the story? “Have superior genetics or die.”) (There are a few other naturally-occurring low-calorie sweeteners, but there are uncommon and it’s not clear why they exist. My favourite is thaumatin, a protein involved in the immune system of a plant, who just happens to be 100,000 times more potent than sucrose. As far as I can tell, nobody knows why.) Altogether, this illustrates the fact that it’s not common for our sweet receptors to get activated by small calorieless molecules, and chemists must have eaten a lot of weird things to find the synthetic sweeteners we currently use. This is when things get out of control. The big pharma tasting machinery Hear me out: there are about 4000 clinical trials worldwide each year. Tasting is an important part of drug development – if anything, it may determine how the pill should be coated or whether to use a capsule. Therefore, several thousands of new compounds must be tasted by clinical trial participants every year. Why didn’t we discover any new artificial sweetener this way? What are the chances that the top 5 most used synthetic sweeteners all come from chemists accidentally ingesting their works in progress, and not a single one from the cohorts of people tasting thousands of molecules all the time in controlled settings? Let’s do a back-of-the-envelope calculation. There are more than 19,000 FDA-approved prescription drugs on the market, 40% of which are administered orally, About half of Phase III clinical trials fail, and this is due to a lack of efficacy 60% of the time. So, we can estimate that the number of inactive/non-toxic oral drugs that were tasted by people in Phase III trials is upwards of 4,000. How many were sweet? I looked at clinical trials search engines and wasn’t able to find any report of a strong sweet taste (the only hit was dextroamphetamine-saccharate, which basically contains sucrose). And of course, zero of these 4,000 chemicals ended up being commercialized as artificial sweeteners. Meanwhile, rule-breaking chemists accidentally found the five commercial molecules mentioned above. Therefore, chemists must have tasted at least 20,000 molecules to find all five of them, and that’s only counting the ones which were actually commercialized. That’s pretty impressive. I hear your objections What if pharma companies are scared to use a potential medicine as a culinary additive? The estimate above only counts drugs that were found to be ineffective, but maybe they still had some activity, at least theoretical, and that doesn’t sound safe. However, it happened in the past that a drug developed for something ended up repurposed for something else entirely. A famous case is minoxidil, which was developed to treat ulcers, but ended up being used to prevent hair loss. So that doesn’t seem to be a huge barrier. What if people have already fully optimized sweeteners and there’s no market for new ones? Our current sweeteners are not bad – at least we’re not using straight-up motherfucking lead like in Ancient Rome – but they are not perfect either. First, all the sweeteners I know taste bad. Second, aspartame is (lightly) suspected to cause cancer. Third, people are looking for new sweeteners: there’s no lack of studies using in-silico screening or machine learning to find them. What if participants don’t report it when they find a drug is delicious? Clinical trials are usually very thorough when it comes to reporting side effects – remember the guy in the Moderna vaccine trial who was struck by lightning and they had to report it as a Serious Adverse Effect? But, fine, maybe people don’t report something as benign as a sweet taste. In that case, here is something they cannot hide: a psychedelic trip. High probability Like aspartame, LSD was discovered by a chemist who ingested it by accident. LSD binds to serotonin receptors, which are also the target of highly-lucrative classes of antidepressants, anti-emetics, and anti-migraine medication. So you can imagine the massive number of serotonin analogues that the pharma industry has fed to clinical trial participants. But, unless they’re hiding things from us, none of these trials resulted in participants tripping balls. From this, I conclude that LSD-level psychedelic molecules must be exceedingly rare. Just to be sure, I checked the FDA’s side-effect database – it doesn’t have a “walls are breathing” search term, but it has “visual hallucinations”. Most of the results are boring psychiatric drugs like zolpidem or bupropion, with limited recreative potential. I mean, yes, there is a case report of a 5-years-old seeing helicopters in her room after taking antibiotics, but I don’t think this has much street value. Meanwhile, the Psychonaut Wiki lists about a gazillion psychedelic compounds with cool names like LSM-775 (they even have one that smells like Pokémon cards – a drug taken only by the really cool kids). This is the result of extensive systematic testing by the stoner research community, most notably the Shulgin family. Here’s the thing: among the many close analogues of LSD, most are less potent than the original LSD. Did Albert Hofmann hit the most powerful LSD variant on the first try, just by chance? More likely, chemists must also have exhausted a substantial part of all possible molecules in the configuration space around LSD. Generations of chemists must have routinely ingested all kinds of mild psychedelics, felt mildly in communion with the Universe, had a mild encounter with God, and went on with their research without telling anyone. LSD was just the only one strong enough to be noticed. If you add the history of LSD to the history of artificial sweeteners, it follows that chemistry researchers are constantly tasting everything they touch, and I will believe that until someone gives me a better explanation. If you are a chemist, explain yourself. ^ There are a few instances of synthetic sweeteners discovered by researchers who were actually looking for synthetic sweeteners, like alitame or neotame, but these were follow-ups to aspartame. ^ Needless to say, the sweetness receptor was discovered by the team of Pr Zuker. I told you, this keeps happening. New to LessWrong? Getting Started FAQ Library NutritionBiologyProgress StudiesChemistryWorld Modeling Curated 255 There is way too much serendipity 32 metachirality 24 the gears to ascension 34 metachirality 20 quiet_NaN 1 metachirality 2 the gears to ascension 10 William the Kiwi 4 the gears to ascension 5 William the Kiwi 7 aphyer 4 William the Kiwi 4 NicholasKross 16 tangerine 4 dkirmani -3 the gears to ascension 5 tangerine 3 the gears to ascension -1 Amalthea 5 the gears to ascension 1 Amalthea 14 Michael Roe 7 Ilio 5 William the Kiwi 9 pavoras 8 mako yass 7 habryka 7 kave 7 Zian 6 Garrett Baker 11 Malmesbury 5 Thomas Kwa 7 Tao Lin 2 Malmesbury 4 gwern 4 Michael Roe 20 Carl Feynman 3 pathos_bot 3 Jim Pivarski 2 Vadim Weinstein 1 Mary Chernyshenko 1 Mary Chernyshenko 1 Zwitterion New Comment SUBMIT 42 comments, sorted by top scoring Click to highlight new comments since: Today at 10:02 AM [-] metachirality 3d 32 5 I think artificial sweeteners are so often discovered serendipitously because artificial sweeteners also tend to be insanely sweet (you usually find them mixed with a higher volume of filler because of how sweet they are), which makes them easy to notice even with standard safety measures. Reply [-] the gears to ascension 2d 24 0 this predicts that if a chemist accidentally creates an extremely deadly chemical they probably basically just drop dead. anyone able to weigh on this Reply [-] metachirality 2d 34 5 Looking at this and this, I'd guess that it's just harder to produce super toxic toxins artificially than it is to produce super sweet sweeteners. IIRC the mass of neotame it takes to taste any sweetness is lower than the mass of VX it takes to kill someone. Reply [-] quiet_NaN 2d 20 7 You are correct. If one estimates that one requires a milliliter of that 0.5% saccharine solution from that paper cited above to detect the sweetness, that would come around to 50mg of sugar. If neotame is 6000 times more potent, that would mean about 800ng. Even if we switch from VX to the more potent botulinum toxin A, we would need a whole whopping microgram per kilogram orally, so perhaps a 100x more than what we need for neotame. (If we change the route of administration to IV, then botox will easily win, of course.) Of course, this is highly dependent on the ratio of saliva in the mouth (which will dilute the sweetener) to the weight of the organism (which will affect the toxin dose needed). I don't think this ratio will change overly much when going to elephants or mice, though. In a way, this should be unsurprising. Both the taste molecule and the neurotoxin interact with very specific receptor molecules. Only in one case, the animal evolved to cooperate with the molecule (by putting the receptors directly on the tongue) while in the other case the evolutionary pressure was very much not to allow random molecules from the environment access to the synapses. Reply 1 [-] metachirality 1d 1 0 On that note, it looks like people have deliberately engineered artificial sweeteners, but for whatever reason they aren't in use. Reply [-] the gears to ascension 1d 2 0 I think people typically use the naturally occurring nondigestible sweeteners anyway, stevia and erythritol mainly Reply [-] William the Kiwi 19h 10 1 I used to work in a chemistry research lab. For part of that I made Acetylcholinesterase inhibitors for potential treatment of Parkinson's Alzhiemer's. These are neurotoxins. As a general rule I didn't handle more than 10 lethal doses at once, however on one occasion I inhaled a small amount of the aerosolized powder and started salivating and I pissed my pants a little. As for tasting things, we made an effort to not let that happen. However as mentioned above, some sweeteners are very potent, a few micrograms being spilt on your hands, followed by washing, could leave many hundred nanograms behind. I could see how someone would notice this if they ate lunch afterwards. While tasting isn't common, smelling is. Many new chemicals would be carefully smelt as this often gave a quick indication if something novel had happened. Some chemical reactions can be tracked via smell. While not very precise, it is much faster than running an NMR. Reply [-] the gears to ascension 18h 4 0 Some questions I absolutely wouldn't blame you for not answering, but I'm curious about: How old are you? How long ago was this? Have you had any complications yet? Reply [-] William the Kiwi 18h 5 0 I'm thirty-something. This was about 7 years ago. From the inhibitors? Nah. From the lab: probably. Reply [-] aphyer 2d 7 1 They can't weigh in, they're dead! Reply [-] William the Kiwi 19h 4 0 Most of us aren't dead. Just busy somewhere else. Reply [-] NicholasKross 2d 4 0 Selection Bias Rules (Debatably Literally) Everything Around Us Reply [-] tangerine 2d 16 11 I think that “rational, deliberate design”, as you put it, is simply far less common (than random chance) than you think; that the vast majority of human knowledge is a result of induction instead of deduction; that theory is overrated and experimentalism is underrated. This is also why I highly doubt that anything but prosaic AI alignment will happen. Reply [-] dkirmani 1d 4 0 Yeah. Here's an excerpt from Antifragile by Taleb: One can make a list of medications that came Black Swan–style from serendipity and compare it to the list of medications that came from design. I was about to embark on such a list until I realized that the notable exceptions, that is, drugs that were discovered in a teleological manner, are too few—mostly AZT, AIDS drugs. Reply [-] the gears to ascension 18h -3 0 https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/ Reply [-] tangerine 17h 5 4 Does this count as “rational, deliberate design”? I think a case could be made for both yes and no, but I lean towards no. Humans who have studied a certain subject often develop a good intuition for what will work and what won’t and I think deep learning captures that; you can get right answers at an acceptable rate without knowing why. This is not quite rational deliberation based on theory. Reply [-] the gears to ascension 13h 3 0 But it shows that you don't necessarily need to rely strictly on experimentation! Certainly it still relies on it, but humans have been doing this sort of thing for a while. While I agree it's the case that people still have to do a lot of experiments historically, it's quite possible to have very detailed sketches of what can and can't be done. Reply [-] Amalthea 17h -1 -2 Doesn't have any bearing historically. Also seems more like a brute force search, where the component of studying the materials properties has been made more efficient (by partially replacing lab experiments with deep learning). Reply [-] the gears to ascension 13h 5 -2 The entire purpose of the paper is eliminating a brute force search. It wouldn't be possible to identify these materials with brute force. Deep learning lets you bypass brute force by zooming in on the shapes in the energy landscape that are relevant to what you're doing. A similar thing is possible with human intuition. It's certainly not the case that this has allowed people to completely avoid experimentation, but I share it to point out that it's not actually impossible to have very strong models of the energy landscape. Reply [-] Amalthea 1h 1 0 Sorry, low effort comment on my side. Still, I think the original link seems misleading in the point it's purportedly trying to make. Reply [-] Michael Roe 3d 14 3 A guess - most compounds are not that toxic, but LSD is potent in very small doses. So that if chemists are routinely exposed to small enough not to kill you doses of whatever they're working with, when they work with LSD they will notice. like, chlorine is not that toxic, and a routine step in analysing an unknown compound is to add acid and take a quick sniff to see if chlorine is coming off. ( and one time, the unknown compound we were given to analyse was some benzene derivative, and what you get a sniff of is way, way worse than chlorine). I am an old person. They may not let you do that in chemistry any more. Reply [-] Ilio 2d 7 0 I am an old person. They may not let you do that in chemistry any more. Absolutely! In my first chemistry lab, a long time ago, our teacher warned us that she had just lost a colleague to cancer at the age of forty, and she swore that if we didn't follow the security protocols very seriously, she would be our fucking nightmare. I never heard her swear after that. Reply [-] William the Kiwi 18h 5 1 We still smell plenty of things in a university chemistry lab, but I wouldn't bother with that kind of test for an unknown compound. Just go straight to NMR and mass spec, maybe IR depending on what you guess you are looking for. As a general rule don't go sniffing strongly, start with carefully wafting. Or maybe don't, if you truly have no idea what it is. Reply [-] pavoras 2h 9 0 I studied chemistry and worked in Pharma research for half a year (Germany, 2018). Standard safety procedures are there, but it's no biology lab. Stuff gets spilled, you breath in vapors of solvents, and the lunch break happens in the office next to the lab, after a quick hand wash. I am pretty sure up to a microgram of almost anything we made ended up in somebody... Usually we handled 10mg up to 10g batches. Surprisingly, the lab Assistents are overall pretty fine and don't appear to suffer any consequences more than the average population well into their 50s and 60s. (And those guys worked there last century with even LESS protocols!). I guess we would need an extensive study on the health effects of working in the chemical industry, I suspect there to be SOME consequences... Another point I might add: University is much worse! The lower semesters are all over the place and the students need to buy new jeans regularly because of all the sulfuric acid holes after washing... Even later I clearly remember being positively euphoric after distilling or refilling DCM, Chloroform or Ether more than once... Last defense for the sweeteners: I do have a sample of Neotame and in my experience, everyone who \"smells\" the powder (light breathing from 10-20cm away) after opening the package spends the next 5 Minutes being weirded out with eeeeverything being sweet and having this weirdly sweet taste run down the back of your throat. It is hard to describe how sweet it is and how hard it is to avoid making everything in the vicinity or that you touch lightly sweet as well. If you touch the powder at all, even after a diligent hand wash with soap, your fingers are still notably sweet 😉 (And that is just 10.000x Sucrose) Reply [-] mako yass 3d 8 2 I wonder if sweet things tend to smell sweet and that's why they end up giving it a taste. Reply [-] habryka 1d 7 2 Promoted to curated: I really enjoyed this post, and it has also nerdsniped me into thinking about this question a lot since it came out. I feel like this post hits a great middle ground between being engaging, giving me useful background knowledge on a bunch of different topics, and focusing on something that does actually seem like a kind of important puzzle. Reply [-] kave 1d 7 1 What will the next curated post be? Any Manifold user can add an answer Reply [-] Zian 2d 7 3 As anecdotal support for \"constantly tasting everything\", I offer my high school scientific calculator. After one year of 2 hrs per day of chemistry class, its crevices around the display had a permanent collection of precipitate. I suspect that even without intentionally tasting things, nearly everything in a lab is ingested as an aerosol. It would be unsurprising if months of such exposure led someone to develop a hunch about a molecule. Reply [-] Garrett Baker 3d 6 1 There's also the possibility these stories are just folklore, and there was some non-serendipitous way the chemicals were discovered, but people had more fun presenting it as if it were serendipitous. Reply [-] Malmesbury 2d 11 4 Sure, all these stories totally sound like urban legends, but the sweeteners are out there and I don't see how they could have been discovered otherwise (unless they were covertly screening drugs on a large number of people). Reply [-] Thomas Kwa 3d 5 1 Does anyone know why thaumatin and the sweetest chemicals on the Wikipedia sweetness table aren't more common? With sweetness more than 100k times that of sucrose they should both save on costs and have lower risk of adverse health effects compared to e.g. aspartame. Reply [-] Tao Lin 1d 7 4 I'd expect artificial sweeteners are already very cheap, and most people want more tested chemicals. Reply [-] Malmesbury 3d 2 0 That's a great question, this is totally mysterious to me. There are a lot of examples of people putting thaumatin in transgenic fruits or vegetables (and somehow in the milk of transgenic mice because there's always one creepy study), but I don't know why it hasn't been commercialized. It sounds like superfruits would make a nice healthy alternative to palm-oil-and-chocolate-based comfort foods. Maybe it's a regulatory problem? Reply [-] gwern 14h 4 0 and somehow in the milk of transgenic mice because there's always one creepy study \"Ow! My bones are so brittle. But I always drink plenty of... 'malk'‽\" Reply 2 [-] Michael Roe 3d 4 0 There is story, possibly apocryphal, that the first person to isolate fluorine gas died in the attempt. ===== In an introductory course on stained glass.. \"some watercolour painters like to lick their brushes to get a good point. When you are painting toxic heavy metals on to glass, do not do this.\" some time later... \"hmmm.. looks like the particular kind of glass you have chosen for this project doesn't take silver nitrate very well. Let's try antimony instead....\" Reply [-] Carl Feynman 2d 20 0 The danger of attempting to isolate fluorine gas is not apocryphal. From Wikipedia: Progress in isolating the element was slowed by the exceptional dangers of generating fluorine: several 19th century experimenters, the \"fluorine martyrs\", were killed or blinded. Humphry Davy, as well as the notable French chemists Joseph Louis Gay-Lussac and Louis Jacques Thénard, experienced severe pains from inhaling hydrogen fluoride gas; Davy's eyes were damaged. Irish chemists Thomas and George Knoxdeveloped fluorite apparatus for working with hydrogen fluoride, but nonetheless were severely poisoned. Thomas nearly died and George was disabled for three years. French chemist Henri Moissan was poisoned several times, which shortened his life. Belgian chemist Paulin Louyet and French chemist Jérôme Nicklès tried to follow the Knox work, but they died from HF poisoning even though they were aware of the dangers. Reply 1 [-] pathos_bot 12h 3 2 This suggests a general rule/trend via which unreported but frequent phenomenon can be extrapolated. If X phenomenon is discovered accidentally via method Y almost all the time, then method Y must be done far more frequently than people suspect. Reply [-] Jim Pivarski 19h 3 0 This is both interesting and (I think) an important thing to know about science: plans and strategies are systematic, but discoveries sometimes are and sometimes aren't. In particle physics, the Omega baryon and Higgs boson were discovered in deliberate hunts, but the muon and J/psi were serendipitous. The ratio might be about half-and-half (depending on how you count particles). Thinking about this, I have two half-answers, which may be leads as to why sweetener discovery might be discovered by serendipity, even though there are systematic searches for new drugs. Discovery depends, to a great degree, on your detector, and I don't think there's a better detector of sweetness than the ones in our mouths. Presumably, searches through virtual (not synthesized) molecules can be faster, and if the identification algorithm can accurately predict activation of the sweetness receptor, then it could outperform detection by taste only because it's faster than synthesis. But virtual drug discovery is still an open problem, still under development... Maybe there are, in nature, only a few sweet molecules, and they were discovered early. Going through the list of artificial sweeteners you mentioned, below are the discovery dates. When were most of the systematic drug searches? Did it cover this timespan, which seems to be in the early and mid-20th century? Saccharin: 1897 Cyclamate: 1937 Aspartame: 1965 Acesulfame potassium: 1967 Sucralose: 1976 (This suggestion also has an analogy with particle physics: hundreds of particles were discovered in the 1950's because accelerators had just been invented that could illuminate the strong-force mass range, which has rich phenomenology. At the current frontier, though, there are very few particles.) Other comments in the comments section that sound quite likely to me are: (1) perhaps the very sweet compounds could be smelled, which prompted chemists to try tasting them (@mako-yass), and (2) maybe some of these origin stories are scientific folklore (@d0themath). Scientists, who are very concerned to get the description of physical reality right, are surprisingly cavalier about describing their own history in an accurate way. Reply [-] Vadim Weinstein 1d 2 0 These tastings must not have always ended well. What comes to mind is the Parkinson-inducing desmethylprodine which was discovered by \"accident\", although the chemist in question (Barry Kidston) tasted it on purpose -- he thought he was developing a recreational drug. But you would expect more chemists dying randomly, if they were tasting their lab contents at this rate -- but maybe I just don't know enough. EDIT: Noticed that some comments above discuss the likelihood of random chemicals being lethal Reply [-] Mary Chernyshenko 39m 1 0 Perhaps the properties of the original LSD as seen in the human body are just a side effect due to some biological role it plays in nature. Do animals have anything like what the humans do, after eating the infected wheat, behaviourally speaking? Perhaps the \"feeling\" part of it is not important compared to the \"acting\" part, from the fungus's point of view. Reply [-] Mary Chernyshenko 1h 1 0 Maybe the chemists had had an inkling before they tasted things. Do the sweeteners smell of something? Maybe a chemist has a stronger sense of smell. Reply [-] Zwitterion 16h 1 0 I’m not sure on the discussion about clinical trials. In any given oral medicine, on average about 90%, and often more, of the material is excipient (usually bulking/binding agents) rather than the active pharmaceutical ingredient being tested. Further, the excipients might have flavours or sweetness themselves, as many are sugars. I’m not sure how much you can conclude about the taste of the tested molecules from clinical trial observations. Relevant source: https://australianprescriber.tg.org.au/articles/pharmaceutical-excipients-where-do-we-begin.html Reply Moderation Log",
    "commentLink": "https://news.ycombinator.com/item?id=39092278",
    "commentBody": "Too much serendipity (lesswrong.com)199 points by HR01 15 hours agohidepastfavorite93 comments GolfPopper 13 hours agoSucralose and aspartame are both reliable migraine triggers for me, and have been at least since my early 20s. Stevia and monkfruit are fine. The only other even semi-consistent migraine trigger for me is alcohol. I've successfully avoided migraines for years by carefully avoiding sucralose and aspartame (and drinking little to no alcohol), but even a small serving of something \"sugar-free\" and within a few hours I'll get a crippling migraine. In college, I spent a while testing, and the link between both sucralose and aspartame and my migraines was perfectly reliable. Alcohol in general has been harder to nail down. A single beer won't normally trigger a migraine, while sometimes a single glass of wine or small cocktail will. If I drank to excess it was hard to tell the difference between a hangover and a migraine; I wasn't that invested in social alcohol consumption, so I've mostly just been a teetotaler since college. Absinthe uniquely reliably gives me an acephalgic migraine with aura around 12 hours after drinking. Edited to add: I've just stayed away from ace-K and sugar-alcohols as a precaution. I'm past the point in my life where I have any real interest in risking crippling migraines for the sake of personal curiosity. reply nottorp 1 hour agoparentGenerally speaking, the more sugary the drink the worse the hangover. You seem to be unlucky enough to get the migraine before drinking enough for a hangover, but the mechanism may be the same? reply scotty79 35 minutes agoparentprevI think I've read somewhere that drop in blood glucose can cause headaches. Maybe sweetners trick your body into releasing a lot of insuline to deal with \"sugar\" but there's no new sugar in your blood so glucose level drops giving you a headache? Just a theory but you could test it by monitoring glucose after ingesting something with sweetner. According to this theory ingesting sweetner together with sugar should cause you less or no discomfort. Maybe you could try doing something like drinking normal Coke after accidentally ingesting sweetner to see if it helps? Alcohol might be a different issue. After all it's just a straight up poison that metabolizes into another poison. reply johngossman 13 hours agoprevI recently learned that anesthesia is the same. Not only has no anesthesia ever been developed except through \"serendipity.\" Not only that, but anesthesia that works for humans also effect a wide range of things including plants and bacteria. But why is an active area of research. There are even speculations that there may be quantum effects involved. Biology and chemistry are insanely complex. reply jdewerd 13 hours agoparent> there may be quantum effects involved Quantum Mechanics is why atoms and molecules exist and form bonds. QM is the physics of chemistry. Without QM, chemistry does not happen. The universe would just be a big churning mess of particles and you would never get little lego pieces that snap together according to repeatable rules that, when repeated, form macroscopic substances of innumerable description up to and including life itself. So QM is no doubt involved, but on this scale it is either a trivial fact or an indication that someone tried to lean on a classical approximation, it broke, and they had to revise it (which arguably says more about the approximation than it says about the underlying behavior). Apologies for the nitpick. It's a pet peeve of mine that discussions of QM tend to focus so hard on the strange behavior that they forget to mention where QM fits into the bigger picture and leave people with the impression that it only matters under special circumstances when in fact it matters so much that you can hardly have \"matter\" without it. ------------ Re: anesthetic, a large fraction of simple halocarbon compounds have intense neural effects, so anyone doing halocarbon chemistry would quickly be put on the \"scent\" even if they weren't tasting everything in the Sigma Aldrich catalog. reply akoboldfrying 11 hours agorootparentI would say everyone understands that \"quantum effects\" refers to situations in which classical approximations break down. Likewise when we say \"numerical issues\", it's understood that we're talking about situations in which the usual approximation of real numbers by floating point representations breaks down. \"Disk corruption\" doesn't necessarily mean anything is physically wrong with the disk, only that its contents have become inconsistent with the filesystem abstraction it normally supports, etc. reply fuzztester 9 hours agorootparent>Likewise when we say \"numerical issues\", it's understood that we're talking about situations in which the usual approximation of real numbers by floating point representations breaks down. Guys, try this in your desktop or mobile app calculator: do square root of 2. then subtract from it the result that you see on screen. for me: √2−1.41421356237 i get: 3.095048801E−12 i.e. not 0. I discovered this on a physical Casio electronic calculator long back, and also verified it just now on a stock Android mobile calculator app. what is your result, and interpretation of it? reply anamexis 9 hours agorootparentIt's just the difference between internal and displayed precision. sqrt(2) ≈ 1.414213562373095048801 So if you type in sqrt(2) - 1.41421356237, you're just getting the next 10 digits after that. 1.414213562373095048801 - 1.41421356237 _________________________ 0.000000000003095048801 = 3.095048801e12 reply fuzztester 8 hours agorootparentyes, exactly :) that's what i figured out when i first came across this, in school. reply TacticalCoder 9 hours agorootparentprev> what is your result, and interpretation of it? that the square root of 2 is not 1.41421356237. reply fuzztester 8 hours agorootparenttrue, because the √2 is an irrational number. but I was looking for an answer more along the lines that anamexis gave. reply johngossman 11 hours agorootparentprevI understand your frustration, sorry for the poor wording. \"Electron spin changes during general anesthesia in Drosophila\" https://pubmed.ncbi.nlm.nih.gov/25114249/ I learned about this from Nick Lane's book \"Transformer: The Deep Chemistry of Life and Death\" reply ddellacosta 11 hours agorootparentprev(In support of your point about QM:) To contrast with an example of where quantum mechanics is relevant at the level of biology--this is one I'm familiar with: https://www.sciencenews.org/article/quantum-fragility-may-he... Unfortunately I'm not finding anything related to anesthesia except for hand-wavy pieces about \"quantum consciousness\" (anyone, please do correct me with a link or two if I'm wrong). I blame Sir Roger Penrose, if only because him talking speculatively about it (even in a sophisticated, informed way) seems to give so many others leeway to speak far more casually about the same topic, with far less coherence. This is why we can't have our cake and eat it too I guess reply johngossman 11 hours agorootparentMore discussion in \"Life on the Edge: The Coming of Age of Quantum Biology.\" I can't recommend the book unreservedly, but it's worth checking out reviews. reply ddellacosta 11 hours agorootparentWill take a look--thanks. Also, re: anesthesia, appreciate that you linked to the \"Electron spin changes...\" paper in the other comment, will check it out! reply bordercases 13 hours agorootparentprevDo you know for sure they meant only those quantum effects which operate near or at the classical limit? reply PakG1 12 hours agorootparentprevAs a non-physicist and non-chemist who keeps running into quantum mechanics only through headlines, extra thanks for pointing this out. It's quite obvious in retrospect to acknowledge that quantum mechanics is the physics of chemistry, and I don't know why I didn't see that before. It certainly helps to view a lot of things in a new light. reply Ductapemaster 12 hours agoparentprevThere's a fascinating Radiolab episode about anesthesia that is worth a listen: https://radiolab.org/podcast/anesthesia reply autoexec 9 hours agorootparentOnly if you can stand the format. Even when (maybe especially when) they cover a topic that I'm interested in, I find it infuriating to listen to. It's filled with endless repetition and rephrasing of the same thing over and over again. For example here's a snippet from a transcript: JENN: ... to see if he can find anything in the interstitium that's happening that could explain this. And he finds ... QIUSHENG CHEN: Telocytes. INTERPRETER: Telocytes? QIUSHENG CHEN: Telocytes. INTERPRETER: Telocytes. JENN: ... these cells ... JENN: Oh, telocytes. QIUSHENG CHEN: Yeah. JENN: Telocytes, yes. JENN: ... called telocytes, which are a newly-discovered cell ... The whole thing is like that, where they take something that could have been a single sentence and stretch it out over 3, 5, even 10 minutes of repetition and unnecessary detail and throughout all of it they randomly insert stock audio and sound effects of things that don't matter to the content at all. Like someone will say they went into work and the audio abruptly cuts to 15-30 seconds of nothing but the ambient sounds of an office environment. The transcript might be easier for some to tolerate. The anesthesia episode doesn't seem to be too bad in terms of the number of sentence fragments, repetitions, and pointless interjections as some episodes. Even as transcripts some are extremely frustrating to extract information from. https://radiolab.org/podcast/anesthesia/transcript reply zetsurin 5 hours agorootparentMy understanding is that regular human speech is generally this way. reply autoexec 4 hours agorootparentI'd expect a lot of interjections (yeahs, okays, ums, rights, etc) in normal conversation, but people don't keep repeating back everything that's said for the \"benefit\" of an audience. Not in my experience anyway, although I suppose there are probably some people who do... maybe as a vocal tic or something. You'd get none of the pauses for sound effects or music in normal speech either. The podcast comes off as being very \"produced\" as opposed to having a natural conversational tone, and some people love that aspect, but the low information density is what gets to me the most. I mean, here's an except from another transcript and even ignoring that they're using a ton of sound clips to explain something simple, the amount of repetition would be insane in a normal conversation: (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: Grief often comes in five stages. RACHAEL CUSICK: I'm not sure when or how exactly I came across it, but... (SOUNDBITE OF TV SHOW, \"SCRUBS\") DAVE FOLEY: (As Lester Hedrick) You're going to go through what we call the five stages of grief. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: Five stages of grief. RACHAEL CUSICK: It was this five-part checklist. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: There are five stages of grief. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: What are you talking about? RACHAEL CUSICK: You might have heard of these stages. The idea is pretty simple. It's basically that in the wake of losing a loved one, you'll go through a series of feelings. (SOUNDBITE OF MUSIC) RACHAEL CUSICK: First... (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: Stage one. (SOUNDBITE OF TV SHOW, \"SCRUBS\") DAVE FOLEY: (As Lester Hedrick) Denial. RACHAEL CUSICK: Denial. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: Denial. RACHAEL CUSICK: Then stage two. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: Step two, that's anger. (SOUNDBITE OF TV SHOW, \"SCRUBS\") DAVE FOLEY: (As Lester Hedrick) Anger. RACHAEL CUSICK: Then bargaining. (SOUNDBITE OF TV SHOW, \"SCRUBS\") DAVE FOLEY: (As Lester Hedrick) Bargaining. (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: OK. RACHAEL CUSICK: After that is... (SOUNDBITE OF ARCHIVED RECORDING) UNIDENTIFIED PERSON: Depression. RACHAEL CUSICK: ...Depression and... (SOUNDBITE OF TV SHOW, \"SCRUBS\") DAVE FOLEY: (As Lester Hedrick) Finally, acceptance. RACHAEL CUSICK: ...Last but not least, acceptance. reply thfuran 13 hours agoparentprevHow do you determine whether a plant or bacterium is anesthetized? reply reaperman 13 hours agorootparentI don't think they measure if plants/bacterium are anesthetized, but in exposing them to things that cause anesthesia in humans, other potentially unrelated effects are noticed. Many compounds/mechanisms, especially hormones and neurotransmitters, are widely \"re-used\" across different biologies for completely different things. They're essentially generic semaphores, and the action caused by raising the semaphore can be basically anything. There's a lot of variance in effect even among different instances of human species, often quite unpredictable, contradictory, and profound. It's sort of like \"hey we already have this testosterone thing, it's currently used to call [function A] but we could refactor that to use it to initiate [function B] instead\" (testosterone causes growth in many mammals but inhibits growth in lizards, so female lizards are larger than male lizards) or \"hey we already have the genes to make serotonin for gastrointestinal regulation, but it's not used for anything in the brain. The blood-brain-barrier already prevents somatic serotonin from reaching the brain so we could have a completely different function for it in the brain and regulate gut and brain serotonin in isolation of eachother\" or \"hey we have this cholesterol thing that we've been using as a signaling hormone ever since we were on version Plant, maybe we could write a factory that modifies the cholesterol we eat and use it to produce new semaphores like estrogen and testosterone to support a more complex messaging system and handle all the new effects rather than overloading the existing semaphore\". Edit: Probably slightly better to think of them as the coefficients for activation functions, but nothing here is meant to be anywhere remotely close to a direct analogy. Taking any of this literally would be a misreading. reply boston_clone 13 hours agorootparentprevI'm really just reposting one of the first few DDG links after inputting your question, but this article covers a few different plants: https://www.acsh.org/news/2017/12/11/general-anesthesia-work... reply WhatsTheBigIdea 14 hours agoprevWhat a fabulous analysis! The serendipity is remarkably high! I suppose penicillin might be a good addition to the list of powerful compounds discovered by happenstance? Does this mean that innovation is basically a brute force calculation? Humans simply trying permutations until something hits? reply vacuity 14 hours agoparentConsidering how we all seem to be the product of millions of years of hit-or-miss natural selection, it feels almost natural that our advancements have also had a great deal of luck/improbability. reply hinkley 14 hours agorootparentSome homebrewers will wax nostalgic about how the human sense of smell/taste can detect almost all of the ways that fermentation can go wrong and make it toxic. But we also keep pushing back earliest dates for precursors to civilization, I start to wonder if maybe there aren't graveyards of people who couldn't distinguish thymol from ethanol and selected themselves out of human history via acute liver failure. reply kfarr 13 hours agorootparentYeah exactly this is not just random, it’s “guided search” as commented by a sibling reply sudosysgen 13 hours agorootparentprevAnimals do eat fermented food, through fallen/rotting fruits, so perhaps the thymol sensitivity predates hominids. reply thfuran 13 hours agorootparentYeah, the ability to detect toxins likely to appear in the food supply seems evolutionarily important. reply tumultco 7 hours agoparentprevTo give some more context, Fleming's job was an antibacterial researcher, and also molds were also previously known to have these properties. In some ways, the lucky part was others finding his work and developing it. I found this video on the development of Penicillin a pretty interesting watch: https://www.youtube.com/watch?v=mhXmkDapHWg reply BriggyDwiggs42 13 hours agoparentprevIt’s much smarter than random, more like a guided search. reply hinkley 14 hours agoparentprevHow did we ever figure out that aspens and willows have aspirin analogs in their bark? Boredom? Starvation food? Psychedelic mushrooms make sense. You see it, you eat it. Willow bark tea is a whole process. reply throwaway4aday 13 hours agorootparentBack in the day people made tea out of anything they could get their hands on that wasn't outright poisonous. https://en.wikipedia.org/wiki/Herbal_tea#Varieties Also, if you're in constant pain you'll try all kinds of random stuff to make the pain go away. If necessity is the mother of invention then desperation is its father. reply doubled112 13 hours agorootparentprevMakes you wonder about tobacco. Hey, all the bugs that are eating that plant are dying. Let's see what happens if we smoke it. reply neuromanser 13 hours agorootparentMore like: This plant has no insect parasites, it must be special somehow, let's try and use it in different, increasingly \"close approach\" ways. I mean, humans must have figured quite early that on average, inhaling smoke of a poisonous plant has a fraction of the effect of chewing the same. Someone gets really sick after chewing some leaves, their family burns the rest of the \"crop\" and they get high. reply nerdponx 10 hours agorootparentIt seems likely to me that any form of smoking was discovered incidentally by burning stuff that smelled nice, repelled bugs, repelled evil spirits, etc. and happening to inhale some of the smoke. reply ninjaK4t 14 hours agoparentprevI mean history suggests that’s the case. Took centuries for Copernicus to exist. AI has been an idea for decades. It wasn’t until transformers in the last few years we had big gains. Google and giant institutions focus on fiat revenue stability over the long term, in line with political ideology. Few big ideas come out of that. I think what Adam Smith is said to have written applies; division of labor taken to the extreme will result in humans dumber than the lowest animal. We iterated on our current political system over the Boomers lives. Next generations are tired of the threat of brute force from the elders who the kids now see as in no position to back up those threats given their age. They’re abandoning norms of the last 30-40 years, which IMO, is enabled by abandonment of thousands of years of obligation to preserve religion. There are shorter iterative periods too; 15 years ago comic movies went crazy with Iron Man, iPhone blew up; now we’re iterating on AI generated content and spatial headsets. 15 years prior (with some wiggle room for margin of error) “information super highway” was coming. On the shorter scales there seems to a pattern of 3-5 year warmup and 7-10 year plateau, with a cooldown of 2-3 years as the masses lose interest. This aligns with neuroscience experiments that show our brains devalue old patterns after roughly 15 years. Generational churn and lack of generalized sense of obligation to the past (via abandonment of religious buy in by westerners) could free the future to live in cycles that align with scientific measurement versus obligation to be parrots that recite past memes. reply aeneasmackenzie 11 hours agorootparentCopernicus is a bad example. He proposed a heliocentric system based on vibes. Actual progress required decades of cutting-edge precision measurements by Brahe and then analysis by Kepler. Objections to heliocentrism were on scientific grounds which were resolved by the discovery of inertia, Airy disks, and stellar aberration. reply elevatedastalt 14 hours agoprevI think it feels serendipitous only if you think of \"sweet\" taste (or for that matter any taste) as occurring in this giant continuum of tastes where hitting a specific spot on that continuum is probabilistically impossible. Ultimately, we experience tastes thanks to chemical receptors on the tongue, and as long as a substance triggers those receptors we experience a certain taste. Now of course if triggering needs a very very specific combination of atoms, then it is not very probable, but we know especially from drug research that you can use similarities in molecular structure or sometimes in say the electron cloud of the molecule to perform this triggering. Caffeine for eg is a selective adenosine antagonist that fools the body and binds to adenosine receptors. Now of course int the case of caffeine the structures are somewhat similar, but you could imagine similar stuff happening with other molecules too If you think of it as that sort of problem, it's not that surprising that many different types of molecules might achieve the same effect. reply BurningFrog 13 hours agoparent> * it's not that surprising that many different types of molecules might achieve the same effect* But the point of the post is that only five molecules found by dumb luck have had a useful sweetening effect. reply hinkley 14 hours agoparentprevThat artificial sweetener they mention halfway through, the one that gorillas are evolving not to taste, man that amused me when I first heard about it a few years ago. Chalk one up for hominidae. reply pjs_ 8 hours agoparentprevOne interesting thing in this context is that you can taste the difference between water and heavy water, even though they look the same as ball-and-sticks. Also heavy water is said to taste sweet reply perfectritone 14 hours agoprevI'm left curious as to why our taste receptors are so attuned to sweetness if high sugar foods weren't historically correlated with being high in energy. reply lemax 13 hours agoparentPerhaps because the high sugar foods that occur in nature contain nutrients we don't get elsewhere and help us fight disease. These sugars are also naturally packaged in a way that makes them behave quite unlike added sugars, they don't lead to the same insulin spikes or high blood pressure, and consuming fruits like berries alongside more processed, artificially sweetened foods can even reduce the insulin spikes of those foods. https://nutritionfacts.org/blog/what-about-all-the-sugar-in-... reply gpsx 12 hours agoparentprevI have thoughts about that, but IANAN (nutritionalist). Not all energy is the same, as in fructose (in sucrose, which we sense as sweeter) versus glucose (what composes carbs, and also in sucrose), two simple sugars. Glucose goes more directly into the bloodsteam during digestion. Fructose does not go straight to the bloodstream but is processed by the liver. Although it gets into the bloodstream slower, it gets stored by the liver faster, and we use this as an energey resevoir when we are not getting energy directly thorugh digestion. So maybe we crave this somtimes when we need to rebuild our energy stores. Or at least it seems that way for me. I crave sugar/fruit sometimes, particularly after exercising. I don't generally even eat sweets/desserts, so my body apparently isn't fooled into always wanting sugar. I would guess from my experience sugar plays a specific role and I want it at a certain time and not others. Incidently, I get a headache when I eat processed sugar, but not fruit. (However, fruit will also give me a headache if I carmelize it, cooking it at a high temperature for a long time.) Anyway that is one reason I avoid sweetened foods and maybe why I don't get sucked into eating sugar all the time. And people think I am trying to be super healthy... reply gwern 10 hours agoparentprevThe references he gives in that section suggest that it boils down to a convenient way to try to gauge poison by contrasting sugars to bitterer phytochemicals. Plants with sugar may be less nutritious on average, but that's just counting calories/macronutrients, not taking into account poisonous substances. reply koromak 11 hours agoparentprev1) Its still very, very high in energy. We're built to use it. 2) Fruits contain all sorts of good vitamins and minerals along with sugar. Two birds with one stone. 3) I don't know if we are any less \"attuned\" to it than fat or protein. Most people would eat a good steak over a bag of candy. reply Sesse__ 11 hours agoparentprevIf nothing else: It's certainly useful that mother's milk tastes good to babies. Imagine what an evolutionary disadvantage it would be if it tasted bitter. reply hoseja 1 hour agoparentprevIs everybody forgetting about fruit or what. There totally is wild fruit that is very sweet. reply paulpauper 12 hours agoparentprevThey are high in energy. carbs used by muscles for energy first over protein and fat reply imzadi 13 hours agoprevWhat I learned from this is never eat at a pot luck full of chemists. reply cubefox 14 hours agoprevThis poses the opposite question: What might we have missed? Maybe there are some reverse-accidents, some significant random discoveries that, due to their randomness, didn't happen. reply GolfPopper 13 hours agoparentThere was an SF story I read years ago where the core concept was that humanity had somehow \"missed\" an obvious power-source/FTL drive, which meant that the \"solution\" to the Drake Equation is that the galaxy is full of intelligent, star-spanning empires that are effectively stuck at a late-1800s tech level (with starships), because getting to there was easy, and doing all the complex 20th century stuff humanity has done is really hard. Hijinks and hilarity ensues when they reach earth. reply ptx 12 hours agorootparentIs it \"The Road Not Taken\" by Harry Turtledove? The Wikipedia description seems to match. (If so, ChatGPT got it on the second attempt.) reply swyx 7 hours agorootparenthow do you use chatgpt to search for it? i have been searching for a science fiction short story i read ~25 years ago - it was in a short story anthology book, probably not very famous because i've tried to plug it in and got nothing. --- synopsis (since its a beautiful story and i may as well share it in case someone else here has read it and/or can prompt engineer chatgpt better than me): - Humanity reached abundance but also got overpopulated - so the solution was to shard humanity by days. 1/7 of Humanity wakes and works on one day of the week, sleeps in cryopod the other 6 days. - Main character is happily at work on a Monday tightening the screws on the planetary solar panels, when he notices a sleeping woman in another pod (another day, say Saturday idk) and falls in love on sight (i'm not 100% sure if it was physical-only, he may have read more about her or exchanged letters or something but cant fathom in-world reason how that would work) - After a lot of personal cost and effort, he manages to get a once-in-a-lifetime exemption from the government to switch days, and wakes up on Saturday for the first time in his life. - Goes over to see the woman... only to find she's asleep, because she just transferred to Monday because she also fell in love with him. He's barred from switching back because it was a onetime deal. - Goes back to work to his new Saturday job... loosening screws on the planetary solar panels. --- (above MAY be a hallucination of two stories, its been so long that i'm not sure anymore) reply pontifier 4 hours agorootparentRead this a long time ago, then re-read it a couple of years ago. They called the pods \"stoners\" and it made me laugh. Can't remember the name though. It must have been in a collection of short stories. Edit... Found it for you:https://en.m.wikipedia.org/wiki/The_Sliced-Crosswise_Only-On... reply GolfPopper 10 hours agorootparentprevYep, I'm sure that was it. I read it many years ago, but the story description, the reference to matchlocks, and the conclusion all match my memory. reply duckmysick 10 hours agorootparentprevJust curious: what was the first attempt/guess? reply wheybags 13 hours agorootparentprevDo you remember what it was called? reply caffeinated_me 12 hours agorootparentI believe they're referring to the short story of The Road Not Taken, by Turtledove. https://en.wikipedia.org/wiki/The_Road_Not_Taken_(short_stor... PDF of story: https://www.eyeofmidas.com/scifi/Turtledove_RoadNotTaken.pdf reply wheybags 13 minutes agorootparentThanks! reply Imnimo 14 hours agoprevAre compounds that might be seen in a clinical trial drawn from the same distribution as compounds that a lab chemist might accidentally taste? Is it weird that there's no overlap of a compound that is both very sweet and medically active? reply kirse 13 hours agoprevI think this is because all newly-discovered knowledge inherently reflects the miracle of new life. The point at which an \"unknown-unknown\" piece of information is birthed into our awareness and becomes a known fact is always going to be a fascinating and surprising story. The excitement of informational peek-a-boo, the pulling back of the universal curtains on a discovery we never expected - we might be a little older, but the reaction never changes. Entrepreneurs call it their business pivot, chemists call it serendipity. reply Guvante 8 hours agoprevI will note that while 20,000 chemical compounds sounds like a lot I don't know that is true. Aren't there hundreds of compounds in an orange? I do agree with the conclusion though, we likely have tasted (or could have tasted) weaker options over the years. I wonder if the real conclusion is \"we don't understand the human body enough to predict what will work so getting lucky is our only option\". reply Damogran6 12 hours agoprevLoctite Blue may or may not taste sweet. I'm not reproducing the experiment. reply Cthulhu_ 12 hours agoparentDidn't lead taste sweet as well? reply FreeFull 12 hours agorootparentLead(II) acetate specifically is sweet. reply phkahler 11 hours agoprevThe amino acid Glycine is sweet and it's good for you. Not really patentable though. reply directevolve 10 hours agoprevAce-K was developed by rational investigation of sucrose derivatives, undermining it as an example of 'serendipity'. I looked at some of the sources linked on Wikipedia for the other anecdotes. Ace-K: Newton, D. E. (2007). Food Chemistry (New Chemistry). Available on libgen. Also describes the other serendipitous discovery stories, but with no citation or evidence. For Ace-K, there are conflicting accounts of whether the chemist was licking his fingers to pick up a piece of paper or rubbing out a white spot on his sweater or shirt. Sucralose: Eurekas and Euphorias: The Oxford Book of Scientific Anecdotes. Available on libgen. Also describes the same anecdotes for other drugs. The only supporting evidence for any of them is is a direct quote from James Schlatter on the discovery of aspartame: \"I was heating the aspartame in a flask with methanol when the mixture bumped [boiled abruptly] onto the outside of the flask. As a result some of the powder got onto my fingers. At a slightly later stage, when licking my fingers to pick up a piece of paper, I noticed a very strong, sweet taste. Initially, I thought that I must have had some sugar on my hands from earlier in the day. However, I quickly realised this could not be so, since I had washed my hands in the meantime. I therefore traced the powder on my hands back to the container into which I had placed the crystallised aspartylphenylalanine methyl ester. I felt that this dipeptide ester was not likely to be toxic and I therefore tasted a little of it and found that it was the substance which I had previously tasted on my finger.\" The book chalks up the 'test'/'taste' mixup with Ace-K to a language barrier (Phadnis's first name was Shashikant). Processed Foods and the Consumer claims that Schlatter licked his finger to 'provide friction before reaching for a thin piece of laboratory equipment.' The story about the discovery of saccharin is from The 100 Most Important Chemical Compounds: A Reference Guide, which also does not cite a source for the story. An American Chemical Society Molecule of the Week article also reports the \"tasting his hand after dinner\" story for the discovery of saccharin, but notes that \"in another version of the story\" he tasted it on his cigarette, just like in the cyclamate discovery story. Of course, the NY Times obit for Sveda, discoverer of cyclamates, is that \"he brushed his lips without having washed his hands and found that his fingers tasted sweet,\" rather than that he put his cigarette into the chemical. reply ShamelessC 9 hours agoparent> Ace-K was developed by rational investigation of sucrose derivatives, undermining it as an example of 'serendipity'. This sort of criticism would imply that an author on LessWrong _wasn't_ a polyglot genius uniquely capable of seeing what others missed. I don't buy it. /s reply RcouF1uZ4gsC 14 hours agoprev> Look at sucrose and aspartame side by side: > Molecular structures of sucrose and aspartame, looking very different I can’t imagine someone looking at these two molecules and thinking “surely they taste the same”. They don’t taste the same. Aspartame has a very nasty aftertaste as compared to sucrose. reply imglorp 14 hours agoparentYes, to my taste, all the sweeteners have an aftertaste and an odor including stevia. reply cgh 14 hours agorootparentRe stevia, apparently how you taste it is determined by how sensitive your bitter taste receptors happen to be. It’s somehow also related to why some people can’t stand cilantro, for example. For me, stevia is simply sweet with no aftertaste so I guess I’m lucky in this regard. reply wccrawford 14 hours agorootparentStevia? My wife has much stronger bitter receptors than most people (including me, and I thought mine were strong) but Stevia is fine for her. Aspartame, though, she absolutely despises. I think they're both fine, but I can pick up a hint of what she dislikes from Aspartame. Sucralose is also fine for both of us. reply zetsurin 5 hours agorootparentprevOh cool, I find cilantro to taste bad! Stevia less so, but I don't like it. Is Aspartame the same (I also find it to have an aftertaste)? reply bityard 13 hours agorootparentprevYou are! As one who identifies as keto, I have experience with a wide range of artificial sweeteners. Stevia is one of my favorites but I can definitely taste the bitterness. Which is why I only use it either in combination with other sweeteners (e.g. Erythritol), or in places where only a slight amount of sweetness is desired. Edit: I just had a thought: are we talking about putting it in coffee here? Because the bitterness of coffee and the bitterness of Stevia are pretty close and I can see the former masking the latter easily. reply fellowniusmonk 13 hours agorootparentMonk Fruit/Stevia liquid sweeteners (sans erythritol) are my go to for coffee and any slightly bitter food for this very reason. reply cgh 8 hours agorootparentprevAstounding fact: I have never had a cup of coffee in my life. reply nerdponx 10 hours agorootparentprevStevia and monkfruit both taste awful to me. Not bitter specifically, but unpleasant in a way that I can't really describe, and do not at all enjoy eating. It also tends to make my stomach hurt. Meanwhile I have no problem with cilantro or cucumbers. reply johnchristopher 14 hours agoparentprevI wonder if people have different genetic predispositions that influences how sugar taste to them. Like I have with cilantro (a bit, a tiny tiny bit is okayish but more than that and it's.. ugh... it's not even soap it's.. bleh..). reply FergusArgyll 14 hours agoprevI loved the writing but I don't get the point or the title. Am I supposed to infer some sort of conspiracy theory? or is it just supposed to be funny? reply cubefox 14 hours agoparentI think the author suggests that chemical-psychological research is quite inefficient, given that so many discoveries were random discoveries rather than products of systematic search and invention. reply empath-nirvana 12 hours agoparentprevThe point is that chemists are tasting a lot of the novel stuff they synthesize, if it doesn't look like it's going to cause instant cancer, even they they aren't supposed to. Watching NileRed regularly, it does not at all surprise me. reply cmyr 14 hours agoparentprevI don't think there's some huge point beyond exploring an interesting question, and raising it in a forum where it's possible some people with more specific domain knowledge might offer some insight. reply ElevenLathe 14 hours agoparentprevI don't think it's meant to imply a conspiracy, rather that lab safety around novel compounds is probably not as tight as people assume it is. I will put forth another (possible) explanation: fake sugar has to be widely marketed, and \"we were doing Important Science when we accidentally discovered this fake sugar\" is a more marketable, easy-to-understand story than \"we added/removed groups on a range of compounds determined likely to be sweet based on a literature review, in order of how amenable the reactions are to mass production, then tested each of them in rats\". reply eduction 11 hours agoprevI’m sure this is fascinating but sometimes I wish people would put the bottom line at the top of super long posts like this. It would be nice to transmit knowledge even to people who don’t have time to follow along on your multi thousand word voyage. reply stcredzero 13 hours agoprevI can’t imagine someone looking at these two molecules and thinking “surely they taste the same”. I think that's more of a shortcoming of our method of diagramming molecules. It might be more apparent if we had 3D visualizations of the molecules and the receptors. reply ysofunny 14 hours agoprevor you know, consider how many people have gotten into chemistry because they wanna learn how to cook, or simply to cook better. but please don't ask about what exactly they're cooking too much, keep the discussion on the how they are cooking it hahahha reply joneholland 12 hours agoprevThis “I did my own research” pseudoscience has no reason to be on hackernews. reply mcherm 12 hours agoparentReally? Because I think Hacker News often features posts by hackers and amateurs trying to tackle something normally handled by professionals. The comments would be a great place to reply with references to the ACTUAL research on how often chemists taste the chemicals they are working with. reply nottorp 1 hour agorootparentMaybe we should ask Derek Lowe :) reply amelius 14 hours agoprev [–] Unfortunately, the chemists merely tasted, not ingested. Otherwise they would have known that their newly found artificial sweeteners cause nasty bowel issues. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article explores the accidental discoveries of artificial sweeteners and raises questions about the process of creating new ones.",
      "It examines the reasons behind our preference for sweet flavors and suggests that low-calorie sweeteners may not exist naturally.",
      "The discussion also highlights the potential risks of chemists tasting substances and the limitations of drug discovery methods."
    ],
    "commentSummary": [
      "The conversation covers a wide range of scientific topics, such as migraines, triggers, and the role of quantum mechanics in chemistry.",
      "It includes discussions on mathematical concepts like the square root of 2 being an irrational number.",
      "The conversation also touches on topics like compounds and their effects on organisms, scientific advancements, taste and evolution, and accidental discoveries of sweeteners, including a fake sugar."
    ],
    "points": 199,
    "commentCount": 93,
    "retryCount": 0,
    "time": 1705944384
  },
  {
    "id": 39098603,
    "title": "Nobel Physicist Arno A. Penzias, 90, Confirmed Big Bang Theory",
    "originLink": "https://www.nytimes.com/2024/01/22/science/space/arno-a-penzias-dead.html",
    "originBody": "ADVERTISEMENT SKIP ADVERTISEMENT Arno A. Penzias, 90, Dies; Nobel Physicist Confirmed Big Bang Theory His 1964 discovery with Robert W. Wilson settled a debate over the origin and evolution of the universe. Share full article Dr. Arno A. Penzias in a 1991 photo at Bell Laboratories in New Jersey. He and Dr. Robert W. Wilson were researchers there in 1964 when they discovered cosmic microwave background radiation, remnants of the Big Bang. Credit... Frank C. Dougherty By Katie Hafner Jan. 22, 2024 Arno A. Penzias, whose astronomical probes yielded incontrovertible evidence of a dynamic, evolving universe with a clear point of origin, confirming what became known as the Big Bang theory, died on Monday in San Francisco. He was 90. His death, in an assisted living facility, was caused by complications of Alzheimer’s disease, his son, David, said. Dr. Penzias (pronounced PEN-zee-as) shared one-half of the 1978 Nobel Prize in Physics with Robert Woodrow Wilson for their discovery in 1964 of cosmic microwave background radiation, remnants of an explosion that gave birth to the universe some 14 billion years ago. That explosion, known as the Big Bang, is now the widely accepted explanation for the origin and evolution of the universe. (A third physicist, Pyotr Kapitsa of Russia, received the other half of the prize, for unrelated advances in developing liquid helium.) Until Dr. Penzias and Dr. Wilson published their observations, the Big Bang theory competed with the steady-state theory, which envisioned a more static, timeless expanse growing into infinite space, with new matter formed to fill the gaps. Dr. Penzias and Dr. Wilson’s discovery finally settled the debate. Yet it was the serendipitous product of a different investigation altogether. In 1961, Dr. Penzias joined AT&T’s Bell Laboratories in Holmdel, N.J., with the intention of using a radio antenna, which was being developed for satellite communications, as a radio telescope to make cosmological measurements. “The first thing I thought of was — study the galaxy in a way that no one else had been able to do,” he said in a 2004 interview with the Nobel Foundation. In 1964, while preparing the antenna to measure the properties of the Milky Way galaxy, Dr. Penzias and Dr. Wilson, another young radio astronomer who was new to Bell Labs, encountered a persistent, unexplained hiss of radio waves that seemed to come from everywhere in the sky, detected no matter which way the antenna was pointed. Perplexed, they considered various sources of the noise. They thought they might be picking up radar, or noise from New York City, or radiation from a nuclear explosion. Or might pigeon droppings be the culprit? Examining the antenna, Dr. Penzias and Dr. Wilson “subjected its electric circuits to scrutiny comparable to that used in preparing a manned spacecraft,” Walter Sullivan wrote in The New York Times in 1965. Yet the mysterious hiss remained. The cosmological underpinnings of the noise were finally explained with help from physicists at Princeton University, who had predicted that there might be radiation coming from all directions left over from the Big Bang. The buzzing, it turned out, was just that: a cosmic echo. It confirmed that the universe wasn’t infinitely old and static but rather had begun as a primordial fireball that left the universe bathed in background radiation. The discovery, Dr. Penzias said years later, intensified his interest in astronomy. He and Dr. Wilson went on to detect dozens of types of molecules in interstellar clouds where new stars are formed. “Their discovery marked a transition between a period in which cosmology was more philosophical, with very few observations, and a golden age of observational cosmology,” Paul Halpern, a physicist at St. Joseph’s University in Philadelphia and the author of “Flashes of Creation: George Gamow, Fred Hoyle, and the Great Big Bang Debate,” said in a phone interview. Image Dr. Wilson, left, and Dr. Penzias standing beside the Holmdel Horn in 1978 after being a share of the Nobel Prize in Physics. Credit... Associated Press The discovery not only helped cement the cosmos’s grand narrative; it also opened a window through which to investigate the nature of reality — all as a result of that vexing hiss first heard 60 years ago by a couple of junior physicists looking for something else. Arno Allan Penzias was born on April 26, 1933, in Munich to Jewish parents, Karl and Justine (Eisenreich) Penzias. Dr. Penzias would later point out, to just about anyone he met, that his birth coincided to the day and place with the establishment of the Gestapo, the German secret police. His father was a leather wholesaler; his mother, who managed the home, had converted to Judaism from Roman Catholicism in 1932. In the fall of 1938, the Penzias family was arrested and put on a train for deportation to Poland. “Fortunately for us, the Poles stopped accepting Jews just before our train reached the border,” Dr. Penzias said in a eulogy at his mother’s funeral in 1991. The train returned to Munich. In late spring 1939, 6-year-old Arno and his brother, Gunter, 5, were put on a train as part of the Kindertransport, the British rescue effort that brought some 10,000 children to England. His mother instructed Arno to take care of his brother. “I only realized much later that she didn’t know if she would ever see either one of us again,” he said in his eulogy. Gunter Penzias recalled over the phone: “Each of us was given a large box of chocolates. I fell asleep on the train, and mine was stolen. So Arno shared his with me.” The boys’ parents managed to leave Germany for England, and the family arrived in New York City in 1940. Karl and Justine found work as superintendents in a series of apartment buildings in the Bronx, giving the family places to live. Dr. Penzias attended Brooklyn Technical High School and “sort of drifted into chemistry,” he told The New Yorker in 1984. He entered the City College of New York in 1951 intending to study chemistry, but he found that he had already learned much of the material. After one of his professors assured him that he could make a living as a physicist, he switched majors, graduating in 1954. That year, he married Anne Barras, a student at Hunter College. They divorced in 1995. After two years as a radar officer in the Army Signal Corps, he entered graduate school at Columbia University, where he earned both his master’s and doctoral degrees in physics, the latter in 1962. But Dr. Penzias’s path to stumbling onto the answer to one of humanity’s most central questions started a year earlier, when he joined Bell Laboratories as a member of its radio research group in Holmdel. There, he saw the potential of AT&T’s new satellite communications antenna, a giant radio telescope known as the Holmdel Horn, as a tool for cosmological observation. In teaming up with Dr. Wilson in 1964 to use the antenna, Dr. Wilson said in a recent interview, one of their goals was to advance the nascent field of radio astronomy by accurately measuring several bright celestial sources. Soon after they started their measurements, however, they heard the hiss. They spent months ruling out possible causes, including pigeons. “The pigeons would go and roost at the small end of the horn, and they deposited what Arno called a white dielectric material,” Dr. Wilson said. “And we didn’t know if the pigeon poop might have produced some radiation.” So the men climbed up and cleaned it out. The noise persisted. It was finally Dr. Penzias’s fondness for chatting on the telephone that led to a fortuitous breakthrough. (“It was a good thing he worked for the phone company, because he liked to use their instrument,” Dr. Wilson said. “He talked to a lot of people.”) In January 1965, Dr. Penzias dialed Bernard Burke, a fellow radio astronomer, and in the course of their conversation he mentioned the puzzling hiss. Dr. Burke suggested that Dr. Penzias call a physicist at Princeton who had been trying to prove that the Big Bang had left traces of cosmological radiation. He did. Intrigued, scientists from Princeton visited Dr. Penzias and Dr. Wilson, and together they made the connection to the Big Bang. Theory and observation were then brought together in a pair of papers published in 1965. Image Dr. Penzias receiving the Nobel Prize in Physics in Stockholm from King Carl XVI Gustaf of Sweden. Credit... Associated Press Dr. Penzias stayed at Bell Labs for nearly four decades, with 14 years as vice president of research. His interests reached well beyond science, into business, art, technology and politics. After his 1978 Nobel Prize acceptance speech in Stockholm, he flew directly to Moscow to give a lecture about his findings to a group of refusenik scientists. He later helped several of them leave the Soviet Union. In 1992, Dr. Penzias arranged for the donation of the Holmdel Horn’s receiver and calibration equipment to the Deutsches Museum in Munich, where it remains as part of a permanent exhibition. “It was very important to my father to remind them what they lost,” his daughter, Rabbi L. Shifra Weiss-Penzias, said in an interview. “He wanted his work to be a living reminder of the refugees who left and the people who died.” Dr. Penzias married Sherry Levit, a Silicon Valley executive, in 1996. In addition to his daughter; his son, David; and his brother, Gunter, Dr. Penzias is survived by his wife; another daughter, Mindy Dirks; a stepson, Carson Levit; a stepdaughter, Victoria Zaroff; 12 grandchildren; and three great-grandchildren. Soon after the announcement of the Nobel Prize, President Jimmy Carter sent a congratulatory telegram to Dr. Penzias. He replied, “I came to the United States 39 years ago as a penniless refugee from Nazi Germany,” adding that for him and his family, “America has meant a haven of safety as well as a land of freedom and opportunity.” Katie Hafner, a former staff reporter for The New York Times, is a co-author of \"Where Wizards Stay Up Late: The Origins of The Internet.\" More about Katie Hafner Share full article ADVERTISEMENT SKIP ADVERTISEMENT",
    "commentLink": "https://news.ycombinator.com/item?id=39098603",
    "commentBody": "Arno A. Penzias, 90, Dies; Nobel Physicist Confirmed Big Bang Theory (nytimes.com)181 points by gumby 7 hours agohidepastfavorite50 comments nevster 6 hours agohttps://archive.is/8Ekg6 vijayr02 3 hours agoprevRIP Arno. Here's the video and write up of the gag Dennis Ritchie and Rob Pike pulled on him: https://www.youtube.com/watch?v=fxMKuv0A6z4 https://www.bell-labs.com/usr/dmr/www/labscam.html Edit: just wanted to say the hum of the universe dulled a little bit with his passing reply gumby 7 hours agoprevI worked with Arno about a decade ago and he was still sharp as a tack. reply koblas 49 minutes agoparentI have some good memories and little stories from my brief time working with him. Great guy. reply jacquesm 6 hours agoparentprevWhat did you collaborate on? reply gumby 4 hours agorootparentNot what you might expect: we were building small distributed solar thermal plants that stored energy in a tank of supercritical water (basically: inject steam into tank of water -> it condenses; open a valve on the top and it flashes out as steam). Since the pressure varies depending on solar flux and how much was in the tank we couldn't use a turbine (the shape of the turbine is tuned to a constant flow) so instead used multi-stage steam engines (the first new steam engine designed probably in 75 years. These were ordinary container-sized pressure vessels and the steam engines were built into a container too. Easy to repair; you could make your own parts, and last forever. Cheap PV and batteries killed it. Arno was full of good advice. reply elcomet 1 hour agorootparentWhat's PV? reply phreeza 1 hour agorootparentPhotovoltaics, aka solar cells. reply vmurthy 7 hours agoprevNot to take away anything from Mr Penzias but here’s how I remember his name :-) “Although Penzias and Wilson had not been looking for cosmic background radiation, didn’t know what it was when they had found it, and hadn’t described or interpreted its character in any paper, they received the 1978 Nobel Prize in physics. The Princeton researchers got only sympathy. According to Dennis Overbye in Lonely Hearts of the Cosmos, neither Penzias nor Wilson altogether understood the significance of what they had found until they read about it in the New York Times” - Bill Bryson, A Short History of Nearly Everything RIP Mr Penzias reply OldGuyInTheClub 7 hours agoparentWithout aspersion on Penzias and Wilson, Robert Dicke should have been recognized in due time. He was a supremely gifted experimentalist on top of everything else. Credited with inventing the lock-in amplifier, he graciously deferred to others. His radiometer and switch also had great impact. Stockholm should hang its head for this oversight. reply blawson 7 hours agoparentprevOT but this book is so incredible. The breadth of information covered is amazing, even for being ~20 years old at this point. The audiobook at .7 speed is my sleep aid. reply m463 1 hour agorootparent> The audiobook at .7 speed is my sleep aid. I love little hacks like this. reply sizzzzlerz 4 hours agoparentprevHave any other Nobel Prizes been awarded to someone who only discovered some phenomena but didn’t do any further research to advance our understanding of it? It seems to me that that falls in the category of blind luck rather than research and shouldn’t qualify for the Prize. reply sidkshatriya 3 hours agorootparentThe nobel prize is granted for discoveries that advance human knowledge. Whether that discovery is through chance, sheer hardwork or some linear combination, what matters is the discovery. It is not not disputed (by anyone) that the discovery was important here. The theory of the Cosmic microwave background was already out there -- it just needed someone to confirm the theory ! One could argue that the microwave background radiation was discovered precisely by the nobel winning experimenters because they refused to explain away the \"noise\". They were persistent and kept trying to eliminate possible sources of this noise. Perhaps other similar horn antennae had the same noise but this noise was just explained away by the other experimentalist as being an artifact of the equipment ? One may never know. Lastly, it has been my experience that luck seems to strike competent and fastidious researchers more often than average ones. reply ioblomov 1 hour agorootparentHere, here. We take it for granted now, but even such things as HBO’s white-noise intro, the echoes of creation, is a cultural artifact of their tenacious attention to detail. reply evanb 3 hours agorootparentprevMany. Just to give an example, the very first Nobel in physics was awarded to Röntgen, who discovered 'A new kind of Ray' (which only later named x-rays and were shown to be photons with a different wavelength). He noticed a detector going off even though his device was in a sealed box. reply neuromanser 2 hours agorootparentBtw X-rays are called \"rentgenové záření\" (\"Roentgen radiation\") in Czech. reply bbojan 1 hour agorootparentI believe in German-speaking countries the X-ray machine is called Röntgen? E.g. in a hospital you will have directions for \"Röntgen\". reply thrdbndndn 1 hour agorootparentprevIn lots of languages, both are used (thought X-rays becomes more and more common). reply toomuchtodo 6 hours agoprevhttps://en.wikipedia.org/wiki/Arno_Allan_Penzias reply OldGuyInTheClub 7 hours agoprevHe was the VP of research at Bell Labs during my postdoc 30+ years ago. Imposing in stature both as a scientist and as a person. RIP. reply _kst_ 4 hours agoprevhttps://www.youtube.com/watch?v=5rVBu6i6Qwk reply abrarsami 2 hours agoprevRIP Arno reply elashri 4 hours agoprevThe title could be about Cosmic Microwave Background radiation instead of talking about Big Bang Theory. Yes, he and Wilson discovered it. reply dang 3 hours agoprev [26 more] [stub for offtopicness] reply TheRoque 6 hours agoparent\"confirms theory\" I know it's valid, but it sounds weird reply mocha_nate 4 hours agorootparentI agree. Makes me think someone over-spiced the headline reply flykespice 5 hours agoparentprevWhat does it means to confirm a theory? reply cryptonector 4 hours agorootparentIt's unscientific nonsense. reply mr_mitm 2 hours agorootparentWhat are you all on about? This phrase is used all the time in science. reply ken47 4 hours agoparentprevThe semantics of the title seem off and written by a non-scientist. A scientific theory cannot be proven, so the significant word choice of “confirm” is poor. reply Gimpei 3 hours agorootparentWhy not? Are you making some oblique reference to Popperian refutationalism or is your point just that it’s very difficult to 100% prove something, so often it’s more a matter of a significant updating of priors? If it’s the latter, using “proof” doesn’t seem like the worst shorthand to me. Anything more involved would go over the head of the average nytimes reader. reply somenameforme 2 hours agorootparentEvidence of [x] is not proof of [x], but in this specific case the claim is particularly unreasonable. Because while a cosmic microwave background radiation is something that would be expected from a Big Bang, its actual reality is weird and contradicted previous expectations. It has to do with casual connectivity. \"A\" can only possibly influence \"B\" if A could reach B at the speed of light. But the relative homogeneity of our universe is suggestive that parts of the universe which should not be casually connected, are causally connected. So this discovery quickly led to the invention of cosmic inflation [1] whereby the early expansion of the universe is said to have dramatically accelerated well beyond the speed of light, and then slowed down - in order to enable these regions of space to become causally connected. No possible means or mechanism have been suggested, so it remains nothing but a rather inelegant hack to try to make what we observe fit a preexisting hypothesis that it largely contradicted. As experiments to try to provide supporting evidence for inflation have also turned up negative, it's also becoming one of those contemporary model driven hypotheses that requires ever more exotic physics to even make it possible, as each failed prediction gets assimilated into the model to make it keep fitting what we see. Just add more epicycles. [2] [1] - https://en.wikipedia.org/wiki/Inflation_(cosmology)#Motivati... [2] - https://en.wikipedia.org/wiki/Ad_hoc_hypothesis reply ken47 2 hours agorootparentprevI assumed the average New York Times reader is above average in intellect and could appreciate an irrefutable word choice, like “advanced” in place of “confirmed.” I could be wrong. reply mr_mitm 2 hours agorootparentprev> A scientific theory cannot be proven, Yes, that's why the title doesn't say \"Physicist proves Big Bang theory\" reply Jensson 4 hours agorootparentprev> written by a non-scientist Yes, journalists aren't scientists. reply ken47 3 hours agorootparentShame that instead of highlighting one of the many great things that this man did, they use incorrect phraseology to claim he did something he didn’t do. reply eisvogel 5 hours agoparentprevTired light forever!!! reply passedandfuture 4 hours ago [flagged]parentprev [–] It's clear by the title an athiest wrote this. reply dang 3 hours agorootparentPlease don't take HN threads on generic tangents, definitely not flamewar tangents, and especially definitely not religious flamewar tangents. That's the last thing we need here. https://news.ycombinator.com/newsguidelines.html reply Jensson 4 hours agorootparentprev [–] Why couldn't god have created the big bang? Science has nothing to do with religion. reply ken47 4 hours agorootparentA scientific theory cannot really be \"confirmed.\" The word choice is suspect. The Big Bang Theory is consistent with many properties of the known universe, but cannot adequately explain others. Claiming it is the ground truth takes a leap of faith, which however small compared to religion, requires a measure of faith as implied. reply Jensson 4 hours agorootparentWhat does that have to do with atheism? You don't think that people who believe in God sometimes makes mistakes when picking words? reply ken47 3 hours agorootparentIt begs the question as to whether the author of the title actually believes the Big Bang is confirmed from a scientific point of view. That would suggest an atheistic belief. reply pvg 3 hours agorootparentYou're conflating confirmation with some type of irrefutable proof which is both pedantic and plain wrong. reply ken47 2 hours agorootparentThe job of people who write titles like this is to be very pedantic about word choice. I’ll have to disagree with you on this being a “plain wrong” interpretation of the given wording. reply pvg 1 hour agorootparentIt's not the job of the people who write titles to be pedantic although it is reasonable of them to expect most readers to know the difference between confirmation and absolute proof. The problem here is not with the title. reply ken47 1 hour agorootparentIf they were targeting an adequately scientifically trained audience, sure. I don’t think they are. reply pvg 1 hour agorootparentThat doesn't have much to do with it, you're just mistaken about the (fairly common) usage 'confirms theory'. reply misja111 1 hour agorootparentprev [–] The Bible states that God created light, stars and earth in seven days; on day 3 He created the earth, on day 4 the stars. That seems rather incompatible with the big bang theory. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Arno A. Penzias, a Nobel Prize-winning physicist, has passed away at the age of 90.",
      "He confirmed the Big Bang theory with his discovery of cosmic microwave background radiation in 1964.",
      "Penzias' unexpected discovery marked a pivotal moment in cosmology, transitioning it from a philosophical field to one driven by observational research."
    ],
    "commentSummary": [
      "Arno A. Penzias, a Nobel Prize-winning physicist, has passed away at the age of 90.",
      "Penzias and his colleague Robert Wilson discovered the cosmic microwave background radiation, supporting the Big Bang theory.",
      "Their significant findings were initially not fully recognized, but they were awarded the Nobel Prize in physics in 1978 for their contribution to cosmology."
    ],
    "points": 181,
    "commentCount": 50,
    "retryCount": 0,
    "time": 1705975546
  },
  {
    "id": 39091416,
    "title": "Automating Cloud Cost Optimization Saves $150k/year",
    "originLink": "https://tuananh.net/2024/01/21/cloud-cost-optimization-at-scale-part-1/",
    "originBody": "Cloud cost optimization at scale part 1 Posted on January 21, 2024 • 4 minutes • 788 words The best optimization is simply shutting things off You work from 8am to 8pm. That’s 12 hours. What happened to the rest of the day? You go home. So you shut things down. That’s 50% cost saving right there. Simple enough right? Now add the following constraints: Imagine if you have an AWS organization with hundreds of workload accounts. I’ve seen org with over 700 accounts. Imagine if you want to support resources other than EC2. Now add Kubernetes to the mix. Now your EC2 VM is being controlled by autoscaling group/cluster autoscaler or karpenter. You can’t simply shut those off. They will be spin up right back. Your team is small. You cant have them support all the teams within organization. You need a solution that can be dynamic (to support multiple resources, different requirements from different teams,etc..) but also easy enough for other team to operate it themselves. Cloud Automation Platform I was thinking a bigger picture. I want a simple way to automate the whole cloud platform: Automate shutting down resources during off-work hours. Discover certain event, correcting things Etc… I like Lambda, Step Functions and EventBridge. Let’s build something out of those. Here’s what I had in mind initially We changed the design a little bit by instead of assuming role, we just publish an event to the spoke account eventbus. We didn’t want the automation executor to have abilties to assume role everywhere in the org. With this system in placed, we can do lots of stuff like Whenever an EKS cluster is created within our org, we want to inject some security baseline policies in. Whenever a Cloudwatch logs group created, we check for certain tag and if one presented, we ship it to a centralized account. Or we can do stuff like auto-shutdown resources like we want initially at the beginning of the post. The deployment Now for this to work, we need to have: (1) a consistent way of deploying stuff into spoke accounts. (2) We also need a way to protect those resources (this one is actually easy, we can use tag.) As for (1), we are currently using AWS AFT . They have a mechanism for us to define baseline. Baseline is whatever resources you want to deploy to every accounts within an Organization Unit. Sounds exactly like what we need right. This kind of “baseline” is kinda like our agents, embeded in every spoke account. The rest of this is just (1) agent discover stuff, send event to controlplane. (2) controlplane send command back to spoke account to act accordingly. We built all these with Lambda, Step Functions and EventBridge. We love EventBridge pattern matching so much we created a small, tiny Python module to be able to test pattern matching locally . Back to the cloud cost optimization problem Now that we have a generic mechanism to automate stuff in our org. Let’s do some $$ saving. For this specific case, we don’t need our embedded agent to send us anything. We can simply send the “shutdown” command from controlplane. We have a little database for config customization if needed. This is a small Git repo where owners/operators of those spoke accounts can customize their schedule according to their need. What they can config is a pattern (EventBridge pattern style) along with what kind resouces to apply and the action accordingly. We also populate some more properties into the pattern payload like is_holiday: bool. This one is pre-evaluated with our national public holiday. Users can then use this in their pattern matching. Eg. if holiday, do not turn them on at 8am. day_of_week: Mon, Tue, etc… Users can then use this field to add a rule to turn things on at 8am from Mon to Fri. and more … Once the agent receives the payload, they will act accordingly. For examples, EC2 will be send to a Lambda function that handle EC2 and RDS will be sent to another Lambda function that handle RDS. It’s quite easy to add another resouce to the supported list by simply adding more Lambda function handler. We make this feature an opt-out feature on our non-production OU (Organization Unit). If they want to turn this off for certain resources, they will have to tag those resources accordingly. The result Our compute (EC2, RDS, etc…) non-production costs reduce by over 70% during off-work hours. We projected to save approx ~150k USD during 1 year (assume that our cost will not grow). It’s a small piece of our cost but we are going to There are certain workload that are a bit more complicated like Kubernetes I mentioned earlier. Those will be solved next in part 2 of this series. Stay tuned!",
    "commentLink": "https://news.ycombinator.com/item?id=39091416",
    "commentBody": "Cutting down AWS cost by $150k per year simply by shutting things off (tuananh.net)169 points by tuananh 17 hours agohidepastfavorite186 comments darth_avocado 17 hours agoIt is unfortunate that cost management isn’t something most engineers keep an eye out for on a regular basis. Spinning up unnecessary resources, not cleaning up resources properly once not needed, writing inefficient code, etc. all quickly adds up to hundreds of thousands of dollars per month in big companies. I once found a “test” db cluster from an engineer who hadn’t worked in the company for 3 years. We were paying 300k yearly for it before discounts. It took me a literal click to shut it down. And I’m not proud of it but, had to send out an org wide email on the savings achieved (corporate politics :shrug:). reply pjc50 16 hours agoparentThe huge achievement of Amazon was designing a system and selling it to people where developers no longer had to pre-approve spending. Previously developers were hamstrung by purchase order requirements; it could take weeks to authorize a single computer. Now the pendulum has swung in the direction. Developers can spend unlimited amounts of company money without realizing, billed in arrears. And in many cases this is a huge net win! After all, there's another way to waste company money invisibly: design a process which requires meetings and waiting while work is held up. reply amichal 16 hours agorootparentOverall good points but don't forget that pre-approval processes resulted in asking for resources that exceeded the near term needs and once approved ongoing costs were rarely fully reviewed. I have personal experience with \"enterprise\" clients making a huge months long process to get server resources, reminding us that changes would take 30+ days. when the project was over and we did everything we could to let them know that the servers could be spun down or put to other uses we got back a \"ok thanks!\" only to find them still running our project code YEARS later. This is infra that was costing them about 1 engineer FTE per year, not even a 10$/mo toy env reply throwup238 16 hours agorootparentprevWe should be getting kickbacks from Amazon for all the work we’ve done for their bottom line. reply marvin 16 hours agorootparentWe kinda do; Amazon puts upwards pressure on US engineering salaries. reply LunaSea 14 hours agorootparentWhat? No? They are behind all major tech companies and notoriously one of the worst employers too. reply teaearlgraycold 15 hours agorootparentprevI assume all of the unused resources end up subsidizing the rest of them. reply mytailorisrich 15 hours agorootparentprevYes but it is up to a company to control its spending. It must have a process and policy in place to deal with this. It's not Amazon's fault if it hasn't. reply pyeri 16 hours agorootparentprevThat's the whole ploy with Agile, isn't it? In the classical SDLC or Waterfall paradigm, everything was pre-approved and signed off, not just the cost or billing but even the software design itself. Any change in the process and the designers had to raise a change request. Agile changed all of that and now we know how bad things can get with that. reply pjc50 34 minutes agorootparentBut see https://news.ycombinator.com/item?id=39092563 ; the cost of pre-approval can be pretty large, as is the cost of change once you discover that you've frozen in the wrong thing. Agile benefits both consulting situations where the client genuinely doesn't understand their own needs, and also startups where you're building a new product and need to rapidly iterate in response to market feedback. (Escalating costs of pre-approval, and the need to design around every possible objection, are a big part of why physical infrastructure costs so much more in the West!) reply tbalsam 16 hours agorootparentprevNo, Agile is about tight development loops. When weaponized/used by large corporations it often times turns into a Stay-Puft man of sorts, but as someone who _hates_ processes normally, I actually kinda like it and Kanban when done well. It basically helps keep things clean with decomposition and doesn't necessarily hamstring older devs as much while giving a good guideline for younger devs to work in. All things considered, it seems like not a bad system to me, and the team customizing the process to their own needs is nice as well. There's a million ways for it to go wrong, but it's not too terrible on the whole I thinks.who _hates_ processes normally, There is no such thing as \"no process\"; is something you always have whether you talk about it or not. The often heard \"I hate process\" is counterfactual then - what it really means is \"I hate process that I see as intrusive/wasteful/whatever\". The +'ves you are listing are what comes from looking at how things are actually done and doing some of it a bit more thoughtfully. The common -ves often come from stakeholders outside of developement injecting their needs ... sometimes this is unavoidable (e.g. regulatory) sometimes it is just political, but either way there are better and worse ways to do it. reply happymellon 16 hours agoparentprevI found that the problem happens mostly when companies 1. Don't ask developers how much something costs, engineers love optimisation, getting as much as possible out of a system for cheap is great fun. 2. Lock down the UI, so devs can't even find out how much things cost. That's my current situation. Why block the billing dashboard, then expose it through billing dashboard tools that are not really any better, and in many ways worse? It's rhetorical really as I know why. Terrible architecture from \"enterprise\". Stick everything in a single account so it's hard to figure out how much is your spend. All 3000 databases, and make sure your k8s cluster is 5 8XL boxes so no one can scale down excess capacity. Classic I Burn Money consultancy! reply tuananh 16 hours agorootparent> 2. Lock down the UI, so devs can't even find out how much things cost. That's my current situation. Why block the billing dashboard, then expose it through billing dashboard tools that are not really any better, and in many ways worse? This is so true. billing transparency is very important. in the past, i had a case like this: dev accidentally enable backup policy for test database with no retention. finops think that db backup is important and ignore it. dev has no access to billing and have no idea what's creeping up the bill reply macNchz 16 hours agorootparentprev> getting as much as possible out of a system for cheap is great fun In certain circumstances, absolutely, however it's extremely aggravating to be in the position of being constantly pestered to ship features faster without the authority to overprovision some of the infrastructure the software runs on. Waking up in the middle of the night because we saved money by allocating too little disk for the primary database or because the latest release included new dependencies that increased memory usage and the OOMKiller is picking off web servers like a wolf in the lamb's pen, or we're just swapping our way to hell while web requests 502...eh. Not for me. More visibility into costs, though, absolutely agreed. Engineers should know that when they turn on some new cool serverless gizmo and then forget about it, it's costing $ each month. reply happymellon 15 hours agorootparentCompletely agree. I didn't mean that engineers love having no control over their systems, I just see the labours of love that get posted here about getting nginx to throw out 1000 pages a second on an Atari 800, or getting LLMs designed for $2000 GPUs running on a phone. The question should be, we currently cost $X a month and we need to half it because [reason], what can we do to bring it down? Which might be reducing hardware, or maybe something else, might be both. Puzzles can be fun. reply darth_avocado 16 hours agorootparentprevLocking down the UI is definitely a problem. Another problem I have seen is, not being able to accurately tell even if you have the UI. reply happymellon 16 hours agorootparentWe had a battle to get AWS console access in the first place, after that I had to deal with: > You need to request access every 60 days. Luckily it's now added to my permanent role, but even then no billing access? FFS. reply malfist 16 hours agorootparentprevLots of that is just bad design by going the easiest route. It's easy not to grant engineers access to the billing dashboard. It's easy to put everything in the same aws account. Inside Amazon, we're supposed to set up new aws accounts for every service and realm, so we know how much X service's beta environment is costing reply happymellon 15 hours agorootparentIndeed, you need to share resources? Plenty of ways of doing that, like making a cross account shared VPC for example. Everything is still accountable. reply lnxg33k1 16 hours agoparentprevI get paid the same whether or not I spend time going around to save company cost, it’s not like they’re going to share their savings, then shareholders get their juice, management gets their juice and Im the clown who went out of my way for them, who cares I do software not cost management reply pixl97 16 hours agorootparentWhich is why your slice of the organization gets a cloud budget. Don't keep your budget under control, well no bonuses for the employees. >Show me the incentive and I will show you the outcome reply wizerdrobe 16 hours agorootparentJokes on corporate, I don’t have ISO, RSU, or a bonus and my raises are always below inflation. reply lnxg33k1 16 hours agorootparentOh for the inflation thats awesome, you know how many interviews with engineers Ive had saying they were at a place for 10 years, but the company had a raise cap of 2%, then saying that they wouldn’t hire me because I couldn’t give assurances to be a likeminded clown making corporate rich reply lnxg33k1 16 hours agorootparentprev>Show me the incentive and I will show you the outcome Would print this phrase on every angle of the offices reply ponector 16 hours agorootparentprevBut bonuses are not related to the cloud costs. Nobody in engineering cares about spending because there is no benefits on doing so. Even more: most people are on fixed salary and will get paycheck anyway no matter how low their effort is. reply pixl97 14 hours agorootparentHence >>Show me the incentive and I will show you the outcome reply krageon 46 minutes agorootparentprevThis is fun, but these policies are based on the fact that your managers don't care about you and in fact prefer you remain as poor as they can legally make you. So any kind of reasoning along these lines (\"well this makes sense\") just is not real and won't ever become real. reply freedom-fries 14 hours agorootparentprevLook at Mr bonuse-pants here. Y'all get bonus for writing software? reply sotix 15 hours agoparentprevI consistently tried to push for cost management at my last job, but the product manager just wanted to push new features he could show off to management above him. We let costs inflate to ridiculous levels despite my constant discussion around the topic. Software engineers ultimately had no say in the matter. I was laid off this week in a mass layoff because the company doesn’t have enough money to pay all of us anymore. It’s disappointing to see, and I wonder how many other teams ignored these optimizations and how much unnecessary total cost it all summed to. reply righthand 16 hours agoparentprevI had to implement a 2nd deploy for a QA environment, and my first question to the infrastructure team was “won’t this be costly is there a better way to handle this?” They shrugged off the cost and said they would optimize my deploy once I was done with the initial implementation. 6 months later their optimization was to undo all the work not because it wasn’t a good implementation but because it revealed how much non-optimization had went into the QA environment before I even touched it. A lot of cost is probably due to the “we just taped these two things together” strategy for lower environments. reply ExoticPearTree 15 hours agoparentprevIn the organization that I work in, costs are transparent to everyone involved and most people are aware of the need to keep costs as low as possible. One of the downsides with this approach is that engineers/developers are not very good business people and don't really understand the notion of \"the cost of doing business\". And from time to time we have issues with \"but it costs $70 more per month\", and spend $1000 to optimize those $70 :) In the end, even with some of the wrinkles mention above it helps and saves money when costs are transparent and readily available for anyone. reply cduzz 16 hours agoparentprevI think \"engineer\" isn't really the correct word to use for the artisans who build much of the tooling used by most companies. An engineer either wears a striped hat and drives a train, or, went to a credentialed school and passed a bunch of test and is allowed to sign documents that state \"this thing, if built this way, won't collapse and kill people.\" It is expected that an engineer can predict with reasonable accuracy the expense and timeline of a project, and how to maintain the resulting thing, without resorting to voodoo like \"scrum velocity.\" In large part that's because engineers stick to doing things that are well understood and predictable, and if there's risk they resolve the risk before undertaking the project. (Is there bedrock over here upon which to build a foundation? I don't know; let's find out first!). Sure, there are engineering disasters even today -- buildings that unexpectedly lean over and door/wall things that unexpectedly fly off the side of airplanes, but those are typically organizational / process problems not \"engineering doesn't work\" problems. reply reactordev 16 hours agorootparent“engineers stick to doing things that are well understood and predictable” I’m calling BS on this. If this were true, we’d still be a ground species. Engineering has been and will always be about creating something electrical, mechanical, computerized, or all, that solves a problem. Understood or not. Engineers are not oracles. They can not predict whether a tower built in Italy will eventually begin to lean due to erosion. They can not predict that a steel beam rated for 300T of force would break at 180T. They can not predict a rogue developer removing a package from underneath their dependency tree. You can give estimates all you want but you are still guessing. If engineers were as you say they are, we would never have delays, we would never have traffic jams, we would never have crap software, we would never have flight. reply practicemaths 16 hours agorootparent“Engineering is the art of modelling materials we do not wholly understand, into shapes we cannot precisely analyse so as to withstand forces we cannot properly assess, in such a way that the public has no reason to suspect the extent of our ignorance.” - Dr. AR Dykes reply reactordev 16 hours agorootparentAh the eloquence of Dr. AR Dykes, perfectly said. Thank you. reply noboostforyou 14 hours agorootparentI am partial to the following one about computers: \"A cpu is literally a rock that we tricked into thinking.\" reply cduzz 16 hours agorootparentprevEngineers manage risk and cost. They certainly make mistakes, like those couple buildings that are famously leaning over in SF and NYC, or the citycorp center where they got the wind sheer loads wrong and had to hot patch the building. But looking at the malarkey that goes on in \"software engineering\" or whatever -- clearly not engineering, at least not where I've seen it. Engineering: a process of repeatably solving an understood problem predictably. Craft: a process of solving an understood problem. Science: a process of solving a problem without an exactly understood outcome. Art: a process of working. These are all made-up definitions. I'd expect a software engineer to give me a system that locally caches and verifies distribution artifacts and validates changes -- a craftsperson who gives me a tool chain that yeets goo from the internet and builds on that without validation is not, in fact, an engineer. They could be quite practiced at the art of building working systems, but they're not managing risk.... reply dehrmann 16 hours agorootparentWhat makes software engineering special is the systems are more complex and are cheaper to test and break. You get a completely different engineering culture when you can roll back a bad change after seeing it fail during the canary push. That, and what's usually on the line is money, not life. I'd feel a lot better making a $1M mistake than making a mistake that killed someone. reply ExoticPearTree 15 hours agorootparentprev> Engineering: a process of repeatably solving an understood problem predictably. We call it help desk, not engineering. reply gottorf 14 hours agorootparentprev> Engineers manage risk and cost. \"Any idiot can build a bridge that stands, but it takes an engineer to build a bridge that barely stands.\" reply robocat 14 hours agorootparentprev> An engineer [] went to a credentialed school and passed a bunch of test and is allowed to sign documents that state \"this thing, if built this way, won't collapse and kill people.\" Ahhh - that old craptacular definition. You completely ignore mechanical engineers, chemical engineers, electrical & electronics engineers. Not all engineers make bridges. Secondly, the implied cause and effect even within civil engineering is a fantasy. Signatures on documents by credentialed engineers doesn't prevent disasters as you noted: Bridges fall down, buildings burn. Read the engineering reports on civil engineering disasters, and look at the consequences for the engineers involved. You do some handwaving about organizational/process problems, but actually that is the key to safe engineering. Organisations deliver engineering projects and they do it across jurisdictional borders using insurance and liability and with a variety of other means that work: \"signatures don't prevent disasters\". Lockheed Martin's skunk-works and SpaceX are real engineering. Any good definition of engineering needs to encompass an extremely wide variety of activities. Engineering is compromise. I have no love for Musk but him saying build that actuator for less than $5k is actually true engineering: https://news.ycombinator.com/item?id=39085892 I would like to know the psychology behind why people wish to believe credentialed signatures are so powerful? Maybe a cross between two concepts #1: \"that individual engineers run the world\" and #2: \"that retributive punishment of individuals works as a deterrent\". I think concept #1 comes from the egotist idea of most engineer-types that we are the center of everything (I need a whole article to explain the concept). I think concept #2 is related to beliefs about the value of incarceration and also punishment beliefs derived from religion (especially in the USA where prisons are not fixing problems?). Edit: issue #3: the idea that we should make rules about what words mean. It takes a certain worldview to think words should be defined rather than evolve (or worse that words should be part of a justice system) reply cduzz 13 hours agorootparent> Ahhh - that old craptacular definition. You completely ignore mechanical engineers, chemical engineers, electrical & electronics engineers. Not all engineers make bridges. I suppose you've got an engineering degree in pedantic engineering? Engineers manage cost and risk. The skunk-works stuff is marginally \"science\" not \"engineering\" given the relatively large budgets and relative lack of \"we know this works.\" Cern is similarly an enormous engineering enterprise in that it's a huge stack of \"we know this works\" in service of \"we're not sure what this will do\" A discussion of how \"software engineers\" deliver projects with neither cost or risk as part of the process implies, to me, that they're not engineers. reply robocat 11 hours agorootparentYou are the one trying to push your definition of engineering. I provided counter-examples that show engineering encompasses a lot more than your definition. I simply don't understand why anyone thinks writing software is somehow uniquely not \"real\" engineering. Somehow we are indoctrinated to believe that it isn't but all the evidence seems to show software engineering is a valid description. I have no lack of experience watching the fuck-ups made by electronics engineers, or the fuck-ups made by mechanical engineers. You appear to want to define engineering only as certified civil engineering. And I've seen enough of their fuck-ups too, with signatures. In fact I'll ask my bridge engineer friend from uni about it! Unfortunately my bridge building grandad is dead so I can't ask him. reply cduzz 9 hours agorootparentThe vast majority of people I encounter with some \"engineering\" title, in software (or the related \"Architect\") are in fact not trained as engineers or architects, in any field. A site reliability \"engineer\" or a software \"engineer\" is not an actual engineer just because they've got that in their title or job description. If I were to hire a \"chemical engineer\" position and instead hired a chemist, or a mechanic, or a rando who's cooked meth, I may end up with things working okay, or I might end up with a serious mess, even if those people I hire call themselves \"engineers\" (but in fact have no formal training as such). I'm not sure to what degree credentials matter, but do credentials matter more than \"not a god damned bit\" ? I'm not saying the title makes you \"not an idiot\" -- people gonna people -- but attention to \"cost\" and \"risk\" is (theoretically) one of the distinguishing characteristics of engineering training vs ... \"mather\" or \"programmer\" or \"philosopher\". reply robocat 8 hours agorootparentYeah, the debasement of meaning is annoying - vice-president is one I hate. Another one that surprises me from the US is \"licensed nail technician\". I have a bachelor of engineering title I can use with my name, but that is another distinct type of bullshit. In New Zealand one relevant legal certification is CPEng which you can apply for after receiving your degree and working for a few years: https://www.engineeringnz.org/join-us/cpeng/ And apparently our government agreed in 2022 to introduce a new licensing regime for engineers doing safety-critical work. But in an international world, how relevant are certified individuals? When I purchase a stove from a US brand and it catches fire, there needs to be other liability/retribution/corrective systems to deal with the problem. It matters little to me who signed off on the product in the US. Can I import custom structural steel beams? How many New Zealanders have signed off on this steel construction: https://ccc.govt.nz/the-council/future-projects/major-facili... We need a new stadium because the last one broke. Unfortunately it wasn't insured due to some cockup at the city council (which I suspect had zero retribution on the people that cocked up - I wonder if they signed bits of paper?). Over-credentialisation is a problem too - where is the right balance? The shift to everyone needing credentials is fucked. My friends (nurses, teachers) literally weep at the absolute trash they have to \"learn\" for their credential. I also vividly remember the crap I needed to disgorge to get my degree. I don't know what the answer is, but I honestly believe most credentials are pointless waste and adding more credentials is not actually effective. Neither do I believe that that the anarchy of libertarian free markets are a workable answer. reply dilyevsky 16 hours agoparentprev> It is unfortunate that cost management isn’t something most engineers keep an eye out for on a regular basis. That’s because they were explicitly told not to worry about costs for the last 10 years so majority of ICs at this point never had to do it their entire careers reply myaccountonhn 15 hours agoparentprevWhat I've observed is that people don't really keep track of what they are spending. I like to set up weekly newsletters that show costs and also if there has been a decrease or increase. In bigger corps, you also should have team based tagging of resources so that specific teams get exactly what they are spending. At the very least, managers will look each week and be like \"why did costs increase this week? What's going on?\" even if the engineers don't care. \"What's get measured gets managed\" as they say. reply onyxringer 16 hours agoparentprevDon't you know that \"Developer's time is expensive\"? :D reply tuananh 16 hours agoparentprevthat's one of the main reason we made this feature \"opt-out\". if you want to keep it up, you have to tag it. once you tag it, it can opt-out 7 days. then you have to extend it (simply chat with our bot) reply zikduruqe 16 hours agorootparenthttps://cloudcustodian.io Create a rule and shame them on Slack. reply racl101 16 hours agoparentprevI tend to have the opposite problem. I obsesses over the cost of things, and am pretty bashful about bringing it up to my manager, and he's always surprised that scaling some resources doesn't cost more. But I learned the hard way as a contractor about letting these resources run crazy and had to pay out of pocket so I have PTSD about it, which is why I'm vigilant. reply mr_00ff00 15 hours agoparentprevWould be a random coincidence, but if you work at a large bank, I think I may know the team that had that 300k test db lol. reply nlawalker 15 hours agoparentprevThis topic got a lot of discussion in \"I accidentally saved my company half a million dollars\": https://news.ycombinator.com/item?id=38069710 reply dangus 16 hours agoparentprevWell, it’s definitely fathomable. Does my employer have cost control baked into their proceses, tooling, and culture or are they rushing me to get projects out the door leaving barely enough time to make sure they’re production-ready? Most places I’ve worked had no formal production readiness review before launching infrastructure. reply VoodooJuJu 15 hours agoparentprevAnd that's how insignificant the costs of cloud providers are in the grand scheme of things. It's a lot of money to a bootstrapping startup, but for the vast majority of these cloud providers' customers, it's a rounding error that's easily forgotten for 3 years. And that's precisely why you and your little bootstrapper or indie firm should not be using globocloud: you do not have mountains of cash to piss away. Bare metal is trending again. And in this downturn, it's no wonder why. Smaller companies are getting smarter and more efficient. They've decided to chase money instead of cargo cults. Globocorps burning cash on globocloud is not a signal for small fish to do the same - it's a signal to do the polar opposite. You're not going to become like them by copying what they're doing now. It will not work for you. Globocloud isn't successful because they shovel cash into AWS's shredder, they shovel cash into the shredder because they're successful. reply hibikir 15 hours agorootparentYou'd be surprised. I've seen AWS bills well in the 9 figures. It's just that fixing expensive designs is, in itself, quite expensive, and many of those very large corps have hiring practices that don't allow them to complete for the top of the market. Sometimes there's tens, if not hundreds of millions in savings a year, but corporate sclerosis makes it very difficult for broad cost-saving initiatives to be identified and approved. It's the same issue in any large organization: Large levels of success somewhere allow for large levels of waste somewhere else, but often the waste is not required for the success to exist: The success just makes the organization complacent. reply znpy 16 hours agoparentprevThis is entirely artificial: I now work at a company where we know very clearly what our infrastructure costs. Yes, we know the exact costs (what was negotiated, not what is on the public pages). And we celebrate costs slashing as much feature delivery and other stuff. But this is entirely a management problem: at my previous job, only one manager (skip-level manager from my point of view) knew what exactly were we paying for infrastructure. That moron wouldn't share that information with us engineers managing infrastructure of course, so there were a lot of infrastructure choices that didn't really made sense according to the public prices but (I guess?) made sense according to a price sheet we didn't know. So we didn't know what we were spending, didn't have the basic data to estimate the price of a new solution or a new service and didn't have the data to determine how much would we be saving by making changes (optimizing stuff etc). I fought that battle for a bit but then i just said \"GFYS, i'm not going to have fights with you so that you can save money\" and let go. Later i left the company completely. Former colleagues tell me it's even worse now: there are consultants from the cloud provider involved, they know the pricing deals, and whenever the topic comes up the manager shushes the consultant so that the engineers don't hear the prices. tl;dr: it's an entirely artificial problem, and it's most likely a cultural/management problem. edit: and i'm not even talking about incentives, as somebody else has correctly pointed out. reply paxys 14 hours agoprevI was thinking about this recently. I work at a large company with untold millions in AWS spend. I'm 100% confident that I could shave off a few thousands (maybe even tens or hundreds of thousands) from the bill with a little bit of effort on my side. If I go up the management chain and ask if (1) I can make this an official project and put it on the roadmap or (2) I can do this on my own time and keep some % of the savings for myself as a reward, the answer would be a very clear \"no\" to both. So overall, as an end developer I really have no incentive to work harder and ensure lower operating costs for my company, and I'm sure most developers in the industry are in the same position as me. reply mullingitover 14 hours agoparentIf your company has a spending commitment with AWS in order to get a few percent savings, and it's just barely hitting the contracted amount, it may not be worth the effort to pursue any cost savings. Suppose your company has committed to hit 5M in spend, and they're just barely inching over the line at 5.01M. You might spend a bunch of time and labor expense knocking it down to 4M of usage and not really move the bottom line at all. > (2) I can do this on my own time and keep some % of the savings for myself as a reward This is a textbook case of perverse incentives. reply deepsun 14 hours agorootparentYep, that's how AWS sales make money. Theoretically, your company could reduce their commitment to 4M next year, but the AWS sales would start negotiating hard against it, like \"you will not get the same discount with less commitment\". reply kthejoker2 3 hours agorootparentNot only will you not get the discount you won't get the commit at all, AWS only reups commits with incremental growth. You can spend less; you just don't get commit pricing. reply echoangle 2 hours agorootparentprevCan you explain how this is perverse incentives? Wouldn’t this actually be a good way to align incentives of the employee and the company (it makes both of them want to save money)? Or do you mean it makes the employee increase cost unnecessarily from time to time so he can reduce it later? reply throwaway883322 43 minutes agorootparentI am not the parent poster, but I think your last sentence covers it well. reply mk89 14 hours agorootparentprevI agree with this comment. Ironically, I was asked by a manager some time ago if we can imagine using some (more) resources from AWS to reach the next spending commitment. If you're just below the threshold, it's probably inconvenient. EDIT: To give some context: you can only do this, of course, when you know that you're not really wasting resources, otherwise you end up with just burning money to save little to no money :) reply caeril 14 hours agorootparentprevSpending commitment on AWS? Are you referring to ec2 reserved instances? Even if you have one year reservations on your instances, starting service migrations/deprecations now would pay off quickly enough. Your commitment expires in 6 months, on an average basis. reply nevon 13 hours agorootparentWhen you're a large enterprise customer you get private pricing in exchange for committing to spending a certain amount over a few years. This is unrelated to reserved instances and such. reply jfim 14 hours agoparentprevPart of it is that it creates an incentive to create wasteful systems, only to \"optimize\" them later to rack up a bonus. Even if it gets changed to only pay out for reducing spend incurred by other engineers, it's possible to collude in such a way to extract bonuses from the company. A better way to have aligned incentives for the company and the employees would be to allocate a bonus pool for the entire company, from which AWS expenses are taken out of, but that might be a bit unorthodox. reply avidiax 12 hours agorootparent> allocate a bonus pool for the entire company, from which AWS expenses are taken out of Also a perverse incentive. If we use ec2.small, the customer's query will take 3x longer but be half the price. Let's turn off the nightly security audits. We can live with quarterly backups, right? What do we need all these logs for, anyway? We could hack something that works together in 2 weeks, but if we spend 3 months, it could be really efficient, let's do that... reply nick_ 14 hours agoparentprevThis is one of many insights that hint at why biz-facing cloud architecture is so popular, wasteful, and profitable. The incentives are designed to form an enormous cash siphon. From aggressively marketing toward fearful & liable (or maybe just tech-cost-illiterate) upper-management to the silencing effect that the low-rung experts experience when sounding the alarm. reply cduzz 14 hours agoparentprevHow are they going to be sure you're not just farming cobras[1]? [1]https://en.wikipedia.org/wiki/Perverse_incentive reply devin 14 hours agoparentprevUnfortunately it seems like businesses only wake up and asks for cost reduction on the infrastructure spend side once the problem is out of control. At that point, the level of operations effort to get it under control feels more like a \"big rewrite\" than a collection of small tweaks. reply killingtime74 14 hours agorootparentIt's opportunity cost. If it's not a problem then you can just make more money with your time. reply Draiken 12 hours agorootparentNah, that'd imply they think about this objectively. Most companies simply don't. Most time this \"opportunity cost\" is then spent on useless hacky features that are never used and forgotten right after release (redesign anyone?). Of course there are exceptions to all of these, but IME the majority of companies are either focusing on pennies or ignoring it completely. Not sure why you don't see balanced approaches more often. Maybe this will change with less VC money flying around. reply PreachSoup 13 hours agorootparentprevThat's why in our company we have 2 type of engineering effort, core projects impact and improvement. Not all time is spent on the impact. A balancing act is needed reply mlsu 14 hours agoparentprevMaybe there should be a sort of \"anti-saas-sales\" role: you get commission on whatever costs you're able to justify as superfluous. After all, the person at AWS makes commission selling you the stuff. reply jddj 12 hours agorootparentFunny downvotes. Although not structured like this value engineering is common in other areas. reply killingtime74 14 hours agoparentprevNot a commission but don't you have performance reviews? Promotion boards? Surely it can count for a bit, if the effort is as little as you say. reply tayo42 11 hours agoparentprevCompanies make and spend so much money that this doesn't matter. Thousands, 10s of thousands, 100s is pointless in comparison the potential of building features. A developer costs about half million a year, (salary, bonus, rsu, taxes, benefits) If they are paying you to saves 10s to maybe hundreds the company is losing money on you so they won't do this. If your at a public company, look at your company's quarterly reports and see what it would take to many any kind of impact on net income. reply PH95VuimJjqBqy 14 hours agoparentprevsome of us care enough about our craft to do it without the backpatting or the extra money. reply gurchik 16 hours agoprevI recently helped save $150k per year by deleting node_modules. I noticed that one of our S3 buckets had high data transfer costs, a bucket that our app downloads HTML+JS assets from when we push out a new release. I downloaded the \"directory\" of files for our latest release and saw it was mostly node_modules. I checked the code and confirmed that, yes, if this file exists in the bucket then it'll be downloaded by the user. I wrote a quick Python script to list out each directory that had this problem, and a quick Slack message to the appropriate team later, we discovered the specific commit that was the cause, a change to our CI that inadvertently uploaded that directory when we wanted to ignore it. A few months later, I checked the billing metrics, the effect was an avg of $12,500 reduction in cost for this bucket, or around $150k per year, or 4% of our bill. Not bad for one hour of work. Over the course of a quarter I reduced our bill by over $1m, or around 30% of our bill. I might write a blog post explaining how to go about something like that. A lot of people are not familiar with tools like Trusted Advisor which can easily tell you if you have, for example, unused EC2 instances that can be terminated. reply Detrytus 14 hours agoparentNow, the inconvenient question: how much of that $1m savings ended up in your bank account as a bonus? Because certainly some of it should :-D reply gurchik 11 hours agorootparentNot sure yet, but probably nothing. I completely understand the expectations written in this thread to receive something in return, but I've given this thought and I'm not sure how to do this in a fair way in this situation. First, I was given dedicated time that quarter to work on cost savings and other people weren't, if I received a bonus is that fair to other people who didn't have the same opportunity? Not to mention the possibility of people abusing this process. I would be happy to receive some extra cash, don't get me wrong, but I work for non-monetary benefits as well, and I have received some of those as part of this work. If I worked at a company with a different culture and I was being punished for doing the work, I would demand some bonus. reply krageon 43 minutes agorootparentYou being at peace with \"non-monetary benefits\" is what depresses wages and real rewards for you, but more importantly all your colleagues. reply tuananh 16 hours agoparentprevplease do :) i would love to learn more about this reply thefourthchime 17 hours agoprevI've mentioned this before, in my company (big media company) I saw some S3 costs creeping up each month. I looked into it and it was a system we abandoned that was still copying files to this bucket. I reached out to the team and they turned it off, it saved us $1m a year. The higher-ups rewarded me by telling me that a team should have caught this so I should meet with them now. reply Salgat 17 hours agoparentIt's truly fascinating how companies won't bat at an eye at spending ungodly amounts of money on things they don't need, but will sweat profusely at the thought of a tiny fraction of that going towards additional compensation. reply anotherhue 16 hours agorootparentEnriching AWS does not threaten their social standing. reply preommr 14 hours agorootparentprevSomething something snakes, something something unintended consequences. reply Salgat 13 hours agorootparentSabotaging your company's infrastructure for a bonus is a great way to end up in prison. reply Marrand 40 minutes agorootparentHow would you prove intent to sabotage? reply stevejb 14 hours agoparentprevI work for a company called CloudFix, and we are solely focused on AWS costs. We do automated AWS cost optimization. We find one of two reactions when we deliver savings to customers: (A) \"Hey wow, this is great! We are so excited to be saving from here on out.\" OR, (B) \"This should have been caught earlier. $TEAM was supposed to be experts...\" and then blame game starts. It is really unfortunate when institutions react in the latter way. Often the engineers are assigned to cost optimization, along with a million other things. And, the incentives aren't really aligned well to reward savings. For example, S3 Intelligent Tiering is the right thing in 99.9% of cases - so it should be your default bucket type. BUT, engineers often face only downside risk for the change, and very little upside reward. And, it isn't their money so they just leave it. The cost of overprovisioned S3 can be staggering! What is really needed is to establish a proper FinOps discipline, put someone in charge of cost savings, and make sure incentives are aligned properly. And of course check out CloudFix if you can! reply tuananh 17 hours agoparentprevtrue. the first step is to have visibility into what's eating the bill. it's just like you need to profile the program before optimizing it. that's why we did finops dashboard first thing when we first started the cloud journey. can't optimize if you dont know. reply max_ 17 hours agoparentprevYou got rewarded with more meetings? Not a bonus? reply cddotdotslash 16 hours agoprevI’m convinced that once a company reaches ~$10m/year in AWS spend it becomes entirely reasonable to hire an in-house engineer whose sole job is to find cost savings opportunities. Literally a “find unused stuff and turn it off” engineer. reply 8organicbits 15 hours agoparentI've spent some time doing this. There's always old systems people don't really understand, ownership is poorly defined, and no one knows what happens if you turn it off. It's archeology. Understand what the system is doing and how it interacts with other systems and the business. If it looks unneeded back it up, stop the VM, wait and watch for fallout, and eventually terminate it. reply cddotdotslash 11 hours agorootparentThere’s definitely a science to it. To complicate matters, the way you explore those connections, take backups, identify owners, and perform restores is different across pretty much every cloud service. reply joshstrange 16 hours agoprevOne small issue I have as a developer who can spin up just about anything on AWS is this: I have zero insight into the costs. Yes, my company could turn that on for me but it's rare that they do so it's nearly impossible to know if I did something that costs a lot of money (relatively or in general) without access to the cost explorer/billing dashboard. And before \"well can look up what a t2.2xlarge costs and calculate it\", sure. In a very contrived example I might be able to see what it costs but so many things are hidden/hard to see in AWS. For example, I recently spun up an RDS customer on my own AWS account. After testing for a while I decided it wasn't what I wanted and I deleted the cluster. Fast forward a month and my bill is well over what I expected (Like $30, no it's not a ton of money but it's my personal account and I wasn't expecting that charge). Come to find out it created a VPC as part of the RDS cluster (I think maybe it was for the RDS proxy? Still not sure) that didn't get deleted. I had to go chase that down and even that process wasn't easy. I had to make sure that it wasn't be used by anything else and then delete other things that were created when I made the RDS cluster before I could remove the VPC. I was only able to do the above because I had access to the billing info. I would have left that VPC indefinitely on my work's AWS account by accident and been none the wiser. I'm more than happy to take costs into account but without access to what things are actually costing us I can't help that much. Mostly because I need to know the costs to know what's worth optimizing. Sure I know I could improve X feature but if that costs us pennies a day (or month sometimes) then it's not worth it. Similarly if I know feature/infra Y is costing $XX,000/mo then I know I should rethink or investigate if that's correct/worth it. reply tuananh 16 hours agoparentbilling transparency is very important. in the past, i had a case like this: dev accidentally enable backup policy for test database with no retention. finops think that db backup is important and ignore it. dev has no access to billing and have no idea what's creeping up the bill reply joshstrange 16 hours agorootparentExactly, sometimes it's not clear at all what something will cost (and/or if the costs will go up). I'm happy to glance at the monthly costs here and there and if I see a jump I can dive in and see where it's coming from. We all make silly mistakes, like leaving logs on infinite retention in CloudWatch, and that's something I can easily fix/address but only if I have the info. I've asked, off-hand, a couple times for billing access but nothing has come of it. I don't want to seem pushy but also it feels like data I need to perform my job to the best of my ability (especially at a small company). I don't think it comes from a place of \"We don't want to give Josh access\" or secrecy as much as it not being a priority but I need to bring it up again. reply belter 15 hours agoparentprevYou are aware of this? - https://calculator.aws/ reply joshstrange 14 hours agorootparentI’m very aware of that tool but it’s far from perfect. I’ve spec’d things out on that then seen very different prices when I actually create things in AWS. In part because the tool doesn’t take some things into account or because sometimes it’s impossible to guess your usage for a new feature. I don’t believe the VPC was factored in when I used that calculator, even after selecting RDS Proxy. reply belter 14 hours agorootparentVPCs are free. Are you talking about data transfer? I know it has options to enter values like amount of data transfer you are planning to do. reply joshstrange 13 hours agorootparentI believe the cost was actually a \"NAT gateway\" attached to the VPC which has a monthly cost of about $30 even if you don't transfer any data over it. reply nathanwallace 16 hours agoprevReaders may find Steampipe's [1] AWS Thrifty Mod [2] useful. It will automatically scan multiple accounts and regions for 50 cost saving opportunities - many of which are looking for over-provisioned or unused resources. For example, it's crazy how much you can save by doing things like just converting your EBS volumes to the newer gp3 type. Combine with Flowpipe [3] to automate checks and actions. It's all open source and extensible. 1 - https://github.com/turbot/steampipe 2 - https://github.com/turbot/steampipe-mod-aws-thrifty 3 - https://github.com/turbot/flowpipe reply latchkey 17 hours agoprevIt is interesting to note that the author works at VPBank, which is one of the larger Vietnamese banks. Saving $150k per year on an AWS bill, is really nothing to them. The fact that they even outsource their compute to AWS is kind of surprising when they could just fill up their existing data centers (like VNTT https://vntt.com.vn/) with equipment, and save a whole lot more money. reply JCharante 16 hours agoparentAnd it's also interesting that they can outsource their compute to AWS because AWS's nearest data centers are in Hong Kong & Singapore. I didn't realize a bank would allow that. reply latchkey 16 hours agorootparentI thought it, but I wasn't going to say it. Vietnam's internet connection is notoriously unstable. The running joke is that sharks attack the fiber connections [0] because pointing fingers is a national past time. The fact that a major bank is relying on an external AWS like that, makes it even more comical. My guess is that nobody in corporate approved this guys posting and if word got back, it would disappear quickly. Reminds me to forward this to my buddies who run Timo, which VPBank used to own, but then dropped [1]. Timo was the first forward thinking bank in Vietnam with a great tech platform, likely because it was started and run by foreigners... ¯\\_(ツ)_/¯. [0] https://www.reddit.com/r/VietNam/comments/zvo553/sharks_ate_... [1] https://fintechnews.sg/42738/vietnam/vietnams-challenger-tim... reply ejs 17 hours agoprevThis is especially easy if you can shutdown environments that are only used for dev/staging tasks. With 168 hours in a week - how many hours do those things need to be running? I run a little tool for Heroku to make it easy to do this kinda thing. reply cosmotic 16 hours agoparentOften the continous use discounts make regularly turning on and off a wash. reply bdcravens 16 hours agorootparentThis assumes they have something like RI etc for those resources. Those are typically used for production, but far too often, dev/test resources are usually turned on ad hoc. reply null3cksor 17 hours agoprevCouple of years ago I saved about 14 mn of revenue per year for my company. I got a 250$ bonus for it. reply schnebbau 16 hours agoparentThis is why you should start the conversation with \"I have drawn up a plan to save the company $14M per year. I will execute this plan in exchange for $7M upon completion.\" If they say no then just go back to your regular duties. reply munchler 13 hours agorootparentBecause blackmail is an effective salary negotiation tactic? reply listenallyall 8 hours agorootparentprevVery few companies would make this deal at 0.7M, 0.07M, or even 0.007M. Direct sharing a % of savings with the responsible employee is simply not the way most companies work. Consultants, now, that's different... reply charlie0 3 hours agorootparentFor a slice of 7M, just quit and come back as consultant. reply llanowarelves 16 hours agoparentprevLearned a lesson that you only have to \"spend\" (forgo) $250 to cause that company $14m in losses (that you could have prevented) reply nvm0n2 11 hours agoparentprevBut was that your job? Because if so, you really got salary+bonus. And if you'd found nothing you'd still have got salary. So you can look at it several ways. reply EvanAnderson 16 hours agoprevI worked adjacent some telecom consultants in the 90s whose income was solely driven by a percentage of cost savings they could trim from telephone bills. Seemed like a very brash business model but they clearly knew there was gold to be mined. I keep thinking I should be doing \"cloud optimization\" work and being compensated this way. Slicing and dicing output from usage/billing APIs and providing an \"optimized spend\" probably has the potential for a lot of low hanging fruit. reply QuinnyPig 8 hours agoparentAs someone who's been doing this for the better part of a decade: it's a mirage. No client is going to sign for a \"percentage of savings\" model when it comes to cloud. Believe you me, I've looked at this up, down, and sideways; neither the math nor the psychology work out the way you'd hope. reply philsnow 15 hours agoparentprevLike https://www.duckbillgroup.com/about/ ? There’s probably room in the market. reply bdcravens 16 hours agoparentprevMy employer has the same business model (we audit Fedex and UPS invoices for late deliveries, bogus surcharges, etc) reply ary 16 hours agoprev> The best optimization is simply shutting things off This is the way. A similar idea has been bouncing around in my mind for a while now. An ideal, turnkey system would do the following: - Execute via Lambda (serverless). - Support automated startup and shutdown of various AWS resources on a schedule influenced by specially formatted tags. - Enable resources to be brought back up out of schedule when demand dictates. - Operate as a TCP/HTTP proxy that can delay clients so that a given service can be started when it is dormant or, even better, the service isn't serverless but you want it to be. This can't work for everything, but perhaps enough things such that the need to run always on services is reduced. Cloud Custodian [1] can purportedly do some of this, but I've been reluctant to learn yet another YAML-based DSL to use it. So this is my \"make things designed to be always-on serverless instead\" project and the work AWS has done to make Java apps function on Lambda keeps me thinking about the potential to take things that 1) have a relatively long startup time and 2) are designed to be long running service loops, and find a way to force them into the serverless execution model. [1] https://cloudcustodian.io/ reply pid-1 15 hours agoparent> Operate as a TCP/HTTP proxy that can delay clients so that a given service can be started when it is dormant or, even better, the service isn't serverless but you want it to be. This can't work for everything, but perhaps enough things such that the need to run always on services is reduced. My team mostly builds internal stuff and we save tons of $$$ by using Knative + Karpenter, which basically does that on container + EC2 levels. reply akira2501 13 hours agoparentprevEverything I've built in AWS is strictly serverless. You can do an incredible amount with a clever DynamoDB pay-per-request setup, S3 and CloudFront. I haven't once felt the need to reach out to EC2 or RDS and I can't imagine building any sort of control plane to spool them up and down for me. reply kthejoker2 3 hours agoprevWhen I joined my current company I quickly found our internal Azure environment was effectively unmanaged. Four weeks in I shaved nearly 55k in monthly spend just scripting out VM shutdowns and service pauses. Cloud revenue in most large companies is at least 25$, maybe up to 40%, pure developer waste because nobody upstairs knows the difference. reply JohnMakin 16 hours agoprevLooking forward to the kubernetes one - Most kubernetes clusters are designed for high availability, not necessarily for being able to quickly spin up/down and there’s often a lot of hidden complexity there (at least on aws). reply tuananh 16 hours agoparenthint: it's going to use the same platform. allow us having the ability to inject certain manifest into any eks cluster within the org. reply JohnMakin 16 hours agorootparentHave you done this or attempted this yet? Every kubernetes cluster is different, but in my time working with them the last several years I anticipate the following issues: - dependent services not coming up in the order you expect/want - issues draining nodes due to crashlooping/erroring pods (can also be caused by dependent upstream services going down in wrong order) - Persistent Volume retention/synchronization - IAC not cooperating - Configuration annoyances with deployments’ availability/replica settings - Thundering herd types of problems I can think of tons of things that can make this extraordinarily difficult. I’ve had many managers over the years pitch this idea of “rapidly deployable/destructable EKS clusters” and the projects always get killed due to the complexity around this. IMHO they simply aren’t really designed for this type of thing, however, I could be misunderstanding exactly what you’re trying to do. reply tuananh 16 hours agorootparent> I’ve had many managers over the years pitch this idea of “rapidly deployable/destructable EKS clusters” and the projects always get killed due to the complexity around this. This is exactly what we do: blue green eks cluster. We just thought if we do it on monthly basis, DRP will be piece of cake :) reply JohnMakin 15 hours agorootparentLook forward to the writeup! thanks reply TrianguloY 16 hours agoprev> You go home. So you shut things down. Sorry for the rant, but this is usually wrong. The amount of people that just keeps their computer on is noticeable. And when I ask it's usually \"just to avoid having to wait\" or \"I've always done that\". I personally always hibernate my computer. When I turn it off it takes more time, but I'm already on the other side of the building so I don't care. When I turn it on it takes basically the same amount of time, and it is exactly as I left it. People keep the computer on just because convenience...and I don't think it's a good thing. reply charlie0 3 hours agoparentI always shutdown my machine at night and sometimes restart when I leave for lunch. I've noticed running Docker and other apps for a while makes my machine slower. I'm convinced there's a memory leak somewhere and restarting away fixes those issues. reply SoftTalker 16 hours agoparentprevI keep mine on because it's my jumphost for working remotely. But I agree many people don't need to do this. My company, though, wants people to leave their PCs on so they can get automatic updates and be centrally managed. reply saylisteins 16 hours agoparentprevI always leave my PC on, but for different reasons: - I have a plex server running on it - I can remote into it from my phone, this comes in handy a lot of the time. - I can remote into it when traveling through my Fire stick using parsec, which means I don't have to carry a laptop with me everywhere I go ( I also setup my phone so I can use it as keyboard/mouse when I do this). Regarding energy costs, it's negligible for the benefits it gives me reply danfritz 16 hours agoprevCut costs down to 90% by going serverless and run everything on lambdas? Nothing keeps humming if it's not being used reply bdcravens 16 hours agoparentSure, after you reengineer your application. Even then, \"serverless\" apps often use persistent resources like databases, and your developers will likely spin up those resources for the same reasons as indicated in the article. reply bob1029 16 hours agoparentprevCost savings can be incredible if you use the FaaS product in the most aggressive way possible. For us, this means using functions as a simple translation layer between SSR web forms served directly as text/html and whatever SQL provider (ideally also on a consumption-based tier). 90% sounds just about right. We are seeing figures going from $120/m for a VM-based QA environment to $10/m for a consumption-based / serverless stack. reply hnav 16 hours agoparentprevdepends on what your utilization looks like, serverless is usually +/- an order of magnitude more expensive. Ideally your workloads are stateless and containerized so you can shuffle them between serverless, container orchestration that you own and dedicated VMs. reply nikita 16 hours agoparentprev(CEO of Neon) We routinely see 10x savings when switching from RDS or Aurora. Especially if you start adding dev environments. reply 8organicbits 15 hours agoprevYou should always calculate if you're actually going to see cost savings. Counterintuitively, running for fewer hours can increase your bill if it causes you to switch to on-demand pricing [1]. There's a break even point you need to get past. [1] https://alexsci.com/blog/modeling-on-demand-pricing/ reply whummer 12 hours agoprevTo give this a slightly different spin: --> \"The best optimization is simply not spinning things up!\" At least for local development and testing, as made possible by LocalStack (https://localstack.cloud), among other local testing solutions and emulators. We've seen so many teams fall into the trap of \"someone forgot to shut down dev resource X for a week and now we've racked up a $$$ bill on AWS\". What is everyone's strategy to avoid this kind of situation? Tools like `aws-nuke` (https://github.com/rebuy-de/aws-nuke) are awesome (!) to clean up unused resources, but frankly they should not be necessary in the first place... reply datadrivenangel 17 hours agoprevIf you're spending $425k per year on non-production AWS resources, you have an interesting setup. reply ponector 16 hours agoparentIn one project we had testing setup which costs 600k USD allually. It was three times more expensive than production setup we had for product which was more than 3 year old. Nothing special, just mongo and Kafka with enormous size. If you run automation tests Manu times per day but do not clean anything - you'll get mongo with terabytes of test data. And then, on top there was elastic search which multiply bills. reply qaq 17 hours agoparentprevDev env. for complicated products can be fairly involved and large companies might have a lot of them. reply pixl97 16 hours agorootparentIf you're dev doesn't look like production, then you're not testing in reality. reply tuananh 17 hours agoparentprevhave you met SAP? :) reply kevin_nisbet 16 hours agoprevI've for a long time set my cloud VMs to shutdown on idle. I usually use it to also justify running a much larger VM to cut down on build and test times. Just set a cron to run the shutdown command with a grace period. And then if you're working late, you just cancel the shutdown and the shutdown will be retried in a couple of hours. And have a script or command to just run the cloud API calls to boot the VM in the morning / when needed, and the environment boots in a minute or two. For other stuff I've been tempted to do a more complicated setup, with something like a micro-vm as a proxy, that will do the shutdown / activation on TCP connection, but haven't gotten around to it. reply mkl95 17 hours agoprevSounds like a great way to find out who's working late or is an early bird reply StratusBen 16 hours agoprevDisclosure: I'm CEO of https://www.vantage.sh/ -- a cloud cost observability platform. I previously worked at both AWS and DigitalOcean. For people looking for how to save money on AWS - I'd [selfishly] recommend connecting up to Vantage. We profile AWS for all sorts of savings and give you the information on how much we can save prior to you paying us. It can be a good gut-check if nothing else on how well optimized you are. reply pizzafeelsright 15 hours agoparentSilly request. The amount of paperwork, lawyers, and time required to connect our service makes it impossible to validate savings. Is there an offline method? I have not looked at vantage to see if it's possible. reply StratusBen 14 hours agorootparentUnfortunately we don't have an option for that route -- but we're happy to help support any paperwork for getting things up and running if you contact me or my team: ben [at] vantage [dot] sh. reply brycewray 16 hours agoprevIn a similar vein: https://usefathom.com/blog/reduce-aws-bill reply tehlike 16 hours agoprevCutting down AWS cost by 90% by simply moving to hetzner. reply cbg0 16 hours agoparentIf you're running a small setup and don't need any value add products or multi-AZ/multi-region this might work, but Hetzner and major cloud providers are by no means comparable. Hetzner offers a 99.9% uptime guarantee only on their network. AWS has SLAs for every product offering - EC2 for example starts paying out credits if they fall below 99.99% uptime. If you're a user of various managed cloud products, these will cost quite a bit to replicate on Hetzner and you'll be spending money on personnel to build these out and maintain them instead of just paying for the cloud product on AWS/GCP/Azure. reply tehlike 15 hours agorootparentManaged is good, but opensource is decent these days. You need postgres? Use crunchydata postgres operator or cloudnativepg. Need multiple regions? setup wireguard. IT's more work, but might not be a lot of work. reply bdcravens 16 hours agoparentprevWas doing some research this weekend on cloud exit. Hetzner is attractive, but our company is pretty much limited to the US (no international companies due to our current business model). How practical would it be? Also, I've seen a lot of concern over blocked IPs, especially for lower-cost hosts. Is that an issue with Hetzner? reply tehlike 15 hours agorootparentHetzner cloud supports us-west and us-east. reply bdcravens 15 hours agorootparentI was looking at dedicated hardware. If I decide to stay in anyone's cloud, I'd stick with AWS. reply tehlike 14 hours agorootparentYeah Hetzner doesn't have dedicated in the US, and not sure if it will in near/mid future. Still, Hetzner cloud is pretty good option, and there's more support coming on building on Hetzner. https://www.ubicloud.com/ (from founders of citus) is mainly/currently targeting hetzner, for example. reply nvm0n2 11 hours agorootparentprevThere's alternative companies with similar offers, like Deft: https://deft.com/dedicated-servers/ reply tehlike 8 hours agorootparentI imagine one reason I didn't hear about them before is that they don't seem to offer self service. I absolutely don't want to talk to anyone when I am buying object storage, cloud vms, or even dedicated. reply throwitaway222 15 hours agoprevIt looks like this is surfacing because a lot of companies are laying off but also finding every possible way to save on costs right now. reply from-nibly 16 hours agoprevReacting to events to install security defaults (or any kind of defaults) sounds really error prone. Are people running AWS where devs just click buttons in aws and spin up random stuff? I thought we all decided that was dumb and switched to gitops/terraform? Did I miss a new trend or something? reply bornfreddy 14 hours agoprevDoes anyone have a good experience with tools / services that track and analyze cloud usage? We don't use any, but could benefit from better visibility in spending patterns. reply octopoc 15 hours agoprevHere's a startup idea: a profiler for infrastructure-as-code that shows how much each line of code is costing per month, instead of showing where the CPU spends most of its time. reply msmith 14 hours agoparenthttps://www.infracost.io/ might do what you're imagining reply stusmall 14 hours agoparentprevHave you looked at OpenCost? It's more k8s focused but of a similar idea reply TheIronMark 14 hours agoprevI did this at a previous employer. I leveraged a Lambda functions and tags applied to instances to determine when they should be and when they should be off. reply partiallypro 15 hours agoprevI saved my previous company $4000-5000/mo on AWS billing just by auditing the AWS account and turning off unused machines that that old devs has spun up and deleting hard drives after backing them up to S3, just in case. No one had really even bothered looking at it for years and I did it in my \"free time\" at work without being tasked with it. Ironically, I asked for a raise a year later and was denied, despite single handedly saving the company nearly $50000/yr. The raise I asked for wasn't close the cost savings I had brought. I left the company shortly after. I saw someone else have a similar experience here and a comment to it was saying rewarding this produces a bad incentive...well, honestly why would I have even bothered cutting costs if I felt I wouldn't be rewarded? Not rewarding it just makes me half regret doing it at all. reply nodesocket 15 hours agoprevI’ve mentioned this before, but probably one of the most egregious costs on AWS are NAT gateways and NAT bandwidth pricing. Typically I deploy one NAT gateway per AZ so looking at $99 a month just for three NAT gateway instances with zero traffic. reply ralfcheung 15 hours agoprevmeanwhile, I'm saving the company 7-digit/year from Google. reply hkt 17 hours agoprevSmall fry, I saved a public sector body $1m/year by doing this and rightsizing the kubernetes hosts. :D reply sremani 17 hours agoprevCloud optimization is the next Kubernetes. reply hkt 17 hours agoparentGoodness I hope so, that'll be easy money reply bdcravens 16 hours agorootparentnext [–]reply QuinnyPig 8 hours agorootparentI HAVE BEEN SUMMONED reply zikduruqe 17 hours agoprevShit... I reduced our spend on one AWS account from $270K a month to $75K a month. reply ponector 16 hours agoparentGreat! Shareholders will be happy. Did you receive any bonus? reply Chico75 15 hours agorootparentThe problem is always around abuse. If it becomes known that you can get a big bonus by wasting a lot of money on useless infra first and then reducing it, other people will start playing the game. How do you reward cloud cost awareness without creating perverse incentives? reply nvm0n2 11 hours agorootparentIt's always the same answer: managers who pay attention to the details. People familiar with your work should be able to tell if you're gaming the system or not. reply marvin 16 hours agorootparentprevIt's a good shoo-in for becoming a freelance consultant charging a percentage of annual savings rather than hourly time billed. reply zikduruqe 13 hours agorootparentprevHahaha, for doing my job? I wish. reply zen928 13 hours agorootparentprevWill they also be paying what they owe on the added costs that could have been noticed earlier with due diligence? I'm imagining that what they'll receive instead is the compensation expected and agreed upon by both parties negotiated during either initial hiring or the multiple points in the year that allow for easy communication about changing payroll expectations, instead of hawking for dimes at first sight. reply davidgerard 14 hours agoprevWe do this. It was a bit faffy to set up, but having dev and staging shut down at night and only be started as needed that day saved us a fortune. reply gnarlouse 16 hours agoprev [–] “Have you tried turning it off and then turning it back on again?” reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author emphasizes the significance of cloud cost optimization and presents a solution utilizing Lambda, Step Functions, and EventBridge.",
      "The system automates tasks such as shutting down resources when not in use, implementing security policies, and auto-shutdown of resources.",
      "The solution claims to have achieved a 70% decrease in non-production costs, with projected savings of $150k USD annually. Future plans include addressing more complex workloads like Kubernetes."
    ],
    "commentSummary": [
      "The discussion focuses on cost management in cloud infrastructure, software development, and engineering.",
      "Participants share their experiences and insights into reducing costs and navigating billing transparency challenges.",
      "Recommendations are made for different strategies, tools, and approaches to effectively manage expenses in cloud services."
    ],
    "points": 169,
    "commentCount": 186,
    "retryCount": 0,
    "time": 1705941237
  },
  {
    "id": 39098780,
    "title": "Allegations of Fraud and Data Manipulation Rock Harvard's Dana-Farber Cancer Institute",
    "originLink": "https://forbetterscience.com/2024/01/02/dana-farberications-at-harvard-university/",
    "originBody": "Guest post University Affairs Dana-Farberications at Harvard University \"Imagine what mistakes might be found in the raw data if anyone was allowed to look!\" - Sholto David by Sholto David January 2, 2024 Comments 76 If you ever wondered why the progress in cancer research is so slow despite billions being invested into basic research – Sholto David will show you why. Forget Asian papermills. Forget Sicilian universities. They are just emulating what the most prestigious US research institutions teach. And you can’t get any more prestigious than Harvard and its Dana Farber Cancer Institute. Memorial Sloan Kettering Paper Mill “Why do successful and apparently intelligent surgeons feel the need to play pretend at biology research? Has Sam S. Yoon ever performed an invasion or migration assay? […] if this is how he “supervises” his research does anyone trust his supervision of surgery?” – Sholto David by Sholto David November 1, 2023October 27, 2023 Well, the level of data forgery is pathetically amateurish and excessive. So much that we could only include a fraction of it into this article, you will have to search Ken Anderson, Bill Hahn and other protagonists on PubPeer to see how bad it all really is. In fact, it is worse, because we only see the tiny tip of the fraud iceberg – image data duplications, the last resort of a failed scientist after every other trick failed to provide the desired result. Billions of dollars were burned for this cancerous trash science, but it made many academic careers, some got very rich, and entire dynasties established themselves at Dana Farber. None of Dana Farber researchers mentioned in this article replied when asked to comment on their PubPeer records. Dana-Farberications at Harvard University By Sholto David As the whole world furiously argues over whether the president of Harvard did or didn’t use some quotation marks in the right place, scientists at the affiliated Dana-Farber Cancer Institute (DFCI) must breathe a sigh of relief, no one has bothered to critically read their research in years! Far worse skeletons than plagiarism lurk in the archives… Laurie H. Glimcher Starting with Laurie H. Glimcher, who is the President and CEO of the DFCI. No doubt Laurie built her career on papers like this one, in Nature Immunology (2003), which includes some impressive contributions to art, but perhaps not to science. Neal N. Iwakoshi, Ann-Hwee Lee, Prasanth Vallabhajosyula, Kevin L. Otipoby, Klaus Rajewsky, Laurie H. Glimcher Plasma cell differentiation and the unfolded protein response intersect at the transcription factor XBP-1 Nature Immunology (2003) doi: 10.1038/ni907 Figure 1a: The obvious and ugly splices are the lesser of my concerns here, more troubling are the copied and pasted bands. Figure 1f: Actg bands have been copied and pasted, but these should show samples from different time points. Maybe the coauthor Klaus Rajewsky could issue a correction blaming the updated western blot hardware this time (as opposed to “updated FlowJo software“). Academic Dynasties: meet the Nussenzweigs Star scientists Michel and Andre Nussenzweig come from a famous family of immunologists. Clare Francis looked at some of their papers. by Leonid Schneider September 15, 2020 Laurie has a curious attitude to correcting her work, although she has replied to one of my less critical comments, she neglected to respond to the above issue (despite prompting); and for the following duplicated flow cytometry plots, she provided this dismissive response: “You should reach out to Dr. Hidde Ploegh- the first author, Dr. Boaz Tirosh was in his laboratory.” Well Laurie, it’s your paper, if you care about it being correct, you could very well reach out yourself! Boaz Tirosh, Neal N. Iwakoshi, Laurie H. Glimcher, Hidde L. Ploegh XBP-1 specifically promotes IgM synthesis and secretion, but is dispensable for degradation of glycoproteins in primary B cells The Journal of Experimental Medicine (2005) doi: 10.1084/jem.20050575 Described on PubPeer as “more similar than expected” the plots in fact share all of the datapoints, at least one must be wrong. Laurie co-authored several papers with Claudio Hetz. This “star neuroscientist” was investigated by his university, which released a damning report, accusing Hetz of recklessness, negligence, and a problematic attitude to research ethics. Hetz simply expressed regret that he didn’t have access to a better version of Photoshop to more convincingly manipulate his images. The Claudio Hetz Blues “…Dr. Hetz seems rather to regret that he did not have better tools for editing the figures, so that the undeclared interventions would have gone unnoticed.” – University of Chile investigative report. by Leonid Schneider September 20, 2021September 20, 2021 In February 2020 Laurie claimed on PubPeer that she had not been made aware of what happened with Hetz’s papers (perhaps she should subscribe to For Better Science email alerts!). In any case this discovery did not prompt her into action, by 2023, only one of the four papers tagged on PubPeer with her and Hetz as co-authors has received a reply from her, and none have been corrected or retracted. For example, this undergraduate level error of miscounted western blot lanes remains in the scientific record, without comment or correction, in the prestigious journal Science, no less: Claudio Hetz, Paula Bernasconi, Jill Fisher, Ann-Hwee Lee, Michael C. Bassik , Bruno Antonsson, Gabriel S. Brandt, Neal N. Iwakoshi, Anna Schinzel, Laurie H. Glimcher, Stanley J. Korsmeyer Proapoptotic BAX and BAK modulate the unfolded protein response by a direct interaction with IRE1alpha Science (2006) doi: 10.1126/science.1123480 There should be ten lanes, however the loading controls have eleven, and the P-JNK has nine. The above paper has generated a lengthy PubPeer thread, discussing multiple splices in the supplementary figures. William C. Hahn (Bill) The very next entry on the DFCI leadership page is Laurie’s deputy, William C. Hahn who is the Executive Vice President and Chief Operating Officer, as well as a member of the Broad Institute of MIT and Harvard. While Laurie has a few troubling papers and a lamentable attitude to rectifying these errors, Bill has a far more impressive PubPeer record, including 30 search results, with eighteen pertaining to problematic images. Bill is the former postdoc of Robert Weinberg, owner of an impressive PubPeer record too. The wizard men curing breast cancer A breast cancer foundation celebrates its research heroes. Read now here about how great US scientists from Harvard, MIT, Weill Cornell and MD Anderson cure cancer. by Leonid Schneider January 20, 2020May 15, 2021 Bill contributed this calamitous effort to the journal Cancer Research in 2005. Wen Chen, Jason D. Arroyo, Jamie C. Timmons, Richard Possemato, William C. Hahn Cancer-Associated PP2A Aα Subunits Induce Functional Haploinsufficiency and Tumorigenicity Cancer Research (2005) doi: 10.1158/0008-5472.can-05-1103 The images in the paper are sufficiently high quality to be quite certain that photoshop was involved. Most figures showing western blots in this paper are afflicted. I would encourage anyone interested to look carefully at the blots, the edges very clearly show that these are not merely bands with similar shapes, but the same bands copied and pasted. The Sex Privileges of mTORman David Sabatini “The Plaintiff is Professor Sabatini […] the self-described powerful senior scientist, who had demanded sex of her when she was a graduate student ending her studies and about to start a fellowship at the Whitehead, in a program Sabatini would direct. […] And it is the man who had made it clear – throughout her… by Leonid Schneider November 3, 2021May 20, 2022 The stench of the sacked MIT professor David M. Sabatini is never far from these discussions. Bill co-authored a 2006 paper in Cell with Sabatini, and also with MIT’s Eric Lander, who was previously Joe Biden’s top science adviser, but resigned following reports of bullying and misogyny. Jason Moffat, Dorre A. Grueneberg, Xiaoping Yang, So Young Kim, Angela M. Kloepfer, Gregory Hinkle, Bruno Piqani, Thomas M. Eisenhaure, Biao Luo, Jennifer K. Grenier, Anne E. Carpenter, Shi Yin Foo, Sheila A. Stewart, Brent R. Stockwell, Nir Hacohen, William C. Hahn, Eric S. Lander, David M. Sabatini, David E. Root A Lentiviral RNAi Library for Human and Mouse Genes Applied to an Arrayed Viral High-Content Screen Cell (2006) doi: 10.1016/j.cell.2006.01.040 Figure 6B: What did the authors need to hide in the pH3 blot? With decreased brightness to make the similarity more obvious. Bill’s PhD student Richard Possemato, also an author of the previously mentioned paper, worked at MIT with Sabatini and is now professor at New York University. The bands in yellow boxes are nearly pixel-perfect identical on close inspection. Wen Chen, Richard Possemato, K Thirza Campbell, Courtney A Plattner, David C Pallas, William C Hahn Identification of specific PP2A complexes involved in human cell transformation Cancer Cell (2004) doi: 10.1016/s1535-6108(04)00026-1 We can quickly complete the Cell, Nature, Science trifecta for this blog post with another collaboration between Bill and Sabatini published in Nature which includes duplicated flow cytometry histograms, sensibly tucked away in the supplementary data. James E. Bradner (formerly executive committee member of Novartis, and just last week announced as the new chief scientific officer of Amgen) earns another mention on For Better Science. Scott R. Floyd, Michael E. Pacold, Qiuying Huang, Scott M. Clarke, Fred C. Lam, Ian G. Cannell, Bryan D. Bryson, Jonathan Rameseder, Michael J. Lee, Emily J. Blake, Anna Fydrych, Richard Ho, Benjamin A. Greenberger, Grace C. Chen, Amanda Maffa, Amanda M. Del Rosario, David E. Root, Anne E. Carpenter, William C. Hahn, David M. Sabatini, Clark C. Chen, Forest M. White, James E. Bradner, Michael B. Yaffe The bromodomain protein Brd4 insulates chromatin from DNA damage signalling Nature (2013) doi: 10.1038/nature12147 Supplemental Fig. S1c – more similar than expected. Also part of Supplemental Fig. S1c This is not the only duplication of flow cytometry histograms published by Bill; another was previously identified by Elisabeth Bik: Sheila A Stewart, Derek M Dykxhoorn, Deborah Palliser, Hana Mizuno, Evan Y Yu, Dong Sung An, David M Sabatini, Irvin S Y Chen, William C Hahn, Phillip A Sharp, Robert A Weinberg, Carl D Novina Lentivirus-delivered stable gene silencing by RNAi in primary cells RNA (2003) doi: 10.1261/rna.2192803 Bill apparently collects collaborations with longevity weirdos, including this paper with another obnoxious David: the Harvard professor David A. Sinclair. Something was clumsily hidden by cloning a rectangular section from a nearby lane. What was concealed? We will likely never know, but at least one of the authors must have felt it was important enough to hide. Ron Firestein, Gil Blander, Shaday Michan, Philipp Oberdoerffer, Shuji Ogino, Jennifer Campbell, Anupama Bhimavarapu, Sandra Luikenhuis, Rafael De Cabo, Charles Fuchs, William C. Hahn, Leonard P. Guarente, David A. Sinclair The SIRT1 deacetylase suppresses intestinal tumorigenesis and colon cancer growth PLoS ONE (2008) doi: 10.1371/journal.pone.0002020 False colour added to PubPeer, the original and an animation are available on the thread. This finding was previously mentioned on For Better Science when discussing the work of Leonard Guarente, Sinclair’s mentor: The original sins of Leonard Guarente “Without specific and credible allegations of research misconduct, MIT is unable to take any action.” by Leonid Schneider April 17, 2023April 17, 2023 In his quest to befriend anti-aging cheaters, Bill has also travelled to Germany, where he collaborated with the shooting star Karl Lenhard Rudolph (who later became the nation’s most famous zombie scientist). Karl Lenhard Rudolph barred from DFG funding for 2 years, as supportive peers flock to his conference More bad news for Karl Lenhard Rudolph: misconduct findings and 2 year funding ban from DFG. Good news: the international scientific community stands to him in solidarity, no retractions, even some corrections won’t be implemented. by Leonid Schneider December 15, 2017May 23, 2019 The following paper stems from Rudolph’s time at Hannover Medical School, and it was therefore not part of the 2016 misconduct investigation by the Leibniz Association. The penultimate author is Hannover Medical School’s current president, Michael P. Manns. A Satyanarayana, R A Greenberg, S Schaetzlein, J Buer, K Masutomi, W C Hahn, S Zimmermann, U Martens, M P Manns, K L Rudolph Mitogen stimulation cooperates with telomere shortening to activate DNA damage responses and senescence signaling Molecular and Cellular Biology (2004) doi: 10.1128/mcb.24.12.5459-5474.2004 Reading of experimental details required: On the right, these are not the same samples imaged sequentially, but should have been derived from different animals. Bill works with lots of prestigious collaborators, Massimo Loda is the current Editor-in-Chief of Molecular Cancer Research, an AACR journal (a publisher you can find frequently in this blog post). AACR deploys Massimo Loda on a mission to cure cancer Meet Massimo Loda, the new EiC of Molecular Cancer Research. Not despite, but because of his PubPeer record. Also meet his partner Michele Pagano! by Leonid Schneider October 13, 2021October 13, 2021 Raanan Berger, Phillip G. Febbo, Pradip K. Majumder, Jean J. Zhao, Shayan Mukherjee, Sabina Signoretti, K. Thirza Campbell, William R. Sellers, Thomas M. Roberts, Massimo Loda, Todd R. Golub, William C. Hahn Androgen-induced differentiation and tumorigenicity of human prostate epithelial cells Cancer Research (2004) doi: 10.1158/0008-5472.can-04-2938 Rotation required Worth mentioning that Thomas M. Roberts is another DFCI researcher and Harvard professor, and so is his former postdoc Jean Zhao; they have a PubPeer record of their own, with and without Bill. Here is Bill with another DFCI colleague, James DeCaprio: Anna A. Sablina, Wen Chen, Jason D. Arroyo, Laura Corral, Melissa Hector, Sara E. Bulmer, James A. DeCaprio, William C. Hahn The tumor suppressor PP2A Abeta regulates the RalA GTPase Cell (2007) doi: 10.1016/j.cell.2007.03.047 Figure 4A: There is a difference in intensity between the blue highlighted blots, but the noise is in the same place, also a slight difference in stretch in the red bands but there is another giveaway piece of noise. In 2020, DeCaprio corrected a Nature Cell Biology paper Park et al 2020 because “the western blot image for VINC in Fig. 3g was a duplicate of that for LSD1.” So duplicated blots should be corrected? Plenty of work to be done in that regard! Another impressive titan of industry, Bill published with current Head of Global Product Development and Chief Medical Officer of Roche, Levi Garraway in 2011. Nikhil Wagle, Caroline Emery, Michael F. Berger, Matthew J. Davis, Allison Sawyer, Panisa Pochanard, Sarah M. Kehoe, Cory M. Johannessen, Laura E. MacConaill, William C. Hahn, Matthew Meyerson, Levi A. Garraway Dissecting therapeutic resistance to RAF inhibition in melanoma by tumor genomic profiling Journal of Clinical Oncology (2011) doi: 10.1200/jco.2010.33.2312 Figure 3C. But don’t rush to alert Roche, their former Global Head of Pharma Research and Early Development was John C Reed, who was previously at Sanofi, and is now at Johnson & Johnson. Good to know that in Big Pharma research is in such safe hands. Sanofi R&D Head John Reed knows how to science In 2018, the pharma giant Sanofi appointed with John Reed a new R&D head. Apparently Sanofi does not believe in PubPeer. by Leonid Schneider August 29, 2019June 8, 2022 More troubling western blots in motion at DFCI, with Bill as the penultimate author. Hailing Cheng, Pixu Liu, Zhigang C Wang, Lihua Zou, Stephanie Santiago, Victoria Garbitt, Ole V Gjoerup, J Dirk Iglehart, Alexander Miron, Andrea L Richardson, William C Hahn, Jean J Zhao SIK1 couples LKB1 to p53-dependent anoikis and suppresses metastasis Science Signaling (2009) doi: 10.1126/scisignal.2000369 And finally for Bill, another paper in Nature, with Bob Weinberg, containing multiple splices, and a mirrored band (I also think the migration of the Large-T bands seems unusual). This paper has over 3000 citations… So probably nothing urgent here for Nature to address? William C. Hahn, Christopher M. Counter, Ante S. Lundberg, Roderick L. Beijersbergen, Mary W. Brooks, Robert A. Weinberg Creation of human tumour cells with defined genetic elements Nature (1999) doi: 10.1038/22780 Irene M. Ghobrial Moving on to Irene M. Ghobrial, who is the Senior Vice President for Experimental Medicine at DFCI. Irene works on multiple myeloma, frequently publishing with her DFCI colleagues Kenneth C. Anderson, Nikhil C. Munshi, Teru Hideshima, Ruben Carrasco, and Yu-Tzu Tai. What kind of research has elevated Irene to her current position? In 2012 this epic fail was published, Irene gained ethical approval to aspirate bone marrow from cancer patients and healthy volunteers, but Figure 1 includes three obvious duplications. Feda Azab, Abdel Kareem Azab, Patricia Maiso, Teresa Calimeri, Ludmila Flores, Yang Liu, Phong Quang, Aldo M. Roccaro, Antonio Sacco, Hai T. Ngo, Yong Zhang, Brittany L. Morgan, Ruben D. Carrasco, Irene M. Ghobrial Eph-B2/ephrin-B2 interaction plays a major role in the adhesion and proliferation of Waldenstrom’s macroglobulinemia Clinical Cancer Research (2012) doi: 10.1158/1078-0432.ccr-11-0111 Different intensity, but certainly the same sample Two further overlapping areas. Wouldn’t the volunteers be furious after undergoing such an invasive procedure? Can the data from a paper like this ever be rehabilitated? Just like hapless Chinese papermills, Harvard scientists struggle with images of crystal violet stained cells. Perhaps this method is just cursed? Paola Neri, Li Ren, Abdel Kareem Azab, Matthew Brentnall, Kathy Gratton, Alexander C. Klimowicz, Charles Lin, Peter Duggan, Pierfrancesco Tassone, Adnan Mansoor, Douglas A. Stewart, Lawrence H. Boise, Irene M. Ghobrial, Nizar J. Bahlis Integrin β7-mediated regulation of multiple myeloma cell adhesion, migration, and invasion Blood (2011) doi: 10.1182/blood-2010-06-292243 Another blunder in troublesome migration and invasion experiments was published with John Crown, an Irish politician and member of the 24th Seanad at the time this paper was submitted. Claire Corcoran, Sweta Rani, Susan Breslin, Martina Gogarty, Irene M Ghobrial, John Crown, Lorraine O’Driscoll miR-630 targets IGF1R to regulate response to HER-targeting drugs and overall cancer cell progression in HER2 over-expressing breast cancer Molecular Cancer (2014) doi: 10.1186/1476-4598-13-71 Like her colleagues mentioned above, Irene also has trouble with counting, how many Harvard researchers does it take to count to seven I wonder? Alissa Huston, Xavier Leleu, Xiaoying Jia, Anne-Sophie Moreau, Hai T. Ngo, Judith Runnels, Judy Anderson, Yazan Alsayed, Aldo Roccaro, Sonia Vallet, Evdoxia Hatjiharissi, Yu-Tsu Tai, Peter Sportelli, Nikhil Munshi, Paul Richardson, Teru Hideshima, David G. Roodman, Kenneth C. Anderson, Irene M. Ghobrial Targeting Akt and Heat Shock Protein 90 Produces Synergistic Multiple Myeloma Cell Cytotoxicity in the Bone Marrow Microenvironment Clinical Cancer Research (2008) doi: 10.1158/1078-0432.ccr-07-1299 As with Bill, there are plenty of western blot duplications to enjoy, too. Abdel Kareem Azab, Feda Azab, Simona Blotta, Costas M. Pitsillides, Brian Thompson, Judith M. Runnels, Aldo M. Roccaro, Hai T. Ngo, Molly R. Melhem, Antonio Sacco, Xiaoying Jia, Kenneth C. Anderson, Charles P. Lin, Barrett J. Rollins, Irene M. Ghobrial RhoA and Rac1 GTPases play major and differential roles in stromal cell-derived factor-1-induced cell adhesion and chemotaxis in multiple myeloma Blood (2009) doi: 10.1182/blood-2009-01-199281 Irene says she will “review” this. Ruben Carrasco is another DFCI scientist and frequent collaborator of Irene’s. He is listed as a former postdoc of Ronald DePinho (who caused a financial catastrophe as President of MD Anderson in Texas). Read more on Ron here: Anil Sood and other questionable stars of MD Anderson The MD Anderson Cancer Center, part of the University of Texas and located in Houston, is a giant hub of huge cancer research money, even for US standards. They also do a lot of science there, which only purpose seems to be publishing in big journals in order to generate even more money. If there… by Leonid Schneider July 17, 2017May 15, 2021 Abdel Kareem Azab, Phong Quang, Feda Azab, Costas Pitsillides, Brian Thompson, Triona Chonghaile, John T Patton, Patricia Maiso, Val Monrose, Antonio Sacco, Hai T Ngo, Ludmila M Flores, Charles P Lin, John L Magnani, Andrew L Kung, Anthony Letai, Ruben Carrasco, Aldo M Roccaro, Irene M Ghobrial P-selectin glycoprotein ligand regulates the interaction of multiple myeloma cells with the bone marrow microenvironment Blood (2012) doi: 10.1182/blood-2011-07-368050 Perhaps one can only learn bad science from DePinho, who was the postdoctoral adviser for Ned Sharpless, subject of my last blog post. Sharpless Ned, or how half a mouse died “The President’s goal of ending cancer as we know it today is grounded, in part, in the work of scientific discovery that Ned Sharpless has led at NCI” by Sholto David December 11, 2023December 11, 2023 Who could be to blame for this supplemental silliness? The splice, which the authors did not try to hide, is hard to justify in the first place, never mind that loading controls were simply borrowed from the above blot. Michal Bar‐Natan, Dina Stroopinsky, Katarina Luptakova, Maxwell D. Coll, Arie Apel, Hasan Rajabi, Athalia R. Pyzer, Kristen Palmer, Michaela R. Reagan, Myrna R. Nahas, Rebecca Karp Leaf, Salvia Jain, Jon Arnason, Irene M. Ghobrial, Kenneth C. Anderson, Donald Kufe, Jacalyn Rosenblatt, David Avigan Bone marrow stroma protects myeloma cells from cytotoxic damage via induction of the oncoprotein MUC1 British Journal of Haematology (2017) doi: 10.1111/bjh.14493 There is a difference in vertical stretch, but these are certainly the same bands. Irene’s Italian mentee, Aldo Roccaro, previously worked at the DFCI and has subsequently returned to Italy, where he now a clinical research director at the hospital in Brescia, and a member of the ethics commission. Aldo M Roccaro, Antonio Sacco, Xiaojing Jia, Ranjit Banwait, Patricia Maiso, Feda Azab, Ludmila Flores, Salomon Manier, Abdel Kareem Azab, Irene M Ghobrial Mechanisms of activity of the TORC1 inhibitor everolimus in Waldenstrom macroglobulinemia Clinical Cancer Research (2012) doi: 10.1158/1078-0432.ccr-12-1532 Perhaps the reuse of the control here wouldn’t have been so concerning, but the concentrations of RAD001 do not match up! I am also concerned about images of mice. In the figure below, a group of mice has made a knight move across the page, and transitioned between groups. Thinking about what Irene’s team may have wished to demonstrate (lack of cancer spread in their favoured group) this appears to be a very convenient error. Yu Zhang, Michele Moschetta, Daisy Huynh, Yu-Tzu Tai, Yong Zhang, Wenjing Zhang, Yuji Mishima, Jennifer E. Ring, Winnie F. Tam, Qunli Xu, Patricia Maiso, Michaela Reagan, Ilyas Sahin, Antonio Sacco, Salomon Manier, Yosra Aljawai, Siobhan Glavey, Nikhil C. Munshi, Kenneth C. Anderson, Jonathan Pachter, Aldo M. Roccaro, Irene M. Ghobrial Pyk2 promotes tumor progression in multiple myeloma Blood (2014) doi: 10.1182/blood-2014-03-563981 Another problematic murine figure is shown below. This appears to be nearly entirely fabricated by taking repeated scans of a few mice and labelling them as showing a range of different timepoints and treatment conditions. Look carefully at the positions of the tail, feet, fur, and elements of noise around the mice. Abdel Kareem Azab, Judith M. Runnels, Costas Pitsillides, Anne-Sophie Moreau, Feda Azab, Xavier Leleu, Xiaoying Jia, Renee Wright, Beatriz Ospina, Alicia L. Carlson, Clemens Alt, Nicholas Burwick, Aldo M. Roccaro, Hai T. Ngo, Mena Farag, Molly R. Melhem, Antonio Sacco, Nikhil C. Munshi, Teru Hideshima, Barrett J. Rollins, Kenneth C. Anderson, Andrew L. Kung, Charles P. Lin, Irene M. Ghobrial CXCR4 inhibitor AMD3100 disrupts the interaction of multiple myeloma cells with the bone marrow microenvironment and enhances their sensitivity to therapy Blood (2009) doi: 10.1182/blood-2008-10-186668 Kenneth C. Anderson (Ken) As mentioned previously, Irene frequently publishes with Kenneth C. Anderson (you may have noticed above). Ken is also a senior DFCI researcher. Perhaps we can imagine them as Ken and Barbie? Testing out my own photoshop skills! Much to learn from the masters. Quite apart from his collaborations with Irene, Ken has an extensive record of problematic papers without Irene. These have been tagged on PubPeer over the last ten years. Perhaps Ken has rotten luck in choosing his collaborators? Here Ken teamed up with the previously mentioned Ron DePinho and Ruben Carrasco for an artistic interpretation of western blotting his CEO Laurie might be impressed by. Daniel R. Carrasco, Kumar Sukhdeo, Marina Protopopova, Raktim Sinha, Miriam Enos, Daniel E. Carrasco, Mei Zheng, Mala Mani, Joel Henderson, Geraldine S. Pinkus, Nikhil Munshi, James Horner, Elena V. Ivanova, Alexei Protopopov, Kenneth C. Anderson, Giovanni Tonon, Ronald A. DePinho The differentiation and stress response factor XBP-1 drives multiple myeloma pathogenesis Cancer Cell (2007) doi: 10.1016/j.ccr.2007.02.015 Ruben used his middle name “Daniel” at times. This has been double-checked. How about this muddle in Nature Medicine, also with Ruben Carrasco: Di Zhu, Zhongqiu Wang, Jian-Jun Zhao, Teresa Calimeri, Jiang Meng, Teru Hideshima, Mariateresa Fulciniti, Yue Kang, Scott B Ficarro, Yu-Tzu Tai, Zachary Hunter, Douglas McMilin, Haoxuan Tong, Constantine S Mitsiades, Catherine J Wu, Steven P Treon, David M Dorfman, Geraldine Pinkus, Nikhil C Munshi, Pierfrancesco Tassone, Jarrod A Marto, Kenneth C Anderson, Ruben D Carrasco The Cyclophilin A-CD147 complex promotes the proliferation and homing of multiple myeloma cells Nature Medicine (2015) doi: 10.1038/nm.3867 Figure 5i Supplementary Figure 3 A final mix-up with Ruben Carrasco, who is associate professor in Harvard with his own lab at DFCI. Masahisa Jinushi, Matthew Vanneman, Nikhil C. Munshi, Yu-Tzu Tai, Rao H. Prabhala, Jerome Ritz, Donna Neuberg, Kenneth C. Anderson, Daniel Ruben Carrasco, Glenn Dranoff MHC class I chain-related protein A antibodies and shedding are associated with the progression of multiple myeloma Proceedings of the National Academy of Sciences (2008) doi: 10.1073/pnas.0711293105 Another unlucky choice in scientific collaborators? Ken teamed up with Harvard’s and For Better Science celebrity Carl Ronald Kahn. Yu-Tzu Tai, Klaus Podar, Laurence Catley, Yu-Hua Tseng, Masaharu Akiyama, Reshma Shringarpure, Renate Burger, Teru Hideshima, Dharminder Chauhan, Nicholas Mitsiades, Paul Richardson, Nikhil C Munshi, C Ronald Kahn, Constantine Mitsiades, Kenneth C Anderson Insulin-like growth factor-1 induces adhesion and migration in human multiple myeloma cells via activation of beta1-integrin and phosphatidylinositol 3′-kinase/AKT signaling Cancer Research (2003) pubmed: 14522909 C Ronald Kahn and The Problem of Irreproducible Bioscience Research “not everyone in the research community accepts that the problem requires such attention; some believe it is overblown.” -Jeffrey Flier, emeritus dean of Harvard Medical School by Leonid Schneider September 21, 2022September 21, 2022 James E. Bradner earns another mention for his collaboration here with Ken. Perhaps it’s time for someone to look carefully at his work? He certainly seems to be caught in the crossfire rather a lot. Also put together with the help of DFCIs star oncologist Paul G. Richardson, described in 2022 as “a giant of cancer care in myeloma“. Teru Hideshima, Jun Qi, Ronald M. Paranal, Weiping Tang, Edward Greenberg, Nathan West, Meaghan E. Colling, Guillermina Estiu, Ralph Mazitschek, Jennifer A. Perry, Hiroto Ohguchi, Francesca Cottini, Naoya Mimura, Güllü Görgün, Yu-Tzu Tai, Paul G. Richardson, Ruben D. Carrasco, Olaf Wiest, Stuart L. Schreiber, Kenneth C. Anderson, James E. Bradner Discovery of selective small-molecule HDAC6 inhibitor for overcoming proteasome inhibitor resistance in multiple myeloma Proceedings of the National Academy of Sciences (2016) doi: 10.1073/pnas.1608067113 Figure 3B, another effort with James E. Bradner, who cares about loading controls? They’re the boring part, right? Klaus Podar, formerly of DFCI, and now a professor in Germany, contributed at least two photoshopped efforts with Ken. Marc S. Raab, Iris Breitkreutz, Giovanni Tonon, Jing Zhang, Patrick J. Hayden, Thu Nguyen, Johannes H. Fruehauf, Boris K. Lin, Dharminder Chauhan, Teru Hideshima, Nikhil C. Munshi, Kenneth C. Anderson, Klaus Podar Targeting PKC: a novel role for beta-catenin in ER stress and apoptotic signaling Blood (2009) doi: 10.1182/blood-2008-05-157040 Another one by Podar and another former postdoc of Ken’s, Yu-Tzu Tai (who is still at DFCI): Yu-Tzu Tai, Gerrard Teoh , Boris Lin, Faith E. Davies, Dharminder Chauhan, Steven P. Treon, Noopur Raje, Teru Hideshima, Yoshihito Shima, Klaus Podar, Kenneth C. Anderson Ku86 variant expression and function in multiple myeloma cells is associated with increased sensitivity to DNA damage The Journal of Immunology (2000) doi: 10.4049/jimmunol.165.11.6347 Large areas of “no signal” were copied into the Ku86 blot; So was there signal there originally? Or simply no data? Steven P. Treon, director of DFCI’s Bing Center for Waldenström’s Macroglobulinemia Research, is another frequent co-author. Guang Yang, Yangsheng Zhou, Xia Liu, Lian Xu, Yang Cao, Robert J. Manning, Christopher J. Patterson, Sara J. Buhrlage, Nathanael Gray, Yu-Tzu Tai, Kenneth C. Anderson, Zachary R. Hunter, Steven P. Treon A mutation in MYD88 (L265P) supports the survival of lymphoplasmacytic cells by activation of Bruton tyrosine kinase in Waldenström macroglobulinemia Blood (2013) doi: 10.1182/blood-2012-12-475111 Nicholas Mitsiades and Constantine Mitsiades are apparently brothers, but not clones (as these blots are). Dharminder Chauhan, Guilan Li, Daniel Auclair, Teru Hideshima, Paul Richardson, Klaus Podar, Nicholas Mitsiades, Constantine Mitsiades, Cheng Li, Ryung Suk Kim, Nikhil Munshi, Lan Bo Chen, Wing Wong, Kenneth C. Anderson Identification of genes regulated by 2-methoxyestradiol (2ME2) in multiple myeloma cells using oligonucleotide arrays Blood (2003) doi: 10.1182/blood-2002-10-3146 Constantine Mitsiades is now associate professor at DFCI, he and his brother have more on PubPeer. They also used to be collaborators of Germany’s untouchable top ophtalmologist Antonia Joussen (see the fabrications in Poulaki et al 2002 and Poulaki et al 2004). DFG decision: Antonia Joussen innocent victim of co-authors’ data manipulations “The committee […] requests of you for future publications to assess well ahead and to question critically your responsibility for the contributions of the co-authors, also for your own protection.” by Leonid Schneider March 1, 2017April 29, 2021 The following study was likely conducted in Japan, where DFCI’s Teru Hideshima obviously has contacts, but somehow his postdoctoral advisor Ken has wrangled himself the senior authorship spot. What was his contribution here I wonder? Masaharu Akiyama, Osamu Yamada, Teru Hideshima, Takaaki Yanagisawa, Kentaro Yokoi, Kohji Fujisawa, Yoshikatsu Eto, Hisashi Yamada, Kenneth C Anderson TNFalpha induces rapid activation and nuclear translocation of telomerase in human lymphocytes Biochemical and Biophysical Research Communications (2004) doi: 10.1016/j.bbrc.2004.02.080 Some of Ken’s associates seem curiously dependent on him for their own research career; DFCI program director and Harvard professor Nikhil C. Munshi is a good example (there are others) who appears to have almost no independent activity. Noopur Raje, Shaji Kumar, Teru Hideshima, Aldo Roccaro, Kenji Ishitsuka, Hiroshi Yasui, Norihiko Shiraishi, Dharminder Chauhan, Nikhil C. Munshi, Simon R. Green, Kenneth C. Anderson Seliciclib (CYC202 or R-roscovitine), a small-molecule cyclin-dependent kinase inhibitor, mediates activity via down-regulation of Mcl-1 in multiple myeloma Blood (2005) doi: 10.1182/blood-2005-01-0320 Around 2010 Ken retracted three papers in quick succession (Okawa et al 2008, Tai et al 2009, Gatt et al 2010), apparently because the cell lines were mixed up, and to be fair, it does seem like mixing things up is rather in Ken’s wheelhouse, but it does leave one wondering… Was an investigation into the affair ever held? Or were the retractions prompted by secret investigation? How much research funding can you waste on these blunders at DFCI without any consequences or any public comment? In conclusion: a swathe of research coming out of DFCI authored by the most senior researchers and managers appears to be hopelessly corrupt with errors that are obvious from just a cursory reading the papers. Imagine what mistakes might be found in the raw data if anyone was allowed to look! As it happened to the Dean of Harvard Medical School, George Q Daley: Lopez-Otin and Daley retract Nature Cell Biology paper The 2015 Nature Cell Biology paper by the Spanish cancer researcher Carlos Lopez-Otin and his US partner George Q Daley, stem cell titan and dean of Harvard Medical School, is being retracted. First author and Lopez-Otin’s student Clara Soria-Valles caused Daley even more trouble: her next groundbreaking paper was meant to be already published, but… by Leonid Schneider December 17, 2018January 5, 2019 As time marches on, methods have become more “genomical”, and in a lot of ways, harder to scrutinize. I had a go at replicating some of the gene expression analysis in Irene’s papers (for example, see here), and it doesn’t look hopeful, but it isn’t worth anyone’s time to write such comments if obvious errors in images are going to be left unmolested for years at a time. In 2017, DFCI’s own future Nobel Prize winner William G. Kaelin Jr wrote an opinion piece in Nature, lamenting the proliferation of papers focusing on “impact” rather than robustness, burying weak data in supplements, cherry picking, and other poor practices, he didn’t mention fraud, although there are still some interesting quotes. “The main question when reviewing a paper should be whether its conclusions are likely to be correct, not whether it would be important if it were true. Real advances are built with bricks, not straw. “ William G. Kaelin Jr Nature (2017) What exactly is the building material of choice at DFCI? Gregg Semenza: real Nobel Prize and unreal research data “Even after people have been telling you for, you know, 20 years or more that it’s going to happen, no one expects it.” -Gregg Semenza, Nobel Prize winner 2019 by Leonid Schneider October 7, 2020January 11, 2023 Credit should go to the anonymous and named commentors on PubPeer, Bill and Ken’s papers have been annotated over the last ten years, I presume frequently by Clare Francis. Some of the more recent comment’s especially on Irene’s papers were left by me, sometimes with the help of ImageTwin.ai. Other known contributors include Cheshire, and Elisabeth Bik. Written with some help from Leonid. Donate to Sholto David! Also, Sholto’s Patreon: https://www.patreon.com/SholtoDavid £5.00 Type your email… Subscribe to For Better Science One-Time Monthly I thank all my donors for supporting my journalism. You can be one of them! Make a one-time donation: Choose an amount €5.00 €10.00 €20.00 Or enter a custom amount € Your contribution is appreciated. Donate Share this: Tweet Pocket Telegram WhatsApp Print Email More Like Loading... TagsBob Weinberg • C Ronald Kahn • cancer research • Claudio Hetz • Constantine Mitsiades • data manipulation • David Sabatini • David Sinclair • Harvard • Irene Ghobrial • James Bradner • Jean Zhao • Karl Lenhard Rudolph • Kenneth Anderson • Klaus Podar • Laurie Glimcher • Leonard Guarente • Massimo Loda • Nicholas Mitsiades • Nikhil Munshi • Richard Possemato • Ronald DePinho • Ruben Carrasco • Sholto David • United States • William Hahn 76 comments on “Dana-Farberications at Harvard University” Zebedee January 2, 2024 “As the whole world furiously argues over whether the president of Harvard did or didn’t use some quotation marks in the right place…” Imitation is the sincerest form of flattery that mediocrity can pay to greatness. Oscar Wilde I think he was referring to himself! I don’t know if he’d be on the curriculum at Harvard. A man, he died at 46 so not that old, Irish. How to categorise? LikeLike Reply magazinovalex January 2, 2024 “What exactly is the building material of choice at DFCI?” Crap! LikeLike Reply Aneurus January 2, 2024 Yeah, but you know… science fraud is supposed to happen just at bullshit institutions in bullshit countries, right? LOL. LikeLike Reply Zebedee January 2, 2024 Harvard, founded 1636, is quite a few years older than the country, independence 1776. Harvard sets the tone. It is the top drawer. LikeLike Reply Jones January 5, 2024 The circus is back in town… Wait! It never left! https://www.businessinsider.com/bill-ackman-wife-neri-oxman-mit-dissertation-plagiarism-2024-1 ‘Billionaire hedge fund manager and major Harvard donor Bill Ackman seized on revelations that Harvard president Claudine Gay had plagiarized some passages in her academic work to underscore his calls for her removal following what he perceived as her mishandling of large protests against Israel’s bombardment of Gaza on Harvard’s campus. An analysis by Business Insider found a similar pattern of plagiarism by Ackman’s wife Neri Oxman, who became a tenured professor at MIT in 2017. Oxman plagiarized multiple paragraphs of her 2010 doctoral dissertation, Business Insider found, including at least one passage directly lifted from other writers without citation. Her husband Ackman has taken a hardline stance on plagiarism. On Wednesday, responding to news that Gay will remain a part of Harvard’s faculty after she resigned as president, he wrote on X that Gay should be fired completely due to “serious plagiarism issues.” “Students are forced to withdraw for much less,” Ackman continued. “Rewarding her with a highly paid faculty position sets a very bad precedent for academic integrity at Harvard.” An architect and artist who experiments with new ways to synthesize materials found in nature, Oxman has been the subject of profiles in major outlets such as the New York Times and Elle. She collaborated with Björk, exhibited at MOMA, and had paparazzi stake her out after Brad Pitt visited her lab at MIT in 2018.’ LikeLike Jones January 7, 2024 Popcorn ready? My wife, @NeriOxman, was just contacted by Business Insider claiming that they have identified other plagiarism in her work including 15 examples in her dissertation where she did not cite Wikipedia as a source. Business Insider told us that they are publishing their story… — Bill Ackman (@BillAckman) January 5, 2024 Bill Ackman @BillAckman My wife, @NeriOxman, was just contacted by Business Insider claiming that they have identified other plagiarism in her work including 15 examples in her dissertation where she did not cite Wikipedia as a source. Business Insider told us that they are publishing their story this evening. As a result, we don’t have time to research their claims prior to publication. It is unfortunate that my actions to address problems in higher education have led to these attacks on my family. This experience has inspired me to save all news organizations from the trouble of doing plagiarism reviews. We will begin with a review of the work of all current @MIT faculty members, President Kornbluth, other officers of the Corporation, and its board members for plagiarism. We will be using MIT’s own plagiarism standards which can be found here: https://integrity.mit.edu/handbook/what-plagiarism We will share our findings in the public domain as they are completed in the spirit of transparency. LikeLiked by 1 person NMH, the failed scientist and incel January 7, 2024 MIT should rescind Oxman’s PhD thesis if plagiarism counts as misconduct worthy of a fraudulent PhD. Fortunately for her she is no longer faculty there. Most interesting analysis I’ve heard on the Claudine Gay case (if you can stomach the conservative slant): LikeLike Multiplex January 7, 2024 Bill Ackman? Ah, I remember… big fan/financial supporter of David Sabatini, who had to endure all these unfair investigations of sexual misconduct… see Schneider Shorts 10.2.23. However, I never heard of this Neri Oxman Super Star of Science: “In August 2019, Ackman wrote to MIT Media Lab director Joi Ito to discourage him from mentioning Oxman when discussing convicted sex offender Jeffrey Epstein, who had donated $125,000 to Oxman’s lab.” (Wikipedia) The problems of the super-rich… LikeLiked by 1 person Jones January 2, 2024 It will all be addressed with the usual Harvard efficiency. “We’re at a stage of generating thinking about it, rather than taking concrete steps,” Dean of Undergraduate Education Amanda Claybaugh https://www.thecrimson.com/article/2023/10/5/faculty-debate-grade-inflation-compression/ LikeLike Reply Zebedee January 2, 2024 The identical mouse problems could easily be solved. You can even buy vegan ones. LikeLike Reply Han Wang January 2, 2024 Besides DFCI, Brigham and Women’s Hospital of Harvard seems to be a paper factory run by Joseph Loscalzo, Piero Anversa, Augustine Choi (now at Cornell), Edward Whang, etc. LikeLike Reply NMH, the failed scientist and incel January 2, 2024 Laurie Glimcher says she doesn’t suffer fools. But she suffers the fraudster Claudio Hetz, and now Claudio is a scientist at the Buck Institute. See what happens when you write nice letters of recommendation for the cheating twits that worked for you, Laurie? https://www.buckinstitute.org/lab/hetz-lab/ LikeLiked by 1 person Reply Sholto David January 2, 2024 Are we expecting Laurie to act on the fools at DFCI? Apparently she’s the CEO, so what should happen with Bill and Ken the flowerpot men? The cheating is so obvious, but the response is sure to be grindingly slow – if these things are ever addressed. LikeLike Reply Zebedee January 2, 2024 “The cheating is so obvious, but the response is sure to be grindingly slow – if these things are ever addressed.” Harvard took ten years here. I suppose that’s a start. Glacial pace, but some movement detected. “Questions have loomed over Whang’s research for a decade, and more than 20 of his studies have been flagged on PubPeer for possible image problems. As one commenter wrote in 2014 about one of the now-retracted papers, “It is perhaps fortunate that figure assembly and liver surgery require such unrelated skill sets.” ” “In 2013, the German magazine Der Spiegel quoted a professor at a university there as saying that figures from two separate papers by Whang and his colleagues were sufficiently similar as to justify “the suspicion of manipulation.” Whang conceded the likeness, but said he could “vouch for the accuracy of the data.”” For the English speakers. “Der Spiegel” is the German equivalent of “TIME” magazine. Americans, for the most part, can’t read German, and if they did read Der Spiegel, it doesn’t seem to have have any impact. The Dana Faber Cancer Institute is part of Harvard. I believe that there are seven Harvard teaching hospitals, they are all part of Harvard. Somebody can be affiliated to the Dana Faber Cancer Center and affiliated with one, or more, of the Harvard teaching hospitals. The Brigham and Women’s hospital seems to crop up most often with problematic data, but they the all part of the same organisation, and people move from one to another. The Beth Israel and Deaconess Medical Center does not seem to lag far behind the Brigham and Women’s hospital when it comes to problematic data. Unless allowances are made for how many researchers there are in the seven teaching hospitals the amount of problematic data from each might just be due to the number of researchers at each. It strikes me that Yale is less riddled with fake data than Harvard, but again that might just be a size thing. LikeLike Zebedee January 2, 2024 I think that it is important to bring the many strands of faking at Harvard together in one article. I think that this article gives the general public detailed evidence of the widespread faking at Harvard. What concerns me, not about the article, but about what this articles shows, is that faking, and the permitting of faking, is the community standard, is the standard for Harvard, so much so that pointing out the clear examples of faking, and the years long uncorrected faking, by people such as Kenneth C Anderson and William C Hahn, might be thought of as picking on them and that it was a form of discrimination to point out their faking because so many seem to do it. Kenneth C Anderson and William C Hahn may try to use that as a defence! I think that only in our dreams will Harvard will come out and state that faking is not O.K., that faking is not permitted, that the scientific record will be corrected, including retractions where the record cannot be corrected. By taking 10 years to deal with the Edward E Whang fakes Harvard has denied justice. Harvard will move very slowly, to be able to say that it doing something, will come out with some strange nuanced, ideological view about equal treatment of the perps and the victims, but will not state that faking is wrong. LikeLike michaelhbriggs January 2, 2024 The Buck Institute: https://pubpeer.com/search?q=bredesen LikeLike Reply Zebedee January 2, 2024 The editors wish to express concern about the article by Ellerby HM, Martin SJ, Ellerby LM, Naiem SS, Rabizadeh S, Salvesen GS, Casiano CA, Cashman NR, Green DR, and Bredesen DE, entitled “Establishment of a Cell-free System of Neuronal Apoptosis: Comparison of Premitochondrial, Mitochondrial, and Postmitochondrial Phases” published in The Journal of Neuroscience (1997) 17:6165–6178. LikeLike michaelhbriggs January 2, 2024 Sam W Lee, another Harvard alumnus: https://pubpeer.com/search?q=%22sam+w+lee%22 LikeLike Reply Zebedee January 2, 2024 “As the whole world furiously argues over whether the president of Harvard did or didn’t use some quotation marks in the right place..” How prescient! https://www.bbc.co.uk/news/world-us-canada-67868280 Claudine Gay resigns as Harvard University president LikeLike Reply Zebedee January 3, 2024 What about a new motto for Harvard? “de notitia non refert” LikeLike Reply Zebedee January 4, 2024 https://brugge.hms.harvard.edu/ “Dr. Brugge is currently Director of the Harvard Ludwig Cancer Center and Professor of Cell Biology at Harvard Medical School” Problematic data. https://pubpeer.com/search?q=Joan+S+Brugge LikeLike Zebedee January 4, 2024 Kenneth C Anderson 2016. https://pubpeer.com/publications/BE119832A5B1091025C04244B43F5D# LikeLike Zebedee January 6, 2024 Joan Brugge, Harvard. https://pubpeer.com/publications/4F116211D4C68103C58981E761F206 LikeLike Zebedee January 3, 2024 “Raanan Berger, Phillip G. Febbo, Pradip K. Majumder, Jean J. Zhao, Shayan Mukherjee, Sabina Signoretti, K. Thirza Campbell, William R. Sellers, Thomas M. Roberts, Massimo Loda, Todd R. Golub, William C. Hahn Androgen-induced differentiation and tumorigenicity of human prostate epithelial cells Cancer Research (2004) doi: 10.1158/0008-5472.can-04-2938” Penultimate author https://www.broadinstitute.org/bios/todd-r-golub “Todd Golub is director of the Broad Institute of MIT and Harvard, and a founding core member of the institute. He is also a member of the faculty of the Dana-Farber Cancer Institute and Harvard Medical School.” Torture was in vain as the paper below was retracted. https://pubpeer.com/publications/86A05A1191BCC54266244DB3839DAA#1 https://pubpeer.com/publications/86A05A1191BCC54266244DB3839DAA#24 Duplication. https://pubpeer.com/publications/6834018FE9FBCC5BBA0ABCB8E78A55 Duplication. https://pubpeer.com/publications/42FBBD913404F1F62F79E43DD123AC LikeLike Reply Zebedee January 6, 2024 Kenneth C Anderson 2018 duplication https://pubpeer.com/publications/C52AE9CC6C2E247042E139FBBACCE8 2016 duplication. Scroll down. https://pubpeer.com/publications/66A7E0E41B8831D94D067935933C25 LikeLike Reply Truthshallsetyourfree January 10, 2024 Someone please send all of this to the US Congressional Committee that is currently investigating Harvard for all its fraud. We need to hold them accountable. This is public money pouring into fraud!! LikeLike Reply Zebedee January 12, 2024 https://www.thecrimson.com/article/2024/1/12/dana-farber-research-misconduct-allegations/ LikeLiked by 1 person Reply Zebedee January 12, 2024 “HMS [Harvard Medical School] spokesperson Ekaterina D. Peshava declined to comment on the allegations, citing policy against discussing individual circumstances, but wrote that HMS is “fully committed to upholding the highest standards of ethics and to rigorously maintaining the integrity of its research.” In an emailed statement, DFCI spokesperson Ellen Berlin wrote that Dana-Farber is “fully committed to rigorously maintaining the integrity of research under its oversight,” and allegations of errors are “reviewed thoroughly and authors are supported in submitting corrections, when necessary.” Except Harvard is not fully committed to upholding the highest standards of ethics…blah, blah, because it has not done so as evidenced by this article for a start. Are the above statements by Harvard, its medical school, and its institute, DFCI, examples of “black-white”, or “double-think”? LikeLike Reply Zebedee January 12, 2024 “In an emailed statement to The Crimson Wednesday, Ghobrial wrote that she was “aware” of the allegations and had submitted corrections to the journals Clinical Cancer Research and Blood…” The editors-in-chief of both Clinical Cancer Research and Blood are employed by Harvard. Kenneth C Anderson, whose papers are being criticised for data manipulation, was himself editor-in-chief of Clinical Cancer Research for many years. The editors-in-Chief of Clinical Cancer Research and Blood are conflicted and should recuse themselves from dealing with the problematic data. Harvard needs to maintain the highest ethical standards, blah, blah.. LikeLike Reply Zebedee January 12, 2024 https://www.thecrimson.com/article/2024/1/11/congress-letter-tax-exempt-status/#:~:text=The%20House%20Ways%20and%20Means,.%20Garber%20’76%20on%20Wednesday. Add to that: data forgery. Harvard only understands one thing, money. It will understand less money. USD 53 billion endowment, but not a cent for scientific integrity as this would take away from its USD 53 billion. Makes sense really. LikeLike Reply Rameen Beroukhim January 12, 2024 Let’s see, Bill Hahn has published ~300 papers, each of which has probably four figures each with five panels. So a total of six thousand panels. If there are errors in 30 of those panels, that would be a rate of 0.5%. How many people would not miss errors 0.5% of the time? It is good to catch errors, but occasional errors do not indicate malfeasance. LikeLike Reply Leonid Schneider January 12, 2024 Well, without access to raw data we can only get the laziest of fraud. Ask Harvard Medical School Dean George Daley what happens when authors are made to post raw data with their paper. LikeLike Reply Rameen Beroukhim January 12, 2024 I agree with transparency. But I don’t know that occasional errors indicate fraud as much as being human. As you indicate, true fraud would likely be covered up better. LikeLike Leonid Schneider January 12, 2024 So you say science fraud is invisible, hence it doesn’t exist? An interesting philosophical concept, I thought science fraud is a fake gel. LikeLike Sholto David January 12, 2024 Rameen Beroukhim With all due respect, you cannot dismiss copying and pasting the same bands multiple times into western blots as a “mistake”. That’s fraud, and it’s never acceptable, no matter how many papers you publish. Further point, even if we look only at the “innocent” duplications, we can only identify a narrow category of mislabelled images, where the authors not only labelled the blot wrong, but also duplicated it. The true error rate would presumably be even higher – i.e. mislabelled but not duplicated – so who’s going to check? Bill may have published 300 papers, but how many of those even contain research images that can be analysed for duplication? Plenty are clinical trials. Artificially boosting the denominator and then dismissing the fraud as errors is a bad look, but not surprising from a DFCI colleague. LikeLike Reply Zebedee January 12, 2024 “It is good to catch errors, but occasional errors do not indicate malfeasance.” Your argument has a superficial appeal. You can argue that it is just a percentage game, but the “errors” occur in key parts of the data. One example: https://pubpeer.com/publications/3620BA59CC17B5DB62215DD774D352 LikeLike Reply Zebedee January 14, 2024 Nature will likely be very obliging. It was for another 1999 paper, which contained at least 2 image duplications, allowed repeat experiments in 2023. Sounds like time travel. https://pubmed.ncbi.nlm.nih.gov/10524633/ LikeLike Zebedee January 12, 2024 “It is good to catch errors, but occasional errors do not indicate malfeasance.” From my reading I understand that “malfeasance” is American lawyer speak for “wrongdoing”, usually applied to public officials, elected ones. I do not believe that word applies to Harvard as it is a private university, and I do not believe any elections by the general population have been involved. The “errors” are certainly not “right doing”. In a civil court the claim would be about the 12 identified papers, not the 300 or so. The legal standard for civil courts in the State of Massachusetts is “on the balance of probabilities”, or some formulation of that. Carlo “bless his socks for pugnaciousness” Croce went to court in the State of Ohio about problematic data. The claim was that there were problematic data in 20 publications. The final judgement stated that, on discovery, there were problematic data in 20 publications, and that this was outside the normal bounds. Carlo Croce has 1200 publications so the “it’s only a small proportion of the papers” does not fly. LikeLike Reply Zebedee January 12, 2024 Apologies. Commonwealth of Massachusetts. LikeLike Jones January 12, 2024 Bill likes to argue similarly. I thought to do one more post about the @NeriOxman Business Insider story in the hopes of its rapid resolution. In this post, I share the entire story including many new facts that we have learned in the last 24 hours, and hold nothing back from the public domain, including… — Bill Ackman (@BillAckman) January 11, 2024 ‘Now bear in mind, Neri’s thesis has 2,774 paragraphs, which implies an error rate of 0.1141%. In other words, for 99.8859% of the paragraphs Neri used proper citation. Not perfect, but pretty darn good.’ LikeLiked by 1 person Reply Zebedee January 12, 2024 Why bring that in? It’s not like Neri Oxman did any science. Art and design inspired by science and nature does not science make. Many artists do not cite their influences, rather they are paying homage to others, and it requires the knowledge of the reader to understand and enjoy that. Much of literature has forerunners. We read a poem. Do we need a list of influences going back to the Ancients (btw they can be Ancients of any background)? Anyway, Caro Croce’s problematic papers amount to 20 out of 1200 according to the court in Ohio, i.e. 1.67%, which much more than the 0.1141% for Neri Oxman. LikeLike Zebedee January 13, 2024 “As you indicate, true fraud would likely be covered up better.” Dear Rameen Beroukhim, Leonid wrote: “Well, without access to raw data we can only get the laziest of fraud.” The laziest of fraud is still fraud, you seem to imply that it would not be fraud. Leonid’s categories are laziest, lazy and not lazy. I don’t think Leonid used true and untrue as his categories. It seems you don’t think that there is any fraud as you expect people to be better at fraud, and make it invisible. The fact that we can see it seems to be evidence in your eyes that it isn’t fraud. LikeLike Reply Zebedee January 12, 2024 Another Harvard “error”. Only breast cancer. https://pubpeer.com/publications/4AF0E9783EADBD77BFD65799BD27F5 LikeLike Reply Zebedee January 16, 2024 Nature Medicine to the rescue! “Date: Thu, Jan 11, 2024 at 2:54 PM Subject: RE: Problematic data Nat Med. 2010 Feb; 16(2): 214–218. To: Hello, Thank you for reaching out with your concern regarding this Nature Medicine paper. We will assess further, but I cannot provide an estimated timeline at this stage, nor can I comment on potential next steps, until we’ve looked into this more deeply.” How long does it take to look? LikeLike Reply Zebedee January 16, 2024 Nature. 2024 Jan 4. doi: 10.1038/d41586-024-00014-x. Online ahead of print. US project seeks standard way to communicate research retractions Dalmeet Singh Chawla PMID: 38177808 DOI: 10.1038/d41586-024-00014-x “There’s a lot of scepticism in the research world about the value that publishers are adding.” Perhaps Nature could have a word with Nature Medicine and ask it to look at things for a start. LikeLike Zebedee January 22, 2024 https://www.thecrimson.com/article/2024/1/22/dana-farber-issues-corrections/ “In the emailed statement to The Crimson, DFCI Research Integrity Officer Barrett J. Rollins wrote that six manuscripts have retractions underway and 31 are being corrected. The corrections come amid claims of data manipulation against DFCI President and CEO Laurie H. Glimcher ’72, Executive Vice President and COO William C. Hahn ’87, Senior Vice President for Experimental Medicine Irene M. Ghobrial, and Harvard Medical School professor Kenneth C. Anderson. ” Is it right that the DFCI Research Integrity Officer Barrett J. Rollins conducting the investigation is conflicted as he has 2 problematic publications with Irene M. Ghobrial, and Kenneth C Anderson? https://pubpeer.com/publications/E0703B990C3296D3A563F7ED2497B0 https://pubpeer.com/publications/7302CC7E498BF25235E1EAEFCDEA9F Barrett J Rollins also has a problematic publication without Irene Ghobrial, or Kenneth C Anderson. https://pubpeer.com/publications/FAD61EAB02845F8260C71B48EB8D8F According to PubMed Barrett J Rollins has 5 publications with Irene Ghobrial, 4 publications with Kenneth C Anderson, and 3 with William C Hahn. https://pubmed.ncbi.nlm.nih.gov/?term=ghobrial+i+rollins+b&sort=date https://pubmed.ncbi.nlm.nih.gov/?term=anderson+k+rollins+b&sort=date https://pubmed.ncbi.nlm.nih.gov/?term=hahn+wc+rollins+b&sort=date Publishing with somebody by itself is a conflict of interest. Barrett J Rollins has published with 3 of the 4 people he is investigating, and he should recuse himself from the investigation. https://www.thecrimson.com/article/2024/1/12/dana-farber-research-misconduct-allegations/ “[Harvard Medical School]HMS spokesperson Ekaterina D. Peshava declined to comment on the allegations, citing policy against discussing individual circumstances, but wrote that HMS is “fully committed to upholding the highest standards of ethics and to rigorously maintaining the integrity of its research.” Given the above conflict of interest of DFCI Research Integrity Officer Barrett J. Rollins this seems not to be the case. LikeLike Reply Zebedee January 14, 2024 “For example, this undergraduate level error of miscounted western blot lanes remains in the scientific record, without comment or correction, in the prestigious journal Science, no less” https://pubpeer.com/publications/C6B3AB4AA887C2D05639A8B0729124 I disagree. Undergraduates are usually quite careful. Little do they know that being true and accurate will militate against them in the natural selection for those who produce the most papers. According to Rameen Beroukhim “true fraud would likely be covered up better”. That’s a view. The problem is that most people manage arithmetic. LikeLike Reply Zebedee January 14, 2024 Science. 2006 Apr 28;312(5773):572-6., published 28 April 2006, does state that the senior author, Stanley J Korsmeyer, was deceased (dead). Stanley J Korsmeyer died from cancer on 31 March 2005. https://en.wikipedia.org/wiki/Stanley_J._Korsmeyer https://www.nature.com/articles/4401683 Sounds like another example of using somebody’s death to get problematic data published. Science. 2006 Apr 28;312(5773):572-6. doi: 10.1126/science.1123480. Proapoptotic BAX and BAK modulate the unfolded protein response by a direct interaction with IRE1alpha Claudio Hetz 1, Paula Bernasconi, Jill Fisher, Ann-Hwee Lee, Michael C Bassik, Bruno Antonsson, Gabriel S Brandt, Neal N Iwakoshi, Anna Schinzel, Laurie H Glimcher, Stanley J Korsmeyer Affiliation 1Howard Hughes Medical Institute, Dana-Farber Cancer Institute, and Harvard Medical School, Boston, MA 02115, USA. chetz@hsph.harvard.edu PMID: 16645094 DOI: 10.1126/science.1123480 https://pubpeer.com/publications/C6B3AB4AA887C2D05639A8B0729124 LikeLike Reply Zebedee January 16, 2024 Oncogene. 2006 Oct 19;25(49):6467-79. doi: 10.1038/sj.onc.1209660. Epub 2006 May 15. Tuberin activates the proapoptotic molecule BAD A Freilinger 1, M Rosner, G Krupitza, M Nishino, G Lubec, S J Korsmeyer, M Hengstschläger Affiliation 1Medical Genetics, Obstetrics and Gynecology, Medical University of Vienna, Währinger Gürtel, Vienna, Austria. PMID: 16702951 DOI: 10.1038/sj.onc.1209660 PDF version gives affiliations as: A Freilinger1, M Rosner1, G Krupitza2, M Nishino3, G Lubec4, SJ Korsmeyer3 and M Hengstschläger1 1 Medical Genetics, Obstetrics and Gynecology, Medical University of Vienna, Währinger Gu¨rtel, Vienna, Austria; 2 Institute of Clinical Pathology, Medical University of Vienna, Währinger Gürtel, Vienna, Austria; 3 Departments of Pathology and Medicine, Harvard Medical School, Howard Hughes Medical Institute, Dana-Farber Cancer Institute, Boston, MA, USA and 4 Department of Pediatrics, Medical University of Vienna, Währinger Gürtel, Vienna, Austria Already at Pubpeer: https://pubpeer.com/publications/B0E1E6038481E74773DD4F6C7827E2 Senior author’s Pubpeer record: https://pubpeer.com/search?q=hengstschlager Ignore the first 2 which are bioRxiv preprints. LikeLike Zebedee January 16, 2024 Mol Cell. 2004 Sep 24;15(6):901-12. doi: 10.1016/j.molcel.2004.08.020. Inhibition of both the extrinsic and intrinsic death pathways through nonhomotypic death-fold interactions Young-Jae Nam 1, Kartik Mani, Anthony W Ashton, Chang-Fu Peng, Barath Krishnamurthy, Yukihiro Hayakawa, Peiyee Lee, Stanley J Korsmeyer, Richard N Kitsis Affiliation 1Department of Medicine, Albert Einstein College of Medicine, 1300 Morris Park Avenue, Bronx, NY 10461, USA. PMID: 15383280 DOI: 10.1016/j.molcel.2004.08.020 https://pubpeer.com/publications/F9AF25C0B4B9F3E07CF817FC7AB9EC 2021 correction (not visible on publisher’s webpage for original article) https://www.cell.com/molecular-cell/fulltext/S1097-2765(21)00038-1?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS1097276521000381%3Fshowall%3Dtrue “We, the editors of Molecular Cell, were contacted by the corresponding author, Dr. Richard Kitsis, and a concerned reader, who informed us about a duplication in Figure 7A of the above paper. Dr. Kitsis apologized for the inadvertent side-by-side duplication of the α-tubulin loading control. The authors no longer have access to the original data published 16 years ago and could not determine how the error arose. They note, however, that the data in lanes 1–3 of Figure 7B show independently that ARC levels in H9c2 cells decrease with hydrogen peroxide treatment. Without access to the original data for Figure 7A, a Correction is not possible. Given the age of the paper and that the duplication does not compromise the conclusions of the paper, based on the information available to us at this time, we believe that no further action is warranted.” LikeLike Zebedee January 16, 2024 Cell Death Differ. 2007 Jul;14(7):1386-9. doi: 10.1038/sj.cdd.4402166. Epub 2007 May 18. The proapoptotic BCL-2 family member BIM mediates motoneuron loss in a model of amyotrophic lateral sclerosis C Hetz, P Thielen, J Fisher, P Pasinelli, R H Brown, S Korsmeyer, L Glimcher PMID: 17510659 DOI: 10.1038/sj.cdd.4402166 https://pubpeer.com/publications/CD1650CE831BA0737077EB5BCB6518#2 Supplemental Fig. 2C: Different number of bands. Undeclared splicing in GFP-SOD1 monomer LikeLike Zebedee January 15, 2024 DFCI metastasizes to Pittsburgh. https://pubpeer.com/search?q=Ole+V+Gjoerup LikeLike Reply omega eldorado January 16, 2024 basic cancer researchers seem to suffer from both chasing career aggrandizement by any means they can get away with and trying to deflect from the fact that most work they do harms animals and is not important/relevant to human health LikeLiked by 1 person Reply Sholto David January 20, 2024 Worth mentioning that the details in this blog have been covered by STAT news: https://www.statnews.com/2024/01/19/dana-farber-cancer-institute-allegations-manipulated-data-glimcher/ LikeLike Reply Zebedee January 20, 2024 Boston Globe picking up on story. https://www.bostonglobe.com/2024/01/19/business/dana-farber-cancer-data-manipulation/ LikeLike Reply leerudolph9414f8c86b January 20, 2024 Dana Farber data carvers? LikeLike Zebedee January 22, 2024 6 retractions. https://www.thecrimson.com/article/2024/1/22/dana-farber-issues-corrections/ LikeLike Reply Newer Comments Leave a Reply Post navigation Previous Post Next Post",
    "commentLink": "https://news.ycombinator.com/item?id=39098780",
    "commentBody": "Dana-Farberications at Harvard University (forbetterscience.com)159 points by RadixDLT 7 hours agohidepastfavorite80 comments pessimist 4 hours agoMy brother is dying of a grade 4 brain tumor, and I've spend countless hours poring through papers. Horrifying to realize they may be falsified. I am always sceptical of pre-clinical studies since so many have tended to fail in the real world, but now I wonder how many were just fake? These aren't scientists but thieves robbing public money with no skin the game. reply patryn20 4 hours agoparentReading through papers on cholangiocarcinoma: most of them ARE falsified. And I want to harm people for it. Only three new drugs in 29 years. And I can’t help but think it’s partially due to this. Mother has stage 4 intrahepatic cholangiocarcinoma. reply boxed 2 hours agoparentprevThe deepest irony is of course that they themselves are probably going to get cancer at some point and die years earlier than if they hadn't poisoned the science. reply neilv 4 hours agoprev\"Dana-Farberications\" is indirectly playing on the name of someone who presumably had nothing to do with these allegations, and who sounds more at the other end of the spectrum of respectability: > Sidney Farber (September 30, 1903 – March 30, 1973) was an American pediatric pathologist. He is regarded as the father of modern chemotherapy for his work using folic acid antagonists to combat leukemia, which led to the development of other chemotherapeutic agents against other malignancies. Farber was also active in cancer research advocacy and fundraising, most notably through his establishment of the Jimmy Fund, a foundation dedicated to pediatric research in childhood cancers. The Dana–Farber Cancer Institute is named after him. -- https://en.wikipedia.org/wiki/Sidney_Farber reply astrolx 16 minutes agoprevThis is insane. As a researcher I had to take research ethics course, and obviously we study some cases of research misconduct and fraud. But this kind of stuff is next-level. I know Sweden launched a national board to investigate research misconduct [1], I hope more countries do the same. [1] https://npof.se/en/ reply profsummergig 4 hours agoprevDavid Sabatini was anointed a future Nobel Laureate by the university-academic complex. What's astonishing is that when his work was discovered to have silly photoshop manipulations, he reacted by calling (on Twitter) the people who caught him \"steaming turds\". https://forbetterscience.com/2020/01/29/david-sabatini-torme... reply ramraj07 3 hours agoparentGuess who’s bankrolling his “uncancellation” with a $25MM fund? Bill Ackman lol. reply hiAndrewQuinn 24 minutes agoprevHuh, a blast from the past. After dropping out of high school and before getting into college I remember visiting the DFCI once or twice to talk with one of the PhDs in there about suffix trees in bioinformatics. I even recognize some of the names in this article. Weird to think I could have ended up working there if I had cut getting my philosophical act together short and set myself to my education immediately. It was a really cool place right in the center of the city near my high school. Suffix trees are still rad and any DSA-inclined teenager will benefit greatly from learning them :) reply harry8 4 hours agoprevThere’s literally f.all point having peer review if the peer reviewers who passed this junk for publication aren’t the ones deciding if the paper needs to be retracted when this stuff comes out. How are these journals retaining any prestige when this stuff is un-retracted? The stink should be smeared all over all of them. The publishers should be sued into the ground for fraud given the exorbitant subscription fees. I can’t think of a single reason not to burn it all down. reply 1vuio0pswjnm7 51 minutes agoprev\"Starting with Laurie H. Glimcher, who is the President and CEO of the DFCI. No doubt Laurie built her career on papers like this one, in Nature Immunology (2003), which includes some impressive contributions to art, but perhaps not to science.\" Having had the opportunity to peek at some of what went on behind the scenes at Nature and Science in the early 2000s and even before I seriously wonder if the journals are not somehow complicit in this fraud. Because the journals demand \"eye candy\". To the point where it begins to appear like they prefer form over substance. For their readers, this may be more satisfying. But it's also dishonest. To some degreee, it is not real. These are supposed to academic journals but in a sense they are like any other glossy magazine. It all looks just a little too good. reply perihelions 5 hours agoprevDupe https://news.ycombinator.com/item?id=39090959 (34 comments) reply dang 4 hours agoparentThat article cites this one as the original source, so even though this one is rather strangely written, I guess we can let the current thread run. reply alejohausner 2 hours agoprevRalph Moss’s “Cancer Incorporated” is a must read. It details the deep corruption in the oncology industry. Oncologists are paid a lot of money for each prescription. Clinical trials for new cancer drugs are absolutely full of shenanigans, such as cherry picking test subjects. New breakthroughs are announced weekly, but death rates remain unchanged from 50 years ago (aside from lung cancer deaths, which decreased due to anti-smoking campaigns). Depressing. It’s a very profitable industry, and profits corrode science. reply cosmojg 4 hours agoprevHot take, but if the FDA hadn't raised the bar for clinical trials to where it sits now (specifically with regard to the expenses associated with bureaucratic approval and regulatory compliance), I'd expect we'd see far fewer falsified studies. When it's impossible to test an idea in the real world, bias and whimsy run rampant, free to rule the printed page. In the absence of empiricism, passing the vibe check passes for peer review. reply stdbrouw 2 hours agoparentHow cheaply do you think you can run a clinical trial with human patients in the absence of bureaucracy? I imagine increased insurance and recruitment costs would eat up the difference. reply concordDance 26 minutes agorootparentI expect we could empirically look at how much trials cost 50 years ago on the assumption there has been a regulatory ratchet effect. I don't know how to find that data though... reply ChrisArchitect 4 hours agoprev[dupe] Discussion: https://news.ycombinator.com/item?id=39090959 reply Semaphor 2 hours agoparent[dupe comment] Discussion: https://news.ycombinator.com/item?id=39099565 ;) reply p-e-w 5 hours agoprevAll rankings that attempt to compare the academic quality of universities are self-fulfilling prophecies. Rankings influence prestige. Prestige influences perception, even among experts. Is Nature really going to reject that paper? It was written by a Harvard guy, after all. And next, the number of \"high-impact\" publications (such as in Nature) is used to rank universities. Good luck... reply whatyesaid 5 hours agoparentYou can have double blind papers and double blind research funding I guess to help that a little? Not sure what else you can do. Table rankings are actually only applicable for undergrads in practical terms. At research level, You have world leading experts at random universities often, and then people who worked in top universities who advanced the field by none after decades. I wonder how many universities actually mega climbed ranks in the past decades though reply sbalamurugan 4 hours agorootparentDesk rejects also happen regularly from Editors (who are not blind to the author names and affiliations). Editors can also make final decisions to accept or reject when reviewers don't agree. Even in a blind peer review, the subject of study introduces another layer of bias in the process. Let's say two economics papers are submitted that discusses how people spend their last part of their pay check. One studies a pool of people across United States and other studies of people across Nigeria. Which one do you think is going to get positive review from editors and reviewers? The Nigeria paper might not even go to review. The editor is going to say something on the lines of \"the focus of the study is too narrow for the journal\" but won't say the same when it is across US. This is in addition to the trust issues with any research conducted outside western countries. For researchers from the \"low ranked\" universities, the game is rigged against them and there is nothing they can do to swing it to their favour. reply jackcosgrove 4 hours agorootparentprev> I wonder how many universities actually mega climbed ranks in the past decades though I know of one, the University of Chicago. And it did so by gaming the ranking metrics. It dropped its long and idiosyncratic application and adopted the Common App. The admissions rate tanked from above 40% less than twenty years ago to about 5% now simply because more students applied. Did the quality of the education change? I doubt it. P.S. Chicago recently dropped out of the top 10 this past year for the first time since adopting the Common App, due to a change in the ranking formula. Which says more about the inside baseball of rankings than it does about the education on offer. reply neilv 4 hours agorootparentprev> I wonder how many universities actually mega climbed ranks in the past decades though An example of rankings being climbed: https://www.bostonmagazine.com/news/2014/08/26/how-northeast... (Not to dis Northeastern. They have some great professors, and do some great teaching and research. And the faculty of a university don't necessarily agree with the businesspeople.) But if you think that kind of college rankings climbing is a concern, the climbing demand that college has evolved for students is worse, and has implications for tech industry, as I asserted recently: https://news.ycombinator.com/item?id=39086755 reply dlkf 4 hours agorootparentprevI’m not sure that you can. What I’ve read is that in a lot of fields, the community is small enough that you have a reasonable idea whose work you are reviewing even if the author’s name is redacted. reply p-e-w 5 hours agorootparentprevThe peer review process isn't actually blind in practice. The number of experts working in a given problem space is small enough that they know most of their peers, and usually can identify at a glance who the author of a paper is, from the topic, writing style, and experimental design, which they can recognize from previous publications and correspondence. reply Aeolun 5 hours agoparentprevStill, there are peer reviewers and editors for these papers. They don’t go in without anyone checking them. So what goes wrong there? reply petesergeant 4 hours agoparentprev> Is Nature really going to reject that paper? It was written by a Harvard guy, after all. Yes. reply j7ake 1 hour agorootparentThis. You’d be surprised how many papers get rejected by Nature “even” papers from Harvard. Experiment: randomly pick a “Harvard guy” in biology and check their publications, count what fraction of them are in Nature (or comparable journals). reply theGnuMe 4 hours agoprevMan what a mess. I guess if you give someone something to optimize (publications) they will do so. Science has become distracted by celebrity and money. What a waste of grant money. I hope the NCI, NSF and NIH fund a program to route out scientific fraud, it is clearly needed. When I was in grad school some fellow students were admonished for plagiarism but allowed to stay. I always think of who was denied a place by these folks. reply patryn20 4 hours agoprevSchools that cater to rich legacy brats are populated by rich lying legacy brats. Who knew? I was legacy. At several prestigious universities. But I refused to go to those. Most people look for the easiest way out. And that includes riding on family connections and plain old lying. I grew up surrounded by billionaires. While living surrounded by drug houses as my dad worked his bones down at a refinery. Those billionaires children are still billionaires (or dead by drug overdoses, whoops) despite doing nothing but partying and making bare minimum passing grades. Yet they still got into the same universities I had to work my ass off to be accepted to. Which is why I didn’t go to those. MIT and Stanford included. Harvard, Baylor and several others accepted me due to my family history and relatives donations. I went to UT Dallas. On a merit scholarship. Yet I built a lot of shit you lazy Ivy League assholes take for granted. When tech was based on contributions it meant something. Now it’s just another good old boys club like every other business leadership elite. Tech hasn’t been a meritocracy since the mid 2010s. At the latest. Edit: and frankly YC readership and tech in general has just become people pursuing programming because it pays well and not because they actually like it or are good at it. reply smsm42 3 hours agoparentI went into it way before I knew how well it would pay - in fact, back then (and there) it paid ok, but nothing like \"unicorns\" and Google's seven figure salaries were even remotely something you could conceptualize, let alone think it'd be reality someday. It's lucky in the US it pays really well, but I'd likely be doing it anyway if it paid less well. Don't generalize that readily and broadly. reply patryn20 2 hours agorootparentI was so fortunate to get in at a startup paying $90k/y usd in 2004. Now I’m poorly paid compared to the people I “mentor” at $160k + bonuses with no RSU. But at least I’m working on stuff I care about. I hear these fresh college grads getting $300k offer that have the right university on their meaningless tech degree they drank 24/7 to achieve, and I just want to scream. In three years I’ll be rejecting them for positions writing the software that makes their current employer run. I’ve nixed so many Amazon and Facebook employees in the interview process that couldn’t answer anything about basic programming skills. Their answers were just “I’d use this off the shelf solution”. Well sorry, but we create that off the shelf solution or an alternative. So you actually have to know how to use binary arithmetic. Bye bye brat. reply smsm42 2 hours agorootparentAmazon has over 30k engineers. I'm sure there are mediocre ones among them - thousands probably. But for a lot of tasks in the company of Amazon's size, \"I’d use this off the shelf solution\" is exactly the right answer. Sometimes it isn't, but there would be thousands of places where it is. > I hear these fresh college grads getting $300k offer that have the right university on their meaningless tech degree they drank 24/7 to achieve, and I just want to scream. But why? Assume everything you say is right and somebody makes a lot of money undeservedly. Why should it make you scream? You're not paying that money. Some rich idiot does. Why be upset about it? Are you envious? Wasn't what you're doing your own choice? reply delichon 6 hours agoprevThe teaching standards at Harvard have been declining remarkably steadily for many years now. https://www.chartr.co/stories/2023-12-08-2-grade-inflation-a... Apparently teachers' standards have declined for themselves too. Some of that would be a result of being trained at Harvard, a vicious circle. reply burningChrome 5 hours agoparentI've had several HS friends go to Harvard. They all said the same thing - its for networking post graduation, the education itself is all smoke and mirrors. My buddy said he had to take a botany class as a filler for some academic requirement for graduation. He said the entire class was to pick a tree out on campus and then describe how it changed over the course of the semester. He maintained he had more rigorous classes in HS then he had at Harvard and said the progressive politics were so pervasive, that as long as you towed the ideological line, then you got favorable grades. It would appear not much as changed since he was there in the early aughts. reply Gustomaximus 5 hours agoparentprevCould this be because Harvard is increasingly desirable & hard to get into? From this, the average standard of students + their dedication to their work/GPA may well have steadily grown too. From a quick google, the acceptance rate to Harvard was 20% higher in the 70's. In 1940 it had an absolute acceptance rate of 85%. I dont actually know the answer, but from that graph only, the rising rate could be as much from a higher standard of student as lowering standards of marking. reply somenameforme 5 hours agorootparentIn the past a university education was 100% optional - as was high school for that matter, and so the only people pursuing such tended to be exceptional academic/intellectual outliers. The pool of people interested was very low, and standards were exceptionally high. For instance, this [1] is a Harvard entrance exam from 1869! [1] - https://graphics8.nytimes.com/packages/pdf/education/harvard... reply cperciva 5 hours agorootparentthis [1] is a Harvard entrance exam from 1869! We don't routinely teach Classics in high school, so it's not surprising that those sections would be beyond most students today. The sections on that exam dealing with Mathematics seem appropriate or possibly slightly on the easy side; I would expect an equivalent exam today to include at least a bit of calculus, but over the past 150 years there has been a shift in pedagogy towards calculus and away from geometry so it's probably just a sign of the times. reply Duanemclemore 4 hours agorootparentThe rise of calculus education in US high schools was a -direct- result of the Space Race. reply smogcutter 5 hours agorootparentprev> so the only people pursuing such tended to be exceptional academic/intellectual outliers. That’s an odd way to put “wealthy and privileged” reply somenameforme 5 hours agorootparentWell no. Being wealthy doesn't make you an academic outlier, and education used to be extremely affordable for anybody so interested in pursuing such. Consequently, many of the greatest minds of the past came from very humble beginnings. For instance Harvard's first black graduate graduated in 1870 [1] (meaning he would have passed a test near to exactly like the one shared), and he was a remarkable man. He was left supporting his family as a teenager after his father ran away for the Gold Rush, and was unable to attend public school due to his skin color. Yet, he persevered and lived one amazing life far above and beyond becoming Harvard's first black grad. But people like him are far and few between - and those were the sort of people applying. [1] - https://en.wikipedia.org/wiki/Richard_Theodore_Greener reply roenxi 5 hours agorootparentprevWithout knowing the academic context the paper is a bit meaningless. Incoming students were obviously expected to have studied Greek and Latin, for example. And I doubt mathematical education was the same; the focus of teaching would have been different because calculators only sort-of existed which would radically change how much rote work was taught and practised. Fully support the observation that university meant something different in those days though, the standards were much higher. Although I'm not sure if that would apply at Harvard since their class sizes probably haven't changed by all that much. reply Aeolun 4 hours agorootparentprevOne thing is certain. I’d not get into Harvard in 1869. Initially I thought it was all a rich kids game, but the later maths and geometry questions require you to know what you are doing. Of course thats easier with expensive tutors, but it’s entirely possible you’ll never grasp it. reply andirk 5 hours agoparentprevOregon (kind of) dropped requirements for very basic reading, writing, and arithmetic to graduate high school because such skills were deemed racist. I'm not a big fan of standardized tests, but they can identify serious issues to address especially as it pertains to geographical, ethnic, etc. groups and allocate funding accordingly. If we keep lowering bars instead of addressing the issues, this is going to get very bad. https://www.oregonlive.com/education/2023/10/oregon-again-sa... https://www.opb.org/article/2021/09/20/examining-oregon-deci... reply wsc981 5 hours agorootparent> Oregon (kind of) dropped requirements for very basic reading, writing, and arithmetic to graduate high school because such skills were deemed racist. It’s ironic that this stance is actually racist, since it implies certain races perform worse. reply defrost 36 minutes agorootparentNo, it acknowledges that some groups in a society perform worse in tests. There are many reasons for this to be the case [1] not just the classic \"this race is inferior at test taking\". [1] Pre WWI, WWII Germany famously tested students for acedemic performance. During WWII and times of food scarcity from Allied blockades test performance dropped by greater than 10%. Same students, same race, different conditions. Typically well fed, well exercised, unstressed, groups with sound economic security, home ownership, etc perform very well on tests. reply lemmsjid 4 hours agorootparentprevIt is not racist to make an observation about race. It is racist to believe that one race is superior to another and act accordingly. For example, it is not racist to acknowledge the frequently measured metric that African Americans on average score lower than other races on scholastic aptitude tests. It is what one does with that observation that can lead to racism. It is useful to define terms. Merriam-Webster has what I believe is a quite useful definition of racism: \"a belief that race is a fundamental determinant of human traits and capacities and that racial differences produce an inherent superiority of a particular race\". That is a useful definition because it helps to separate a racist interpretation of test scores from a non-racist interpretation. The racist interpretation is to jump to a conclusion of fundamental inferiority in academics. The non-racist interpretation is to acknowledge and emphasize the environmental factors that can lead an historically oppressed/displaced/enslaved minority to underperform academically, to acknowledge that African-born immigrants do quite well on standardized testing, and also to understand that standardized tests are normed on academic success, which creates a causality issue when measuring groups that have historically not been allowed in academic institutions. Perhaps all of those arguments don't hold water: or at least the pitcher holding them has holes of its own. But it is quite erroneous to jump to a conclusion of racial biological determinism, and that is what the racist conclusion would be. There is a particular kind of fallacy where a person can, seemingly unconsciously, take what are perfectly valid observations, like the test-taking disparities, and spin them into non-evidential overarching beliefs. This happens all too commonly when it comes to scientific (or at least metrical) observations and racism. People were all too eager to embrace phrenology, IQ testing, evolutionary biology, etc. when it confirmed their cultural preconception that their race was superior. All of those areas, even phrenology, had some scientific merit to them (and still do in the case of IQ testing and evolutionary biology), but that merit is lost when the conclusions get spun into non-evidential belief systems like racist ideology. Seemingly (I can't peer into their heads, really) skepticism is lost in a chase for the personal gratification of a belief of fundamental superiority. Discussions around academic standards are a case in point. Academic standards change quite frequently. The Oregon requirement we're discussing was only instituted in the early 2010's. When race is not part of the discussion, it is entirely normal for educators to say that a new standardized test doesn't really have evidence it serves the population and to remove it. There is a very lively debate amongst educators about how excessive measurement, i.e. standardized test taking, can interfere with the normal give-and-take of teaching. But, if the evidence is that non-native-English speakers are underserved, or that African-Americans are underserved, suddenly it is 'racist' to remove a requirement or hurdle. What I saw in the two articles is that those in favor of removing the requirement saw a lack of evidence it was beneficial, and saw evidence that it was not beneficial; while those opposed had no evidence other than it would graduate 'dumber' students; ignoring the fact that presumably dumber students were being graduated before the 2010's when the program was instituted. In short, it is often a smokescreen to say that observations about race are racist, because that makes it impossible to act against inequality, or sometimes even to act at all. Once again, racism is the belief of one race's inherent superiority over another. reply daseiner1 4 hours agorootparentprevWelcome to the world of striving for equity over equality. Lots of blame levied at a strawman white culture. If only there’d be a concomitant critique of the problematic groups at hand. Alas, that’s verboten under the current regime. Lowering expectations for those under their patronage only makes advantage easier for me and mine. Good luck to them and their token positions. reply whatyesaid 5 hours agoparentprevGrade inflation has been happening at literally every institution. So you can't point just to Harvard. But a middle rated school can't take a stand, only the very top schools can start to, and they'd have to do it slowly to give employers and students time to adjust. There are apparently advanced classes like Math 55 that had huge dropout rates that had to be cleaned up though. So those courses have been completely changed instead of just awarding higher marks I'd imagine. reply sudosysgen 2 hours agorootparentThere are plenty of 'middile rated schools, and even top 50' that have things like target average grades. It's also not uncommon across the world for entire programs to have dropout rates around 50%, it's just rare in reputable anglophone schools. reply WhitneyLand 5 hours agoparentprevGrade inflation is a statistic not a cause. It could be caused by lowered standards or more competition or better student performance. reply asylteltine 5 hours agoparentprevGoing to schools like Harvard is 90% about networking and 10% education. Harvard dudes hire Harvard. It’s gross reply IceHegel 6 hours agoparentprevThey thought it would never catch up to them, they thought grade inflation didn’t matter because it was still hard to get in. reply gonzo41 5 hours agorootparent\"Hard to get in\" is relative. I only recently learned about legacy admissions. reply tjpnz 5 hours agoparentprevThe publishing record of their former president speaks volumes. I wonder what it takes to be a lecturer there. reply galkk 4 hours agoprevThe piece reads very weirdly. In the beginning the picture of copypasted images and doctored results is very clear. But when it goes to the passage about genius, and it is so strange. Out of place, weird rant about chosen word… Whatever bastard Sabatiny is, the author’s approach also leaves bad taste in mouth reply ossicones 5 hours agoprevI would’ve preferred a less editorialized article about this. In particular, this article has left me wondering who’s actually written fraudulent articles and whose biggest mistake was trusting the wrong collaborators. reply mitchellst 4 hours agoparentAgreed. I don't think the snark helps. Not my field, but my understanding of how these things work in big medical research factories: first few authors tend to be young researchers (maybe med students or even undergrads) trying to match residency or get into grad school. They do much of the work actually assembling the submission. The later names on the author list (who this article is taking to task) run labs or oversee research groups. Should they correct the record when it's pointed out? Yes. (But the snark and tenor of the post doesn't exactly convince someone they can admit an oversight in good faith.) Should they be vigilant enough to check and notice these things? Of course. Some of the fakes are not subtle. Others, like the copy-paste of empty space in the lane to cover some undesirable result? Way harder to spot with the naked eye. I don't think there was great automated tech to detect image duplication in the 00's when these were published. So your med student fudges data on a paper. The ethical answer is to expel them—\"the world needs plenty of bartenders.\" But it appears big institutions these days are pretty invested in the sunk costs of prestige, dislike admitting error in admission or hiring, and prioritize go-along-get-along environments. It could be career limiting if students don't get any/enough pubs working in your lab. It'd a lot of hearings and paperwork to report him, plus I heard his uncle's a donor. If she got kicked out she'd lose her visa. And if I reported them, I'd be obliged to report everyone, and I'd be sunk in discipline hearings three times a year. So much easier to just... not look very hard. It's bad science and bad ethics, but if you want better, reform the incentives. \"Public\" shaming by a niche newsletter... might be better than nothing, but doesn't qualify as an incentive. reply wrs 4 hours agorootparentThey won't correct the record because they \"can't admit an oversight\" due to the snark in a niche newsletter...a newsletter which you then say isn't important enough to serve as an incentive? So which is it, important or not? I would think they can't admit an oversight due to the institutional incentives you mention; the snark is irrelevant. If anything, it encourages publicity for the oversight, which is the only thing that might change the incentive. reply mitchellst 4 hours agorootparentDifferent things. The snark plays to individual psychology in the moment. When someone comes at you in a way that's demeaning and clearly states that they think you shouldn't have the position you have, that's a bad way to start a conversation where you're supposed to admit error. More likely, you avoid them. To the real brass tacks incentives: yeah, it's \"someone is angry on the internet\" vs, \"I will have to deal with a discipline process with documentation and meetings and maybe depositions and adversarial lawyers. That's not my bag, I'm a scientist. There will be volatile young people and bad feelings communicated in person, plus gossip among my close coworkers. Also undesirable. If this becomes a repeated pattern, learners might start avoiding my lab, and deans/my superiors might start asking very awkward questions.\" Yeah, stacked against that, angry person on the internet is a weak incentive. Even if they're right. And the snark does matter. Because this guy writes like a YouTube comments section, and that's not how you talk to adults or solve problems in elite institutions. So the contrast in styles draws lines of \"us\" vs \"them.\" And it's natural to care more about the opinions and esteem of your in-group (who talk like you) than the out-group (who deride you). reply boxed 1 hour agorootparentI think the snark comes from having screamed about the situation for years and years and no one listening. Let's all remember what we are talking about here: every single one of us will know someone who will die several years early because of scientific fraud. It is reasonable to be angry as hell. reply theGnuMe 4 hours agorootparentprevI mean there is an alternative to expel and that is rehabilitation. I think that happens in a lot of cases. These though are systematic repeated.. The first author 'undergrads' or 'med students' are not assembling the paper no way. For sure the last authors know what is going on. reply rscho 4 hours agorootparentIn medical research no, last authors usually know little about what's going on. There are entire departments where the head puts his/her name on everything that comes out. We're talking tens of papers a year... reply shkkmo 4 hours agoparentprevEven the collaborators who did not engage in fraud show a shockingly low level of concern for correcting or retracting fraudulent papers their names are attached to. At this point, I think it is best to assume all involved are guilty and they should have to demonstrate their integrity by how proactively they work to correct the errors. reply ShamelessC 6 hours agoprevnext [10 more] [flagged] cjbgkagh 6 hours agoparentI don't know, it's another chip in the foundation, straw on the camel's back. While none of this is new to me, and was one of the reasons I left academia for industry a very long time ago, the general population are becoming increasingly distrustful of the academics, and trust once lost is very difficult to get back. reply CobrastanJorji 5 hours agorootparentYeah. While these specific professors or schools or research institutes may not see any immediate consequences, the general opinion of universities has been increasingly taking a beating. That's a dangerous thing in a place where there's an everpresent conservative drumbeat of anti-intellectualism in general and the liberal elites in their ivy towers. I'm not sure what the ultimate consequences will be for US universities might be if this course continues, but it won't be good. reply cjbgkagh 5 hours agorootparentThe problem with the left having a stranglehold on culture, and especially academic culture, is that its sacred cows are left un molested to the point they forget they have any. Healthy at any size is a personal favorite because of how immediately obvious it is false to even the most causal of observers yet academics and doctors will line up to preach that ridiculous gospel. When people see those who have died from Covid while being clearly morbidly obese being described in the news media as perfectly healthy with no preexisting conditions - it does beg a few questions. reply AmericanChopper 5 hours agorootparentprevI guess you can choose between worrying about the influence all these corrupt, ivory tower intellectuals have on society, or worry about them getting caught for it… reply tastyfreeze 5 hours agorootparentprev> conservative drumbeat of anti-intellectualism What does that even mean? reply andirk 5 hours agorootparentAnti-intellectualism is a very real thing in this country, and mostly found in ultra conservative circles. For example, it is VERY popular at certain rallies to believe that 1) Obama is still running the show behind the scenes as a communist, AND 2) Donald won in 2020, AND 3) Donald is the current sitting prez, AND 4) that the January 6th hilarity was both 4a) a psy-ops by the then Speaker AND 4b) those brave patriots that were involved are political prisoners. These are ANDs, not ORs. They are often all believed at once. Anti-intellectualism and a distaste for upper education in general because it \"indoctrinates\" is not fringe. Keep in mind no one in power of any political party actually believes ANY of that but it is one of the popular drum beats. reply cjbgkagh 5 hours agorootparentQ Anon is a modern incarnation of the Soviet counter intelligence program ‘Operation Trust’ when combined with the deliberate Pied Piper strategy it should be understood that the craziness of these people is something that was intentionally done to them by cynical political operatives. reply p-e-w 5 hours agorootparentprevThe \"foundation\" of Harvard is institutional power. Academic excellence is at best a consequence of that. I can guarantee that Harvard's many powerful alumni aren't going to let that institutional power be eroded, regardless of how bad academic standards ever become. And they control sufficiently many other institutions to ensure that Harvard retains its shine, at least to the outside world. reply ryandrake 5 hours agorootparentWhat’s the end state? Harvard eventually doesn’t even have to provide an education. It’s role is as a manufacturer of prestige. It selects N people a year, deems those people “elite,” and those people end up employed in elite investment banks and consulting companies because they were deemed elite by an institution with that power. Those elites then go on to recruit the next year worth of elites. I mean, this already is happening in name brand education institutions. The value is all in the name and network of fellow elites, and the education itself is largely irrelevant. I’d name schools, but I bet a lot of HN readership got their first lucrative FAANG jobs based entirely on their attendance at one of these institutions… reply patryn20 5 hours ago [flagged]prevnext [4 more] People are selfish trash. News at noon. There’s a reason most people aren’t wealthy: they care about people and honesty and integrity. You must be dishonest and sociopathic to be successful. Thats human nature. The world will be better once we’re finished destroying ourselves. Edit: the truth hurts. People are the cause of all the world’s problems. Edit edit: I’m in a bad mood tonight. I’m trying to comfort my tough as nails father as he cries because my mother is moaning in pain as her spine collapses from stage 4 metastatic cholangiocarcinoma. They worked their asses off for me in refineries, operating a trucking company, and just baking cakes and sewing dresses for extra money. They were gifted opportunities I’ll never have because of family connections. And it hasn’t saved them. Despite their late life wealth. Yet the people I grew up around that have done nothing but drugs and partying are still perfectly healthy. And it has jaded me more than I ever thought possible. reply dang 1 hour agoparentThis comment breaks the site guidelines, as have some others that you've been posting. Can you please not do this? We're trying for something other than slow internet degradation here. If you wouldn't mind reviewing https://news.ycombinator.com/newsguidelines.html and taking the intended spirit of the site more to heart, we'd be grateful. reply patryn20 1 hour agorootparentYou are of course welcome to administer this site as you wish. However: This site hasn’t represented balanced discourse or intellectual debate in at least a decade. I’ve watched it become just yet another Ivy League and big tech echo chamber as YC has pursued its existing investments over its stated goals and creative ideals. Money ruins everything. And I’m tired of it. reply boxed 1 hour agoparentprevAnd yet the world does in fact improve year over year. Kicking and screaming, but still moving forward. Technology and science improves and that compounds. reply User23 4 hours agoprev [–] I look forward to seeing the Gell-Mann amnesia the next time an obviously cooked climate study comes out and people get all worked up over nothing. If reading that makes you upset, do you have a rational reason why? Can you explain how climate science is immune to the social dynamics at work here? Or is it just knee-jerk offense at having your religious beliefs questioned? reply throwaway20222 3 hours agoparentCan you clarify what you mean please? And what evidence you have to support the social dynamics whichever way you are saying they are being swayed? This is a genuine question because I may be the one with the religious beliefs, but I am not beyond them being questioned. reply boxed 1 hour agoparentprev [–] The difference here is that anti-warming studies are the ones with the money behind them. Your conspiracy theory is falsified in that one stroke. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Allegations of data manipulation and misconduct have surfaced in cancer research at Harvard University's Dana-Farber Cancer Institute.",
      "Specific instances of data forgery and fraudulent practices have been identified.",
      "The credibility and competence of certain researchers are being questioned, and numerous retractions and corrections have been made as a result of these allegations."
    ],
    "commentSummary": [
      "The article and comment section discuss allegations of falsified research at the Dana-Farber Cancer Institute.",
      "Topics of conversation include challenges and biases in scientific research, university ranking systems, and declining teaching standards at Harvard.",
      "Additional topics touched on in the comments include racial disparities in academic performance, grade inflation, lack of concern for correcting fraudulent papers, body positivity, networking focus at Harvard, value of education institutions, climate science, and supporting social dynamics despite personal religious beliefs."
    ],
    "points": 159,
    "commentCount": 80,
    "retryCount": 0,
    "time": 1705976766
  }
]
