[
  {
    "id": 40053774,
    "title": "Unraveling the Life of Alex: A Hispanic Teenager's Journey from 1997 to 2021",
    "originLink": "https://pudding.cool/2024/03/teenagers/",
    "originBody": "1997 Avg age 13 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2013 2015 2017 2019 2021 Favorite ice cream (1997) No data This is a teenager by Alvin Chang This is a teenager. We'll call him Alex. It's 1997. Alex is a 13-year-old Hispanic kid being raised by his dad and step-mom. His family's net worth is less than $2,000. And his parents are neither supportive nor involved in his life. When researchers assessed his home and family life, they determined he was in a fairly risky environment. Over the next 25 years, researchers will continue interviewing Alex regularly. He'll be bullied at school. He'll be held back a few grades. He won't go to college. As an adult, he'll often live in poverty while struggling with his physical and mental health. But, right now, Alex is a teenager. And there's only so much control he has over his life. He thinks his teachers are pretty good. He'll soon go on his first date with a girl. And he's pretty optimistic about the future. In other words, he's like most other teenagers. Let's meet everyone else. In this story, we'll follow hundreds of teenagers for the next 24 years, when they’ll be in their late-30s. They're among the thousands of kids who are part of the National Longitudinal Survey of Youth. This means researchers have followed them since their teenage years to the present day – and beyond. (Click a kid to see details) A notable portion of kids, including Alex, have parents who are uninvolved and \"show less warmth.\" A lot of kids are in high-risk environments. Researchers determined risk by asking lots of questions. For example, they asked whether the kid has basic necessities, like electricity or a quiet place to study. They also asked about factors that could destabilize the home environment – chaotic routines, parents who have disabilities, or relatives struggling with substance abuse. And a lot of kids are growing up extremely poor – which, in and of itself, can be traumatic. A year from now, in 1998, a researcher named Vincent Felitti will publish a paper that drastically changes the way we think about these kids – and their childhood. The research will show that these childhood stressors and traumas – called Adverse Childhood Experiences – have a lifelong effect on our health, relationships, happiness, financial security, and pretty much everything else that we value. It will kickstart decades of research that shows that our childhood experiences shape our adulthood far more than we ever thought. While we can't track everything about these kids' lives, we can track how many of the following adverse experiences each kid experiences: Having uninvolved parents Held back in school School suspensions Being bullied Seeing someone shot with a gun In addition, I'll add in the home/family environment risk score to calculate the total adverse experience number. To make this easier to see, let's split the kids up into three groups – kids who experience… No adverse experiences (0) Some adverse experiences (1-4) Many adverse experiences (5+) Let's go forward in time. I'll check in at the end of high school. Keep scrolling ↓ 1998 1999 2000 It's 2001 – senior year of high school for a lot of these kids. Let's check in on the types of adverse experiences they've endured. Here are the kids who grew up in a high-risk home and family environment, based on their latest interviews. Some kids were held back in school. A few were held back multiple years. Some kids were suspended from school, a few were suspended multiple times. Many kids were bullied. And these are the kids who witnessed gun violence. Growing up around violence can lower a kid's attention, impulse control, and early academic skills. When we look at their high school GPA, we see that kids who experienced adverse events are more likely to struggle in school. Wooo! High school is over! Partayyyyy!!! Let's look at who is going to college It's now 2002. Most kids are done with high school and figuring out what's next. Kids who experienced adverse events were less likely to go directly to college. They were more likely to jump straight into the workforce or get stuck in the purgatory between high school and some version of adulthood. College isn't just a place that teaches you how to do a job; it's also a safe, structured, and productive environment for people to continue growing up – and to fend off adulthood for a bit. In the last few decades, more people have taken this path: Percentage of people 25 to 29 years old with a bachelor's degree Source: National Center for Education Statistics Sure, these kids are now 18 – technically adults. But psychologist Jeffrey Arnett argues that, in developed countries, there is an era between ages 18 and 25 when we collectively agree to let people explore the world and figure out what role they want to have in it. He calls it \"emerging adulthood\". And college is an environment built for emerging adults – a place where kids can leave their family environment and finally have a chance to independently shape their futures. It's 2003. More kids will eventually go to college – but it'll be uncommon for kids who experienced many adverse events. It's a cruel irony. Research has shown that even one year of college or technical school can mitigate some of the effects of adverse childhood experiences. 2004 Let's check in on Alex. He's 20 now. The last few years were tough. He was held back in high school, graduated with a 2.9 GPA, and didn't go to college. But he moved out of his parents' house and he briefly had a job as a grounds maintenance worker. He's not employed now. 2005 2006 2007 2008 2009 It's now 2010. About half of each group is now working. But the type of jobs they work largely depends on their education. Let's look at educational attainment Some of them have college degrees and are working jobs that pay them accordingly. Not surprisingly, the people with bachelor's degrees tend to be people who experienced fewer adverse events as kids. Bachelor's degrees have become essential for well-paid jobs in the US. Starting in the 1980s, people with four-year college degrees started earning more and more, while everyone else earned less. Median income for people ages 25 to 34, by educational attainment Source: National Center for Education Statistics, College Board, US Census Meanwhile, for the last several decades, people with four-year college degrees have reported being happier than people without. Happiness, by educational attainment Source: General Social Survey But in 2022, the average cost for first-time college students living in campus was $36,000 – nearly $10,000 higher than a decade prior. It's made college inaccessible for kids who need it most. 2011 Let's look at annual income It's 2013. By their late-20s, we can already see the income gaps among these groups. Notice who is earning less than $15,000 each year and less than $30,000 each year. The US poverty line is about $15,000 for an individual in 2024 – low enough that the government offers healthcare benefits for people who earn up to 4x the poverty line. It's 2015. In one year, the US will elect Donald Trump as president – a man who constantly insults poor people and calls them \"morons.\" This generation grew up hearing presidents say similar things. Ronald Reagan said people go hungry because of \"a lack of knowledge,\" and that people are homeless \"by choice.\" Bill Clinton said \"personal responsibility\" is the way to overcome poverty. We grew up in a country where most people believed the top reason for poverty was drug abuse, and half of Americans blamed poor people for being poor. 2017 2019 It's 2021. The research participants are in their late-30s now, which means they've had plenty of time to shape their own destinies. But we can clearly see that the experiences of their childhood had a huge effect on their financial situation as adults. It also has an effect on virtually everything else in their lives. This is how many times each person was a victim of a violent crime. This is how many parents, siblings, or partners have died thus far Remember, these people are only in their late-30s. This is the most recent answer each person gave about how often they were happy in the past month. All of this affects Black and Hispanic people significantly more – something Felitti found in his initial study. Ultimately, our childhood experiences affect our lifespan. People who endure adverse experiences report more health problems. Research has found that adverse childhood experiences increase the likelihood of being diagnosed with cancer, heart disease, mental illness – and all of this can lead to premature death. The survey only asks each person about their health every few years, so this visualization shows their most recent response. So, how is Alex doing? He's 37 now, living with his partner and two kids. After decades of working as a cook, he recently moved to a retail job. Over the last few years, his annual income was around $20,000. He has struggled with his weight for much of his adult life, and it affects his overall health. The last time he was asked about his mental health, he said he was depressed some of the time. The world has a lot of compassion for kids. When we're young, we have so little control over their lives. We play around, mess up, and get into trouble. We also endure dysfunctional homes, family chaos, violence, bullies, and whatever else comes our way. But we tell ourselves that, eventually, we'll get to shape our own lives. Then we turn 18 and we're expected to be \"adults\" and figure things out. If we fail, we are punished. We are blamed for not going to college, for being unhealthy, for being poor, for not being able to afford healthcare and food and housing. But it's not Alex’s fault. This is the same person we met 24 years ago. The world we've built has shaped his life. So he is our collective responsibility. They all are. Explore the data Select other factors to sort these people using the pulldown option on the top. Don't feel like scrolling? Watch the video instead! Click to close The Pudding is a digital publication that explains ideas debated in culture with visual essays. ABOUT FACEBOOK TWITTER INSTAGRAM PATREON PRIVACY NEWSLETTER RSS",
    "commentLink": "https://news.ycombinator.com/item?id=40053774",
    "commentBody": "This is a teenager (pudding.cool)1254 points by gmays 17 hours agohidepastfavorite676 comments cameldrv 12 hours agoThe punchline is this: \"It's 2021. The research participants are in their late-30s now, which means they've had plenty of time to shape their own destinies. But we can clearly see that the experiences of their childhood had a huge effect on their financial situation as adults. It also has an effect on virtually everything else in their lives.\" You cannot infer the direction of causality from this data, i.e. that the traumatic experiences themselves cause the poorer outcomes. I remember reading about how in Chicago someone had noticed that kids who did better had more books at home, so they decided to give poor kids books. Certainly not a bad thing to do, but just giving them some books is not going to make them like the better off kids in all of the other (highly correlated) ways that they're different. Just as an example, one of the traumatic factors they identify is if a kid had witnessed someone being shot. The wealthy kids are way less likely to see anyone get shot, because if people were regularly getting shot in their neighborhood, they would move. The poor kids' parents don't always have that option. In this case it could be the poverty itself, not the shooting that is causing the poor outcomes. But then you get into why the parents are poor in the first place, and there are many causes, but a lot of them get passed down to the next generation in one way or another. reply GuB-42 11 hours agoparentI think witnessing someone being shot is a good metric because it is factual. Either you saw someone being shot or your didn't, no ambiguity there, and no matter where you live, someone being shot is someone being shot. Not like \"uninvolved parents\" and \"bullying\" which are open to interpretation. This metric is also a proxy for living in a violent environment. It correlates with wealth, but it is also kind of the point. Children who lived in a wealthy environment are better off as adults in terms of income. It is not that obvious, as rich kids could simply burn through their family wealth. reply ip26 10 hours agorootparentIt’s likely strongly subject to Goodheart’s Law, however. In other words, there are probably many things you could do to improve the goal (e.g. figure out how to keep kids from seeing the violence) without improving outcomes for these kids (because they remain just as poor) reply wyre 8 hours agorootparentNot really? All things being equal a child that sees someone get shot will grow up more traumatized than a child that does not. reply kvdveer 3 hours agorootparentThat makes intuitive sense, but that's not enough. Many untrue things make intuitive sense, especially when it comes to poverty. Is there any research that shows that having witnessed someone get shot affects future prospects INDEPENDENT of the factors that lead to the kid witnessing someone get shot? reply ernst_klim 1 hour agorootparentprev> This metric is also a proxy for living in a violent environment. Probably, probably not. The probabilities of witnessing someone being shot is extremely low in both environments. If amount of people who are living in violent environment is much lower, it may be that a person who witnessed someone being shot is more probable from a good environment. https://www.anesi.com/bayes.htm https://www.youtube.com/watch?v=HZGCoVF3YvM reply renjimen 12 hours agoparentprevGiven the order of events (childhood trauma THEN adult outcomes), and the strong relationships identified in the source material (while controlling for confounding factors), I think it's about as close as we can get to inferring directionality. reply concordDance 11 hours agorootparent> I think it's about as close as we can get to inferring directionality. No, we can try interventions (e.g. do a big and expensive anti-violence/CCTV/policing campaign in a neighborhood) and record the result. I do think the grandparent has a point and a lot of these could have a common cause. e.g. a violent environment and poor educational attainment could both be caused by poverty or genes for impulse control or a subculture with a higher acceptance these things. reply cycomanic 1 hour agorootparent> I do think the grandparent has a point and a lot of these could have a common cause. e.g. a violent environment and poor educational attainment could both be caused by poverty or genes for impulse control or a subculture with a higher acceptance these things. How does a gene for (presumably less) impulse control make you more likely to have seen someone shot? And yes growing up in a poor/more violent environment makes you more likely to end up poor with health problems later in life is exactly the point of the study. reply renjimen 8 hours agorootparentprevFair. You can do those kinds of analyses from historical data too, though I don’t think CCTV would have much of an effect. Try free school lunches, after school support, parental benefits etc. reply ta1243 1 hour agorootparentHow would providing free school lunches reduce the chance of seeing someone shot? reply atoav 3 hours agoparentprevIf one feels unhappy about the causality link between a good childhood and a better life as an adult please remember that we are talking about statistical effects here. If more people who were bullied end up in unfortunate positions that doesn't imply direct causality, it implies that people whose live paths lead to bad places often had being bullied as a station on it. There will always be the tail ends of the statistical function, so people who became phenomenal adults despite all hardships, but also people who had a good childhood and became utterly disfunctional adults. But if we think about devising utilitarian political measures knowing what \"broadly\" has an effect on people is useful. Ideally you discover small things that if changed would have huge positive downstream effects. E.g. if bullying would be shown to have a big impact on later lives, it could be justified to pick up more funds to prevent it, to help victims and/or to change the way schools work in order to minimize chances someone is being bullied. Bullying is just an example, one could also pick other triggers. reply hn_throwaway_99 10 hours agoparentprevThere are common statistical techniques to better get at causality in this situation. E.g. given how relatively unlikely and random \"seeing someone getting shot while still a child\" is, it should be fairly easy to match this up with other variables to tease out causality, e.g. just looking at someone in the same socioeconomic situation, same parental situation (i.e. married/single), and then comparing gunshot witnesses vs. others. reply cycomanic 1 hour agoparentprevYou seem to construct a straw man. The whole point of the study is to show that kids that grow up with more adverse effects which are out of their control makes them more likely to have problems as an adult. You seem to say we can't infer causality, but that's exactly what they do. They show that having been affected by more adverse effects does make you more likely to suffer in the future. As the study says being poor is one of the adverse effects but not all. So that's your control right there. reply refulgentis 12 hours agoparentprev> Certainly not a bad thing to do, but just giving them some books is not going to make them like the better off kids in all of the other (highly correlated) ways that they're different From personal experience, I can absolutely vouch for that. 35, came from nowhere with nothing, absentee parents, out of house by 15. Dropped out of college, waited tables, did a startup, sold it, worked for 7 years at Google, now I'm doing my 2nd startup. Does it fix everything? No. But it gave me something to do that wasn't TV, and it kept me safe from [redacted] dad and [redacted] mom, I could hole up wherever I wanted and spend hours in them. You'd be surprised at the things that are lifelines. I had a really hard time explaining to this CS PhD dude who ran a weekend night basketball league for no particular reason how different and better that kept my life the last couple years of high school. You aren't shifting the whole distribution with one act, but just like the little shifts add up in the negative, they add up in the positive too. I remember a woman in her 30s running into me in the library lugging around those 7 volume MSDN published sets at 9 years old. She was incredulous and told me to keep it up. That mattered! No one had even noticed me or remarked on it before, gave me pride. reply brailsafe 8 hours agorootparentUp front, I have no intention of trying to detract from any of those accomplishments, because you've obviously been grinding pretty hard for a while and admire the tenacity you must have had as a kid and the progression you've seemed to follow. I do however find it under-discussed how many subsequent dice rolls have to at least partially work out for that tenacity, and those little shifts, to be a compounding positive instead of negative, and usefully applied long-term. I'd be curious if you had any major setbacks that you rebounded from after things started rolling successfully forward for you. Now at 32, unemployed with a spotty resume and no prospects, I could really use a &pointer (or reference ;)) Reading through your comment and picturing my own upbringing (poor, abusive, but I guess I got a handle on it and discovered programming through gaming eventually, it does make me sad that although there were hand-me-down computers available that I gravitated toward and experimented with, I could not picture where the nearest library was, and had to Google it now. I'm not particularly resentful though, I did get out, and I'm grateful for that. I wonder if the books alone would have been enough, but having the books and the physical escape together is kind of incredible, and it's heartening to hear you used the hell out of that space. Much earlier on, I had some exposure to small motors, and had some mentorship from my extended family on the programming front, but didn't really have a sense of how to build on that; no conception of how to connect motors with gears in a more complex system, no business exposure at all, no ability or framework for learning how to execute on any project, and just a debilitating lack of motivation up until around 17, along with no appreciation for the idea of proving myself measurably; I thought I was capable, but apparently wasn't. I got my little bots for Runescape running though, and that was empowering. Thankfully, I did and continue to have a similar refuge at the skatepark, which provided me some social and physical benefits for free, much like your basketball league, that a surprising amount of people I meet now don't have. I was nerdy, but couldn't execute, and couldn't see how I'd get there. My first job was a glimpse into how much potential there was available; I made more than my father who I was on good terms with, but then I was laid off for lack of reason to have me on the payroll, which took a positive signal and turned it into hopelessness in a way. I experienced adult job loss my first time trying. It was a great opportunity that I relish in some ways still. I then got another job as a frontend developer, making a bit more, and then burnt out, slowed down, and got fired, partially because I was trying to do CSS things that nobody was paying me to do, instead of just writing some JavaScript to handle dynamic layout and getting the job done. I was too deep in the weeds and got stuck there, but the idea of just cranking out things quickly wasn't stimulating enough and I'd just sit there trying to convince my brain to do the work. Since then, it's just been gradual pay increases, some early freelance clients that worked out for a while, but at this point I've never held a continuous job for longer than a year and a half, and I feel like the pieces of minor success are hard to stabilize, despite being in a wildly better situation still than I'd ever have imagined in high school, and a hell of a lot of personal inward reflection. My last job title was Software Engineer II, but really I'm just a generalist that keeps failing upward, and I don't know whether if I were to double-down and specialize more, go deeper, or pivot out completely, I'd be able to do that well; it's a bit of a constant existential crisis. It's hard to be consistent over a long period of time without a manager deciding I was a liability or me just burning out so badly, or a series of unfortunate life events coming together for the negative, and once you're out, it's extremely hard to get back in. For the last year, I've been working my way through Nand2Tetris, because in a career highlight I landed an actual interview with Apple (that ended up going nowhere, rightfully so because my lowest level knowledge didn't exist) as well as building a small SwiftUI project that may or may not see the light of day, and while I think those are positive moves, it's going to be a hard year ahead that may take me to net zero again unless I can pick up something in general labor for while (Waiting tables would be quite difficult without a solid short-term memory, and don’t believe someone would hire me for that with largely tech experience and random interspersed menial work). Anyhow, ultimately I wholeheartedly agree with your sentiment, those little shifts really do add up for either the positive or sometimes negative. I think the longer you can keep them positive, keep the ball rolling forward, the more likely things will work out, and as a society it's crucial we continue making it possible to smooth out the experience of life, especially for people who grow up in volatile situations. reply noisy_boy 7 hours agorootparent> My last job title was Software Engineer II, but really I'm just a generalist that keeps failing upward, and I don't know whether if I were to double-down and specialize more, go deeper, or pivot out completely, I'd be able to do that well; it's a bit of a constant existential crisis. As a generalist that still has the title Software Engineer after over 25 years of experience, I think I am able to empathize. I think, if you are a generalist and, like me, if you like \"laying the pipes\" to connect things end-to-end and see the satisfaction of having built the entire thing, embrace it. You should be proud that you can build a complete application though OS infra to database to backend services to frontend UIs and provide the glue of scripts as needed, all by yourself (not shitting on working in a team setting, just knowing that you could). I treat that as a badge of honor. Sure, I can't get super deep into one of these verticals, but then I'm a \"builder\" and I like the feeling it brings. reply brailsafe 6 hours agorootparentWell, I have and can do those things, and agree that those are very valuable skills to be proud of. I've just been kind of frontend only in team settings in a professional capacity lately, so it's something I'll be continuing to improve on. Most of my work has been jumping into some crazy existing codebase and figuring out how to understand and contribute to it, so greenfield buildouts are just not something I've repped out, and think that's a bit of a weakness. As in, I can set up a database, build an API, wrangle a vps, and then build the front-end, but I don't really have much of a sense of how to do it quickly or by using decoupled cloud service providers, simply because I've never been in that position. Laying the pipes is sort of the essense of productive software engineering in my mind. It is quite gratifying though to gradually be working my way to understanding how each layer of the hardware software stack work, and I'm starting to see those layers in real-world contexts, such as in getting a fault when compiling Swift, it'll show me the lower levels where the problem occured. reply noisy_boy 5 hours agorootparent> Most of my work has been jumping into some crazy existing codebase and figuring out how to understand and contribute to it, so greenfield buildouts are just not something I've repped out, and think that's a bit of a weakness. Without any context of the details of the work, one thing that has helped me is to lookout for scope of improvements beyond on the codebase itself. E.g. is there opportunity to provide a UI to the end-users of the code base. If so, since you have touched the codebase to contribute to it, your suggestion to work on those things to improve end-user's life may get accepted and then you have something relatively greenfield to work on. Doesn't always work out that way but sometimes it might. Another approach is building something on the side that you know will be very useful, even though nobody asked for it - helps you figure out the quick way of doing it (what you mentioned) since these are POCs and you can't repeatedly spend too long on them. reply brailsafe 4 hours agorootparentReally good suggestions, thanks. I suppose since I'm looking for work, it might not be a bad idea to do this externally as well, if any prospective companies' APIs are available, and use them as portfolio items. reply peoplefromibiza 33 minutes agoparentprev> In this case it could be the poverty itself unfortunately in the US socialists theories, even the most diluted ones, are almost entirely removed from the public discourse. These kinds of issues can be better analyzed in the context of the class struggle (or class conflict), of which they are a textbook example. On a personal level people can get over hardships and have a successful happy life, but statistically, on a societal level, those who are born poor will, more often than any other group, end up being poor(er) adults. reply James_K 12 hours agoparentprev> But then you get into why the parents are poor in the first place, and there are many causes, but a lot of them get passed down to the next generation in one way or another. Are you trying to say that these people are genetically poor? reply int_19h 3 hours agorootparentTo give one example, today's wealth distribution in UK still correlates quite strongly with Norman descent from the original participants of the Conquest. That's over 1,000 years of still-measurable generational wealth transfer. reply Staple_Diet 2 hours agorootparentprevI took it to infer that there are systemic factors that disadvantage segments of the population disproportionately and across generations. Having worked with disadvantaged and vulnerable populations I would agree, we only hear about the pulled up by the bootstraps success stories and readily ignore the 99.99% of cases where offspring are worse off financially than their parents. reply what-the-grump 9 hours agorootparentprevGenerational wealth is a thing... reply subpixel 17 hours agoprevPositive relationships with adults is shown to be means of counteracting adverse childhood experiences. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8237477/ I volunteer in a local school. It's not always fun, but something has to change. reply fzeindl 14 hours agoparentThis is old news. Basically children in bad situations need just one reliable person who believes in them in their lives. What it does is making them realize that it’s not them who are doing something wrong but that their surroundings are flawed. The problem begins when children start to believe everything is their own fault. reply hn_throwaway_99 12 hours agorootparent> What it does is making them realize that it’s not them who are doing something wrong but that their surroundings are flawed. The problem begins when children start to believe everything is their own fault. This is only tangentially related, but I think your point is critically important. Relatively recently I did ketamine infusion therapy for depression, and it was life changing for me. Ketamine is a \"dissociative\", and one thing that it seriously helped me do was separate my \"self\" from my depression, which I've never really been able to do before despite decades of trying through therapy. That is, now that I see depression as a chronic condition I have (say perhaps analogous to people that have to deal with migraines), as opposed to something that I am at my core, it makes it much, much less scary and threatening to me. In my experience, I've noticed that the people who I think of as the most successful (both from a society-wide and personal perspective) have the clearest view of what is their control and what they can accomplish, and also what is not. A huge benefit of this is that when they see an obstacle that some person could potentially overcome, even if it would be very, very difficult, they tend to think \"Heck, why not me?\" And when they do hit setbacks because of the unpredictability of the world, they don't take it personally, they just tend to think \"Well, the world is chaotic - is this new problem something that can reasonably be overcome?\" I contrast with a mindset I had for a long time (which a large part I think was a consequence of being bullied) that if I put a lot of effort into something and just didn't succeed, it was fundamentally because I wasn't \"good enough\", so why bother trying that hard at something else as I'm likely not going to be good enough there either. reply quadragenarian 8 hours agorootparentVery true. Self-confidence and grit are immensely important in overcoming or even just rationalizing the obstacles of life, doing so in an almost logical way without letting a person's self-defeating emotions or perceived shortcomings get in the way. It's such a huge divider and it singularly is based on what kind of adult(s) that person had in their life when they were young. reply quadragenarian 8 hours agorootparentprevThis is 100% accurate. In the wise words of the late child psychologist Urie Bronfenbrenner, Professor Emeritus of Human Development and Psychology at Cornell: In order to develop – intellectually emotionally, socially and morally – a child requires participation in progressively more complex reciprocal activity on a regular basis over an extended period in the child's life, with one or more persons with whom the child develops a strong, mutual, irrational, emotional attachment and who is committed to the child's well-being and development, preferably for life. (Bronfenbrenner, 1991, p. 2) Or paraphrased by him: “Every child needs at least one adult who is irrationally crazy about him or her.” reply naasking 12 hours agorootparentprev> What it does is making them realize that it’s not them who are doing something wrong but that their surroundings are flawed Speculative. I rather think that it shows them that there are other ways of living and that they have agency to get there. reply fzeindl 3 hours agorootparentNo. They can see that there are other ways of living by watching their peers or random people on the street. But they need somebody to make them understand that it’s not them who are destroying the relationship with their surroundings and their chance for being happy but the other way around. Some children in bad situations understand that without guidance but they are rare. reply concordDance 11 hours agorootparentprev> The problem begins when children start to believe everything is their own fault. My experience is it's the opposite and you need to overcome learned helplessness and understand that you can change your life. Are there any good studies that could tell us which of us is correct? reply anotheruser13 6 hours agorootparentLearned helplessness can also afflict adults, especially those who are not accustomed to dealing with computers. I get that quite a bit, and it's not just older people. Young people who have only used tablets, phones, and Chromebooks also are affected. YMMV reply Jerrrry 14 hours agorootparentprevding ding ding! I call it \"Bastard's Syndrome\" reply causal 15 hours agoparentprevYou can select a dropdown at the end for \"Parenting style\" which divides the groups by number of parents involved. This seems to be the strongest correlator of any of the data shown. reply zeroonetwothree 14 hours agorootparentParenting style is much more likely to not be causative though reply drawkward 14 hours agorootparentCitation, please? reply naasking 5 hours agorootparentStudies of identical twins raised apart, like MISTRA, show remarkable similarities in life outcomes and big five personality traits. There's a big genetic component, and the time spent with parents is dwarfed by time spent with others once they start school, so the role of parenting style on life outcomes would have to have a significant outsized effect compared to all other life experiences. If I recall correctly, absent neglect or abuse, parental influence doesn't matter as much as people think. reply gowld 11 hours agorootparentprevStrong argument for helping more parents be more involved. reply richardlblair 14 hours agoparentprev> I volunteer in a local school. It's not always fun, but something has to change Teachers and volunteers are how I was able to find a better life. What you're doing matters. reply kulahan 11 hours agoparentprevHow do you volunteer at the local school? My wife and I are both passionate about and interested in improving children’s lives, but not super sure how best to do it outside of donations and big brother big sister-type programs. As an aside, maybe it’s because I’m inexperienced, but I’m finding it surprisingly hard to get connected with a group to help people that isn’t a highly specific cause like religion, LGBTQ, children of certain races, etc.??? Is it just me? I am clearly very ignorant about all this reply anotheruser13 6 hours agorootparentI would like to volunteer as well, but it would have to be outside of home and school since I live in Texas. I would like to help young people learn to cope with being LGBTQ+, ADD, and other things, but I don't think parents would appreciate it. reply valval 11 minutes agorootparentYeah maybe you should reserve that stuff for your own kids. Which you probably don’t have. reply seb1204 10 hours agoparentprevThank you for volunteering. reply toomuchtodo 17 hours agoparentprevLess kids in households that don't want them. This is a pipeline problem. Intentional children only. Hard topic to cover online, nuance and emotions on the topic. > I volunteer in a local school. It's not always fun, but something has to change. You're a good person doing necessary work. There aren't enough humans doing it, but it matters to who you're helping. reply bumby 17 hours agorootparentRelated, and equally hard to cover online: https://en.wikipedia.org/wiki/Legalized_abortion_and_crime_e... reply toomuchtodo 17 hours agorootparenthttps://news.ycombinator.com/item?id=39978774 (\"HN: Steven Levitt and John Donohue defend the abortion-crime hypothesis\") https://www.economist.com/by-invitation/2024/04/08/steven-le...https://archive.today/m3zl0 (\"Steven Levitt and John Donohue defend a finding made famous by “Freakonomics”\") reply throwway120385 16 hours agorootparentprevIt would also help if more people that are doing marginal work could receive a wage that they felt secure with. Money is one of the biggest stressors for couples and families. reply bsimpson 15 hours agorootparentI don't understand how to practically make this work. There's a strong case to be made that a minimum wage helps people whose value approaches the minimum while hurting people above or below (e.g. $12 and $18 wages in an unlimited market both round to $15 with a minimum, while someone who only produces $7 of value is no longer employable). Similarly with cash infusions - giving people more money is inflationary. Nobody wants to live in a world where people are trying to participate in society and failing. That's truly heartbreaking. At the same time, naive solutions (decide a \"living wage\" and force people to pay it, set up and enforce rent control, give out stimulus payments) seem to have a lot of second-order effects/unintended consequences without actually solving the problems they're meant to solve. reply Misdicorl 14 hours agorootparentMy personal position is to abolish the minimum wage and update the tax scale with negative tax rates that support a reasonable quality of life at all income levels. The market will find its own balance for what a true minimum wage is in that environment (and not have weird perverse incentives like you state). Yes, this is UBI. But phrased as a tax cut makes it politically viable (at least in the US). reply bsimpson 14 hours agorootparentI would be interested to see this modeled. One of the classic unintended consequences of social welfare is making someone at the bottom unwilling to work. We saw this during the pandemic when people in formerly low-wage jobs got a lot of cash assistance and stopped being interested in low-wage jobs. (Remember all the \"help wanted\" signs and early closing hours at local restaurants?) I'm curious to see an example scale that would continue to incentivize social behavior the whole way up the chain - avoiding the \"oh I don't want to make $100 more dollars because I'm in a sweet spot now and bad things happen at $99.\" You can certainly argue that many of the current disincentives are bugs in the bureaucracy. I'd like to see a proposal for the UBI tax scale you describe that doesn't have any bugs (that is, bumps in the distribution where people are afraid to reach for state C from state A, because the intermediary state B is worse than A). reply int_19h 3 hours agorootparentThat \"classic unintended consequence\" was specifically tested many times in UBI context, and study after study doesn't find it in any noticeable amount. In any case, given how badly broken the current system is, surely it's at least worth a try? reply Misdicorl 13 hours agorootparentprevVery much agreed that there should be no cliffs. Every dollar earned should at minimum increase your usable cash flow by at least X amount no matter where you are in the income distribution and other tax incentive phase space reply magicalist 12 hours agorootparentprev> We saw this during the pandemic when people in formerly low-wage jobs got a lot of cash assistance and stopped being interested in low-wage jobs. (Remember all the \"help wanted\" signs and early closing hours at local restaurants?) Unwilling to work or temporarily not desperate to stay alive? How many receiving assistance were still working, just doing it less? The only studies on outcomes I recall is that a lot of kids were no longer experiencing food insecurity. reply kulahan 11 hours agorootparentI can’t imagine they were very compelling studies if the only changes they could come up with was “some kids were less hungry” reply naasking 5 hours agorootparentSomeone doesn't understand the effects of food insecurity: https://www.heart.org/en/news/2021/09/22/food-insecuritys-lo... reply Freebytes 14 hours agorootparentprevWe should not make it more than $1000 per month. Very few would choose to be poor. It would put a lot of pressure on companies to pay decent wages, though. reply Misdicorl 13 hours agorootparent$1000/month is $12,000/year. Thats far far below poverty levels. It needs to be enough that people can choose to supplement in order to engage with luxury consumption. If people are forced to supplement to just survive, then we need to maintain the minimum wage and a whole host of other weird baggage. reply naniwaduni 10 hours agorootparentThe 2024 FPL figure is $15060 + 5380 per additional person family member past the first. $12k/head/year comes up a bit short for an individual, but it's not that far off—expenses involved in holding down a job probably actually account for the difference anyway. It also becomes clearly tenable with households of more than 1. Supporting a family of 2-3 on $24k-36k is like, yep, I've met married international grad students. Of course they'll spring for supplemental income where available, but as a baseline it is tenuously \"enough\". reply Misdicorl 8 hours agorootparentThe goal can't be to solve every desperation case. But if the program wouldn't allow individuals living in dangerous and exploitative situations to confidently leave them (financially) Id argue the program was a failure reply KittenInABox 13 hours agorootparentprev> One of the classic unintended consequences of social welfare is making someone at the bottom unwilling to work. We saw this during the pandemic when people in formerly low-wage jobs got a lot of cash assistance and stopped being interested in low-wage jobs. (Remember all the \"help wanted\" signs and early closing hours at local restaurants?) I remember this, the cash assistance gave people back their time to focus on starting their own businesses, pursuing self-education, taking care of their kids, etc. It was fully apparent to me that these low-wage jobs effectively trapped people by sucking up all the time they had for self-improvement. reply Freebytes 14 hours agorootparentprevIt is important that this is based on all income levels equally. Yes, some will pay back that money in taxes, but the important part is keeping the amount equal. It would be even more effective if you gave them a monthly check (even if you would eventually take it all back via a consumption tax on people earning more). A ~25% national sales tax should be sufficient to cover a UBI program. (We should still have an income tax, though.) Furthermore, a consumption tax would decrease unnecessary spending since you can target only new products and not used products to encourage people to reduce, reuse, and recycle. reply Misdicorl 13 hours agorootparentIf UBI is encoded as a negative tax rate at low income levels, it no longer really makes sense to talk about it as applying to all income levels equally. It naturally gets distributed as 1) A check (issued by Social Security service?) if income is less than X 2) Less of your paycheck being withheld if your income is greater than X (or more if you're significantly above X, depending on how this gets funded) reply naniwaduni 12 hours agorootparentprevWe have a tax rate with negative tax rates at the low end of the scale. For sketchy social policy/political tenability reasons it doubles as a child subsidy and phases in up to a nominal amount of preexisting so-called earned income, but functionally that's what the earned income tax credit is. Expansion of the EITC program is fairly well-regarded among economists and has been historically quite popular! We should do more of it! reply Misdicorl 12 hours agorootparentTrue. It would be nice to decouple it from children and expand its scope of economic impact dramatically. reply pants2 14 hours agorootparentprevI have a family member that is severely disabled. She used to be on a program where the government would supplement her wages - she worked at Jack in the Box, where her employer would pay like $3/hr and the government would top that up to $10/hr. Now that program is gone and minimum wage for fast food is $20/hr. She simply cannot perform $20/hr worth of work, so she's unemployed (and living on government assistance). The previous arrangement was fantastic because the work gave her a purpose and something to do all day, and she contributed to society while saving the government money. Now she stays home and watches TV endlessly. This has informed my ideas - I think supplementing minimum wages could be a better alternative to UBI (with some exceptions). reply Clubber 13 hours agorootparentThere's usually carveouts for people with certain disabilities. It allows companies to pay them below the minimum wage. I would be surprised if that was abolished with the increase in minimum wage. reply ineptech 14 hours agorootparentprev> someone who only produces $7 of value is no longer employable This is the wrong model. You're using a worker's wage to describe their productivity, and a big reason for the mess we're in is that wages stopped increasing with productivity fifty years ago. (search \"wages productivity graph\") reply bsimpson 14 hours agorootparentThis feels like you're nitpicking the language, not the thinking. Imagine someone's contribution to a business increases revenue by $1000 and the total cost to employ that person for the same period is $800. Do you think most businesses would go \"nope, we only hire highly leveraged people who produce $2000 in revenue\"? There are inefficiencies in scale (like communication/bookkeeping overhead) that might disincentivize a business from growing, but generally speaking, I think it's fine to model decisions as rational cost/benefit ones. Workers who are only \"worth it\" at some wage. Nobody is going to pay you a million dollars to go sell a hundred dollars worth of stuff. If the value you can earn on the market is sufficiently lower than what someone is allowed to pay, they simply won't hire you. That's bad for everyone. reply ineptech 10 hours agorootparentComparing someone's wage to the value they produce is a fine way for a company to decide whether to hire someone, but I didn't think that was the question you were trying to answer, was it? Perhaps I misunderstood, but I thought you were asking something like, what policy would help people who are at the margins, which is an economy-wide question that can't be answered from one employer's perspective. Workers may only be \"worth it\" at some wage, at some point in time, but that wage is subject to supply and demand just like everything else. A policy intervention like raising the minimum wage will alter that supply/demand curve. For example, suppose janitors all make the minimum wage. If we increase it, there might be some company at the margin that will go without janitorial services, but most companies will pay their janitors the new wage, which (from the \"a worker costs $X and produces $Y\" model's perspective) will look a lot like the nation's janitors suddenly started producing more value. Ergo, it's not to say that that model is wrong, just that it's not useful in answering a question like should we increase the minimum wage. reply valrix 10 hours agorootparentprevExcept an employee's contribution isn't static, while their cost is. $7.25 x 8 hours = $58 for the day. However what they create is based on output, which for most businesses, varies day by day along with their sales. A McDonald's could sell 500 burgers in one day at one location, but only 300 at another. In this case the employee at the larger restaurant generates 2/5 more value than the employee at the smaller one, even if both can output at the same speed and quality. So, in reality, the employee at the larger restaurant is being exploited by 2/5 more than the employee at the smaller location. Which also means the employee at the smaller location is getting paid more for doing 2/5 less work than an equally capable employee. Profits are multiplicative yet unpredictable, while labor is static and predictable. reply Aunche 13 hours agorootparentprevA rather low hanging fruit is smoothing out welfare cliffs so poor people don't feel stuck in an position of a local maximum of utility near the bottom. The problem is that these initiatives are very complicated, and you get more public support just blindly throwing money at the problem. reply throwway120385 13 hours agorootparentprevThe real issue is that a few people have accumulated a lot of wealth and property, and they use it as a tool to extract even more money. It's basically the late stage capitalism money vacuum hoovering up everything. In the past the only levers we had against this were breaking up firms and enforcing anti-monopoly and preventing capital from even entering certain parts of our economy. We could, for example, ban private equity companies from buying houses and healthcare companies, break up national monopolies into regional companies, and eliminate a lot of the consolidation that has traditionally enhanced the bargaining power of the company owner against the employees. In the short term it would make a lot of stuff less efficient, but when people talk about \"efficiency\" they really mean driving costs down and driving income up. So we really don't want an efficient capitalist economy, we want a capitalist economy that is just efficient enough to meet our needs while not being so efficient that a few people can exploit that efficiency and run away with our things. reply carom 12 hours agorootparentprevAbolish the minimum wage along with density restrictions in zoning. Make it affordable for someone making $300 per month to have shelter. reply bjt 15 hours agorootparentprevI don't think it works if we're narrowly focused just on wages, but I don't know why that has to be the only focus. If we as a society want to support people having a baseline quality of life, then let's pay for it together rather than pushing it all on employers. I don't think we put enough money behind it today, but the Earned Income Tax Credit is designed to do this while minimizing the disincentives for people to work. https://www.cbpp.org/research/policy-basics-the-earned-incom... reply SkyBelow 14 hours agorootparent>If we as a society want to support people having a baseline quality of life, then let's pay for it together rather than pushing it all on employers. Baseline quality of life isn't decided just by pay. I find that society doesn't support people having a baseline quality of life when it comes to areas other than pay, so it makes me question the motives of society in the case of pay. reply toomuchtodo 16 hours agorootparentprevI do not disagree. But it will take years, if not decades, for labor rights and organizing to improve the situation you mention. Preventing unwanted children takes less time and effort, tragic as it is to type out. reply smeej 14 hours agorootparentprevYou can change up the emotions on the topic pretty quickly if you change the framing to \"intentional sex only\" rather than \"intentional children only,\" even though the former accomplishes the latter. It's fun, because you can get virtually everyone to agree that people should only have sex they mean to have, but as soon as you suggest they should only have sex when all parties involved have carefully and accurately assessed the risk of pregnancy, you're a killjoy. reply NoMoreNicksLeft 15 hours agorootparentprevnext [21 more] [flagged] toomuchtodo 15 hours agorootparentAre you suggesting that humanity will die out so long as only willing, intentional parents have children? That is an interesting thesis and conclusion to come to (total fertility rate = 0 vs somewhere between 0 and 2.1 [replacement rate]). We should empower people who want children to succeed, and empower people who don't want children to never have them. What happens after that, we can solve for. reply somenameforme 14 hours agorootparentI think many people have a misunderstanding of what lower fertility means. Imagine a country has a fertility rate of 1. It doesn't seem that bad because it's pretty close to replacement. But fertility is an exponential system. So a fertility rate of means that each following generation (~20 years) will be half as large as the one that came before it (the formula is simply a ratio of fertility_rate/2). So you won't see any problem at all until the first generation to have low fertility starts to die. At that point you suddenly start seeing a rapid exponential effect. Every ~20 years (the size of a generation) your population size will be decreasing by 50% !!! And this never stops until you go extinct (which won't take particularly long), or start having children again. It's easy to imagine 'oh we'll just fix it if it becomes a serious problem like that' but imagine the state of society when that starts happening. All markets/consumption will be decreasing by 50% every 20 years, there will be a very upside down population pyramid where the overwhelming majority of the population will be elderly and need care, so forth and so on. Japan, for instance, hasn't even hit the worst of it yet. Their fertility plummeted about 40 years ago. So their 'final stage' is yet still about 20 years away. Today are the good times for Japan, relative to what they have ahead of them. Given most of the Western world can't maintain a remotely stable fertility rate in the current situation, doing something that would likely quite substantially lower it even further is indeed speedrunning the extinction of Western civilization! reply maherbeg 12 hours agorootparentSaying society is going to collapse due to population decline is about as absurd as saying society will collapse due to overpopulation. What will end up happening is that some groups will maintain replacement fertility levels or higher, and some groups won't, but we'll trend towards an equilibrium, or flop between too little fertility and too much fertility. Either way we won't go extinct unless we explicitly kill each other. reply somenameforme 39 minutes agorootparentIt depends on what you mean by society. If you mean the entire world, then of course I agree with you. Fundamentalist Islamic nations will never have a problem with fertility. China was happy to force abortions on people to fix one perceived fertility problem in the past, and I think there's 0 doubt they'd go equal but opposite in the other direction to fix another one in the present, if necessary. And so on. But again there's this irony in that the nations and cultures that are most susceptible to collapse due to lack of fertility, are also those least able to centrally fix the problem. But that lack of central dominance is also precisely why it's such a shame to see these societies dying off - and yes, they can and will go extinct. I am not exaggerating in the least about what a fertility rate of 1 would look like. You can see Japan's population pyramid here [1]. It really really emphasizes that today is the good times for them, relative to what they have to look forward to. Their current population pyramid is vaguely pyramidish. But over the next ~20 years, it will be completely and absolutely flipped upside down, and their population, economy, and everything else will fall into complete collapse. No culture will be able to sustain itself when its population starts declining at an exponential rate. This will be made even more true by the fact that it will likely trigger a vicious cycle where the massively skewed age ratios, collapsing economy, and other issues will further reduce fertility. [1] - https://en.wikipedia.org/wiki/Demographics_of_Japan#/media/F... reply lancesells 14 hours agorootparentprev> imagine the state of society when that starts happening The state of my own personal society is my apartment costs $1000 more than it did two years ago and my food costs about 25-30% more than it did two years ago. I definitely wouldn't consider having another kid, nor would I encourage my own to have one. reply somenameforme 13 hours agorootparentAbsolutely understandable, yet it also leads to a somewhat odd and undesirable reality. The problems you're talking about are ones that we inflicted upon ourselves, owing largely due to poor systems (and societies) enabling the turds of society rise to the top. So what will the systems and societies of tomorrow look like? Every child born tomorrow is basically just a lottery roll against all people having children today. And today you have vast numbers of intelligent, educated, conscientious, and far thinking individuals are simply removing themselves from the gene pool; that lottery roll for the children of tomorrow is looking less and less pleasant. There's this irony that the sort of mindset that might consciously make the decision to not have children is the exact sort that should be raising a family 1800s style, if we want a better world. Maybe there's just something about successful urbanization that ultimately causes societies to reboot. The Roman Empire also faced a major fertility crisis in its final years. reply et-al 14 hours agorootparentprevWe understand the implications, but I would not want anyone to bring forth children that are unwanted, thus bringing more unhappiness into those parents' lives (and their surrounding community). Our society needs to treat children as a gift and not just \"thoughts and prayers\" about raising them. reply toomuchtodo 14 hours agorootparentprevCurrent global population is ~8 billion. Momentum will take us to ~10 billion by 2100. Will it be a challenge to manage this rapid population decline and attempting to provide real, meaningful community support [1] and social systems [2] to potential parents to encourage them to have children (in order to raise the total fertility rate to a steady state level)? Certainly, without any doubt or hesitation. We spent up a credit card balance of sorts with a ballooning global population, that debt will need to be paid back in various ways. But extinction? Hardly. [1] https://www.theguardian.com/world/2023/may/29/baby-boomtown-... [2] https://ifstudies.org/blog/pro-natal-policies-work-but-they-... reply worik 14 hours agorootparentprev> speedrunning the extinction of Western civilization! Western civilization was always a good idea, never achieved, and they have had their day reply NoMoreNicksLeft 14 hours agorootparentprev> I think many people have a misunderstanding of what lower fertility means. Imagine a country has a fertility rate of 1. It doesn't seem that bad because it's pretty close to replacement. That's not even close to replacement. It's somewhere above 2 (often cited as 2.1, but it may be more like 2.04 in times of peace) for it to be replacement. If you could magically make fertility be that number, population increases would only come as a matter of life expectancy increase. 1 actually implies some sort of high-speed demographic implosion that will wreck an economy within a single human lifetime. > It's easy to imagine 'oh we'll just fix it if it becomes a serious problem like that' If it takes 30 years to recognize the problem, then one generation has already aged out of ever possibly being able to fix the problem, and the next generation is getting too old to be able to fix it (unless you can do so instantly). You've only got a few generations at a given time that can fix it. > Japan, for instance, hasn't even hit the worst of it yet. Their fertility plummeted about 40 years ago. There are fewer people living in Japan today, than there were a year ago. They didn't leave to go elsewhere. They died. And it will be like that every year until there are zero Japanese left. They have been functionally extinct for a few years now, though they may not know it yet. > it even further is indeed speedrunning the extinction of Western civilization! There haven't been distinct, compartmentalized civilizations on Earth for over a century at this point. There's only the one civilization. And, if it dies, there likely won't be another. Who had \"can't be bothered to fuck\" on their Fermi's Paradox bingo card? reply prmoustache 14 hours agorootparentprevEven if it was the case, would it be a problem? What is more important, having less humans being happy or more humans having a crappy life. Why should specie survival be more important than overall happiness of those that would have lived? reply somenameforme 14 hours agorootparentThis is a tangent, but when in the world did \"happiness\" become a desirable metric? If you think about it, it's really quite absurd. Happiness is a brief liminal state that should be triggered by relatively infrequent events. It is not a normal, nor desirable, default state. Contentedness, satisfaction, at-peace, and so on - there endlessly more rational, logical, desirable, and attainable things to aim for. Yet everybody always says happy. Maybe this even goes some way towards explaining the plummeting mental state of the West at large. If one sets their life goal towards happiness, then they're ironically certain to end up unhappy, unsatisfied, and discontented. reply lancesells 14 hours agorootparent> Contentedness, satisfaction, at-peace, and so on Could you explain to me how this is not another name for happiness? reply somenameforme 14 hours agorootparentThey are extremely different states of being. You are happy to receive good news, or for something to turn out well, or whatever else. But it is not a resting state. It's a liminal state. Contentedness, by contrast, is a resting state. You can awake contented, fall asleep contented, and spend your days contented. You may rarely, if ever, experience happiness - yet find yourself able to find satisfaction in life nonetheless. By contrast a pitiful, depressed, self loathing individual, can experience happiness as much as anybody else. But he is most certainly not content nor satisfied. Perhaps a junky would be another example. A junky certainly experiences happiness when his poison enters his veins, yet he almost certainly is far from content or satisfied. reply prmoustache 13 hours agorootparentYou are just being pedant while answering to a non native english speaker. reply zbentley 14 hours agorootparentprevAgreed that affirmative happiness is very hard to think about as a target. But I find that most people, when they say that, actually mean reduction of suffering. That's easier to quantify--but still quite difficult, like most quantities in social research. reply somenameforme 12 hours agorootparentDo you not then run into other problems? For instance I find that lifting brings an immense amount of contentedness, yet it's essentially hours upon hours of self inflicted suffering and pain. The same is true of family. Somebody raising a 2 year old could describe it in many ways, but reduction of suffering would not be one. Such things greatly contribute to this sense of contentedness and satisfaction. reply worik 14 hours agorootparentprev> when in the world did \"happiness\" become a desirable metric? Happiness is not a metric, cannot be measured, and is one of the most important things Despite it being unmeasurable we know that economic security increases it reply nsxwolf 12 hours agorootparentprev\"Happiness\", not \"perpetual state of unbridled ecstasy\" reply styxfrix 15 hours agorootparentprevPresenting dire conclusions without providing a shred of substance? reply doctorpangloss 17 hours agoprevThe visualization will frequently incorrectly show something of the form:True True False False True True False False reply sweetbacon 16 hours agoparentYes I saw this on a few \"screens\" and it really confused me at first. They flashy visuals detract from the message in a variety of ways. reply flanbiscuit 14 hours agoparentprevI thought I just wasn't understanding the visualizations. Glad it wasn't just me. It also wasn't very clear to me what I was supposed to be noticing in the visualizations that was related to whatever text was currently popped up. In the end I just watched the youtube video that was linked to at the very beginning and it made everything much clearer to me. reply pteraspidomorph 16 hours agoparentprevI noticed this on Relatives died (thus far). reply poutrathor 3 hours agoparentprevAlso, the visualization let you think that all the leftmost teenagers are the same ones stacking the bad things. That might be true, but I doubt it is. The part around Highschool was especially unclear. Are they the same teenagers getting all the bad stuff. That would be plausible but not to the extend the visual displays I guess. In other news, I hate that trend of scrolling to animate to get content. reply sebstefan 2 hours agorootparentThat's why you can see them run to one place or another It's the same cohort of people all the way through and each little character moves according to the survey they filled out each year reply fillskills 12 hours agoparentprevSaw in the \"Parents Involved\" section reply SuperHeavy256 16 hours agoparentprevYeah I agree this was very confusing. reply imacomputertoo 14 hours agoprevThe conclusion of this data presentation is that so of these people are our collective responsibility, and I just wasn't convinced. I wish they had shown percentages with the visualization. They choose not to. I was underwhelmed by some points that seemed like they should have been more shocking. Look at the huge number of people in the many adverse experiences category who made it to college, and make a high salary. that was shocking! and look at the people who had no adverse experiences and still managed to end up poor. how does that happen? I was left with the impression that if the government threw a lot of resources at it we might be able to move a noticeable percentage of those people in a better direction, but not most of them. The questions that remain are, how many people's lives could we improve and by how much? And, critically, how much are we willing to collectively sacrifice to move that percentage of people in a positive direction? reply Red_Leaves_Flyy 10 hours agoparentThe point is, likely intentionally, understated. I cannot speak for the author, but the gist I got is that our society thrusts wholly unprepared people into adulthood and we could get a lot of improvements from just making it harder for people to fail at adulting. IYKYK and if you don’t you will get fucked - repeatedly. Basic life skills are not taught so it’s up to the individual if their family fails. Importantly, it is unreasonable to expect someone to teach another how to do something they don’t know how to do. I’m talking about stuff like navigating health insurance, paying taxes, budgeting, managing credit, home maintenance, vehicle care. Mistakes in any one of these domains can have devastating consequences that profoundly change one’s life. Simple things like single payer health care (only complex because of greedy people demanding a tax for the privilege the laws wrote grant them), personal budgeting education, and teaching basic home improvement skills will markedly improve many people’s lives. We could also discuss more difficult topics like the complete lack of a meaningful social safety net, and the rippling consequences of systemic injustice but that’s less on topic and more likely to get me flamed or trolled. reply sabarn01 10 hours agorootparentThe outcome of this has been to make it harder to fail as a kid. We don't hold kids back anymore and we don't suspend kids anymore. At some point in time the rubber meets the road and you will be held accountable and have to be. We could improve the social safety net but we never want to match other countries that have more supervision of their at risk population. When I worked temp jobs there wasn't a place I worked where if you showed up on time two days in a row and worked hard I wasn't offered a job. All of these places paid well over minimum wage you just had to be willing to do hard physical work. Society plays some role but I have zero trust that our institutions know how to help people. reply batshit_beaver 10 hours agorootparent> The outcome of this has been to make it harder to fail as a kid. I'd like to go a little further and suggest that more recently there's been a trend of not holding the adults accountable either. Instead of trying to improve outcomes for all, we seem to have decided to choose the path of collective failure. reply Red_Leaves_Flyy 7 hours agorootparentWhen do the greater communities need to pay their dues? Schools cost money to run but taxpayers balk and cry over every cent increase. There are crumbling schools with toxic air and water that lack adequate HVAC paying their teachers unlivable salaries. This is the result of neglect to preserve and invest which is a condemnation of those who allowed such neglect on their watch when they should have championed such plights before they reached these new heights. Teachers can literally be miracle workers but that makes no difference if the communities their students return to undervalue education or lack the resources to foster healthy environments to grow and learn in. Broken communities create broken school districts. This goes back to the point I make in another comment on this page. We must invest in underperforming communities to bring them up to the average if we want to see improvements. This necessarily requires such difficult conversations like the poor Hispanic or black majority cities getting some of the education tax from rich white suburbs or something to the same effect. reply llm_trw 6 hours agorootparentSchools in the US cost more than schools in any other developed nation. Every institution in the US has been taken over by careerists and credentialists who produce nothing of value and are a drain on the system. For a simple example in our area look at twitter: we were told the servers would catch fire, the end times will be upon us and cats will live with dogs. Instead the servers kept chugging along just as well as they did before with a 20th the staff. At this point everything is so bad I'd support sortition for every public managerial position. You can't do worse than what we have today. reply tatrajim 5 hours agorootparentAs an anecdote on the topic of education, as a US Peace Corps Volunteer in rural South Korea in the 1970s, I routinely visited secondary schools that (at the time) were little more than drab warehouses for large (-70 students/class) using ragged textbooks and ancient furniture. Spirits were high, though, and these farm kids were successfully learning math through basic differential calculus plus a daunting array of other subjects. Thereafter, I have only felt (perhaps unfairly) mild contempt for the perennial whining of US critics who blame low funding for educational failing in the public schools. In my opinion the blame lies elsewhere, starting with the family. reply sabarn01 4 hours agorootparentprevMy kids school is terrible and they get about 22k per student per year in a rich area. The system is failing because it's designed to fail. reply monero-xmr 4 hours agorootparentprevAs a counterpoint, Boston spends more per student than every other city ($31.3k in 2023 dollars): https://www.bostonglobe.com/2023/05/30/metro/boston-now-spen.... But the outcomes are quite poor. How can society justify spending more on the same institutions that have miserable outcomes? In the private sector, less revenue forces belt-tightening, purchasing software and tools that enhance productivity, and ultimately bankruptcy if it can't work. Where in the public sector is anyone held accountable for failure? When will we accept that simply throwing more money down the pit won't solve what is a multi-faceted issue that primarily isn't about money? reply ZhadruOmjar 10 hours agorootparentprevThere are so many teachers explaining how and why kids don't fail anymore and that leads to issues from grade 1 to graduation. At some point people just need to _do the thing_. reply makeitdouble 6 hours agorootparentprevAnswering: > I’m talking about stuff like navigating health insurance, paying taxes, budgeting, managing credit, home maintenance, vehicle care. With: > We don't hold kids back anymore and we don't suspend kids anymore is a truely weird logic to me. Is it related ? Or are you offering to let kids get credit lines and suspend them over their mismanagement ? That could actually be a great idea TBH. And while we're at it, adults could also get suspended or have to attend additional courses, instead of getting thrown into debt spirals. reply jandrese 8 hours agorootparentprevI went to primary school in the 80s and 90s and even back then it was pretty hard to be held back a grade level. Typically it only happened when a kid missed a lot of school, like they were hit by a car and spent 2 months in the hospital. Failing grades alone didn't usually cause it, at least the kids who seemed completely uninterested in school still somehow managed to graduate. reply lynx23 1 hour agorootparent\"Everybody is a unique snowflake\" attitude is causing way more problems then we publically admit. Setting boundaries is important. As is seeing the consequences of your own actions. I was held back in school for a year. Looking back, this was one of the most important things in my school time. I am glad it happened. reply awwaiid 9 hours agorootparentprevI think ... right ok, I guess harder-to-fail but really it is easier-to-fail, easier to remain in a failure state, as a kid right? Same thing eh? reply chefandy 7 hours agorootparentprev> We don't hold kids back anymore and we don't suspend kids anymore. Does that contradict real data that shows holding kids back and suspending them makes them more successful? reply Red_Leaves_Flyy 7 hours agorootparentReal data on holding kids back is actively harmful. https://www.tandfonline.com/doi/abs/10.1080/03055698.2014.93... The same holds for suspensions. https://edsource.org/wp-content/uploads/2018/09/Noltemeyer_W... reply sabarn01 4 hours agorootparentIn both cases the point is benefit the system at the expense of the child with the issues. One kid should not be allowed to ruin a class. My kids school has emotionally disturbed kids in the classroom making it impossible to have regular lessons. When I was kid we had people that brought guns to school and were kicked out it seemed reasonable to me. I also think alternate school is a reasonable answer for kids who are violent or have been otherwise expelled. I was suspended for fighting and it seemed like an appropriate punishment. reply chefandy 3 hours agorootparent> One kid should not be allowed to ruin a class. The only thing I ruined for other students when I was in class was forcing them to look at my stupid haircut. My punishments were for truancy. I went to school, but spent all of my time in the computer lab because with severe ADHD without any academic support rendered class pointless. One crusty old Korean War vet teacher flat-out told me he \"didn't believe in IEPs,\" and the administrators refused to even address the problem. I never once started a fight, brought drugs to school, or had a gun. While people found me pretty intimidating looking at first, I had a genuinely warm, mature, and mutually respectful relationship with damn near anybody I interacted with. No students really had a problem with me, but the adults actually enjoyed interacting with me more. Most teachers, administrators, librarians, etc would stop me for a quick chat to catch up, talk about current events, or whatever if we passed each other. I didn't ruin shit, and neither did a hell of a lot of other kids that were punished because the school didn't hold up their end of the bargain for academic accessibility. > When I was kid we had people that brought guns to school and were kicked out it seemed reasonable to me. Whoa there straw man. It's completely ridiculous to lump academically struggling kids or kids with run-of-the-mill behavioral problems in with kids that bring deadly weapons to school. Nobody is arguing that kids who bring guns into school should be sent on their way after a stern talking to. Also, nobody said that alternative schools weren't on the table. I, myself, graduated in a night school program designed for failing high school students who'd been successful at work, and it was a phenomenal experience. They gave us a lot more leeway and expected us to do schoolwork mostly independently while working at least 20 hours per week, and we'd fail the entire term for all classes if we missed a single assignment. It was precisely the lack of patronizing meddling you're advocating for that allowed hundreds of kids to graduate through that program. > In both cases Kids are generally held back because they're struggling with the material, not because they're being disruptive. How exactly does holding a kid back help the system if there's any expense to the child? > I was suspended for fighting and it seemed like an appropriate punishment. I'm glad you think so, but that doesn't actually counter any of the data presented. reply chefandy 5 hours agorootparentprevAs someone subjected to both of these actions, plus expulsion, in lieu of anybody bothering to try and figure out what was wrong, that certainly rings true. However, people just really really love a) nostalgia, b) validating their compulsion to inflict the same pain they experienced as children on young people, and c) watching people in out-groups get punished. It's a lovely thought, but I'll believe that there have been real changes, rather than overblown facets of moral panic about abandoning those bad habits, when I see them. reply chefandy 3 hours agorootparentHappy to address any counterarguments from the multiple downvoters. reply mortify 9 hours agorootparentprev>making it harder for people to fail at adulting That has been the direction school has gone and, at least from my perspective, it seems worse. It has lead to a loss of agency among now so-called adults who expect to always be in a situation which guides them toward success. They struggle without a guidebook. Learning to fail, and crucially, how to handle failure and recover are better approaches. reply coopierez 1 hour agorootparentThe things the above poster suggested are largely man-made, artificially complex things seemingly designed to trap people. Things like paying taxes and handling healthcare are pretty much automatic in most European countries for example. reply internet101010 9 hours agorootparentprevThis is how you end up with people that are \"book smart\" but do not how to create something from vague instructions or connect the dots. The easiest way to weed these people out of the applicant pool is if they link to their github and it is just projects from online courses. reply mbesto 9 hours agorootparentprev> navigating health insurance, paying taxes, budgeting, managing credit, home maintenance, vehicle care The self-perpetuating lie in American life is that all of these get solved by . Silicon Valley has only made it worse because these solutions are just monkey-patching poor \"source code\". Why learn how to balance a checkbook when Chase online can do it for you? Our parents' generation had it different. They had fewer health provider options, a smaller tax code, fewer financial products, simpler home setups, engines that didn't have planned obsolescence built into them, etc, etc. We assume that things like 6 different options for MRIs or 2,304 different credit cards mean better products/services, but what is ignored is that these have only made for more complex and yet brittle systems that are harder to navigate and create much greater analysis paralysis. reply SoftTalker 7 hours agorootparentI learned out to fill out a basic Form 1040 tax return in middle school (late 1970s). Banking now is WAY easier. Balancing a checkbook? All your transactions and your balance are available 24/7 on your phone. Your paycheck appears in your bank account automatically. You used to have to get a paper check at work and then take it to the bank (open 9-5, maybe a little later on Fridays, and 9-12 or maybe 2pm on Saturday) to deposit it. Paying for stuff at the store today? Tap your phone. You used to have to carry cash, or a checkbook (if the merchant would accept checks) and hope you had figured your balance correctly. I don't remember a lot of lessons about managing credit but we did study simple and compound interest in math and talked about how that can work for and against you depending on whether you're borrowing or saving. Home maintenance and vehicle care --- covered the essentials in home economics and driver's education. Most people then and now paid others to do that, or went to the effort to teach themselves what they needed to know. Cars back then were much less reliable than today. Today's cars will go 100K miles easy with little more than oil changes and maybe a new set of brake pads and tires. Cars then needed regular tune-ups and generally started having more major problems after only a few years. Health care does seem worse now. You don't have as many family doctors with their own or small group practices, where getting an appointment was pretty easy and they actually knew you. But overall daily life is way more convenient now than it was 30 years ago. reply Red_Leaves_Flyy 7 hours agorootparentYou focus on banking, I’m talking about budgets. Do you track every cent in and out and have a quarterly updated forecast of your financial position a decade out? How close are you to that? If your answer doesn’t include a spreadsheet of some sort you’re not budgeting but taking a shortcut on faith your intuitions are correct. Did you get taught how credit applications work, how banks determine credit worthiness, how to depreciate an asset, how to calculate lifetime cost of a vehicle, how to draft a bill of materials for a project? All things everyone should be able to do. It’s the lack of these skills and the cost of living crisis that creates ripe markets of ignorant people to exploit for profit through their financial mistakes. Your school offers home ec? Mine dropped it forever ago. Only the trade school kids learned anything more hands on AP chem. Cars are more reliable sure, but less maintainable in a home garage. I didn’t bring them up because the best argument I have for cars is repealing cafe and taxing cars annually with a multiplier for wheelbase squared x miles. reply SoftTalker 5 hours agorootparentNo I don't budget to that degree. Neither did most people 30 years ago. I put a percentage of my income into an investment account automatically every payday and forget about it. What I have left is my spending money. That's very simple and tends to work for me. reply Gigachad 4 hours agorootparentprevMy bank app does that stuff automatically. They have identified and classified pretty much all merchants so the app can breakdown spending and tell you exactly how much you spend on essentials, restaurants, alcohol, etc. You get the same info as your spreadsheet, but without any work. reply saltminer 5 hours agorootparentprev> Today's cars will go 100K miles easy with little more than oil changes and maybe a new set of brake pads and tires. Cars then needed regular tune-ups and generally started having more major problems after only a few years. Yeah, getting your engine rebuilt used to be a fairly common occurrence. Now, unless you own a vintage car, it's quite rare. > You don't have as many family doctors with their own or small group practices, where getting an appointment was pretty easy and they actually knew you. Very true. The US health insurance industry is to blame for a lot of the consolidation; it's getting harder and harder for independents to survive as time goes on, with smaller providers being less attractive for insurers to begin with and the ones who will deal with them squeezing them more and more. The increasing documentation requirements by insurers are also much harder for independents to meet. reply Red_Leaves_Flyy 7 hours agorootparentprevSociety is consciously created by the active participants in that system. Government fails to hold them accountable for directly creating unwanted outcomes. Task companies with robust interoperability and let’s see how that goes. reply gentleman11 10 hours agorootparentprevIf you say the problem is social class and poverty, and not having available role models to show how adult life actually works, you’ll get flamed and trolled. If you say the problem is racial issues, you’ll get upvotes. I’ll just sit here and await my downvotes now reply bccdee 10 hours agorootparentRole models are kind of a non-answer to the question. It's like saying the problem is \"bad luck.\" Role-model-based policy solutions are, if not impossible, at least deeply impractical. Childcare subsidies and other forms of welfare, including simple direct cash transfers, have been shown to be strongly beneficial and are much simpler to implement. What people dislike about those is that they involve starting fights with lobbyists. Hence non-actionable perspectives like \"the problem is role models\" or \"the problem is personal responsibility,\" which are not solutions so much as excuses for collective inaction. reply Thorrez 6 hours agorootparentprev> Please don't comment about the voting on comments. It never does any good, and it makes boring reading. https://news.ycombinator.com/newsguidelines.html reply 6510 9 hours agorootparentprevThe pattern I've noticed is that the poor and the poorly educated have no career expectations from their kids. If the kids with wealthy and/or highly educated parents showed up at home with just one poor grade all hell broke lose. Grounded for 6 months, allowance cuts, no more TV or video games etc One kept his kid at home during the holidays to tutor him himself, screaming 90% of the time. The other parents would look at the grades forscreaming 90% of the time. Well isn't that just awesome parenting. reply 6510 5 hours agorootparentI'm not suggesting it is a good idea, it was just to illustrate the difference. He did learn grammar and went to university. I'm pretty sure he is equally stubborn and hot headed as his dad if not more so. Now that I think about it, he even believes in pulling oneself up by the bootstraps. ha-ha reply richardlblair 14 hours agoparentprevIt's hard to look at visualizations like this and reflect on the experiences of the individuals living through hardship. Even those who 'make it out' may struggle in ways not fully captured in the data or this visualization. I grew up in a 'high risk environment', and experienced all the adverse experiences with the exception of gun violence (yay Canada). I'm one of the few that 'made it out'. Many of my childhood friends are dead (usually overdoses), suffer from substance abuse, or are still stuck in the poverty cycle (on average it takes 7 generation to break the cycle). I look at this visualization and I can feel, to my core, what these folks feel. Even for those that 'made it out', I feel for them. I struggle with my mental health, I've had to actively reparent myself, and I feel pretty lonely. Many of the people I'm surrounded by don't know what it feels like to carry all the weight from your childhood. I do agree that the government shouldn't just throw resources at the problem. There are some things the government can do, though. 1. Teach conflict resolution skills to young children. This mitigates adverse experiences and prepares the children for adulthood (especially if they 'make it out') 2. Address addiction as a health problem and not a criminal problem. Children don't need to see their parents as criminals, they need to witness them get better. 3. Reduce the burden of poverty. For instance, the poorer you are the further you have to travel to the grocery store. The people who often don't have the means to easily travel for food have to travel for food. 4. Access to education. The people I grew up around who have found success did so because our schools were really well equipped. You'll notice I didn't list access to support systems. Honestly, they are kind of useless. As a child you understand that if you open up about your experience there is a solid chance your parents will get in trouble or you'll be removed from your home. No child wants this. You end up holding it all in because you can't trust adults. These are just some of my thoughts. Definitely not comprehensive, I could ramble on about this for ages. (edit - formatting) reply no-dr-onboard 12 hours agorootparent> Teach conflict resolution skills to young children. This is pretty huge. A lot of my experience growing up in California during the 90s was \"tell an adult\" and \"zero tolerance\" coming down from school administrators. This is useful at a very young age, but it neglects to equip the children with agency for when the adults aren't around. You can't tell an adult when you're on the school bus and conflict breaks out. You can't tell an adult when you're out on a soccer trip and people are getting rowdy in the locker room. The bystander effect is very strong in school aged children because we neglect to introduce them to their inherent agency in conflict. There is also a degree of antifragility that parents could teach as well. Your emotions aren't reality. What people say about you isn't either. Again, these should come from parents. reply tomp 11 hours agorootparentWhat do you mean? In the adult world, you'd just call the police. In the child world, sometimes you tell the adults, but they don't do anything, and the abuse continues. That's at least my experience with bullying in primary school. \"Conflict resolution\" and such virtue-signalling buzzwords don't work against violent bullies. reply Red_Leaves_Flyy 10 hours agorootparentSometimes the only resolution for a conflict is murder. Even in non stand your ground states. I do not think you understand conflict resolution and should probably study it a bit before speaking so authoritatively. The basic gist of it is to identify the root cause of contention and identify the best practical solution. Most people bad at managing conflict fail to correctly identify the cause and empathize with the opposing view. Keep in mind - you do not need to agree with a perspective to understand it and failure to understand the other party is a responsibility shared jointly regardless of righteousness. reply tomp 10 hours agorootparentHow would you attempt “conflict resolution” with primary school bullies? Sometimes the only resolution to violence is (threat of) superior violence. If you’re a child and a group of kids attacks you, that’s “adults resolving the situation”. Anything else is a failure of the schooling system. reply dotnet00 10 hours agorootparentIt's telling that you seemingly only think of extreme cases when it comes to conflict resolution, and not all the mundane conflicts kids get into, eg arguing over who gets to play with a toy, arguing over who's whose friend etc, teasing that doesn't rise to the level of bullying, or kids interacting with/being watched by someone who is both meaningfully older than themselves and is also too young for the kid to acknowledge them as having authority (eg an older cousin), or teenagers arguing over/teasing over crushes etc. These are all things kids need to have the freedom to learn to resolve without a parent just jumping in all the time. Such conflict resolution can come handy in adulthood for things like dealing with angry/complaining customers, miscommunications causing arguments, professional disagreements etc. I've seen so many people who are completely unable to do conflict resolution of any sort, everyone's always walking on eggshells around them knowing that any conflict is going to end up blowing up into full \"Karen\"-esque argument. reply Red_Leaves_Flyy 7 hours agorootparentprevEasy and unethical? Give them a weaker target than me. Find and exploit their weakness publicly thereby robbing them of their power. reply germinalphrase 11 hours agorootparentprevThe role of law enforcement is rarely about direct intervention to stop criminal behavior (or in your example, violent bullying). They investigate and, potentially, punish criminal behavior that has happened in the past. They act as a deterrent to crime, but also to vigilante justice. Conflict resolution provides the potential victim with agency to intervene in a situation on their own behalf. Of course, this doesn't preclude the option of calling the police. Why not expand someone's options for keeping safe? reply makeitdouble 10 hours agorootparentprev> In the adult world, you'd just call the police. We deal with a lot more conflict than you're accounting for. Someone can be shouting at a waiter at a restaurant and people around will try to deascalate and help or consolate the waiter. Af short fight breaks ? People close enough to the participants will act, and bystanders might stay as witnesses to not make it a \"he said she said\" situation etc. In general people aren't playing heroes but will do a ton of small and cumulative effort to make tensed situations not expand further into chaos. reply richardlblair 10 hours agorootparentprev\"You'd just call the police\" This is funny because you'd be hard pressed to find someone from a low income neighborhood calling the police. reply coldtea 10 hours agorootparentNot to mention, easy to find some killed by the very police they themselves called. Aderrien Murry, 11, called 911 for help at his home in Indianola, Miss., last weekend. But after police arrived, an officer shot him in the chest. The boy is recovering, but his family is asking for answers — and they want the officer involved to be fired. https://www.npr.org/2023/05/26/1178398395/mississippi-11-yea... A Los Angeles county sheriff’s deputy shot and killed a 27-year-old woman who had called 911 to report that she was under attack by a former boyfriend, police officials and lawyers for the victim’s family said on Thursday. Records show the deputy had killed another person in similar circumstances three years ago. https://www.theguardian.com/us-news/2023/dec/21/los-angeles-... And this is just 2 random cases from 2023 reply Aeolun 10 hours agorootparentYeah, stories like that would make you not want to call the police all right. reply quacked 9 hours agorootparentprevYou clearly have no experience with what you're talking about. In the low income neighborhoods near me, in which my sister lives (of her own free will, despite other options) due to chronic cognitive issues, the police are visiting constantly. People in low income neighborhoods call the police all the time. Surveys show that most low income people in dangerous neighborhoods are in favor of more, better policing, not less. reply Zpalmtree 8 hours agorootparentprevThis is just false reply RajT88 10 hours agorootparentprevAdults largely do nothing, agree. I recall trying once, it got to the principal level. Nothing happened. The kids got a talking to by the principal, but their parents did not care. Child bullies have parents who do not care what their kids do. Fighting back works - against a single bully. If there is more than one, they will make the fight unfair. After all, it is about dominance and not proving yourself. Bullies eventually usually grow out of it. That is the fix in my experience. reply int_19h 3 hours agorootparentIn my experience, they do not, they just become someone else's problem. (Or everyone's, if they end up in top management.) reply anotheruser13 8 hours agorootparentprevMaybe in school. I was bullied by a supervisor at a previous job. Didn't want to get fired for insubordination. reply jaysinn_420 11 hours agorootparentprevCall the police? I don't need two problems. reply mr_toad 9 hours agorootparentprev> don't work against violent bullies. De escalating is about dealing with angry people, especially people who aren’t usually violent. Habitually violent bullies aren’t doing it out of anger, they’re using violence to provoke and manipulate. reply hirsin 11 hours agorootparentprevThis is fairly literally how people watch a homeless guy get choked to death in the New-York subway. \"Someone will call the cops eventually\". No, you can't be a bystander, even if it might be dangerous. reply sabarn01 10 hours agorootparentthat was someone stepping in because the homeless person had been threatening. In general people won't help because the risk of helping is too high. reply coldtea 10 hours agorootparentprev>In the adult world, you'd just call the police. Good luck with that. reply anon291 13 hours agorootparentprevUnfortunately a solid number of these things would rely on the moral equivalent of slavery. > Reduce the burden of poverty. For instance, the poorer you are the further you have to travel to the grocery store. The people who often don't have the means to easily travel for food have to travel for food. No one wants to work in these neighborhoods because they are invariably awful. At some point the risk of an employee being murdered / assaulted means stores close down. There's no good answer for this, other than to keep doing what we're doing. Our current economic system has consistently lifted large numbers of people out of poverty historically, and is still doing it today. We should at least give it a go for seven more generations. That's not to say we should do nothing, but large overhauls seem uncalled for given the data. reply magicalist 13 hours agorootparent> Unfortunately a solid number of these things would rely on the moral equivalent of slavery. Weird conclusion to jump to. GP did not suggest grocery stores staffed under threat of jail time anywhere. Better public transit benefits everyone. Better urban design favoring walkable neighborhoods benefits everyone. Better zoning allowing neighborhood shops at street level benefits everyone. reply anon291 12 hours agorootparent> Better public transit benefits everyone. Better urban design favoring walkable neighborhoods benefits everyone. Better zoning allowing neighborhood shops at street level benefits everyone. Sure, as someone who is raising a family in a city, I completely agree. But the reason why stores leave is invariably safety issues. reply KTibow 11 hours agorootparentThe point isn't necessarily that stores need to spring up nearby, the point is that it needs to be easier to access stores (eg by making it easier to get transportation). reply anon291 6 hours agorootparentWell in my experience the rich and poor rely on public transit in mostly similar numbers, so I don't really see what transit in particular has to do with it. reply fragmede 12 hours agorootparentprevOTOH, if being a cashier at the 7-11 paid $100k/yr in hazard pay, I'm sure you could find people willing to work there. the only question is where that money comes from. reply nox101 12 hours agorootparentThat sounds like it has possible unintended consequences? \"Go shoot lots of guns and do violent things and then our hazard pay will go up!\" reply coldtea 10 hours agorootparentOnly as much as any other such thing. Did home insurance availability increased arsons in any significant number? reply nox101 10 hours agorootparentI could be wrong but rarely do you get > replacement money from insurance. Pay $1000k for home, burn it down, get $800k from ins. You're out $200k. Or am I mis-understanding how home insurance would incentivize arson reply richardlblair 13 hours agorootparentprevMy context is Canada where getting killed at work wouldn't been an issue. In the context I'm speaking about it would likely drive opportunity in low income neighborhoods. Canada also have horrific city planning, so when I say people need to travel far I mean they need to spend up to 3 hours in some major (major for us) cities just to get groceries. The US is a whole other can of worms, I don't know how to solve those problems. I'm also not as familiar with the nuances. reply anon291 13 hours agorootparent> Canada also have horrific city planning, so when I say people need to travel far I mean they need to spend up to 3 hours in some major (major for us) cities just to get groceries. I can't imagine anyone in a major US city spending 3 hours. Maybe rurally, but even the so-called 'food deserts' in a big city like LA ... it's just a few miles. At the end of the day, look... my mother taught in inner-city public schools. I know the problems these kids have. They're given meals and such (and they should be), but that is not going to solve a cheating father, a mother too depressed by said cheating to lift a finger to do anything (and maybe whoring herself out or doing drugs to damp the pain?), and a family that sees the child as a cash bag. I mean what are we possibly to do? You give the food and still the child doesn't get it. I feel these policies end up failing because the policy makers are from whole families (And are likely extremely socially conservative in their own life) and can't imagine anything so debased. reply parpfish 11 hours agorootparent3 hours seems plausible if you need to take a bus trip with a transfer. 1:15 each way on the bus and 30min in the store reply richardlblair 10 hours agorootparentBingo. Especially in poorly laid out cities. reply lazyasciiart 10 hours agorootparentprev> At the end of the day, look... my mother taught in inner-city public schools. I know the problems these kids have. > I feel these policies end up failing because the policy makers are from whole families (And are likely extremely socially conservative in their own life) and can't imagine anything so debased. I feel like you don't know any better than these policy makers you are dismissing. reply anon291 6 hours agorootparentI'm not a policy maker nor claim to be one. reply lazyasciiart 4 hours agorootparentNo. And you’re apparently not someone who knows about this topic, but you are claiming to be. reply ska 11 hours agorootparentprev> 3 hours in some major That doesn't sound plausible. Got some examples? reply ozymandias204 3 hours agorootparentI live in Los Angeles. Driving to work takes 15 minutes. Taking the bus _home_ from work takes an hour. Taking the bus _to_ work would require extra time -- leaving early to make sure I don't miss the bus. And this is only a 3-5 mile ride, where the bus picks up half a block from my work and drops me off a block from home. There's a shopping center with multiple markets and Walmart and Kohl's that the bus comes up along then turns away from on the way to work; I can use this as an example of shopping from home, as I can probably get 90% of my living supplies there. Ralphs, Target, Walmart, Kohls, Trader Joe's, etc are all here. The bus transfer here is not an easy one, though, as the bus timings overlap going in both directions, meaning you have to leave early and get back later (about 1 in 4 trips I can transfer without waiting. _Not_ good odds with an hourly bus). 0:00 5 minutes: walk to bus stop 1. 0:05 5 minutes: wait for the bus (best to be at the stop early in case your bus is early, though this bus is usually exactly on time) 0:10 10 minutes: take the bus to stop 2. 0:20 3 minutes: cross the street to get on the other bus 0:23 12 minutes: wait for the next bus 2, the previous one left while you were crossing (yes, seriously) 0:35 10 minutes: take bus 2 to stop 3 where the shopping center is 0:45 90 minutes: cross the parking lot to get to the store (5~10 minutes), then try and get all your shopping done in under 40 minutes so you can take the next bus back home. Nope, today you had to go to the supermarket pharmacy, which is a 20 minutes walk across the shopping center, wait for your meds, _and_ walk back to the cheaper market to do your shopping as well. 2:15 30 minutes: shopping is done a bit early. Yay. You have time to walk back to the bus stop and wait in the sun until the next bus 2 comes. Yay. 2:45 10 minutes: Bus 2 comes. Take it back to the transfer bus stop. 2:55 15 minutes: Cross the street again, and wait for bus 1 so you can get home 3:10 10 minutes: take bus 1 home. 3:20 15 minutes: Now you're a block away from home, carrying bags of groceries, _and you had to get off 2 stops early so you could use a crosswalk_, because there's no crosswalks on this street and people don't stop. Walk home. 3:35. Tadah. You're home. Just a bit over 3.5 hours! Unfortunately, since you don't have a car, you're limited to buying what you can carry. I hope you're ready to go shopping again later this week! You have family? Oh, well then you'll be shopping again 3 times this week. Maybe even 4 times. I hope you like waiting in the sun/rain, LA Metro only puts up cover where they can make money off advertising, so all the bus stops we've used only have benches (except one, but that one's further away). If all you needed was medication, you'll probably want to get your shopping done anyways, as this is otherwise a > 2hr trip just for that (remember, bus 2 is hourly, so you're spending an hour at the shopping center _minimum_, including walking to/from the bus stop). There's five other stores across the street from the shopping center that you'd like to check out sometime, including a new grocery store, but it takes 20 minutes to cross the shopping center, then probably another 10 to cross the street and the parking lot in front of the other stores. Add the time spent in these stores, and you've just added another hour to your shopping trip. This is only _partially_ offset by crossing to the supermarket pharmacy, as that supermarket is nowhere near the corner, and keep on kind that anything you have to carry will slow you down more. ----- Buses: - Bus 1 goes EW near home, turns NS between home and the transfer point (about 10 minutes), then goes EW again. - Bus 2 goes EW, turns NS between the shopping center and transfer point, and goes EW again. - There _was_ a bus that went NS along the east side of the shopping center (which also would have dropped me off at home, cutting out the need for a second bus entirely), but this bus route was changed in 2019 to turn away from the shopping center once it gets to the NE corner. - There's a bus that goes EW along the other side of the shopping center, but that's not helpful. ----- You're forgetting about just how much convenience your car gives you _besides_ the ability to get to and from the store. - You don't have to wait for transfers or make what is effectively two trips to get somewhere. - You don't have to cross parking lots or go in and out of stores from the street (you can park up near the store, then drive to the other side of the shopping center). - You can make a quick 5 minute stop on the way home without increasing your travel time by a full hour (because the bus only comes hourly). - You don't have to wait outside. - You don't have to hope that the bus was cancelled without notification (two weeks ago I was lucky to get a ride, as my once-an-hour bus was straight up cancelled without prior warning; if I didn't use the former-official Transit app to check times, I wouldn't have known, and would have been waiting at the stop for 80 minutes like one of my less fortunate coworkers did, or taking a different once-an-hour bus home with extra transfers and lots of waiting, to only get home ~10 minutes earlier) - You don't have to only buy as much as you can carry on a single trip (I work in a grocery store, people can and do fill _multiple_ shopping carts to avoid having to go shopping a second time in a week. People can and do purchase groceries for elderly relatives they don't live with). - if there's a detour, it only costs you the time it takes to make said detour. If the bus has to make a detour and you have tight timing, you might miss your transfer, adding 10-60 minutes to your commute. - You're not dependent someone being willing to pick you up. When I was in college, a full bus would often just go right by without stopping, since there wasn't enough room. - You're not dependent on your fellow passengers being rule-abiding or polite. Last year the bus driver stopped for an entire 50 minutes at a high school because the kids weren't being safe or quiet. Not that they're ever quiet, or that a full bus in general is quiet, but they were throwing condoms across an overcrowded bus and yelling, and the bus driver understandably didn't want to deal with it when _he couldn't close the door_, so he stopped and said those past the yellow line on the floor needed to get off and wait for the next bus. Instead, they made fun of him, continued talking loudly, and those near the door who shoved their way into a full bus refused to move. (The next month or so was _very_ quiet on the bus) - general garbage is everywhere. The filth that people leave behind when they cram into a bus and then leave. The noise of competing music playing",
    "originSummary": [
      "The article chronicles the journey of Alex, a Hispanic teenager, from 1997 to 2021, highlighting his challenges in education, employment, and mental health due to adverse childhood experiences.",
      "It underscores how childhood trauma can significantly influence an individual's financial status, educational achievements, and general welfare in the long term.",
      "The research stresses the significance of tackling childhood trauma and offering assistance to those who have faced adversity during their formative years."
    ],
    "commentSummary": [
      "The discussion delves into various topics such as childhood experiences' influence on adult outcomes, financial situations, poverty, Universal Basic Income, education, and societal well-being.",
      "Perspectives on supportive relationships, education, financial literacy, and systemic inequalities are highlighted to enhance outcomes for individuals and society collectively."
    ],
    "points": 1254,
    "commentCount": 676,
    "retryCount": 0,
    "time": 1713283679
  },
  {
    "id": 40051191,
    "title": "Loading Trillion Weather Data Rows in TimescaleDB: Efficiency, Challenges, and Solutions",
    "originLink": "https://aliramadhan.me/2024/03/31/trillion-rows.html",
    "originBody": "Ali Ramadhan About Blog Research Publications Resources Links Resume / CV Email Github Google Scholar LinkedIn YouTube Building a weather data warehouse part I: Loading a trillion rows of weather data into TimescaleDB 31 March 2024 GitHub Discussion What are we even doing? Why build a weather data warehouse? What’s the data? The insert statement Starting with just the single-row insert statement Multi-valued insert The copy statement Upgrading to the copy statement Sustaining copy insert rates Parallel copy Tools pg_bulkload and timescaledb-parallel-copy Multiple workers with timescaledb-parallel-copy Tweaking Postgres settings So what’s the best method? Appendices Source code Benchmarking methodology Footnotes Global snapshot of surface temperature at 2018-12-04 04:00:00 UTC. What are we even doing? Why build a weather data warehouse? I think it would be cool to have historical weather data from around the world to analyze for signals of climate change we’ve already had rather than think about potential future change. If we had a huge weather data warehouse we could query it to figure out whether Jakarta is actually warmer or stormier these days and exactly how is it warmer (heat waves, winter highs, etc.). Or whether Chile is warming or getting cloudier as a whole, which could be region-specific. We could do this kind of analysis for every city or region on Earth to find out which places have already experienced the most climate change and what kind of change. But to do this analysis globally we need to make querying the data warehouse fast, and there’s a lot of data. The first step is to load the data into a database of some kind. I’m going to try using PostgreSQL here. It should be a good learning experience and using TimescaleDB to speed up time-based queries and eventually PostGIS to speed up geospatial queries seems promising. To get there though we first need to load all this data into Postgres and this is what this post is about. Initial attempts at loading the data seemed slow so I wanted to investigate how to do this fast, leading me down a rabbit hole and me writing this.1 Are we building a data warehouse? I think so…? Is a relational database even appropriate for gridded weather data? No idea but we’ll find out. What’s the data? We are not working with actual weather observations. They are great, but can be sparse in certain regions especially in the past. Instead, we will be working with the ERA5 climate reanalysis product2. It’s our best estimate of the historical state of the Earth’s weather and is widely used in weather and climate research. The data is output from a climate model run that is constrained to match weather observations. So where we have lots of weather observations, ERA5 should match it closely. And where we do not have any weather observations, ERA5 will be physically consistent and should match the climatology, i.e. the simulated weather’s statistics should match reality. At the top of this page is a snapshot of what global surface temperature looks like and below is a snapshot of global precipitation. Global snapshot of precipitation rate at 2018-12-04 04:00:00 UTC. Here’s what a time series of temperature looks like at one location. Time series of surface temperature near Durban, South Africa. ERA5 covers the entire globe at 0.25 degree resolution, and stretches back in time to 1940 with hourly resolution. Hourly data stretching back to 1940 is 727,080 snapshots in time for each variable like temperature, precipitation, cloud cover, wind speed, etc. A regularly-spaced latitude-longitude grid at 0.25 degree resolution has 1,038,240 grid points or locations (1440 longitudes and 721 latitudes including both poles). Together that’s 753,836,544,000 or ~754 billion rows of data if indexed by time and location. That’s a good amount of data. And as I found out, it’s not trivial to quickly shove this data into a relational database, much less be able to query it quickly. The ERA5 data is distributed as NetCDF3 files. You can query an API for the data or download it from certain providers but generally each file contains data for a day, a month, or a year. This chunking by time makes it quick and easy to query the dataset at single points in time, but looking at temporal patterns is very slow as many files need to be read to pull out a single time series. It takes like 20~30 minutes to pull out temperature data for one location to make the plot above! Complex geospatial queries, especially over time, will be slow and difficult to perform. Packages like xarray and dask (and efforts by Pangeo) speed things up but it’s still a slow process. We’ll just load in temperature, zonal and meridional wind speeds, total cloud cover, precipitation, and snowfall for each time and location so we’ll use this table schema: sqlcreate table weather ( time timestamptz not null, location_id int, latitude float4, longitude float4, temperature_2m float4, zonal_wind_10m float4, meridional_wind_10m float4, total_cloud_cover float4, total_precipitation float4, snowfall float4 ); And before you mention database normalization, yes I have both a location_id column and latitude and longitude columns. It’s for later benchmarking with queries and indexes. The insert statement Starting with just the single-row insert statement The simplest way to load data into a table is by using the insert command to insert a single row. This looks something like sqlinsert into weather ( time, location_id, latitude, longitude, temperature_2m, zonal_wind_10m, meridional_wind_10m, total_cloud_cover, total_precipitation, snowfall ) values ('1995-03-10 16:00:00+00', 346441, -30, 30.25, 15.466888, -2.0585022, 0.25202942, 0.9960022, 0.007845461, 0); and so you can just loop over all the data doing this row-by-row. Unfortunately it is quite slow as quite a bit goes on behind the scenes here: Postgres needs to parse the statement, validate table and column names, and plan the best way to execute it. Postgres may need to lock the table to ensure data integrity.4 The data is written to a buffer as Postgres uses a write-ahead logging5 (or WAL) system. Data from the buffer is actually inserted into the table on disk (which may involve navigating through and updating indexes, but we won’t have any here). If the insert statement is part of a transaction6 that is committed, then the changes are made permanent. So there’s a lot of overhead associated with inserting single rows, especially if each insert gets its own transaction. How many rows can we actually insert per second using single-row inserts? After loading the data from NetCDF into a pandas dataframe I found three7 ways to insert the data into Postgres from Python so let’s benchmark all three: pandas: You can insert data straight from a dataframe using the df.to_sql() function with the chunksize=1 keyword argument to force single-row inserts. psycopg3: You can use parameterized queries to protect against SQL injection, not that it’s a risk here (yet) but it’s good to practice safety I guess. All inserts are part of one transaction that is committed at the end. SQLAlchemy: You can similarly use named parameters in a parameterized query to prevent SQL injection attacks. Blue bars show the median insert rate into a regular PostgreSQL table, while orange bars show the median insert rate into a TimescaleDB hypertable. Each benchmark inserted 20k rows and was repeated 10 times. The error bars show the range of insert rates given by the 10th and 90th percentiles. I benchmarked inserting into a regular Postgres table and a TimescaleDB hypertable8. Pandas and psycopg3 perform similarly, with a slight edge to psycopg3. SQLAlchemy is the slowest even though we’re not using its ORM tool. This may be because it introduces extra overhead with its abstractions around session management and compiled SQL expressions. Inserting into a Timescale hypertable is a bit slower. This is maybe because rows are being inserted into a hypertable with chunks so there may be some overhead there, even if there’s only one chunk. So at best we’re only getting ~3000 inserts per second with single-row inserts at which rate we’re gonna have to wait ~8 years for all the data to load 🦥 There must be a faster way. Multi-valued insert You can insert multiple rows with one insert statement. This is called a multi-valued or bulk insert and looks like this: sqlinsert into weather ( time, location_id, latitude, longitude, temperature_2m, zonal_wind_10m, meridional_wind_10m, total_cloud_cover, total_precipitation, snowfall ) values ('1995-03-02 04:00:00+00', 346444, -30, 31, 21.54013, 7.1091003, 5.9887085, 1, 2.7820282, 0), ('1995-03-02 05:00:00+00', 346444, -30, 31, 21.596466, 7.0369415, 6.2397766, 0.95751953, 2.1944494, 0), ('1995-03-02 06:00:00+00', 346444, -30, 31, 21.660583, 6.303482, 6.017273, 0.88571167, 1.9253268, 0); This is faster for a few reasons. There’s less network overhead as each single-row insert requires a network round trip for each row inserted. Postgres also only has to parse and plan once. Multi-row inserts can also be further optimized when it comes to updating indexes. It seems that you can bulk insert as many rows as you want as long as they fit in memory. In pandas it sounds like you can do this by passing the method=\"multi\" keyword argument to the df.to_sql() function but I found this to be a bit slower than single-row inserts with chunksize=1. So I just didn’t set a method or chunk size and supposedly all rows will be written at once, and it was faster. With psycopg3 you can construct or stream a list of tuples, one for each row, and insert them all at once. With SQLAlchemy it’s a dict of tuples. This time each benchmark inserted 100k rows and was repeated 10 times. Now there’s a clear winner with psycopg3 at 25~30k inserts/sec. I’m not sure why psycopg3 is faster but it looks like pandas is using dictionaries to insert which can be slower than just plain tuples. SQLAlchemy might be extra slow slow here because of additional overhead like with single-row inserts and I also passed it dictionaries. With multi-row inserts there’s an order-of-magnitude improvement but at ~30k inserts per second, we’re still gonna have to wait ~0.8 years or almost 10 months for all the data to load 🐢 The copy statement Upgrading to the copy statement For loading in larger amounts of data, Postgres has the copy statement allowing us to insert rows from a CSV file or from a binary file.9 copy is faster than multi-row inserts as Postgres reads data straight from the file and optimizes parsing, planning, and WAL usage knowing there is a lot of data to load. Once you have a CSV file it’s as simple as sqlcopy weather from some_big.csv delimiter ',' csv header; We have the option of saving data from NetCDF files as CSV files then using copy. This honestly feels inefficient as saving timestamps and floating-point numbers as plaintext to disk takes up more space that it should then reading it from disk seems like it would be slow, but Postgres seems to have optimized this operation. We also have the option of not saving the data into CSV files and streaming it straight into Postgres using psycopg3’s cusor.copy() function. When benchmarking copy vs. psycopg3.cursor.copy() we are starting with a pandas dataframe so we must account for the time it takes to save all the data to CSV files on disk in the case of copy csv. In the case of cursor.copy() if we stream a list of tuples then the only overhead is creating the cursor and tuple generator. Here the full rate includes overhead (writing CSV files or constructing tuples) while the copy rate does not. This time each benchmark inserted 1,038,240 rows (1 day of ERA5 data) and was repeated 10 times. We see that copy can actually insert close to 400k rows per second, but that is if you already have the CSV file ready to go. Including overhead, both copy and psycopg3 can manage around 100k inserts/second with psycopg3 being a bit faster. For some reason there seems to be no difference between regular table and hypertable performance for psycopg3. At ~100k inserts/second we’re still talking about ~3 months to load all the data 🐌 Sustaining copy insert rates When inserting many rows, Postgres may encounter bottlenecks10 so it’s important that the insert rate can be sustained. To look at this, we can insert hundreds of millions of rows and watch for fluctuations in the insert rate. For this benchmark, rows were inserted in 744 batches of 1,038,240 rows for a total of ~772 million rows. The overall insert rate is plotted. The dots show the insert rate for each batch while the solid lines show a 10-batch rolling mean. The straight horizontal lines show the mean insert rate over the entire benchmark. Note that the lines orange and blue straight lines are right on top of each other. It seems that, at least with one worker, we don’t see huge drops in insert rates although copy csv shows frequent drops and seems more susceptible to fluctuations. psycopg3 is generally faster and interestingly there isn’t much of a difference between copying into a regular table or hypertable. Parallel copy Inserting data with copy is fast but can we speed it up by executing multiple copy operations in parallel? Using the joblib package we can execute multiple copy statements or psycopg3 cursors in parallel. The overall insert rate is plotted as a function of the number of workers. Each benchmark inserted 128 hours of ERA5 data (~133 million rows). Inserting data into a single table is not super parallelizable so it looks like performance generally plateaus after 16 workers.11 Tools pg_bulkload and timescaledb-parallel-copy Beyond the copy statement, there are external tools for loading large amounts of data into Postgres. I’ll benchmark two of them, pg_bulkload and timescaledb-parallel-copy. Blue and orange bars show results from benchmarks that inserted 1,038,240 rows (1 day of ERA5 data) and were repeated 10 times. The sustained insert rates are from benchmarks that inserted 256 hours of ERA5 data (~266 million rows) into a hypertable. In these benchmarks the CSV files were already written to disk so the insert rate corresponds to the \"copy rate\" from the copy benchmarks. The insert rate including overhead accounts for the time it takes to write the CSV files to disk. At first it would seem that pg_bulkload is much faster, however, this is because by default it bypasses the shared buffers and skips WAL logging so data recovery following a crash may not be possible while timescaledb-parallel-copy does not and does things more safely. On a level playing field with fsync off (see next section for an explanation) timescaledb-parallel-copy with multiple workers beats out pg_bulkload. Multiple workers with timescaledb-parallel-copy timescaledb-parallel-copy lets you specify the number of workers inserting data in parallel. Let’s see how much performance we can squeeze out with more workers, and if that performance can be sustained. The insert rate as a function of the number of rows inserted. In this benchmark the CSV files were already written to disk so the insert rate corresponds to the \"copy rate\" from the copy benchmarks. Each benchmark inserted 256 hours of ERA5 data (~266 million rows). Note the vertical log scale. Initial performance looks great! But eventually, before 100 million rows on my system, a bottleneck is reached and the insert rate tanks before picking back up in waves. The maximum sustained insert rate is around 600~700k inserts/sec for regular tables and ~300k for hypertables. pg_bulkload doesn’t let you specify the number of threads or workers, but does have a writer=parallel option which uses multiple threads to do data reading, parsing and writing in parallel. We’ll look at its insert rate later. Tweaking Postgres settings There are a couple of other things we can try to speed up inserts, but are basically some form of tweaking Postgres’ non-durable settings. Some extra performance can be squeezed out of tweaking non-durable settings specifically for loading data following suggestions by Craig Ringer on StackOverflow. Some of the settings can be dangerous for database integrity in the event of a crash though. The main settings to change are turning off fsync to avoid flushing data to disk and also turning off full_page_writes to avoid guarding against partial page writes. You can also insert data into an unlogged table that generates no WAL and gets truncated upon crash recovery but is faster to write into. While inserting into an unlogged table might be fast, you still have to convert it to a regular logged table afterwards which can be a slow single-threaded process. And hypertables cannot be unlogged, so if you want a hypertable you need to further convert/migrate the regular logged table to a hypertable which can also be slow. So what’s the best method? Short answer: Use psycopg3 to directly copy data into a hypertable. If you already have CSV files then use timescaledb-parallel-copy. For parallelization the sweet spot seems to be 12~16 workers on my system. We want to end up with a hypertable but it seems like inserting into a regular table is faster. So is it faster to insert into a regular table then convert it to a hypertable? Or is it faster to just insert data straight into a hypertable? Blue bars show the wall clock time taken to insert data into the table and the orange bar shows the time taken to convert the regular table into a hypertable. A quick test with inserting ~772 million rows with psycopg3’s copy and 16 workers shows that inserting data into a hypertable is faster as it takes roughly 80% of the time in this case. This may not always be the case but inserting into a regular table then converting it to a hypertable and migrating the data will probably always be slower as the conversion/migration process is not super fast and seems to be single-threaded. Now that we’ve concluded we want to be inserting data into a hypertable, let’s take a look at the all hypertable insert rates we’ve considered in one plot. Sustained hypertable insert rates including overhead (writing CSV files) for different insertion methods. Here \"tpc\" is short for timescaledb-parallel-copy and \"pgb\" is short for pg_bulkload. \"32W\" means 32 workers were used for that benchmark. For pg_bulkload with a single worker the the writer=buffered option was used. For multiple workers, the writer=buffered and multi_process=yes options were used. Then for multiple workers with fsync off, the writer=parallel option was used. So what can we conclude? At least on my hardware it seems there’s a ceiling of ~140k sustained inserts/sec with overhead when using a single worker with protections on. pg_bulkload wins here by quite a bit. You can use multiple workers to increase the sustained insert rate up to ~250k inserts/sec with psycopg3’s copy cursor while still being protected. The insertion process is not very parallelizable so the sweet spot is 4-16 workers. The benchmarks used 32 workers to maximize insert rates. If you’re okay living a bit dangerously you can turn off fsync and sustain an insert rate of ~462k inserts/sec with psycopg3! You’ll also squeeze out a bit more performance out of timescaledb-parallell-copy. Be careful when using pg_bulkload as it disables fsync by default. These conclusions assume you need to do extra work to convert data to CSV files which is why psycopg3 was the clear winner, although it does seem pretty fast. If you’re starting with CSV files timescaledb-parallel-copy is probably faster (and quicker to set up). Some closing thoughts: Want even faster inserts? You should probably upgrade your hardware. A nice enterprise-grade NvME SSD and lots of high-speed DDR5 RAM will help a lot. I used hardware that is roughly 5 years old so newer hardware should be able to easily beat these benchmarks. I know the general wisdom is to just dump this data into Snowflake or BigQuery and get fast analytics for relatively cheap. But I like working with my own hardware and learning this way. Plus I have no real budget for this project. I’d be curious how ClickHouse performs on these benchmarks. My impression is that it would probably be faster out of the box. But I want to learn PostgreSQL and like the fact that TimescaleDB is just a Postgres extension so I went with TimescaleDB. At a sustained ~462k inserts per second, we’re waiting ~20 days for our ~754 billion rows which is not bad I guess 🐨 It’s less time than it took me to write this post. Appendices Source code The code used to download the ERA5 data, create the tables, insert/copy data, run benchmarks, and plot figures is at the timescaledb-insert-benchmarks repository. Benchmarking methodology To ensure a consistent environment for benchmarking, a new Docker container was spun up for each individual benchmark. No storage was persisted between Docker containers. Data including NetCDF and CSV files were read from a HDD and the database was stored on an NvME SSD. Hardware: CPU: 2x 12-core Intel Xeon Silver 4214 RAM: 16x 16 GiB Samsung M393A2K40CB2-CTD ECC DDR4 2666 MT/s SSD: Intel SSDPEKNW020T8 2 TB NvME HDD: Seagate Exos X16 14TB 7200 RPM 256MB Cache Software: Ubuntu 20.04 with Linux kernel 5.15 PostgreSQL 15.5 TimescaleDB 2.13.0 pg_bulkload 3.1.20 Postgres configuration chosen by timescaledb-tune: shared_buffers = 64144MB effective_cache_size = 192434MB maintenance_work_mem = 2047MB work_mem = 13684kB timescaledb.max_background_workers = 16 max_worker_processes = 67 max_parallel_workers_per_gather = 24 max_parallel_workers = 48 Recommendations based on 250.57 GB of available memory and 48 CPUs for PostgreSQL 15 wal_buffers = 16MB min_wal_size = 512MB default_statistics_target = 100 random_page_cost = 1.1 checkpoint_completion_target = 0.9 max_locks_per_transaction = 512 autovacuum_max_workers = 10 autovacuum_naptime = 10 effective_io_concurrency = 256 For benchmarking I set the WAL size: min_wal_size = 4GB max_wal_size = 16GB And for the fsync off benchmarks I set: max_wal_size = 32GB fsync = off full_page_writes = off Footnotes The Postgres documentation does have a nice list of performance tips for populating a database but I wanted some benchmarks and to consider some external tools as well. ↩ ERA5 is the latest climate reanalysis product produced by the ECMWF re-analysis project. ECMWF is the European Centre for Medium-Range Weather Forecasts. ↩ NetCDF (Network Common Data Form) files are ubiquitous in distributing model output in climate science, atmospheric science, and oceanography. They typically store multi-dimensional arrays for each variable output along with enough metadata that you don’t need to refer to external documentation to understand and use the data. The good ones do this at least. ↩ Postgres may need to perform a full table lock for some operations that modify the entire table. But for row-level operations no locking is necessary as Postgres uses multiversion concurrency control (MVCC) to allow multiple transactions to operate on the database concurrently. Each transaction sees a version of the database as it was when the transaction began. ↩ Write-ahead logging is how Postgres ensures data integrity and database recovery after crashes. All committed transactions are recorded in a WAL file before being applied to the database. In the event of a crash or power failure, the database can recover all committed transactions from the WAL file so the database can always be brought back to a consistent, uncorrupted state. ↩ Postgres transactions execute multiple operations as a single atomic unit of work so that either all operations execute successfully (and are written to WAL on disk), or none are applied. This ensures the database is always in a consistent state even if something goes wrong in the middle of a transaction. ↩ I wanted to try a fourth method using SQLAlchemy’s Object Relational Mapper (ORM) which maps rows in a database to Python objects. But, the ORM requires a primary key and Timescale hypertables do not support primary keys. This is because the underlying data must be partitioned to several physical PostgreSQL tables and partitioned lookups cannot support a primary key. ORM was probably going to be the slowest at inserting due to extra overhead anyways. ↩ TimescaleDB hypertables automatically partition data by time into chunks and have extra features that make working with time-series data easier and faster. ↩ Binary is usually a more compact representation for floats and timestamps than plaintext so I was hoping to also benchmark copy with the binary format thinking it might be much faster. Unfortunately the format Postgres expects seems non-trivial and I couldn’t easily find a library that would give me the binary format I needed. And Nick Babcock actually found that binary is no faster than csv, so it didn’t seem worth trying. For reference, 31 days of ERA5 data takes up 7.8 GiB in NetCDF form and 71 GiB in CSV form. ↩ Bottlenecks include the disk being overloaded with writes, usually made worse when the WAL and row insertion are competing for disk I/O. Autovacuuming, which removes dead rows, can also compete for I/O although when populating a database we can ensure that there won’t be any duplicates so this could potentially be turned off. Postgres also periodically performs checkpoints to flush all outstanding WAL data to disk. Heavy writes can lead to more checkpoints and more competitinon for I/O. ↩ I probably should have run the parallel copy benchmarks using more rows to measure a better sustained rate. I wish there was an easy way to keep track of the total number of rows when inserting many rows in parallel but querying the row count seems very slow as Postgres is busy. I guess I could log a timestamp every time a worker inserted a batch of rows. Also wish I repeated this benchmark but it takes quite a while to run. ↩ © Ali Ramadhan. Design: HTML5 UP",
    "commentLink": "https://news.ycombinator.com/item?id=40051191",
    "commentBody": "Loading a trillion rows of weather data into TimescaleDB (aliramadhan.me)356 points by PolarizedPoutin 21 hours agohidepastfavorite144 comments ZeroCool2u 19 hours agoI've done a good amount of geospatial analysis for work. One thing you quickly realize with geospatial data is that it's incredibly nuanced. You have to be quite careful about understanding which coordinate reference system (CRS) and for visualization which projection is being used. The CRS is somewhat paranoia inducing if you don't have great infrastructure setup with the right tools to carry that metadata with your geospatial data. I've tested everything AWS has to offer, Postgres/PostGIS, Spark/DB, Snowflake, Trini, and ARCGis. I'm convinced the best tool for large scale geospatial work is Google BigQuery and it's not even close. It took an expensive multi hour query running on PostGIS deployed on an enormous m6a EC2 instance to less than 5 seconds that ran in the BigQuery free tier. It does make sense if you think about it, Google was early with Maps, but it is just stunning how much better they are in this specific niche domain. This was using publicly available FEMA data that Snowflake and AWS services would just choke on, because the geometry column exceeded their maximum byte size. Spark doesn't even have geospatial data types and the open source packages that add support leave a lot to be desired. This guy is running on-prem, so maybe this made sense, but I just would never bother. The storage for BQ would probably be less than $100/months for 20 TB. reply ingenieroariel 19 hours agoparentI went through a similar phase with a process that started with global OSM and Whosonfirst to process a pipeline. Google costs kept going up (7k a month with airflow + bigquery) and I was able to replace it with a one time $7k hardware purchase. We were able to do it since the process was using H3 indices early on and the resulting intermediate datasets all fit on ram. System is a Mac Studio with 128GB + Asahi Linux + mmapped parquet files and DuckDB, it also runs airflow for us and with Nix can be used to accelerate developer builds and run the airflow tasks for the data team. GCP is nice when it is free/cheap but they keep tabs on what you are doing and may surprise you at any point in time with ever higher bills without higher usage. reply nojvek 8 hours agorootparentDuckDB is the real magic. On an nvme disk with decent amount of RAM, it goes brrrrrr. I would love it if somehow Postgres got duckdb powered columnstore tables. I know hydra.so is doing columnstores. DuckDB being able to query parquet files directly is a big win IMO. I wish we could bulk insert parquet files into stock PG. reply jfim 18 hours agorootparentprevBigQuery is nice but it's definitely a major foot-gun in terms of cost. It's surprisingly easy to rack up high costs with say a misconfigured dashboard or a developer just testing stuff. reply mrgaro 3 hours agorootparentDefinitively agree here. Once the data is in BigQuery, people will start doing ad-hoc queries and building Grafana dashboards on top of it. And sooner or later (usually sooner) somebody will build a fancy Grafana dashboard and set it to refresh every 5 second and you will not notice it until it's too late. reply dekhn 17 hours agorootparentprevFrankly I think this is just a sign that it's a power tool for power users. reply carlhjerpe 17 hours agorootparentSadly my colleagues aren't always \"power users\" reply brailsafe 11 hours agorootparentNobody starts as a power user reply ZeroCool2u 17 hours agorootparentprevThat is a very cool setup! My org would never allow that as we're in a highly regulated and security conscious space. Totally agree about the BQ costs. The free tier is great and I think pretty generous, but if you're not very careful with enforcing table creation only with partitioning and clustering as much as possible, and don't enforce some training for devs on how to deal with columnar DB's if they're not familiar, the bills can get pretty crazy quickly. reply hawk_ 15 hours agorootparentprev> and may surprise you at any point in time with ever higher bills without higher usage. What? really? Do they change your pricing plan? How can they charge more for the same usage? reply ingenieroariel 10 hours agorootparentWhen you queried their 'Open Data' datasets and linked with your own it was absurdly cheap for some time. Granted we used our hacking skills to make sure the really big queries ran in the free tier and only smaller datasets got in the private tables. I kept getting emails about small changes and the bills got bigger all over the place including BigQuery and how they dealt with queries on public datasets. Bill got higher. There is a non zero chance I conflated things. But from my point of view: I created a system and let it running for years - afterwards bills got higher out of the blue and I moved out. reply johnymontana 17 hours agoparentprev> Spark doesn't even have geospatial data types and the open source packages that add support leave a lot to be desired. Could you say more about this? I'm curious if you've compared Apache Sedona [0] and what specifically you found lacking? I currently work at Wherobots [1], founded by the creators of Apache Sedona and would love to hear any feedback. [0] https://sedona.apache.org/latest/ [1] https://wherobots.com/ reply xyzzy_plugh 19 hours agoparentprevDo you mind linking the specific dataset? I agree very wide columns break a lot of tools but other columnar postgres forks should support this no problem. It sounds like you didn't use Redshift, which I find surprising as it directly competes with BQ. Redshift has \"super\" columns that can be very large, even larger than the maximum supported by BigQuery. I constantly see folks finding out the hard way that PostGIS is really hard to beat. The fact that Trini/Presto and Spark have languished here is particularly telling. reply ZeroCool2u 18 hours agorootparentIt's FEMA's NFHL. I can't recall the specific layer of the GDB file, but you could probably figure it out. Try loading up Iowa into redshift and if that works for you I'd be quite surprised. My org has a very large AWS spend and we got to have a chat with some of their SWE's that work on the geospatial processing features for Redshift and Athena. We described what we needed and they said our only option was to aggregate the data first or drop the offending rows. Obviously we're not interested in compromising our work just to use a specific tool, so we opted for better tools. The crux of the issue was that the large problem column was the geometry itself. Specifically, MultiPolygon. You need to use the geometry datatype for this[1]. However, our MultiPolygon column was 10's to 100's of MB's. Well outside the max size for the Super datatype from what I can tell as it looks like that's 16 MB. [1]: https://docs.aws.amazon.com/redshift/latest/dg/GeometryType-... reply beeboobaa3 14 hours agorootparentJust split it out into multiple polygons, one per row. If you're using a relational database, do as relational databases do. reply xyzzy_plugh 9 hours agorootparentprevThe description you provide appears too large for even BigQuery. reply Cthulhu_ 19 hours agoparentprevI hear so much good things about BigTable / BigQuery, it's a shame I've had no opportunity to use it yet. reply detourdog 18 hours agoparentprevI’m glad to hear this first hand experience. I’m pretty sure that want to build and refine my own geospatial data horde. I wonder if you think a longterm project is better rolling their own. The second priority is that I expect it all to be locally hosted. Thanks for any considerations. reply ZeroCool2u 17 hours agorootparentFrankly I think for long term hoarding BQ is hard to beat. The storage costs are pretty reasonable and you never pay for compute until you actually run a query, so if you're mostly just hoarding, well, you're probably going to save a lot of time, money, and effort in the long run. reply rtkwe 18 hours agoparentprevEven more fun I bet would be if you're getting data with different reference spheroids. reply PolarizedPoutin 19 hours agoparentprevThank you for the insights! Yeah I'm still not sure how Postgres/PostGIS will scale for me, but good to know that BigQuery does this nicely. This is not something I'm productionizing (at least not yet?) and I'm giving myself zero budget since it's a side project, but if/when I do I'll definitely look into BigQuery! reply ZeroCool2u 17 hours agorootparentGood luck! They have some really great tutorials on how to get started with BQ and geospatial data. One other nuance of BigQuery that doesn't seem to apply to many other tools in this space is that you can enable partitioning on your tables in addition to clustering on the geometry (Geography in BQ) column. https://cloud.google.com/bigquery/docs/geospatial-data#parti... reply winrid 18 hours agoparentprevM6a is not even remotely enormous. Also were you using EBS? reply ZeroCool2u 18 hours agorootparentm6a.48xlarge: 192 vCPU & 768 GiB If that's not a large node for you, well you're living in a different world from me. Yes to using EBS. reply winrid 17 hours agorootparentThanks for the info. The issue is using EBS. If you used an instance with NVME drives it would probably have been faster than BQ (and you aren't billed per-query...). I would suggest an R or I4 class instance for this, m6 is not good for the money here. You would just have to setup replication for backups, but this could just be rsync to EBS or some other replication solution depending on your database. reply sgarland 16 hours agorootparentprevUnless you’re CPU-bound (unlikely), the r-family is usually a better fit for RDBMS, IME. 8x RAM:vCPU ratio, compared to 4x for m-family. Then there’s the x-family, which can go up to bonkers levels, like 4 TiB of RAM (and local NVMe). As a sibling comment mentioned, though, if you can fit the data into local storage, that’s the way to do it. EBS latency, even with io2.blockexpress, simply cannot compete. That said, if I did the math correctly based on the 71 GB/31 days footnote, you’re looking at about 2.15 PB to load the entire thing, so, uh, good luck. reply ayewo 14 hours agorootparentIn your experience, is the r-family better than the c-family for running an RDBMS? reply sgarland 12 hours agorootparentYes. Most RDBMS are memory-bound (indexes, connection overhead, buffer pool…), so the more, the better. At my last job, I switched a large-ish (100K QPS at peak) m5 MySQL instance to a smaller (in AWS numbering) r6i that was cheaper, despite having the same amount of RAM and being generation newer. That, combined with careful tuning and testing, resulted in queries speeding up 20-40%, AND we then had plenty of room for vertical scaling again if necessary. reply aPoCoMiLogin 13 hours agorootparentprevthings are changing recently in aws, but few years ago R-family instances had one of the fastest uplink to EBS. for example only the larges M4 instance (m4.16xlarge) has 10gbps uplink, versus R5b where it starts from 10gbps (for the lowest tier) and ends on 60gbps @ 260k IOPS. you can very easily choke EBS with DB. EDIT: only newer C instances have comparable uplink to EBS, C5 or C4 (and some C6) starts from ~4.7gbps. just compare the EBS bandwidth column in https://aws.amazon.com/ec2/instance-types/ reply sgarland 12 hours agorootparentThe other fun thing about AWS instances is that the network uplink speed isn’t always what’s advertised. There is of course the “Up to X Gbps” levels (30 minutes of rated speed guaranteed every 24 hours), but there are also other limits, like cross-region speed being capped at 50% of rated capacity. This rarely matters until it does, like if you’re setting up an active-active DB across regions. Then it’s a fun surprise. reply user3939382 3 minutes agoprev> 727,080 snapshots in time for each variable like temperature Maybe we should be recording and transmitting the parameters for a function or set of functions that describes these temperature etc changes rather than a set of points to drastically reduce the size of these data sets. reply jamesgresql 16 hours agoprevThis is super cool! I run DevlRel @ Timescale, and I love seeing our community create well written posts like this! My initial reaction is that I think one of the reasons you're seeing a hypertable being slower is almost certainly that it creates an index on the timestamp column by default. You don't have an index on your standard table which lets it go faster. You can use create_hypertable with create_default_indexes=>false to skip creating the index, or you can just drop the index before you ingest data. You'll eventually want that index - but it's best created after ingestion in a one-shot load like this. I'd also be interested in how the HDD you're reading data from is holding up in some of the highly parallel setups? reply PolarizedPoutin 12 hours agoparentThank you for reading and for your kind words! Ah I did not know about the `create_default_indexes=>false` and that a time index is created by default for hypertables. I'll add a note to explain this! Also curious to benchmark inserting without the time index then creating it manually. Even with 32 workers I think the HDD was fine. I did monitor disk usage through btop and the SSD that Postgres lived on seemed to be more of a bottleneck than the HDD. So my conclusion was that a faster SSD for Postgres would be a better investment than moving the data from HDD to SSD. reply rabernat 15 hours agoprevGreat post! Hi Ali! I think what's missing here is an analysis of what is gained by moving the weather data into a RDBMS. The motivation is to speed up queries. But what's the baseline? As someone very familiar with this tech landscape (maintainer of Xarray and Zarr, founder of https://earthmover.io/), I know that serverless solutions + object storage can deliver very low latency performance (sub second) for timeseries queries on weather data--much faster than the 30 minutes cited here--_if_ the data are chunked appropriately in Zarr. Given the difficulty of data ingestion described in this post, it's worth seriously evaluating those solutions before going down the RDBMS path. reply PolarizedPoutin 12 hours agoparentHey Ryan and thank you for the feedback! I agree that storing the data is appropriately chunked Zarr files is almost surely going to be faster, simpler to set up, and take up less space. Could even put up an API in front of it to get \"queries\". I also agree that I haven't motivated the RDBMS approach much. This is mainly because I took this approach with Postgres + Timescale since I wanted to learn to work with them, and playing around with ERA5 data seemed like the most fun way. Maybe it's the allure of weather data being big enough to pose a challenge here. I don't have anything to back this up but I wonder if the RDBMS approach, with properly tuned and indexed TimescaleDB + PostGIS (non-trivial to set up), can speed up complex spatio-temporal queries, e.g. computing the 99th percentile of summer temperatures in Chile from 1940-1980, in case many different Zarr chunks have to be read to find this data. I like the idea of setting up different tables to cache these kinds of statistics, but it's not that hard to do with Zarr either. I'm benchmarking queries and indexes next so I might know more then! reply ohmahjong 14 hours agoparentprevThis is a bit off-topic but I'm interested in the same space you are in. There seems to be an inherent pull between large chunks (great for visualising large extents and larger queries) vs smaller chunks for point-based or timeseries queries. It's possible but not very cost-effective to maintain separately-chunked versions of these large geospatial datasets. I have heard of \"kerchunk\" being used to try and get the best of both, but then I _think_ you lose out on the option of compressing the data and it introduces quite a lot of complexity. What are your thoughts on how to strike that balance between use cases? reply rabernat 13 hours agorootparent> It's possible but not very cost-effective to maintain separately-chunked versions of these large geospatial datasets. Like all things in tech, it's about tradeoffs. S3 storage costs about $275 TB a year. Typical weather datasets are ~10 TB. If you're running a business that uses weather data in operations to make money, you could easily afford to make 2-3 copies that are optimized for different query patterns. We see many teams doing this today in production. That's still much cheaper (and more flexible) than putting the same volume of data in a RDBMS, given the relative cost of S3 vs. persistent disks. The real hidden costs of all of these solutions is the developer time operating the data pipelines for the transformation. reply ohmahjong 12 minutes agorootparentThat's a great point, it really is all about tradeoffs. In my use case there is strong motivation to keep data creation times low, so writing out multiple datasets comes at a product/opportunity cost moreso than a storage cost. Thanks for the insight. reply counters 15 hours agoprevWhy? Most weather and climate datasets - including ERA5 - are highly structured on regular latitude-longitude grids. Even if you were solely doing timeseries analyses for specific locations plucked from this grid, the strength of this sort of dataset is its intrinsic spatiotemporal structure and context, and it makes very little sense to completely destroy the dataset's structure unless you were solely and exclusively to extract point timeseries. And even then, you'd probably want to decimate the data pretty dramatically, since there is very little use case for, say, a point timeseries of surface temperature in the middle of the ocean! The vast majority of research and operational applications of datasets like ERA5 are probably better suited by leveraging cloud-optimized replicas of the original dataset, such as ARCO-ERA5 published on the Google Public Datasets program [1]. These versions of the dataset preserve the original structure, and chunk it in ways that are amenable to massively parallel access via cloud storage. In almost any case I've encountered in my career, a generically chunked Zarr-based archive of a dataset like this will be more than performant enough for the majority of use cases that one might care about. [1]: https://cloud.google.com/storage/docs/public-datasets/era5 reply PolarizedPoutin 12 hours agoparentThe main reason why was that it's a personal project and I wanted to do everything on my home server so that I wouldn't have to pay for cloud resources, and so that I could learn Postgres, TimescaleDB, and eventuallly PostGIS. But as rabernat pointed out in his comment, pulling out a long time series from the cloud replica is also slow. And I know I eventually want to perform complex spatio-temporal queries, e.g. computing the 99% percentile of summer temperatures in Chile from 1940-1980. I don't doubt that a cloud replica can be faster, but it's at odds with my budget of $0 haha. reply roter 11 hours agorootparentI too need to do percentiles. One option is loop through the grids but bin/histogram it. You'll get a really good 99% from a 1 Kelvin bin width. Also, I've found the diurnal profile from ERA5 analysis can be abysmal in some locations. ERA5-Land is much better, high resolution, though only available over... er... land. To your point about not relying on cloud. Noted in the Google option [1] link above: > Update Frequency: The ERA5 dataset is currently not refreshed in the Google Cloud Public Dataset Program. The program provides ERA5 data spanning from 1940 to May 2023. Another alternative, Amazon [2], also deprecated: > The provider of this dataset will no longer maintain this dataset. We are open to talking with anyone else who might be willing to provide this dataset to the community. [2] https://registry.opendata.aws/ecmwf-era5/ reply rabernat 15 hours agoparentprevTrue, but in fact, the Google ERA5 public data suffers from the exact chunking problem described in the post: it's optimized for spatial queries, not timeseries queries. I just ran a benchmark, and it took me 20 minutes to pull a timeseries of a single variable at a single point! This highlights the needs for timeseries-optimized chunking if that is your anticipated usage pattern. reply bitschubser_ 2 minutes agorootparenta good source for ERA5 historical data is https://open-meteo.com/en/docs/historical-weather-api (not affiliated, just a happy user) you can also run open-meteo locally, its quite fast for spatial and timeseries queries reply orhmeh09 15 hours agoparentprevThat might be nice if someone would do it and teach others to use it. Some labs have an RDBMS-based pipeline with published algorithms and data that nobody wants to try to reimplement (and which nobody would be paid to do). About the best improvement we could get was moving from an ancient version of MySQL to Postgres + PostGIS. I think Timescale would have helped. There were other reasons also to run locally due to privacy, cluster access, funds etc. reply gingerwizard 1 hour agoprevFew in this thread have suggested ClickHouse would do well here. We tested 1 trillon rows recently, albeit much simpler data - https://clickhouse.com/blog/clickhouse-1-trillion-row-challe... This is a good dataset though and the level of detail in the post is appreciated. I'll give ClicKHouse a go on the same... Disclaimer: I work for ClickHouse reply carderne 19 hours agoprevHey OP (assuming you're the author), you might be interested in this similar experiment I did about four years ago, same dataset, same target, similar goal! https://rdrn.me/optimising-sql/ Similar sequence of investigations, but using regular Postgres rather than Timescale. With my setup I got another ~3x speedup over COPY by copying binary data directly (assuming your data is already in memory). reply PolarizedPoutin 19 hours agoparentWish I saw this before I started haha! I left a footnote about why I didn't try binary copy (basically someone else found its performance disappointing) but it sounds like I should give it a try. footnote: https://aliramadhan.me/2024/03/31/trillion-rows.html#fn:copy... reply carderne 18 hours agorootparentYeah I imagine it depends where the data is coming from and what exactly it looks like (num fields, dtypes...?). What I did was source data -> Numpy Structured Array [0] -> Postgres binary [1]. Bit of a pain getting it into the required shape, but if you follow the links the code should get you going (sorry no type hints!). [0] https://rdrn.me/optimising-sampling/#round-10-off-the-deep-e... [1] In the original blog I linked. reply anentropic 17 hours agorootparentI'd love to hear from anyone who's done the same in MySQL reply d416 10 hours agoprev“Is a relational database even appropriate for gridded weather data? No idea but we’ll find out.” Love this. It’s the exact opposite of all the other ‘well actually’ mainstream tech posts and I am here for all of it. Props for keeping the reader fully engaged on the journey. reply RyanHamilton 19 hours agoprevIf you want to plot time-series charts or many other charts directly from sql queries, qStudio is a free SQL IDE and works with everything including TimescaleDB: https://www.timestored.com/qstudio/database/timescale Disclaimer, I am the author. reply ayewo 13 hours agoparentWhat's the process for adding support for other databases to your tool qStudio? I'm thinking perhaps you could add support for Timeplus [1]? Timeplus is a streaming-first database built on ClickHouse. The core DB engine Timeplus Proton is open source [2]. It seems that qStudio is open source [3] and written in Java and will need a JDBC driver to add support for a new RDBMS? If yes, Timeplus Proton has an open source JDBC driver [4] based on ClickHouse's driver but with modifications added for streaming use cases. 1: https://www.timeplus.com/ 2: https://github.com/timeplus-io/proton 3: https://github.com/timeseries/qstudio 4: https://github.com/timeplus-io/proton-java-driver reply postgresperf 14 hours agoprevContributor to the PG bulk loading docs you referenced here. Good survey of the techniques here. I've done a good bit of this trying to speed up loading the Open Street Map database. Presentation at https://www.youtube.com/watch?v=BCMnu7xay2Y for my last public update. Since then the advance of hardware, GIS improvements in PG15, and osm2pgsql adopting their middle-way-node-index-id-shift technique (makes the largest but rarely used index 1/32 the size) have gotten my times to load the planet set below 4 hours. One suggestion aimed at the author here: some of your experiments are taking out WAL writing in a sort of indirect way, using pg_bulkload and COPY. There's one thing you could try that wasn't documented yet when my buddy Craig Ringer wrote the SO post you linked to: you can just turn off the WAL in the configuration. Yes, you will lose the tables in progress if there's a crash, and when things run for weeks those happen. With time scale data, it's not hard to structure the loading so you'll only lose the last chunk of work when that happens. WAL data isn't really necessary for bulk loading. Crash, clean up the right edge of the loaded data, start back up. Here's the full set of postgresql.conf settings I run to disable the WAL and other overhead: wal_level = minimal max_wal_senders = 0 synchronous_commit = off fsync = off full_page_writes = off autovacuum = off checkpoint_timeout = 60min Finally, when loading in big chunks, to keep the vacuum work down I'd normally turn off autovac as above then issue periodic VACUUM FREEZE commands running behind the currently loading date partition. (Talking normal PG here) That skips some work of the intermediate step the database normally frets about where new transactions are written but not visible to everyone yet. reply kabes 3 hours agoparentDo you have more info on the GIS improvements in PG15? reply postgresperf 1 hour agorootparentThere's a whole talk about it we had in our conference: https://www.youtube.com/watch?v=TG28lRoailE Short version is GIS indexes are notably smaller and build faster in PG15 than earlier versions. It's a major version to version PG improvement for these workloads. reply lawn 19 hours agoprevWhat an interesting post! > At a sustained ~462k inserts per second, we’re waiting ~20 days for our ~754 billion rows which is not bad I guess It’s less time than it took me to write this post. Hah, as I've been gravitating more to writing larger and more in depth blog posts I can relate to the surprising effort it can require. reply PolarizedPoutin 19 hours agoparentThank you! Yeah haha some of the benchmarks took several hours (and a few re-runs) and there was a lot of learning done along the way. reply to11mtm 9 hours agoprevAs someone who used to do some GIS hacking in an office job[0] before I was a 'Software Developer/engineer' this is super cool. [0] - Honestly some of the coolest stuff I ever got to do in it's own right. Building tools that could move data between AutoCAD, Microstation, and Google Earth while also importing other bits with metadata from Trimble units[1]. Also it was probably the most I ever used math in my entire career [2], so there's that. [1] - I wound up finding a custom font maker, and one of our folks made a font library with the symbols, made it easy to write a parser too :D [2] - One of those PDFs I always seem to wind up having floating on a hard drive is the USGS 'Map Projections, a working manual'. At one point I used it as a reference to implement a C# library to handle transforms between coordinate systems... alas it was internal. reply tmiku 17 hours agoprev> I think it would be cool to have historical weather data from around the world to analyze for signals of climate change we’ve already had rather than think about potential future change. This is a very good instinct! A pretty major portion of modern climate science is paleoclimatology, with a goal of reaching far beyond reliable human measurements. A lot of earth's previous climate states were wildly different from the range of conditions we have experienced in the past 10,000 years, and a better climate record is essential to predicting the effects of massive carbon emission. Ice cores from Antarctica/Greenland are the most famous instance of this, but there's a lot of other cool ones - there are chemical records of climate change in cave stalactites, ocean floor sediments, etc. reply randrus 13 hours agoparentSomewhat relevant (which I posted a while back): https://climate.metoffice.cloud/ reply semiquaver 20 hours agoprevAny idea why hypertable insert rates were slower? I though hypertables were supposed to _increase_ insert rates? reply perrygeo 19 hours agoparentHypertable insert rates are faster and more predictable over time. Each individual insert might incur a small bit of extra overhead, but they scale forever since each temporal chunk is indexed separately vs a regular table where the entire index needs to fit in memory. This is a case where you can't make meaningful inferences from micro-benchmarks (they tested 20k rows, you probably need 200M to start seeing the diff) reply PolarizedPoutin 19 hours agorootparentThanks for the insight! It is true that I started with a micro-benchmark of 20k rows for slower inserts, but I also did some longer benchmarks with ~772 million rows. reply leros 20 hours agoparentprevIsn't data inserted into basically a normal Postgres table with hypertable extensions? I don't know the details of Timescale but that sounds like it would incur a cost of a normal Postgres insert, plus potentially extra work at insert time, plus extra work in the background to manage the hypertable. reply rahkiin 16 hours agorootparentNot entirely. A hypertable is a postgres table chunked over time. There is the assumption that most data and queries are time-relevant, but also that older data is less relevant than new data. Indexes are per chunk. So if the query analyzer understands you only touch 2023 it can omit looking at any chunk that is from other years and keep those out of memory. Same with the indexes. reply PolarizedPoutin 20 hours agoparentprevYeah I'm curious about this too. Been meaning to ask on the Timescale forums. My only guess is that there's some small extra overhead due to hypertable chunking. I know Timescale has a blog post from 2017 claiming a 20x higher insert rate but that's for inserting into a table with an index. The general wisdom for loading huge amounts of data seems to be that you should insert into a table with no indexes then build them later though. So with no index, inserting into a hypertable seems a bit slower. Timescale blog post: https://medium.com/timescale/timescaledb-vs-6a696248104e reply h4kor 19 hours agorootparentTimescale hypertables automatically have an index on the timestamp. To make this more comparable you could create the same index on the normal table and test the ingestion rate. reply dcreater 2 hours agoprevNice! Can you upload the data to hf, oxen.ai, kaggle or something? reply islandert 10 hours agoprevIf you don't have access to COPY if the postgres instance is managed, I've had a lot of luck with encoding a batch of rows as a JSON string, sending the string as a single query parameter, and using `json_to_recordset` to turn the JSON back into a list of rows in the db. I haven't compared how this performs compared to using a low-level sql library but it outperforms everything else I've tried in sqlalchemy. reply twoodfin 7 hours agoparentWhen I see suggestions like this (totally reasonable hack!), I do have to wonder what happened to JDBC’s “addBatch()/executeBatch()”, introduced over 25 years ago. https://docs.oracle.com/javase/8/docs/api/java/sql/PreparedS... Did modern APIs & protocols simply fail to carry this evolutionary strand forward? reply tonymet 16 hours agoprevI encourage people to look into the ERA5 dataset provenance especially when you approach the observations made toward the \"pre industrial date\" of 1850 . Remember that modern global surface temperatures are collected by satellites, and the dataset is comingled with recordings observed visually & made by hand using buckets by sailors who were not primarily academic researchers. Segments of high resolution, low noise data (satellites) are mixed with low resolution, low coverage, high noise records (hand records on a boat made surrounding the united kingdom). My point is to be in awe of the technical aspects of this effort but also keep in mind that we are only making copies of low resolution, noisy manuscripts from sailors 170 years ago. reply shoyer 10 hours agoparentERA5 covers 1940 to present. That's well before the satellite era (and the earlier data absolutely has more quality issues) but there's nothing from 170 years ago. reply tonymet 9 hours agorootparentSimilar noise issues apply. Most of the other surface temp models have to cover 1850 reply relaxing 13 hours agoparentprevOk? And what’s your point in pointing out that? reply tonymet 10 hours agorootparentThe data is noisy so be careful when using it for research. Always account for the provenance of the records when working with \"data\". reply relaxing 9 hours agorootparentSo like basically every data science effort. reply tonymet 10 hours agorootparentprevone of the project's goals was to load the data and make predictions. The page covered the data loading part, but not the methods and error tolerance in the predictions reply hyperman1 19 hours agoprevTwo remarks with postgres and lots of data: 1) I always wonder of there is a better way than COPY. I tend to quickly get 100% CPU without saturating I/O 2) The row overhead seems big. A row has 23 bytes overhead, this table has 48 bytes data per row, so even without page overhead, we lose ~1/3 of our storage. This is pure data storage, without any index. reply PolarizedPoutin 19 hours agoparent1. Yeah to me it seems very hard to saturate I/O with Postgres unless maybe you insert into an unlogged table. I guess there's quite a bit of overhead to get all the nice stability/consistency and crash-resistance. 2. That is a good point. I'm hoping TimescaleDB's compression helps here but yeah I don't think you'll ever get the database size below the data's original footprint. reply feike 18 hours agorootparentTimescaler here, if you configure the timescaledb.compress_segmentby well, and the data suits the compression, you can achieve 20x or more compression. (On some metrics data internally, I have 98% reduction in size of the data). One of the reasons this works is due to only having to pay the per-tuple overhead once per grouped row, which could be as much as a 1000 rows. The other is the compression algorithm, which can be TimescaleDB or plain PostgreSQL TOAST https://www.timescale.com/blog/time-series-compression-algor... https://www.postgresql.org/docs/current/storage-toast.html reply hyperman1 18 hours agorootparentprevIf I look into perf, it seems mostly parsing overhead. I can saturate a create newtable as select from oldtable. Unfortunately, CSV seems still the lingua franca for transport between DBs. Maybe some day a more binary oriented transport protocol will appear( e.g parquet?) reply feike 18 hours agorootparentMany libraries for python, Rust, golang support COPY BINARY. The times I've tested it, the improvement is very small as compared to plain copy, or copy with CSV, whereas it does require more work and thought upfront to ensure the binary actually works correctly. https://www.postgresql.org/docs/current/sql-copy.html reply roter 17 hours agoprevI too use the ERA5 reanalysis data and I too need quick time series. As the data comes in [lat, lon] grids, stacked by whatever period you've chosen, e.g. [month of hourly data, lat, lon], it becomes a massive matrix transpose problem if you want 20+ years. What I do is download each netCDF file, transpose, and insert into a massive 3D HDF file organized as [lat, lon, hour]. On my workstation it takes about 30 minutes to create one year for one variable (no parallel I/O or processes) but then takes milliseconds to pull a single (lat, lon) location. Initial pain for long-term gain. Simplistic, but I'm just a climatologist not a database guru. reply curious_cat_163 8 hours agoprev> The data is output from a climate model run that is constrained to match weather observations. That's interesting. Why store it? Why not compute it as needed using the model? FWIW, I am not an expert in this space and if someone is, it would be good to understand it. reply hendiatris 17 hours agoprevIf you’re going to work with weather data use a columnar database, like BigQuery. If you set things up right your performance will generally be a few seconds for aggregation queries. I setup a data platform like this at my previous company and we were able to vastly outperform our competitors and at a much lower cost. The great thing about this data is it is generally append only, unless errors are found in earlier data sets. But it’s something that usually only happens once a year if at all. reply koliber 14 hours agoprevCurious if he could squeeze more performance by using a different structure to store the same data. Some of these float4 cols could probably be stored as int2. Depending on how many decimal places are needed, can divide the int to get the resulting floating point value. reply sammy2255 17 hours agoprevClickhouse will eat this for breakfast. And has built-in compression even at the column level reply nojvek 8 hours agoprevThis should be a benchmark. Could someone post me to where I can download the whole dataset? reply smellybigbelly 16 hours agoprevCan anyone give some advice on how they run TimeScale in Kubernetes? I’m seeing they dropped support for their Helm chart. reply jamesgresql 15 hours agoparentHi, we updated our docs with the best options. https://docs.timescale.com/self-hosted/latest/install/instal... I'd personally recommend StackGres, it's a great project. reply dunefox 19 hours agoprevOT: Does anyone know if DuckDB would be of use here? reply wiredfool 19 hours agoparentNot likely, unless it's a set of parquet files already. Clickhouse would be a better bet. reply Aeroi 8 hours agoprevWhat did Google use to train GraphCast? reply kinj28 8 hours agoprevIt's a tangent. I am curious if we query the data to give us temperature at a given time for all lat n long and plot it geo spatially , would the result give anything on heat distribution of energy received across the lat and long at that point in time? reply rkwasny 19 hours agoprevYeah, don't use TimescaleDB, use ClickHouse - I have 10 years of NOAA climate data on my desktop that I query when I want to go on holidays ;-) reply mfreed 18 hours agoparentOur experience is that Clickhouse and Timescale are designed for different workloads, and that Timescale is optimized for many of the time-series workloads people use in production: - https://www.timescale.com/blog/what-is-clickhouse-how-does-i... Sidenote: Timescale _does_ provide columnar storage. I don't believe that the blog author focused on this as part of insert benchmarks: - Timescale columnar storage: https://www.timescale.com/blog/building-columnar-compression... - Timescale query vectorization: https://www.timescale.com/blog/teaching-postgres-new-tricks-... reply rkwasny 18 hours agorootparentWell, as a Co-founder and CTO of Timescale, would you say TimescaleDB is a good fit for storing weather data as OP does? reply mfreed 12 hours agorootparentTimescaleDB primarily serves operational use cases: Developers building products on top of live data, where you are regularly streaming in fresh data, and you often know what many queries look like a priori, because those are powering your live APIs, dashboards, and product experience. That's different from a data warehouse or many traditional \"OLAP\" use cases, where you might dump a big dataset statically, and then people will occasionally do ad-hoc queries against it. This is the big weather dataset file sitting on your desktop that you occasionally query while on holidays. So it's less about \"can you store weather data\", but what does that use case look like? How are the queries shaped? Are you saving a single dataset for ad-hoc queries across the entire dataset, or continuously streaming in new data, and aging out or de-prioritizing old data? In most of the products we serve, customers are often interested in recent data in a very granular format (\"shallow and wide\"), or longer historical queries along a well defined axis (\"deep and narrow\"). For example, this is where the benefits of TimescaleDB's segmented columnar compression emerges. It optimizes for those queries which are very common in your application, e.g., an IoT application that groups by or selected by deviceID, crypto/fintech analysis based on the ticker symbol, product analytics based on tenantID, etc. If you look at Clickbench, what most of the queries say are: Scan ALL the data in your database, and GROUP BY one of the 100 columns in the web analytics logs. - https://github.com/ClickHouse/ClickBench/blob/main/clickhous... There are almost no time-predicates in the benchmark that Clickhouse created, but perhaps that is not surprising given it was designed for ad-hoc weblog analytics at Yandex. So yes, Timescale serves many products today that use weather data, but has made different choices than Clickhouse (or things like DuckDB, pg_analytics, etc) to serve those more operational use cases. reply dangoodmanUT 19 hours agoparentprevAgreed, clickhouse is faster and has better features for this reply RyanHamilton 18 hours agorootparentI agree. Clickhouse is awesomely powerful and fast. I maintain a list of benchmarks, if you know of any speed comparisons please let me know and I will add them to the lsit: https://www.timestored.com/data/time-series-database-benchma... reply lyapunova 15 hours agorootparentThanks for maintaining benchmarks here. Is there a github repo that might accompany the benchmarks that I could take a look at / reproduce? reply anentropic 19 hours agoparentprevDo you say that because its quicker to insert large batches of rows into Clickhouse, or because it's better in other ways? (I'm currently inserting large batches of rows into MySQL and curious about Clickhouse...) reply rkwasny 19 hours agorootparentIt's better in insert speed, query speed and used disk storage. reply gonzo41 19 hours agoparentprevSo click house is a column db. Any thoughts on if the performance would be a wash if you just pivoted the timescale hypertable and indexed the time + column on timescale? reply PolarizedPoutin 19 hours agoparentprevHaha very cool use! Yeah reading up on TimescaleDB vs. Clickhouse it seems like columnar storage and Clickhouse will be faster and better compress the time series data. For now I'm sticking to TimescaleDB to learn Postgres and PostGIS, but might make a TimescaleDB vs. Clickhouse comparison when I switch! reply cevian 16 hours agorootparentPlease note that TimescaleDB also uses columnar storage for its compressed data. Disclosure: I am a TimescaleDB engineer. reply rkwasny 19 hours agorootparentprevI can replicate your benchmark when I get a moment, the data, is it free to share if I wanted to make an open browser? I have a feeling general public has very limited access to weather data, and graphs in news that state \"are we getting hotter on colder\" are all sensational and hard to verify. reply sigmonsays 19 hours agoprevwhere can I download the weather data? Is it free or available if I sign up? reply roter 17 hours agoparentIf I read correctly, they are using the ERA5 dataset [0]. It is freely available and can be downloaded through a Python API called cdsapi. [0] https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysi... reply _bax 20 hours agoprevnext [18 more] [flagged] sixo 20 hours agoparentThe timeseries graph is too fine-grained to discern any long-term trends in average temperatures. This comes up a lot, these kinds of graphs always give people problems. A quick google turned up this website which shows the trend in annual means: https://www.meteoblue.com/en/climate-change/durban_south-afr... reply throwaway918274 20 hours agorootparentthe heisenburg climate change principle reply sixo 14 hours agorootparentNo, you misunderstand (probably because you wish that was true). It's a basic conclusion of data visualization principles. Any method that could detect the trendline would—e.g. basic statistics—but \"plot every single datapoint in a big cluster\" can't, so it doesn't. reply spi 20 hours agoparentprevIf climate change were visible at that scale (tiny resolution between 0 and 40 degrees) we'd be all boiled since a while. Still, you can see signs: the maximum temperature until 1990 or so seems to be around 35 degrees, since then there are several peaks above that value and in 2016 (?) it looks to be 38-39. It's certainly less visible on the peaks in the low, because maybe the absolute lowest scores appear to be in the 1990-2000 decade, but then again, all years in the 2010-2020 decade seem to be slightly higher than the minimum temperature in any other decade. That said, there is massive downscaling involved in such scale, so I wouldn't be too surprised if some details were just skipped and not visible. I wouldn't trust this interpretation much - if a visualization it needs to be, I'd rather plot a moving average with a window of 6 months at least (or even 1 year to entirely rule seasonalities out), and see if that one has an upward trend or not (I bet it does). [EDIT] I now see the post below with the year averages since 1979. It does indeed seem that 1995-1997 were abnormally cold years, and also that 2010-2020 is the warmest decade since then (and likely since quite a bit longer). So the outliers analysis here above seem to stand :-) reply _bax 19 hours agorootparentToo small signs in a very noisy data to permit to give panic to the world & people reply PolarizedPoutin 19 hours agorootparentI have not analyzed any data yet and the purpose of plotting the time series was to show an example of the data as a function of time. As others have already mentioned, the swing in Durban temperatures over the seasonal cycle is ~25°C while global temperature increases due to climate change so far are on the order of 1°C. Plus weather data tends to be quite noisy, just think how variable the weather can be day-to-day and we're squishing 80 years of that into one plot. Also worth noting that different places may experience climate change differently. Some places may be the average temperature go up, some maybe only in the summer, so you'll have to look at averages. Some places may see more extreme summer highs, so then you can't just look at averages but the average extremes or the tail end of the temperature distribution. So it'll be hard to discern any climate change from just a cursory glance. I'm not saying it's there, just that it requires more analysis. reply bo0tzz 19 hours agorootparentprevHave you read any climate science? These are not the (only) numbers that the knowledge is based on. reply _bax 19 hours agorootparentI am an engineer, I read a lot and numbers are numbers, not a religion. reply bigbillheck 16 hours agorootparentprevHuh, I wonder if climatologists might have based their analyses on more than just this single time series. No way of knowing. reply dfraser992 18 hours agorootparentprevTech lead for WEMC here - see https://tealtool.earth Straightforward charts of climate related data for different countries and regions around the globe For temperature and a few other variables, it shows historical data from the EU Copernicus service (C3S) along with three different projected series out to 2100 for CO2, it shows the latest historical data The charts are concerning and I am sure my co-workers are not hell bent on faking data to scare people just to get more funding; they work too much and go to too many meetings. reply jefb 19 hours agoparentprevThis must be what \"doing your own research\" looks like. Maybe try here instead of squinting at single time series of Durban: https://www.ipcc.ch/report/ar6/syr/downloads/report/IPCC_AR6... reply _bax 19 hours agorootparentWhy trust IPCC ? reply royjacobs 19 hours agorootparentWhy not trust experts in their field? reply _bax 18 hours agorootparentconflict of interest reply royjacobs 17 hours agorootparentRight, they got all of them, did they? reply TickleSteve 20 hours agoparentprevYou're trying to see a small change over a large fuzzy area. You need to look at the deviation from the mean (or some other reference), maybe filtered to remove the seasonal changes. reply _bax 19 hours agorootparentI know what I am seeing. I know how is made a graph like that. 80 years of noisy data in a small row. But it's enough to lowering the panic speech about C.C. reply jhoechtl 18 hours agoprevFor the German speaking among us: That should be 1 000 000 000 000 rows Million - million Milliarde - billion Billion - trillion reply aftbit 17 hours agoparentYes, this article is actually loading (order of magnitude) 10^12 rows. I was very surprised to learn about this the first time. In USA English: 10^3 -> thousand 10^6 -> million 10^9 -> billion 10^12 -> trillion 10^15 -> quadrillion 10^18 -> quintillion But there's another scale too! See this Wiki article for more https://en.wikipedia.org/wiki/Names_of_large_numbers reply samatman 15 hours agorootparentUnless you're reading historical documents, these words have only one meaning in the English language. It is in no sense American English specific. One advantage of the short scale is that its cadence is the same as that of SI units: million is mega, billion is giga, trillion is tera, and so on. The long scale of course has the -ard names, so this isn't exactly a problem, any more than French saying \"four twenties and nineteen\" for ninety-nine is a problem. The difference is one of the major reasons the SI scale names exist in the first place, in fact. It also matches the decimal separators used by everyone but Indians, which strikes me as more logical. reply aftbit 15 hours agorootparentSorry, I wasn't intending to imply that they were American English specific, just that I only have experience with English as spoken in the USA. What's the advantage of the long scale? Just historical? reply samatman 10 hours agorootparentI don't think the long scale actually has advantages compared to the short. One could argue euphony: the alternative repetition of million, milliard, billion, billiard, and so on, is pleasing in a way. But really, each is just a quirk of languages. I don't expect 7 to sound like \"seven\" in other languages, why expect billion to sound like \"billion\" rather than \"milliard\" or \"trillion\"? When conveying information across languages, we use numerals, sometimes with the SI scales, which are universal. Just another confusing faux amis to figure out when learning another language, really. reply rjmunro 19 hours agoprev [–] This annoys me: > You can use parameterized queries to protect against SQL injection, No, that't not what parameterized queries are for. That's just a side benefit. Parameterized queries are so that the database doesn't have to parse the data from SQL to do the query. It's stupid to turn all the data to text, send the text over the wire, escaped and quoted as appropriate, then parse it back to whatever form it was in originally when you can just send it binary straight to the database. And if you are doing many similar queries with different results, e.g. inserts as here, or maybe querying the user table by user id every page load, the database doesn't have to parse any SQL each time and can even reuse the query plan. This may be why psycopg3 performed better than pandas df.to_sql() function in the single row insert case. reply davedx 17 hours agoparentI disagree that the security is a side benefit. It’s why most people choose to use parameterized queries. reply PolarizedPoutin 19 hours agoparentprevThank you for reading through it thoroughly and pointing this out! I'm still new and learning Postgres so this is good to know. I will update the post. reply munk-a 17 hours agorootparentI would note that I think the above is a rather minority opinion - while parameterized queries are great for reuse simply using them to guard against SQL injection is still an excellent use of them. If your query is reusable then go for it, but most complex queries tend not to be reusable if they involve optional inputs. reply dboreham 18 hours agoparentprevQuick note to observe that all of the above while true becomes less of a practical issue as CPUs become faster vs i/o, which they have done and probably will keep doing. reply simiones 17 hours agorootparentIsn't the opposite mostly happening, with CPU's single-core performance mostly flat for the last decade, while I/O speeds have been improving every year (especially in networking, not as sure about disk I/O). reply forrestthewoods 17 hours agorootparentprevWait what no stop that what are you talking about. Transforming data and then transforming it back will always be stupid and costly – in more ways than one. reply relaxing 11 hours agorootparentNot if you save transit time that can be more quickly made up in processing time. reply rjmunro 1 hour agorootparentTrue, but in this case we are converting to strings that are longer because they require quoting and escaping etc. Transit time will be longer. reply pphysch 17 hours agorootparentprevFeels like you're a couple decades behind. Single core speeds haven't gone up much in the last decade, especially for \"server\" CPU models. reply eddd-ddde 18 hours agoparentprev [–] Can it actually reuse the query plan? Couldn't that lead to a worse performing plan eventually? Say after inserting lots of data such that a different plan becomes a better option. reply rjmunro 16 hours agorootparentI think it's going to be very rare that a database changes enough for a plan to be significantly worse than the optimal plan during the life of a prepared query. A database engine could easily mitigate against it by re-planning queries every 1000 runs of the query or have the query plan expire after 60 seconds or something. It might be worth trying to re-prepare the query from time to time as part of this kind of bulk insertion benchmarking. The other thing that would be good to try is closing the transaction every 1000 inserts or some other number. Doing several inserts in a single transaction is certainly better than 1 insert per transaction, but there may come a point where the transaction being too big starts to slow things down. reply munk-a 17 hours agorootparentprev [–] Usually the TTLs you'll set on these plans[1] are pretty short. It's helpful if you need to do a burst of a thousand queries of the same form over a short interval. 1. At least effectively - this is usually manged via persisting handles or plans themselves. 2. Which is usually a bad code smell from ORM overuse. Every time you dispatch a query you're paying latency overhead - so in an ideal world your report delivery will involve a single query against the database per report request (whether this is a website and those requests are page loads or you're running some internal system for business reporting). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Ali Ramadhan delves into constructing a weather data warehouse in TimescaleDB to analyze historical weather data for climate change signals using the ERA5 climate reanalysis product.",
      "Methods such as single-row inserts, multi-row inserts, and leveraging psycopg3 for quicker data loading into PostgreSQL are compared for efficiency.",
      "Techniques like using pg_bulkload and timescaledb-parallel-copy to enhance performance in inserting data into a hypertable and optimizing PostgreSQL configurations are underscored, addressing challenges like preventing duplicates and managing disk overload."
    ],
    "commentSummary": [
      "The discussion delves into managing and analyzing extensive weather datasets using tools like BigQuery, Redshift, Clickhouse, and TimescaleDB.",
      "Topics include optimizing data storage costs, query performance, geospatial analysis, and database configurations for improved efficiency when handling large datasets.",
      "Emphasis is placed on understanding coordinate reference systems, data provenance, transformation methods, and the impact of climate change when working with climate and weather data."
    ],
    "points": 356,
    "commentCount": 144,
    "retryCount": 0,
    "time": 1713271147
  },
  {
    "id": 40050844,
    "title": "NPR Editor Suspended for Criticizing Network's Bias",
    "originLink": "https://www.npr.org/2024/04/16/1244962042/npr-editor-uri-berliner-suspended-essay",
    "originBody": "Media NPR suspends veteran editor as it grapples with his public criticism Updated April 16, 202411:27 AM ET Originally published April 16, 20245:01 AM ET David Folkenflik Enlarge this image NPR suspended senior editor Uri Berliner for five days without pay after he wrote an essay accusing the network of losing the public's trust and appeared on a podcast to explain his argument. Uri Berliner hide caption toggle caption Uri Berliner NPR suspended senior editor Uri Berliner for five days without pay after he wrote an essay accusing the network of losing the public's trust and appeared on a podcast to explain his argument. Uri Berliner NPR has formally punished Uri Berliner, the senior editor who publicly argued a week ago that the network had \"lost America's trust\" by approaching news stories with a rigidly progressive mindset. Berliner's five-day suspension without pay, which began last Friday, has not been previously reported. Yet the public radio network is grappling in other ways with the fallout from Berliner's essay for the online news site The Free Press. It angered many of his colleagues, led NPR leaders to announce monthly internal reviews of the network's coverage, and gave fresh ammunition to conservative and partisan Republican critics of NPR, including former President Donald Trump. Conservative activist Christopher Rufo is among those now targeting NPR's new chief executive, Katherine Maher, for messages she posted to social media years before joining the network. Among others, those posts include a 2020 tweet that called Trump racist and another that appeared to minimize rioting during social justice protests that year. Maher took the job at NPR last month — her first at a news organization. In a statement Monday about the messages she had posted, Maher praised the integrity of NPR's journalists and underscored the independence of their reporting. \"In America everyone is entitled to free speech as a private citizen,\" she said. \"What matters is NPR's work and my commitment as its CEO: public service, editorial independence, and the mission to serve all of the American public. NPR is independent, beholden to no party, and without commercial interests.\" The network noted that \"the CEO is not involved in editorial decisions.\" In an interview with me later on Monday, Berliner said the social media posts demonstrated Maher was all but incapable of being the person best poised to direct the organization. \"We're looking for a leader right now who's going to be unifying and bring more people into the tent and have a broader perspective on, sort of, what America is all about,\" Berliner said. \"And this seems to be the opposite of that.\" Enlarge this image Conservative critics of NPR are now targeting its new chief executive, Katherine Maher, for messages she posted to social media years before joining the public radio network last month. Stephen Voss/Stephen Voss hide caption toggle caption Stephen Voss/Stephen Voss Conservative critics of NPR are now targeting its new chief executive, Katherine Maher, for messages she posted to social media years before joining the public radio network last month. Stephen Voss/Stephen Voss He said that he tried repeatedly to make his concerns over NPR's coverage known to news leaders and to Maher's predecessor as chief executive before publishing his essay. Berliner has singled out coverage of several issues dominating the 2020s for criticism, including trans rights, the Israel-Hamas war and COVID. Berliner says he sees the same problems at other news organizations, but argues NPR, as a mission-driven institution, has a greater obligation to fairness. \"I love NPR and feel it's a national trust,\" Berliner says. \"We have great journalists here. If they shed their opinions and did the great journalism they're capable of, this would be a much more interesting and fulfilling organization for our listeners.\" A \"final warning\" The circumstances surrounding the interview were singular. Berliner provided me with a copy of the formal rebuke to review. NPR did not confirm or comment upon his suspension for this article. In presenting Berliner's suspension Thursday afternoon, the organization told the editor he had failed to secure its approval for outside work for other news outlets, as is required of NPR journalists. It called the letter a \"final warning,\" saying Berliner would be fired if he violated NPR's policy again. Berliner is a dues-paying member of NPR's newsroom union but says he is not appealing the punishment. The Free Press is a site that has become a haven for journalists who believe that mainstream media outlets have become too liberal. In addition to his essay, Berliner appeared in an episode of its podcast Honestly with Bari Weiss. A few hours after the essay appeared online, NPR chief business editor Pallavi Gogoi reminded Berliner of the requirement that he secure approval before appearing in outside press, according to a copy of the note provided by Berliner. In its formal rebuke, NPR did not cite Berliner's appearance on Chris Cuomo's NewsNation program last Tuesday night, for which NPR gave him the green light. (NPR's chief communications officer told Berliner to focus on his own experience and not share proprietary information.) The NPR letter also did not cite his remarks to The New York Times, which ran its article mid-afternoon Thursday, shortly before the reprimand was sent. Berliner says he did not seek approval before talking with the Times. Media NPR defends its journalism after senior editor says it has lost the public's trust Berliner says he did not get permission from NPR to speak with me for this story but that he was not worried about the consequences: \"Talking to an NPR journalist and being fired for that would be extraordinary, I think.\" Berliner is a member of NPR's business desk, as am I, and he has helped to edit many of my stories. He had no involvement in the preparation of this article and did not see it before it was posted publicly. In rebuking Berliner, NPR said he had also publicly released proprietary information about audience demographics, which it considers confidential. He said those figures \"were essentially marketing material. If they had been really good, they probably would have distributed them and sent them out to the world.\" Feelings of anger and betrayal inside the newsroom His essay and subsequent public remarks stirred deep anger and dismay within NPR. Colleagues contend Berliner cherry-picked examples to fit his arguments and challenge the accuracy of his accounts. They also note he did not seek comment from the journalists involved in the work he cited. Morning Edition host Michel Martin told me some colleagues at the network share Berliner's concerns that coverage is frequently presented through an ideological or idealistic prism that can alienate listeners. \"The way to address that is through training and mentorship,\" says Martin, herself a veteran of nearly two decades at the network who has also reported for The Wall Street Journal and ABC News. \"It's not by blowing the place up, by trashing your colleagues, in full view of people who don't really care about it anyway.\" Several NPR journalists told me they are no longer willing to work with Berliner as they no longer have confidence that he will keep private their internal musings about stories as they work through coverage. \"Newsrooms run on trust,\" NPR political correspondent Danielle Kurtzleben tweeted last week, without mentioning Berliner by name. \"If you violate everyone's trust by going to another outlet and sh--ing on your colleagues (while doing a bad job journalistically, for that matter), I don't know how you do your job now.\" Berliner rejected that critique, saying nothing in his essay or subsequent remarks betrayed private observations or arguments about coverage. Other newsrooms are also grappling with questions over news judgment and confidentiality. On Monday, New York Times Executive Editor Joseph Kahn announced to his staff that the newspaper's inquiry into who leaked internal dissent over a planned episode of its podcast The Daily to another news outlet proved inconclusive. The episode was to focus on a December report on the use of sexual assault as part of the Hamas attack on Israel in October. Audio staffers aired doubts over how well the reporting stood up to scrutiny. \"We work together with trust and collegiality everyday on everything we produce, and I have every expectation that this incident will prove to be a singular exception to an important rule,\" Kahn wrote to Times staffers. At NPR, some of Berliner's colleagues have weighed in online against his claim that the network has focused on diversifying its workforce without a concomitant commitment to diversity of viewpoint. Recently retired Chief Executive John Lansing has referred to this pursuit of diversity within NPR's workforce as its \"North Star,\" a moral imperative and chief business strategy. In his essay, Berliner tagged the strategy as a failure, citing the drop in NPR's broadcast audiences and its struggle to attract more Black and Latino listeners in particular. \"During most of my tenure here, an open-minded, curious culture prevailed. We were nerdy, but not knee-jerk, activist, or scolding,\" Berliner writes. \"In recent years, however, that has changed.\" Berliner writes, \"For NPR, which purports to consider all things, it's devastating both for its journalism and its business model.\" NPR investigative reporter Chiara Eisner wrote in a comment for this story: \"Minorities do not all think the same and do not report the same. Good reporters and editors should know that by now. It's embarrassing to me as a reporter at NPR that a senior editor here missed that point in 2024.\" Some colleagues drafted a letter to Maher and NPR's chief news executive, Edith Chapin, seeking greater clarity on NPR's standards for its coverage and the behavior of its journalists — clearly pointed at Berliner. A plan for \"healthy discussion\" On Friday, CEO Maher stood up for the network's mission and the journalism, taking issue with Berliner's critique, though never mentioning him by name. Among her chief issues, she said Berliner's essay offered \"a criticism of our people on the basis of who we are.\" Berliner took great exception to that, saying she had denigrated him. He said that he supported diversifying NPR's workforce to look more like the U.S. population at large. She did not address that in a subsequent private exchange he shared with me for this story. (An NPR spokesperson declined further comment.) Late Monday afternoon, Chapin announced to the newsroom that Executive Editor Eva Rodriguez would lead monthly meetings to review coverage. \"Among the questions we'll ask of ourselves each month: Did we capture the diversity of this country — racial, ethnic, religious, economic, political geographic, etc — in all of its complexity and in a way that helped listeners and readers recognize themselves and their communities?\" Chapin wrote in the memo. \"Did we offer coverage that helped them understand — even if just a bit better — those neighbors with whom they share little in common?\" Berliner said he welcomed the announcement but would withhold judgment until those meetings played out. In a text for this story, Chapin said such sessions had been discussed since Lansing unified the news and programming divisions under her acting leadership last year. \"Now seemed [the] time to deliver if we were going to do it,\" Chapin said. \"Healthy discussion is something we need more of.\" Disclosure: This story was reported and written by NPR Media Correspondent David Folkenflik and edited by Deputy Business Editor Emily Kopp and Managing Editor Gerry Holmes. Under NPR's protocol for reporting on itself, no NPR corporate official or news executive reviewed this story before it was posted publicly. Katherine Maher uri berliner NPR journalism Facebook Flipboard Email",
    "commentLink": "https://news.ycombinator.com/item?id=40050844",
    "commentBody": "NPR suspends veteran editor as it grapples with his public criticism (npr.org)356 points by RickJWagner 22 hours agohidepastfavorite687 comments crmd 7 hours ago>… concerns that coverage is frequently presented through an ideological or idealistic prism that can alienate listeners. Speaking as a lifelong NPR listener who recently had to cut them off because of ideological exhaustion: yep! reply trothamel 7 hours agoparentThis. As someone who's probably center-right, I used to listen to NPR a lot because they'd often offer new perspectives on things. But now, they offer the same perspective, over and over and over again - and so I usually turn them off. reply crmd 6 hours agorootparentI feel like a person who’s been eating only one food group for the past couple of years and has developed a scurvy-like disorder. My mind is craving thoughtful, non-partisan, deeply intellectual conservative analysis of current events. Not cult of personality American GOP pop conservatism, not the dumb-dumb outrage machine new media personalities, but rather seriously legitimate academic right wing thought leadership to expand and challenge my thinking about the world. I honestly don’t know where to find it. reply soderfoo 3 hours agorootparentThe GoodFellows podcast by the Hoover Institute is high quality. [0] The group is composed of former National Security Advisor, H.R. McMaster, an economist and a historian, so you get diverse intellectual conservative perspectives. [0] https://www.hoover.org/publications/goodfellows reply jazzdev 3 hours agorootparentprevRichard Hanania's substack is intellectual, non-partisan and conservative. reply ragazzina 1 hour agorootparentRichard \"black-people-are-animals\" Hanania? reply ExpectedLizard 6 hours agorootparentprevI'm a big fan of the Dispatch. Solid center-right reporting with newsletters and podcasts. National Review is further right but still pretty good. reply sandspar 2 hours agorootparentprevUncommon Knowledge with Peter Robinson is good if you like interviews. It's like Charlie Rose meets conservatism. Full episodes on YouTube. reply mixmastamyk 2 hours agorootparentprevWilliam Buckley and his contingent died off around the turn of the century. I don't know what would bring those kind of folks out of hiding, if they even exist any longer. The magazine is still around. reply read_if_gay_ 4 hours agorootparentprev>academic there’s your problem, academia skews overwhelmingly leftist reply WalterBright 7 hours agoparentprevI have liberal friends who, before this essay appeared, told me that they'd stopped listening to NPR because of its slide into advocacy rather than reporting news. reply mixmastamyk 1 hour agorootparentIt's happening everywhere, people no longer can help themselves in professional situations. They must tell us what to think, rather than help (or teach us) how to think. For example, I don't remember any overt politics at high school, but ours feeds a steady diet. Entertainment is another area. reply pj_mukh 6 hours agoparentprevQuestion is..when are we going to see this kind of reckoning and self-analysis at Fox News? reply tim333 1 hour agorootparentIt's a bit of a different category in that NPR is public funded and so has kind of an obligation to represent the public. Fox is owned by the Murdochs and so it's a bit down to them. At least they fired Tucker. reply duped 6 hours agoparentprevGive me less 1A, and more Reveal or Snap Judgement. 1A is probably the poster child for transition from interesting content from marginalized voices that informs listeners into whatever it is today, which is circling around the drain of angry media told through a calming voice. I still think that the major national shows like Morning Edition and All Things Considered hit just like they did a decade ago - albeit one dimension of our politics is less deserving of air time of the other, and that is reflected in their coverage. reply datavirtue 6 hours agorootparentI have had to turn them off numerous times because they play Trump's sound bites too much. I guess they want to exact full impartial coverage but it comes to a point where some swill just doesn't need air time. I'm totally cool with paraphrased coverage but not giving that pig a megaphone. reply colechristensen 7 hours agoparentprevThat’s the perfect word: exhaustion. Every nth (for some small value of n) segment is on a progressive zeitgeist topic, the vast majority of them are not newsworthy, those that are are mostly unnecessarily projecting on top of a newsworthy story, and they seem designed simply to elicit an emotional response. reply datavirtue 6 hours agoparentprevSame. I still listen because, after all, it is still reporting as opposed to other outlets that are pure entertainment. But David Broncochio needs to go and they are over the top when it comes to actively promoting LGBTQ and race division. It is so bad that you have to call it propaganda and question their motives and sanity. At the end of the day these people at NPR care, and they are compassionate, but I do not find them credible on a lot of topics because they are only employing people who fit in. Economic and science issues? A joke. Horrible. reply ein0p 7 hours agoparentprevI thought it was just me. I put them on every now and then while driving, go “wtf?!”, and switch to listening to audiobooks or music again. reply datavirtue 6 hours agorootparentIf you want a real WTF put on the local AM right wing talk shows. It does get worse. They straight up spew lies and dog whistle hate on repeat. I can't accuse NPR of lying. reply ein0p 5 hours agorootparentI never listened to those. I used to listen to NPR pretty often while commuting. Nowadays it’s just incessant agenda forcing, 100% of the time. It’s tiresome and, I’d think, counterproductive. I used to donate as well. reply ilamont 12 hours agoprevIf the numbers Berliner revealed about audience losses are correct, the impact is surely being most felt at the local station level. If fewer people are tuning in, fundraising will suffer and cuts are inevitable. For instance, Boston has two NPR stations, WGBH and WBUR, and both are in trouble. This article talks about declining numbers of live listeners and resistance to digital transformation, but never mentions the issues brought up by Berliner. https://www.boston.com/news/the-boston-globe/2024/04/11/two-... reply wolverine876 10 hours agoparentEvery news outlet and broadcast media outlet are losing audience rapidly. There's no evidence of correlation between NPR's coverage and that outcome. For example, https://www.theatlantic.com/politics/archive/2024/04/conserv... reply coldtea 7 hours agorootparentEven if \"every news outlet and broadcast media outlet losing audience rapidly\", the criticism wasn't about NPR losing audience in general, but about NPR losing audience from a particular side predominantly. reply evantbyrne 7 hours agorootparentIt needs to be repeated that broadcast media is experiencing a bloodbath right now across all areas of the political spectrum. And according to publicly available data, the downwards trend is far more extreme for right-leaning publications[1]. So, pandering to right-leaning audiences is not the winning market strategy at the moment either. It's a lose-lose situation. Can NPR go too woke? Sure, anything is possible. And maybe they have on radio–I wouldn't know. It certainly doesn't seem to be that way from their website though. [1] https://www.theatlantic.com/politics/archive/2024/04/conserv... reply mike_hearn 38 minutes agorootparentThat article needs to be interpreted with caution. Firstly, as they say themselves, their data is misleading because it's comparing with Feb 2020 which was the start of the COVID crisis. Of course news outlets are going to have less traffic in Feb 2024 vs Feb 2020. And although they start by saying there's a bloodbath specifically in right-wing publications, later they admit their dataset yields huge drops also for left-wing publications as well e.g. Slate and The Washington Post are both down ~40%. Secondly, when they do a quick reality check by asking the owners of one of the sites whether the number for his site is true (a 90% drop!), he tells them it's \"laughably inaccurate\", feedback they then just ignore rather than trying to work out why there's disagreement. And although the authors assert that the drop is worse on the right, they don't really show that with data. They also point out that (assuming they effect they're talking about really does exist) it's probably driven by Facebook and Google manipulating their news feeds to suppress conservative news, not an actual drop in organic demand. So I think the article can't lead to many conclusions about market strategy other than don't trust Facebook or Google, which everyone on the right knew already. reply anonporridge 12 hours agoparentprevKUOW in Seattle seems to be in trouble too. Their sponsors have been getting increasingly cringeworthy. Just last week I heard a long sponsor message from Christian Science. They seem to scraping deeper into the bottom of the barrel and sponsor message seem to be increasing in quantity. reply dhosek 12 hours agorootparentAre you sure it was from Christian Science and not the Christian Science Monitor which has been a long-time sponsor (and if I recall occasional reporting partner) of NPR. reply anonporridge 11 hours agorootparentIt was a pretty explicit message along the lines of \"come discover how to connect with God with us\". reply SV_BubbleTime 5 hours agorootparentNot one is actually going to make you do it though. I’m not sure what the complaint is. You’re mad they paid to sponsor an NPR station? reply wolverine876 10 hours agorootparentprevThe Christian Science Monitor has long had top quality journalism, and sections on religion. What is their relationship with Christian Science (and what is that? is there a unified institution?)? reply ekidd 7 hours agorootparentChristian Science is a church founded by Mary Baker Eddy in 1879. They're most famous for believing in faith healing, which is part of a larger collection of beliefs with a \"mind/faith over matter\" flavor. They have churches throughout New England. In practice, they tend to be pretty mellow in that old-school \"New England religious movement\" sort of way. The Christian Science Monitor is a well-respected newspaper associated with the church. The few times I've read a paper copy, there was usually one editorial with a religious theme. Their religion did not otherwise color their reporting. I am not the least bit suprised that the Christian Science church might support NPR. Demographically, Christian Science members probably have very high overlap with NPR listeners. And they are, after all, a church which is best known for being associated with a newspaper. I would not be the least bit surprised if they donate to NPR mostly because they want to support public radio. reply lowkey_ 8 hours agorootparentprevChristian Science Monitor has been my favorite news source for a long time despite not really being religious. It's so objective and fair even on the most contentious of topics, and really tries to understand the human experience. They were founded by the same woman, Mary Baker Eddy, like a century ago, but today they just have one specifically-cited Christian Science article in each print. It seems they may have the same owner still, but seems like they very much have editorial independence, judging by the things I read. reply slackfan 9 hours agorootparentprevThere is, main HQ over in Boston. Both orgs were founded by the same people. I believe the Monitor offices are right next to the main Christian Science complex/are in it. reply tzs 7 hours agorootparentprevI definitely listen to KUOW a lot less than I used to. The main reason is that a few years ago they had a lineup on Saturdays that kept me listening most of the day. It included: • A Prairie Home Companion [1] • The Vinyl Cafe [2] • Wait Wait...Don't Tell Me! [3] • Says You! [4] • The Swing Years and Beyond [5] Those I'm sure were on Saturday. I know I'm missing 4 other programs from Saturday. I remember the following programs as being on weekends at the time, but can't remember which were Saturday and which were Sunday. • This American Life [6] • The Moth Radio Hour [7] • Snap Judgement [8] • Radiolab [9] • Freakonomics Radio [10] [1] https://en.wikipedia.org/wiki/A_Prairie_Home_Companion [2] https://en.wikipedia.org/wiki/The_Vinyl_Cafe [3] https://en.wikipedia.org/wiki/Wait_Wait..._Don%27t_Tell_Me! [4] https://en.wikipedia.org/wiki/Says_You! [5] https://archive.kuow.org/show/swing-years-and-beyond [6] https://en.wikipedia.org/wiki/This_American_Life [7] https://en.wikipedia.org/wiki/The_Moth [8] https://en.wikipedia.org/wiki/Snap_Judgment_(radio_program) [9] https://en.wikipedia.org/wiki/Radiolab [10] https://en.wikipedia.org/wiki/Freakonomics_Radio reply colmmacc 10 hours agorootparentprevI was a long time listener and donor to KUOW. My intention was to support local radio, hopefully get some more community pieces, and so on ... and that matched the donation drive pitches. When they decided to use their cash to empire build and buy a Jazz station for $8M, I completely gave up and could no longer even stomach listening. I like Jazz, but it hardly needs the help. It seemed like an utter betrayal. reply boringg 10 hours agorootparentI understand your position. I only want to add that jazz most definitely needs help it is not a financially successful genre for radio. That being said i bet you they bought it on the hopes of getting more revenue and programming and not to save jazz. reply UncleOxidant 9 hours agorootparentprevBut Seattle (and Boston) are very blue areas, the issues that Berliner is suggesting wouldn't seem to apply to those areas, they would seem more likely to apply to the hinterlands. The Boston situation probably implies that they just need to combine the two stations into one. reply mlinhares 7 hours agorootparentMostly because the issues he mentions are not related to the actual reason people are dropping, the reason is _there is a lot of competition for everyone's attention_. Last time I turned on the radio i was a teenager, maybe 20 years ago, with the internet and podcasts there's very little reason to tune in, i have car play in the car and both pocketcasts and spotify, why would I ever tune in? I listen to a lot of NPR podcasts and contribute to WHYY monthly but I don't think i ever tuned into WHYY, i don't even know the frequency they use :P Reality is that all media is pretty much toast unless you're some big name like the NYT and this is really sad because i really love the NPR podcasts, but not sure how they can survive long term without the local radios. reply sanderjd 6 hours agorootparentNPR is (or was) one of those \"big names like the NYT\". reply mlinhares 5 hours agorootparentThe goal wasn’t really to make a profit so no, definitely not like the NYT. And that might be the demise, had they done something like NPR one much sooner with direct subscriptions it might have been easier to make it work. I hope they survive though, hard to find content with the same quality and consistency elsewhere. reply rokkitmensch 6 hours agorootparentprevEither that or their ideological fart-huffing has finally alienated even the \"very blue\" reservoirs. reply paulmd 3 hours agorootparentNPR has always been a very particular neoliberal bent as well, with equal skepticism for leftists as for the right. The both-sidesism and he-said she-said reporting, and the general third-way-ness of it all has always been fucking intolerable for half of the blue party too. There probably is no org more emblematic of the backslide of journalism into “reporting facts, not taking perspectives” than NPR. They are simply craven, they have no perspective or spine, they stand for nothing, and that makes them instinctually repulsive. Like they literally are the journalists in movies who will happily say whatever their masters want this week. It’s disgusting, you might as well be VOA for all the perspective you’re getting. Swapping Diane Rehm for JJ Johnson or whatever is emblematic of that change for example. Diane Rehm never let a guest gish-gallop unopposed etc, JJ just went into sputter mode and was like “I don’t think all of that is true but-“ and gets run over again on his own show etc. It’s just bad in an aggressively “it’s your right to feel that way but…” kinda milquetoast way. They stand for nothing and have no position or perspective. And I know that’s the new school of journalism today but jesus christ it’s pathetic to see in action. What makes a man turn neutral? Unironically. reply sanderjd 6 hours agorootparentprevI think if they really are losing a lot of listeners (for whatever reason), it must be mostly from exactly those blue areas, because that's where they have a lot of listeners. You can't lose listeners in places you don't already have them! reply mywittyname 12 hours agoparentprevI still listen live on their app when I can, but a lot of their programs are also available in podcast format. I'm sure its hard to compete with so many alternatives to the same content. reply corn13read2 7 hours agorootparentOr possibly their content is far too weighted ;) reply jimbokun 11 hours agoparentprevIt is kind of comical that NPR made racial and gender diversity their main priority…and still have a listener base that’s much whiter than the country as a whole. reply samatman 9 hours agorootparenthttps://stuffwhitepeoplelike.com/2008/01/31/44-public-radio/ reply Wolfenstein98k 6 hours agorootparentprevIt's getting whiter! reply astrange 11 hours agorootparentprevNo conflict there if you're used to their New England shows like Car Talk and Wait Wait (* pretending Chicago is New England for the moment). They were always woke, as in constantly making identity jokes about ethnic white people like Jews and Italians. reply vanattab 2 hours agorootparentThis was a joke right? Of all the shows to use as an example of being woke your going to pick Car Talk? I can't stand NPR News but CarTalk was great. reply jimbokun 8 hours agorootparentprevThat’s kind of the opposite of woke. reply astrange 5 hours agorootparentNo, it's the same thing the article is complaining about now - constantly talking about different identity groups. Those are comedy shows so they present things through comedy. Also, the panelists only ever talk about their own identity group. Non-woke would be talking about other people's. reply mrcwinn 11 hours agoparentprevIs this really a business model issue? I understood that NPR’s funding primarily comes from corporate sponsors, not listeners or the government. If that’s true, there is less incentive to preserve local affiliates. Consolidation is inevitable, I would imagine. reply tootie 11 hours agorootparentThat's not exactly true. For one, all the local affiliates have their own budgets, their own expenses, revenues and staff. Up until this year, NPR was specifically prohibited from collecting donations from listeners. If you had gone to npr.org and clicked \"donate\" it would force you to donate to your local affiliate. Affiliates do not give NPR a cut of their donations. Instead they pay (on a sliding scale I think) for the rights to content produced by NPR. If you look at the sources of revenue for any given affiliate, it will probably be mostly donation from listeners. So taken as a whole, public radio is very much paid for by listeners. reply ed-209 10 hours agorootparentTheir actual problem would then seem to be this rather complex explanation to a simple question (ie. Where does your money come from / what are your conflicts of interest). Any time I've considered donating, I remember all the \"this segment brought to you by ...\" and figure they don't need me (and I don't need them). reply tootie 10 hours agorootparentDonations go straight to the local affiliate who probably did not get any corporate sponsorship and very much need listener support. And besides just saying \"corporate sponsor\" is not an actual indication of them buying coverage. It doesn't really track that corporations bribed them into being too liberal. reply tootie 11 hours agoparentprevJust to be clear, when it comes to radio audience, the audience is 100% local stations. NPR does not own any radio spectrum. The structure is that local stations have complete editorial control of their programming so long as it adheres to the principles of the NPR mission and in exchange they get access to the network of content produced by NPR and local affiliates. Bigger affiliates like the ones in Boston produce a lot of local content or even sell back to the national network, while smaller ones are mostly running NPR content. They are also all running their own budgets and revenue operations. reply crackercrews 11 hours agoparentprevWAMU had layoffs recently. reply cyanydeez 12 hours agoparentprevI assume the lost is entirety into the cloud of podcasts. I don't listen to any radio programs anymore and part of that is work from home. But I do listen to NPR programming via podcasts. Their business model seems to not survive the move to work from home. It's got nothing to do with politics. It's entirely the same technologies disrupting all media. We have simply stopped wholesale media consumption for the modern network. reply burningChrome 12 hours agoparentprevAs an aside, I always wondered why conservative radio always dominated liberal radio. Nearly every conservative pundit has a national show and is syndicated far and wide on AM/FM radio stations. Liberal shows you can't find with a search light. Remember Air America? It lasted two years before a host of scandals and a bankruptcy put it into the \"where are they now?\" bin as it limped along for another 4 years before shuttering. But I digress. Just curious why conservatives still love radio after so many decades and liberals have almost nothing comparable to listen to locally or nationally. reply csnover 11 hours agorootparentThere is a podcast miniseries called The Divided Dial[0] that answers the question of why conservative radio dominates in America, but very briefly, based on my understanding of their reporting: 1. The elimination of the Fairness Doctrine meant that radio stations no longer had a legal obligation to provide a fair reflection of differing viewpoints on matters of public importance; 2. The elimination of national ownership caps in the 1996 Telecommunications Act enabled a rapid and extreme consolidation of radio stations; 3. These new national radio conglomerates slashed costs by vertically integrating production, creating fewer shows, and rebroadcasting them to all their owned stations; 4. The concept of “format purity” spilled over from music radio into talk radio, causing commercial talk stations to switch from showcasing a variety of opinions to airing one political perspective all day; 5. The conservative talk radio format was perceived as less risky by radio executives, and so that was the format that commercial talk radio switched to. Air America may have eventually succeeded despite its many other flaws—except they owned no radio stations of their own, so there was no place for them to go in this hyper-consolidated, format-pure commercial market. [0] https://www.wnycstudios.org/podcasts/otm/divided-dial reply pesus 12 hours agorootparentprevI imagine at least part of it is age and aversion to change/new technology. reply nickff 11 hours agorootparentIt might just be that conservatives drive more (and that’s how most radio listeners tune in). reply disgruntledphd2 51 minutes agorootparentIf conservatives tend to live in less urban areas, this could definitely be true. reply anon291 11 hours agorootparentprevBecause many conservatives have blue collar jobs that have them in their car / at a work site by themselves where they can listen. Whereas progressives seem more likely to listen at a desk, hence the plethora of leftist podcasts. reply wolverine876 10 hours agorootparentBlue collar workers used to be overwhelmingly Democrats. reply int_19h 9 hours agorootparentThat was back when Democrats talked about class a lot more than they did about race. (Which, to be fair, they did in part because many early trade unions were openly and blatantly racist.) reply wolverine876 9 hours agorootparent> That was back when Democrats talked about class a lot more than they did about race. Democrats took on race as an issue in the early 1960s, leading the Republican's 'Southern Strategy'. White blue collar workers only shifted to the GOP in the last 10 years, I think, especially for Trump. reply rayiner 7 hours agorootparent> Democrats took on race as an issue in the early 1960s, That’s a weird way of framing a law that got only 60-40 support among democrats in Congress but 80-20 among republicans. But it was more of a “Mad Men/Lucky Strikes” situation: https://youtu.be/8Nvf4BteCR4?si=snxZFKB1HmaljUmR. Once both parties supported the civil rights act, the had to campaign on different issues. Democrats turned to a more academic and activist view on race, focused on using government to undo the effects of past discrimination. Legal equality having been achieved, republicans turned to social and religious conservatism. The more important piece of the puzzle is economics. In the 1950s, Ohio had a median income 57% higher than North Carolina. Illinois was 75% higher than Georgia. That gap started to close dramatically in the 1970s and 1980s, opening the door to republicans coming into the south on a pro-business and deregulation platform. It’s not a coincidence that nearly all of Toyota’s manufacturing facilities in the US except one are in what are now low regulation red states. reply matthewdgreen 7 hours agorootparentThat's a weird and extremely sanitized view of what happened. A more accurate explanation is that the Democratic party was a coalition of (mainly) Northern progressives who supported integration, and Southern \"Dixiecrats\" who were opposed. Following the Act's passage, the Southern coalition switched en masse to the Republican party. Many of these voters continued to oppose racial (and \"legal\") equality for many years, and in some cases are still do. Even as recently as last year, Alabama had a Congressional map struck down by the US Supreme Court for violations of the Voting Rights Act -- and the Alabama legislature then proceeded to ignore the decision and write a new map with exactly the same deficiencies [1]. [1] https://www.brennancenter.org/our-work/analysis-opinion/alab... reply wolverine876 4 hours agorootparentprevDemocrats of the time, when making the decision, knew they would lose the South. They did and the Republicans pursued what was/is called the \"Southern Strategy\", appealing to the former Dems. > That’s a weird way of framing It's not weird; it's conventional political history retold over and over. reply Loughla 7 hours agorootparentprevWhite blue collar workers switched during Reagan. reply ars 7 hours agorootparentprevTo me it seemed like the Democrats changed, not the blue collar workers. A lot of it is race, Democrats started essentially calling all White people racist, i.e. blaming all white people for generational racism. Tons of people who were not racist did not appreciate that - personally I believe it's why Hillary lost, her \"basket of deplorables\" comment lost her a lot voters. reply shrimp_emoji 7 hours agorootparentAt this point, to me the Democrats seem more racist than the Republicans. All you hear from the latter is oldschool liberal egalitarianism, whatever the sincerity; the former are obsessed about race and make everyone else obsessed by proxy with, for example, how they cast white guys as the antagonists in every piece of media. reply corn13read2 6 hours agorootparentThey did start the kkk… not a shock that it’s part of their party. So glad I’m not in America! reply int_19h 5 hours agorootparentYou should probably look up what a \"Dixiecrat\" is, and where they (and their voters) ended up eventually. reply wolverine876 4 hours agorootparentprev> Republicans. All you hear from the latter is oldschool liberal egalitarianism That seems very hard to reconcile with Donald Trump, overwhelmingly popular party leader, as well as many, many other Republicans who express hatred to contempt and create legalized discrimination against many groups, LGBTQ+ people and immigrants for example. reply int_19h 5 hours agorootparentprevLee Atwater has this to say about Republican politics of 1980s: \"All that you need to do to keep the South is for Reagan to run in place on the issues that he's campaigned on since 1964, and that's fiscal conservatism, balancing the budget, cut taxes, you know, the whole cluster. ... You start out in 1954 by saying, \"Nigger, nigger, nigger\". By 1968, you can't say \"nigger\" — that hurts you. Backfires. So you say stuff like forced busing, states' rights and all that stuff. You're getting so abstract now, you're talking about cutting taxes, and all these things you're talking about are totally economic things and a byproduct of them is, blacks get hurt worse than whites. And subconsciously maybe that is part of it. I'm not saying that. But I'm saying that if it is getting that abstract, and that coded, that we are doing away with the racial problem one way or the other. You follow me — because obviously sitting around saying, \"We want to cut this\", is much more abstract than even the busing thing, and a hell of a lot more abstract than \"Nigger, nigger\". So, any way you look at it, race is coming on the back-burner.\" I would say that the present state of the Republican party is very much in line with that, except that race didn't ever truly come onto the back-burner; somehow they're still talking about \"those people\" all the time. So, Republicans are the party of white people who are proud that they are white (and annoyed that they can't show their pride openly anymore), while Democrats are the party of white people who feel guilty about it (and annoy everyone else by trying to make everyone aware of how badly they feel about it). reply ars 4 hours agorootparent\"blacks get hurt worse than whites. And subconsciously maybe that is part of it. I'm not saying that\" He's \"not\" saying that, and then he goes and says that. He's point being that Republicans are most concerned with economic things, but because he things a better economic policy hurts blacks, therefor Republican are racist? Or, maybe, they just want a better economic policy, and they don't view everything through the lens of race? (i.e. color blind) While Democrats in contrast do view every decision through that lens. > somehow they're still talking about \"those people\" all the time. And \"they\" being Democrats or Republicans? Because I see Democrats talking about race far more than Republicans. Your last sentence tells me you view everything through the lens of race, but not everyone does that. reply ars 7 hours agorootparentprevAnd Republicans used to have way more college educated. Both parties seem to have swapped some people. reply kevin_thibedeau 10 hours agorootparentprevThe rich Republicans figured out the con in the 70's. Lure more non-rich people into your flock and you can eventually take control of Congress with a bloc you'd never be able to build with your own ilk. Then keep them sated by never delivering on promised reforms while blaming the other side. reply salad-tycoon 9 hours agorootparentThose days of drawing distinctions and flaunting moral superiority are over. It’s all one uni party. Time to stop fighting each other. Much of the same criticisms can be leveled at leading democrats. What they once championed they no longer do and have radically shifted multiple core positions. The “uni parties” are covered in filth and it benefits them for us to be at each other throats like it was some sports ball game. reply EasyMark 6 hours agorootparentThat's simply not true. The party platforms couldn't be much more diametrically opposed than they are these days. I would say almost all criticism can be laid on Republicans who have dropped all pretence of compassion for those who are poor or that don't share their values 100% and it has only intensified with MAGA. reply sandspar 2 hours agorootparentRepublicans are the party of poor people now. Democrat support is falling everywhere except among affluent whites. reply hellgas00 7 hours agorootparentprev“For every blue-collar Democrat we lose in western Pennsylvania, we will pick up two moderate Republicans in the suburbs in Philadelphia, and you can repeat that in Ohio and Illinois and Wisconsin.” Chuck Schumer, 2016. reply KerrAvon 11 hours agorootparentprevNope, it's the Reagan-era relaxation of ownership rules. Guess who owns the stations. And lots of liberals drive cars. reply quantumfissure 9 hours agorootparent1996, Clinton-D, not Reagan-R. I often see people blame Reagan for things he didn't do. For example Airline Deregulation has been making the rounds recently, blaming Reagan, that happened in 1978, Carter-D, several years before Regan became president. Did he deregulate a lot? Sure, but not everything. I am of no appreciation for Reagan (I identify as an Eisenhower/Rockefeller Centrist-Republican), but I like to put blame where blame is due. reply anon291 10 hours agorootparentprevWhy did Air America have so many issues? reply dhosek 12 hours agorootparentprevhttps://bigthink.com/guest-thinkers/why-do-conservatives-dom... reply jimbokun 11 hours agorootparentprevIsn’t NPR the glaring exception? reply thomastjeffery 11 hours agorootparentprevThere is no one true liberal narrative. There are somewhere in the range of 1 to 3 true conservative narratives. Politics in the US is represented with two parties: the right, and the tent. No one person can represent everyone in the tent. Anyone can represent the right. reply mindslight 10 hours agorootparentprevI'm libertarian with blue tribe sympathies. Whenever I try listening to NPR, I literally fall asleep. It's just something about the combination of cadence, intonation, and premium mediocre presentation of ideas that just paralyzes me. I'm powerless to stop it. Meanwhile I can listen to reactionary [0] talk radio just fine, and sometimes do just for cringe entertainment value. The bellowing of indignant righteousness is stimulating regardless of whether one agrees with the ideas. I don't know that this explains the overall popularity per se, but take it as one data point. [0] it's a grave mistake to refer to the current Republican party as conservatives. If anything, actual conservatism these days means supporting the Democratic party - respect for American institutions, the rule of law, strong foreign policy, gradual change, etc. The Democrats are even checking the box of fiscal conservatism compared with the past two decades of ZIRP. reply sandspar 2 hours agorootparentDemocrats are the rule of law party now? When did that happen? reply EasyMark 6 hours agorootparentprevI think all the yelling, fear mongering, hate for \"the other\" on the MAGA based shows just hits your brain different, they will push your buttons whether you enjoy it or it makes you angry at the ignorance, and that can be enjoyable in odd ways. NPR tries to be more reserved and academic, you won't find the cussin' and spittin' that you find on AM talk radio. reply CamperBob2 11 hours agorootparentprevConservatives are easier to herd. They may argue with what the Republican party is doing at any given moment, but ultimately they will fall in line. Centralized messaging is simply a better fit for the conservative worldview than it is for those on the liberal/left-leaning side of the spectrum. (Which in reality occupies a bigger tent than the GOP ever pitched.) reply angiosperm 11 hours agorootparentprevRight-wing radio is heavily subsidized by right-wing institutions funded by the billionaires we all know about. It is happy to repeat lies, without shame, at length until they are believed. No non-subsidized radio station can compete, so all AM talk radio is openly right-wing, and is all there is on the dial in most rural settings. reply r14c 11 hours agorootparentprevIME a lot of millenials and younger that are \"progressive\" are too left for what can be considered \"acceptable content\" on corporate platforms. Tiktok has pretty good content in this area, but I think a lot of it boils down to the biases of liberal broadcast media owners not keeping up with the kind of content people want to hear. Anything further left than liberalism is considered a \"national security issue\" so you don't get any interesting viewpoints. Even breadtube is pretty milk toast. I'll just stick to my hip hop and punk jams tyvm. reply cantaloupe 19 hours agoprevNPR is not a monolithic media organization. In my experience, local NPR stations are one of the best sources of interesting and relevant local news. In contrast, most local TV/Radio news is borderline a crime blotter ginned up to keep people outraged. Regarding the national NPR newsroom, I think this story will provoke positive change, as indicated in the article. There is no media which every person would consider unbiased, and very few media organizations take action to even attempt to reign in biases. The fact that editors will start reviewing coverage more closely to remove tilt sets a higher bar than all but a few news organizations. I chuckle thinking about a reporter stepping out of another random news room in the country and spreading outrage that the coverage has a bias. The response would generally be: “Yes, duh.” reply InTheArena 19 hours agoparentI think as shown by similar scandals at NYT and WSJ, that the media press do not accept feedback, and instead will rally around extending and furthering their ideological anti-liberal (authoritarian) monoculture, and instead get rid of dissenting voices. see James Bennet at NYT (who was fired for publishing a op-ed from a sitting American senator) or even Kevin D. Williamson at the Athletic. reply dekhn 13 hours agorootparentI can't see why everybody got so worked up about the op-ed you're referring to. The Times has traditionally been a venue for voices that are not in its constituency, and in this particular case, Cotton wrote such a crazy article that it reduced his credibility significantly in front of the nation. He proposed using the military to quell protests, https://www.nytimes.com/2020/06/03/opinion/tom-cotton-protes... reply eej71 13 hours agorootparentThe Atlantic ran an interesting piece about the details of how that op-ed came together. https://www.theatlantic.com/ideas/archive/2024/02/tom-cotton... reply dekhn 13 hours agorootparentI think if all op-eds published in the Times were inspected for factual accuracies, they'd find plenty in the ones that align with the Times's employees (the Cotton op-ed has a long preface which basically says \"we shouldn't have published this because facts\") reply faeriechangling 4 hours agorootparentprevNever look NPR or NYT to have an anti-liberal monoculture, I thought they were mostly liberal. reply cscurmudgeon 7 hours agoparentprev> There is no media which every person would consider unbiased, and very few media organizations take action to even attempt to reign in biases. NPR receives public money. They should be unbiased and objective. reply slibhb 12 hours agoprevThe lapse of journalistic objectivity over the past ~10 years is a dead horse. I do think we've turned a corner for the better. I haven't listened to NPR in years but the Times has improved over the past few years. One of the themes of Civil War, the new Alex Garland movie, concerns this dynamic. See his interview in the Times: https://archive.is/pzs1a. His theory is that the press is supposed to check polarization by disseminating objective facts (which never fit one faction's worldview perfectly) and this process' failure has led to increasing polarization. reply mannyv 11 hours agoparentJournalistic objectivity was basically invented by anti-Roosevelt media. Before that newspapers were explicitly partisan. Roosevelt was so dominant that Republicans felt the need to change the script. FYI, the Civil War was really started by the ridiculously stupid attack on Fort Sumter. If SC hadn't gone off and attacked the fort the US would have split into two or three countries...and everyone would have been OK with that. reply woopsn 8 hours agorootparentJoseph Pulitzer's retirement letter from 1907 (referencing St. Louis's metro paper): I know that my retirement will make no difference in its cardinal principles, that it will always fight for progress and reform, never tolerate injustice or corruption, always fight demagogues of all parties, never belong to any party, always oppose privileged classes and public plunderers, never lack sympathy with the poor, always remain devoted to the public welfare, never be satisfied with merely printing news, always be drastically independent, never be afraid to attack wrong, whether by predatory plutocracy or predatory poverty. It was he of course who had previously declared: Our Republic and its press will rise or fall together. An able, disinterested, public-spirited press, with trained intelligence to know the right and courage to do it, can preserve that public virtue without which popular government is a sham and a mockery. A cynical, mercenary, demagogic press will produce in time a people as base as itself. reply mcmcmc 10 hours agorootparentprev“Civil War, the new Alex Garland movie” is not actually about the US Civil War. FYI. And how exactly do you think everyone would have been ok with the US splitting into multiple countries? reply sanderjd 6 hours agorootparentYeah I've read some stuff about this guy named Abraham Lincoln, and somehow I got the impression he had quite a bit of power and was none too pleased by the idea of any dissolution of the union. reply Supermancho 10 hours agorootparentprevhttps://en.wikipedia.org/wiki/Journalistic_objectivity I think it's more like something that was formalized around the turn of the 19th century. Maybe wikipedia isn't accurate here. reply slibhb 11 hours agorootparentprev\"Telling the truth was invented in the 1930s\" Hmm reply tootie 11 hours agorootparentBias and accuracy are separate axes. Mother Jones is explicitly liberal media, but their reporting is highly factual. Fox News is right-wing and has settled multiple cases over defamation for spreading false stories. reply slibhb 11 hours agorootparentI don't agree. I think lies by omission are lies. reply verall 10 hours agorootparentAre you referring to something in particular? reply Wolfenstein98k 6 hours agorootparentThe Biden laptop story is the best-known recent example. Many relatively \"factual\" outlets refused to cover it, whereas if it were beneficial to the liberal cause, you know they'd have done the 12-36 goes hours of work required to verify it reply unethical_ban 8 hours agorootparentprevYou assert that bias implies leaving out information. That is false. reply ametrau 12 hours agoparentprevThe guardian seems less polemic and agitation propagandising recently also. reply spaceprison 20 hours agoprevI grew up listening to NPR, it was always on. Car talk with my dad on the weekends, Prarie home, etc. It's been programed in every car I've owned since I was a teenager. My wife and I have listened together and donated for years. But starting around 2019ish it gotten harder and harder to stay engaged with the programming. Almost every piece of reporting is now some kind of soft-outrage human-interest pseudo news. I want to listen but every other story is a tale of victim hood and oppression. It's just too much. reply wumeow 20 hours agoparentEvery time I tune in, I measure the time-to-race, which is the amount of time that passes before race becomes the main topic of discussion. Usually it’s less than 15 minutes. reply jiscariot 13 hours agorootparent20 year listener here. I now listen until they force identity politics in to the subject at hand, then change the channel. In my experience it's much less than 10m, but could be my market too. reply gosub100 8 hours agorootparentI play this game with my XM radio. On First Wave, I listen until they play The Police, then I switch to Lithium and listen until they play Foo Fighters. I cycle back and forth constantly. reply LVB 12 hours agorootparentprevSprinkle in climate change, and you'll be down to 5! I may be grading them too critically at this point, but in recent years, it feels like that XKCD about Wikipedia and how all roads lead to \"Philosophy.\" Sometimes, I'll sit there wondering how the leap will be made from some benign story to these anchor topics, but they usually manage. I don't like that predictability at all. reply sobellian 12 hours agorootparentprevI once tuned in to NPR when they were talking about artificial intelligence, and they were talking about how the seminal figures in the field (e.g. McCarthy) were white men. I reflected that if I had to pick the least interesting possible topic on AI, it would probably be how white the AI researchers were in the 1950s. I think this is the transcript: https://www.npr.org/transcripts/1161883646. > The Dartmouth conference has become an origin myth... Of course, the origin myth served to empower these men to tell their own story. And it's a story full of erasure... We hear nothing in that origin myth about the relationship that AI has to industrialization or to capitalism or to these colonial legacies of reserving reason for only certain kinds of people and certain kinds of thinking. (later, same show): > White men wanted to call themselves universal and produce themselves in the machine. I mean, seriously? reply jimbokun 11 hours agorootparentIt’s just odd that they feel the need to explain to their audience most professors, and especially mathematics and computer science professors, in the 1950s were white men. Or that a lot of the funds for the research came from industry or the military. It’s just not interesting or newsworthy. reply ForHackernews 11 hours agorootparent> Or that a lot of the funds for the research came from industry or the military. I think that's interesting and newsworthy. Maybe because we know it already it seems obvious, but a younger generation might not understand how deeply enmeshed the military-industrial complex was (and to some extent still is) in academia. reply treflop 9 hours agorootparentIt's something you are supposed to learn in history class and there are many, many topics that you should learn from history at some point in your life. A news publication is not a replacement for history class and is probably one of the worst places to get a complete view of history. reply dllthomas 9 hours agorootparent> A news publication is not a replacement for history class and is probably one of the worst places to get a complete view of history. Okay, but is it an okay topic for a history podcast like the one sobellian was apparently listening to? https://www.npr.org/podcasts/510333/throughline reply sobellian 8 hours agorootparentTo be clear, I have no problem with history podcasts (especially episodes about computing history), I just think they chose a poor lens for the subject matter. The military-industrial complex's influence on computing? Incredibly germane. But, and this is just how I heard it: > You know, the most disturbing part of the history of AI for me comes from the fact that these men who were working in artificial intelligence looked at those massive, noisy, hot mainframe computers and saw themselves in it. They looked at them and identified a deep affinity that there was something fundamentally shared between their minds and these machines. > White men wanted to call themselves universal and produce themselves in the machine. > I think underneath all of that arrogance and hubris is a real lack of faith in people. > And what I have always found so shocking about the Turing test is that it reduces intelligence to telling a convincing lie, to putting on the performance of being something that you're not. > ...And in effect, replace God with science? To me, it felt as if the piece was dripping with contempt for people that actually started the work on the basis that they were nerds with the wrong identity. reply jimbokun 7 hours agorootparentYes, there’s a lot of putting words in the mouths of people who are no longer alive and can no longer express their thoughts for themselves. reply dllthomas 8 hours agorootparentprevYeah, I wasn't really endorsing the program, just pointing out that \"history doesn't belong in news\" is a weird critique (not yours) of a history program. reply dumbo-octopus 10 hours agorootparentprev> to some extent To every extent, I would counter. It's incredibly rare that a person is currently involved in STEM research at a university and some sort of US Military grant isn't providing at least some amount of funding to it/them. My university for one allowed students to view comprehensive data about grants provided to research groups (after much internal campaigning), but absolutely refused allowing that data to be accessed by the general public. The reason was obvious when you looked at the data: 70%+ of the bucks came directly from various militaries. reply bawolff 9 hours agorootparentprev> The Dartmouth conference has become an origin myth... Of course, the origin myth served to empower these men to tell their own story. And it's a story full of erasure... We hear nothing in that origin myth about the relationship that AI has to industrialization or to capitalism or to these colonial legacies of reserving reason for only certain kinds of people and certain kinds of thinking. It would be nice if instead of harping on this point they actually told what they think the missing story is. If they think that AI is missing certain kinds of thinking - by all means tell me what they are concretely. Like i don't know if i buy that lack of diversity in earlier AI research meant that the AI research only allowed certain kind of \"thinking\" (AI doesn't really match anybody's thinking regardless of race), but i would be interested in a well researched argument that it did and what those other kinds of thinking are. I think the problem fundamentally is that these stories tend to be platitudes (lack of diversity = bad) but don't actually go deep into what that concretely means. Ultimately i want something that makes me think about the topic in new ways; you need to dig beyond the surface to do that. reply LamaOfRuin 8 hours agorootparent>Ultimately i want something that makes me think about the topic in new ways; you need to dig beyond the surface to do that. You, a person that already knows about this, will never get that from a short segment on a public radio show intended for a general audience that knows next to nothing about it. It is intended to make that audience think about it differently, because they have not even bothered to think critically about this at a surface level. The fact that these models (along with every structure in any society) embeds the biases of those that designed them seems to continue to elude so many commenters here though, so it seems they really need to keep hearing this too. reply jimbokun 7 hours agorootparentIt’s NPR, all their listeners think about these topics obsessively. Also, the models embeds the biases of all the people who create content on the Internet. The designers biases come in to l look at with the prompt hacks put in place later to prevent the AI from expressing bad thoughts. reply goatlover 7 hours agorootparentprevThey embed biases from the training data, which is taken from the internet at large. The models themselves aren't inherently biased. They're just trying to generate the next token or scene. And these models aren't from the 50s, or made by researchers in the 1950s. The models have guardrails added to try and prevent bias (and other deemed harmful content) being generated. reply MrMember 10 hours agorootparentprev>We hear nothing in that origin myth about the relationship that AI has to industrialization or to capitalism or to these colonial legacies of reserving reason for only certain kinds of people and certain kinds of thinking. It must be exhausting thinking like this all the time. reply zarathustreal 8 hours agorootparentIf you can convince yourself that something is a struggle for survival you can sustain it indefinitely. It’s part of our physiology reply sandspar 2 hours agorootparentprevStudies consistently show that liberals are unhappier and more mentally ill than conservatives. There was even a big Gallup poll about it this month. One of the floated reasons is that liberals tend to think negative thoughts, as you say. reply unethical_ban 19 hours agorootparentprevSimilar! I think some of the flagship programs talk nonstop about LGBT and minority issues, but this has been a thing for some years. I remember pre COVID driving to work chuckling at how every time I turned on the radio, it was a story on those topics. There is a lot more going on in the world that can also be discussed. I like Weekend edition and All Things Considered, and their hourly news updates. Finally: there is a distinction between a faux \"both sides\" centrism and constant focus on identity. Having a liberal bias can exist while providing a wide range of coverage and de-emphasizing identity politics. reply 01HNNWZ0MV43FF 12 hours agorootparentIf my local affiliate talked about LGBTQ stuff I would probably start turning the radio on again. reply superb_dev 12 hours agorootparentprevIf we could solve these issues, maybe we could stop talking about them reply brightball 10 hours agorootparentVirtually every person I talk to who remembers will tell you that the 90s were the peak of just about everybody in the country getting along and feeling good about the future. Then widespread internet and social media happened, which shortly led people into echo chambers while simultaneously gutting the institution of journalism. reply mr_toad 8 hours agorootparentWe didn’t light the fire https://en.wikipedia.org/wiki/1992_Los_Angeles_riots “the most destructive period of local unrest in U.S. history” reply Loughla 7 hours agorootparentprev>90s were the peak of just about everybody in the country getting along and feeling good about the future. What? All joking aside, what the Fuck? Rodney King. LA riots. NAFTA protests. Branch Davidians. Presidential sex scandals. First desert storm. Political correctness. Women's movements. Bosnia et.al. Rwanda. HIV/AIDS epidemic. Ruby Ridge. Wage stagnation. The final death of small town commerce due to Wal-Marts. And these are just the things I thought of in the last minute. I'm sure there are a ton more issues I'm missing. The people you talk to are doing what people have done since the beginning of time. They're remembering the past fondly while forgetting the bad. reply jimbokun 11 hours agorootparentprevEven minorities sometimes want to hear about something other than being a minority. reply rayiner 8 hours agorootparentEspecially minorities. Listening to liberal white people talk to other liberal white people about race is the eye-gougingly terrible. reply jimbokun 8 hours agorootparentI’m white, but I often notice all white panels discussing how awful white people are, but somehow neglected to invite a non-white person to participate in the discussion. reply rayiner 8 hours agorootparentAnd when they do, it’s someone they hand-picked to say the things they want to hear. I’ll note this though: I think the people who do this don’t really identify as “white.” It’s become a class marker. Your plumber is “white.” NPR listeners aren’t “white.” reply Loughla 7 hours agorootparentWhat are you talking about? Honestly what weird strawman are you trying to set up? NPR listeners aren't white? What are you trying to say here? reply willis936 12 hours agorootparentprevDoes force feeding develop an appetite? reply hackable_sand 11 hours agorootparentnext [4 more] [flagged] willis936 10 hours agorootparentForce feeding. NPR's stated goals are to tell stories with equitable representation but then take actions consistently contrary to that. reply lurking15 9 hours agorootparentprev> What would you call a century of heteronormative, white media? Much better than whatever activist garbage we have now. reply chris_wot 11 hours agorootparentprevIt was a bad thing. Not sure how this negates the argument. It was bad back then, and it's bad now? reply subsistence234 10 hours agorootparentprevWhen progressive \"solutions\" are implemented, if their success is measured empirically at all, they usually just make things worse. See e.g. the outcome of most educational reforms of the past 20 years, or the attempts in America's most progressive cities to solve homelessness by incentivizing and rewarding homelessness. Not surprising, a lot of profitable rackets depend on those issues remaining unsolved. What would e.g. all the homelessness experts do, who collect billions annually to hold conferences where they agree with each other on settler-colonialism and the newest genders, if they were to actually solve homelessness? Climate change is being solved, but it's being solved through technological innovation, not masturbatory discourse. reply xracy 14 hours agorootparentprevnext [39 more] Hot take... How many other news sources discuss race? I think this is an under-discussed topic for how pervasive a problem it is in our country. And I think we do ourselves a disservice by trying to hide from it. The more we talk about it, the easier it is to pick up a discussion where we left off. And my guess here is that the proportion of news about this relative to proportion of people affected by that news is way off. reply thegrim33 13 hours agorootparentIt completely, utterly, baffles me how other people live in such a different interpretation of reality than me. For myself, the last decade of media has been completely dominated by race-based and identity-based ideology and discussion. I've given up all mainstream media and I still cannot escape it. The fact that someone exists where they can with a straight face say that they think race is an under-discussed topic just blows my mind. To the point where I seriously have to consider whether I'm even replying to a real human being and not a shill / LLM. reply subjectsigma 12 hours agorootparentAccording to a 2001 poll, nearly 50% of the population drastically over-estimated the black population: https://news.gallup.com/poll/4435/public-overestimates-us-bl... I’m pretty sure more recent polls show the same thing is true now but I can’t find something more recent so take that with a grain of salt. Think of one of the 17% of people polled who think 50% of America is black. First, it’s baffling to me to understand how that’s even possible. They must be living in an extreme bubble where they seldom interact with other races. Likely this isn’t even their fault, so how would they ever know to correct it? Second, if I was one of those people, I would probably think the US is hopelessly racist seeing white people “over-represented” in basically every area of life. reply afavour 12 hours agorootparentprevMy perception is the opposite of yours so I guess we can be as equally confused as each other. Every time I tune into the news it's usually dominated by foreign policy, domestic horserace politics coverage and soft human interest stories. There was definitely an uptick in racial discussions during all the BLM stuff but \"completely dominated\" is very, very far from my lived experience. But I guess that's exactly what an LLM would say, isn't it? reply drewrv 11 hours agorootparentprevI’m curious to hear about your media diet because that has not been the case for me. reply crackercrews 11 hours agorootparentColeman Hughes shares a good perspective on this in his new book. He cites surveys showing that people changed their opinions for the worse regarding racism in america around 2010. This could be explained by an increase in racism or by increased awareness of racism. But he shows it is not based on these things because surveys also show that people significantly overestimate the number of black people killed by police. The rise in media coverage has led them to think that certain events are much more common than they are. reply xracy 8 hours agorootparent> But he shows it is not based on these things because surveys also show that people significantly overestimate the number of black people killed by police. What is the proper number of people to be killed by the police that we shouldn't worry about it? (hint: don't answer this question) The fact that people think the number is more than it actually is feels pretty moot when the number is more than zero. It's also more as a percentage of police killings than one would expect if we were just talking about police incompetence. If this has improved in the years since, then I think that's probably a good thing? Hot take, In general I think the police killing fewer people is a good thing. reply salad-tycoon 10 hours agorootparentprevAgreed. I’ve stopped watching anything recent. Theres a few Apple TV shows that I’ve enjoyed but it’s always so predictable that an engineered and coerced social justice issue is forced in. This is what the theory of equity is about, it goes over and beyond equality and actively helps by increasing exposure, opportunities, resources for historically disadvantaged people communities groups. I wish we could stop making everything into a war and fight and just let things speak for themselves. Show don’t tell. Lately I have been working in DEI committees to better understand. I believe in helping the poor and marginalized but having officially reviewed equity theory I feel a bit disgusted with myself. reply uejfiweun 12 hours agorootparentprevI don't know what that guy is smoking. It is objectively the case that the topics of race, identity politics, etc have skyrocketed since the mid-2010s [1]. [1] https://marginalrevolution.com/marginalrevolution/2019/06/th... reply xracy 8 hours agorootparentYeah... I don't know what you think I was saying. But I'm making the case that the news should cover issues that people actually face... And that an uptick in representation of different people's viewpoints and experiences is actually a positive and reasonable thing. It would be more informative if your data had that as a percentage of total articles. Because the case I'm making is that we went up from likeI think this is an under-discussed topic for how pervasive a problem it is in our country. The parts of it that are under-discussed are the parts they're still not discussing. The unsolved parts are unsolved as a result of bipartisan unwillingness to solve them. Example: Historical racism caused black people to lack the generational wealth to own a home. The solution to this is to make home ownership more affordable, i.e. to build more housing and bring down the market price of buying a home. But many of the existing homeowners, who don't want home prices to become more affordable, are Democrats, so this problem is unsolved even in areas like San Francisco under 100% Democratic control. Example: Parents want their kids to attend schools with smart kids so the other students aren't disruptive and don't require the teacher to slow the pace of the class. The solution to this is to put kids of similar intelligence in the same class, which also helps smart minority students who can get in based on test scores rather than money. But then affluent parents with disruptive or less intelligent kids don't get into the smart class, so they prefer solutions where the metric is parental income (e.g. ability to afford a home in the \"good\" school district) rather than test scores, and then get to feel good about themselves because even though this result is even worse for poor minority students, they can point to the statistics that minority students from poor backgrounds have lower test scores as a reason to refuse to use test scores and wrap their self-interest in the flag of anti-racism. Then many of those parents are Democrats, and moreover the alternate solution where you break up the \"income buys a good school district\" system would be things like school vouchers, which are opposed by public school teachers unions because they allow non-affluent parents to choose a private school if it's better, and those unions are a Democratic constituency. So again the problem goes unsolved, even in areas controlled by the people who claim to want to solve it. So these aren't the problems they spend most of their time talking about, because that would be goring the wrong ox. Instead they talk about identity politics and historical circumstance which cannot be solved because they are just abstract ideas and empty rhetoric instead of anything attached to a reasonable policy proposal that might actually do some good. reply jimbokun 11 hours agorootparentprevYes, race is a topic that is almost never discussed in 21st century USA. reply xracy 8 hours agorootparentThis person gets it. They read what I wrote. Definitely didn't just respond to a straw man version of what I said. /s reply akira2501 13 hours agorootparentprev> The more we talk about it, the easier it is to pick up a discussion where we left off. Sure.. but.. does it lead to problems actually being fixed? reply cooper_ganglia 12 hours agorootparentThe more we talk about it, the angrier people get. That's truly the only reason to even have a conversation about my ancestors I never met owning your ancestors you never met. Things have gotten substantially worse in the last 16 years, not better, and that's because we've been using a spotlight to point out how different everyone is from each other. It's literally counterintuitive. reply majormajor 12 hours agorootparent> The more we talk about it, the angrier people get. That's truly the only reason to even have a conversation about my ancestors I never met owning your ancestors you never met. Maybe the \"statute of limitations\" for these things should be long - not to mention the idea that racism wasn't magically fixed by ending slavery and the \"owning\" you mention. You don't think anyone was actively racist and causing harm 10 years ago? 20 years ago? 50? So if person Y's grandparent was harmed by, say, person X's racist grandparent in the 1950s, and that caused person Y's family to suffer for generations compared to what likely would've happened otherwise, and that's leading to ongoing societal harms, it could be legitimate public policy interest to try to even out opportunity. Of course, this hasn't actually changed in the last few decades - terms like \"equal opportunity\" and \"affirimative action\" have those words in their very name. But certain interests have made very successful pushes in the past few decades to brand policies under those umbrellas as \"actually the real racism\", or paint everyone supporting them as \"actually trying to guarantee equality of outcome,\" while continuing to beat the very-old drum of \"people being worse off implies worse ability, it's just science\" which couples oh-so-very-nicely with the more active forms of denying people opportunity that hardly ended in the 1960s. reply cooper_ganglia 10 hours agorootparentThe last time slaves were held in the US was by Native Americans in 1865. The country was founded in 1776. That means in a country that is nearly 250 years old, we haven't owned slaves for more than 160 years, well over half the nation's entire existence. No, we do not need a statute of limitations that encompasses generations for the sins of their fathers. In fact, how far back do you even take this? Should we also hold the people of the modern-day Republic of Benin responsible for what their ancestors, the Kingdom of Dahomey, did, which was abducting and selling their African brothers for a bit of cash? To address your other point, if my grandfather did something to hurt your grandfather in the 1950s, and it's been 70+ years and the only thing your family has figured out how to do since then is complain about how some guy was mean to your ancestor that one time, and that life is so unfair and you're so oppressed because of it, the issue may not be my grandfather, it may just be your family and their victimhood mindset. It feels good to \"be oppressed\" and have personal responsibility taken away, because when it's always someone else's fault, it can never be yours. reply carapace 8 hours agorootparentIt's hard to read > I was giving a lecture on genealogy and reparations in Amite, Louisiana, when I met Mae Louise Walls Miller. Mae walked in after the lecture was over, demanding to speak with me. She walked up, looked me in the eye, and stated, “I didn’t get my freedom until 1963.” https://www.vice.com/en/article/437573/blacks-were-enslaved-... It's an unspeakable evil. The \"statute of limitations\" on sins of the fathers is \"until atonement\". reply cooper_ganglia 6 hours agorootparentThat's awful that happened to those people, but this doesn't really serve the point imo, as this was already illegal by then. They even said that people hearing their story suggested they should've gone to the police for help, but the land was too large for them to escape. Even today, there's plenty of forced labor and sex slaves that exist in the USA. That doesn't mean it's an active systemic issue of oppression, it just means that bad people do bad things when they can get away with it, in spite of a system to stop it. We shouldn't hold people who didn't do bad things accountable for the actions of people who have done bad things. I don't think that's a radical idea. reply jimbokun 11 hours agorootparentprevIt is a legitimate topic of discussion. The problem comes when it’s the ONLY topic of discussion. Inflation is only relevant in how it impacts minorities. COVID is only relevant as it impacts minorities. Climate change is only relevant as it impacts minorities. Quality of schools is only relevant as it impacts minorities. When it’s your only lens, it can distort your views, and in the case of NPR, caused them to get some stories wrong. Which then destroys your credibility which is really the only currency a journalist has. reply xracy 8 hours agorootparentprevOn one of my other comment threads someone told me how the world is \"so much better then any point in history\" :). I'm not sure I believe that. I think in general being able to have these conversations means that we can progress slowly. It's tough recently, cause we've definitely regressed a lot. It would definitely help if the internet weren't effective at telling us only what we want to hear. I guess, in general I think the problem is we aren't actually able to progress discussions. So my original wording \"pick up where we left off\" was wrong. We can already do that, we just can't move on from there. I think if we can figure out how to do that without hitting each other then maybe the problems would get fixed. reply TheSoftwareGuy 13 hours agorootparentprevNot by itself. But if we don't talk about it, how could there possibly be hop of fixing anything? reply rurp 12 hours agorootparentBy focusing less on race and other identity issues, and working to remove racist policies across the board regardless of which group they benefit or harm. The 20th century saw immense improvements for almost all underserved groups, without any talking heads bickering about intersectionality or identity. Now that kind of coverage is everywhere and progress has stalled or even reversed in many areas. My impression is that a huge portion of identity politics and coverage is more about picking fights where each side can feel smug and superior, rather than actually changing things for the better. reply xracy 8 hours agorootparentHow does one address racist policies without discussing race? That's a knot I can't tie myself into. reply redserk 11 hours agorootparentprevSo this would be under the umbrella of \"Critical Race Theory\" which has been misrepresented and unjustly criticized as of late. reply recursive 9 hours agorootparentMaybe. Despite the number of times I've heard about CRT and how bad it is, and how good it is, I still don't have the foggiest idea what it is. One time, I tried to look it up on the internet. Signal to noise was so bad, I still don't know. I've written off the possibility of ever knowing what it is or understanding anyone who's talking about it. reply redserk 6 hours agorootparentAs with any field, there's a number of detractors and charlatans. With CRT in the political spotlight, I completely understand how it becomes a mess to make heads-or-tails of. Broadly speaking, it's the study of how law and media impacts society's view and treatment of others through the lens of race. As for my opinion on it: I generally support it because it advocates for another mechanism to study the impact of law. I think most legislation should be regularly studied for impact, effectiveness, and fairness. For example, stimulus bills ought to be reviewed for economic impact, regulation ought to be reviewed for effectiveness/relevance, laws with social impact ought to be reviewed to ensure it doesn't harm the people. A number of scholars seem to adopt a blameless mentality to figure out how laws (even unintentionally) have negative impact, and use that to propose solutions. I admire this approach to legislative critique. While I think a few ideas some scholars advocate for are infeasible to implement, the broader field has a lot of merit. reply onemoresoop 12 hours agorootparentprevI think it's a question of measure. When things are talked about fairly and equally then progress is made - there are serious pressing issues right not and they're not only about identity/race/gender, that these things are ignored is a big problem. When things turn full on on one direction they don't accelerate any progress, it may actually do more harm than good. reply johndhi 13 hours agorootparentprevis listening to people talk about it on the radio the 'discussion' we need to make progress on an issue like this? compare to Car Talk - a show that entertains you and teaches you about engineering. different value propositions of these two things reply taeric 13 hours agorootparentprevConverse hot take, the shallow \"race lens\" that is all too often used is not helping anyone. Quite the contrary, it is growing counterproductive. Especially with stories like this where they are working backwards from the framing, not using it to learn something. reply dekhn 13 hours agorootparentprevThe New York Times, which is the other bastion of the liberal establishment, also covers race a lot (in regular news journalism, opinion, and topical articles). . It's gotten to the point where lots of comments on articles (many articles have active comments sections) ask \"why are you making this about race?\". I think NYTimes swung heavily progressive a few years ago, and it was very unpopular, and they're recalibrating to be more relevant to centrists. reply xracy 8 hours agorootparentprevThis take appears to have been \"too hot for HN to handle\" For everyone downvoting, I'd be interested in the proportion of articles that discuss race as a percentage of total articles. If the number is more than the number of black people in America, I'd be willing to consider that we've \"overcorrected\". But a quick search around the web isn't yielding any clear results on this to me. reply burningChrome 12 hours agorootparentprev>> Hot take... How many other news sources discuss race? Almost all the conservative media outlets and all their pundits do simply because the whole topic has been an arms race for years. Conservatives are finally attempting to sway the popular narrative that race and identity politics are the only thing that determines your future. They often discuss race within the context of identity politics and the far-left idea that \"all white people are racist\" versus their notion that race doesn't determine who you are, how smart you are and how successful you can become. Its the age old philosophical idea of determinism vs. free will reply justin66 12 hours agorootparentprev> Hot take... How many other news sources discuss race? Heh. If you tune into the AM band you'll hear plenty of guys \"discuss race.\" It'll make you fear for the species, but still. reply pessimizer 13 hours agorootparentprevNPR, being the whitest place on Earth, is a bad place to discuss race. And the only reason they discuss it constantly is because they think that Democrats own any issue relating to it. Actual interest in issues around America's race problems would result in knowing a single person with two black parents, who is not an immigrant or the child of immigrants, and who is not wealthier and more privileged than you are. This probably describes a single-digit percentage of the people who produce content on NPR. They know the two or three black people who hung out in their circles in the elite colleges they went to, and only stay in contact with zero to one of them. They pretend to know every famous black person they met at a dinner party, or a conference; there are probably 100 black professors, writers, and entertainers that a million or two white NPR-Americans are pretending to be friends with. Black wealth peaked in 1997. NPR supplies itself from the most elite circles in American society, who largely control its wealth. They're not actually concerned, they're consistently using race as a cudgel to attack other white people for their own purposes. reply otterley 13 hours agorootparent> NPR, being the whitest place on Earth Come on, they're about as diverse as America itself is: https://www.npr.org/about-npr/179803822/people-at-npr In fact, Black people are proportionally overrepresented relative to the U.S. population at NPR, while Hispanics are underrepresented. Sunday morning's Weekend Edition host, Ayesha Rascoe, is a Black woman. Unfortunately they lost Audie Cornish, also a Black woman, from All Things Considered. She was great. I do think it's fair to argue that NPR is far more liberal than it ever was, and Uri Beliner's inside story provides some credence to that. But that's different from being \"white.\" (Keep in mind that the vast majority of Trump voters are white and would bristle at NPR's news programming; support by non-whites is relatively low.) reply Modified3019 10 hours agoparentprevYep, many years ago NPR was quite eclectic and a great way to satisfy my curiosity on weekends as a kid or later at my factory job. Incidentally, it now occurs to me that HN is basically my current replacement. Even if I have zero interest in a linked topic, I’ll often find a comment or discussion that’s enlightening and furthers my perspective of something in a meaningful and positive way. reply kulahan 9 hours agorootparentI treasure this site and the insightful comments I read. It’s one of the last places I’ve found on the internet where someone can bring up a controversial topic and get legitimate responses. reply ryanisnan 9 hours agorootparentThis is an under appreciated statement. Pre-Elon takeover of Twitter, I tried for months to engage in legitimate, argumentative yet constructive conversations. I was met time and time again with reductive, inflammatory, and disingenuous responses. And this was in the infosec community, one that for years was one of, in my opinion, last places on Twitter for that kind of discourse. I quit shortly after. reply cvwright 8 hours agorootparentThis might be the first time I’ve heard someone refer to infosec Twitter in a positive light. They were always inflammatory and belligerent and spiteful. The same crowd has now migrated to ActivityPub. You can find them there still acting the same way today. reply megous 9 hours agorootparentprevOnly about computer technology. Other topics here are undistinguashable from the rest of the general Internet. reply disgruntledphd2 15 minutes agorootparentI dunno, certainly there's much more a norm of politeness here, along with decent moderation. To contrast, the FT used to have good comments (and still does on really obscure topics) but overall they've gone massively downhill because of the lack of decent norms around thoughtful conversation. reply zarathustreal 8 hours agorootparentprevWhile I do agree to some extent, and I’m grateful for what we have here, I think it’s important to call out that we are largely moderated by the community (flagging, downvoting) and still relatively conservative about what is able to be discussed. Pretty much any controversial concept related to anything going on in USA politics is going to get shut down. reply shiroiushi 8 hours agorootparent>Pretty much any controversial concept related to anything going on in USA politics is going to get shut down. Good! USA politics are so ridiculous and toxic, it's better to have a place that's free of all that garbage. However, I don't agree that these discussions get \"shut down\": I see Americans posting their libertarian, gun-fetish stuff here all the time. reply GolberThorce 8 hours agorootparentprevnext [3 more] [flagged] thanksgiving 8 hours agorootparentPlease flag obvious spam and ragebait. Even if you're a spammer or rage baiter yourself, you will benefit from the site getting rid of other spam and rage bait. reply tangentstorm 6 hours agorootparentYou have unlocked: Irony!! reply BeetleB 13 hours agoparentprev> I grew up listening to NPR, it was always on. Car talk with my dad on the weekends, Prarie home, etc. Note: These are not NPR shows. They're merely shows that your (and most) local NPR affiliates purchased for broadcasting. If you think your local affiliate doesn't have enough of these types of shows, let them know! Many local affiliates have wide discretion on the programming. More details: https://www.npr.org/about-npr/178640915/npr-stations-and-pub... reply CoastalCoder 10 hours agorootparent> Note: These are not NPR shows. They're merely shows that your (and most) local NPR affiliates purchased for broadcasting. That seems like a pretty fine distinction. If nothing else, NPR makes decisions about which externally produced shows to license. In the end, NPR deserves all the credit / responsibility for what it broadcasts. It reminds me of the distinction NPR makes (used to make?) between \"advertising\" and \"underwriting\". Maybe the distinction was relevant for some legal / regulatory things. But it wasn't relevant for e.g. discussions about whether or not they were subject to \"advertiser\" pressure on their content. reply UncleOxidant 9 hours agorootparent> In the end, NPR deserves all the credit / responsibility for what it broadcasts. I think it's more like: In the end, your local NPR station deserves credit/responsibility for what it broadcasts. Not all of the shows they broadcast are NPR shows. There's PRI, for example. The other thing is that sometimes there are multiple NPR options that could be presented. EDIT: I checked the schedule of my local NPR station: The morning/afternoon schedule: - Morning Edition (NPR) - On Point (seems to be NPR out of WBUR) - Here and Now (also NPR out of WBUR - I actually think this one is generally pretty good) - Think Out Loud (local production of OPB - really good local interview show, the guy is an excellent interviewer, I wouldn't be surprised if he gets moved up to the national level at some point) - BBC Newshour (BBC, of course) - The World (PRI - actually a really informative program about events outside the US) - All Things Considered (NPR) - Today Explained (VOX podcast) - Marketplace (APM - informative business show) So not all are NPR. And in some cases the non-NPR shows are some of the better ones. reply eurleif 12 hours agorootparentprevCar Talk was an NPR show. reply BeetleB 12 hours agorootparentnext [17 more] Nope. It was a locally produced show that was licensed to NPR. reply eurleif 11 hours agorootparentI assumed you were trying to distinguish shows like A Prairie Home Companion, which was distributed by American Public Media, from shows distributed by NPR; and that you were simply mistaken about Car Talk. That would have been a somewhat meaningful point, despite the incestuous relationship between all of these organizations. However, if you're arguing that shows distributed by NPR aren't \"NPR shows\" unless they are also directly produced by NPR, then you're not only being pedantic, but being pedantic on the basis of a definition that is not widely shared. NPR's own Terry Gross has described Car Talk in these exact words, \"an NPR show\".[0] A large part of NPR's mission is distributing shows produced by local affiliates, and no doubt they exercise significant editorial discretion in determining which shows to distribute. For the purposes of this discussion, who cares if a show is produced directly by NPR, or if it is produced by another organizaton using NPR's money and then distributed by NPR? [0] https://www.npr.org/transcripts/361408028 reply BeetleB 11 hours agorootparent> NPR's own Terry Gross has described Car Talk in these exact words, \"an NPR show\".[0] It's interesting that you invoke Terry Gross as being part of NPR, when NPR actually says otherwise: > Several programs that NPR distributes are produced by NPR Member Stations, not NPR. These include top-rated news and cultural programs such as Fresh Air with host Terry Gross from WHYY... https://www.npr.org/about-npr/178640915/npr-stations-and-pub... NPR shows are things like All Things Considered. Car Talk was produced by an independent affiliate (just like Fresh Air). Yes, I am distinguishing between the two. If NPR doesn't consider Fresh Air to be an \"NPR show\", then nor do they consider Car Talk to be an NPR show. There's a difference between these and things like TV shows. Stuff like The Simpsons is actually a FOX show (as in whatever company makes them is owned by Fox). Whereas NPR never \"owned\" Car Talk, just as they don't currently \"own\" Fresh Air. These shows can always choose not to be part of NPR syndication. It's ultimately a licensing deal. They do own All Things Considered. reply eurleif 11 hours agorootparentNPR does not say Fresh Air is \"not an NPR show\". They say it is produced by a member station. You are then superimposing your own personal definition of \"NPR show\", which Terry Gross for example does not share, onto that statement. Fresh Air's X handle is @nprfreshair, and you want to tell me it's not an NPR show? More importantly: how is any of this possibly relevant to the original conversation? reply BeetleB 11 hours agorootparent> NPR does not say Fresh Air is \"not an NPR show\". They say it is produced by a member station. They also explicitly say that it is not produced by NPR. It's not an NPR show in the sense that when the licensing deal expires, Fresh Air can choose not to be syndicated on NPR. It's an independent show that licenses itself to NPR. A show like All Things Considered has no such freedom. > More importantly: how is any of this possibly relevant to the original conversation? The original conversation was about how one can influence their local affiliate to change their programming, until someone came and nitpicked about whether NPR owns Car Talk or not. reply eurleif 11 hours agorootparentNo, the original conversation was about the perceived decline in quality at NPR. You then popped in to say that it was up to the local affiliates, not up to NPR, because these aren't \"NPR shows\" (by your own personal definition of that phrase). But regardless of whether we adopt your definition (and forsake the definition that Terry Gross and just about everyone else use), how is a decline in the quality of shows that NPR distributes not their responsibility, given that they can and do choose which shows to distribute? reply shagie 10 hours agorootparentprevhttps://youtu.be/aChiTXPHpCY?si=Y0dp431Kq96peLwl&t=3205 > Car talk is a production of Dewey Cheetah Howe and WBUR in Boston. And even though NPR staffers pass through the stages of denial anger depression and acceptance then go back to denial again whenever they hear us say it this is NPR. https://youtu.be/1ExBaSRyXEM?si=mszTKyvWhrOU2Ezg&t=3180 > Car talk is a production of Dewey Cheetah Howe and WBUR in Boston. And even though hearing aid salesmen are consumed by guilt whenever they hear us say it this is NPR. How much less NPR can it be when the hosts say at the end \"This is NPR\"? reply foobar1962 9 hours agorootparent> Car talk is a production of Dewey Cheetah Howe... Dewey, Cheatum & Howe. It's a joke name like Sillius Soddus or Biggus Dickus. reply shagie 9 hours agorootparent(edit: I was copying from the auto-generated YouTube transcript) https://www.cartalk.com/content/history-car-talk > Nine months after starting with Susan, in the fall of 1987, NPR agreed to launch \"Car Talk\" nationally. So there we were, following in the footsteps of award programs like \"All Things Considered,\" \"Weekend Edition,\" and \"Morning Edition.\" We, like you, remain entirely mystified and have no idea what combination of prescription medicines brought about a decision like this out of NPR's management. We can only assume that they were looking for some cultural diversity, trying somehow to balance their high quality programming with crud like ours. Stations turned to us in droves - much in the same way that lemmings flock to the sea. > We discovered pretty quickly that producing a national radio show is a lot of work! Shortly after going national, we decided we needed a staff. That way, our afternoon naps could continue uninterrupted and, when not napping, we could still pursue our CAFE study. (Don't confuse this with the government's Corporate Average Fuel Economy report. Ours is about latte and cappucino in the greater metropolitan Boston area.) So, in 1989, we founded Dewey, Cheatem and Howe. reply BeetleB 10 hours agorootparentprevNot sure what your point is. The hosts are clearly pointing out that NPR staffers disagree with them on this point. reply shagie 9 hours agorootparentYou haven't listened to enough of the end credits. https://www.npr.org/people/2100834/tom-and-ray-magliozzi In 1977, Tom and Ray were invited to the studios of NPR member station WBUR in Boston, along with other area mechanics, to discuss car repair. Tom accepted the invitation, and when he was invited back the following week, he asked, \"Can I bring my brother, Ray?\" The rest, as they say, is history. The Magliozzis were subsequently given their own weekly program, Car Talk, which soon attracted a large local following. In January 1987, then host Susan Stamberg asked Tom and Ray to be weekly contributors to NPR's Weekend Edition and on October 31, 1987, Car Talk premiered as a national program, presented by NPR. ... Car Talk is produced for NPR by Dewey, Cheetham & Howe and WBUR in Boston. Doug Berman is the Executive Producer. --- https://www.npr.org/podcasts/510208/car-talk > The Best of Car Talk From NPR > What are you going to do if two NPR hosts proclaim that you are so friendless you have to hang a porkchop around your neck just to get the dog to play with you? And what if those heartless hosts were your very own uncles? Find out on this episode of the Best of Car Talk. That's listed under https://www.npr.org/podcasts/organizations/s1 which is \"NPR Podcasts & Shows\" with \"NPR\" in the top left corner of the icon. If you go to Spotify https://open.spotify.com/show/7y4fsFiHniACpBxFbvYKzY - this is from NPR and not On The Media (which I also listen to on a public radio station) https://open.spotify.com/show/3ge9HkAgzE1PP6193I1Vet and in https://www.npr.org/podcasts/452538775/on-the-media it is listed in a different organization ( https://www.npr.org/podcasts/organizations/s552 ). reply dylan604 9 hours agorootparentprevWow, I've never heard Cheetah. It's like totally missing the punny. Did you miss half the jokes if you heard Cheetah? Do you hear Tony Danza instead of tiny dancer too? reply wannacboatmovie 12 hours agorootparentprev\"Car Talk was originally a radio show that ran on National Public Radio (NPR) from 1977 until October 2012, when the Magliozzi brothers retired.\" Source: Wikipedia I'm laughing at everyone trying to split hairs over Car Talk in this thread. Most long-running programming on PBS were all \"locally produced\". reply BeetleB 11 hours agorootparent> Most long-running programming on PBS were all \"locally produced\". And I would split hairs with PBS as well :-) As I mentioned in a sibling post, NPR shows are things like \"All Things Considered\". Stuff like \"Fresh Air\" and Car Talk are not considered as NPR shows - not even by NPR themselves. reply wannacboatmovie 10 hours agorootparent> Stuff like \"Fresh Air\" and Car Talk are not considered as NPR shows - not even by NPR themselves. So when NPR themselves[1] refer to it as \"NPR's Car Talk\" in official copy, you're saying that is horseshit? You should write to NPR and let them know they are wrong. Also this: Please respond to the strongest plausible interpretation of what someone says, not a weaker one that's easier to criticize. Assume good faith. [1] https://www.npr.org/2014/11/03/357428287/tom-magliozzi-popul... reply CoastalCoder 10 hours agorootparentprevThis makes me nostalgic. I wish I could hear Tom weigh in on this discussion, and then hear Ray talk him down from the ledge. reply shagie 7 hours agorootparentFrom their 25th year anniversary - https://www.youtube.com/watch?v=A6atDT3cmrs reply willcipriano 13 hours agorootparentprevnext [6 more] [flagged] nostromo 12 hours agorootparentThe funny thing is that NPR's listenership is extremely white. National Review showed a while back that 90% of listeners are white. More recent numbers estimate it's around 80%. Believe it or not, Fox News' viewership is much more diverse than NPR's. NPR fails dramatically by the standards of diversity that they hold everyone else to. It makes sense that they now cater exclusively to self-hating white people, since that's the only market they have left after years of declines in programming quality. reply wannacboatmovie 12 hours agorootparentThey have to cater to the group of people that would willingly spend $50 on a tote bag with their logo on it. reply cyanydeez 12 hours agorootparentprevBelieve it or not, they're losing viewers because of technology and consumption patterns. Programming is still available in podcasts and elsewhere. reply dingnuts 12 hours agorootparentprevThis isn't surprising and doesn't contradict the parent at all. The most obnoxiously nouveau-racist people are predominantly white, like Robin DiAngelo Like, seriously, do you think Latin people came up with the term \"Latinx\"? This strain of self-hating progressivism has been predominantly white for-ev-er reply BeetleB 12 hours agorootparentprevDid the affiliate say it or just some random show said it? reply ordinaryradical 19 hours agoparentprevA useful heuristic for measuring news quality is to ask yourself, “Am I more informed about what’s happening or about what people are angry about.” Like you, I was a life-long listener and donater. I stopped both during the pandemic when I noticed NPR was playing the anger game, like every other outlet, for social media points. reply lainga 13 hours agorootparentBig context shift for me was realising roughly 2019 that Portal:Current events could efficiently replace 95% of my news scrolling https://en.wikipedia.org/wiki/Portal:Current_events reply int_19h 10 hours agorootparentSome other useful things in this department: https://thenewpaper.co - sends you a daily text message with a short, concise daily news report that is sufficient to be aware of anything major. https://www.boringreport.org - aggregates news stories on a particular subject from various outlets and produces AI-generated summaries of them with all the clickbaity headlines and other forms of button-pushing removed. reply vraylle 13 hours agorootparentprevI had no idea this existed. Me likey. Now if I can just get this as an RSS feed.... reply pulpfictional 12 hours agorootparenthttps://www.to-rss.xyz/wikipedia/ reply wizardwes 12 hours agorootparentprevSame here. Let me know if you find a good solution reply lainga 12 hours agorootparentHere's as close as I got with a bit of fiddling. You may or may not be able to winnow out minor changes using the inverttags parameter [] https://en.wikipedia.org/w/api.php?hidebots=1&hidecategoriza... [] https://en.wikipedia.org/w/api.php?action=help&modules=feedr... reply pulpfictional 12 hours agorootparentprevhttps://www.to-rss.xyz/wikipedia/ reply bmitc 10 hours agorootparentprevInteresting. Although I have a growing concern about how much of my knowledge base is informed or misinformed by Wikipedia. reply grotorea 10 hours agorootparentprevThat only covers global news though. reply trashface 13 hours agoparentprevI don't listen anymore but still like to use text.npr.org for news, its pretty easy to scan the headlines and mentally filter out most of the social justice pieces (and there are a lot of them). TBF I don't think NPR is really much different then most other mainstream lefty sources. I think axios is way worse than NPR (a lot of their \"articles\" are just vibes with really poor evidence, at least NPR still tries to do some traditional reporting). reply doublepg23 12 hours agorootparentYou may like https://brutalist.report/ - very easy to filter just by changing the URL query too. reply bevekspldnw 9 hours agorootparentprevSame, and it’s pretty good. Not sure what the complaints are as I only use the text only version. reply aaronax 19 hours agoparentprevImages of what I imagine to be their yearly performance goals rush through my head as soon as it turns to victim, race, oppression, etc. \"25% of stories uplifting Black voices\" etc. It just seems so forced. reply kenjackson 19 hours agorootparentWould “a focus on making sure we also give the conservative angle“ also seem forced? reply aaronax 17 hours agorootparentYes I have started reading the piece by Uri now and it basically confirms what I was imagining. \"He declared that diversity—on our staff and in our audience—was the overriding mission\" \"Journalists were required to ask everyone we interviewed their race, gender, and ethnicity (among other questions), and had to enter it in a centralized tracking system.\" Pretty much guaranteed that they were trying to hit race/gender quotas. reply josephg 13 hours agorootparentI always find it on the nose when “diversity” is used to mean “aligned with modern leftist political ideals”. That’s just not what that word means. If NPR wants actual diversity (of opinion), they should consider tracking the political affiliation of the people they interview in their database. But in my experience, DEI (Diversity, Equity, Inclusion) never seems to include a diversity of political views. I find that very suspicious. reply edflsafoiewq 11 hours agorootparentWhen we can talk about \"a diverse candidate\", the word has obviously become untethered from any ordinary meaning. reply Wolfenstein98k 6 hours agorootparentRead \"non-white\", \"non-straight\", and/or \"non-male\" in place of \"diversity\" and you will see that it works 100% of the time. reply faeriechangling 4 hours agorootparentprevIt is strange to me. It seems obvious to me that it would result in more diversity to have say, a latino network with all latino reporters who interviewed latinos, and a different network which was multi-ethnic, and a Fox News like network which was white as heck, than it is to just have the 2nd. Diversity is implemented in a strangely homogenous way where there is only one monocultu",
    "originSummary": [
      "NPR suspends senior editor Uri Berliner for publicly criticizing the network's progressive bias and trust issues, igniting controversy internally.",
      "Colleagues are divided, with some agreeing with Berliner's concerns and others feeling betrayed by his actions.",
      "New NPR chief executive Katherine Maher encounters conservative backlash over past social media posts, prompting monthly internal coverage reviews."
    ],
    "commentSummary": [
      "Criticism is directed at NPR for presenting coverage with an ideological bias, alienating center-right audiences, leading them to seek non-partisan conservative analysis elsewhere.",
      "Concerns include bias, agenda-pushing, lack of viewpoint diversity, and the decline in broadcast media viewership.",
      "Discussions cover political party history, conservative talk radio impact, media representation critiques, focus on systemic issues like Critical Race Theory, and skepticism towards NPR's quality and diversity initiatives."
    ],
    "points": 356,
    "commentCount": 687,
    "retryCount": 0,
    "time": 1713268162
  },
  {
    "id": 40055120,
    "title": "Rewilding the Internet: Embracing Diversity and Democracy",
    "originLink": "https://www.noemamag.com/we-need-to-rewild-the-internet/",
    "originBody": "We Need To Rewild The Internet The internet has become an extractive and fragile monoculture. But we can revitalize it using lessons learned by ecologists. Noah Campeau for Noema Magazine EssayDigital Society By Maria Farrell and Robin Berjon April 16, 2024 Facebook Twitter Email Credits Maria Farrell is a writer and speaker on technology and the future. She has worked on technology policy at the International Chamber of Commerce, ICANN and The World Bank. Robin Berjon is an expert in digital governance and has contributed to numerous web standards, including the Global Privacy Control (GPC). He works on novel web protocols such as IPFS and sits on the W3C’s Board of Directors and the ICO’s Technology Advisory Panel. “The word for world is forest” — Ursula K. Le Guin In the late 18th century, officials in Prussia and Saxony began to rearrange their complex, diverse forests into straight rows of single-species trees. Forests had been sources of food, grazing, shelter, medicine, bedding and more for the people who lived in and around them, but to the early modern state, they were simply a source of timber. So-called “scientific forestry,” was that century’s growth hacking: it made timber yields easier to count, predict and harvest, and meant owners no longer relied on skilled local foresters to manage forests. They were replaced with lower-skilled laborers following basic algorithmic instructions to keep the monocrop tidy, the understory bare. Information and decision-making power now flowed straight to the top. Decades later when the first crop was felled, vast fortunes were made, tree by standardized tree. The clear-felled forests were replanted, ready to extend the boom. Readers of the American political anthropologist of anarchy and order, James C. Scott, know what happened next. It was a disaster so bad that a new word, Waldsterben, or “forest death,” was minted to describe the result. All the same species and age, the trees were flattened in storms, ravaged by insects and disease — even the survivors were spindly and weak. Forests were now so tidy and bare they were all but dead. The first magnificent bounty had not been the beginning of endless riches, but a one-off harvesting of millennia of soil wealth built up by biodiversity and symbiosis. Complexity was the goose that laid golden eggs, and she had been slaughtered. The story of German scientific forestry transmits a timeless truth: When we simplify complex systems, we destroy them, and the devastating consequences sometimes aren’t obvious until it’s too late. That impulse to scour away the messiness that makes life resilient is what many conservation biologists call the “pathology of command and control.” Today, the same drive to centralize, control and extract has driven the internet to the same fate as the ravaged forests. The internet’s 2010s, its boom years, may have been the first glorious harvest that exhausted a one-time bonanza of diversity. The complex web of human interactions that thrived on the internet’s initial technological diversity is now corralled into globe-spanning data-extraction engines making huge fortunes for a tiny few. Our online spaces are not ecosystems, though tech firms love that word. They’re plantations; highly concentrated and controlled environments, closer kin to the industrial farming of the cattle feedlot or battery chicken farms that madden the creatures trapped within. We all know this. We see it each time we reach for our phones. But what most people have missed is how this concentration reaches deep into the internet’s infrastructure — the pipes and protocols, cables and networks, search engines and browsers. These structures determine how we build and use the internet, now and in the future. They’ve concentrated into a series of near-planetary duopolies: For example, as of April 2024, Google and Apple’s internet browsers have captured almost 85% of the world market share, Microsoft and Apple’s two desktop operating systems over 80%. Google runs 84% of global search and Microsoft 3%. Slightly more than half of all phones come from Apple and Samsung, while over 99% of mobile operating systems run on Google or Apple software. Two cloud computing providers, Amazon Web Services and Microsoft’s Azure make up over 50% of the global market. Apple and Google’s email clients manage nearly 90% of global email. Google and Cloudflare serve around 50% of global domain name system requests. Two kinds of everything may be enough to fill a fictional ark and repopulate a ruined world, but can’t run an open, global “network of networks” where everyone has the same chance to innovate and compete. No wonder internet engineer Leslie Daigle termed the concentration and consolidation of the internet’s technical architecture “‘climate change’ of the Internet ecosystem.” Walled Gardens Have Deep Roots The internet made the tech giants possible. Their services have scaled globally, via its open, interoperable core. But for the past decade, they’ve also worked to enclose the varied, competing and often open-source or collectively provided services the internet is built on into their proprietary domains. Although this improves their operational efficiency it also ensures the flourishing conditions of their own emergence aren’t repeated by potential competitors. For tech giants, the long period of open internet evolution is over. Their internet is not an ecosystem. It’s a zoo. Google, Amazon, Microsoft and Meta are consolidating their control deep into the underlying infrastructure through acquisitions, vertical integration, building proprietary networks, creating chokepoints and concentrating functions from different technical layers into a single silo of top-down control. They can afford to, using the vast wealth reaped in their one-off harvest of collective, global wealth. “Our online spaces are not ecosystems, though tech firms love that word. They’re plantations; highly concentrated and controlled environments … that madden the creatures trapped within.” Facebook Twitter Email Taken together, the enclosure of infrastructure and imposition of technology monoculture forecloses our futures. Internet people like to talk about “the stack,” or the layered architecture of protocols, software and hardware, operated by different service providers that collectively delivers the daily miracle of connection. It’s a complicated, dynamic system with a basic value baked into the core design; key functions are kept separate to ensure resilience, generality and create room for innovation. Initially funded by the U.S. military and designed by academic researchers to function in wartime, the internet evolved to work anywhere, in any condition, operated by anyone who wanted to connect. But what was a dynamic, ever-evolving game of Tetris with distinct “players” and “layers” is today hardening into a continent-spanning system of compacted tectonic plates. Infrastructure is not just what we see on the surface; it’s the forces below, that make mountains and power tsunamis. Whoever controls infrastructure determines the future. If you doubt that, consider that in Europe we’re still using the roads and living in many towns and cities the Roman Empire mapped out 2,000 years ago. In 2019, some internet engineers in the global standards-setting body, the Internet Engineering Task Force (IETF), raised the alarm. Daigle, a respected engineer who had previously chaired its oversight committee and internet architecture board, wrote in a policy brief that consolidation meant network structures were ossifying throughout the stack, making incumbents harder to dislodge and violating a core principle of the internet: that it does not create “permanent favorites.” Consolidation doesn’t just squeeze out competition. It narrows the kinds of relationships possible between operators of different services. As Daigle put it: “The more proprietary solutions are built and deployed instead of collaborative open standards-based ones, the less the internet survives as a platform for future innovation.” Consolidation kills collaboration between service providers through the stack by rearranging an array of different relationships — competitive, collaborative — into a single predatory one. Since then, standards development organizations (SDOs) started several initiatives to name and tackle infrastructure consolidation, but these floundered. Bogged down in technical minutiae, unable to separate themselves from their employers’ interests and deeply held professional values of simplification and control, most internet engineers simply couldn’t see the forest for the trees. Up close, internet concentration seems too intricate to untangle; from far away, it seems too difficult to deal with. But what if we thought of the internet not as a doomsday “hyperobject,” but as a damaged and struggling ecosystem facing destruction? What if we looked at it not with helpless horror at the eldritch encroachment of its current controllers, but with compassion, constructiveness and hope? Technologists are great at incremental fixes, but to regenerate entire habitats we need to learn from ecologists who take a whole-systems view. Ecologists know something just as important, too; how to keep going when others first ignore you and then say it’s too late, how to mobilize and work collectively, and how to build pockets of diversity and resilience that will outlast them, creating possibilities for an abundant future they can imagine but never control. We don’t need to repair the internet’s infrastructure. We need to rewild it. What Is Rewilding? Rewilding “aims to restore healthy ecosystems by creating wild, biodiverse spaces,” according to the International Union for Conservation of Nature. More ambitious and risk-tolerant than traditional conservation, it targets entire ecosystems to make space for complex food webs and the emergence of unexpected interspecies relations. It’s less interested in saving specific endangered species. Individual species are just ecosystem components, and focusing on components loses sight of the whole. Ecosystems flourish through multiple points of contact between their many elements, just like computer networks. And like in computer networks, ecosystem interactions are multifaceted and generative. Rewilding has much to offer people who care about the internet. As Paul Jepson and Cain Blythe wrote in their book “Rewilding: The Radical New Science of Ecological Recovery,” rewilding pays attention “to the emergent properties of interactions between ‘things’ in ecosystems … a move from linear to systems thinking.” It’s a fundamentally cheerful and workmanlike approach to what can seem insoluble problems. It doesn’t micromanage. It creates room for “ecological processes [which] foster complex and self-organizing ecosystems.” Rewilding puts into practice what every good manager knows: hire the best people you can, provide what they need to thrive, then get out of the way. It’s the opposite of command-and-control. “Rewilding the internet is more than a metaphor. It’s a framework and plan.” Facebook Twitter Email Rewilding the internet is more than a metaphor. It’s a framework and plan. It gives us fresh eyes for the wicked problem of extraction and control, and new means and allies to fix it. It recognizes that ending internet monopolies isn’t just an intellectual problem. It’s an emotional one. It answers questions like how do we keep going when the monopolies have more money and power? How do we act collectively when they suborn our community spaces, funding and networks? And how do we communicate to our allies what fixing it will look and feel like? Rewilding is a positive vision for the networks we want to live inside, and a shared story for how we get there. It grafts a new tree onto technology’s tired old stock. And embodied in rewilding’s ecological tools is the collective wisdom of an entire discipline already tackling humanity’s toughest, systemic problems. What Ecology Knows Ecology knows plenty about complex systems that technologists can benefit from learning. First, it knows that shifting baselines are real. If you were born around the 1970s, you probably remember many more dead insects on the windscreen of your parents’ car than on your own; global land-dwelling insect populations are dropping about 9% a decade. If you’re a geek, you probably programmed your own computer to make basic games. You certainly remember a web with more to read than the same five websites. You may have even written your own blog. But many people born after 2000 probably think a world with few insects, little ambient noise from birdcalls, where you regularly use only a few social media and messaging apps (rather than a whole web) is normal. As Jepson and Blythe wrote, shifting baselines are “where each generation assumes the nature they experienced in their youth to be normal and unwittingly accepts the declines and damage of the generations before.” Damage is already baked in. It even seems natural. Ecology knows that shifting baselines dampen collective urgency and deepen generational divides. People who care about internet monoculture and control are often told they’re nostalgists harkening back to a pioneer era. But it’s fiendishly hard to regenerate an open and competitive infrastructure for younger generations who’ve been raised to assume that two or three platforms, two app stores, two operating systems, two browsers, one cloud/mega-store and a single search engine for the world comprise the internet. If the internet for you is the massive sky-scraping silo you happen to live inside and the only thing you can see outside is the single, other massive sky-scraping silo, then how can you imagine anything else? The answer isn’t to make everyone learn about how the original protocols were designed to separate key functions and the power that goes with them (though that’s certainly good to know). It’s to change how we feel and react to living inside a complex system that needs our care. Tech toxicity stems from there being only one business model for how to internet: concentration, surveillance, control. Further centralizing and managing this broken system will only make it worse. Rewilding the internet is not a nostalgia project for middle-aged nerds who miss IRC and Usenet. For many people across the generations today, platforms like Facebook or TikTok are the internet. They’ve long dwelled in walled gardens they think are the world. Concentrated digital power produces the same symptoms that command and control produces in biological ecosystems; acute distress punctuated by sudden collapses once tipping points are reached. Rewilding is a way to collectively see the counterintuitive truth; today’s internet isn’t too wild, even if it feels like that. It’s simply not wild enough. It’s important to share that ecological rewilding is a work in progress. What do you rewild to? Humans have shaped and cultivated landscapes for tens of thousands of years, so what does “wild” even mean? Just as there’s no ecosystem on Earth untouched by human actions, there’s no “true” wildness to return habitats to. And what scale is needed for rewilding to succeed? It’s one thing to reintroduce wolves to the 3,472 square miles of Yellowstone, quite another to cordon off about 20 square miles of a reclaimed polder near Amsterdam. Large and diverse Yellowstone is likely complex enough to adapt to change, but the small Dutch reserve known as Oostvaardersplassen has struggled. “For tech giants, the long period of open internet evolution is over. Their internet is not an ecosystem. It’s a zoo.” Facebook Twitter Email In the 1980s, the Dutch government attempted to regenerate a section of the overgrown Oostvaardersplassen. An independent-minded government ecologist, Frans Vera, said reeds and scrub would dominate unless now-extinct herbivores grazed them. In place of ancient aurochs, the state forest management agency introduced the famously bad-tempered German Heck cattle, and in place of an extinct steppe pony, a Polish semi-feral breed. Some 30 years on, with no natural predators, and after plans for a wildlife corridor to another reserve came to nothing, there were many more animals than the limited winter vegetation could sustain. People were horrified by starving cows and ponies, and beginning in 2018, government agencies instituted animal welfare checks and culling or removals. Just turning the clock back was insufficient. The segment of Oostvaardersplassen was too small and too disconnected to be rewilded. Its effectively landlocked status made over-grazing and collapse inevitable, an embarrassing but necessary lesson. Rewilding is a work in progress. It’s not about trying to revert ecosystems to a mythical Eden. Instead, rewilders seek to rebuild resilience by restoring autonomous natural processes and letting them operate at scale to generate complexity. But rewilding, itself a human intervention, can take several turns to get right. Whatever we do, the internet isn’t returning to old-school then-common interfaces like FTP and Gopher, or each organization running its own mail server, rather than operating off G-Suite. But shifting baselines mean that everyone using it today needs a shared way to articulate what’s happening, and a collective sense of purpose and possibility. Some of what we need is already here, especially on the web. Look at the resurgence of RSS feeds, email newsletters and blogs as we discover (yet again) that relying on one app to host global conversations creates a single point of failure and control. New systems are growing, like the Fediverse with its federated islands, or Bluesky with algorithmic choice and composable moderation. We don’t know what the future holds. Our job is to keep open as much opportunity as we can, trusting that those who come later will use it. Rewilding gets this. Instead of setting purity tests for which kind of internet is most like the original, we can test changes against the values of the original design. Do new standards protect the network’s “generality,” i.e. its ability to support multiple uses, or is functionality limited to optimize efficiency for the biggest tech firms? The internet is the technological expression of hard-won human wisdom; general-purpose systems are the most resilient, and it’s risky to concentrate information and control. Ecologists also know that complexity is not the enemy, it’s the goal. The story of scientific forestry shows the conflict isn’t just between duopoly platforms and everyone else; it’s between a brittle, command-and-control mentality, and the ability to have faith in complex systems that may produce things you didn’t foresee for people you don’t control. As early as 1985, plant ecologists Steward T.A. Pickett and Peter S. White wrote in “The Ecology of Natural Disturbance and Patch Dynamics,” that “[an] essential paradox of wilderness conservation is that we seek to preserve what must change.” Some internet engineers know this. David Clark, a Massachusetts Institute of Technology professor who worked on some of the internet’s earliest protocols, wrote an entire book about other network architectures that might have been built, if different values, like security or centralized management, had been prioritized by the internet’s creators. But our internet took off because it was designed as a general-purpose network, built to connect anyone. Our internet was built to be complex and unbiddable, to do things we cannot yet imagine. When we interviewed Clark for rewilding project, he told us that “‘complex’ implies a system in which you have emergent behavior, a system in which you can’t model the outcomes. Your intuitions may be wrong. But a system that’s too simple means lost opportunities.” Or, as Daigle wrote in that 2019 policy brief, “simplicity is not always the best outcome.” Everything worthwhile we collectively make is complex or, honestly, messy. The cracks are where new people and ideas get in. Internet infrastructure is a degraded ecosystem, but it’s also a built environment, like a city. Its complexity and unpredictability make it generative, worthwhile and deeply human. In 1961, Jane Jacobs, an American-Canadian activist and author of “The Death and Life of Great American Cities,” argued that mixed-use neighborhoods were safer, happier, more prosperous, and more livable than the sterile, highly controlling designs of urban planners like New York’s Robert Moses. “Crashes, fires and floods may simply be entropy in action, but systemically concentrated and risky infrastructures are choices made manifest — and we can make better ones.” Facebook Twitter Email Both Jacobs and the anthropologist Scott showed that top-down planning is often disastrous because instead of setting the stage for generative interactions it tries to control what people will do. Just like the crime-ridden, Corbusier-like towers Moses crammed people into when he demolished mixed-use neighborhoods and built highways through them, today’s top-down, concentrated internet is, for many, an unpleasant and harmful place. Its owners are hard to remove, and their interests do not align with ours. As Jacobs wrote: “As in all Utopias, the right to have plans of any significance belonged only to the planners in charge.” As a top-down, built environment, the internet has become something that is done to us, not something we collectively remake every day. In an ecosystem, everything is infrastructure for everything else. Ecosystems endure because species serve as checks and balances on each other. They have different modes of interaction, not just extraction, but mutualism, commensalism, competition and predation. In flourishing ecosystems, predators are subject to limits. They’re just one part of a complex web that passes calories around, not a one-way ticket to the end of evolution. Ecologists know that diversity is resilience. On July 18, 2001, 11 carriages of a 60-car freight train derailed in the Howard Street Tunnel under Mid-Town Belvedere, a neighborhood just north of downtown Baltimore. Within minutes one carriage containing a highly flammable chemical was punctured. The escaping chemical ignited and soon adjacent carriages were alight in a fire that took about five days to put out. The disaster multiplied and spread. Thick, brick tunnel walls acted like an oven, and temperatures rose to nearly 2,000 degrees Fahrenheit. A more than three-foot-wide water main above the tunnels burst, flooding the tunnel with millions of gallons within hours. It only cooled a little. Three weeks later, an explosion linked to the combustible chemical blew out manhole covers located as far as two miles away. WorldCom, then the second largest long-distance phone company in the U.S., had fiber-optic cables in the tunnel carrying high volumes of phone and internet traffic. However, according to Clark, WorldCom’s resilience planning meant traffic was spread over different fiber networks in anticipation of just this kind of event. On paper, WorldCom had network redundancy. But almost immediately, U.S. internet traffic slowed, and WorldCom’s East Coast and transatlantic phone lines went down. The region’s narrow physical topography had concentrated all those different fiber networks into a single chokepoint, the Howard Street Tunnel. WorldCom’s resilience was, quite literally, incinerated. It had technological redundancy, but not diversity. Sometimes we don’t notice concentration until it’s too late. Read Noema in print. Clark tells the story of the Howard Street Tunnel fire to show that bottlenecks aren’t always obvious, especially at the operational level, and huge systems that seem secure due to their size and resources, can unexpectedly crumble. In today’s internet, much traffic passes through tech firms’ private networks, for example, Google and Meta’s own undersea cables. Much internet traffic is served from a few dominant content distribution networks, like Cloudflare and Akamai, who run their own networks of proxy servers and data centers. Similarly, that traffic goes through an increasingly small number of domain name system (DNS) resolvers, which work like phone books for the internet, linking website names to their numeric address. All of this improves network speed and efficiency but creates new and non-obvious bottlenecks like the Howard Street Tunnel. Centralized service providers say they’re better resourced and skilled at attacks and failures, but they are also large, attractive targets for attackers and possible single points of system failure. On Oct. 21, 2016, dozens of major U.S. websites suddenly stopped working. Domain names belonging to Airbnb, Amazon, PayPal, CNN, The New York Times simply didn’t resolve. All were clients of the commercial DNS service provider, Dyn, which had been hit by a cyberattack. Hackers infected tens of thousands of internet-enabled devices with malicious software, creating a network of hijacked devices, or a botnet, that they used to bombard Dyn with queries until it collapsed. America’s biggest internet brands were brought down, essentially, by a network of baby monitors. Although they all likely had resilience planning and redundancies, they went down because a single chokepoint — in one crucial layer of infrastructure — failed. Consolidation drives fragility. It imposes narrowness. Arranging deliberately varied networks and services into centralized stovepipes means we repeatedly fail to think of the internet as a complex system. Just like the train crash that caused a chemical leak that resulted in burst water pipes, melted fiber-optic cables and blown up manholes, single points of failure in complex critical systems have wide, unpredictable and devastating effects. “Just imagine what the future possibilities for internet innovation could be if “capitalism without competition” was rooted out all the way up and down the stack.” Facebook Twitter Email Widespread outages due to centralized chokepoints have become so common that investors even use them to identify opportunities. When a failure by cloud provider Fastly took high-profile websites offline in 2021, its share price surged. Investors were delighted by headlines that informed them of an obscure technical service provider with an apparent lock on an essential service. To investors, this critical infrastructure failure doesn’t look like fragility but like a chance to profit. The result of infrastructural narrowness is baked-in fragility that we only notice after a breakdown. But monoculture is also highly visible in our search and browser tools. Search, browsing and social media are how we find and share knowledge, and how we communicate. They’re a critical, global epistemic and democratic infrastructure, controlled by just a few U.S. companies. Crashes, fires and floods may simply be entropy in action, but systemically concentrated and risky infrastructures are choices made manifest — and we can make better ones. The Look & Feel Of A Rewilded Internet A rewilded internet will have many more service choices. Some services like search and social media will be broken up, as AT&T eventually was. Instead of tech firms extracting and selling people’s personal data, different payment models will fund the infrastructure we need. Right now, there is little explicit provision for public goods like protocols and browsers, essential to making the internet work. The biggest tech firms subsidize and profoundly influence them. Part of rewilding means taking what’s been pulled inside the big tech stack out of it, and paying for the true costs of connectivity. Some things like basic connectivity we will continue to pay for directly, and others like browsers we will support indirectly but transparently, as described below. The rewilded internet will have an abundance of ways to connect and relate to each other. There won’t be just one or two numbers to call if leaders of a political coup decide to shut the internet down in the middle of the night, as has happened in places like Egypt and Myanmar. No one entity will permanently be on top. A rewilded internet will be a more interesting, usable, stable and enjoyable place to be. Through extensive research, Nobel-winning economist Elinor Ostrom found that “when individuals are well informed about the problem they face and about who else is involved, and can build settings where trust and reciprocity can emerge, grow, and be sustained over time, costly and positive actions are frequently taken without waiting for an external authority to impose rules, monitor compliance, and assess penalties.” Ostrom found people spontaneously organizing to manage natural resources — from water company cooperation in California to Maine lobster fishermen monitoring each other and repelling outsiders to prevent over-fishing. Self-organization also exists as part of a key internet function: traffic coordination. Internet exchange points (IXPs) are an example of common-pool resource management, where internet service providers (ISPs) collectively agree to carry each other’s data for low or no cost. Network operators of all kinds — telecoms companies, large tech firms, universities, governments and broadcasters — all need to send large amounts of data through other ISPs’ networks so that it gets to its destination. If they managed this separately through individual contracts, they’d spend much more time and money. Instead, they often form IXPs, typically as independent, not-for-profit associations. As well as managing traffic, IXPs have, in many — and especially developing — countries, formed the backbone of a flourishing technical community that further drives economic development. Both between people and on the internet, connections are generative. From technical standards to common-pool resource management and even to more localized broadband networks known as “altnets,” internet rewilding already has a deep toolbox of collective action ready to be deployed. The Road To Rewilding The list of infrastructures to be diversified is long; as well as pipes and protocols, there are operating systems, browsers, search engines, DNS, social media, advertising, cloud providers, app stores, AI companies and more. Not only are the technologies involved complex, but they’re also intertwined. But freedoms are additive. Showing what can be done in one area creates opportunities in others. First, let’s start with regulation. The New Drive For Antitrust & Competition You don’t always need a big new idea like rewilding to frame and motivate major structural change. Sometimes reviving an old idea will do. President Biden’s 2021 “Executive Order on Promoting Competition in the American Economy” revived the original, pro-worker, trust-busting scope and urgency of the early 20th-century legal activist and Supreme Court Justice Louis D. Brandeis, along with rules and framings that date back to before the 1930s New Deal. “Rewilding an already built environment isn’t just sitting back and seeing what tender, living thing can force its way through the concrete. It’s razing to the ground the structures that block out light for everyone not rich enough to live on the top floor.” Facebook Twitter Email U.S. antitrust law was created to break the power of oligarchs in oil, steel and railroads who threatened America’s young democracy. It gave workers basic protections and saw equal economic opportunity as essential to freedom. This expansive and interdependent view of competition and antitrust as essential for fairness and democracy was whittled away by Chicago School economists’ policies in the 1970s and Regan-era judges’ court rulings over the decades and sidelined by the narrow economic doctrine that intervention is only permitted when monopoly power causes consumer prices to rise. The intellectual monoculture of the consumer harm threshold has since spread globally. It’s why governments just stood aside as 21st-century tech firms romped to oligopoly. If a regulator’s sole criterion for action is to make sure consumers don’t pay a penny more, then the free or data-subsidized services of tech platforms don’t even register. (Of course, consumers pay in other ways, as these tech giants exploit their personal information for profit.) This laissez-faire approach allowed the biggest firms to choke off competition by acquiring their competitors and vertically integrating service providers, creating the problems we have today. Regulators and enforcers in Washington and Brussels now say they have learned that lesson and won’t allow AI dominance to happen as internet concentration did. Federal Trade Commission Chair Lina Khan and U.S. Department of Justice antitrust enforcer, Jonathan Kanter, are identifying chokepoints in the AI “stack” — concentration in control of processing chips, datasets, computing capacity, algorithm innovation, distribution platforms and user interfaces — and analyzing each potential bottleneck to see if it affects systemic competition. This is potentially good news for people who want to prevent the current dominance of tech giants being grandfathered into our AI future. In his 2021 signing of the executive order on competition, President Biden said, “capitalism without competition isn’t capitalism; it’s exploitation.” Biden’s enforcers are changing the kinds of cases they take up and widening the applicable legal theories on harm that they bring to judges. Instead of the traditionally narrow focus on consumer prices, today’s cases argue that the economic harms perpetrated by dominant firms include those suffered by their workers, small companies and the market as a whole. Khan and Kanter have jettisoned narrow and abstruse models of market behavior for real-world experiences of healthcare workers, farmers, writers. They get that shutting off economic opportunity fuels far-right extremism. They’ve made antitrust enforcement and competition policy explicitly about coercion versus choice, power versus democracy. Kanter told a recent conference in Brussels that “excessive concentration of power is a threat … it’s not just about prices or output but it’s about freedom, liberty and opportunity.” Enforcers in Washington and Brussels are starting to preemptively block tech firms from using dominance in one realm to take over another. After scrutiny by the U.S. FTC and European Commission, Amazon recently abandoned its plan to acquire the home appliance manufacturer, iRobot. Regulators on both sides of the Atlantic have also moved to stop Apple from using its iPhone platform dominance to squeeze app store competition and dominate future markets through, for example, pushing the usage of CarPlay on automakers and limiting access to its tap-to-pay digital wallet in the financial services sector. Still, so far, their enforcement actions have focused on the consumer-facing, highly visible parts of the tech giants’ exploitative and proprietary internet. The few, narrow measures of the 2021 executive order that aim to reduce infrastructure-based monopolies, only prevent future abuses like radio spectrum-hogging, not those already locked in. Sure, the best way to deal with monopolies is to stop them from happening in the first place. But unless regulators and enforcers eradicate the existing dominance of these giants now, we’ll be living in today’s infrastructure monopoly for decades, perhaps even a century. Just imagine applying Khan and Kanter’s chokepoint identification and whole-system harms investigation to the internet’s invisible layers — and doing so today. Just imagine what the future possibilities for internet innovation could be if “capitalism without competition” was rooted out all the way up and down the stack. The recent return to more muscular competition enforcement still isn’t radical enough. So far, even activist regulators have shied away from applying the toughest remedies for concentration in long-consolidated markets, such as non-discrimination requirements, functional interoperability and structural separations, i.e. breaking companies up. And talk of declaring the so-called “natural monopolies” in search and social media to be public utilities — and forcing them to act as common carriers open to all — is still too extreme for most. But taken together, these are some of the most powerful tools we have to rewild the internet’s silent and existing vertical integrations; its deliberately moated fiefdoms squatting on bottlenecks of critical national and global infrastructure. Rewilding an already built environment isn’t just sitting back and seeing what tender, living thing can force its way through the concrete. It’s razing to the ground the structures that block out light for everyone not rich enough to live on the top floor. “Regulators’ efforts to make the visible internet competitive will achieve little unless they also tackle the devastation that lies beneath.” Facebook Twitter Email When the writer and activist Cory Doctorow wrote about how to free ourselves from the clutches of Big Tech, he said that though breaking up big companies will likely take decades, providing strong and mandatory interoperability would open up innovative space and slow the flow of money to the largest firms — money they would otherwise use to deepen their moats. Doctorow describes “comcom,” or competitive compatibility, as a kind of “guerrilla interoperability, achieved through reverse engineering, bots, scraping and other permissionless tactics.” Before a thicket of invasive laws sprung up to strangle it, comcom was just how people figured out how to fix cars and tractors or re-write software. Comcom drives the try-every-tactic-until-one-works behavior you see in a flourishing ecosystem. In an ecosystem, diversity of species is another way of saying “diversity of tactics,” as each successful new tactic creates a new niche to occupy. Whether it’s an octopus camouflaging itself as a sea snake, a cuckoo smuggling her chicks into another bird’s nest, orchids producing flowers that look just like a female bee, or parasites influencing rodent hosts to take life-ending risks, each evolutionary micro-niche is created by a successful tactic. Comcom is simply tactical diversity; it’s how organisms interact in complex, dynamic systems. And humans have demonstrated the epitome of short-term thinking by enabling the oligarchs who are trying to end it. Efforts are underway. The EU already has several years of experience with interoperability mandates and precious insight into how determined firms work to circumvent such laws. The U.S., however, is still in its early days of ensuring software interoperability, for example, for videoconferencing.” Perhaps one way to motivate and encourage regulators and enforcers everywhere is to explain that the subterranean architecture of the internet has become a shadowland where evolution has all but stopped. Regulators’ efforts to make the visible internet competitive will achieve little unless they also tackle the devastation that lies beneath. Next Steps Much of what we need is already here. Beyond regulators digging deep for courage, vision and bold new litigation strategies, we need vigorous, pro-competitive government policies around procurement, investments and physical infrastructure, that require interoperability at every level and the resources to make it happen. Universities must reject research funding from tech firms because it always comes with conditions, both spoken and unspoken. Instead, we need more public-funded tech research with publicly released findings aimed at serving the collective good. Such research should investigate power concentration in the internet ecosystem and practical alternatives to it. We need to recognize that much of the internet’s infrastructure is a de facto utility, that any monopolies are critical public resources that we must regain control of. We must ensure regulatory and financial incentives and support for alternatives including common-pool resource management, community networks, and the myriad other collaborative mechanisms people have always used to provide essential public goods like clean water, roads and defense. All this takes money. Governments are starved of tax revenue by the once-in-history windfalls seized by today’s tech giants, so it’s clear where the money is. We need to get it back. We need to stop talking about ethics and hoping the next generation will do a better job and start making demands of power. We know all this, but still find it so hard to collectively act. Why? Herded into rigid tech plantations rather than functioning, diverse ecosystems, it’s tough to imagine alternatives. Even those who can see clearly may feel helpless and alone. Rewilding unites everything we know we need to do and brings with it a whole new toolbox and vision. Ecologists face the same systems of exploitation and are organizing urgently, at scale and across domains. They see clearly that the issues aren’t isolated but are instances of the same pathology of command and control, extraction and domination that political anthropologist Scott first noticed in scientific forestry. The solutions are the same in ecology and technology; aggressively use the rule of law to level out unequal capital and power, then rush in to fill the gaps with better ways of doing things. Keep The Internet, The Internet Susan Leigh Star, a sociologist and theorist of infrastructure and networks, wrote in her 1999 influential paper, “The Ethnography of Infrastructure”: “Study a city and neglect its sewers and power supplies (as many have), and you miss essential aspects of distributional justice and planning power. Study an information system and neglect its standards, wires, and settings, and you miss equally essential aspects of aesthetics, justice, and change.” “If fundamental internet protocols don’t maximize the values of interoperability, generality and openness, then they’re simply not the internet.” Facebook Twitter Email The technical protocols and standards that underlie the internet’s infrastructure are ostensibly developed in open, collaborative SDOs, but are also increasingly under the control of a few companies; so what appear to be “voluntary” standards are often the business choices of the biggest firms. The dominance of SDOs by big firms also shapes what does not get standardized — for example, search, which is effectively a global monopoly. While efforts to directly address internet consolidation have been raised repeatedly within SDOs, little progress has been made. This is damaging SDOs’ credibility, especially outside the U.S. SDOs must radically change or they will lose their implicit global mandate to steward the future of the internet. We need internet standards to be global, open and generative. They’re the wire models that give the internet its planetary form, the gossamer-thin but steely-strong threads holding together its interoperability against fragmentation and permanent dominance. We need them to work. As internet engineer Jari Arkko put it in 2020, for the health of the network and all our security, SDOs must “ensure that key aspects of the evolving internet stay open, e.g. through open, standardized interfaces and that open source continues to be an important building block.” If fundamental internet protocols don’t maximize the values of interoperability, generality and openness, then they’re simply not the internet. Make Laws & Standards Work Together In 2018, a small group of Californians maneuvered the California Legislature into passing the California Consumer Privacy Act (CCPA). Nested in the statute was an unassuming provision, the “right to opt out of sale or sharing” your personal information via a “user-enabled global privacy control” or GPC signal that would create an automated method for doing so. The law didn’t define how GPC might work. As a technical standard was required for browsers, businesses and providers to speak the same language, the signal’s details were delegated to a group of experts. In July 2021, California’s Attorney General mandated that all businesses use the newly created GPC for California-based consumers visiting their websites. The group of experts is now shepherding the technical specification through global web standards development at the World Wide Web Consortium (W3C). For California residents, GPC automates the request to “accept” or “reject” sales of your data, such as cookie-based tracking, on its websites; however, it isn’t yet supported by major default browsers like Chrome and Safari. Broad adoption will take time, but it’s a small step in changing real-world outcomes by driving antimonopoly practices deep into the standards stack — and it’s already being adopted elsewhere. GPC is not the first legally mandated open standard, but it was deliberately designed from day one to bridge policymaking and standards-setting. The standard provides a mechanism and the law makes it mandatory. It’s a powerful dynamic where all play to their strengths. The idea is gaining ground. A recent United Nations Human Rights Council report recommends that states delegate “regulatory functions to standard-setting organizations.” Our technical standards can be crossbred with institutions to produce protocols for governance that let people shape their online world. Make Service-Providers — Not Users — Transparent Today’s internet offers minimal transparency of key internet infrastructure providers. For example, browsers are highly complex pieces of infrastructure that determine how billions of people use the web, yet they are provided for free. That’s because the most commonly used search engines enter into opaque financial deals with browsers, paying them to be set as the default. Since few people change their default search engine, browsers like Safari and Firefox make money by defaulting the search bar to Google, locking in its dominance even as the search engine’s quality of output declines. This creates a quandary. If antitrust enforcers were to impose competition, browsers would lose their main source of income. Infrastructure requires money, but the planetary nature of the internet challenges our public funding model, leaving the door open to private capture. However, if we see the current opaque system as what it is, a kind of non-state taxation, then we can craft an alternative. Search engines are a logical place for governments to mandate the collection of a levy that supports browsers and other key internet infrastructure, which could be financed transparently, under open, transnational, multistakeholder oversight. Imagining new institutional methods to solve old problems at a global scale is one way to rewild the web. “Our technical standards can be crossbred with institutions to produce protocols for governance that let people shape their online world.” Facebook Twitter Email Make Space To Grow We need to stop thinking of internet infrastructure as too hard to fix. It’s the underlying system we use for nearly everything we do. The former prime minister of Sweden, Carl Bildt, and former Canadian deputy foreign minister, Gordon Smith wrote in 2016 that the internet was becoming “the infrastructure of all infrastructure.” It’s how we organize, connect and build knowledge, even — perhaps — planetary intelligence. Right now, it’s concentrated, fragile and utterly toxic. Ecologists have re-oriented their field as a “crisis discipline,” a field of study that’s not just about learning things but about saving them. We technologists need to do the same. Rewilding the internet connects and grows what people are doing across regulation, standards-setting and new ways of organizing and building infrastructure, to tell a shared story of where we want to go. It’s a shared vision with many strategies. The instruments we need to shift away from extractive technological monocultures are at hand or ready to be built. Just as a diverse “pocket forest” is the surest way to regenerate urban vegetation, a global network with multiple different ways “to internet” is the best insurance policy for future innovation and resilience. We need to rewild the internet for the future, for our freedom to build tools and spaces, and to share knowledge, ideas and stories that haven’t been anticipated by the internet’s current overlords and cannot be contained. Enjoy the read? Subscribe to get the best of Noema. More From Noema Magazine Feature Climate Crisis The Rise Of The Bee Bandits Oliver Milman Essay Climate Crisis Parenthood Amid California's Devastating Climate Crisis Stephanie Kotin Essay Climate Crisis Thirsty Bots Are Drinking Our Scarce Water Nathan Gardels",
    "commentLink": "https://news.ycombinator.com/item?id=40055120",
    "commentBody": "We need to rewild the internet (noemamag.com)329 points by robin_reala 16 hours agohidepastfavorite196 comments urda 14 hours agoMore people should be tending their \"digital gardens\" just to have something to curate and share. Bring back the weird, the odd, the deep-dives into topics you never imagined having such detail, bring back the strange animated GIFS, the websites always \"under construction\", give me your weird, give me your odd, this should be everyone's place to be free. reply blazingbanana 12 hours agoparentIf you haven't checked out the \"Surpise me\" button on https://wiby.me I strongly suggest you do. These digital gardens are still out there even if sadly stuck in time. reply diggum 11 hours agorootparentHow odd. My 3rd click, it brought me to news.ycombinator.com - exactly what brought me there in the first place and open in another tab. I was very confused as I assumed the tab had closed but no, it just appeared to be luck of the surprise. reply sirspacey 5 hours agorootparentprevWow!! Thank you so much. I had no idea this existed and it’s so fun. reply lancesells 12 hours agorootparentprevI've used that button so many times to find things. I wish there was another directory that had the same unique feel but a bit more modernized. Wiby seems to be only one era or feel. reply jmholla 12 hours agorootparentI like Marginalia: https://search.marginalia.nu/explore/random reply marginalia_nu 11 hours agorootparentI got some quality issues right now. Kinda works but not as well as I'd want. reply Filligree 11 hours agorootparentIt's gotten famous enough that people have started optimizing for it? reply marginalia_nu 11 hours agorootparentNah, just been working on low level stuff and I haven't had the time to do a lot of the manual quality tuning that's always been necessary once in a blue moon. Downside of being a one man show is I only have so much attention to spend on tasks, so sometimes quality tuning gets a bit neglected. reply makeworld 9 hours agorootparentprevCheck out https://ooh.directory/ reply ya1sec 11 hours agorootparentprevGonna plug my project: https://moonjump.app/ I have a keyboard shortcut that opens moonjump.app/jump, which will redirect to a random site reply thaumasiotes 5 hours agorootparentprevThe guy behind StumbleUpon still seems to be doing things in that area. I'm not sure what went wrong with StumbleUpon; it worked well from the user end of things. reply ricktdotorg 9 hours agorootparentprevhah, this is great! reminds me of webrings in the 90s reply 082349872349872 14 hours agoparentprevI feel like letting my geek flag fly / Yes, I feel like I owe it to someone __________ ____\\ /__ _| /| |\\ \\_____\\ |________\\|reply aidenn0 13 hours agorootparentAlmost uninstalled Linux / It happened just the other day reply dhosek 12 hours agorootparentPoints for the CSNY deep cut reference. reply aidenn0 10 hours agorootparentJust picked up on what GP did first. I'm one of those weird people who listen to albums more than singles, so I don't always know what's a deep-cut. Deja Vu is in regular rotation for me. Country Girl is the only track I'm tempted to skip on that whole album. reply fsckboy 12 hours agorootparentprevfor CSNY \"Almost Cut my Hair\", 1970? may I please have points, too sir? Jimi Hendrix \"If 6 was 9\", 1967: \"If all the hippies cut off all their hair I don't care, I don't care... White collar Windows users, flashin down the street pointing their plastic finger at me, hoping soon my kind will drop and die: I'm gonna wave my freak flag high!\" reply notaustinpowers 14 hours agoparentprevI was just thinking today that I should just start posting my stuff on a personal site. Photos, blogs, life updates, etc. It gives me that control that I want, and I don't have to worry about Meta, etc trying to monetize it. Plus I get to unplug from those \"n liked your post\" dopamine hits. reply digging 13 hours agorootparentI don't really share anything I make/do on the internet but I've found posting my photography on a personal site feels pretty good even if I don't expect anyone to ever see it there. Recently I made a /now page and even started blogging infrequently - again, there are likely 0 viewers, but there's some pride in knowing I've staked out a corner of the internet for myself. reply ozim 11 hours agorootparentI think everyone is done with influencers. People would like to find genuine ones instead of just cash grabs. reply sowbug 8 hours agorootparentprevMy daughter asked me to set her up with a blog on her own server. We did it, and there is now a Hello World post somewhere out there on the web. But it wasn't a good experience, compared to what I know posting on Instagram is like. And that's just on the publishing side. The chances of her site being even indexed, let alone visited, is tiny. It's surprising* that rolling your own on the web has gotten harder, not easier, in the last 20 years. But it has. *Surprising in the narrow sense that technology usually improves. I know all too well why lone sites don't do well on the web. reply yaky 6 hours agorootparentprevI would encourage it. A few years ago, I started a site/blog on both Web and Gemini. I mostly keep track of projects I build, books I read, some interesting events, and some cheatsheets and links I revisit a lot. Writing things down definitely keeps me more motivated, and it is nice to look back in time and reflect occasionally. And I made the site simple and contrary to current Web on purpose: plain HTML, no JS, no popups, only graphics are photos and diagrams. reply criddell 13 hours agorootparentprevI've thought about building personal site, but the opportunity cost is just too high. The time I spend making a site has to come from somewhere and there just isn't anything I do that I want to take time from. reply jstanley 12 hours agorootparentDon't take it from the time you spend doing something you care about. Take it from the time you spend doing nothing. reply criddell 12 hours agorootparentThe time I have set aside for me to do nothing is my most valuable time! I'd rather take it from just about anything else reply II2II 11 hours agorootparentA personal site should be personal, in the sense that the things you share are things you value and the only value derived from it is from sharing what you value with other. That's why personal sites are the ones that are easy to tell apart. Personal should be personal, otherwise it is simply trying to establish a professional profile.) Of course, if you have no interest in sharing your passions, don't do it. Your free time is valuable and you should only use it for things that you value. reply analog31 5 hours agorootparentprevThat's perfectly fair. In my own case, blogging has changed how I conduct my hobbies, for the better. They say you don't understand something until you have to explain it to someone else. It has made my projects more robust, and often simpler, and the process has helped me uncover mistakes. I'm not sure that it has actually come with a cost. The mechanics are trivial thanks to GitHub and of course Git. reply sojournerc 11 hours agorootparentprevOne hour with Hugo (and netlify) and you have a site, come on... reply makeitshine 9 hours agorootparentYou still have to spend time adding content. I enjoy doing that, but not everyone has the time. reply cyanydeez 12 hours agorootparentprevYes, but does it give you the validation you deserve as a member of society? reply Vrondi 11 hours agorootparentNobody is owed validation. We want it, but we are not owed it. Learning to cope with that desire is one of the chief signs of maturity and wisdom. reply notaustinpowers 8 hours agorootparentprevThis is purely my personal philosophy but I view that, ultimately, my validation is when I'm happy with my actions, position in life, and personal accomplishments. I love sharing those with friends and family. But I hate the opaque algorithms on social media sites that are tuned to prioritize engagement & time spent on the service. I find that these algos are rewarding rage bait due to their engagement farming. Sharing with those that I want to share with, in an environment free of algorithms and capitalistic intentions provides that validation. And having a random site visitor here and there who enjoys what I have to say is just icing on the cake. As I get older, I realize that we were never meant to have 10,000 people looking at what we ate for lunch, or the mess our cat made while we were at work. Interactions like that back in the 1500s would have made us more influential than Kings. A close-knit community that cherishes the expression of all our oddities, curiosities, and hobbies is something that just can't be replicated in current social media. reply m-i-l 12 hours agoparentprevAs much as I like \"digital gardens\" and think that they are a step in the right direction, rewilding requires much more than cultivating lots of isolated patches of ground - the article even mentions a nature reserve which was \"too small and too disconnected to be rewilded. Its effectively landlocked status made over-grazing and collapse inevitable\". reply winternett 4 hours agoparentprevI have been doing so for a long time, but man, hosting costs are rising, and they really don't give me much storage to host community functionality at all. :| http://www.ruffandtuffrecordings.com/ reply nicbou 3 hours agorootparentConsider caching to reduce hosting costs. If you are patient, consider static site generators. The latter made running a website so much simpler. There is virtually no maintenance left to do. reply jallmann 13 hours agoparentprevMy Twitter feed has a bunch of people doing extremely niche deep dives into Cold War weapons systems. For example, on the design of MiG-23 air intakes: https://twitter.com/BaA43A3aHY/status/1753715489686057384 reply jozzas 11 hours agorootparentThis is literally the point of the article. That's all posted inside the twitter walled garden. Great content, wrong platform. reply idle_zealot 4 hours agorootparentRSS handles syndication, but how would their deep-dives be discoverable if they had been posted to personal blog sites? That's a missing piece of the puzzle for the distributed web; curation and recommendation are ad-hoc and don't scale. reply RGamma 1 hour agorootparentThey'd be discoverable by search or linking. The distributed web is compatible with feeds. It's what's happening on this site too. Except the content would be packaged and redistributable and wouldn't disappear when the original service goes down or such. reply autokad 13 hours agoparentprevI remember when they were on the agenda path to eliminate gifs. You'd see posts on hacker news written by Giffy, a company seeking to make its existence justified by getting rid of gifs, all over hacker news. When I would say: This is a terrible idea, they just want to control and own the content. a gif you own, a gif you can download and so what ever you wanted, etc. boy was I down voted into oblivion. One of the reasons for this is many posts aren't on hacker news by accident, they are hidden advertisements. Chances are, you got the karma to downvote people. Just starting off at -1 is a disadvantage. You can try it too: just disagree with what an article says in the comments, 99% of the time you will be downvoted. Agree and you get free karma. This is not always the case, but generally is. reply ants_everywhere 9 hours agorootparent> One of the reasons for this is many posts aren't on hacker news by accident, they are hidden advertisements. obligatory https://paulgraham.com/submarine.html reply gentleman11 11 hours agorootparentprevIt’s the opposite. Being contrarian is free karma, generally. The downvotes come from picking unpopular positions, whether you’re wright or wrong. I used to get downvoted a lot when talking to people about privacy, but now those sorts of posts get upvotes. Times and prevailing opinions change reply crooked-v 13 hours agoparentprevThat stuff is still around, it's just on Discord and VRChat now. reply FroshKiller 12 hours agorootparentThen it isn't around. It's walled off. And it will disappear. reply haswell 12 hours agorootparentI’m somewhere in the middle on this. These communities are real, and the value of the content shared by users is real. On the one hand, I prefer the old open web. On the other, I can’t deny the existence and value of these walled gardens. The wall doesn’t erase the value, even if I strongly prefer there wasn’t a wall to begin with. To your point, discord comes with downsides, and does raise questions about the longevity of the content. Most of the old web disappeared too. I think that in order to have any hope of returning to something more open and public on a wider scale, it’s necessary to understand why these communities are thriving on discord. I don’t have the answer to that question, but I suspect that the wall is actually a benefit to some. reply ants_everywhere 9 hours agorootparent> are thriving on discord This is always multiply determined, but part of the answer is that Discord and these other walled gardens are subsidized by VC funding with the expectation that at some point they'll turn on the garbage compactor and squeeze out the juicy value. reply Almondsetat 12 hours agorootparentprevDo you spend your life reading 20yo blog posts? Just live the moment with the opportunities it gives you reply mauvehaus 12 hours agorootparentA forum for my car just went dark. Gone are not quite 20 years of posts and discussions that I've searched many times over while troubleshooting mine or in preparation for some job that needed to be done. I made this discovery after going to post a novel solution for getting into the hood with a seized hood release in hopes that it might spare someone else some trouble in the future. All things are not in the moment. Sometimes it's good to learn from the past. reply Terr_ 4 hours agorootparentIt makes me wish for forums where posts and comments are automatically synced and cached locally. Then everyone has a copy in case it closes down, and someone could use the content to bootstrap a replacement. Note, that isn't the same as an ambitious fully federated mesh network or anything, but being limited in certain ways makes it efficient and easier in others. reply Vrondi 11 hours agorootparentprevThe current moment is great if you never need to find an old book, fix an old car, fix an old house, identify an old variety of plant, or any of a thousand other things. Basically, if you have no other interests, responsibilities, or desires other than fads, like a child. That's fine. Otherwise, there are decades of useful experience of large groups of human out there to be mined. It can often save you thousands of dollars, but then again, that's something only a grown-up cares about. reply baubino 10 hours agorootparentFunnily, the walled garden often gives people the impression that it is not walled but in fact infinite. I‘m always shocked at how many people don‘t realize how little information is accessible on the internet anymore. Yes, as soon as you need something that isn‘t faddish/of the immediate current moment (especially if you‘re not trying to buy something), the internet fails. There is so much useful info that is extremely difficult to find online these days. reply gary_0 4 hours agorootparent> I‘m always shocked at how many people don‘t realize how little information is accessible on the internet anymore. Any millennial or nerdy Xer with a good memory would tell you what the deal is. We were the first ones who came of age online, and saw what the Internet was like when it came to prominence. Now, our libraries, town halls, and weird gardens are all ash. Try to surf like you did in 2010 and all you'll see is spam; the filters work in reverse now. Log in to what \"social media\" is these days (after filling in your phone number and uploading your driver's license) and it's just cable TV with some extra widgets. At least Wikipedia is still there. reply 1shooner 12 hours agorootparentprevI think the point is that passively feeding your content to a profit engine actually limits your opportunities vs a modicum of agency over how you want to share your knowledge and experiences. If you honestly don't care about anything beyond your tiktok feed, that's fine, but there is interest out there in a longer time envelope of cultural production. reply dhosek 12 hours agorootparentprevSometimes. I stumbled across somebody’s 20+-year-old collection of deep musical analyses of early Chicago songs. That same material on Twitter/Facebook/Discord/whatever is going to be inaccessible. reply mauvehaus 11 hours agorootparentI'd be interested in a link. Live At Carnegie Hall was my paper-writing music in college. I discovered later that I don't much care for it when I don't have anything else occupying my mind. I'm unsure if it's because there isn't really that much to the songs of if I'm missing some nuance and complexity that makes them worth another listen. reply dhosek 9 hours agorootparentI believe this is the one I found: https://www.paulmorellimusic.com/notes-on-chicago-transit-au... reply waprin 11 hours agorootparentprevIronic you post this because I just rediscovered a 20yo blog post that I remembered reading, re-read it again and found it very motivating and inspiring and submitted it. reply ajuc 10 hours agorootparentprevIt would disappear anyway. All those personal webpages will be lost in time, like images hosted on long-dead servers and phpbb forums. \"Internet never forgets\" is one of these naive fairytales we told ourselves when we were young and internet was new, along with \"information wants to be free\", \"censorship on the internet is impossible\", and \"easy access to information helps democracy\". I prefer open web for sentimental reasons, but I don't think it's naturally better at preserving the information than walled gardens. reply raxxorraxor 3 hours agorootparentprevPeople that want to stay more anonymous are exemt from Discord. I get the draw of the platform, but the negatives are undeniable. reply the_af 10 hours agorootparentprevDiscord for me is chat & FOMO. I'm not into that, so if Discord or other chat walled gardens are the answer, then I've lost. reply Repulsion9513 5 hours agoparentprevYou know what, I'm gonna go find an \"under construction\" animated gif to add to my website now. Thank you. reply Vrondi 11 hours agoparentprev...these things still exist. They are not surfaced by search engines, because they are too small, and no companies are paying the search engines to do so. Most people stay inside their social network bubbles and never look at regular websites outside them. reply solarpunk 11 hours agoparentprevmy friend has a thing that really fits the vibe of \"digital gardens\" well: https://greenhouse.server.garden/ reply mmaniac 15 minutes agoprev> ... So-called “scientific forestry,” was that century’s growth hacking: it made timber yields easier to count, predict and harvest, and meant owners no longer relied on skilled local foresters to manage forests. ... This is a general principle of optimization, which maximises efficiency at the expense of redundancy and anti-fragility. Unfortunately, the ability to withstand an unexpected catastrophe does not appear on a balance sheet until the catastrophe strikes. reply euroderf 2 minutes agoparentAnd then (as a general rule in the modern economy), if catastrophe strikes and you get a bailout, you have successfully socialized the risk. Profit! reply anal_reactor 8 hours agoprevThe old internet is not coming back. 1. The old internet had certain entry barrier, which in turn allowed higher-quality content. We cannot tell all people below certain IQ to just stop using the internet so that websites wouldn't cater to them in the pursue of maximizing their userbase. 2. The internet's main purpose was entertainment, which allowed huge experimentation and creativity. Nowadays we mainly use the internet as a tool to complete daily tasks, which means it should be boring and reliable. Imagine going to a subway station only to learn that on that particular day only people in green trousers are allowed. 3. Big tech appeared because big tech allows to do things more efficiently, which is something we do want. 4. The internet was new, therefore interesting. 5. Most of our lives were offline, therefore spending time online was inherently fun because it was something different. Games! Music! Chat rooms! Pedophiles! Nowadays we mostly live online - we work online, we socialize online, we date online, we consume media online, we do all the boring life responsibilities online. It's the offline life that actually allows certain forms of escapism. Internet was fun, but now it's dead, time to move on and stop molesting a spasming corpse. reply gilmore606 6 hours agoparent> We cannot tell all people below certain IQ to just stop using the internet What, why not? It couldn't be that hard to trick them. reply sneak 6 hours agorootparentStart an ssh-only bbs. reply probably_wrong 1 hour agorootparentI have considered something like that in the past. The problem is that you can't stop someone else from making a web frontend (with ads on it, of course) and now you're back to were you started. reply bingbangboom 4 hours agoparentprev>We cannot tell all people below certain IQ to just stop using the internet Not directly, but we can ban children from the internet (or at least force them into read-only mode) via some legally enforced anonymized federal ID system. I'd accept the privacy risks associated with that in order to fix the internet. reply renegat0x0 13 hours agoprevI am running my personal web crawler since September of 2022. I gather internet domains and assign them meta information. There are various sources of my data. I assign \"personal\" tag to any personal website. I assign \"self-host\" tag to any self-host program I find. I have less than 3k of personal websites. Data are in the repository. https://github.com/rumca-js/Internet-Places-Database I still rely on google for many things, or kagi. It is interesting to me, what my crawler finds next. It is always a surprise to see new blog, or forgotten forum of sorts. This is how I discover real new content on the Internet. Certainly not by google which can find only BBC, or techcrunch. reply squigz 11 hours agoparent> I have less than 3k of personal websites. It is extraordinarily difficult for me to believe this is indicative of the reality of the entire Internet. I've probably visited a good chunk of this amount of personal sites. reply nvy 10 hours agorootparentIt's missing my personal site at least. I'm sure there are many others. reply blackhaj7 8 hours agoparentprevThis site alone has links to 6k personal sites - https://aboutideasnow.com/ reply marginalia_nu 13 hours agoprevThere's plenty of great fascinating wild Internet out there. The problem is finding it, since it typically struggles to be heard in a sea of well funded SEO-optimized garbage. It's a discovery problem, above all. reply emporas 6 hours agoparentThe reinforcement learning algorithm of facebook, instagram, youtube, tik-tok needs to move at the edge for that to happen. Nowadays, this seems much more probable than any other time in history, i.e last 15 years. Personal recommendation algorithms using RL, are certainly possible, because RL is the least computationally intensive training, compared to supervised, unsupervised etc. They also require some kind of social structure, but that will be setup on a blockchain, in an totally open and transparent way, yet private. reply realusername 3 hours agorootparentTikTok is actually the only one from your list which gives a real chance to newcomers through the algorithm. That's also one of the reasons why the app is popular, the algorithm is just very different from anything else. reply Timwi 12 hours agoparentprevI agree with you, although I wonder how we found the great fascinating wild websites before search engines even existed. I remember “web rings” but I don't think that's the whole story. reply squigz 11 hours agorootparentFor a long time, search engines weren't nearly as SEO-gamed as they are now reply marginalia_nu 12 hours agorootparentprevWe've more or less always had search engines as long as there has been a public Internet. But web directories were a thing early on too. reply chadsix 13 hours agoprevSelf hosting is definitely the way toward rewilding the internet. If everyone ran their own blogs, chats, email, etc. and let their systems federate thru protocol rather than on a single platform, the forests will thrive again. reply dangrossman 12 hours agoparentSelf-hosting means taking a second job as a sysadmin, applying software updates all the time, patching emergency security vulnerabilities, hardening services against constant attacks, deciding whether the daily \"I have found a bug in your system and will disclose\" mails are legitimate threats. If you fail at any of these tasks, relatively new regulations mean various governments can fine you more than your net worth over failing to report a data breach to the right agency on the right timeline. To me, that's what prevents people from standing up their own blogs and other services on a home-brew server in their house or a $5 VPS these days. reply fullspectrumdev 12 hours agorootparentVaguely sensible initial setup (doable from a template or script) and automatic updates (enable these from a script) solves like 99% of this problem. reply logicprog 12 hours agorootparentprev> Self-hosting means taking a second job as a sysadmin True, in the literal sense that you will have to administrate your own system, but I'm sure there must be ways to make this easier for people. Perhaps small computers that come pre-configured with the correct specifications for being a good small-scale server and pre-installed with software that provides a simple web GUI dashboard that you can just drop files on and it will serve them up for you, with everything else taken care of under the hood. > applying software updates all the time With image based distros, containers, and their respective auto-updating schemes, this — and the concomitant problems updates may bring given the extensive and hairy state of most systems — should hopefully become a thing of the past. > patching emergency security vulnerabilities I mean, unless you are running something that is incredibly visible online and linked to you from a lot of places or used by a lot of people and so you need to take extreme extra security steps, shouldn't this be taken care of by just regularly updating your software? For a small scale self-hosted blog or personal email server this seems a hardly proportional. > hardening services against constant attacks Again, it seems like you are projecting the requirements of a much larger scale endeavor onto small-scale personal self hosting of a blog or email server only you use. And to the degree that system hardening is necessary for a small cell posted system, once again image-based operating systems with hardened Pam authentication rules that run everything in rootless podman containers and keep SELinux enabled should be more than hard enough and all that can be configured and set up upstream to the user. > deciding whether the daily \"I have found a bug in your system and will disclose\" mails are legitimate threats What are you even talking about here? For the third time, it seems like you are projecting the requirements of a much larger scale thing onto small-scale personal self hosting. > If you fail at any of these tasks, relatively new regulations mean various governments can fine you more than your net worth over failing to report a data breach to the right agency on the right timeline A data breach? On a small self-hosted blog or email server? Who's data would those regulations be punishing you for leaking, your own, maybe two freinds'? And those regulations, if you are speaking about the ones in the EU that I am thinking of, have pretty clear cut offs and requirements and stuff that really wouldn't apply to someone's little self-hosted thing. reply seabass-labrax 11 hours agorootparent> Perhaps small computers that come pre-configured with the correct specifications for being a good small-scale server Great minds think alike. See FreedomBox[1] for a totally FOSS implementation of that idea; one of my friends runs all of his internet services at home with it: email, file storage, contacts synchronisation etc. [1]: https://wiki.debian.org/FreedomBox reply dangrossman 12 hours agorootparentprev> A data breach? On a small self-hosted blog or email server? Who's data would those regulations be punishing you for leaking, your own, maybe two freinds'? Say you host your own WordPress blog with comments enabled. A few of your posts get to the front page of Hacker News, and you collect a couple hundred comments from California techies. Your WordPress instance is breached because you didn't patch a zero day vulnerability quickly enough. You have to personally notify every California resident of the breach, and California's Attorney General. Government regulation means that part of your job in self-hosting a simple blog is knowing that CCPA exists, along with every similar regulation passed by every other state, now and into the future. Worse, you may not even realize you're holding regulated \"personal data\" and how much. Maybe you try to avoid this liability by turning off comments and uploads, but you don't realize your web server has access logs enabled, and some state or country considers this personal data as well. GDPR does for one. reply Vrondi 11 hours agorootparentThe easy solution is to just not enable comments. Nobody has an account on my site but me. I'm endangering nobody's data but mine. No worries. reply rchaud 10 hours agorootparentprev> Say you host your own WordPress blog with comments enabled. A few of your posts get to the front page of Hacker News, and you collect a couple hundred comments from California techies. The possibility of this is less than 1% purely due to commenting friction. Hacker News already has a comment section. No one's going to sign up for a Wordpress account in order to post their comments there. reply telmo 11 hours agorootparentprevCome on now. Can you point to one single case of something close to this scenario having happened in real life to a small self-hosted WordPress blog? Or even a big one? Governments are not that stupid, they are not that malicious and they do not have infinite resources to pursue such frivolous and nonsensical activities. This reads like some weird sort of paranoid legal fanfic. reply squigz 12 hours agorootparentprev> You have to personally notify every California resident of the breach, and California's Attorney General. And if one doesn't? reply dangrossman 11 hours agorootparentThe state can get an injunction against you, fine you, and in some situations creates a private cause of action for the people whose information was breached to individually sue you with statutory damages available. reply squigz 11 hours agorootparentI'm in Canada. Can they still do that? reply j-bos 11 hours agorootparentprevDo those laws apply even if you charge no fees and serve no ads? reply johnny22 13 hours agoparentprevI have the technical capability to host, but I don't want the responsibility or want to spend the time. I don't wanna be interrupted from my actual work or hobbies to fix up my personal hosted stuff. I just want to let someone or some group handle that for me. reply ipaddr 13 hours agorootparentGet shared hosting. reply johnny22 12 hours agorootparentI still have to manage the service or the code. I don't wanna worry about software updates. reply logicprog 12 hours agorootparentauto updating podman containers. reply johnny22 3 hours agorootparentso you're saying there will never be a breaking change when the or bugs when the software updates? ever? That seems unlikely. I've managed my own servers many times before and that's never been the case. reply myself248 13 hours agoparentprevI already have plenty of hobbies, though -- the ones I want to write about on my blog. I don't need self-hosting to be a new hobby. I would love if I could pay a \"gardener\" to maintain my self-hosted stuff. My hardware (that's one hobby!), my bandwidth, my choice of non-invasive species (informed by consultation with the gardener, which ones they'd like to maintain), but ultimately someone else does the planting, the weeding, the single sign-on integration, the updates. reply cgh 13 hours agoparentprevThe article is primarily about breaking up certain technology companies into their broad constituent parts and mandating interoperability, presumably by law. That seems a lot more realistic than your average person self-hosting, which almost everyone would view as a hassle and a usability fail. reply aeturnum 11 hours agoparentprevI personally am more interested in micro-hosting services like omg.lol[1]. I do self host a few services, but they are generally on VPNs that have smaller attack surfaces. I don't think it's practical for most individuals to maintain secure web servers, but I think most people can pay a very small amount of money to get most of the benefit. [1] https://home.omg.lol/ reply sdsd 5 hours agoparentprevI just made a personal site (darigo.su), it's buggy as hell but fun to just make whatever I want. Do any of you have a quirky personal site? reply gitgud 13 hours agoparentprevI wonder if self-hosting away from giant cloud providers like AWS, Azure and GCloud would be better for the internet too. Self host at home should be much more common than it is… reply giantrobot 10 hours agorootparent> Self host at home should be much more common than it is… It violates your ISP's EULA and they'll shut you down. Unless of course you pay twice as much (for the same service) with a \"business\" account. reply ranger_danger 13 hours agoparentprev> federate But I think the problem quickly turns to over-fragmentation, even with something like matrix, mastodon or lemmy today, there are so many different instances and many of them are blocking several other instances from federating with them, people don't know where to go and have too many different places to look for stuff that they simply won't even try. reply johnny22 12 hours agorootparentI still don't have a matrix account anywhere because of that. Let alone the others. I want my own identity, but I don't wanna host a whole server. It'd be cool if fastmail offered such a service reply drak0n1c 13 hours agoprevSmartphones enabled internet addiction to reach the masses. Previously it only engaged those who had the patience and wits to maintain a PC and sit down at a desk to read and write on the internet. As long as the professional and volunteer skills to invest in a \"wild\" project are transferrable to that of a centralized platform, creators will gravitate to the side that has more users and can pay them more and consumers will gravitate toward the one that is easier to access and worth the according cost of entry (oftentimes free). This isn't a unique phenomenon. Look at video games. When any subculture/industry goes mainstream you don't see it preserving the thriving ecosystem of before and just adding on a synergistic mainstream bloc. Rather, the mainstream bloc cannibalizes and leaves dwindling ghost towns. That's the price of attention competition and opportunity cost. reply juancn 14 hours agoprevIt reads like something a GPT model would write, they go all over the place. I also wonder what would their point of view be if the internet truly was wilder, like 4chan or everything behind TOR. On one hand, I kind of like the idea of spreading and atomizing the internet to the masses, a bit like it was at the beginning, but at the beginning the people that were part of the internet were a particularly well educated elite. It was definitely not for everyone. Not everyone was online, what we had was people with disposable income to have a computer, usually with some form of college education and some technical knowledge. Then the masses came, and social media thrived. reply digging 13 hours agoparent> It reads like something a GPT model would write, they go all over the place. It's nothing like GPT IMO; it's opinionated, for one thing. I think it's just hard to follow because it does draw from a large variety of sources and ideas and doesn't weave them together very smoothly. This is showcased with the horrible modern trend of displaying a large quote of something the article literally just said, or will say next. This practice is always a bad choice, but failure scales with the complexity of the article; the author would have done better to use more headings, which would help them organize the work. I do completely agree with your assessment of why the earlier internet was... a more rewarding experience. (I don't think \"better\" is a meaningful term here.) But, the article is explicitly rejecting the idea of returning to the past, and instead building something new that fosters emergent behavior and diversity. The goal is empowerment of agents who have a will to cultivate their ideas. reply sapphicsnail 12 hours agorootparentWilder doesn't necessarily equal more anonymous. I spend a lot of time on little forums where I might not know someone's irl name but I know who they are. reply kombookcha 2 hours agorootparentOne of the great losses that came with the end of the forum era was all these little communities with distinct cultures and regulars that you got to know. I kind of found a similar vibe in some discord servers, but they're much more difficult to join as a new user than a forum because old information isn't easily searchable and discussions aren't as longform as old forum posts were. reply mondobe 13 hours agoparentprev> if the internet truly was wilder, like 4chan I don't think they were referring to moderation in itself. 4chan isn't a decentralized platform like the Fediverse (AFAIK), so, if it was theoretically large enough (like, TikTok-sized), it would be just as big a problem. reply statquontrarian 13 hours agoprevI like: 1. The Neocities random page: https://neocities.org/browse?sort_by=random 2. The Neocities recently updated page: https://neocities.org/activity 3. Status Cafe: https://status.cafe/ 4. The MidnightPub: https://midnight.pub/ reply spxneo 13 hours agoprevNo matter what you do, 90/00s internet is never coming back They demonstrated the ready willingness use of violence by special interest groups bankrolling and giving insider trading tips to your political leaders. Meanwhile some college student torrents some Hollywood movie and gets arrested. ex) Megaupload reply thesagan 11 hours agoprevThe Internet is not Detroit, but like Detroit it expanded very very fast and was very very innovative and open. Then it consolidated, which is ongoing. Things settled… for a while. The early days of Detroit saw hundreds of little innovative experimental companies that over time became the big three. The rest is history. Detroit is the victim of its own invention. Luckily my analogy isn’t a very good one, the Internet is nothing like Detroit. It is, however, interesting how quickly wild things can be tamed, and that may beckon a cautionary lesson. reply efields 13 hours agoprevI'm pessimistic about the likelihood of any new \"wild\" internet ecologies thriving the way they did in the past. If you think of hypertext as a medium, it's been pushed to its most logical extreme already. There's not a lot of boundaries to explore. There are weird places online already, sure. And curiosities still abound. But idk… I wonder if a generation lost in cyberspace is exactly the reason why we are actually so unhappy with the internet writ large. Nostalgia is a hell of a drug. reply SamBam 12 hours agoparent> If you think of hypertext as a medium, it's been pushed to its most logical extreme already. I think this is an odd framing. It's not about pushing things to further extremes, it's about using all these mediums that have been allowed to wither and die. It's about getting out of the sterile high-walled gardens. reply 1970-01-01 10 hours agoprevStart by bringing back Stumble. It was a very good discovery service, with plenty of variety. https://web.archive.org/web/20060902030402fw_/http://www.stu... reply curious_cat_163 9 hours agoparentI recall using a plugin for it! I can imagine a place for it today. reply bingbangboom 4 hours agoparentprevHere's a modern equivalent: https://cloudhiker.net/ reply kderbyma 12 hours agoprev\"The story of German scientific forestry transmits a timeless truth: When we simplify complex systems, we destroy them, and the devastating consequences sometimes aren’t obvious until it’s too late.\" I really agree. so often simplification alone is not enough. it should be a guiding principle, but not ignorant of the necessary complexity reply d--b 14 hours agoprevThe geocities times weren’t THAT cool. We mostly had to go through pages and pages of crap stuff, cause we couldn’t find shit. reply rchaud 10 hours agoparentGeocities wasn't something you discovered via search. It was a directory you could explore, like walking through the streets of a city instead of cabbing it straight to a top-rated restaurant. reply vaylian 4 hours agorootparentAnd then there were webrings, which allowed one to walk all over many different loosely related websites. https://en.wikipedia.org/wiki/Webring reply gspencley 13 hours agoparentprevCan we find shit now? I swear search engines are getting borderline useless. And it's not necessarily their fault, it's just very difficult to find search results these days that aren't lists of affiliate links. And in fairness, this is probably less to do with SEO'ers \"gaming\" the system and more to do with the fact that all of the content we actually cared about just disappeared behind walled gardens. I abandoned Google Search a few years ago because I was tired of it trying to guess what I was \"really\" trying to search for instead of just matching my keywords. But DuckDuckGo seems to be getting worse in this regard lately too. I will get a bunch of results that are about something I don't care about, identify a keyword that I could easily filter out and add the \"-\" modifier to my search string only to see it have no effect what-so-ever. Grrr.... I also can't believe the amount of search results that take you to a walled garden. Instagram for example. You can't lurk without an account and every single time I've tried to create an account it gets automatically suspended after a few days because their system flags it as a \"fake account\", whatever that means. It used to mean a bot but I can easily prove that I'm a real human being and yet when I've appealed the suspension it has stuck. So it blows my mind that they rank so highly in the search results despite doing everything they can to prevent people without accounts from even being able to create accounts, let alone browse those results. The Web is dead. reply deepspace 8 hours agorootparent> Can we find shit now? Absolutely. Like with anything else, you literally get what you pay for. Free search services like Google and DuckDuckgo indeed suck. I pay for Kagi, and have a wonderful search experience. I seldom have trouble finding anything. Once in a while I use someone else's computer and have to use Google. What a jarring, unpleasant experience. reply krapp 13 hours agorootparentprevI find shit all the time. Google still works fine for me. I don't know what people on Hacker News are doing that they can never seem to find a single useful result on any search engine, ever. reply akira2501 13 hours agorootparentI search for somewhat obscure and highly specific things. There are queries which used to reliably return the results I was looking for. Over the past several years these queries no longer work. It feels like the search engine used to accept whatever I typed and used that to query. The results had to be a good match for all the words in my query. This made it very easy to find exactly what I was looking for. Now google feels free to ignore terms from my query or to use synonyms instead of the exact term. The results are now exceptionally broad and no longer are a good match for the specific query I've entered. I now have to use \"quotes\" \"around\" \"every\" \"single\" \"term\" or go several pages deep to find the page I want. reply rhdunn 12 hours agorootparentprevSearching e.g. \"xq31 curlyarrayconstructor does not use enclosedexpr\": 1. In google, I get 9 results, none of which are a reference to the W3C XQuery/XPath issue. This is because google is actually \"helpfully\" searching for \"xq31 curly array constructor does not use enclosed expr\" -- telling it to not do that finds the issue, returning two results. 2. In bing, the bug report (bug 29989) is the top result. 3. In DuckDuckGo, it is also the top result. I've had other problems searching google for specific error messages as it decides to remove quoting, tends to show results for space-separated compound identifiers (see the \"curly array constructor\" in what google searches for by default), does word stemming, and other \"helpful\" transformations on the search. reply valicord 12 hours agorootparent\"I had to do an extra click on a clearly labeled link\" does not equate \"search engines are borderline useless\" reply calgoo 12 hours agorootparentprevI have been using DDG for the last 5 years and used google the other day after a fresh install, and i find the results are just worse (IMHO). Now, you also learn to write searches in specific ways depending on the search engine. DDG reminds me more of old school search engines (or at least it used to). Again, this is just my experience, there might also be something with google adapting to your search habits if you allow it, but not sure if that exists in google search like it does in YT. reply valicord 12 hours agorootparentprev> I don't know what people on Hacker News are doing that they can never seem to find a single useful result on any search engine, ever. And they never give specific examples either. reply bigstrat2003 12 hours agorootparentprevYeah this one perplexes me too. I won't deny that there's a ton of blogspam on Google (or any other search engine), but Google is still the best search out there imo. I certainly don't understand people who say they find ChatGPT to be superior. Like, what? They don't even fill the same role! reply tsunamifury 13 hours agorootparentprevBecause they remember the past far more fondly than it was and converge the utility they get from the current day with the ideals of back then. The reality is there is far more creative and well produced content on the web and closed platforms today than there ever was in any other point in time. And if you can't find it fast enough, AI can generate direct, customized answers. reply Gud 12 hours agorootparentNo, it is not us \"remembering the past far more fondly\". Today Google search results yield the same crap without fail. For example, a search for \"how do I repair widget X\" will inevitable take me to a garbage SEO site following the same patterns. The internet is less useful today than it was 10 years ago. Probably it peaked 20 years ago, before big money completely took over and ruined the place. reply tsunamifury 11 hours agorootparentIt did not peak 20 years ago. This is laughable rose tinted glasses. Also you can ask almost any LLM model for that type of answer and get it instantly and personalized. reply Vrondi 11 hours agorootparent...and you will not know if the answer is accurate or where it came from. reply tsunamifury 11 hours agorootparentThis is outdated. You can source with many modern public LLM products now. Why is hacker news full of so much outdated thinking? reply int_19h 9 hours agorootparentprevIt is objectively worse than it used to be because it tries to be too \"smart\" by rewriting queries in a way that genericizes them. Instead of getting 10 search results that are actually relevant to the question being asked, you get \"10,000,000\" results that are only vaguely related. I suspect the reason for this is that more search results -> more pages the user will likely click through just to make sure they are looking at the best links -> more ads served. Related to this is the dumbing down of query input. Like, in many cases, even using double quotes around search terms doesn't guarantee that the page that will be shown in search results actually contains the quoted word anywhere. I've had one case where Google would helpfully find articles talking only about Linux given a query like '\"freebsd\" foo bar' (and where it showed the snippets for them, highlighting the phrase it treated as a match, it would say 'linux foo bar', so you could tell right away what exactly was substituted). It used to be a power tool that had well-defined knobs and buttons that you could master to find exactly what you want. Now it's a magic wand that tends to do something inscrutable every time you wave it, and you \"master\" it by flailing around until it kinda sorta does what you want. reply postalrat 5 hours agorootparentGoogle says it finds millions of hits but only lets you scroll though a few hundred. The rest they don't want to show you. reply rchaud 10 hours agorootparentprevI can honestly say the Internet doesn't provide any additional utility compared to what I used it for 20 years ago: email, messageboards, P2P filesharing, Wikipedia, search and online shopping. Sure we have Youtube and streaming now. But streaming today is just cable TV by another name. You could subscribe to them all and still not have the selection that a Blockbuster or torrent tracker would. reply marginalia_nu 11 hours agoparentprevGeocities was mostly a creative experience, more so than something you'd consume. The fun was in participation. reply DebtDeflation 13 hours agoparentprevYeah, but music would auto-play in the background, random stuff would float across your screen, and your cursor would turn into......something. reply xandrius 13 hours agorootparentI think it's all nostalgia: I remember it and it was mostly crap. What made it super special for me was when I was looking at a site made by someone I knew, and I could learn more about them through what they made. reply SteveNuts 13 hours agorootparentprevMy favorite was an ascii 12 hour clock with hour, minute, and second hands rotating about the cursor. Really helped me know exactly what time it was while surfing those Freewebs sites! reply rajeshp1986 13 hours agoprevI think internet was way awesome before digital advertising era. There should be areas of internet which should try to replicate the early internet i.e text-only design, bring back forums and no advertising revenue. reply rchaud 10 hours agoparentEven with advertising, Google Ads (then called AdSense) was a lot more generous with their revenue share than they are now. Lots of little blogs and forums thrived because the money coming in was enough to keep the proprietors interested in maintaining a community of commenters. Then comment bot spam started growing out of control, webmasters ceded the comment platform to centralized services like Disqus, and finally Google slashed ad premiums, so websites either started disappearing or getting folded into the properties of 'blog networks' and lost their personality in search of pageviews. reply peppertree 12 hours agoprevTruth is with LLM and chat as main interface, it's going to be harder and harder for niche sites to be surfaced. reply rchaud 10 hours agoparentChat isn't a main interface now by any means and I doubt it will be beyond answering queries of an academic/work nature. reply kristopolous 14 hours agoprevThere's lots of these projects already, such as https://neocities.org/browse And even people with weird myspace style social networking sites like https://spacehey.com/browse Then there's telegram and discord, full of all kinds of strange stuff. They're just not mainstream. Probably better that way. reply wk_end 12 hours agoparentNot to disparage spaces like NeoCities, but to quote the great American singer/songwriter David Berman: Punk rock died when the first kid said \"Punk's not dead, punk's not dead\" That is - these spaces will never quite be the same as what we once had, because they're fundamentally nostalgic. The \"wild\" internet was also once the vanguard, the future, and that excitement won't be recaptured by going back. reply kristopolous 12 hours agorootparentIs it going back or picking up where the other left off? I think these have inspirational historical bases as opposed to being renfairs. They are in the hands of a younger generation. reply quartesixte 14 hours agoparentprev>Then there's telegram and discord, full of all kinds of strange stuff I was just about to say this. The new pockets of internet communities are filled with interesting and fun things again. It’s just that a lot of them are for and run by teenagers. As adults, we either feel immediately out of place or just get banned outright. The ones run by adults are incredibly niche focusing on “non-mainstream” interests. Discovery is a little harder, yes. And it feels more ephemeral than the static sites of yore. But they’re out there. And they thrive. And 4chan still exists. The kids will be alright. reply kristopolous 13 hours agorootparentyeah, click around the spacehey site. It's a bunch of kids 13-18 or so. I had initially thought this site was going to be mostly nostalgic people in their 30s and 40s.* It's fascinating how people born after the decline of myspace has found a clone and helped bring it to life. --- * maybe it is and a bunch of people are just playing a character ... unlikely though. reply katehikes88 13 hours agorootparentprevsad thing is that you need to provide a phone number just to read those telegram and discord groups. 4chan is nothing like it used to be. it is run over by bot posts and shills from various interest groups. the amount of genuine anon posts is quite low. reply dang 12 hours agorootparentSorry for the offtopicness, but trollish usernames aren't allowed on HN, so we've banned this account. If you want to pick a different username, we can rename it for you and unban the account, as long as the username is genuinely neutral. https://hn.algolia.com/?sort=byDate&dateRange=all&type=comme... reply rumdz 14 hours agoparentprevI was just about to post neocities, but then got distracted by all the cool sites. reply bsaul 13 hours agoparentprevfriend of mine told me this exactly : going on telegram feels like being back to the original internet. But that's probably because of all the illegal/uncensored content you can find there reply kristopolous 13 hours agorootparentever go to onion sites? They're pretty great. No javascript, social media share buttons, ad-tech or click-jacking or videos popping up or walls in front of the content. No liking or subscribing or logging in with your facebook account... it's ironic how tor is the clean internet. reply Timwi 12 hours agorootparentWhile all of what you said is technically true, you left out the fact that Tor sites are majority about illegal activity, such as hiring assassins, ordering forged identity documents, and buying and selling controlled substances. I'm afraid that kind of kills the nostalgia for me. reply philwelch 11 hours agorootparentHalf the fun is trying to guess which 40% of those are just larping, which 40% are feds, and which 20% are actually real. reply CaptainFever 6 hours agorootparentThe Internet, where the women are men, the men are boys, and the boys are FBI agents. reply runjake 12 hours agorootparentprevWhich onions sites do you find interesting or useful? reply 1vuio0pswjnm7 8 hours agoprev\"Whatever we do, the internet isn't returning to old-school then-common interfaces like FTP and Gopher, or each organization running its own mail server, rather than operating off G-Suite.\" It does not need to \"return\" to FTP because FTP is still in use. Periodically I am submitting examples of FTP servers to HN that are still operated today that are often critical to software developers and so-called \"tech\" companies. reply sirspacey 5 hours agoprevBring back the 90s blog culture. I’m over everything else. reply mikewarot 12 hours agoprevThere aren't any operating systems capable of withstanding the internet for a year without updates and/or active management. Until this becomes possible, and widespread, the idea of self-hosting is going to remain niche. Furthermore, because no browser is totally secure, users are going to avoid non-popular links, perpetuating the walled gardens they already use. I fear our monoculture of operating systems has to be addressed first, before any of the rest can really take hold. reply wswope 12 hours agoparentWhere’s that idea coming from? I’d wager >95% of LTS linux distributions released in the past ten years are still rock-solid if you’re just exposing 22, 80, and 443. Even PHP5 is still de facto bulletproof if you’re paranoid about not trusting user input. Vulns don’t come out of nowhere. reply toast0 5 hours agoparentprev> There aren't any operating systems capable of withstanding the internet for a year without updates and/or active management. I regularly update my personal systems, but when I ran FreeBSD for work, we only did kernel and OS updates for important things. Most of our machines would turn on, build and install our latest tested and patched version and reboot, and that would be the last time it rebooted until it was time to recycle it 3-5 years later. Yeah, I looked at every update, and most of them weren't important enough to upgrade existing working machines; I guess that's active management: I did consider and most often choose to take no action, but it's still not many actions taken. That's not to say FreeBSD is perfect; it's not, there were certainly some updates that needed to go to all machines right quick. Some of the applications we ran on the other hand... It was a great day when we replaced Wordpress, because I was no longer on the hook to spend a day updating that with no notice. I would be ok leaving my current hosting for a year and confident it will be fine when I get back; other than it a disk encryption passphrase to boot after a power incident. Mostly Linux with minimal software that's mostly finished is fine too; although I won't run it personally anymore, except in OpenWRT and other embedded systems where I don't have much choice. reply squigz 11 hours agoparentprev> monoculture of operating systems There are several widely-used OS, one of which (Linux) has plenty of very different versions depending on your usecase. I'm not really sure that counts as a \"monoculture\" reply Nifty3929 4 hours agoprevAll of the same sorts of people are willing and able today (myself included) to host our own content on quirky little sites. But the difference is that before, we were it - there was nobody else hosting anything much. There wasn't anything else. Now everybody, and every company is online. We're simply drowned in an overwhelming sea of mundane banality. And, frankly - normal, ordinary, every-body stuff. Reading the news. Doing some banking. It's not that we're not here - it's just that everybody else is here too. The internet is now the place that nobody goes to because it's too crowded. reply pupppet 14 hours agoprevBring back user-curated indexes. reply tmaly 14 hours agoparentI would take these over ad infested search results any day. reply mistermann 14 hours agoparentprevMakes discovery too easy, and memory holing too hard. (Conspiratorial/Systems thinking acknowledged.) https://twitter.com/reddit_lies/status/1778899250857783731 https://news.ycombinator.com/item?id=40043297 https://en.m.wikipedia.org/wiki/Closed-world_assumption reply royaltjames 4 hours agoprevReminds me of mmm.page reply web-cowboy 11 hours agoprevI wanted Tim Berners-Lee's \"Solid\" project to be at least a partial foundation for this, but it doesn't seem to be getting traction. Some way to \"self-host\" or \"own my own data\" isn't very possible in a world of complex system administration and cloud providers, GDPR/Privacy, etc. Maybe solving this problem (via FOSS, hopefully) and giving people their own internet-in-a-kit could be a good way to go? Are there active projects like this? Is it even feasible? reply Kon-Peki 10 hours agoparentI don’t think the solid project is feasible. If you create a pod and give someone access to information within it, you probably have to assume that they are going to make a copy of it and that they will share this copy with others. So what’s the point of going to all the trouble of setting it up in the first place? reply higgins 9 hours agoprevSit at 24HourHomepage.com for a minute or two reply seoulmetro 10 hours agoprevPeople who think this is possible are naive. It's not just the internet that has become a monoculture, it's the entire planet. Thanks to the internet. reply shadowgovt 12 hours agoprevIt's a nice essay but like so many in this space, it's really missing a key force that encourages consolidation, fights diversification, and pushes companies to centralize as much as they can into as few services as possible: Users. Users demonstrate, by their behavior, again and again, that they want simple, they want turnkey, they want inter-operation, and they want stability. And the big corps generally succeed in providing those things. Web 1.0 and most of Web 2.0 was none of those things. The easiest way to engineer those goals is to consolidate. Consolidated software developed under one roof can use incentive of money to induce engineers to figure out the \"grimy\" problems to make pieces of the tooling work with each other (such as GMail interoperating with Docs, or Facebook Messenger and Facebook itself). And the centralized dev houses force consolidation; for all the jokes (justifiable) about Google having too many chat options, how does it compare to the world outside their garden? How many people actually go through the hassle to get XMPP inter-operating, since Google dropping support for it didn't mean it actually went away? Not enough. A consolidated dev house doesn't have to have multiple competing social network platforms or image hosts, etc... They can maintain one, maybe float the occasional experiment, and users don't have to worry too much about change. Users actually hate many kinds of change because they build their lives atop the predictability of systems they don't control and they get angry when those systems are altered without their consent. And to head off the argument \"But the FAANGs change shit all the time...\" They sure do. Now compare that rate of churn to the rate of churn of how one stands up a web server, or how many non-FAANG online services have come up, run out of money, and evaporated. Inter-operation, at first glance, is the thing the walled gardens fight against. But ironically, by consolidating to one solution and letting just about everyone on the internet use it, they create an \"interior inter-operation\" to rival the inter-operation you can get with diverse solutions. Consider how much of Twitter you see doing a Twitter search vs. a Mastodon search, or how easy it is to find your coworkers on LinkedIn vs. finding (and using, successfully, through their spam filters) their email addresses. The single-service, one-API, few-clients model consolidates multiple things that users don't want multiple of... chat clients, search corpuses, histories, administration stories, username / password combinations, privacy policies. And this all carries consequences (get locked out for legitimate or illegitimate reasons and you're now locked out of a very big garden), but they're consequences that hit the few; for the near totality, \"it just works.\" Regarding stability: Google has been around a quarter of a century at this point. Facebook about two decades. They may not last forever, but users understandably are more likely to stick with them than to try new things. Every new thing is more likely to be a pets-dot-com than the next Amazon, and who has time to build familiarity with someone's bespoke solution just to get that inevitable \"We never hoped to have to send this message...\" turndown email? And this all culminates in the lack of proliferation of browsers, because the browser enables all these features and, at the end of the day, no user (rounding for error) wants to run two or more browsers and no web developer (rounding for error) wants to accomodate the undocumented implementation quirks of two or more browsers (quirks that always exist no matter how spec-compliant they are... If they didn't exist, they'd be the same browser). So there's the occasional cry of \"Why aren't there dozens of browsers\" without anyone really answering who wants that? Nothing about a \"rewilded\" net is impossible, but users don't want to do it and don't really want to use it. Not nearly enough to make it into something as big as the consolidated service providers. It's a nice hobby for people who really care about how the machinery works to dabble in (I, too, run a Mastodon node...), but I don't see anything pushing people out of their Facebooks, their Amazons, or their Googles, in the near future. Not if the alternative is for regular users to have to do more work than they do today. reply hfgjbcgjbvg 12 hours agoprevWould agree. reply thomastjeffery 11 hours agoprevThis is a great article, but it's missing the elephant in the room: copyright. There's a reason that our society has allowed >\"...a timeless truth: When we simplify complex systems, we destroy them, and the devastating consequences sometimes aren’t obvious until it’s too late.\" This a thousand times over. This should be taught in schools and repeated from the earliest grades to post-grad. It bears repetition, sufficient that it is seen by everyone as obvious. Then, perhaps the society will value complexity and diversity as the true strengths that they are. Homogenizing and monoculture will kill us all, whether it is forestry, agriculture, human cultures, economies, the internet, or mil or corporate orgs. Uniformity is brittleness. reply nf3 14 hours agoprevFrankly, I'd much rather tend to my real-life garden than any digital metaphor for one. I find it so much more gratifying! reply moshun 11 hours agoparentI’m not sure it’s a binary choice, blog your garden! reply shadowgovt 12 hours agoparentprevHardly anyone ever complains that I'm doing it wrong, for starters. reply voxelghost 9 hours agorootparentDid you forget to enable the comment plugin on your fence-module? reply giantrobot 8 hours agoprevWhile I don't disagree with the general sentiment the old Internet (late 90s to first half of the 00s) was very different topologically from the Internet of today. Colleges especially were lousy with IP addresses. They'd assign a routable IP to every computer on their campus, including the ResNets. They also handed out shared disk space on servers because that was new and novel. If you knew the right people you could even get a vanity subdomain on the school's domain. A significant amount of \"wild\" Internet content was found on those servers that lived on college campuses. Another significant source of \"wild\" content was ISP-provided web space. Most ISPs offered a few tens of megabytes of web hosting easily accessed via FTP. A lot of HTML authoring software of the era could publish directly to an FTP account. While the likes of GeoCities and AngelFire get a lot of the nostalgia today, they were far from the only source of garish web pages filled with questionable content. In the early 00s colleges stopped giving out routable IPs to everything with a NIC and started cracking down on closet servers and web space. Besides the network security angle the MPAA and RIAA were on a tear suing colleges for \"hosting\" pirated content. ISPs also dropped their shared hosting around the same time. By 2010 all the major free web hosting services had shut down or enshittified to uselessness. Without those places a lot of \"wild\" content just died out. People did move to hosted blogs like LiveJournal, Wordpress, or Tumblr but even those weren't the same type of \"wild\" sites that came before. The \"wild\" content has been further hurt by Google's algorithms' insistence that only content published in the past ten seconds is worthwhile and even then its only worthwhile if its SEOed to death and has Google Ads running on it. For the most part people just moved to social media. There's zero need for administration, bandwidth is a non-issue, and publishing is pushing \"Post\". The downside is its social media and discoverability or even availability outside the platform is challenging or non-existent. For non-technical people it's extremely difficult to set up a \"wild\" site on the Internet anymore. Even for the technically inclined it's a thankless pain in the ass to administer and keep up. If you do set something up it becomes a constant battle against spammers and script kiddies. Even just something going viral can get your account suspended or rack up huge egress fees on a cloud provider. I love the idea of re-wilding the Internet we shouldn't pretend the Internet of today is the same as the Internet of the last century. Edit: I a word. reply k310 13 hours agoprevWild is good as long as it's not wild as in battlefield. It's not like you get diverse opinions; no matter what you post, you get warring factions, vulgar insults, and sometimes, death threats. And bots. I posted earlier today: Anil Dash argues [1] that things can be better in not so wide-open communities. And then there’s someone like Darius Kazemi, a computer programmer and community organizer who has been patiently toiling away building tools that let others build healthy, constructive, human-scale online communities — the sort that are full of acts of kindness and genuine connection, instead of incessant fights about hate speech. There’s been a huge uptick in interest in Darius’ work as networks like Twitter have fallen apart, and a new generation discovers the joys of an internet that’s as intimate and connected as a friendly neighborhood. And this hearkens back to that surprising, and delightful, discovery that often underpinned the internet of a generation ago — sometimes the entire platform you were using to talk to others was just being run by one, passionate person. We’re seeing the biggest return to that human-run, personal-scale web that we’ve witnessed since the turn of the millennium, with enough momentum that it’s likely that 2024 is the first year since then that many people have the experience of making a new connection or seeing something go viral on a platform that’s being run by a regular person instead of a commercial entity. It’s going to make a lot of new things possible. [1] https://www.rollingstone.com/culture/culture-commentary/inte... I have had ideas for a different internet -- a network of networks. Since Stewart Brand and Ted Nelson and Kevin Kelly, technology was meant to empower and liberate people. Only a handful actually do, as in Wikipedia. People have enormous computing power in their computers and mobile devices. It often just liberates cash from their pockets. Time to reinvent. I'll share ideas later. No need for this crap: [2] Well, things changed a little bit in tech of late. Often, the power shifts in the tech world because of a dramatic new invention that solves an old problem a whole lot better. But in the current era, when most of what's getting funded and hyped up are just various attempts to undermine workers and control consumers, we're instead seeing lots of major players lose power because their signature offerings have gotten so much worse. Search engines are becoming far more useless as they attempt to chase AI hype and shoehorn in less reliable results, even as their legitimate search results get cluttered up with AI-generated crap. The most culturally influential social network has had its cultural relevance destroyed by its billionaire man-child owner's tantrum-based managemenet style. And the major mobile phone platforms overplayed their hand so badly in exerting power over their app ecosystems that regulators around the globe have responded by prying open these heretofore-closed markets. [2] https://www.anildash.com//2024/01/03/human-web-renaissance/ Time for a network of networks, not a network of 4 or 5 castles surrounded by serfs. reply helpfulContrib 14 hours agoprevIt is surprisingly easy to set up a phpBB forum or some such thing, on a tiny VM tucked into some obscure pool, set up a nameserver for it, invite a couple of your friends, and start your own community, completely disengaged from any major corporate shenanigans. Big things can happen with such a virtual machine. Even better, you could a mailman server, and get yourself a proper mailing-list of like-minded subscribers - for any subject under the sun - chatting away on a regular basis. Fact is, all it takes to set up a decent social network is to do some social engineering. Even real, meat-space activities can be completely divulged of their toxic mainstream membership in the info-spectacle industrial complex, and replaced with a couple smart services, on a tightly bound VM, somewhere functional. Big things happen because of the people, not the technology involved... reply sourcepluck 12 hours agoparentSounds very cool, I am interested in hearing more. I'm not sure how well I follow how one could set this up, basically. What constitutes a \"tiny VM tucked into some obscure pool\", what are you thinking of here? And are you saying this could work with a few technically-minded friends, or with anyone? It's hard to imagine it working with anyone. Not to be a negative Nancy or anything, I wish it were possible all the same. reply kristov 12 hours agoparentprevThe barrier to entry for running your own site (something dynamic, as opposed to static hosting) is essentially the same in 2024 as it was in 2004. Gotta get a domain (and keep it registered). Gotta have an SSL cert (letsencrypt makes it better). Raspberry pi can run a lot, but getting your consumer ISP to give you a static IP is still more money. Definitely more VPS options now though. But self hosting your still basically looking at the same experience as 20 years ago, with slightly better options. Ipv6 had this promise that everyone could have mutiple public ips just for themselves, but this hasn't really been realized to its potential for some reason. We never reached that point where average Joe can run their own web presence without relying on some provider or \"walled garden\" owning them. If your average Joe and you want to share your thoughts online, best option today is a walled garden. How could it be made better? Average Joe can't run mailman, and certainly can't run their own mailserver. reply autokad 13 hours agoprevnext [3 more] [flagged] akozak 13 hours agoparentGreat meta-comment: The attention economy has changed a lot over the last 2 decades. reply autokad 13 hours agorootparentits not about attention. it would take me 2 or 3 hours to read this. and frankly, it doesnt have that much content to it. Edit: its about 42,000 characters, the size of a small book. reply mrd3v0 14 hours agoprev [–] No, we don't. The issue was never and will never be the content, it is with the curation and mobility deeply and systemically taken over by social media and smart phone makers. If Google displays spam and AI generated nonsense, and almost everyone relies on it (including those who use \"alternative front-ends\"), then how do you expect anyone to find a healthy internet? Even if the \"extractive and fragile monoculture\" made up 0.001% of the internet, it doesn't matter if people can never reach the 99.999%. reply digging 13 hours agoparent [–] That's what the article is addressing. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The internet is criticized for being controlled by a handful of tech giants, hindering innovation and competition.",
      "A proposal to \"rewild\" the internet suggests decentralizing infrastructure, fostering competition, and dismantling monopolies for a sustainable and democratic online ecosystem.",
      "Embracing ecological principles like diversity and resilience is key to transforming the internet, with a focus on government regulations, antitrust actions, and collaborative efforts for inclusivity and diversity."
    ],
    "commentSummary": [
      "The rewilding the internet movement aims to recreate the early internet vibe with unique content, free from social media, focusing on self-hosting.",
      "Discussions cover challenges and advantages of personal websites, limitations of search engines, and the shift from niche to mainstream online communities.",
      "Participants emphasize reclaiming personal expression, community, and exploration in online spaces to detach from centralized platforms' influence."
    ],
    "points": 329,
    "commentCount": 196,
    "retryCount": 0,
    "time": 1713290251
  },
  {
    "id": 40059887,
    "title": "Stop Pretending to Be a Celebrity",
    "originLink": "https://ajkprojects.com/stopactinglikeyourefamous",
    "originBody": "Home Stop Acting Like You're Famous Advice for myself around leisure activities. You aren’t famous. Anything you do or create will probably receive little to no attention, so stop optimizing for a non-existent audience and instead focus on what makes you enjoy the activity. Want to try a craft or artistic hobby? Focus on mastering the skill and enjoy the variety it can provide. You don’t need to build a personal style. No one will care. Want to do photography and think black and white photos are cool? Great! You don’t need to create an Instagram branded all around your moody black and white photos. Most likely you’ll get bored of it and want to try a different type of photography, and that’s great. You aren’t Ansel Adams, no one will care if your “style” is all over the place. Do you want to build an app or website but don’t enjoy the process of designing? Then make it ugly. Who cares! Design is for an audience and you don’t have one. Functionality is more important right now. Maybe a designer will notice and want to improve it for you, but until then take pride in your crappy UI. Blogging is fun and therapeutic. Grammar and editing aren’t. As long as your thoughts are coherent, don’t worry too much about writing mistakes or filtering yourself. Just use Grammarly to fix elementary-level errors and move on. It’s more about the writing process than the final product. The most egregious thing you can do with any activity is daydream about how you can make money off of it. That’s the quickest way to optimize for the wrong things and suck the fun right out of it. Most likely you will stop doing the activity almost immediately, so save the money-making schemes for work. In the end, find something you enjoy doing and just do it because you enjoy it. If you have to, make some goals for yourself, but never for your “audience”.",
    "commentLink": "https://news.ycombinator.com/item?id=40059887",
    "commentBody": "Stop Acting Like You're Famous (ajkprojects.com)245 points by ashleynewman 7 hours agohidepastfavorite121 comments mapreduce 42 minutes agoWhy is it so that every so often one of these feel-good LinkedIn-style posts make it to the front page? Is there so much demand for banality on this site? I come here to read good tech articles or articles that stimulate my curiosity and it is sad to see these articles upvoted to the top when so many other good articles at https://news.ycombinator.com/newest continue to languish. reply blowski 11 minutes agoparentSimplicity bias, kind of like \"bikeshedding\". A large number of people can read this quickly, agree with it, and upvote it. Whereas articles about optimising machine code for the Apple Silicon CPU, or \"Risks to the Glen Canyon Dam\" are a lot more niche. Personally, I follow specific people who regularly submit interesting content, and pay less attention to the homepage. reply emotional_fool 6 minutes agorootparentCould you mind sharing those accounts? reply throwaway290 7 minutes agoparentprevAt first I thought it's generally good and can help with perfectionism and apathy, but the completely unnecessary plug of Grammarly put it in question. I'm on mobile but if there is any analytics tracking views (hard to say if it is server side) then I am positive the author gets paid reply Springtime 27 minutes agoprev> Anything you do or create will probably receive little to no attention, so stop optimizing for a non-existent audience [...] The most egregious thing you can do with any activity is daydream about how you can make money off of it [...] In the end, find something you enjoy doing and just do it because you enjoy it. I don't understand the equivalence the author is ultimately making in suggesting that creating something with polish is expecting to make money (and fame) from it. Yet notice all the things in the blog post are creations that are exposed to the internet (photography blogging, releasing a program online, making a website), rather than private, non-published hobbies which literally have no audience. Some prefer to make things polished/more complete as they want others to enjoy something more and polish makes it more accessible/usable/meaningful, or because it's a reflection on them (whether using an IRL identity or even a pseudonymous one) and part of their enjoyment of the process. Since we all know the audience is whoever will come across it, once something is released in the wild. Might be an audience of one, or many. Which isn't to say people need to care about the output of hobby but I disagree that money/fame is the only motivation in improving something. reply egeozcan 22 minutes agoparentI think if you like polishing things than do so. As far as I could understand, the point is not doing it with the expectation of any recognition. reply rndmio 13 minutes agoparentprevI would suggest you don't understand the equivalence because it's not there. They aren't saying that creating something with polish is expecting to make money off it. They're saying when you start a new hobby or interest forget about trying to tailor your output to an audience that doesn't exist and focus on skill. Maybe you really want to produce something polished, but the reasoning for that should be for your own development and edification not because you want it to appeal to others. Prioritise what makes you happy and gives you enjoyment not what you think other people want. reply pat64 3 hours agoprevI’m going to hard disagree with this. A lot of the enjoyment I get from creation is the process of others enjoying what I’ve built. Further more, building for others is great for building out areas you’re weak or inexperienced in. Like, I was poor on the accessibility front until I found the thing I created resonated with the visually impaired folk. reply bayindirh 2 hours agoparentI think you're agreeing with the article without knowing it. Because you're doing what you enjoy at the end of the day. For example, I design logos and small branding for my (mostly) CLI tools which I write for myself first. Seeing these projects at completion levels comparable with other, bigger projects brings a lot of joy to me. A coherent README.MD, nice documentation and good written code is what I aspire to do, and I do it for myself first. If others like it, that's great. If it doesn't get any attention, then it's OK, because I wrote that tool to fill my needs first. reply Retr0id 1 hour agoparentprevI only half-disagree. I also get a big kick out of sharing my work with the world. But I think it's quite easy to lose yourself in it. Whether you're conscious of it or not, you start optimizing for what you think the audience wants, and not what you want (which is what the article is getting at I suppose). So, I make a conscious effort to work on projects that are \"just for me\" from time to time, and I try to make that decision up-front. I think I get the most out of my \"for the world\" projects overall - it's where I really push myself, like you describe - even though they're \"leisure activities\". But I still need the just-for-me projects to stay sane. reply prmoustache 2 hours agoparentprevApparently you skipped that one: \"Advice for myself around leisure activities.\" reply spencerflem 3 hours agoparentprevdepends on the project for me, but I'm totally with you there's the things I do for me, because i would like for them to exist and have fun making it. But for anything that's not exactly that, having someone else care is extremely motivating reply matheusmoreira 42 minutes agoparentprevYeah. I've been a lone programmer for a long time. It's very difficult to maintain focus and motivation. Sometimes it feels like it doesn't matter and that there's just no point to it all. Yet people somehow find my work and tell me what they think of it. One day I came to HN and saw my project on the front page. At first I thought someone else had had the same idea as me. Then I started getting emails about it, about my website. Every time it happens it's incredibly motivating. It feels like I finally reached out to someone. Making things just for yourself and your own enjoyment can be a very lonely activity and you might find yourself with some kind of audience anyway even without trying. That experience can change everything. reply mixmastamyk 2 hours agoparentprevThis frivolous article is not fodder for \"hard disagree\"-ment. reply d--b 1 hour agoprevOk the most important sentence in this essay is its subtitle: “Advice for myself around leisure activities” Personally I am no perfectionist at all, but I don’t see the fun in making stuff myself that I could otherwise buy. I took up sewing, not because I want to sew the perfect shirt, but because men’s fashion sucks. That said, I sew stuff I can wear. So it needs to look at least as good as what I could buy. I don’t think that it’s “acting like being famous”. Similarly, I am writing a screenplay, because I have a lot of experience reading bad screenplay that were actually made into movies, and I think I can write one that is at least as good as the worst ones I read. I don’t paint or take photographs because I know mine will look terrible. Maybe that would be my advice in taking up hobbies: aim to be better than the worst people who do it professionally. reply brandall10 6 minutes agoparent\"I took up sewing, not because I want to sew the perfect shirt, but because men’s fashion sucks\" I applaud taking up this skill, but there absolute is stellar men's fashion out there, it's just not outwardly public. I was a member of https://www.styleforum.net for many years and highly recommend it. And you can get MTM custom clothing on the relative cheap. I've used https://www.divij.com/ in the past, a small family run business where they book appointments in major cities to do measurements and provide sample books so you can inspect the fabrics in person, then order online any shirts and suiting. reply atoav 1 hour agoparentprevThis is good advice, I'd like to ad one caveat tho: It is not a good idea to base your decision whether to start doing a hobby on your own perceived ability to get good at it. For one the much more important metric is whether you enjoy doing it, because ultimately you are doing it for yourself. Secondly, as an educator I have to say that many people absolutely suck at predicting their own inability to learn a thing. There are many people who say they will be bad at $X and because of that prediction they avoid doing $X, which in turn is the reason they can never become good at $X. It is much better to just take the gift that Punk culture has given us and focus on finding A) joy in doing things even if you are not good at them and B) finding your own way of doing them, because for many hobbies there just isn't one objectively good way of doing them. reply bloak 16 minutes agoparentprev> Maybe that would be my advice in taking up hobbies: aim to be better than the worst people who do it professionally. Then I hope you don't count sports as hobbies because being better than the worst people who do it for fun is a stretch goal if it's me doing any competitive sport. reply selestify 1 hour agoparentprev> Maybe that would be my advice in taking up hobbies: aim to be better than the worst people who do it professionally. I guess this is my gripe with advice in general: Why should anyone else make that their aim? It’s great that it works for you, but I don’t think that’s applicable to me :) reply humbleferret 1 hour agoprevAsh's main takeaway is solid — Immerse yourself in hobbies or creative pursuits without being overly concerned about external validation or the potential for monetary gain. Other takeaways that stuck with me were: — Finding enjoyment in the process of learning and improving your own skills is crucial. — Setting personal goals can help fuel growth. — Sharing your work with others is a way to receive feedback and learn from other perspectives, but don't let pursuit, perfection or seeking monetisation overshadow the joy of the activity itself. — Intrinsic rewards of your hobby or pursuit trump validation or financial gain. In the end, you need to find something you enjoy doing, and do it because you ENJOY it. reply resource_waste 57 minutes agoparent200 years later, people rewrite Nietzsche. reply keithalewis 30 minutes agorootparentOr the more practical, and hilarious, J. P. Donleavy. \"Scrub your floors!\" reply TrackerFF 4 minutes agoprevSoftware engineering aside, I've definitely noticed that in other sectors - especially on the creative side of things - there's this expectation of you not only to be a good craftsman, but also have a brand. Be a personality. reply keiferski 2 hours agoprevI haven’t studied the topic enough, but it would be very interesting to see when this dichotomy of money-making vs. self-interest really embedded itself into the act of creation. Somewhere during the Industrial Revolution, I suppose. But I also think the default mode of “art as self expression” plays a big part, and that’s more early-mid 20th century. Because when you read about creators during say, the Renaissance, you don’t really have this much of a dichotomy. Da Vinci worked on a paid portrait project, and then did unpaid experiments on his own which ended up being useful for his paid projects. It was a very loop-like thing and I think he would find the explicit framing of “I’m doing this to make money” and “I’m doing this purely to create something I want to create” as alien. Ditto for most forms of art in most parts of the world, prior to the late 19th century. The solution, I think, might be to focus primarily on the craft and not on the end product. You see this a lot with early 20th century fiction writers that moved in and out of journalism, with the idea that they were becoming better at the craft of writing, not at creating a final product or “being a good fiction writer.” reply Narciss 15 minutes agoparentWhilst visiting Florence, I found out that Michelangelo is broadly considered the first artist to set his own price for his work. Before him, the patron would go “I need this cathedral painted” and the patron would also decide how much to pay the artist (generally they’d be paltry sums). With Michelangelo, the patron would go “I need this cathedral painted,” and Michelangelo would go “sure, that’d be 400 golden florins, take it or leave it.” There are stories of him not delivering his work when the patron decided to change the price after the fact. On the topic, I think that if the “money-making” bit is defined broadly enough, then it merges very well with self-interest. Like if someone asked me to make a remix of a song, but then how I’d do that is left completely up to me, it’s a broad enough task as to feel like I’m in control. At that point, there’s very little of the feeling of “I’m doing this for money.” reply timwaagh 2 hours agoparentprevIn the sense of the article artists have always done their best to cater to the taste of the people who might pay them. At least from the 16th century. They were typically paid for and protected by a mecenas (wealthy Merchant or nobility). There are no doubt exceptions but in general the art was to please their mecenas. reply keiferski 2 hours agorootparentRight but (and I could be wrong here) it seems like lamenting this is largely a recent thing. Renaissance artists were focused on creating the best possible work, not lamenting that they had to make paintings for money and not for their own desires. reply dommrr 1 hour agorootparentYou are making quite the assumption there. There are quite a few among us focusing on creating the best possible work. And there are quite a few back then who did the opposite. Wouldn't you wonder where the conception that things were different came from? reply keiferski 1 hour agorootparentAs I said, I could be wrong. If you have an example of artists in the distant past lamenting the fact that they can’t do what they want and instead must make art for money, I’d be glad to read them. I didn’t say people today aren’t creating the best possible work, I said this focus on the juxtaposition between the market and the self seems like a recent thing to me. reply detourdog 58 minutes agoparentprevI would place somewhere between the 1970s and 1980s. Prior to Reaganomics I remember it being pretty easy to get by on very little money. The go go eighties really changed materialism from my perspective. reply keiferski 45 minutes agorootparentIn the fine art world, definitely I agree. That’s when Gagosian etc. really started taking off and making the fine art market a thing for billionaires. reply level87 4 hours agoprevAgree with focusing on doing it because you enjoy it, something gets lost when we try and impress others; I'm sure we can all remember being a child and doing things purely because we enjoyed it. However, I disagree with the personal style part of things, or trying to make things look good. These things don't have to be about impressing an audience. It can be just as much about enjoying the process. reply peddling-brink 3 hours agoparentThe way I read “personal style” was, don’t make it your whole personality. And making things look good is in the eye of the beholder. If you like design and want to make pretty things, do that and don’t worry about the criticism. For me, pretty is my code, I couldn’t care less about the UX because I’m the only user. reply Gualdrapo 3 hours agorootparentBut the second you want to make things for anyone else is when UI/UX matters. Some people (and many people on HN) take graphic design for granted, but it's the first thing they seem about your product. It matters. Your app can work flawlessly but nobody will use it if the text has poor contrast or the buttons are comically small, for example. reply anonzzzies 2 hours agorootparentPeople say this but I never saw it matter like you say. I know many ux/ui people in my network who I ask for feedback and help; I have never seen any difference in uptake from the vanilla theme version I did myself through to months of tweaking these guys did. Sure it looks tons nicer but it doesn’t reflect at all in (measured) user satisfaction, signups or usage. The default themes these days (shadcn etc) don’t make any of the mistakes you mentioned and users that are not obsessed with tech don’t really care ‘it looks like everything else’. Maybe it’s because I never do b2c and only b2b, but I never saw the difference, not in the last 30 years anyway. Even when these design systems and widgets etc didn’t exist, people didn’t care because there was nothing better; now there is ‘more than good enough by default’. reply prmoustache 2 hours agorootparentprevThere are myriad of software that have been super popular despite having dodgy UI. But the whole point is not to care if your app is popular or not if the whole point is enjoying the process of building the app more than seeing it used by many. reply spencerflem 3 hours agorootparentprevthat's the point of the article- that you shouldn't be worried about making things for something else if they're unlikely to care either way with that said, I agree with your overall point, if you're determined to make something popular you shouldnt skimp on design reply devjab 3 hours agoparentprevI think the sentiment is ok, but like you, I think the overall message is completely nonsense. It’s obviously fine to do things you aren’t enjoying as part of a process of to achieve your goals, and that doesn’t need to be about outside validation at all. I’m not very good at design, I don’t too much enjoy the process. Well I do enjoy parts of it when the hyperfocus sets in, but as a whole I don’t enjoy the process. I still do it, not because I care what anyone else will think about the end result but because, I, care about what, I, will think about it. I’m sure the author is doing some sort of simplification of things. A lot of learning processes aren’t necessarily enjoyable and almost none are enjoyable all the time. I spend years learning how to airbrush while absolutely hating the process because I wanted to be able to do certain things. Now that I can actually make the stuff I envision I enjoy the process, but sucking at the beginning? Yeah that sucked. Hell, even if your end goal, is, outside validation… go for it! But I do agree with the whole “life is short, so what you love” sentiment. It’s just that you could put it so much better and less condescending than the author does here. reply geoffyoungs 3 hours agorootparentIt might be better to interpret in the context of the subtitle: “Advice for myself around leisure activities.” If my advice is to myself, I don’t see how it is condescending. It seems by definition that it can’t be. I cannot pretend to be above me. My summary of the sentiment would be “don’t allow the weight of imagined judgmental eyeballs to steal your joy in trying or pursuing your personal creative endeavour” There is an irony in the blog now being seen at HN scale and judged. reply asta123 2 hours agoprevMy son put effort into dressing up for his Year 10 formal as that is what you do. But during the event he observed that most kids focused on how they looked rather than noticing others, and thought he should have spent a lot less effort into his outfit :) reply m463 2 hours agoparentI think kids are constantly comparing. Watch how many of them are buying exactly what their friends buy, so they can fit in and relax a bit. reply damsalor 2 hours agorootparentNot just kids reply watwut 2 hours agoparentprev> But during the event he observed that most kids focused on how they looked rather than noticing others Genuinely, that is good. People who care and judge how others look beyond normal social propriety tend to be pretty bad to be around. reply loup-vaillant 2 hours agorootparentOr that's bad, because all those kids who spent time looking good, didn't get as many compliments for their effort than they expected, or perhaps even deserved. There are many caveats with judging people by their appearances, but complimenting someone on the part of their look they put unusual effort in for some unusual occasion, seems pretty healthy to me. (Well, there's also the caveat that the rich ones will have better access to exquisite clothes to begin with…) reply jshorty 2 hours agorootparentprevI think the point is more around the collective distraction of dressing to a formal (and for kids, unfamiliar) standard, rather than enjoying time with the people around you at a party. reply VelesDude 3 hours agoprevFor there to be famous people, there has to be non famous people. In the same way that up needs down, black needs white. A figure I saw once was based on \"do they have a Wikipedia page\" as counting as famous. And the ratio was something like 50,000:1 relative to the population. Would you bet your lives actions on a 50,000 to 1 chance? And even then do you think it would be possitive? Sometimes fame is the worst thing that can happen to someone. Being anonymous can be a blessing in disguise. reply prmoustache 2 hours agoparentI would totally hate being famous. reply fcatalan 1 hour agoprevThis resonates with me. I can't have relaxing hobbies: I take them up, find some measurable/competitive/social aspect in them, smash my way from \"rookie\" to \"advanced beginner\" to \"top 20 percentile\" in very little time and then agonize about the ensuing plateau and how getting into the top 1% would require complete dedication or might be realistically out of reach for me. By that time I stop enjoying doing whatever the thing is. Not fun anymore. People will sincerely praise me and it will feel empty because I know there are millions of better painters, my laptimes are a full second off the ultimate pace, my guitar skills only good enough for playing alone in my office, my leisure programming projects all pointless and abandoned. I envy two kinds of people: those that have found some thing they are very good at and keep enjoying it forever, but also those that can enjoy something for years even if they are realistically mediocre at it and never improve. And the money making part is also true for me. I took up miniature painting and quite soon was at the level where people will pay you decent money to paint their miniatures for them. I started getting offers and accepted one, not for the money, probably just out of pride. It was complete hell, I hated the result and every minute I spent painting it. The client was happy, me, I guess I learned my lesson: never again. reply _glass 1 hour agoparentThanks for giving me ideas to add to my 200 hobbies, miniature painting it is. I even have an app (Streaks) that reminds me daily of thousands of things I want to do, and it just shows how unrealistic it is. I want to concentrate on just two things, but then I hate to plateau on the other skill. Right now I concentrate on guitar and trumpet. But my painting and drawing skills are really not that good anymore. I try to also publish some academic papers, repair my bike, lift weights, run, get better at theorem proving, read into all of the social sciences, read philosophy (Kant, Hegel), experiment with Arduino, build a repertoire in Rebetiko ... If you find a trick to just do one things, tell me. reply bayindirh 1 hour agoparentprevWhy not apply kaizen to your hobbies? 1. Do the best you can. 2. Identify the weakest part of the artifact you just made. 3. Design an improvement to that part only. 4. GOTO 1. This will make you cherish the progress and only \"compete\" with yourself, on your terms. reply helboi4 44 minutes agoparentprevBruh I am this exact same person. I am mediocre enough at everything I do to impress laymen but leave me feeling like shit because I know anyone who knows the craft will think I suck. I have the ability to learn up to intermediate stage of pretty much anything way faster than most people but I just stress myself out and there's always other fish in the sea for me. reply nickelpro 1 hour agoprevHard disagree with this as a polemic When I take photographs of my friends, it is incredibly important to me that they be in focus, sharp, with a good depth of field bokeh that brings out their face and presents an attractive image. I take a huge amount of pride when a photo I shot ends up as a profile picture or widely shared. That's a large part of why I take them, to share with others. Got my current job because of my public code and the quality of articles I have written on subjects relevant to the employer. When I interviewed they largely skipped the technical parts and focused on cultural fit and the kinds of projects I wanted to be involved with, because my publicly demonstrated track record left no question about the quality of my work. It is fine to have some activities you enjoy without perfectionism, but there is a world of advantages that can come from a focus on quality. reply yawboakye 1 hour agoparentyou talk about something else. sometimes, in fact most times, this imaginary and large audience (who also happen to be great critics) can beat anyone into inaction and kill whatever little flame we were nursing. it’s all in our heads, which means we end up defeating ourselves. it isn’t an ode to mediocrity, imo. if anything, practice and more practice is the best cure for mediocrity. reply titchard 54 minutes agorootparentI think this is spot on - how many times have people (and I include myself) stopped on a project that is even remotely public facing because it isn't perfect-grade work before you've even learnt the first steps? reply zjp 3 hours agoprevI recommend against Grammarly because I like when I see peoples' idiosyncrasies and it's little fuckups that move language forward. reply sph 3 hours agoparentI defiantly do not want to see people's typos move the language forward, especially when it's native speakers always making the same silly mistakes, for some reason (could of, they're/their, your's, etc.). I do like non-native speakers translating and incorporating their local sayings into English prose. reply snakeyjake 2 hours agorootparent> people's The possessive apostrophe originated as a mistake or idiosyncrasy, credited to one of two people in the early 1500s depending on who's making the assertion, that became widely adopted. Possession should be, in static, unchanging, OBJECTIVELY CORRECT DON'T YOU DARE GO CHANGIN IT English, written \"peoplees\" (or something like that but you get the point). Merely calling \"'\" an \"apostrophe\" was a mistake for over a century, as the word was a well-defined rhetorical term that was later adopted to describe the mark sometime during the mark's slow acceptance. Grammarly makes people sound like soulless automatons who have been trained to write by similarly soulless and robotic corporate ad copy writers. Sometimes it seems like half the English language is just Shakespeare or some other writer making up shit that sticks-- and that's awesome. reply mns 37 minutes agorootparentprevBut that's how languages evolved and will keep evolving. Whatever you take now as rules and whatever you write now thinking to be correct, was probably a mistake, shortcut or misunderstanding ages ago. reply Hackbraten 2 hours agorootparentprevIf enough people are making the same mistake, won't it eventually stop being a mistake? reply sph 2 hours agorootparentNot until my dying breath. The English language is a mess because of its haphazard evolution mostly driven by immigration over centuries. Which is what bugs me about native grammar mistakes: only native people make them. No one that has learned English as second language could ever construct \"could of\" as it makes no sense. And the act of being defiant is very very different than being definite about something. Yet people get this wrong all the time, as if they never learned grammar at school, or let alone read ONE book. (My pet peeve is native speakers unable to pronounce \"aesthetics\" correctly. Drives me nuts. ) reply VelesDude 1 hour agorootparentI do see where you are coming from but alas, language is an ever moving democracy. As much as many would like to define it in certain terms - it is largely beyond control. This is why the English of Shakespeare doesn't hold up today because we are constantly adding and changing these things in a wonderfully organic fashion. It just makes it difficult to define. The question is should we define it or is it like catching the wind with a net? Another example is the word Monetize. It used to mean to turn a item into a form of money like currency. Almost nobody uses it like this nowadays. Decimate is another one. reply curtisblaine 1 hour agorootparentDecimate meaning \"kinda reduce the number\" instead of \"kill one person in 10\"? I think it's been used with the first meaning in every language (including latin ones) for a long while. reply curtisblaine 1 hour agorootparentprev> Which is what bugs me about native grammar mistakes: only native people make them Why does it bug you? They are different classes of mistakes but both have driven the language over the centuries. Why are native mistakes wrong but immigrant mistakes good? reply keiferski 2 hours agorootparentprevIf everyone started putting ketchup into their coffee, would that make it a good thing? Some notions of quality are not dependent on their popularity. reply dgellow 1 hour agorootparentYou can make a really ugly, low quality change to a language, wait for a new generation to grow up with it and it will now be accepted as perfectly fine. There isn’t any objective notion of quality here reply keiferski 1 hour agorootparentThat doesn’t imply it’s perfectly fine, it might just mean that the arbiters have lost their ability to detect quality. Which is exactly what I think has happened. reply VelesDude 1 hour agorootparentprevI would have considered the use of the Ad populum falacy more for use in terms of testable facts rather than opinions. Is quality of language and taste opinion or fact? I could see the debate being vigorous on that one. reply keiferski 1 hour agorootparentIt’s a big debate indeed and I don’t want to get into it here, but I think I come down on the side of, “some standards are not purely popularity contests, but are based on other things.” There are a lot of reasons I think this way, but even if someone doesn’t agree, I do think a purely consequentialist approach is illustrative. Would we have better food if the top chefs in the world designed our meals, or if the entire population voted on them? For some topics (including the arts) I think a purely subjective approach has worse outcomes. reply __MatrixMan__ 2 hours agorootparentprevDepends on the mistake, some constructions just fail to do what the author wanted them to. Those are unlikely to catch on. reply kynetic 1 hour agorootparentprevLiterally, yes. reply zjp 2 hours agorootparentprevI saw what was deleted out of this. ;) reply sph 2 hours agorootparentIn my defense I just woke up, did not understand what you were referring to and did not use Grammarly. (I originally corrected the use of \"it's\") reply tdeck 2 hours agorootparentprevNo, they meant \"it is [the act of making] little fuckups\". It's is correct. reply bayindirh 3 hours agoparentprevI use Grammarly, but seldomly. It's useful for longer piece of writing, esp. if I don't want to edit it later, over and over. Other than that, in normal conversations, these mistakes are part of our personal identity if you ask me. reply mixmastamyk 2 hours agoparentprevIt's also a surveillance capitalism product, beware. proselint is BSD-licensed. reply trueismywork 2 hours agoprevEven Terence Tao advises mathematicians to develop their own personal style when writing mathematics. Your own personal style, based on your experiences, is what makes you unique and hence might make you famous. You might not become famous by developing your own style but you'll definitely not be you if you don't. reply nicbou 3 hours agoprevThis is excellent advice, but there is one exception: on the internet, behave as if your username was your real name and everyone was in the room with you. Don’t use the cover of anonymity to be mean. Act as if people knew you and remembered you. reply bee_rider 3 hours agoparentAlso these accounts will probably be de-anonymized at some point, between leaks and style matching. We aren’t famous, so mostly it is the same as anonymously yelling on a crowded street corner, but then most of our IRL interactions are similarly anonymous-by-obscurity… reply Joel_Mckay 3 hours agorootparentlol, many wrongly assume the convenient services haven't made billions off those that assume any anonymity exists in their metadata. Prior to ML, it was computationally unfeasible to develop speculative dossiers on the majority of populations. Best not think too deeply on the matter... Have a wonderful day =) reply b33j0r 3 hours agoprevSo, if I’m hearing the zeitgeist correctly. Currently, the best competing advice I hear along venn intersections is: 0. Do less things 1. Do things at a natural pace 2. Obsess over quality 3. Don’t obsess over quality, eff the haters! 4. Do more things ;) reply Arcanum-XIII 3 hours agoparentUhmm, the correct way would be: 0. focus on less thing; 1. have fun doing them; don't stress about it, you're not competing; 2. do it, more and more. The important point is to DO. Everything else will come, eventually. reply b33j0r 2 hours agorootparentTouché. It’s just fun to watch this attitude towards productivity go in cycles. Yesterday they recommended that it was a glass of wine, and before that two glasses of red, then a bottle of white. We kinda always knew it was wine, and like… stuff we can abstract away about blood pressure, moderation, hangovers, fond memories, and whatever, uncle Conrad. No, we solve that particular thing like all things. But as a consumer, you’ve gotta average out the signal of strong claims. “Eff it!” and “Obsess over it!” are diametrically-opposed opinions about work, which I suspect many of us agree with both, a bit. We try to explain it over and over again as each generation finds the same problems with new tools. Maybe it’s not that bad—but hey, at least my commentary was coherent enough to post ;p reply VelesDude 3 hours agoparentprevIt is a strong theme of taoism that in giving up on clinging to a set goal, it allows you more flexibility to do great things when needed. reply tiborsaas 28 minutes agoprevTitle should have been: \"You can still enjoy building things regardless of how many followers you have\". reply aerhardt 3 hours agoprevI had a goal of programming every morning for an hour, and it wouldn’t stick… Once I dropped any pretense of making money, or getting anything out of it other than enjoying the craft, it has finally stuck. reply VelesDude 1 hour agoparentThere is the idea that I kind of subscribe too but not entirely, that if you want to improve yourself - you would just do it rather than force it. In that trying to force it is like Sisyphus. In giving up the battle, you are freed up to actually do it. So your exaple is great, you had a good intention but once you gave up that structure then you actually got to the goal. reply imhoguy 1 hour agoprevWell, there is also a thing called reputation, especially when job background checks include internet/social-media scan these days. So I try to build trashy things under my anonymous nickname(s), it is much more fun like in 90s. reply boo-ga-ga 1 hour agoprevThis really resonates with me. Of course it's great to create something that other people enjoy, share it with them, get feedback etc. But it also might create a pressure that will strip off all the pleasure from the activity completely. So for myself, I decided to share my creations and welcome feedback, but also see the main goal in creation itself. reply resonious 3 hours agoprev> ... stop optimizing for a non-existent audience and instead focus on what makes you enjoy the activity. What if I enjoy optimizing for a non-existent audience? reply nurettin 2 hours agoparentYou are not alone. reply Techbotch 1 hour agoprevI disagree to some extent. E.g. I like to tinker, repair and build stuff and then blog about it. I put quite some effort into the writing part and every finished post gives me joy. Blogging has become a hobby in its own right. It also helps with getting projects done, because now the project is finished only when the post is out. All of my three regular readers like the results, too. Of course, earning money from that never crossed my mind – in that respect, I fully agree. reply etrautmann 4 minutes agoparentSeems like the blogging is still primarily motivated by your own enjoyment which makes perfect sense and seems to agree with the article. reply nercury 2 hours agoprevThis sounds like \"You aren't having fun properly\". reply amadeuspagel 1 hour agoprev> The most egregious thing you can do with any activity is daydream about how you can make money off of it. That’s the quickest way to optimize for the wrong things and suck the fun right out of it. Most likely you will stop doing the activity almost immediately, so save the money-making schemes for work. People will even ask you about this. reply rsynnott 1 hour agoprevMinor gripe, but I don’t love the setting off of design against functionality. UX Design isn’t just about aesthetics; a good design makes an object, or piece of software, or whatever, more functional. reply Aromasin 3 hours agoprev> The most egregious thing you can do with any activity is daydream about how you can make money off of it. That’s the quickest way to optimize for the wrong things and suck the fun right out of it. Most likely you will stop doing the activity almost immediately, so save the money-making schemes for work. I disagree wholeheartedly with this statement. It implies several things that aren't true: * That a hobby done for profit that can't also be done for fun. * That a hobby for profit can't start as a profit-making venture, but turn into a passion. * That work should be the only route to wealth. * That optimizing for wealth can't go hand-in-hand with fun. I despise the adage that a hobby is only a hobby if you aren't making money from it. I'm passionate about fantasy/sci-fi miniature building/painting, terrain modelling, and prop making. I love the expression of taking a universe that exists within the realms of novels and movies, and bringing it to the real world - to scale or in miniature. I design STLs/CAD models for 3D printing, scratch together terrain boards for people to play games on, paint miniatures any hour I get free, machine parts for various outfits and armaments, and spend hours fantasising about what universe I'm going to delve into next. None of that would be possible if I didn't monetise the process. Most of what I build, I sell. If I didn't, I would neither be able to afford the hobby nor store the stuff I make. It would end up in a landfill. Parts of the hobby I took up explicitly because they demand higher prices when I sell it, but now they're some of the things I'm most passionate about. Realistically, I'd love to do it as a full-time venture, but the semiconductor industry pays well and I'm not a famous maker so couldn't make it work - as the article states well enough. To suggest that hobbies can't both be fun and profitable though is a philosophy I think should be quashed. reply Arcanum-XIII 3 hours agoparentAnd after a while, your hobby if physical will take space for things that are done, and that you don't need anymore. So why not resell them and fund the next steps ? reply Aromasin 2 hours agorootparentYou raise another good point; is all monetization of a hobby the same? No, of course not. It's such a blanket statement to say \"Save the money-making schemes for work\". Plenty of people have collecting hobbies that almost go hand-in-hand with monetization. Stamp or coin collecting comes to the top of my mind. Value is discussed almost constantly in those communities, and people are forever uptrading their collection to become more valuable over time; it's an investment as much as anything else, and while you will seldom make money there's always an element of minimizing loss. Same with cars, video games, cards, records, and comic books. We collect them because we think they hold value, join communities that also think they hold value, and use money as a scale to judge what value it holds. reply imiric 2 hours agoprevI generally agree with the sentiment of doing what you enjoy first, and not thinking about an audience that may or may not exist, but the suggestions themselves will vary from person to person. > Design is for an audience and you don’t have one. It's wrong to generalize like this. Good design drives your work forward, and if you enjoy doing it, then by all means focus on that first. Not appeasing an audience even when you have one is also a good idea. Art is an expression of the artist, and it dies once it starts being created for an audience. reply hiAndrewQuinn 2 hours agoprevThis is probably good advice for someone else. For me, life got both a whole lot more interesting and a whole lot more fun when I started acting like I was famous. reply tayo42 2 hours agoparentWhat do you do to act famous? Obsess over your Instagram image and dress covertly when you go out grocery shopping? reply kunley 2 hours agoprevGood article, there is a certain level of being tied by what others might think while they don't give a damn actually. I guess this is somehow inherent to the US society (I am from Central Europe and don't observe this things at such scale) and it would be actually interesting to scientifically track the origins how it went this way reply wolframhempel 3 hours agoprevI feel, the current generation of AAA Games are a great example for why you should make something you enjoy rather than something designed to make money. Look at Baldurs Gate 3 vs Skull and Bones. Make something you love and are passionate about. Get good at it. Use the Internet to show it to others that are passionate about the same thing - and there's a decent likelihood of success. reply spencerflem 3 hours agoparentBaldurs Gate 3 is not really an indie passion project labor of love, its \"just\" a really good game. to me that's the big problem with AAA is that a lot of the time they're not even good reply wolframhempel 3 hours agorootparentFair enough - let's take Stardew Valley as an example then. reply sidcool 2 hours agoprevIs it bad if I yearn and pine for fame? I know it's frowned upon, and I bury the urge for fame, but deep down, I am hooked to other's validation and fame. reply VelesDude 1 hour agoparentI wouldnt say it is bad but be careful of what you wish for. On the flip side if someone is merely after fame that is not such a bad thing by comparison. It is not like those that seek wealth and power, it is at least going a more ethereal and potentially much less destructive goal. Having lots of fame doesn't necessarily take from others. The problem with a lot of fame is that the more of it you get, the more decided people will be. A good modern example would be Taylor Swift. Some people love her, many don't even notice but some call her Taydolf Swifler. Turns out you cannot get just the good, you have to have the bad come with it as well. Unless you are Weird Al, everybody loves him! reply shymaple 3 hours agoprevAwesome blog, I just loved the way wrote it. And I am totally agree with you. We don't need to be perfect or try to make perfect at first attempt, the more important is to enjoy the journey and honing the skills. reply emp_ 3 hours agoprev> daydream about how you can make money off of it. I do this but mostly from trauma of having to kill things due to hosting costs (happened before), as long as its cheap/self sufficient enough the fun part dominates. reply daghamm 3 hours agoprevThat's easier said and done when half of the population between 15 and 45 are addicted to Instagram. Even if a small percentage of them, say 5%, decide to emulate the famous people they follow we will have too many people who believe they are important. reply onion2k 3 hours agoparentInstagram likes and GitHub stars are exactly the same mechanism as far as someone who owns the content is concerned. Doing something 'to get famous', be it for your pouting selfies or your open source work, is basically the same, and anyone making a repo for the wrong reasons should heed the advice in this article. It is directly applicable to what they're doing. reply tayo42 2 hours agoparentprevInstagram really does seem to give some people delusions or let's delusional people really embrace their disconnect from reality. It's so weird some of the things people do. reply bitwize 2 hours agoprevMain character syndrome is a disease. But you can get help. Call the number on your screen. reply fedeb95 2 hours agoprevas Marcus Aurelius reminds us, the applause of the crowd matters very little since our life is very short and death will cancel it anyway for us. reply anonzzzies 2 hours agoprevI have done this all my life (which is also ‘my career’); it paid off accidentally big time in the beginning (90-00s) which made me enough to retire, but I like what I do (as per the article). Now I still work the same way but the money just isn’t there anymore, and the only thing to at I don’t do vs people who make a lot worse stuff but make millions is: get out there and act like I am a rockstar/musk. I hate that social media posing but it seems it’s the difference; we all have seen products here on hn that were absolute garbage but because the creator is acting like some football hero who just scored, many people go for it and they get subscription payments and vc moneys. I won’t change my ways as I have enough money, but I would be quite… not happy starting out in these times and having to pose and fake until I make etc. reply ionwake 2 hours agoparentSounds like a SF VC domain thing, I’m sure BS doesn’t work in other realms. Or was this early success in a different area? reply redder23 55 minutes agoprevI actually have the opposite problem. I think it would help me if I would act MORE like I was, well I would not say famous but like promoting stuff, talking about it. Marketing myself and by business, just \"presenting\" myself. It does not help someone create the greatest thing ever in silence when nobody will ever know about it. reply Joel_Mckay 3 hours agoprevUnfortunately, for some people an illusion of themselves is all they have left: https://en.wikipedia.org/wiki/Narcissus_(mythology) Postulating intellectual artifacts somehow bring contentment also can become unhealthy. As some folks spend their entire lives solving a civilizations perceived problems, and only later conclude most of the planet just isn't worth saving... if one becomes hapless as a consequence. One may disagree, but that is an indulgence youthful idealism often prescribes. In conclusion, goldfish crackers are awesome... =3 reply Animats 3 hours agoparent> In conclusion, goldfish crackers are awesome... Until the cheese mafia got to them. It's hard to get them without cheese now. reply hoseja 2 hours agoprevLiving in the panopticon, it's really hard to take this advice to heart. You have to self-censor all the time as if in front of large audience, the leap to believing that audience is interested in the positive aspects of your output, to make the constant vigilance worth it, isn't large, however delusional it may be. reply radres 3 hours agoprev [–] Anyone knows how I can get the same style of this blog? I can't access the css for some reason /s reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "When engaging in leisure activities, focus on enjoying the activity rather than trying to impress an imaginary audience or act like a celebrity.",
      "Master skills, experiment, and prioritize enjoyment over developing a personal style or monetizing hobbies.",
      "Simply do what you love for the pleasure it brings without the pressure of fame or financial gain."
    ],
    "commentSummary": [
      "The discussion on ajkprojects.com explores various topics like the motivation behind creating polished content and the challenges faced by solo programmers.",
      "Emphasis is placed on enjoying creative pursuits, balancing personal and audience-driven projects, and the value of authenticity in personal and professional endeavors.",
      "The consensus reached is that creating should stem from personal enjoyment and self-improvement, rather than seeking external validation or financial gain."
    ],
    "points": 247,
    "commentCount": 122,
    "retryCount": 0,
    "time": 1713321898
  },
  {
    "id": 40061068,
    "title": "Covid-19 Infections Linked to IQ Drops and Brain Aging",
    "originLink": "https://www.cbc.ca/radio/quirks/long-covid-brain-1.7171918",
    "originBody": "COVID infections are causing drops in IQ and years of brain aging, studies suggestCBC Radiowindow.__CONFIG__={\"Client\":{\"contentApi\":{\"baseURL\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Faggregate_api\\u002Fv1\\u002F\",\"baseGraphqlURL\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fgraphql\",\"contentList\":{\"lineupIds\":{\"news\\u002Fmulti-lineup-listing\":\"2.9543\"}}},\"detailApi\":{\"previewImageBase\":\"http:\\u002F\\u002Fpolopoly.nm.cbc.ca:8080\"},\"environment\":{\"domain\":\"www.cbc.ca\"},\"path\":{\"root\":\"\\u002Fa\"},\"searchApi\":{\"baseURL\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fsearch_api\\u002Fv1\\u002Fsearch\",\"autocompleteURL\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fsearch_api\\u002Fv1\\u002Fautocomplete\"},\"stats\":{\"urlTop\":\"\\u002F\\u002Fwww.cbc.ca\\u002Fg\\u002Fstats\\u002Fjs\\u002Fcbc-stats-top.js\",\"urlBottom\":\"\\u002F\\u002Fwww.cbc.ca\\u002Fg\\u002Fstats\\u002Fjs\\u002Fcbc-stats-bottom.js\",\"urlABTestsConfig\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fnetwork\\u002Fincludes\\u002Fstats\\u002Fab-tests-config.json\"},\"cookieJar\":{\"baseURL\":\"\\u002F\\u002Fwww.cbc.ca\\u002Fcookie-jar\"},\"scheduleApi\":{\"sportsBaseURL\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fsports-content\\u002Fv11\\u002Fincludes\\u002Fjson\\u002Fschedules\\u002Fbroadcast_schedule.json\"},\"login\":{\"viafoura\":{\"loginradius_api_key\":\"3f4beddd-2061-49b0-ae80-6f1f2ed65b37\",\"loginradius_app_name\":\"cbc-login\"}},\"membershipApi\":{\"baseURL\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fapi\",\"middlewareURL\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fmembership\",\"loginradiusAppName\":\"cbc-login\",\"loginradiusDomain\":\"https:\\u002F\\u002Flogin.cbc.ca\",\"loginradiusApiKey\":\"3f4beddd-2061-49b0-ae80-6f1f2ed65b37\",\"cbcDomain\":\"cbc.ca\",\"recaptchaPublicKey\":\"6LcvyCsjAAAAAOx_jCT1tUNspqHaTpRqBLPU3XEJ\",\"socialLoginApple\":\"apple\",\"socialLoginAppleNews\":\"apple_cbcnews\",\"socialLoginAppleSports\":\"apple_cbcsports\",\"socialLoginAppleListen\":\"apple_cbclisten\"},\"newsletters\":{\"ApiUrl\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fapi\",\"recaptchaPublicKey\":\"6LebIREmAAAAAIqgeE35Xv2jdca9DTeeoWqEVJur\"},\"membership\":{\"baseURL\":\"https:\\u002F\\u002Fwww.cbc.ca\",\"root\":\"\\u002Fg\\u002Fmembercentre\",\"localURL\":\"http:\\u002F\\u002Flocal.cbc.ca:8081\",\"localRoot\":\"\\u002Fa\",\"privacyPath\":\"\\u002Faccount\\u002Fprivacy\\u002Fcbc-and-your-data-1.5514726\",\"offersPath\":\"\\u002Faccount\\u002Foffers\\u002Fcbc-your-account-perks-1.6662018\",\"subCentre\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Flistmanagement\"},\"uieApi\":{\"baseURL\":\"https:\\u002F\\u002Fuie.data.cbc.ca\\u002Fv0\"}}}; if (!window.CBC) { window.CBC = {}; } if (!CBC.APP) { CBC.APP = {}; } if (!CBC.APP.SC) { CBC.APP.SC = {}; } if ( typeof CBC !== \"undefined\" && typeof CBC.APP !== \"undefined\" && typeof CBC.APP.SC !== \"undefined\" ) { CBC.APP.SC.preventDefault = true; } {\"image\":[{\"datePublished\":\"2024-04-12T20:03Z\",\"@type\":\"ImageObject\",\"name\":\"Shutterstock - Large file\",\"description\":\"Researchers have found evidence that a COVID infection can cause cortisol and serotonin levels to drop in the brain, which could result in \\\"brain fog.\\\"\\r\\r\",\"dateModified\":\"2024-04-12T20:01Z\",\"@context\":\"http://schema.org/\",\"url\":\"https://i.cbc.ca/1.7172513.1712952079!/fileImage/httpImage/shutterstock-large-file.jpg\"},{\"datePublished\":\"2024-04-12T19:09Z\",\"@type\":\"ImageObject\",\"name\":\"Ziyad Al-Aly\",\"description\":\"Ziyad Al-Aly studies patients with Long COVID and looks at how repeated infections can alter brain function.\",\"dateModified\":\"2024-04-12T19:09Z\",\"@context\":\"http://schema.org/\",\"url\":\"https://i.cbc.ca/1.7172079.1712948983!/fileImage/httpImage/ziyad-al-aly.jpg\"},{\"datePublished\":\"2024-04-12T19:11Z\",\"@type\":\"ImageObject\",\"name\":\"1830629750\",\"description\":\"A protester holding a placard relating to long covid poses during a gathering outside the UK Covid-19 Inquiry building in west London, on December 7, 2023 as Britain's former Prime Minister Boris Johnson is inside giving evidence regarding his management of the pandemic. Boris Johnson on Wednesday apologised for \\\"the pain and the loss and the suffering\\\" caused by the Covid-19 pandemic but defended his government at a public inquiry into its handling of the crisis. (Photo by JUSTIN TALLIS / AFP) (Photo by JUSTIN TALLIS/AFP via Getty Images)\",\"dateModified\":\"2024-04-12T19:11Z\",\"@context\":\"http://schema.org/\",\"url\":\"https://i.cbc.ca/1.7172139.1712949067!/fileImage/httpImage/1830629750.jpg\"},{\"datePublished\":\"2024-04-12T20:10Z\",\"@type\":\"ImageObject\",\"name\":\"Shutterstock - medium file\",\"description\":\"In one study, researchers found that the protective blood-brain barrier was leaky after a COVID infection, which could drive changes in neural function.\",\"dateModified\":\"2024-04-12T20:08Z\",\"@context\":\"http://schema.org/\",\"url\":\"https://i.cbc.ca/1.7172530.1712952480!/fileImage/httpImage/shutterstock-medium-file.jpg\"},{\"datePublished\":\"2023-09-21T19:44Z\",\"@type\":\"ImageObject\",\"name\":\"Long COVID\",\"description\":\"GENOA, ITALY - JULY 24: Sandra Cabreras, 57, years-old rides on an exercise bike to strengthen muscle tone at the Department of Rehabilitative Cardiology of ASL 3 Genova on July 24, 2020 in Genoa, Italy. Sandra contracted the covid and remained in hospital for 15 days, currently suffering from great fatigue. There is an estimated 14 million confirmed Coronavirus cases worldwide, with a reported 8 million recovering patients. In 80 percent of COVID-19 cases, recovery is complicated, leaving physical and psychological consequences that do not easily subside or allow survivors to return to normal life. The first Covid veterans gym, a project of Genoas ASL 3 and rehabilitation cardiology department directed by Cardiologist Piero Clavario, aims to address this. Designed to reactivate the heart, muscles and lungs, the gyms rehabilitation process gives patients a 2-month protocol, overseen by a staff of four doctors, three nurses, one psychologist and a physiotherapist. Of approximately 700 patients who were previously hospitalized with Covid, 100 patients were selected to participate in the gym, with the first 12 patients now 6 weeks into their rehabilitation process. Treatment is designed to combat numerous effects of the disease. Half of all patients hospitalized for Covid will be left with lung fibrosis, causing difficulty breathing. The nervous system is also affected as evidenced by the loss of taste and smell. Meanwhile 80 percent of patients suffer cognitive impairments such as memory loss and orientation difficulties, 40 percent show symptoms of depression and others experience chronic fatigue and rapid weight loss. \",\"dateModified\":\"2023-09-21T19:43Z\",\"@context\":\"http://schema.org/\",\"url\":\"https://i.cbc.ca/1.6974405.1695325434!/fileImage/httpImage/long-covid.jpg\"},{\"datePublished\":\"2024-04-12T17:21Z\",\"@type\":\"ImageObject\",\"name\":\"Chronic Fatigue Syndrome\",\"description\":\"FILE - Nancy Rose, who contracted COVID-19 in 2021 and exhibits long-haul symptoms including brain fog and memory difficulties, pauses while organizing her desk space, Tuesday, Jan. 25, 2022, in Port Jefferson, N.Y. Rose, 67, said many of her symptoms waned after she got vaccinated, though she still has bouts of fatigue and memory loss. U.S. health officials estimate 3.3 million Americans have chronic fatigue syndrome — a bigger number than previous studies have suggested, and one likely boosted by patients with long COVID, according to results released by the Centers for Disease Control and Prevention on Friday, Dec. 8, 2023. (AP Photo/John Minchillo, File)\",\"dateModified\":\"2024-04-12T17:19Z\",\"@context\":\"http://schema.org/\",\"url\":\"https://i.cbc.ca/1.7172134.1712942385!/fileImage/httpImage/chronic-fatigue-syndrome.jpg\"},{\"datePublished\":\"2024-04-12T17:24Z\",\"@type\":\"ImageObject\",\"name\":\"1935287561\",\"description\":\"WASHINGTON, DC - JANUARY 18: People with symptoms of long Covid sit in the audience as they listen during a Senate Committee on Health, Education, Labor and Pensions hearing titled \\\"Addressing Long COVID: Advancing Research and Improving Patient Care\\\" on Capitol Hill January 18, 2024 in Washington, DC. The hearing focused on long Covid research and patient care. (Photo by Drew Angerer/Getty Images)\",\"dateModified\":\"2024-04-12T17:22Z\",\"@context\":\"http://schema.org/\",\"url\":\"https://i.cbc.ca/1.7172145.1712942578!/fileImage/httpImage/1935287561.jpg\"}],\"author\":[{\"workLocation\":{\"@type\":\"Place\",\"name\":\"Toronto\",\"@context\":\"http://schema.org/\"},\"image\":{\"datePublished\":\"2024-02-02T17:11Z\",\"@type\":\"ImageObject\",\"name\":\"Amanda Buckiewicz headshot\",\"description\":\"Amanda Buckiewicz is an award-winning science journalist.\",\"dateModified\":\"2024-02-02T17:03Z\",\"@context\":\"http://schema.org/\",\"url\":\"https://i.cbc.ca/1.7103151.1706893380!/fileImage/httpImage/amanda-buckiewicz-headshot.jpg\"},\"knowsAbout\":[\"Science\"],\"contactPoint\":{\"@type\":\"ContactPoint\",\"@context\":\"http://schema.org/\",\"url\":\"https://www.cbc.ca/radio/quirks/author/amanda-buckiewicz-1.7103140\",\"email\":\"\"},\"@type\":\"Person\",\"jobTitle\":\"Producer\",\"name\":\"Amanda Buckiewicz\",\"description\":\"Amanda Buckiewicz is an award-winning science journalist with CBC Radio's legendary science show, Quirks & Quarks. Her work can be found on Discovery Channel, BBC Earth, Smithsonian, and Amazon Prime.\",\"@context\":\"http://schema.org/\"}],\"@type\":\"ReportageNewsArticle\",\"description\":\"Researchers are trying to understand the profound effects of COVID-19 on the brain, looking at how it disrupts the blood-brain barrier, how it affects brain volume, and showing that even a mild case of COVID can lead to the equivalent of seven years of brain aging.\",\"alternateName\":\"Researchers are trying to explain COVID's profound effects on the brain\",\"dateModified\":\"2024-04-12T20:54Z\",\"@context\":\"http://schema.org/\",\"datePublished\":\"2024-04-12T20:54Z\",\"name\":\"COVID infections are causing drops in IQ and years of brain aging, studies suggest\",\"publisher\":{\"foundingDate\":\"1936-11-02T05:00Z\",\"ethicsPolicy\":\"https://www.cbc.ca/news/about-cbc-news-1.1294364\",\"verificationFactCheckingPolicy\":\"https://www.cbc.ca/news/about-cbc-news-1.1294364\",\"@type\":\"NewsMediaOrganization\",\"@context\":\"http://schema.org/\",\"ownershipFundingInfo\":\"https://www.cbc.ca/news/about-cbc-news-1.1294364\",\"actionableFeedbackPolicy\":\"https://www.cbc.ca/news/about-cbc-news-1.1294364\",\"missionCoveragePrioritiesPolicy\":\"https://www.cbc.ca/news/about-cbc-news-1.1294364\",\"diversityPolicy\":\"https://www.cbc.ca/news/about-cbc-news-1.1294364\",\"masthead\":\"https://www.cbc.ca/news/about-cbc-news-1.1294364\",\"diversityStaffingReport\":\"https://www.cbc.ca/news/about-cbc-news-1.1294364\",\"unnamedSourcesPolicy\":\"https://www.cbc.ca/news/about-cbc-news-1.1294364\",\"correctionsPolicy\":\"https://www.cbc.ca/news/about-cbc-news-1.1294364\"},\"audio\":[{\"identifier\":\"2328172611572\",\"expires\":\"2079-04-06T04:00Z\",\"@type\":\"AudioObject\",\"description\":\"For many people COVID was more than a respiratory disease. We’re learning now just what kind of impact an infection can have on the brain. It can affect cognition – leading to the famous brain fog – and even shrink and prematurely age the brain. One of the researchers studying these effects is Dr. Ziyad Al-Aly, he has found COVID patients suffering from brain fog, confusion, tingling, mini strokes, and even seizure disorders.\",\"dateModified\":\"2024-04-12T18:41Z\",\"@context\":\"http://schema.org/\",\"datePublished\":\"2024-04-13T04:00Z\",\"alternativeHeadline\":\"For many people COVID was more than a respiratory disease. We’re learning now just what kind of impact an infection can have on the brain. It can affect cognition – leading to the famous brain fog – and even shrink and prematurely age the brain. One of the researchers studying these effects is Dr. Ziyad Al-Aly, he has found COVID patients suffering from brain fog, confusion, tingling, mini strokes, and even seizure disorders.\",\"contentUrl\":\"https://www.cbc.ca/player/play/2328172611572\",\"dateCreated\":\"2024-04-12T04:00Z\",\"uploadDate\":\"2024-04-13T04:00Z\",\"name\":\"COVID infections are causing brain inflammation, drops in IQ, and years of brain aging\",\"thumbnailUrl\":\"https://thumbnails.cbc.ca/maven_legacy/thumbnails/287/434/quirks_quarks_16x9.jpg\"}],\"headline\":\"COVID infections are causing drops in IQ and years of brain aging, studies suggest\",\"articleSection\":\"Quirks & Quarks\",\"thumbnailUrl\":\"https://i.cbc.ca/1.7172513.1712952079!/fileImage/httpImage/shutterstock-large-file.jpg\"} [9351,8825,6163,8089,7270,4736,7673,2904,2816,827,1857]{\"namedChunks\":[\"GlobalHeader-component\",\"Logos\",\"pages-Detail-component\",\"GlobalFooter-component\"]} {\"image\":[{\"datePublished\":\"2024-04-12T20:03Z\",\"@type\":\"ImageObject\",\"name\":\"Shutterstock - Large file\",\"description\":\"Researchers have found evidence that a COVID infection can cause cortisol and serotonin levels to drop in the brain, which could result in \\\"brain fog.\\\"\\r\\r\",\"dateModified\":\"2024-04-12T20:01Z\",\"@context\":\"http://schema.org/\",\"url\":\"https://i.cbc.ca/1.7172513.1712952079!/fileImage/httpImage/shutterstock-large-file.jpg\"},{\"datePublished\":\"2024-04-12T19:09Z\",\"@type\":\"ImageObject\",\"name\":\"Ziyad Al-Aly\",\"description\":\"Ziyad Al-Aly studies patients with Long COVID and looks at how repeated infections can alter brain function.\",\"dateModified\":\"2024-04-12T19:09Z\",\"@context\":\"http://schema.org/\",\"url\":\"https://i.cbc.ca/1.7172079.1712948983!/fileImage/httpImage/ziyad-al-aly.jpg\"},{\"datePublished\":\"2024-04-12T19:11Z\",\"@type\":\"ImageObject\",\"name\":\"1830629750\",\"description\":\"A protester holding a placard relating to long covid poses during a gathering outside the UK Covid-19 Inquiry building in west London, on December 7, 2023 as Britain's former Prime Minister Boris Johnson is inside giving evidence regarding his management of the pandemic. Boris Johnson on Wednesday apologised for \\\"the pain and the loss and the suffering\\\" caused by the Covid-19 pandemic but defended his government at a public inquiry into its handling of the crisis. (Photo by JUSTIN TALLIS / AFP) (Photo by JUSTIN TALLIS/AFP via Getty Images)\",\"dateModified\":\"2024-04-12T19:11Z\",\"@context\":\"http://schema.org/\",\"url\":\"https://i.cbc.ca/1.7172139.1712949067!/fileImage/httpImage/1830629750.jpg\"},{\"datePublished\":\"2024-04-12T20:10Z\",\"@type\":\"ImageObject\",\"name\":\"Shutterstock - medium file\",\"description\":\"In one study, researchers found that the protective blood-brain barrier was leaky after a COVID infection, which could drive changes in neural function.\",\"dateModified\":\"2024-04-12T20:08Z\",\"@context\":\"http://schema.org/\",\"url\":\"https://i.cbc.ca/1.7172530.1712952480!/fileImage/httpImage/shutterstock-medium-file.jpg\"},{\"datePublished\":\"2023-09-21T19:44Z\",\"@type\":\"ImageObject\",\"name\":\"Long COVID\",\"description\":\"GENOA, ITALY - JULY 24: Sandra Cabreras, 57, years-old rides on an exercise bike to strengthen muscle tone at the Department of Rehabilitative Cardiology of ASL 3 Genova on July 24, 2020 in Genoa, Italy. Sandra contracted the covid and remained in hospital for 15 days, currently suffering from great fatigue. There is an estimated 14 million confirmed Coronavirus cases worldwide, with a reported 8 million recovering patients. In 80 percent of COVID-19 cases, recovery is complicated, leaving physical and psychological consequences that do not easily subside or allow survivors to return to normal life. The first Covid veterans gym, a project of Genoas ASL 3 and rehabilitation cardiology department directed by Cardiologist Piero Clavario, aims to address this. Designed to reactivate the heart, muscles and lungs, the gyms rehabilitation process gives patients a 2-month protocol, overseen by a staff of four doctors, three nurses, one psychologist and a physiotherapist. Of approximately 700 patients who were previously hospitalized with Covid, 100 patients were selected to participate in the gym, with the first 12 patients now 6 weeks into their rehabilitation process. Treatment is designed to combat numerous effects of the disease. Half of all patients hospitalized for Covid will be left with lung fibrosis, causing difficulty breathing. The nervous system is also affected as evidenced by the loss of taste and smell. Meanwhile 80 percent of patients suffer cognitive impairments such as memory loss and orientation difficulties, 40 percent show symptoms of depression and others experience chronic fatigue and rapid weight loss. \",\"dateModified\":\"2023-09-21T19:43Z\",\"@context\":\"http://schema.org/\",\"url\":\"https://i.cbc.ca/1.6974405.1695325434!/fileImage/httpImage/long-covid.jpg\"},{\"datePublished\":\"2024-04-12T17:21Z\",\"@type\":\"ImageObject\",\"name\":\"Chronic Fatigue Syndrome\",\"description\":\"FILE - Nancy Rose, who contracted COVID-19 in 2021 and exhibits long-haul symptoms including brain fog and memory difficulties, pauses while organizing her desk space, Tuesday, Jan. 25, 2022, in Port Jefferson, N.Y. Rose, 67, said many of her symptoms waned after she got vaccinated, though she still has bouts of fatigue and memory loss. U.S. health officials estimate 3.3 million Americans have chronic fatigue syndrome — a bigger number than previous studies have suggested, and one likely boosted by patients with long COVID, according to results released by the Centers for Disease Control and Prevention on Friday, Dec. 8, 2023. (AP Photo/John Minchillo, File)\",\"dateModified\":\"2024-04-12T17:19Z\",\"@context\":\"http://schema.org/\",\"url\":\"https://i.cbc.ca/1.7172134.1712942385!/fileImage/httpImage/chronic-fatigue-syndrome.jpg\"},{\"datePublished\":\"2024-04-12T17:24Z\",\"@type\":\"ImageObject\",\"name\":\"1935287561\",\"description\":\"WASHINGTON, DC - JANUARY 18: People with symptoms of long Covid sit in the audience as they listen during a Senate Committee on Health, Education, Labor and Pensions hearing titled \\\"Addressing Long COVID: Advancing Research and Improving Patient Care\\\" on Capitol Hill January 18, 2024 in Washington, DC. The hearing focused on long Covid research and patient care. (Photo by Drew Angerer/Getty Images)\",\"dateModified\":\"2024-04-12T17:22Z\",\"@context\":\"http://schema.org/\",\"url\":\"https://i.cbc.ca/1.7172145.1712942578!/fileImage/httpImage/1935287561.jpg\"}],\"author\":[{\"workLocation\":{\"@type\":\"Place\",\"name\":\"Toronto\",\"@context\":\"http://schema.org/\"},\"image\":{\"datePublished\":\"2024-02-02T17:11Z\",\"@type\":\"ImageObject\",\"name\":\"Amanda Buckiewicz headshot\",\"description\":\"Amanda Buckiewicz is an award-winning science journalist.\",\"dateModified\":\"2024-02-02T17:03Z\",\"@context\":\"http://schema.org/\",\"url\":\"https://i.cbc.ca/1.7103151.1706893380!/fileImage/httpImage/amanda-buckiewicz-headshot.jpg\"},\"knowsAbout\":[\"Science\"],\"contactPoint\":{\"@type\":\"ContactPoint\",\"@context\":\"http://schema.org/\",\"url\":\"https://www.cbc.ca/radio/quirks/author/amanda-buckiewicz-1.7103140\",\"email\":\"\"},\"@type\":\"Person\",\"jobTitle\":\"Producer\",\"name\":\"Amanda Buckiewicz\",\"description\":\"Amanda Buckiewicz is an award-winning science journalist with CBC Radio's legendary science show, Quirks & Quarks. Her work can be found on Discovery Channel, BBC Earth, Smithsonian, and Amazon Prime.\",\"@context\":\"http://schema.org/\"}],\"@type\":\"ReportageNewsArticle\",\"description\":\"Researchers are trying to understand the profound effects of COVID-19 on the brain, looking at how it disrupts the blood-brain barrier, how it affects brain volume, and showing that even a mild case of COVID can lead to the equivalent of seven years of brain aging.\",\"alternateName\":\"Researchers are trying to explain COVID's profound effects on the brain\",\"dateModified\":\"2024-04-12T20:54Z\",\"@context\":\"http://schema.org/\",\"datePublished\":\"2024-04-12T20:54Z\",\"name\":\"COVID infections are causing drops in IQ and years of brain aging, studies suggest\",\"publisher\":{\"foundingDate\":\"1936-11-02T05:00Z\",\"ethicsPolicy\":\"https://www.cbc.ca/news/about-cbc-news-1.1294364\",\"verificationFactCheckingPolicy\":\"https://www.cbc.ca/news/about-cbc-news-1.1294364\",\"@type\":\"NewsMediaOrganization\",\"@context\":\"http://schema.org/\",\"ownershipFundingInfo\":\"https://www.cbc.ca/news/about-cbc-news-1.1294364\",\"actionableFeedbackPolicy\":\"https://www.cbc.ca/news/about-cbc-news-1.1294364\",\"missionCoveragePrioritiesPolicy\":\"https://www.cbc.ca/news/about-cbc-news-1.1294364\",\"diversityPolicy\":\"https://www.cbc.ca/news/about-cbc-news-1.1294364\",\"masthead\":\"https://www.cbc.ca/news/about-cbc-news-1.1294364\",\"diversityStaffingReport\":\"https://www.cbc.ca/news/about-cbc-news-1.1294364\",\"unnamedSourcesPolicy\":\"https://www.cbc.ca/news/about-cbc-news-1.1294364\",\"correctionsPolicy\":\"https://www.cbc.ca/news/about-cbc-news-1.1294364\"},\"audio\":[{\"identifier\":\"2328172611572\",\"expires\":\"2079-04-06T04:00Z\",\"@type\":\"AudioObject\",\"description\":\"For many people COVID was more than a respiratory disease. We’re learning now just what kind of impact an infection can have on the brain. It can affect cognition – leading to the famous brain fog – and even shrink and prematurely age the brain. One of the researchers studying these effects is Dr. Ziyad Al-Aly, he has found COVID patients suffering from brain fog, confusion, tingling, mini strokes, and even seizure disorders.\",\"dateModified\":\"2024-04-12T18:41Z\",\"@context\":\"http://schema.org/\",\"datePublished\":\"2024-04-13T04:00Z\",\"alternativeHeadline\":\"For many people COVID was more than a respiratory disease. We’re learning now just what kind of impact an infection can have on the brain. It can affect cognition – leading to the famous brain fog – and even shrink and prematurely age the brain. One of the researchers studying these effects is Dr. Ziyad Al-Aly, he has found COVID patients suffering from brain fog, confusion, tingling, mini strokes, and even seizure disorders.\",\"contentUrl\":\"https://www.cbc.ca/player/play/2328172611572\",\"dateCreated\":\"2024-04-12T04:00Z\",\"uploadDate\":\"2024-04-13T04:00Z\",\"name\":\"COVID infections are causing brain inflammation, drops in IQ, and years of brain aging\",\"thumbnailUrl\":\"https://thumbnails.cbc.ca/maven_legacy/thumbnails/287/434/quirks_quarks_16x9.jpg\"}],\"headline\":\"COVID infections are causing drops in IQ and years of brain aging, studies suggest\",\"articleSection\":\"Quirks & Quarks\",\"thumbnailUrl\":\"https://i.cbc.ca/1.7172513.1712952079!/fileImage/httpImage/shutterstock-large-file.jpg\"}window.__INITIAL_STATE__ = {\"app\":{\"adOrder\":\"\",\"path\":\"radio\\u002Fquirks\\u002Flong-covid-brain-1.7171918\",\"baseSection\":\"radio\",\"baseNavSection\":\"radio\",\"subSection\":\"quirks\",\"section\":\"radio\\u002Fquirks\",\"sponsored\":false,\"pageType\":\"detail\",\"ad_hierarchy\":\"\\u002F5876\\u002Fradio\\u002Fquirks\\u002Fstory\",\"contentId\":\"1.7171918\",\"contentType\":\"story\",\"title\":\"COVID infections are causing drops in IQ and years of brain aging, studies suggestCBC Radio\",\"a11yNotificationText\":\"COVID infections are causing drops in IQ and years of brain aging, studies suggestCBC Radio Loaded\",\"meta\":{\"canonical\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fradio\\u002Fquirks\\u002Flong-covid-brain-1.7171918\",\"jsonld\":{\"image\":[{\"datePublished\":\"2024-04-12T20:03Z\",\"@type\":\"ImageObject\",\"name\":\"Shutterstock - Large file\",\"description\":\"Researchers have found evidence that a COVID infection can cause cortisol and serotonin levels to drop in the brain, which could result in \\\"brain fog.\\\"\\r\\r\",\"dateModified\":\"2024-04-12T20:01Z\",\"@context\":\"http:\\u002F\\u002Fschema.org\\u002F\",\"url\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172513.1712952079!\\u002FfileImage\\u002FhttpImage\\u002Fshutterstock-large-file.jpg\"},{\"datePublished\":\"2024-04-12T19:09Z\",\"@type\":\"ImageObject\",\"name\":\"Ziyad Al-Aly\",\"description\":\"Ziyad Al-Aly studies patients with Long COVID and looks at how repeated infections can alter brain function.\",\"dateModified\":\"2024-04-12T19:09Z\",\"@context\":\"http:\\u002F\\u002Fschema.org\\u002F\",\"url\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172079.1712948983!\\u002FfileImage\\u002FhttpImage\\u002Fziyad-al-aly.jpg\"},{\"datePublished\":\"2024-04-12T19:11Z\",\"@type\":\"ImageObject\",\"name\":\"1830629750\",\"description\":\"A protester holding a placard relating to long covid poses during a gathering outside the UK Covid-19 Inquiry building in west London, on December 7, 2023 as Britain's former Prime Minister Boris Johnson is inside giving evidence regarding his management of the pandemic. Boris Johnson on Wednesday apologised for \\\"the pain and the loss and the suffering\\\" caused by the Covid-19 pandemic but defended his government at a public inquiry into its handling of the crisis. (Photo by JUSTIN TALLIS \\u002F AFP) (Photo by JUSTIN TALLIS\\u002FAFP via Getty Images)\",\"dateModified\":\"2024-04-12T19:11Z\",\"@context\":\"http:\\u002F\\u002Fschema.org\\u002F\",\"url\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172139.1712949067!\\u002FfileImage\\u002FhttpImage\\u002F1830629750.jpg\"},{\"datePublished\":\"2024-04-12T20:10Z\",\"@type\":\"ImageObject\",\"name\":\"Shutterstock - medium file\",\"description\":\"In one study, researchers found that the protective blood-brain barrier was leaky after a COVID infection, which could drive changes in neural function.\",\"dateModified\":\"2024-04-12T20:08Z\",\"@context\":\"http:\\u002F\\u002Fschema.org\\u002F\",\"url\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172530.1712952480!\\u002FfileImage\\u002FhttpImage\\u002Fshutterstock-medium-file.jpg\"},{\"datePublished\":\"2023-09-21T19:44Z\",\"@type\":\"ImageObject\",\"name\":\"Long COVID\",\"description\":\"GENOA, ITALY - JULY 24: Sandra Cabreras, 57, years-old rides on an exercise bike to strengthen muscle tone at the Department of Rehabilitative Cardiology of ASL 3 Genova on July 24, 2020 in Genoa, Italy. Sandra contracted the covid and remained in hospital for 15 days, currently suffering from great fatigue. There is an estimated 14 million confirmed Coronavirus cases worldwide, with a reported 8 million recovering patients. In 80 percent of COVID-19 cases, recovery is complicated, leaving physical and psychological consequences that do not easily subside or allow survivors to return to normal life. The first Covid veterans gym, a project of Genoas ASL 3 and rehabilitation cardiology department directed by Cardiologist Piero Clavario, aims to address this. Designed to reactivate the heart, muscles and lungs, the gyms rehabilitation process gives patients a 2-month protocol, overseen by a staff of four doctors, three nurses, one psychologist and a physiotherapist. Of approximately 700 patients who were previously hospitalized with Covid, 100 patients were selected to participate in the gym, with the first 12 patients now 6 weeks into their rehabilitation process. Treatment is designed to combat numerous effects of the disease. Half of all patients hospitalized for Covid will be left with lung fibrosis, causing difficulty breathing. The nervous system is also affected as evidenced by the loss of taste and smell. Meanwhile 80 percent of patients suffer cognitive impairments such as memory loss and orientation difficulties, 40 percent show symptoms of depression and others experience chronic fatigue and rapid weight loss. \",\"dateModified\":\"2023-09-21T19:43Z\",\"@context\":\"http:\\u002F\\u002Fschema.org\\u002F\",\"url\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.6974405.1695325434!\\u002FfileImage\\u002FhttpImage\\u002Flong-covid.jpg\"},{\"datePublished\":\"2024-04-12T17:21Z\",\"@type\":\"ImageObject\",\"name\":\"Chronic Fatigue Syndrome\",\"description\":\"FILE - Nancy Rose, who contracted COVID-19 in 2021 and exhibits long-haul symptoms including brain fog and memory difficulties, pauses while organizing her desk space, Tuesday, Jan. 25, 2022, in Port Jefferson, N.Y. Rose, 67, said many of her symptoms waned after she got vaccinated, though she still has bouts of fatigue and memory loss. U.S. health officials estimate 3.3 million Americans have chronic fatigue syndrome — a bigger number than previous studies have suggested, and one likely boosted by patients with long COVID, according to results released by the Centers for Disease Control and Prevention on Friday, Dec. 8, 2023. (AP Photo\\u002FJohn Minchillo, File)\",\"dateModified\":\"2024-04-12T17:19Z\",\"@context\":\"http:\\u002F\\u002Fschema.org\\u002F\",\"url\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172134.1712942385!\\u002FfileImage\\u002FhttpImage\\u002Fchronic-fatigue-syndrome.jpg\"},{\"datePublished\":\"2024-04-12T17:24Z\",\"@type\":\"ImageObject\",\"name\":\"1935287561\",\"description\":\"WASHINGTON, DC - JANUARY 18: People with symptoms of long Covid sit in the audience as they listen during a Senate Committee on Health, Education, Labor and Pensions hearing titled \\\"Addressing Long COVID: Advancing Research and Improving Patient Care\\\" on Capitol Hill January 18, 2024 in Washington, DC. The hearing focused on long Covid research and patient care. (Photo by Drew Angerer\\u002FGetty Images)\",\"dateModified\":\"2024-04-12T17:22Z\",\"@context\":\"http:\\u002F\\u002Fschema.org\\u002F\",\"url\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172145.1712942578!\\u002FfileImage\\u002FhttpImage\\u002F1935287561.jpg\"}],\"author\":[{\"workLocation\":{\"@type\":\"Place\",\"name\":\"Toronto\",\"@context\":\"http:\\u002F\\u002Fschema.org\\u002F\"},\"image\":{\"datePublished\":\"2024-02-02T17:11Z\",\"@type\":\"ImageObject\",\"name\":\"Amanda Buckiewicz headshot\",\"description\":\"Amanda Buckiewicz is an award-winning science journalist.\",\"dateModified\":\"2024-02-02T17:03Z\",\"@context\":\"http:\\u002F\\u002Fschema.org\\u002F\",\"url\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7103151.1706893380!\\u002FfileImage\\u002FhttpImage\\u002Famanda-buckiewicz-headshot.jpg\"},\"knowsAbout\":[\"Science\"],\"contactPoint\":{\"@type\":\"ContactPoint\",\"@context\":\"http:\\u002F\\u002Fschema.org\\u002F\",\"url\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fradio\\u002Fquirks\\u002Fauthor\\u002Famanda-buckiewicz-1.7103140\",\"email\":\"\"},\"@type\":\"Person\",\"jobTitle\":\"Producer\",\"name\":\"Amanda Buckiewicz\",\"description\":\"Amanda Buckiewicz is an award-winning science journalist with CBC Radio's legendary science show, Quirks & Quarks. Her work can be found on Discovery Channel, BBC Earth, Smithsonian, and Amazon Prime.\",\"@context\":\"http:\\u002F\\u002Fschema.org\\u002F\"}],\"@type\":\"ReportageNewsArticle\",\"description\":\"Researchers are trying to understand the profound effects of COVID-19 on the brain, looking at how it disrupts the blood-brain barrier, how it affects brain volume, and showing that even a mild case of COVID can lead to the equivalent of seven years of brain aging.\",\"alternateName\":\"Researchers are trying to explain COVID's profound effects on the brain\",\"dateModified\":\"2024-04-12T20:54Z\",\"@context\":\"http:\\u002F\\u002Fschema.org\\u002F\",\"datePublished\":\"2024-04-12T20:54Z\",\"name\":\"COVID infections are causing drops in IQ and years of brain aging, studies suggest\",\"publisher\":{\"foundingDate\":\"1936-11-02T05:00Z\",\"ethicsPolicy\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fnews\\u002Fabout-cbc-news-1.1294364\",\"verificationFactCheckingPolicy\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fnews\\u002Fabout-cbc-news-1.1294364\",\"@type\":\"NewsMediaOrganization\",\"@context\":\"http:\\u002F\\u002Fschema.org\\u002F\",\"ownershipFundingInfo\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fnews\\u002Fabout-cbc-news-1.1294364\",\"actionableFeedbackPolicy\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fnews\\u002Fabout-cbc-news-1.1294364\",\"missionCoveragePrioritiesPolicy\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fnews\\u002Fabout-cbc-news-1.1294364\",\"diversityPolicy\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fnews\\u002Fabout-cbc-news-1.1294364\",\"masthead\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fnews\\u002Fabout-cbc-news-1.1294364\",\"diversityStaffingReport\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fnews\\u002Fabout-cbc-news-1.1294364\",\"unnamedSourcesPolicy\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fnews\\u002Fabout-cbc-news-1.1294364\",\"correctionsPolicy\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fnews\\u002Fabout-cbc-news-1.1294364\"},\"audio\":[{\"identifier\":\"2328172611572\",\"expires\":\"2079-04-06T04:00Z\",\"@type\":\"AudioObject\",\"description\":\"For many people COVID was more than a respiratory disease. We’re learning now just what kind of impact an infection can have on the brain. It can affect cognition – leading to the famous brain fog – and even shrink and prematurely age the brain. One of the researchers studying these effects is Dr. Ziyad Al-Aly, he has found COVID patients suffering from brain fog, confusion, tingling, mini strokes, and even seizure disorders.\",\"dateModified\":\"2024-04-12T18:41Z\",\"@context\":\"http:\\u002F\\u002Fschema.org\\u002F\",\"datePublished\":\"2024-04-13T04:00Z\",\"alternativeHeadline\":\"For many people COVID was more than a respiratory disease. We’re learning now just what kind of impact an infection can have on the brain. It can affect cognition – leading to the famous brain fog – and even shrink and prematurely age the brain. One of the researchers studying these effects is Dr. Ziyad Al-Aly, he has found COVID patients suffering from brain fog, confusion, tingling, mini strokes, and even seizure disorders.\",\"contentUrl\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fplayer\\u002Fplay\\u002F2328172611572\",\"dateCreated\":\"2024-04-12T04:00Z\",\"uploadDate\":\"2024-04-13T04:00Z\",\"name\":\"COVID infections are causing brain inflammation, drops in IQ, and years of brain aging\",\"thumbnailUrl\":\"https:\\u002F\\u002Fthumbnails.cbc.ca\\u002Fmaven_legacy\\u002Fthumbnails\\u002F287\\u002F434\\u002Fquirks_quarks_16x9.jpg\"}],\"headline\":\"COVID infections are causing drops in IQ and years of brain aging, studies suggest\",\"articleSection\":\"Quirks & Quarks\",\"thumbnailUrl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172513.1712952079!\\u002FfileImage\\u002FhttpImage\\u002Fshutterstock-large-file.jpg\"},\"description\":\"Researchers are trying to understand the profound effects of COVID-19 on the brain, looking at how it disrupts the blood-brain barrier, how it affects brain volume, and showing that even a mild case of COVID can lead to the equivalent of seven years of brain aging.\",\"vf:container_id\":\"1.7171918\",\"twitter:card\":\"summary_large_image\",\"twitter:site\":\"@cbc\",\"twitter:title\":\"COVID infections are causing drops in IQ and years of brain aging, studies suggestCBC Radio\",\"twitter:description\":\"Researchers are trying to understand the profound effects of COVID-19 on the brain, looking at how it disrupts the blood-brain barrier, how it affects brain volume, and showing that even a mild case of COVID can lead to the equivalent of seven years of brain aging.\",\"twitter:image\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172513.1712952079!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9_620\\u002Fshutterstock-large-file.jpg\",\"twitter:player\":\"\",\"twitter:player:width\":\"640\",\"twitter:player:height\":\"360\",\"google-site-verification\":\"pJy_QmRvDz2tr7X8eQ6Y1L3Se-8RWM_PpJX42Pr_fYo\",\"robots\":\"\",\"vf:url\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fradio\\u002Fquirks\\u002Flong-covid-brain-1.7171918\",\"vf:section\":\"2.4884\",\"og:url\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fradio\\u002Fquirks\\u002Flong-covid-brain-1.7171918\",\"og:title\":\"COVID infections are causing drops in IQ and years of brain aging, studies suggestCBC Radio\",\"og:image\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172513.1712952079!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9_620\\u002Fshutterstock-large-file.jpg\",\"og:description\":\"Researchers are trying to understand the profound effects of COVID-19 on the brain, looking at how it disrupts the blood-brain barrier, how it affects brain volume, and showing that even a mild case of COVID can lead to the equivalent of seven years of brain aging.\",\"og:type\":\"article\",\"fb:pages\":\"124120290989171\",\"og:locale\":\"en_US\",\"og:site_name\":\"CBC\",\"video:duration\":\"\",\"video:release_date\":\"\"},\"defaultHeadlineImage\":\"\",\"statusCode\":200,\"themeClassNames\":\"detail radioTheme storyType quirksSection\"},\"author\":{\"profile\":{\"title\":\"\",\"name\":\"\",\"biography\":\"\",\"sourceId\":\"\",\"url\":\"\",\"links\":null,\"photoDerivatives\":{}},\"recentContent\":{}},\"content\":{\"list\":[],\"section\":\"\",\"status\":\"\",\"contentType\":\"\",\"hasNextPage\":false},\"cookieJar\":{\"cookies\":null},\"detail\":{\"content\":{\"id\":\"1.7171918\",\"type\":\"story\",\"source\":\"\",\"corrections\":[],\"clarifications\":[],\"headline\":\"COVID infections are causing drops in IQ and years of brain aging, studies suggest\",\"shareHeadline\":\"COVID infections are causing drops in IQ and years of brain aging, studies suggest\",\"deck\":\"Researchers are trying to explain COVID's profound effects on the brain\",\"description\":\"Researchers are trying to understand the profound effects of COVID-19 on the brain, looking at how it disrupts the blood-brain barrier, how it affects brain volume, and showing that even a mild case of COVID can lead to the equivalent of seven years of brain aging.\",\"byline\":\"CBC Radio\",\"highlights\":{\"bullets\":[],\"label\":\"Updated\"},\"intlinks\":[],\"externalLinks\":[],\"updatedAt\":1712955283053,\"publishedAt\":1712955283053,\"publishedAtVerbal\":\"Apr 12, 2024 4:54 PM EDT\",\"displayComments\":false,\"url\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fradio\\u002Fquirks\\u002Flong-covid-brain-1.7171918\",\"category\":\"\",\"flag\":\"Q&A\",\"body\":[{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"polopoly_media\",\"attribs\":{\"localTitle\":\"COVID infections are causing brain inflammation, drops in IQ, and years of brain aging\",\"captionVisible\":false},\"content\":{\"id\":4202578,\"title\":\"COVID infections are causing brain inflammation, drops in IQ, and years of brain aging\",\"description\":\"For many people COVID was more than a respiratory disease. We’re learning now just what kind of impact an infection can have on the brain. It can affect cognition – leading to the famous brain fog – and even shrink and prematurely age the brain. One of the researchers studying these effects is Dr. Ziyad Al-Aly, he has found COVID patients suffering from brain fog, confusion, tingling, mini strokes, and even seizure disorders.\",\"source\":\"Polopoly\",\"sourceId\":\"1.7172343\",\"version\":\"1712947505\",\"publishedAt\":1712980800000,\"readablePublishedAt\":\"Apr 13 2024, 00:00:00 AM EDT\",\"updatedAt\":1712980800000,\"readableUpdatedAt\":\"Apr 13 2024, 00:00:00 AM EDT\",\"type\":\"audio\",\"draft\":false,\"embedTypes\":null,\"images\":{\"square_140\":\"https:\\u002F\\u002Fthumbnails.cbc.ca\\u002Fmaven_legacy\\u002Fthumbnails\\u002F287\\u002F434\\u002Fquirks_quarks_16x9.jpg\"},\"language\":\"en\",\"showName\":\"Quirks and Quarks\",\"showSlug\":\"quirks-and-quarks\",\"media\":{\"duration\":901.015,\"streamType\":\"On-Demand\",\"hasCaptions\":false,\"textTracks\":[]},\"consideredForLineup\":false,\"url\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fradio\\u002Fcovid-infections-are-causing-brain-inflammation-drops-in-iq-and-years-of-brain-aging-1.7172343\",\"urlSlug\":\"\",\"deck\":\"For many people COVID was more than a respiratory disease. We’re learning now just what kind of impact an infection can have on the brain. It can affect cognition – leading to the famous brain fog – and even shrink and prematurely age the brain. One of the researchers studying these effects is Dr. Ziyad Al-Aly, he has found COVID patients suffering from brain fog, confusion, tingling, mini strokes, and even seizure disorders.\",\"imageSmall\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172343.1712947505!\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9tight_140\\u002Fimage.jpg\",\"imageLarge\":\"https:\\u002F\\u002Fthumbnails.cbc.ca\\u002Fmaven_legacy\\u002Fthumbnails\\u002F287\\u002F434\\u002Fquirks_quarks_16x9.jpg\",\"imageAspects\":\"16x9_940,square_220,16x9_300,16x9_620,original_620,original_300,16x9tight_140,square_140,16x9_780,16x9_460,square_60\",\"flag\":\"Audio\",\"flags\":{\"status\":\"\",\"label\":\"\"},\"mediaDuration\":901,\"mediaId\":\"2328172611572\",\"mediaStreamType\":\"On-Demand\",\"mediaCaptionUrl\":\"\",\"show\":\"Quirks and Quarks\",\"authorDisplay\":\"\",\"shareHeadline\":\"COVID infections are causing brain inflammation, drops in IQ, and years of brain aging\",\"byline\":null,\"externalLinks\":null,\"extlinks\":null,\"newsletter\":null,\"editorialSource\":null,\"polopolySource\":null,\"syndicate\":null,\"onTime\":1712894400000,\"offTime\":3447979200000,\"readableOnTime\":\"Apr 12 2024, 00:00:00 AM EDT\",\"readableOffTime\":\"Apr 06 2079, 00:00:00 AM EDT\",\"advertising\":{\"exclusions\":\"\"},\"highlights\":null,\"clarifications\":null,\"corrections\":null}}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"text\",\"content\":\"When COVID-19 first reared its head back in 2019, it brought with it a slew of strange symptoms beyond just respiratory problems. One of the most puzzling symptoms in those early days was something called \\\"brain fog\\\" — cognitive issues like confusion, forgetfulness, and trouble focusing.\"}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"text\",\"content\":\"And while other symptoms have changed as the virus mutated, brain fog is still a common complaint of COVID sufferers not only during the initial illness, but extending for months or even years afterwards. Several recent studies have been trying to understand exactly what this virus is doing to our brains — and how to stop it.\"},{\"type\":\"html\",\"tag\":\"br\",\"attribs\":null,\"content\":[]},{\"type\":\"text\",\"content\":\" \"},{\"type\":\"html\",\"tag\":\"br\",\"attribs\":null,\"content\":[]},{\"type\":\"text\",\"content\":\" Dr. Ziyad Al-Aly, the chief of research and development at the VA St. Louis Health Care System, spoke with \"},{\"type\":\"html\",\"tag\":\"em\",\"attribs\":null,\"content\":[{\"type\":\"text\",\"content\":\"Quirks & Quarks\"}]},{\"type\":\"text\",\"content\":\" host Bob McDonald about what he's seeing in his research on COVID and the brain. Here is part of their conversation.\"}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"html\",\"tag\":\"strong\",\"attribs\":null,\"content\":[{\"type\":\"text\",\"content\":\"Take me through some of these effects that COVID has had on the brain. What have you seen? \"}]}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"text\",\"content\":\"One of \"},{\"type\":\"html\",\"tag\":\"a\",\"attribs\":{\"href\":\"http:\\u002F\\u002Fwww.nejm.org\\u002Fdoi\\u002Ffull\\u002F10.1056\\u002FNEJMe2400189\",\"target\":\"_blank\"},\"content\":[{\"type\":\"text\",\"content\":\"the key manifestations\"}]},{\"type\":\"text\",\"content\":\" that people experience after SARS-CoV-2 infection is what we call colloquially as brain fog. That's the mental haziness, the inability to remember things, to connect the dots, to really think clearly.\"}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"polopoly_image\",\"attribs\":{\"localCaption\":\"Ziyad Al-Aly studies patients with long COVID and looks at how repeated infections can alter brain function.\",\"captionVisible\":true,\"size\":\"full\"},\"content\":{\"id\":3431639,\"sourceId\":\"1.7172079\",\"url\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172079.1712948983!\\u002FfileImage\\u002FhttpImage\\u002Fziyad-al-aly.jpg\",\"width\":8192,\"height\":5464,\"title\":\"Ziyad Al-Aly\",\"description\":\"Ziyad Al-Aly studies patients with Long COVID and looks at how repeated infections can alter brain function.\",\"credit\":\"Matt Miller\\u002FWashington University School of Medicine\",\"altText\":\"A man with silver-black hair smiles at the camera, he is wearing a pink shirt and dark blue tie with a white medical jacket overtop.\",\"createdAt\":1712948997000,\"modifiedAt\":1712948983000,\"readableCreatedAt\":\"Apr 12 2024, 15:09:57 PM EDT\",\"readableModifiedAt\":\"Apr 12 2024, 15:09:43 PM EDT\",\"useOriginalImage\":false,\"derivatives\":{\"16x9tight_140\":{\"w\":140,\"h\":79,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172079.1712948983!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9tight_140\\u002Fziyad-al-aly.jpg\"},\"16x9_940\":{\"w\":940,\"h\":529,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172079.1712948983!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9_940\\u002Fziyad-al-aly.jpg\"},\"16x9_300\":{\"w\":300,\"h\":168,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172079.1712948983!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9_300\\u002Fziyad-al-aly.jpg\"},\"16x9_620\":{\"w\":620,\"h\":349,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172079.1712948983!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9_620\\u002Fziyad-al-aly.jpg\"},\"16x9_460\":{\"w\":460,\"h\":259,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172079.1712948983!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9_460\\u002Fziyad-al-aly.jpg\"},\"original_620\":{\"w\":620,\"h\":414,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172079.1712948983!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002Foriginal_620\\u002Fziyad-al-aly.jpg\"},\"original_300\":{\"w\":300,\"h\":200,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172079.1712948983!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002Foriginal_300\\u002Fziyad-al-aly.jpg\"}},\"version\":\"1712948983\"}}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":null,\"content\":[{\"type\":\"text\",\"content\":\"In addition to brain fog, \"},{\"type\":\"html\",\"tag\":\"a\",\"attribs\":{\"href\":\"https:\\u002F\\u002Fwww.nature.com\\u002Farticles\\u002Fs41591-022-02001-z\",\"target\":\"_blank\"},\"content\":[{\"type\":\"text\",\"content\":\"we see people\"}]},{\"type\":\"text\",\"content\":\" coming back to the clinics with mini strokes. We see a lot of people with headache disorders, sleep disturbances, sleep problems. A lot of people come back to the clinic with tingling of the extremities, tingling in the legs or sometimes in the arms. In rare cases, seizure disorders. So really a variety of health problems in the brain.\"}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"html\",\"tag\":\"strong\",\"attribs\":null,\"content\":[{\"type\":\"text\",\"content\":\"Is COVID actually affecting the way the brain functions?\"}]}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"text\",\"content\":\"There are \"},{\"type\":\"html\",\"tag\":\"a\",\"attribs\":{\"href\":\"https:\\u002F\\u002Fwww.nejm.org\\u002Fdoi\\u002F10.1056\\u002FNEJMoa2311330\",\"target\":\"_blank\"},\"content\":[{\"type\":\"text\",\"content\":\"studies that have been done\"}]},{\"type\":\"text\",\"content\":\" comparing people who had COVID-19, versus people who didn't, and then gave them cognitive testing to measure their ability to cognitively process information and test their IQ. And there's very clear differences in the IQ of people who had been infected with COVID-19 versus people who did not. Even mild COVID can give people about a three-point loss of IQ.\"}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"polopoly_image\",\"attribs\":{\"localCaption\":\"A protester holding a placard relating to long COVID poses during a gathering outside the U.K. Covid-19 Inquiry building in London, England.\",\"captionVisible\":true,\"size\":\"small\"},\"content\":{\"id\":3431667,\"sourceId\":\"1.7172139\",\"url\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172139.1712949067!\\u002FfileImage\\u002FhttpImage\\u002F1830629750.jpg\",\"width\":3712,\"height\":5568,\"title\":\"1830629750\",\"description\":\"A protester holding a placard relating to long covid poses during a gathering outside the UK Covid-19 Inquiry building in west London, on December 7, 2023 as Britain's former Prime Minister Boris Johnson is inside giving evidence regarding his management of the pandemic. Boris Johnson on Wednesday apologised for \\\"the pain and the loss and the suffering\\\" caused by the Covid-19 pandemic but defended his government at a public inquiry into its handling of the crisis. (Photo by JUSTIN TALLIS \\u002F AFP) (Photo by JUSTIN TALLIS\\u002FAFP via Getty Images)\",\"credit\":\"Justin Tallis\\u002FAFP via Getty Images\",\"altText\":\"A woman faces the camera, she wears a facemask. She's holding up a sign that says \\\"Forgotten, Unheard, Disbelieved, Isolated, Unemployed, Disabled, Immobile\\\"\",\"createdAt\":1712949084000,\"modifiedAt\":1712949067000,\"readableCreatedAt\":\"Apr 12 2024, 15:11:24 PM EDT\",\"readableModifiedAt\":\"Apr 12 2024, 15:11:07 PM EDT\",\"useOriginalImage\":false,\"derivatives\":{\"16x9tight_140\":{\"w\":140,\"h\":79,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172139.1712949067!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9tight_140\\u002F1830629750.jpg\"},\"16x9_940\":{\"w\":940,\"h\":529,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172139.1712949067!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9_940\\u002F1830629750.jpg\"},\"16x9_300\":{\"w\":300,\"h\":168,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172139.1712949067!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9_300\\u002F1830629750.jpg\"},\"16x9_620\":{\"w\":620,\"h\":349,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172139.1712949067!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9_620\\u002F1830629750.jpg\"},\"16x9_460\":{\"w\":460,\"h\":259,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172139.1712949067!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9_460\\u002F1830629750.jpg\"},\"original_620\":{\"w\":620,\"h\":930,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172139.1712949067!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002Foriginal_620\\u002F1830629750.jpg\"},\"original_300\":{\"w\":300,\"h\":450,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172139.1712949067!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002Foriginal_300\\u002F1830629750.jpg\"}},\"version\":\"1712949067\"}}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"text\",\"content\":\"The key caveat to those studies is that most of these were done from the original phase of the pandemic when we didn't have vaccination, when the virus was very different and we didn't have antivirals. And we also don't really know whether those cognitive losses that I just described, that three-point IQ loss, we don't really know how permanent that is. \"}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"html\",\"tag\":\"strong\",\"attribs\":null,\"content\":[{\"type\":\"text\",\"content\":\"What about brain aging?\"}]}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"text\",\"content\":\"It's very, very clear that in some individuals they do experience structural abnormalities and some other abnormalities of imaging that are commensurate with what normal people experience with about \"},{\"type\":\"html\",\"tag\":\"a\",\"attribs\":{\"href\":\"https:\\u002F\\u002Fwww.pnas.org\\u002Fdoi\\u002F10.1073\\u002Fpnas.2217232120\",\"target\":\"_blank\"},\"content\":[{\"type\":\"text\",\"content\":\"seven years of brain aging\"}]},{\"type\":\"text\",\"content\":\". \"}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"text\",\"content\":\"Now the hope is that those effects are not are not long lasting and then the brain with neuroplasticity and other other sort of mechanisms will be able to restore itself back to normal health. But I think that really remains to be characterized and seen in studies.\"}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"html\",\"tag\":\"strong\",\"attribs\":null,\"content\":[{\"type\":\"text\",\"content\":\"How is the virus that comes in through the lungs affecting the brain so much?\"}]}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"text\",\"content\":\"It \"},{\"type\":\"html\",\"tag\":\"a\",\"attribs\":{\"href\":\"https:\\u002F\\u002Fwww.nature.com\\u002Farticles\\u002Fs41586-022-04569-5\",\"target\":\"_blank\"},\"content\":[{\"type\":\"text\",\"content\":\"induces inflammation of the brain\"}]},{\"type\":\"text\",\"content\":\". One of the clear signals from the studies that we've done and a lot of other people have done over the past several years is that clearly in some people it can provoke inflammatory reactions in the brain or neuroinflammation and that may explain some of the symptoms that are experienced by some individuals.\"}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"polopoly_image\",\"attribs\":{\"localCaption\":\"In one study, researchers found that the protective blood-brain barrier was leaky after a COVID infection, which could drive changes in neural function.\",\"captionVisible\":true,\"size\":\"full\"},\"content\":{\"id\":3431873,\"sourceId\":\"1.7172530\",\"url\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172530.1712952480!\\u002FfileImage\\u002FhttpImage\\u002Fshutterstock-medium-file.jpg\",\"width\":0,\"height\":0,\"title\":\"Shutterstock - medium file\",\"description\":\"In one study, researchers found that the protective blood-brain barrier was leaky after a COVID infection, which could drive changes in neural function.\",\"credit\":\"sfam_photo \\u002F Shutterstock\",\"altText\":\"A hand points to brain scans.\",\"createdAt\":1712952621000,\"modifiedAt\":1712952480000,\"readableCreatedAt\":\"Apr 12 2024, 16:10:21 PM EDT\",\"readableModifiedAt\":\"Apr 12 2024, 16:08:00 PM EDT\",\"useOriginalImage\":false,\"derivatives\":{\"16x9tight_140\":{\"w\":140,\"h\":79,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172530.1712952480!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9tight_140\\u002Fshutterstock-medium-file.jpg\"},\"16x9_940\":{\"w\":940,\"h\":529,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172530.1712952480!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9_940\\u002Fshutterstock-medium-file.jpg\"},\"16x9_300\":{\"w\":300,\"h\":168,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172530.1712952480!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9_300\\u002Fshutterstock-medium-file.jpg\"},\"16x9_620\":{\"w\":620,\"h\":349,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172530.1712952480!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9_620\\u002Fshutterstock-medium-file.jpg\"},\"16x9_460\":{\"w\":460,\"h\":259,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172530.1712952480!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9_460\\u002Fshutterstock-medium-file.jpg\"},\"original_620\":{\"w\":620,\"h\":414,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172530.1712952480!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002Foriginal_620\\u002Fshutterstock-medium-file.jpg\"},\"original_300\":{\"w\":300,\"h\":200,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172530.1712952480!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002Foriginal_300\\u002Fshutterstock-medium-file.jpg\"}},\"version\":\"1712952480\"}}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"text\",\"content\":\"It can affect multiple substances on which the brain really depends to function normally. One of them is cortisol. So \"},{\"type\":\"html\",\"tag\":\"a\",\"attribs\":{\"href\":\"https:\\u002F\\u002Fwww.nature.com\\u002Farticles\\u002Fs41586-023-06651-y#:~:\",\"target\":\"_blank\"},\"content\":[{\"type\":\"text\",\"content\":\"people have studied this out of Yale University\"}]},{\"type\":\"text\",\"content\":\" and have shown that people with long COVID or people after SARS-CoV-2 infection can experience inappropriately low level of cortisol and that's actually an important hormone for a lot of certain normal physiologic processes, including cognitive performance, and a lower cortisol level than where it should be can explain some of the cognitive dysfunction that is seen in some people after SARS-CoV-2 infection. There are some studies also done on \"},{\"type\":\"html\",\"tag\":\"a\",\"attribs\":{\"href\":\"https:\\u002F\\u002Fjamanetwork.com\\u002Fjournals\\u002Fjama\\u002Farticle-abstract\\u002F2811556\",\"target\":\"_blank\"},\"content\":[{\"type\":\"text\",\"content\":\"showing low serotonin levels\"}]},{\"type\":\"text\",\"content\":\" and that can also explain some of the symptoms.\"}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"html\",\"tag\":\"strong\",\"attribs\":null,\"content\":[{\"type\":\"text\",\"content\":\"But doesn't the brain have a protective mechanism, the so-called blood brain barrier? \"}]}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"text\",\"content\":\"By inducing that inflammation of the brain, that actually \"},{\"type\":\"html\",\"tag\":\"a\",\"attribs\":{\"href\":\"https:\\u002F\\u002Fwww.nature.com\\u002Farticles\\u002Fs41593-024-01576-9\",\"target\":\"_blank\"},\"content\":[{\"type\":\"text\",\"content\":\"makes that barrier leaky\"}]},{\"type\":\"text\",\"content\":\", so it starts leaking things into the brain that shouldn't be there. And things from the brain to outside the brain that also should not exit outside the brain. So a leaky blood brain barrier can also explain, you know, some of the manifestations that we see after COVID-19 infection.\"}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"html\",\"tag\":\"strong\",\"attribs\":null,\"content\":[{\"type\":\"text\",\"content\":\"How is this changing over time as the virus mutates? \"}]}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"text\",\"content\":\"We're in a different phase of the pandemic now than what we all experienced in March 2020. We have a different virus. We have availability of vaccines now. We have antivirals. So all of these drivers have now sort of reduced overall the burden of long COVID.\"}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"polopoly_image\",\"attribs\":{\"localCaption\":\"A woman rides on an exercise bike to strengthen muscle tone for fatigue related to long COVID at the Department of Rehabilitative Cardiology in Genoa, Italy.\",\"captionVisible\":true,\"size\":\"full\"},\"content\":{\"id\":3316416,\"sourceId\":\"1.6974405\",\"url\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.6974405.1695325434!\\u002FfileImage\\u002FhttpImage\\u002Flong-covid.jpg\",\"width\":6720,\"height\":4480,\"title\":\"Long COVID\",\"description\":\"GENOA, ITALY - JULY 24: Sandra Cabreras, 57, years-old rides on an exercise bike to strengthen muscle tone at the Department of Rehabilitative Cardiology of ASL 3 Genova on July 24, 2020 in Genoa, Italy. Sandra contracted the covid and remained in hospital for 15 days, currently suffering from great fatigue. There is an estimated 14 million confirmed Coronavirus cases worldwide, with a reported 8 million recovering patients. In 80 percent of COVID-19 cases, recovery is complicated, leaving physical and psychological consequences that do not easily subside or allow survivors to return to normal life. The first Covid veterans gym, a project of Genoas ASL 3 and rehabilitation cardiology department directed by Cardiologist Piero Clavario, aims to address this. Designed to reactivate the heart, muscles and lungs, the gyms rehabilitation process gives patients a 2-month protocol, overseen by a staff of four doctors, three nurses, one psychologist and a physiotherapist. Of approximately 700 patients who were previously hospitalized with Covid, 100 patients were selected to participate in the gym, with the first 12 patients now 6 weeks into their rehabilitation process. Treatment is designed to combat numerous effects of the disease. Half of all patients hospitalized for Covid will be left with lung fibrosis, causing difficulty breathing. The nervous system is also affected as evidenced by the loss of taste and smell. Meanwhile 80 percent of patients suffer cognitive impairments such as memory loss and orientation difficulties, 40 percent show symptoms of depression and others experience chronic fatigue and rapid weight loss. \",\"credit\":\"Marco Di Lauro\\u002FGetty Images\",\"altText\":\"Woman rests her hands on handle bars of an exercise bike used to strengthen muscle tone for fatigue after COVID-19.\",\"createdAt\":1695325452000,\"modifiedAt\":1695325434000,\"readableCreatedAt\":\"Sep 21 2023, 15:44:12 PM EDT\",\"readableModifiedAt\":\"Sep 21 2023, 15:43:54 PM EDT\",\"useOriginalImage\":false,\"derivatives\":{\"16x9tight_140\":{\"w\":140,\"h\":79,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.6974405.1695325434!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9tight_140\\u002Flong-covid.jpg\"},\"16x9_940\":{\"w\":940,\"h\":529,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.6974405.1695325434!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9_940\\u002Flong-covid.jpg\"},\"16x9_300\":{\"w\":300,\"h\":168,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.6974405.1695325434!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9_300\\u002Flong-covid.jpg\"},\"16x9_620\":{\"w\":620,\"h\":349,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.6974405.1695325434!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9_620\\u002Flong-covid.jpg\"},\"16x9_460\":{\"w\":460,\"h\":259,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.6974405.1695325434!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9_460\\u002Flong-covid.jpg\"},\"original_620\":{\"w\":620,\"h\":413,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.6974405.1695325434!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002Foriginal_620\\u002Flong-covid.jpg\"},\"original_300\":{\"w\":300,\"h\":200,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.6974405.1695325434!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002Foriginal_300\\u002Flong-covid.jpg\"}},\"version\":\"1695325434\"}}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"text\",\"content\":\"That doesn't mean that SARS-CoV-2 infection is benign, is inconsequential. We still see people in the hospital with SARS-CoV-2 infection, we still see people with strokes and heart attacks and, you know, brain fog and all other manifestations. But overall, I think the risk has sort of declined over the course of the pandemic.\"}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"html\",\"tag\":\"strong\",\"attribs\":null,\"content\":[{\"type\":\"text\",\"content\":\"How unique are these other effects on the body and the brain and the heart to COVID?\"}]}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"text\",\"content\":\"The big revelation or the big aha moment in this pandemic was the realization that SARS-CoV-2 infection can produce this really large basket of long-term adverse health effects. When we went back and started thinking about what happened to people after the Spanish flu, we found historical accounts that sort of resemble, to a large degree, what we described now after SARS-CoV-2 infection, people having increased risk of Parkinson's disease, headaches, brain fog etcetera.\"}]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":{\"dir\":\"ltr\"},\"content\":[{\"type\":\"text\",\"content\":\"I think one of the major lessons that I learned from this pandemic is that there are long tails to pandemics. And yes, we can focus all we want on the acute phase, or the tip of the iceberg, but the reality is that there is a really much larger chunk of disability and disease beneath that tip of the iceberg.\"}]},{\"type\":\"html\",\"tag\":\"hr\",\"attribs\":null,\"content\":[]},{\"type\":\"html\",\"tag\":\"p\",\"attribs\":null,\"content\":[{\"type\":\"html\",\"tag\":\"em\",\"attribs\":null,\"content\":[{\"type\":\"text\",\"content\":\"Q&A edited for length and clarity. \"}]}]}],\"keywords\":{\"tags\":[{\"name\":\"Amanda Buckiewicz\",\"type\":\"person\"},{\"name\":\"Dr. Ziyad Al-Aly\",\"type\":\"person\"}],\"concepts\":[{\"type\":\"subject\",\"path\":\"Health\"},{\"type\":\"subject\",\"path\":\"Health\\u002FDiseases and conditions\\u002FHeadaches\"},{\"type\":\"subject\",\"path\":\"Health\\u002FDiseases and conditions\\u002FInfectious diseases\\u002FCoronavirus\"},{\"type\":\"subject\",\"path\":\"Health\\u002FDiseases and conditions\\u002FInfectious diseases\\u002FCoronavirus\\u002FLong COVID\"},{\"type\":\"subject\",\"path\":\"Health\\u002FDiseases and conditions\\u002FStroke\"}]},\"segments\":[],\"episode\":{},\"vfsection\":\"2.4884\",\"headlineImageURL\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172513.1712952079!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9_620\\u002Fshutterstock-large-file.jpg\",\"categories\":[],\"attribution\":{\"level1\":\"radio\",\"level2\":\"talk\"},\"isPreview\":false,\"adExclusions\":[],\"sponsorBio\":{},\"subject\":[{\"type\":\"subject\",\"name\":\"Health\"},{\"type\":\"subject\",\"name\":\"Health\\u002FDiseases and conditions\\u002FHeadaches\"},{\"type\":\"subject\",\"name\":\"Health\\u002FDiseases and conditions\\u002FInfectious diseases\\u002FCoronavirus\"},{\"type\":\"subject\",\"name\":\"Health\\u002FDiseases and conditions\\u002FInfectious diseases\\u002FCoronavirus\\u002FLong COVID\"},{\"type\":\"subject\",\"name\":\"Health\\u002FDiseases and conditions\\u002FStroke\"}],\"authorList\":[{\"title\":\"Producer\",\"name\":\"Amanda Buckiewicz\",\"biography\":\"Amanda Buckiewicz is an award-winning science journalist with CBC Radio's legendary science show, Quirks & Quarks. Her work can be found on Discovery Channel, BBC Earth, Smithsonian, and Amazon Prime.\",\"url\":\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fradio\\u002Fquirks\\u002Fauthor\\u002Famanda-buckiewicz-1.7103140\",\"links\":[],\"photoDerivatives\":{\"square_140\":{\"w\":140,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7103151.1706893380!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002Fsquare_140\\u002Famanda-buckiewicz-headshot.jpg\"},\"square_620\":{\"w\":620,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7103151.1706893380!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002Fsquare_620\\u002Famanda-buckiewicz-headshot.jpg\"}}}],\"newsletters\":\"false\",\"embedtypes\":[\"audio\",\"photo\"],\"contentarea\":\"radio\",\"subsection1\":\"radio1\",\"subsection2\":\"news\",\"subsection3\":\"quirks_quarks\",\"subsection4\":\"\",\"wordcount\":1025,\"leadMedia\":{\"__typename\":\"Image\",\"sourceId\":\"1.7172513\",\"url\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172513.1712952079!\\u002FfileImage\\u002FhttpImage\\u002Fshutterstock-large-file.jpg\",\"title\":\"Shutterstock - Large file\",\"description\":\"Several studies have found evidence that a COVID infection can cause cortisol and serotonin levels to drop in the brain, which could result in 'brain fog.' \",\"altText\":\"A woman sits at a laptop looking stressed\",\"credit\":\"Shutterstock\",\"useOriginalImage\":false,\"derivatives\":{\"_16x9_780\":{\"w\":780,\"fileurl\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172513.1712952079!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9_780\\u002Fshutterstock-large-file.jpg\"}}},\"photoGallery\":null,\"gs_categories\":[\"gl_english\",\"gv_safe\",\"canimmunize\",\"gb_safe\",\"gs_health\",\"shadow9hu7_pos_covid_test\",\"gs_health_misc\",\"gs_covid19\",\"gt_positive\",\"gt_positive_curiosity\",\"shadow9hu7_pos_corona\",\"gs_science\"],\"gs_keywords\":[\"COVID\",\"COVID-symptoms\",\"infections\",\"CoV\",\"SARS\",\"SARS-CoV-2\",\"pandemic\",\"virus\",\"COVID-19\",\"symptoms\",\"Health\",\"vaccination\",\"Care\",\"cases\",\"COVID-19-infection\",\"flu\",\"long-term\",\"protective\",\"brain-infections\",\"cognitive\",\"clinics\",\"disease\",\"headache\",\"strokes-brain\",\"adverse-health-effects\",\"disability\",\"Health-Care\",\"health-problems\",\"heart-attacks\",\"hospital\",\"illness\",\"Parkinson-s\",\"Parkinson-s-disease\",\"respiratory\",\"sleep-disturbances\",\"sleep-problems\",\"who\",\"Researchers\",\"tingling\",\"aha-moment\",\"benign\",\"depends\",\"know-how\",\"puzzling\",\"revelation\",\"unique\",\"fog\",\"iceberg\",\"physiologic\"],\"authorDisplay\":\"bioandphoto\",\"departments\":[{\"id\":70,\"name\":\"Radio\",\"type\":\"generic\",\"slug\":\"radio\",\"path\":\"Radio\"},{\"id\":306,\"name\":\"Quirks and Quarks\",\"type\":\"show\",\"slug\":\"quirks-and-quarks\",\"path\":\"Radio\\u002FQuirks & Quarks\"}],\"language\":\"en\",\"tags\":[{\"name\":\"Amanda Buckiewicz\",\"type\":\"person\"},{\"name\":\"Dr. Ziyad Al-Aly\",\"type\":\"person\"}],\"concepts\":[{\"type\":\"subject\",\"path\":\"Health\"},{\"type\":\"subject\",\"path\":\"Health\\u002FDiseases and conditions\\u002FHeadaches\"},{\"type\":\"subject\",\"path\":\"Health\\u002FDiseases and conditions\\u002FInfectious diseases\\u002FCoronavirus\"},{\"type\":\"subject\",\"path\":\"Health\\u002FDiseases and conditions\\u002FInfectious diseases\\u002FCoronavirus\\u002FLong COVID\"},{\"type\":\"subject\",\"path\":\"Health\\u002FDiseases and conditions\\u002FStroke\"}]},\"leadVideoOrAudio\":null,\"leadImage\":{\"title\":\"Shutterstock - Large file\",\"credit\":\"Shutterstock\",\"url\":\"https:\\u002F\\u002Fi.cbc.ca\\u002F1.7172513.1712952079!\\u002FfileImage\\u002FhttpImage\\u002Fimage.jpg_gen\\u002Fderivatives\\u002F16x9_780\\u002Fshutterstock-large-file.jpg\",\"showcaption\":true,\"description\":\"Several studies have found evidence that a COVID infection can cause cortisol and serotonin levels to drop in the brain, which could result in 'brain fog.' \",\"type\":\"leadmedia-story\",\"mass\":\"full\",\"alt\":\"A woman sits at a laptop looking stressed\",\"isAboveFold\":true},\"leadPhotogallery\":null,\"leadSegmentOrEpisodeAudio\":null},\"featureflags\":{\"election.region.news\":null,\"election.region.news.canada\":null,\"election.region.news.canada.british-columbia\":null,\"election.region.news.canada.calgary\":null,\"election.region.news.canada.edmonton\":null,\"election.region.news.canada.hamilton\":null,\"election.region.news.canada.kitchener-waterloo\":null,\"election.region.news.canada.london\":null,\"election.region.news.canada.manitoba\":null,\"election.region.news.canada.montreal\":null,\"election.region.news.canada.montreal.quebecvotes2018\":\"QC2018\",\"election.region.news.canada.new-brunswick\":null,\"election.region.news.canada.new-brunswick.nbvotes2018\":\"NB2018\",\"election.region.news.canada.newfoundland-labrador\":null,\"election.region.news.canada.north\":null,\"election.region.news.canada.nova-scotia\":null,\"election.region.news.canada.ottawa\":null,\"election.region.news.canada.prince-edward-island\":null,\"election.region.news.canada.saskatchewan\":null,\"election.region.news.canada.saskatoon\":null,\"election.region.news.canada.sudbury\":null,\"election.region.news.canada.thunder-bay\":null,\"election.region.news.canada.toronto\":null,\"election.region.news.canada.windsor\":null,\"election.region.news.elections.albertavotes2019\":\"AB2019\",\"election.region.news.elections.nlvotes2019\":\"NL2019\",\"election.region.news.elections.peivotes2019\":\"PE2019\",\"election.region.news.politics\":null,\"externalsignin.display\":true,\"globalEventNav\":\"{\\t\\\"links\\\": [{\\t\\t\\t\\\"link\\\": \\\"https:\\u002F\\u002Fwww.cbc.ca\\u002Fnews\\u002Flocal\\\",\\t\\t\\t\\\"title\\\": \\\"Local updates\\\"\\t\\t},\\t\\t{\\t\\t\\t\\\"link\\\": \\\"\\u002Fplayer\\u002Fnews\\u002Flive\\\",\\t\\t\\t\\\"title\\\": \\\"Watch live\\\"\\t\\t}\\t]}\",\"lineup.news\":\"news-hybrid\",\"lineupoptimization\":[\"\\u002Fnews\\u002Fcanada\\u002Fbritish-columbia\",\"\\u002Fnews\\u002Fcanada\\u002Ftoronto\",\"\\u002Fnews\\u002Fcanada\\u002Fmontreal\",\"\\u002Fnews\\u002Fcanada\\u002Fedmonton\",\"\\u002Fnews\\u002Fcanada\\u002Fmanitoba\",\"\\u002Fnews\\u002Fcanada\\u002Fthunder-bay\",\"\\u002Fnews\\u002Fcanada\\u002Fhamilton\",\"\\u002Fnews\\u002Fcanada\\u002Fsudbury\"],\"membership.chatbot.enable\":true,\"membership.featurepage.gemtext\":\"Your CBC account lets you watch all on-demand series and films on CBC Gem, plus stream any of the 14 live, local CBC TV channels, CBC News Explore and CBC Comedy. Stay signed in to continue streaming across devices.\",\"membership.password.recaptcha\":true,\"membership.signin.recaptcha\":true,\"membership.signup.birthDateRequired\":true,\"membership.signup.postalCodeRequired\":true,\"membership.signup.recaptcha\":true,\"membership.sociallogin.apple.usemultiple\":true,\"newsletters.recaptcha\":false,\"olympics.briefing\":true,\"olympics.summernav\":false,\"olympics.winternav\":false,\"page_analytics.life.backintimeforwinter\":{\"cmfAppId\":\"1819.11213.306358\"},\"page_analytics.television.higharctichaulers\":{\"cmfAppId\":\"1819.11213.305211\"},\"player\":{\"useContentItem\":false,\"genericCategorySlugFilter\":[\"live\",\"live-streaming\",\"sports-live\",\"news-live\",\"local-news-shows\",\"livestreams\",\"homepage\"]},\"search.types\":[{\"label\":\"All\",\"value\":\"all\",\"enabled\":true},{\"label\":\"News\",\"value\":\"news\",\"enabled\":true},{\"label\":\"Sports\",\"value\":\"sports\",\"enabled\":true},{\"label\":\"Radio\",\"value\":\"radio\",\"enabled\":true},{\"label\":\"Music\",\"value\":\"music\",\"enabled\":true},{\"label\":\"Arts\",\"value\":\"arts\",\"enabled\":true},{\"label\":\"Comedy\",\"value\":\"comedy\",\"enabled\":true},{\"label\":\"Books\",\"value\":\"books\",\"enabled\":true},{\"label\":\"Life\",\"value\":\"life\",\"enabled\":true}],\"sentry\":{\"enabled\":true,\"tracesSampleRate\":0.1,\"beforeSendFilter\":\"updateTimeRemaining|googleads|pubads_impl|viafoura|BOOMR|\\\\[object HTMLScriptElement\\\\]\",\"allowUrls\":[{\"regex\":\"https:\\\\\\u002F\\\\\\u002F.*\\\\.cbc\\\\.ca\"}],\"denyUrls\":[\"https:\\u002F\\u002Fsecurepubads.go.doubleclick.net\",\"https:\\u002F\\u002Fsecurepubads.g.doubleclick.net\",\"https:\\u002F\\u002Fcdn.viafoura.net\",\"https:\\u002F\\u002Ftagan.adlightning.com\\u002F\"],\"ignoreErrors\":[{\"regex\":\"^AbortError: The play\\\\(\\\\) request was interrupted.*\"},{\"regex\":\"Object Not Found Matching Id\"},{\"regex\":\"webkitExitFullScreen\"},\"AbortError: The operation was aborted.\",\"NotAllowedError: The play method is not allowed by the user agent or the platform in the current context, possibly because the user denied permission.\",\"TypeError: cancelled\",\"Non-Error promise rejection captured with value: Utils::waitUntil: max iteration count reached\",\"n is null\",\"null has no properties\",\"null is not an object (evaluating 'n.getRemainingTime')\",\"Cannot read property 'apiKey' of undefined\"]},\"story.abTest\":{},\"viafoura.header.login\":true,\"weatherwidget.display\":true},\"feedback\":{\"formName\":null,\"referringButtonId\":\"\"},\"fixed\":{\"year\":2024},\"flp\":{\"authorinfo\":\"{ \\\"hidden\\\": [ \\\"television\\\", \\\"radio\\\", \\\"life\\\", \\\"home\\\", \\\"documentaries\\\", \\\"archives\\\" ]}\"},\"gdpr\":{\"euCountry\":false},\"liveRadio\":null,\"loader\":{\"loaded\":true,\"count\":0},\"navigation\":{\"inPage\":{}},\"newsletters\":{\"newsletters\":[{\"id\":134,\"newsletter_id\":\"COM003\",\"name\":\"CBC Research and Updates\",\"slug\":\"cbc-research-updates\",\"description_widget\":\"Sign up to be notified about opportunities to participate in CBC surveys and other research.\",\"description_subscription_centre\":\"Sign up to be notified about opportunities to participate in CBC surveys and other research.\",\"description_subscription_page\":\"Sign up to be notified about opportunities to participate in CBC surveys and other research.\",\"button_text\":\"Sign up\",\"success_message\":\"Thank you. You have been added to our mailing list.\",\"active\":true,\"public\":true,\"show_sample\":false,\"list_order\":-1,\"category\":{\"name\":\"@CBC\",\"id\":1},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":null,\"logo_image_apple_news\":null,\"has_message\":true},{\"id\":2,\"newsletter_id\":\"COM001\",\"name\":\"CBC Newsletter\",\"slug\":\"cbcnewsletter\",\"description_widget\":\"Entertaining and insightful stories delivered weekly. Read, watch and listen to highlights from across the CBC.\",\"description_subscription_centre\":\"Entertaining and insightful stories delivered weekly. Read, watch and listen to highlights from across the CBC.\",\"description_subscription_page\":\"Entertaining and insightful stories delivered weekly. Read, watch and listen to highlights from across the CBC.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of CBC Newsletter will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":1,\"category\":{\"name\":\"@CBC\",\"id\":1},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":{\"filename\":\"1581432639211-cbc-newsletter.png\",\"width\":520,\"height\":108,\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1581432639211-cbc-newsletter.png\"},\"logo_image_apple_news\":{\"filename\":\"1581432639211-cbc-newsletter.png\",\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1581432639211-cbc-newsletter.png\"},\"has_message\":true},{\"id\":1,\"newsletter_id\":\"REV001\",\"name\":\"Contests and Promotions\",\"slug\":\"contests-promotions\",\"description_widget\":\"Subscribe to receive the latest email updates about CBC Contests and Promotions.\",\"description_subscription_centre\":\"Get the latest updates about CBC Contests and Promotions.\",\"description_subscription_page\":\"Subscribe to receive the latest updates about CBC Contests and Promotions.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of Contests and Promotions newsletter will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":false,\"list_order\":2,\"category\":{\"name\":\"@CBC\",\"id\":1},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":null,\"logo_image_apple_news\":null,\"has_message\":true},{\"id\":119,\"newsletter_id\":\"CMP001\",\"name\":\"CBC's Flashback Newsletter\",\"slug\":\"flashback\",\"description_widget\":\"Sign up for this biweekly blast from the past, straight from the CBC Archives.\",\"description_subscription_centre\":\"Sign up for this biweekly blast from the past, straight from the CBC Archives.\",\"description_subscription_page\":\"Sign up for this biweekly blast from the past, straight from the CBC Archives.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of Flashback will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":3,\"category\":{\"name\":\"@CBC\",\"id\":1},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":{\"filename\":\"1597671173450-cbc-archives-flashback-logo.png\",\"width\":520,\"height\":152,\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1597671173450-cbc-archives-flashback-logo.png\"},\"logo_image_apple_news\":{\"filename\":\"1597671217926-cbc-archives-flashback-apple-news.png\",\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1597671217926-cbc-archives-flashback-apple-news.png\"},\"has_message\":true},{\"id\":79,\"newsletter_id\":\"COM002\",\"name\":\"CBC Gem\",\"slug\":\"cbc-gem\",\"description_widget\":\"Subscribe to the CBC Gem newsletter to keep up-to-date with new shows, returning series and exclusives.\",\"description_subscription_centre\":\"Subscribe to the CBC Gem newsletter to keep up-to-date with new shows, returning series and exclusives.\",\"description_subscription_page\":\"Subscribe to the CBC Gem newsletter to keep up-to-date with new shows, returning series and exclusives.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of CBC Gem newsletter will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":1,\"category\":{\"name\":\"TV\",\"id\":8},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":{\"filename\":\"1581433595760-cbc-gem.png\",\"width\":0,\"height\":0,\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1581433595760-cbc-gem.png\"},\"logo_image_apple_news\":{\"filename\":\"1570813761586-cbcgem-logo-apple-news.png\",\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1570813761586-cbcgem-logo-apple-news.png\"},\"has_message\":true},{\"id\":78,\"newsletter_id\":\"DOC001\",\"name\":\"CBC Documentaries\",\"slug\":\"docsnewsletter\",\"description_widget\":\"Get our curated selection of must-watch docs from CBC in your inbox every week!\",\"description_subscription_centre\":\"Get our curated selection of must-watch docs from CBC every week!\",\"description_subscription_page\":\"Get our curated selection of must-watch docs from CBC in your inbox every week!\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of Documentaries newsletter will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":2,\"category\":{\"name\":\"TV\",\"id\":8},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":{\"filename\":\"1570813740102-cbcdocs-logo.svg\",\"width\":0,\"height\":0,\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1570813740102-cbcdocs-logo.svg\"},\"logo_image_apple_news\":{\"filename\":\"1570813719120-cbcdocs-logo-apple-news.png\",\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1570813719120-cbcdocs-logo-apple-news.png\"},\"has_message\":true},{\"id\":120,\"newsletter_id\":\"RAD005\",\"name\":\"CBC Listen\",\"slug\":\"listennewsletter\",\"description_widget\":\"Subscribe to the CBC Listen newsletter to keep up-to-date with the latest and best audio content from CBC Radio, CBC Podcasts, and CBC Music. \",\"description_subscription_centre\":\"Subscribe to the CBC Listen newsletter to keep up-to-date with the latest and best audio content from CBC Radio, CBC Podcasts, and CBC Music. \",\"description_subscription_page\":\"Subscribe to the CBC Listen newsletter to keep up-to-date with the latest and best audio content from CBC Radio, CBC Podcasts, and CBC Music.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of the CBC Listen newsletter will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":1,\"category\":{\"name\":\"Listen\",\"id\":6},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":{\"filename\":\"1598972883657-cbc-listen.png\",\"width\":414,\"height\":44,\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1598972883657-cbc-listen.png\"},\"logo_image_apple_news\":{\"filename\":\"1598972934888-cbc-listen-apple-news.png\",\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1598972934888-cbc-listen-apple-news.png\"},\"has_message\":true},{\"id\":73,\"newsletter_id\":\"RAD001\",\"name\":\"Radio One\",\"slug\":\"radio-one\",\"description_widget\":\"Get the CBC Radio newsletter. We'll send you a weekly roundup of the best CBC Radio programming every Friday.\",\"description_subscription_centre\":\"A weekly roundup of the best CBC Radio programming, on air and online.\",\"description_subscription_page\":\"Get the CBC Radio newsletter. We'll send you a weekly roundup of the best CBC Radio programming every Friday.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of Radio One newsletter will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":2,\"category\":{\"name\":\"Listen\",\"id\":6},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":{\"filename\":\"1581433263108-cbc-radio-one.png\",\"width\":520,\"height\":44,\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1581433263108-cbc-radio-one.png\"},\"logo_image_apple_news\":{\"filename\":\"1581433263108-cbc-radio-one.png\",\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1581433263108-cbc-radio-one.png\"},\"has_message\":true},{\"id\":74,\"newsletter_id\":\"RAD002\",\"name\":\"Sounds Good: CBC’s Podcast Newsletter\",\"slug\":\"podcastsnewsletter\",\"description_widget\":\"From the finest podcast recommendations to behind the scenes exclusives, CBC's biweekly podcast newsletter brings you the latest in news, events and industry buzz from wide world of podcasts. Hear what sounds good from your favourite podcast creators.\",\"description_subscription_centre\":\"From the finest podcast recommendations to behind the scenes exclusives, CBC's biweekly podcast newsletter brings you the latest in news, events and industry buzz from wide world of podcasts. Hear what sounds good from your favourite podcast creators.\",\"description_subscription_page\":\"From the finest podcast recommendations to behind the scenes exclusives, CBC's biweekly podcast newsletter brings you the latest in news, events and industry buzz from wide world of podcasts. Hear what sounds good from your favourite podcast creators.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of Sounds Good: CBC’s Podcast Newsletter will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":3,\"category\":{\"name\":\"Listen\",\"id\":6},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":{\"filename\":\"1581433280757-cbc-podcasts.png\",\"width\":0,\"height\":0,\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1581433280757-cbc-podcasts.png\"},\"logo_image_apple_news\":{\"filename\":\"1581433280757-cbc-podcasts.png\",\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1581433280757-cbc-podcasts.png\"},\"has_message\":true},{\"id\":75,\"newsletter_id\":\"RAD003\",\"name\":\"The Ideas Newsletter\",\"slug\":\"ideas-newsletter\",\"description_widget\":\"Subscribe to our newsletter to find out what's on, and what's coming up on Ideas, CBC Radio's premier program of contemporary thought.\",\"description_subscription_centre\":\"What's on, and what's coming up on Ideas, CBC Radio's premier program of contemporary thought.\",\"description_subscription_page\":\"Subscribe to our newsletter to find out what’s on, and what’s coming up on Ideas, CBC Radio’s premier program of contemporary thought.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of Ideas newsletter will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":4,\"category\":{\"name\":\"Listen\",\"id\":6},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":null,\"logo_image_apple_news\":null,\"has_message\":true},{\"id\":115,\"newsletter_id\":\"RAD004\",\"name\":\"Music Teacher News\",\"slug\":\"musicteachernews\",\"description_widget\":\"Music teachers – stay up to date with all of CBC Music’s upcoming and ongoing music education initiatives.\",\"description_subscription_centre\":\"Music teachers – stay up to date with all of CBC Music’s upcoming and ongoing music education initiatives.\",\"description_subscription_page\":\"Music teachers – stay up to date with all of CBC Music’s upcoming and ongoing music education initiatives.\",\"button_text\":\"Subscribe\",\"success_message\":\"Next issue of the Music Teacher News will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":5,\"category\":{\"name\":\"Listen\",\"id\":6},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":{\"filename\":\"1594152878580-cbc-music.png\",\"width\":520,\"height\":64,\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1594152878580-cbc-music.png\"},\"logo_image_apple_news\":{\"filename\":\"1594150689073-cbc-music-apple-news.png\",\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1594150689073-cbc-music-apple-news.png\"},\"has_message\":true},{\"id\":145,\"newsletter_id\":\"RAD006\",\"name\":\"The NXNW Newsletter\",\"slug\":\"nxnwnewsletter\",\"description_widget\":\"Every recipe, every book recommendation, every music pick. The perfect companion for B.C.’s beloved arts and culture show with Margaret Gallagher.\",\"description_subscription_centre\":\"Every recipe, every book recommendation, every music pick. The perfect companion for B.C.’s beloved arts and culture show with Margaret Gallagher.\",\"description_subscription_page\":\"Every recipe, every book recommendation, every music pick. The perfect companion for B.C.’s beloved arts and culture show with Margaret Gallagher.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of the NXNW Newsletter will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":6,\"category\":{\"name\":\"Listen\",\"id\":6},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":{\"filename\":\"1689702361570-cbc-north-by-northwest.png\",\"width\":520,\"height\":130,\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1689702361570-cbc-north-by-northwest.png\"},\"logo_image_apple_news\":{\"filename\":\"1689702394771-cbc-north-by-northwest-apple-news.png\",\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1689702394771-cbc-north-by-northwest-apple-news.png\"},\"has_message\":true},{\"id\":77,\"newsletter_id\":\"SPO001\",\"name\":\"The Buzzer\",\"slug\":\"thebuzzer\",\"description_widget\":\"Get up to speed on what's happening in sports. Delivered weekdays.\",\"description_subscription_centre\":\"Get up to speed on what's happening in sports. Delivered weekdays.\",\"description_subscription_page\":\"Get up to speed on what's happening in sports. Delivered weekdays.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of The Buzzer will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":1,\"category\":{\"name\":\"Sports\",\"id\":7},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":{\"filename\":\"1581432954501-the-buzzer.png\",\"width\":520,\"height\":140,\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1581432954501-the-buzzer.png\"},\"logo_image_apple_news\":{\"filename\":\"1581432954501-the-buzzer.png\",\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1581432954501-the-buzzer.png\"},\"has_message\":true},{\"id\":76,\"newsletter_id\":\"DIG018\",\"name\":\"Top Headlines from CBC Sports\",\"slug\":\"sports-headlines\",\"description_widget\":\"Get the latest sports news, videos and features, including exclusive content on Canada’s Olympic athletes in our daily email digest.\",\"description_subscription_centre\":\"Get the latest sports news, videos and features, including exclusive content on Canada’s Olympic athletes.\",\"description_subscription_page\":\"Get the latest sports news, videos and features, including exclusive content on Canada’s Olympic athletes in our daily email digest.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of Sports Headlines newsletter will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":2,\"category\":{\"name\":\"Sports\",\"id\":7},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":null,\"logo_image_apple_news\":null,\"has_message\":true},{\"id\":19,\"newsletter_id\":\"NEW001\",\"name\":\"CBC News Morning Brief\",\"slug\":\"morningbrief\",\"description_widget\":\"Start the day smarter. Get the CBC News Morning Brief, the essential news you need delivered to your inbox.\",\"description_subscription_centre\":\"The news you need to know to start the day.\",\"description_subscription_page\":\"Start the day smarter. Get the CBC News Morning Brief, the essential news you need delivered to your inbox.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of CBC News Morning Brief will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":1,\"category\":{\"name\":\"News\",\"id\":5},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":{\"filename\":\"1581433312957-cbc-news-morning-brief.png\",\"width\":520,\"height\":108,\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1581433312957-cbc-news-morning-brief.png\"},\"logo_image_apple_news\":{\"filename\":\"1570814006209-morningbrief-logo-apple-news.png\",\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1570814006209-morningbrief-logo-apple-news.png\"},\"has_message\":true},{\"id\":25,\"newsletter_id\":\"NEW009\",\"name\":\"Minority Report\",\"slug\":\"politicsnewsletter\",\"description_widget\":\"Your weekly guide to what you need to know about federal politics and the minority Liberal government. Get the latest news and sharp analysis delivered to your inbox every Sunday morning.\",\"description_subscription_centre\":\"Your weekly guide to what you need to know about federal politics and the minority Liberal government. Get the latest news and sharp analysis delivered to your inbox every Sunday morning.\",\"description_subscription_page\":\"Your weekly guide to what you need to know about federal politics and the minority Liberal government. Get the latest news and sharp analysis delivered to your inbox every Sunday morning.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of Minority Report will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":2,\"category\":{\"name\":\"News\",\"id\":5},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":{\"filename\":\"1633444389543-cbc-news-minority-report.png\",\"width\":520,\"height\":92,\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1633444389543-cbc-news-minority-report.png\"},\"logo_image_apple_news\":{\"filename\":\"1633444443943-cbc-news-minority-report-apple-news.png\",\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1633444443943-cbc-news-minority-report-apple-news.png\"},\"has_message\":true},{\"id\":22,\"newsletter_id\":\"NEW003\",\"name\":\"What on Earth?\",\"slug\":\"whatonearth\",\"description_widget\":\"The environment is changing. This newsletter is your weekly guide to what we’re doing about it.\",\"description_subscription_centre\":\"The environment is changing. This is your weekly guide to what we’re doing about it.\",\"description_subscription_page\":\"The environment is changing. This newsletter is your weekly guide to what we’re doing about it.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of What on Earth will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":3,\"category\":{\"name\":\"News\",\"id\":5},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":{\"filename\":\"1600710512132-WOE-logo_v4.png\",\"width\":414,\"height\":72,\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1600710512132-WOE-logo_v4.png\"},\"logo_image_apple_news\":{\"filename\":\"1600710561026-WOE-logo-apple-news_v4.png\",\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1600710561026-WOE-logo-apple-news_v4.png\"},\"has_message\":true},{\"id\":18,\"newsletter_id\":\"NEW004\",\"name\":\"CBC Health's Second Opinion\",\"slug\":\"secondopinion\",\"description_widget\":\"A vital dose of the week's news in health and medicine, from CBC Health. Delivered to your inbox every Saturday morning.\",\"description_subscription_centre\":\"A vital dose of the week's news in health and medicine, from the CBC Health team. Delivered Saturday mornings.\",\"description_subscription_page\":\"A vital dose of the week's news in health and medicine, from the CBC Health team. Delivered Saturday mornings.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of CBC Health's Second Opinion will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":4,\"category\":{\"name\":\"News\",\"id\":5},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":{\"filename\":\"1581433540995-cbc-news-second-opinion.png\",\"width\":520,\"height\":130,\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1581433540995-cbc-news-second-opinion.png\"},\"logo_image_apple_news\":{\"filename\":\"1570814044931-secondopinion-logo-apple-news.png\",\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1570814044931-secondopinion-logo-apple-news.png\"},\"has_message\":true},{\"id\":23,\"newsletter_id\":\"NEW011\",\"name\":\"The Royal Fascinator\",\"slug\":\"royalfascinator\",\"description_widget\":\"Your deep dive into all things royal, delivered to your inbox every other Friday.\",\"description_subscription_centre\":\"Your deep dive into all things royal, delivered every other Friday.\",\"description_subscription_page\":\"Your deep dive into all things royal, delivered to your inbox every other Friday.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of The Royal Fascinator newsletter will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":5,\"category\":{\"name\":\"News\",\"id\":5},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":null,\"logo_image_apple_news\":null,\"has_message\":true},{\"id\":16,\"newsletter_id\":\"NEW006\",\"name\":\"The Marketplace Watchdog\",\"slug\":\"marketplace-watchdog\",\"description_widget\":\"Subscribe to our newsletter for consumer news, tips and insider info to help you save cash and stay healthy.\",\"description_subscription_centre\":\"Consumer news, tips and insider info to help you save cash and stay healthy.\",\"description_subscription_page\":\"Subscribe to our newsletter for consumer news, tips and insider info to help you save cash and stay healthy.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of The Marketplace Watchdog newsletter will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":6,\"category\":{\"name\":\"News\",\"id\":5},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":{\"filename\":\"1570813988133-marketplacewatchdog-logo.svg\",\"width\":0,\"height\":0,\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1570813988133-marketplacewatchdog-logo.svg\"},\"logo_image_apple_news\":{\"filename\":\"1570813972634-marketplacewatchdog-logo-apple-news.png\",\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1570813972634-marketplacewatchdog-logo-apple-news.png\"},\"has_message\":true},{\"id\":17,\"newsletter_id\":\"NEW005\",\"name\":\"The Fifth Estate\",\"slug\":\"fifth-estate\",\"description_widget\":\"Subscribe to our newsletter for the latest news, updates and information from The Fifth Estate.\",\"description_subscription_centre\":\"The latest news, updates and information from the fifth estate.\",\"description_subscription_page\":\"Subscribe to our newsletter for the latest news, updates and information from The Fifth Estate.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of The Fifth Estate newsletter will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":7,\"category\":{\"name\":\"News\",\"id\":5},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":null,\"logo_image_apple_news\":null,\"has_message\":true},{\"id\":128,\"newsletter_id\":\"NEW016\",\"name\":\"Mind your Business\",\"slug\":\"mindyourbusiness\",\"description_widget\":\"Your weekly look at what’s happening in the worlds of economics, business and finance. Senior business correspondent Peter Armstrong untangles what it means for you, in your inbox Monday mornings.\",\"description_subscription_centre\":\"Your weekly look at what’s happening in the worlds of economics, business and finance. Senior business correspondent Peter Armstrong untangles what it means for you, in your inbox Monday mornings.\",\"description_subscription_page\":\"Your weekly look at what’s happening in the worlds of economics, business and finance. Senior business correspondent Peter Armstrong untangles what it means for you, in your inbox Monday mornings.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of the Mind your Business will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":15,\"category\":{\"name\":\"News\",\"id\":5},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":{\"filename\":\"1616703190016-cbc-news-mind-your-business (1).png\",\"width\":520,\"height\":140,\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1616703190016-cbc-news-mind-your-business (1).png\"},\"logo_image_apple_news\":{\"filename\":\"1614023615822-cbc-news-mind-your-business-apple-news.png\",\"link\":\"https:\\u002F\\u002Fsubscriptions.cbc.ca\\u002Fnewsletter_static\\u002Fimages\\u002F1614023615822-cbc-news-mind-your-business-apple-news.png\"},\"has_message\":true},{\"id\":55,\"newsletter_id\":\"DIG001\",\"name\":\"Morning Headlines from CBC News\",\"slug\":\"top-headlines-morning\",\"description_widget\":\"Get all the top stories from Canada and the World delivered to your inbox every morning.\",\"description_subscription_centre\":\"Get all the top stories from Canada and the World.\",\"description_subscription_page\":\"Get the latest top stories from across Canada in your inbox every weekday morning.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of the morning edition of the CBC News Top Headlines newsletter will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":1,\"category\":{\"name\":\"Headline News\",\"id\":4},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":null,\"logo_image_apple_news\":null,\"has_message\":true},{\"id\":56,\"newsletter_id\":\"DIG002\",\"name\":\"Evening Headlines from CBC News\",\"slug\":\"top-headlines-evening\",\"description_widget\":\"Get all the top stories from Canada and the World delivered to your inbox every evening.\",\"description_subscription_centre\":\"Get all the top stories from Canada and the World.\",\"description_subscription_page\":\"Get the latest top stories from across Canada in your inbox every weekday evening.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of CBC News Top Headlines newsletter will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":2,\"category\":{\"name\":\"Headline News\",\"id\":4},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":null,\"logo_image_apple_news\":null,\"has_message\":true},{\"id\":71,\"newsletter_id\":\"DIG017\",\"name\":\"CBC News Politics Headlines\",\"slug\":\"politics-headlines\",\"description_widget\":\"Sign up to receive our CBC Politics digest for news and analysis of the day from Parliament Hill, every weekday afternoon.\",\"description_subscription_centre\":\"\",\"description_subscription_page\":\"Get the latest top stories from CBC Politics in your inbox every weekday.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of CBC News Politics Headlines newsletter will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":3,\"category\":{\"name\":\"Headline News\",\"id\":4},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":null,\"logo_image_apple_news\":null,\"has_message\":true},{\"id\":67,\"newsletter_id\":\"DIG016\",\"name\":\"CBC News Entertainment Headlines\",\"slug\":\"entertainment-headlines\",\"description_widget\":\"Get the latest entertainment headlines in inbox every weekday.\",\"description_subscription_centre\":\"\",\"description_subscription_page\":\"Get the latest top stories from CBC Entertainment in your inbox every weekday.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of CBC News Entertainment Headlines newsletter will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":4,\"category\":{\"name\":\"Headline News\",\"id\":4},\"provider\":{\"id\":2,\"name\":\"Adestra\"},\"logo_image\":null,\"logo_image_apple_news\":null,\"has_message\":true},{\"id\":57,\"newsletter_id\":\"DIG003\",\"name\":\"CBC British Columbia\",\"slug\":\"british-columbia-headlines\",\"description_widget\":\"Get the latest top stories from across B.C. in your inbox every weekday.\",\"description_subscription_centre\":\"\",\"description_subscription_page\":\"Get the latest top stories from across British Columbia in your inbox every weekday.\",\"button_text\":\"Subscribe\",\"success_message\":\"The next issue of CBC British Columbia newsletter will soon be in your inbox.\",\"active\":true,\"public\":true,\"show_sample\":true,\"list_order\":5,\"category\":{\"name\":\"Headline News\",\"id\":4},",
    "commentLink": "https://news.ycombinator.com/item?id=40061068",
    "commentBody": "Covid infections are causing IQ drops and years of brain aging, studies suggest (cbc.ca)227 points by luu 3 hours agohidepastfavorite211 comments jl6 3 hours agoThe title is misleading, at least on the IQ claim. The underlying study was observational and did not claim causation. Medical journals and their associated media summaries are awash with low-certainty studies that get de-nuanced and overhyped. These studies are relatively easy to do - not to denigrate the work put in by the original study authors (it’s not their fault the press sensationalize their work). But for strong causative claims you ideally would have blinded controlled randomized trials. Sometimes such trials are not feasible, but that doesn’t mean you get to claim causation anyway just because the work to prove it is too hard. reply dathinab 2 hours agoparentIt's less misleading then you might think. There are cases (enough to not be treated as just some random coincidence; with and without vaccination) where people after Covid had a sever reduction of cognitive abilities that they have problems filling out relatively simple forms. So we can clearly speak of a reduction of IQ. And while there is a lot of research needed and a lot of open questions and already half a year (or so?) into the pandemic we did know from how we observed COVID affecting the brain that it likely will cause a reduction in cognitive abilities at least in some cases which matches the (very many) observations we had since then. This also matches what we know from other older studies about other virus infections which have some similarity with covid, e.g. causing Neuroinflammation. So every thing we observe and do know about the underlying mechanics implies that there is very likely a causation. (Through we need more studies for Neuroinflammation.) So scientist do need to research it more and do need to pin down the exact mechanisms this causes reduction of cognitive abilities in some cases to largely varying degrees and how much of it is a direct and a indirect e.g. psychological effect. But for everyone else it best to assume that getting COVID might reduce IQ if they are unlucky. Because if you aren't a doctor, scientist or similar but just a random person who got COVID it kind doesn't matter if your IQ gets reduced by COVID directly or indirectly or just correlated due to some other factor you don't know about and in turn can't systematically avoid. reply freehorse 1 hour agorootparentI do not think IQ tests are meant or designed to measure effects from illness. We do not say that IQ drops when you are sick with the flu and have high fever that makes it hard to think. IQ does not drop when you wake up groggy in the morning up until you drink coffee. I am not sure why people use IQ here instead of referring specifically to brain fog and other symptoms of post-COVID infection. IQ does not drop when you get sick and does not increase when you get treatment. If that is the case, then it is even worse measure than it is supposed to be, because it does not measure anything remotely stable. reply DoughnutHole 1 hour agorootparentWho says there’s even anything stable to measure? If “intelligence” is just the degree/quality of someone’s brain function then your intelligence does drop when your brain function is impaired, and you would expect that to be picked up in an intelligence test. Culturally we may want to imagine intelligence as something constant and innate to somebody’s soul, but in reality it’s innate to a fragile and fickle organ inside your skull. reply freehorse 45 minutes agorootparent> Culturally we may want to imagine intelligence as something constant and innate to somebody’s soul, but in reality it’s innate to a fragile and fickle organ inside your skull. Which is also my issue with IQ tests and the relationship of the tests and what they measure. But that changes nothing: IQ tests are not supposed to measure _that_ variability of when you get sick. Or you wake up. Or you are drunk. It makes no sense to use a psychometric test completely out of the context of which it was validated and meant to be used, because then the results are not interpretable. People with that kind of post-covid syndrom have an illness and experience cognitive deficits because of that, but using IQ to measure intelligence in that case when what you actually (want to) measure is brain fog is absurd. That should require a brain fog screening, not an IQ test. reply Vecr 15 minutes agorootparentOne of the major uses of IQ tests is to measure brain damage, otherwise why would they be mostly calibrated in the downwards direction? Who do you think pays for these things? reply k__ 44 minutes agorootparentprevThe tests themselves are misleading. If I do one sober, I have an IQ of 110, if I do it with my ADHD meds, it goes up to 120. Is that because the meds make me smarter? No, it's because I don't lose attention after 10 minutes of test and can concentrate until the end. However, the numbers imply otherwise. reply CiteXieAlAlyEtc 25 minutes agorootparentprevSeconding the sentiment here. Quibbling about \"causal\" being used in the title aside, we're building more observational evidence that there's negative cognitive effects associated with covid and long covid. Zhao et al '24 [0] comes immediately to mind; operating on a radically different set of people and data sets from Al-Aly and the gang, they find substantial differences in simple reaction time between covid and no-covid groups. It seems we're in a phase of the pandemic where denying long tail effects is going to become more prominent as concerning evidence continues to pile up. The name of the game is still to reduce your infection count as much as possible. Stay safe out there, share aerosols unprotected at your own peril. [0] https://www.thelancet.com/journals/eclinm/article/PIIS2589-5... \"Long COVID is associated with severe cognitive slowing: a multicentre cross-sectional study\" reply cies 1 hour agorootparentprev> But for everyone else it best to assume that getting COVID might reduce IQ if they are unlucky. Why assume it if it's not proven yet? We had crazy assumptions on the way lightning worked, I'm glad we have empirical natural sciences now to counter these \"wide spread assumptions\". Wrt covid and IQ. Say that during covid IQs measurably dropped: why says its not the WFH, the fear that was rampant or the mRNA vaccine that was massively injected? There for we need bigger studies, control groups, randomization. All of this is there for good reasons (and a lot of this was skipped in the development of mRNA vaccines for a bad reason: fear). reply aredox 1 hour agoparentprevThat's why the most important element in science is replication - direct (repeating the same experiment) and indirect (refining the original experiment, or testing for the opposite result, or an alternative - here we could test reflexes instead of IQ). And here there is already a lot of replication for several years - plus direct observation of bleeding into the brain. https://www.tcd.ie/news_events/articles/2024/trinity-team-di... reply stuaxo 1 hour agorootparentThanks for this link on brain fog, I had that for a year after getting Covid early before vaccines were available. I know there was work recently that found blood tests can find various markers of long covid, I'd like to get this done - but it's not something widely available. A relative has been getting strange post-viral like symptoms since before Xmas (though no obvious Covid infection), would be great to get them tested too. reply p0w3n3d 2 hours agoparentprevI experienced some limitations of cognitive abilities but not sure was it due to COVID or some sort of depression reply shutupnerd0000 2 hours agorootparentThat settles it then. Causation proved. reply varjag 51 minutes agorootparentI too have lost solid 20 points immediately after infection. Took the test precisely because the fall was so sharp and deep. Scoff all you want but for me it's settled. reply camgunz 2 hours agoparentprevIt says \"studies suggest\"; how much more hedging do you want? reply rjmunro 1 hour agoparentprevCould it be just a correlation? Maybe something like people with lower IQ are less likely to do a job where they can work from home so they are more likely to have been exposed to the virus? reply jonathanstrange 1 hour agorootparentThe article mentions studies that measure IQ before and after a Covid infection, it's hard to see how working from home or not could be relevant for these. reply throwaway48476 2 hours agoparentprevThere's a lot of data on the cognitive effects of elevated CO2 exposure. The years of masking would likely have a similar effect. reply aloisdg 15 minutes agorootparentAlthough, significant increase in CO2 concentrations are noted with routinely used face-masks, the levels still remain within the NIOSH limits for short-term use. Therefore, there should not be a concern in their regular day-to-day use by people. https://bmcinfectdis.biomedcentral.com/articles/10.1186/s128... reply dukeyukey 1 hour agorootparentprevI'll bet surgeons wear masks for more time than most people did during COVID, but we don't see surgeon IQs tanking. reply fxtentacle 1 hour agorootparentprevThat must be why everyone in Japan is an idiot ;P They've been wearing masks on the daily commute since long before Covid. /s reply orwin 1 hour agorootparentAnd why surgeons are stupid and have fine motricity issues, with 4 hours shift wearing masks nonstop! reply npunt 3 hours agoprevNeuroinflammation is a poorly studied area and one likely cause of brain fog and mecfs/long covid symptoms. Jarred Younger [1] is a researcher studying this and has a youtube channel [2] where he discusses his latest work. If you're interested in this it's worth watching! Also please ignore anyone that posts 'oh its just X' or 'just do X', whatever X may be. This is a complicated medical mystery that millions of patients have suffered from for decades if you include mecfs and postviral conditions. There is no X, patients have tried every possible X. Anyone who tells you its just X is revealing their lack of understanding of the issue (this happens every time this subject shows up on HN and elsewhere). [1] https://me-pedia.org/wiki/Jarred_Younger [2] https://www.youtube.com/channel/UCoVoOvIX90IMEZCbBf_ycEA reply ImHereToVote 2 hours agoparentWasn't there a paper that proposed the idea that long Covid is actual replicating virus that replicates in very low numbers. reply npunt 49 minutes agorootparentViral persistence is one theory that may explain a subset of long covid and other postviral cases (mecfs example [1]). There's lots of theories including itaconate shunt [2] and metabolic trap [3], glutamate toxicity [4], cranio-cervical instability and infections causing breakdown of tendons [5], gut biome imbalances [6][7], endothelial damage, and others. I'm not sure what the most promising ones are but the search for biomarkers continues. What we call long covid or mecfs or chronic lyme or fibromyalgia etc etc are all umbrella terms for symptom clusters so they lack some specificity and are likely multi-causal. There are many overlaps in LC and mecfs patient populations in studies (vs controls), so its likely a fair-to-significant amount of LC is actually what we call mecfs. Much of the mystery and suffering of long covid would have been avoided if we'd invested anything in investigating these conditions pre-covid (mecfs has the least funding to disease burden and the worst quality of life of all major diseases), but the medical community generally didn't want to believe these diseases were real. Same thing happened with autoimmune conditions like lupus and MS before their mechanisms were discovered. All of these conditions predominately affect women. [1] https://me-pedia.org/wiki/Persistent_infection_hypothesis [2] https://me-pedia.org/wiki/Itaconate_shunt_hypothesis [3] https://me-pedia.org/wiki/Metabolic_trap [4] https://me-pedia.org/wiki/Glutamate [5] https://me-pedia.org/wiki/Craniocervical_instability [6] https://me-pedia.org/wiki/Microbiome [7] https://www.remissionbiome.org/ reply skrebbel 3 hours agoprevMy wife has postcovid and this is a profound effect. Yesterday she tried to add up a series of numbers by heart, and she simply couldn’t. For context, she’s pretty mathy and used to mock me for taking out a calculator or a spreadsheet for stuff like that. She also forgets everything, which is another thing she used to be great at. It’s pretty confronting, especially now that the idea that maybe this isn’t temporary is taking shape. reply strogonoff 14 minutes agoparentThere is an interesting paper (the main author seems to be from Netherlands, too) about potential mechanism of action that makes Covid infections have long-term repercussions published last year at https://www.frontiersin.org/journals/immunology/articles/10..... > Based on these data, we hypothesize that the shift to anaerobic respiration causes an acid-base disruption that can affect every organ system and underpins the symptoms of PASC. (Brain, of course, is also an organ, so brain fog and reduction in mental capabilities could easily fall under this.) reply underdeserver 3 hours agoparentprevHow long ago was she sick? For me it's gotten better. I got sick in August 2021, the major effects (fatigue and brain fog) were most pronounced between September and February, and I've been steadily improving since. I think I'm somewhere around 80% pre-Covid capacity most days. reply skrebbel 3 hours agorootparentWoa this is a great thing to hear actually. She got sick January 2022 so a bit over 2 years now. Our current fear is that this is the kind of thing where either you recover within ~2 years or it’s forever. Appreciate your story and will share with her. Her other symptoms, particularly the fatigue, do seem to improve (though the weather and being about to be outside impacts it a lot, so she had a huge setback 2 winters in a row now), though very slowly. I like hearing that for you the brain thing has improved over time too, thanks! reply underdeserver 2 hours agorootparentHappy my story can help. It's been a long ride, coming up on three years. Because of the symptoms I was prompted to invest in self improvement as well. I am much more diligent about regularly exercising, eating better, minding my mental health* and minding my sleep; and there are still bad days where I can't concentrate at all. But things are getting better overall. * Every day I don't read or watch the news I'm happier. reply skrebbel 2 hours agorootparentThe news trick really helps huh. We do it too and wow reply vincvinc 1 hour agorootparentprevI am in 3 years post-covid. Definitely saw some lows but have been recovering steadily. Definitely look into Hyperbaric Oxygen Treatment for this, for me it helped fight the plateau. reply pjerem 2 hours agorootparentprevWeather impact on mood and energy is unbelievable, at least in myself. Also, if that can help, contrary to my previous beliefs, light therapy is no bullshit. I thought it was another pseudo science but studies are in fact proving that it’s incredibly effective (but it’s pretty inconvenient to incorporate in your habits). reply silicon_wally 12 minutes agorootparentwhat sort of light therapy have you found beneficial? Like blue light in the eyes or red light on the skin? reply nirui 2 hours agoparentprevOdd. I also felt that my memory is not as good as it was used to, and I experiences Doorway effect (https://en.wikipedia.org/wiki/Doorway_effect) more frequently in recent years compare to before. But I'm not really sure it was due to COVID or just simply being old (or due to been forced to live in a confined space for long period of time). reply sorwin 2 hours agorootparentThis seems to be the issue - we're trying to study COVID, which affected literally the whole population of earth at this point, and we have no control group of people who have not been affected. People have gotten older, have gone through mass societal change, and any other number of things in between. reply prmoustache 2 hours agorootparentAlso in every study we need a control group. How do you know for sure that the control group didn't get COVID. For all I know there is no way to know for sure you haven't been infected or sick of Covid in those 4 years, it is not like testing has been very reliable. reply lazyasciiart 1 hour agorootparentI know people who have been in long term covid studies since maybe April 2020, in what was the Seattle Flu study: self testing at least weekly back then and sending in samples regularly. The first people recruited were all the medical researchers at that institution and nearby who worked on things with no connection to covid, but relevant enough lab knowledge that they were trusted to understand how to self test. reply lelanthran 1 hour agorootparentprev> How do you know for sure that the control group didn't get COVID. I dunno if that is relevant - it's the symptoms (brain inflammation[1]) that is hypothesised to correspond with lower cognitive function. Even if you did have COVID, but didn't get brain inflammation[1], you can still serve as a control. [1] I say \"brain inflammation\"; the exact mechanism is explained by a comment upthread. reply newsclues 38 minutes agorootparentprevAnd some people might be faking it. reply dguest 2 hours agorootparentprevI always thought \"getting old\" was in part an accumulation of damage from infections and slow wear on the body. reply TheFreim 3 hours agoparentprevCould it be that she's simply out of practice? reply skrebbel 3 hours agorootparentI doubt it. It’s not like you ever stop practicing remembering things or doing everyday math tasks. Look, this is anecdata. I’m not making a claim about this article. I just know that my wife got substantially stupider overnight 2 years ago (plus lots of other symptoms). Via a postcovid support group organized by a semi government club here in NL she knows a bunch more people with long-term postcovid and this is the one symptom they all share. Might well still be some weird sort of selection bias etc; again, I’m not saying this is generalizable science, I’m saying that my wife got noticeably less smart and it started at the same time as all her other postcovid symptoms. reply cjk2 2 hours agorootparentI would be careful with some of the self-help groups. They can reinforce symptoms rather than improve them. A case in point is a friend of mine who thought she had post-covid symptoms and cognitive decline. She went to a similar group and found a lot of very miserable people with depression. When talking to them they had similar stories of lost relatives, lost jobs and general social decline. But all had a common thing which was they weren't dealing with those issues in a healthy ways. It was turning into a drinking club pretty much after the event was over. Even if you had problems this would exacerbate them over time. There was a hefty dividing line among my friendship group after covid which was people who went down that route and people who went down the health and fitness route. The latter seem to be doing much better in all aspects even if they started in the former group. I was running 10km once a week before. It knocked me off my feet entirely and I don't run often now which got to me for a few months. But I fought it and can do that again now if I need to. It hurt that it did that to me but it taught me how fragile things are. Also to note I'm a qualified mathematician but some days to months I can't add numbers together and this turns out mostly to be I'm tired or not in the right state of mind rather than anything persistent. Lots of causes - best to look for the obvious ones first. reply resource_waste 59 minutes agorootparentI'm pretty scared to look into many mental health issues. I know a buddy who had a super mild case of tourette syndrome but after he researched things started getting worse. On a similar note, I had some mild PTSD symptoms until I looked into it, then I started having additional symptoms(or at least I was noticing them now). On the flip side having ChatGPT4 combine my PTSD with phenomenology, positive psychology, and CBT, I think I'm close to cured. My favorite part was being able to disagree with chatGPT. When I disagree with humans, I feel like I was getting stuck in a loop. reply abyssin 46 minutes agorootparentCould you give more explanations about how you’ve been using ChatGPT to get better? reply skrebbel 2 hours agorootparentprevYeah, well aware. Fortunately her group is an impressively can-do-attitude bunch, and she’s getting very concrete ideas from it. I think it helps that it was organized by a government-y instance who did some checking, and isn’t some facebook group turned physical or sth. (Also, no drinking. That would be madness. Nobody in that group drinks at all anymore, it would set back their recovery by a lot) For example, she had a cold last march which totally knocked her remaining bits of physical energy out of her, and she worried that she wouldn’t be able to come on a planned family trip to the zoo, because of the lengthy walking involved. She shared that worry in the group and another woman said “but then just rent a wheelchair at the entrance and get yourself rolled around most of the time! That’s kinda confronting, “I’m now a person who sometimes needs a wheelchair”, but it’s also very constructive and awesome if you get over that and it helps you get out of the house. We did that trip, it was super awkward (esp when she stood up from the wheelchair and everybody could see that well actually, her legs work fine), and we had a great time and for the first time after a big outing she wasn’t sick in bed for 3 days after. Btw, a sidenote, “just go sporting” is terrible advice for people with heavy postcovid. Look up “post exertional malaise”. You gotta build it up very carefully, never push yourself too hard. It’s very hard but it appears to work (given enough patience). reply kadkadels 2 hours agorootparentDon't worry about it being awkward, there are many reasons for being in a wheelchair besides not being able to walk at all. I mean, the fact that wheelchairs are provided at the entrance indicates that there is a demand from individuals who can walk to access them. reply skrebbel 2 hours agorootparentNice observation! Appreciate this reply nine_k 2 hours agorootparentprevCam relate. After a month of being floored by COVID in 2020 (thankfully without a need for forced ventilation), I had pretty terrible brain fog. After a year, the fog was gone, but to this day many mental tasks take more effort and time, even though I can still do them again. reply changelink 2 hours agoparentprevMy girlfriend has pretty bad post-covid, the thing that helped her most was getting on Citalopram. There's suspicions that many SSRI's have a anti-inflammatory effect on the brain, which might be the mechanism. She's currently in oxygen therapy, we're positive about the effects but since it's dominating her life (and energy) at the moment it's hard to say what the effect is. reply vertis 2 hours agorootparentSSRIs have some nasty side effects though, including sexual dysfunction. reply changelink 7 minutes agorootparentFor sure, but so does heavy post-covid. It's a trade-off that's well worth it for her. It improved her memory and brain clarity a fair bit to the point that it's doable to get through the days hoping for a cure (or at least a way to get back to a normal life). reply sneak 3 hours agoparentprevI have a very math-heavy competitive hobby which I found myself suddenly very bad at (relative to my previous performance) for 6-8 weeks after all my other covid symptoms were gone. I appear to have recovered within 90 days, but I don’t know if I was left with permanent damage or not. I had no way to objectively test. reply yyyfb 2 hours agorootparentYour competitive performance level seems like a pretty objective way to test reply regularfry 1 hour agorootparentI've noticed this with bullet chess, although not with reference to COVID specifically. My rating (such as it is) is extremely sensitive to mental health issues. reply piva00 1 hour agoparentprevI got infected with COVID twice, thankfully only after being vaccinated and didn't experience heavy physical effects but the cognitive issues I had on my first infection triggered a really bad anxiety. At some point I was playing chess with my girlfriend after 4-5 days from getting symptoms, after 3 moves (so pretty much still in the beginning of the openings) my brain simply could not understand what I was trying to do, like I couldn't connect a previous thought to the next, there was an impassable barrier between these thoughts that I couldn't get through. It got worse when I realised I had no idea what I was doing and while looking at the board I couldn't remember how the pieces moved, I looked at my bishop on the board and couldn't think where it could go. I started to cry, my whole job is to think things through, connect diverging thoughts, I got extremely scared I was going to lose this basic ability that enables me to do my job... I lost the senses of smell and taste for about 10 days so definitely had neurological issues from COVID, the feeling of not being able to think properly was the worse of the feelings. The fever was ok, never broke too high and it was short-lived for about a couple of hours, feeling my heart was not comfortable but bearable and I wouldn't get too tired. I'm so glad it was temporary, and so sorry for your wife having this lingering on her, I do not wish it to anyone. reply jeffhuys 3 hours agoparentprevHas she been vaccinated? How many times? reply defrost 2 hours agorootparentAustralia has a population of 26 million, longer life expectancy than the USofA, and comprehensive medical records going back many decades. With ~98% of the population COVID vaccinated multiple times any vaccine effects would stick out like dogs balls. If anyone wants to posit \"vaccines cause { stupidstrokesetc }\" Then such correlations should be readily apparent in the beforeafter records of 26 million test subjects. reply t0bia_s 1 hour agorootparentMortality of productive population 15-44 still rising after 2021 in EU. https://euromomo.eu reply defrost 1 hour agorootparentThank you for the link. Currently the pooled deaths 15-44 are reported as below the long term 1,600 baseline ( https://euromomo.eu/graphs-and-maps/ ). Regardless, I'm not sure if you have a point here that connects vaccination to significant numbers adverse after effects. reply kadkadels 20 minutes agorootparentprevCould also be: delayed checkups, e.g. blood pressure, cancer. Delayed treatment. Other transmittable diseases, for example novel flu variants. But no, it has to be the vaccine. reply logicchains 2 hours agorootparentprevAustralia's excess death rate is still higher than pre-covid: https://www.abc.net.au/news/2023-12-20/mortality-rates-austr... . If the Australian government released the data it'd be trivial to identify whether these deaths were concentrated in the vaccinated or the unvaccinated, but so far it hasn't released such data (if it even possesses such data in aggregate format). reply kadkadels 2 hours agorootparentWell, in that case, good that Australia isn't the only country in the world: https://www.ons.gov.uk/peoplepopulationandcommunity/birthsde... which clearly shows that all-cause deaths in England were higher among the unvaccinated. Here is a Reuters fact check: https://www.reuters.com/fact-check/no-evidence-link-uk-exces... reply mike_hearn 1 hour agorootparentThe ONS data cannot be used that way unfortunately, and the ONS themselves have admitted to that fact. If you take the ONS data literally then COVID vaccines are a magical elixir of luck that protect against every cause of death including car crashes. There are at least two problems: 1. The statistically invalid time-windowing games the public health agencies all played in which people who had taken vaccines were classed as unvaccinated. The sibling comment talks about that. Prof Norman Fenton has shown how this yields incorrectly high calculations of effectiveness. 2. Healthy vaccinee bias, in which people who are about to die aren't vaccinated at all because there's no point, and the sort of people who take lots of vaccines tend to be obsessive about health and risk in other ways. Problems like these with observational data are why drugs are put through trials before being launched to market. If we check the data for say the Pfizer trial, what we see is no effect on mortality and serious problems with statistical power also, simply because so few people were dying of COVID it was basically impossible to run a trial large enough to prove it had any effect. reply kadkadels 26 minutes agorootparentWe are not talking about vaccine efficiency or efficacy here though but adverse events specifically. >If we check the data for say the Pfizer trial, what we see is no effect on mortality The point is that there are almost no serious adverse events occuring post-vaccination. There is no pharmacovigilance safety signal or concerning pattern wrt MRNA-vaccines. As we have seen in 2021 with the astra zeneca vaccine, post market surveillance worked and it was promptly investigated as soon as a safety signal emerged. EMA (and other regulatory agencies) communicated transparently about their findings and decisions. Thank you for reminding me of this fine example of post-market surveillance, I almost forgot :) reply armchairdweller 2 hours agorootparentprevThe UK ONS data used to have the obvious flaw that people dropping dead 3 days after their first dose were defined as „unvaccinated“. In this regard, the significant uptick in deaths in summer 2021 in „unvaccinated“ looked interesting to say the least. How are they doing their definitions now? Could not find it. Meanwhile this new Dutch data looks quite interesting, maybe someone here would like to give it a shot: https://www.cbs.nl/nl-nl/longread/rapportages/2022/sterfte-e... https://opendata.ecdc.europa.eu/covid19/ By now there is a hell lot of data pointing at issues with the novel pharmaceutical product. Meanwhile governments all over the world continue to be reluctant to issue data on e.g. „long covid“ or immune system problems in vaccinated vs. unvaccinated, although this data would settle the debate once and for all if its outcome is in favor of getting the shots. „Fact checkers“ have been shown again and again to be politically biased up to active manipulation. Only people who always agree with the politics they want to push have not noticed at this point. reply mike_hearn 1 hour agorootparentThe Dutch data has the same problem as the ONS data that I describe in my other comment: > For the first mRNA vaccination, almost all [hazard ratios], for the different weeks after vaccination and different subgroups, are significantly lower than 1, which indicates a reduced risk of non-COVID-19 death shortly after vaccination. Obviously vaccination is not supposed to reduce your risk from things unrelated to COVID-19 yet in their data it does. This isn't because it's some magical cure-all. It's because their data is biased. The correct hazard ratio is found only in the 12-49 age group. As they say in their discussion, this is because \"in the event of a very short remaining life expectancy, (a subsequent dose of) COVID-19 vaccination may be decided against ... Vaccination is also postponed in the event of a fever ... This is also called the 'healthy vaccine effect' and it is difficult to correct for this in observational research ... the results should be interpreted with caution\" They also have the problem that \"people who have been vaccinated, but have not given permission for registration in CIMS, have been incorrectly classified as unvaccinated (5 to 7 percent of the people)\" which means that \"This will have influenced the analysis of the risk of death after the first vaccination, because in this analysis the unvaccinated person time served as a reference\" So this dataset is not usable for detecting vaccine side effects or induced deaths. It is a hopelessly corrupted dataset in which large numbers of people are misclassified and it's not a randomized study. But this is public health research, so after saying results should be interpreted with caution they go ahead and make the totally incautious claim that \"Based on these results there are no indications at a population level that COVID-19 vaccination increases the risk of death due to an adverse event.\" which is a false statement. Their data doesn't allow them to draw such conclusions. The reason this topic is neuralgic is because if you want to be scientifically correct then you can't actually know whether vaccines reduced deaths or not, but people really want to believe they did. The trials that were supposed to unambiguously measure this failed, in the sense that they showed no effect on deaths and yielded incorrect conclusions about reductions in infection rates (the number started at 95% effective and then was regularly revised, this is not supposed to happen and was due to their time windowing problems). Then attempts to measure it using mass datasets collected outside of RT context all yield incorrect conclusions due to dataset bias, but are presented as definitive anyway for political reasons. Science is truly in a bad shape :( reply armchairdweller 26 minutes agorootparentThank you for this summary. I have seen some analyses (including from the „pro vax“ side, even though it is to be despised for their fascist behavior alone) that made the Dutch data appear more useful for these questions. But tbh, what did I expect - last time I checked the Dutch government still voted against doing an honest investigation into the ongoing excess deaths too. Is there any official data of this kind you would trust right now? To be fair, I have seen enough by now; the main open question for me is what role endemic covid in all those believing the cure-all narrative and going out partying before milder variants came out played. reply kadkadels 1 hour agorootparentprevRule number 1: don't argue with conspiracy theorists, they will come up with all sorts of nonsense. >„Fact checkers“ have been shown again and again to be politically biased up to active manipulation. Only people who always agree with the politics they want to push have not noticed at this point. Have they? Show me some of those manipulated fact checkers, please. With reputable sources, I'm not inclined to read bullshit. >By now there is a hell lot of data pointing at issues with the novel pharmaceutical product. Not really, no. Or show us some data. >Meanwhile this new Dutch data looks quite interesting, maybe someone here would like to give it a shot: conveniently only available in dutch, but online translators to the rescue: >https://www.cbs.nl/nl-nl/longread/rapportages/2022/sterfte-e... where the first paragraph in 6.4 translates as follows: >The analyses show a lower or comparable likelihood of dying from causes other than COVID-19 in the first weeks after receiving a COVID-19 vaccine, compared to the vaccination status before receiving the respective dose. This holds true for all age groups and also for long-term care users. Based on these results, there is no population-level evidence that COVID-19 vaccination increases the risk of death due to an adverse reaction. Oops. Well, you tried. reply armchairdweller 1 hour agorootparentYou seem to be really emotional about this. Need to say, the psychogram of those who defend pharma relentlessly to this day is very interesting. I see anger, insults, trampling onto what an obviously fascist authority teached them at the end of 2021 is „below them“ to elevate oneself. Ignorance of obvious data, sidelining. By now I have seen enough actual data - and on top of that a lot of sad „anecdata“ in my direct surroundings - to pay attention at least. Given its prevalence I am sure you have seen the anecdata as well. Have you called someone who claimed to have a vax damage or who told you a relative died from the vax a „conspiracy theorist“ yet? One good alternative point to start your journey in 2024 could be the US civilian labor force disability data. And no, I won’t waste time to search for that for you, given that you began your „response“ by attaching a label because it soothes you mentally. It has all been out there for long time and has been linked countless times. The little section you got from google translating is a government-approved interpretation of this data. I have seen several others. You can download the data and fact-check for yourself. reply kadkadels 1 hour agorootparent>You seem to be really emotional about this. I'm not, don't worry :) >Need to say, the psychogram of those who defend pharma relentlessly to this day is very interesting. I see anger, insults The only one who is hurling insults and seems quite angry is you. >fascist authority ah yes, just call regulatory agencies \"fascist\", that makes total sense. >attaching a label because it soothes you mentally once again an insult instead of data >The little section you got from google translating is a government-approved interpretation of this data No it's not. It's what scientists say. Are they fascists too now? reply armchairdweller 47 minutes agorootparent> ah yes, just call regulatory agencies \"fascist\", that makes total sense People were fired for not taking a novel pharmaceutical product, warp-speeded by Donald Trump and sold as a cure-all. They lost their livelihood over exercising a fundamental human right. In places like Canada they could not use public transport or leave the country. In some countries they could not enter supermarkets. Austria decided to fine them steeply and monthly. In the US 60% of one political side agreed with throwing them into camps and discussed it openly. Across all media and in society they were labeled and treated as pariahs. Meanwhile they observed obvious carnage in their friends and family and could only watch. What did you do at the end of 2021 while all of this happened? reply lazyasciiart 1 hour agorootparentprev> Have you called someone who claimed to have a vax damage or who told you a relative died from the vax a „conspiracy theorist“ yet? Not to her face, no. Not in those words, at least. But that’s because it would upset my mother to have to deal with her idiot conspiracy theorist sister complaining about me being rude, so I am scrupulously polite while dismissing all her crap and she is increasingly sensible about not mentioning it around me. Just as well I started this effort before covid happened or it might not have been enough. reply t0bia_s 1 hour agorootparentprevIm not sure how anyone could still belive in factcheckers that claims previously that vaccination gives immunity to virus or that there are no side effects from vaccination even though we have plenty studies that monitor it quite precisely. https://news.ycombinator.com/item?id=39772059 reply kadkadels 1 hour agorootparentAnd I'm not sure how anyone can be a conspiracy theorist and honestly believe that all governments and regulatory agencies all over the world try to hide some grand conspiracy about vaccine efficacy and safety, but here we are. >vaccination gives immunity to virus or that there are no side effects from vaccination Who claims that? Nobody does or did. This is a familiar tactic: shifting the goalposts from vaccinations have very rare side effects and do not guarantee one hundred percent immunity to complete immunity and no side effects at all. Rarely does a vaccine provide complete 100% immunity from a disease, in rare cases it does, e.g. Polio. Also nobody ever claimed that vaccines have no side effects, they are just very, very rare. So rare that the expected benefits far outweigh any risk of side effects. reply sampo 38 minutes agorootparentprev> whether these deaths were concentrated in the vaccinated or the unvaccinated Sweden let Covid go through their population in 2020. New Zealand allowed Covid to spread only in 2022. Both countries vaccinated their population during 2021. Sweden had a wave of excess deaths in 2020. In New Zealand, excess mortality started to increase in 2022. Excess deaths are correlated with the times of Covid waves, not with vaccinations. https://ourworldindata.org/grapher/cumulative-excess-mortali... reply baseline-shift 2 hours agorootparentprevGood that Australia is not the only country in the world, indeed. In the US, there was a measurable change in death rates once Blue states (Democrats) got vaccinated by May of 2021. Red states, led by Trumpism, where right wing media trumpeted vaccine conspiracy theories, which lowered their vaccination rate, had higher death rates. https://www.npr.org/sections/health-shots/2021/12/05/1059828... reply defrost 1 hour agorootparentprev> If the Australian government released the data it'd be trivial to identify whether these deaths were concentrated in the vaccinated or the unvaccinated, but so far it hasn't released such data (if it even possesses such data in aggregate format). The ABS releases all manner of aggregate anon data, eg: https://www.abs.gov.au/articles/measuring-australias-excess-... The Australian states all have comprehensive medical records that have allowed people such as Prof. Fiona Stanley, an Australian epidemiologist to gain international recognition for their work teasing out relationships in many different medical disorders. Access to fine grained records is limited to researchers for privacy reasons but the pool of researchers across the country is large enoug to rule out any conspiracy to keep secrets (unless someone is deep down a truther well). https://en.wikipedia.org/wiki/Fiona_Stanley reply smallstepforman 1 hour agorootparentprevDont believe the 98% vax rate nonsense, in my circle of friends (including 4 doctors, 3 nurses) most got fake certificates. Dont be suprised that a bulk of the medical profession also did the same. I’m disappointed these professionals with very smart people just decided to bypass the mandates in order to keep their jobs instead of taking a firm stand for the benefit of society. Thats humanity for you. Lots of people navigate through the cracks. The least you can do (as an individual) is acknowledge the corrupt system and stop spreading the 98% stat which is purely propaganda. And hold the politicians accountable. reply defrost 1 hour agorootparentWhy should anyone believe your brand of nonsense instead though? > most got fake certificates. I got vaccinated for COVID in Australia multiple times and never once got a certificate .. my ID and the fact of vaccination were recorded in the medical database which is where the vaccination numbers come from. How many of the 26 million Australians in toto can you demonstrate faked their vaccination status? The least you can do (as an individual) is back up your bogus smelling anecdata. reply lazyasciiart 1 hour agorootparentprevWell, people self select their associates to a large extent. I’m sure bank robbers know other bank robbers, but that doesn’t make it a common characteristic. reply skrebbel 2 hours agorootparentprevYes. The symptoms started the day after her 3rd jab. Basically she got that “woa i got jabbed yesterday and feel like shit” thing many people got, but then it just never went away. She got covid 1.5 months after that jab so there’s no telling whether the jab or the covid was the key cause to her long-time postcovid. The covid infection itself wasn’t particularly heavy. We very much want to stay out of the “covid vaccines are evil/fantastic” debate. It’s fruitless. I will not respond to any comment suggesting that somehow we’re wrong and well actually, it couldn’t have been caused by the vaccine / the covid infection, or that postcovid is an imaginary disease. reply mnw21cam 1 hour agorootparentI had four covid vaccines with no trouble whatsoever, but after catching covid for real, I was hospitalised for a night, slowly recovered, and then a few months later got the fatigue and brain fog. Now, 2 years later, I have recovered my physical fitness, but the brain fog remains. Different people respond to the vaccines and the real thing differently. But this is an anecdatum saying the vaccines were fine. reply baseline-shift 2 hours agorootparentprevThe earliest vaccines injected at tiny amount of the disease, from smallpox on. So she probably expected to feel like shit, and that's why she felt like shit. However the covid vaccines did not have any covid in them. So there is no way she got a taste of covid shittyness. Most traditional vaccines consist of either killed or weakened forms of a virus or bacterium. These provoke an immune response that allows the body to fight off the actual pathogen later on. Instead of delivering a virus, RNA vaccines deliver genetic information that allows the body’s own cells to produce a viral protein which stimulate the immune system to mount a response, without posing any risk of infection. reply t0bia_s 1 hour agorootparentAlso cause great potential for autoimmune reaponse. Because spike proteins wont stay only in muscle where was mRNA vaccine injected. Thats why dedinition of vaccines was changed in 2020. reply armchairdweller 2 hours agorootparentprevThere are some expansive postcovid & postvax protocols on the flccc pages. Early on I had checked some of their references (e.g. on natto) and it looked good enough considering the speed of real science. Maybe good to check them out, and natto makes a good breakfast. In general they assume the condition is some kind of spikeopathy, wherever it comes from. (The holy-vax crowd has discredited anything not approved by the marketing departments of the pharmaceutical industry from the beginning, so they won’t like this either.) reply armchairdweller 2 hours agorootparentprevThank you for your honesty. Maybe one of the downvoters would like to explain why he / she finds this question so emotionally triggering. reply baobabKoodaa 3 minutes agorootparentI didn't downvote, but I'll answer anyway: there was a propaganda campaign of unprecedented scale espousing the \"safety\" of these covid vaccines (which didn't go through the normal, rigorous testing process that vaccines usually go through). The propaganda was so successful that a large percentage of the population is still fanatically making false claims about vaccine safety, despite the large number of people who reported serious adverse side effects. At this point pretty much every person on earth knows at least one person who has \"long covid symptoms\" from the covid vaccine. And yet we have a bunch of people re-enacting the propaganda of 2021 and downvoting any comment that talks about vaccine side effects... reply vertis 2 hours agoparentprevI watched my partner have a very bad run in with Epstein-Barr Virus (a decade ago). Wiped her out for months and months. It took years before she was back to 100%. It might not be clear what all the causes of long-covid, brain fog are but it's definitely possible for viruses to have long lasting and poorly studied impacts. reply michael9423 2 hours agoprevThat's not exlusive to covid, but happens with all acute infections. It has long been known that the number of infections correlate with IQ loss. \"People with five or more hospital contacts with infections had an IQ score of 9.44 lower than the average.\" https://www.sciencedaily.com/releases/2015/05/150521095016.h... reply dukeyukey 1 hour agoparentI've been hospitalised three times with severe infections, and sent to the minor injuries unit once with an infection that would've gotten severe, but caught early with oral antibiotic. Makes me wonder what I'd be like if I never got hit by anything. reply Aardwolf 2 hours agoparentprevPermanent impairment or temporary? Brain cells / connections permanently gone forever, or a temporary loss of some nutrients in the brain that can be recovered later on? reply michael9423 2 hours agorootparentProbably depends on the individual, whether the immune system is able to fully go back to baseline, or remains activated chronically. reply madballster 2 hours agoprev2015 observational study suggests these results are not unique to Covid: \"\"Our research shows a correlation between hospitalisation due to infection and impaired cognition corresponding to an IQ score of 1.76 lower than the average. People with five or more hospital contacts with infections had an IQ score of 9.44 lower than the average. The study thus shows a clear dose-response relationship between the number of infections, and the effect on cognitive ability increased with the temporal proximity of the last infection and with the severity of the infection.\" Source: https://www.sciencedaily.com/releases/2015/05/150521095016.h... reply bratbag 2 hours agoprevI had similar symptoms. In my case it was just a vitimin D deficiency caused by working from home. I don't doubt some people have suffered neurological damage, but I wouldn't be surprised if many are suffering the same as I was. reply prmoustache 1 hour agoparentAre you working in the cellar at home? Why would you have less vitamin D working from home than say, an office? reply danparsonson 1 hour agorootparentUnless you're teleporting or living there, getting from home to the office normally involves at least some exposure to sunlight. reply prmoustache 35 minutes agorootparentWorking from home doesn't mean you are forced or even have any incentive to stay at home all day long. reply fragmede 1 hour agorootparentprevEven just 15 mins in the sun helps vitamin D levels so walking from the house to the car into the office and back again is enough to make a difference compared to not going out at all, no? reply neilwilson 16 minutes agorootparentDepends upon what latitude you live at. Where I am, the sun isn't strong enough to trigger synthesis from October until the end of April. After that you need a lot of skin exposed to get anything, which is difficult 'cos it ain't that warm. There is a reason red hair appeared in Southern Scotland/Northern Ireland. reply prmoustache 44 minutes agorootparentprevI am working from home and the first thing I want to do when I stop working is going outside. I also, and as do many of my coworker, go for a walk at least once a day. WFH doesn't mean you are chained to your desk from 8am to 5pm reply orwin 1 hour agorootparentprevYou mean some people only go out to work? If true, OK, I was wrong about WFH/hybrid being superior to RTO, I guess different people mean different habits. reply dukeyukey 1 hour agorootparentI live in the UK, it was only a few weeks ago it stopped being dark when I finish work. Climate is a bitch. reply prmoustache 41 minutes agorootparentThat is offtopic no? people driving/walking/cycling to work face the same climate and hours with sunlight. One of the nice things that usually comes from WFH is a bit more flexibility in work time schedule. I usually go for a walk at least once during the day, usually to get groceries, and every other week I have to pickup my smallest daughter in primary school. And when the weather is nice and the days short in winter I would often do a 2 to 3h pause during the day to go for a cycling ride. reply negamax 1 hour agoprevThe cognitive decline can also be attributed to extreme isolation that everyone was subjected to. My super enriched, flow like heavenly water life came to a complete standstill. All future plans and timelines thrown in disarray. Covid has ended but my life trajectory has completely changed reply jug 42 minutes agoparentYes, I mean it's been shown how people with a loss of hearing have an increased risk of dementia and it's natural that this would apply here as well. It's just another form of lost stimuli. But there might also be medical reasons behind this and this serves to show how complex this is to research. reply henrikschroder 3 hours agoprevIf this is true, you should expect results to be the same across countries, and the measurable drops in IQ should correlate to infection rates. Five bucks says you won't find that. reply kromem 3 hours agoparentIn about 5 minutes I found papers on cognitive impairment for post-covid groups vs healthy controls in multiple non-North American countries, including one from Spain [1] where there was a measurably lower correlating IQ estimation for the PCC group than the HC group. 1 - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9631485/ Do you have a more specific hypothesis of what wouldn't be found elsewhere? Because it looks like you'd be out of $5 from any takers currently. reply helsinkiandrew 2 hours agoparentprevIf you accept \"cognitive impairment\" as a drop in IQ, then this meta study seems to find that globally after infection, and I claim my $5: https://pubmed.ncbi.nlm.nih.gov/34973396/ > A significant proportion of individuals experience persistent fatigue and/or cognitive impairment following resolution of acute COVID-19. > Ten studies analyzed data from Italy, nine from Spain, eight from the US, seven from China, six from the UK, three from Denmark, France, and Norway, respectively, two from Australia, Austria, Brazil, Canada, Egypt, Germany, Israel, Russia, and the Netherlands, respectively, and one from Belgium, the Czech Republic, England, Faroe Islands, Iran, Japan, Mexico, Pakistan, Singapore, Sweden, Switzerland, and Turkey, respectively... reply alliao 1 hour agorootparentgood save. The impairment is quite apparent from what I've observed and I suspect this is even harder to accept in work context; no one wants to admit they're impaired if you work in knowledge economy where IQ is seen as somewhat correlated to your performance career wise. reply danielheath 3 hours agoparentprevIt's almost impossible to compare IQ tests across countries, because cognition test results are really only valid for people from the cultural context the test was written for. reply invalidusernam3 2 hours agorootparentIt's not about comparing IQ between countries, it's about pre covid IQ scores with post covid IQ scores within the same country reply Terr_ 2 hours agorootparentprevExcept parent poster isn't talking about doing that, they're talking about computing the relative change over time separately and individually for each country within its own singular unique cultural-context et-cetera. Then those ratios are what you compare across countries, which is significantly less-problematic. reply resonious 3 hours agorootparentprevIf true, this seems quite damning for IQ as a metric. If the kilogram varied across countries, we would be in big trouble. reply seanmcdirmid 3 hours agorootparentIQ metrics are continually damned. There is still no real consensus that they are useful at all. But the social sciences is filled with a lot of soft metrics like IQ because studying complex human behavior with hard science is very hard, and probably impossible. reply chefandy 2 hours agorootparentprevThe approach's limitations are pretty well-known, though the results have always been considered estimates rather than absolute values. The \"Reliability and validity\" section of the wikipedia page for Intelligence_quotient is actually a pretty solid and well-written overview if you're looking for one. reply abdullahkhalids 1 hour agorootparentprevThe mere assumption that something as multidimensional as intelligence can be quantified along a single axis is already highly suspect. How can you compare the mental abilities of Einstein and Bach and Shakespeare and Magnus Carlesen and Zidane? reply viraptor 2 hours agorootparentprevOne of them is https://sci-hub.se/https://pubmed.ncbi.nlm.nih.gov/16123251/ but there's a few more. Anyway, it seems obvious it's culture dependent. Given a person with usual western education and a person who's never dealt with shapes and abstract straight lines in their life - would you expect them to score similarly if they're both very intelligent people? These tests heavily rely on geometric puzzles which already make an assumption about education. The tricky question is how much does that affect differences you'd see between random people tested for comparison. reply DoreenMichele 3 hours agorootparentprevIt's true, fwiw. reply pjerem 3 hours agorootparentprevI suppose you can’t compare the absolute results but you can see if there is a similar trend. reply concordDance 1 hour agorootparentprev> cognition test results are really only valid for people from the cultural context the test was written for. They're perfectly good for looking at changes over time within a population, you just have to be careful doing between-population comparisons (whether a test is \"valid\" at that point depends what specific question you want to answer). reply logicchains 3 hours agorootparentprevThat's why Raven's progressive matrices were invented. And in spite of it being developed by westerners, East Asian countries tend to score highest on it. reply rchtwlm 3 hours agoparentprevYou should not assume that the effects of a disease are the same for all populations on the planet. reply jonathanstrange 1 hour agorootparentOP didn't assume it, they hypothesized about it and laid out a way to falsify the hypothesis. reply skrebbel 3 hours agoparentprevI’m curious why you don’t think you’d find that. I very much believe you would. reply solumunus 2 hours agoparentprevFive $ isn’t very much. reply Satam 1 hour agoprevI've noticed my memory and math skills getting worse as well. Can't say it's covid for sure though as there are other likely explanations too: getting fatter and out of shape, finishing uni and going out of practice, long-term sleep deprivation, etc. Either way, if not covid, then I think at least the lockdowns did have an impact on my health and ability to stay fit. reply zjp 3 hours agoprevI felt much sharper before I had COVID last December and I still don't feel like I've fully recovered from the fog. reply atleastoptimal 55 minutes agoprevA Chinese researcher speculated this would be a major source of worldwide cognitive decline. Perhaps the Chinese knew something about the disease we don't leading to their extreme lockdowns. reply jonathanstrange 1 hour agoprevI'm dismayed by the number of people who apparently don't subscribe to evidence-based medicine using proper statistical methodology and instead rely on anecdotal evidence whenever Covid comes up. It's very irrational, though some of this behavior can probably be explained by the extreme politicization of the topic in the US. reply bayindirh 3 hours agoprevI went through a mild COVID-19 infection, and felt the sluggishness in my brain for quite some time. I think I recovered, but it was a jarring experience. reply fransje26 1 hour agoparentSame here. And the random days where I wouldn't be able to concentrate, even a few months later after feeling better, were really unpleasant. reply feverzsj 3 hours agoprevI thought tiktok caused that. reply fransje26 1 hour agoparentDouble whammy! reply senectus1 2 hours agoparentprevtiktok was doing this before covid was around. reply jonathanstrange 1 hour agorootparentThere was a massive increase in TikTok's user base between 2020 to now, though, so it's worth investigating. reply pvaldes 1 hour agoprevMy bet is that trolls in government, fake news and anti-scientific facts injected in vein 24/7/365, helped a lot to grease that piggy. Accelerated brain decay with the last and current politics in the planet is basically guaranteed. reply maaaaattttt 2 hours agoprevThe symptoms of tingling in the extremities resemble what my wife experienced after her chemo when the treatment damaged her neurons' myelin sheath. This causes 'noisy' neuron signals to be transmitted (tingles, lack of sensation, etc.). If the same thing happened in the brain, I bet the noisy signals would lead to brain fogginess, forgetfulness, etc. Back then, she was taking omega-3 supplements and received shots of vitamin B12. After a year, it removed almost all the tingling, but some sensations never fully came back. This is not medical advice (and check with your doctor beforehand), but it feels like taking vitamin B12 and omega-3 supplements should be a pretty low-risk bet to try to improve your situation if you're experiencing the long COVID symptoms described in the article. reply 8note 3 hours agoprevI think I'd attribute my newfound slowness to lockdowns, rather than COVID? Lack of practicing talking and remembering stuff is bad for talking and remembering stuff, and it definitely happened long before I caught COVID, because I caught COVID super late reply robocat 1 hour agoparent> it definitely happened long before I caught COVID Why do you assume that you weren't asymptomatic? 40.5% of people who contract the virus have no symptoms. reply throwing_away 2 hours agoprevI feel like this is a messaging roll out? https://www.medpagetoday.com/opinion/faustfiles/109672 -- an interview from 2 days ago where the NIH leader says covid is persistent for months or years in people and the MD interviewer did a double take. Not sure why there's a shift happening, maybe some new publication? This has been shaping up for years now. reply greyface- 1 hour agoparentThe relevant bit of transcript: Faust: I just want to follow up on something you said a moment ago about where this virus can be found in tissues. Are you suggesting that long COVID is actually, the mechanism of that, is persistent live virus in humans? Bertagnolli: We see evidence of persistent live virus in humans in various tissue reservoirs, including surrounding nerves, the brain, the GI [gastrointestinal] tract, to the lung. Faust: OK. And you're saying this goes beyond the PCR's [polymerase chain reaction test] ability to get it in a regular swab so that we are missing chronic cases of SARS‑CoV‑2? Bertagnolli: Correct. The virus can persist in tissues for months, perhaps even years. Faust: OK. I think that's certainly one theory, but I'm not sure that that's settled. Is that fair? I mean, there's one thing between people who are autopsy, they died of viral sepsis, as opposed to people walking around. Is there a distinction there? Bertagnolli: Our emerging data shows that the virus can persist into tissues in the long term, and I think that's really critical because it does help us think about possible ways to combat it, one being better antivirals. I think there's a lot of focus on developing new antivirals as a possible way of preventing long COVID, and the other might be more aggressive treatment with antiviral therapy upon initial diagnosis. Faust: If that's the case, then it could be reactivated just like herpes is and shingles. Are we going to start seeing people get COVID not from infection, but from themselves in reactivation? Bertagnolli: I don't believe I've seen or heard of any instance of that, and I don't think you can ever assume that one virus is going to act like another. Certainly every virus that we know of seems to have a different effect in the body long term. reply shrubble 3 hours agoprevI definitely feel 'less sharp' at times, however in my case I may have other factors going on. I did have Omicron and possibly something very weird about the time Covid started to show up in the USA reply tunn3l 2 hours agoprevI am suffering from LC myself for 2 years now. The Brainfog is intense, the fatigue tiring. I'm mentally really low. It's nice to see some news regarding COVID. My story is on my blog https://tunn3l.pro reply woopwoop24 1 hour agoprevi did pure oxygen in a pressure chamber for about 4 weeks, it got significantly better after that, at least in my perception. Could be also just the time after the infection reply eru 3 hours agoprev> There are studies that have been done comparing people who had COVID-19, versus people who didn't, and then gave them cognitive testing to measure their ability to cognitively process information and test their IQ. And there's very clear differences in the IQ of people who had been infected with COVID-19 versus people who did not. Even mild COVID can give people about a three-point loss of IQ. That sounds like an observational study. It's hard to get from that correlation to the causation in the headline. The article does not mention this problem at all as far as I can tell. reply mort96 3 hours agoparentI guess the title wouldn't have been as interesting if it was \"Stupid people more likely to get covid\" reply ETH_start 59 minutes agoprevThe studies cited do not prove any causative association. They lack controls and other design elements that would account for misattribution and the impact of COVID-related psychosocial factors on observed symptoms. In general, there is an extreme lack of rigor seen in claims about long COVID. The best evidence available suggests most cases of \"long COVID\" are misattribution: https://jamanetwork.com/journals/jamainternalmedicine/fullar... reply ggm 3 hours agoprevA reading of Oliver Sach's pop-sci books on encephalitis & PVS suggests we've actually been here before in the 19th and 20th century with cognitive and other effects down to post viral conditions. It's just another instance of something we probably know happens. reply sorwin 2 hours agoprev> There are studies that have been done comparing people who had COVID-19, versus people who didn't, and then gave them cognitive testing to measure their ability to cognitively process information and test their IQ. And there's very clear differences in the IQ of people who had been infected with COVID-19 versus people who did not. Even mild COVID can give people about a three-point loss of IQ. Hasn't pretty much the entire population of earth been infected by COVID at least once at this point? reply timeon 1 hour agoparentI had no symptoms and was not tested for COVID. I have been testing my self regularly during pandemic, however not always with PCR. reply solumunus 1 hour agoparentprevThe people reporting not to have been infected would have had extremely mild, non symptomatic infections, such that they didn’t notice them. I assume the symptoms are what cause the long term damage. reply peter_retief 3 hours agoprevI had brain fog with COVID but it does get better with time. Still sometimes struggle to find the right word. There is also Alzheimer's in my family so that is something additional to worry about reply senectus1 2 hours agoprevcould they express their findings in a way that doesn't include \"IQ\" in it? I really dont think we know what Intelligence is, let alone how to define it universally enough to make a quotient of it. reply nostrebored 3 hours agoprevYet another observational study with no real comparison. The REACT dataset used by the study relies mostly on self reported surveys and data from people sick enough to go to the doctor. This has obvious problems. The idea that there is a control arm of people who haven’t gotten Covid large enough to power a real control here is farcical. Again, a problem of self selection for people committed enough to maintain a large level of isolation for multiple years. Any causal study here should be looking at a differential in pre and post covid cognitive scores with a control of known uninfected participants in the same time window. reply Animats 2 hours agoparent> The idea that there is a control arm of people who haven’t gotten Covid large enough to power a real control here is farcical. Why? Whether someone has ever been infected is testable. About 20% of the US population had not had COVID as of some point in 2023. reply NotGMan 2 hours agoprevnext [2 more] [flagged] timeon 1 hour agoparentSeems like you had COVID. reply decremental 3 hours agoprevnext [7 more] [flagged] resolutebat 3 hours agoparentI'm genuinely curious about what mechanism would make this possible. reply zer00eyz 2 hours agorootparentTo bring in some actual research: https://www.cureus.com/articles/196275-increased-age-adjuste... A study of more than 50,000 employees at a medical institution in the United States observed the incidence of the Omicron variant epidemic based on the number of vaccine doses received (0, 1, 2, 3, and 4 or more doses) over a period of 26 weeks and showed that the number of vaccines received was positively correlated with the cumulative incidence rate of COVID-19 [46]. Susceptibility to COVID-19 infection after multiple vaccinations may be enhanced by antibody-dependent enhancement [47], immune imprinting [39,48], and immunosuppression [25-27]. This can result in a risk of exposure to viral S-protein in addition to vaccine S-protein for the multiple-vaccinated. That whole paper is... interesting. I will be curios to see if any one else finds corroborating data or if it gets toss to the side and forgotten. reply logicchains 2 hours agorootparentprevThey were presumably referring to https://pubmed.ncbi.nlm.nih.gov/36548397/ , where repeated vaccination with the mRNA vaccines was associated with antibody class switching towards IgG4 \"tolerance\" antibodies. In theory, this class switching could allow the spike proteins from the virus to persist longer in the body, causing more damage. The same phenomenon wasn't observed with non-mrna vaccines or covid infection, which has been hypothesized to be because they don't make the body's own cells produce the spike protein, so don't invoke that same immune tolerance mechanism. reply yieldcrv 3 hours agorootparentprevwhat I find the most humorous from people that base their whole identity around a problem with mRNA vaccines is that everything they identify as a problem is caused by a natural covid infection too and in vast greater quantities and severity, and they amusingly don’t seem to know that reply frereubu 3 hours agoparentprevOn what basis? reply reaperman 3 hours agorootparentNone. The studies show that COVID infections cause this in a significant fraction of those affected by them. Studies do not show the vaccine causes these symptoms. reply CapeTheory 3 hours agoprevMaybe Covid will be The End of the Whole Mess after all. reply alsaaro 3 hours agoprevI wonder if covid infections, perhaps in rare cases the vaccines themselves, is to blame for the post-pandemic phenomena of ADHD medication shortages, the crime wave, school discipline (lack of), the so-called college enrollment crises (precipitous decline), and record low OECD test scores.All of these can be explained by dopamine dysregulation. reply logicchains 3 hours agoparentMost of those can be explained by the lockdowns and school closures, which made millions of lower class people even poorer and worsened their children's educational outcomes. More poverty + missed schooling = more crime, less college enrollment and worse test scores. reply DoreenMichele 3 hours agoprevA quick search suggests zinc deficiency can cause brain fog. Zinc deficiency also causes the symptom covid is infamous for: loss of ability to smell. One of the links that came up when I searched: https://centerforfamilymedicine.com/nutritional-information/... reply croes 3 hours agoparentMilllions of people worldwide suddenly have Zinc deficiency in the same year and now it's gone? reply maxbond 3 hours agorootparentI think the suggestion is, maybe there is a shared mechanism of action, like perhaps Covid damages your ability to uptake zinc. (I have no idea if that's the case, I only claim it's a more accurate reading of GP.) reply croes 27 minutes agorootparentBut loss of smell was an early symptom of a corona infection. Sudden deficiency symptoms would mean the virus removed all the Zinc from the body before even reaching the inner organ. And normally deficiency symptoms need time to emerge. reply DoreenMichele 2 hours agorootparentprevThe body doesn't store zinc, so if you become deficient and make no effort to remedy it, you probably will remain deficient. I'm suggesting this article is possibly talking about a different aspect of an already known side effect of covid. One could do a study and check this. And if not: whatever. Just making conversation online at some ungodly hour as distraction from my sucktastic life. (Shrug) reply DoreenMichele 3 hours agorootparentprevI have no idea what you are trying to say. reply croes 1 hour agorootparentWouldn't it be a strange coincidence that people have a Zinc deficiency just at the time the are infected with Corona? Why only the infected? reply DoreenMichele 1 hour agorootparentThe infection causes zinc deficiency. reply croes 30 minutes agorootparentDeficiency symptoms are slow, loss of smell was an early infection symptom. How could Corona cause a body wide deficiency if it was still early on? And where did the Zinc go? reply DoreenMichele 0 minutes agorootparentI don't know the answers to that. I'm not a medical professional. https://news.ycombinator.com/item?id=22845133 That comment is by a researcher. We talked. I was realizing I had covid and was recovering and we had been eating a lot of zinc rich foods and this helped mitigate our symptoms. I don't really understand why people feel so compelled to gang up on me on HN about really mild observations supported with citations that \"x and y could be related.\" I have a serious medical condition which explains my interest in medical topics on hn. But after 14 years, I still cannot fathom why I get such hostility on hn for that interest. I'm probably done with trying to defend my really mild comment on this forum that other people are reacting so rabidly to. It's pointless to engage with this garbage. inferiorhuman 2 hours agorootparentprevAnd yet the zinc conspiracy is not the craziest thing that users has posted regarding COVID. reply robocat 47 minutes agorootparentActually the problem could be that zinc deficiency causes paranoia, or alternatively perhaps causes conspiracies. However one paper said: This study does not provide a clear answer as to whether the observed differences represent a causal relationship between zinc deficiency and psychiatric symptoms Which could imply that psychiatric symptoms cause zinc deficiency. And clearly a conspiracy already exists to feed zinc supplements to paranoid people: that's why we should support the zinc supplement psychoindustrial complex. This message sponsored by Sanderson and the zinc suppliments consortium. reply DoreenMichele 2 hours agorootparentprev\"Zinc conspiracy\"? It's a known fact that zinc deficiency causes loss of smell. https://www.healthline.com/health/zinc-deficiency#_noHeaderP... It's also a known fact covid causes a loss of smell. I spoke with a researcher at the time I was getting over covid, so it's clear in my mind extra dietary zinc helped mitigate symptoms. But even if I hadn't, conspiracy seems like a rather strong word for noting a few medically established facts and suggesting they could be interrelated and not wild coincidence. reply ororroro 1 hour agorootparentCauses of loss of smell include but are not limited to: * Alzheimer’s disease * Brain aneurysm * Brain surgery * Cancer * Chemical exposures to insecticides or solvents * Diabetes * Huntington’s disease * Kallmann’s syndrome * Klinefelter's syndrome * Korsakoff’s psychosis * Malnutrition * Multiple sclerosis * Multiple system atrophy (MSA) * Paget’s disease * Parkinson’s disease * Pick’s disease * Radiation therapy * Rhinoplasty * Schizophrenia * Sjorgren’s syndrome * Traumatic brain injury * Zinc deficiency (why obsess over this one?) Zinc is not used to treat symptoms, the mechanism (observed in a test tube) is that intercellular zinc inhibits RNA virus replication. Zinc supplementation above the RDI is NOT recommended for treatment of covid. Long term excess zinc can cause potentially irreversible neurologic manifestations (i.e., myelopathy, paresthesia, ataxia, spasticity). Source: https://www.covid19treatmentguidelines.nih.gov/therapies/sup... reply DoreenMichele 55 minutes agorootparentI have no idea how we get from \"x and y are potentially both symptoms of zinc deficiency\" to \"clearly, this is a dangerous comment that requires warnings against taking excess zinc...\" Not that I actually expect anyone to launch a scientific study based on my hn comment, but that is the only actual course of action I suggested. reply max_ 3 hours agoprev [–] Is there any real world test where higher IQ people do better than anyone else deterministically? I am not talking about correlations. I am talking about people with higher IQ can solve X problem that people with lower IQs can't. Not the ability to spot back & white patterns on a screen. The riches people are not the highest IQ people The Noble Prize Winners are not the highest IQ people The best sports players are not the highest IQ people The best mathematicians are not the highest IQ people The best musicians are not the highest IQ people. None of the these have excellence that can be deterministically determined by IQ. So what exactly is it good for? reply bryanrasmussen 2 hours agoparentthis is just a guess but I'm betting that if you take the best Nobel Prize Winners, and the best Mathematicians and give them IQ tests their average will be higher than the average of the general population (or the mean if you will) the best sports players and best musicians I would also suppose were higher IQ on average than normal sports players (I suppose this term means professional athletes) and normal musicians (professional musicians) In the musicians category I wouldn't be surprised if the best musicians were also smarter than the general population. In short I think IQ tests are a lousy metric, sure, but it is not a completely useless measure - just nearly useless. On edit: clarification, fixed wrong capitalization. reply zer00eyz 2 hours agorootparenthttps://www.theguardian.com/us-news/2015/nov/05/fact-check-b... The funny thing about specialists is that they can easily embarrass themselves when outside their field. Watching your average politician talk about technology is a pretty great example of people speaking with zero context. Some of them clearly aren't dumb but they can get in over their heads with ease. reply bryanrasmussen 53 minutes agorootparentThe funny thing about this conversation here though is I was talking about means and averages and you gave me an example of a single guy, also I said outliers in their fields might have higher than average general ability - I don't know why anyone would assume that higher than average general ability would equal expertise in unstudied specialties? reply torginus 2 hours agoparentprevThis is like saying that since the tallest people are not the best basketball players, height in basketball doesn't matter. reply __MatrixMan__ 3 hours agoparentprevIt's supposed to measure general intelligence (supposing there is such a thing, which I doubt). Correlation is the only thing it's trying to do. If you had a specific test like you're after... that would be by definition not general intelligence, but rather some specific intelligence. reply numpad0 3 hours agoparentprevIQ tests being useful and IQ bragging or judgement being stupid are not mutually exclusive reply jrflowers 2 hours agorootparentThis makes sense. Making any sort of judgment based on IQ scores is stupid but IQ test scores are useful because reply thomasskis 2 hours agoparentprevIQ doesn’t seem to be an ideal metric for macro level civilisation intelligence. A lot of what it optimises for is replaced by technology. If we’re measuring intelligence at that level I’d hope for something like theory of mind/spiral dynamics, testing how wide someone’s perspective is. Is their world model limited to themselves, their family, their village, their country, their race, their world, their species, their system? Combined with their understanding of social dynamics at each level. I think the average GenZ in the US is at beginning of global scale, which is great for acceptance of different cultures; but without understanding the social dynamics yet. Giving us “woke” because they understand different cultures have different contexts, this is good but incomplete. E.g. some cultures aren’t aligned with the entire species, like controlling women in a way that leads to bad outcomes as technology adds complexity to our social dynamics. Measuring this perspective level would help us understand which cultural models work best in adapting with growth and are more likely to be sustainable post-scarcity. reply dakiol 3 hours agoparentprevHere in western europe IQ tests are unheard of. I never met anyone (people at work, family members, close friends, acquaintances, etc.) who has taken such a test. reply ahazred8ta 2 hours agorootparentTry the British Army Cognitive Test https://www.google.com/search?q=british+army+cognitive+test , the Danish draft board's Børge Priens Prøve (BPP), and any other european country with an induction test for military recruits. reply jaynetics 3 hours agorootparentprevWestern Europe is a big place. I guess it also depends on whether you only count tests conducted in a professional setting. Here in Germany, i guess at least 20% of people between 20 and 40 have taken some kind of IQ test, e.g. online. Not everyone might admit it because it could come of as a little \"vain\", but there are even IQ test shows on popular TV channels (Sat 1, RTL 2) where the viewers do the test. They run them at primetime every few years. reply Toutouxc 3 hours agorootparentprev(Czech Republic) I took one in high school, it was optional and paid, but sort of organized by the school. I think we took it instead of some morning classes. I got an official-looking Mensa application form attached to the results, so I think it was pretty legit. (never applied though) reply dijit 3 hours agorootparentprevYou have to take an IQ test as part of an ADHD diagnosis. I genuinely don't know why, but you do have to. reply yial 3 hours agorootparentThis is not correct. At least not in the U.S. as a whole. It may of course be true where you are. The DSM does not even suggest an IQ test as part of the diagnostic. Knowing 6+ people in several states with ADHD diagnoses (some as children, some as adults) none have had anything beyond questions with their physician… or diagnoses by a psychiatrist after starting therapy for a different issue. https://www.cdc.gov/ncbddd/adhd/diagnosis.html https://www.ncbi.nlm.nih.gov/books/NBK519712/table/ch3.t3/ https://add.org/adhd-dsm-5-criteria/ reply sph 2 hours agorootparentprevI got diagnosed for ADHD in 2021 and nobody asked me to do an IQ test. In fact, no one has ever asked me to do one in my entire life. Your anecdote is not data. Also makes no sense whatsoever as ADHD does not affect IQ in any form. It's a form of dopamine dysregulation, not a learning disability. Let's stop perpetuating this stupid myth. reply kadkadels 1 hour agorootparentIt does affect concentration and if an IQ test goes on for long enough and is not interesting enough to warrant some kind of hyperfocus, I get distracted. And so my IQ score will be worse. Or not? I have ADHD and it doesn't make me dumber, but since focus is a critical part of thinking, it does make it harder if I'm not interested. I'd rather say that IQ tests are designed for neurotypical people. reply rvense 3 hours agorootparentprevErh, where in the world? reply dijit 2 hours agorootparentEvidently I am an outlier? I was asked to do it twice as part of ADHD diagnosis. Once in the UK as a pre/early teen (2003~), and once in Sweden to be rediagnosed via a private company named Modigo (so, now I am not sure if it's standard as part of the healthcare system here). Since I was 2/2 and that was 100% of my experiences throughout two countries, I thought it was the standard. reply ginko 2 hours agorootparentprevPretty much every adult male in Austria did an IQ test during draft inspection. reply teaearlgraycold 3 hours agoparentprevI doubt such a test is possible because IQ is trying to be a single number. Intelligence isn’t really generalized like that. reply dijit 3 hours agorootparentIQ as measured seems to be intended to be a \"mental maturity index\". It's best administered to children to understand where they are relative to other children and makes little sense in adults. That's why IQ is age dependent. A 12 year old with a 140 IQ should have the mental pattern matching recognition as a normal 14 year old with 100 IQ. The standard deviation is based on the next year ups mean IQ and so on. It doesn't necessarily measure intelligence, but in children it can be used as a proxy for intelligence since usually brain maturity is directly linked to intelligence. reply max_ 3 hours agorootparentprevMy question is. What is this Number measuring? reply Toutouxc 3 hours agorootparentWhy don't you crack open Wikipedia and start from there? reply logicchains 3 hours agoparentprev>The riches people are not the highest IQ people The Noble Prize Winners are not the highest IQ people The best sports players are not the highest IQ people The best mathematicians are not the highest IQ people The best musicians are not the highest IQ people. These may not be the highest IQ people, but very few are low IQ (below 90). reply xcv123 3 hours agoparentprevThese \"facts\" pulled out of your ass are demonstrably false. A typical IQ score for a mathematics PhD graduate is a couple of standard deviations above the mean. IQ is correlated with intelligence, and higher intelligence enables greater achievements. reply leovingi 3 hours agoparentprev [–] Maintaining a high level of civilization reply max_ 3 hours agorootparent [–] So when civilizations collapse like a Ancient Egyptian, Rome, Ancient China & Ancient Greece. What it the IQ that also dropped to cause the civilization collapse? reply mirekrusin 1 hour agorootparent [–] They probably got covid. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Research indicates that COVID-19 infections may result in decreased IQ, accelerated brain aging, and cognitive issues like brain fog and seizures.",
      "Long COVID patients are facing physical and psychological symptoms, highlighting the necessity for tailored rehabilitation programs.",
      "A specialized gym rehabilitation process is being utilized to aid COVID-19 patients in recovering from both physical and cognitive impairments post-infection."
    ],
    "commentSummary": [
      "Studies suggest a potential link between Covid infections and cognitive decline, although causation is uncertain, raising concerns about long-term impacts on IQ.",
      "Debates encompass the use of IQ tests to measure post-Covid cognitive deficits, cognitive effects of mask-induced CO2 exposure, and the overlap between ME/CFS and long COVID.",
      "Discussions also cover vaccine effectiveness, conspiracy theories, repeated vaccinations' impact, zinc deficiency, cultural variations in IQ testing, and IQ tests' correlation with achievements in different fields."
    ],
    "points": 227,
    "commentCount": 211,
    "retryCount": 0,
    "time": 1713334475
  },
  {
    "id": 40051975,
    "title": "Unlocking Vision Transformers: A Visual Walkthrough",
    "originLink": "https://blog.mdturp.ch/posts/2024-04-05-visual_guide_to_vision_transformer.html",
    "originBody": "A Visual Guide to Vision Transformers This is a visual guide to Vision Transformers (ViTs), a class of deep learning models that have achieved state-of-the-art performance on image classification tasks. Vision Transformers apply the transformer architecture, originally designed for natural language processing (NLP), to image data. This guide will walk you through the key components of Vision Transformers in a scroll story format, using visualizations and simple explanations to help you understand how these models work and how the flow of the data through the model looks like. Please enjoy and start scrolling! 0) Lets start with the data Like normal convolutional neural networks, vision transformers are trained in a supervised manner. This means that the model is trained on a dataset of images and their corresponding labels. Mountain Bird Train Castle Lynx 1) Focus on one data point To get a better understanding of what happens inside a vision transformer lets focus on a single data point (batch size of 1). And lets ask the question: How is this data point prepared in order to be consumed by a transformer? 2) Forget the label for the moment The label will become more relevant later. For now the only thing that we are left with is a single image. 3) Create patches of the image To prepare the image for the use inside the transformer we divide the image into equally sized patches of size p x p. 4) Flatting of the images patches The patches are now flattened into vectors of dimension p'= p²*c where p is the size of the patch and c is the number of channels. 5) Creating patch embeddings These image patch vectors are now encoded using a linear transformation. The resulting Patch Embedding Vector has a fixed size d. 6) Embedding all patches Now that we have embedded our image patches into vectors of fixed size, we are left with an array of size n x d where n is the the number of image patches and d is the size of the patch embedding 7) Appending a classification token In order for us to effectively train our model we extend the array of patch embeddings by an additional vector called classification token (cls token). This vector is a learnable parameter of the network and is randomly initialized. Note: We only have one cls token and we append the same vector for all data points. 8) Add positional embedding Vectors Currently our patch embeddings have no positional information associated with them. We remedy that by adding a learnable randomly initialized positional embedding vector to all our patch embeddings. We also add a such a positional embedding vector to our classification token. 9) Transformer Input After the positional embedding vectors have been added we are left with an array of size (n+1) x d . This will be our input for the transformer which will be explained in greater detail in the next steps 10.1) Transformer: QKV Creation Our transformer input patch embedding vectors are linearly embedded into multiple large vectors. These new vectors are than separated into three equal sized parts. The Q - Query Vector, the K - Key Vector and the V - Value Vector . We will have (n+1) of a all of those vectors. 10.2) Transformer: Attention Score Calculation To calculate our attention scores A we will now multiply all of our query vectors Q with all of our key vectors K. 10.3)Transformer: Attention Score Matrix Now that we have the attention score matrix A we apply a `softmax` function to every row such that every row sums up to 1. 10.4)Transformer: Aggregated Contextual Information Calculation To calculate the aggregated contextual information for the first patch embedding vector. We focus on the first row of the attention matrix. And use the entires as weights for our Value Vectors V. The result is our aggregated contextual information vector for the first image patch embedding. 10.5)Transformer: Aggregated Contextual Information for every patch Now we repeat this process for every row of our attention score matrix and the result will be N+1 aggregated contextual information vectors. One for every patch + one for the classification token. This steps concludes our first Attention Head. 10.6)Transformer: Multi-Head Attention Now because we are dealing multi head attention we repeat the entire process from step 10.1 - 10-5 again with a different QKV mapping. For our explanatory setup we assume 2 Heads but typically a VIT has many more. In the end this results in multiple Aggregated contextual information vectors. 10.7)Transformer: Last Attention Layer Step These heads are stacked together and are mapped to vectors of size d which was the same size as our patch embeddings had. 10.8)Transformer: Attention Layer Result The previous step concluded the attention layer and we are left with the same amount of embeddings of exactly the same size as we used as input. 10.9)Transformer: Residual connections Transformers make heavy use of residual connections which simply means adding the input of the previous layer to the output the current layer. This is also something that we will do now. 10.10)Transformer: Residual connection Result The addition results in vectors of the same size. 10.11)Transformer: Feed Forward Network Now these outputs are feed through a feed forward neural network with non linear activation functions 10.12)Transformer: Final Result After the transformer step there is another residual connections which we will skip here for brevity. And so the last step concluded the transformer layer. In the end the transformer produced outputs of the same size as input. 11) Repeat Transformers Repeat the entire transformer calculation Steps 10.1 - Steps 10.12 for the Transformer several times e.g. 6 times. 12) Identify Classification token output Last step is to identify the classification token output. This vector will be used in the final step of our Vision Transformer journey. 13) Final Step: Predicting classification probabilities In the final and last step we use this classification output token and another fully connected neural network to predict the classification probabilities of our input image. 14) Training of the Vision Transformer We train the Vision Transformer using a standard cross-entropy loss function, which compares the predicted class probabilities with the true class labels. The model is trained using backpropagation and gradient descent, updating the model parameters to minimize the loss function. Conclusion In this visual guide, we have walked through the key components of Vision Transformers, from the data preparation to the training of the model. We hope this guide has helped you understand how Vision Transformers work and how they can be used to classify images. I prepared this little Colab Notebook to help you understand the Vision Transformer even better. Please have look for the 'Blogpost' comment. The code was taken from @lucidrains great VIT Pytorch implementation be sure to checkout his work. If you have any questions or feedback, please feel free to reach out to me. Thank you for reading! Acknowledgements VIT Pytorch implementation All images have been taken from Wikipedia and are licensed under the Creative Commons Attribution-Share Alike 4.0 International license.",
    "commentLink": "https://news.ycombinator.com/item?id=40051975",
    "commentBody": "A Visual Guide to Vision Transformers (mdturp.ch)222 points by md2rp 20 hours agohidepastfavorite24 comments challenger-derp 19 hours agoVery nice. I wish I could do this sort of scroll story in my digital notes. Is this done with a javascript library? reply md2rp 18 hours agoparentYes this was done with a combination of GSAP Scrolltrigger https://gsap.com/docs/v3/Plugins/ScrollTrigger/ and https://d3js.org/ reply TuringTest 17 hours agorootparentThat kind of scroll is OK-ish for a background parallax effect, or maybe some pretty fade-in/out effects while elements scroll into view (without changing their relative position in the page). When it interferes with the main functionality of the page, namely reading the content, they break accessibility, distract over understanding the difficult topic, make the content brittle against changes in the platform (different browsers or future standard updates), and as others pointed out make it difficult or impossible to use alternative presentations. With most comments commenting on the presentation and not on the content, I think it makes clear that it detracts from the experience more than helps. reply md2rp 20 hours agoprevA Visual Guide to Vision Transformers This is a visual guide to Vision Transformers (ViTs), a class of deep learning models that have achieved state-of-the-art performance on image classification tasks. Vision Transformers apply the transformer architecture, originally designed for natural language processing (NLP), to image data. This guide will walk you through the key components of Vision Transformers in a scroll story format, using visualizations and simple explanations to help you understand how these models work and how the flow of the data through the model looks like. reply bArray 19 hours agoparentNice! A small piece of feedback: I would have the dimensions mentioned in the text also annotated on the diagram. It wasn't exactly clear how the input data was flattened for example. reply byteknight 19 hours agorootparentWould also add, as a 100% math idiot, linear transformations, and how it performs them is not explained. Entirely plausible this is intended for someone more \"mathmatical\" than myself but appreciate the work regardless. reply md2rp 18 hours agorootparentThanks for the feedback! I left it out intentionally but probably worth thinking about doing a more fundamental guide! reply md2rp 18 hours agorootparentprevThanks for the feedback! Will add it in the revision! reply causal 17 hours agoprevI like this, but think there is some crucial motivation missing in steps 10.1-10.3 regarding what query/key weights are and why they're needed. reply abhgh 8 hours agoparentThey are like \"continuous\" databases. See slides 4-5 here [1] - this is from a talk I had given a while ago. [1] https://drive.google.com/file/d/12uHo9QIfS-jBpVTs3lmQ3BEpxhD... reply vikiomega9 13 hours agoparentprevthis post made sense to me https://teltam.github.io/posts/soft-dictionary-keys.html It helps to think of kqv as a form of look up. reply ThouYS 16 hours agoparentprevyes, same issue in all transformer tutorials reply lordswork 14 hours agorootparentI suspect this is because most people (including people writing these tutorials) don't have a strong grasp on this piece as well. reply causal 15 hours agorootparentprevThe 2b1b video was the first to make it click for me reply hotdogscout 14 hours agorootparentYou mean 3b1b (three blue one brown)? reply causal 14 hours agorootparentAh that's right, miscounted the blues reply lyapunova 16 hours agoprevTo be honest, I actually really like the visual delivery here. It's especially helpful for understanding what's going on with computer vision problems. Please make more! reply nothrowaways 9 hours agoprevNeat reply SpaceManNabs 18 hours agoprevLucas Beyer has a lot of references and material as well that I recommend. reply tantalor 18 hours agoprev [–] Stop scrollytelling! It's awful, nobody should do this. reply 4chandaily 18 hours agoparentAgreed. My scroll wheel should scroll the page, not advance slides or split birds or whatever else. If you need to do this kind of information display, use buttons or a UI widget to control it. Don't hijack the HID devices I use for accessibly operating my computer. This goes for Scroll Wheels, Scrollbars, the Back Button, the Right Click Button, or any other standard input paradigm. (please) Don't fuck with these! Some of us make use of accessibility features, and messing with our interfaces makes these break or behave in unexpected ways. reply elicash 17 hours agoparentprevI'd be annoyed if my bank did this, or airlines, or anything where I just need to get a task done. For personal websites, I actually think individuality and fun and creativity are good. reply observationist 17 hours agoparentprevIt's aggressively inaccessible. I don't know if it's a \"I'm a web designer, I know better\" thing or what. Web designers: Don't let form interfere with function. The function of this page is to communicate information about transformers. The form effectively prevents that from happening. Don't do it. No, bad, stop. reply layer8 18 hours agoparentprev [–] This. You can’t use reader mode, you can’t save the page as a PDF, you can’t use PageUp/PageDown because you’ll miss some in-between state, and the scroll position where a certain image is shown may not be the preferred one for reading the corresponding text. And the JS will invariably break sooner or later. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Vision Transformers (ViTs) are deep learning models utilizing transformer architecture for image classification tasks by dividing data into patches, flattening, embedding, and processing them through multiple transformer stages.",
      "The model is trained with cross-entropy loss and backpropagation, using a classification token for image label prediction, iteratively updating parameters to minimize loss.",
      "This guide offers a visual explanation of the operations involved in Vision Transformers, enhancing understanding of their functionality."
    ],
    "commentSummary": [
      "A visual guide introduces Vision Transformers (ViTs), deep learning models for image classification tasks.",
      "The guide simplifies ViTs' key components through visuals in a scroll story format.",
      "It also addresses feedback on presentation style and accessibility concerns related to scroll-trigger effects."
    ],
    "points": 222,
    "commentCount": 24,
    "retryCount": 0,
    "time": 1713276058
  },
  {
    "id": 40054326,
    "title": "Forking Paths: Redis 2024 Licensing Changes and Future Uncertainty",
    "originLink": "https://vickiboykis.com/2024/04/16/redis-is-forked/",
    "originBody": "Redis is forked Apr 16 2024 I, like many developers who have worked on high-scale, low-latency web services over the last fifteen years, have an intimate relationship with Redis. At any new job, when you ask where the data is, and someone points you to a server address with port 6379, you know you will meet an good, reliable friend there. When you shell into the redis box or container or pod, you know what you’re going to find. When you run redis-cli, and it, immediately, presents you with the >, you know you’re ready to work. And, when it returns PONG to your ping, you are filled with a sense of ritualistic completion and assurance that it is as ready to read your code as you are to write it. I have been running ping now across countless Redis installs, across web services written in PHP, in Go, Java, in Python, (unfortunately) Scala, across industries, across space and time, across multiple products that use and have used Redis as cache by default. I, as a cynical Eastern European, hate almost everything in software. But I love Redis with a loyalty that I reserve for close friends and family and the first true day of spring, because Redis is software made for me, the developer. Everything about Redis is meant to empower, to clarify, to show, where abstractions need to be shown, and neatly tuck them away where they don’t. It is made to work, to handle my throughput and stay out of the way. Even the documentation is written for developers who want to learn more, not to obscure or to sell. I used it at work, and I used it as my vector search engine in Viberary, where it worked brilliantly and with extremely low latency out of the box. In a world full of junk, bloated, and obfuscated software, Redis just works. Or, as the best software developers I know would say, it is elegant. Unfortunately, my Redis doesn’t make money. While I was working on Viberary, I remarked to friends that Redis was a better vector store for probably 80% of use-cases than actual vector databases, and that it was a shame that the Redis Search module wasn’t well-known. Of course, in the heat of the generative AI landscape, the monkey’s paw curled, the over-leveraged business decided they needed to actually make money, and they changed the license, ostensibly so that AWS and other large-scale cloud vendors couldn’t sell it as a product offering. In a pattern that has previously happened for Elasticsearch and Terraform, and one that was extremely predictable, Redis is now forked at least three different ways - the original Redis, now no longer under BSD, Valkey, under BSD and led by previous contributors to Redis from AWS and other companies, and Redict, under GPL, led by an open-source coalition. Both of these licenses allow commercial reuse. The old Redis was for developers. The new Redis is for enterprise sales, and for generative AI. It’s true that it’s not yet entirely clear what all of this means for the future of Redis the software. Some say the licensing changes won’t impact much if you’re not a large-scale Redis reseller. Because it’s true that the license changes were all legal and all parties acted in accordance with both what’s acceptable and what the market dictates to sustain the software. And yet at the same time, projects that depend on Redis are withholding updates or migrating. But the problem is not only that the license changed suddenly, without warning, it’s the messaging behind the change, and the message is, even though there is extremely active community development, Redis is no longer in and of the community. We are no longer being consulted. As for the forks, there is no clear direction yet either. Valkey is on GitHub and already extremely active, yet there is already a debate about what OSS means in the context of the fork. Redict is entirely open but has opened contributions on Codeberg, where they are far from GitHub’s AI training data collection, but also miss out on the network effect for contribution in the GitHub social network, the friction of which can’t be underestimated. All of this is very sad, because it’s not clear which path forward makes sense in this new landscape for me as a builder if I want to now use Redis in a new project, particularly since neither of the new projects support modules like Redis Search yet. (Or if they do, it’s not clear to me whether it’s directly drop-in compatible and what the licensing issues are there.) It seems like the new, corporate Redis is no longer for me or my personal projects, which is ironic given that my interests lie squarely in the vector search and generative AI spaces. It’s also not clear if I wanted to contribute, which one I should give my time and energy into. So, it seems like I’ll be staying on the Redis available before this commit for the time being, until the dust settles. Good things don’t scale, except in the case of Redis, where all it does scale in O(1) time, but doesn’t align with the realities of markets in software. Fortunately, in redis/valkey/redict codebases, one thing that is still a constant is that whenever there is a ping, you can always expect a quiet, resounding pong, and in that regard, with the current energy around open-source, particularly in the ML space, I’m optimistic.",
    "commentLink": "https://news.ycombinator.com/item?id=40054326",
    "commentBody": "Redis is forked (vickiboykis.com)221 points by colton_padden 17 hours agohidepastfavorite174 comments hnthrowaway6543 16 hours agoGenerally my stance with these forks born from community drama is \"wait and see.\" Sometimes the fork will gain traction and become the de facto \"true\" version (see Hudson -> Jenkins). Sometimes the fork will flop and people will largely stick to the original despite whatever caused the schism (see Terraform -> OpenTofu). Many of these recent forks are being done because people won't want AWS/GCP/Azure to slap a UI on top of their free open-source product and resell it, making tens of millions of dollars per day in the process. I can't really blame them. reply dainiusse 16 hours agoparentI think it is too early to evaluate Terraform/OpenToFu. They're diverging now and it looks like OpenToFu are bringing on some wanted features. reply noodlesUK 16 hours agorootparentI agree. It has only been a few months since the split. I have noticed more and more uptake of OpenTofu amongst colleagues, and I've personally switched. The thing that makes the difference is what is running on people's laptops, because that's what people will eventually put into prod. reply jareklupinski 15 hours agorootparent> The thing that makes the difference is what is running on people's laptops, because that's what people will eventually put into prod. \"It works on my machine!\" \"Then we'll ship your machine\" Docker: https://miro.medium.com/v2/resize:fit:720/format:webp/1*Ibnw... reply kdmccormick 15 hours agorootparentLol plain old VMs have been shipping your machine since well before Docker was around. reply TheNewsIsHere 14 hours agorootparentAnd in some cases, unfortunate git commands will ship your machine too! reply waiwai933 15 hours agorootparentprevThat works while OpenTofu and Terraform files are compatible - but once they no longer are, presumably you'd have to standardise on one or the other. reply xmprt 15 hours agorootparentThe point is that once they are no longer compatible, people would standardize on the one that they're familiar with which is most likely the one that's running on their machine. reply NegativeLatency 15 hours agorootparentThere are enough pretty annoying and long standing terraform issues that if opentofu started picking them off I'd consider switching. You can kinda see this with vim and neovim where both are continuing to exist and benefit each other. reply wlonkly 9 hours agorootparentEncrypted state files are either done or coming soon. That's going to be a big one, since Hashicorp used that as a selling point for Terraform Cloud. reply waiwai933 12 hours agorootparentprevBut currently, people are equally comfortable with both; the CLI commands are exactly identical between the two, save for the name of the binary itself. In any org where both are in use, if people are forced to choose at some point, they will have to balance many other factors besides familiarity, such as features and confidence in the platform. reply hiatus 11 hours agorootparentprev> The thing that makes the difference is what is running on people's laptops, because that's what people will eventually put into prod. I disagree—I think support of deployment tooling (like Atlantis) is the bigger proof. If you are running terraform on your local machine it is likely a very small company. reply Thev00d00 16 hours agorootparentprevThere is no incentive for users of tf to move, consumers are not impacted by the licensing changes. Opentofu hasn't shipped a 1.7 stable with removed blocks yet, whilst terraform is already on 1.8 with provider functions reply cube2222 16 hours agorootparentHey, tech lead of the project here! Just to clarify, provider-defined functions are coming in OpenTofu 1.7, along with e2e state encryption. Generally, I recommend not comparing version numbers of Terraform and OpenTofu post-1.6. Implementing the e2e state encryption was non-trivial, and we wanted to make sure we get it right, so that's why the release took us a while. We also got a slight additional delay due to needing to handle the C&D letter OpenTofu got from HashiCorp[0], but that's all sorted now. The beta for 1.7 however is coming out this week, with the stable release planned in the next ~3 weeks. [0]: https://opentofu.org/blog/our-response-to-hashicorps-cease-a... reply vertis 13 hours agorootparentI'm definitely in the camp that has moved my tiny company infra to opentofu. Thanks for all your hard work. reply cube2222 12 hours agorootparentThat’s awesome! Appreciate the kind words :) reply vertis 11 hours agorootparentIn the very early days of Terraform, when it was 2 months or so old I helped a little. How many people did (so much more than me) with all these projects to be later betrayed by relicensing. > git log --pretty=format:\"%h %an %ad %s\" --date=shortgrep \"Luke Chadwick\" dcd6449245 Luke Chadwick 2014-07-30 Add documentation for elb health_check 0eed0908df Luke Chadwick 2014-07-30 Add health_check to aws_elb resource 96c05c881a Luke Chadwick 2014-07-30 Update documentation to include the new user_data attribute on aws_launch_configuration 15bdf8b5f9 Luke Chadwick 2014-07-30 Add user_data to aws_launch_configuration 8d2e232602 Luke Chadwick 2014-07-29 Update documentation to reflect the addition of associate_public_ip_address to the aws_instance resource 974074fee9 Luke Chadwick 2014-07-29 Add associate_public_ip_address as an attribute of the aws_instance resource reply eadmund 7 hours agorootparent> How many people did (so much more than me) with all these projects to be later betrayed by relicensing. Were you betrayed? They did a thing you licensed them to do. That’s the whole point of non-copyleft free software licenses, after all! It’s kind of odd to specifically choose a license which allows others to use one’s code in proprietary software, then be upset when others use one’s code in proprietary software. If one wishes one’s software and its users to remain free, the answer is to use a copyleft license. reply Fnoord 11 hours agorootparentprevDon't use your project (nor Terraform) but great project name! reply 46Bit 16 hours agorootparentprevAnecdotally, I know several teams likely to adopt OpenTofu when state encryption ships https://terrateam.io/blog/opentofu-feature-preview-state-enc... reply silisili 15 hours agorootparentprevIIRC, Gitlab runners gives you a big warning with tf telling you to use opentf, so that provides some incentive. reply darby_eight 15 hours agorootparentprevThere's also not any incentive to use the original terraform. reply snotrockets 16 hours agoparentprev> Many of these recent forks are being done because people won't want AWS/GCP/Azure to slap a UI on top of their free open-source product and resell it, making tens of millions of dollars per day in the process. I can't really blame them. I won't blame them for regretting their past actions, but I hope the lesson would be learned: if you want to put limitation on the use of your software, you shouldn't have licensed it in a way that doesn't allow such. You can't recall a gift because you don't like how the recipient is using it. Though you are more then welcome not to gift them ever again. reply bloppe 15 hours agorootparent> Though you are more then welcome not to gift them ever again That's actually an accurate description of what's happening with these re-licensing dramas. Redis versions from before the split are still BSD-licensed. They can't recall those gifts. The re-licensing only applies to newer versions. Of course, it wasn't just people employed by Redis, Inc. who were providing the gifts. My (vague) understanding is that a lot of contributions came from people at AWS, etc. Technically, AWS was providing those \"gift\" contributions to Redis -- because Redis, Inc. maintains the copyright for community contributions -- and Redis was then re-gifting those contributions to the world, via the BSD license. That's all fine and dandy until the big contributors realized they're actually fiercely competing with each other for cloud customers, and it's not realistic for Redis, Inc. to compete with the largest cloud provider on Earth without any technical moat whatsoever. Hence the re-licensing. I think the breakdown in trust for Redis, Inc. is overblown. By far the biggest contributor to valkey (madolson) is employed by AWS. Does the OSS community really think that's a better organization to back in the long term? reply sanderjd 15 hours agorootparentprevFirst of all, you can \"recall a gift\", if it is legal. I don't think I've seen people arguing that these re-licensing actions are not legal. But if you mean that you can't recall a gift without making people mad, then yep, that's true! But keeping people from being mad is not the only thing that matters. But the real problem I have with, \"I hope the lesson will be learned\" is that the lesson people are learning is \"don't try to build ambitious software requiring a lot of work from a dedicated core team using an open source license; you're going to find yourself damned if you do and damned if you don't\". And I think that really sucks! And sadly the ship has already sailed here. I'm certain we're going to see way fewer products with open source licenses because of all of this. And I think it's unlikely we'll even see as many products with \"source available\" licenses because, how is it worth the hassle to release source when the community has shown more good will to projects that are entirely proprietary? I really think I'm going to look back at the last 15 years or so in awe at how often I had the luxury of digging into the behavior of software I rely on, by reading the code. reply snotrockets 15 hours agorootparentWe're living in a society, with certain systems baked into it. Fighting to change those systems and norms is a Good Thing™, but I'm too pessimistic to act today as if the change is coming tomorrow. I'll both work to change those systems, AND act as if they're here to stay. reply sanderjd 15 hours agorootparentI'm not sure if this was a purposeful allusion or not, but I entirely agree that you \"can't recall a gift\" in exactly the same sense that you can't keep talking on a payphone for a long time when someone is clearly waiting; because, you know, we're living in a society!! reply snotrockets 15 hours agorootparentUnlike the payphone analogy, where there is metering rules, and as long as you abide by them, you can keep talking, even if it's impolite, there is a legal definition for gifts (it is, after all, a transfer of ownership, and such things are regulated), which, as far as I know, matches the social norm of not being able to claw those back. reply sanderjd 13 hours agorootparentHa, except, to go full circle, then if you're using that legal definition of a \"gift\" requiring a transfer of ownership, it's clear that open source software is not actually a gift, because there is no transfer of ownership. You can't have it both ways! If it's a \"gift\" in the colloquial sense of something freely given, then it breaks social convention to claw it back, but it is not illegal. If you want to use the legal definition of a \"gift\" - ie. for tax purposes, implying a transfer of ownership - then contributions to open source software are not at all a \"gift\" in that sense. reply bornfreddy 15 hours agorootparentprev> And I think it's unlikely we'll even see as many products with \"source available\" licenses because, how is it worth the hassle to release source when the community has shown more good will to projects that are entirely proprietary? This, thousand times. Looks like FOSS advocates feel so threatened by \"source available\" licenses that they will do everything possible to keep them from gaining momentum (see Commons Clause). It's a shame really. It would be nice to have a standard and well known license that would both allow users to use software freely (for using and adapting) and still protect makers from their competitors (AWS comes to mind) undercutting them with their own product. Oh well. EDIT: ...and here comes the first (anonymous) downvote. Proves my point about FOSS sentiment, I guess? Come on, it's a discussion, you can do better than that. reply dvfjsdhgfv 13 hours agorootparentprev> But the real problem I have with, \"I hope the lesson will be learned\" is that the lesson people are learning is \"don't try to build ambitious software requiring a lot of work from a dedicated core team using an open source license; you're going to find yourself damned if you do and damned if you don't\". Yeah, for things running server-side that could be used by Amazon and Microsoft, they should use SSPL from the start. In this case everything is clear and everybody knows what to expect. For regular users, there is absolutely zero difference between SSPL and OSI-certified licenses. reply sanderjd 12 hours agorootparentYes, they definitely should use something like that from the start, in my view. But I think they won't, because there is now a lot of evidence that this will result in a bunch of bad press, whereas just making it proprietary will go without comment. What incentive do companies have to do this? reply slashdev 16 hours agorootparentprevWith the noise people make when you don't use an OSI open-source license, little wonder. But when Redis started I think we didn't yet boardly understand the limitations of OSI licenses in the era of big tech cloud computing. To people starting projects today, you have no excuse, we know better. Don't use OSI open-source unless it's entirely a labor of love that you're giving away free. OSI Open-source business models are dead. Don't make that mistake. reply snotrockets 15 hours agorootparent> But when Redis started I think we didn't yet boardly understand the limitations of OSI licenses in the era of big tech cloud computing. According to antirez, he understood the implications of licensing Redis as BSD: https://news.ycombinator.com/item?id=39863371 reply bornfreddy 15 hours agorootparentBut not in \"the era of big tech cloud computing\": > That is, if I was still in charge, would I change license? But that's an impossible game to play, I'm away from the company for four years and I'm not facing the current issues with AWS impossible-to-compete-with scenario. Not saying he would have decided any differently, just that cloud computing has changed open source situation for the worse. reply jrochkind1 15 hours agorootparentprevIt looks to me like people start with open source licenses because that's helpful to get them market-share (and in some cases community contributors and maintainers), and then they switch to a non-open-source license reserving certain rights to profit to them hoping to maintain the users that they wouldn't have gotten if they started with that license. I am curious for examples of any projects now *starting( with one of these non-open-source rights-to-profit-reserved licenses, now that is clearly \"understood\". Are there any examples? Are they successfully attracting users? Contributors? reply growse 12 hours agorootparentCockroachdb started with BSL I think. reply jrochkind1 12 hours agorootparentWikipedia suggests it was initially apache, changed to BSL in 2019. https://en.wikipedia.org/wiki/CockroachDB. But still perhaps an example of fairly quick switch to BSL before it had gotten wide market/mind share? I don't know the history. reply orangechairs 11 hours agorootparentThat's correct - CockroachDB's license changed in 2019, from Apache 2.0 to a permissive version of the BSL: https://www.cockroachlabs.com/blog/oss-relicensing-cockroach... \"We’re adopting an extremely permissive version of the Business Source License (BSL). CockroachDB users can scale CockroachDB to any number of nodes. They can use CockroachDB or embed it in their applications (whether they ship those applications to customers or run them as a service). They can even run it as a service internally. The one and only thing that you cannot do is offer a commercial version of CockroachDB as a service without buying a license.\" reply Pet_Ant 16 hours agorootparentprev> Don't use OSI open-source unless it's entirely a labor of love that you're giving away free. Well, know what your secret sauce is. I think performance is really the best differentiator. Make a fully behaviourally compatible (maybe not bug for bug) version available and then sell a proprietary faster version. Think an compiler that doesn't due any optimisation and outputs naive code. You know have a useful OSI project, and a clear value add, and a clear boundary between the two. This is really applicable for databases, and it still leaves you with something useful for learning small projects, and for developers to run locally on their own machines. reply wing-_-nuts 14 hours agorootparent>I think performance is really the best differentiator. I don't understand this at all. Most open source projects start out equally as a means to give back to and collaborate with the community as well as showcase one's skills. You're asking me to purposefully publish badly optimized software? I don't have it in me. That offends my sensibilities of craftsmanship. A 'slow' open source project will never get traction. Even if it did, I also don't know how an open source project wouldn't immediately have it's obvious performance bottlenecks fixed. Far better to go the VSCode route. Release a useful project, then release paid extensions for it. reply snotrockets 15 hours agorootparentprevIt ain't performance, it's cost (which increased performance can improve, but it's far from the only factor). reply lenerdenator 16 hours agorootparentprevThey're not dead, just mostly dead. You can probably set up a decent privately-funded venture to deal in OSI software. The problem comes (as it always does) when the founders think they're the reincarnation of Steve Jobs and deserve a nine-or-ten-figure net worth for making a few nice, but ultimately not earth-shattering, software tools. Then they have to enshittify to get ready for the IPO. reply snotrockets 15 hours agorootparentnit: s/founders/executives/ (sometimes they are the same, but not always) reply Comma2976 16 hours agoparentprevThe hyperscalers weren't selling Terraform though, Hashicorp was losing to Terragrunt, Spacelift et al, who had decent to good offerings. I'd say Elasticsearch is still the comparison to make here for a product that clouds just resold, then again, Redis the company didn't build Redis the software and their latest marketing smells more of VC hawkery than any reasonable pitch reply alex_lav 15 hours agorootparent> Hashicorp was losing to Terragrunt, Spacelift et al, who had decent to good offerings. Would love some data in support of this statement. Not saying you're wrong necessarily, just it feels like a perception vs. reality comment potentially. reply Comma2976 13 hours agorootparentDon't have any data I must say, just a series of discussions here and elsewhere over the years, voicing either appreciation for the platforms above or displeasure with the logistics/price of TFcloud, e.g. with one I remember https://old.reddit.com/r/devops/comments/eaq8bh/terraform_em... The vast majority of projects that I've come across at work just had the tf CLI wrapped in a CI/CD pipeline + some bucket, but the managed competitors seemed to come up more often (compared to other tools/platforms) when people wanted to use/were using something as a service. Perhaps it's really just a perception thing, I never checked their financials edit: I think I went off-track from the point. The point was that the ones selling managed versions were not the hyperscalers, but people building wrappers and similar around it. The other commenter with the list of fork-backers illustrates that nicely. reply pbronez 13 hours agorootparentprevEasiest to check out the list of companies sponsoring OpenTofu https://opentofu.org/supporters/ For more details, you could listen to: https://oxide.computer/podcasts/oxide-and-friends/1471446 reply alex_lav 11 hours agorootparentA list of companies sponsoring OpenTofu confirms that TF was \"losing\" to tools like TerraGrunt and SpaceLift? Sorry I don't follow. reply vdfs 16 hours agoparentprev> people won't want AWS/GCP/Azure to slap a UI on top of their free open-source product and resell it If it wasn't open-source it won't be as popular as it is in the first place, Redis is also using ton of open source software or libraries for free. Not defending AWS/GCP/Azure, I actually got my software used when i was young by a large company for free (not even a mention- Still using it i think, 5M+ Play Store Downloads), but that is the spirit of open source reply snotrockets 16 hours agorootparentRedis actually has very few external dependencies compared to most modern FOSS: IIRC, it only need libc and OpenSSL (the latter only if you build TLS) on your system, and provide their forked copies of Jemalloc, LUA, fpconv, HiRedis, Linenoise and Hdr_Histogram. reply nextaccountic 13 hours agorootparentthose vendored dependencies are still libraries that redis couldn't use if those libraries weren't open source for example, something like jemalloc is highly nontrivial reply hcrean 16 hours agorootparentprevOpen Source is, by and large, intended by the creator to donate their ideas to benefit humanity as a whole; many people feel that using their thing to help strengthen an (ethically questionable) monopoly is acting against that core goal. reply OkayPhysicist 15 hours agorootparentThis is why the most viral AGPL license you can find is the only right license for Open Source. There is no downside: for honest, upstanding developers who want to use your code, there's no problem, because their code was going to be Open Source, too. For the soulless corporations, they'll either not use your code, or contribute back their modifications, a win-win for everyone. The only losers are people who are engaging with the Open Source community in bad faith, viewing it as something to steal from, rather than participate in. reply ethbr1 15 hours agorootparentprevI think it's less about strengthening a monopoly and more about zero sum business models. If AWS/Azure/GCP/et al. ran a cloud version of X and the main company supporting the open source project was a going concern, I doubt many would have a problem with the entire scheme. However, in reality, every enterprise support dollar that goes through a third party cloud-managed offering is one that doesn't go to the first party. In which case, what dollars are left to pay the independent company that creates and supports the software? Granted, there are a lot of nuances to the above, but I think it's generally fair to say that third-party cloud companies are making more off managed open source offerings than they're paying to contribute to them. reply toast0 13 hours agorootparentprevWhen I fix a bug in OpenSSL, it benefits humanity regardless of who deploys it. Maybe slightly less beneficial if it fixes an evil service, but still... Better to have a successful TLS handshake and get on with the evil. I don't think anything else open source I've done has been widely deployed, but if I save a bit of someone's time because they can use something I did, or save some users' cpu and bandwidth, it doesn't matter to me if that's a user of a free service or a propriatary one, I still helped their user. reply styfle 16 hours agoparentprevDon’t forget the Node.js and io.js fork. That’s one where the fork basically became the blessed branch and the two communities merged back together. reply cube2222 16 hours agoparentprevHey, tech lead of OpenTofu here! I might be in a bubble of course, but from what I've seen, I've been positively surprised by the uptake of OpenTofu so far! I do also expect OpenTofu 1.7 to be more interesting for people to migrate to, as it'll include a bunch of OpenTofu-exclusives. reply dimitrios1 15 hours agorootparentI work for a large enterprise and we use terraform open source atm, but I can pretty much guarantee we move to OpenTofu once the dust settles a little more. reply davidw 15 hours agoparentprevYes, a real fork isn't just a new repository, it's recreating the community that drives an open source project around the new codebase. reply sdesol 15 hours agoparentprevI'm actually surprised by how well received Valkey is right now, given that they forked not too long ago. Below are GitHub engagement insights for the two projects: https://devboard.gitsense.com/redis?id=f66d8a46ef&nb=all For a longer case study, you might be interested in Elastic and OpenSearch https://devboard.gitsense.com/opensearch-project?id=e766e581... Both projects are quite healthy, but Elastic is still clearly more popular. reply lenerdenator 16 hours agoparentprevPart of the bet is probably that since Terraform is almost necessarily going to run on a cloud service that is already being paid for, the user might not care that a bit gets added to the bill. Redis? I'm not sure. Like you say, they don't want Big Tech to slap a UI on it and profit. And, really, once they start competing on price (which they might sooner rather than later to keep people from going back to on-prem) you can guess they might use something that's free so they don't have to pay the license on a per-server instance. reply paxys 13 hours agoparentprev> because people won't want AWS/GCP/Azure to ... Important to note that the \"people\" in this case is the company that bought the rights to the Redis trademark from the original creator and then took on $350M in VC funding. The community that created and supported Redis since its inception was not involved in the decision, and isn't getting any of the benefit. reply karmakaze 15 hours agoparentprevThe Hudson/Jenkins reference is interesting in that Hudson was soon later abandoned (i.e. donated to Eclipse Foundation). For Redis there could be space for both, but if I want anything larger than a single instance I'd just sooner use MS Garnet[0]. [0] https://news.ycombinator.com/item?id=39752504 reply weinzierl 15 hours agoparentprevI think Hudson -> Jenkins is not really a fitting example here, because it was more of a rename caused by Oracle owning the Hudson trademark. While Oracle halfheartedly indicated to continue Hudson on its own, from what I remember it was pretty clear from the start that Jenkins was the new Hudson. reply geek_at 16 hours agoparentprevBest example for a successful fork is gitea. It's so incredible what they have achieved since the fork from gogs reply notyoutube 16 hours agorootparent(And then gitea was forked into forgejo, which is used by codeberg, iiuc) reply rhdunn 14 hours agorootparentI'm using Gitea locally. I haven't come across forgejo before. It seems to be forked on the basis of providing a fully open source solution (it states that some Gitea enterprise features are not present). Do you know what the main differences are and whether it is worth switching? reply Macha 13 hours agorootparentThere are two main differences: 1. Gitea, Inc replaced the prior community leadership structure with one controlled by Gitea, Inc and have pivoted into being primarily about a hosted code SaaS they launched this year that didn't exist for the first 7 years of the project. They could do it because they had 2 of the 3 seats on the community stewardship organisation at the time to vote for it, but that wasn't always true. For a lot of people, Gitea's primary motivation was for the self-hosted and fully open use case, but now it's not clear that that will remain any more of a priority for Gitea Inc as it is for e.g. Gitlab 2. There were also some contributors who were interested for the sake of decentralising code forges (and not just git), and so were big into the forgefed project which was an activitypub based federation protocol. Gitea Inc were officially on board with that, but the contributors felt they weren't helpful enough to actually implementing that with missteps like letting grant funding from NLNet expire. The contributors from groups 1 and 2, and Codeberg banded together to make Forgejo after the Gitea change. Until the latest release (just last month) Forgejo was one way pulling all changes from Gitea, but they've diverged enough that they're now hard forks. Both have active development, but Forgejo's main unique focus is the federation stuff, while Gitea's was enhancing their CI offering. But while Gitea may have more development effort, being funded by a commercial organisation rather than volunteers and a non-profit, I think they have a long way to catch up to Gitlab in that front, so it feels like they've dropped their unique factor to try chase Gitlab. reply rhdunn 13 hours agorootparentThanks. At the moment forgejo does not have features I'm interested in as I'm not making my internal gitea instance public. However, I'd be interested in it if it provides support for things like hierarchical project grouping and organiation/group-wide project and issue tracking. So I'm going to keep an eye on the project to see how it evolves. One thing that concerns me is Forgejo's statement that they will diverge further from Gitea without migration commitments. This would make it harder for me -- and others -- to switch after upgrading to Gitea 1.22 and later. A similar concern is the other way around: if I switch to Forgejo and want to move back to Gitea, I won't be able to if the projects have sufficiently changed (e.g. if they implement project grouping differently). reply paxys 13 hours agoparentprevThat project was dead in the water once they decided to name it \"OpenTofu\". reply zer00eyz 16 hours agoparentprev>> Generally my stance with these forks born from community drama is \"wait and see.\" Whats funny about this one is as follows: 1. The license: BSD? LGPL? Do both... nothing says that you cant make the product available under both licenses. You prevent another rug pull... 2. The platform: Do both, Run the thing on GitHub like it always has been and back it up to the other platform. If MS makes GitHub into the next source forge... then you're already half way out. 3. The name: Not a hill any one should die on. Pick three, ask amazon legal to clear them or FSF legal to clear them and vote. Redict and valkey are both fucking stupid names... Yea you might have to live with storage mcstoreface but that would be better than either of the current options. As for FOSS drama... Lacking any clear leadership, peoples ability to self organize is limited. These sorts of things happen all the time (systemd, x vs waylaid, how many unix forks?) The winning side is almost always the one with clear leadership. reply stefan_ 16 hours agoparentprevI remember when we had the libav fork. Turns out 10 bickering idiots pronouncing the freedom revolution were ultimately not half as productive as one Michael Niedermayer. reply merb 16 hours agorootparenttbf, ffmpeg‘s api and thus libav is so obscure that it would be impossible to fork it and create a nice looking api, without bothering people. I vaguely remember the news about the Microsoft guy that was called out on twitter and I’ve read the issue and looked at some cli/api parameters and was stunned. So much possible flags. I hope that I will never need to deal with it. But still kudos for all the maintainers. It works and has probably support for all codes and all options of these codes. reply NewJazz 16 hours agoparentprevYou think opentofu is a flop? reply ChuckMcM 9 hours agoparentprevMany of these recent forks are being done because people won't want AWS/GCP/Azure to slap a UI on top of their free open-source product and resell it, making tens of millions of dollars per day in the process. I can't really blame them. I can. Free software is, at its core, wage theft. It is truly unfortunate that so many people don't understand \"big picture\" economics, not the $2 for a loaf of bread economics, but the creation of value, it's consumption, and allocating resources to maximize creating the \"right kind\" of value. Most programmers \"get\" that writing code has value, hell tech companies will pay $5,000/week in total compensation just to do that, of course \"coding for money\" isn't the same as \"coding on something you love\", or are invested in, or does something you want. But here is the detail that so many miss, it is still $5,000/week \"worth\" of value. Whether or not someone is paying you to do it, there is still value there. And when you think about it is it all that surprising that putting that value into a thing doesn't make it something others might value too? Others who don't have the chops or the time to make it themselves? THAT is economics. And there are so many people who can see that value just sitting there and say, \"Hmm, I bet I know someone who would value that, and I could get them to pay me some of that value in cash money in exchange for hooking them up.\" And it's game on buddy. And what is that game? Stealing the 'value' that would have been returned as a wages to a coder if they had been hired and keeping it for themselves. You might as well decorate your front lawn with $100 bills and put up a sign that says, \"These bills are decoration I forbid you from using them to go buy things for yourself.\" Sure some folks would respect that sign but a whole lot more would say, \"Uh, good luck with that, and thanks for the cash.\" If you write code for \"free\", no matter what license you try to put it under that \"prevents\" people from profiting off of it, they are gonna profit. You can either make it possible for them to profit and cut you in on some of that, or you can decide for yourself that you're okay with all of that value you created funding someone else's lifestyle. reply kyriakos 15 hours agoparentprevWhat's the situation with Opensearch which came from elastic after license changed so it stops getting get abused by aws. reply sdesol 14 hours agorootparentOpenSearch is doing quite well, but Elastic is still more dominate based on my GitHub insights tool. reply asia92 16 hours agoparentprevRemember the joke that was ayojs? reply ramesh31 16 hours agoparentprev>Generally my stance with these forks born from community drama is \"wait and see.\" They are generally a good thing, save for the poor souls that will end up maintaining a project that was started with them while they were still active. The success of the fork doesn't matter so much as the direction it inevitably pulls the original project. The io.js drama gave us a huge step forward with NPM once their hand was forced. Hopefully some good ideas come of this too. reply jedmeyers 15 hours agoparentprevWhat AWS/GCP/Azure provide with managed Redis is much much more than just a UI. There are a lot of technical issues that come with running Redis reliably in the cloud. I’d say UI work is probably 5% of that at most. reply sjm 15 hours agoprevCan someone ELI5 why I should feel bad about Redis charging massive companies like AWS and Google to use/sell/profit from their software? Am I really supposed to feel like Amazon is the good guy in this situation? As far as I can tell this change doesn't affect 99.999% of Redis users, but I understand I could be missing something? reply hnarn 14 hours agoparentFor a lot of people, myself included, it’s ideological. It’s not about “feeling bad” for gigantic corporations, it’s about what FLOSS stands for. Something either is or isn’t free software, and for a lot of people that doesn’t matter, but it matters to me. reply theamk 13 hours agoparentprevIt affects users who were using hosted redis - there is no longer any competition. If you liked to use redis hosted by someone else (for example for lower price, or better integration, or something like this), it is no longer possible. It's Redis Lab way or highway, and they can jack up prices as high as they want. reply forgotpwd16 14 hours agoparentprevSee: https://news.ycombinator.com/item?id=39915700 & https://news.ycombinator.com/item?id=39915741 reply redskyluan 16 hours agoprevSomething need to be fixed. Opensource is not about license, it's about people. OSS companies are fighting against cloud vendors to survice. https://zilliz.com/blog/Redis-tightens-its-license-How-can-a... reply eatonphil 16 hours agoprevWhy, unlike Redis and Elasticsearch and Terraform, was there no big community fork of MongoDB when it was relicensed? Cockroach, Materialize, and MariaDB were all also relicensed without massive backlash, I think. But I think that's because they had fewer users at the time. But Mongo's was the one relicense event that didn't produce so much shock that a new fork came out of it. And Mongo's stock is doing great, if that's a good proxy for their overall success. I wonder what the difference is. reply bit_flipper 15 hours agoparentDefinitely an interesting question. Some things that may explain why -- Mongo was always AGPL and relicensed to SSPL. This had the following consequences: * Very few companies and zero large cloud companies ever attempted to run the MongoDB codebase in production as a managed service, other than MongoDB the company. * Mostly because of the above, MongoDB did not receive many code contributions that did not originate from within the company. There were some, but not nearly to the extent of the others you listed * The difference between AGPL and SSPL is not nearly as large as the difference between BSD and SSPL or Apache and SSPL. reply jillesvangurp 14 hours agorootparentThe license matters less than copyright ownership. Prior to the license change, MongoDB insisted on copyright transfers. Every line of code was owned by them. That's why they were able to re-license it. Elastic, which used to use the Apache 2.0 license, did the same thing: they insisted on copyright transfers as well. Other people that didn't own the copyright to any mongodb source code of course had the right to take the source code and fork it under the AGPL. But there would have been no choice about the license under which to distribute that fork because of how strict AGPL is. By insisting on copyright transfers, Mongodb was able to dodge that and re-license the entire code base without having to require permission from anyone because they owned all of it. For the same reason, there never was much of a community of contributors outside of Mongo. Most large companies would have steered clear of that legal mess and declined to contribute or fork. The flip side of course is that this strong ownership marginalized mongodb as a community even before the license change. It simply didn't matter much to most large companies as they would have steered clear of it anyway. With Redis, this is not the case. Redis the company was an active contributor to the code base but most of the contributions actually came from the outside and they never owned the copyright to those contributions. The BSD license allows anyone (including Redis Inc.) to redistribute the code under whatever license. Which is why Redis can do this. But for the same reason everybody else can continue as is using Valkey without having to worry much about Redis the company having retired from what otherwise is a thriving OSS community. reply weinzierl 15 hours agorootparentprevWere there any MongoDB cloud offerings of the AGPL version? If SSPL is effective as a poisonous pill against AWS and Co. but SSPL is not, that's a big difference in my book. reply byroot 13 hours agorootparentYes, MongoHQ, later renamed compose.io: https://en.wikipedia.org/wiki/Compose.io reply plorkyeran 13 hours agorootparentprevmLab/MongoLab, which MongoDB acquired when they started to seriously pursue their own cloud offering. reply SSLy 14 hours agorootparentprevnone that immediately spring to mind. aws reimplemented just the API. reply mattnewton 15 hours agoparentprevAnecdotal but the perception in my corner around the time of the license switch was that Postgres was good enough now with bjson columns, and we could just use that for new projects. Projects that needed to scale beyond what is easy with Postgres are fairly rare and usually mean you’re doing well enough to throw money at citus or whoever to prevent everything from melting. reply gen220 14 hours agoparentprevMy impression, as somebody who's had to choose this layer of infrastructure a few times. Many people who \"start\" with those database choices either (1) \"end up\" elsewhere or (2) are perfectly content with treating their database as a SaaS business expense. Their communities have self-selected into a [profitable] niche that excludes the kinds of people (like the author of this blog post) who are both capable and motivated to maintain a fork. The author of TFA would probably categorize MongoDB as \"software I hate\" (a category he touches on in the article), to put it in perspective. reply evanelias 14 hours agoparentprevThe MariaDB database server was never relicensed. As a fork of MySQL, it is GPL, and as far as I understand must always remain GPL. You may be thinking of their separate MaxScale proxy product, which uses the Business Source License, which MariaDB created. reply eatonphil 10 hours agorootparentGot it, yeah I always get this confused. Thank you. reply thiago_fm 15 hours agoparentprevMost people I know (including me) that used MongoDB in their companies for sometime, already carried a lot of regrets after 6 months. I think the community were not so excited about forking or doing anything with it, when there are better battle-tested solutions I believe it takes an organization a huge inertia to be stuck with it, possibly if you managed to grow your company really quickly. But most projects I've seen that used mongoDB ended up being rewritten/thrown away I find it astonishing when I see how much mongoDB has managed to grow its revenues, now its like $2B trailing when literally nobody needs it RDBMS's can be scaled to \"planetary\" scale and once you throw in a few other noSQL DBs to fill in other gaps (like Redis), you literally will never need to pay a license MongoDB's success is probably a similar case to Oracle, there are many OSS alternatives but some old businesses have the foundation of its business on that db and moving out would have a huge cost, so they accept shredding millions to software that OSS have even better alternatives reply AtlasBarfed 15 hours agoparentprevProbably because the people that care about such things will just move to Postgres, if they already haven't. reply zzzeek 14 hours agoparentprevmongodb is not very good, is the difference reply laurent123456 16 hours agoprevIf AWS and other cloud providers gave if only 0.1% of the profit they generate out of these open source projects back to the developers we probably wouldn't have this problem. Unfortunately they don't and it's only fair that eventually those developers take it in their own hands. It's not a great situation but it's certainly understandable. reply kbenson 15 hours agoparent> Unfortunately they don't and it's only fair that eventually those developers take it in their own hands. \"Fair\" is somewhat loaded. The developers certainly have a right to change their product and charge for it, but it's not nearly as cut and dry in my opinion. How many contributions were made because of the completely open nature of the product? Is it \"fair\" to those people that the controllers of the project want to change how it's offered at a later date? Some people are happy to feel like something they have contributed is in use by a lot of people regardless of whether someone else is making money from it. There are often lots of entangled assumptions in open projects like these. Ultimately, people have a right to offer their work as they want, so I see no problem with projects trying to request additional restrictions on how their work is used, but I also don't see a problem with companies using open projects as offerings. It was offered for free, and it's not like the cost of the offering isn't usually just the cost of the underlying resources plus some additional amount for ease of management. reply snotrockets 16 hours agoparentprevDisregarding the question of if the CSP compete fairly with providers of open source SaaS, your math is broken: AWS revenue is about $90.76B, though most of it isn't from Redis, I'd assume. But let's be generous, and assume 10% of that is. So about $10mm. For the recent version, Redis-the-company contributions to Redis-the-software were less than third of the code base, so let's say they get $4mm. That's very little revenue for a company that has a valuation of over $2B. reply kbenson 15 hours agorootparentRevenue is irrelevant, profit is what matters. The vast majority of the cost of any redis service offered is going to be the cost of the underlying compute and disk resources used. In some cases the margins could be close to nonexistent after paying for the resources utilized and the people to manage it, and it's used as a table stakes service that needs to exist for people to want to use your platform. The vast majority of these projects didn't seem to have a problem with large companies like Netflix using their software, even if it was put on a cloud server, as long as it was managed by Netflix. Now that the management portion is moved to the cloud provider, along with some amount of possible profit, it's a large problem? Was it not a large problem when the companies were using these projects directly? Was there not some assumption and hope these companies would use these projects by the people contributing to them? reply diath 15 hours agorootparentprevThe $2B is fake money though, the $4M would be actual paycheck. reply philshem 15 hours agorootparentYes, and per year. reply snotrockets 15 hours agorootparentLast year, valuation multiple for public across tech was about 2.6. That would make this contribution worth less than $11M in valuation. Considering Redis raised about $350M, $11M valuation is minuscule compared to the expected valuation to make that investment worthwhile. reply Jtsummers 15 hours agorootparentprevYou must mean something other than 10%, that would be $9 billion by your numbers. 0.1% would be about $9 million so would be closer to your envelope math. reply snotrockets 15 hours agorootparentOP suggested the CSP should give 0.1% of their revenue to open source software they make their revenue from. As Redis isn't the only SaaS AWS offers, I was gracious and allocated 10% of those 0.1% to it. reply Jtsummers 15 hours agorootparentGot it. It would actually be less than that since the number you selected was for revenue, not profit which is what OP asked for. Looking at some prior years it looks like AWS profit is close to 1/4 their revenue, so $90 billion would leave them with (generously) $25 billion profit. 0.1% of that would be $25 million and 10% of that (your estimate of Redis's share) would be $2.5 million. reply snotrockets 15 hours agorootparentWhat can I say? I'm very generous with other people's money. reply abraae 15 hours agoparentprevWhich contributors would get the money? Would you allocate it according to number of commits? Would it include corp contributors? reply zoogeny 14 hours agoprevThere were copious reference links in the article but the one that stood out to me and that I spent some time reading was this GitHub issues discussion on license [1] (but really on the differing opinions of two of the larger forks communities and values). I am pleased that Valkey has made the decision to remain independent from the competing Redict fork project. The dogmatism on display in that thread is frustrating. It is one thing to stand by your own principles and opinions, it is entirely another thing to aggressively push your opinions onto others. With the two projects remaining independent, we will get to see which kind of community stewardship results in project success and longevity. The alternative, I fear, might have been technically minded people being railroaded by ideologically driven zealots. Dogmatism and zealotry are words we probably mostly associate with religion, but I think they apply exactly to the kind of people I would proactively exclude from any public community I was trying to build. 1. https://github.com/valkey-io/valkey/issues/18#issuecomment-2... reply hn72774 16 hours agoprevI had a bad experience with redis salespersons sending me unsolicited calendar invites. They were promptly deleted and marked as spam. You are pushing away potential customers with this behavior. reply rafaelturk 16 hours agoprevI can't wait for comunity consensus of the new de facto Redis Fork. So far seems that the current choice is in favor of Valkey. reply mirekrusin 15 hours agoprevPlease also use old redis.io layout for docs/command docs, not this abomination marketing team released recently. reply binary132 16 hours agoprevMore and more software which was previously small-proprietor / artisan / commons is now, and will become, essentially corporate property. reply eigenvalue 15 hours agoprevI really don’t get why Redis didn’t keep the same license but with an addendum that companies (where this applies to the ultimate parent company) that have more than some huge amount of revenue (say, $5 billion), can’t resell it. Similar to the license provision for the Llama2 model. That wouldn’t upset people nearly as much, since it would only apply to a handful of hyper scalers, and then Redis the company could cut deals with all of them to make a bit of money. Now they will end up with much less because of these fully open source competitors that were only created because of their foolish license change. reply abdullahkhalids 13 hours agoparentI agree. The dual licenses don't really slice reality the way they want them to - that large companies should pay, and everyone else should use for free. So just say that. Pick a revenue number, and say that anyone above that number should contact for an alternate license. To answer sibling comment by theamk: I think such projects should just disclose their rates up front. reply theamk 15 hours agoparentprevThis would mean every startup that hopes to be acquired will avoid it. (\"resell\" is legally complex, and lawyers during acquisitions are very cautious) reply malkosta 16 hours agoprevValkey is the way for me...not that much to think or consider... reply Diederich 16 hours agoparentCan you expand on the high points from your perspective? Thanks. reply richardwhiuk 15 hours agorootparentIt's the most straightforward change from a community and development perspective, and it's got most of the core developers. reply moufestaphio 15 hours agorootparentprevAs I understand it, linux has dependencies on Redis. Since redis licence change, that can't work so linux needs an alternative. Linux foundation made a fork of redis named valkey, which was instantly backed by most of the Core maintainers, and many large companies (AWS, GCP, Oracle, Snap). So it seems obvious that this fork which will be part of linux dependencies, has backing of big corps and core maintainers isn't going anywhere and is likely the 'new redis'. https://www.linuxfoundation.org/press/linux-foundation-launc... reply NewJazz 15 hours agorootparentAre you saying Linux, the kernel, has a dependency on Redis? That is not true. reply cyberpunk 15 hours agorootparentprevLinux depends on Redis? Do you mean the foundation uses it to serve some marketing sites or it's part of some build farm/infra or something? reply malkosta 14 hours agorootparentprevI don't have many points to consider, that's the root of my comment. The only point I can think of is that a project is a result of its people. And Redis people are now Valkey. Thus, for me, the word Redis is a deprecated word that in past meant what is now named Valkey. reply skybrian 16 hours agoprevIn the short term, how about not upgrading? Stick with the current stable version as long as makes sense. reply MrOxiMoron 16 hours agoprevat work we're still at the pre-fork drama version of elasticsearch, it works and leaves us open to pick whichever direction we want to go at a later date when we need it. The same will be for redis, it works, no need to stay up to date. reply cyberpunk 15 hours agoparentI guess you're lucky not to work in a regulated industry: https://discuss.elastic.co/c/announcements/security-announce... reply NewJazz 15 hours agoparentprevDo you still get support from Elastic with that version? Why not just move to OpenSearch? reply Rapzid 15 hours agoprevMy prediction is that Valkey will be the dominant \"fork\" moving forward. I don't believe the Redict team's prioritization on their niche values is going to resonate well with businesses that are going to end up both using and contributing to development in this space.. However I think the RESP is going to become even more of a standard protocol that many viable implementations support moving forward. Microsoft is working on Garnet, and I can imagine other major cloud providers having their own implementations or forks at least. reply mfru 14 hours agoprevIt seems to me that dual-licensing AGPL + commercial and no CLAs is the best way to go. AGPL ensures no one will serve your software via network without also showing their code (which they can avoid by buying commercial licenses and thus funding the project) and no CLAs ensures that the project can not be relicensed without every contributor agreeing. (after all, if really every person can agree it is a good idea there might be some merit to it). reply kubanczyk 13 hours agoparentNo CLA for commercial license would mean that any contributor retains copyright to their work. So, you mean they get proportional dividend from commercial contracts? Is there a precedent? I'm honestly curious. reply Pet_Ant 10 hours agorootparentNo, because the license. If you release code as FooOSS please will remain as FooOSS. With a CLA the company that owns it can also offer it as a different license. If you don’t sign the CLA, you can offer it as a different license… But regardless I sell your code under FooOSS until the cows come home without paying anymore else a dime. If I couldn’t, then it wouldn’t be OSS. Now, depending on the FooOSS I might have to share any changes I make… but you cannot revoke my license to your’s. reply CyanLite2 15 hours agoprevMicrosoft Garnet is the only redis fork that actually improves upon it, improving performance by an order of magnitude. Maybe the others will catch up, but it's very \"Hacker News-esque\" to fail to mention something the largest company in the world is working on. reply dindresto 14 hours agoparentAs far as I know, Garnet is not a Redis fork, but a new software that happens to implement the Redis protocol to be compatible with its clients. reply forgotpwd16 14 hours agoparentprevBesides that Garnet is not a fork, as sibling comment mentions, it was recently discussed in HN: https://news.ycombinator.com/item?id=39752504 reply snotrockets 15 hours agoprevI really like Redis, used it a lot in the past, but I just don't see a place for it in a modern, cloud-hosted architecture. A central cache just doesn't make sense in any scenario I can imaginewhen considering the cost/performance curve (my imagination might be limited though). reply qudat 15 hours agoprevConsidering the recent xz backdoor exploit, I think we might have a cultural problem within foss: https://bower.sh/xz-backdoor reply tayo42 16 hours agoprevThe post is a little conflicted? It starts by saying he likes redis becasue it was a great fit for his personal vector db he wrote on top of it but then goes on to say he's worried about the future of the company for focusing on ai support, which is what he was already doing with it? I'm not sure I really get the concern about trying to support ai needs though. Atleast it's relevant compared to making changes to a stable kv store for the sake of change. Are these redis forks just vocal terminally online internet people fighting, like most people don't resell redis and don't care, don't comment or get involved? reply bevekspldnw 16 hours agoprevAs somebody who pulled down an open source project I find this trope of FOSS losing its way for greed utterly disingenuous. People need to put food on the table, period. I’ve had people take my work and make more money from it than I have, with no attribution or credit. Then I pulled it down and got a lot of people asking why it’s no longer free, when I tell them I can sell them a license they don’t even reply. I care more about my family than your FOSS ideals. End of story. reply nurple 15 hours agoparentThe point is to benefit humanity, not make money. With that said, sure this ideology seems impossible to realize change in the current money-obsessed world, but the thing is that these ideologies are simply incompatible. Yes, I'd say that you are also unfortunately money-obsessed, your whole post is centered on it; I don't blame you though, I also have to focus a majority of my life on money to raise my children, but it doesn't have to be like that. We won't make change until enough people are willing to sacrifice to make it, and most people just aren't willing to bear that load when going with the existing system, while broken, alleviates that pain while unfortunately also acting as a network multiplier to not just perpetuate, but also strengthen, the status quo. Make no mistake, the existing system is built with features made explicitly to limit your freedom. You simply can't live without selling your labor. If you look at a lot of people trying to start their own businesses (FLOSS or not), they're trying to use the system to earn their freedom in a system where true self-determination and financial independence are synonymous. That's all the FLOSS ideology is, buying our common self determination through leveraging the power of computing. And looking at how much money it's generated, you can't say that was an impossible dream, but we're losing the war. reply bevekspldnw 14 hours agorootparentCool, I’ll send you my land lords banking info so you can pay my rent, the info for my kids childcare, and where to contribute to my retirement account. Also, for what it’s worth, I’ve forgone millions of dollars in compensation over the years to work on socially beneficial projects, and literally just left $250k on the table last year alone. But yeah, wanting to support my family makes me greedy. reply nurple 4 hours agorootparentI said money-obsessed, not greedy, not really the same thing, but it is something that's becoming more and more pervasive. Case in point, your whole response was focused purely on money. I actually find it pretty interesting that there's so much hostility towards my posts when I attempt to start a discussion on alternatives to our present and decaying society. Like, it is a lack of imagination? A resignation to reality? Belief that this is the best we can do? Personally, I /can/ envision a future without landlords, banking info, and retirement accounts, where we direct the immense productivity of our people away from one that necessitates a focus on money. I guess I'll just need to work on my pitch. reply lelanthran 13 hours agorootparentprev> The point is to benefit humanity, not make money. Maybe, but the priority is to put food on the table. You can't benefit humanity if you're dead. reply bevekspldnw 13 hours agorootparentNever underestimate the willingness of people to have others suffer for their ideals. reply sanderjd 15 hours agoparentprevYep, free software originated among academics employed by institutions who paid them for things things other than their creation of software. And good on them! But there just aren't enough professors, grad students, or young people with plenty of extra time on their hands, to build all this stuff for free. reply bevekspldnw 14 hours agorootparentAlso, that code tends to be proof of concept, full of bugs, and never audited or used in production. Academic peer review doesn’t include code review. reply marcos100 12 hours agoparentprevDo you share a part of you charge with the people that contributed? If not, what you're doing is the same as what others did when they took your work and profited from it. You've all agreed what can and can't be done with your code based on the license you used. If you want to make money with software, it's proprietary or dual-licensed (A)GPL with CLA. Anything else you'll bait and switch on people. reply aydyn 15 hours agoparentprevHave you made meaningfully more money from it after taking it down? reply bevekspldnw 14 hours agorootparentYes. reply globular-toast 15 hours agoparentprevHow did stopping them use it feed your family? reply bevekspldnw 14 hours agorootparentBecause I’m in the same market now and they are my competitors so I can’t monetize my work at cost if somebody else is taking my labor for free and undercutting me on price. These aren’t hard concepts. reply mfru 14 hours agorootparentprevWell, apparently it stopped people who made profit specifically by using their project without giving back and that might have alleviated the OPs frustration, which is fair enough. reply wood_spirit 16 hours agoprevChanging from an open source license is a kind of tech product spin on enshitification. Even if the product continues to get features and active development, like Oracle poured into MySQL after its acquisition, a database or framework going less open source is still a death knell. The mainstream programmers will move on and use something else. Anything else. Who who has heard that the license can change would now pick it for a new project? reply strbean 15 hours agoparentI'm confused. Does SSPL not require more sharing of source code than GPL and the like? I've always thought the \"we're just hosting a server with our modified version of this OSS code, so we don't have to share the source\" argument was trash. reply globular-toast 14 hours agorootparentIt's not even being compared to GPL. The original licence was BSD. You're right, these licences are meant to make things more free. It's incredible to me that even software engineers can't see beyond local optima and convenience to the bigger picture. BSD is like saying \"you're free to do anything including throwing shit over your neighbour's fence\". Individual freedom is basically irrelevant. It's community freedom that matters. reply bevekspldnw 16 hours agoparentprevPay up or roll your own. Nobody owes you free labor. reply starttoaster 16 hours agorootparentNobody owes you free labor, but it's a world of uncertainty. People don't like uncertainty. Will this labor be priced reasonably, at around the same level that I value it, or will they pull a vmware and ask for my first born? reply bevekspldnw 16 hours agorootparentThere’s no greater certainty than rolling your own and building your own stack. If you want perfect reliability, use the standard library of a language and nothing else. It’s actually possible. Ironically, the best open source projects are the ones that have no other dependencies beyond the standard library. reply refulgentis 16 hours agoprevI thought the author was bringing in genAI for little reason...then I clicked through. For some reason storing vectors and chats is core to Redis' vision of its future. (https://redis.io/blog/the-future-of-redis/) I'm, generally, a mobile dev so I'm not familiar with redis. My handwave-y understanding is its a in-memory key/value DB. I don't understand how that brings anything to the table for genAI. Couldn't the pitch read the same if you were mongoDB, postgres, whoever? Also, my goodness, my eternal enemy, the idea a vector DB is something different than keeping a store of file -> pair>. The odds you need a vector DB unless you're doing insanely high scale stuff with AI are very low. If you're doing consumer stuff, please use ONNX and keep the pair, and thus the file, local and private. reply erikbye 16 hours agoparentRedis will be used for genAI as it's always used: answer queries faster. Users are not interesting in waiting, answers need to be immediate. Plus reducing load on whatever you got behind Redis is a nice bonus. reply xvinci 16 hours agorootparentI did evaluate a few vector databases for our RAG PoCs with quite a significant amount of metadata for permission handling on both the vector and the query, and execution time was in the area of milliseconds as far as I remember. The RAG performance hit pales in comparison to what computing time larger LLMs need, so I am not sure you are on the right track here. reply refulgentis 15 hours agorootparentprevNaively, I don't understand how Redis would be involved at all. Ex. in simplest system set up, we're running O(seconds) network request that relays output from a GPU to a client. Again, I'm a naive mostly mobile dev, but I'd presume the same machine running inference would stream the response JSON. reply miki123211 14 hours agoparentprevA vector DB is the complete opposite of what you describe, it maps list to pair. The queries it's good at are not \"what vectors map to this filename\", but \"what pieces of text are closest to this vector, and what metadata do we have about them?\" This is a non-trivial problem to solve if you don't want your queries to be O(n) where n is the dataset size. This is useful because AI models can transform any kind of content (usually text or images) into vectors, in a way that content similar in meaning is transformed to vectors that are close to each other. This can be used e.g. find all documents related to your search query, even if your search keywords are never directly mentioned, to find articles similar to the one you're currently reading, to search images by their descriptions, or even to see how closely a user submission matches \"undesirable\" content, like spam or porn. I agree that specialized vector databases are a little silly though, considering that Postgres and others have vector extensions now. reply Bella-Xiang 7 hours agorootparentThe specialized vector database performs well when processing pure vector tasks but performs badly when it comes to SQL compatibility and integration with the existing system; And the traditional database with vector algorithm or vector plug-in like ES, PG, and Redis, achieves the vector function, the advantage is that it is very easy to create tasks in a production environment, but when the data scale is relatively large, they will quickly encounter performance bottlenecks. There is a new type of vector database that combines the best of both worlds, which is MyScale, the SQL vector database. You can refer to the following blogs to see the comparison. our comprehensive benchmark evaluation reveals that MyScale exceeds other products in terms of filtered vector search accuracy, performance, cost-efficiency, and index build time by a long way. Importantly, MyScale is the only product tested that delivers healthy search accuracy and QPS across various filter ratios. https://myscale.com/blog/myscale-outperform-specialized-vect... https://myscale.com/blog/myscale-vs-postgres-opensearch/ reply refulgentis 12 hours agorootparentprevI know vector DBs x embeddings, so I'm afraid I'm just awful at communicating: to wit, and much to my consternation, I have to write and maintain code for both image and text embeddings, on 6 platforms. I think we're getting to the heart of my confusion, and I only assume it's because of different use cases/expectations on privacy. Lets say I'm CEO of Mousetrap Inc., and I got this .txt file, our top secret plan for a better mousetrap. I want genAI to pick out the parts about the new metal alloy. I upload the file to B2BAI LLC, who turns it into List, then we give it to the model and get back List>. Vector DBs store the List and the List> for retrieval. I, the top secret mouse-trap inventor, do not want my plan stored on any 3rd party computer. But, this app I use puts it in an a16z approved Vector DB™. The vector DB provider now has the embeddings (List>) and the chunks (List), which violate my desire to not have my top secret mousetrap plan stored at rest anywhere . reply esafak 16 hours agoparentprevVector databases aren't for key value retrieval, they're for similarity search. What's that got to do with onnx? reply all2 16 hours agorootparentOnnx allows arbitrary bundling and execution of ML models.. so maybe something to with the \"run it local and private\"? reply esafak 15 hours agorootparentVector databases don't contain ML models. There is nothing that is learned. Here is a typical algorithm: https://www.pinecone.io/learn/series/faiss/hnsw/ It is all about performance; latency and recall. reply refulgentis 15 hours agorootparentPresumably the output of an ML model, the titular vector, and the chunk of text that created that vector are stored in a vector DB? (that probably read aggressive, ignore my tone. At length: I run the model locally and store the vector locally, but I'm doing consumer use cases so I have different tradeoffs, so I'm glad to have someone who uses them interlocuting.) reply refulgentis 15 hours agorootparentprevONNX runs the ML model that is f(string) -> vector. The similarity search is done using those vectors, and needs to return the original strings. reply archsurface 15 hours agoprev [–] Isn't this a little old? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article delves into the 2024 fork of Redis, emphasizing the licensing alterations' influence on the software's future.",
      "It points out the shift from the original developer-centric Redis to a new enterprise-oriented version, leading to various forks like Valkey and Redict.",
      "There is uncertainty regarding Redis's future direction and challenges in selecting the suitable fork for new projects, yet the author is positive about Redis's ongoing evolution within the open-source community."
    ],
    "commentSummary": [
      "The conversation touches on open-source software, licensing changes, community forks, and how cloud providers affect projects such as Terraform, Redis, MongoDB, and Valkey.",
      "It explores the difficulties of sustaining open-source projects, the significance of licensing tactics, and the discussion on managing financial gains alongside societal advancements.",
      "The dialogue also looks into the utilization of vector databases for processing image and text data, privacy issues, and the efficiency of these systems."
    ],
    "points": 221,
    "commentCount": 174,
    "retryCount": 0,
    "time": 1713286294
  },
  {
    "id": 40056640,
    "title": "Quantum Algorithm Threatens Encryption Security",
    "originLink": "https://blog.cryptographyengineering.com/2024/04/16/a-quick-post-on-chens-algorithm/",
    "originBody": "If you’re a normal person — that is, a person who doesn’t obsessively follow the latest cryptography news — you probably missed last week’s cryptography bombshell. That news comes in the form of a new e-print authored by Yilei Chen, “Quantum Algorithms for Lattice Problems“, which has roiled the cryptography research community. The result is now being evaluated by experts in lattices and quantum algorithm design (and to be clear, I am not one!) but if it holds up, it’s going to be quite a bad day/week/month/year for the applied cryptography community. Rather than elaborate at length, here’s quick set of five bullet-points giving the background. (1) Cryptographers like to build modern public-key encryption schemes on top of mathematical problems that are believed to be “hard”. In practice, we need problems with a specific structure: we can construct efficient solutions for those who hold a secret key, or “trapdoor”, and yet also admit no efficient solution for folks who don’t. While many problems have been considered (and often discarded), most schemes we use today are based on three problems: factoring (the RSA cryptosystem), discrete logarithm (Diffie-Hellman, DSA) and elliptic curve discrete logarithm problem (EC-Diffie-Hellman, ECDSA etc.) (2) While we would like to believe our favorite problems are fundamentally “hard”, we know this isn’t really true. Researchers have devised algorithms that solve all of these problems quite efficiently (i.e., in polynomial time) — provided someone figures out how to build a quantum computer powerful enough to run the attack algorithms. Fortunately such a computer has not yet been built! (3) Even though quantum computers are not yet powerful enough to break our public-key crypto, the mere threat of future quantum attacks has inspired industry, government and academia to join forces Fellowship-of-the-Ring-style in order to tackle the problem right now. This isn’t merely about future-proofing our systems: even if quantum computers take decades to build, future quantum computers could break encrypted messages we send today! (4) One conspicuous outcome of this fellowship is NIST’s Post-Quantum Cryptography (PQC) competition: this was an open competition designed to standardize “post-quantum” cryptographic schemes. Critically, these schemes must be based on different mathematical problems — most notably, problems that don’t seem to admit efficient quantum solutions. (5) Within this new set of schemes, the most popular class of schemes are based on problems related to mathematical objects called lattices. NIST-approved schemes based on lattice problems include Kyber and Dilithium (which I wrote about recently.) Lattice problems are also the basis of several efficient fully-homomorphic encryption (FHE) schemes. This background sets up the new result. Chen’s (not yet peer-reviewed) preprint claims a new quantum algorithm that efficiently solves the “shortest independent vector problem” (SIVP, as well as GapSVP) in lattices with specific parameters. If it holds up, the result could (with numerous important caveats) allow future quantum computers to break schemes that depend on the hardness of specific instances of these problems. The good news here is that even if the result is correct, the vulnerable parameters are very specific: Chen’s algorithm does not immediately apply to the recently-standardized NIST algorithms such as Kyber or Dilithium. Moreover, the exact concrete complexity of the algorithm is not instantly clear: it may turn out to be impractical to run, even if quantum computers become available. But there is a saying in our field that attacks only get better. If Chen’s result can be improved upon, then quantum algorithms could render obsolete an entire generation of “post-quantum” lattice-based schemes, forcing cryptographers and industry back to the drawing board. In other words, both a great technical result — and possibly a mild disaster. As previously mentioned: I am neither an expert in lattice-based cryptography nor quantum computing. The folks who are those things are very busy trying to validate the writeup: and more than a few big results have fallen apart upon detailed inspection. For those searching for the latest developments, here’s a nice writeup by Nigel Smart that doesn’t tackle the correctness of the quantum algorithm (see updates at the bottom), but does talk about the possible implications for FHE and PQC schemes (TL;DR: bad for some FHE schemes, but really depends on the concrete details of the algorithm’s running time.) And here’s another brief note on a “bug” that was found in the paper, that seems to have been quickly addressed by the author. Up until this week I had intended to write another long wonky post about complexity theory, lattices, and what it all meant for applied cryptography. But now I hope you’ll forgive me if I hold onto that one, for just a little bit longer. Related Tagged Cryptography cybersecurity innovation quantum-computing technology Matthew Green I'm a cryptographer and professor at Johns Hopkins University. I've designed and analyzed cryptographic systems used in wireless networks, payment systems and digital content protection platforms. In my research I look at the various ways cryptography can be used to promote user privacy. Published April 16, 2024April 16, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40056640",
    "commentBody": "A quick post on Chen's algorithm (cryptographyengineering.com)217 points by feross 13 hours agohidepastfavorite44 comments jonahx 11 hours agoI appreciate short, accessible writeups on subjects like these. reply faitswulff 11 hours agoprevIt seems like the post-quantum algorithm that Signal selected [0] involves lattices [1] somehow: > Kyber is an IND-CCA2-secure key encapsulation mechanism (KEM), whose security is based on the hardness of solving the learning-with-errors (LWE) problem over module lattices. Curious to see if Chen's work will eventually lead to Signal selecting a different algorithm. [0]: https://signal.org/blog/pqxdh/ [1]: https://pq-crystals.org/kyber/ reply contact9879 10 hours agoparentGreen explicitly mentions Kyber in his post. > NIST-approved schemes based on lattice problems include Kyber and Dilithium (which I wrote about recently.) > Chen’s algorithm does not immediately apply to the recently-standardized NIST algorithms such as Kyber or Dilithium. And it's not just Signal. Apple's new iMessage protocol, PQ3, uses Kyber, too. [1] Most deployments of PQ-crypto that I know of have used Kyber. [1] https://security.apple.com/blog/imessage-pq3/ reply glitchc 9 hours agorootparentdeleted reply jameswryan 9 hours agorootparentRainbow is not a KEM, but a signature scheme. reply tptacek 9 hours agoparentprevMore or less all of the \"serious\", actually deployed PQC schemes involve lattices, going back before the competition. reply contact9879 8 hours agorootparentI've seen some Classic McEliece deployments, too. Well, I know of only one: Mullvad. https://github.com/mullvad/mullvadvpn-app/blob/main/talpid-t... reply tptacek 7 hours agorootparentSome helpful, perhaps valid context is that lattice-based cryptography was a contender even before PQC became a thing (NTRU being the obvious example). Really the only point I'm trying to make here is that there's nothing eyebrow-raising about systems using lattice crypto; after IFP/FFDLP stuff like RSA and ECDLP, lattices are maybe the next most mainstream approach to constructing asymmetric systems. reply keepamovin 7 hours agoprevThere'll probably be discovered a class of problems which is hard and Q hard, provably and we'll be set. Something basically new to cryptography, that was tried once in the past but failed until seen through a new light of more recent maths or research fashion. But until then it seems to me like something based on the difficulty of attacking hash functions would be a good bet for Q resistant. Totally unsure how to make a PK scheme from that, but it has a few nice properties: - hashes are often tuneable, you can add more state and increase the keyspace/security - good hashes don't have any weaknesses that Q can exploit - hashes are often pretty fast - hashes are well studied - hashes seem to be hard in C and hard in Q reply karl_gluck 7 hours agoparentCheck out the Winternitz One-Time Signature Sphere10.com/articles/cryptography/pqc/wots Signing many things with one identity is possible by precomputing a Merkle tree, but this takes time and the signatures get big. reply er4hn 4 hours agorootparentSPHINCS+ is a complete signature scheme that carries that idea to it's completion. Shameless plug for where I explain it: https://er4hn.info/blog/2023.12.16-sphincs_plus-step-by-step... reply Vecr 7 hours agoparentprevhttps://en.wikipedia.org/wiki/McEliece_cryptosystem Yes, it's reasonably fast even at very long lengths, but the main problem is the very long lengths. Code based and not hash based though. Edit: not very fast to generate a key though. It's mostly used for non-ephemeral stuff. reply amluto 6 hours agorootparentThe new algorithm purports to solve LWE with certain choices of parameters. LWE is the problem of solving a linear system of equations over a finite ring, where each equation has an additive error from a certain distribution. McEliece has a public key that is a general linear code. A code is a bunch of linear equations constraining codewords, and codewords are vectors over a finite field, and decoding a code is solving those equations subject to errors from a given distribution. Sounds familiar? They’re not the same problem, and the distributions are different in rather fundamental ways (which may or may not make a difference), but they are quite related. I would not move my eggs to the McEliece basket right now. Hash-based signatures sound as safe as ever. reply YoumuChan 7 hours agoparentprevWe don't even know the complexity class of factorization or discrete log, yet we still use those problems in DH, RSA, ECDSA, ... reply eru 6 hours agorootparentAll of those problems are known to be in NP and co-NP. In that sense, we know some complexity classes they belong to. However, we don't know if these bounds are tight, or whether they are eg in P, or something in between. reply keepamovin 6 hours agorootparentWe don't know that factorization is NP-complete> Show me a reduction from SAT to factorization. It's kind of trivial to say it's in NP because we can verify in P time, that's not a criticism of you just of the definition!! I think a better definition of NP is \"only nonpoly algos can exist, no P algos can exist\". By that definition of NP, we don't even know that it's in NP strictly because there could exist P algorithms for solving it. It's more in 'unknown-NP' if that were a class! hahaha! :) reply SJC_Hacker 4 hours agorootparentI think this what alot of people get wrong. \"N' in NP does not stand for \"not\" it stands for \"non-deterministic\". Meaning you can solve in P time with a non-deterministic Turing machine, or alternatively, a function executing on all inputs in parallel. So maybe it should really be P and NDP. reply eru 4 hours agorootparentprev> We don't know that factorization is NP-complete. Yes? No one ever said it was. None of the common cryptographic problems are expected to be NP-complete, even if they aren't in P. That's because they are known to be in both NP and in co-NP, and it's expected that NP != co-NP. > I think a better definition of NP is \"only nonpoly algos can exist, no P algos can exist\". In what sense is that a 'better' definition than the standard definition? It sounds like what you are talking about is NP\\P (where \\ is set subtraction, ie 'NP minus P'). reply ihm 6 hours agoparentprevThere are a bunch of hash based signature schemes, e.g., SPHINCS https://sphincs.org/ reply glitchc 12 hours agoprevLet's wait for the dust to settle to see if Chen has indeed broken the Shortest Vector Problem for lattices. Bold claims require strong evidence. reply gradschoolfail 10 hours agoparentAs pointed out by archgoon (and also pbsd) only for specific parameters is the problem broken, somewhat akin to saying a problem is NP-hard doesnt mean all instances of a problem are hard. But in this case for those parameters it isnt even NP hard. reply vitus 11 hours agoparentprevI was under the impression that the Shortest Vector Problem was NP hard (at least, in some cases). If this works even in those scenarios, that's an even bolder claim by far. reply pbsd 10 hours agorootparentSVP is NP-hard for approximation factors much smaller than this algorithm reaches. This algorithm solves approximation factors of at best O(n^4.5), but NP-hardness is only shown for approximation factors well below n^(1/2). See Figure 1 in page 2 of [1] for a breakdown of the hardness of various approximation factors. [1] https://cims.nyu.edu/~regev/papers/cvpconp.pdf reply stoniejohnson 12 hours agoparentprevExtra-ordinary claims require extra-ordinary evidence! (my favorite maxim from 2012 era atheism debates on youtube) But yeah I totally agree. Putting the cart before the horse here with the outlined consequences probably isn't smart, but to be fair, I haven't (and probably couldn't) read the paper. reply deathanatos 10 hours agorootparent> (my favorite maxim from 2012 era atheism debates on youtube) It's a bit older than that: https://en.wikipedia.org/wiki/Extraordinary_claims_require_e... reply ecosystem 3 hours agorootparentprev\"Bananas: atheists worst nightmare\"? reply archgoon 11 hours agoparentprevChen has not claimed to have broken SVP; only in certain situations. This is a better LLL; not a polynomial hierarchy overturning. reply dang 11 hours agoprevRecent and related: Quantum Algorithms for Lattice Problems - https://news.ycombinator.com/item?id=39998396 - April 2024 (118 comments) reply anonymous-panda 6 hours agoprevI find the panic over potential threat of quantum quite amusing when the machine is still extremely theoretical - all existing machines are slower than classical and it’s not even clear they can scale to the required number of qubits. There’s nowhere near the same urgency and significantly more denial over global warming. A bit apples and oranges but climate models have a better track record, are wildly conservative (ie our present is much worse than the past climate models predicted) and it’s a real problem we know exists and is a bullet train headed our way. reply ziddoap 6 hours agoparentI think this may be your technology bubble showing. Global warming is talked about on the radio, TV, movies. It's sung about in songs. There's several conferences, widely-attended protests, stickers on appliances, tax initiatives, etc. A few comments on obscure sites like HN can hardly be called a panic. It is silly to suggest that there is more urgency about post-quantum attacks on crypto than global warming. reply CobrastanJorji 6 hours agoparentprevI totally agree that climate change is a far more serious problem than quantum computing, but we do actually spend quite a bit on climate change, if not nearly as much as the problem warrants. Depending on how you measure it, we theoretically spend about a trillion bucks a year worldwide on climate change ( https://www.climatepolicyinitiative.org/publication/global-l... ) and maybe a couple billion bucks a year on quantum computers. reply eru 6 hours agorootparentMeasures to combat climate change are weird, and also politicised in weird ways. Eg the US administration like to pretend that climate change is a top priority, but then complains when Chinese tax payers generously subsidise solar cells and electric cars for American consumers. reply jon_richards 6 hours agoparentprevWhen global warming hits, the people who benefited from ignoring it will be best off. When quantum computers hit, the people benefiting the most right now will be the worst off as all their communications from this era are decrypted. reply SJC_Hacker 4 hours agoparentprevThe slow poison is alot less alarming than the sharp knive. reply enthdegree 10 hours agoprevThe Wikipedia link on \"lattice\" points to the wrong article. \"Lattice-based cryptography\" usually means group-sense lattices, not order-sense lattices. Lattice groups are even directly involved in the rest of the article. reply matthewdgreen 10 hours agoparentThanks, it's fixed. That's what I get for quickly adding links. reply Upvoter33 10 hours agoparentprevfix it! (please) reply mvkel 4 hours agoprevQuantum Luddite here. So much of quantum computing is theory, and so much of current crypto is applied. Is it realistic to think the first applied quantum computers could quickly get us to a P=NP solution, rendering all crypto effectively irrelevant? reply foota 3 hours agoparentQuantum computers are only tangentially related to P=NP, in that some problems in NP may have solutions that are polynomial time on quantum computers, but this says nothing about the rest of them. This class of problems is know as BQP. Is is possible, but unknown, that all problems in NP may belong to BQP, which would imply that a sufficiently large quantum computer could solve all problems in NP in polynomial time, but even then it would be possible that P does not equal NP, since P and NP are about classical computers. reply givemeethekeys 9 hours agoprev [–] If the research is true, will it make it possible for someone to easily steal my bitcoin? reply fsmv 5 hours agoparentThis research has no impact on Bitcoin, it was already broken post quantum by Shor's algorithm. They will simply change the signature algorithm when quantum computers become available so it should be fine. reply peddling-brink 8 hours agoparentprevOnly if you give them the keys. reply harryp_peng 5 hours agorootparentdude are you nuts the whole point of all that was to steal you without the key reply tptacek 7 hours agoparentprev [–] Yes, if they have a sufficiently powerful quantum computer. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author introduces a new quantum algorithm with the potential to compromise particular lattice-based encryption schemes, raising concerns about the security of current encryption methods.",
      "The algorithm's precise implications are under scrutiny, highlighting the need for further research to assess its impact on cryptography."
    ],
    "commentSummary": [
      "The focus is on Chen's algorithm in post-quantum cryptography, notably its inapplicability to NIST-approved lattice-based algorithms like Kyber.",
      "Mention of other PQC schemes like Classic McEliece, Winternitz One-Time Signature, and the use of lattice-based cryptography in NTRU is made.",
      "Debates include the complexity of cryptographic problems in both classical (C) and quantum (Q) settings, encryption schemes, the impact of quantum algorithms, urgency of global warming versus quantum computing threats, potential effects on cryptography, and the necessity of changing signature algorithms."
    ],
    "points": 217,
    "commentCount": 44,
    "retryCount": 0,
    "time": 1713298607
  },
  {
    "id": 40054019,
    "title": "Google DeepMind's Aloha Unleashed: Robot Dexterity Revolutionized",
    "originLink": "https://twitter.com/tonyzzhao/status/1780263497584230432",
    "originBody": "Introducing 𝐀𝐋𝐎𝐇𝐀 𝐔𝐧𝐥𝐞𝐚𝐬𝐡𝐞𝐝 🌋 - Pushing the boundaries of dexterity with low-cost robots and AI. @GoogleDeepMindFinally got to share some videos after a few months. Robots are fully autonomous filmed in one continuous shot. Enjoy! pic.twitter.com/5cREmQ9hqL— Tony Z. Zhao (@tonyzzhao) April 16, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40054019",
    "commentBody": "Google DeepMind's Aloha Unleashed is pushing the boundaries of robot dexterity (twitter.com/tonyzzhao)217 points by modeless 17 hours agohidepastfavorite94 comments modeless 17 hours agoMore videos: Hanging multiple shirts in a row: https://twitter.com/ayzwah/status/1780263770440491073 Generalizing to unseen sweater: https://twitter.com/ayzwah/status/1780263771858194809 Struggling to unfold a shirt: https://twitter.com/DannyDriess/status/1780270239185588732 Assembling gears: https://twitter.com/ayzwah/status/1780263775213629497 reply godelski 13 hours agoparentI wish more of these were shown at 1x (last one is). Sure, a bit slower, but if you watch the OP link at 1/2 speed it is still impressive. Here's a shoe tying at 1x: https://twitter.com/ayzwah/status/1780263776694182311 Interesting here how it ties the knot. The first knot is already in place and they just do the bow. I don't think how most people would tie their shoes would work well for a robot (bunny around tree method[0]) but I actually tie my shoes like this[1]. This is the same way the robot ties. So I gotta ask, was this a purely learned policy or was this taught or pushed in that direction[^]? I suspect the latter. [0] https://www.youtube.com/watch?v=YwqQvKtmefE [1] https://www.youtube.com/watch?v=XPIgR89jv3Q [^] by pushed in that way, would include watching that video or any other videos like it. reply polygamous_bat 7 hours agorootparentThis is an imitation learning policy running on the robot, which means that the robot was taught by hours and hours of humans controlling it to do the task. The robot didn’t learn to do anything from scratch. reply GuB-42 9 hours agorootparentprevHere is the source for that knot (as well as many others) https://www.fieggen.com/shoelace/ianknot.htm reply fragmede 11 hours agoparentprevUBTECH and Baidu out of China demoed a clothes folding robot demo two weeks ago (early April 2024), and the video is claimed to be 1x/realtime. https://youtu.be/8MRDF2pkIRs reply modeless 9 hours agorootparentCool video. I would say that the clothes folding task in the UBTECH video is much, much easier than the ones in Google's videos. In fact, it could potentially be performed by simple replay of scripted motions with no sensing at all (with low reliability). I have some things I always look for when I'm watching robot demo videos: 1. Are there cuts in the video? If so, the robot may not be able to perform the entire task by itself without help. UBTECH's video has a couple of cuts. Google's videos have none. 2. Is the video sped up? If so, the robot may be very slow. UBTECH's video is 1x which is good, but you can see that the robot does move somewhat slowly and does not switch fluidly between actions. Google posted both 1x and 2x-20x videos so that you can easily see both real time speed and long duration reliability. In the 1x videos Google's robot is also somewhat slow, however it seems to switch more fluidly between actions than UBTECH. 3. Is the initial state at the start of the video realistic? If not, the robot may not be able to start the task without help. UBTECH's video starts with a carefully folded and perfectly flat shirt already in the hands of the robot. Google's videos start with shirts relatively messily placed on tables and somewhat crumpled. 4. Is the task repeated? If not, the robot may be very unreliable at finishing the task. Google's videos show a lot of repetition without cuts. UBTECH's video shows only one instance (with cuts). You could still produce this video even if the UBTECH robot fails 90% of the time. 5. Is there variation in the repeated tasks? If not, the robot may fail if there is any variation. Google shows different colors and initial states of shirts, and also a much larger sweater. That said, almost all the shirts are small polo shirts and the robot would certainly not generalize to anyone's real closet of varied clothes. 6. Does the robot react to mistakes or other unexpected events? If not, it may be mostly playback of pre-recorded motions with little or no sensing influencing the robot's behavior. UBTECH's video shows alleged sensing, but doesn't show any unexpected things or mistakes. Google's videos show the robot recovering from mistakes. reply readyplayernull 7 hours agorootparentprev> 1x/realtime You can see at 1:05 how the man suddendly accelerates. reply NicoJuicy 4 hours agorootparentprevYeah, some weird things. Eg. Folding the shirt already in his hands, ... Like it can't pick it up with 2 hands? Some other weird things in that video too reply hammock 10 hours agoparentprevAt the end of the OP video, why does the right arm grab the hanger then hand it to the left arm? Seems like a highly unnecessary and inefficient step reply pulvinar 10 hours agorootparentThere will always need to be a \"wasted\" step. The hanger comes off the rack with the hook away from the robot and has to go back on that way. Since there are two shoulders of the hanger to put into the shirt, they have to hand the hanger to from one to the other, and that would leave the hook facing the last robot if they don't do this extra step. reply hammock 9 hours agorootparentCouldn't the left arm just take the hanger off the rod? reply pulvinar 5 hours agorootparentI don't see that it helps, since the task is symmetrical. Whichever arm takes it off has to give it to the other arm at some point so both sides of the hanger are used, and it needs to be given back either before or after doing the hanging so the hanger's hook is back on the outside (facing away from the arm holding it), before replacing it on the rack. Would be interesting to know if the AI figured this out (the hard way, I'm sure). reply yosito 15 hours agoprevThere's something odd about the way the arms move, like they are two distinct entities cooperating rather than being part of one coordinated mind. Maybe this is an example of the uncanny valley, or maybe it's because they are two physically separate arms, but it seems to me like one arm moves while the other waits for its turn. It's as if engineers programmed them to work sequentially. I wonder if it might be beneficial for engineers to study videos of humans doing these tasks and try to mimic those movements rather than trying to program a sequential procedure. reply patcon 15 hours agoparentNow I'm trying to imagine how our limb movements might be perceived by a creature that natively evolved the style of coordination in the video :) it would be \"weird\" but how might they describe that weirdness and what might underlie it in us..? reply dylan604 13 hours agorootparentLook, it moves its mouth while it reads. Like it can't do one thing without doing the other thing moving at the same time reply pixl97 13 hours agorootparentWhich reminds me of my favorite interpretation of \"They're made out of meat\" https://www.youtube.com/watch?v=7tScAyNaRdQ reply visarga 11 hours agorootparentthis was good reply williamcotton 12 hours agoparentprevIf you monitor your own movements you’ll find plenty of sequential procedures. The big difference with how these robot arms move is that they are firmly planted on a large table, whereas your arms attached to this self-balancing, lightweight gyrating torso. reply wantsanagent 8 hours agoparentprevThe policy is trained from human demonstrations. When you're teleoperating a robot you tend to \"not do anything\" with the arm you're not actively focusing on unless you need to do bi-manual operations. reply yosito 7 hours agorootparentThis would make sense. If I'm tele-operating a robot, I'm going to be much less coordinated than if I just use my own two hands to hang a tshirt. reply voxelizer 10 hours agoparentprevI feel we tend to coordinate movements to balance ourselves, specially with arms. In this case, the arms are independent from each other and are each firmly fixed to the table. reply lachlan_gray 15 hours agoparentprevSometimes I feel this about myself... I don't think much to walk or do something with both hands, they work stuff out on their own. How much do my legs or hands understand about each other? reply rotexo 15 hours agoparentprevI’ve been reading Vernor Vinge’s A Fire Upon the Deep where that is a characteristic of one of the species in the novel, and had the exact same thought. reply QuercusMax 10 hours agorootparentThe Tines are such a fascinating concept of how a pack intelligence could work. That whole universe has so many interesting ideas. reply CooCooCaCha 14 hours agoparentprevThat's because the robot has gone ultra instinct. reply macromaniac 15 hours agoprevIt's impressive that transformers, diffusion, and human generated data can go so far in robotics. I would have expected simulation would be needed to achieve such results. My fear is that we see a similar problem with other generative AI in that it gets stuck in loops on complex problems and is unable to correct itself because the training data covers the problem but not the failure modes. reply visarga 11 hours agoparentThat's because most models have been trained on data created by humans for humans, it needs data created by AI for itself. Better learn from your mistakes than from the mistakes of others, they are more efficient and informative. When an AI is set up to learn from its own mistakes it might turn out like AlphaZero, who rediscovered the strategy of Go from scratch. LLMs are often incapable of solving complex tasks, but they are greatly helped by evolutionary algorithms. If you combine LLMs with EA you get black box optimization and intuition. It's all based on learning from the environment, interactivity & play. LLMs can provide the mutation operation, or function as judge to select surviving agents, or act as the agents themselves. reply btbuildem 16 hours agoprevI guess this is obvious in retrospect.. but having two arms vs one greatly expands the range of possible tasks. reply baron816 16 hours agoparentSkin is also incredible when you think about it. Each square cm is able to sense temperature, texture, whether it’s wet, sticky, etc. And it’s self healing. It’s hard to imagine robots getting very far without artificial skin. reply reaperman 14 hours agorootparentYour point is entirely valid despite my critiques. Technically, skin can't sense whether something is wet, and isn't particularly great at sensing temperature. Skin senses pressure and heat flow (derived via sensing temperature change of the flesh itself, rather than the temperature of the object it is touching), and perhaps can sense shear (there is a unique sensation when skin is stretched/pulled apart), as well as the weight of an object (if it is absorbent and more wet than damp). This distinction about what skin can directly sense manifests itself to deceive the human brain about wetness and temperature, specifically. Wetness is a perception derived from feeling higher-than-expected heat loss and unusual pressure/sheer, and even through the sound made when squeezing an absorbent material or the sensation of water pooling around the finger (broadening the area of heat loss) when you squeeze into the material. Damp laundry at room temperature is perceived as obviously wet because it feels colder than it should if it were dry, but when we're pulling laundry out of a dryer we often can't tell if it's dry vs. still a bit damp -- the higher temperature of the object removes the sensation of heat flowing away from our fingers, so there's nothing our fingers can sense to tell us the clothes aren't dry until the clothes finally cool down to room temperature. Our skin also doesn't sense the temperature of an object well if that object has a particularly high or low heat transfer coefficient of conduction. I recently bought a 6-pack of beer cans which have a moderately thick plastic vinyl label shrunk around the can. When I reach in my fridge, I can't convince myself to perceive it as chilled no matter how hard I try. Even though the vinyl is the same temperature as everything else in the fridge, it doesn't pull heat out of my finger tissue, so my brain cannot perceive that it isn't \"room temperature\". Conversely, picking up a normal metal can of beer that is just barely below room temperature, my brain perceives it to be much colder than it actually is because the metal draws heat away from my fingers so quickly compared to other objects. If wood is cooled 5 degrees below room temperature, it doesn't feel cold, but a can of beer certainly does! It is absolutely incredible that our skin can sense things to such a high resolution that it seems like we have a lot more abilities than we actually have. It is also amazing how our brain integrates this into a rich perception. But there actually aren't many physical properties actually being measured, and this distinction matters sometimes for edge cases, some of which are quite common. reply pixl97 12 hours agorootparent> skin can't sense whether something is wet Ah the \"Are the clothes in the dryer cold or are they wet\" effect. reply yakz 14 hours agorootparentprevSkin can’t sense “wet” can it? I thought it was mostly just temperature, but also in combination with a few other properties you perceive it as moisture but it can be easily fooled because there’s not a direct sense for it. reply CooCooCaCha 9 hours agorootparentI think you answered your own question. reply mikepurvis 15 hours agoparentprevTrue, and I think it was on that basis that PR2 was conceived as a bi-manual mobile manipulator... it just also has a massive impact on cost. reply netcan 14 hours agoprevOoh! it can almost fold a shirt. Shade aside, robotics is so damned hard. The current under/over for godlike superintelligence before a robot that can make sandwiches and work the laundry machine... So unintuitive. reply nabla9 14 hours agoparentLook at the grapples it has to work with. If you would have to work with two chopsticks, or two spanners as hands, you would not do any better. reply tho23i43423434 10 hours agorootparentActually, you typically don't use all the dexterity of a five-fingered hand for most tasks. Opposing-digits gets you 90% of the way there, which is why when you finally start using fingers independently (say while playing the violin), a large amount of time is spent getting around the learnt 'synergies'. Unclear why we actually evolved fingers in the first place (balance ?). reply moffkalast 12 hours agoparentprevThe neat part about working in robotics is that nobody can tell if you're a genius or a moron because neither of the two can get the damn thing working properly. reply sheepscreek 9 hours agoprevWhat makes this special to me is their claim of using cheap robots. So, how much would one of those robo arms cost? In other words, what would it set me back to recreate this? reply krasin 9 hours agoparentHere is the tutorial how to recreate an ALOHA cell: https://docs.google.com/document/d/1sgRZmpS7HMcZTPfGy3kAxDrq... The robot arm kits are available commercially: https://www.trossenrobotics.com/aloha-kits It's my belief that 10x cheaper arms with the same performance are possible, and the only reason they don't exist is because nobody needs them in sufficient quantities. reply quux 13 hours agoprevUnexpected Ian Knot https://www.fieggen.com/shoelace/ianknot.htm reply ozten 16 hours agoprevThe speed that these arms/hands move at is incredible compared to 4 months ago. reply chabons 16 hours agoparentThe videos are all shown at 2x speed, but your point stands, this is still pretty quick. reply bastawhiz 13 hours agoprevThis makes me wonder whether Google regrets spinning Boston Dynamics back off as its own entity. reply modeless 2 hours agoparentThey should. If they don't yet, I bet they will in a few years. One of the most shortsighted decisions Google has ever made. reply throwaway29303 13 hours agoparentprevhttps://www.theverge.com/2017/6/8/15766434/alphabet-google-b... reply jjjjjjjkjjjjjj 16 hours agoprevI've worked in humanoid robots and manipulation for the past decade and this is mind blowing. For robots. Still pathetic compared to any human, but mind blowing for robots. I remember when we were hoping one humanoid would someday be able to replace a broken limb on another humanoid and we were designing super easy quick disconnects to make that possible. This is already way beyond that. Very impressive. reply fragmede 9 hours agoparenthave you seen https://youtu.be/8MRDF2pkIRs ? reply margorczynski 9 hours agoprevQuestion is how well it handles unseen variations and environments? This is a well controlled lab setup, with probably cherry-picked videos showing positive results. reply riidom 17 hours agoprevReminds me about \"Foxes in Love\" somehow. reply taylorfinley 16 hours agoprevI wonder if this unfortunate naming choice will cause a stir similar to: https://kawaiola.news/cover/aloha-not-for-sale-cultural-in-a... reply bastawhiz 13 hours agoparentThe story you linked either omits the information or buries it deep enough to obscure the _actual_ source of the controversy. I was living in Chicago at the time, and the scandal wasn't the name choice, it was the fact that Aloha Poke sent cease and desist letters to other poke shops across the country demanding that they remove \"aloha\" from their names: https://chicago.eater.com/2018/7/31/17634686/aloha-poke-co-c... > the Chicago-born restaurant chain whose attorneys sent cease and desist messages to poke shop owners in Hawai’i, Alaska, and Washington state demanding they change names by dropping the terms “aloha” and “poke” when used together. While Aloha Poke contends it sent notes in a “cooperative manner” to defend intellectual property, Native Hawaiians feel the poke chain is trying to restrict how they can embrace their own heritage. reply math_dandy 16 hours agoparentprevHopefully DeepMind will think twice before sending cease-and-desist orders to any Hawaiian AI robotics businesses with aloha in the name! reply taylorfinley 16 hours agorootparentI should definitely hope so! Though I think the name would cause a stir in local circles even without any legal actions. Tech companies in general are deeply unpopular here (see: Larry Ellison, Mark Zuckerburg, and Marc Benioff buying up big chunks of land, AirBnB and digital nomads driving up rental prices so high such that more native Hawaiians now live on the mainland than in Hawai`i, and perceived lack of cultural respect from projects like the Thirty Meter Telescope leading to major protests). The other thing is that words have a lot of power in the cultural frame, even just the concept of aloha being something that could be \"unleashed\" is likely to offend. All to say nothing off the palpable fear people have here of robots taking hospitality industry jobs like housekeeping (which are unionized in many hotels out here, and are actually one of the few low-barrier-to-entry jobs out here that can support a reasonable quality of life) I'm sure I'll get a ton of downvotes for bringing up cultural sensitivity and pointing out these concerns -- I don't mean to imply they're all 100% rational nor that no one should say \"aloha\" unless they're Hawaiian, but if anyone at DeepMind had a Hawaiian cultural frame I think they likely would have flagged these concerns and recommended a different name. reply 1024core 15 hours agorootparent> Tech companies in general are deeply unpopular here Which is such a shame, as Univ of Hawaii was one of the pioneers of the Internet: https://en.wikipedia.org/wiki/ALOHAnet reply n0us 13 hours agoprevCan't wait for someone to turn this into a product and make this available to the public! reply a_wild_dandan 12 hours agoparentThe Laundry Folding Helping Hands will sell so goddamn hard. When the tech gets there, I'll be first in line. I'll even buy the Vegetable Chopping DLC. reply throwup238 17 hours agoprevFinally, a robot that can tie my shoes for me! reply linsomniac 16 hours agoparentIn the last year I've started using that knot that those robots used, the \"Ian Knot\", to tie my shoes, and I'm loving it. https://www.fieggen.com/shoelace/ianknot.htm reply mjamesaustin 15 hours agorootparentYeah these robots tie a better shoelace than most humans! reply p1mrx 14 hours agorootparentDo most humans leave their laces dragging on the ground? Though to be fair, those laces are really long. The robot needs to unlace the shoes, cut some length from the middle, tie a double fisherman's knot, and relace them. reply im3w1l 16 hours agoprevTo me the most impressive thing is the arms servicing each other. When they can self replicate it could potentially have big consequences. I have a dream that we put self-replicating robots on Mars and let them build a mostly by-robots for-robots civilization that can potentially export stuff to earth, do various science projects and build spacecraft. reply adolph 15 hours agoprevIt isn't clear how \"Aloha Unleashed\" is different from \"Mobile ALOHA\" Paper: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation: https://arxiv.org/abs/2401.02117 Video set: https://mobile-aloha.github.io/ Tutorial: https://docs.google.com/document/d/1_3yhWjodSNNYlpxkRCPIlvIA... Kits for sale: https://www.trossenrobotics.com/aloha-kits reply ingend88 15 hours agoparentIs there ready made low-cost arm that is available ? reply adolph 14 hours agorootparentYou have to think of this as an entire system. The arm is necessary but not sufficient. An \"arm\" could be as simple as small servos and popsicle sticks [0]. In the case of ALOHA, below is an outline of the basic components. * arms (aka follower arms) - effector (i.e. gripper) - sensors (i.e. cameras, depth sensors, specced Intel RealSense D405) - gravity compensation (so the relatively delicate servos aren't overloaded) * controller - runs Robot Operating System (ROS [1]) plus other software (i.e. arm, gripper interfaces [2]) - runs ALOHA model in inference to tell ROS what to do based on task and sensor input - trains ALOHA models using arm motion encoder and ACT: Action Chunking with Transformers [4] * leader arms - motion encoders (essentialy an arm in reverse that can be used by a human to telecontrol the arm to encode motions into model training) The system at this point is \"research grade\" which is at once expensive due to custom/nice materials/units and not super user friendly--you must know a lot. See the build instructions [5]. 0. https://github.com/evildmp/BrachioGraph 1. https://www.ros.org/ 2. https://github.com/interbotix 3. https://www.trossenrobotics.com/aloha-kits 4. https://github.com/tonyzhaozh/act 5. https://docs.google.com/document/d/1sgRZmpS7HMcZTPfGy3kAxDrq... reply lyapunova 16 hours agoprevSorry, but this is a lot of marketing for the same thing over and over again. I'm not against Aloha as an _affordable_ platform, but skimping on hardware is kind of a bug not a feature. Moreover it's not even _lowcost_, its BoM is still like 20k and collecting all the data is labor intensive and not cheap. And if we're focusing on the idea, it has existed since the 1950s and they were doing it relatively well then: https://www.youtube.com/watch?v=LcIKaKsf4cM reply xg15 16 hours agoparent> skimping on hardware is kind of a bug not a feature. I have to disagree here. Not for 20k, but if you could really build a robot arm out of basically a desk lamp, some servos and a camera and had some software to control it as precisely as this video claims it does, this would be a complete game changer. We'd probably see an explosion of attempts to automate all kind of everyday household tasks that are infeasible to automate cost-effectively today (folding laundry, cleaning up the room, cooking, etc) Also, every self-respecting maker out there would probably try to build one :) > And if we're focusing on the idea, it has existed since the 1950s and they were doing it relatively well then: I don't quite understand how the video fits here. That's a manually operated robot arm. The point of Aloha is that it's fully controlled by software, right? reply modeless 16 hours agoparentprevThese videos are all autonomous. They didn't have that in the 1950s. reply lyapunova 16 hours agorootparentI can appreciate that, but also they are recording and replaying motor signals from specific teleoperation demonstrations. Something that _was_ possible in the 1950s. You might say that it is challenging to replay demonstrations well on lower-quality hardware. And so there is academic value in trying to make it work on worse hardware, but it would not be my goto solution for real industry problems. E.g. this is not a route I would fund for a startup, for example. reply modeless 16 hours agorootparentThey do not replay recorded motor signals. They use recorded motor signals only to train neural policies, which then run autonomously on the robot and can generalize to new instances of a task (such as the above video generalizing to an adult size sweater when it was only ever trained on child size polo shirts). Obviously some amount of generalization is required to fold a shirt, as no two shirts will ever be in precisely the same configuration after being dropped on a table by a human. Playback of recorded motor signals could never solve this task. reply adolph 15 hours agorootparent> recorded motor signals only to train neural policies Is interesting that they are using \"Leader Arms\" [0] to encode tasks instead of motion capture. Is it just a matter of reduced complexity to get off the ground? I suppose the task of mapping human arm motion to what a robot can do is tough. 0. https://www.trossenrobotics.com/widowx-aloha-set reply klowrey 10 hours agorootparentprevWe're building software for neuromorphic cameras specifically for robotics. If robots could actually understand motion in completely unconstrained situations, then both optimal control and modern ML techniques would easily see uplift in capability (i.e. things work great in simulation, but you can't get good positions and velocities accurately and at high enough rate in the real world). Robots already have fast, accurate motors, but their vision systems are like seeing the world through a strobe light. reply ewjt 16 hours agorootparentprevThis is not preprogrammed replay. Replay would not be able handle even tiny variations in the starting positions of the shirt. reply lyapunova 15 hours agorootparentSo, a couple things here. It is true that replay in the world frame will not handle initial position changes for the shirt. But if the commands are in the frame of the end-effector and the data is object-centric, replay will somewhat generalize.(Please also consider the fact that you are watching the videos that have survived the \"should I upload this?\" filter.) The second thing is that large-scale behavior cloning (which is the technique used here), is essentially replay with a little smoothing. Not bad inherently, but just a fact. My point is that there was an academic contribution made back when the first aloha paper came out and they showed doing BC on low-quality hardware could work, but this is like the 4th paper in a row of sort of the same stuff. Since this is YC, I'll add - As an academic (physics) turned investor, I would like to see more focus on systems engineering and first-principles thinking. Less PR for the sake of PR. I love robotics and really want to see this stuff take off, but for the right reasons. reply modeless 15 hours agorootparent> large-scale behavior cloning (which is the technique used here), is essentially replay with a little smoothing A definition of \"replay\" that involves extensive correction based on perception in the loop is really stretching it. But let me take your argument at face value. This is essentially the same argument that people use to dismiss GPT-4 as \"just\" a stochastic parrot. Two things about this: One, like GPT-4, replay with generalization based on perception can be exceedingly useful by itself, far more so than strict replay, even if the generalization is limited. Two, obviously this doesn't generalize as much as GPT-4. But the reason is that it doesn't have enough training data. With GPT-4 scale training data it would generalize amazingly well and be super useful. Collecting human demonstrations may not get us to GPT-4 scale, but it will be enough to bootstrap a robot useful enough to be deployed in the field. Once there is a commercially successful dextrous robot in the field we will be able to collect orders of magnitude more data, unsupervised data collection should start to work, and robotics will fall to the bitter lesson just as vision, ASR, TTS, translation, and NLP before. reply lyapunova 14 hours agorootparentThank you for your rebuttal. It is good to think about the \"just a stochastic parrot\" thing. In many ways this is true, but it might not be bad. I'm not against replay. I'm just pointing out that I would not start with an _affordable_ 20k robot with fairly undeveloped engineering fundamentals. It's kind of like trying to dig a foundation to your house with a plastic beach shovel. Could you do it? Maybe, if you tried hard enough. Is it the best bet for success? doubtful. reply klowrey 11 hours agorootparentprevThe detail about end-effector frame is pretty critical as doing this BC with joint angles would not be tractable. You can tell there was a big shift from the RL approaches trying to do very generalizing algorithms to more recent works that are heavily focused on this arms/manipulators because end-effector control enables more flashy results. Another limiting factor is that data collection is a big problem: not only will you never be sure you've collected enough data, they're collecting data of a human trying to do this work through a janky teleoperation rig. The behavior they're trying to clone is of a human working poorly, which isn't a great source of data! Furthermore limiting the data collection to (typically) 10Hz means that the scene will always have to be quasi-static, and I'm not sure these huge models will speed up enough to actually understand velocity as a 'sufficient statistic' of the underlying dynamics. Ultimately, it's been frustrating to see so much money dumped into the recent humanoid push using teleop / BC. It's going to hamper the folks actually pursing first-principles thinking. reply modeless 7 hours agorootparentWhat's your preferred approach? reply johntb86 15 hours agorootparentprevWhat do you mean by saying that they're replaying signals from teleoperation demonstrations? Like in https://twitter.com/DannyDriess/status/1780270239185588732, was someone demonstrating how to struggle to fold a shirt, then they put a shirt in the same orientation and had the robot repeat the same motor commands? reply sashank_1509 14 hours agoparentprevI follow this space closely and I never saw the 1950 teleoperation video which literally blows my mind that people had this working in 1950. Now you just need to connect that to a transformer / diffusion and it will be able to perform that task autonomously maybe 80% of the time with 200+ demonstrations and close to 100% of the time with 1000+ demonstrations. Aloha was not new, but it’s still good work because robotics researchers were not focused on this form of data collection. The issue was most people went into the simulation rabbit hole where they had to solve sim-to-real. Others went into the VR handset and hand tracking idea, where you never got super precise manipulations and so any robots trained on that always showed choppy movement. Others including OpenAI decided to go full reinforcement learning foregoing human demonstrations which had some decent results but after 6 months of RL on an arm farm led by Google and Sergey Levine, the results were underwhelming to say the least. So yes it’s not like Aloha invented teleoperation, they demonstrated that using this mode of teleoperation you could collect a lot of data that can train autonomous robot policies easily and beat other methods which I think is a great contribution! reply markisus 6 hours agorootparentI’m not sure you can say that imitation learning has been under-researched in the past. Imitation learning has been tried before alongside RL. But it did not generalize well until the advent of generative diffusion models. reply m3kw9 16 hours agoprevDon’t look impressive because this is what you see a lot in factories anyways, maybe a little better then Sota reply danpalmer 16 hours agoparentThe difference with factories is that every movement is programmed by someone in quite intricate detail. Factory robots aren't \"smart\" in any sense. reply adrr 16 hours agoparentprevFactories/Distribution Centers are doing hard goods not soft goods. reply julienreszka 16 hours agoprevI am skeptical. Lots of fake demos out there. Can we rule out that it turns out it’s actually remotely controlled by some dude in india, just like the amazon’s « Just walk out »? reply InPanthera 15 hours agoparentAmazons was remote controlled? reply sp332 15 hours agorootparenthttps://gizmodo.com/amazon-reportedly-ditches-just-walk-out-... \"Just Walk Out relied on more than 1,000 people in India watching and labeling videos to ensure accurate checkouts.\" reply RobotToaster 14 hours agoprevI assume, being google, none of this is going to be open source? reply krasin 14 hours agoparentIt's already open-source; most of it anyway: 1. https://github.com/tonyzhaozh/aloha 2. https://aloha-2.github.io/ 3. https://github.com/tonyzhaozh/aloha/tree/main/aloha2 reply we_love_idf 13 hours agoparentprevGoogle's days are numbered. OpenAI showed that AI is about delivering AGI, not playing some board games and doing PR stunts. Unfortunately Google hasn't learned its lessons. It's still doing PR stunts and people are falling for it. reply InPanthera 15 hours agoprevWas expecting more from google than a robot that can tie shoe laces, wats the use case for this? Toys for the 1%? reply mikepurvis 15 hours agoparentI don't think the point is lace-tying; it's to demonstrate what is possible in terms of analogous tasks requiring a similar level of dexterity and environmental adaptability. In any case, the real start of this show is clearly the shirt hanging. reply smusamashah 15 hours agoprevThis is very exciting but because its from Google this tech won't get out of their quarters. reply dghughes 16 hours agoprev [–] If I ever move and end up living in a giant concrete warehouse devoid of furniture I'll keep this robot in mind. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tony Z. Zhao launches Aloha Unleashed, demonstrating affordable robots and AI advancing dexterity limits.",
      "The fully independent robots are captured in a single continuous shot, with the videos released several months later."
    ],
    "commentSummary": [
      "Google DeepMind's Aloha Unleashed demonstrates robot dexterity in tasks like shirt hanging and shoe tying through imitation learning, showcasing superior performance compared to other robots.",
      "Discussions explore current robotic limitations, potential industry impacts, and controversies around tech companies, emphasizing the importance of data, AI development, and challenges in mimicking human-like coordination.",
      "Participants express excitement about advanced robot technologies and their potential applications, debating training techniques' effectiveness and the need for more robust data, with some expressing skepticism over accessibility and practicality."
    ],
    "points": 217,
    "commentCount": 94,
    "retryCount": 0,
    "time": 1713284896
  },
  {
    "id": 40057289,
    "title": "Unveiling the Dark Side of Amazon's Flooded Ebook Market",
    "originLink": "https://www.vox.com/culture/24128560/amazon-trash-ebooks-mikkelsen-twins-ai-publishing-academy-scam",
    "originBody": "Share this story Share this on Facebook Share this on Twitter Share this on Reddit Share All sharing options Share All sharing options for: Amazon is filled with garbage ebooks. Here’s how they get made. Reddit Pocket Flipboard Email Constance Grady is a senior correspondent on the Culture team for Vox, where since 2016 she has covered books, publishing, gender, celebrity analysis, and theater. If you’re a millennial, you may remember that specific moment in time around the late 2000s when streaming video technology had just gotten good but there weren’t that many legitimate streaming platforms available yet. So if you were a student without a TV and you wanted to watch a show, you would go to a website that aggregated lists of illegal streams. It would be covered in banner ads and autoplaying video ads decorated with little play button arrows, and in order to watch your show, you would have to solve the puzzle of figuring out which play button to click that would actually get you to your show instead of spiriting you away to a website that sold upsetting porn or amateurish video games. It could be done, but you had to be paying attention, and you had to have the barest modicum of web savvy to do it right. Right now, navigating the ebook and audiobook marketplaces is like being back on those sites. There are a thousand banner ads larded with keywords, and they’re all trying to get your clicks. Take, for example, when tech journalist Kara Swisher’s Burn Book came out this February. A host of other books hit the Kindle store along with it. They all had bizarre, SEO-streamlined titles, like those new businesses that are named Plumbing Near Me to game the Google algorithm. “I found ‘Kara Swisher: Silicon Valley’s Bulldog,’ and ‘Kara Swisher Book: How She Became Silicon Valley’s Most Influential Journalist,’ and ‘Kara Swisher Biography: Unraveling the Life and Legacy,’ by a ‘guy’ who ‘wrote’ four biographies this month,” said Ben Smith when he interviewed Swisher for Semaforum. Swisher was less than flattered by the biographies. “I wrote [Amazon CEO] Andy Jassy and I said, ‘You’re stealing my IP! What is going on?’” she told Smith. (Disclosure: Swisher works for Vox Media.) Here is almost certainly what was going on: “Kara Swisher book” started trending on the Kindle storefront as buzz built up for Swisher’s book. Keyword scrapers that exist for the sole purpose of finding such search terms delivered the phrase “Kara Swisher book” to the so-called biographer, who used a combination of AI and crimes-against-humanity-level cheap ghostwriters to generate a series of books they could plausibly title and sell using her name. The biographer in question was just one in a vast, hidden ecosystem centered on the production and distribution of very cheap, low-quality ebooks about increasingly esoteric subjects. Many of them gleefully share misinformation or repackage basic facts from WikiHow behind a title that’s been search-engine-optimized to hell and back again. Some of them even steal the names of well-established existing authors and masquerade as new releases from those writers. According to the Authors Guild, it would be impossible for anyone but Amazon to quantify these books — and that’s not information Amazon is sharing. All of this means that to buy the book you want — to buy Kara Swisher’s Burn Book instead of Kara Swisher Book: How She Became Silicon Valley’s Most Influential Journalist — you have to know what you’re looking for and pay a modicum of attention to your purchase. Who wants to do that? Especially in a marketplace like Amazon, where we are trained to buy quickly and thoughtlessly with a single click and where writers have been trained to send their wares without even thinking about it because where else are you going to sell an ebook. It’s so difficult for most authors to make a living from their writing that we sometimes lose track of how much money there is to be made from books, if only we could save costs on the laborious, time-consuming process of writing them. The internet, though, has always been a safe harbor for those with plans to innovate that pesky writing part out of the actual book publishing. On the internet, it’s possible to copy text from one platform and paste it into another seamlessly, to share text files, to build vast databases of stolen books. If you wanted to design a place specifically to pirate and sleazily monetize books, it would be hard to do better than the internet as it has long existed. Now, generative AI has made it possible to create cover images, outlines, and even text at the click of a button. If, as they used to say, everyone has a book in them, AI has created a world where tech utopianists dream openly about excising the human part of writing a book — any amount of artistry or craft or even just sheer effort — and replacing it with machine-generated streams of text; as though putting in the labor of writing is a sucker’s game; as though caring whether or not what you’re reading is nonsense is only for elitists. The future is now, and it is filled with trash books that no one bothered to really write and that certainly no one wants to read. The saddest part about it, though, is that the garbage books don’t actually make that much money either. It’s even possible to lose money generating your low-quality ebook to sell on Kindle for $0.99. The way people make money these days is by teaching students the process of making a garbage ebook. It’s grift and garbage all the way down — and the people who ultimately lose out are the readers and writers who love books. None of this is happening through any willful malice, per se, on the part of the platforms that now run publishing and book-selling. It’s happening more because the platforms are set up to incentivize everything to cost as little as possible, even if it’s garbage. In a statement, Amazon spokesperson Ashley Vanicek said, “We aim to provide the best possible shopping, reading, and publishing experience, and we are constantly evaluating developments that impact that experience, which includes the rapid evolution and expansion of generative AI tools.” Yet the garbage books predate the problem of AI. Here’s how they get made in the first place. The scammy underbelly of online self-publishing These days, the trash ebook publishing landscape is fully saturated with grifters. There are blogs that talk about the industry, but they tend to be clickbait sites riddled with SEO keywords and affiliate links back and forth between each other. Virtually every single part of the self-publishing grift world that can be automated or monetized has been automated and monetized. If you look a few years back, however, you find a very different landscape. In the internet of the early 2010s, lots of people were excited about making money with trash ebooks and trash audiobooks, but they weren’t yet all trying to scam one another on top of their eventual readers. They wrote real blogs about the process of the grift with their real human voices. They distinguished between schemes that were “white hat” (following the rules) and “black hat” (violating some terms of service). They compared strategies and teachers of strategies. According to the blogs of the era, one of the most infamous teachers was a man who went by Luca de Stefani, or Big Luca. Legend had it he held the world record for making the most money using Kindle Publishing in a single day. What set Big Luca’s Self Publishing Revolution course apart from the rest — Big Luca’s “black hat breadwinner video lesson,” per one review — was that he gave his students access to a secret Facebook group where self-publishers organized review swaps and buys. For the self-publishing grift, good reviews are crucial. The more five-star reviews a book has, the more likely Amazon’s algorithm is to push it toward readers. If you’re mostly publishing trash books, you’re not going to get tons of five-star reviews organically. Big Luca’s Facebook group gave grifters a place to offer to swap five-star reviews or sell five-star reviews for $0.99 a pop. As far as Amazon’s algorithm was concerned, there was no difference between that kind of review and the one a real reader might leave. The results were extremely lucrative. Luca didn’t invent this formula. He learned it from the OG self-publishing grift course K Money Mastery, now apparently defunct, where he excelled. Eventually, Big Luca had wrung all the money he could from the self-publishing hustle. He climbed up to the next level of the pyramid and became a teacher, and in 2016, the lore goes, a man named Christian Mikkelsen enrolled in Big Luca’s Self Publishing Revolution. The Mikkelsen twins are named Christian and Rasmus, and they are 28 years old. They have dark blond hair and blue eyes and meticulously groomed facial hair, and they always seem to be posting to Instagram from luxurious private pools that are also somehow exotic beaches. They have managed, in what must be fairly acknowledged as a feat of branding wizardry, to hold on to the domain Publishing.com, and there they peddle their wares: a course they say can help students make a lifetime of easy cash off the revenue from books they don’t even have to bother to write themselves. If they happen to land a student who wants to write a book in good faith and just doesn’t understand how to sell their book on their own, well, they’re happy to take money from that student, too. According to a profile in Inc., Christian found his way into the self-publishing world by googling “how to make money online.” His first book was a brief ebook titled How to Be a 4.0 Student in College, Like Me. Big Luca’s method apparently served him well enough that he thought it would be worthwhile to bring his twin Rasmus into the fold. Together, the Mikkelsens published trash book after trash book, guides to keto and sex and crystals. Then they started running their manuscripts through Google Translate to start selling foreign language editions, an innovation on the old grift that bumped their income into six figures and, after a few months, got Amazon to block their publishing account. The time had come, as Inc. would put it, to “do what entrepreneurs do”: pivot. They started a YouTube channel so they could teach the business of self-publishing to anyone else who wanted to learn. Six months and 1,000 subscribers later, they launched their first paywalled online course. First it was called Audiobook Impact Academy. Then it was Publishing Life. Now, with AI an ever-more-fashionable buzzword, it’s AI Publishing Academy. It’s always more or less the same method, with a few new tweaks in every new iteration. But the method doesn’t change as much as you might think. How the garbage books get made: A case study If you want to take the Mikkelsen course, the first thing you do is sit through their sales pitch. It runs for two hours, and it’s just a video of Christian sitting in a dark room, drinking thirstily from a water bottle as he shows you screencaps of his students’ royalty checks and repeats that he is already rich; he doesn’t have to show you how to make this kind of money. He’s doing it for fun. Christian’s offer is, he says, unbeatable. He will show you how to produce a book without having to write it. “What used to be the hardest part in the process — which was creating that book that you upload onto Amazon — is now the easiest part, and the most fun,” Christian explains. “Because AI can help do that for you.” Specifically, AI will write your outline for you. The twins feel that the quality isn’t there on purely AI-generated books yet; they demand better for their readers than AI prose. Still, they say using AI to outline saves their students weeks of researching their own manuscripts. That AI is part of but not central to the process is a helpful talking point for the Mikkelsens as Amazon strengthens its regulations against purely AI-generated text for sale. “Last year we began requiring all publishers using our Kindle Direct Publishing service to provide information about whether their content is AI-generated and further reduced the total number of titles that can be published in a day,” said Amazon spokesperson Vanicek. Vanicek added that they have “a robust set of methods” to detect content that violates their guidelines, and they regularly remove those books and sometimes suspend the publishing accounts of repeat offenders. “The thought of human creativity being overshadowed by robots isn’t exactly the prettiest picture,” Christian wrote in a (suspiciously ChatGPT-sounding) blog post in March. “But we’re here to set the record straight. There are two camps of AI use out there: AI-generated and AI-assisted. They are completely different.” In April, however, the Mikkelsens announced that they were preparing to launch a new proprietary AI program, Publishing.ai, that they promise will write a manuscript for you, “Soooo much faster than a ghostwriter!” Under the Mikkelsen model, you also don’t have to pick your own topic. They give you access to keyword scrapers that have pulled trending topics off Kindle and Audible. And once AI is finished with your outline, you can send it over to a ghostwriter to turn into a book for a mere $500. For a 30,000-word book, that works out to a fee of $0.016667 per word. (The Mikkelsens work with a ghostwriting company developed by two former students. There, ghostwriters get hired on a freelance basis and are kept anonymous.) Once you have your manuscript, Christian promises, the twins will show you how to hire audiobook narrators for a flat $20 fee by haggling their prices down. They’ll introduce you to a network of people who are generous with their five-star ratings and will push your book up the algorithmic Amazon rankings for you. All you have to do is sit back and collect your royalty checks as you rake in month after month of passive income. As Christian talks, he seems to cast a kind of hypnotic spell. You start to wonder: “Am I stupid for not having invested in ebook publishing before now?” You see five-figure check after five-figure check. A ticking clock turns on. Christian wants to give you a special rate for the course. The course is worth $15,500. The regular price is $6,000. But he’s willing to give it to you for just $1,995 as long as you’re one of the first people to sign up after the webinar ends! None of what the Mikkelsens are describing here is illegal, but if you know the norms of publishing, you know it’s unethical. As documentarian Dan Olson lays out in a lengthy 2022 deep dive into the Mikkelsens, their method of publishing means developing a book you’re probably not qualified to write. It used to mean paying a ghostwriter starvation-level wages to churn out a manuscript; now it means creating a book likely riddled with misinformation and minimal means of correcting it. It means deceiving readers with fake reviews. (That one can get you kicked off Amazon if you get caught.) If you aren’t versed in publishing’s industry standards, however, the Mikkelsen model can seem incredibly attractive. “I was like, ‘Yup, this would be great.’ I was totally vibing with it,” recalls Jennifer (whose name has been changed to protect her privacy), a 37-year-old in Virginia, who signed up for the twins’ course in September. She had recently written and self-published her own book on Amazon, but she wasn’t sure what to do to start promoting it. The twins’ sales pitch struck her as the perfect solution: She could provide her ideas, people who were good with words could rewrite her draft and polish the whole thing up into a sales-ready package, and everyone would get paid. “I was like, ‘Okay, I’m ready to invest and make this a better thing.” “It was a very efficient, friendly, interactive webinar,” recalls Cecilia (also not her real name), a Seattle-based 50-year-old who works in the health space. She signed up for the twins’ seminar in September, and she remembers being pleasantly won over by that two-hour sales pitch. It seemed to be so transparent. “It promoted trustworthiness,” she says. Once Jennifer and Cecilia had turned over their money for the course, both of them changed their minds fast. “I started listening to the modules,” Jennifer says. “And I don’t know how else to describe them” — there is a long, expressive pause — “but as a jet stream of bullshit.” “All it does is tell you to buy different products,” Jennifer recalls of the Mikkelsen course. “It’s like there’s nothing that they actually provide themselves, other than ‘Go buy other people’s products’ — and I’m sure they own those too, right?” (We can’t confirm that the Mikkelsens do own the products they recommend, but the ghostwriting company they work with is run by former Mikkelsen students.) The course cost $2,000, but that wasn’t enough. To get a Mikkelsen-approved topic for your book, you had to pay for access to the software that analyzed Amazon keywords to tell you what was trending. To create the perfect outline, you had to pay for access to the AI that outlined and drafted your book for you. You had to pay for the cover design and for the reviewers. The $2,000 fee was supposed to guarantee you frequent one-on-one calls with a publishing coach, but the coaching calls were mostly about upselling to the premium $7,800 course, Jennifer says. After a few weeks of phone calls, Cecilia decided she had had enough of the program and asked for her money back. She didn’t expect it would be a problem, since the course was advertised as fully refundable. Customer service reps then told her that, to qualify for a refund, she had to prove she’d sat through the whole course, published a book, and failed to make her money back. She says she had to threaten to call the attorney general before they sent her money back without such proof. Jennifer wrote off her $2,000 as the cost of learning a bad lesson, but she wanted to warn other people against making the same mistake. She posted a negative review of Publishing.com on the user review site TrustPilot, where it has a 4.7 rating. So did Cecilia. Both of them found their reviews queried by TrustPilot, which required them to submit proof of going through the course in order to keep their reviews up. Most of the course’s five-star reviews, however, remain unverified. I reached out to some of the five-star TrustPilot reviewers to get their take on the Mikkelsens’ course, but I never heard back from any of them. After the Mikkelsens’ course got big, the rumors on Reddit say Big Luca was furious that they’d ripped off his business model. He used to brag that he was going to drag them into court. Instead, he got out of the self-publishing game. Now he runs a program called Big Luca International or, more informally, School for the Rich, self-described as “the world’s leading company in online marketing training.” It’s supposed to teach you how to monetize any online business — the self-publishing black hat breadwinning tricks, extrapolated out to all the other industries of the internet. With the advent of AI, it’s easier than ever to flood the whole digital ecosystem with trash in pursuit of passive income. The neverending grift “It sucks that I did this,” Jennifer says of her experience at AI Publishing Academy. “But I mean, it’s put some fire under my butt to do it [marketing] myself for my own books.” Jennifer didn’t set out to make a quick buck with a garbage ebook. She did the work of writing a book because she believed in it. The Mikkelsens got her because she couldn’t figure out how to sell her book on her own, and part of the reason she couldn’t sell it is because the marketplace is already so flooded with books. Many of which are garbage books. The Mikkelsens are not the chief villains of this story. They are small-time operators working one level of a very big grift industry. The grift is that technology and retail platforms have incentivized a race to the bottom when it comes to selling books. Together, without ever caring enough about the issue to deliberately try to do so, they have built a landscape in which it’s hard to trust what you read and hard to sell what you write. The incentives of the modern book-selling economy for writers are to keep your costs low, low, low and your volume high, high, high, and definitely put your book on Amazon because where else are you going to sell an ebook? The incentive of the modern book-buying economy for readers is to go onto Amazon and lazily click around with a few search terms, and then buy the first book that looks right with the click of a single button. The incentives are, in other words, driving us all straight into a flood of garbage. That’s what the grift does. It finds every spot in the process of making and selling a book that is inconvenient or laborious, and it exploits those spots. It exploits our cultural belief that books are meaningful, that writing a book is a valuable act, that reading a book will enrich your life. When it’s finished, you’re left with something that’s not a real book but a book-shaped digital file filled with nothing of any use to anyone at all. Will you support Vox today? We believe that everyone deserves to understand the world that they live in. That kind of knowledge helps create better citizens, neighbors, friends, parents, and stewards of this planet. Producing deeply researched, explanatory journalism takes resources. You can support this mission by making a financial gift to Vox today. Will you join us? One-Time Monthly Annual $5/month $10/month $25/month $50/month Other $ Yes, I'll give $5/month Yes, I'll give $5/month We accept credit card, Apple Pay, and Google Pay. You can also contribute via",
    "commentLink": "https://news.ycombinator.com/item?id=40057289",
    "commentBody": "Amazon is filled with garbage e-books, this is how they get made (vox.com)203 points by crescit_eundo 12 hours agohidepastfavorite108 comments CSMastermind 6 hours ago> Amazon is filled with garbage Full stop, no need to specify e-books. I've moved my purchasing dollars elsewhere at this point with the exception of some very specific categories. Amazon is just too unreliable. reply faluzure 2 hours agoparentEvery once in a while I try to become an Amazon seller through FBA. I sell quality goods without a huge markup. My latest attempt resulted in 1/3 of the goods being returned because they could be sourced cheaper elsewhere (eg AliExpress) after they were tested, or they were swapped by the buyer with defective goods purchase elsewhere returned (they had heavy wear). In the case where the goods were still functional, the packaging was damaged and I had to pay for the returns. My conclusion is that amazons liberal return policies only allow something to be sold profitably at a 2-3x markup. Hence everything being garbage. I won’t attempt it again. reply devoutsalsa 2 hours agorootparentAs a consumer I love liberal return policies, as I can buy something I’m not sure about to try it out. When I was in Colombia, but avoided buying some AirPods because I couldn’t try them on, and once purchased I could not return them. When I buy shoes online , I’ll buy maybe 7 pairs, find the best fit, and return rest. As a vendor, I can’t imagine dealing with a high percentage of returns. I don’t know what the solution is. reply myaccount80 2 hours agorootparenthow do you feel about the resource waste of such return policies? Shipping all these items to then return most of them is an insane amount of waste. And sometimes returned items cannot be resold so they are thrown away. Imo there should be clear limits on returns reply devoutsalsa 56 minutes agorootparentI'd prefer not to. In my case, it's nearly impossible to find shoes that fit. My size is not available in stores. reply mattmanser 2 hours agorootparentprevThe solution is pricing it in. Like stores that sold physical goods had to price in things like staff or shoplifting or business rates. reply ethbr1 6 hours agoparentprevAgreed. I let my Prime subscription expire a few days ago. Amazon's thrown in the towel on streaming and decided it'd rather be an ad-supported model with Freevee (which also eventually inherits Prime-exclusive shows). For the rare instances I do want to order something physical from Amazon, waiting isn't a problem. For the rest of the time, I'd rather my dollars go anywhere else. When someone like me looks at Walmart and says \"There's a more ethical alternative\", Amazon might have a problem. But I assume Amazon retail leadership is self aware they're milking the long tail of historical brand cachet while under-investing in their future, at this point. AWS and maybe some other businesses spin off, adios to the remainder as Sears 2.0. reply BLKNSLVR 4 hours agorootparentThe thing I always underestimate, in regards to bad / anti-consumer behaviour, is the size of the customer base (and sometimes in combination with the bell curve / normal distribution of intelligence, but maybe not in this case). The scope of humanity that use Amazon is gigantic, and the percentage of us pedantic, technically knowledgeable folks who'd give up some convenience, time, or even the purchase of an item, as part of a \"righteous crusade\" is vanishingly small. Amazon may even increase their sales as consumer X purchases a replacement \"thing 2\" because \"thing 1\" was dead on arrival, and money these days ain't worth the hassle of returning or getting a refund on \"thing 1\". reply kredd 4 hours agorootparentIt works until people look at things like Temu, and figure out “why should I get from Amazon, when I can buy something for the same quality”. It’s just a battle of giants. reply sph 3 hours agorootparentWho? There's your problem. I have never heard of Temu, and neither have many who know (and use) Amazon. reply kredd 1 minute agorootparentIf you believe their stats, they have 70M+ active users in US, so I’m assuming you’re just not their target customer. amanaplanacanal 3 hours agorootparentprevThat’s only a matter of time and advertising though. reply Animats 5 hours agoparentprevRight. After I got a recall notice on some Amazon vitamins because they were counterfeit, I stopped ordering much of anything from Amazon. A competing \"direct from the manufacturer\" sales site would be useful. Only the business that actually makes the thing could sell it. No intermediaries. reply enobrev 5 hours agorootparentI canceled prime earlier this year when they announced ads in prime video, which I rarely used but seemed like a red flag. I'd lost trust in them for consumable products long ago. I've saved a good deal of money, and waiting a couple extra days for delivery has only been a minor inconvenience . I had the same thought a couple weeks ago regarding direct-from-manufactuerer product search / purchase. I'd use it regularly. reply jillesvangurp 3 hours agorootparentI did the same thing. But for different reasons. I cancelled both Netflix and Amazon Prime last year and decided to do account hopping on a month by month basis. Their greed is what triggered this. Both have gone down a path of less and less interesting content, price increases, and now indeed this ad driven bullshit. I realized I'm paying 20 euros per month for both and struggling to find something worth watching for months. So, I cancelled both and went over to Apple TV. They don't have a lot but what they have is quite nice. Once I'm done with that, I'll consider what to subscribe to next. Criteria here: whatever I pick has to have stuff I want to watch (ad free, obviously) and I'll unsubscribe the second I get bored. I'm done paying for multiple streaming services at the same time. I only want the good stuff. I'm not interested in the generic filler content, B movies, etc. that are cheap to license but horrible to watch. I might come back to Netflix or Prime periodically to check certain shows. But not on a permanent basis. If more people do this, that gives them a nice price signal that they need to invest in quality. reply Beldin 3 hours agoparentprevNevertheless, spam ebooks do deserve a separate category. There is an actual industry around spam ebooks that offers aspiring fraudsters the tools to quickly cobble together junk with enough stolen content that readers are getting their $0.99 worth. (That latter part helps raise the bar against refund requests.) Moreover, this industry has developed tools and mechanisms to SEO and spam your fraudulent book. Why launch it as ine book, instead of 20 by 20 different authors, with slight variations on the title? Some of these tricks may be used for physical products as well, but at least they need to deliver a physical product. In the case of spam ebooks, this pipeline takes care of everything. You click around for a few hours, launch the software: bam, 20 junk books with just enough content to not warrant refunds and SEO'd titles to get at least a few dozen sales. I looked into this well over a decade ago; back then, you could buy a set of DVDs that would teach (and, perhaps, assist) you in perpetrating this. I get the appeal: back then a popular category was tax tip books. Scrape some tips from here and there, slap on a dozen titles and author names, and sell for low enough that people don't mind taking a chance on your crap. Pity Amazon still hasn't made any successful steps against this. reply SXX 3 hours agorootparentDont you think problem with spam books is that someone actually buys them? Without enough demand no one would do it. reply jajko 2 hours agorootparentI don't get it, who buys them? My time is so precious (raising 2 small kids now) that I do quite a bit of research before committing so much of my time to it. There are literally millions of books out there. Even without kids, life is just so wonderful and intense to actually experience, stuff I pick up for just sitting and reading better have amazing reviews by many thousands. Yes, I won't support much some new starting author this way (but then if he wins say Hugo it gets on my list) but my time and well being is simply higher priority for me. reply SXX 1 hour agorootparentMost likely same people who buy random trash because it's cheap and \"FREE DELIVERY WITH PRIME\". People really need to be educated not to spend money on trash. reply Beldin 2 hours agorootparentprevIf you can get a copy of \"100 money-saving tax tips\" for $1.99, and the preview already sounds promising, wouldn't you? Even one good tip would easily save you the price of the book. (Also: to me, blaming demand is a bit victim blaming - these scams are deliberately set up to entice as many readers as possible.) reply SXX 1 hour agorootparentI mean if a person buys one such book for $1.99, not refunds it and afterwards he continue to buy same low-quality books then might be it wasn't such a bad deal for them? Might be they actually think they extracted enough value from it? How it's different than any other low-quality literature which was always in abundance on the market? Except before copywriting cost $500 and now generated one cost $20 to produce. Who should be ultimate censor of what books must be released an what books shouldn't? PS: Same way some people love to buy cheap trash from AliExpress for $1.99 and at least generated books dont create actual e-waste, CO2 footprint and other pollution. And Amazon is now full with trash from AliExpress just 2x more expensive. reply birracerveza 29 minutes agoparentprevThe problem is that unless you buy Brand stuff, you're just paying double the price for being dropshitted Aliexpress garbage. Yes, that was a typo but I'm leaving it. reply ungreased0675 11 hours agoprevThe focus of the article is on charlatans selling courses, but I think Amazon deserves more scrutiny. They enable this entire ecosystem of fake books and fake products. I have to imagine that with the amount of data Amazon collects, they could do a lot more to counter the epidemic of fakes. reply janice1999 10 hours agoparentAmazon's 'solution' has been to limit self published authors to 3 new books a day. What a joke. https://www.theguardian.com/books/2023/sep/20/amazon-restric... reply bambax 1 hour agorootparentYes, that number is absurd. A normal person can maybe write a book a year, max. Stephen King can write two, but there's only one of him. Maybe simple self-help books can be written a little faster, because they are shorter. A maximum set in words -- instead of books -- might work better; say 100-150k words per year? But would that be enough? Wouldn't grifters create a multitude of accounts to publish their books? Maybe the solution would be some kind of automated system to evaluate the quality of a book? It needn't be fine-grained, it would simply output \"trash/non trash\" and could try to test whether the book contains any new information not available elsewhere, or whether there are entire paragraphs taken verbatim from wikihow. But AI-generated books may be hard to spot. Maybe there's no solution. reply beauzero 6 hours agoparentprevUgh Whisper. On Audible I used to browse the free sci fi. Found James S. A. Corey, etc. there and ended up spending a lot...and I mean a lot of money buying authors series that I had never heard of. Now all the Whisper read...stuff crowds everything out. I don't even bother anymore and look elsewhere for books, use podcasts, listen to long youtube lectures while on the tractor, doing fence, clearing land on the weekends and during my commute. reply insane_dreamer 9 hours agoparentprev> they could do a lot more to counter the epidemic of fakes they are not incentivized to do so, since they profit from the fakes; only if/when this results in customers disengaging from Amazon altogether and total sales decreasing, will it be seen as a problem reply IG_Semmelweiss 6 hours agorootparentMBAs will never understand intrinsically that $1 in extra profit from a ripped-off customer is dangerous profit if that churn it increases your future CAC by more than >$1 reply morkalork 11 hours agoparentprevHow about this account for a \"polymath\" who manages to churn out a \"book\" almost every week on topics from AI to installing cabinets and a whole bunch of pseudo science woo woo: https://www.amazon.ca/stores/D.R.-T-Stephens/author/B0CC62LX... They buy a lot of ads so I guess Amazon likes them. reply because_789 8 hours agorootparentI wouldn’t wish a forced-read of that guy’s bio on my worst enemy. (But I did just send it to all my close friends.) reply cscurmudgeon 6 hours agorootparentprevLol, even the author's image is AI generated. The text screams ChatGPT. Amazon has given up on AI. reply pquki4 9 hours agoparentprevAmazon is not incentivized to do so for as long as they maintain a monopoly in the book market, just like they are not putting enough effort in removing knock off products. reply csours 11 hours agoprevhttps://www.youtube.com/watch?v=biYciU1uiUw Linked doco by Dan Olson. Strong recommend. reply mey 10 hours agoparentHis other long form videos are also a great combination of research and comedy. Worth your time in my opinion. reply well_actulily 10 hours agorootparentHighlights include: - \"In Search of a Flat Earth\" (2020): https://www.youtube.com/watch?v=JTfhYyTuT44 - \"Line Goes Up\" (2022): https://www.youtube.com/watch?v=YQ_xWvX1n9g - \"The Future is a Dead Mall\" (2023): https://www.youtube.com/watch?v=EiZhdpLXZ8Q reply kiernanmcgowan 9 hours agorootparentThis is Financial Advice (2023) is also excellent - https://www.youtube.com/watch?v=5pYeoZaoWrA Its about the fallout of the memestock craze and where the community ends up. reply jcranmer 5 hours agorootparentSo I thought I had known most of that material, but I had somehow missed the entire thing about the mother-of-all-short-squeezes being a global financial apocalypse that brings down the entire system... Which actually raises the interesting question: just how can you even entertain a belief that you'd somehow leverage billion-dollar profits in such a situation? I mean, I know it's completely divorced from reality. I can understand the appeal of the conspiracy theory, I can understand how they think they've found the one loophole that makes infinite money, I can understand the proof-by-contortionist-decoding-of-secret-messages. But not how you can even seriously think, in a world where you found the thread that will let you pull one over the big guys in a serious way, that the government--the people whom you believe to be colluding with the Big Them™ to screw you over--will willingly be held hostage by you in the grand climax and let you extort great gains rather than changing the rules of the game (which is literally their job). I mean, in standard apocalyptic literature, it's the role of God--a higher power not subject to the observable rules of the game--who plays the role of wreaking the final vengeance and rewarding the pious faithful; but here, in this theory, it's the bad guys who are supposed to wreak vengeance against themselves and reward their enemies for being faithful to the higher script. The other thing is I think the video is still a bit premature; it was written soon after the bankruptcy of Bed, Bath & Beyond, so it doesn't have much to go on for how the movement evolved afterwards, and I would have liked to learn more about that. reply john-tells-all 6 hours agorootparentprev`Line Goes Up` was jaw-droppingly awesome, about the Venn diagram of crypto and scammers being (maybe) a circle :) reply ansible 11 minutes agorootparentAnother great resource for crypto news is Molly White's website: https://www.web3isgoinggreat.com/ What's amazing to me is reading about what the crypto thieves and smart contract exploiters do with the funds they've taken. It seems common for a person who's managed to acquire $100K USD worth of ETH or Solana to then turn around and buy some NFTs with it. I'm like: \"Dude, you just had a great payday, now you get to ride off into the sunset with your ill-gotten gains.\" But either they are crazy, the world is crazy, or I'm going crazy. I don't understand it all. reply DavidPiper 8 hours agoparentprevYes! Came here to recommend this video as well. Dan Olson (Folding Ideas) has some of the best long-form content on YouTube, period. reply blackhaj7 11 hours agoparentprevDamn this is so good reply all2 8 hours agorootparentI watched the whole thing. I was bummed that he didn't do the whole course; have the audio book made, sell it, profit. reply paulette449 8 hours agorootparentprev\"Dollar General Winklevi\" Amazing reply jamesdhutton 45 minutes agoprevI want to be a bit contrarian here. I buy books on my Kindle all the time. I always download a sample before buying. I do recall a couple of occasions where I got a sample that was one of the garbage books that this article is talking about. So yes, I've experienced it. But I don't experience it often. Nearly all the time, a search for an author or a topic brings up real, relevant books. Nearly all the time, Amazon's recommendations are for real, relevant books. So based on my own experience, I'd question how big the problem really is for the end consumer of books. reply webwielder2 11 hours agoprevStick to checking out books from your library / Libby. You'll save money and might end up making discoveries you otherwise wouldn't. reply cheese_van 9 hours agoparentLibraries are excellent gatekeepers for content quality. Generally, library books are scrutinized and determined as \"worth paying for\". Previously, publishers would do similar due diligence - someone at the house would actually read the book, if not work with authors. Due diligence of this nature is not now particularly common. There is no cost risk with a digital book. It could be, and often is, written by those without the talent to match their ambition. If one is looking for new authors to read, it's very difficult to sort through the dreck. Let the libraries sort through the dreck for you. Pro-tip: gift your local library a mere $100. They'll love you for it and invite you to all sorts of interesting gatherings. You might meet a good book, or even better, a good person. reply grugagag 9 hours agorootparentIt’s true that nooks could be written by novice writers but even worse they could be entirely generated with LLMs, never even read end to end by the presumed authors. Sometimes there’s no human in the loop, just some scripts running. reply nytesky 9 hours agorootparentprevIf you look for a reputable publisher like McSweeney or Scholastic, does that help? reply pquki4 9 hours agoparentprevIn principle yes, but it is often far less than ideal. Popular books often have only 2 copies, physical or digital, so you need to wait in the queue for a while before you can get to read it. And that's only if your library carries the book, physical or digital -- there is often a long delay before you can find book in a bookstore and you can find it on the shelve. For various reasons the library might not want to purchase a book so you are on your own. reply beezlebroxxxxxx 8 hours agorootparentIf you want to purchase a book on Amazon, then your best bet is to just only look for books sold and published by a reputable or at least known publisher. This is especially true for a lot of older books, which often have great editions in the midst of awful \"print on demand\" versions or new typesettings of old public domain version with awful quality and editing. (Then, even further, you might as well just cut out Amazon as the middleman if you're so inclined, or look for a local book store that has online purchases.) reply Turing_Machine 5 hours agorootparent\"Reputable publishers\" put out garbage books all the time. Remember \"Snooki\"? Four books for sale on Amazon, all published by a \"reputable publisher\" (in fact, a division of Simon and Schuster). Of the current top 20 best sellers list: https://www.amazon.com/best-sellers-books-Amazon/zgbs/books/... I'd rate at least 15 as \"garbage books\" on sight. \"Reputable publishers\" are in business to make money. Some of them may put on airs of intellectual quality, but in fact if trash books make a buck, they will publish them. If high-quality books are unlikely to make a profit, they will not. reply sussmannbaka 2 hours agorootparentprevIn the former case, congratulations, you now know which book to buy that’s likely not an AI scam. reply cogman10 7 hours agorootparentprevIf a book is popular, then you don't really have a problem with it being a fake or garbage right? The problem with amazon books is all the garbage books, but if you know you want \"the wheel of time\" then it's pretty easy to track that down on amazon or elsewhere. The only problem popularity presents is if it's somewhat underground. Then you might not stumble on it. Otherwise, just perusing the library should give you a lot of books which are reputable. reply MrVandemar 6 hours agorootparentprev> Popular books often have only 2 copies, physical or digital, so you need to wait in the queue for a while before you can get to read it. Wait in the queue? What's fundamentally wrong with that? If something's worth reading, it's worth waiting for. Do we really have to have everything right now? reply Barrin92 5 hours agorootparent>What's fundamentally wrong with that? Nothing of course, patience is a lost virtue and relevant to the topic of the thread, it's no accident that the only thing Amazon has gotten better at is shipping garbage faster. In fact when it comes to the current popular thing, just go for the unpopular thing that nobody else is interested in because chances are there's something more interesting in it anyway. There's also this community on reddit of people who on principle wait a few years before they play games. Time is literally the best filter. Anything that's popular and good will still be interesting in three years. reply AlbertCory 6 hours agoparentprevIn doing my research on current books, I checked out a few from the library that were really good and definitely HN-worthy: Bill and Dave (Michael Malone). The history of Hewlett-Packard. Legends & Lattes: A Novel of High Fantasy and Low Stakes (Travis Baldree) Hitch-22 (Christopher Hitchens). his memoir reply whartung 11 hours agoparentprevAt my library you can check out sewing machines and power tools as well. And CA libraries have access to courses on Coursera. reply jtotheh 11 hours agorootparentMy local library gives access to O'Reilly Safari. You have to click around a bit more than if you had a full price account but I think it's a great resource. reply al_borland 10 hours agorootparentMine gives access to the Wall Street Journal. As well as some other papers. It only lasts 3 days, but clicking the link again gets another 3 days. reply johngossman 7 hours agoparentprevI love Libby. But buying from Amazon isn’t that hard. Check the sample first, look at the publisher. Worst comes to worst, Amazon refunds you if you return an eBook. I don’t know if there is any time limit, but I have returned several books within the first day or two for full credit. reply nl 6 hours agoprevThe problem here isn't the garbage ebooks (there will always be garbage). It's the terrible Amazon shopping experience which puts no value on the customer getting a good product. EBay (!) is much better. reply herbst 3 hours agoparentAliExpress has by far better customer support und money back solutions and even better shipping times in many cases if you don't live in a official Amazon country. Even wish refunds faster and less complicated in case of no delivery. reply SXX 1 hour agoparentprevThis. Subpar products will always be on market. Problem is that Amazon making a lot of money selling trash and everyone okay with this. reply outofpaper 10 hours agoprevThis Vox article feels almost as clickbate and turn key as its subject material. It's filled with links upon links, not to sources but to other vox articles and vox search results. It's like reading in a library only to realize, as you go for another book, that you are in a carnival mirror maze! reply layman51 10 hours agoparentAgreed. I was surprised to see a link in that article to a long, ranty Reddit post when they refer to what “the rumors on Reddit say.” reply protocolture 7 hours agoprevThe biggest issue is how Amazon fails to police its genre categorisations. The advice from the guys making garbage books, which they quite obviously put into practice, is to apply for genres with low book counts to more easily get a best seller badge for that category. Which makes Amazon a war to get miscategorised. reply whyenot 7 hours agoparentI'd argue an even bigger issue is the way Amazon clogs up its search and product listings with sponsored results. Amazon isn't interested in selling you what you are looking for. They want you to buy some sponsored junk instead. This mindset permeates the site and IMO is the source of many current problems. reply johngossman 6 hours agorootparentThis is my biggest complaint. I look for CS and history books and get ads for romances. At least I figured that out. It turns out romance books make more money than any other category. Apparently, Amazon's ad inventory is mostly romance. reply bambax 1 hour agorootparentprevSponsored results are marked as such and therefore are filtered out by most adblockers. This is not a perfect solution, as grifters also game reviews, but it helps. reply tamimio 9 hours agoprevWhat other good alternatives to amazon for publishing books? Preferably not filled with garbage ones. reply silisili 10 hours agoprevNot just e-books, garbage print too! Link below. The kid likes brain teasers, so I'll grab a book here and there. The last one I ordered was seemingly AI generated. The wording was off, the answers were either completely incorrect, or theoretically correct but not matching the question or parameters asked. I can't quite remember the last time I became as infuriated at a...book. https://www.amazon.com/gp/aw/d/B0CQQPPQS4 reply bambax 32 minutes agoparentThe image cover is visibly AI-generated. I find AI images are easier to spot than AI text. reply magnetowasright 7 hours agoparentprevOne of the reviews says to look at the sample, but it looks like the sample isn't available any more. Might just be a regional thing that the sample isn't available but maybe they removed it intentionally lol reply plorkyeran 7 hours agorootparentThe first puzzle from the sample: > A seller has some quantity of floor with him. The seller offered his customer that if he/she buys half of the floor he has, he will give half kg of floor as a discount. The first customer accepted his offer and he purchased half of the floor and got half kg as extra. After selling the floor to the first customer he again makes the same offer for the second customer, and so on. The seller left with no quantity of floor after he made the fifth transaction. The initial quantity of floor the seller had? It's a solvable math puzzle, but has some obvious issues. One of the later puzzles is pretty funny: > There are three people (Deepak, Avinash, Prateek ), one is a robot, one is a missile, one of them is a Gun, the robot can either lie or tell the trust, and the Gun always tells the truth, the missile always lies, and the robot can do both he can either lie or he can tell the truth. > Statements : > Deepak: Prateek is a missile.\" > Avinash: \"Deepak is a Gun.\" > prateek : \"I am the robot.\" > Find out the Gun, the missile, and the robot? The typos and weird formatting from the original. The final puzzle in the sample is just sort of an incoherent mess: > You have twenty white and thirteen black balls in a bag. You pull out two balls one after another. If the balls are of identical color, you then definitely replace them with a white ball - however, if they're of various colors, you update them with a black ball. Once you are taking out the balls, you do now no longer place them returned back in the bag - so the balls keep reducing. What will be the color of the final ball remaining in the bag? reply forgotusername6 3 hours agorootparentThe mangled puzzle: replace identical pairs with single white balls. Replace non-identical pairs with black balls. Number of balls reduces by one after each action resulting in a single ball at the end. I'm not certain of the analogy, but it appears to be something like the white balls are even and the black are odd. Both odd+odd and even+even equal an even (white) number. But odd + even equal an odd number. Since it starts with an odd number of odd numbers, the answer is black. reply owlninja 7 hours agoparentprevThis seems like the more lucrative route. As an avid reader I do a bit of research before deciding on a book. I suppose someone had to read it first but a lot of good books seem to bubble up to the top. I would never dig 5 pages deep into Amazon and buy a book with zero reviews based on...what? After raising small kids however, the classics are fun but buying some easy time-filling books or activity books doesn't require as much thought. I do hope that AI can also be used to combat books like the one you linked. Straight garbage looking for a sucker. reply treflop 5 hours agoparentprevThe cover looks pretty AI-generated to me too. No object on the cover makes any sense. reply precompute 3 hours agoprevFanfiction sites filled with \"garbage\" are arguably worth more than Amazon's online free-for-all (now with AI!). You know fanfiction sites are great when they train models on the millions of human-written, well-tagged fics reply Dibby053 8 hours agoprevThe web has been filled with garbage books since its inception. Now it's Amazon, before that it was SEO-optimized e-book landing pages that showed up on Google searches for barely related topics. As long as there are people who buy them they'll keep popping up. reply neilv 3 hours agoprevEven when Bezos was at the helm, Amazon had tons of trash and nonsense in its marketplace... But it'd be epic if Bezos got all pissed off about things like these books -- and various other declines of his baby -- so he forced his way back, to smack it into better shape. https://en.wikipedia.org/wiki/Cleansing_of_the_Temple (Why I'd guess there's a nonzero chance of this happening is that, externally, it sounded like Bezos cared strongly about certain ideas, and implemented them forcefully, yet it seems like lately those ideas are being disregarded. If there's any truth to that external impression, then there could be a reckoning. Billions of dollars buys some latitude.) reply cush 11 hours agoprevIf there any kind of liability for Amazon, they’d be able to stop this practice in a heartbeat reply wepple 8 hours agoparentIs there not a liability of people getting sick or trawling through nonsense and bad quality products? reply simonw 10 hours agoprev\"The saddest part about it, though, is that the garbage books don’t actually make that much money either. It’s even possible to lose money generating your low-quality ebook to sell on Kindle for $0.99. The way people make money these days is by teaching students the process of making a garbage ebook. It’s grift and garbage all the way down — and the people who ultimately lose out are the readers and writers who love books.\" reply fyrepuffs 10 hours agoprevNobody at all guessed that this would happen. reply melondonkey 11 hours agoprevUsually pretty scam savvy but dropped my guard and bought an absolute garbage AI translation of The Little Prince on Amazon. Now I research anything before buying reply bitwize 10 hours agoprevhttps://news.ycombinator.com/item?id=39512600 In this post I opined that software engineering is akin to chemical engineering: the goal is a process to churn out software at massive scale much like chemical engineers find ways to produce chemicals by the kilolitre. In the software case this comes at the expense of grace, finesse, and craftsmanship, and I suggested another analogy to being a writer vs. a \"literature engineer\". This... is exactly what I meant by \"literature engineering\". reply thrownaway561 8 hours agoprevfrom 6 years ago... it's simple... the booksstore has always been used as a money laundering operation. https://www.theguardian.com/books/2018/apr/27/fake-books-sol... reply jhallenworld 8 hours agoprevSadly the technology for these scams is also very useful for legitimate new self-published authors. How much gatekeeping do we really want? I would think the review system would weed out the junk. So here is a recent example of modern publishing: I've been reading things on r/hfy.. which I don't think anyone would argue is high quality literature, but the stories are fun and the premiss of the entire genre works as background structure. One of the better stories is \"Nature of Predators\" by SpacePaladin15. https://www.reddit.com/r/HFY/comments/u19xpa/the_nature_of_p... Ok, so the author wants to publish on Amazon, no big deal these days, here it is: https://www.amazon.com/Nature-Predators-Book-One/dp/B0CQ5QNW... (I would argue that the reddit experience is actually better, because there are reader comments after each chapter. Where did the cover art come from? I don't know..) What about the passive income? Well he's making $6K a month on Patreon. Maybe not passive since he's working on the sequel.. Well, how about an audiobook? Well patreon.com/Adastra650 has made it into an excellent audiobook on youtube: https://www.youtube.com/playlist?list=PLm8OjwhOz-FZG_M_A1u9d... Some of the voices are AI generated, but they are not bad.. it has me curious what software is used. The thumbnails are also AI. All significant books require a page on TV Tropes: https://tvtropes.org/pmwiki/pmwiki.php/Literature/TheNatureO... Naturally the story has grown out of HFY, has its own subreddit for fans and fanfiction: https://www.reddit.com/r/NatureofPredators/ Naturally there is a NSFW version: https://www.reddit.com/r/NatureOfPredatorsNSFW/ The Silo series (or Wool Omnibus) had similar beginnings: https://en.wikipedia.org/wiki/Silo_(series) reply pico303 8 hours agoparentI took a glance at that “Nature of Predators” book, but couldn’t get past the first sentence. Did the author really mean “sapience,” or “sentience?” reply jhallenworld 7 hours agorootparentHah, you should make this comment on reddit, because I'm curious about the response you'll get. https://www.reddit.com/r/litrpg/comments/tw44yu/sentient_vs_... reply johngossman 7 hours agorootparentprevThey use sapient a few paragraphs later. Hardly the only problem with the writing though. reply nytesky 9 hours agoprevI mean Amazon and Hulu are chock full of terrible B VOD movies that never went to theaters. reply TechSculptor89 6 hours agoprevIt's all business. reply swatcoder 11 hours agoprev [–] Example #732453111 of how the 2000's-era dreams of technology-enabled infinite scaling were misguided and potentially damning for the Internet. Turns out scammers and spammers can technologically scale their side too, and can even do so in a way that the host profits from so that the host is disincentivized from doing anything about it until it's already gone too far. Now everything is flooded with noise and supported by ads and meaningful human participation in content approval or customer service is infeasible because we're already on the far side of the transition and they can't match the established scale. Whoops! reply isx726552 11 hours agoparentThis is why gatekeepers are still a good thing, and now the gatekeepers are themselves somewhat democratized. Lots of people now have their favorite YouTuber / Podcaster / etc. and get book recommendations from them. And there is now a much bigger selection of gatekeepers to choose from given the variety of content on those platforms. It’s not perfect, but overall it is an improvement in many ways. Now yes I would agree that a fully flat, bottom up, “everyone is a publisher” world where everyone is on equal footing (including the spammers) is impractical, but then again who ever said that would happen or work if it did? It remains to be seen what effect genAI has on all this, though. Long term, it seems likely that the need for gatekeeping will only increase due to the inevitable flood of more and more generated junk. reply al_borland 10 hours agorootparentI’ve been trying to divest from Amazon as a source for products, and am looking to use more traditional stores to curate my options. They have buyers that vet stuff to some degree, and if the product is bad, they’ll take it back. If enough people think it’s bad, they’ll stop selling it. reply pavel_lishin 11 hours agorootparentprevI don't think that \"gatekeeper\" is the correct term for what you're describing. Isn't it closer to \"influencer\", or \"taste-maker\"? reply swatcoder 11 hours agorootparentCurator. But otherwise, they're right. Curation and informal trust networks are the only way out. reply panick21_ 1 hour agorootparentprevExactly, I would never go to Amazon to look for books. I go to Amazon to buy a specific book. I already know what I want, Amazon is just the place I order it. Some stuff is also Amazon exclusive. reply MR4D 11 hours agoparentprevClearly we didn't think this through back then. It should have been obvious to us that it is easier to create noise than it is to create signal. The idyllic dream of a free internet with long tails would be replaced with a steadily worsening signal to noise ratio. We cheered on when Google supplanted Yahoo! because the computers were better at curating than humans. Man, we clearly sucked at choosing the right path. Dreams of utopia always hit hard when they fall. reply ori_b 11 hours agorootparent*looks nervously at the AI revolution* reply al_borland 10 hours agorootparentIt seems a lot of people see the issues of AI coming, maybe not all of them, but certainly some. Too bad so many people and blinded by potential profits to care. reply bryanrasmussen 2 hours agorootparentthe issues of AI are pretty much the same issues of \"software eating the world\" and the internet, only more of it, so it's not that hard to be prescient on this matter. reply munk-a 9 hours agorootparentprevUnfortunately all the people with money can see nothing but upsides. reply VHRanger 9 hours agorootparentprevAI content generation is only different in scale to content farms. Not in principle. All this means is that relying on an algo to filter things for you will become less useful (easy to game). You'll need to rely more on real humans you actually trust. reply blululu 9 hours agorootparentThat’s sort of like saying a ripple and a tsunami are the same thing and they differ only in terms of energy, or New York City only differs from Utica in scale. Maybe it is true in some technical sense, but that is missing a lot of important information. Being able to churn out content fast, cheaper and with algorithmic optimizations starts to look like a different beast. The level at which human overrides can keep this process in check has a limit and it can be inundated. New solutions will be needed. reply GuB-42 9 hours agoparentprevI prefer it like that than how it was before. Sure, lots of noise, but lots of signal too. Meaningful human participation in content approval is still possible of course, with social networks. It can go from getting recommendation from an influencer you align with (for once \"influencer\" is an appropriate term), or join a community, for example on a Discord server, or for something more public, Reddit. For customer service, Amazon is surprisingly human, at least for the few times I had to deal with them, but generally, it is something you have to pay for, and most people don't want to, but if you do, you can get a human on line. But the most important part is that you can explore now the wild on your own. Of course, you will be bombarded with noise, but the stuff is there for you to find. Before, it was made much more complicated. For example, now, getting a book in a foreign language may be one click away, before that, you had to hunt for some specialized bookstores, regular bookstores only have best sellers, and ordering, if successful, was slow and sometimes expensive. So, sure, few garbage books, but also a much more limited choice of good books. reply alexashka 8 hours agoparentprev [–] Take it to its root, the only reason we have spammers is because people who control technology instead of reducing work hours created marketing and advertising agencies to create artificial desire to buy useless garbage. If you think spammers are the problem, you're missing the bigger picture, by a lot. Technology should've brought about fewer work hours, instead it brought about mass insanity. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Amazon is overwhelmed with subpar ebooks generated through keyword scraping and AI, exploiting trendy search terms for rapid, inexpensive book production, resulting in misinformation and recycled material.",
      "The surge in low-quality ebook production is propelled by automation, manipulation strategies, and unethical marketing tactics taught in self-publishing courses, like the Mikkelsen twins' controversial ebook publishing program.",
      "This trend devalues high-quality literature and engaging reading encounters, impacting the book market driven by tech and undermining the significance of enriching content."
    ],
    "commentSummary": [
      "The discussion addresses concerns regarding Amazon's return policies, product quality, and ethics, particularly highlighting issues with counterfeit goods and content quality.",
      "Dissatisfaction with Amazon's practices has prompted some customers to cancel Prime memberships or switch to other streaming services to avoid low-quality items and support trustworthy publishers.",
      "The conversation also delves into the role of AI in publishing, emphasizing the significance of human oversight in upholding content quality and the impact of technology on content creation."
    ],
    "points": 203,
    "commentCount": 108,
    "retryCount": 0,
    "time": 1713301937
  },
  {
    "id": 40053710,
    "title": "Firefox 125: New Features and Enhanced Security",
    "originLink": "https://www.mozilla.org/en-US/firefox/125.0.1/releasenotes/",
    "originBody": "See what’s new in Firefox! Release Notes tell you what’s new in Firefox. As always, we welcome your feedback. You can also file a bug in Bugzilla or see the system requirements of this release. Download Firefox — English (US) Windows 64-bit Windows 64-bit MSI Windows ARM64/AArch64 Windows 32-bit Windows 32-bit MSI macOS Linux 64-bit Linux 32-bit Android iOS Your system may not meet the requirements for Firefox, but you can try one of these versions: Download Firefox — English (US) Windows 64-bit Windows 64-bit MSI Windows ARM64/AArch64 Windows 32-bit Windows 32-bit MSI macOS Linux 64-bit Linux 32-bit Android iOS Firefox is no longer supported on Windows 8.1 and below. Please download Firefox ESR (Extended Support Release) to use Firefox. Download Firefox ESR 64-bit Download Firefox ESR 32-bit Download a different build Firefox is no longer supported on macOS 10.14 and below. Please download Firefox ESR (Extended Support Release) to use Firefox. Download Firefox ESR Download Firefox Download Firefox Download Firefox Download Firefox Download Firefox Download Firefox Download for Linux 64-bit Download for Linux 32-bit Firefox for Android Firefox for iOS Firefox Privacy Notice 125.0.1 Firefox Release April 16, 2024 Version 125.0.1, first offered to Release channel users on April 16, 2024 New Firefox now supports the AV1 codec for Encrypted Media Extensions (EME), enabling higher-quality playback from video streaming providers. The Firefox PDF viewer now supports text highlighting. This feature is part of a progressive roll out. What is a progressive roll out? Firefox View now displays pinned tabs in the Open tabs section. Tab indicators have also been added to Open tabs, so users can do things like see which tabs are playing media and quickly mute or unmute across windows. Indicators were also added for bookmarks, tabs with notifications, and more! Firefox now prompts users in the US and Canada to save their addresses upon submitting an address form, allowing Firefox to autofill stored address information in the future. Firefox now more proactively blocks downloads from URLs that are considered to be potentially untrustworthy. The URL Paste Suggestion feature provides a convenient way for users to quickly visit URLs copied to the clipboard in the address bar of Firefox. When the clipboard contains a URL and the URL bar is focused, an autocomplete result appears automatically. Activating the clipboard suggestion will navigate the user to the URL with 1 click. Users of tab-specific Container add-ons can now search in the Address Bar for tabs that are open in different containers. Special thanks to volunteer contributor atararx for kicking off the work on this feature! Firefox now provides an option to enable Web Proxy Auto-Discovery (WPAD) while configured to use system proxy settings. Fixed Various security fixes. Changed In a group of radio buttons where no option is selected, the tab key now only reaches the first option rather than cycling through all available options. The arrow keys navigate between options as they do when there is a selected option. This makes keyboard navigation more efficient and consistent. Enterprise You can find information about policy updates and enterprise specific bug fixes in the Firefox for Enterprise 125 Release Notes. Developer Developer Information Following several requests, we have reintroduced the option to disable the Pause Debugger Overlay (devtools.debugger.features.overlay). This overlay appears over the page content when the debugger pauses JavaScript execution. In certain scenarios, the overlay can be intrusive, making it challenging to interact with the page, for instance, evaluating shades of color underneath. We've added a new drop-down menu button at the bottom of the source view in the Debugger panel, specifically designed for Source Map related actions. Users can now easily disable or enable Source Maps support, open the Source Map file in a new tab, switch between the original source and the generated bundle, toggle the \"open original source by default\" option, and view the Source Map status such as errors, loading status, etc. Web Platform Firefox now supports the popover global attribute used for designating an element as a popover element. The element won't be rendered until it is made visible, after which it will appear on top of other page content. WebAssembly multi-memory is now enabled by default. Wasm multi-memory allows wasm modules to use and import multiple independent linear memories. This enables more efficient interoperability between modules and provides better polyfills for upcoming wasm standards, such as the component model. Added support for Unicode Text Segmentation to JavaScript. Added support for contextlost and contextrestored events on HTMLCanvasElement and OffscreenCanvas to allow user code to recover from context loss with hardware accelerated 2d canvas. Firefox now supports the navigator.clipboard.readText() web API. A paste context menu will appear for the user to confirm when attempting to read clipboard data not provided by the same-origin page. Added support for the content-box and stroke-box keywords of the transform-box CSS property. The align-content property now works in block layout, allowing block direction alignment without needing a flex or grid container. Support for SVGAElement.text was removed in favor of the more widely-implemented SVGAElement.textContent method. Community Contributions With the release of Firefox 125, we are pleased to welcome the developers who contributed their first code change to Firefox in this release, 12 of whom were brand new volunteers! Please join us in thanking each of these diligent and enthusiastic individuals, and take a look at their contributions: Artem Manushenkov: 1619201 Bojidar Marinov [:bojidar-bg]: 1839845 daxpedda: 1873642 Dmitri: 1881682 Hovav Shacham: 1880366 jsharp@fastly.com: 1861533 marten.richter: 1872496, 1873263 Nikki Bernobic [:echrs]: 1862253, 1878635 Patrycja Rosa [:ptrcnull] (she/her): 1881979 rushliu: 1883600 uhhadd: 1883184 zhanghe9702: 1881896 Get the most recent version Download Firefox — English (US) Windows 64-bit Windows 64-bit MSI Windows ARM64/AArch64 Windows 32-bit Windows 32-bit MSI macOS Linux 64-bit Linux 32-bit Android iOS Your system may not meet the requirements for Firefox, but you can try one of these versions: Download Firefox — English (US) Windows 64-bit Windows 64-bit MSI Windows ARM64/AArch64 Windows 32-bit Windows 32-bit MSI macOS Linux 64-bit Linux 32-bit Android iOS Firefox is no longer supported on Windows 8.1 and below. Please download Firefox ESR (Extended Support Release) to use Firefox. Download Firefox ESR 64-bit Download Firefox ESR 32-bit Download a different build Firefox is no longer supported on macOS 10.14 and below. Please download Firefox ESR (Extended Support Release) to use Firefox. Download Firefox ESR Download Firefox Download Firefox Download Firefox Download Firefox Download Firefox Download Firefox Download for Linux 64-bit Download for Linux 32-bit Firefox for Android Firefox for iOS Firefox Privacy Notice All Firefox downloads Need help or want to leave feedback? File a bug Leave feedback Get Involved Did you know that most of the content in Firefox Support was written by volunteers? Find out more Other Resources Developer Information Complete list of changes for this release Press Kit Mozilla and Firefox News Firefox Extended Support Release All Firefox for Desktop Releases Learn more about Firefox",
    "commentLink": "https://news.ycombinator.com/item?id=40053710",
    "commentBody": "Firefox 125 (mozilla.org)200 points by N19PEDL2 17 hours agohidepastfavorite90 comments zeotroph 17 hours agoFirefox View slowly getting more features, nice. I hope one day it supports the very real use case of using/abusing browser tabs as bookmarks. I know I am not the only one who keeps lots of windows with currently almost 100 tabs open. Windows represent certain topics, which I finish in a day, in in months, or sometimes never - but still they re-open everytime I open Firefox. I know there are add-ons to help with \"tab overflow\", but native support would be better. reply enriquto 16 hours agoparent> abusing browser tabs as bookmarks This is not really abuse, it's very natural. Tabs, bookmarks and browser history should be one and the same thing. reply doubled112 16 hours agorootparentI don't like this idea. I can see how others would though. I have a really hard time getting back into things. Any state a thing remembers is the opposite of helpful. For example, I don't suspend laptops, I turn them off. It'd be a waste of time closing what I was doing before because I don't remember where I was earlier and it's easier to start over. Browser tabs are a current state. If I will want to use it multiple times, I use a bookmark. History is for the rare time I think \"wait, I swear I read that earlier\". It isn't just computing, I forget where I was in books and games sometimes and start over too. I can't be the only one. reply safety1st 16 hours agorootparentI feel like I start to lose focus when I have more than a few tabs open. When I see people with dozens of tabs open I get a little panicky. I want to focus on the task at hand and if I want to recall an old site I'll use bookmarks and history for that, not bloat my active session state. reply bbarnett 16 hours agorootparentUh. Afraid to say this, but I've had more than 1000 tabs open. It's useful too. One time I remembered doing something, found the tab. Sure, it was a 6 year old tab, but it was helpful. reply philsnow 3 hours agorootparentI periodically take firefox's recovery.jsonlz4 and pipe it through a bunch of jq to export it as a datestamped bookmarks-$(date ...).org file. Right now I have 15 windows and 5351 tabs. Then I close all but the windows I have actually looked at in the last month. reply doubled112 15 hours agorootparentprevThis doesn't sound too much different than my garage, really. Just junk and half finished projects piled up. That might be useful some day. toss I get why it'd be useful, and stretching the capabilities like that means that browsers get more and more useful and efficient. It just doesn't work for me, and that's alright. If you can open 1000 tabs, my 10 are no problem! reply cryptonector 13 hours agorootparentprevRelationally the only difference between a \"tab\", \"history\", and a \"bookmark\" is a column that tells you what kind it is. reply 01HNNWZ0MV43FF 12 hours agorootparentKinda but not really. I expect a tab to stay in memory, at least in virtual memory, and not need to hit the server if it's in the background for a while and then I click on it again, unlike history or bookmarks. I expect history to not be loaded in memory, it would be a waste of memory. I expect bookmarks to show up in search. Searching history is different, history has a ton of useless garbage in it. \"Bookmark\" means \"When I type this, show this one URL before all the other ones that seem relevant\". reply cryptonector 11 hours agorootparentAll browsers use SQLite3 DBs for this stuff. History is not \"loaded into memory\" -- it's in the SQLite3 DB, which has a page cache, so some of it might be in memory, but the amount of memory dedicated to the page cache is fixed, so you need not worry about it. > I expect bookmarks to show up in search. Searching history is different, history has a ton of useless garbage in it. There is truly no difference except that some URIs are bookmarked (and maybe also in recent history) and some are not. Either way searching these is just a SQL query against the SQLite3 DB. You really need not worry about the efficiency of the search -- it's fast enough. The table(s) that store these things have enough metadata to enable the kinds of searches you might want to do. Among such metadata is \"this is a bookmark\" and \"this is not a bookmark\". > \"Bookmark\" means \"When I type this, show this one URL before all the other ones that seem relevant\". That doesn't preclude treating it not that differently from history. You really need not worry about those details. Also, a \"bookmark\" is more than that: it also has a name, it may be in a folder, it may be in the bookmark toolbar, it may have tags to help you search for it, etc. reply aaplok 11 hours agorootparentprevIf they serve the same purpose, doesn't that open the possibility to use one of them for something else? I'm also in the group that never keeps more than 10 tabs open, much fewer usually. The idea of having all my bookmarks displayed in a long sequence of tabs sounds horrific to me personally. It would just be an overwhelming amount of information. I would love it if bookmarks and history were easier to search and retrieve. Having them displayed in a thousand tabs on my screen constantly not so much. reply cryptonector 11 hours agorootparentThey don't have to be displayed in tabs. reply Mathnerd314 16 hours agorootparentprevIn some sense they are the same thing, they are all stored in the Places database in a relatively uniform schema. In another sense they aren't, history is erased after a few months whereas losing bookmarks or tabs would be very bad. Firefox Mobile has this \"inactive tabs\" feature which is OK but I would never turn on the option to auto-close tabs. reply araes 15 hours agorootparentprevNot here. Would much prefer something like evolved bookmarks, yet still very much separate from a bookmark I specifically chose. Evolved would be more like the browser just hangs onto History longer if you go there more. Tabs, I donno, I'm not the target audience for Tabs. I'm on somewhat limited resource hardware, so keeping 100 tabs open is simply not feasible. All browsers run like slugs with 100 tabs open from personal experience. With four tabs open on Firefox Developer 125.0b9 I have (6) main process groups with (8) further subprocesses for 1075 MB of memory while doing ... nothing. The tabs are Not impressive. HN (2,text), WP (text), Gmail (text). Adding only Redfin and Facebook jumps me up 500 further MB in resident memory. Two tabs, no actions. How do the landing pages for two major companies eat 500 MB of resident memory statically?... reply lambdaba 16 hours agorootparentprevI'm totally with you on this. I remember seeing this concept video from Mozilla in the mid '00s, IIRC it's pretty much this vision, and more: https://www.youtube.com/watch?v=AYMA5W8b1zY. I'm actually a bit shocked that almost 20 years later and the browser interface is pretty much still the same. reply pxc 15 hours agorootparentSome of the ideas about being able to work with data in different formats are cool, as is attempting to group related sites/pages/topics together, but that UI looks way too visual and spatial for me. Too much movement, too many images. Text is much less work to parse for me. reply lambdaba 14 hours agorootparentIt's a concept, but I agree. It does show nicely what a true \"user agent\" could look like. I want this, the browser as sidekick not machinery with multiple buttons etc. I want my browser to be my operating system. reply Too 14 hours agorootparentprevDumping everything you have onto the kitchen counter is also very natural. It just doesn’t remain effective for long. The cabinets are a pretty darn useful invention. Don’t mix work surface with storage. reply solarkraft 16 hours agorootparentprevDamn, thanks! I've been saying this for a while. They're all references to pages, optionally with some state. reply nashashmi 15 hours agorootparentprevDid you know that history in Chrome only remains for three months? reply Mathnerd314 16 hours agoparentprevI have tried add-ons, but for me, the native workflow of selecting tabs with ctrl, right click to bookmark them to a folder and then close them all, and then later opening them from the bookmark manager is good enough. They added multi-select around Firefox 63 and it has improved this workflow to be pretty enjoyable. reply a_subsystem 15 hours agorootparent* in the address bar (that is: 'star' 'space') will put you in bookmark search. I use tags prolifically and can search for keywords here and it works pretty well. reply Mathnerd314 12 hours agorootparentI guess I was not clear - what I do in the bookmarks manager is go into a folder, and select and open them in groups of 20 or so, then delete them as bookmarks. So bookmarks are sort of like \"tab cold storage\" for me. So for me tagging them would be wasted work since they will eventually be deleted regardless. I also do use bookmarks for the awesomebar but it is mainly so they are prioritized in normal autocomplete. reply olejorgenb 16 hours agoparentprevI like the the https://josh-berry.github.io/tab-stash/ extension. Not exactly what you want, but IMO a very well-done extension in that space. Firefox handle lots of tabs very well though since the tabs are lazily loaded. Only issue is that the session system once in a while mess up and you lose everything. reply lionelw 3 hours agorootparentJust adding this here: I created a multi-window management addon Winger[0] which lets you treat windows as 'tab groups', and it has a stashing feature (windowsbookmark folders / tabsbookmarks) similar to Tab Stash. Despite the ostensible scope overlap, I know there's at least one person (not me) who uses both, setting Winger's stash home folder to be the same as Tab Stash's to achieve a manner of integration. [0] https://addons.mozilla.org/en-US/firefox/addon/winger/ reply Cyuonut 1 hour agorootparentNow you know a second one. reply TillE 16 hours agoparentprevI don't know if there have been any real studies about this, but I certainly hear about a ton of people with a million open tabs and very few who productively use bookmarks in any significant way. My theory is that bookmarks are a failed concept, putting something out of sight but not in a useful system, so it's guaranteed to be forgotten. If I really need to save an important link, it goes in a notebook which is far more flexible. reply Semaphor 16 hours agorootparentI love bookmarks, they are simple and just work. I have a few pinned tabs I want always open, and 5-15 tabs for current things. And the way browsers are designed, this works very well, so I rarely have reasons to write about it. But the people who don’t like bookmarks and want tabs for organization have to work against the browser, so they are far more likely to write about their issues. reply OkayPhysicist 10 hours agorootparentprevI use the bookmark bar a lot, for sites that I want to open new instances of, somewhat regularly. Like this one. I use \"tabs as bookmarks\" for things that are relatively time limited: My current PR, a dozen bug reports, etc, get piled into a tab tree that lives for as long as that PR does. Then on any given day I'll have a bunch of tabs I open for random nonsense, which I'll clean up periodically. Non-bar bookmarks only get used for sites that I want to find again some day but I know I won't remember. reply godelski 14 hours agorootparentprevI like bookmarks, but you're right about the retrieval problem. I think this is one of the actual problems LMs (not even LLMs) could help resolve. Semantic tagging and generating auto-tags can really help in the retrieval process. Because I do pull things back up but even the placing in the correct place to \"live\" task isn't straight forward. Because to actually be organized, your bookmarks need depth in their categories. Not to mention the whole issue of selecting a URL, dragging it over the toolbar, holding it over a folder, and then battling dragging it into the next frame that's showing the contents of that folder and all the menu disappearing because I went 0.5 pixels out of bounds (god, who designs these? Same people who design window edges? You really want me to grab a 1 pixel line on a 4k monitor?!) The reason I end up tab hoarding is because these are temporary bookmarks. Things I might want to but don't know yet if I want to long term store (because the bigger your bookmark library is, the more difficult it is to find things! Major flaw). Or things I'm concurrently working on (tab groups help here). A classic example of the latter is when debugging I might be tracing down an error and do not yet know if I can leave a SO post or doc closed because I haven't resolved the issue yet and there's a good chance I come back. So I can easily solve a bug and that frees up 10+ tabs. Maybe I'm doing it wrong reply sethhochberg 14 hours agorootparentprevI'd agree with that theory - a big part of why I leave tabs open is that I'm not deliberately stashing them until later, I just get pulled into (or sometimes distracted by) something else and will, eventually, make my way back by virtue of closing tabs I'm done with. Its a fun surprise for later, and lets me entirely skip the mental friction of deciding whether something is important enough to bookmark and how I'd categorize that bookmark etc. I look at Bookmarks vs Tabs in the same way I look at Google+'s Circles vs Instagram's \"everyone vs close friends\". One is far more powerful, the other forces far fewer decisions and is seemingly good enough for just about everyone. reply Anthony-G 11 hours agoparentprevI also suffer from “tab overflow” so I convert them to bookmarks. Unlike tabs, bookmarks use minimal browser resources – just an entry in an SQLite database. Every so often, I use Ctrl-Shift-D to save all the tabs in the current window to their own folder in Bookmarks. I follow this with Ctrl-Shift-W which closes all the tabs in the current window. I repeat this for each browser window containing multiple tabs. That way, the tabs that had been open are stored in a loosely organised way and are easily retrieved if I ever feel the need to. More often than not, I won’t look at them again but they’re there if I want to. reply eviks 15 hours agoparentprevIndeed, Vivaldi's workspaces is just great for this (although unsynced) as it allows you to maintain your work in the same \"visual\" place you usually organize your pages - in the tab row! But this also allows you to close any window and restore it freely without losing that information/alignment reply k8svet 15 hours agoparentprevI can't help myself... Firefox View poorly duplicates functionality found in other places of the browser already. Meanwhile, the rest of the product just languishes. Here's a fun thought: You have Firefox Android setup to sync with your Mozilla account. You have made the questionable decision to have 300 tabs loaded into that session. You made further questionable decisions to place your phone on a precipice about a ceramic tile floor. Cut to a few hours later, you're performing \"new Android phone setup\". How do you recover those tabs? (No no no, there's three different ways of viewing Synced Tabs in Firefox Desktop and none of them let you bulk save as bookmarks; and no, Firefox for Android can't replicate them over or help you at all either). The answer, after 6 years of users asking about this, is to enable internal debug features and run a user script from some Gist on GitHub. .... seriously. Meanwhile in addition to the new, disjoint Firefox View, there's the disjoint hodge-podge of functionality duplicated between the sidebar, library view, and Firefox Account \"menu\". Firefox's product development and vision is one of the most infuriating of any product, open source or otherwise, that I use. Also, the new Firefox View has the same completely BAFFLING padding that the settings and addon pages have expect its /not even consistent/!!? Just shrink the browser window and watch as 90% of websites out their properly adapt and adjust, but nope! not Firefox's own internal \"new\" pages. reply solnyshok 15 hours agorootparentyou can \"open all tabs from other device here\" to bring all tabs from your mobile to your PC reply causi 16 hours agoparentprevPeople wouldn't have to do that if they implemented \"progressive bookmarks\". I want to be able to bookmark a top-level web page and then later when I click the bookmark go back to the last page I was on, on that site. reply godelski 14 hours agoparentprevThere's only a few more things I really want from a browser and I'll admit, at this point most things are minor. - Tab groups: I handle this by Simple Tab Groups[0], which how it seems to do this is by bookmarks. I think something like this is a godsend for tab hoarders like us (100? Gotta pump up those numbers!). Making native would be really nice and would be great to containerize them. Especially across devices and accounts (I never want to use my Firefox account on a work computer. I wish send would come back so I can send tabs and links between different devices. Or maybe like \"partnered\" or \"containered\" accounts because we all have multiple machines we want to not just share everything across, but I'm digressing). Grouping tabs by projects or tasks really helps manage so you don't get a ton of orphaned tabs. You can just close the group when you complete. - Duplicate Tab Closer[1]. For the tab hoarder, this is such a critical addon. When treating tabs like temporary bookmarks (which is why we hoard?), you may run into a tab again and not having dupes really prevents things from exploding. I just wish I had a bit more control/explanantions and could interact with the tab groups. - Auto Tab Discard[2]: \"caches\" tabs so they aren't actively running or taking up your ram. I wish it actually cached though because this more seems like it just bookmarks because you reload the page when you come back rather than quickly pull a cache. I know because sometimes a page will go down and I have lost it I know us tab hoarders aren't a big market share, but I think these types of tools, if made native, could also really help a lot of other people. I'd really love the idea of being able to sync specific tab groups across containerized accounts, so I can do things like share a group of tabs between home and work machines. IDK, what do others think? What do you use to manage tab hoarding? My hardest problem is properly purging[side note] [0] https://addons.mozilla.org/en-US/firefox/addon/simple-tab-gr... [1] https://addons.mozilla.org/en-US/firefox/addon/duplicate-tab... [2] https://addons.mozilla.org/en-US/firefox/addon/auto-tab-disc... [side note] this is an extra problem on mobile. Often I don't realize I'm opening new tabs because when I just open the browser sometimes it drops me in the last one and sometimes I'm on the \"home page\" (new tab). So I get orphaned tabs this way. And on mobile it is even harder to group and deal with \"inactive\" tabs. Which for me are frequently research papers. And we all know Zotero and Mendeley are shit. Seriously, why can't I easily load them into my ipad and mark up those documents, share marked or unmarked, attach notes or communal share? Do the creators do research? Sorry, off topic again. reply MisterTea 16 hours agoparentprevI hate lots of tabs with a burning passion. The whole 100+ tabs open mess is simply due to no one bothering to solve bookmarking properly. Of course I do keep a few tabs open for long lengths, I avoid growing the count past 20 or so. If a website is worth saving, I put the link in a text file with the page title and maybe some keywords if the title isn't relevant to the subject. Usually that is enough to find the link with a quick grep. Kill the tabs reply spuzz 16 hours agoparentprevI hope it never optimizes around this. There are a million ways to organize information and \"leaving it all over the fuckin place in case I ever need it\" is not an organization method, it's the absence of one. Then again I do love seeing people have fifteen million tabs open, it helps me set my expectations about what they can handle. reply ziddoap 15 hours agorootparent>Then again I do love seeing people have fifteen million tabs open, it helps me set my expectations about what they can handle. What do you mean by this? Are you implying they can't handle something? Or can? And how did you come to that conclusion? reply eviks 15 hours agorootparentprevHow come 'Windows represent certain topics' didn't fit the million? reply widdershins 17 hours agoprevAfter updating (from 124) I saw this message, although it wasn't in the release notes: > With this latest version of Firefox, you’ll experience 25% quicker on-average page loads, which means more all-around zippiness anywhere you go online. Any details on how this performance improvement was acheived? reply akshayKMR 16 hours agoparent> Firefox is, on-average, 25% quicker with page loads than it was last year – and it was pretty fast then, too Is the copy I saw on update. Suggests incremental wins over the year rather than all in this release. reply boyaka 13 hours agorootparentAnecdote: Firefox (124) has become unusable for me on Arch Linux. Highly resource intensive applications (particularly including video streaming, even worse with camera / webrtc) consume way more CPU resources to the point of my laptop lagging out. I don't remember the exact versions, but I upgraded to 124 earlier this month. Brave works great for video streaming, but breaks applications I use. Firefox development (125) also breaks some applications I use, and has the same performance issues as regular firefox. The fan on my laptop has been broken for most of this year (replaced due to first fan making noise, but havent had time to figure out why the replacement stopped working). I have been running with all high performance CPU features disabled in the BIOS for years with the purpose of not using the fan anyways, and it has been working great with 1 or 2 intensive applications max running in firefox (and as much other low intensity stuff that basically doesn't consume much resource such as another browser - chromium, vscode, my development environment including a vm). I don't currently really have any conclusive idea about what's going on. I will have to start using my desktop, fixing issues with my laptop, and do some more testing out of my workloads. Overall I'm just getting the feeling that something is horribly wrong with Firefox. reply free_bip 8 hours agorootparentSounds like a hardware decoding issue to me reply godelski 14 hours agorootparentprevThat's probably a good note though. If you're a FF user these gains probably won't be noticed because your window shifts to keep up (just like it is hard to see improvements in yourself because day to day gains might be small but add up over time). But if you aren't a FF user, this is quite the speedup and might be enticing enough to get you to try it again. Because there's still a general belief that FF is slow. reply pavon 16 hours agoparentprevThis announcement has some links to articles with more details: https://hacks.mozilla.org/2024/03/improving-performance-in-f... The SpiderMonkey devs also post newsletters every few months detailing all the recent improvements they've made: https://spidermonkey.dev/ reply MrRadar 15 hours agorootparentIt's too bad the Mozilla Gfx blog is dead, I liked reading their updates on the graphics rendering subsystem: https://mozillagfx.wordpress.com/ reply hysan 16 hours agoprevI tried switching back to Firefox to take advantage of their multi account containers, but there were a couple of UX things that kept me from sticking with it: - still no way to remap Ctrl+T to open a new tab in the current container context - lack of native tab grouping, meaning that it works with plugins like OneTab - vertical tabs that play nicely (meaning integrates nicely so that the UX feels good) with some sort of separation feature, for example, tab groups, workspaces, profiles, etc. Leaving this up to extensions seems to lead to a suboptimal experience here. Very willing to try suggestions as I’ve spent days testing every other browser to compare (Chrome, Edge, Brave, Vivaldi, Arc, Floorp, Orion, etc) with Safari being my current default. Surprisingly, Edge has the nicest vertical tab implementation so far. reply lionelw 5 hours agoparent>still no way to remap Ctrl+T to open a new tab in the current container context A quick search found me this: https://addons.mozilla.org/en-US/firefox/addon/new-container..., using Alt+C. There are other similar addons too. I'll assume given the solution was pretty easy to find, your issue must be with the inability to repurpose Ctrl+T... reply morsch 16 hours agoparentprevHave you tried the Sidebery vertical tabs extension for Firefox? https://github.com/mbnuqw/sidebery reply squigz 15 hours agorootparentSidebery comes up a lot, but I've been using Tree Style Tab [0] for many years without issue. Does Sidebery offer anything TST doesn't? https://github.com/piroor/treestyletab reply HelloMcFly 15 hours agorootparentSidebery has a form of Tab Groups called panels (see: https://imgur.com/Za9lwej) that are built into the functionality. I also have just found Sidebery to work a little better than any of the alternatives. But they're not too far apart. reply squigz 13 hours agorootparentTST is a hierarchy as well, so you can have children, and you can create groups (folders) too (Not trying to crap on Sidebery at all; just checking the differences :) Edit: Oh, are you referring to those icons at the very top of the tabs? If so, that is sort of neat indeed reply rraghur 8 hours agorootparentprevYes lots... Container integration... So certain urls always open in certain container... Snapshots I moved from tst a few years ago.. Let me get rid of a few more extensions too reply hysan 13 hours agorootparentprevYup, tried sideberry, tree style tabs, and simple tab groups since I think there was a Mozilla page they suggested those three. None of them felt as usable as native tab groups in any of the other browsers that support tab groups (or workspaces, profiles, or whatever else their approximate implementation is). Especially in terms of interactions with other tab-related extensions. Part of the problem with having that be a non-native feature. reply orbital-decay 13 hours agorootparentThat's kind of the opposite of my experience. Can't leave Sidebery, it's so well integrated and usable when combined with a couple userchrome.css tweaks that any other browser simply feels subpar to that; not nearly the same attention to detail. reply hysan 11 hours agorootparentDo you have links to the userchrome tweaks that you use? I hear that mentioned a lot as a way to make sideberry better. IMO, the default for sideberry isn’t good. The few tweaks that weren’t outdated made things a bit better, but nothing really hit the mark. Like I said above, very willing to give suggestions a try. edit: my main worry with relying on userchrome tweaks is things falling out-of-date or abandoned. I saw this a lot in my testing. reply orbital-decay 10 hours agorootparentI only hide the main tab bar and sidebar header, both tweaks are copied verbatim from [0] (which is linked from the sidebery github readme) and the size of the window buttons block is corrected manually in a var. Yes, tweaks going out of date was also my concern, but they're already a few years old and nothing broke so far. I reckon I could just adapt them if anything breaks, they're easy. [0] https://mrotherguy.github.io/firefox-csshacks/ reply hawski 15 hours agoparentprevI don't have a solution for the Ctrl+T issue. I use mainly the default container as with First Party Isolation I don't need to use containers beside some specific use case (multiple accounts), otherwise I put things in containers strictly for the thing like banking. For the latter I would recommend Simple Tab Groups. It also has a sidebar which I like very much. I understand it may not be the thing you are looking for, but there are a lot of things you will not get anywhere. https://addons.mozilla.org/en-US/firefox/addon/simple-tab-gr... reply sikhnerd 15 hours agoparentprevSideberry extension gives you a lot of what you're looking for. You can have separate tab panels for each container, and a keybinding (ctrl+space by default iirc) that opens a new tab in the current panel and the current container. reply HelloMcFly 15 hours agorootparentSidebery has been \"good enough\" for me to transition from Edge back to Firefox, but I wish you could put your Tab Groups into a named folder vs. just having some tabs nested under a \"parent tab\". reply emaro 15 hours agorootparentActually you can: right click on a tab, select \"Group\" and name it. Edit: in the Sideberry side bar to be clear. reply clumsysmurf 11 hours agoparentprevSome other UX things missing: * profile switcher * Safari \"Show tab overview\" reply hysan 11 hours agorootparentYup, those would also be great UX improvements. Firefox used to have something like “Show tab overview”. I think it was called Panorama? I might try looking for it to see if it still exists and works. edit: the problem with relying on extensions for all of this is that it’s not cohesive. I found that to be the main problem with Firefox that makes me put it a tier lower than everything else right now. reply clumsysmurf 7 hours agorootparentYes, also if you work at a company that locks down the machine its likely you can't install browser extensions. reply presbyterian 15 hours agoprevStill no swipe to change tabs on iOS, which makes it a non-starter on that platform for me, and a browser I can't use on every device can't be my primary browser. Bummer, because I really want to use Firefox. reply freedomben 15 hours agoparentI recognize that you aren't placing any (explicit at least) blame here, simply stating facts, so I want to make clear that what I'm saying is simply an addition to your comment, not a refutation. Is it really surprising that iOS isn't an important target for Firefox, given that Apple's draconian rules don't allow them to do much? If we're going to assign responsibility for this, I think Apple is who deserves (at least most of) it. Given the major (artificial) impediments imposed by the gatekeeper of the platform, were I in their shoes I would never invest scarce dollars into such a risky undertaking where there is a hostile* and extremely powerful overseer. *To be clear, I mean hostile to 3rd party browsers, such as Firefox reply AiAi 15 hours agoparentprevI also miss this feature, I hope they have that on their roadmap. But the whole UI is kinda janky to me, sometimes it flashes when I'm interacting with it, and the closing tab gesture seems to have a sensitivity issue, sometimes I try to close a tab, and it doesn't close because I didn't swipe enough. reply nahtnam 16 hours agoprev> We’re still preparing the notes for this release, and will post them here when they are ready. Please check back later. Seems like people are getting the release notes from somewhere else (presumably from in app). Could someone share it here? reply mondobe 16 hours agoparentArchived here: http://web.archive.org/web/20240416154930/https://www.mozill... Tried it on Edge, the page loaded fine. Maybe it was just a server/caching hiccup. reply trog 12 hours agoprevHmm, can you disable the av1 support? I use Firefox on streaming services specifically because it uses an older codec which is easy to capture video clips from in OBS reply azinman2 17 hours agoprev> Firefox now prompts users in the US and Canada to save their addresses upon submitting an address form, allowing Firefox to autofill stored address information in the future. Seems odd this is just coming now in 2024? reply sp332 17 hours agoparentIt's been a feature for years, so I don't know what the announcement is about. E.g. https://support.mozilla.org/en-US/questions/1267609 reply eyelidlessness 17 hours agoparentprevI don’t use Firefox often outside of cross browser testing, and this initially surprised me a little too. But as a software developer, my second thought was “well yeah, there are probably thousands of potential features like that which they will or won’t get to eventually, like basically every other software project”. reply alostsock 17 hours agoparentprevIt's had autofill, I think just the prompt is new? reply aprilnya 14 hours agoparentprevprotip: you can enable this in other countries through some options in about:config https://wiki.mozilla.org/Firefox/Features/Form_Autofill reply inferiorhuman 15 hours agoparentprevWhat would be really nice is the ability to clear form data separate from search history. Even better would be the ability to disable saving form data while saving search history. e.g. https://support.mozilla.org/en-US/questions/1167882 reply mvdtnz 16 hours agoparentprevFirefox still won't remember and autofill my credit cards even though I know it's a feature they offer in other countries, it's a feature offered in my country by other browsers and it's even a feature they mistakenly expose in the settings page of some but not all of their browsers (mobile only, if I remember right), despite not working. reply 2-3-7-43-1807 14 hours agoprevthere's one and only one feature that i care about ... ... borders between tabs. reply calvinmorrison 17 hours agoprev> Firefox now more proactively blocks downloads from URLs that are considered to be potentially untrustworthy. Firefox sends all your download Requests to a 3rd party source then? reply jonchang 17 hours agoparentYou'll be interested to know that Safe Browsing was introduced in Firefox 2 in 2006, and the malware check feature was introduced in 2014! I suggest you search using your favorite search engine to see how this feature works while also preserving the privacy of your URLs and downloads. It uses hashing! Here's a good link, I suggest you scroll down to the Privacy section and read carefully: https://feeding.cloud.geek.nz/posts/how-safe-browsing-works-... reply ptx 15 hours agorootparentYes, but the phrasing about being \"more proactive\" seems to suggest that perhaps this approach has now been adjusted. However, according to Bugzilla [0] it seems to be about blocking HTTP downloads on all pages compared to previously only blocking HTTP downloads on HTTPS pages, and then someone tweaked the wording to add the sinister-sounding part about being \"more proactive\". [0] https://bugzilla.mozilla.org/show_bug.cgi?id=1877195#c12 reply bink 17 hours agoparentprevMore likely it either keeps a local copy of the DB or sends a hash. reply cassianoleal 17 hours agoparentprevI would imagine it syncs down block lists rather than checking each download with a 3rd party. reply ok123456 16 hours agoparentprevBloom filter or cuckoo hash. reply smodo 17 hours agoparentprevThey could just pass the browser a blocklist? reply ekns 15 hours agoprevWow, that's a lot of security fixes. If there were so many security bugs, that would suggest that there's a lot more still to be found. Probably going to look into sandboxing the browser much more in the future... reply dsign 15 hours agoprev [–] > Firefox now more proactively blocks downloads from URLs that are considered to be potentially untrustworthy. I got a hit yesterday from a plugin I wanted to download from blendermarket. After fiddling with Firefox, I downloaded the plugin. It had a binary component that I audited with several anti-virus programs. The rest was Python code, that I also audited. No auditing found anything suspicious. Yes, there is such a thing as malicious binaries in places, but I would like to know more about how Firefox is deciding to flag download links. Is it AI working on generic signals? If Americans can carry guns, while does Firefox doesn’t let me download things? A warning about “our foxy algorithm doesn’t like this egg” would be okay, but blocking the download and forcing the user to jump through hoops is flat disrespectful. reply kbenson 15 hours agoparent [–] > If Americans can carry guns, while does Firefox doesn’t let me download things? I'm almost afraid to ask the reasoning behind this, but to go out on a limb and assume I understand your question accurately, because one is a constitutional right of citizens protected by the government and the other is a private company making a judgement call and you're opting into their reasoning by using their product? Regardless of whether you agree with the specifics of either of them or whether they are good choices or systems, they have nothing to do with each other, at all. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Firefox version 125.0.1 update introduces support for the AV1 codec, text highlighting in the PDF viewer, and enhancements in tab management and address autofill.",
      "Security fixes and developer improvements are included such as disabling the Pause Debugger Overlay and new web platform support.",
      "Firefox ESR is provided for older operating systems; however, support for Windows 8.1 and macOS 10.14 has been discontinued. Community contributions are emphasized, promoting user engagement in the Firefox community."
    ],
    "commentSummary": [
      "Users engage in a comprehensive discussion about tab management in Firefox, encompassing tab usage as bookmarks, distinctions between bookmarks and history, preferences for tab organization, and challenges related to browser performance.",
      "The conversation also delves into recommendations for tab management extensions, strategies for efficient tab and bookmark handling, concerns regarding Firefox's functionality and ongoing development, as well as debates on the efficacy of various tab management tools.",
      "Furthermore, the discourse touches upon additional topics like download blocking, form autofill, and the balance between government protection and private company decisions concerning browser functionalities."
    ],
    "points": 200,
    "commentCount": 90,
    "retryCount": 0,
    "time": 1713283330
  },
  {
    "id": 40054580,
    "title": "usbredir: Sending USB Device Traffic Over Network",
    "originLink": "https://www.spice-space.org/usbredir.html",
    "originBody": "Toggle navigation HOME FAQ Features Documentation Demos Support Download Developers Contact usbredir usbredir is the name of a network protocol for sending USB device traffic over a network connection. It is also the name of the software package offering a parsing library, a usbredirhost library and several utilities implementing this protocol. The protocol is documented here, this document also explains what is meant with a usbhost and a usbguest. usbredir was created for use with Spice, which is why it is hosted on spice-space.org, but the protocol and the usbredirhost are completely independent of spice, they could for example also be used to create a vnc extension for redirecting USB devices over a vnc connection to a qemu virtual machine. The usbguest side is currently only implemented for qemu, and shipped as part of qemu (enabling this in qemu requires the libusbredirparser library to be available when building qemu). usbredir supports USB device filtering configurable by a filter string Host Configuration For redirection to work, the virtual machine must have a supported USB controller. Qemu supports both USB2 and USB3, but at the time of writing, USB3 has had less testing. For USB2 support, the guest must have a EHCI controller and companion UHCI controller (companion UHCI is needed in order to support also USB1.x devices). For USB3 support, an XHCI controller is required. It also needs to have Spice channels for USB redirection. The number of such channels determine the number of USB devices that it's possible to redirect at the same time. More information about USB controllers in qemu could be found here. Using virt-manager Virtual machines created with virt-manager should have a USB controller by default. In the virtual machine details, select \"Controller USB\" in the left pane. If you only need to support USB2 devices, make sure its model is set to \"USB2\". For USB 3.0 support, select \"USB3\" for the model type. You can then click on \"Add Hardware\" and add \"USB Redirection\" items as the number of USB devices you want to be able to redirect simultaneously. Using libvirt The following libvirt XML will configure a guest with USB2 support and the ability to redirect 3 devices simultaneously:For USB3 support, the configuration can be simplified to: Using QEMU The following qemu options will configure a guest with USB2 support and the ability to redirect 3 devices simultaneously -device ich9-usb-ehci1,id=usb \\ -device ich9-usb-uhci1,masterbus=usb.0,firstport=0,multifunction=on \\ -device ich9-usb-uhci2,masterbus=usb.0,firstport=2 \\ -device ich9-usb-uhci3,masterbus=usb.0,firstport=4 \\ -chardev spicevmc,name=usbredir,id=usbredirchardev1 \\ -device usb-redir,chardev=usbredirchardev1,id=usbredirdev1 \\ -chardev spicevmc,name=usbredir,id=usbredirchardev2 \\ -device usb-redir,chardev=usbredirchardev2,id=usbredirdev2 \\ -chardev spicevmc,name=usbredir,id=usbredirchardev3 \\ -device usb-redir,chardev=usbredirchardev3,id=usbredirdev3 For USB3 support, the configuration can be simplified to: -device nec-usb-xhci,id=usb \\ -chardev spicevmc,name=usbredir,id=usbredirchardev1 \\ -device usb-redir,chardev=usbredirchardev1,id=usbredirdev1 \\ -chardev spicevmc,name=usbredir,id=usbredirchardev2 \\ -device usb-redir,chardev=usbredirchardev2,id=usbredirdev2 \\ -chardev spicevmc,name=usbredir,id=usbredirchardev3 \\ -device usb-redir,chardev=usbredirchardev3,id=usbredirdev3 Client Configuration The client needs to have support for USB redirection. In remote-viewer, you can select which USB devices to redirect under the \"File->USB device selection\" menu once the Spice connection is established. There are also various command line redirection options which are described below and when running remote-viewer with --help-spice. To get USB redirection working on Windows clients, you need to install UsbDk Filter String Format A filter is used to set blocking or autoconnect rules for USB devices. It consists of one or more rules, where each rule has the form of: ,,,, Use -1 for class/vendor/product/version to accept any value. And the rules themselves are concatenated like this: rule1|rule2|rule3 A client's default auto-connect filter string is 0x03,-1,-1,-1,0|-1,-1,-1,-1,1 This filters out HID (class 0x03) USB devices from auto connect and auto connects anything else. Note the explicit allow rule at the end, this is necessary since by default all devices without a matching filter rule will not auto-connect. Client Filtering Set a string specifying a filter to determine which USB devices, when plugged in, are allowed/blocked to auto-redirect USB traffic to the guest (client's window has to be in focus). remote-viewer --spice-usbredir-auto-redirect-filter=\"0x03,-1,-1,-1,0|-1,-1,-1,-1,1\" Set a string specifying a filter to determine which USB devices, that are already plugged in, to redirect on connect once Spice connection is established. remote-viewer --spice-usbredir-redirect-on-connect=\"0x03,-1,-1,-1,0|-1,-1,-1,-1,1\" Host Filtering Set a string specifying a filter to determine which USB devices are allowed/blocked to redirect USB traffic to the guest. Using QEMU -device usb-redir,filter='0x03:-1:-1:-1:0|-1:-1:-1:-1:1',chardev=usbredirchardev1,id=usbredirdev1 Note that in a QEMU command, the filter string should use a ':' character as a separator within the rule. Using libvirt ......... Download Latest versions: For use with qemu 1.3 and newer: usbredir-0.7.1.tar.bz2 For use with qemu 1.0 / 1.1 / 1.2 release: usbredir-0.4.4.tar.bz2 Older versions: 0.7, 0.6, 0.5.2, 0.5.1, 0.5, 0.4.3, 0.4.2, 0.4.1, 0.4, 0.3.3, 0.3.2, 0.3.1, 0.3 sourcecode You can find the official usbredir git repository here.",
    "commentLink": "https://news.ycombinator.com/item?id=40054580",
    "commentBody": "usbredir: A protocol for sending USB device traffic over a network connection (spice-space.org)187 points by sipofwater 16 hours agohidepastfavorite95 comments tombert 14 hours agoThere was a thing that was around 10 or 11 years ago called \"Wireless USB\", and it was actually kind of cool. It did exactly what it sounds like, you could plug in two different arbitrary USB devices into hubs or a computer that supported wireless USB, and the computer would just recognize it as a vanilla USB device. I don't actually know why it never caught on, I thought it was neat, and it seemed to work fine. I guess due to the popularity and ubiquity of bluetooth? https://en.wikipedia.org/wiki/Wireless_USB EDIT: Looks like it was more than 10 years ago, circa 2009 or so. Time has no meaning. reply codetrotter 14 hours agoparentFirst time I heard about Wireless USB was just the other day, in a video looking at a luggable computer from 2006. https://youtu.be/OO5hYhdxIuk Video is worth a watch although it also doesn’t give an answer for why Wireless USB actually disappeared. I was wondering after watching that video, if it could be due to security concerns? Like, is the Wireless USB protocol encrypted? And if so, does it use sufficiently strong encryption? I did find a document that talks a bit about Wireless USB encryption. https://cdn.teledynelecroy.com/files/appnotes/wireless_usb_e... reply tombert 14 hours agorootparentI actually saw that video, but I had heard of Wireless USB for awhile. My manager at my first job after dropping out of college the first time got it for his computer and he was super excited. At least according to Wikipedia, it was encrypted. reply lupusreal 14 hours agorootparentThere's \"encrypted\", and there's encrypted. WEP for wifi was encryption, but cracking it was so trivial that for some years it was practical to crack yourself access to wifi wherever you were on a casual whim. Still, even if the encryption was very weak, wireless wifi sounds appealing to me, at least for my old trusty wired mouse. Somebody snooping on or spoofing my mouse seems like an academic threat. reply gruez 13 hours agorootparent> Somebody snooping on or spoofing my mouse seems like an academic threat. You're not concerned that anyone within a 100 ft radius of you can inject arbitrary keystrokes/mouse movements to your PC? reply toast0 13 hours agorootparentAnyone within a 100 ft radius of my PC is in my house and could just come over and poke at the mouse. reply coderjames 10 hours agorootparentOr in the apartment or condominium above or below you across any of a few floors, depending on shielding. For those not living in standalone houses. reply chrsig 12 hours agorootparentprevThat's somehow both a much less elaborate and significantly more invasive approach to pranking wfh coworkers. reply codetrotter 11 hours agorootparent> wfh coworkers I usually call her my girlfriend, but I suppose that’s another way to refer to one’s significant other :p reply taneq 2 hours agorootparentprevI inadvertently pranked myself like this. For a while my laptop got super haunted, occasionally the cursor would jiggle slightly even when I wasn’t touching the mouse. Eventually I realised that the Bluetooth mouse in my bag which I’d totally forgotten about was getting bumped and turning on. reply lupusreal 13 hours agorootparentprevNo, definitely not. The intersection of people who would think to do that, have the skill for it, and the inclination to view me personally as their target is probably zero. reply tombert 12 hours agorootparentNot to mention that I don't think anyone but a few niche enthusiasts even have the hardware to do it even if they had the skills and inclination. Who's going to walk around with a 15 year old laptop brute-forcing wireless USB encryption, or find some obscure hub and do it on a modern laptop? I'm not going to say the likelihood is \"zero\", but I am going to say it's so close to zero that it's really not worth even considering. reply Terr_ 12 hours agorootparentprevImagine someone driving around neighborhoods with a laptop and a very good antenna. 1. Detect vulnerable networks in computers 2. Pretend to be a USB keyboard. 3. Trigger blind key combos that will visit a website, confirm downloading a file, execute that file, and \"OK\" privilege escalation prompts. 4. Move on to the next block or cul-de-sac, while the malware finishes unpacking and reporting home for further use. reply eichin 11 hours agorootparentThat doesn't need wireless USB, we already have CVE-2023-45866 https://news.ycombinator.com/item?id=38661182 for \"bluetooth stack more than 6 months old\" reply draugadrotten 1 hour agorootparentprevYeah, that has been a thing since the last millenium. https://en.wikipedia.org/wiki/Wardriving reply hex4def6 9 hours agoparentprevI remember actually testing that stuff out when it came out. I was working as an intern at Philips Semiconductors in their Wi-Fi chip division. Speaking of time has no meaning... :) I think there were a couple of consumer products that got released - Linksys and netgear perhaps? They consisted of a USB Hub + Dongle. They actually worked ok, but the speed would drop off quickly with distance. Across the room, and you'd be at 50% of the rated speed, at best. The technology was interesting. Basically they were transmitting over a whole slew of spectrum simultaneously (from like 2.5GHz to 5.5GHz), but they kept the transmit power low enough that it didn't exceed some FCC threshold. reply jsxlite 1 hour agoparentprevWhen Mac first took out the DVD drive, there was also a away to mount remote DVD drive. I guess the push was to get everyone on the network. So stuff like that just didn't take off. reply monocasa 5 hours agoparentprevWireless USB was also the format used by the Xbox 360 controllers. reply themoonisachees 3 hours agorootparentHuh. I had a 360 controller and a wireless adapter for my pc. It died a few years back though. reply taneq 44 minutes agorootparentprevDoes that mean they’ll work with any wireless USB adapter? I have a 360 controller but haven’t been able to source a USB dongle to use it with my computer. reply y04nn 13 hours agoparentprevI recently found out that a manufacturer (TI, NXP, murata, infineon?) was demonstrating a USB2 live camera capture using a UWB (Ultra-Wide-Band) short range wireless transmission. But I can't found the source again. reply dmead 4 hours agoparentprevI've been saying this should be a thing for decades. Not sure why I never heard of this. reply hbossy 13 hours agoparentprevMost USB devices either rely on the connection to provide power or are too clunky to move around, so having a wire doesn't really matter. reply paulkon 12 hours agoprevI'm running virtualhere on thousands of raspberry pi's sharing various USB devices to cloud machines over vpn. It's been working without issues for years now. Seems to be a solo developer in Australia that's been working on it for a really long time. https://www.virtualhere.com/ reply msbroadf 5 hours agoparentCorrect, i am a solo developer working on it since 2010 :) reply zbrozek 4 hours agorootparentI love your website. It's clear and to the point. I immediately knew what your product did; thank you. reply rawbot 3 hours agorootparentprevThanks for your work. I tried using it to use the official Gamecube Controller USB Adapter through Steam Link, but there was some spiking noise that killed playability. Nevertheless, I think it is amazing stuff. reply dmead 4 hours agorootparentprevThis sounds like a cool use case for my observatory control box. Do you ever have issues with latency pushing the bounds of the USB spec wrt latency? Can I use this with my camera? reply msbroadf 4 hours agorootparentI have a lot of astro users. But you need to use Ethernet for the connection between VirtualHere server and client and not wifi. A pi5 is very good for this. reply fasteo 2 hours agoparentprev>>> I'm running virtualhere on thousands of raspberry pi's Offtopic. Mind sharing what are you doing with thousands of pi's ? Thanks reply kotaKat 11 hours agoparentprev$49 and a byzantine per-server-cannot-be-transferred license is not palatable to me. I hate hardware-bound licensing. reply rrr_oh_man 7 hours agorootparentWould you rather like a monthly subscription per client? reply HeatrayEnjoyer 4 hours agorootparentThey didn't say anything about monthly subscriptions. reply brcmthrowaway 11 hours agoparentprevWhat possible use case is this for reply zymhan 9 hours agorootparent> This USB server solution is perfect for allowing USB devices to be used remotely over a LAN network, over the Internet, or in the Cloud without the USB device needing to be physically attached to remote client machine. reply scintill76 7 hours agorootparentWell, speaking for myself, I was curious why someone would need \"thousands\", and what the devices are. reply nicman23 4 hours agorootparentit is probably printers the horror reply numpad0 3 hours agorootparentprevVDI? reply bdittmer 5 hours agorootparentprevclickbot farm? reply theogravity 9 hours agoparentprev> thousands of raspberry pi's so you've spent 1000s * $49 for the license? reply rrr_oh_man 7 hours agorootparentWhy not? reply squarefoot 6 hours agorootparentPossibly because a developer hired to write something around usbip would cost a lot less. https://usbip.sourceforge.net/ reply dabber 3 hours agorootparent> Possibly because a developer hired to write something around usbip would cost a lot less. https://usbip.sourceforge.net/ Would it? For the sake of discussion, I'll assume \"thousands of raspberry pi's\" = 2,000 RBpis, or something around $10,000 in license fees. I don't know anything about either project beyond the links shared by you and the root comment, but based on the information at each link and the assumption of $10,000 spend: I would choose the one time cost of VirtualHere's purpetual update license and release cadence over a some short dev for hire contract to write some unmaintained wrapper code around a sourceforge library that hasn't been been touched in over a decade. reply pmontra 2 hours agorootparent$49 times 2,000 is $98,000, not around $10,000. Yet your argument still holds. There are many reasons for that. 1. You are paying a developer that works 100% on that, year after year, and not a hire that won't be there when something goes wrong in the future after an OS update, new hardware, anything. This is basically your argument. Let me add: 2. In some parts of the world far away from SV but still in the West, $100k are about two years of gross developer salary, not what the developer actually gets at the end of the month. Point 1 still holds. Where it's 10 years of salary maybe companies could be tempted by a custom solution. 3. You are giving $49 per server to that developer but you are probably getting more per server from your customers. If you have thousands of servers you probably have a viable business, so that's just yet another cost of doing business. reply beefnugs 3 hours agorootparentprevusbip has made me angry for 5 years now, there is supposedly an open source windows client, but you have to put windows into some unsafe bullshit mode to be able to use unsigned drivers?? So you have to compromise your entire system to use one program reply WhatIsDukkha 15 hours agoprevThe one hack I keep hopingwill do the actual work for is - redirecting my steamdeck control via usb to my linux gaming rig and expose it as a usb device(s) for steaminput. It seems like a natural and perhaps even \"straightforward\" hack but I've seen no evidence of one so far, perhaps there is something in usb that limits the ability to proxy it correctly. reply yjftsjthsd-h 14 hours agoparentTo clarify: You want the Steam Deck to run USB gadget mode and use that to expose its input devices to the other machine? If so... while searching to see if the SD supports gadget mode (answer: yes) I happened across https://github.com/Frederic98/GadgetDeck - have you tried that? reply WhatIsDukkha 14 hours agorootparentThis looks closer then anything I've seen so far, thanks! From what I can discern its missing important bits of the controller and doesn't have any steaminput profile etc so it's clunky still. Worth poking at though. edit - to clarify, ideally steaminput would be tricked on the host to think that it \"was\" a steamdeck so all the mapping features would be available. I don't really need another usb game controller its more the steamdeck touchpad etc. reply delecti 14 hours agoparentprevIt seems people have achieved that: https://www.reddit.com/r/SteamDeck/comments/v22ddf/guide_how... reply WhatIsDukkha 14 hours agorootparentThat's over wifi not usb. Even a bluetooth mouse has too much latency for many games. reply mike_d 11 hours agorootparentVirtualHere will run over any network connection, that user just happened to use wifi. All the mice I am aware of that use Bluetooth are travel mice and the like. Even cheap gaming mice use RF dongles that do not have the Bluetooth polling limitations. You can tell your friends in Fortnite you died because of mouse latency, but lets keep HN discussions grounded in reality please. reply ranger_danger 12 hours agorootparentprevDemonstrably false: https://www.rtings.com/assets/pages/46ieCfmI/mxvertical1-2gr... https://old.reddit.com/r/MouseReview/comments/hf1vrd/is_inpu... https://www.youtube.com/watch?v=orhb7Njj3h8 https://www.youtube.com/watch?v=yy0xmcBg_IY https://www.rtings.com/mouse/tests/control/latency reply delecti 12 hours agorootparentI didn't watch those youtube videos, but the text links either agree with the opinion that bluetooth is worse, or don't refute it. That rtings graph shows that bluetooth is pretty clearly worse in the majority of cases from wired or wireless (which is distinct from bluetooth). \"Bluetooth\" doesn't appear anywhere on that Reddit thread. And from the second rtings: > The mouse's connection type affects the click latency. Generally, wired mice have the lowest latency, and Bluetooth mice have the highest latency. A Bluetooth connection isn't recommended for gaming, but it's still good for office use, and most people won't notice any delay unless the latency is extremely high. reply ranger_danger 8 hours agorootparentIf you look at the graph again, not all of them are worse, which means it's obviously possible to make it on par or close to it. But regardless, the complaint was \"too much latency for many games\", which is not the same as \"worse\". And I don't think this amount of latency is too much for the vast majority of games. reply WhatIsDukkha 12 hours agorootparentprevFew/none of the wireless gaming mice are using bluetooth, its proprietary radio protocols and usb dongles. Click latency is not too useful vs swipe latency. Bluetooth has a pretty low polling rate iirc and that kills the swipe latency. More generally and responsive to what we were talking about, proprietary radios are not bluetooth and they are not WiFi which is the latency we are actually talking about (which is usable but not for me to play elden ring by direct experience). reply dingensundso 4 hours agoparentprevI don't own a steamdeck. But searching around the web a bit I found out that raw-gadget^1 was merged into the linux kernel in 5.7. And AFAIU that would enable proxying of USB devices. [1] https://github.com/xairy/raw-gadget reply all2 15 hours agoparentprevMaybe allow mounting of the the steamdeck input devices in steamdeck:/dev over network on your linux box so it is exposed as an additional device? Something like plan9 does. reply KeplerBoy 15 hours agoparentprevSounds like something Valve should make. Kinda weird they haven't so far. reply ShamelessC 15 hours agorootparentValve's \"remote play\" allows one to play games on another host computer via your Steam Deck as though you are plugged straight. If you don't need the video streaming, you can lower the settings. edit; oh nm, user wants a direct wired connection to work. reply jxy 15 hours agoparentprevit may not be practical given the possible high latency. reply crtasm 15 hours agorootparentover USB. what might introduce high latency? reply all2 15 hours agorootparentOh, oh, I get what you're saying. You want the steamdeck to expose its controls as a hotpluggable USB device on another machine. That would depend on the USB hardware on the device, I think. reply klinquist 5 hours agoprevIt may not be well known that VMWare Fusion supports this. I run Windows on a Mac Mini functioning as an ESXi server. From my Macbook Pro, I can connect to it with Fusion Pro and attach USB devices to the Windows VM. I use this to program ham radios and troubleshoot my vehicle with Toyota Techstream + USB OBD2 adapter. reply sandreas 14 hours agoprevSounds a bit like USB/IP (https://wiki.archlinux.org/title/USB/IP). Is this a new thing? reply eqvinox 12 hours agoparentBoth projects seem to be >10 years old at this point… might be a case of \"convergent evolution\"? reply ktm5j 11 hours agorootparentI'm on my phone so I can only do so much digging, but from the usbip sourceforge page that's linked above, it says that development has moved into the Linux kernel: For Linux, the source code of usbip was merged into the staging tree, and finally has been moved to the mainline since Linux-3.17. Development is ongoing in the kernel community, not here. Linux distributions will provide binary packages of usbip.* reply sandreas 12 hours agorootparentprevProbably... I thought of passing my iPod Nano 7g through USB/IP to my Proxmox Windows iTunes VM, but I never had the urge to do it. Although it'd probably work and would be great in combinaton with Wireguard on vacation, I did not want to setup a \"risky\" driver / kernel module on my main proxmox server :-) USB passthrough always was enough. reply kimown 6 hours agoprevhttps://www.usb-over-network.com/usb-over-network-download.h... I have used this software for adb debugging in rdp window10, it's really cool, but it don't meet all cases. reply 2024throwaway 10 hours agoprevThis would have been great for me to have around 25 years ago, when I wanted to mount a USB web cam in my bedroom window on the second floor and connect it to a computer in my basement. I was a dumb middle school kid and just spliced the usb wires onto Ethernet cable ends, and plugged them into the existing Ethernet run. That’s when I learned about maximum USB lengths the hard way, by frying some perfectly good hardware. reply dTal 10 hours agoparentI would not expect this hack to fry hardware - at worst, voltage drop, interference, signal distortion, and impedance mismatch will simply cause the device to not work. But I've successfully run USB 2.0 over 30ft cables despite it being illegal per spec, so your idea wasn't radically wrongheaded. \"Fried\" is an extreme result - are you sure you didn't just connect V+ and V- backwards or something? reply 2024throwaway 9 hours agorootparentIt being 25 years ago, I’m not sure of anything. But glad to know I’m not too far off or alone. reply worewood 9 hours agorootparentprevYeah, probably just jerry-rigged an extension and plugged V+ into a data line or something like that. Properly wired, it just wouldn't detect the device. reply tamimio 9 hours agoprevFunny how yesterday I was using Winding Sandbox to test a software and wanted to connect an iPhone to it, problem was that there’s not interface like a fully fledged VM and had to use VMware instead, maybe this will do the trick? reply naikrovek 7 hours agoparentUSB/IP works fine on Windows, and there are a couple of good open source packages for it. It works well. reply hsbauauvhabzb 7 hours agoprevWhat’s input latency and throughput on this like? Would it support near native keyboard / mouse, webcam or display output? What about mass storage devices? reply MisterTea 15 hours agoprevOn Plan 9 I just rimport the remote machines /dev/usb. Since this is all over 9P it can go over any 2-way pipe, even rs232. reply all2 15 hours agoparentAnd, the linux kernel has plan 9 FS drivers. [0] [0] https://www.kernel.org/doc/html/latest/filesystems/9p.html reply alchemist1e9 14 hours agoparentprevIs Plan 9 alive and kicking? reply yjftsjthsd-h 14 hours agorootparentPlan 9 is like BSD; the original is no longer developed, but its forks ( https://en.wikipedia.org/wiki/Plan_9_from_Bell_Labs#Derivati... ) are healthy enough. reply rcarmo 14 hours agorootparentprevDepends. There are still many enthusiasts (even new ports to random chipsets), but there is also a bit of a schism between the 9front folk and… let’s say our current timeline. The good news is that there are interesting things going on, fortunately none of which related to current tech trends. reply MisterTea 12 hours agorootparent> but there is also a bit of a schism between the 9front folk and… let’s say our current timeline. As a 9front user who attended IWP9 the past two years (just was in Philly this past weekend) this really isn't the case anymore. reply MisterTea 14 hours agorootparentprevhttp://9front.org/releases/ reply dotnet00 10 hours agoprevDoesn't WSL2 use something very similar for forwarding usb devices from Windows to the Linux VM? Through \"usbipd\" reply naikrovek 15 hours agoprevDoes this offer anything over USB/IP? reply k8svet 15 hours agoparentI think just that it's plugged in at the SPICE level. So I guess you could redirect a USB device into a guest that isn't running Linux and can't run usb/ip? But also, this is how usb redirection works with virt-viewer/virt-manager, I'm fairly sure. reply touisteur 13 hours agorootparentNow I'm wondering whether someone has built support for this in rust-vmm... reply naikrovek 13 hours agorootparentprevUSB/IP works fine on Windows and MacOS, though MacOS has no ability to use devices shared by USB/IP; it can only share devices to others. What is this “SPICE level” you mentioned? reply touisteur 13 hours agorootparenthttps://www.spice-space.org/spice-user-manual.html that qemu uses a lot. reply k8svet 13 hours agorootparentprevImagine you had a qemu/libvirt guest that understands USB, but doesn't have support for USB/IP in \"kernel/userspace\". usbredir gets your device to qemu, looking like it's a USB device attached to the VM, without cooperation from the guest. reply naikrovek 7 hours agorootparent“Without cooperation from the guest” is the bit of info I was missing. Thank you. reply c_o_n_v_e_x 10 hours agoprevSo basically this a USB device server but you can BYO hardware. Use cases for USB device servers (according to https://www.seh-technology.com/products/usb-deviceserver.htm...): external disks dongles card readers telephone systems barcode scanners medical devices mobile gauges output devices audio/video streaming devices scanners etc. ...connecting a computer to a USB device that is further away than what USB is physically limited to (distance wise). reply j45 6 hours agoprevA software based Tibbo for usb, awesome! https://tibbo.com/ reply andrewmcwatters 13 hours agoprevRemember Remote Disc from Mac OS X? https://support.apple.com/en-us/101323 reply lynx23 4 hours agoprevusbip is a thing, is even a Debian package... reply sipofwater 16 hours agoprev [2 more] [flagged] 2OEH8eoCRo0 14 hours agoparent [–] Nice bot account. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "usbredir is a network protocol designed to transmit USB device data over a network connection, featuring a parsing library, usbredirhost library, and protocol implementation utilities.",
      "The protocol is spice-independent and compatible with various virtual machine platforms, offering configuration instructions for virtual machines, libvirt, QEMU, client setup, and USB device filtering.",
      "Versions of usbredir tailored for various QEMU releases are accessible for download to enable users to leverage USB devices over network connections efficiently."
    ],
    "commentSummary": [
      "The discussion delves into USB redirection protocols like usbredir, Wireless USB security risks, and VirtualHere for network USB device sharing.",
      "Topics also include hacking the Steam Deck for gaming, low latency mice and Bluetooth for gaming, USB/IP for device sharing, and Plan 9 compatibility with USB devices.",
      "Users contribute experiences and insights on USB connectivity, device compatibility, and remote device use through USB server solutions."
    ],
    "points": 187,
    "commentCount": 95,
    "retryCount": 0,
    "time": 1713287639
  },
  {
    "id": 40051818,
    "title": "Achieving Strong Product-Market Fit for B2B Startups",
    "originLink": "https://pmf.firstround.com/levels",
    "originBody": "Preface Levels of PMF Level 1: Nascent Level 2: Developing Level 3: Strong Level 4: Extreme Session 01 LEVELS OF PMF Product-Market Fit Isn’t a Black Box — A New Framework to Help B2B Founders Find It, Faster Most people describe finding product-market fit as an art, not a science. But when it comes to sales-led B2B startups, we’ve reverse engineered a method to increase the odds of unlocking it. We’ve worked with some of the world’s most iconic enterprise founders and distilled what they did in their first six months into a series of tactical sessions for taking a straighter path to PMF. Today, we’re open sourcing the first session, which shares how we’ve broken PMF down to an incredibly granular level in order to help more builders get there faster. Even though finding product-market fit is the single most important objective any company must achieve, it’s still a black box for founders — seemingly requiring some mysterious combination of luck, market timing, and sheer grit. While much has been written on the topic, no one has broken down the inputs, leading indicators and tactical steps that could increase the odds of finding PMF. In our view, one of the key reasons this topic hasn’t been successfully “framework-ified” is that most folks still talk about the concept in generalities. Fish are jumping into the boat. Leads are falling on the floor. You’re chasing — not pushing — a boulder. The product is being pulled out of your hands. You’ll know it when you see it. You can feel when it’s not happening. These descriptions lack the specificity that we see in other areas of company building. Another issue? Most resources target a wide range of builders — which results in watered-down advice that’s more hand-wavy than tactical. Founders are left with approaches that are neither replicable nor practical, ranging from unhelpful “We kind of just started going viral” founding stories to one-size-fits-all frameworks that don’t address the nuances of their specific business. While the hunt for PMF can be more of a dice game with poor odds for, say, consumer apps, our experience has taught us that for sales-led B2B companies, you can reduce the role of luck. Here’s how we’ve developed that conviction: As a firm of former founders and startup builders, we’re coming up on our 20th year of investing in pre-product-market fit startups. Along with Square, Notion, Looker, and Verkada, we’ve backed hundreds of companies when they were just a couple of founders and an idea. Drawing on our own data and observations from more than 500 investments, we’ve spent the last several years breaking down the decade-long project of building a category-defining B2B company into more concrete steps — in hopes that we could create this resource that we feel is missing from the ecosystem. To put it even more plainly: Despite all the talk about product-market fit, it didn’t seem like anyone was meaningfully putting the pieces together to help B2B founders increase their odds of finding it. We wanted to change that. In addition to distilling lessons from decades of our own experience, we’ve layered on hundreds of hours of research and interviewed dozens of founders who’ve gotten further on this journey than most, from both within and outside of our community. (If you’re a regular reader of The Review, you may have noticed that we’ve started publishing some of these stories in our series Paths to PMF — focusing on the down-in-the-weeds details of how companies from Vanta to Vercel took off.) As the culmination of that work, today we’re proud to unveil Product-Market Fit Method, an intensive 14-week experience designed to help exceptional pre-seed founders build epic B2B SaaS companies. From proven tactics to detailed benchmarks that help founders gauge progress, we’ve mined the secrets that are buried in the best B2B builders’ experiences to create a new methodology that helps startups take a straighter path to PMF. The program (which kicks off on May 29 and can be applied for here) consists of eight tactical sessions. We’ll cover everything from validating your product insight and market, to approaching positioning, design partners, and product iteration, to learning the ropes of what we call “dollar-driven discovery” and founder-led sales. While this is currently a highly-curated program with a limited number of spots, our tradition here at First Round has been to open-source everything we can to share what we’ve learned with the broader startup ecosystem — whether it’s a decade of advice on The Review or years spent building communities like Angel Track. In keeping with that spirit, we’ve decided to publish the content of the first session in full, as a preview. No commitment to applying. No email paywall (although of course, we’d highly recommend subscribing to The Review newsletter). No charge. And no equity required for the program either — our aim is to increase the odds that more founders find product-market fit, whether they’re backed by First Round or not. The deck may seem stacked, but we're on a mission to rebalance the odds in the early founder's favor. We’re firm believers that you don’t have to go at it alone — and you don’t have to make it all up as you go either. We’ve taken a systematic approach to defining what product-market fit is and identifying truly tactical steps for how to go about finding it. But to be very clear, the first session that we’re open-sourcing here is all about exploring the former, breaking down the elements of PMF so it’s easier to unlock. The rest of the sessions in PMF Method tackle the latter in much greater detail — with expert-led workshops, practical exercises and templates, actual customer introductions, and unique access to our network of established founders and angel investors. With that, let’s dive in. Levels of PMF INTRODUCING EXTREME PRODUCT-MARKET FIT Let’s start with the obvious question: What is product-market fit? Here’s our specific definition of what founders are aiming for — and why it’s a bit different from what you may have seen before: Extreme product-market fit is a state of widespread demand for a product that satisfies a critical need and — crucially — can be delivered repeatably and efficiently to each customer. As we’ve noted, the way most people have talked about product-market fit feels insufficient. More specifically, there are certain nuances that we feel haven't yet been adequately addressed: Product-market fit progresses in levels. It’s not a binary state where you have it and you’re golden, or you don’t and you’re not on the path. But it’s also not a vague spectrum or fuzzy sliding scale, where you can’t be sure where you’re at. There are a few dimensions of PMF that are in tension with each other, requiring deliberate tradeoffs at each level. There isn’t just one element to emphasize above all else. And importantly, there are different times in a company’s lifespan where founders need to intentionally prioritize one over another. There isn’t nearly enough emphasis on repeatability. As you move your way up through the different levels of product-market fit, what you’re really unlocking is repeatability — across demand generation, product development, customer satisfaction and unit economics. A startup hasn’t reached the upper levels of product-market fit until it has developed a fine-grained understanding of who the right customer is, how to land them, and how to deliver a product that (in most cases) isn’t bespoke for each customer yet consistently solves a significant need. In a nutshell, you don’t have to work as hard to find what we’re calling your “marginal customer” (more on that in the section below). Repeatability is the holy grail on the hunt for product-market fit. Without any patterns, it’s nearly impossible to generate early momentum and plot a course forward. That’s why when we picture product-market fit, we see three distinct building blocks: Four Levels: Nascent, Developing, Strong, Extreme Product-Market Fit Three Dimensions: Satisfaction, Demand, Efficiency Levers: Change your persona, your problem, your promise, or your product (our version of the 4Ps) to make the work of finding, winning, serving and retaining your marginal customer easier. To be clear, pieces of this approach may exist here or there in different frameworks — after all, all great ideas are built on what's come before. But our hope is that we're bringing things together and diving deeper into the details in a way that's unique and valuable — plus we’ll take the added step of sharing how a company, Looker, made its way to product-market fit, sharing its actual data across the years. Let’s dig into each one. The 4 Levels of Product-Market Fit We’ve identified four concrete levels of PMF, with what we’re calling extreme product-market fit as the ultimate goal — what’s required to build a generational company. Rather than a blanket “you’ll know it when you have it,” we’ve found that for enterprise companies there's a clear progression that companies go through that is fairly predictable. Many founders have walked this path, and based on what we’ve seen from the hundreds of early-stage startups we’ve backed, this journey to extreme product-market fit usually takes anywhere from two to six years. Our contention is that product market fit is not binary — and it's not something that you get to overnight, despite what all of those “we just went viral” founding stories may have you believe. Once in a while, a company shoots up the ladder quickly, but that tends to be the exception. Some companies move up the progression over a longer period of time, while others skip ahead at different moments, leapfrogging from nascent to strong, for example. In our experience, there are a couple of specific timing patterns here that companies fall into, whether it’s making their way through Levels 1-3 quickly but stalling out and never quite hitting Level 4, or reaching Level 1 in record time but then getting trapped at Level 2. We’ll also note that the journey to PMF is a two-way ladder, meaning levels aren’t permanently unlocked — it is possible to regress. To use another metaphor, think of it more as a dynamic equilibrium. Just like in chemistry, your actions, your competitors’ actions, customer expectations, technology shifts and other factors are all at a tentative equilibrium state. If one of those elements changes — maybe your competitor makes a huge leap forward — and you don’t adjust, you may not be at the same level of PMF anymore. (For another take on this topic, we recommend three-time founder Bob Moore’s framework here.) We’ll get deep into the details of what progressing through each level feels like further down, but staying at our current altitude, there’s one high-level question you can ask here if you’re unsure where your company stacks up: How hard is it to find and serve your next “marginal” customer? As you move up, it becomes easier and easier, and increasingly more repeatable. At Levels 3 and 4, for example, demand swells. Contrast that with Levels 1 and 2, where the search for the marginal customer feels more like a battle on several fronts — finding the right person, convincing them that your solution is what they need, converting them, and turning them into satisfied customers. The 3 Dimensions of Product-Market Fit We like to break that definition down even further into what we call the three dimensions of product-market fit: Satisfaction: How happy are customers with the product? Are you able to effectively retain customers? How much do people need the product? It may seem like splitting hairs, but we’ll note that we’ve intentionally used “need” and not “love” here — there are many companies that have extreme PMF without customer love. (Salesforce is an example of a product that’s perhaps not beloved, but certainly satisfies a critical customer need.) Demand: Do you have the proverbial “lines out the door”? How quickly and how much can you sell your product for? Efficiency: Can the product grow repeatably and scale effectively? In our view, this is the critical piece that many definitions of PMF gloss over — even though it’s one of the most important things that a company must achieve. Here’s how we like to break it down: efficiency in finding the customer (CAC), selling and closing the customer (sales conversion, magic number, CAC payback), in activating and supporting the customer (length of integration cycle, NRR), in producing the product (COGS, gross margin), and across the company as a whole (burn multiple). Some may bristle at that last one. Most other definitions of product-market fit overtly focus on satisfaction and demand, implying that efficiency takes care of itself in the long run. But our belief is that while your business won’t be efficient from day one — and may in fact be intentionally inefficient while you work to figure things out at Level 1 — there is tremendous value in building with an eye towards efficiency throughout your entire journey as a founder. As a thought exercise, say you open up a stand on the side of the road where anyone who lines up and gives you $1 gets $100 in return. By classic definitions, product-market fit would be off the charts. You'd have lines out the door. Customers would be very satisfied. But it would be a mirage. And a particularly dangerous one at that, given the risk that you’ll lull yourself into the false belief that you’ve unlocked PMF. When you’re starting a marathon, it’s helpful to know what the final mile needs to look like — even if you don’t alter your approach to mile 1. If you keep efficiency in the back of your mind even at the earlier stages, you’ll be able to stay intentional in the tradeoffs you’re making as you build. One final note here: These dimensions are actually highly intertwined — and require distinct tradeoffs in different scenarios. For example, you can take on several initiatives to increase efficiency — spending less time doing unscalable things, automating and operationalizing more aspects of your business — but in many instances, that harms the customer experience, reducing satisfaction. (Of course, there are times where there’s more harmony than discord — for example, shipping a critical product feature might improve all three at once.) The First Round 4Ps: Levers to Get Unstuck on Your Path to Product-Market Fit As for how you can get unstuck and catapult your company to the next level, you can experiment with pulling these four levers at various points along your journey (our spin on the classic 4Ps framework): The persona: Who would benefit most from your insight? Do you have a person in a specific type of role or function who faces a set of challenges in mind? In this context, the persona can be the buyer (CTO or CRO), the company type (e.g. Fortune 10,000 financial services) or both. As you look to make adjustments here, consider whether it’s too narrow, too broad — or needs to be abandoned altogether. Example from Plaid: Consumers didn’t need another budgeting app, but fintech apps needed the integrations the team had built on the back-end. The problem: Is this an urgent and important problem that your target persona has? It’s the classic painkiller versus vitamin question. You’re looking to solve a problem that is urgent and, if solved, will provide huge relief to your potential customers. If not, you may need to explore adjacent problems for your persona, or rethink your persona and problem entirely. Example from Lattice: Stayed focused on people leaders, but went from building a solution for OKRs to solving performance management. The promise: Are those customers interested in what you’ll do to address that need — your unique value proposition? It’s easy to confuse this for your actual product, but the promise is how you communicate the benefit your product will deliver. In our experience, this is the lever that is most overlooked. Example from Ironclad: Repositioned from an AI legal assistant (that promised to automate lawyers’ tasks) to a Contract Lifecycle Management platform (that promised to help enterprise companies create and manage legal contracts end-to-end) in order to play in an existing category. The product: Will the product that you’re building actually deliver on this promise? Are customers interested in this particular solution to their problem? Would they pay for it? Example from Alma: Pivoted during the pandemic from a community-based physical office concept for therapists, to giving providers a suite of digital tools to build thriving private practices. For example, a startup might be stuck at developing PMF, but later find that pivoting to a new buyer is the move that unlocks the next level. Or another company may have stumbled onto nascent PMF with resonant positioning, but then can’t ship the correct product that delivers on its promise. These levers, of course, fall more into the camp of how to go about finding PMF, which is not the focus of this essay. (But in PMF Method, we have a full session dedicated to each of these levers, filled with tactics, real-world examples, and hands-on exercises to better guide you.) What Extreme Product-Market Fit Looks Like in the Real World: To bring this new concept of extreme product-market fit to life, let’s walk through an example from Looker when it reached this Level 4 milestone as a Series C company with a team of 235 at the end of 2016 — after five years of hard work progressing through Levels 1-3. (You’ll notice how the curve starts to inflect right around this timeframe in the chart below, taken from the company’s April 2018 board deck.) Here’s how Looker checked each box on our list of the three dimensions at this time: Demand: Went from 450 to nearly 800 customers in a year, growing revenue from $11.5M to $27M and increasing ACV to $57.7K. Satisfaction: Extremely happy customers, with 141% NRR, an 18 month-streak of net negative churn and renewals higher than plan. Efficient: The company had built an efficient model, with 77.6% gross margin. Looker’s path is a great example of each dimension coming together to contribute to extreme product-market fit (and we’ll be sharing more of it in the pages below). But just as there are success stories like these, in our experience, the absence of one or more of these dimensions is also the biggest reason that startups fail. We believe that if you think about PMF in these terms, you'll be more likely to end up at strong or extreme levels of PMF than all the companies that didn't make it — in part because you and your co-founders will have a shared framework to more precisely diagnose where you might be falling short. To help your startup achieve that feat, we’ll dive into each of these levels and unpack them with more detail. In the sections that follow, we’ll walk you through what it feels like to be at each one, the most important thing to focus on, and thought starters from fellow founders on what you might try to level up. In addition to benchmarks based on data we’ve gathered and insights from established B2B founders who are where you want to be, we’ve also drawn from the hundreds of companies we’ve worked with to create composite case studies of fictitious startups that bring each level to life. We’ll also return to our example of how Looker actually progressed through every level and eventually reached extreme product-market fit. Level 1 Nascent WATCH (7 MIN)•JASON BOEHMIG ON IRONCLAD’S PATH TO PMF At this stage, a founder’s primary job is to find a problem worth solving for three to five customers. And by problem we mean an issue that is urgent and important for them, where solving it would be immensely valuable for their business. With that in sight, it’s time to build a product that solves the problem and delivers high satisfaction for those customers. You may need to iterate across our 4Ps (persona, problem, promise, product) a few times to land on the right combination. And while it’s not the primary focus of Level 1, you should also be thinking about demand here, exploring whether you’ll be able to find enough customers with that problem who will need your product. While efficiency isn't something to spend much time on yet, we’d recommend trying to start constructing a credible argument in your head about how the business could eventually scale efficiently. In our experience, we’ve found that a fairly high percentage of startups get stuck at Level 1 — and stay stuck for six, 12, or even 18 months. That's when we’ve seen founders get bogged down in “the grind.” That’s because the hallmark of this level is that it lacks repeatability — that’s to be expected at Level 1. Finding the marginal customer is the opposite of easy. At this point, you’re not entirely clear on what a customer needs to look like in order to be the right fit for what you’re building. And even when you land on a rough hypothesis, acquisition is usually unscalable. Then when you do get in front of a prospect, your messages don’t always land — some customers immediately “get it,” while for others it doesn’t resonate. Each customer’s needs (and thus the solutions that you build) are slightly different. Some come in asking for a slew of additional features and functionality to meet their requirements, while others see an immediate impact even with an early version of the product. What is Feels Like As we talked to sales-led B2B founders about how they navigated this first phase, unsurprisingly, several different paths emerged. Some grappled with finding the right product, others struggled to identify the target user, still others doubted the size of the market or even abandoned their original idea entirely. The common thread was wasted cycles before they stumbled upon those first few sparks of traction — which were more in the vein of “aha moments” or “this might just work” inklings, rather than hard metrics and the beginning of a hockey stick growth curve. Here are three specific examples from Ironclad, Verkada and Plaid on what that early traction felt like (after a winding path to getting there): Ironclad: Unlocking a more specific product by visiting customers IRL Ironclad is now a well-known digital contracting platform (last valued at $3.2B), but unpacking their journey to nascent product-market fit highlights just how much manual, non-scalable work you’ll find at this level. After serving as outside counsel for startups, co-founder Jason Boehmig started tinkering with automating parts of his own practice with coding that he had taught himself along the way. After realizing that there was a gap both in new solutions for lawyers and a team that understood how they actually used software products, he decided to leave his law firm life behind. “It’s wild to think back to this now that we're in the AI assistant boom because the first version of Ironclad was an automated assistant for legal paperwork — but it was really me on the backend of an email address,” Boehmig says. “We started by doing everything from corporate filings to NDAs and everything in between. So if you were doing an NDA, you would CC admin@ironclad.ai and be like, ‘Hey, I'm doing an NDA with Joe Smith. Can you help us get it done?’ And I would send a series of responses as the Ironclad AI such as, ‘Hey, I need the following information,’ or ‘Here's the template. I filled it in.’ I later met my co-founder, Cai, who was an amazing engineer, and he’d watch me work as the assistant and then would automate what could be automated away.” But it was spending time in person with their initial customers that helped narrow their product’s focus. “I remember that we flew out the whole company — which was four or five people at the time —to Boulder to see a customer who had a bunch of product feedback for us. We all just sat around and watched them use the product,” Boehmig says. “They had two monitors, one showing their email and on the other was Ironclad. They were just doing their day-to-day routine, working on a legal team. And by observing them and seeing how important these routine, repeatable transactions were to their profession, we realized we had to go all in on these corporate legal teams — particularly legal operations, which was a role that was just getting defined back in 2015.” \"At Ironclad, the early ‘aha moments’ came from watching our customers use the product. It enabled us to pick a niche audience that we could go after — legal operations — and the specific task they were doing — routine transactions.\" Jason Boehmig• Ironclad WATCH (7 MIN)•FILIP KALISZAN ON VERKADA’S PATH TO PMF Verkada: Going unreasonably deep and moving fast to wow customers Verkada has six different physical security product lines today, but the founders started back in 2016 with video security. Inspired by IoT devices in the consumer space, they realized the enterprise equivalent of applying software to video security was missing. When co-founder Filip Kaliszan thinks back to the nascent stage of their journey to PMF, the team’s speed and willingness to get in the field stand out. “The very first version looked very funny. It was a Raspberry Pi based camera. We just hacked it together from different parts — the supply chain was Amazon.com, We built maybe 100 of these little cameras and we gave them to friends and businesses we knew,” says Kaliszan. “The whole idea was to just learn what it takes to build the software and build out the solution. The prototype was just about learning if the technology would work and convincing ourselves that there was a there there.” What struck us about Verkada’s approach here was how hands on it was — and how quickly they moved to extract those insights. “For the first few customers, we just did whatever it took to get the customer deployed as fast as possible and then stayed close to that customer to learn what was working for them and what wasn’t. That approach has been extremely informative in shaping everything we build in our products,” he says. Take this example: “We were very aggressive with sending trial units. The moment we had a positive signal from the customer, we would get them a camera. We made sure that the process was seamless, fast, and easy from the very beginning. If you talked to us today, 24 hours later you’d have a nice box of kit to try it out,” Kaliszan says. “We were also going on site and doing a lot of installs for customers. This is before we had a partner ecosystem, before we had installers who wanted to work with us. I remember flying to L.A. to install cameras at the Beverly Hills Equinox, one of our early customers. The gym closed at midnight, and we had until 5:00 AM to get the cameras installed. So I'm buying some drills, getting up on a ladder and pulling cables myself. We were making sure whatever needed to get done got done so that the product worked.” That commitment paid off. “We spent two years building, so 2018 was our first year of sales. In the first couple of months of that year, we got enough pull and demand on the camera front where it was that validation ‘aha’ moment,” Kaliszan says. “We sort of always thought the idea was cool, but that was that moment that really crystallized that what we had built was beginning to work for the customers.” \"When you have any signs of early demand pull, you have to follow it and work as hard as it takes to just make it happen. \" Filip Kaliszan• Verkada WATCH (7 MIN)•ZACH PERRET ON PLAID’S PATH TO PMF Plaid: Pivoting and building conviction in an unproven market But even with early customer traction, it can still feel like a slog. Plaid is now known as the pioneer of the “invisible plumbing” fintech infrastructure that powers everything from paying your friends back on Venmo, to making an investment on Robinhood, to buying Bitcoin on Coinbase. But the product actually started out as a consumer budgeting app. “The response of consumers was such that we pretty quickly realized we weren't going to be any good at building budgeting apps. We tried for a while. We built six different versions of budgeting management, analyzing your spend types of applications and they didn't really ever get any traction,” says co-founder Zach Perret. “One day, a friend who was one of the early engineers at Venmo came to us and said, ‘Hey, your consumer products are not so good, but I'd like to license the backend that you've built, the way that you've integrated with the banks in order to get the data into your app.’ It took awhile, but eventually we decided to make this pivot and shift over to building the platform,” he says. “We then had five or seven people who said that they would probably use the product if we built it. We even had one who said they would pay us if we built it. And so it was pretty easy to find nascent product-market fit in that very early stage once we landed on the right idea,” says Perret. “The bigger challenge for Plaid was that the market didn't really exist. So while we had these five or seven companies that wanted us to build something, the real challenge was market development. That was the reason that almost every VC passed on investing in Plaid in the early stages. They said, ‘You've built an interesting product, it could be useful for some customers, but the market's not very big.’” \"When I think back to the earliest stage of Plaid’s product-market fit journey, the challenge wasn't finding a product that people would buy. It was creating a market around the product that we believed would be very important for the future, but was yet to be proven.\" Zach Perret• Plaid “Convincing ourselves and everyone else that there was a real market here — that financial services space would eventually have startups in it — that was the hardest thing we had to do in the earliest phase,” says Perret. What it Looks Like: A Case Study on Nascent Demand, Satisfaction, Efficiency in Action To return to the very beginning of our example from Looker’s actual trajectory, the data platform startup spent roughly from August 2011 to March 2013 in the nascent stage. “We waited almost a year to raise money, until we knew it was a venture business — not every startup is, and you don’t want to get locked in. We were cranking along with customers and revenue. And it wasn’t all nailed down, but we had figured out enough of how go-to-market might work to know that it was workable,” says Looker co-founder Lloyd Tabb. Here’s how the business looked at the nascent stage: With that early glimmer of traction, Looker raised a $2 million seed round from First Round and PivotNorth in August 2012. By the end of Q1 of 2013, the 9-person team was at $357K in ARR with 15 customers. (Incidentally, five of Looker’s first six customers were First Round companies.) Outside of warm intros, one tactic the team found effective early on was customer events. “We started doing ‘Look&Tell’ customer events super early on compared to other startups. I remember sweating and dragging cases of beer down Market Street to get to an event we were hosting — I think we probably had about eight attendees, but we kept at it and they paid off eventually,” says Tabb’s co-founder Ben Porterfield. But to show an even more detailed snapshot of what “good” looks like at this level, we present a case study on our first fictitious startup: Ledgerly, a platform that uses LLMs to help accounting teams — complete with very detailed qualitative descriptions and metrics. The Ledgerly team is pretty new, with just five full-time employees. They raised a $1.2 million pre-seed round earlier this year and they don't even know what their burn multiple is — but that’s appropriate, given how early they are. Let’s walk through each of the dimensions of product-market fit: Demand: They only have four customers, all of which they acquired through warm intros from their personal networks or from investors. They don't have an outbound sales machine yet. Three of those customers are paying $15K each, putting them at $45K ARR. (The fourth one is a design partner and isn’t paying yet.) But what you don’t see in these stats is that they actually had to connect with 46 different companies to land these four customers. That’s fairly typical. In the early stages, you're going to talk to a ton of different prospects as you try to figure out the following: who are the customers that are right for you, and who are the customers with a problem that you can solve with a single product. The Ledgerly team doesn’t have any renewals yet since they're so early that none of their customers have hit the one-year mark. They did have one design partner churn due to lack of fit, but they don't consider that regretted churn. Satisfaction: All of their customers are starting to use the product regularly, at least once a week. Two of them have onboarded multiple users internally. Ledgerly is getting good feedback from the three customers they’ve asked, so they’re starting to see signs of satisfaction, but it's early and anecdotal at this point. Efficiency: This is the area the team has paid the least attention to, which is appropriate given their stage. Customer onboarding feels too lengthy (currently at eight weeks) and it takes them a lot of time to find qualified leads for new customers (at least five meetings to just find one prospective — not even signed — customer). What’s more is that customers are making a lot of special requests, which is a common feature of the nascent stage. Four customers making four different sets of requests is pulling the team in different directions, but that’s not unusual, given that the team is still zeroing in on the shape of the product and their ideal customer. Benchmarks: While the above hopefully gives you a sense of what nascent product-market fit feels like, we’ve also collected these benchmarks that indicate your company is at Level 1 so you can more closely compare your progress. You’ll notice that repeatability is nowhere in sight — and that some of these metrics that we would take very seriously for later-stage companies don't even apply yet. Your team is less than 10 people. You’re at the pre-seed or seed-stage. Demand source is mostly friends and network, some cold outreach. Sales conversions: 1 out of every 10-20 warm intros converts to a customer. 3-5 customers. $0-$500K ARR. No renewals yet (too early). No regretted churn (some non-regretted). You don’t even know what your gross margin and burn multiple are ( for each). What Matters Most: But in addition to knowing how they stack up, founders also need to focus on what matters most in terms of getting to the next level of product-market fit. To reach Level 2, founders need to focus on satisfaction most. In the case of our fictitious startup, Ledgerly, in order to move the needle on satisfaction, their goal is to increase daily use and number of users at each company and start sending NPS surveys as a regular part of customer interactions. But don’t focus solely on these satisfaction metrics here — staying outcome-oriented is key. This simple rule of thumb is a great reminder: Are you moving the business outcome you promised to move for your customer, and by how much? With satisfaction front and center, demand is second at this stage. While you don't need heaps of customers, you do need an inkling that if you make this small set of customers happy, eventually there will be that proverbial line out the door. For Ledgerly, this means setting a target of landing two or three more customers in the next few months. Efficiency comes in last at this stage. We’d recommend that you don't spend a ton of time thinking about it at Level 1. For Ledgerly, its planned optimizations are in service of improving satisfaction and demand. Their goal is to trim customer onboarding down to four weeks and create a faster, more repeatable sales process to improve conversion. To reach developing product-market fit, you have to build something that at least a handful of customers really, really need. It solves a problem for them. They can't live without it. And that requires earning a very high level of satisfaction first. WATCH (5 MIN)•LLOYD TABB ON LOOKER’S PATH TO PMF Sticking close to your customers early on is key to cultivating that satisfaction. But there’s an important distinction here. “If you're not giving them value, they don't want to be near you,” says Looker’s Lloyd Tabb. “How many surveys do you get as a customer yourself? I just went and had my car repaired and I got a survey, which isn’t going to give me any value. You need to bring them something of value — otherwise you’re just annoying them,” he says. “For Looker, the search for product-market fit was about figuring out how to deliver the value. For our first four customers, I had different ways of delivering what I was building, from pure consulting to a standalone product. But the problem with just giving someone the software was that they didn't get as much value out of it. I realized I needed to be teaching how to do data as well as delivering a product,” says Tabb. “What worked best was part product, part service. We used the demo as a chance to build a proof of concept, so we didn’t have a dummy sales pitch version — we always asked the prospect for an actual dataset to play with. Then it was a very quick forward-deploy. We would come in and do a free trial where we would set up the software and teach them how to use it. And then we would watch for engagement,” he says. “Only when there was engagement would we close the deal. We had almost no early churn because we only sold customers who got the value out of it.” As we’ve noted, inefficiency is expected at Level 1. “The downside was that some of the trials ran a really long time. If we didn’t see that momentum, I’d go back in and be more hands-on to make sure they understood how we could answer all of their data questions,” he says. \"I had no qualms about sinking a disproportionate amount of resources here. In SaaS, focusing on making your customer successful is a retention strategy, not a cost center. Complicated products require education. If you're selling a complicated product, you are in the education business — so the support you provide is everything.\" Lloyd Tabb• Looker Yellow Flags to Watch Out for: It’s hard to tell in the moment if you’re mistaking a more tepid response for early traction, or if an extra dose of founder grit will unlock the breakthrough your business needs. Knowing when to keep forging ahead versus when to pull back and pivot is one of the trickiest parts of Level 1. Persona founder Rick Song had a thoughtful take here on why startups tend to get bogged down at Level 1. “The biggest challenge for a lot of early-stage startups is that they lie to themselves about customer satisfaction. It’s a common trap at this stage to have customers who like but don't love or need your product. I use the relationship analogy a lot internally at Persona: knowing when someone likes but doesn’t love something is very, very difficult but if you’ve ever been in the friend zone, you know that answer,” he says. To figure out if he was being “friend-zoned” in the early years of Persona, Song sought to build trusted relationships with a handful of his early customers. He talked to them all the time, texted them, grabbed coffee with them, and made sure he was very candid and self-deprecating to get that real feedback. Song would ask questions like the ones below over and over until he got strong enough answers that convinced him they were on the right track: If a competitor came along and charged you 50% less than what I'm charging you, would you switch? If we increase the price, is that something that you're going to have a lot of pushback around? Is Persona a game changer for your company? If Persona went away, how big a deal would that be for you? You can’t be in the friend zone forever — eventually you have to ask the hard question to find out if they desperately need what you’re building. If you’re still unsure, here are the yellow flags we recommend watching out for: Customers respond that if your product disappeared tomorrow, they wouldn’t be disappointed. Usage is low and not growing over a period of six months. You have a handful of happy customers, but the most important feature is actually different for each of them — in other words, you're customizing the product too much to make each customer happy. (That puts you in consulting business territory.) It feels incredibly hard to find new customers. The sales cycles take too long, and if your champion quits the company, then the deal just falls apart. Design partners are hesitant about converting to paid or customers just don't seem willing to pay the ACV you’re after. WATCH (8 MIN)•JACK ALTMAN ON LATTICE’S PATH TO PMF With the original idea for Lattice, Jack Altman felt this firsthand. “The first thing we built was an OKR software tool. We had seen at our last company that quarterly OKR planning was a painful process for companies. We validated that there was a real problem by talking to a bunch of other companies about how hard it is. The execs get mad at each other. The employees are like, why are we doing this? Nobody adheres. But while it was in fact a real problem, we never built software that truly solved that problem,” says Altman. “There were two things that made that clear. One, it was really hard to get people to actually pull out their credit cards and pay us. And so a lot of CEOs and people leaders were saying, ‘We'd really like this. We want to manage our OKRs better. We're doing this in spreadsheets currently, it's not great. Can we try it? Can we test it for a quarter? Can we do a monthly thing? Can you give me a login so I can poke around?’ But when we tried to get them to pay, it was very challenging,” says Altman. “The second issue was that once we did get people into the product — even entire teams where the whole company would do an OKR planning cycle in Lattice — the next quarter didn't come easily. They were like, ‘Ugh, we’ve got to do this.’ And then by the third quarter they were like, ‘This is not happening naturally.’ For the employees, the retention was just not there. So both sides were not strong enough,” he says. His advice on how to get unstuck rang true for us: “If you’re stuck, your only way out of the nascent stage is big changes, not small changes. A lot of people hang out in nascent and make small changes for years, and that's just a literal waste of everybody's time and resources. Moving up levels of product-market fit or getting unstuck requires more than just incremental steps or giving it more time,” he says. “You need to remind people of that reality so they don't stay in these loops for too long and instead say, ‘Alright, we're going to just move on and wholesale try a new thing.’ After nine or 10 months of working on OKRs, we got to a place where there wasn’t any line we could draw between what was happening and a business that was going to make any sense for the next round of funding by the time we ran out of money. So we ended up needing to pivot a couple of times before we landed on the thing that really worked, which was performance management.” \"More often than not, if you're working on something that is not getting great traction, you’re probably not a 10% adjustment away — you’re probably a 200% adjustment away.\" JACK ALTMAN• LATTICE For more on how to make the right adjustments that will get you unstuck and out of nascent product-market fit, apply to join PMF Method. Level 2: Developing By Level 2, you’ve ticked through some of the classic milestones, such as building something 20 or so customers need, or getting to your first $1M in ARR. And while you haven’t totally figured it all out, there’s also much more repeatability in your customers’ needs, the messaging that resonates, and the product solution you’re delivering. Developing PMF is when you're starting to see glimmering signs that the marginal customer is more within reach — but it by no means feels easy yet. This is one of the reasons why Level 2 is one of the two levels that the majority of companies get stuck on. Think of it as reaching \"lite\" product-market fit. What it Feels Like: After Lattice’s product pivot, the difference in traction immediately felt clear to co-founder Jack Altman. “It was very obvious what market pull looked like then. We had people paying us annual upfront contracts without ever touching a real product, just on our design mocks — which couldn't be more different than the experience we’d been having with OKRs,” he says. “There were leads coming organically from all over the place. People were telling their friends about us. People would say, ‘I'm kicking off this performance cycle in two weeks and I need this feature. You don't have it yet. Can we schedule time tomorrow to talk it through so we can help you build it?’ I think we booked twice as much revenue in the first month on that product than we had booked in the previous year on the last one.” \"Once you've got something that's going to work, it's going to work even in the face of a pretty undeveloped, incomplete, semi-broken product, which we certainly had at this stage.\" Jack Altman• Lattice What it Looks Like: A Case Study on Developing Demand, Satisfaction, Efficiency in Action Let’s rewind the clock back to when Looker was at Level 2, roughly from March 2013 to Nov 2014. The startup emerged from stealth in March 2013 with a 34-person team and 20 customers, many of which were also in the First Round community. For co-founder Lloyd Tabb, this was the stage when he knew they were onto something. “By the time we had 20 customers, I knew it was going to be big,” he says. By August 2013, they’d made enough progress to raise a $16M Series A round. By November 2013, they’d hit the coveted $1M in ARR milestone and grown to over 40 customers. New channels outside of warm intros were proving successful, from outbound and sales prospecting with two SDRs, to the potential of partner marketing with AWS Redshift and Snowplow. At this stage, that repeatability was starting to peek through. The Looker team had a better sense of who they were selling to (see their ICP from their Jan 2014 board deck below), having recently closed clients like Thumbtack, The RealReal, and Gilt. They were interested in going upmarket, and although they landed their first enterprise logos in April 2014 (eBay’s Venmo and NYTimes’ Baseline), for the most part, enterprise remained “elusive” at this level. By July 2013, there was an average of 1600+ hours spent per month by customers in Looker. As the team put it in a board update, they continued to see amazing satisfaction by “maintaining insane referenceability and minimal churn (3).” As for efficiency, by the end of 2013, their CAC was $54,000 with a payback period of 19 months. At this time they noted that from first meeting to close ranged from two to seven months, with three months as the average. Customizations were still cropping up, as white-labeling and self-service had unique requirements. While sales process improvements were underway, their unique forward-deploy trials were still taking 30-40 hours, which needed to be brought down. As for our example company at this level, meet Hire Hero, a fictional interviewing platform that uses AI to help teams hire better. They've got 18 full-time employees. They recently raised a $10M Series A round, and they've got a burn multiple of 4.2x. That's not where it should be — but at least they know what it is. Let’s walk through each of the dimensions of product-market fit: Demand: They’re at $1.2M in ARR and they have 22 customers. Their average sales cycle is 32 days, but that's an improvement over 47 days. One promising sign is that their most recent five customers are paying twice as much as what their early customers paid. Increasing ACV is a very strong sign on the demand side. Satisfaction: This team has been around long enough to actually track renewals, and four of their customers have renewed. After selling for more than 12 months, four customers have churned, which isn’t ideal, but two of those were non-regretted. This is pretty typical. As the product direction changes, you’ll often realize that certain early customers are just not the right fit anymore. Additionally, weekly active usage is going up. The team is running classic NPS style surveys and getting roughly half nines and tens. Efficiency: At this stage, it’s still not a focus, but at least they know they're at 65% gross margin. Software companies should be shooting for 75-80%+ when it comes to gross margin, so something to improve on. Their onboarding has gotten 15% faster. And very critically, fewer customizations are being asked for by each of their customers, meaning they’re able to sell the same product to now 22 customers. Benchmarks: These are the benchmarks that we would consider very typical of Level 2: Team size: Up to 20 people. You're either a Seed or Series A company. Demand source: Early signs of a scalable channel that doesn't depend on warm intros from your VCs or your friends. Without a warm intro, your first call to closed-won percentage is approaching 10%. “Magic Number” (ARR/CAC) in the .5 to .75 range. 5-25 customers. $500K to $5M ARR. Some renewals, 10-20% regretted churn, NRR at least 100%. Not less than 50% gross margin. Not worse than 5X burn multiple. WATCH (8 MIN)•CHRISTINA CACIOPPO ON VANTA’S PATH TO PMF What Matters Most: To level up, your priority needs to shift to demand. Here’s why: We’ve seen true grind-it-out founders muscle their way to 20 customers through sheer willpower and effort. But you can’t will your way to 100 or 200 customers — the satisfaction and demand for the product are starting to do the work for you. Startups need to find a way to effectively drive demand. But in addition to working on lengthening those lines out the door, you still have to maintain the satisfaction bar that you’ve reached. And while efficiency still isn't the primary concern, it’s something that you’ll need to pay more and more attention to. Now is the time to start thinking about if this is going to be an efficient business, and starting to do the work to make it one. The most helpful levers for pushing on demand are typically fine-tuning your product positioning and finding scalable channels, whether that’s outbound sales or SEO, paid marketing, or referrals. For Vanta’s Christina Cacioppo, the latter is an area she would revisit with a time machine. “There was this kind of tight call-and-response where someone in a First Round community Slack channel or a YC forum would say ‘SOC 2’ and someone else would say ‘Vanta,’” she says. “We were so proud of ourselves with our brilliant guerilla marketing or whatever we thought we were doing, but I think what actually happened is we really underinvested in learning how to explain what we were building, outside of ‘Vanta gets you SOC 2, and that's important for your business.’ In retrospect, we should have started on some of that earlier versus coasting on what was initially great word of mouth.” \"Word of mouth is wonderful, but in B2B you also need to scale ways you're acquiring customers. You need to start on that work earlier than you think. \" Christina Cacioppo• Vanta Yellow Flags to Watch Out for: If you’re still unsure of whether your business can be classified as developing product-market fit, here are the yellow flags we recommend watching out for: Current customers are happy, but you're having trouble opening the floodgates. Regretted churn is greater than 20%. People bought your product and paid for it and just weren't happy — and one out of five of them is churning off. Starting to get concerned about your burn multiple, e.g. you're burning $200K per month and only bringing in $20K net new ARR per month, which would put you at a less-than-stellar burn multiple of 10. (You’re aiming for as close to 1 as possible, or even below 1). But you feel like if you stop spending the money you're just going to stop growing. Sales cycle is taking too long and you lose deals late in the funnel. You start to lose to competitors. And you're just not feeling the urgency from potential customers. You're struggling to hit the price point that you want. This is often a signal that customers see your product as a nice-to-have versus a must-have. You often hear “we don't have the budget,” or “it's not the right time for us,” in conversations. For more on how to get your sales machine firing on all cylinders to propel your company out of developing product-market fit, apply to join PMF Method. Level 3: Strong In our experience, the majority of startups never get to this level. If you get to strong product-market fit, your company has the potential to be very valuable. The big difference between Level 2 and Level 3 is that the demand floodgates have opened. There’s even more repeatability. Demand is coming inbound. People have heard of you. The company feels like it’s humming. You know who your customer is with much more specificity (for example, the head of security at a company with 250-1000 employees who’s already using a Palo Alto Networks product). Your costs to acquire them are still a little high, but you’ve found that (most of the time) you can engage them with a clear and consistent message that resonates. You’re able to deploy a solution with little customization that delivers a ton of value to the customer. What it Feels Like: This is the stage where most of those PMF analogies you’ve previously heard start to make sense — you start to really feel the effects of product-market fit. David Hsu from Retool put it this way: “We talked to someone who said that finding product-market fit was so visceral that you immediately felt it — like a geyser exploding. And we honestly never felt that in the first couple years. At Retool every early customer we got, whether that was number four or number 14, felt like the last customer we were ever going to find. It felt more like rolling the stone uphill. If you stop pushing, it'll start rolling back on you immediately. That's how it felt until we had a few million in ARR — that's when the boulder went down the other side, and we had to chase it to keep up.” Here’s how Jack Altman from Lattice describes it: “The biggest shift was in the ease of getting leads. I remember thinking, ‘I don't even know where these leads are coming from.’ And more just kept showing up every month,” he says. For Verkada’s Filip Kaliszan, the feeling was similar. “After our first year of sales in 2018, those next two years were crazy. We were barely keeping up with production. We had to scale all the systems. A lot of things had to happen in the span of the next 12-18 months in order to deliver on everything that customers were hoping the solution was going to do for them. That in itself was a very formative and tricky part of the journey.” For Plaid’s Zach Perret, this level felt noticeably different. “2016 to 2017 is when we knew we had very strong product-market fit, and that was mostly because we just saw rapid growth of many of our customers,” he says. “This is around when Robinhood and Coinbase started to take off, and we saw it in our numbers. One of the benefits of being a platform company is that you're able to install really early with a customer and then grow over time, and that scale is really, really lovely,” says Perret. “For example, we eventually branded a portion of the user signup flow within Venmo and other apps. And that minor branding actually drove a lot of demand for our product because people would see it and they would say, ‘Hey, this is a new financial services experience. I should call Plaid because Plaid helped build this one.’ And so we had this nice cycle of our customers doing well, driving demand back to us and then so on and so forth every time,” he says. “But keep in mind, by this point it had been four or four and a half years since we started the company. And so a lot of the early journey was based on faith and belief that this market would exist, on good early signals that were partially real and partially manufactured by the work that we'd done.” What it Looks Like: A Case Study on Strong Demand, Satisfaction, Efficiency in Action Let’s start again by tracing back to when Looker fit into Level 3, roughly from November 2014 to December 2015. By November 2014, the team had passed the $5M ARR mark. CEO Frank Bien put it best in an email update to us: “The general feeling is that lots of companies can make it to $1 or 2M ARR, but hitting 5M shows you’ve built something very real and sustainable. Also impressive is how quickly we hit this point.” In February 2015, the company raised a $30M Series B round, noting they’d grown to 110 employees, and increased revenue by 400%. At this point, Looker had 250 customers, adding new customers such as Uber, Instacart, Plaid, and Etsy. Continuing to open up new channels, the team closed its first reseller deal in Q2 2015 and started expanding into Europe, opening their first EMEA office in London. The team was focused on growing organic inbound through PR, community, content & thought leadership, in addition to scaling their partnerships. For example, early in 2015 bringing on Segment and Snowflake was a big win, as was being selected as one of three top launch partners for Microsoft SQL DW & Azure. However, a main focus during this period was fine-tuning their positioning, to better appeal both to data experts and general business users. Customers experienced as much as 80-90% user adoption across an employee base. By the end of 2015, 97% of customers were accessing Looker at least once per day, CAC had decreased to $45,300 and payback period was down to 17.5 months. Gross margin improved from 62.5% in 2014 to 72.6% in 2015. The sales trial model continued to get more efficient, and the company had a 70% trial to win rate. But perhaps what was most impressive as efficiency shifted into the forefront was their planning accuracy: In seven years, the Looker team never missed a bookings plan, achieving a rare 28-quarter streak of pure execution. For our Level 3 case study, let’s introduce GuardDog, a fictional incident management tool to help security teams seamlessly identify and address security issues. They have 64 full-time employees and are planning to raise a $25M Series B. Their burn multiple is now 2.1, which is getting respectable. Let’s walk through each of the dimensions of product-market fit: Demand: GuardDog has 88 customers. With $10M in ARR and 14% sales conversion, they've got inbound and outbound channels that are working, and awareness is starting to spread. They're starting to see word of mouth traction, which is very common at this level. (Think: “Oh, you're a startup? You need a security solution, you should choose Guard Dog, that's what all the startups are using.”) They're projecting 3x growth in ARR over the next 12 months. Satisfaction: They're at 108% net revenue retention, and 8% regretted churn per year. Quarterly user engagement has gone from 5.5 average users per customer up to nine. Efficiency: GuardDog is getting into the respectable zone here. The team did a ton of work both on the infrastructure side, and the services side to improve to a 71% gross margin. Onboarding has gotten faster by 60%. Benchmarks: Team size: 30-100 people. Stage: Series A/B/C company. Demand source: 1+ scalable channels (you’ve cracked marketing and sales). >10% of your inbound is coming from referrals/WoM. 1st call to closed won 10-15%. Magic number >.75, CAC payback 110% NRR. 15%. Magic number >1, CAC payback 120% NRR. <10% churn. 80+% gross margin. Burn multiple 0-1X. What Matters Most: Whether it’s Salesforce, Stripe, or Datadog, all the legendary enterprise companies have managed to enter multiple markets with multiple products and reach extreme PMF time and time again in new customer segments. If you want your company to compound value, you’re not done with growth in year six or seven, or just because you’ve reached a certain size. Growing over a long period of time is actually the act of finding extreme PMF over and over again. The very best companies are forever focused on expanding TAM and re-finding PMF in new markets. In our experience, most founders struggle with the timing, and start thinking about expanding TAM too late. When you’re focused on getting the core business to fire on all cylinders, it’s hard to split your time. Atlassian stands out as an example to take inspiration from here — the company pursued this work much earlier than most. “We created our second product, Confluence, in Atlassian’s second year, which is really unusual — especially when you’ve got a breakout product like Jira that’s growing really nicely. Conventional company-building wisdom would say not to do it — there’s still a ton of work you need to do on the first product, and it will fragment your focus once you start working on a second thing, which could be a death sentence for a young company,” says Jay Simons. But not only did Confluence become a cornerstone of Atlassian’s product suite, Simons also spots plenty of less-tangible upsides. “We began to build this muscle around cross-merchandising, cross-selling, and upselling. How do you think about pricing and packaging around multiple things? How do you do product planning and prioritization and budgeting and staffing? At a really early age at Atlassian, we began to wire our brain to think about all of those things that companies need to do as they get bigger,” he says. “It’s harder as you get bigger too, because the product you add actually needs to be bigger. For a company like Atlassian that has individual products that are hundreds of millions of ARR, a new thing that you add has got to grow pretty quickly, and it’s got to have a big market it can grow into,” he says. Timing aside, the other hurdle is tackling the question of which path to take once you hit that inflection point of needing to grow your TAM. We like this framework from Persona founder Rick Song: Once you’ve reached extreme product-market fit and are ready to attempt to do it all over again, you can effectively expand your TAM — which is how you compound revenue — by remixing one (and eventually all) of three components: your product, your market or your buyer. That could look like: Creating new product use cases by adding new features and functionality. Creating a net-new product that you sell in the same market to the same buyer. Taking the same product and expanding into a different market (whether that’s a different sector or moving upmarket/downmarket). Selling the same product in the same market to a different buyer. Over time, you will likely need to expand all of these. Let’s return to Atlassian. For simplicity’s sake, say Act I was JIRA (a single product) sold to an engineering leader (buyer) at companies between 10 to 100 people in size (market segment). You can change any of those three elements — but in most cases, it’s best to do this serially. In our Atlassian example, say Act II was taking the same product and selling it to security leaders to manage all the security tickets (new buyer, same market segment). Say Act III was introducing Confluence (a new internal wiki product), in many cases sold to the same buyers and market segment. Or consider how Salesforce sells into healthcare, financial services, and government. The product use case is relatively similar, the buyer may be different or the same, but the market segment is very different. Other times, the buyer remains relatively consistent but the product use cases change. In its most simplistic form, Amazon’s AWS has hundreds of use cases and SKUs sold into the same (or a similar) buyer. As you can see, there are countless combinations to explore. But in our experience, when most founders think about building concentric circles around their core business, too often they concentrate on selling a new product use case into an existing buyer. It seems like a lower lift, in the sense that you already have the customer. And sometimes that's the correct thing to do — Square expanding into lending is a great example — but if that’s the only path you pursue, there will be a ceiling on your TAM. What’s crucial is that for each one of these new bets, you need to get to strong and extreme product-market fit. In some cases, the journey is much shorter, given the work your company has already put into understanding a customer or developing a market. But in other cases, this requires starting at the very bottom of the ladder, moving through the levels of product-market fit all over again. In other words, a company might have extreme product-market fit with one product, buyer and segment, and perhaps only nascent product-market fit for another product with the same buyer and segment. That is the journey of building compounding revenue over years and then decades. It’s like you’re assembling a layer cake over many many years. You’re adding on new features, new products, new customers, new market segments — trying to stack your way to a multi-billion dollar valuation without knocking the whole thing over. That’s been the recipe for every fantastic B2B company. That journey of climbing your way to extreme product-market fit only to have to move through the levels once more is certainly daunting — particularly when you remember how tough those early days without traction were. “As much as I now joke about Vanta’s early days and how confused we were with all of our different startup ideas, the truth is that period is really rough,” says Christina Cacioppo. “From the outside, you can romanticize a founder’s life and think, ‘The world's your oyster. You can build whatever you want. You're going to start a company, it'll be great. You get to decide everything.’ And that's all true, but in the day-to-day, it's more like, ‘What am I doing? Does anyone care? Will anyone ever care?’ There’s so much in the early days where you can't tell if you’re doing it wrong. Having a group of other people going through a similar experience to talk about that with and normalize some of the uncertainty is really, really helpful. It can help you figure out if it’s an idea that you’re pulled toward because you’re just looking for something in all of the uncertainty, or if it’s actually viable.” We’ve purpose-built PMF Method to solve those challenges. Building a company is lonely, but we’re firm believers that there’s no cure like working alongside a tight group of other top 1% B2B founders at your same stage. Apply here to join them. This article was about what extreme PMF looks like, but in our program, we dive into the details of how you can get there — you can preview the other seven sessions you’d learn from in the program here.",
    "commentLink": "https://news.ycombinator.com/item?id=40051818",
    "commentBody": "A framework to help B2B founders find product-market fit (firstround.com)180 points by yarapavan 20 hours agohidepastfavorite62 comments light_triad 17 hours agoGreat they share specific metrics and give a sense of how they evaluate companies, although all First Round articles have this tendency to be so overdesigned it takes away from the content. The discussion on Lenny's podcast is easier to digest: https://www.youtube.com/watch?v=yc1Uwhfxacs I think the 4 P's is a valuable framework for getting to PMF: Persona, Problem, Promise (Pitch), and Product. Usually startups that don't work have a fundamental problem with a few of these (they try to solve too many problems for too many people in a subpar way - i.e. they don't have a strong answer to: which problem are you solving, for who and why it is better than alternatives?). Startups that are willing to iterate over the 4 Ps have a shot at getting to PMF, those that don't usually will struggle and then die. reply rgalate 4 hours agoparentThanks for linking the interview! I listened to it and found it insightful. reply bjornsing 18 hours agoprevInteresting, but I get the feeling that they’ve packed so much into the PMF concept that it’s basically the whole business. In my mind it makes more sense to reserve the term for what’s called Level 1 here: the early stage where you need to make big adjustments and potentially pivot to an entirely different product or market. reply hampowder 1 hour agoprevIf anyone has any useful frameworks for B2C PMF/iteration I'd be very appreciative reply niles 18 hours agoprevMy initial impression of this was poor, but I think that has more to do with the formatting of the slides not being conducive to mobile and the sheer volume of text. After reading through and watching the candid videos, I can say there are valuable things that early stage founders can pull out so it is worth powering through. I think more iterations and more depth on the case studies would be helpful for levels 1&2. Meaning, a broader sample of startups in the portfolio, and more tangible examples of changes that made a customer stop chrunning or sign up faster. Using hypothetical or anonymized case studies and too few largely keeps this framework in the level 1-2 quadrant, which is ironic. Almost like this document was built (a technology) rather than helping a specific reader (a problem + solution). reply turnsout 18 hours agoprevThis framework is attempting to make PMF seem so hopelessly complicated that you'll need their magic system (pricing unavailable, apply here). Trust me, you don't need a system. It's so much simpler than this. First, start with the M, not the P. Talk to real human beings in any market to find a painful problem, create a product that addresses (doesn't need to 100% solve) that problem, and (don't forget this part) charge money for the product. That's it. Anything else is galaxy brain. Don't get distracted. reply billmalarky 16 hours agoparent>First, start with the M, not the P. Well said! This is basically the linchpin behind the Jobs to be Done theory commonly attributed to Clayton Christensen (many thought leaders developed the theory over the years which he credits, but Clay seemed to be the leader of the group). The premise of Clay's book \"Competing Against Luck\" is about this JTBD theory (he also calls \"Jobs Theory\") which aims to surface the fundamental causality of customer purchase behavior, which allows you to build your solution such that it aligns directly with that behavior instead of all the abstractions and manifestations of that behavior that are often red herrings (ie: pointless features, solutions that in reality not aligned but this is not obvious, etc). Ultimately, Jobs Theory is an approach that that aims to objectively lower risk around innovation. It teaches you to think of customer needs/objectives independently from solutions/technologies, so that it's more natural to innovate to serve the ultimate need rather than compete against preexisting solutions or be misled by customers who are not often able to articulate (or they may not even consciously know what drove their behavior -- it can be very complex) what a better solution for them would be and as such will just mislead you. Instead of taking shots in the dark, you build better solutions around preexisting needs (more accurately called \"jobs\" in JTBD Theory, and it isn't just semantics). Apologies for nerding out on a topic you probably already know well but I figured I'd add to the conversation for others who might be interested. I really love JTBD. It helps solve imo the single hardest problem around startups -- product/market fit. Read Competing Against Luck folks! reply turnsout 15 hours agorootparentAbsolutely! I work in design/innovation consulting, and we use JTBD quite a bit. It's a nice way to interpret user needs and avoid the pitfall of taking user feedback too literally. I view JTBD as a flavor of Human-Centered Design that gives teams structure around the \"Desirability\" piece of the Desirability / Viability / Feasibility framework. We also hear the advice to \"make a painkiller, not a vitamin,\" which is sort of a JTBD/HCD test in disguise. reply billmalarky 15 hours agorootparentIs there a way I can connect with you off HN? I'm a student of design/innovation though my background is in engineering. I'd love to be able to learn from you from time to time. reply neom 14 hours agorootparentprevI'd also recommend Elliot Parker who worked with Clay for a long time. His new book is a great read (I read a pre-print version, but I'd guess they are the same): https://www.amazon.com/Illusion-Innovation-Efficiency-Unleas... reply godzillabrennus 16 hours agoparentprevExactly. Technology can be used to solve problems. Understanding a problem is the key. If you don't have the domain expertise to identify the problem you need to speak with stakeholders in the industry to grasp the problem and solve it. reply ericmcer 16 hours agoparentprevThe Mom Test is a great book around this. It really helped me (as an engineer) realize that the M part isn't a daunting sales task, it just involves talking to people about what they do. And people like to talk about themselves. reply cameron_b 18 hours agoprevalternatively - https://www.sequoiacap.com/article/pmf-framework/ https://news.ycombinator.com/item?id=40020601 reply altdataseller 17 hours agoparentI found that one lacking a lot compared to this one reply semanser 18 hours agoprevThere is also a video on Lenny's podcast (feat First Round Capital) about the exact same topic: https://www.youtube.com/watch?v=yc1Uwhfxacs reply Cilvic 16 hours agoprevI'd love to find something like this for \"product-led growth\" ideally not B2C but still B2B just not \"sales led\". Any ideas? reply neom 16 hours agoparentElena Verna is the name you're looking for. She's got a lot of stuff all over the place so might take a bit of digging, but imo there is nobody better, she's a PLG wizard for sure. reply tempusalaria 16 hours agoprevThey talk about selling a dollar for less than it’s worth as not being a PMF indication. However, this does not match with actual VC funding trends, as the vast majority of private unicorns are loss making. In fact if you can consistently sell a dollar for 90 cents it’s likely you can raise a lot of money on the back of that. It’s funny also that their Ironclad example clearly describes fraud/dishonesty on the part of the founder and this is celebrated as hustle. Seems far too common that this type of behavior is not only tolerated but actually suggested by VCs. reply dangoodmanUT 19 hours agoprevThe amount of design that went in to this, and the fact that they override the scrolling logic, makes me curious what their intention actually is. They do a lot of telling what if \"feels like\". Also... does this site... have motion blur? reply monitron 18 hours agoparentI just want to meet the dev who wakes up in the morning, sits down with their cup of coffee, rubs their hands together and says \"Boy, I just can't wait to make a website where scrolling feels like trying to run across an ice rink!\" reply adtac 18 hours agoparentprevIf you're on Safari, open devtools, click the monitor icon on the top left, disable javascript, reload. The content reads fine without JS. reply j45 18 hours agoparentprevThe idea is likely to increase production value to boost engagement, and help signal value of what's written to be of higher quality. As someone who has worked in building multiple B2B SaaS when it was not popular at all in the retail VC funded universe (consumer social media apps were the rage for a long time), ostracized, and more, it's kind of fun to see VC's trying to play in this area. reply dartos 18 hours agorootparentAre platforms like Vercel not B2B? reply slimsag 18 hours agorootparentprevI mean, it's working. They are #1 on Hacker News currently - so like it or not they should continue doing this if that is their goal. reply swatcoder 18 hours agorootparent#1 on Hacker News while none of the comments engage with the content, and instead just express confusion, uncertainty and aesthetic discomfort is not what you want. It means you probably struck onto a concept or theme that's speaks to active interests (the headline draw) but delivered on it poorly. Which is sort of beautifully ironic, given the subject. reply geraldhh 18 hours agorootparentmaybe the irony implies some sort of artisanal play reply j45 16 hours agorootparentprevEyeballs are one thing, and maybe useful for brand mindshare, or worse some vanity metrics... but action is another. It's good to try and take the good from anything. reply yuppiepuppie 19 hours agoprevEverybody complaining about the design. But from a non-founder, does this framework actually seem legit? Or is it a fluff piece? reply bko 18 hours agoparentSeems like a lot of fluff. I think the key is to deliver value to customers. Do something you know. Don't be arrogant and enter a random business you know nothing about. Trying to get a startup off the ground is basically 2 things: pushing code and talking to customers. Set a metric like how much users are using the app (proxy for usefulness). If you create something that's valuable, it'll probably work out. There are a few exceptions which they touch on a bit. Movie pass model where you sell $10 for $5 obviously doesn't work. Or if you take investors and scale up too fast also screws you. You could be perfectly profitable and have fit but if some VC throws money at you and convinces you to hire dozens of people, then it could sink your product despite fit. An investor doesn't necessarily have your best interests in mind. They may prefer a 10% chance of $100m co where as you prefer a 50% chance of a $20m company in that time frame. reply leetrout 18 hours agorootparent> Do something you know. Don't be arrogant and enter a random business you know nothing about. Within reason. I think there is a path to success in partnering with customers in the market you want to serve. Following Seth Godin's advice- \"Don't find customers for your products, find products for your customers.\" As devs we have the capacity to join ranks with our customers and learn and build to solve their problems. reply luckyou 18 hours agorootparentprevIt's easier to find PMF by myself them to read this post and find a template there. Very difficult text with a lot of words. reply hackitup7 17 hours agoparentprevThe content is pretty accurate reply mritchie712 18 hours agoparentprevat a glance, it seems like good advice. It's just waayyyyy to long. reply bendecoste 19 hours agoprevThe scroll on this website made me bounce reply fouc 18 hours agoprevA lot of people upvoting this without wading through this fluff. Someone break out the chatgpt summarizer ha reply svnt 16 hours agoprevThe tldr is provided in the article itself: > To be clear, pieces of this approach may exist here or there in different frameworks — after all, all great ideas are built on what's come before. If you haven’t spent much time seeking PMF in a B2B company there is useful information here, but to me it looks like a refactoring of what’s come before. The article under-delivers on its promise, which is stated up front as: > Most people describe finding product-market fit as an art, not a science. But when it comes to sales-led B2B startups, we’ve reverse engineered a method to increase the odds of unlocking it. We’ve worked with some of the world’s most iconic enterprise founders and distilled what they did in their first six months into a series of tactical sessions for taking a straighter path to PMF. And later on it even admits it doesn’t actually do what it promised: > For example, a startup might be stuck at developing PMF, but later find that pivoting to a new buyer is the move that unlocks the next level. Or another company may have stumbled onto nascent PMF with resonant positioning, but then can’t ship the correct product that delivers on its promise. > These levers, of course, fall more into the camp of how to go about finding PMF, which is not the focus of this essay. Isn’t “tactical” advice supposed to be the answer to the question “how?” reply nextworddev 18 hours agoprevVCs have very much embraced content marketing reply awad 14 hours agoparentIt should be noted that First Round have done a great job putting out content for many years. https://review.firstround.com is a great resource for founders reply bradhe 18 hours agoparentprevYeah was going to say...with this and the Sequoia article the other day... reply kenm47 19 hours agoprevtl;dr us someone. No way anyone actually reads that. Especially the people who want it. reply jasonjmcghee 18 hours agoparentIt doesn’t take that long to read - probably 15 minutes, and a few minutes to skim. For tl;dr use ChatGPT. “Summarize this ”. You can ask questions about it if you’re interested. In general it talks about pmf, gives advice on different stages of pmf, and some strategies to be used to help navigate towards pmf at each stage. As far as people that want this, i sure hope they can spend 15 minutes (or even 30!) on an article if they are spending years trying to achieve pmf. reply ericdfoley 19 hours agoprevnext [3 more] [flagged] Lucasoato 19 hours agoparentThe quality of the web design is exceptional but maybe it hasn't been tested on giant monitors :/ reply williamdclt 18 hours agoparentprevit's not even great on laptops. to be fair, HN is just as bad in terms of font size (too small), but at least it respects browser zoom which this website doesn't reply tomschwiha 18 hours agoprevnext [4 more] [flagged] neom 18 hours agoparentJust an FYI, as dang told me in an email when I complained about being flagged last year for using ChatGPT to do something similar in a comment: comments written primarily by bots are not welcome on hn. reply tomschwiha 18 hours agorootparentI see, thanks for your feedback. Would make sense if that ruling is still active. However at least I would know for the future, though I also learned reading this article would probably be helpful. So guess it woudn't be a bad thing either way :) reply pvg 17 hours agorootparenthttps://hn.algolia.com/?dateRange=all&page=0&prefix=false&qu... Same now and likely for the future. reply RivieraKid 18 hours agoprevAnyone got a tldr? reply godzillabrennus 16 hours agoparentIt's the Lean Startup Method but in course form. reply mdekkers 18 hours agoparentprevtoday we’re proud to unveil Product-Market Fit Method, an intensive 14-week experience designed to help exceptional pre-seed founders build epic B2B SaaS companies. Its spam reply from-nibly 17 hours agoprevIf you believe this I've also got a remote job where you just write a couple posts every day. I've pulled in $41,485 just this last month. reply darkhorse13 17 hours agoparentWhat do you actually write? reply haliskerbas 17 hours agorootparentI think it’s sarcasm. It must be saying both the remote job and this article are not believable. reply from-nibly 10 hours agorootparentYes and apparently it wasnt received well. :/ reply makerdiety 18 hours agoprev [–] I'm here to tell the tragic truth that there isn't a lot of opportunity for large scale repeatability that happens after the minimum viable product is discovered. Startups are basically cryptocurrency pyramid schemes hidden in the domain of legitimate entrepreneurship. You shouldn't seek anything that is beyond one order of magnitude higher than the minimum viable product. Because Facebook already exists. World of Warcraft already exists. You're not gonna discover the product-market fit for the next Google. Because there is no next Google. It's sad how people are being lied to. Startups are literally just another lottery to waste resources, time, and energy on. The bubble will eventually pop and people will be disappointed with reality. reply neom 18 hours agoparentI can tell you how finding PMF went at DigitalOcean. Moisey one of the founders was like \"EC2 is expensive and hard to use, lets make it easy and cheap\" - then you all used it. Without AWS there would be no DigitalOcean, and although (much to my displeasure) our CEO often said \"we're going to be the next AWS\", we really just wanted to be a $500MM ARR alternative. Not sure if I'm agreeing with you or disagreeing with you tho, hah. reply makerdiety 18 hours agorootparentMaybe the next Google or the next Amazon is an inheritable phenomenon that exists on a spectrum? If the next Google is just a class of points within a gradient, and if Google, Facebook, and Amazon are runaway monopolies formed by a stacking of scalable wins like the successful implementation of a theoretical product-market fit framework, then what is a local optimum which is where a perfect neo-Google sits at? And what would be its antithesis (i.e. the local minimum to its corresponding local maximum)? reply neom 17 hours agorootparentI call it \"Wake building\" basically building a business in the wake of a new giant. Amazon was building AWS for itself and for businesses like it. It was inventing cloud services, and it was always going to be complex and expensive. The rest of the industry didn't seem particular bothered to go run behind them, so we did. It was a fine enough strategy I think, the only people I really ever felt I needed to be careful of were Linode and Joyent, so I kept a sharp eye on them, but otherwise yeah, no much to do but listen to how SMB devs didn't want to consume AWS services and build them in a way they did. These days it seems like Matthew and Michelle over at cloudflare have picked that mission up and are doing an amazing job at it, so kudos to that team for sure! :) reply jnovek 18 hours agoparentprevFirst: both Blizzard and Google started out as small startups. I'm curious, if not startups, where do you expect innovation to come from? The things startups do are usually too high risk for large corporations to take on. reply makerdiety 18 hours agorootparentNo civilization is entitled to technological innovation and economic growth just because it demands it. You have to engineer that goal into reality. And if you want risk-taking startups and entrenched corporations to work together, you are going to have to devise a communist style government or something. The free markets would just forgo national gross domestic product maximization and line up their own little pockets, free from the top-down authoritarianism required to organize large scale techno-economic activity. The Roman empire started out as two little baby boys and a momma wolf's teats. Good luck replicating that evolution from zero to hero without the right framework. reply tannedNerd 18 hours agoparentprev [–] Yes there is probably no next Google, but you know what there are a lot of. Companies that the googles etc buy because they have gotten to the point where they can’t really develop new products themselves. So yes you many not be able to make the next Google or Microsoft, but there are plenty of success stories to still be had. reply makerdiety 18 hours agorootparent [–] A lifestyle business isn't the success this \"black boxed highly performant Product-Market Framework\" article is advocating for. A lifestyle business is a small business that is no better than a corner store selling liquor and cigarettes or whatever. The popular trend is to chase after frameworks and technology that enables inhuman commercial growth. You don't need a Kubernetes cluster for a little gas station business. You also don't even need the concept of a minimum viable product for that \"humble\" little success which is more probable for the average person to achieve. Silicon Valley startups want big achievements. Astronomical stuff. That's why MVPs, PMFs, and KPIs were invented. For doing aerospace science as opposed to building local mud huts. reply citizenpaul 16 hours agorootparent [–] >You don't need a Kubernetes cluster for a little gas station business. Chick-fil-a has a Kubernetes cluster at each location. Maybe its not so far fetched. Gas stations are all franchises of a major corp. Source: https://www.youtube.com/watch?v=il7Ww_AlhAg reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A new framework, the Product-Market Fit Method program, aids sales-led B2B startups in achieving Extreme Product-Market Fit by focusing on satisfaction, demand, and efficiency dimensions.",
      "Successful startups including Looker, Ironclad, Verkada, and Plaid showcase the significance of understanding customer needs, delivering value, and continuous improvement for various levels of product-market fit.",
      "Strategies like the First Round 4Ps framework and significant adjustments, rather than minor ones, are crucial for success in product-market fit, emphasizing the growth journey towards strong product-market fit and long-term success."
    ],
    "commentSummary": [
      "Achieving product-market fit (PMF) is crucial for B2B founders by addressing customer needs, solving real issues, and leveraging technology.",
      "Strategies like the 4 P's framework, Jobs to be Done theory, and Human-Centered Design principles are advised to reach PMF effectively.",
      "Emphasizes on content quality, engagement, and meeting customer preferences, while warning about rapid scaling and unwarranted risks to optimize startup success."
    ],
    "points": 180,
    "commentCount": 62,
    "retryCount": 0,
    "time": 1713275335
  },
  {
    "id": 40050717,
    "title": "Analyzing Cloud Impact on Low Latency Trading",
    "originLink": "https://blog.abctaylor.com/what-would-happen-to-low-latency-trading-if-exchanges-moved-to-the-cloud/",
    "originBody": "Diary of a SysAdmin About InfoSec Trading Linux Networking CV What would happen to low latency trading if exchanges moved to the cloud? A thought experiment on the winners, losers and new players in a cloud-driven stock, fx and commodity-trading world 21 March 2024 Trading A small UK stock exchange called Aquis hosts their matching engine in the cloud. Crypto exchanges are all cloud-based too. What if the NYSE, CME and others followed? The NASDAQ is already thinking about this since 2021. Let’s evaluate how HFTs would adapt, and how small-time traders could benefit. A brief intro to ULL (ultra low latency) trading infra ULL trading firms go to a lot of trouble to get their servers and switches within the same buildings as the exchanges they trade with to reduce latency. Some firms don’t even use layer 1 switches to be competitive. Most major exchanges run out of public data centres, for example: The NASDAQ is in an Equinix facility called NY11 in Carteret, NJ The CME is in a site called Aurora The LME sits in a Digital Realty facility at Brick Lane (London), and so on The NYSE runs out of a facility in Mahwah, NJ, and whilst not a public colo, you can still rent a rack there. You’ll pay heavily for a rack in one of these data centres. Individual quant traders cannot simply deploy ULL strategies here, due to the difficulty of sourcing the right hardware and getting set up in a site. You could go to a provider like Pico, but good infra is half of the competitive advantage when trading ULL (the other half is writing the trading strategy in low-level code and deploying it before other HFTs). It’s not unthinkable that these facilities migrate to the cloud (which is where Coinbase’s trading happens). Can I trade stocks at low latency myself? Reach out to Equinix, get a rack in NY4 with 5kVA power and about 4 cross connects Reach out to Arista and get a 7130 low latency switch Buy a Xilinx card that supports Onload, put it in a server, do this x2 for reliability Buy a time server to handle PTP so you know exactly when the market opens Become a member of the NYSE Pay for market data Get an Internet connection so you can connect your server to deploy strats You’re looking at six to seven figures of set up costs and about that in yearly running costs. In other words, it’s not happening. Similar pains will exist for trading FX with some bank like Citi, or futures with the CME. Low(ish) latency crypto Smart quant crypto traders will spin up many thousands of EC2 instances, measure latency to the exchange, and only keep the best instances, continually. This process allows them to eventually get EC2 instances to trade from probably in the same rack, or maybe on the same bare metal box, as the exchange. This is pretty good going. Traffic will still be routed through some sort of overlay network, but everyone has that penalty. There are no large upfront costs to setting up a strategy and market data is free. So, what happens if exchanges move? The NASDAQ is already bringing AWS infra into their facilities (link). So let’s say the NYSE went full cloud tomorrow with AWS – what happens? You’re already behind ULL leaders like Jump Trading, XTX and PDT Partners probably already thought this through, so will already be ahead of you. They’d immediately deploy the abovementioned gameplan to continually find the lowest latency boxes. They’d probably try some EKS instances and bare metal instances too, in case the matching engines are now containerized, or bare-metal-ized. Time-keeping If you’re on a VM, you’re probably getting time via the hypervisor, but is it accurate enough? If you have a great strategy that needs to buy and sell two stocks 100 nanos (ns) after the open, and you know you’re 3 mics (µs) away from the matching engine, you may want to send those two trades at 09:29:59.9999971 at the hope your order arrives at 100ns past 9.30am. Since 2023, high-precision PTP has been available on some EC2 instances (link), so it’s likely you’d want to be on one of these boxes. Amazon says the accuracy is “within microseconds” so this could be problematic. A strategy could be to send multiple trades at 09:29:59.99999{…, 4,5,6} and see if they get filled, but this risks double-trading (and breaking the exchange’s rules) which may well itself affect the strategy’s alpha. Market data The NYSE might make it your problem to deal with recording data, or they may make an S3 bucket available to anyone who pays them. Anyways, you probably want to pipe that directly into the strategy at filesystem level from the virtual NIC, and secondarily into Kafka/RedPanda and into something like kdb+ or ClickHouse to look at later. Strats/alphas Next, the strats. All HDL-written code for FPGAs becomes irrelevant, and winning strategies would quickly be ported to C, compiled with some optimal compiler for wahtever CPU the VM/container/bare metal box chosen is running on. For example, Intel has Math Kernel Library, whilst AMD has their own called AOCL. Strategies may even need to be compiled with optimizations for both. Optimisations Running some more lightweight OS like an ultra-thinned down Debian guest might yield some advantages to stock EC2 instances. This would be at the trade-off of increasing the time to build VMs, which will almost always land up somewhere useless for latency purposes. Going to bare metal even when the exchange is on EC2 might still be better if you could remove some crucial mics of hypervisor overhead. The create and kill game might need to be played with EC2 Bare Metal, and this may offend Amazon. Hogging instances might pay, if this stops competitors getting good hosts. This would eat into PnL and is also wasteful on energy. Nasty stuff that might happen Bad players could do a ping test to many thousands of EC2 instances, find those which are also at very low latency to their good boxes (assuming these are competitors), and DDoS them during trading hours to hammer the hypervisor’s NIC. This would result in critical overhead occurring for competitors sending orders out. Bad players could also generate large bursts of traffic on good EC2 instances they control on hypervisors believed to be close to the exchange, but don’t plan to trade on, to achieve the same thing in a less blatantly offensive way (e.g. say 40 servers in a rack, you guesstimate which physical box your trading VM is on, but also try find VMs on the other 39 good servers, and just generate spam traffic to add overhead to the actual NICs). Unsolved problems How would you short AMZN stock if there was an outage on AWS? This is super convenient for Amazon. Winners and losers Smaller quant shops and sophisticated home traders are the winners, whilst professional HFTs would be losers due to their sophisticated hardware advantages becoming worthless. Equinix is also a loser, and obviously Amazon is the biggest winner overall. Why did Aquis move? They probably can’t differentiate massively with LSEG or Euronext (who have way bigger tech spend), so being cloud-based is something different to do. AWS might have given them a good deal versus a colo, since Aquis doesn’t command the same sway someone like NYSE does to bless Equinix with a cash cow dairy farm. The UK is an HFT-hostile country (due to share trading taxes) where most UK ULL trading is on swaps and CFDs anyways, so they have little to lose by angering the HFTs by levelling the playing field. abctaylor.com contact me website hosted in my flat",
    "commentLink": "https://news.ycombinator.com/item?id=40050717",
    "commentBody": "Thoughts on low latency trading if exchanges went full cloud (abctaylor.com)180 points by arcza 22 hours agohidepastfavorite165 comments posnet 20 hours agoThe biggest current limitation with cloud providers when it comes to exchange tech is the lack of real multicast support. It is rare outside of exchanges, but extremely low latency L1 multicast market data has become the backbone of exchanges, both for fairness and for scalability. Knowing you can saturate your entire network with 10G traffic and every participant will get the same market data packets at the same time[0], and there will be zero queuing or bottlenecks is very hard to do otherwise. There is a pretty good podcast episode about it out of Jane Street[1]. I know AWS have 'multicast support' but last time I tested it, it was clearly just uni-cast traffic with a software switch doing fan-out/copying, I assume using the same tech as their transit gateway, I think it was called hyperplane or something. [0]: for some definition of the same time, at least low enough that you can't measure it without equidistant optical splitters or White Rabbit synced devices. [1]: https://signalsandthreads.com/multicast-and-the-markets/ reply amluto 19 hours agoparent> Knowing you can saturate your entire network with 10G traffic and every participant will get the same market data packets at the same time[0] Hold on a second. Multicast is nifty, but it does not perform miracles. If you operate a 10G multicast network and actually saturate it, you will experience drops and buffering-induced delays. Perhaps you can play games with time-synchronous networking, but as far as I know the exchanges don’t do this, and it likely needs special hardware. The point of 10G multicast is to use simple, standard (but complex to configure!) equipment to distribute much less than 10Gbps simultaneously. reply seanhunter 15 hours agorootparentGenerally speaking how multicast is used in trading situations[1] is you have two networks. On the primary network you do most of your normal IP traffic between applications etc. Then you have a seperate marketdata network that has most of the multicast traffic and it's exclusively used for marketdata. Marketdata generally is delivered on an \"As fast as possible\" basis[2]. So you don't care too much about occasional drops although fewer is obviously better. [1] At least in my time in the front office. [2] For example a very common pattern at the very low level for a marketdata subscription is when you subscribe to marketdata for some symbol the system will actually have a double buffer where it writes into one slot and you read from another slot and every time you read it switches the slots around. This means you can generally accept marketdata as fast as it arrives and process it when you can and you will always get the most recent packet when you ask for the next packet. reply amluto 11 hours agorootparentI’ve seen some multicast market data protocols with remarkably poor ability to detect or recover from drops. And they are very much not of the form where a newer datagram supersedes the older one. reply seanhunter 5 hours agorootparentYes, although people have been doing marketdata networks the way I said above using IP multicast for at least 20 years now, so in general they choose protocols and network architectures carefully to minimise problems. You do see problems from time to time but they are somewhat rare. Some of the restrictions are interesting. For example IP multicast was basically completely banned on the trading floor where I worked except for marketdata, because of an IP multicast snafu from some random application that took out the whole network once. One thing to realise about marketdata specifically is it's really different from other low-latency situations most people are familiar with (netcode in a game for example). As I mentioned before, it's not that big a deal generally to miss a few packets - the thing that is a big deal is to make decisions based on stale data. So you're not generally trying to reconstruct the full state after a drop- you just want the freshest current packet as fast as possible. If/when you need to reconstruct state you can make specific requests if needed. reply HALtheWise 17 hours agorootparentprevI'm curious if you know what, at a switch level, would actually cause drops and buffering for a 1:N (near-) saturated multicast flow. If all the packets are coming from the same source machine at (perhaps) 9.9Gbps and flowing into the switch, I would expect the switch to robustly redirect all that data with near-zero latency or packet drops to all its output ports. I don't think 10G Ethernet has \"backpressure\" in a way that would allow some output ports to get slowed down. If there are other data flows also going through the switch, that could obviously change things, and the sending computer could drop packets if there's jitter in how quickly the application produces them, but it seems impossible for the sending computer to burst packets into the switch any faster than it can handle because all the incoming packets are coming over the same 10G link. Not an expert here, legitimately curious. reply hinkley 16 hours agorootparentPeriodic background traffic like DHCP and background noise causing packet loss. You can’t run a queue at 100% and have any expectations of latency. In fact the rule of thumb from queueing theory is 50% to avoid latency spikes. reply kristjansson 13 hours agorootparentI mean it's not that hard to eliminate all other traffic on a closed network like that, at least where there's millions of dollars at stake. Must be nice to open Wireshark and see _nothing_. reply hinkley 10 hours agorootparentThat would be highly uninteresting to the rest of us don’t you think? reply bitcharmer 15 hours agorootparentprevThe nature of this kind of traffic is that it's pretty bursty. Think 100x-200x the normal packet rate in the span of a millisecond. Perfect opportunity for drops. Ultra low latency switches have tiny buffers. reply hinkley 16 hours agorootparentprevPeople get weird about the word “saturate”. I think GP is switching to “max sustainable” and expecting everyone else to come along for the ride. Queuing theory has many many bad things to say about actual saturation. reply mistrial9 14 hours agorootparentadd \"carrier sense multiple access\" packet creation reply aftbit 17 hours agorootparentprevAre people still using 10G in PROD? I thought 40G and 100G had generally replaced that. I have 10G cards in my homelab that are a decade old and cost less than $100. reply latchkey 16 hours agorootparentWe are 400G to the machines and 800G on the splines. reply aftbit 15 hours agorootparentCool! Even faster than I realized. What application(s) can actually push 400G through a machine though? reply latchkey 15 hours agorootparentData transfer for training. It goes directly to the GPUs via RoCE. That said, we also stack the boxes with a bunch of NVMe as well, so you can cache there first, if you want so that you don't have to worry about network bandwidth as much. We're flexible on customers needs. When 800G nic's come out next year (along with PCIe6), we will start buying those as well so that it is 800G everywhere. Let's see how long that lasts before it is considered slow... heh. reply oarsinsync 10 hours agorootparentprev10G serialisation delay is lower than 40G or 100G. Most markets can disseminate their feeds on 10G effectively. This isn’t true of the major US exchanges. reply bitcharmer 15 hours agorootparentprevAlmost all exchanges disseminate market data over multicast these days. If you miss a tick it doesn't matter because by the time a tcp retransmission completes this is old, useless data. reply georgelyon 17 hours agoparentprevI ran into this problem a while back working at a company that was working to distribute video streams with low latency (lower than Low-Latency HLS) to a large number of viewers. Initially a prototype was built on top of AWS with fan-out/copying and it was terrible. This was partially due to inefficiency, but also due to each link being a reliable stream, meaning dropped packets were re-broadcast even though that isn't really useful to live video. Moving to our own multicast hardware not only greatly improved performance, but also greatly simplified the design of the system. We required specialized expertise, but the overall project was reasonably straightforward. The biggest issue was that now we had a really efficient packet-machine-gun which we could accidentally point at ourselves, or worse, can be pointed at a target by a malicious attacker. This 1-N behavior of multicast is both a benefit and a significant risk. I really think there is opportunity for cloud providers to step in and provide a packaged solution which mitigates the downsides (i.e. makes it very difficult to misconfigure where the packet-machine-gun is pointing). My guess is that this hasn't happened yet because there aren't enough use-cases for this to be a priority (the aforementioned video use case might be better served by a more specialized offering), but exchanges could be a really interesting market for such a product. It would be pretty efficient to multi-cast market state in an unreliable way, and have a fallback mechanism to \"fill in\" gaps where packets are dropped that is out-of-band (and potentially distributed, i.e. asking your neighbors if they got that packet) reply vegardx 20 hours agoparentprevIn AWS you don't even do neighbour discovery through ARP. Or that's a lie, you do, you get a arp reply, but it's not from any of your devices on the network. And traffic is authenticated and authorized at both the source and destination, so you can't do fun things like manipulating arp tables. You get a lot of nice features when you have a fully software defined network, but it comes with a couple of caveats, like you mentioned here. I doubt we'll ever see \"real multicast support\" in the sense that network engineers are used to. reply pclmulqdq 20 hours agoparentprevUnless you build your network for it, multicast is a huge pain in the ass to administer. None of the big cloud providers built for it at the scale that traders use it, and I think they prefer things that way. When customers want it, they all just fake it by doing fan-out unicast. reply rcarmo 14 hours agorootparentAll hyperscalers have an SDN that essentially spoofs local ARP/DHCP inside the hypervisor and does not support broadcast or multicast by design (there are some caveats here, since some telco protocols that require them can be made to work). reply haseeblums 11 hours agoparentprevHere is a research paper I recently wrote about a fair and scalable multicast in the cloud: https://arxiv.org/abs/2402.09527 I would love some feedback! reply secondcoming 20 hours agoparentprev> lack of real multicast support Yup, this is a problem for us in GCP today even outside of trading. I don't know how Pub/Sub works for them. reply pclmulqdq 20 hours agorootparentPub/sub systems in unicast-only environments are very complex distributed systems to handle the load involved in fan-out routing while maintaining a global order. I had an interviewer once get annoyed with me for suggesting using multicast to solve the fan-out part of a pub/sub system, which made the global ordering part small and simple. We lost a lot by thinking of HTTP as the one true level of network abstraction. reply amluto 19 hours agorootparentA reliable multicast network that preserves global order even during maintenance and doesn’t drop packets is not something you will find off the shelf. A reliable multi-tenant multicast network also appears to be a rare beast. I’ve only heard of it in finance, and that’s only because it’s private and expensive and all the participants need to be generally nice to each other because it’s a repeated game and the operator can literally pull the plug if the rules are broken. reply hinkley 16 hours agorootparentThere was a time in the 00’s where a lot of server hardware had 3 NICs and you could use those for redundancy but a better use was to create three networks: inbound, service and database calls, and administrative. You had more control over your services talking to each other and control plane tech, thus could make some more guarantees than with inbound data. Don’t cross the streams. reply pclmulqdq 18 hours agorootparentprevDo you regularly allow untrusted machines onto your private pub/sub instances? I'm not sure the \"operator pulling the plug\" part is unique to the finance industry. Also, yeah, you have to do some engineering around your multicast distribution to make a pub/sub system, but multicast pretty much solves the data rate scaling problem - you are now basically O(1) in the number of connected subscribers. reply lucianbr 20 hours agoparentprevI find it sad that equal access between the entities doing HFT and regular Joes is not required for fairness, but god forbid one HFT having some milisecond advantage over another. That would be unfair. Can't have that. reply SJC_Hacker 20 hours agorootparentBecause average Joes don't do algorithmic trading, and if they do it not at the level that HFT does. Not even all the big financial players care about HFT and millisecond timing, so they're in the same boat. reply Kranar 18 hours agorootparentprevWhat do you think an Average Joe is going to do with that extra millisecond available to them? reply lucianbr 17 hours agorootparentI think Average Joe has quite a lot of disadvantages compared to some hedge fund or whatever it is that does HFT. Do you really think the only difference between them is a millisecond? That's only between HFT traders. My point was that other advantages/disadvantages are not being cared about, not that we should provide milisecond access to Average Joe. reply Kranar 17 hours agorootparentOf course the Average Joe has disadvantages compared to a hedge fund or people who are experts in a field and spend the bulk of their life dedicated to some aspect that the Average Joe is not dedicated towards. If Average Joe wants returns comparable to these hedge funds, then they should stop trying to time to market and instead stick to diversified ETFs and stop worrying about millisecond differences in the stock market. Believe it or not, if Average Joe does that they can actually beat most hedge funds over a long time horizon [1]. https://www.cnbc.com/2018/02/16/warren-buffett-won-2-point-2... reply lucianbr 17 hours agorootparentI mean, you could also write \"of course whoever has servers closer to the exchange can do HFT better\". \"Of course companies that invested a lot in having servers closer will reap the advantages.\" No, expertise is not the difference. If you're a private person with 100 years experience in trading, you still can't do HFT. You need to be an instutition, have lots of capital to invest in servers, software development maintainance etc. As a private person I think you don't even get access to the API. \"Of course whoever has more capital has advantages in the market\"? Of course they do, but I don't think \"of course they should\". For this discussion, that funds don't beat the market average over a long term is irrelevant. Why not say \"who cares you get more latency than the other bank, if you want money just invest in S&P500 and long term you'll beat them\". But you don't apply that to banks against banks, only to banks against Joe. Why? reply Kranar 16 hours agorootparent>of course whoever has servers closer to the exchange can do HFT better Can do better at what? Can get their trade in the order book faster? Yes they can. But does that automatically mean they will make more money? No it does not. >If you're a private person with 100 years experience in trading, you still can't do HFT. Of course not, 100 years ago there were no computers. Having 100 years of experience in trading on the pit would not give you any expertise in software development. Someone with 10 thousand years of experience plowing can't compete against someone with a tractor. That's kind of the point of the tractor... I'm sorry that it disturbs you that the Average Joe sitting at home with his discount online brokerage account is unable to gain the same kind of benefits putting out individual orders here and there on speculative stocks that he likely knows nothing about, that hedge funds, institutions, and other highly specialized and skilled professionals are able to gain by doing this for a living. The Average Joe does have access to highly diversified and low fee ETFs, and as I said the Average Joe can reap almost all of the rewards that the best hedge funds and banks do by sticking to those instead of trying to play the market. reply lxgr 9 hours agorootparentprevRegular Joe is so bad at trading stocks that hedge funds literally give him a discount on the national best bid/offer for the privilege of being allowed to trade with him. Giving retail traders access to the \"actual market\" would most likely result in worse execution on average, according to some studies. reply RunSet 14 hours agorootparentprevLikewise, banks chronologically rearrange the transactions in checking accounts to maximize overdraft fees. Yet when I suggest batching and chronologically randomizing the transactions on exchanges to reduce the benefits of low latency / centrality, people behave as though I have transgressed against Moloch. reply msarrel 17 hours agorootparentprevI think it's just beautiful how you made this point and other people couldn't even understand the concept of equal access to data and trading platforms. Yes, that's your point exactly. reply bitcharmer 15 hours agorootparentprevIt seems you're confused about how competition works among hft shops. There is no regulated, same latency for us. We compete for faster access just like everyone else. reply KiranRao0 21 hours agoprevOne key consideration is “provable fairness”. It’s my understanding that exchanges use techniques like long, same length fiber optic cables to all racks within the exchange datacenter to convince customers that everyone is on a fair playing field. This is a lot harder to do when a server is virtualized somewhere on some rack on EC2. Exactly as mentioned, people will try to optimize by spinning up/down instances as close to the exchange server as possible. Customers will be unhappy because they can’t prove that it’s fair, even if they have the closest server. Overall great, thought provoking writing btw reply aynyc 20 hours agoparentIt's provable that it's not fair. AWS multicast is software based, not hardware based. reply vegardx 20 hours agorootparentI think the lines between software and hardware-based are a little blurred these days with accelerator cards and whatnot. It's just a lot harder to come with the same level of guarantees when you're basically running a hypervisor on top of it. reply bitcharmer 15 hours agoparentprevThe only exchange I know that does the cable in a box trick is IEX. Everyone else is based on \"the closer, the better\". Colocation is king. reply re-thc 21 hours agoparentprev> This is a lot harder to do when a server is virtualized somewhere on some rack on EC2. There are bare metal EC2 instances. reply checker659 21 hours agorootparentIt's about the interconnect and the proximity. reply blibble 20 hours agorootparentand not having a for() loop doing multicast fanout in software which sounds like what AWS Transit Gateway is reply Shrezzing 20 hours agorootparentprevAt some point, someone has the shortest route connecting to the exchange's bare metal EC2 instance, and that organisation has a significant advantage in high frequency trading. reply jstsch 21 hours agoprevNice article. Wondering though why trading is not done in discrete batches, e.g. 5 second intervals? Trades in the same interval get filled equally or stochastically? Info about trades with that same 5 second batch delay? Is there some (theoretical) market efficiency thing at play? All this HFT feels wasteful and bad for 'regular' human investors. reply misja111 21 hours agoparent> All this HFT feels wasteful and bad for 'regular' human investors. Quite the opposite, thanks to the tough competition the market makers are setting the bid/asks spreads as minimal as possible. Which leads to less costs for human investors, pension funds, insurance companies etc. I used to be a market maker in the 90's before HFT took off. The margins we kept sometimes felt like a rip off but customers had no other choice but to accept them. People who ask for transaction fees, forced delays in executing or whatever, tend to forget that these force market makers to increase their spreads, which means customers eventually pay the price. reply Shrezzing 20 hours agorootparent>Quite the opposite, thanks to the tough competition the market makers are setting the bid/asks spreads as minimal as possible. Which leads to less costs for human investors, pension funds, insurance companies etc. It's not automatically the case that the disappeared margins & thinning of bid/asks have been shared equitably between the trading firms and customers. Take two exaggerated markets for example: 1) No HFTs: The customer wants 100 shares in Company A. The shares are available on two exchanges, one at $100, and another at $105. A market maker charges the customer $5 to access the 100 shares at $1 each. The customer pays $105. The market maker earns $5. 2) With HFTs: The customer wants 100 shares in Company A. The shares are available on two exchanges, one at $100, and another at $105. The customer clicks \"buy\" on their trading platform, the HFT races to the $100 shares, and purchases them, then fulfills the order at $105. The customer pays $105. The HFT firm earns $5. For the end-customer, all that's happened is the margin goes to another firm. The consumer still has no other choice but to accept these transaction fees. There was arguably a need for HFTs to reduce the market-makers exorbitant fees in the 2000's, but that requirement has been served, and the technology now exists to remove both from the market entirely. HFTs are a rent-seeking entity interjecting in a market which, at least in theory, exists to most efficiently allocate capital to the productive benefit of all. reply thinkharderdev 19 hours agorootparentIn the United States at least both scenarios you mentioned are illegal. Market makers are not just sitting in the middle of orders. They buy without a seller lined up and then fill orders from their own inventory (or route orders to an exchange in the case where they can't fill a buy order from their own inventory). In cases where they route to an exchange they are required by law to fill the order at the lowest price available. Typically they fill orders at better prices than what you can get on an exchange. So you, as a retail investor, are actually getting better prices than you would if your broker just filled orders on an exchange. How the price improvement gets allocated is complicated. Some of the price improvement goes to the broker (in the form a payment-for-order-flow) and some goes to the actual investor (you). But in either case the retail investors are strictly better off. reply Shrezzing 18 hours agorootparentLatency Arbitrage still exists in a world with NBBO regulations. Research consistently finds that not only does the strategy work in theory, but that it is consistently put into practice by HFT firms to the detriment of other market participants. If a firm can calculate the NBBO ahead of other market participants and the market regulator, it can still legally front-run the market, and risklessly extract rents from end-customers. The NBBO formula is not computationally expensive, and its underlying data is necessarily publicly available to all trading firms. This occurs in the real world, in the order of $billions annually. The UK's Financial Conduct Authority: >We use stock exchange message data to quantify the negative aspect of high-frequency trading, known as “latency arbitrage.” The key difference between message data and widely-familiar limit order book data is that message data contain attempts to trade or cancel that fail. This allows the researcher to observe both winners and losers in a race, whereas in limit order book data you cannot see the losers, so you cannot directly see the races. We find that latency-arbitrage races are very frequent (one per minute for FTSE 100 stocks), extremely fast (the modal race lasts 5-10 millionths of a second), and account for a large portion of overall trading volume (about 20%). Race participation is concentrated, with the top-3 firms accounting for over half of all race wins and losses. Our main estimates suggest that eliminating latency arbitrage would reduce the cost of trading by 17% and that the total sums at stake are on the order of $5 billion annually in global equity markets https://www.fca.org.uk/publication/occasional-papers/occasio... The University of Michigan's Economics department: >We illustrate this process and the potential for latency arbitrage in Figure 1. Given order information from exchanges, the SIP takes some finite time, say δ milliseconds, to compute and disseminate the NBBO. A computationally advantaged trader who can process the order stream in less than δ milliseconds can simply out-compute the SIP to derive NBBO,a projection of the future NBBO that will be seen by the public. By anticipating future NBBO, an HFT algorithm can capitalize on cross-market disparities before they are reflected in the public price quote, in effect jumping ahead of incoming orders to pocket a small but sure profit. Naturally this precipitates an arms race, as an even faster trader can calculate an NBBO* to see the future of NBBO, and so on. http://strategicreasoning.org/wp-content/uploads/2013/02/ec3... The Bank for International Settlements: >Conservative estimates suggest that at least 4% of dark trading occurs at stale reference prices. High-frequency trading firms (HFTs) almost always benefit from such stale prices, being on the profitable side of the trades between 96 and 99% of the time. Furthermore, stale trading does not happen at random but is driven by the behaviour of HFTs. HFTs as a group almost never provide marketable liquidity in the dark and rather behave strategically to exploit their speed advantage by submitting marketable orders to execute against stale quotes. https://www.bis.org/publ/work1115.htm reply gpderetta 18 hours agorootparentWhat you described is not latency arbitrage. reply WiSaGaN 7 hours agorootparentprevThe cause of latency arbitrage is not HFT, it is the fragmentation of liquidity. reply mtoner23 20 hours agorootparentprevThis is entirely false and ignores Reg NMS. Everyone must execute at the NBBO. As well customer orders are often given better and tighter prices than other market participants. HFT firms will often offer them better than NBBO prices. As well, none of these prices you quote would not exist without market makers, its just now the fact that to be a market maker you must be an HFT firm as well due to the scale that is now required. reply Tesl 21 hours agoparentprevThis is how the Taiwan exchange used to do matching, and I still think it's the best system I've seen. I don't think the reason has anything to do with price discovery, it's just because exchanges want to maximise their trading fees. Continuous order book trading leads to more trades and hence more profit for the exchange. reply sparsely 21 hours agorootparentThey also charge differently (extortionately, some might say) for different speeds of data feed, although I'm not sure if they have tiers just for HFTs. reply sparsely 21 hours agoparentprevThe HFT is wasteful but isn't bad for human traders, they tend to get better prices. It's bad (sometimes) for huge investors (VHNW individuals, hedge funds, pension funds which I guess represent regular people) that want to make large trades without moving the market but there are also winners here - e.g. if Johnny the day trader buys a stock that Texas Teachers Fund is selling huge batches of, he's better off if HFTs are causing price changes to propagate more quickly. reply senkora 19 hours agoparentprevThose batched trades are called “auctions” and they are a part of many exchanges. I think it’s pretty uncommon to do them every N seconds. A common pattern is to collect quotes before the market open, do an “opening auction” to set the opening price, and then switch to continuous trading for the rest of the day. If trading in a stock ever pauses (which can happen for a variety of reasons) then another auction occurs when trading is restarted. reply jeffreyrogers 16 hours agoparentprevIIRC they have done trials of this on some exchanges. It didn't make a big difference either way. The opening and closing auctions are already sort of done the way you describe. reply cosmic_quanta 20 hours agoparentprevThis is how wholesale electricity is traded, although for unrelated technical reasons. Bids and offers are collected for auctions that happen at regular known intervals, for example every 15min. reply blitzar 16 hours agoparentprev> All these people in tech optimising clicks, ads and engagment feels wasteful and bad for 'regular' humans. reply snapcaster 21 hours agoparentprevI guess the argument would be that this limits price discovery? I hear this proposed a lot and haven't heard super compelling arguments against it reply marcosdumay 21 hours agorootparentThe reason the GP is proposing it is because it limits price discovery. IMO, if you have a problem with limiting it to 5 seconds long quanta, you are doing something wrong. reply kasey_junk 21 hours agoparentprevHow do you tie break? If there are more sellers than buyers (or vice versa) at the clearing price? reply xnorswap 20 hours agorootparentThe same way you would without a clock I guess? You could match what you can distributed equally and leave the rest unsettled. You could let people decide whether to roll-over the partial bid into a new bid on the next clock or to cancel unsettled. You could clock to something both very fast on a human scale (50ms), quick enough it'd still feel instant but slow enough that it could reduce HFT silliness and need for extreme low latencies. reply WiSaGaN 20 hours agorootparent> You could match what you can distributed equally and leave the rest unsettled. Equally per market participant? Do large participant like banks trade same amount as retail investor one trade at a time? Per quantity? HFT will time the end of the interval and decide to place a large order or not. reply xnorswap 19 hours agorootparentIt would be weighted by bid size. If there's $10m of bids one side and $5m of offers on the other, you match up the $5m on that side and every bid gets 50% settled. I'm not sure I understand the problem with \"waiting\" for the end of the clock. The pool wouldn't be public so you couldn't get knowledge inspecting the pool. All bids and offers would be published on the clock and settled by weighing all the bids and offers against each other and matching by volume. The trickier issue is what happens in this scenario (assuming limit orders): Person A bids for 500 units @62 Person B offers 100 units @61 Person C offers 400 units @60 Clearly there needs to be full settlement, we have a bidder who wants to buy 500 units at a price which sellers are happy to sell at. Correct me if I'm wrong, but in a traditional market it would depend on the order they came in. Here we would need a formula to work out the correct settlement price. Intuitively this ought to be somewhere just above 61. ( If it were just two people, a bid at 62 and an offer at 60, you could intuit a fair settlement would be 61. ) I'm sure fair formulae can be derived however. reply WiSaGaN 18 hours agorootparentThe general rule is HFT will have the elite level mathematicians to figure out what is the most optimal strategy is, and the most exotic hardware to implement it to the extreme. Other party will fall further behind given the more complex and unconventional exchange rule. reply gpderetta 18 hours agorootparentprevYou trade latency arbitrage with statistical arbitrage were participants try to estimate the market and overbid to try to capture as much of the market as possible. That seems dangerous and unstable. reply kasey_junk 14 hours agorootparentprevPer rata matching is already a thing in some financial markets. They tend to be _more_ latency sensitive as size gets inflated to game the matching algo, thus risk being inflated and thus the value of timely cancels. reply renewiltord 17 hours agorootparentprevThis is all gameable. Like he said, you just time it with an over-large order and let the remainder expire. Everyone gets 10% of their order but my order is 10x what I actually want so I actually get 100%. reply jeffreyrogers 16 hours agorootparentprevThey already do this for the opening and closing auctions. You can have a market-on-close order or limit-on-close order for example. The market on close orders are guaranteed to fill. The limit orders are filled using price-time priority, so best prices submitted earliest fill first, after the market price orders. I guess it is possible that there are remaining marketable orders that never fill because of an imbalance one way or the other, but I doubt that ever happens in practice. reply kasey_junk 14 hours agorootparentIf you keep price/time priorities you still get a race to pile into new levels after the previous batch. reply jeffreyrogers 12 hours agorootparentThat's what happens with the opening and closing auctions currently. reply kasey_junk 12 hours agorootparentYep. They are more latency sensitive than the continuous portion of the day. reply crote 20 hours agorootparentprevFlip a coin. reply ramon156 21 hours agoparentprevShareholders would fume hearing this reply IshKebab 21 hours agoparentprevIf you think about it you can never eliminate the advantage of being faster. If you do 5 seconds batches it just means the edges of the batches become the time-sensitive points. If you want to kill HFT you can do it directly via very very small transaction fees. But guess how popular that is... reply gpderetta 2 minutes agorootparentExchanges already extract per order commissions. You do not pay per message (so add, cancels and amends are free, you only pay when you get traded [1]). A per-message would probably significantly affect existing strategies and greatly increase spreads, but I don't think it would prevent all forms of ULL trading. [1] But even there exchanges offer rebates, if not outright incentives, for market makers to provide liquidity. reply bloak 20 hours agorootparentprev> If you think about it you can never eliminate the advantage of being faster. If you do 5 seconds batches it just means the edges of the batches become the time-sensitive points. You mean you can never completely eliminate the advantage? But mostly eliminating it might still be useful? Suppose the rule is that if you get your request in by 01:23:45 then it gets handled in the following 5-second period and the response is sent out at 01:23:50. Does someone (A) who finalises their request at 01:23:44.9999 and gets the result back at 01:23:50.0001 have an advantage over someone else (B) who has to finalise their request by 01:23:44.8 and gets their result back at 01:23:50.2? Yes, certainly, but it doesn't seem to be much of an advantage ... So person A can take account of exciting news that arrives at 01:23:44.9, while person B can't, true, but when it comes to reacting to other trades, person A has 4.9998 seconds to think about the news, while person B has 4.6 seconds to think about it, which doesn't seem like a huge difference. Compared to how things work today. reply chardz 21 hours agorootparentprevYou can certainly alleviate the disadvantage of being slower though - and that’s exactly what literature in batch trading argues. reply xrd 20 hours agorootparentprevIt's funny that you say that, with the crypto transaction fees still a big problem. Feels like HFT and crypto are on a convergence towards that concern. reply SkipperCat 18 hours agoprevWhy would the exchanges want to move their colos into the cloud. They've spent the capex and they can charge lots of money to rent data center space, cross connects and other services to their customers. If they moved to the cloud, all that revenue would go from them to AWS/GCP/etc. Doesn't seem like a profitable move for the exchanges. They make more $$$ with on prem setups. reply lulznews 9 hours agoparentSomebody needs a project to get promoted … reply ch33zer 17 hours agoprevI used to work in machine maintenance, so I always think about what will happen when the machines involved fail. NYSE machines hosting the trading server fail: presumably they have hot backups they're ready to switch to but that takes time and will interrupt trading during the cut over. Not to mention that not all failures are hard failures, what if the NIC is downtrained to a lower speed, RAM is slower than it should be, or a single hard drive storing important data crashes? Lots of interesting failure modes. When the NYSE owns their own machines they can handle these cases directly. When they don't and Amazon is responsible for repairing these machines it might take a lot longer to get things fixed. I hope NYSE is thinking about hardware failures and building a system to check performance of their trading servers before letting them become the active host. Thinking about failures on the side of the traders: basically if they get unlucky then there could be delays as Amazon rerprovisions them replacement servers in the case of failures. This likely impacts what trading strategies are viable, and could cause them to lose money if machines fail at unlucky times. reply rcarmo 14 hours agoparentThey literally have triple hardware redundancy. They can afford it. reply onionisafruit 21 hours agoprevIf a big exchange goes to the cloud it won’t look like a regular company setting up an aws account and getting a bunch of ec2 instances in us-east-1. They would at least have dedicated racks. I suspect the provider would end up with a plan where traders can get servers that all have the same network distance from the exchange’s nics (down to the same length of fiber). reply SamuelAdams 21 hours agoparentHonestly for something this niche I wonder if AWS would make a new region, like gov-cloud or secret. The goal would be to use tech that can accommodate ULL deployments. reply posnet 20 hours agorootparentThis seems to the most likely, and it's not unheard of, they have 'local regions' like osaka in Japan. Alternatively they could just stick a bunch of Outpost racks in the NYSE/NASDAQ data center and create an 'eXn.8xlarge' instance type and charge 100$ an hour. reply kikimora 19 hours agoparentprevI think and exchange would use AWS Outposts to get hardware in their data center. reply gpderetta 15 hours agoparentprevBut at that point it would be just another colo, right? reply notyourwork 22 hours agoprevNothing to add but I found this to be a well written thought exercise on a space I realize I know very little technical detail of. Thanks for writing this! reply arcza 22 hours agoparentThank you. I'm only happy to hear this was helpful. I'd also love comments from any HDL quants about how portable FPGA code would be to virtualized environments and what other languages might stand out particularly well for a VM stack. reply pclmulqdq 21 hours agorootparentThere are cloud FPGAs, but they are offered as a compute accelerators, and have no access to the network. Trading FPGAs need network access for latency. reply KiranRao0 21 hours agorootparentIm also sure that if theres enough customer demand (from people willing to spend $M), AWS will make network connected FPGA happen. reply pclmulqdq 20 hours agorootparentI'm sure they won't, at least without a lot of development. FPGA networking used for trading is borderline abusive of networking protocols, and I assume that Amazon doesn't want that on their production network. reply _benj 21 hours agoprevThis is an interesting read but I think it leaves outside what kind of trading is the one that would benefit from ULL. ULL and currently HFT seems to be very useful for market making (buying the ask and selling the bid and profiting from the bid-ask spread making parts of a cent per transaction, done a few million times a day), but there are other uses for HFT. One of them would be to execute very big orders over time to instead of drastically rising the price of the security they can get a better cost basis by performing a set of trades, letting the market absorb the impact and continuing with the order. The thought of having the market in a cloud provider like AWS scares me! Although I’m sure that AWS might have pitched the idea already. If the markets could be controlled by a private company that could schedule “maintenance” at convenient times for them, that sounds like a recipe for market manipulation bay trillion dollar company. Sounds like something the SEC wouldn’t stand for. reply minimax 18 hours agoparentYour description of market making is backwards. If you're buying the ask and selling the bid, you're paying the spread not collecting it. reply _benj 12 hours agorootparentThanks for the correction! reply kristjansson 17 hours agoprevI like the assumption that NYSE would just grab some EC2 instances and run an exchange on them, and that AMZN wouldn't bend several directions at once to deliver new products that just happen to exactly replicate the environment they're 'leaving'. reply eschneider 20 hours agoprevWhy on earth would exchanges go full cloud when running the trading infrastructure reliably and predictably is their whole reason for existing? reply nhourcard 21 hours agoprevAbout the market data & monitoring element, beyond the usual suspects Clickhouse and kdb+ - worth noting that Aquis mentioned in this article uses QuestDB ( https://questdb.io/case-study/aquis/) reply mikewarot 14 hours agoprevIt's my opinion that stock exchanges should batch trades every 30 seconds, or longer (depending on the market), so that millisecond arbitrage becomes impossible. Front running the market in any manner should be illegal. reply seanhunter 14 hours agoparentYes. There is actually some research into this idea where markets would effectively conduct rolling auctions, but I'm struggling to find it at the moment because I'm in a work meeting. Iirc the evidence suggests this would reduce market dislocations when news comes out etc so would generally improve price discovery. Markets already conduct an opening and closing auctions and conduct an auction to resume after a volatility break (what people often call a \"circuit breaker\" in the press although it's a volatility break) so this would not be as much of a technological lift to implement this as it may appear. How it works from a practical perspective is the exchange suspends matching for a period (so say 30mins) but order placement still works. Then when the market comes out of suspension a single print runs to uncross the order book, and everyone who submitted an order which matched gets executed at a single price. So as you say timing arbitrages of the current kind are effectively impossible. So in the case of a rolling auction you would do that print and then immediately suspend matching again and do another auction. Here's some background on how auctions work in financial markets in general but it's not the specific paper I was referring to https://www.princeton.edu/~jkastl/auctions_finance.pdf reply pclmulqdq 10 hours agoparentprev10-100 millisecond auctions have been proposed seriously. 30 seconds is pretty long. If you would like a counter-argument, the options markets essentially trade on an auction basis: every time someone* sends an order that crosses a spread, there's an auction that does price discovery of the security. That does not promote having narrow spreads or transparent prices - options markets have huge spreads even for very liquid products. *someone who is not a market maker reply jwie 21 hours agoprevOrders should have some durability and it would probably change behaviors enough to make hft go away. If you list a buy or sell order it just has to be in force for some period of time, say a minute or something. HFT shops will say this would reduce liquidity, but it would only make clear what real liquidity was in the first place. reply smabie 20 hours agoparentIf orders has to been good for atleast a minute it would massively increase the spread (by like over a 1000x probably) Also, what's wrong with hft? reply tourist2d 20 hours agoparentprevWhy would anyone want that? Everyone would just quote a lot wider to make up for the potential volatility of the next minute, probably leading to worse trades for retail orders. reply Bluescreenbuddy 17 hours agoprevI work at a prop firm/mm. You mention cloud and you'll be taken back behind the chemical shed. reply yafetn 21 hours agoprevInteresting read, thanks! Some points: > Hogging instances might pay, if this stops competitors getting good hosts. This would eat into PnL and is also wasteful on energy. Aren’t reserved instances cheaper than spot? > Bad players could do a ping test to many thousands of EC2 instances, find those which are also at very low latency to their good boxes (assuming these are competitors), and DDoS them during trading hours to hammer the hypervisor’s NIC. This would result in critical overhead occurring for competitors sending orders out. Leaving out the logistics of how someone could do this (why are your instances reachable from the internet?), wouldn’t you have a good case with your exchange to get them kicked out? reply onionisafruit 21 hours agoparent> wouldn’t you have a good case with your exchange to get them kicked out? Would you know who is doing the pinging? The cloud provider would know what account was pinging, but somebody doing this as a trading tactic would have the resources to churn through aws accounts as quickly as they are banned. reply moomin 17 hours agoprevThe author of this article clearly knows a lot about the subject, but I think this would have been better titled \"Low Latency Trading isn't Going To The Cloud and Here's Why\". Or to put it a different way, infra peeps within exchanges have a very specialized skill set and priorities. General cloud infra peeps don't. No shade, but there's always going to be some business that doesn't make sense to switch to the standardized solution. reply allenrb 20 hours agoprevFun thought exercise, thanks! My question is, what advantage would a large exchange find in moving to cloud? They’ve already got the personnel capable of managing their environment. They’re not a rapidly-growing startup in need of flexibility. They’re large enough to get at least decent deals purchasing gear. “The cloud” will naturally expect to make a profit on the deal, which likely eats up (and then some) any savings which might otherwise be delivered. I “get” cloud in a lot of circumstances but it doesn’t seem to make much sense here. reply SJC_Hacker 19 hours agoparentWithout reductions in personnel, then none. That's essentially what you're buying from a cloud provider. Most of the time its not so much renting the hardware as renting their labor in maintenance. That is assuming your hardware needs don't have a wide enough variance from time to time (scale up/scale down) reply mcconaughey 19 hours agoprevAt first glance, it seems this would even the playing field. However, large players will allocate resources to spinning up instances and overloading machines. Similar to how we're seeing the DDOS shenanigans going on in crypto. Net-net, it still benefits startup quant shops and sophisticated independents. Most retail isn't doing HFT or really any quant. But for people wanting to have their own shops, this is a better version than having to build hardware and colo. reply hn8305823 19 hours agoprevIt's amazing to me that regulators have not required a minimum latency, or random latency dispersion in orders/trades to level the playing field. reply dusted 20 hours agoprevShowing that I don't understand economics while also telling that I don't understand economics: It would probably do the world more good to tweak the structures making ULL trading profitable anyway, it's not like the trading in and of itself brings any value to the broader world, while consuming enormous amounts of resources that could have been spent on actually improving systems that create real value. reply cosmic_quanta 20 hours agoparent> it's not like the trading in and of itself brings any value to the broader world This is a common sentiment, but the reality is that increasing market participation is good for everyone. Yes, even retirement funds benefit from the presence of market-makers. Liquid markets allow for better price discovery and cheaper transaction costs. reply dusted 20 hours agorootparentI specifically the high-speed trading. I definitely agree that actual investment and trading has some benefit. Nobody was helped by the 200 nanosecond thing that the machines did when the marked opened (except the owners of said machines, of course) reply johngladtj 20 hours agorootparentPlenty you people were, you just don't notice it. reply hackerlight 15 hours agoparentprevRiddle me this. If you got what you wanted, and these value destroying people went away, what would they be replaced with? reply dusted 3 hours agorootparentI don't want them to go away, I want their talents put to use towards creating broader value. reply h4kor 21 hours agoprevI'm a total trading noob. Can you explain why a low latency is worth so much? And how these traders \"exploit\" that advantage to make a profit? reply ioblomov 21 hours agoparentHigh-frequency trading is essentially a low-margin, high-volume play that exploits small price differences for profit. The most obvious example would be arbitraging the same security on different exchanges (buying low on one and selling pennies higher on another). Similarly, algorithmic models could exploit price volatility for individual securities on the same exchange. Under such conditions, fractions of a second can determine whether a given trade is a winning or losing one. reply toast0 18 hours agoparentprevMost exchanges prioritize their order book by price, then time. If you're a market maker, you (usually) want your orders to be selected. A common market making strategy is to issue a buy order a bit lower than the last executed price and a sell order a bit higher than the last executed price with the assumption that there's a lot of random and small price motion up and down. If you can consistently process order fills and update/replace your orders in the book faster than the other traders, you'll get more of the trading volume, and other traders will have to compete with you on price. For some stocks where the minimum price increment is large relative to share price, most market making traders will converge on the same buy and sell prices, so latency is it. There's also value in responding to filled orders in one venue at other venues. Many stocks have a 'home' exchange, but trade at many exchanges, if there's a significant price movement at one exchange, other venues will quickly follow, but if you can follow quicker than most, you can execute against the now mispriced orders on the book, etc. reply washedup 21 hours agoparentprevThis is extremely simplifying the nuance, but imagine there are two traders who want to buy what a single trader is willing to sell at a given price? Well, the first one to get to the exchange and \"lift the offer\" will get the price, while the second trader will have to pay a higher price. reply sgarland 21 hours agoparentprevReaction time to events, be it news, securities movements, etc. If you’re early, you can open positions before large movement has taken place; you can also close those positions precisely when you want. Sometimes this may be making pennies (x N shares), other times it may be quite substantial. reply smabie 21 hours agoparentprevWhat would you do if you knew the price of something 1 day before everyone else? How would you make money off that? reply lotsofpulp 21 hours agoparentprevIn any market, a seller or buyer willing to close the transaction quicker is a benefit for the opposing seller or buyer. Quicker can be on the scale of years to milliseconds, depending on what is being exchanged and amongst whom. reply re-thc 21 hours agoparentprevA delay could impact the price. Say you wanted it at $5.00 but it becomes $5.01 by the time your bid comes in you either miss out or pay more. These traders do it at high frequency. Imagine $0.01 x millions at a time. reply kjkjadksj 18 hours agoprevLets start with exchanges allowing for trades during roman catholic holidays reply minimax 21 hours agoprev> The NYSE runs out of a public data centre (called NY4) which is run by Equinix. No. NY4 is in Secaucus. NYSE operates out of an ICE (NYSE parent co) owned facility in Mahwah about 25 miles north of there. They managed to pick out the one big US equities exchange operator _not_ running in an equinix facility. Sorry but this whole post sounds like someone who is sort of HFT adjacent but doesn't really know what they are talking about. Sending orders at \"09:29:59.9999971 at the hope your order arrives at 100ns past 9.30am.\" What? reply pclmulqdq 18 hours agoparent> Sending orders at \"09:29:59.9999971 at the hope your order arrives at 100ns past 9.30am.\" What? This literally does happen, though. One of the things the hyperscalers have convinced the world is that precise time is hard. Precise time is easy if you are willing to pay extra for your hardware. Sub-10-ns precision is unremarkable when you use PTP. reply minimax 18 hours agorootparentIt doesn't happen. All the exchanges have a \"Day\" order type that you can send before 9:30 that will be live on the book when it opens at 9:30 (or transitions to the \"core\" session at 9:30, most US exchanges have a premarket session prior to that). The idea of having some sophisticated strategy that sends 100ns before 9:30 is nonsense. reply pclmulqdq 18 hours agorootparentAs far as I know, you're correct that this exact trade probably doesn't happen on the US exchanges - day orders do have a matching phase before market open, so it may be advantageous to slide in right afterward, but you likely wouldn't do it without knowledge of the state of the opening auction. However, sending things just a hair early for scheduled events to catch an exact time is a pretty well-known trick at this point. I remember complaining to the exchange that their clocks weren't precise enough for this to be reliable. reply JackMorgan 21 hours agoprevIEX Exchange is building a cloud-first stock exchange that uses the concept of \"slowed trading\" to eliminate some of the worst practices of HFT. They even use a 38 mile loop of fiber to slow down connections that are \"too close\". https://en.m.wikipedia.org/wiki/IEX reply pclmulqdq 18 hours agoparentI'm not sure IEX is cloud-first, that would be a recent development. Their 38 mile fiber gimmick is also kind of silly because they have to provide data to a consolidated feed with no delay. reply sesuximo 20 hours agoprevIf an exchange goes to AWS, would it suddenly look worse than its competitors? And would that hurt the exchange’s revenue? reply TacticalCoder 21 hours agoprevHow would any cloud offering deal with something like, say, the full options data feed, which is close to 40 Gb/s of binary packed goodies? You need both a very fat pipe and ultra-low latency: does the cloud, any cloud, offer that? Also: how often have you guys seen the stock market being down? What's the \"x nines\" availability of, say, the US stock market and US options feed? Now: do we wanna talk about the various cloud outages that made the news? Sometimes lasting hours? Also what I've seen with the cloud is websites are now displaying spinners everywhere, for the myriad of not-low-latency-at-all microservices often taking seconds to respond. And that'd be on an ultra low latency fiber to the home setup, with 2 Gb/s down (and the ISP really supporting that). Why the heck do I have to wait seconds for oh-so-many things to display in my browser, on a last gen Ryzen ultra-speedy machine, with a super fat and low-latency Internet pipe? The worst offenders being all those banking websites showing a balance of 0 instead of \"-\" or \"n/a\" while fetching my info: nearly gives me heart attack every single time. I take it it has to do with micro-services all contacting shitloads of other micro-services, all living in the not-low-latency cloud. The problem being compounded by an army, a generation, of programmers who have never learned anything about optimization or latency and who solve every problem they have with the only hammer they have: the cloud. All these programmers know are JSON (or, worse, XML)... I mean: JSON vs 40 Gbit/s of interrupted bit-packed binary feeds? How could these two world ever reconcile? Now I don't do HFT but I do trade options and I do it through a desktop app and that app also offers an API through which I can fetch prices, send orders, etc. It's a good old Java app. And it's more advanced than any website I've ever used. Can we please not enshittify everything with countless micro-services and JSON files in the cloud? reply toast0 18 hours agoparent> Also: how often have you guys seen the stock market being down? What's the \"x nines\" availability of, say, the US stock market and US options feed? Well, it goes down every afternoon :p and it's down on the weekends. More like seven sevens than nine nines. It's been a while since I noticed a story about a significant stock exchange disruption, but there's a lot of things going on there. Tickers are largely independent, so it's easy to shard, and exchanges do shard them; (operational) trading outages often affect only a single stock, or rarely a set of stocks. There are procedures for administrative trading halts on individual stocks or the whole market and procedures to resume trading during the market day. There are also procedures for resuming trading after an operational error halted trading; I'm sure most traders don't like brief outages, and exchanges do their best to avoid them, but they can happen and be resolved with out a lot of confusion because the procedures are known and manageable. There's also redundancy. If one exchange is having difficulty, there are many others that likely still work. Whole system events are usually not operations issues, but trading issues --- one or several participants placed weird orders, the exchanges processed them, and weird things resulted. 'Circuit breakers' have been designed in to pause trading when this happens. This is an intentional outage, and it's ok because it's intentional and the parameters are known. reply __alexs 20 hours agoparentprevOnly someone that both misunderstands microservices and HFT architectures could have this opinion. 100 Gb/s is possible on AWS via Direct Connect. reply smabie 20 hours agoparentprevWhat does cloud have to do with micro services and json? Crypto exchanges already run on the cloud and it works.. mostly fine? reply RyanHamilton 21 hours agoprevThere's no directly colocated hosting that I know of, but \"direct connect\" gets you close: https://www.equinix.co.uk/partners/aws reply netfortius 21 hours agoprevSee Morningstar's choice of being among the first customers of AWS Outpost. reply aynyc 20 hours agoparentOutpost is literally a AWS branded server rack that is installed into a data center, I don't think it's considered \"cloud\" in modern stack. reply netfortius 18 hours agorootparentThe end-to-end infrastructure (and parts of PaaS delivered within Outpost, from within the set of AWServices) is in fact a cloud \"extension\", with an on-prem leg, NOT an on-prem solution connected to the cloud. There are a lot of constructs which force the AWS cloud products/services usage, not the traditional on-prem ones. reply aynyc 17 hours agorootparentThe outpost deployment in financial services that I've seen is the opposite in cloud extension. These companies want to say they are cloud-enabled, rather than push a full stack onto the AWS regions, they essentially buy AWS outpost rack as a way to extend their on-prem environment to the cloud (mainly S3). reply munk-a 17 hours agoprevLow latency/rapid trading seems to have added no value to the stock market and eroded a large portion of the fair market evaluation of companies. Whether it's technically possible or not it's likely we'll need to artificially add delay into the system - ideally that can be done in a way that makes it fair to traders that have a naturally high latency connection to even the playing field. reply maerF0x0 17 hours agoparentThe claim is they play marketmaker (they hold stocks for short periods of time so your trades execute faster), they obviously want a profit incentive for doing so. reply kstrauser 17 hours agorootparentThat begs the question by assuming that trades executing faster are a good thing to optimize for. Is the stock market for investing in a company, or for extracting money from momentary fluctuations? Those seem to be mutually exclusive. reply anamax 13 hours agorootparent> Is the stock market for investing in a company With the exception of the IPO and buy-backs, the stock market is NOT for/about investing in the company. When you buy IBM on NYSE, IBM doesn't get anything. Similarly for selling stock. If IBM doesn't see any money from a trade, how is that trade an \"investment\" in IBM? Stock trading is trading partial ownership. That's very different from investing. In other news, there's no money in the stock market. The money that you pay for IBM stock does not go to the \"stock market\". It goes to whomever owned the stock that you bought. reply pas 21 hours agoprev... is there some movement toward \"upload the strategy and let the exchange run it\"? which would provide a more level playing field, reduce energy and hardware costs, etc? reply dchftcs 21 hours agoparentYou mean enabling arbitrary code execution from a third party when you can lose billions of people's money in half a second? Also if two people want to make the same trade, who gets it? Exchanges do provide very limited special conditional execution instructions such as peg orders or stop orders, but it seems like a hard problem for them to support anything more sophisticated and general. reply infecto 21 hours agorootparentI agree with your sentiment but I would clarify that historically some exchanges did indeed create custom order types for larger clients that were not always public knowledge. I think that has mostly been eliminated but there are still a range of unique order types you can utilize depending on the exchange. reply pas 20 hours agorootparentprevIt doesn't have to be arbitrary machine code. eBPF / WASM coupled with a standard library supplied by the exchange. (Plus the exchange can run it in a VM.) > Also if two people want to make the same trade, who gets it? Whoever pays more currently, right? So it can be uniform random and folks can pay for better than random chance, etc. reply smabie 20 hours agorootparentOr we could just.. not do that? It provides no benefit and a host of downsides reply pas 19 hours agorootparentIt seems a lot more elegant/efficient/sane to me than trying to squeeze more and more racks into one building. So that's why I'm asking, as I think the benefits are clear, much better scalability, fairness (or getting as close to it as the exchange wants), cheaper (no need for fancy hardware), probably it would attract more market participants (lower barriers to entry). It's the same hypothetical \"EC2 model\" without the meta-game of trying to get closer to the cores the exchange runs at a given time. Can you elaborate on the downsides besides security? reply dchftcs 17 hours agorootparentWhat you're trying to eliminate is just a small part of the infra arms race. But it's not even effective at that. You say one of the goals is to not have to squeeze many racks into a building, but your proposal assumes the servers placed in the racks are running at low capacity, which is completely not true. On the other hand, if you trade say GOOG, the matching engine for that is going to mainly sit in one place, how do you stuff everyone's program to run on the same server, and how do you make sure they each have fair access? reply yafetn 21 hours agoparentprevI wonder if that’d make them a broker and not an exchange. Different set of rules, regulations, and licenses. reply kwhitefoot 21 hours agoprevLow latency trading should be forbidden. The exchanges should add random amounts of latency to every trade. reply infecto 21 hours agoparentWhy? Historically spreads have been much wider, I for one appreciate how close they trade now. Instead of a guy on the floor taking dollars from you, you have a bot taking pennies. Even large funds like Vanguard have said that the current system has reduced costs in net. reply arcza 21 hours agorootparent> Instead of a guy on the floor taking dollars from you, you have a bot taking pennies. ^ excellent way to put it reply smabie 20 hours agoparentprevThey do it's called exchange jitter. And you still have HFT reply hoseja 20 hours agoprev [–] Cloud of rapidly expanding ionized gasses, yeah. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The document examines the implications of exchanges transitioning to the cloud on low latency trading in stock, FX, and commodity markets.",
      "It delves into the challenges and benefits for various stakeholders like high-frequency trading firms and small-scale traders.",
      "The discussion includes setting up low latency trading strategies, risks, and potential outcomes, highlighting winners and losers, such as the UK stock exchange Aquis and the influence of AWS in the sector."
    ],
    "commentSummary": [
      "The debate delves into the challenges and benefits of low latency trading, focusing on cloud services and multicast support for market data.",
      "Various topics covered include the effects of high-frequency trading on market efficiency, latency arbitrage issues, fairness in finance, and solutions like batch trading or auctions.",
      "Discussions also entail moving exchanges to the cloud, utilizing FPGA code in trading, and distinct trading strategies for high-frequency traders, emphasizing the intricate nature of market dynamics in high-speed trading."
    ],
    "points": 180,
    "commentCount": 165,
    "retryCount": 0,
    "time": 1713266892
  }
]
